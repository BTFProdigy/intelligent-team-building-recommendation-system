Varying Cardinality in Metonymic Extensions to Nouns 
Helmut Horacek
Universit?t des Saarlandes
F.R. 6.2 Informatik
Postfach 151150
 D-66041 Saarbr?cken, Germany
 email: horacek@cs.uni-sb.de
Abstract
Meaning shifting phenomena such as metonymy 
have recently attracted increasing interest of 
researchers. Though these phenomena have been 
addressed by plenty of computational methods, 
the impacts of cardinalities of metonymically 
related items have been widely ignored in all of 
them. Motivated by this lack of analysis, we have 
developed a method for representing expect-
ations and knowledge about the cardinalities of 
metonymically related entities and for exploiting 
this information to build logical forms express-
ing metonymic relations, the entities related, and 
their cardinalities. The representation of lexically 
motivated knowledge is realized as an enhan-
cement to Pustejovsky's Generative Lexicon, and 
the process of building logical forms takes into 
account overwriting of default information and 
mismatch of cardinality requirements. Our 
method enables a precise attachment of sentence 
complements, and it supports reference reso-
lution in the context of metonymic expressions.
1 Introduction
Meaning shifting phenomena such as metonymy 
have recently attracted increasing interest. Com-
putational approaches to these phenomena aim at 
inferring implicitly represented relations, predict-
ing meaning shifts of nouns or verbs, expressing 
restrictions on these meaning shifts in depen-
dency of context- or language-specific factors, 
and facilitating reference resolution. Measures to 
achieve these issues include representation of 
default knowledge and various sorts of inference 
methods and constructive procedures. However, 
the entities in the texts examined almost always 
appear in singular form so that issues of cardina-
lity of metonymically related items have been 
widely ignored by the approaches made so far. 
Motivated by this lack of analysis, we have 
examined metonymic expressions by varying 
cardinalities of the items appearing explicitly or 
implicitly, to analyze effects of these alternations. 
The results have inspired us to build increasingly 
explicit versions of logical forms, and to formul-
ate conditions on pronominal accessibility. The 
insights gained improve analysis methods for 
relating contextual specifications to the appropri-
ate entity, and for supporting reference reso-
lution to entities related implicitly.
This paper is organized as follows. We review 
computational approaches to metonymy. Then 
we illustrate the phenomena investigated. We 
elaborate suitable techniques to deal with these 
phenomena, that is, an enhancement to entries in 
the Generative Lexicon, and a procedure for 
building a logical form. Finally, we discuss im-
pacts of our analysis on pronominal resolution.
2 Approaches to Metonymy
Metonymy is a natural language phenomenon 
that contributes to expressing information in an 
effective and economic way. It involves what has 
been termed 'transfers of meaning' by (Nunberg 
1995), i.e., the meaning of some constituent does 
not correspond to what can be expected accord-
ing to the syntactic and semantic environment ? 
the speaker is "using one entity to refer to 
another that is related to it" (Lakoff and 
Johnson 1980). For example, in the utterance 
"The ham sandwich is waiting for his check" , it is 
not literally the ham sandwich, which wants to 
pay, but the person who ordered it. 
Computational approaches such as the NL 
database interface TEAM (Grosz et al 1988) are 
concerned with inferring implicitly expressed 
metonymic relations, mostly in English; some 
analyses consider German (Horacek 1996) and 
French (Kayser 1988, Pustejovsky and Bouillon 
1995). Prominent representatives include Fass' 
program met* (1991), which makes use of 
formal definitions of several kinds of metonymic 
relations, Sowa's conceptual graphs (1992), in 
which an a priori unspecific relation is inserted  
between a concept of the type expected and the 
concept appearing on the surface, and the 
TACITUS system (Hobbs et al 1993) which 
treats metonymy as a special case of reference 
resolution, in a uniform abduction process to 
?find the best explanation for the observables? . 
Altogether, these approaches have two charac-
teristic properties: (1) The conditions expressing 
when leaving a metonymic relation implicit or 
not is possible are too unconstrained to cover a 
larger number of examples in several languages, 
or to generate sentences with metonymic expres-
sions systematically. (2) The intended and the lit-
eral referent always appear in singular definite 
form. There are only three approaches which in 
some aspects deviate from this characterization.
Pustejovsky's Generative Lexicon (1991) ad-
dresses the first aspect. He proposes a Theory of 
Qualia, with an explanation of systematic polyse-
my. Applying type coercion enables one to arri-
ve at cases of ordinary metonymy which can be 
grounded in terms of the semantics of lexemes, 
as well as at word senses which Pustejovsky has 
termed logical metonymy, like the reading of a 
book in the sentence ?Mary enjoyed the book?. 
Such contexts reflect prototypical knowledge 
derived from AGENTIVE or TELIC roles of the 
lexical entry for 'book', which are prominent 
roles in the Qualia Structure of nouns. Particular-
ities of the Qualia Structure of nouns regulate the 
acceptability of leaving a metonymic relation 
implicit (Pustejovsky and Bouillon 1995).
Stallard (1993) indirectly addresses the 
second aspect by taking into account scoping re-
lations and impacts on pronominal reference. He 
introduces a distinction between referential and 
predicative metonymy, depending on whether 
the intended or the literal argument is accessible 
for subsequent pronominal reference. This dis-
tinction manifests itself in different scope rela-
tions that hold between these arguments in the 
corresponding logical forms. We will argue 
against his usage of scoping and the resulting 
strict distinction of pronominal accessibility.
Markert and Hahn (1997) address interactions 
of metonymic relation extension and anaphora 
resolution, which enables them to handle textual 
ellipsis references. They apply extensive langu-
age independent conceptual definitions with rela-
tional path classifications and preference rules. 
In their corpus, there are also cases of indefinite 
metonymic NPs, which is an indication for 
metonymic relations to several objects.
Though neither Pustejovsky's nor Stallard's 
approach address cardinalities, we show that both 
can be extended accordingly: we augment the 
Generative Lexicon by representing cardinality 
information, and techniques for building logical 
forms are enhanced to yield more precise speci-
fications of the metonymically related entities.
3 Phenomena Investigated
For a number of metonymic relations, such as 
PRODUCER for PRODUCT (?I bought a Ford?), 
ARTIST for ARTWORK  (?He plays Bach?), as 
well as eventualities involved in logical meto-
nymy, cardinalities are not a problem because 
the literal referents are expressed as proper 
names. For other metonymic relations, especially 
ORGANIZATION for M E M B E R  and P A R T  for 
WHOLE, several complications may arise, as the 
following examples demonstrate. Let us start with 
two contrastive sentences (1) and (2), taken from 
(Lakoff and Johnson 1980), and (Hobbs et al 
1988), respectively (see also (Horacek 1994)):
(1) The ham sandwich is waiting for his check.
(1a) He is getting impatient.
(1b)It is 2 $.
(2) The Boston office called.
? (2a) He was angry.
(2b)It is our head quarter.
(2c) They want us to organize a meeting.
Following Stallard, (1) is interpreted as an ex-
ample of referential reading, while (2) as an ex-
ample of predicative reading: (1) can be rephras-
ed more explicitly by The manx who has eaten a 
ham sandwichy is waiting for hisx check, while (2) in a similar way gets expanded to The Boston 
officex represented by one of itsx employeesy call-
ed. These reformulations suggest that the man  in 
(1) and the Boston office in (2) have wider scope 
in Stallards representation than the ham sand-
wich in (1) and the employee in (2), which pre-
dicts pronominal accessibility in (1a) and (2b), as 
opposed to (1b), (2a) and (2c). We challenge this 
analysis with evidence from the examples above. 
Pronominal reference in (1b) is also possible, but 
may be less common than in (1a).  (2c) seems 
even more natural than (2b), only (2a) is unclear.
Further complications arise when variations of 
cardinality in sentence (1) (see sentences (3) to 
(6) and their follow-ups), and variation of cir-
cumstances in sentence (2) (see the follow-ups of 
sentences (7) and (8)) are considered. For dishes 
made of animals ('the mussels'), complications 
arise through interference between animals and 
persons as pronominal referents. Because we 
want to study the effects of cardinality variations 
per se, we avoid such examples.
(3) The pizzas are waiting for their checks.
? (3a) He/she is getting impatient. 
(3b)They are getting impatient. 
(4) The fruit dumplings is/are waiting for
 his/her/their check(s).
(4a) He/she/they is/are getting impatient.
(5) The meat plate is/are waiting for his/her/ 
their check(s).
(5a) He/she/they is/are getting impatient.
(6) Table 7 is/are waiting for his/her/their 
check(s).
(6a) He/she/they is/are getting impatient.
These sentences demonstrate that both intra- ((1) 
and (3)) and intersentential ((1a) and (3b)) pro-
nonminal reference work fine, if the literal ref-
erents (here, various sorts of food) and the real 
referents (here, the persons) agree in number. 
Otherwise, a variety of complications arise in in-
trasentential reference, which also demonstrate a 
specificity of English. Whereas pronouns agree 
with the literal referent in most languages, it is 
the intended referent that determines verb agree-
ment and pronominal reference in the same 
sentence in English.  For example, metonymic 
extension to the expression 'fruit dumplings' is 
ambiguous in the sense that it can refer to one 
plate of dumplings to be eaten by a single per-
son, or to several plates, each for another person 
(see the variants in (4)). Conversely, metonymic 
extension to the expression 'meat plate' can also 
be interpreted as a reference to several persons 
sharing that dish (see the variants in (5)). Finally, 
metonymic extension to the expression 'table' 
seems to be more neutral with respect to the 
number of persons sharing it (see the variants in 
(6)). Thus, the syntactic subject and the verb 
would not agree in number in English, when the 
default situation concerning these dishes is pre-
sent. Hence, English is, in principle, more infor-
mative than other languages when the cardinality 
of the intended referent differs from the number 
of the literal referent. However, those expressions  
without subject/verb agreement are unlikely to 
occur in practice, since they appear to be strange.  
Unlike with intrasentential reference, intersen-
tential pronominal reference with number feat-
ures deviating from the referent that is pronomi-
nally accessible intrasententially is possible due 
to default expectations about the cardinality of 
the real referents (compare complementary vari-
ants in (4a) and (5a)). It is more problematic in 
other cases (see (3a)). In the less precise referen-
ce by the table, all variants in (6a) are felicitous. 
(7) The Boston office is represented in the 
meeting.
? (7a) He/she is an expert in marketing.   
(7b) They are experts in marketing.   
(7c) They always send someone to meetings.
(8) The Boston office will meet for an
excursion today.
* (8a) He/she likes to walk.
(8b) They will make a lunch break at 2 pm.
(8c) They like to organize social events.
The following sentences ((2), (7), (8), and their 
follow-ups) involve slightly harder restrictions. 
Plural pronominal references as in sentences 
(7b), (7c), (8b) and (8c) are felicitous, but there 
is a difference between the sets of entities the 
plural pronouns refer to. While in (7c) and (8c), 
the pronouns refer to the entire set of employees 
of the Boston office, they more plausibly refer to 
the representatives in the meeting in (7b) and to 
the excursion participants in (8b). These examp-
les indicate an additional demand on the treat-
ment of cardinalities and referential accessibility 
of metonymic expressions: a distinction is to be 
made between the entities referred to metonymi-
cally (here: employees of the Boston office), and 
those of its members involved in the event 
expressed by the sentence (here: the meeting and 
the excursion). For the restaurant scenario, these 
sets of persons are mostly identical except to 
those cases where one person out of a group of 
persons eating together and referred to metony-
mically is the one who intends to pay.
(9) Which airlines serve food from Boston to 
New York ?
(9a) In the first class?
As a further aspect of metonymic expressions, 
the last two examples demonstrate chaining of 
metonymic relations and the relevance of each 
set of items involved for the associated analysis. 
In sentence (9), the airlines are the literal, and the 
persons the real referents. However, relating these 
two entities directly by an employment relation is 
problematic, since it is impossible to connect the 
locality information (from Boston to New York) 
and the first class restriction to either of them. 
Linking this information appropriately requires 
explicit elaboration of the relation between the 
airlines and their employees to include the 
implicitly referred flights.
The consideration exposed so far primarily 
hold for English. Apart from the partitive con-
struction, which seems to be a specificity of Eng-
lish in comparison to many other languages, the 
results can widely be transferred to other langu-
ages. However, there are a lot a language-specific 
subtleties which may influence the felicity or 
non-felicity of some of the expressions discussed 
in one particular language. In order to find out, 
to what extent other languages behave similar to 
English, we have asked native speakers of Ger-
man, French, Italian, Spanish, Russian, and Viet-
namese about the transferabillity of the English 
sentences to these languages. Though the results 
are subjective to a certain extent (only one 
speaker was available for most of these langu-
ages), some tendencies became apparent. Even 
sentence (1) was considered unacceptable in 
some languages, in which there is more emphasis 
on referring to persons explicitly. In Spanish, 
this seems to be caused syntactically, by the 
absence of personal pronouns, while the reasons 
seem to be more pragmatically or culturally 
related in French and Vietnamese, respectively. 
Moreover, references to objects ((1b) and (2b)) 
appeared unusal in some languages, including 
Vietnamese and Italian. Also in German, a de-
monstrative pronoun seems to be preferable to a 
personal pronoun. Finally, (2) is quite weird in 
Spanish, since the alternative 'From the Boston 
office _ called'  exists (unlike in (7) and (8)). In 
constrast, precisely (2) is acceptable in Vietname-
se, because only 'calling' is considered technical.
                                                                                                                                                                                                                                                
f o o d ( x )
CONST = {ingredients, ?}
FORMAL = eatable(x)
TELIC = eat(eT , y ,x )
AGENTIVE = cook(e ' T , z ,x )
sandwich(x)
CONST = {ham, bread, ?}
FORMAL = eatable(x)
TELIC = eat(eT , y ,x )
AGENTIVE = prepare(e'T ,z ,x)
p i z z a ( x )
CONST = {dough, tomato, ?}
FORMAL = eatable(x)
TELIC = eat(eT , y ,x )
AGENTIVE = bake(e'T ,z ,x)
                                                                                                                                                                                                                                                   
Fig. 1. Some 'standard' examples of Qualia Structures, for 'food', 'sandwich', and 'pizza'
4 Expressing Lexical Knowledge
In order to capture distinctions between the 
varying interpretations of metonymic expres-
sions, knowledge about the lexical items involved 
plays a crucial role. For adequately expressing 
this knowledge, we make use of entries in the 
Generative Lexicon (see Figure 1). Since the 
information represented there is insufficient for 
reasoning about cardinalities, we extend the 
entries in the Generative Lexicon, prominently 
the TELIC role, by quantifier specifications. In 
the original form, the entities involved (typically, 
the lexical item itself and some related entity) are 
implicitly quantified, and a typed event variable 
is used (an event may be a state (S), a process 
(P), or a transition (T)). A similar exploitation of 
taxonomic knowledge in terms of cardinality 
restrictions has been undertaken for scope dis-
ambiguation in (Fliegner, 1988).
In the extended form, we introduce explicit 
quantifiers and scoping, and we optionally add 
sort restrictors to variables referred to by event 
predicates. We introduce new quantifiers to cover 
the cases elaborated in the previous section, in 
addition to the usual NL quantifiers EXIST and 
WH: SINGLE and MULTIPLE for a single resp. 
multiple objects without defaults, DEFSINGLE 
and DEFMULTIPLE for the same with defaults.
Figures 2 and 3 show entries in the Generative 
Lexicon with extended TELIC roles. The same 
extensions also apply to the AGENTIVE roles, but 
we do not elaborate this aspect here. Figure 2, for 
example, shows some sorts of food associated 
with different expectations about how many per-
sons typically eat them. Fruit dumplings appear 
as sets (quantified by DEFMULTIPLE), to be eaten 
as a dish by a single person (quantified by DEF-
SINGLE). For the meat plate, cardinality relations 
are inverted. For a table in a restaurant, relations 
to food eaten at that table are not specified.   
Unlike in the restaurant scenario, cardinality 
relations are less vague for some relations in 
organizations. Each office and each airline are 
supposed to employ several persons, and each 
person is working for one organization only, at 
least in his/her individual activities (this is ex-
pressed by the quantifiers SINGLE and MULTIPLE 
in the lexical entries in Figure 3). Each flight 
carries some set of people, each of which parti-
cipates in one flight only (at the same time). 
These extensions allow us to derive cardina-
lities for the referents involved in a metonymic 
expression ? compare the entries for FRUIT-
DUMPLING  and MEAT-PLATE , as contrasting 
examples. To achieve this goal, the knowledge 
represented in the lexicon entries is used for 
building logical forms in which metonymic 
relations are made entirely explicit. The event 
predicates in the TELIC (or AGENTIVE) roles are 
exploited to infer the relation involved. More-
over, the new quantification specification yields 
crucial information to build an explicit logical 
form with cardinality specifications from concise 
surface expressions in a precise manner.
5 Building Logical Forms
Based on entries in the Generative Lexicon and 
on the context given by a sentence to be inter-
preted, logical forms can be built that represent 
the semantic relations involved more explicitly  
than this is the case with previous approaches. In 
a nutshell, metonymic extensions are tried 
according to specifications found in the lexicon, 
as long as the sort of an NP  and the sort of the 
referring case role are incompatible. In addition, 
agreement between syntactic number and seman-
                                                                                                                                                                                                                                                 
fruit -dumpling(x)
CONST = {dough, fruit, ?}
FORMAL = eatable(x)
TELIC = (DEFSINGLE y
(DEFMULTIPLE x
( e a t ( e T , y , x ) ) ) )
AGENTIVE = cook(e ' T , z ,x )
meat -p la te (x )
CONST = {pork, beef, ?} 
FORMAL = eatable(x)
TELIC = (DEFSINGLE x
(DEFMULTIPLE y
( e a t ( e T , y , x ) ) ) )
AGENTIVE = prepare(e'T ,z ,x)
t a b l e ( x )
CONST = {legs, plate, ?}
FORMAL = physobj (x )
TELIC = (DEFSINGLE x
(DEFMULTIPLE y
( s i t - a t ( e S , y , x ) ) ) )
AGENTIVE = build(e'T , z ,x)
                                                                                                                                                                                                                                                 
Fig. 2. Some 'extended' examples of Qualia Structures, for special food sorts and 'table'
                                                                                                                                                                                                                                                 
o f f i c e ( x )
CONST = {employees, ...}
FORMAL = organizat ion(x)
TELIC = (SINGLE x
(MULTIPLE y
PERSON                         
( w o r k ( e P , y , x ) ) ) )
AGENTIVE = establ i sh(e ' T , z , x )
a i r l i n e ( x )
CONST = {planes, office, ...}
FORMAL = organizat ion(x)
TELIC = (SINGLE x
(MULTIPLE y
FLIGHT
( o r g a n i z e
( e T , y , x ) ) ) )
AGENTIVE = found(e'T,z,x)
f l i g h t ( x )
 CONST = {place, source, ...}
 FORMAL = loc -change(x)
 TELIC = (SINGLE x
(DEFMULTIPLE y
 PERSON
 (carry(eT , y ,x ) ) ) )
AGENTIVE = organize(e'T,z,x)
                                                                                                                                                                                                                                                 
Fig. 3. Some 'extended' examples of Qualia Structures, for 'office', 'airline', and 'flight'
tic cardinality specifications is achieved, which 
may require overwriting defaults or introducing 
a new set of entities as a subset of a known set. In 
concrete, logical forms are built by pursuing the 
procedure in Figure 4.  Logical forms appear as 
(Q x S <P>), where Q is a quantifier, x and S its 
associated variable and sortal restrictor, and <P> 
the predication related. In step 2a, metonymic 
extensions are carried out, which can potentially 
be chained, and in step 2c a final extension is 
performed in case of a cardinality mismatch. In 
the following, we illustrate the procedure by 
some examples. For sentence (4), ?The fruit 
dumplings wants to pay?, the initial logical form   
(MULTIPLE x FRUIT-DUMPLING (WANT-PAY x))
contains a sortal incompatibility. Using the lexi-
cal entry for 'fruit dumplings' and expanding the 
expression according to the TELIC role yields
(SINGLE y PERSON
(MULTIPLE x FRUIT-DUMPLING
(AND (EAT y x) (WANT-PAY y))))
where the sortal incompatibility is removed. 
Note, that the cardinality of PERSON is singular, 
due to the inflection of the predicate 'wants'. In 
German, the quantifier is unspecific concerning 
the cardinality, because the sentence predicate 
would not give the same indication as this is the 
case in English. For another predicate, such an 
ambiguity may not be present, as in the example
(SINGLE x OFFICE (AND (BOSTONIAN x)
           (CALL x)))
Making use of the TELIC role in the lexical entry 
of 'office', as exposed in Figure 3, yields
(SINGLE x OFFICE (AND (BOSTONIAN x)
(MULTIPLE y PERSON (AND (WORK y x) 
 (CALL y)))))
                                                                                                                                                                                                                                                 
1. Build an initial logical form out of the surface expression.
The representation is composed as an expression of the form (QE xE SE <P>):
xE being the variable whose representation is to be extended (initially x, the literal referent),
QE being its quantifier, and SE its sort (initially Q and S, associated with the literal referent), and
<P> being a structured representation of the sentence predicate and its modifiers.
SR is the sort required in the referring case frame, and QR the quantifier of its case slot restrictions.
2. Extend the meaning of noun phrases which are involved in a sortal incompatibility.
2a. Build a metonymically extended expression by consulting lexical knowledge.
Merge the partial expression (QE xE SE <P>) with the extended lexicon representation for SE:
(Q1 x1 S1 (AND <P1> (Q2 x2 S2 <P2>))) ? 
from the lexicon, <Q1,x1,S1> = <QE,xE,SE> and <Q2,x2,S2> = <QN,xN,SN> 
if the referent with the same sort as xE has wider scope in the lexicon, or with inverted equalities.
<P> is partitioned according to sortal compatibility of its components:
if x1 = xE then <P1> contains parts that refer to xE, sortally compatible with SE, otherwise <P2>.           
The remaining parts of <P> become <P2>, if x1 = xE, and <P1> otherwise.
2b.Test the compatibility of the newly inserted sort with the restrictions to be met.
If SN (SN = S2, if S1 = SE, and SN = S1 otherwise) is incompatible with SR, 
then repeat step 2a with xN, SN, QN and <PN> as xE, SE, QE and, <P>, respectively;
else QR overwrites QN if QN is a default quantifier compatible with QR.
2c. Test the cardinality compatibility of the new sort with the restrictions to be met.
If SN is compatible with SR, but QN is incompatible with QR, insert MEMBER between xE and xN.
If not the whole set of entities bound to xE participates in the eventuality, insert SUBSET instead.
                                                                                                                                                                                                                                                 
Fig. 4. The procedure for building logical forms with extended metonymic relations
which still contains a cardinality incompatibility. 
Further expanding this form by performing step 
2c in the procedure leads to the insertion of a 
MEMBER relation, yielding
(SINGLE x OFFICE (AND (BOSTONIAN x)
(MULTIPLE y PERSON (AND (WORK y x) 
(SINGLE z PERSON (AND (MEMBER z y) 
(CALL z))))))) 
in which all incompatibilities are resolved. Pro-
ceeding in the same manner, the analysis of the 
sentence ?The Boston office makes an excur-
sion? yields a similar result, with only two minor 
deviations, partially grounded in the semantic 
difference between 'calling' and 'excursion 
making': (1) The variable z  is quantified by 
M U L T I P L E  instead of S I N G L E , and (2) the 
expression (SUBSET z y) replaces (MEMBER z y). 
However, obtaining precisely this representation, 
that is, performing the insertion of the SUBSET 
relation, additionally requires some sort of prag-
matic knowledge: typically not all members of 
an organization participate in events such as 
excursions. Nevertheless, suitable ways to repre-
sent such domain-dependent pieces of knowl-
edge adequately are delicate. 
Finally, sentence (9), ?Which airlines serve 
food from New York to Boston??, shows how 
chained metonymic extensions are handled:
(WH x AIRLINE (AND (SERVE x FOOD)
(SOURCE x NEW YORK) (GOAL x BOSTON)))
The first metonymic extension, based on the 
lexicon entry for 'airline' (see Figure 3), tenta-
tively inserts 'flights' linked to 'airline' via an 
ORGANIZE relation, and yields
(WH x AIRLINE (MULTIPLE y FLIGHT
(AND (ORGANIZE x y) (SERVE y FOOD)
  (SOURCE y NEW YORK) (GOAL y BOSTON))))
and the final operation based on the lexicon 
entry for 'flight' (see Figure 3) leads to a similar 
extension, inserting 'person' related to 'flight' via 
a CARRY relation:
(WH x AIRLINE
(MULTIPLE y FLIGHT (AND (ORGANIZE x y)
(SOURCE y NEW YORK) (GOAL y BOSTON)
(MULTIPLE z PERSON
(AND (CARRY y z) (SERVE z FOOD))))))
Note the distinguished treatment of the predi-
cations containing the variable which represents 
the phrase to be extended, as opposed to the 
previous examples. In all cases discussed so far, 
appearances of this variable are replaced by the 
new variable introduced in the course of an 
extension. Here, replacing y by z in the second 
extension step is only carried out in (SERVE y 
FOOD), while y remains unchanged in (SOURCE y 
NEW YORK) and (GOAL y BOSTON) . This is 
because SOURCE and GOAL can be established as 
properties of flights, while C A R R Y  needs a 
further extension to 'person' to be connected 
appropriately. Building explicit logical forms in 
this way demonstrates a number of achievements 
over other methods:
? Scoping of variables reflects their depen-
dencies in the event they are involved in.
? More referents than just the real and the literal 
referent may be introduced, through chained 
metonymic extensions or through member-
ship/subset insertions.
? An additional referent may provide a proper 
place to relate sentence complements.
Note, that there is a scoping difference between 
?one and the same person eating several fruit 
dumplings? and ?several persons sharing a meat 
plate?, which contrasts Stallard's approach.
Finally, we have to admit that this procedure 
is overgenerating, as it does not take into account 
the restrictions imposed on the use of metonymic 
expressions discussed in Section 3. The proce-
dure is cooperative in the sense that it attempts to 
interpret a given metonymic expression, but it is 
not strong enough to distinguish felicity or infe-
licity of a metonymic expression, which may be 
due to various lexical and pragmatic factors.  
6 Impacts on Reference Resolution
Empirically supported by the considerable 
number of examples discussed in section 3, our 
approach is able to explain more pronominal 
references to metonymic expressions than others:
? Reference to literal and intended referents is 
possible in an increasing number of cases.
? Pronominal reference in plural form may ha-
ve as antecedents distinguished sets of entities.
? Cross-language differences in the treatment of 
intersentential pronominal reference exist.
In order to express scoping relations among sets 
properly, the logical forms representing meto-
nymic expressions with entities of cardinality 
greater than one must deviate from Stallard's 
methods. According to Stallard, pronominal 
reference to literal and real referents is regulated 
by their scope, which distinguishes referential 
from predicative kinds of metonymy. Unfortu-
nately, this realization of metonymic extension is 
incompatible with the common use of scoping. 
However, we believe that Stallards distinction is in 
some sense artificial, because the felicity of pro-
nominal reference seems to be more complex 
and influenced by other factors than scoping. 
For example, the sentence ?the ham sandwich is 
waiting for his check? can be followed by some 
information useful to a novice waiter: ?It costs 
2$.? Moreover, the message ?The Boston office 
called? can be followed by the remark ?He spo-
ke angrily? in some plausible contexts. Hence, it 
does not seem to be referential inaccessibility 
which makes many similar examples sound odd, 
but the rare occurrence and the low coherence in 
neutral contexts. For example, it is usually of 
minor interest whether the person calling on 
behalf of the Boston office is angry himself; it is 
the attitude of the responsible representatives at 
the office that is usually more interesting.
Given these pieces of evidence, reference 
resolution is supported by the explicit logical 
form built through our techniques, and it is addi-
tionally guided as follows:
Intrasentential reference
Possessive pronouns always relate to the intended 
referent. Since possessive pronouns in the same 
sentence agree with the real referent in English, 
while they agree with the literal referent in most 
other languages, only English sentences contain 
information about cardinality and gender of the 
intended referent. For example, the sentence 'the 
fruit dumplings is waiting for his check' carries 
the additional implication that there is one male 
person who wants to pay.
Intersentential reference
Reference through personal pronouns is possible 
to the literal and to the real referent, and to refer-
ents of the same sort but with possibly different 
cardinality as the real referent. Thus, all entities 
involved in a metonymic expression in its ap-
pearance in the explicit logical form are potential 
antecedents, except to internal elements of a 
metonymic chain. For example, following the 
sentence ?The Boston office called?, pronomi-
nal reference is possible to the office (the literal 
referent), to the caller (the real referent), and to 
the people at the office (differing from the caller 
by number only). However, 'the flights' appear-
ing in the logical form representing the sentence 
?Which airlines serve diet food from New York 
to Boston?? are not pronominally accessible.
7 Conclusion
In this paper, we have presented an approach to 
deal with cardinality aspects of metonymic ex-
tensions to nouns. We have discussed a variety of 
constellations with pronominal references to im-
plicitly related items, sometimes associated with 
subtle conditions, focusing on English, also in-
cluding some language specificities. In order to 
build explicit logical forms with cardinality 
specifications, we have extended entries in Pustej-
ovsky's Generative Lexicon by default quantifier 
specifications. Through exploiting these entries, 
metonymic extensions are introduced on the 
basis of events represented in the roles of the 
Qualia structure, and member or subset relations 
are introduced on the basis of the associated 
quantifier specification. Our method for building 
explicit logical forms challenges Stallard's 
distinction of predicative and referential readings 
of metonymic expressions: it produces scopings 
that reflect proper quantifier dominance relations 
rather than pronominal accessibility conditions, 
and it allows for additional cases of pronominal 
reference. In addition, our method enables a 
more precise attachment of contextual specifi-
cations to related entities, and it supports refer-
ence resolution to metonymically related entities. 
References
(Fass, 1991) Dan Fass. met*: A Method for Discrimina-
ting Metonymy and Metaphor by Computer. 
Computational  Linguistics 17(1), pp. 49-90,1991.
(Fliegner, 1988) Michael Fliegner. HOKUSKOPUS - 
Verwendung terminologischen Wissens bei der Ana-
lyse von Quantorenskopus und Distributivit?t. In 
Proc. of GWAI-88, pp. 112-117, 1988.
(Grosz et al 1988) Barbara Grosz, Doug Appelt, Paul 
Martin, and Fernando Pereira. TEAM: An Exper-
iment in the Design of Transportable Natural-Langu-
age Interfaces. Artificial Intelligence 32, pp. 173-
243, 1987.
(Hobbs et al 1993) Jerry Hobbs, Mark Stickel, Doug 
Appelt, and Paul Martin. Interpretation as Abduc-
tion. Artificial Intelligence, pp. 69-142, 1993.
(Horacek 1994) Helmut Horacek.  Some Issues in 
Dealing with Metonymy. In Proc. of KONVENS-94, 
pp. 171-180, Vienna, Austria, 1994.
(Horacek 1996) Helmut Horacek. On Expressing Meto-
nymic Relations in Multiple Languages. Machine 
Translation 11, pp. 109-158, 1996. 
(Kayser 1988) Daniel Kayser. What Kind of Thing is a 
Concept. Computational Intelligence 4(2), pp. 158-
165, 1988.
(Lakoff and Johnson 1980) George Lakoff and M. 
Johnson. Metaphors We Live By. Univ. of Chicago 
Press, 1980.
(Markert and Hahn 1997) Katja Markert and Udo Hahn. 
On the Interaction of Metonymies and Anaphora. In 
Proc. of IJCAI-97, pp. 1010-1015, Nagoya, Japan, 
1997.
(Nunberg 1995) Geoffrey Nunberg. Transfers of 
Meaning. Journal of Semantics 12, pp. 109-132, 
Oxford University Press, 1995.
(Pustejovsky 1991) James Pustejovsky. The Generative 
Lexicon. Computational Linguistics 17(4), pp. 409-
441, 1991.
(Pustejovsky and Bouillon 1995) James Pustejovsky, 
and P. Bouillon. Aspectual Coercion and Logical 
Polysemy. Journal of Semantics 12, pp. 133-162, 
Oxford University Press, 1995.
(Sowa 1992) John Sowa. Logical Structures in the 
Lexicon. In J. Pustejovsky, S. Bergler (eds.): Lexical 
Semantics and Knowledge Representation, pp. 39-
60, Springer, 1992.  
(Stallard 1993) David Stallard. Two Kinds of Meto-
nymy. In Proc. of ACL-93, pp. 87-94, Columbus, 
Ohio, USA, 1993. 
A System for Generating Descriptions of Sets of Objects in a Rich Variety 
Helmut Horacek
Universit?t des Saarlandes
F.R. 6.2 Informatik
Postfach 151150
 D-66041 Saarbr?cken, Germany
 email: horacek@cs.uni-sb.de
Abstract
Even ambitious algorithms for the gener-
ation of referring expressions that iden-
tify sets of objects are restricted in terms 
of efficiency or in their expressive reper-
toire. In this paper, we report on a system 
that applies a best-first searching proce-
dure, enhancing both its effectiveness and 
the variety of expressions it can generate.  
1 Introduction
Generating referring expressions has recently 
been extended from the identification of single 
to sets of objects. However, existing algorithms   
suffer in terms of efficiency and expressiveness. 
In this paper, we report on a system that applies a 
best-first searching procedure, with an enhanced 
effectiveness and a larger variety of expressions 
it can generate. The system's repertoire includes 
compositions of partially identifying expressions 
and descriptions of objects to be excluded, there-
by taking into account impacts on surface forms.
Throughout this paper, we refer to a scenario 
with a set of 12 vehicles as defined in Figure 1. 
All vehicles are identifiable individually, to make 
the identification task meaningful. Only minor 
differences hold between some of these vehicles, 
which makes the identification task challenging.
This paper is organized as follows. First, we 
motivate our goals. Then we describe techniques 
for enhancing efficiency. We follow by illus-
trating improvements of expressiveness. Finally, 
we evaluate several efficiency-related techniques.
2 Motivation
Identifying sets of objects originally followed the 
incremental algorithm (Dale and Reiter 1995), as  
in (Bateman 1999), (Stone 2000) and (Krahmer 
et al 2003), with limited coverage, since only few 
attributes typically apply to all intended referents 
and to none of the potential distractors. There-
fore, van Deemter (2002) has extended the set of 
descriptors to boolean combinations of attributes, 
including negations. Unfortunately, when apply-
ing the incremental strategy, this may lead to the 
inclusion of too many redundant descriptors in 
the final specification. This deficit disappeared 
using an exhaustive search (Gardent 2002), but   
run-time then increases considerably. Mediating 
between these two extreme search paradigms, we 
have developed a best-first searching algorithm 
that avoids the major deficit of the incremental 
approach (Horacek 2003). Since its intermediate 
results can also be used as partial descriptions, we 
build on the flexibility of this new algorithm to 
extend its expressive capabilities. In addition, we 
further enhance its efficiency-seeking measures.
These extensions attack the deficits previous 
algorithms share, according to (Horacek 2004):
? Expressions produced may become lengthy: 
for identifying sets of vehicles in the scenario 
in Figure 1, we have obtained non-redundant 
specifications with up to 8 descriptors. 
? Specifications may contain some disjunctions, 
frequently causing the production of structur-
ally ambiguous expressions (Gardent 2002) ? 
?trucks and sportscars which are white or in 
the center? referring to x1, x5, x11 (Figure 1). 
We avoid these deficits by not restricting boolean 
expressions to a form with conjunctions as top 
level operators, as others always do. This allows 
us to incorporate descriptions of objects to be 
excluded, to produce enumerations and compo-
sitions of descriptions of subsets of the intended 
referents, and to build compositions of increa-
singly restricting descriptions of these referents.
                                                                                                                  
                                                                                                                  
Objects
Descriptors x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12
                                                                                                                  
vehicle ? ? ? ? ? ? ? ? ? ? ? ?
car ? ? ? ? ? ? ? ?
sportscar ? ? ? ?
truck ? ? ? ?
blue ? ? ?
red ? ? ? ? ? ?
white ? ? ?
center ? ? ? ?
left ? ? ? ?
right ? ? ? ?
big ? ? ? ? ? ?
small ? ? ? ? ? ?
new ? ? ? ? ? ?
old ? ? ? ? ? ?
                                                                                                                  
                                                                                                                  
Figure 1. Example scenario with 12 vehicles
3 The Best-First Procedure
The basic mechanism of the best-first search 
algorithm is a generalization of the incremental 
version: instead of successively adding attributes 
to the full expression generated so far, all inter-
mediate results are accessible for this operation, 
producing an optimal solution, if completed ? 
see (Horacek 2003) for details. This algorithm 
uses two cut-off techniques, assuming conflation 
(e.g., the descriptors man and unmarried can be 
verbalized as ?bachelor?) is not possible: 
? A dominance cut-off is carried out locally for 
sibling nodes, when two partial descriptions 
exclude the same set of potential distractors,  
the same set of descriptors still being available. 
The variant evaluated worse is discarded.
? A value cut-off is carried out globally after a 
solution has been found. It is done for nodes 
whose most optimistic evaluation (including 
the minimal value of the description required 
for excluding the remaining potential distrac-
tors), surpasses the evaluation of that solution. 
Applying any of these cut-offs only serves to 
gain speed and does not change the final result. 
3.1 Efficiency-Enhancing Measures 
We have enhanced this repertoire by a complexity 
cut-off, carried out prior to further expanding a 
node if the boolean combination of descriptors 
build leads to a description that is more complex 
than a given threshold. For this threshold, we use 
the complexity of descriptions identifying each 
referent individually, which is an enumeration.   
The generation of boolean combinations is a 
critical part of the algorithm, since it is its most 
time-consuming component. Redundancies must 
be avoided, which requires more effort than pre-
vious approaches due to our hierarchical organi-
zation of property values. This burden is split 
between a static representation of implications, 
compiled from the underlying knowledge base 
about specializations, and the function Generate-
Next, which accesses these data. Four implications 
hold between properties and their negations:
implies (p,q) if specializes(p,q) holds
implies (p,?q) if incompatible(p,q) holds 
implies (?p,q) if opposite(p,q) holds 
implies (?p,?q) if generalizes(p,q) holds
Then the predicates subsumes and redundant can 
be defined for properties (or their negations):
subsumes(p,q) ? implies (q,p)
redundant(p,q) ? ?(subsumes(p,q) ?
subsumes(q,p))
The function Generate-Next (Figure 1) success-
ively builds increasingly complex disjunctions of 
descriptors and their negation. To start with, the 
procedure Increment produces the next property 
combination with given complexity, if existing 
(1). Otherwise (2), that complexity is augmented 
(9) before generating the next combination, 
unless the complexity limit is reached (8), 
causing a complexity cut-off . For a property 
combination, it is tested whether all its properties 
are pairwise redundant (3), then the next combi-
nation is built. If a non-redundant combination 
is found, it must pass the following tests: 
1. It subsumes the target set (4). 
2. It further reduces the set of distractors (5).
3. The reduced set of distractors is not equal to 
or a superset of the distractor associated with 
a sibling node already created; otherwise, a 
dominance cut-off applies (6).
If  successful, that combination is returned, 
otherwise building combinations is resumed (7).
3.2 Enhancing the Best-First Procedure
We have incorporated a number of improve-
ments over the original version of the procedure:  
? Treating linguistically motivated preferences 
as options rather than restrictions 
? Putting limitations on the complexity of 
specifications, to control comprehensibility
? Enhancing the expressive repertoire by 
descriptions of subsets of referents and by 
descriptions of referents to be excluded  
? Producing a sequence of increasingly res-
tricting descriptions rather than a single one.
                                                                                                                 
Procedure Generate-Next(Current-Prop-Comb)
1 Nextprop ? Increment(Current-Prop-Comb) (1)
if Nextprop = nil then goto Step 2  endif (2)
if redundant(p,q) for any p,q ? Nextprop  (3)
then goto Step 1 endif
if subsumes(Nextprop,Properties-of(T))
for all T ? Target and (4)
?subsumes(Nextprop,Props(D)) 
for some D ? Distractors(Best-Node) (5)
 and ? Q ? R, where 
R = {subsumes(Properties-of(P),Nextprop)}, 
Q ={subsumes(Properties-of(P),
Description(N))} 
for all P ? Distractors, (6)
some N ? successor(Best-Node)
then return Nextprop (7)
else goto Step 1 endif (Dominance cut-off)
2 if (Score(Description(Best-Node)) + 
Score(Nextprop)) ? Complexity-limit (8)
then return nil (Complexity cut-off)
else Nextprop ? Increment-size(Nextprop)
goto Step 1 endif (9)
                                                                                                                 
Figure 2. Pseudo-code of descriptor generation
In the following, we summarize each of these 
(see (Horacek 2004) for details).
The following linguistically motivated prefer-
ences are treated as options: a boolean combina-
tion of descriptors that express the category of 
the object (by a head noun) is chosen first, other 
(attribute) descriptors later, since a category must 
be chosen anyway. Moreover, we reduce the set 
of potential solutions by excluding ?mixed? 
boolean combinations, that is disjunctions of a 
category and attributes, such as car ? red, which 
are unnatural and awkward to express verbally.
To strengthen comprehensibility, we specify 
limitations on the surface form of descriptions, 
including places for the head noun, pre- and 
postnominal modifiers, and relative clauses. 
Maximum numbers for each of these positions 
can be given, also specifying places as alternative 
ones, thus limiting the number of components in 
conjoined expressions. By associating descriptors 
with surface positions they can take, these speci-
fications allow one to control the surface struc-
ture of the descriptions during searching. 
For partial descriptions with multiple disjunc-
tions, recasting the expression built as a partial 
description is attempted to remain within given 
limits. These descriptions are always of the form 
^ i=1,n (?j=1,mi P ij), where each P ij is a positive or 
negative descriptor. Even in moderately complex 
instances of this conjoined expression, several 
elements may consist of  disjunctions of more 
than one descriptor. In such a constellation,we 
pick up one disjunction, for example ? j=1,mk Pkj 
for some k, transforming that expression by 
applying distributivity. This amounts to parti-
tioning the set of intended referents into subsets, 
where each of the components of the new top 
level disjunction describes one of these subsets. 
Consider, for example, ?the sportscars that are 
not red and the small trucks? identifying x5, x7, 
x8, and x12 in two components rather than by the 
involved one-shot ?the vehicles that are a sports-
car or small, and either a truck or not red.? In 
addition, descriptions may specify exceptions: 
describing some of the referents to be excluded 
may lead to shorter expressions than expanding 
the description of the intended referents, so that 
we integrate it in the expressive repertoire ? for 
example, ?the vehicles on the right, but not the 
red truck?, identifying x 1 , x 3 , and x 6  by 
excluding x7 in the locally restricted context. 
In accordance with these specifications, the 
best-first search is invoked to produce an identi-
fying description. This may not always be 
possible in complex situations. If this is the case, 
the best partial solution is taken, and the search is 
repeated within the restricted context defined by 
the descriptions generated so far. By this proce-
dure, a sequence of descriptions is generated 
rather than a single one. Consider, for example, 
?one of the trucks and the sportscars, all not 
white. The truck stands on the right?, identifying 
x6, x7, x11 and x12 out of all 12 vehicles (in Figure 
1) in two passes. 
3.3 An Example 
We illustrate the behavior of the system by a 
small example. Let {x1, x3, x6} in Figure 1 be the 
set of intended referents. Specifications for max-
imum complexity of surface forms allow head 
nouns, pre- and postnominal modifiers, at most 
one of them as a conjoined expression, and a 
relative clause or a ?but?-modifier expressing 
an exception. Only two descriptors apply to all 
intended referents, vehicle  and right . Even if 
vehicle is chosen first, subsequent searching only 
expands on the partial description with right, 
since it excludes a superset of the objects vehicle 
does: only x7 is remaining. The next simplest 
descriptor combination is car ?  white, which 
would allow complete identification of the inten-
ded referents. Since it can only be expressed by 
a relative clause, for which conjoined expressions 
are not allowed, recasting the description is 
attempted. This yields  (car ^ right)  ?  (white ^ 
right), which is a possible solution. Since a head 
noun is required for the second part, adding a 
further descriptor, an attempt is made to improve 
the solution, through finding an alternative to car 
?  white. Describing the complement constitutes 
such an alternative, since identification is 
required for x 7  only. This can be done by 
selecting t ruck  and, afterwards, any of the 
descriptors red, small, and old (let us say, we pick 
red). This yields right ^ ? (truck ^ red) as an 
alternative solution, with vehicle being added to 
obtain a head noun. Altogether, a surface gener-
ator could then generate ?the vehicles on the 
right, but not the red truck ?, resp. ?the cars and 
the white vehicle, both on the right? ? the latter 
with a clever aggregation module.
4 Experimental Results
We have implemented the algoritm in Common 
Lisp, on an Intel Pentium processor with 2600 
MHz. In the following elaborations, we use 
natural language descriptions for reasons of 
readability, even though our algorithm only 
produces boolean combinations of descriptors. 
We evaluate our algorithm from three 
perspectives: 1) effects of the linguistically moti-
vated restrictions, 2) effectiveness of the cut-off 
techniques, and 3) the behavior in scaling up for 
larger examples. For this purpose, we have built 
all subsets of two, three, and four vehicles, out of 
the vehicles x1 to x6, which yields 50 cases. 
In order to test the effects of the linguistically 
motivated reductions,  we have used  two versions 
                                                                                                                  
                                                                                                                  
cut-offs (v=value, d=dominance, c=complexity)
v&d&c v&c d&c c v&d d v
                                                                                                                  
time (msec)
minimum 10 10 60 90 10 90 10
maximum 690 1150 1910 19210 1100 4550 2320
average 121.5 131.6 354.8 1133.1 140.5 595.0 168.1
tree size (nodes)
maximum 9 71 11 945 9 11 71
average 2.2 3.86 2.33 61.64 2.2 2.33 3.88
                                                                                                                 
                                                                                                                 
Table 1. Searches comparing effects of cut-offs
of the 50 cases, one with all properties, and one 
without size and age. In these runs, the maximum 
number of descriptors chosen was 5, and search 
trees grew up to 9 with and 20 nodes without 
using the linguistically motivated reductions. The 
average search times were 127.7 resp. 440.5 
msec, with a maximum of 950 resp. 2590 msec.
In order to compare the effectiveness of the 
cut-off techniques, we have run the same sample 
of 100 cases (50 with and 50 without size and 
age), with all combinations of at least one cut-off 
technique. Table 1 illustrates the results. Among 
others, they demonstrate that search times are not 
proportional to tree sizes, since a lot of effort is 
devoted to justify the avoidance of expansions, 
which varies among cut-off techniques. It turns 
out that the value cut-off is the most effective 
one, which underpins the importance of finding 
a solution quickly. Looking at individual 
examples reveals that the complementary effects 
of dominance and complexity cut-offs are signi-
ficant only for examples with larger solutions.
Finally, we have tested the algorithm's scala-
bility, by increasing the number of distractors, 
with up to 25 vehicles (similar to x1 to x12, but 
distinct from one another). The same 100 cases 
have been used as before, with all cut-off criteria. 
The results appear in Table 2. They demonstrate 
that the problem tends to get unmanagable for 
more than 12 distractors in both search time and 
number of descriptors needed for identification, 
the latter being the reason for the former. 
However, descriptions consisting of up to 10 
descriptors are unlikely to be understandable for 
humans, anyway ? consider, for example, ?the 
cars which are not blue, are old or stand in the 
center, are new or stand on the right side, are big 
or not white, and are small or not red? (108110 
msec, identifying x 3 , x 4 , and x 6  out of 25 
vehicles). For such complicated cases, identifying 
objects is broken down into simpler tasks (see 
Section 3.2). Conversely, useful results may be 
obtained for a large number of distractors ? for 
example, ?the old cars on the right side? (120 
msec, identifying x3 and x6 out of 25 vehicles).
                                                                                                                  
                                                                                                                  
nr. of distractors
6 7 8 9 10 12 15 20 25
                                                                                                                  
time (msec)
minimum 10 10 10 10 10 30 60 100 120
maximum 490 2300 3880 4100 4430 6530 53390 88120 141200
average 116 282 417 484 7051120 5366 12325 24838
max nr. of
tree nodes 9 10 12 16 27 61 106 303 907
descriptors 5 5 5 5 5 5 6 8 10
                                                                                                                  
                                                                                                                 
Table 2. Searches with varying sets of distractors
5 Conclusion
We have presented a system that can produce 
referring expressions for identifying sets of 
objects. It has a number of exceptional features, 
including several efficiency-enhancing measures, 
the incorporation of exclusion descriptions, and 
partitioning the identification task into subtasks. 
The results show that our system has an increased 
repertoire compared to its predecessors, and it 
can compute these expressions reasonably fast.
References
John Bateman 1999. Using Aggregation for Selecting 
Content when Generating Referring Expressions. In 
Proc. of 37th Annual Meeting of the Association for 
Computational Linguistics (ACL'99), pp. 127-134.
Robert Dale and Ehud Reiter 1995. Computational Inter-
pretations of the Gricean Maxims in the Generation 
of Referring Expressions. Cognitive Science 18: 
233-363.
Claire Gardent 2002. Generating Minimal Definite 
Descriptions. In Proc. of 40th Annual Meeting of 
the Association for Computational Linguistics 
(ACL'2002), pp. 96-103. 
Helmut Horacek 2003. A Best-First Search Algorithm 
for Generating Referring Expressions. In Proc. of 
10th Conference of The European Chapter of the 
A s s o c i a t i o n  for Computational Linguistics 
(EACL'2003), short paper, pp. 103-106.
Helmut Horacek 2004. On Referring to Sets of Objects 
Naturally. In Proc. of Third International Natural 
Language Generation Conference (INLG-2004).
Emiel Krahmer, Sebastiaan van Erk, and Andr? Verleg 
2003. Graph-Based Generation of Referring Expres-
sions. Computational Linguistics, 29(1):53-72.
Matthew Stone 2000. On Identifying Sets. In Proc. of 
First International Natural Language Generation 
Conference (INLG-2000), pp. 116-123. 
Kees van Deemter 2002. Generating Referring 
Expressions: Boolean Extensions of the Incremental 
Algorithm. Computational Linguistics, 28(1):37-52.
103
104
105
106
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 377?384,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Transformation-based Interpretation of Implicit Parallel Structures:
Reconstructing the meaning of vice versa and similar linguistic operators
Helmut Horacek
Fachrichtung Informatik
Universit?at des Saarlandes
66041 Saarbr?ucken, Germany
horacek@ags.uni-sb.de
Magdalena Wolska
Fachrichtung Allgemeine Linguistik
Universit?at des Saarlandes
66041 Saarbr?ucken, Germany
magda@coli.uni-sb.de
Abstract
Successful participation in dialogue as
well as understanding written text re-
quires, among others, interpretation of
specifications implicitly conveyed through
parallel structures. While those whose re-
construction requires insertion of a miss-
ing element, such as gapping and ellip-
sis, have been addressed to a certain extent
by computational approaches, there is vir-
tually no work addressing parallel struc-
tures headed by vice versa-like operators,
whose reconstruction requires transforma-
tion. In this paper, we address the mean-
ing reconstruction of such constructs by
an informed reasoning process. The ap-
plied techniques include building deep se-
mantic representations, application of cat-
egories of patterns underlying a formal
reconstruction, and using pragmatically-
motivated and empirically justified prefer-
ences. We present an evaluation of our al-
gorithm conducted on a uniform collection
of texts containing the phrases in question.
1 Introduction
Specifications implicitly conveyed through paral-
lel structures are an effective means of human
communication. Handling these utterances ade-
quately is, however, problematic for a machine
since a formal reconstruction of the representation
may be associated with ambiguities, typically re-
quiring some degree of context understanding and
domain knowledge in their interpretation. While
parallel structures whose reconstruction mainly re-
quires insertion, such as gapping and ellipsis, have
been addressed to a certain extent by computa-
tional approaches, there is virtually no work ad-
dressing parallel structures whose reconstruction
requires transformation. Several linguistic opera-
tors create specifications of this kind, including:
the other way (a)round, vice-versa, and analo-
gously. Consider, for example, the following state-
ment made by a student in an experiment with a
simulated tutoring system for proving theorems in
elementary set theory (Benzmu?ller et al, 2003):
?If all A are contained in K(B) and this also holds
the other way round, these must be identical sets?
(K stands for set complement). The interpreta-
tion of the the other way round operator is am-
biguous here in that it may operate on immediate
dependents (?all K(B) are contained in A?) or on
the embedded dependents (?all B are contained in
K(A)?) of the verb ?contain?. The fact that the
Containment relation is asymmetric and the con-
text of the task ? proving that ?If A ? K(B), then
B ? K(A)? holds ? suggest that the second inter-
pretation is meant. Assuming this more plausible
reading enables a more goal-oriented dialog: the
tutorial system can focus on a response to the false
conclusion made by the student about the identity
of the sets in question, rather than starting a boring
clarification subdialog.
The above example and several similar others
motivated us to look more systematically at lexi-
cal devices that create specifications of this kind.
We address the interpretation of such structures by
a well-informed reasoning process. Applied tech-
niques include building deep semantic represen-
tations, application of patterns underlying formal
reconstruction, and using pragmatically-motivated
and empirically justified preferences.
The outline of the paper is as follows: We de-
scribe phenomena in question. Then we illustrate
our natural language analysis techniques. We cate-
377
gorize underlying interpretation patterns, describe
the reconstruction algorithm, and evaluate it.
2 Data Collected From Corpora
In order to learn about cross-linguistic regularities
in reconstructing the underlying form of propo-
sitions specified by vice versa or similar opera-
tors, we first looked at several English and Ger-
man corpora. These included, among others, the
Negra, the Frankfurter Rundschau, the Europarl
corpora and a corpus of tutorial dialogs on math-
ematics (Wolska et al, 2004). We also performed
several internet searches. We looked at the Ger-
man phrases andersrum and umgekehrt, and their
English equivalents vice versa and the other way
(a)round. We only considered instances where the
parallel structure with a pair of items swapped is
not stated explicitly. We excluded cases of the
use of umgekehrt as a discourse marker, cases in
which the transformation needed is of purely lex-
ical nature, such as turning ?augment? into ?re-
duce?, and instances of andersrum as expressing a
purely physical change, such as altering the orien-
tation of an object (cf. the Bielefeld corpus1).
The classification of vice versa utterances pre-
sented in Figure 1, reflects the role of the items
that must be swapped to build the parallel propo-
sition conveyed implicitly. The examples demon-
strate that the task of reconstructing the proposi-
tion left implicit in the text may be tricky.
The first category concerns swapping two case
role fillers or Arguments of a predicate head. This
may be applied to Agent and Patient dependents,
as in (1), or to two directional roles as in (2). In
the last example in this category, complications
arise due to the fact that one of the arguments
is missing on the surface and needs to be con-
textually inserted prior to building the assertions
with exchanged directional arguments. Moreover,
the swap can also work across clauses as in (3).
Complex interrelations may occur when the fillers
themselves are composed structures, is in (4),
which also makes swapping other pairs of items
structurally possible. In this example, the need for
exchanging the persons including their mentioned
body parts rather than the mere body parts or just
the persons requires world knowledge.
The second category comprises swapping ap-
plied to modifiers of two arguments rather than the
arguments themselves. An example is (5); the ut-
1http://www.sfb360.uni-bielefeld.de/
terance is ambiguous since, from a purely struc-
tural point of view, it could also be categorized as
an Argument swap, however, given world knowl-
edge, this interpretation is rather infelicitous. Sim-
ilarly to (3), a contextually-motivated enhance-
ment prior to applying a swapping operation is re-
quired in (6); here: a metonymic extension, i.e.
expanding the ?strings? to ?the strings? tones?.
The third category comprises occurrences of a
?mixed? form of the first two with a modifier sub-
stituted for an argument which, in turn, takes the
role of the modifier in the reconstructed form. The
first example, (7), has already been discussed in
the Introduction. The next one, (8), illustrates re-
peated occurrences of the items to be swapped.
Moreover, swapping the items A and B must be
propagated to the included formula. The next ex-
ample, (9), is handled by applying the exchange
on the basis of the surface structure: swapping the
properties of a triangle for the reconstructed asser-
tion. If a deeper structure of the sentence?s mean-
ing is built, this would amount to an implication
expressing the fact that a triangle with two sides
of equal length is a triangle that has two equal
angles. For such a structure, the reconstruction
would fall into the next category, exchange of the
order of two propositions: here, reversing the im-
plication. In (10), the lexeme ?Saxophonist? needs
to be expanded into ?Saxophone? and ?Spieler?
(?player?), prior to performing the exchange.
The fourth category involves a swap of entire
Propositions; in the domain of mathematics, this
may pertain to formulas. In (11), swapping applies
to the sides of the equation descriptively referred
to by the distributivity law. In (12), this applies to
the arguments of the set inclusion relation, when
the arguments are interpreted as propositions. The
last example, (13), requires a structural recasting
in order to apply the appropriate swapping oper-
ation. When the utterance is rebuilt around the
RESULT relation, expressed as an optional case
role on the surface, swapping the two propositions
? ?branching out of languages? and ?geographical
separation? ? yields the desired result.
3 The Interpretation Procedure
In this section, we illustrate our technical contri-
bution. It consists of three parts, each dealt with in
a separate subsection: (1) the linguistic/semantic
analysis, (2) definitions of rules that support build-
ing parallel structures, and (3) the algorithm.
378
Arg
um
ent
sw
ap ( 1) Technological developments influence the regulatory framework and vice versa.
( 2) It discusses all modes of transport from the European Union to these third countries and viceversa.
( 3) Ok ? so the affix on the verb is the trigger and the NP is the target. . . . No; the other way round
( 4) Da traf Vo?ller mit seinem Unterarm auf die Hu?fte des fu?r Glasgow Rangers spielenden Ukrain-ers, oder umgekehrt
Then Vo?ller with his lower arm hit the hip of the Ukrainian playing for Glasgow Rangers, or
the other way round
Mo
difi
ers
wa
p
( 5) Nowadays, a surgeon in Rome can operate on an ill patient ? usually an elderly patient ? inFinland or Belgium and vice versa.
( 6) Der Ton der Klarinette ist wirklich ganz komplementa?r zu den Seiteninstrumenten undumgekehrt
The clarinet?s tone is really very complimentary to strings and vice-versa
Mi
xed
swa
p
( 7) Wenn alle A in K(B) enthalten sind und dies auch umgekehrt gilt, mu? es sich um zwei iden-tische Mengen handeln
If all A are contained in K(B) and this also holds vice-versa, these must be identical sets
( 8) Dann ist das Komplement von Menge A in Bezug auf B die Differenz A/B = K(A) undumgekehrt
Then the complement of set A in relation to B is the difference A/B = K(A) and vice-versa
( 9) Ein Dreieck mit zwei gleichlangen Seiten hat zwei gleichgro?e Winkel und umgekehrt
A triangle with two sites of equal length has two angles of equal size, and vice-versa
( 10) . . . Klarinette fu?r Saxophonist und umgekehrt . . .
. . . a clarinet for a saxophonist and vice-versa . . .
Pro
pos
itio
ns
wa
p ( 11) Man mu? hier das Gesetz der Distributivita?t von Durchschnitt u?ber Vereinigung umgekehrtanwenden
It is necessary here to apply the law of distributivity of intersection over union in reverse
direction
( 12) Es gilt: P (C ? (A ?B)) ? P (C) ? P (A ?B). . . . . Nein, andersrum.
It holds: P (C ? (A ?B)) ? P (C) ? P (A ?B). . . . . No, the other way round.
( 13) Wir wissen, da? sich Sprachen in Folge von geographischer Separierung auseinanderentwick-eln, und nicht umgekehrt
We know that languages branch out as a result of geographical separation, not the other way
round
Figure 1: Examples of utterances with vice versa or similar operators
379
contain.PRED : Containment ? ?,?,?
TERM:K(B).ACT : Container TERM:A.PAT : Containee
Figure 2: Interpreted representation of the utter-ance ?all A are contained in K(B)?
3.1 Linguistic Analysis
The linguistic analysis consists of semantic pars-
ing followed by contextually motivated embed-
ding and enhancements. We assume a deep se-
mantic dependency-based analysis of the source
text. The input to our reconstruction algorithm is
a relational structure representing a dependency-
based deep semantics of the utterance, e.g. in the
sense of Prague School sentence meaning, as em-
ployed in the Functional Generative Description
(FGD) at the tectogrammatical level (Sgall et al,
1986). In FGD, the central frame unit of a clause
is the head verb which specifies the tectogram-
matical relations (TRs) of its dependents (partici-
pants/modifications). Every valency frame spec-
ifies, moreover, which modifications are obliga-
tory and which optional. For example, the utter-
ance (7) (see Figure 1.) obtains the interpretation
presented in Figure 2.2 which, in the context of
an informal verbalization of a step in a naive set
theory proof, translates into the following formal
statement: ??x.x ? A? x ? K(B)?.
The meaning representations are embedded
within discourse context and discourse relations
between adjacent utterances are inferred where
possible, based on the linguistic indicators (dis-
course markers). The nodes (heads) and de-
pendency relations of the interpreted dependency
structures as well as discourse-level relations serve
as input to instantiate the reconstruction pat-
terns. Contextual enhancements (e.g. lexical or
metonymic extensions) driven by the reconstruc-
tion requirements may be carried out.
Based on analysis of corpora, we have iden-
tified combinations of dependency relations that
commonly participate in the swapping operation
called for by the vice versa phrases. Examples of
pairs of such relations at sentence level are shown
in Figure 3.3 Similarly, in the discourse context,
arguments in, for example, CAUSE, RESULT ,
CONDITION , SEQUENCE or LIST rela-
2We present a simplified schematic representation ofthe tectogrammatical representations. Where necessary, forspace reasons, irrelevant parts are omitted.3PRED is the immediate predicate head of the corre-sponding relation.
Exchangeable(ACTOR, PATIENT)
Exchangeable(DIRECTION-WHERE-FROM,DIRECTION-WHERE-TO)
Exchangeable(TIME-TILL-WHEN,TIME-FROM-WHEN)
Exchangeable(CAUSE, PRED)
Exchangeable(CONDITION, PRED)
Figure 3: Examples of exchangeable relations
tions are likely candidates for a swapping opera-
tion. During processing, we use the association
table as a preference criterion for selecting candi-
date relations to instantiate patterns. If one of the
elements of a candidate pair is an optional argu-
ment that is not realized in the given sentence, we
look at the preceding context to find the first in-
stance of the missing element. Additionally, utter-
ance (10) would call for more complex procedures
to identify the required metonymic expansion.
3.2 Interpretation Patterns
In order to accomplish the formal reconstruction
task, we define rules that encapsulate specifica-
tions for building the implicit parallel text on the
basis of the corresponding co-text. The rules con-
sist of a pattern and an action part. Patterns are
matched against the output of a parser on a text
portion in question, by identifying relevant case
roles, and giving access to their fillers. Moreover,
the patterns test constraints on compatibility of
candidates for swapping operations. The actions
apply recasting operations on the items identified
by the patterns to build the implicit parallel text.
Within patterns, we perform category member-
ship tests on the representation. Assuming x re-
ferring to a semantic representation, Pred(x) is
a logical function that checks if x has a Pred-
feature, i.e., it is an atomic proposition. Simi-
larly, Conj(x) and Subord(x) perform more spe-
cific tests for complex propositions: coordina-
tion or subordination, respectively. Moreover,
Pred1(x, x1) accesses the first proposition andbinds it to x1, while Pred2(x, x2) does the samefor the second one. Within a proposition, argu-
ments and modifiers are accessed by Case(x, y),
where y specifies the filler of Case in x, and in-
dices express constraints on identity or distinc-
tiveness of the relations. Case+ is a generaliza-
tion of Case for iterative embeddings, where in-
dividual cases in the chain are not required to be
380
1a. Argument swap within the same clause
Pred(x) ? Case1(x, y) ?Case2(x, z)?
Type? compatible(y, z) ?
Exchangeable(Case1, Case2)?
Swap(x, y, z, xp)
1b. Argument swap across two clauses
Conj(x) ? Case1(x, y) ?Case(y, u) ?
Case2(x, z) ? Case(z, v)? Swap(x, u, v, xp)
2. Modifier swap
Pred(x) ? Case1(x, y) ? Case+11(y, u) ?
Case2(x, z) ?Case+21(z, v)?
?(Case1 = Case2) ? Type?
compatible(u, v)? Swap(x, u, v, xp)
3. Mixed swap
Pred(x) ? Case1(x, y) ? Case11(y, u) ?
Case2(x, z)?
?(Case1 = Case2) ? Type?
compatible(u, z)? Swap(x, u, z, xp)
4. Proposition swap
Subord(x) ? Case1(x, y) ? Case2(x, z) ?
?(Case1 = Case2)? Swap(x, y, z, xp)
Figure 4: Reconstruction patterns
identical. In addition to access predicates, there
are test predicates that express constraints on the
identified items. The most basic one is Type-
compatible(x, y), which tests whether the types
of x and y are compatible according to an underly-
ing domain ontology. A more specific test is per-
formed by Exchangeable(Case1, Case2) to ac-cess the associations specified in the previous sec-
tion. The action part of the patterns is realized by
Swap(x, y, z, xp) which replaces all occurrencesof x in z by y and vice-versa, binding the result to
xp. Different uses of this operation result in dif-ferent instantiations of y and z with respect to the
overarching structure x.
There are patterns for each category introduced
in Section 2 (see Figure 4). All patterns are tested
on a structure x and, if successful, the result is
bound to xp. For Argument swap there are twopatterns. If the scope of the swap is a single
clause (1a), two arguments (case roles) identified
as exchangeable are picked. Their fillers must be
compatible in types. If the swapping overarches
two clauses (1b), the connecting relation must be
a conjunction and subject to swapping are argu-
ments in the same relations. For Modifier swap
(2), type compatible modifiers of distinct argu-
ments are picked. For Mixed swap (3), a depen-
1. Lexical expansion
Pred(x) ? Case1(x, y) ? Lex?
Expand(y, u, Case, v)?
Case2(x, z) ? ?(Case1 =
Case2) ? Type? compatible(v, z) ?
Swap(x, y, Case(u, v), xp) ? Swap(xp, z, v, xp)
2. Recast optional case as head of an obligatory
Pred(x) ?Case1(x, u) ?Case2(x, v) ?
Type(u, tu) ? Type(v, tv)?
Recastable(tv, Case2, tu, Case3) ?
Case3(x,w) ? Type? compatible(v, w)?
?(Case1 = Case2) ? ?(Case1 =
Case3) ? ?(Case2 = Case3)?
Swap(x, u, v, xp) ?Add(xp, Case3(v, u)) ?
Remove(xp, Case2)
3. Recast an optional case as a discourse relation
Pred(x) ? Case(x, y) ?
Member(Case, Subords)?
Build(Case(xp, Case2(xp, y) ?
Case1(xp, Remove(x, y))
Figure 5: Recasting rules
dent is picked, as in (1a) and a type-compatible
modifier of another argument, as in (2). Proposi-
tion swap (4) inverts the order of the two clauses.
In addition to the the pattern matching tests,
the Argument and the Proposition swap operations
undergo a feasibility test if knowledge is avail-
able about symmetry or asymmetry of the relation
(the Pred feature) whose cases are subject to the
swapping operation: if such a relation is known as
asymmetric, the result is considered implausible
due to semantic reasons, if it is symmetric, due to
pragmatic reasons since the converse proposition
conveys no new information; in both cases such a
swapping operation is not carried out.
To extend the functionality of the patterns, we
defined a set of recasting rules (Figure 5) invoked
to reorganize the semantic representation prior to
testing applicability of a suitable reconstruction
rule. In contrast to inserting incomplete informa-
tion contextually and expanding metonymic rela-
tions the recasting operations are intended purely
to accommodate semantic representations for this
purpose. We have defined three recasting rules
(numbered accordingly in Figure 5):
1. Lexical recasting
The semantics of some lexemes conflates the
meaning of two related items. If one of them
is potentially subject to swapping, it is not ac-
cessible for the operation without possibly af-
381
Build-Parallel-Structure (x)
1. Determine scopes for applying swap operations
Structures? ?
if Pred(x) then Scopes? {x} else
if Subord(x) ? Conj(x) ? Case2(x, z)
then Scopes? {z, x}
endif endif
2. Match patterns and build swapped structures
forall Scope1 in Scopes do
Structures? Structures?
< X ? swap(Scope1) >
< X ? swap(Y ? recast(Scope1)) >
end forall
return Sort(Apply ? priorities(Structures))
Figure 6: Reconstruction algorithm
fecting the other so closely related to it. The
representation of such lexemes is expanded,
provided there is a sister case with a filler that
is type compatible.
2. Case recasting
The dependency among items may not be re-
flected by the dependencies in the linguistic
structure. Specifically, a dependent item may
appear as a sister case in overarching case
frame. The purpose of this operation is to
build a uniform representation, by removing
the dependent case role filler and inserting it
as a modifier of the item it is dependent on.
3. Proposition recasting
Apart from expressing a discourse relation
by a connective, a proposition filling a sub-
ordinate relation may also be expressed as a
case role (argument). Again, uniformity is
obtained through lifting the argument (case
filler) and expressing the discourse relation as
a multiple clause construct.
Additional predicates are used to implement re-
casting operations. For example, the predicate
Lex?Expand(y, u, Case, v) re-expresses the se-
mantics of y by u, accompanied by a Case role
filled by v. Type(x, y) associates the type y
with x. The type information is used to access
Recastable(t1, C1, t2, C2) table to verify whethercase C1 with a t1-type filler can also be expressedas case C2 with type t2. Build(x) creates a newstructure x. Remove(x, y) is realized as a func-
tion, deleting occurrences of y in x, and Add(x, y)
expands x by an argument y.
3.3 The Structure Building Algorithm
In this section, we describe how we build implic-
itly conveyed parallel structures based on the def-
initions of swapping operations with optional in-
corporation of recasting operations if needed. The
procedure consists of two main parts (see Fig-
ure 6). In the first part, the scope for applying the
swapping rules defined in Figure 4 is determined,
and in the second part, the results obtained by ex-
ecuting the rules are collected. Due to practical
reasons, we introduce simplifications concerning
the scope of vice-versa in the current formulation
of the procedure. While the effect of this operator
may range over entire paragraphs in some involved
texts, we only consider single sentences with at
most two coordinated clauses or one subordinated
clause. We feel that this restriction is not severe
for uses in application-oriented systems.
The procedure Build-Parallel-Structure takes
the last input sentence x, examines its clause
structure, and binds potential scopes to vari-
able Scopes. For composed sentences, the en-
tire sentence (x) as well as the second clause
(Case2(x, z)) is a potential scope for building par-allel structures.
In the second part of the procedure, each swap-
ping pattern is tested for the two potential scopes,
and results are accumulated in Structures. The
call < X ? swap(Scope1) >, with X beingeither Case, Argument, Mixed, or Prop ex-
presses building a set of all possible instantiations
of the pattern specified when applied to Scope1.Some of these operations are additionally invoked
with alternative parameters which are accommo-
dated by a recasting operation fitting to the pat-
tern used, that call being < X ? swap(Y ?
recast(Scope1)) >, where Y is Case, Lex, or
Prop. Finally, if multiple readings are generated,
they are ranked according to the following priori-
tized criteria:
1. The nearest scope is preferred;
2. Operations swapping ?duals?, such as left-right, aregiven priority;
3. Candidate phrases are matched against the corpus;items with higher bigram frequencies are preferred.
Linguistic analysis, structure reconstruction
patterns, recasting rules, and the algorithms oper-
ating on top of these structures are formulated in
a domain-independent way, also taking care that
the tasks involved are clearly separated. Hence, it
is up to a concrete application to elaborate lexical
382
semantic definitions required (e.g. for a saxophon-
ist to capture example (10) in Figure 1) to define
the tables Exchangeable and Recastable, and to
enhance preference criteria.
4 Evaluation
We conducted an evaluation of the parallel struc-
ture building algorithm on a sample of sentences
from Europarl (Koehn, 2002), a parallel corpus of
professionally translated proceedings of the Euro-
pean Parliament aligned at the document and sen-
tence level. At this point, we were able to conduct
only manual evaluation. This is mainly due to the
fact that we did not have access to a wide-coverage
semantic dependency parser for English and Ger-
man.4 In this section, we present our corpus sam-
ple and the evaluation results.
Evaluation sample To build the evaluation sam-
ple, we used sentence- and word-tokenized En-
glish German part of Europarl. Using regular ex-
pressions, we extracted sentences with the follow-
ing patterns: (i) for English, phrases the other way
a*round or vice versa (ii) for German: (ii-1) the
word umgekehrt preceded by a sequence of und
(?and?), oder (?or?), sondern (?but?), aber (?but?)
or comma, optional one or two tokens and op-
tional nicht (?not?), (ii-2) the word umgekehrt pre-
ceded by a sequence gilt (?holds?) and one or two
optional tokens, (ii-3): the word anders(he)*rum.
We obtained 137 sentences.
Next, given the present limitation of our algo-
rithm (see Section 3.3), we manually excluded
those whose interpretation involved the preceding
sentence or paragraph,5 as well as those in which
the interpretation was explicitly spelled out. There
were 27 such instances. Our final evaluation sam-
ple consisted of 110 sentences: 82 sentences in
English?German pairs and 28 German-only.6
4In the future, we are planning an automated evaluation inwhich as input to the implemented algorithm we would passmanually built dependency structures.5For example, sentences such as: ?Mr President , concern-ing Amendment No 25 , I think the text needs to be lookedat because in the original it is the other way round to how itappears in the English text .?6The reason for this split is that the English equivalentsof the German sentences containing the word umgekehrt maycontain phrases other than the other way round or vice versa.Depending on context, phrases such as conversely, in or the
reverse, the opposite, on the contrary may be used. Here, wetargeted only the other way round and vice versa phrases. Ifthe German translation contained the word umgekehrt, andthe English source one of the alternatives to our target, in theevaluation we included only the German sentence.
Category No. of instances
Arg 64
Modifier 5
Arg/Mod 3
Mixed 6
Arg/Mixed 2
Prop 1
Arg/Prop 1
Lex 18
Other 10
Total 110
Table 1: Distribution of patterns
Distribution of categories We manually cate-
gorized the structures in our sample and marked
the elements of the dependency structures that par-
ticipate in the transformation. Table 1. presents
the distribution of structure categories. We ex-
plicitly included counts for alternative interpreta-
tions. For example Arg/Mod means that either
the Argument or Modifier transformation can be
applied with the same effect, as in the sentence
?External policy has become internal policy, and
vice versa?: either the words ?external? and ?in-
ternal? may be swapped (Modifier), or the whole
NPs ?external policy? and ?internal policy? (Ar-
gument). Lex means that none of the patterns was
applicable and a lexical paraphrase (such as use of
an antonym) needed to be performed in order to re-
construct the underlying semantics (i.e. no paral-
lel structure was involved). Other means that there
was a parallel structure involved, however, none of
our patterns covered the intended transformation.
Evaluation results The evaluation results are
presented in Tables 2. and 3. Table 2. shows an
overview of the results. The interpretation of the
result categories is as follows:
Correct: the algorithm returned the intended reading asa unique interpretation (this includes correct identi-fication of ?lexical paraphrases? (the Lex categoryin Table 1.);
Ambig.: multiple results were returned with the intendedreading among them;
Wrong: the algorithm returned a wrong result (if multi-ple results, then the intended one was not included);
Failed: the algorithm failed to recognize a parallel struc-ture where one existed because no known patternmatched.
Table 3. shows within-category results. Here, Cor-
rect result for Other means that the algorithm cor-
rectly identified 8 cases to which no current pat-
tern applied. The two Wrong results for Other
383
Result No. of instances
Correct 75
Ambig. 21
Wrong 4
Failed 10
Total 110
Table 2: Evaluation results
Category Correct Ambig. Wrong Failed Total
Arg 46 17 0 1 64
Mod 3 2 0 0 5
Arg/Mod 3 ? 0 0 3
Mixed 4 2 0 0 6
Arg/Mixed 2 ? 0 0 2
Prop 1 0 0 0 1
Arg/Prop 0 ? 0 1 1
Lex 16 0 2 0 18
Other 8 0 2 0 10
Table 3: Within-category results
mean that a pattern was identified, however, this
pattern was not the intended one. In two cases
(false-negatives), the algorithm failed to identify
a pattern even though it fell into one of the known
categories (Argument and Prop).
Discussion The most frequently occurring pat-
tern in our sample is Argument. This is often a
plausible reading. However, in 3 of the 4 false-
positives (Wrong results), the resolved incorrect
structure was Arg. If we were to take Arg as base-
line, aside from missing the other categories (al-
together 12 instances), we would obtain the final
result of 63 Correct (as opposed to 96; after col-
lapsing the Correct and Ambig. categories) and
15 (as opposed to 4) Wrong results.
Let us take a closer look at the false-negative
cases and the missed patterns. Two missed known
categories involved multiple arguments of the
main head: a modal modifier (modal verb) and an
additive particles (?also?) in one case, and in the
other, rephrasing after transformation. To improve
performance on cases such as the former, we could
incorporate an exclusion list of dependents that the
transformation should disregard.
Among the patterns currently unknown to the
algorithm, we found four types (one instance of
each in the sample) that we can anticipate as fre-
quently recurring: aim and recipient constructs
involving a head and its Aim- and Beneficiary-
dependent respectively, a temporal-sequence in
which the order of the sequence elements is re-
versed, and a comparative structure with swapped
relata. The remaining 6 structures require a more
involved procedure: either the target dependent is
deeply embedded or paraphrasing and/or morpho-
logical transformation of the lexemes is required.
5 Conclusions and Future Research
In this paper, we presented techniques of for-
mal reconstruction of parallel structures implicitly
specified by vice versa or similar operators. We
addressed the problem by a domain-independent
analysis method that uses deep semantics and con-
textually enhanced representations, exploits re-
casting rules to accommodate linguistic variations
into uniform expressions, and makes use of pat-
terns to match parallel structure categories.
Although we dedicated a lot of effort to building
a principled method, the success is limited with
respect to the generality of the problem: in some
cases, the scope of reconstruction overarches en-
tire paragraphs and deciding about the form re-
quires considerable inferencing (cf. collection at
http://www.chiasmus.com/). For our purposes, we
are interested in expanding our method to other
kinds of implicit structures in the tutorial context,
for example, interpretations of references to analo-
gies, in the case of which structure accommoda-
tion and swapping related items should also be
prominent parts.
References
C. Benzmu?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayova?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B.Q.
Vo, and M. Wolska. 2003. A Wizard-of-Oz experiment
for tutorial dialogues in mathematics. In Supplementary
Proceedings of the 11th Conference on Artificial Intelli-
gence in Education (AIED-03); Vol. VIII. Workshop on
Advanced Technologies for Mathematics Education, pages
471?481, Sydney, Australia.
P. Koehn. 2002. Europarl: A multilingual corpus for evalua-
tion of machine translation, Draft, Unpublished.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The meaning of
the sentence in its semantic and pragmatic aspects. Reidel
Publishing Company, Dordrecht, The Netherlands.
M. Wolska, B.Q. Vo, D. Tsovaltzi, I. Kruijff-Korbayova?,
E. Karagjosova, H. Horacek, M. Gabsdil, A. Fiedler, and
C. Benzmu?ller. 2004. An annotated corpus of tutorial
dialogs on mathematical theorem proving. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC-04), pages 1007?1010,
Lisbon, Potugal.
384
Lexical-Semantic Interpretation of Language Input
in Mathematical Dialogs
Magdalena Wolska1 Ivana Kruijff-Korbayova?1 Helmut Horacek2
1Fachrichtung Computerlinguistik 2Fachrichtung Informatik
Universita?t des Saarlandes, Postfach 15 11 50
66041 Saarbru?cken, Germany
{magda,korbay}@coli.uni-sb.de, horacek@ags.uni-sb.de
Abstract
Discourse in formal domains, such as mathematics,
is characterized by a mixture of telegraphic natu-
ral language and embedded (semi-)formal symbolic
mathematical expressions. Due to the lack of em-
pirical data, little is known about the suitability of
input analysis methods for mathematical discourse
in a dialog setting. We present an input understand-
ing method for a tutoring system teaching mathe-
matical theorem proving. The adopted deep anal-
ysis strategy is motivated by the complexity of the
language phenomena observed in a corpus collected
in a Wizard-of-Oz experiment. Our goal is a uni-
form input interpretation, in particular, considering
different degrees of formality of natural language
verbalizations.
1 Introduction
In the DIALOG1 project (Benzm u?ller et al, 2003a),
we are investigating and modeling semantic and
pragmatic phenomena in tutorial dialogs focused on
problem solving skills in mathematics. Our goal is
(i) to empirically investigate the use of flexible natu-
ral language dialog in tutoring mathematics, and (ii)
to develop a dialog-based tutoring system for teach-
ing mathematical theorem proving. The experimen-
tal system will engage in a dialog in written natural
language to help a student understand and construct
mathematical proofs. In this paper, we address a
strategy for user input interpretation in our setting.
Because of the lack of empirical dialog-data on
the use of natural language in formal domains, such
as mathematics, we conducted a Wizard-of-Oz ex-
periment to collect a corpus of dialogs with a sim-
ulated system teaching proofs in naive set theory.
An investigation of the corpus reveals language phe-
nomena that present challenges to the existing com-
monly used input understanding methods. The chal-
1The DIALOG project is a collaboration between the Com-
puter Science and Computational Linguistics departments of
University of the Saarland, and is a part of the Collaborative
Research Center on Resource-Adaptive Cognitive Processes,
SFB 378 (www.coli.uni-sb.de/sfb378).
lenges lie in (i) the tight interleaving of natural and
symbolic language, (ii) varying degree of natural
language verbalization of the formal mathematical
content, and (iii) informal and/or imprecise refer-
ence to mathematical concepts and relations.
These phenomena motivate the use of deep syn-
tactic and semantic analysis of user input. We de-
veloped a grammar that allows a uniform treatment
of the linguistic content on a par with the math-
ematical content and thus supports analysis of in-
puts of different degrees of verbalization. We em-
ploy a domain-motivated semantic lexicon to medi-
ate between the domain-independent semantic rep-
resentation obtained through semantic construction
during parsing and domain-specific interpretation.
This serves to achieve a consistent semantic anal-
ysis while avoiding example-based heuristics.
The paper is organized as follows: In Sect. 2,
we present the setup of the system and the corpus
collection experiment. In Sect. 3 and show exam-
ples of language phenomena from our dialogs. In
Sect. 4, we first summarize the approach to pars-
ing the mixed symbolic and natural language in-
put and then present a lexical-semantic interface to
a domain-specific interpretation of the input. We
show example analyzes in Sect. 5. In Sect. 6,
we summarize relevant existing approaches to input
analysis in (tutorial) dialog systems on the one hand
and analysis of mathematical discourse on the other.
Sect. 7 is the conclusion.
2 System setup and corpus collection
Our system scenario is illustrated in Fig. 1:
? Learning Environment: Students take an inter-
active course in the relevant subfield of mathe-
matics.
? Mathematical Proof Assistant (MPA): Checks
the appropriateness of user specified inference
steps with respect to the problem-solving goal;
based on ?MEGA.
? Proof Manager (PM): In the course of tutor-
ing session the student may explore alternative
PEDAGOGICAL
KNOWLEDGE
US
ER
MO
DE
L
LEARNING
ENVIRONMENT
MATHEMATICAL
PROOF ASSISTANT
DIALOG MANAGERG
EN
ER
A
TI
O
N
PRO
O
F M
A
N
A
G
ER
A
N
A
LY
SIS
MATHEMATICAL
KNOWLEDGE
(MBASE)
ACTIVEMATH OMEGA
RESOURCES
LINGUISTIC DIALOG
RESOURCES
TUTORING
RESOURCES /
MANAGER
U
SE
R
Figure 1: DIALOG project scenario.
proofs. The PM builds and maintains a repre-
sentation of constructed proofs and communi-
cates with the MPA to evaluate the appropriate-
ness of the student?s contributions for the proof
construction.
? Dialog Manager: We employ the Information-
State (IS) Update approach developed in the
TRINDI project2
? Tutorial Manager (TM): This component
incorporates extensions to handle tutorial-
specific dialog moves, such as hinting.
? Knowledge Resources: This includes peda-
gogical knowledge (teaching strategies), and
mathematical knowledge.
In order to empirically investigate the use of nat-
ural language in mathematics tutoring, we collected
and analyzed a corpus of dialogs with a simulated
tutoring system.
24 subjects with varying educational background
and little/fair prior mathematical knowledge partic-
ipated in a Wizard-of-Oz experiment (Benzm u?ller et
al., 2003b). The experiment consisted of 3 phases:
(i) preparation and pre-test (on paper), (ii) tutor-
ing session (mediated by a WOz tool (Fiedler and
Gabsdil, 2002)), (iii) post-test and evaluation ques-
tionnaire (on paper). At the tutoring session, they
were asked to prove 3 theorems3: (i) K((A ? B) ?
(C ? D)) = (K(A) ? K(B)) ? (K(C) ? K(D));
(ii) A ? B ? P ((A ? C) ? (B ? C)); (iii) If
A ? K(B), then B ? K(A). The subjects were
instructed to enter proof steps, rather than complete
proofs at once, to encourage interaction with the
system. The subjects and the tutor were free in for-
mulating their turns.4
2http://www.ling.gu.se/research/projects/trindi/
3K stands for set complement and P for power set.
4Buttons were available in the interface for inserting math-
ematical symbols, while literals were typed on the keyboard.
The collected corpus consists of 66 dialog log-
files, containing on average 12 turns. The total num-
ber of sentences is 1115, of which 393 are student
sentences. The students? turns consisted on aver-
age of 1 sentence, the tutor?s of 2. More details on
the corpus itself and annotation efforts that guide
the development of the system components can be
found in (Wolska et al, 2004).
3 Linguistic data
In this section, we present an overview of the lan-
guage phenomena prominent in the collected di-
alogs to indicate the overall complexity of input un-
derstanding in our setting.5
Interleaved natural language and formulas The
following examples illustrate how the mathematical
language, often semi-formal, is interleaved with the
natural language informally verbalizing proof steps.
A auch ? K(B) [Aalso ? K (B)]
A?B ist ? von C?(A?B) [... is ? of ...]
(da ja A?B=?) [(because A?B=?)]
B enthaelt kein x?A [B contains no x?A]
The mixture affects the way parsing needs to be
conducted: mathematical content has to be identi-
fied before it is interpreted within the utterance. In
particular, mathematical objects (or parts thereof)
may lie within the scope of quantifiers or negation
expressed in natural language (as in the last example
above).
Imprecise or informal naming Domain relations
and concepts are described informally using impre-
cise and/or ambiguous expressions.
A enthaelt B [A contains B]
A muss in B sein [A must be in B]
B vollstaendig ausserhalb von A liegen muss, also im
Komplement von A
[B has to be entirely outside of A, so in the complement of A]
dann sind A und B (vollkommen) verschieden, haben keine
gemeinsamen Elemente
[then A and B are (completely) different, have no common
elements]
In the above examples, contain and be in can ex-
press domain relations of (strict) subset or element,
while be outside of and be different are informal
descriptions of the empty intersection of sets.
To handle imprecision and informality, we have
designed an ontological knowledge base that in-
cludes domain-specific interpretations of concep-
tual relations that have corresponding formal coun-
terparts in the domain of naive set theory.
The dialogs were typed in German.
5As the tutor was also free in wording his turns, we include
observations from both student and tutor language behavior.
Metonymy Metonymic expressions are used to
refer to structural sub-parts of formulas, resulting
in predicate structures acceptable informally, yet in-
compatible in terms of selection restrictions.
Dann gilt fuer die linke Seite, wenn
C ? (A ? B) = (A ? C) ? (B ?C), der Begriff A ? B dann ja
schon dadrin und ist somit auch Element davon
[Then for the left hand side it is valid that..., the term A ? B is already
there, and so an element of it]
where the predicate be valid for, in this domain,
normally takes an argument of sort CONSTANT,
TERM or FORMULA, rather than LOCATION;
de morgan regel 2 auf beide komplemente angewendet
[de morgan rule 2 applied to both complements]
where the predicate apply takes two arguments: one
of sort RULE and the other of sort TERM or FOR-
MULA, rather than OPERATION ON SETS.
Informal descriptions of proof-step actions
Wende zweimal die DeMorgan-Regel an
[I?m applying DeMorgan rule twice]
damit kann ich den oberen Ausdruck wie folgt schreiben:. . .
[given this I can write the upper term as follows:. . . ]
Sometimes, ?actions? involving terms, formulae
or parts thereof are verbalized before the appropri-
ate formal operation is performed. The meaning of
the ?action verbs? is needed for the interpretation of
the intended proof-step.
Discourse deixis
der obere Ausdruck [the above term]
der letzte Satz [the last sentence]
Folgerung aus dem Obigen [conclusion from the above]
aus der regel in der zweiten Zeile
[from the rule in the second line]
This class of referring expressions includes also
references to structural parts of terms and formu-
las such as ?the left side? or ?the inner parenthe-
sis? which are incomplete specifications: the former
refers to a part of a formula, the latter, metonymic,
to an expression enclosed in parenthesis. More-
over, they require discourse referents for sub-parts
of mathematical expressions to be available.
Generic vs. specific reference
Potenzmenge enthaelt alle Teilmengen, also auch (A?B)
[A power set contains all subsets, hence also(A?B)]
Generic and specific references can appear within
one utterance as above, where ?a power set? is a
generic reference, whereas ?A?B? is a specific ref-
erence to a subset of a specific instance of a power
set introduced earlier.
Co-reference6
Da, wenn Ai?K(Bj) sein soll, Ai Element von K(Bj) sein
muss. Und wenn Bk?K(Al) sein soll, muss esk auch
Element von K(Al) sein.
[Because if it should be that Ai?K(Bj), Ai must be an
element of K(Bj). And if it should be that Bk?K(Al), it
must be an element of K(Al) as well.]
DeMorgan-Regel-2 besagt: K(Ai ? Bj) = K(Ai) ? K(Bj)
In diesem Fall: z.B. K(Ai) = dem Begriff
K(Ak ? Bl) K(Bj) = dem Begriff K(C ? D)[DeMorgan-Regel-2 means:
K(Ai ? Bj) = K(Ai) ? K(Bj) In this case: e.g. K(Ai) =
the term K(Ak ? Bl) K(Bj) = the term K(C ?D)]
Co-reference phenomena specific to informal
mathematical discourse involve (parts of) mathe-
matical expressions within text. In particular, enti-
ties denoted with the same literals may not co-refer,
as in the second utterance.
In the next section, we present the input interpre-
tation procedure up to the level of lexical-semantic
interpretation. We concentrate on the interface be-
tween the linguistic meaning representation (ob-
tained from the parser) and the representation of
domain-knowledge (encoded in a domain ontol-
ogy), which we realize through a domain-motivated
semantic lexicon.
4 Interpretation strategy
The task of the input interpretation component is
two-fold. Firstly, it is to construct a representation
of the utterance?s linguistic meaning. Secondly, it is
to identify within the utterance, separate, and con-
struct interpretations of:
(i) parts which constitute meta-communication
with the tutor (e.g., ?Ich habe die Aufgaben-
stellung nicht verstanden.? [I don?t understand
what the task is.] that are not to be processed
by the domain reasoner; and
(ii) parts which convey domain knowledge that
should be verified by a domain reasoner; for
example, the entire utterance ?K((A ? B)) ist
laut deMorgan-1 K(A) ? K(B)? [... is, ac-
cording to deMorgan-1,...] can be evaluated
in the context of the proof being constructed;
on the other hand, the reasoner?s knowledge
base does not contain appropriate representa-
tions to evaluate the appropriateness of the fo-
cusing particle ?also? in ?Wenn A = B, dann ist
A auch ? K(B) und B ? K(A).? [If A = B,
then A is also ? K(B) and B ? K(A).].
Domain-specific interpretation(s) of the proof-
relevant parts of the input are further processed by
6To indicate co-referential entities, we inserted the indices
which are not present in the dialog logfiles.
Proof Manager, a component that directly commu-
nicates with a domain-reasoner7 . The task of the
Proof Manager is to: (i) build and maintain a repre-
sentation of the proof constructed by the student;8
(ii) check appropriateness of the interpretation(s)
found by the input understanding module with the
state of the proof constructed so far; (iii) given the
current proof state, evaluate the utterance with re-
spect to soundness, relevance, and completeness.
The semantic analysis proceeds in 2 stages:
(i) After standard pre-processing9 , mathematical
expressions are identified, analyzed, catego-
rized, and substituted with default lexicon en-
tries encoded in the grammar. The input is then
syntactically parsed, and an formal abstract
representation of its meaning is constructed
compositionally along with the parse;
(ii) The obtained meaning representation is subse-
quently merged with discourse context and in-
terpreted by consulting a semantic lexicon of
the domain and a domain-specific ontology.
In the next sections, we first briefly summa-
rize the syntactic and semantic parsing part of the
input understanding process10 and show the for-
mat of meaning encoding constructed at this stage
(Sect. 4.1). Then, we show the lexical-semantic in-
terface to the domain ontology (Sect. 4.2).
4.1 Linguistic Meaning
By linguistic meaning (LM), we understand the
dependency-based deep semantics in the sense of
the Prague School sentence meaning as employed in
the Functional Generative Description (FGD) (Sgall
et al, 1986; Kruijff, 2001). It represents the lit-
eral meaning of the utterance rather than a domain-
specific interpretation.11 In FGD, the central frame
unit of a sentence/clause is the head verb which
specifies the tectogrammatical relations (TRs) of
7We are using a version of ?MEGA adapted for assertion-
level proving (Vo et al, 2003)
8The discourse content representation is separated from the
proof representation, however, the corresponding entities must
be co-indexed in both.
9Standard pre-processing includes sentence and word to-
kenization, (spelling correction and) morphological analysis,
part-of-speech tagging.
10We are concentrating on syntactically well-formed utter-
ances. In this paper, we are not discussing ways of combin-
ing deep and shallow processing techniques for handling mal-
formed input.
11LM is conceptually related to logical form, however, dif-
fers in coverage: while it does operate on the level of deep
semantic roles, such aspects of meaning as the scope of quan-
tifiers or interpretation of plurals, synonymy, or ambiguity are
not resolved.
its dependents (participants). Further distinction is
drawn into inner participants, such as Actor, Pa-
tient, Addressee, and free modifications, such as Lo-
cation, Means, Direction. Using TRs rather than
surface grammatical roles provides a generalized
view of the correlations between the conceptual
content of an utterance and its linguistic realization.
At the pre-processing stage, mathematical ex-
pressions embedded within input are identified, ver-
ified as to syntactic validity, categorized, and sub-
stituted with default lexical entries encoded in the
parser grammar for mathematical expression cate-
gories. For example, the expression K((A ? B) ?
(C ?D)) = (K(A?B)?K(C ?D)) given its top
node operator, =, is of type formula, its ?left side?
is the expression K((A ? B) ? (C ? D)), the list
of bracketed sub-expressions includes: A?B, C?D,
(A ? B) ? (C ? D), etc.
Next, the pre-processed input is parsed with
a lexically-based syntactic/semantic parser built
on Multi-Modal Combinatory Categorial Gram-
mar (Baldridge, 2002; Baldridge and Kruijff, 2003).
The task of the deep parser is to produce an FGD-
based linguistic meaning representation of syntac-
tically well-formed sentences and fragments. The
linguistic meaning is represented in the formalism
of Hybrid Logic Dependency Semantics. Details on
the semantic construction in this formalism can be
found in (Baldridge and Kruijff, 2002).
To derive our set of TRs we generalize and sim-
plify the collection of Praguian tectogrammatical
relations from (Hajic?ova? et al, 2000). One rea-
son for simplification is to distinguish which re-
lations are to be understood metaphorically given
the domain-specific sub-language. The most com-
monly occurring relations in our context (aside from
the roles of Actor and Patient) are Cause, Condi-
tion, and Result-Conclusion (which coincide with
the rhetorical relations in the argumentative struc-
ture of the proof):
Da [A ? K(B) gilt]<CAUSE>, alle x, die in A sind sind nicht in B
[As A?K(B) applies, all x that are in A are not in B]
Wenn [A ? K(B)]<COND>, dann A ? B=?
[If A?K(B), then A?B=?]
For example, in one of the readings of ?B en-
thaelt x ? A?, the verb ?enthaelten? represents
Figure 2: TRs in ?B contains x ? A?.
contain
FORMULA:B
<ACT>
FORMULA:x ? A
<PAT>
the meaning contain and in this frame takes de-
pendents in the relations Actor and Patient, shown
schematically in Fig. 2 (FORMULA represents the
default lexical entry for the identified mathematical
expressions categorized as formulas). The linguis-
tic meaning of this utterance returned by the parser
obtains the following representation:
@h1(contain ? <ACT>(f1 ? FORMULA:B) ? <PAT>(f2 ?
FORMULA: x ? A)
where h1 is the state where the proposition contain
is true, and the nominals f1 and f2 represent depen-
dents of the head contain, in the relations Actor and
Patient, respectively.
More details on our approach to parsing inter-
leaved natural and symbolic expressions can be
found in (Wolska and Kruijff-Korbayova?, 2004a)
and more information on investigation into tec-
togrammatical relations that build up linguistic
meaning of informal mathematical text can be found
in (Wolska and Kruijff-Korbayova?, 2004b).
4.2 Conceptual Semantics
At the final stage of input understanding, the lin-
guistic meaning representations obtained from the
parser are interpreted with respect to the given
domain. We encode information on the domain-
specific concepts and relations in a domain ontol-
ogy that reflects the knowledge base of the domain-
reasoner, and which is augmented to allow res-
olution of ambiguities introduced by natural lan-
guage (Horacek and Wolska, 2004). We interface
to the domain ontology through an upper-level on-
tology of concepts at the lexical-semantics level.
Domain specializations of conceptual relations
are encoded in the domain ontology, while a seman-
tic lexicon assigns conceptually-oriented semantics
in terms of linguistic meaning frames and provides a
link to the domain interpretation(s) through the do-
main ontology. Lexical semantics in combination
with the knowledge encoded in the ontology allows
us to identify those parts of utterances that have an
interpretation in the given domain. Moreover, pro-
ductive rules for treatment of metonymic expres-
sions are encoded through instantiation of type com-
patible concepts. If more than one lexical-semantic
interpretation is plausible, no disambiguation is per-
formed. Alternative conceptual representations are
further interpreted using the domain ontology, and
passed on to the Proof Manager for evaluation. Be-
low we explain some of the entries the semantic lex-
icon encodes:
Containment The Containment relation special-
izes into the domain relations of (strict) SUB-
SET and ELEMENT. Linguistically, it can be re-
alized, among others, with the verb ?enthalten?
(?contain?). The tectogrammatical frame of
?enthalten? involves the roles of Actor (ACT)
and Patient (PAT):
contain(ACTtype:F ORMULA, PATtype:F ORMULA) ?
(SUBFORMULAP AT , embeddingACT )
contain(ACTtype:OBJECT , PATtype:OBJECT ) ?
CONTAINMENT(containerACT , containeeP AT )
Location The Location relation, realized linguisti-
cally by the prepositional phrase introduced by
?in?, involves the tectogrammatical relations
HasProperty-Location (LOC) and the Actor of
the predicate ?sein?. We consider Location
in our domain as synonymous with Contain-
ment. Another realization of this relation, dual
to the above, occurs with the adverbial phrase
?au?erhalb von ...(liegen)? (?lie outside of?)
and is defined as negation of Containment:
in(ACTtype:OBJECT ,LOCtype:OBJECT )
? CONTAINMENT(containerLOC , containeeACT )
outside(ACTtype:OBJECT ,LOCtype:OBJECT )
? not(in(ACTtype:OBJECT ,LOCtype:OBJECT ))
Common property A general notion of ?common
property? we define as follows:
common(Property, ACTplural(A:SET,B:SET))
? Property(p1, A) ? Property(p1, B)
Property is a meta-object that can be instanti-
ated with any relational predicate, for example
as in ?(A und B)<ACT> haben (gemeinsame
Elemente)<PAT>? (?A and B have common
elements?):
common(ELEMENT, ACTplural(A:SET,B:SET))
? ELEMENT(p1 ,A) ? ELEMENT(p1 , B)
Difference The Difference relation, realized
linguistically by the predicates ?verschieden
(sein)? (?be different?; for COLLECTION or
STRUCTURED OBJECTS) and ?disjunkt (sein)?
(?be disjoint?; for objects of type COLLEC-
TION) involves a plural Actor (e.g. coordinated
noun phrases) and a HasProperty TRs. De-
pending on the type of the entity in the Actor
relation, the interpretations are:
different(ACTplural(A:SET,B:SET)) ? A 6= B
different(ACTplural(A:SET,B:SET))
? (e1 ELEMENT A ? e2 ELEMENT B ? e1 6= e2)
different(ACTplural(A:ST RUCTUREDOBJECT
,B:STRUCT UREDOBJECT ))
? (Property1(p1, A) ? Property2(p2, B) ?
Property1 = Property2 ? p1 6= p2)
Mereological relations Here, we encode part-
of relations between domain objects. These
concern both physical surface and ontologi-
cal properties of objects. Commonly occurring
part-of relations in our domain are:
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED OBJECTSUBT ERM,SUBF ORMULA)
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED
OBJECTENCLOSEDT ERM,ENCLOSEDF ORMULA)
hasComponent(STRUCTURED OBJECTterm,formula ,
STRUCTURED
OBJECTT ERMCOMP ONENT,FORMULACOMP ONENT )
Moreover, from the ontology we have:
Property(STRUCTURED OBJECTterm,formula ,
componentterm?side,formula?side)
Using these definitions and polysemy rules
such as polysemous(Object, Property), we can
obtain interpretation of utterances such as
?Dann gilt f u?r die linke Seite, . . . ? (?Then
for the left side it holds that . . . ?) where the
predicate ?gilt? normally takes two arguments
of types STRUCTURED OBJECTterm,formula ,
rather than an argument of type Property.
For example, the previously mentioned predicate
contain (Fig. 2) represents the semantic relation of
Containment which, in the domain of naive set the-
ory, is ambiguous between the domain relations EL-
EMENT, SUBSET, and PROPER SUBSET. The al-
ternative specializations are encoded in the domain
ontology, while the semantic lexicon provides the
conceptual structure of the head predicate. At the
domain interpretation stage, the semantic lexicon is
consulted to translate the tectogrammatical frame of
the predicate into a semantic relation represented
in the domain ontology. For the predicate contain,
from the semantic lexicon, we obtain:
contain(ACTtype:F ORMULA, PATtype:F ORMULA)
? (SUBFORMULAP AT , embeddingACT )
[?a Patient of type FORMULA is a subformula embedded within a
FORMULA in the Actor relation with respect to the head contain?]
contain(ACTtype:OBJECT , PATtype:OBJECT )
? CONTAINMENT(containerACT , containeeP AT )
[?the Containment relation involves a predicate contain and its Actor
and Patient dependents, where the Actor and Patient are the container
and containee parameters respectively?]
Translation rules that consult the domain ontology
expand the conceptual structure representation into
alternative domain-specific interpretations preserv-
ing argument structure. As it is in the capacity of
neither sentence-level nor discourse-level analysis
to evaluate the appropriateness of the alternative in-
terpretations in the proof context, this task is dele-
gated to the Proof Manager.
5 Example analysis
In this section, we illustrate the mechanics of the
approach on the following example:
A enthaelt keinesfalls Elemente, die auch in B sind.
[A contains no elements that are also in B]
The analysis proceeds as follows.
The mathematical expression tagger first iden-
tifies the expressions A and B. If there was no
prior discourse entity for ?A? and ?B? to verify
their types, they are ambiguous between constant,
term, and formula12 . The expressions are substi-
tuted with generic entries FORMULA, TERM, CONST
represented in the parser grammar. The sentence is
assigned alternative readings: ?CONST contains no
elements that are also in CONST?, ?CONST contains
no elements that are also in TERM?, ?CONST con-
tains no elements that are also in FORMULA?, etc.
Here, we continue only with ?CONST contains no
elements that are also in CONST?; the other readings
would be discarded at later stages of processing be-
cause of sortal incompatibilities.
The linguistic meaning of the utterance obtained
from the parser is represented by the following for-
mula13:
@n1(no ? <Restr>e1 ?
<Body>(p1 ? contain ? <ACT>(a1 ? A) ? <PAT> e1)) ?
@e1(element ?
<GenRel>(b1 ? be ? <ACT>e1 ? <HasProp-Loc>(b2 ? B)))
[?(set) A contains no elements that are in (set) B?]
Next, the semantic lexicon is consulted to trans-
late the linguistic meaning representation into a con-
ceptual structure. The relevant lexical semantic en-
tries are Containment and Location (see Sect. 4.2).
The transformation is presented schematically be-
low:
contain(ACTOBJECT :A, PATOBJECT :element) ?
CONTAINMENT(containerA , containeeelement)
(ACTOBJECT :element, HasProp-LocOBJECT :B )
? CONTAINMENT(containerB , containeeelement)
Finally, in the domain ontology, we find that the
conceptual relation of Containment, in naive set the-
ory, specializes into the domain relations of ELE-
MENT, SUBSET, STRICT SUBSET. Using the lin-
guistic meaning, the semantic lexicon, and the do-
main ontology, we obtain all the combinations of
interpretations, including the target one paraphrased
below:
?it is not the case that there exist elements e, such that e ? A and e ? B?,
Using translation rules the final interpretations
are translated into first-order logic formulas and
passed on for evaluation to the Proof Manager.
6 Related work
Language understanding in dialog systems, be it
with speech or text interface, is commonly per-
formed using shallow syntactic analysis combined
12In prior discourse, there may have been an assignment
A := ?, where ? is a formula, in which case, A would be known
from discourse context to be of type FORMULA (similarly for
term assignment); by CONST we mean a set or element variable
such as A, x denoting a set A or an element x respectively.
13Irrelevant parts of the meaning representation are omitted;
glosses of the formula are provided.
with keyword spotting. Tutorial systems also suc-
cessfully employ statistical methods which com-
pare student responses to a model built from pre-
constructed gold-standard answers (Graesser et al,
2000). This is impossible for our dialogs, due to
the presence of symbolic mathematical expressions
and because of such aspects of discourse meaning
as causal relations, modality, negation, or scope
of quantifiers which are of crucial importance in
our setting, but of which shallow techniques remain
oblivious (or handle them in a rudimentary way).
When precise understanding is needed, tutorial sys-
tems use closed-questions to elicit short answers of
little syntactic variation (Glass, 2001) or restricted
format of input is allowed. However, this conflicts
with the preference for flexible dialog do achieve
active learning (Moore, 1993).
With regard to interpreting mathematical
texts, (Zinn, 1999) and (Baur, 1999) present DRT
analyzes of course-book proofs. The language in
our dialogs is more informal: natural language and
symbolic mathematical expressions are mixed more
freely, there is a higher degree and more variety
of verbalization, and mathematical objects are not
properly introduced. Both above approaches rely on
typesetting information that identifies mathematical
symbols, formulas, and proof steps, whereas our
input does not contain any such information.
Forcing the user to delimit formulas would not
guarantee a clean separation of the natural language
and the non-linguistic content, while might reduce
the flexibility of the system by making the interface
harder to use.
7 Conclusion and Further Work
In this paper, we reported on the use of deep syn-
tactic and semantic analysis in the interpretation
of mathematical discourse in a dialog setting. We
presented an approach that uses domain-motivated
semantic lexicon to mediate between a domain-
independent representation of linguistic meaning of
utterances and their domain-specific interpretation.
We are incrementally extending the coverage of
the deep analysis components. Our current parser
grammar and upper-level ontology cover most of
the constructions and concepts that occur most fre-
quently in our corpus. The module will be evaluated
as part of the next Wizard-of-Oz experiment.
We are planning to investigate the possibility
of using FrameNet resources developed within the
SALSA project (Erk et al, 2003) at the intermedi-
ate interpretation stage between the linguistic mean-
ing and domain-specific interpretation. Presently,
the semantic lexicon we have constructed encodes,
for instance, a general conceptual relation of CON-
TAINMENT evoked by the verb ?enthalten? (?con-
tain?), with dependents in relations Actor and Pa-
tient, which corresponds to the FrameNet CON-
TAINING domain with frame elements CONTAINER
and CONTENTS. In the course of further work, we
would like to investigate ways of establishing inter-
face between the linguistic meaning TRs and frame
elements, and attempt to use FrameNet to interpret
predicates unknown to our semantic lexicon. Tak-
ing a hypothetical example, if our parser grammar
encoded the meaning of the verb ?beinhalten? (with
the intended meaning contain) in the same linguis-
tic meaning frame as ?enthalten? (contain), while
the sense of ?beinhalten? were not explicitly defined
in the semantic lexicon, we could attempt to inter-
pret it using the FrameNet CONTAINING domain
and the existing lexical semantic entry for ?enthal-
ten?.
References
J. Baldridge. 2002. Lexically Specified Derivational Control
in Combinatory Categorial Grammar. Ph.D. Thesis, Uni-
versity of Edinburgh, Edinburgh.
J. M. Baldridge and G.J. M. Kruijff. 2002. Coupling CCG with
hybrid logic dependency semantics. In Proc. of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia PA.
J. M. Baldridge and G.J. M. Kruijff. 2003. Multi-modal com-
binatory categorial grammar. In Proc. of the 10th Annual
Meeting of the European Chapter of the Association for
Computational Linguistics (EACL?03), Budapest.
J. Baur. 1999. Syntax und Semantik mathematischer Texte.
Diplomarbeit, Fachrichtung Computerlinguistik, Universit a?t
des Saarlandes, Saarbr u?cken, Germany.
C. Benzm u?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayov a?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B. Q. Vo,
and M. Wolska. 2003a. Tutorial dialogs on mathematical
proofs. In Proc. of IJCAI?03 Workshop on Knowledge Rep-
resentation and Automated Reasoning for E-Learning Sys-
tems, Acapulco, Mexico.
C. Benzm u?ller, A. Fiedler, M. Gabsdil, H. Horacek, I. Kruijff-
Korbayov a?, M. Pinkal, J. Siekmann, D. Tsovaltzi, B. Q. Vo,
and M. Wolska. 2003b. A Wizard-of-Oz experiment for tu-
torial dialogues in mathematics. In Proc. of the AIED?03
Workshop on Advanced Technologies for Mathematics Edu-
cation, Sydney, Australia.
K. Erk, A. Kowalski, and M. Pinkal. 2003. A corpus re-
source for lexical semantics. In Proc. of the 5th Interna-
tional Workshop on Computational Semantics, Tilburg, The
Netherlands.
A. Fiedler and M. Gabsdil. 2002. Supporting Progressive Re-
finement of Wizard-of-Oz Experiments. In Proc. of the
ITS?02 Workshop on Empirical Methods for Tutorial Dia-
logue, San Sebastian, Spain.
M. Glass. 2001. Processing language input in the CIRCSIM-
Tutor intelligent tutoring system. In Proc. of the 10th Con-
ference on Artificial Intelligence in Education (AIED?01),
San Antonio.
A. Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings, D. Har-
ter, and N. Person. 2000. Using latent semantic analysis to
evaluate the contributions of students in autotutor. Interac-
tive Learning Environments, 8.
E. Hajic?ov a?, J. Panevov a?, and P. Sgall. 2000. A manual for tec-
togrammatical tagging of the Prague Dependency Treebank.
TR-2000-09, Charles University, Prague, Czech Republic.
H. Horacek and M. Wolska. 2004. Interpreting Semi-Formal
Utterances in Dialogs about Mathematical Proofs. In Proc.
of the 9th International Conference on Application of Nat-
ural Language to Information Systems (NLDB?04), Salford,
Manchester, Springer. To appear.
G.J.M. Kruijff. 2001. A Categorial-Modal Logical Architec-
ture of Informativity: Dependency Grammar Logic & In-
formation Structure. Ph.D. Thesis, Institute of Formal and
Applied Linguistics ( ?UFAL), Faculty of Mathematics and
Physics, Charles University, Prague, Czech Republic.
J. Moore. 1993. What makes human explanations effective?
In Proc. of the 15th Annual Conference of the Cognitive Sci-
ence Society, Hillsdale, NJ.
P. Sgall, E. Hajic?ov a?, and J. Panevov a?. 1986. The meaning of
the sentence in its semantic and pragmatic aspects. Reidel
Publishing Company, Dordrecht, The Netherlands.
Q.B. Vo, C. Benzm u?ller, and S. Autexier. 2003. An approach
to assertion application via generalized resolution. SEKI
Report SR-03-01, Fachrichtung Informatik, Universit a?t des
Saarlandes, Saarbr u?cken, Germany.
M. Wolska and I. Kruijff-Korbayov a?. 2004. Analysis of mixed
natural and symbolic language input in mathematical di-
alogs. In Proc.of the 42nd Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain. To ap-
pear.
M. Wolska and I. Kruijff-Korbayov a?. 2004. Building a
dependency-based grammar for parsing informal mathemat-
ical discourse. In Proc. of the 7th International Conference
on Text, Speech and Dialogue (TSD?04), Brno, Czech Re-
public, Springer. To appear.
M. Wolska, B. Q. Vo, D. Tsovaltzi, I. Kruijff-Korbayov a?,
E. Karagjosova, H. Horacek, M. Gabsdil, A. Fiedler,
C. Benzm u?ller, 2004. An annotated corpus of tutorial di-
alogs on mathematical theorem proving. In Proc. of 4th In-
ternational Conference On Language Resources and Evalu-
ation (LREC?04), Lisbon, Portugal. To appear.
C. Zinn. 1999. Understanding mathematical discourse. In
Proc. of the 3rd Workshop on the Semantics and Pragmat-
ics of Dialogue (Amstelogue?99), Amsterdam, The Nether-
lands.
Generating Referential Descriptions Under Conditions of Uncertainty
Helmut Horacek
Universit?t des Saarlandes
F.R. 6.2 Informatik
Postfach 151150,  D-66041 Saarbr?cken, Germany
 email: horacek@cs.uni-sb.de
Abstract
Algorithms for generating referring expressions 
typically assume that an object in a scenary can be 
identified through a set of commonly agreed 
properties. This is a strong assumption, since in 
reality properties of objects may be perceived differ-
ently among people, due to a number of factors 
including vagueness, knowledge discrepancies, and 
limited perception capabilities. Taking these discre-
pancies into account, we reinterpret concepts of 
algorithms generating referring expressions in view 
of uncertainties about the appearance of objects. Our 
model includes two complementary measures of 
likelihood in object identification, and adapted 
property selection and termination criteria. The 
approach is relevant for situations with potential 
perception problems and for scenarios with knowl-
edge discrepancies between conversants.
1 Introduction
Generating referring expressions is a traditional, standard 
task in natural language generation. Over the past two 
decades, a number of algorithms have been proposed which 
differ among each other in terms of efficiency and coverage. 
To the best of our knowledge, all algorithms share the 
assumption that objects can be identified by a description 
consisting of attribute values ascribed to these objects. 
Moreover, the results are specified in a way that implicitly 
assumes complete agreement about these properties, 
provided they are known to the audience. We feel that this 
assumption may be too strong in reality so that, for 
instance, a dialog system in which the reference generation 
algorithm is embedded is unlikely to behave adequately when 
a misunderstanding occurs due to a perception mismatch.
In this paper, we address this problem by incorporating 
measures to deal with uncertainties into a standard algorithm 
that generates referring expressions. In order to represent 
uncertainties, we propose two complementary measures 
expressing the likelihood of object identification. We define 
computation schemes for combining descriptions with 
boolean combinations of  attribute values, and we extend the 
incremental standard reference generation algorithm by 
adapting property selection and termination criteria.
This paper is organized as follows. First, we motivate our 
approach in more detail. Then we introduce our method for 
representing aspects of uncertainty. We follow by illustrating 
the propagation of uncertainty assessments for several attri-
bute values, including boolean combinations, and we give 
examples of the effects. Then we describe extensions to the 
incremental algorithm, and we discuss their impact.
2 Motivation
In the scope of this paper, we adopt the terminology origin-
ally formulated in [Dale 1988] and later used by several 
others. A referential description [Donellan 1966] serves the 
purpose of letting the hearer or reader identify a particular 
object or set of objects in a given situation. The referring 
expression to be generated is required to be a distinguishing 
description, that is a description of the enitties being referred 
to, but not to any other object in the context set. A context 
set is defined as the set of the entities the addressee is 
currently assumed to be attending to ? this is similar to the 
set of entities in the focus spaces of the discourse focus stack 
in Grosz' and Sidner's [1986] theory of discourse structure. 
Moreover, the contrast set  (or the set of potential 
distractors [McDonald 1981]) is defined to entail all 
elements of the context set except the intended referents.
Generating referring expressions is pursued since the 
eighties [Appelt 1985, Kronfeld 1986, Appelt and Kronfeld 
1987]. Subsequent years were characterized by a debate about 
computational efficiency versus minimality of the elements 
appearing in the resulting referring expression [Dale 1988, 
Reiter 1990, Reiter and Dale 1992]. In the mid-nineties, this 
debate seemed to be settled in favor of the incremental 
approach [Dale and Reiter 1995] ? motivated by results of 
psychological experiments [Levelt 1989, Pechmann 1989], 
certain non-minimal expressions are tolerated in favor of 
adopting the fast strategy of incrementally selecting ambi-
guity-reducing attributes from a domain-dependent preference 
list. Recently, algorithms have been applied to the identifi-
cation of sets of objects rather than individuals [Bateman 
1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001], and 
the repertoire of descriptions has been extended to boolean 
combinations of attributes, including negations [van Deemter 
2002]. To avoid the generation of redundant descriptions that 
is typical for incremental approaches, Gardent [2002] and 
Horacek [2003] proposed exhaustive resp. best-first searches.
All these procedures more or less share the design of the 
underlying knowledge base. Objects are conceived in terms 
of sets of attributes, each with an atomic value as its filler. 
Some models distinguish specializations of these values 
according to a taxonomic hierarchy, so that the most accu-
rate value can be replaced by one of its generalizations if 
there are reasons to assume this alternative is preferable ? 
due to insufficient knowledge attributed to the audience, or 
to prevent unintended implications. A few approaches also 
deal with relations to other objects, whose representation 
differs from that of attributes only by the reference to the 
related object. Typically, a user model is assumed to guide 
the choice among available descriptors; the user model 
expresses taxonomic knowledge attributed to the user,  indic-
ating for a descriptor whether it is known to the user or not.
While a knowledge base developed and interpreted in this 
manner is adequate for generating referring expressions in 
most application-relevant settings, there may be circum-
stances in which uncertainties are prominent, so that the 
simple boolean attribution of properties to objects becomes 
problematic and may prove insufficient. Uncertainties may 
manifest themselves in at least the following three factors:
? Uncertainty about knowledge
There may not be sufficient evidence to assume that 
the user is or is not acquainted with a specific term. In 
fact, most of today's user model components assign 
some probability to statements about a user's knowl-
edge or capabilities, for example on the basis of infer-
ences obtained through a belief network [Pearl 1988].
? Uncertainty about perception capabilities
There is an increasing number of applications with 
natural language interaction where the objects of the 
discourse do not appear on the computer screen (e.g., 
ubiquitous tools guiding a user in environments such 
as airports and tourist attraction areas, e.g., [Wahlster 
2004]). In such situations, perception and recognition 
of object properties is much harder to assess; for 
example, the visibility of some object or of one of its 
parts may not be derivable with complete certainty.
? Uncertainty about conceptual agreement
While ascribing a value to an attribute is straightfor-
ward for certain categories of attributes, problems may 
occur, e.g., in connection with vagueness. This 
concept may be relevant for a number of commonly 
used properties such as size and shape, and even with 
colors, transitions between adjacent color tones may 
not be firmly categorized as one of the two candidates.
To illustrate these manifestations of uncertainty, let us 
consider a scenario with three similar dogs, one of which is 
a bassett, which is also the intended referent. In addition, 
the bassett is brownish and has a long tail. The other two 
dogs have shorter tails and their skin is also brown, but 
with some white resp. black portions. Furthermore, we 
assume that the audience has little knowledge about dog 
specifics, that is, it is not very likely that they may recog-
nize the intended referent as a bassett. We also assume that 
the tails of the dogs cannot be observed easily by the 
audience under the given local circumstances.
Hence, the three attributes ?category?, ?color?, and ?tail 
length? each fall into one of the categories of uncertainty 
introduced above: the categorization of the intended referent 
as a bassett is associated with uncertainty about knowledge, 
the limited visibility which may not enable the spectators 
to see the tails of the dogs in each moment constitutes an 
uncertainty about perception capabilities, and the similarity 
of the dogs' colors may yield uncertainty about conceptual 
agreement, that is, it is doubtfull whether the descriptor   
?brownish? is attributed only to the intended referent or also 
to some of the other dogs in the given situation.
Apparently, these uncertainties have consequences on 
building human-adequate referring expressions, especially in 
contexts where most of the descriptors available are asso-
ciated with some kind of uncertainty. Intuitively, we would 
expect people to produce referring expressions with several 
of these descriptors, being redundant in case they are all 
recognized, but also hoping that the identification will 
succeed if the audience can identify only some part of the 
overall description in the given situation. Moreover, we 
would expect people only to use descriptors that have some 
reasonable chance of being understood.
Unfortunately, traditional generation algorithms do not 
enable us to model such a behavior, since none of the 
options available does justice to the uncertainty involved. If 
a descriptor is modeled as applying to all entities (e.g., for 
?brownish?), it will never be chosen since it yields no 
discrimination. A similar consequence is obtained when the 
capabilities of the audience are interpreted pessimistically. 
Finally, if a descriptor is assumed to be understood, it 
might be chosen without considering any of the other candi-
dates associated with uncertainty. Thus, modeling in the 
existing algorithms forces us to make crisp decisions, with 
strong impacts on the result of the algorithm. Redundant 
expressions motivated by uncertainties about recognition 
cannot be generated under any modeling alternative.
There are only a few computational approaches which 
address the problem of uncertainty about the recognition of 
referring expressions. For example, [Edmonds 1994] and 
[Heeman and Hirst 1995] describe both plan-based methods, 
where a vague and partial description is produced initially, 
which is narrowed and ultimately confirmed in the subse-
quent discourse. However, the documented examples do only 
emphasize incomplete, but never incorrect interpretations. 
An approach that fits better to our intentions is the work  
by Goodman [1987], which emphasizes reference identi-
fication and associated failures in task-oriented dialogs 
[Goodman 1986]. This case study demonstrates various 
impacts of limitations and discrepancies of expertise on 
referential identification: subjects exhibit uncertainty in 
identification, which manifests itself in tentative actions and 
changes of mind, they misinterpret descriptions (e.g., 
'outlet' interpreted as 'hole'), and they may find no 
appropriate referent at all. In the latter case, subject even 
undertake attempts to repair an otherwise uninterpretable 
description by relaxing descriptors. In the following, we 
interpret some of these findings for our model of uncer-
tainty, including a model of a repair mechanism.
3 Representing Uncertainties
Basically, our model of uncertainty combines the three kinds 
of uncertainty described in the previous section. Each of 
them is expressed in terms of a probability, associated with 
a triple consisting of an object, an attribute applicable to 
that object, and the value ascribed to this pair. The follow-
ing probabilities each express the likelihood that the user 
recognizes a description correctly from the perspectives of: 
pK The user is acquainted with the terms mentioned
pP The user can perceive the properties uttered
pA The user agrees to the applicability of the terms used 
In order to identify an intended referent successfully, all 
three factors must be assessed positively, so that the 
probability of recognition p becomes the product of these 
three probabilities. Since the individual properties refer to 
factors outside the scope of proper generation, we only deal 
with p in the scope of this paper, although it is clear that 
this assessment requires contributions from several sources.
The concept of using individual probabilities to represent 
manifestations of uncertainty is not only simple, it also fits 
to knowledge sources where data about these probabilities 
could be found. For example, user models, the potential 
sources for assessing pK, typically assign assessments to 
user capabilities on the basis of belief networks. Similar 
considerations hold for representations of vague properties, 
which fall under the concept of term agreement. These 
properties can be modeled by fuzzy logic systems [Zadeh 
1984, 1996], which allow for an interpretation in terms of a 
single probability value, representing the likelihood that a 
precise value is perceived as a given vague term. 
The association of a probability with the applicability of  
a descriptor to an object not only expresses the somehow 
direct likelihood of success of this task, but the application 
of this likelihood to several candidate objects also gives an 
indication of the likelihood of success of the overall identi-
fication goal. If a descriptor is assumed to be associated with 
several candidate objects by the audience, with certain 
degrees typically different among these objects, several cases 
can be distinguished: (1) correct identification, where the 
audience relates the description only to those objects to 
which this descriptor indeed applies, (2) misinterpretation, 
where none of these objects, but others are associated with 
the descriptor by the audience, (3) ambiguity, which is a 
combination of (1) and (2), and, finally, (4) the case of an 
uninterpretable description, where the audience does not 
relate the descriptor to any of the candidate objects. In the 
last case, people are known to make an attempt to repair 
their unsuccessful interpretation, since they assume that the 
expression communicated is indeed intended to refer to some 
object or objects in the domain of discourse, according to the 
work by Goodman. In order to simulate the effect of this 
behavior, we compute the probability of the occurrence of an 
uninterpretable description, which we call the repair 
factor, and we increase the probability of identification of 
the candidate objects (which we call our repair mechanism), 
based on the amount of the repair factor and the context of 
these objects in the overall identification task. 
                                                                                 
R F f f( , , , ) ( ( ( , , ), )| ( , , )|)k p p j m n p j m nn i
i
n
m j
n
m
j
k
1
10
1
? =
==
?
??
?
??
=
? ???
f(j,m,n) recursively enumerates all combinations of m out of 
n elements (here: natural numbers 1 ? n) and returns the j-th 
combination as a set of numbers M={i1,?,im} with 1?ik?n 
(k= 1,?,m)
F(M,p ) p(1 p )
i M
i M
i
i
i
=
?
?
?
???
???
if
if
                                                                                                                          
Figure 1. Repair factor for insufficient recognition of k objects
In concrete terms, if we have n objects for which a descriptor 
is recognized with probability pi for object i, the probability 
that none of the objects is recognized by a referring 
expression built from that descriptor is ?(1-pi), (1 ? i ? n). 
Although this number tends to be small if there are several 
objects to which the description matches with some reason-
able degree of confidence, the associated need for invoking a 
repair mechanism becomes increasingly urgent when further 
descriptors are added to the description built so far, as well as 
when the task is to identify multiple referents rather than a 
single one. For the case of 2 objects, the need for invoking a 
repair mechanism can be quantified by the repair factor  
2?i=1,n(1-pi)+?i=1,n(pi?j=1,n(j?i)(1-pj)). The general case, if 
needed, gets increasingly complex, as illustrated in Figure 1, 
for k objects to be identified, out of n candidates (k ? n). 
Thus, for the likelihood of recognition failure, a  
mechanism is required that simulates identification repair 
under these conditions. Apart from the likelihood of failure, 
repair should be guided by potential confusability of objects 
in view of some given descriptor. Hence, while we think it 
is virtually impossible to confuse an animal and a piece of 
equipment, at least under any reasonable conditions of visibi-
lity, we assume that objects of some degree of appearance 
similarity (size and shape) may potentially be confused with 
each other. Hence, we consider a potentially confusable 
object a candidate for being interpreted as an intended referent 
in case a repair of a reference failure is required. Confusion in 
this sense may be interpreted in two ways: from the 
perspective of the speaker, those objects are candidates which 
the speaker thinks the hearer could confuse. From the hearers 
perspective, those objects are candidates which the hearer 
thinks the speaker might have confused in producing a badly 
interpretable description. Since the latter constellation 
corresponds to the situation present for repair attempts, we 
model potential candidates quasi ?objectively? by incorpor-
ating annotations in the knowledge base. The dependency of 
user capabilities as assessed by a user model influences these 
assessments indirectly through the probability of recognition 
attributed to the user for each descriptor-object pair.
                                                                                                                          
Determine probability of identification (D, k, O1, ?, On)
O1, ?, Om Objects to which descriptor D applies
Om+1, ?, On Objects to which repair with D is applicable
pi, ?, pm Probability that D is recognized for Oi
Objects ordered along degrees of recognition confidence:
 ?i,j(1 ? i, j ? m): (pi > pj) ? (i > j)
Rprop ? R(k, p1, ?, pm), i ? 1
1. if (i?m) 
then Rc ? Min(Rprop/n, 1-pi), p-idi ? pi+Rc
else Rc ? Rprop/n, p-idi ? Rc 
endif
if (i<n) 
then i ? i+1, Rprop ? Rprop - Rc, goto 1 
endif
                                                                                                                           
Figure 2. Assessing identification probabilities including repair
In order to keep the repair mechanism simple, we approxi-
mate confusability of an object by augmenting its represen-
tation with annotations of all property-value combinations 
that do not apply to it, but which could somehow be 
perceived as holding for this object. The potentially large 
amount of data created this way can be significantly reduced 
by making use of inheritance. For example, one can state 
that blue and purple (physical) objects can be confused, by 
making annotations about confusability with blue for purple 
objects, and vice-versa. This annotation is then inherited to 
all entities that are specializations of (physical) objects.
The proper repair is then simulated by collecting all 
candidates to which the descriptor in question could arguably 
apply, and by assigning these candidates a probability of 
identification through repair , according to the repair factor, 
as assessed above. There are two kinds of candidates: (1) 
those to which the descriptor is recognized with some  
probability, and (2) those to which it could apply with some 
relaxation, that is, which contains a suitable confusability 
annotation. The repair factor, which is computed according 
to the schema in Figure 1, is then evenly distributed among 
these two sets of candidates, provided the added probabilities 
of recognition and repair do not get greater than 1 for some 
object; this can only be the case if the number of  objects to 
identify is close to the number of candidates. In such a case, 
the extra amount is distributed recursively among the 
remaining candidates, always respecting the upper limit of 1. 
If the number of objects to identify even exceeds the number 
of candidates, the effect of the repair mechanism results in a 
modification of the number of objects to identify, reducing it 
to the number of available candidates. The computation of 
the probability of identification through repair is illustrated 
in Figure 2.  Three examples in Figure 3 illustrate the effect 
of the repair mechanism in quantitative terms. They empha-
size the relation between expectations about the number of 
objects to be identified and probabilities of identification. 
                                                                                                                          
For k objects to be identified out of n, judging identifi-
cation by descriptor D, which may involve repair measures
(D applies to m out of these n with probabilities pi,?,pm)
1. k=1, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 0.12
p-id1 = 0.83, p-id2 = 0.43, p-id3 = p-id4 = 0.03
2. k=2, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 0.96
p-id1 = 1, p-id2 = 0.6533, p-id3 = p-id4 = 0.2533
3. k=3, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 2.1
p-id1 = 1, p-id2 = 1, p-id3 = p-id4 = 0.65
                                                                                                                           
Figure 3. Examples of assessing identification probabilities
Specifically, the increasing contributions of the repair 
facility are shown, which will be even more pronounced with 
several attributes associated with limited recognition expect-
ations. We will see this effect in context with building 
descriptor combination in the next section, as well as in the 
detailed exposition of an example in Appendix II.
4 Identifiability of Descriptor Compositions
Since a single descriptor is rarely sufficient for identifying 
one or several objects in scenarios of interesting complexity, 
boolean compositions of descriptors are generated for this 
purpose, conjunctions being required for building identifying 
expressions for single objects. Their probability of recog-
nition is a simple extension of the case of single descriptors. 
If pi is the probability of recognition of descriptor D i for 
some object O , an expression consisting of several D i  
(i=1,pn) is identified with O through recognition if all Di  are 
attributed to O. The probability of this coincidence amounts 
to the product of all probabilities ?pi (i=1,pn).  
The probability of identification through repair is 
computed by distributing the repair factor R(k,P1,?,Pm), 
where each Pj=?pji ( j=1,m;i=1,pn), among all objects quali-
fying for the repair measure. While this distribution is an 
equal one for the case of a single descriptor, apart from using 
the upper limit of 1 for the total probability, such an even 
distribution would not do full justice here. We propose to 
distribute the likelihood proportionally to the probabilities of 
recognition for each descriptor, which makes repair more 
likely applicable to those objects which are also more likely 
to be identified anyway. In order to perform this operation 
properly, ?average? probabilities (ap) for only reparable 
descriptors must be estimated. Moreover, we want to favor 
repairs for objects which require fewer ?average? probabilities 
for this computation, by incorporating a "scale-down factor? 
(sdf) for each additional repair. The computation schema is 
given in Figure 4. For concrete computations, we choose 0.5 
for both factors ap and sdf ? see the examples in Figure 5. 
The first one demonstrates the partitioning of the repair 
factor according to the number of attributes which require 
repair. Specifically, the first three objects get the same share 
of the repair factor, while the fourth object gets only half of 
it,  since  its identification  is  the  only one  which  requires  
                                                                                                                         
Compute identification probability (D1,?,Dnp,k,O1,?,On)
O1,?,Om Objects to which all D1,?,np are applicable
Om+1,?,On Objects with repair possible for all D1,?,np
pi1,?,pinp Probability that D1,?,np is attributed to Oi
Objects ordered along degrees of identification confidence:
 ?i,j(1?i,j?m): (?l=1,nppil > ?l=1,nppjl) ? (i > j)
for i from 1 to n do 
Pi ? 1, sdfi ? 1/sdf
for j from 1 to np do
if  pij > 0 
then Pi ? Pipij
else Pi ? Piap, sdfi ? sdfisdf 
endif
endfor
endfor
Rprop ? R(k,Pi,?,Pm), i ? 1, P ? ?i=1,nPi 
1. if (i?m) 
then Rc ? Min(Rprop(Pi/P),1-Pi), p-idi ? Pi+Rc
else Rc ? Rprop(Pisdfi/P), p-idi ? Rc 
endif
if (i<n) 
then i ? i+1, Rprop ? Rprop - Rc, goto 1 
endif
                                                                                                                           
Figure 4. Identification probabilities for several descriptors
repair regarding two descriptors. The second example features 
the impact of multiple intended referents on the repair factor, 
which increases the probabilities of identification substan-
tially. The last example illustrates the compensative effect 
between comparably low probabilities of recognition and 
higher ones in connection with the requirement of using the 
repair facility. Specifically, this example demonstrates that 
the probability of identification for an object (the second 
one) that is only identifiable through the repair mechanism 
can even become higher than the probability of identification 
for an object (the second one) that does not require repair for 
being identified. However, such an effect is only possible in 
the context of descriptors applicable with some degree of 
confidence to both candidates, but strongly favoring the 
object whose identification relies on the repair mechanism 
due to mismatch with another descriptor. This is the most 
critical effect in choosing descriptors.
The incorproation of disjunctions and negations is more 
local, since this extension only generalizes the probability 
of recognition of a single property. This is because these 
operators appear only in embedded boolean combinations 
[van Deemter 2002], which are the basis for building larger 
varieties of expressions [Horacek 2004]. For disjunctions of 
two descriptors with associated probabilities p1 and p2, the 
joint probability amounts to p1+p2-p1p2, assuming indepen-
dence, which is quite normal for descriptors originating from  
                                                                                  
For k objects to be identified out of n, judging identifi-
cation by np descriptors D, at least repair possible for all
(Dj applies to object i with probability pji, ?i?m: pji > 0)
1.  k=1, n=4, m=1, np=2 (p11 =0.5, p21 =0.5, p12 =0.5,
p22 =0, p13 =0, p23 =0.5, p14 =0, p24 =0): Rprop = 0.75
p-id1 = 0.464, p-id2 = 0.214, p-id3 = 0.214, p-id4 = 0.107
2. k=2, n=3, m=1, np=2 (p11 =0.5, p21 =0.6, p12 =0.6,
p22 =0.5, p13 =0, p23 =0.55): Rprop = 1.4
p-id1 = p-id2 = 0.766, p-id3 = 0.466
3. k=1, n=2, m=1, np=3 (p11 =0.5, p21 =0.5, p31 =0.5,
p12 =0.9, p22 =0.9, p32 =0): Rprop = 0.875
p-id1 = 0.331, p-id2 = 0.668
                                                                                                                          
Figure 5. Examples of assessing identification probabilities
distinct properties. For some properties, prominently those 
associated with vagueness, building disjunctions of 
descriptors originating from the same property may be 
beneficial. For example, disjunctions of similar colors or 
shapes may reduce the uncertainty through combining the 
identifiability of both. A simple way to model this constel-
lation is by assigning probabilities to the set of applicable 
values so that their sum does not exceed 1, thereby modeling 
exclusion of the co-occurrence of more than one value. 
Consequently, the associated probabilities can simply be 
added. Propagation of the ?confusable? annotation is treated 
similarly ? if at least one of the descriptors is marked as 
?confusable?, this also holds for the disjunction. For dealing 
with negation, the probability of identification is simply 
inverted (1-p). The treatment of the ?confusable? annotation, 
however, is a bit problematic. The invertion operation needs 
modification through anticipating the amount of the repair 
factor, but this cannot be done locally. Therefore, this factor, 
rf, must be estimated in advance. For concrete computations 
we use a value of 0.1, so that ?p for a ?confusable? p 
amounts to 0.9.
5 An Algorithm Incorporating Uncertainties
In this section, we describe extensions to the algorithm by 
Dale and Reiter [1995] that take into account the measures 
addressing uncertainty introduced in previous sections. This 
reference algorithm takes an intended referent r (the generali-
zation to several referents is straightforward), the attributes P 
that describe r, and a contrast set C, and incrementally builds 
an identifying description L, if possible. The algorithm 
assumes an environment with three interface functions: 
BasicLevelValue, accessing basic level categories of objects 
[Rosch 1978], MoreSpecificValue for accessing incremen-
tally specialized values of an attribute according to a taxo-
nomic hierarchy, and UserKnows for judging whether the 
user is familiar with the attribute value of an object.
The algorithm basically iterates over the attributes P, 
according to some predetermined ordering which reflects 
preferences in the domain of application. For each attribute 
in P, a value assumed to be known to the user is determined, 
so that this value describes the intended referent and rules out 
at least one potential distractor which is still in the contrast 
set C in the iteration step considered. If such a value can be 
found, a pair consisting of the attribute and this value is 
included in the identifying description L . This step is 
repeated until the list P is exhausted or a distinguishing 
description is found, that is, the contrast set C  is empty. 
Unless the distinguishing description L does not contain a 
descriptor expressible as a head noun, such a descriptor is 
added. Choosing the value of an attribute is done by an 
embedded iteration. It starts with the basic level value  attri-
buted to r, after which more specific values also attributed to 
r and assumed to be known to the user are tested for their 
discriminatory power. Finally, the least specific value that 
excludes the largest number of potential distractors and is 
known to the user is chosen. The schema of this procedure 
is given in Appendix I. The only modification we have done 
to the original version is the result of L as a non-distin-
guishing description in case of identification failure.
The algorithm by Dale and Reiter contains the principal 
operations that also other algorithms for generating referring 
expressions apply. The extension to boolean combinations 
of descriptors by van Deemter is essentially realized as an 
iteration around the Dale and Reiter algorithm, through 
building increasingly complex combinations, which other 
control regimes generate and maintain more effectively.
In order to control effects of facilities dealing with uncer-
tainty, the extended algorithm has four control parameters: 
? pmin, the minimal probability of recognition required 
for an attribute-value pair applicable to the intended 
referent, to justify its inclusion in the description, 
? ?p1, the minimal improvement in terms of probabi-
lity of identification of the intended referent over a 
potential distractor obtained through an additional 
attribute-value pair,
? ?p2, the minimal preference in terms of probability 
of identification of the intended referent over all poten-
tial distractors obtained through a description, and
? Complexity-limit, an upper bound on the number of 
descriptors collected in the distinguishing description.
In order to incorporate our concepts of representing 
uncertainty in this algorithm, we have to replace the inter-
face functions which access crisp data and we must modify 
yes-no decisions. These enhancements concern:
? the decision about whether a descriptor excludes a 
potential distractor (in the function RulesOut), 
? the choice of a value for an attribute (in the function 
FindBestvalue), and
? the termination of the overall procedure (in the 
function MakeReferringExpression)
Modifications of the reference algorithm are given in 
detail in the extended version in Appendix I ? some lines are 
marked by labels [Ni] for references from the text. 
Expressions of the form pr(r,L) compute the probability of 
identification of referent r  through the description L , 
according to the schema described in the previous sections.
Under conditions of uncertainty, determining whether a 
descriptor excludes a potential distractor may become a 
proper decision rather than a mere computation. A clear-cut 
case is only present if the repair facility is not applicable to 
one of the members of the contrast set, so that its associated 
probability of identification amounts to 0. This condition 
replaces the criterion that the user must know that this 
descriptor does not apply to some potential distractor in the 
function RulesOut [N7]. However, it would be a rather 
restrictive strategy to accept only those descriptors which 
definitely exclude a potential distractor. In fact, none of the 
descriptors that make up the example in Appendix II yield 
such a crisp discrimination. In addition to that, a descriptor is 
also valuable if it contributes to a better identification of the 
intended referent by increasing the difference to a potential 
distractor in the associated probabilities of identification by a 
significant margin (?p1). This criterion is added to the crisp 
criterion described above, encapsulated in the function Domi-
nate [N8], which is used for this decision instead of the 
function RulesOut [N2]. The idea is that subsequently chosen 
descriptors have comparable effects on the identification of 
some of the other potential distractors, so that the intended 
referent ultimately gains over all of them. The significance 
of this margin must be tuned in such a way that the gain 
over some potential distractors is not outweighted by a loss 
over some other potential distractors.
The suitability of a value for an attribute depends on two 
factors associated with uncertainty: the probability of recog-
nition associated with that value for the present user, and the 
effect of this value on excluding elements from the set of 
potential distractors. These two factors have adverse effects: 
while a more specific value has the potential of excluding an 
increasing number of potential distractors, its probability of 
recognition when applied to the intended referent may be 
lower than that of a less specific value. Consequently, it is 
not necessarily the case that an improved discriminatory 
power leads to a better overall effect. Hence, the choice of a 
value requires a minimal probability of recognition (p min , 
[N6]), and calls to Dominate replace calls to RulesOut. Addi-
tional variants of descriptors can be generated by enhancing 
the interface function MoreSpecificValue, also building 
disjunctions of values excluding each other, to cover cases 
described at the end of Section 4, that is, building 
disjunctions of descriptors by composing descriptors 
(possibly vague ones) that cover adjacent value ranges. 
The third factor, the termination criterion, is adapted to 
uncertainties by enhancing it in two ways: (1) a complexity 
limit is applied to the specifications in the description L 
[N3]; while this cut-off may serve practical considerations 
also without conditions of uncertainty (for a partitioning into 
sequences of descriptions [Horacek 2004]), it gains on rele-
vance in uncertain environments. (2) a certain degree of being 
Dominant  in the probability of identification over all 
potential distractors is considered sufficient (?p2, [N4]) rather 
than requiring the ultimate exclusion of all potential 
distractors. Finally, the conditions under which descriptors 
are selected, give rise to an optional optimization step. The 
prerequisite for this step is the distinction between 
descriptors which definitively exclude at least one potential 
distractor (Lro in the extended algorithm, [N1]) and others 
which only affect their associated probabilities of identifi-
cation, but do not make them 0. Then all subsets of the 
description built which contain at least Lro are examined 
[N5] whether they yield a better preference over all potential 
distractors in terms of their probabilities of identification 
[N9]. Through this measure, an early chosen descriptor with 
a probability of identification lower for the intended referent 
than for some potential distractors can finally be discarded, 
provided the discriminating effect on other potential 
distractors is also achieved by later chosen descriptors. In the 
example in Appendix II, all descriptors are categorized as 
optional ones, but for the one expressing the head noun ? 
which is precisely the reason why it is not optional.
Altogether, the algorithm selects descriptors which either 
exclude some potential distractors definitively, makes some 
of them rely on the repair mechanism, or simply increases 
the probability of identification of the intended referent 
considerably in comparison to elements of the contrast set. 
While this selection process works reasonably in most 
cases, it may turn out as problematic when several of the 
descriptors chosen are associated with limited probabilities 
of recognition for the intended referent in comparison to 
potential distractors not completely excluded. As a conse-
quence, these potential distractors may be judged superior in 
terms of the probability of identification even though they 
rely on the repair mechanism (see example 3 in Figure 5). 
This risk can be circumvented by using a relatively high 
pmin parameter, but this measure may easily lead to the 
exclusion of an otherwise beneficial descriptor under normal 
conditions. An improvement can be obtained by the call to 
the procedure Optimize. If one of the first two descriptors 
used in example 3 in Figure 5 does not definitively exclude a 
potential distractor, the procedure Optimize tests descriptor 
combinations without it, and one of those may yield a better 
result ? see also the example in Appendix II. A possible 
variations would be to allow just a single violation of the 
pmin restriction, for a descriptor with very good discrimi-
natory power.
So far, we have only elaborated changes for incorpo-
rating uncertainty concepts to the reference algorithm per se. 
Handling boolean combinations of descriptors through 
applying the reference algorithm to increasingly complex 
combinations also works with uncertainties, since all 
computations required are defined. More difficulties arise 
with ambitious control regimes, which rely on cut-off 
techniques, in addition to the complexity cut-off, such as 
dominance and value cut-offs, as introduced in [Horacek 
2004]. A complexity cut-off is already included in the 
extended reference algorithm. The two other cut-offs can be 
generalized, but this is likely to be associated with a 
significant loss of efficiency. In order for a descriptor to 
dominate another one, the dominating one must not only 
exclude all potential distractors that its competitor does, but 
it must also favor the intended referents over all potential 
distractors in terms of the associated  probabilities of identi-
fication ? this requirement reduces the application frequency 
of this cut-off considerably. A value cut-off, in turn, is 
applicable to a partial solution if a solution has already been 
found, and there are no descriptor combinations untested for 
the partial solution which may yield a solution with less 
complex specifications. This condition can also be met in 
the environment associated with uncertainties. In this 
environment, however, there is another factor that has an 
impact on the quality of the solution, that is the probability 
of identification, which cannot be assessed prior to actually 
choosing a descriptor and testing its effects. 
6 Conclusion
In this paper, we have presented an approach for generating 
referential descriptions under conditions of uncertainty. The 
approach combines a proper recognition of objects associated 
with some degree of uncertainty, as well as identification 
through a repair mechanism, motivated by the need to 
identify objects even for descriptions that originally appear 
uninterpretable. On these lines, we have reinterpreted 
concepts of algorithms generating referring expressions in 
view of uncertainties about the appearance of objects. 
Incorporating measures of uncertainty in such an algorithm 
attacks strong assumptions and effects underlying most of 
the existing algorithms:
? They typically require crisp specifications concerning 
attribution of descriptors to referents and knowledge of 
the audience. Especially the connection to modern user 
models may require coarse-grained interpretations here.
? A single result is produced even if several reasonable 
variants exist, and this choice is implicitly determined 
by the preference ordering imposed on the descriptors.
? The interaction with other components of an NL gener-
ation system and an embedding dialog system is rather 
limited. Reference generation is typically conceived as 
a pure functional service, with no feedback, taking into 
account syntactic constraints, at best (e.g., [Horacek 
1997]). An embedding dialog system has no chance to 
find out possible sources for an identification failure. 
The algorithm incorporating measures to deal with uncer-
tainties provides facilities to improve this situation:
? Specifications concerning attribution of descriptors to 
referents and knowledge of the audience can be done in 
a direct fashion, requiring no interpretations.
? There are some parameters to control the choice of 
descriptors, the conciseness and expected effectiveness 
of the result, including an afterwards optimization 
which only requires re-calculation of probabilities.
? The probabilities of identification associated with the 
intended referents and those potential distractors that 
fall under the repair facility give an indication about 
the likelihood of success of the identification task and 
also about potential sources for a failure. Moreover, 
the situation about probabilities and descriptors may 
suggest variants in building surface expressions, such 
as putting emphasis on a critical descriptor.
References
[Appelt 1985] Doug Appelt. Planning English Referring 
Expressions. Artificial Intelligence 26:1-33, 1985.
[Appelt and Kronfeld 1987] Doug Appelt and Amichai 
Kronfeld. A Computational Model of Referring . In Proc. 
of the 10th International Joint Conference on Artificial 
Intelligence (IJCAI-87), pp. 640-647, Milano, Italy, 
1987.
[Bateman 1999] John Bateman. Using Aggregation for 
Selecting Content when Generating Referring 
Expressions. In Proc. of the 37th Annual Meeting of the 
Association for Computational Linguistics (ACL-99), 
pp. 127-134, University of Maryland, 1999.
[Dale 1988] Robert Dale. Generating Referring Expressions 
in a Domain of Objects and Processes . PhD Thesis, 
Centre for Cognitive Science, University of Edinburgh, 
1988.
[Dale and Reiter 1995] Robert Dale and Ehud Reiter. 
Computational Interpretations of the Gricean Maxims in 
the Generation of Referring Expressions. Cognitive 
Science 18:233-263, 1995.
[Donellan 1966] K. Donellan. Reference and Definite 
Description. Philosophical Review 75:281-304, 1966.
[Edmonds 1994] Phil Edmonds. Collaboration on Reference 
to Objects that are not Mutually Known . In Proc. of the 
15th International Conference on Computational Lingu-
istics (COLING-94), pp. 1118-1122, 1994. 
[Gardent 2002] Claire Gardent. Generating Minimal Definite 
Descriptions. In Proc. of the 40th Annual Meeting of 
the Association for Computational Linguistics (ACL-
2002), pp. 96-103, Philadelphia, Pennsylvania, 2002.
[Goodman 1986] Bradley Goodman. Reference Identification 
and Reference Identification Failures . Computational 
Linguistics 12:273-305, 1986.
[Goodman 1987] Bradley Goodman. Communication and 
Miscommunication . Association of Computational 
Linguistics Series of Cambridge University Press, 
London, England, 1987.
[Grosz and Sidner 1986] Barbara Grosz and Candace Sidner. 
Attention, Intention, and the Structure of Discourse. 
Computational Linguistics 12:175-206, 1986.
[Heeman and Hirst 1995] Peter Heeman and Graeme Hirst. 
Collaborating on Referring Expressions . Computational 
Linguistics 21:351-382, 1995.
[Horacek 1997] Helmut Horacek. An Algorithm for 
Generating Referential Descriptions with Flexible Inter-
faces. In Proc. of the 35th Annual Meeting of the 
Association for Computational Linguistics and 8th 
Conference of the European Chapter of the Association 
for Computational Linguistics (ACL-EACL'97), pp. 
206-213, Madrid, Spain, 1997.
[Horacek 2003] Helmut Horacek. A Best-First Search 
Algorithm for Generating Referring Expressions. In 
Proc. of the 10th Conference of the European Chapter 
of the Association for Computational Linguistics 
(EACL-2003), Conference Companion (short paper), pp. 
103-106, Budapest, Hungary, 2003.
[Horacek 2004] Helmut Horacek. On Referring to Sets of 
Objects Naturally. In Proc. of the Third International 
Conference on Natural Language Generation (INLG-
2004), pp. 70-79, Brockenhurst, UK, 2004. 
[Krahmer, v. Erk and Verleg 2001] Emiel Krahmer, S. v. 
Erk, Andr? Verleg. A Meta-Algorithm for the Generation 
of Referring Expressions. In Proc. of the 8th European 
Workshop on Netural Language Generation (EWNLG-
2001), pp. 29-39, Toulouse, France, 2001.
[Kronfeld 1986] Amichai Kronfeld. Donellan's Distinction 
and a Computational Model of Reference. In Proc. of  the 
24th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-86), pp. 186-191, New York, 
NY, 1986.
[Levelt 1989] William Levelt. Speaking: From Intention to 
Articulation. MIT Press, 1989.
[McDonald 1981] David McDonald. Natural Language Gener-
ation as a Process of Decision Making under Constraints. 
PhD thesis, MIT, 1981.
[Pearl 1988] Judea Pearl. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inferences . 
Morgan Kaufman, San Mateo, California, 1988.
[Pechmann 1989] Thomas Pechmann. Incremental Speech 
Production and Referential Overspecification . Linguistics 
27:89-110, 1989.
[Reiter 1990] Ehud Reiter. The Computational Complexity 
of Avoiding Conversational Implicatures. In Proc. of the 
28th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-90), pp. 97-104, Pittsburgh, 
Pennsylvania, 1990. 
[Reiter and Dale 1992] Ehud Reiter and Robert Dale. 
Generating Definite NP Referring Expressions . In Proc. 
of the 14th International Conference on Computational 
Lingustics (COLING-92), pp. 232-238, Nantes, France, 
1992. 
[Rosch 1978] Eleanor Rosch. Principles of Categorization. 
In E. Rosch and B. Llyod  (eds.) Cognition and Catego-
rization, pp. 27-48, Hillsdale, NJ: Lawrence Erlbaum, 
1978.
[Stone 2000] Matthew Stone. On Identifying Sets. In Proc. 
of the First International Conference on Natural Langu-
age Generation (INLG-2000), pp. 116-123, Mitzpe 
Ramon, Israel, 2000. 
[van Deemter 2002] Kees van Deemter. Generating Referring 
Expressions: Boolean Extensions of the Incremental 
Algorithm. Computational Linguistics , 28 (1):37-52, 
2002.
[Wahlster 2004] Wolfgang Wahlster. REAL: REssourcen-
Adaptive Lokalisation . Project in SFB 378, Saarland 
University, 2004.
[Zadeh 1984] Lofti Zadeh. Making Computers Think like 
People. IEEE Spektrum, 8:26-32, 1984.
[Zadeh 1996] Lofti Zadeh. Fuzzy Logic and the Calculi of 
Fuzzy Rules and Fuzzy Graphs. International Journal of 
Multi-Valued Logic, 8:1-39, 1996.
Appendix I: Reference Algorithm ([Dale and Reiter 1995], left) and Extended Algorithm (right)
                                                                                                                           
MakeReferringExpression (r,C,P)
L ? {} 
for each member Ai of list P do
V = FindBestValue(r,Ai,BasicLevelValue(r,Ai))
if RulesOut(<Ai,V>) ? nil
then L ? L ? {<Ai,V>}
C ? C - RulesOut(<Ai,V>)
endif
if C = {} then
if <type,X> ? L for some X
then return L (an identifying description)
else return L ? {<type,BasicLevelValue(r,type)>}
endif
endif
return L (a non-identifying description)
FindBestValue (r,A,initial-value)
if UserKnows(r,<A,initial-value>) = true
then value ?  initial-value 
else value ? no-value 
endif
if (spec-value ?  MoreSpecificValue(r,A,value)) ? nil ^
(new-value ?  FindBestValue(r,A,spec-value)) ? nil ^
(|RulesOut(<A,new-value>)| > |RulesOut(<A,value>)|)
then value ?  new-value 
endif
return value
RulesOut (A,V)
if V = no-value
then return nil
else return {x: x ? C ^ UserKnows(x,<A,V>) = false}
endif 
                                                                                                                           
                                                                                                                           
MakeReferringExpression (r,C,P)
L ? {}, Lro ? {} [N1]
for each member Ai of list P do
V = FindBestValue(r,Ai,BasicLevelValue(r,Ai))
if Dominate(Ai,V) ? nil [N2]
then L ? L ? {<Ai,V>}
C ? C - RulesOut(<Ai,V>)
if RulesOut(<Ai,V>) ? nil
then Lro ? Lro ? {<Ai,V>}
endif  endif
if (C = {}) ? (|L| > Complexity-limit) ? [N3]
(?x ? C: (pr(r,L) - pr(x,L)) > ?p2) then [N4]
L ? Optimize(L,Lro) (optional) [N5]
if <type,X> ? L for some X
then return L (a likely identifying description)
else return L ? {<type,BasicLevelValue(r,type)>}
endif  endif
return L (an unlikely identifying description)
FindBestValue (r,A,initial-value)
if pr(r,{<A,initial-value>}) > pmin [N6]
then value ?  initial-value 
else value ? no-value 
endif
if (spec-value ?  MoreSpecificValue(r,A,value)) ? nil ^
(new-value ?  FindBestValue(r,A,spec-value)) ? nil ^
(|Dominate(A,new-value)| > |Dominate(A,value)|)
then value ?  new-value 
endif
return value
RulesOut (A,V)
if V = no-value then return nil
else return {x: x ? C ^ (pr(x,L ? {<A,V>}) = 0)} [N7]
endif 
Dominate (A,V)
if V = no-value then return nil
else return {x: x ? C ^ ((pr(x,L ? {<A,V>}) = 0) ? 
((pr(r,L ? {<A,V>}) - pr(x,L ? {<A,V>})) - [N8]
(pr(r,L) - pr(x,L))) > ?p1)}
endif 
Optimize (L1,L2)
Lopt ? L1, ppopt ? Min?x ? C(pr(r,L1) - pr(x,L1)) [N9]
forall L (L1 ? L ? L2) do
pp ? Min?x ? C(pr(r,L) - pr(x,L))
if ppopt < pp then Lopt ? L, ppopt ? pp endif
endforall
return Lopt
                                                                                                                          
Appendix II: An Example of the Extended Algorithm at Work
In this section, we demonstrate the functionality of the 
extended algorithm by an example that illustrates several 
features of this algorithm. As in the discussion in Section 
2, the scenario consists of three similar dogs, one of which 
is a bassett, which is also the intended referent. In addition, 
the bassett is brownish and has a long tail. The other two 
dogs have shorter tails and their skin is also brownish, but 
with some white resp. black portions, which makes the 
descriptor 'brownish' less appropriate than for the bassett. 
To make the example suitable for our purposes, the 
audience is assumed to have little knowledge about dog 
specifics, that is, they may recognize the intended referent 
as a bassett, but this is not very likely (we assume the 
category identification has a likelihood of 30%). In 
addition, it is assumed that the tails of the dogs, specific-
ally the one of the bassett, cannot be observed easily by the 
audience (again, we assume the recognition has a likelihood 
of 30%). Both, dog category and tail length are potentially 
confusable for all dogs.  These properties and associated 
probabilities of identification as listed below.
                                                                                                                           
Objects Attributes
category color tail-length
                                                                                                                           
dog1 bassett brownish long
dog2 dog brown-white short
dog3 dog brown-black short
                                                                                                                           
Probabilities of identification (per attribute-value and object)
bassett: p(dog1) = 0.3, p(dog2) = 0.0, p(dog3) = 0.0
brownish: p(dog1) = 0.9, p(dog2) = 0.8, p(dog3) = 0.8
long-tail: p(dog1) = 0.3, p(dog2) = 0.0, p(dog3) = 0.0
                                                                                                                           
Hence, the intended referent r is {dog1}, and the contrast set 
C is {dog2, dog3}. For demonstration purposes, we choose 
the following parameterizations:
? The attributes are considered according to the ordered 
preference list P = (color, category, tail-length), which 
in some sense reflects the ease of perception of color
? We choose 5% (0.05) for ?p1, which indicates suffi-
cient dominance, and 30% (0.3) for ?pmin, which 
indicates sufficient identification potenial (it always 
succeeds in the example); similarly, we choose 50% 
(0.5) for ?p2 which indicates sufficient discrimination 
(it never succeeds here, hence all descriptors are tried)
? We allow a maximum complexity of 3 descriptors, so 
that this cut-off criterion does not apply in our simple 
example, and we use the optional optimization step
? In order to compute probabilities of identification, we 
need to choose a values for the ?scale-down factor?, 
which will be 0.5, as mentioned in Section 4. Since 
no disjunctions and negations of descriptors are needed 
for our example, no further parameters are required.
We now illustrate the generation process step by step.
In the first step, ?brownish? is chosen as the value of the 
attribute ?color? of dog1, and its contribution to discriminate 
the intended referent from the elements of the contrast set is 
checked. To start with, its  identification potential, 0.9, is far 
higher than pmin (0.25).Since this descriptor is also applic-
able to both other dogs, dog2 and dog3, but with lower 
probability, these two objects still remain in the contrast set. 
Despite this limited discrimination, the attribute is chosen 
because it achieves more than the minimal dominance 
required: 0.9 - 0.8 equals 0.1, which is higher than ? p 2  
(0.05). Thus, the situation after step 1 is as follows (we 
neglect the repair factor, which is as low as 0.1 x 0.2 x 0.2):
pr(dog1) = 0.9, pr(dog2) = pr(dog3) = 0.8 
?pr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.1 > ?p1
In the next step, ?bassett? is chosen as the value of the 
attribute ?category? of dog1, and again its contribution to 
discriminate the intended referent from the elements of the 
contrast set is checked. Its  identification potential, 0.3, is 
sufficient, since it is higher than pmin (0.25). This descriptor 
is not applicable to the other dogs, dog2 and dog3. Never-
theless, they still remain in the contrast set since they poten-
tially are subject to the repair mechanism. The probabilities 
of identification are computed according to the schema in 
Figure 4: the product of the probabilities of "bassett" and 
"brownish" associated with dog1 yields 0.27. The repair 
factor, the complementing 0.73, is distributed evenly among 
all three dogs. Hence, the degree of dominance of this 
descriptor amounts to 0.27, which is higher than ?p2 (0.05).  
Thus, the situation after step 2 is as follows:
pr(dog1) = 0.513, pr(dog2) = pr(dog3) = 0.243 
?pr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.27 > ?p1
In the last step, ?long? is chosen as the value of the attri-
bute ?tail-length?.  Again, its  identification potential, 0.3, 
is sufficient, since it is higher than pmin (0.25). As in the 
previous step, dog2 and dog3 still remain in the contrast set 
since they potentially are subject to the repair mechanism. 
The product of the probabilities for dog1 results from the 
previous one, multiplied by 0.3, which yields 0.081. The 
repair factor, the complementing 0.919, is distributed by 
giving two parts to dog1 (the scale-down factor applies) and 
one part to each of the other dogs. Hence, the degree of 
dominance of this descriptor amounts to 0.31075, which is 
higher than ?p2 (0.05), which gives the final situation:
pr(dog1) = 0.5405, pr(dog2) = pr(dog3) = 0.22975 
?pr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.31075 > ?p1
Optimization attempts show that ?bassett? only is slightly 
inferior (pr(dog1) = 0.53, pr(dog2) = pr(dog3) = 0.23, ?pr = 
0.3), while ?bassett? together with ?long-tailed? is slightly 
superior (pr(dog1) = 0.545, pr(dog2) = pr(dog3) = 0.225, ?pr 
= 0.32) to the combination of all descriptors. Hence, the 
example demonstrates benefits and risks are comparable when 
only limited discrimination is possible by each descriptor.
Proceedings of the Fourth International Natural Language Generation Conference, pages 47?54,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating References to Parts of Recursively Structured Objects
Helmut Horacek
Universit?t des Saarlandes
FB 6.2 Informatik
 horacek@cs.uni-sb.de
Abstract
Algorithms that generate expressions to 
identify a referent are mostly tailored 
towards objects which are in some sense 
conceived as holistic entities, describing 
them in terms of their properties and 
relations to other objects. This approach 
may prove not fully adequate when 
referring to components of structured 
objects, specifically for abstract objects 
in formal domains, where scope and 
relative positions are essential features. 
In this paper, we adapt the standard Dale 
and Reiter algorithm to specifics of such 
references as observed in a corpus about  
mathematical proofs. Extensions incor-
porated include an incremental speciali-
zation of property values for metonymic 
references, local and global positions 
reflecting group formations and impli-
cature-based scope preferences to justify 
unique identification of the intended 
referent. The approach is primarily 
relevant for domains where abstract 
formal objects are prominent, but some 
of its features are also useful to extend 
the expressive repertoire of reference 
generation algorithms in other domains.
1 Introduction
Over the past two decades, a number of algo-
rithms for generating referring expressions 
have been proposed. Almost all of these 
algorithms conceive objects in some sense as 
holistic entities, describing them in terms of 
their properties and relations to other objects, 
but not treating components of an object as 
objects in their own rights. This approach may 
yield inadequate results for references to 
components of recursively structured objects. 
Consider, for instance, a Rubic?s cube where 
one side is currently visible, and reference is 
intended to a square consisting of the visible 
squares of four white subcubes, which are the 
only white elements on the visible side. The 
best way to refer to this composed structure is 
the concise ?the white square?, which exploits 
a preference for maximum scope objects, 
typical for such recursive structures. However, 
most reference generation algorithms would 
attempt to disambiguate the intended referent 
from its four components, producing an unne-
cessarily long expression, such as ?the big 
white square? or ?the white square which is 
composed of four squares?. These expressions 
are not really bad, especially the first one, but 
things might turn out really awkward for more 
complex structural compositions, where the 
maximum scope preference often allows the 
identification in a surprisingly concise form.
In this paper, we address this problem by 
examining referring expressions produced by 
humans in domains with recursively structured 
objects playing a prominent role. Specifically, 
we have studied referring expressions in a 
corpus of simulated human-computer dialogs 
about tutoring mathematical problem-solving 
(Wolska et al 2004, with recent additions in 
this paper). We express the criteria and prefer-
ences observed in a way compatible with the 
incremental reference generation algorithm of 
Dale and Reiter (1995), and we extend their 
algorithm by adapting the property selection 
and discrimination testing criteria accordingly.
This paper is organized as follows. First, we 
motivate our approach. Then we describe our 
corpus and the relevant phenomena observed 
in it. Next, we present extensions to the incre-
mental algorithm that allow the generation of 
this kind of referring expressions. Finally, we 
illustrate how some examples from the corpus 
are handled and discuss our achievements.
47
2 Previous Work
Within this paper, we adopt Dale?s terminology 
(1988). A referential description (Donellan 
1966) serves the purpose of letting the hearer 
or reader identify a particular object or set of 
objects in a situation. Referring expressions to 
be generated are required to be distinguishing 
descriptions, that is, descriptions of the entities 
being referred to, but not to any other object in 
the context set. A context set is defined as the 
set of the entities the addressee is currently 
assumed to be attending to ? this is similar to 
the concept of focus spaces of the discourse 
focus stack in Grosz? & Sidner?s (1986) theory 
of discourse structure. Moreover, the contrast 
set (the set of potential distractors (McDonald 
1981)) is defined to entail all elements of the 
context set except the intended referents.
Generating referring expressions is pursued 
since the eighties (e.g., (Appelt 1985), among 
several others). Subsequent years were charac-
terized by a debate about computational effi-
ciency versus minimality of the elements 
appearing in the resulting referring expression 
(Dale 1988, Reiter 1990, and several others). In 
the mid-nineties, this debate seemed to be 
settled in favor of the incremental approach 
(Dale and Reiter 1995) ? motivated by results 
of psychological experiments (e.g., Levelt 
1989), certain non-minimal expressions are 
tolerated in favor of adopting the fast strategy 
of incrementally selecting ambiguity-reducing 
attributes from a domain-dependent preference 
list. Complementary activities include the 
generation of vague descriptions (van Deemter, 
2000) and extensions to multimodal 
expressions (Van der Sluis 2005). Recently, 
algorithms have also been developed to the 
identification of sets of objects rather than 
individuals (Bateman 1999, Stone 2000, 
Krahmer, v. Erk, and Verweg 2001), and the 
repertoire of descriptions has been extended to 
boolean combinations of attributes, including 
negations (van Deemter 2002). To avoid the 
generation of redundant descriptions what 
incremental approaches typically do, Gardent 
(2002) and Horacek (2003) proposed exhaust-
ive resp. best-first searches.
All these procedures more or less share the 
design of the knowledge base which bears 
influence on the descriptor selection. Objects 
are conceived as atomic entities, which can be 
described in terms of sets of attributes and 
relations to other objects. In such a setting, a 
structured object can be represented, among 
others, by a set of relations to its components, 
which are themselves conceived as objects. An 
exception to this method is the work by Para-
boni and van Deemter (2002) who use hierar-
chical object representations to refer to parts of 
a book (figures, sections., etc.). Reference to 
such a component is made identifiable by iter-
atively adding a description of embedding 
structures until obtaining uniqueness. There 
are, however, no approaches addressing identi-
fication of objects or their components when 
the structures in these objects are of a recursive 
nature. Objects of this kind are mostly abstract 
ones, such as formulas, but also some sorts of 
geometric objects. Typical applications where 
such objects are prominent include scientific-
technical documentation and tutoring systems. 
As we will see in the next section, naturally 
observed references to such objects have a 
number of particularities which are not 
addressed by existing generation algorithms.
3 A Corpus with References to Formulas
In this paper, we analyze some phenomena in 
the context of references to mathematical 
formulas and their components, as observed in 
a corpus on simulated man-machine tutoring 
dialogs (Wolska et al, 2004). These dialogs 
constitute the result of Wizard-of-Oz exper-
iments in teaching students mathematical 
theorem proving in naive set theory resp. 
mathematical relations. In these experiments, a 
human wizard took the role of the tutor, with 
constraints on tutoring strategy and on use of 
natural language, although the constraints on 
natural language use were relaxed to encour-
age natural behavior on behalf of the student.
In the corpus obtained this way, a number 
of quite particular expressions referring to 
components of recursively structured objects ? 
the formulas ? showed up. Consequently, it is 
our goal to automate the production of these 
kinds of referring expressions in a more elab-
orate version of the simulated tutoring system, 
with full-fledged natural language generation.
Representative examples originating from 
our corpus appear in Figure 1. Each example 
consists of two parts: 1. a student utterance, 
mostly a formula, labeled by (#a), which is the 
context for interpreting subsequent referring 
expressions, the intended referent appearing in 
48
                                                                                                                                                                                                                                
1. Reference to the typographic order
(1a) (R?S)-1 = {(x,y) | (y,x) ? R?S} = {(x,y) | ?z (z ? M ^ (x,z) ? R-1 ^ (z,y) ? S-1)} = R-1?S-1
(1b) Das geht ein wenig schnell. Woher nehmen Sie die zweite Gleichheit?
(That was a little too fast. How did you find the second equality?)
(2a) Nach 9 ? ((y,z) ? R ^ (z,y) ? S)
(2b) Fast korrekt. Das zweite Vorkommen von y mu? durch x ersetzt werden.
Almost correct. The second occurrence of y must be replaced by x.
(3a) (R ? S)?T ist dann {(x,y) | ?z (z ? M ^ ((x,y) ? R ? (x,y) ? S) ^ (y,z) ? T)}
(3b) Nicht korrekt. Vermutlich liegt der Fehler nach der letzten ?und?-Verkn?pfung
Not correct. The mistake is probably located after the last ?and?-operation
2. Reference by exploiting default scope and metonymic relations
(4a) (R?S)-1 = {(x,y) | ?z (z ? M ^ (y,z) ? R-1 ^ (z,x) ? S-1)} ? S-1?R -1
(4b) Nein, das ist nicht richtig! Vergleichen Sie den zweiten Term mit Ihrer vorhergehenden 
Aussage!
No, this is not correct! Compare the second term with your previous assertion!
(5a) {(x,y) | (y,x) ? (R?S)} =  {(x,y) | (x,y) ? {(a,b) | ?z (z ? M) ^ (a,z) ? R ^ (z,b) ? S}}
(5b) Das stimmt so nicht. Die rechte Seite w?re identisch mit R?S.
This is not correct. The right side would be identical to R?S.
(6a) {(x,y) | ?z (z ? M) ^ ((x,z) ? R ? (x,z) ? S) ^ (z,y) ? S} =
{(x,y) | ?z (z ? M) ^ (z,y) ? S ^ ((x,z) ? R ? (x,z) ? S)} ? ((y,z) ? S ^ (z,y) ? S)
(6b) Auf der rechten Seite ist z nicht spezifiziert
On the right side, z is not specified
(7a) {(x,y) | ?z (z ? M) ^ ((x,z) ? R ? (x,z) ? S) ^ (z,y) ? S} = {(x,y) | ?z (z ? M) ^ 
(z,y) ? S ^ ((x,z) ? R ? (x,z) ? S)} ? ?z (z ? M ^ ((y,z) ? S ^ (z,y) ? S))
(7b) Diese Aussagen scheinen nicht gleichwertig zu sein. Ein z, das die Bedingung der rechten 
Aussage erf?llt, mu? nicht die Bedingung der linken Menge erf?llen.
These assertions do not seem to be of equal range. A z which fulfills the condition of the 
right assertion does not necessarily fulfill the condition of the left set.
3. Reference by exploiting default scope for building groups of objects
(8a) K((A ? B) ? (C ? D)) = K(A ? B) ? K(C ? D)
(8b) De Morgan Regel 2 auf beide Komplemente angewendet.
De Morgan Rule 2, applied to both complements.
(9a) (T-1?S-1)-1 ? (T-1?R-1)-1 = {(x,y) | (y,x) ? (T-1?S-1) ^ (y,x) ? (T-1?R-1)}
(9b) Dies w?rde dem Schnitt der beiden Mengen entsprechen.
This would correspond to the intersection of both sets.
4. Reference to regions by expressions involving vagueness
(10a) Also ist (R ? S)?T = {(x,z) | ?v (((x,v) ? R ? (x,v) ? S) ^ (z,v) ? T)}
(10b) Fast richtig. Am Ende der Formel ist ein Fehler.
Almost correct. At the end of the formula, there is a mistake.
(11a) Wegen der Formel f?r die Komposition folgt (R ? T)?(S ? T) = 
{(x,z) | ?z ((x,z) ? R ^ (z,y) ? T) ? ?z ((x,z) ? R ^ (z,y) ? T)}
(11b) Fast richtig. In der zweiten H?lfte der Formel ist ein Fehler.
Almost correct. In the second half of the formula, there is a mistake.
                                                                                                                                                                                                                                   
Figure 1: References to components of mathematical objects in dialog fragments of our corpus
49
bold, and 2. a tutor response labeled by (#b), 
with at least one referring expression, in italics. 
Texts are given in the original German version, 
accompanied by a translation into English.
The examples are partitioned into four cate-
gories. The first one, (examples 1 to 3), illus-
trate references by the typographical position,  
from left to right. Items referred to in this 
manner are qualified by their formal category. 
(1) refers to an equality ? two terms joined by 
an equal sign ? in a sequence of three equa-
lities. (2) refers to an instance of a variable, y, 
which must be further qualified by its position 
to distinguish it from another occurrence. (3) 
refers to the last occurrence of the and oper-
ator. Distinct surface forms are used for objects 
referred to by category (?second equality?) 
resp. by name (?second occurrence of y?).
The second category, the only one specific 
to recursively structured objects, comprises 
references which look similar to the previous 
ones, but they do not reflect the typographical 
position but structural embeddings. Objects 
referred to by this kind of expressions are 
found on the top level of the embedding object 
or close to it. In most cases, references to the 
embedding level where the intended referent is 
to be found are left unexpressed, which carries 
the implicit meaning that the referent appears 
at the top most level in which the referred cate-
gory can be found. In (4), for example, the 
entire formula contains many terms as its 
components, in various levels of embedding, so 
that orientation on typographic positions is not 
clear. However, on top level of the inequation 
chain, there are only three terms and the order 
among these is perfectly clear. (5) and (6) 
illustrate the role of incompleteness ? only 
?right side? is mentioned, leaving the object 
whose right side is meant implicit. Conse-
quently, this must be the right side of the whole 
formula. The last example in this category, (7) 
shows the reference to different levels of 
embedding in one sentence. While ?right 
assertion? refers to the expression on the right 
side of the equivalence on top level, ?left set? 
refers to the left of the two sets in the equation 
on the left side of that equivalence.
The third category, which features the refer-
ence to sets of objects, shows the interpretation 
of the embedding level in which the intended 
referent is to be found on the basis of number 
constraints. In precise terms, this is an instance 
of implicature (Grice 1975): if the number of 
objects that are on top level of the embedding 
object and satisfy the description, exceeds the 
cardinality specified, identification of the 
intended referents is transferred to one of the 
embedded substructures. In (8), three subex-
pressions satisfy the metonymic description 
?complement?, but the expression refers only 
to two. Consequently, the intended referents 
must be found in one of the substructures 
where a precise cardinality match is present ? 
here, the right side of the equation. Due to the 
implicature, expressing this additional qualifi-
cation is not required. An additional compli-
cation arises in the context of interference 
across referring expressions in one sentence. In 
(9), ?both sets? would be resolved to the two 
sides of the equation, without the context of the 
whole sentence. However, since ?this? refers 
to the result of the preceeding assertion, that is, 
the right side of the equation, this part is in 
some sense excluded from the context for 
resolving the next referring expressions. 
Hence, the left side of the equation yields the 
two sets on top level as interpretation.
The fourth category comprises examples of 
references which are in some sense associated 
with vagueness. In references to formulas, we 
consider the end  (example (10)) ? which 
means the region towards the end, as a vague 
expression, but also the second half (example 
(11)), since it is not entirely clear whether this 
expression must be interpreted structurally or 
typographically, and a precise interpretation of 
?half? in the typographical sense is pointless.
In the following, we present methods for the 
automated generation of referring expressions 
of the kind illustrated in Figure 1 ? concise 
ones. We address the following phenomena:
? Implicit scope interpretation 
? Incomplete or metonymic expressions
? Implicatures of category and cardinality
We do, however, restrict our task to the 
generation of single referring expressions with 
precise references. Hence, we do not address 
vagueness issues, since the meaning of 
expressions as occurring in (10) and (11) is 
not fully clear. Moreover, we do not accom-
modate the context due to previously gener-
ated referring expressions as in (9), which we 
assume to be done by the embedding process.
50
3 Operationalization
In this section, we describe an operationali-
zation of generating referring expressions of 
the kind discussed in the previous section. This 
operationalization is realized in terms of 
extensions to the algorithm by Dale and Reiter 
(1995). This algorithm assumes an envir-
onment with three interface functions: Basic-
LevelValue, accessing basic level categories of 
objects (Rosch 1978), MoreSpecificValue for 
accessing incrementally specialized attribute 
values according to a taxonomic hierarchy, and 
UserKnows for judging whether the user is 
familiar with the attribute value of an object. In 
a nutshell, MakeReferringExpression (Figure 2, 
including our extensions) iterates over the attri-
butes P of an intended referent r (or a set of 
referents). In FindBestValue, a value is chosen 
that is known to the user and maximizes discri-
mination (RulesOut) ? this value describes the 
intended referent and rules out at least one 
potential distractor in C. If existing, such values 
are iteratively collected in L, until P is empty or 
a distinguishing description is found. The 
value V  of an attribute A is chosen within an 
embedded iteration, starting with the basic level 
value attributed to r, after which more specific 
values also attributed to r and assumed to be 
known to the user are tested for their discri-
minatory power. Finally, the least specific value 
that excludes the largest number of potential 
distractors and is known to the user is chosen.
The extensions to handle particularities for 
our concerns comprise several components:
? The knowledge representation of objects 
is enhanced by properties expressing  
positions in some context and by a meta-
property about the use of descriptors  ? 
metonymic use of a descriptor when 
standing in relation to another one.
? The value selection for context-dependent 
descriptors requires special treatment; 
moreover, metonymic expressions are 
built in some sort of a two-step process.
? The discriminatory power in the subpro-
cedure RulesOut  is interpreted in local 
contexts for attributes expressing position.
? Termination criteria include a test whether 
a cardinality or position-based impli-
cature establishes a unique preference.
                                                                                                             
Group(x) ::=
G ? {y | ?z (?y dominates(z,y))} ^ G ? x
T-group-items :: = 
{x | ?y (??z dominates(z,y) ^ ?x dominates(y,x))}
L1-items :: = 
{x | ?y (y ? T-group-items ^ dominates(y,x))}
Group-pref(Group,N,V) :: =
  |(r ? C) ? Group| = N ^
?x ? ((r ? C) ? Group): Position(x,Group,N) = V
T-group-pref(N,V) ::=
Group-pref(T-group-items,N,V)
L1-group-pref(x,N,V) ::= ?T-Group-pref(N,V) ^
L1-items ? Group(x) ^ Group-pref(x,N,V) ^
(?y (Group(y) ^ L1-items ? Group(y)): 
   (x?y ? ?Group-pref(y,N,V)))
                                                                                                             
Figure 2: Definitions with group components 
In order to precisely define the extensions, 
we introduce some predicates and formal defi-
nitions for them (Figure 2). Composition in 
recursively structured objects is built on domi-
nates(x,y), expressing that component y is part 
of component x ; chained compositions of 
dominates are acyclic. On that basis, groups of 
items are built according to local contexts. A 
Group which some items x belong to is the set 
of items dominated by one same item, if 
existing. Otherwise, Group is empty. A special 
group is the set of items on top level, T-group-
items, which are all dominated by the entire 
structure, the root item, which is not domi-
nated by any item. These items also build a 
group. In contrast, L1-items, which comprise 
the items one level below the T-group-items, 
are not all in one group. Intersection with the 
Group predicate yields subsets, where each 
element in these sets is dominated by one and 
the same T-group-item (see the definition of 
L1-group-pref). A central definition is Group-
pref (group preference), used for testing the 
effect of implicatures. It is defined for the set 
of relevant items to be used within the algo-
rithm (r ?  C), that is, the intended referents 
and still existing distractors, in relation to a 
Group , in the context of cardinality N  and 
position V, which apply to the set of items. For 
that group to be preferred, the relevant items 
falling into that group must match the given 
cardinality and the position description (see 
the definition of Position in the next para-
graph). On that basis, T-group-pref expresses
51
                                                                                                             
MakeReferringExpression (r,C,P)
L ? {}, Ctotal ? C       [1]
for each member Ai of list P do   
case Ai of    [2]
cardinality: V ? |r|      [3]
global-position: V ? Position(r,Ctotal,|r|)    [4]
local-position: V ? Position(r,Group(r),|r|)    [5]
other: V = FindBestValue(r,Ai,BasicLevelValue(r,Ai))
end case
if RulesOut(<Ai,V>,C) ? nil then
if metonymic(Ai,X) and <type,X> ? L for some X      [6]
  and RulesOut(<Ai,V>,Ctotal) ?     [7]
       RulesOut(<type,X>,Ctotal)     [8]
then L ? L  \ {<type,X>} ? {<type,V>}      [9]
else L ? L ? {<Ai,V>} end if
C ? C - RulesOut(<Ai,V>,C)    [10]
end if
if C = {} or Preference-by-Implicature then   [11]
if <type,X> ? L for some X
then return L (an identifying description)
else return L ? {<type,BasicLevelValue(r,type)>}
end if end if
end for
return L (a non-identifying description)
FindBestValue (r,A,initial-value)
if UserKnows(r,<A,initial-value>) = true
then value ?  initial-value 
else value ? no-value end if
if (spec-value ? MoreSpecificValue(r,A,value)) ? nil ^
(new-value ? FindBestValue(r,A,spec-value)) ? nil ^
(|RulesOut(<A,new-value>,C)| > 
|RulesOut(<A,value>,C)|)  [12]
then value ?  new-value end if  
return value
RulesOut (<A,V>,C)  [13]
if V = no-value then return nil
else case Ai of  [14]
cardinality: return C ? ? Group(c) c ? C,
         where |Group(c) ? C | < V  [15]
global-position: return {x : x ? C ^ Position(x,Ctotal,|r|) ? V  [16] 
local-position: return {x : x ? C ^ Position(x,Group(x),|r|) ? V  
other: return {x: x ? C ^ UserKnows(x,<A,V>) = false}
end case end if 
      Preference-by-Implicature  [17]
V ?  any, N ?  any
 if <global-position,V> ? L ? <local-position,V> ? L ?
   <cardinality,N> ? L  then  [18]
 return (T-group-pref(|r|,V) ^ T-group-items ? r ) ?
 (L1-items ? r ^  L1-group-pref(r,|r|,V))  [19]
else return false end if
                                                                                                             
Figure 3: The algorithm in pseudo-code 
preference for top-group items, when bound 
to Group, and L1-group-pref expresses prefer-
ence for such a group with x one level below.
The knowledge representation of objects is 
enriched by some properties which are not 
intrinsic to an object itself. These properties 
comprise descriptors cardinality, position, and 
the meta-property metonymic. The predicate 
metonymic(x,y) expresses the acceptability of 
a metonymic reference of a descriptor x for a 
category y (e.g., an operator for a formula, in  
mathematical domains). The descriptor cardi-
nality easily fits in the standard schema of the 
procedure. However, it only contributes to the 
discrimination from potential distractors in the 
context of effects of implicature. The most 
complex addition is the descriptor position, 
which expresses some sort of relative position 
of an object considered within the context of a 
set of comparable objects (e.g., first, second). 
There are two dimensions along which such 
descriptors are meaningful in the domain of 
mathematical formulas and in similar domains 
with recursively structured objects: (1) the 
typographical position within the entire object, 
referred to by the descriptor global-position, 
and (2) the position within the structural level 
where the object in question resides, referred 
to by the descriptor local-position. Moreover, 
that position also depends on the number of 
objects considered, if subgroups of objects are 
built prior to checking their position within 
the entire group (e.g,: the first two items). This 
information is encapsulated in the function 
Position(x,y,n), where x denotes the object or 
set of objects whose position within group y is 
the value of that function, where subgroups of 
n  objects are formed. In order to yield a 
proper result, x must be a subset of y and the 
position value within y must be the same for 
all elements of x. Otherwise, the value is unde-
fined. For example, for a group G=<1,2,3,4, 
5,6>, Position({3},G,1) = 3, Position({3},G,2) 
= 2, and Position({2,3},G,2) = undefined. In 
some sense, this handling of positions is a 
generalization of the ordering for vague 
descriptors in (van Deemter 2006). Also in 
accordance with van Deemter, we separate 
descriptor selection from surface form deter-
mination, yielding, for example, ?left set? for 
{<type,set>, <local-position,first>}, the first 
part of an equation, and ?second occurrence 
of x? for {<type,x>, <local-position,second>}.  
52
In order to process these enhanced represen-
tations adequately, we have incorporated  
appropriate modifications in the procedure 
MakeReferringExpression (labeled by [#] in 
Figure 3). First, the original set of potential 
distractors is stored for computations within a 
global context [1]. Then the value selection 
for the attribute currently considered is done 
[2], which is different from the usual call to 
FindBestValue for cardinality [3], global-
position [4], and local-position [5]; the latter 
two are realized by the function Position, with 
appropriate instantiations for the group para-
meter. Next, the treatment for the inclusion of 
metonymic properties in the description is 
addressed. If the metonymic descriptor fits to 
the object category [6], and its discriminatory 
power [7] dominates that associated with the 
type descriptor [8], the descriptor values are 
conflated by overwriting the type value by 
that of the metonymic descriptor [9]. The two 
calls to RulesOut involved in the above test 
([7] and [8]) are the only references to Rules 
Out where effects on the original, entire set of 
distractors are tested. Therefore, the parameter 
C is added in the definition of RulesOut [13] 
and in all other places where that procedure is 
called [10], [12]. Similarly to the inclusion of 
attribute-value pairs in the description, the 
exclusion tests in RulesOut are specific for 
non-intrinsic attributes [14]. For cardinality, 
those distractors are excluded which belong to 
a group where the number of still relevant 
distractors (those consistent with the partial 
description built so far) is below that cardina-
lity [15]. Similarly, for testing position values, 
those distractors are picked for which the 
values returned by the function Position, in 
dependency of the relevant scope ? the group 
the intended referent(s) belong to, are not 
consistent with value of the attribute consi-
dered (global-position resp. local-position) 
[16]. Finally, the termination criterion [11] is 
enhanced, by taking into account the effect of 
implicatures through cardinality and position 
descriptors, by the function Preference-by-
implicature [17]. In this function, the values 
of cardinality and global-position or local-
position are instantiated, provided they appear 
in the description L [18]. The return value is 
the result of a test whether there exists prefer-
ence for the top-level, or for that level 1 group 
which contains the intended referents [19]. 
4 Examples
In this section, we illustrate how particularities 
of our application domain are modeled and 
how the procedure behaves in generating the 
referring expressions observed in our corpus. 
The ordered list of attributes, P, consists of 
<type, form, cardinality, global-order, local-
order> for atomic items and of <type, oper-
ator, cardinality, local-order, dominated-by> 
for the composed expressions ? dominated-by 
is the inverse of dominates. The meta-predi-
cate metonymic is instantiated for pairs <vari-
able, form>, <expression, local-order>, and 
<term, operator> for producing expressions 
such as ?x? referring to variable x, ?left 
side? referring to the left part of an assertion 
or equation, and ?complement? referring to 
a term with complement as top level operator. 
We show the generation of two examples.
1. example: ?Left set? in (7) in Figure 1. 
It is generated by choosing ?set? as the type, 
followed by unsuccessful attempts to pick an 
operator attribute (there is none defined for 
that set), and a cardinality (which yields no 
discrimination). Then ?first? is chosen for 
local-ordering, yielding unique identification 
(the embedding is left implicit), and this value 
is expressed by ?left? on the surface. 
2. example: ?both complements? in (8). 
It is generated by choosing ?term? as the 
type, followed by ?complement? as the oper-
ator , which overwrites ?term? due to its 
specification as metonymic with respect to that 
category. Then ?2? is chosen for cardinality, 
which yields unique identification since a 
subgroup preference for level one is present.
Altogether, the algorithm is able to gener-
ate the expressions occurring in our corpus, or 
quite similar ones, assisted by the application-
specific tailored list P. Exceptions constitute 
reference to regions related to some formula 
component, such as (3) in Figure 1, effects of 
interference of scope across several referring 
expressions, such as (9), and expressions 
involving vague region descriptors, such as 
(10) and (11). While the last set of examples 
comprises more than referring expressions, 
the first two can be handled, but the generated 
expressions are typically a bit cumbersome, 
such as ?the third term in the condition of the 
set? instead of ?after the last ?and?-oper-
ation? in (3) and ?both sets on the left side? 
instead of simply ?both sets? in (9).
53
5 Conclusion and Discussion
In this paper, we have presented an approach to 
generating referring expressions that identify 
components of recursively structured objects. 
Known techniques are enhanced by measures 
building metonymic expressions, descriptors 
expressing positions relative to some subgroup 
of object components, and exploiting the effect 
of implicatures due to cardinality and position 
descriptors. Concise expressions can be gener-
ated, in accordance with those in our corpus.
While our elaborations are domain-specific 
to a certain extent, several parts of our method 
are also much broader applicable. Metonymic 
expressions are quite common, and we think 
that building them within the task of reference 
generation is superior to doing this in a process 
thereafter, because this enables an easy compu-
tation of the discrminatory power of both alter-
natives, the implicit and the explicit one. 
Another aspect of broader relevance concerns 
the effect of implicatures in connection with 
object subgroups. While the group building 
itself, which is based on compositions of the 
relation dominates, is specific to our envir-
onment, the techniques to establish preferences 
among groups and deriving identification from 
that pertain to other environments. For 
instance, when a subgroup of two items of 
some kind is visually identifiable in the context 
of a few other subgroups with different cardi-
nalities, ?the two X?s? would lead to the identi-
fication of the subgroup in focus, through the 
effect of implicature, the group formation 
being based on local proximity. Thus, only the 
group formation schema needs to be changed.  
References
Appelt, D. 1985. Planning English Referring 
Expressions. Artificial Intelligence 26, pp. 1-33.
Bateman, J. 1999. Using Aggregation for Selecting 
Content when Generating Referring Expressions. 
In Proc. of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-99), 
pp. 127-134, University of Maryland.
Dale, R. 1988. Generating Referring Expressions in a 
Domain of Objects and Processes. PhD Thesis, 
Centre for Cognitive Science, Univ. of Edinburgh.
Dale, R., and Reiter, E. 1995. Computational Inter-
pretations of the Gricean Maxims in the Gener-
ation of Referring Expressions. Cognitive Science 
18, pp. 233-263.
Donellan, K. 1966. Reference and Definite Descrip-
tion. Philosophical Review 75, pp. 281-304.
Gardent, C. 2002. Generating Minimal Definite Des-
criptions. In Proc. of the 40th Annual Meeting of 
the Association for Computational Linguistics 
(ACL-2002), pp. 96-103, Philadelphia, Pennsylvania.
Grice, H. 1975. Logic and Conversation. In Syntax 
and Semantics: Vol. 3, Speech Acts, pp. 43-58, 
Academic Press.
Grosz, B., and Sidner, C. 1986. Attention, Intention, 
and the Structure of Discourse. Computational 
Linguistics 12, pp. 175-206.
Horacek, H. 2003. A Best-First Search Algorithm for 
Generating Referring Expressions. In Proc. of the 
10th Conference of the European Chapter of the 
Assoc ia t ion  for  Computa t iona l  Linguistics 
(EACL-2003), Conference Companion (short 
paper), pp. 103-106, Budapest, Hungary.
Krahmer, E., v. Erk,  S., and Verleg, A. 2001. A 
Meta-Algorithm for the Generation of Referring 
Expressions. In Proc. of the 8th European 
Workshop on Natural Language Generation 
(EWNLG-2001), pp. 29-39, Toulouse, France.
Levelt, W. 1989. Speaking: From Intention to Articu-
lation. MIT Press.
McDonald, D. 1981. Natural Language Generation as 
a Process of Decision Making under Constraints. 
PhD thesis, MIT.
Reiter, E. 1990. The Computational Complexity of 
Avoiding Conversational Implicatures . In Proc. of 
the 28th Annual Meeting of the Association for 
Computational Linguistics (ACL-90), pp. 97-104, 
Pittsburgh, Pennsylvania. 
Rosch, E. 1978. Principles of Categorization . In E. 
Rosch and B. Llyod (eds.) Cognition and Catego-
rization, pp. 27-48, Hillsdale, NJ: Lawrence Erlbaum.
Stone, M. 2000. On Identifying Sets. In Proc. of the 
First International Conference on Natural Langu-
age Generation (INLG-2000),  pp. 116-123, 
Mitzpe Ramon, Israel. 
van Deemter, K. 2000. Generating Vague Descrip-
tions. In Proc. of the First International  Natural 
Language Generation Conference (INLG-2000),  
pp. 179-185, Mitzpe Ramon, Israel. 
Paraboni, I., and van Deemter, K. 2002. Generating 
Easy References: the Case of Document Deixis. In 
Proc. of the Second International Natural Langu-
age Generation Conference (INLG-2002) , pp. 
113-119, Harriman, NY, USA. 
van Deemter, K. 2002. Generating Referring Expressi-
ons: Boolean Extensions of the Incremental Algo-
rithm. Computational Linguistics, 28(1), pp. 37-52.
van Deemter, K. 2006. Generating Referring 
Expressions that Involve Gradable Properties. 
Computational Linguistics, to appear.
van der Sluis, I. 2005. Multimodal Reference. Disser-
tation, Tilburg University.
Wolska, M., Vo, B., Tsovaltzi, D., Kruijff-Korbayo-
v?, I., Karagjosova, E., Horacek, H., Gabsdil, M., 
Fiedler, A., and Benzm?ller, C. 2004. An Anno-
tated Corpus of Tutorial Dialogs on Mathematical 
Theorem Proving. In Proc. of the 4th Inter-
national Conference on Language Resources and 
Evaluation, pp. 1007-1010, Lisbon, Portugal.
54
