Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1015?1023, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Word Sense Disambiguation Using Topic Features
Jun Fu Cai, Wee Sun Lee
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{caijunfu, leews}@comp.nus.edu.sg
Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
17 Queen Square, London WC1N 3AR, UK
ywteh@gatsby.ucl.ac.uk
Abstract
This paper presents a novel approach for ex-
ploiting the global context for the task of
word sense disambiguation (WSD). This is
done by using topic features constructed us-
ing the latent dirichlet alocation (LDA) al-
gorithm on unlabeled data. The features are
incorporated into a modified na??ve Bayes
network alongside other features such as
part-of-speech of neighboring words, single
words in the surrounding context, local col-
locations, and syntactic patterns. In both the
English all-words task and the English lex-
ical sample task, the method achieved sig-
nificant improvement over the simple na??ve
Bayes classifier and higher accuracy than the
best official scores on Senseval-3 for both
task.
1 Introduction
Natural language tends to be ambiguous. A word
often has more than one meanings depending on the
context. Word sense disambiguation (WSD) is a nat-
ural language processing (NLP) task in which the
correct meaning (sense) of a word in a given context
is to be determined.
Supervised corpus-based approach has been the
most successful in WSD to date. In such an ap-
proach, a corpus in which ambiguous words have
been annotated with correct senses is first collected.
Knowledge sources, or features, from the context of
the annotated word are extracted to form the training
data. A learning algorithm, like the support vector
machine (SVM) or na??ve Bayes, is then applied on
the training data to learn the model. Finally, in test-
ing, the learnt model is applied on the test data to
assign the correct sense to any ambiguous word.
The features used in these systems usually in-
clude local features, such as part-of-speech (POS)
of neighboring words, local collocations , syntac-
tic patterns and global features such as single words
in the surrounding context (bag-of-words) (Lee and
Ng, 2002). However, due to the data scarcity prob-
lem, these features are usually very sparse in the
training data. There are, on average, 11 and 28
training cases per sense in Senseval 2 and 3 lexi-
cal sample task respectively, and 6.5 training cases
per sense in the SemCor corpus. This problem is
especially prominent for the bag-of-words feature;
more than hundreds of bag-of-words are usually ex-
tracted for each training instance and each feature
could be drawn from any English word. A direct
consequence is that the global context information,
which the bag-of-words feature is supposed to cap-
ture, may be poorly represented.
Our approach tries to address this problem by
clustering features to relieve the scarcity problem,
specifically on the bag-of-words feature. In the pro-
cess, we construct topic features, trained using the
latent dirichlet alocation (LDA) algorithm. We train
the topic model (Blei et al, 2003) on unlabeled data,
clustering the words occurring in the corpus to a pre-
defined number of topics. We then use the resulting
topic model to tag the bag-of-words in the labeled
corpus with topic distributions. We incorporate the
distributions, called the topic features, using a sim-
ple Bayesian network, modified from na??ve Bayes
1015
model, alongside other features and train the model
on the labeled corpus. The approach gives good per-
formance on both the lexical sample and all-words
tasks on Senseval data.
The paper makes mainly two contributions. First,
we are able to show that a feature that efficiently
captures the global context information using LDA
algorithm can significantly improve the WSD ac-
curacy. Second, we are able to obtain this feature
from unlabeled data, which spares us from any man-
ual labeling work. We also showcase the potential
strength of Bayesian network in the WSD task, ob-
taining performance that rivals state-of-arts meth-
ods.
2 Related Work
Many WSD systems try to tackle the data scarcity
problem. Unsupervised learning is introduced pri-
marily to deal with the problem, but with limited
success (Snyder and Palmer, 2004). In another ap-
proach, the learning algorithm borrows training in-
stances from other senses and effectively increases
the training data size. In (Kohomban and Lee,
2005), the classifier is trained using grouped senses
for verbs and nouns according to WordNet top-level
synsets and thus effectively pooling training cases
across senses within the same synset. Similarly,
(Ando, 2006) exploits data from related tasks, using
all labeled examples irrespective of target words for
learning each sense using the Alternating Structure
Optimization (ASO) algorithm (Ando and Zhang,
2005a; Ando and Zhang, 2005b). Parallel texts is
proposed in (Resnik and Yarowsky, 1997) as po-
tential training data and (Chan and Ng, 2005) has
shown that using automatically gathered parallel
texts for nouns could significantly increase WSD ac-
curacy, when tested on Senseval-2 English all-words
task.
Our approach is somewhat similar to that of us-
ing generic language features such as POS tags; the
words are tagged with its semantic topic that may be
trained from other corpuses.
3 Feature Construction
We first present the latent dirichlet alocation algo-
rithm and its inference procedures, adapted from the
original paper (Blei et al, 2003).
3.1 Latent Dirichlet Allocation
LDA is a probabilistic model for collections of dis-
crete data and has been used in document model-
ing and text classification. It can be represented
as a three level hierarchical Bayesian model, shown
graphically in Figure 1. Given a corpus consisting of
M documents, LDA models each document using a
mixture over K topics, which are in turn character-
ized as distributions over words.
?
wz??
N
M
Figure 1: Graphical Model for LDA
In the generative process of LDA, for each doc-
ument d we first draw the mixing proportion over
topics ?d from a Dirichlet prior with parameters ?.
Next, for each of the Nd words wdn in document d, a
topic zdn is first drawn from a multinomial distribu-
tion with parameters ?d. Finally wdn is drawn from
the topic specific distribution over words. The prob-
ability of a word token w taking on value i given
that topic z = j was chosen is parameterized using
a matrix ? with ?ij = p(w = i|z = j). Integrating
out ?d?s and zdn?s, the probability p(D|?, ?) of the
corpus is thus:
M?
d=1
?
p(?d|?)
(
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?)
)
d?d
3.1.1 Inference
Unfortunately, it is intractable to directly solve the
posterior distribution of the hidden variables given a
document, namely p(?, z|w, ?, ?). However, (Blei
et al, 2003) has shown that by introducing a set of
variational parameters, ? and ?, a tight lower bound
on the log likelihood of the probability can be found
using the following optimization procedure:
(??, ??) = argmin
?,?
D(q(?, z|?, ?)?p(?, z|w, ?, ?))
1016
where
q(?, z|?, ?) = q(?|?)
N?
n=1
q(zn|?n),
? is the Dirichlet parameter for ? and the multino-
mial parameters (?1 ? ? ??N ) are the free variational
parameters. Note here ? is document specific in-
stead of corpus specific like ?. Graphically, it is rep-
resented as Figure 2. The optimizing values of ? and
? can be found by minimizing the Kullback-Leibler
(KL) divergence between the variational distribution
and the true posterior.
?
?
?
z
N
M
Figure 2: Graphical Model for Variational Inference
3.2 Baseline Features
For both the lexical sample and all-words tasks,
we use the following standard baseline features for
comparison.
POS Tags For each training or testing word, w,
we include POS tags for P words prior to as well as
after w within the same sentence boundary. We also
include the POS tag of w. If there are fewer than
P words prior or after w in the same sentence, we
denote the corresponding feature as NIL.
Local Collocations Collocation Ci,j refers to the
ordered sequence of tokens (words or punctuations)
surrounding w. The starting and ending position of
the sequence are denoted i and j respectively, where
a negative value refers to the token position prior to
w. We adopt the same 11 collocation features as
(Lee and Ng, 2002), namely C?1,?1, C1,1, C?2,?2,
C2,2, C?2,?1, C?1,1, C1,2, C?3,?1, C?2,1, C?1,2,
and C1,3.
Bag-of-Words For each training or testing word,
w, we get G words prior to as well as after w, within
the same document. These features are position in-
sensitive. The words we extract are converted back
to their morphological root forms.
Syntactic Relations We adopt the same syntactic
relations as (Lee and Ng, 2002). For easy reference,
we summarize the features into Table 1.
POS of w Features
Noun Parent headword h
POS of h
Relative position of h to w
Verb Left nearest child word of w, l
Right nearest child word of w, r
POS of l
POS of r
POS of w
Voice of w
Adjective Parent headword h
POS of h
Table 1: Syntactic Relations Features
The exact values of P and G for each task are set
according to cross validation result.
3.3 Topic Features
We first select an unlabeled corpus, such as 20
Newsgroups, and extract individual words from it
(excluding stopwords). We choose the number of
topics, K, for the unlabeled corpus and we apply the
LDA algorithm to obtain the ? parameters, where
? represents the probability of a word wi given a
topic zj , p(wi|zj) = ?ij . The model essentially
clusters words that occurred in the unlabeled cor-
pus according to K topics. The conditional prob-
ability p(wi|zj) = ?ij is later used to tag the words
in the unseen test example with the probability of
each topic.
For some variants of the classifiers that we con-
struct, we also use the ? parameter, which is doc-
ument specific. For these classifiers, we may need
to run the inference algorithm on the labeled corpus
and possibly on the test documents. The ? param-
eter provides an approximation to the probability of
1017
selecting topic i in the document:
p(zi|?) =
?i
?
K ?k
. (1)
4 Classifier Construction
4.1 Bayesian Network
We construct a variant of the na??ve Bayes network
as shown in Figure 3. Here, w refers to the word.
s refers to the sense of the word. In training, s is
observed while in testing, it is not. The features f1
to fn are baseline features mentioned in Section 3.2
(including bag-of-words) while z refers to the la-
tent topic that we set for clustering unlabeled corpus.
The bag-of-words b are extracted from the neigh-
bours of w and there are L of them. Note that L can
be different from G, which is the number of bag-of-
words in baseline features. Both will be determined
by the validation result.
? ? ?
? ?? ?
baselinefeatures
w
s
fnf1
b
z
L
Figure 3: Graphical Model with LDA feature
The log-likelihood of an instance, `(w, s, F, b)
where F denotes the set of baseline features, can be
written as
= logp(w) + logp(s|w) +
?
F
log(p(f |s))
+
?
L
log
(
?
K
p(zk|s)p(bl|zk)
)
.
The log p(w) term is constant and thus can be ig-
nored. The first portion is normal na??ve Bayes. And
second portion represents the additional LDA plate.
We decouple the training process into three separate
stages. We first extract baseline features from the
task training data, and estimate, using normal na??ve
Bayes, p(s|w) and p(f |s) for all w, s and f . The
parameters associated with p(b|z) are estimated us-
ing LDA from unlabeled data. Finally we estimate
the parameters associated with p(z|s). We experi-
mented with three different ways of both doing the
estimation as well as using the resulting model and
chose one which performed best empirically.
4.1.1 Expectation Maximization Approach
For p(z|s), a reasonable estimation method is to
use maximum likelihood estimation. This can be
done using the expectation maximization (EM) algo-
rithm. In classification, we just choose s? that maxi-
mizes the log-likelihood of the test instance, where:
s? = argmax
s
`(w, s, F, b)
In this approach, ? is never used which means the
LDA inference procedure is not used on any labeled
data at all.
4.1.2 Soft Tagging Approach
Classification in this approach is done using the
full Bayesian network just as in the EM approach.
However we do the estimation of p(z|s) differently.
Essentially, we perform LDA inference on the train-
ing corpus in order to obtain ? for each document.
We then use the ? and ? to obtain p(z|b) for each
word using
p(zi|bl, ?) =
p(bl|zi)p(zi|?)
?
K p(bl|zk)p(zk|?)
,
where equation [1] is used for estimation of p(zi|?).
This effectively transforms b to a topical distri-
bution which we call a soft tag where each soft
tag is probability distribution t1, . . . , tK on topics.
We then use this topical distribution for estimating
p(z|s). Let si be the observed sense of instance i
and tij1 , . . . , t
ij
K be the soft tag of the j-th bag-of-
word feature of instance i. We estimate p(z|s) as
p(zjk|s) =
?
si=s t
ij
k
?
si=s
?
k? t
ij
k?
(2)
This approach requires us to do LDA inference on
the corpus formed by the labeled training data, but
1018
not the testing data. This is because we need ? to
get transformed topical distribution in order to learn
p(z|s) in the training. In the testing, we only apply
the learnt parameters to the model.
4.1.3 Hard Tagging Approach
Hard tagging approach no longer assumes that z is
latent. After p(z|b) is obtained using the same pro-
cedure in Section 4.1.2, the topic zi with the high-
est p(zi|b) among all K topics is picked to represent
z. In this way, b is transformed into a single most
?prominent? topic. This topic label is used in the
same way as baseline features for both training and
testing in a simple na??ve Bayes model.
This approach requires us to perform the transfor-
mation both on the training as well as testing data,
since z becomes an observed variable. LDA infer-
ence is done on two corpora, one formed by the
training data and the other by testing data, in order
to get the respective values of ?.
4.2 Support Vector Machine Approach
In the SVM (Vapnik, 1995) approach, we first form a
training and a testing file using all standard features
for each sense following (Lee and Ng, 2002) (one
classifier per sense). To incorporate LDA feature,
we use the same approach as Section 4.1.2 to trans-
form b into soft tags, p(z|b). As SVM deals with
only observed features, we need to transform b both
in the training data and in the testing data. Compared
to (Lee and Ng, 2002), the only difference is that for
each training and testing case, we have additional
L ?K LDA features, since there are L bag-of-words
and each has a topic distribution represented by K
values.
5 Experimental Setup
We describe here the experimental setup on the En-
glish lexical sample task and all-words task.
We use MXPOST tagger (Adwait, 1996) for POS
tagging, Charniak parser (Charniak, 2000) for ex-
tracting syntactic relations, SVMlight1 for SVM
classifier and David Blei?s version of LDA2 for LDA
training and inference. All default parameters are
used unless mentioned otherwise. For all standard
1http://svmlight.joachims.org
2http://www.cs.princeton.edu/?blei/lda-c/
baseline features, we use Laplace smoothing but for
the soft tag (equation [2]), we use a smoothing pa-
rameter value of 2.
5.1 Development Process
5.1.1 Lexical Sample Task
We use the Senseval-2 lexical sample task for
preliminary investigation of different algorithms,
datasets and other parameters. As the dataset is used
extensively for this purpose, only the Senseval-3 lex-
ical sample task is used for evaluation.
Selecting Bayesian Network The best achievable
result, using the three different Bayesian network
approaches, when validating on Senseval-2 test data
is shown in Table 2. The parameters that are used
are P = 3 and G = 3.
EM 68.0
Hard Tagging 65.6
Soft Tagging 68.9
Table 2: Results on Senseval-2 English lexical sam-
ple using different Bayesian network approaches.
From the results, it appears that both the EM and
the Hard Tagging approaches did not yield as good
results as the Soft Tagging approach did. The EM
approach ignores the LDA inference result, ?, which
we use to get our topic prior. This information is
document specific and can be regarded as global
context information. The Hard Tagging approach
also uses less information, as the original topic dis-
tribution is now represented only by the topic with
the highest probability of occurring. Therefore, both
methods have information loss and are disadvan-
taged against the Soft Tagging approach. We use
the Soft Tagging approach for the Senseval-3 lexical
sample and the all-words tasks.
Unlabeled Corpus Selection The unlabeled cor-
pus we choose to train LDA include 20 News-
groups, Reuters, SemCor, Senseval-2 lexical sam-
ple data and Senseval-3 lexical sample data. Al-
though the last three are labeled corpora, we only
need the words from these corpora and thus they can
be regarded as unlabeled too. For Senseval-2 and
Senseval-3 data, we define the whole passage for
each training and testing instance as one document.
1019
The relative effect using different corpus and com-
binations of them is shown in Table 3, when validat-
ing on Senseval-2 test data using the Soft Tagging
approach.
Corpus |w| K L Senseval-2
20 Newsgroups 1.7M 40 60 67.9
Reuters 1.3M 30 60 65.5
SemCor 0.3M 30 60 66.9
Senseval-2 0.6M 30 40 66.9
Senseval-3 0.6M 50 60 67.6
All 4.5M 60 40 68.9
Table 3: Effect of using different corpus for LDA
training, |w| represents the corpus size in terms of
the number of words in the corpus
The 20 Newsgroups corpus yields the best result
if used individually. It has a relatively larger corpus
size at 1.7 million words in total and also a well bal-
anced topic distribution among its documents, rang-
ing across politics, finance, science, computing, etc.
The Reuters corpus, on the other hand, focuses heav-
ily on finance related articles and has a rather skewed
topic distribution. This probably contributed to its
inferior result. However, we found that the best re-
sult comes from combining all the corpora together
with K = 60 and L = 40.
Results for Optimized Configuration As base-
line for the Bayesian network approaches, we use
na??ve Bayes with all baseline features. For the base-
line SVM approach, we choose P = 3 and include
all the words occurring in the training and testing
passage as bag-of-words feature.
The F-measure result we achieve on Senseval-2
test data is shown in Table 4. Our four systems
are listed as the top four entries in the table. Soft
Tag refers to the soft tagging Bayesian network ap-
proach. Note that we used the Senseval-2 test data
for optimizing the configuration (as is done in the
ASO result). Hence, the result should not be taken
as reliable. Nevertheless, it is worth noting that the
improvement of Bayesian network approach over its
baseline is very significant (+5.5%). On the other
hand, SVM with topic features shows limited im-
provement over its baseline (+0.8%).
Bayes (Soft Tag) 68.9
SVM-Topic 66.0
SVM baseline 65.2
NB baseline 63.4
ASO(best configuration)(Ando, 2006) 68.1
Classifier Combination(Florian, 2002) 66.5
Polynomial KPCA(Wu et al, 2004) 65.8
SVM(Lee and Ng, 2002) 65.4
Senseval-2 Best System 64.2
Table 4: Results (best configuration) compared to
previous best systems on Senseval-2 English lexical
sample task.
5.1.2 All-words Task
In the all-words task, no official training data is
provided with Senseval. We follow the common
practice of using the SemCor corpus as our training
data. However, we did not use SVM approach in this
task as there are too few training instances per sense
for SVM to achieve a reasonably good accuracy.
As there are more training instances in SemCor,
230, 000 in total, we obtain the optimal configura-
tion using 10 fold cross validation on the SemCor
training data. With the optimal configuration, we
test our system on both Senseval-2 and Senseval-3
official test data.
For baseline features, we set P = 3 and B = 1. We
choose a LDA training corpus comprising 20 News-
groups and SemCor data, with number of topics K
= 40 and number of LDA bag-of-words L = 14.
6 Results
We now present the results on both English lexical
sample task and all-words task.
6.1 Lexical Sample Task
With the optimal configurations from Senseval-2,
we tested the systems on Senseval-3 data. Table 5
shows our F-measure result compared to some of the
best reported systems. Although SVM with topic
features shows limited success with only a 0.6%
improvement, the Bayesian network approach has
again demonstrated a good improvement of 3.8%
over its baseline and is better than previous reported
best systems except ASO(Ando, 2006).
1020
Bayes (Soft Tag) 73.6
SVM-topic 73.0
SVM baseline 72.4
NB baseline 69.8
ASO(Ando, 2006) 74.1
SVM-LSA (Strapparava et al, 2004) 73.3
Senseval-3 Best System(Grozea, 2004) 72.9
Table 5: Results compared to previous best systems
on Senseval-3 English lexical sample task.
6.2 All-words Task
The F-measure micro-averaged result for our sys-
tems as well as previous best systems for Senseval-2
and Senseval-3 all-words task are shown in Table 6
and Table 7 respectively. Bayesian network with soft
tagging achieved 2.6% improvement over its base-
line in Senseval-2 and 1.7% in Senseval-3. The re-
sults also rival some previous best systems, except
for SMUaw (Mihalcea, 2002) which used additional
labeled data.
Bayes (Soft Tag) 66.3
NB baseline 63.7
SMUaw (Mihalcea, 2002) 69.0
Simil-Prime (Kohomban and Lee, 2005) 66.4
Senseval-2 Best System 63.6
(CNTS-Antwerp (Hoste et al, 2001))
Table 6: Results compared to previous best systems
on Senseval-2 English all-words task.
Bayes (Soft Tag) 66.1
NB baseline 64.6
Simil-Prime (Kohomban and Lee, 2005) 66.1
Senseval-3 Best System 65.2
(GAMBL-AW-S(Decadt et al, 2004))
Senseval-3 2nd Best System (SenseLearner 64.6
(Mihalcea and Faruque, 2004))
Table 7: Results compared to previous best systems
on Senseval-3 English all-words task.
6.3 Significance of Results
We perform the ?2-test, using the Bayesian network
and its na??ve Bayes baseline (NB baseline) as pairs,
to verify the significance of these results. The result
is reported in Table 8. The results are significant at
90% confidence level, except for the Senseval-3 all-
words task.
Senseval-2 Senseval-3
All-word 0.0527 0.2925
Lexical Sample <0.0001 0.0002
Table 8: P value for ?2-test significance levels of
results.
6.4 SVM with Topic Features
The results on lexical sample task show that SVM
benefits less from the topic feature than the Bayesian
approach. One possible reason is that SVM base-
line is able to use all bag-of-words from surround-
ing context while na??ve Bayes baseline can only use
very few without decreasing its accuracy, due to the
sparse representation. In this sense, SVM baseline
already captures some of the topical information,
leaving a smaller room for improvement. In fact, if
we exclude the bag-of-words feature from the SVM
baseline and add in the topic features, we are able
to achieve almost the same accuracy as we did with
both features included, as shown in Table 9. This
further shows that the topic feature is a better rep-
resentation of global context than the bag-of-words
feature.
SVM baseline 72.4
SVM baseline - BAG + topic 73.5
SVM-topic 73.6
Table 9: Results on Senseval-3 English lexical sam-
ple task
6.5 Results on Different Parts-of-Speech
We analyse the result obtained on Senseval-3 En-
glish lexical sample task (using Senseval-2 optimal
configuration) according to the test instance?s part-
of-speech, which includes noun, verb and adjec-
tive, compared to the na??ve Bayes baseline. Ta-
ble 10 shows the relative improvement on each part-
of-speech. The second column shows the number
of testing instances belonging to the particular part-
of-speech. The third and fourth column shows the
1021
0.64
0.645
0.65
0.655
0.66
0.665
0.67
0.675
0.68
0 2 4 6 8 10 12 14 16 18
L
K=10
3
3
3
3 3
3 3
3
3
K=20
+
+
+
+ + +
+
+
+
K=40
2 2
2
2
2
2
2
2
2
K=60
? ?
? ?
? ?
?
?
?
K=80
4 4
4
4
4 4
4 4
4
Figure 4: Accuracy with varing L and K on
Senseval-2 all-words task
accuracy achieved by na??ve Bayes baseline and the
Bayesian network. Adjectives show no improve-
ment while verbs show a moderate +2.2% improve-
ment. Nouns clearly benefit from topical informa-
tion much more than the other two parts-of-speech,
obtaining a +5.7% increase over its baseline.
POS Total NB baseline Bayes (Soft Tag)
Noun 1807 69.5 75.2
Verb 1978 71.1 73.5
Adj 159 57.2 57.2
Total 3944 69.8 73.6
Table 10: Improvement with different POS on
Senseval-3 lexical sample task
6.6 Sensitivity to L and K
We tested on Senseval-2 all-words task using differ-
ent L and K. Figure 4 is the result.
6.7 Results on SemEval-1
We participated in SemEval-1 English coarse-
grained all-words task (task 7), English fine-grained
all-words task (task 17, subtask 3) and English
coarse-grained lexical sample task (task 17, subtask
1), using the method described in this paper. For
all-words task, we use Senseval-2 and Senseval-3
all-words task data as our validation set to fine tune
the parameters. For lexical sample task, we use the
training data provided as the validation set.
We achieved 88.7%, 81.6% and 57.6% for coarse-
grained lexical sample task, coarse-grained all-
words task and fine-grained all-words task respec-
tively. The results ranked first, second and fourth in
the three tasks respectively.
7 Conclusion and Future Work
In this paper, we showed that by using LDA algo-
rithm on bag-of-words feature, one can utilise more
topical information and boost the classifiers accu-
racy on both English lexical sample and all-words
task. Only unlabeled data is needed for this improve-
ment. It would be interesting to see how the feature
can help on WSD of other languages and other nat-
ural language processing tasks such as named-entity
recognition.
References
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP.
B. Snyder and M. Palmer. 2004. The English All-Words
Task. In Proc. of Senseval-3.
U. S. Kohomban and W. S. Lee 2005. Learning Semantic
Classes for Word Sense Disambiguation. In Proc. of
ACL.
R. K. Ando. 2006. Applying Alternating Structure Op-
timization to Word Sense Disambiguation. In Proc. of
CoNLL.
Y. S. Chan and H. T. Ng 2005. Scaling Up Word Sense
Disambiguation via Parallel Texts. In Proc. of AAAI.
R. K. Ando and T. Zhang. 2005a. A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning Re-
search.
R. K. Ando and T. Zhang. 2005b. A High-Performance
Semi-Supervised Learning Method for Text Chunking.
In Proc. of ACL.
P. Resnik and D. Yarowsky. 1997. A Perspective on
Word Sense Disambiguation Methods and Their Eval-
uation. In Proc. of ACL.
D. M. Blei and A. Y. Ng and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research.
1022
A. Ratnaparkhi 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP.
E. Charniak 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
V. N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
R. Florian and D. Yarowsky 2002. Modeling consensus:
Classifier Combination for Word Sense Disambigua-
tion. In Proc. of EMNLP.
D. Wu and W. Su and M. Carpuat. 2004. A Kernel PCA
Method for Superior Word Sense Disambiguation. In
Proc. of ACL.
C. Strapparava and A. Gliozzo and C. Giuliano 2004.
Pattern Abstraction and Term Similarity for Word
Sense Disambiguation: IRST at Senseval-3. In Proc.
of Senseval-3.
C. Grozea 2004. Finding Optimal Parameter Settings
for High Performance Word Sense Disambiguation. In
Proc. of Senseval-3.
R. Mihalcea 2002. Bootstrapping Large Sense Tagged
Corpora. In Proc. of the 3rd International Conference
on Languages Resources and Evaluations.
V. Hoste and A. Kool and W. Daelmans 2001. Classifier
Optimization and Combination in English All Words
Task. In Proc. of Senseval-2.
B. Decadt and V. Hoste and W. Daelmans 2004.
GAMBL, Genetic Algorithm Optimization of
Memory-Based WSD. In Proc. of Senseval-3.
R. Mihalcea and E. Faruque 2004. Sense-learner: Mini-
mally Supervised Word Sense Disambiguation for All
Words in Open Text. In Proc. of Senseval-3.
1023
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783?792,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Generative Model for Parsing Natural Language to Meaning
Representations
Wei Lu1, Hwee Tou Ng1,2, Wee Sun Lee1,2
1Singapore-MIT Alliance
2Department of Computer Science
National University of Singapore
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
Luke S. Zettlemoyer
CSAIL
Massachusetts Institute of Technology
lsz@csail.mit.edu
Abstract
In this paper, we present an algorithm for
learning a generative model of natural lan-
guage sentences together with their for-
mal meaning representations with hierarchi-
cal structures. The model is applied to the
task of mapping sentences to hierarchical rep-
resentations of their underlying meaning. We
introduce dynamic programming techniques
for efficient training and decoding. In exper-
iments, we demonstrate that the model, when
coupled with a discriminative reranking tech-
nique, achieves state-of-the-art performance
when tested on two publicly available cor-
pora. The generative model degrades robustly
when presented with instances that are differ-
ent from those seen in training. This allows
a notable improvement in recall compared to
previous models.
1 Introduction
To enable computers to understand natural human
language is one of the classic goals of research in
natural language processing. Recently, researchers
have developed techniques for learning to map sen-
tences to hierarchical representations of their under-
lying meaning (Wong and Mooney, 2006; Kate and
Mooney, 2006).
One common approach is to learn some form of
probabilistic grammar which includes a list of lexi-
cal items that models the meanings of input words
and also includes rules for combining lexical mean-
ings to analyze complete sentences. This approach
performs well but is constrained by the use of a sin-
gle, learned grammar that contains a fixed set of
lexical entries and productions. In practice, such
a grammar may lack the rules required to correctly
parse some of the new test examples.
In this paper, we develop an alternative approach
that learns a model which does not make use of
an explicit grammar but, instead, models the cor-
respondence between sentences and their meanings
with a generative process. This model is defined
over hybrid trees whose nodes include both natu-
ral language words and meaning representation to-
kens. Inspired by the work of Collins (2003), the
generative model builds trees by recursively creating
nodes at each level according to a Markov process.
This implicit grammar representation leads to flexi-
ble learned models that generalize well. In practice,
we observe that it can correctly parse a wider range
of test examples than previous approaches.
The generative model is learned from data that
consists of sentences paired with their meaning rep-
resentations. However, there is no explicit labeling
of the correspondence between words and meaning
tokens that is necessary for building the hybrid trees.
This creates a challenging, hidden-variable learning
problem that we address with the use of an inside-
outside algorithm. Specifically, we develop a dy-
namic programming parsing algorithm that leads to
O(n3m) time complexity for inference, where n is
the sentence length and m is the size of meaning
structure. This approach allows for efficient train-
ing and decoding.
In practice, we observe that the learned generative
models are able to assign a high score to the correct
meaning for input sentences, but that this correct
meaning is not always the highest scoring option.
783
To address this problem, we use a simple rerank-
ing approach to select a parse from a k-best list of
parses. This pipelined approach achieves state-of-
the-art performance on two publicly available cor-
pora. In particular, the flexible generative model
leads to notable improvements in recall, the total
percentage of sentences that are correctly parsed.
2 Related Work
In Section 9, we will compare performance with
the three existing systems that were evaluated on
the same data sets we consider. SILT (Kate et al,
2005) learns deterministic rules to transform either
sentences or their syntactic parse trees to meaning
structures. WASP (Wong and Mooney, 2006) is a
system motivated by statistical machine translation
techniques. It acquires a set of synchronous lexical
entries by running the IBM alignment model (Brown
et al, 1993) and learns a log-linear model to weight
parses. KRISP (Kate and Mooney, 2006) is a dis-
criminative approach where meaning representation
structures are constructed from the natural language
strings hierarchically. It is built on top of SVMstruct
with string kernels.
Additionally, there is substantial related research
that is not directly comparable to our approach.
Some of this work requires different levels of super-
vision, including labeled syntactic parse trees (Ge
and Mooney, 2005; Ge and Mooney, 2006). Others
do not perform lexical learning (Tang and Mooney,
2001). Finally, recent work has explored learning
to map sentences to lambda-calculus meaning rep-
resentations (Wong and Mooney, 2007; Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007).
3 Meaning Representation
We restrict our meaning representation (MR) for-
malism to a variable free version as presented in
(Wong and Mooney, 2006; Kate et al, 2005).
A training instance consists of a natural language
sentence (NL sentence) and its corresponding mean-
ing representation structure (MR structure). Con-
sider the following instance taken from the GEO-
QUERY corpus (Kate et al, 2005):
The NL sentence ?How many states do
not have rivers ?? consists of 8 words, in-
cluding punctuation. The MR is a hierarchical tree
QUERY : answer (NUM)
NUM : count (STATE)
STATE : exclude (STATE STATE)
STATE : state (all) STATE : loc 1 (RIVER)
RIVER : river (all)
Figure 1: An example MR structure
structure, as shown in Figure 1.
Following an inorder traversal of this MR tree, we
can equivalently represent it with the following list
of meaning representation productions (MR produc-
tions):
(0) QUERY : answer (NUM)
(1) NUM : count (STATE)
(2) STATE : exclude (STATE1 STATE2)
(3) STATE : state (all)
(4) STATE : loc 1 (RIVER)
(5) RIVER : river (all)
Each such MR production consists of three com-
ponents: a semantic category, a function symbol
which can be omitted (considered empty), and a list
of arguments. An argument can be either a child se-
mantic category or a constant. Take production (1)
for example: it has a semantic category ?NUM?, a
function symbol ?count?, and a child semantic cate-
gory ?STATE? as its only argument. Production (5)
has ?RIVER? as its semantic category, ?river? as the
function symbol, and ?all? is a constant.
4 The Generative Model
We describe in this section our proposed generative
model, which simultaneously generates a NL sen-
tence and an MR structure.
We denote a single NL word as w, a contiguous
sequence of NL words as w, and a complete NL
sentence as w?. In the MR structure, we denote a
semantic category as M. We denote a single MR
production as ma, or Ma : p?(Mb,Mc), where Ma
is the semantic category for this production, p? is the
function symbol, and Mb,Mc are the child semantic
categories. We denote ma as an MR structure rooted
by an MR production ma, and m?a an MR structure
for a complete sentence rooted by an MR production
ma.
The model generates a hybrid tree that represents
a sentence w? = w1 . . . w2 . . . paired with an MR
structure m?a rooted by ma.
784
Ma
ma
w1 Mb
mb
. . . . . .
w2 Mc
mc
. . . . . .
Figure 2: The generation process
Figure 2 shows part of a hybrid tree that is gen-
erated as follows. Given a semantic category Ma,
we first pick an MR production ma that has the form
Ma : p?(Mb,Mc), which gives us the function sym-
bol p? as well as the child semantic categories Mb
and Mc. Next, we generate the hybrid sequence of
child nodes w1 Mb w2 Mc, which consists of NL
words and semantic categories.
After that, two child MR productions mb and mc
are generated. These two productions will in turn
generate other hybrid sequences and productions, re-
cursively. This process produces a hybrid tree T ,
whose nodes are either NL words or MR produc-
tions. Given this tree, we can recover a NL sentence
w by recording the NL words visited in depth-first
traversal order and can recover an MR structure m
by following a tree-specific traversal order, defined
by the hybrid-patterns we introduce below. Figure 3
gives a partial hybrid tree for the training example
from Section 3. Note that the leaves of a hybrid tree
are always NL tokens.
. . .
STATE
STATE : exclude (STATE STATE)
STATE
STATE : state(all)
states
do not STATE
STATE : loc 1(RIVER)
have RIVER
RIVER : river(all)
rivers
Figure 3: A partial hybrid tree
With several independence assumptions, the
probability of generating
?
w?, m?,T ? is defined as:
P(w?, m?,T ) = P(Ma) ? P(ma|Ma) ? P(w1 Mb w2 Mc|ma)
?P(mb|ma, arg = 1) ? P(. . . |mb)
?P(mc|ma, arg = 2) ? P(. . . |mc) (1)
where ?arg? refers to the position of the child se-
mantic category in the argument list.
Motivated by Collins? syntactic parsing models
(Collins, 2003), we consider the generation process
for a hybrid sequence from an MR production as a
Markov process.
Given the assumption that each MR production
has at most two semantic categories in its arguments
(any production can be transformed into a sequence
of productions of this form), Table 1 includes the list
of all possible hybrid patterns.
# RHS Hybrid Pattern # Patterns
0 m ? w 1
1 m ? [w]Y[w] 4
2 m ? [w]Y[w]Z[w] 8m ? [w]Z[w]Y[w] 8
Table 1: A list of hybrid patterns, [] denotes optional
In this table, m is an MR production, Y and Z
are respectively the first and second child seman-
tic category in m?s argument list. The symbol w
refers to a contiguous sequence of NL words, and
anything inside [] can be optionally omitted. The
last row contains hybrid patterns that reflect reorder-
ing of one production?s child semantic categories
during the generation process. For example, con-
sider the case that the MR production STATE :
exclude (STATE1 STATE2) generates a hybrid se-
quence STATE1 do not STATE2, the hybrid pattern
m ? YwZ is associated with this generation step.
For the example hybrid tree in Figure 2, we can
decompose the probability for generating the hybrid
sequence as follows:
P(w1 Mb w2 Mc|ma) = P(m ? wYwZ|ma) ? P(w1|ma)
?P(Mb|ma, w1) ? P(w2|ma, w1,Mb)
?P(Mc|ma, w1,Mb, w2) ? P(END|ma, w1,Mb, w2,Mc) (2)
Note that unigram, bigram, or trigram assump-
tions can be made here for generating NL words and
semantic categories. For example, under a bigram
assumption, the second to last term can be written
as P(Mc|ma, w1,Mb, w2) ? P(Mc|ma, wk2), where
wk2 is the last word in w2. We call such additional
information that we condition on, the context.
Note that our generative model is different from
the synchronous context free grammars (SCFG) in
a number of ways. A standard SCFG produces a
correspondence between a pair of trees while our
model produces a single hybrid tree that represents
785
the correspondence between a sentence and a tree.
Also, SCFGs use a finite set of context-free rewrite
rules to define the model, where the rules are possi-
bly weighted. In contrast, we make use of the more
flexible Markov models at each level of the genera-
tive process, which allows us to potentially produce
a far wider range of possible trees.
5 Parameter Estimation
There are three categories of parameters used in the
model. The first category of parameters models
the generation of new MR productions from their
parent MR productions: e.g., P(mb|ma, arg = 1);
the second models the generation of a hybrid se-
quence from an MR production: e.g., P(w1|ma),
P(Mb|ma, w1); the last models the selection of a hy-
brid pattern given an MR production, e.g., P(m ?
wY|ma). We will estimate parameters from all cate-
gories, with the following constraints:
1.
?
m? ?(m?|m j, arg=k)=1 for all j and k = 1, 2.
These parameters model the MR structures, and
can be referred to as MR model parameters.
2.
?
t ?(t|m j,?)=1 for all j, where t is a NL word,
the ?END? symbol, or a semantic category. ?
is the context associated with m j and t.
These parameters model the emission of NL
words, the ?END? symbol, and child semantic
categories from an MR production. We call
them emission parameters.
3.
?
r ?(r|m j) = 1 for all j, where r is a hybrid
pattern listed in Table 1.
These parameters model the selection of hybrid
patterns. We name them pattern parameters.
With different context assumptions, we reach dif-
ferent variations of the model. In particular, we con-
sider three assumptions, as follows:
Model I We make the following assumption:
?(tk|m j,?) = P(tk|m j) (3)
where tk is a semantic category or a NL word, and
m j is an MR production.
In other words, generation of the next NL word
depends on its direct parent MR production only.
Such a Unigram Model may help in recall (the num-
ber of correct outputs over the total number of in-
puts), because it requires the least data to estimate.
Model II We make the following assumption:
?(tk|m j,?) = P(tk|m j, tk?1) (4)
where tk?1 is the semantic category or NL word to
the left of tk, i.e., the previous semantic category or
NL word.
In other words, generation of the next NL word
depends on its direct parent MR production as well
as the previously generated NL word or semantic
category only. This model is also referred to as Bi-
gram Model. This model may help in precision (the
number of correct outputs over the total number of
outputs), because it conditions on a larger context.
Model III We make the following assumption:
?(tk|m j,?) =
1
2 ?
(
P(tk|m j) + P(tk|m j, tk?1)
)
(5)
We can view this model, called the Mixgram
Model, as an interpolation between Model I and II.
This model gives us a balanced score for both preci-
sion and recall.
5.1 Modeling Meaning Representation
The MR model parameters can be estimated inde-
pendently from the other two. These parameters can
be viewed as the ?language model? parameters for
the MR structure, and can be estimated directly from
the corpus by simply reading off the counts of occur-
rences of MR productions in MR structures over the
training corpus. To resolve data sparseness problem,
a variant of the bigram Katz Back-Off Model (Katz,
1987) is employed here for smoothing.
5.2 Learning the Generative Parameters
Learning the remaining two categories of parameters
is more challenging. In a conventional PCFG pars-
ing task, during the training phase, the correct cor-
respondence between NL words and syntactic struc-
tures is fully accessible. In other words, there is a
single deterministic derivation associated with each
training instance. Therefore model parameters can
be directly estimated from the training corpus by
counting. However, in our task, the correct corre-
spondence between NL words and MR structures is
unknown. Many possible derivations could reach
the same NL-MR pair, where each such derivation
forms a hybrid tree.
786
The hybrid tree is constructed using hidden vari-
ables and estimated from the training set. An effi-
cient inside-outside style algorithm can be used for
model estimation, similar to that used in (Yamada
and Knight, 2001), as discussed next.
5.2.1 The Inside-Outside Algorithm with EM
In this section, we discuss how to estimate the
emission and pattern parameters with the Expecta-
tion Maximization (EM) algorithm (Dempster et al,
1977), by using an inside-outside (Baker, 1979) dy-
namic programming approach.
Denote ni ? ?mi, wi? as the i-th training instance,
where mi and wi are the MR structure and the NL
sentence of the i-th instance respectively. We also
denote nv ? ?mv, wv? as an aligned pair of MR
substructure and contiguous NL substring, where
the MR substructure rooted by MR production mv
will correspond to (i.e., hierarchically generate) the
NL substring wv. The symbol h is used to de-
note a hybrid sequence, and the function Parent(h)
gives the unique MR substructure-NL subsequence
pair which can be decomposed as h. Parent(nv) re-
turns the set of all possible hybrid sequences un-
der which the pair nv can be generated. Similarly,
Children(h) gives the NL-MR pairs that appear di-
rectly below the hybrid sequence h in a hybrid tree,
and Children(n) returns the set of all possible hybrid
sequences that n can be decomposed as. Figure 4
gives a packed tree structure representing the rela-
tions between the entities.
hp1 ? Parent(nv) . . . . . . hpm ? Parent(nv)
nv? ? ?mv? , wv? ? nv ? ?mv, wv?
hc1 ? Children(nv) . . . . . . hcn ? Children(nv)
Hybrid Sequence Contains
Can be Decomposed As
Figure 4: A packed tree structure representing the relations
between hybrid sequences and NL-MR pairs
The formulas for computing inside and outside
probabilities as well as the equations for updating
parameters are given in Figure 5. We use a CKY-
style parse chart for tracking the probabilities.
5.2.2 Smoothing
It is reasonable to believe that different MR pro-
ductions that share identical function symbols are
likely to generate NL words with similar distribu-
tion, regardless of semantic categories. For example,
The inside (?) probabilities are defined as
? If nv ? ?mv, wv? is leaf
?(nv) = P(wv|mv) (6)
? If nv ? ?mv, wv? is not leaf
?(nv) =
?
h?Children(nv)
(
P(h|mv) ?
?
nv??Children(h)
?(nv? )
)
(7)
The outside (?) probabilities are defined as
? If nv ? ?mv, wv? is root
?(nv) = 1 (8)
? If nv ? ?mv, wv? is not root
?(nv) =
?
h?Parent(nv)
(
?
(
Parent(h)
)
?P
(
h|Parent(h)
)
?
?
nv??Children(h),v?,v
?(nv? )
)
(9)
Parameter Update
? Update the emission parameter
The count ci(t, mv,?k), where t is a NL word
or a semantic category, for an instance pair ni ?
?mi, wi?:
ci(t, mv,?k) =
1
?(ni) ?
?
(t,mv ,?k) in h?Children(mv)
(
?(niv)
?P(h|mv) ?
?
niv??Children(h)
?(niv? )
)
The emission parameter is re-estimated as:
??(t|mv,?k) =
?
i ci(t, mv,?k)?
t?
?
i ci(t?, mv,?k)
(10)
? Update the pattern parameter
The count ci(r, mv), where r is a hybrid pattern,
for an instance pair ni ? ?mi, wi?:
ci(r, mv) =
1
?(ni) ?
?
(r,mv) in h?Children(mv)
(
?(niv)
?P(h|mv) ?
?
niv??Children(h)
?(niv? )
)
The pattern parameter is re-estimated as:
??(r|mv) =
?
i ci(r, mv)?
r?
?
i ci(r?, mv)
(11)
Figure 5: The inside/outside formulas as well as update
equations for EM
RIVER : largest (RIVER) and CITY : largest (CITY)
are both likely to generate the word ?biggest?.
In view of this, a smoothing technique is de-
ployed. We assume half of the time words can
787
be generated from the production?s function symbol
alone if it is not empty. Mathematically, assuming
ma with function symbol pa, for a NL word or se-
mantic category t, we have:
?(t|ma,?) =
{ ?e(t|ma,?) If pa is empty(
?e(t|ma,?) + ?e(t|pa,?)
)
/2 otherwise
where ?e models the generation of t from an MR
production or its function symbol, together with the
context ?.
6 A Dynamic Programming Algorithm for
Inside-Outside Computation
Though the inside-outside approach already em-
ploys packed representations for dynamic program-
ming, a naive implementation of the inference algo-
rithm will still require O(n6m) time for 1 EM iter-
ation, where n and m are the length of the NL sen-
tence and the size of the MR structure respectively.
This is not very practical as in one of the corpora we
look at, n and m can be up to 45 and 20 respectively.
In this section, we develop an efficient dynamic
programming algorithm that enables the inference
to run in O(n3m) time. The idea is as follows. In-
stead of treating each possible hybrid sequence as
a separate rule, we efficiently aggregate the already
computed probability scores for hybrid sequences
that share identical hybrid patterns. Such aggregated
scores can then be used for subsequent computa-
tions. By doing this, we can effectively avoid a large
amount of redundant computations. The algorithm
supports both unigram and bigram context assump-
tions. For clarity and ease of presentation, we pri-
marily make the unigram assumption throughout our
discussion.
We use ? (mv, wv) to denote the inside probabil-
ity for mv-wv pair, br[mv, wv, c] to denote the aggre-
gated probabilities for the MR sub-structure mv to
generate all possible hybrid sequences based on wv
with pattern r that covers its c-th child only. In addi-
tion, we use w(i, j) to denote a subsequence of w with
start index i (inclusive) and end index j (exclusive).
We also use ?r~mv, wv to denote the aggregated in-
side probability for the pair ?mv, wv?, if the hybrid
pattern is restricted to r only. By definition we have:
? (mv, wv) =
?
r
?(r|mv)??r~mv, wv??(END|mv) (12)
Relations between ?r and br can also be estab-
lished. For example, if mv has one child semantic
category, we have:
?m?wY~mv, wv = bm?wY[mv, wv, 1] (13)
For the case when mv has two child semantic cat-
egories as arguments, we have, for example:
?m?wYZw~mv, w(i, j) =
?
i+2?k? j?2
bm?wY[mv, w(i,k), 1]
?bm?Yw[mv, w(k, j), 2] (14)
Note that there also exist relations amongst b
terms for more efficient computation, for example:
bm?wY[mv, w(i, j), c] = ?(wi|mv)
?
(
bm?wY[mv, w(i+1, j), c] + bm?Y[mv, w(i+1, j), c]
)
(15)
Analogous but more complex formulas are used
for computing the outside probabilities. Updating of
parameters can be incorporated into the computation
of outside probabilities efficiently.
7 Decoding
In the decoding phase, we want to find the optimal
MR structure m?? given a new NL sentence w?:
m?
? = arg max
m?
P(m?|w?) = arg max
m?
?
T
P(m?,T |w?) (16)
where T is a possible hybrid tree associated with
the m?-w? pair. However, it is expensive to compute
the summation over all possible hybrid trees. We
therefore find the most likely hybrid tree instead:
m?
?=arg max
m?
max
T
P(m?,T |w?)=arg max
m?
max
T
P(w?, m?,T ) (17)
We have implemented an exact top-k decoding al-
gorithm for this task. Dynamic programming tech-
niques similar to those discussed in Section 6 can
also be applied when retrieving the top candidates.
We also find the Viterbi hybrid tree given a NL-
MR pair, which can be done in an analogous way.
This tree will be useful for reranking.
8 Reranking and Filtering of Predictions
Due to the various independence assumptions we
have made, the model lacks the ability to express
some long range dependencies. We therefore post-
process the best candidate predictions with a dis-
criminative reranking algorithm.
788
Feature Type Description Example
1. Hybrid Rule A MR production and its child hybrid form f1 : STATE : loc 1(RIVER) ? have RIVER
2. Expanded Hybrid Rule A MR production and its child hybrid form expanded f2 : STATE : loc 1(RIVER) ? ?have, RIVER : river(all)?
3. Long-range Unigram A MR production and a NL word appearing below in tree f3 : STATE : exclude(STATE STATE) ? rivers
4. Grandchild Unigram A MR production and its grandchild NL word f4 : STATE : loc 1(RIVER) ? rivers
5. Two Level Unigram A MR production, its parent production, and its child NL word f5 : ?RIVER : river(all), STATE : loc 1(RIVER)? ? rivers
6. Model Log-Probability Logarithm of base model?s joint probability log (P?(w, m,T )).
Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if
the combination is present, and 0 otherwise. Feature 6 takes real values.
8.1 The Averaged Perceptron Algorithm with
Separating Plane
The averaged perceptron algorithm (Collins, 2002)
has previously been applied to various NLP tasks
(Collins, 2002; Collins, 2001) for discriminative
reranking. The detailed algorithm can be found in
(Collins, 2002). In this section, we extend the con-
ventional averaged perceptron by introducing an ex-
plicit separating plane on the feature space.
Our reranking approach requires three compo-
nents during training: a GEN function that defines
for each NL sentence a set of candidate hybrid trees;
a single correct reference hybrid tree for each train-
ing instance; and a feature function ? that defines a
mapping from a hybrid tree to a feature vector. The
algorithm learns a weight vector w that associates a
weight to each feature, such that a score w??(T ) can
be assigned to each candidate hybrid tree T . Given
a new instance, the hybrid tree with the highest score
is then picked by the algorithm as the output.
In this task, the GEN function is defined as the
output hybrid trees of the top-k (k is set to 50 in our
experiments) decoding algorithm, given the learned
model parameters. The correct reference hybrid tree
is determined by running the Viterbi algorithm on
each training NL-MR pair. The feature function is
discussed in section 8.2.
While conventional perceptron algorithms usually
optimize the accuracy measure, we extend it to allow
optimization of the F-measure by introducing an ex-
plicit separating plane on the feature space that re-
jects certain predictions even when they score high-
est. The idea is to find a threshold b after w is
learned, such that a prediction with score below b
gets rejected. We pick the threshold that leads to the
optimal F-measure when applied to the training set.
8.2 Features
We list in Table 2 the set of features we used. Ex-
amples are given based on the hybrid tree in Figure
3. Some of the them are adapted from (Collins and
Koo, 2005) for a natural language parsing task. Fea-
tures 1-5 are indicator functions (i.e., it takes value
1 if a certain combination as the ones listed in Table
2 is present, 0 otherwise), while feature 6 is real val-
ued. Features that do not appear more than once in
the training set are discarded.
9 Evaluation
Our evaluations were performed on two corpora,
GEOQUERY and ROBOCUP. The GEOQUERY cor-
pus contains MR defined by a Prolog-based lan-
guage used in querying a database on U.S. geogra-
phy. The ROBOCUP corpus contains MR defined by
a coaching language used in a robot coaching com-
petition. There are in total 880 and 300 instances for
the two corpora respectively. Standard 10-fold cross
validations were performed and the micro-averaged
results are presented in this section. To make our
system directly comparable to previous systems, all
our experiments were based on identical training and
test data splits of both corpora as reported in the ex-
periments of Wong and Mooney (2006).
9.1 Training Methodology
Given a training set, we first run a variant of IBM
alignment model 1 (Brown et al, 1993) for 100 iter-
ations, and then initialize Model I with the learned
parameter values. This IBM model is a word-to-
word alignment model that does not model word
order, so we do not have to linearize the hierarchi-
cal MR structure. Given this initialization, we train
Model I for 100 EM iterations and use the learned
parameters to initialize Model II which is trained for
another 100 EM iterations. Model III is simply an
interpolation of the above two models. As for the
reranking phase, we initialize the weight vector with
the zero vector 0, and run the averaged perceptron
algorithm for 10 iterations.
789
9.2 Evaluation Methodology
Following Wong (2007) and other previous work,
we report performance in terms of Precision (per-
centage of answered NL sentences that are correct),
Recall (percentage of correctly answered NL sen-
tences, out of all NL sentences) and F-score (har-
monic mean of Precision and Recall).
Again following Wong (2007), we define the cor-
rect output MR structure as follows. For the GEO-
QUERY corpus, an MR structure is considered cor-
rect if and only if it retrieves identical results as
the reference MR structure when both are issued as
queries to the underlying Prolog database. For the
ROBOCUP corpus, an MR structure is considered
correct if and only if it has the same string represen-
tation as the reference MR structure, up to reorder-
ing of children of MR productions whose function
symbols are commutative, such as and, or, etc.
9.3 Comparison over Three Models
Model GEOQUERY (880) ROBOCUP (300)Prec. Rec. F Prec. Rec. F
I 81.3 77.1 79.1 71.1 64.0 67.4
II 89.0 76.0 82.0 82.4 57.7 67.8
III 86.2 81.8 84.0 70.4 63.3 66.7
I+R 87.5 80.5 83.8 79.1 67.0 72.6
II+R 93.2 73.6 82.3 88.4 56.0 68.6
III+R 89.3 81.5 85.2 82.5 67.7 74.4
Table 3: Performance comparison over three models
(Prec.:precision, Rec.:recall, +R: with reranking)
We evaluated the three models, with and with-
out reranking. The results are presented in Table 3.
Comparing Model I and Model II, we noticed that
for both corpora, Model I in general achieves bet-
ter recall while Model II achieves better precision.
This observation conforms to our earlier expecta-
tions. Model III, as an interpolation of the above two
models, achieves a much better F-measure on GEO-
QUERY corpus. However, it is shown to be less ef-
fective on ROBOCUP corpus. We noticed that com-
pared to the GEOQUERY corpus, ROBOCUP corpus
contains longer sentences, larger MR structures, and
a significant amount of non-compositionality. These
factors combine to present a challenging problem for
parsing with the generative model. Interestingly, al-
though Model III fails to produce better best pre-
dictions for this corpus, we found that its top-k list
contains a relatively larger number of correct pre-
dictions than Model I or Model II. This indicates
the possibility of enhancing the performance with
reranking.
The reranking approach is shown to be quite ef-
fective. We observe a consistent improvement in
both precision and F-measure after employing the
reranking phase for each model.
9.4 Comparison with Other Models
Among all the previous models, SILT, WASP, and
KRISP are directly comparable to our model. They
required the same amount of supervision as our sys-
tem and were evaluated on the same corpora.
We compare our model with these models in Ta-
ble 4, where the performance scores for the previous
systems are taken from (Wong, 2007). For GEO-
QUERY corpus, our model performs substantially
better than all the three previous models, with a no-
table improvement in the recall score. In fact, if we
look at the recall scores alone, our best-performing
model achieves a 6.7% and 9.8% absolute improve-
ment over two other state-of-the-art models WASP
and KRISP respectively. This indicates that over-
all, our model is able to handle over 25% of the
inputs that could not be handled by previous sys-
tems. On the other hand, in terms of F-measure,
we gain a 4.1% absolute improvement over KRISP,
which leads to an error reduction rate of 22%. On
the ROBOCUP corpus, our model?s performance is
also ranked the highest1.
System GEOQUERY (880) ROBOCUP (300)Prec. Rec. F Prec. Rec. F
SILT 89.0 54.1 67.3 83.9 50.7 63.2
WASP 87.2 74.8 80.5 88.9 61.9 73.0
KRISP 93.3 71.7 81.1 85.2 61.9 71.7
Model III+R 89.3 81.5 85.2 82.5 67.7 74.4
Table 4: Performance comparison with other directly com-
parable systems
9.5 Performance on Other Languages
As a generic model that requires minimal assump-
tions on the natural language, our model is natural
language independent and is able to handle various
other natural languages than English. To validate
this point, we evaluated our system on a subset of
1We are unable to perform statistical significance tests be-
cause the detailed performance for each fold of previously pub-
lished research work is not available.
790
the GEOQUERY corpus consisting of 250 instances,
with four different NL annotations.
As we can see from Table 5, our model is able
to achieve performance comparable to WASP as re-
ported by Wong (2007).
System English SpanishPrec. Rec. F Prec. Rec. F
WASP 95.42 70.00 80.76 91.99 72.40 81.03
Model III+R 91.46 72.80 81.07 95.19 79.20 86.46
System Japanese TurkishPrec. Rec. F Prec. Rec. F
WASP 91.98 74.40 82.86 96.96 62.40 75.93
Model III+R 87.56 76.00 81.37 93.82 66.80 78.04
Table 5: Performance on different natural languages for
GEOQUERY-250 corpus
Our model is generic, which requires no domain-
dependent knowledge and should be applicable to
a wide range of different domains. Like all re-
search in this area, the ultimate goal is to scale to
more complex, open-domain language understand-
ing problems. In future, we would like to create a
larger corpus in another domain with multiple natu-
ral language annotations to further evaluate the scal-
ability and portability of our approach.
10 Conclusions
We presented a new generative model that simulta-
neously produces both NL sentences and their cor-
responding MR structures. The model can be effec-
tively applied to the task of transforming NL sen-
tences to their MR structures. We also developed
a new dynamic programming algorithm for efficient
training and decoding. We demonstrated that this
approach, augmented with a discriminative rerank-
ing technique, achieves state-of-the-art performance
when tested on standard benchmark corpora.
In future, we would like to extend the current
model to have a wider range of support of MR for-
malisms, such as the one with lambda-calculus sup-
port. We are also interested in investigating ways to
apply the generative model to the inverse task: gen-
eration of a NL sentence that explains a given MR
structure.
Acknowledgments
The authors would like to thank Leslie Pack Kael-
bling for her valuable feedback and comments on
this research. The authors would also like to thank
the anonymous reviewers for their thoughtful com-
ments on this paper. The research is partially sup-
ported by ARF grant R-252-000-240-112.
References
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65:S132.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
M. Collins. 2001. Ranking algorithms for named-entity
extraction: boosting and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), pages
489?496.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 1?8.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL 2005), pages 9?
16.
R. Ge and R. J. Mooney. 2006. Discriminative rerank-
ing for semantic parsing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL 2006),
pages 263?270.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING/ACL
2006), pages 913?920.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the Twentieth National Conference on Ar-
tificial Intelligence (AAAI 2005), pages 1062?1068.
791
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 35(3):400?401.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proceedings of the 12th Euro-
pean Conference on Machine Learning (ECML 2001),
pages 466?477.
Y. W. Wong and R. J. Mooney. 2006. Learning for
semantic parsing with statistical machine translation.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT-NAACL
2006), pages 439?446.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
2007), pages 960?967.
Y. W. Wong. 2007. Learning for Semantic Parsing and
Natural Language Generation Using Statistical Ma-
chine Translation Techniques. Ph.D. thesis, The Uni-
versity of Texas at Austin.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of the 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the 21st Conference on Uncertainty in Ar-
tificial Intelligence.
L. S. Zettlemoyer and M. Collins. 2007. Online learning
of relaxed CCG grammars for parsing to logical form.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 678?687.
792
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1523?1532,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Domain adaptive bootstrapping for named entity recognition
Dan Wu
1
, Wee Sun Lee
2
, Nan Ye
2
1
Singapore MIT Alliance
2
Department of Computer Science
National University of Singapore
{dwu@,leews@comp,g0701171@}nus.edu.sg
Hai Leong Chieu
DSO National Laboratories
chaileon@dso.org.sg
Abstract
Bootstrapping is the process of improving
the performance of a trained classifier by
iteratively adding data that is labeled by
the classifier itself to the training set, and
retraining the classifier. It is often used
in situations where labeled training data is
scarce but unlabeled data is abundant. In
this paper, we consider the problem of do-
main adaptation: the situation where train-
ing data may not be scarce, but belongs to
a different domain from the target appli-
cation domain. As the distribution of un-
labeled data is different from the training
data, standard bootstrapping often has dif-
ficulty selecting informative data to add to
the training set. We propose an effective
domain adaptive bootstrapping algorithm
that selects unlabeled target domain data
that are informative about the target do-
main and easy to automatically label cor-
rectly. We call these instances bridges, as
they are used to bridge the source domain
to the target domain. We show that the
method outperforms supervised, transduc-
tive and bootstrapping algorithms on the
named entity recognition task.
1 Introduction
Most recent researches on natural language pro-
cessing (NLP) problems are based on machine
learning algorithms. High performance can often
be achieved if the system is trained and tested on
data from the same domain. However, the perfor-
mance of NLP systems often degrades badly when
the test data is drawn from a source that is differ-
ent from the labeled data used to train the system.
For named entity recognition (NER), for example,
Ciaramita and Altun (2005) reported that a system
trained on a labeled Reuters corpus achieved an
F-measure of 91% on a Reuters test set, but only
64% on a Wall Street Journal test set.
The task of adapting a system trained on one do-
main (called the source domain) to a new domain
(called the target domain) is called domain adap-
tation. In domain adaptation, it is generally as-
sumed that we have labeled data in the source do-
main while labeled data may or may not be avail-
able in the target domain. Previous work in do-
main adaptation can be classified into two cate-
gories: [S+T+], where a small, labeled target do-
main data is available, e.g. (Blitzer et al, 2006;
Jiang and Zhai, 2007; Daum?e III, 2007; Finkel and
Manning, 2009), or [S+T-], where no labeled tar-
get domain data is available, e.g. (Blitzer et al,
2006; Jiang and Zhai, 2007). In both cases, and es-
pecially for [S+T-], domain adaptation can lever-
age on large amounts of unlabeled data in the tar-
get domain. In practice, it is often unreasonable
to expect labeled data for every new domain that
we come across, such as blogs, emails, a different
newspaper agency, or simply articles from a differ-
ent topic or period in time. Thus although [S+T+]
is easier to handle, [S+T-] is of higher practical
importance.
In this paper, we propose a domain adaptive
bootstrapping (DAB) approach to tackle the do-
main adaptation problem under the setting [S+T-].
Bootstrapping is an iterative process that uses a
trained classifier to label and select unlabeled in-
stances to add to the training set for retraining
the classifier. It is often used when labeled train-
ing data is scarce but unlabeled data is abundant.
In contrast, for domain adaptation problems, we
may have a lot of training data but the target ap-
plication domain has a different data distribution.
Standard bootstrapping usually selects instances
that are most confidently labeled from the unla-
beled data. In domain adaptation situations, usu-
ally the most confidently labeled instances are the
ones that are most similar to the source domain in-
1523
stances - these instances tend to contain very little
information about the target domain. For domain
adaptive bootstrapping, we propose a selection cri-
terion that selects instances that are informative
and easy to automatically label correctly. In addi-
tion, we propose a criterion for stopping the pro-
cess of bootstrapping before it adds uninformative
and incorrectly labeled instances that can reduce
performance.
Our approach leverages on instances in the tar-
get domain called bridges. These instances con-
tain domain-independent features, as well as fea-
tures specific to the target domain. As they contain
domain-independent features, they can be classi-
fied correctly by classifiers trained on the source
domain labeled data. We argue that these instances
act as a bridge between the source and the target
domain. We show that, on the NER task, DAB
outperforms supervised, transductive and standard
bootstrapping algorithms, as well as a bootstrap-
ping variant, called balanced bootstrapping (Jiang
and Zhai, 2007), that has recently been proposed
for domain adaptation.
2 Related work
One general class of approaches to domain adap-
tation is to consider that the instances from the
source and the target domain are drawn from dif-
ferent distributions. Bickel et al (Bickel et al,
2007) discriminatively learns a scaling factor for
source domain training data, so as to adapt the
source domain data distribution to resemble the
target domain data distribution, under the [S+T-]
setting. Daume III and Marcu (Daum?e III and
Marcu, 2006) considers that the data distribution is
a mixture distribution over general, source domain
and target domain data. They learn the underlying
mixture distribution using the conditional expec-
tation maximization algorithm, under the [S+T+]
setting. Jiang and Zhai (2007) proposed an in-
stance re-weighting framework that handles both
the [S+T+] and [S+T-] settings. For [S+T-], the
resulting algorithm is a balanced bootstrapping al-
gorithm, which was shown to outperform the stan-
dard bootstrapping algorithm. In this paper, we
assume the [S+T-] settings, and we show that the
approach proposed in this paper, domain adaptive
bootstrapping (DAB), outperforms the balanced
bootstrapping algorithm on NER.
Another class of approaches to domain adap-
tation is feature-based. Daume III (Daum?e III,
2007) divided features into three classes: domain-
independent features, source-domain features and
target-domain features. He assumed the existence
of training data in the target-domain (under the
setting [S+T+]), so that the three classes of fea-
tures can be jointly trained using source and target
domain labeled data. This cannot be done in the
setting [S+T-], where no training data is available
in the target domain. Using a different approach,
Blitzer et al (2006) induces correspondences be-
tween feature spaces in different domains, by de-
tecting pivot features. Pivot features are features
that occur frequently and behave similarly in dif-
ferent domains. Pivot features are used to put
domain-specific features in correspondence. In
this paper, instead of pivot features, we attempt
to leverage on pivot instances that we call bridges,
which are instances that bridge the source and tar-
get domain. This will be illustrated in Section 3.
It is generally recognized that adding informa-
tive and correctly labeled instances is more useful
for learning. Active learning queries the user for
labels of most informative or relevant instances.
Active learning, which has been applied to the
problem of NER in (Shen et al, 2004), is used in
situations where a large amount of unlabeled data
exists and data labeling is expensive. It has also
been applied to the problem of domain adaptation
for word sense disambiguation in (Chan and Ng,
2007). However, active learning requires human
intervention. Here, we want to achieve the same
goal without human intervention.
3 Bootstrapping for domain adaptation
We first define the notations used for domain adap-
tation in the [S+T-] setting. A set of training data
D
S
= {x
i
, y
i
}
1?i?|D
S
|
is given in the source do-
main, where the notation |X| denotes the size of a
set X . Each instance x
i
in D
S
has been manually
annotated with a label, y
i
, from a given set of la-
bels Y . The objective of domain adaptation is to
label a set of unlabeled data, D
T
= {x
i
}
1?i?|D
T
|
with labels from Y . A machine learning algorithm
will take a labeled data set (for e.g. D
S
) and out-
puts a classifier, which can then be used to classify
unlabeled data, i.e. assign labels to unlabeled in-
stances.
A special class of machine learning algorithms,
called transductive learning algorithms, is able to
take the unlabeled data D
T
into account during
the learning process (see e.g. (Joachims, 1999)).
1524
However, such algorithms do not take into account
the shift in domain of the test data. Jiang and Zhai
(2007) recently proposed an instance re-weighting
framework to take domain shift into account. For
[S+T-], the resulting algorithm is a balanced boot-
strapping algorithm, which we describe below.
3.1 Standard and balanced bootstrapping
We define a general bootstrapping algorithm in Al-
gorithm 1. The algorithm can be applied to any
machine learning algorithm that allows training in-
stances to be weighted, and that gives confidence
scores for the labels when used to classify test
data. The bootstrapping procedure iteratively im-
proves the performance of a classifier SC
t
over a
number of iterations. In Algorithm 1, we have left
a number of parameters unspecified. These param-
eters are (1) the selection-criterion for instances to
be added to the training data, (2) the termination-
criterion for the bootstrapping process, and (3) the
weights (w
S
, w
T
) given to the labeled and boot-
strapped training sets.
Standard bootstrapping: (Jiang and Zhai,
2007) the selection-criterion is based on selecting
the top k most-confidently labeled instances in R
t
.
The weight w
S
t
is equal to w
T
t
. The value of k is a
parameter for the bootstrapping algorithm.
Balanced bootstrapping: (Jiang and Zhai,
2007) the selection-criterion is still based on se-
lecting the top k most-confidently labeled in-
stances in R
t
. Balanced bootstrapping was for-
mulated for domain adaptation, and hence they set
the weights to satisfy the ratio
w
S
t
w
T
t
=
|T
t
|
|D
S
|
. This
allows the small amount of target data added, T
t
,
to have an equal weight to the large source domain
training set D
S
.
In this paper, we formulate a selection-criterion
and a termination-criterion which are better than
those used in standard and balanced bootstrap-
ping. Regarding the selection-criterion, standard
and balanced bootstrapping both select instances
which are confidently labeled by SC
t
to be used
for training SC
t+1
, in the hope of avoiding us-
ing wrongly labeled data in bootstrapping. How-
ever, instances that are already confidently labeled
by SC
t
may not contain sufficient information
which is not in D
S
, and using them to train SC
t+1
may result in SC
t+1
performing similarly to SC
t
.
This motivates us to select samples which are both
informative and easy to automatically label cor-
rectly. Regarding the termination-criterion, which
Algorithm 1 Bootstrapping algorithm
Input: labeled data D
S
, test data D
T
and a ma-
chine learning algorithm.
Output: the predicted labels of the set D
T
.
Set T
0
= ?, R
0
= D
T
, and t = 0
Repeat
1. learn a classifier SC
t
with (D
S
, T
t
) with
weights (w
S
t
, w
T
t
)
2. label the set R
t
with SC
t
3. select S
t
? R
t
based on selection-criterion
4. T
t+1
= T
t
? S
t
, and R
t+1
= R
t
\ S
t
.
Until termination-criterion
Output the predicted labels of D
T
by SC
t
.
is not mentioned in the paper (Jiang and Zhai,
2007), we assume that bootstrapping is simply run
for either a single iteration, or a small and fixed
number of iterations. However, it is known that
such simple criterion may result in stopping too
early or too late, leading to sub-optimal perfor-
mance. We propose a more effective termination-
criterion here.
3.2 Domain adaptive bootstrapping (DAB)
Our selection-criterion relies on the observation
that in domain adaptation, instances (from the
source or the target domain) can be divided into
three types according to their information content:
generalists are instances that contain only domain-
independent information and are present in all do-
mains; specialists are instances containing only
domain-specific information and are present only
in their respective domains; bridges are instances
containing both domain-independent and domain-
specific information, also present only in their re-
spective domains but are useful as a ?bridge? be-
tween the source and the target domains.
The implication of the above observation is
that when choosing unlabeled target domain data
for bootstrapping, we should exploit the bridges,
because the generalists are not likely to contain
much information not in D
S
due to their domain-
independence, and the specialists are difficult to be
labeled correctly due to their domain-specificity.
In contrast, the bridges are informative and eas-
ier to label correctly. Choosing confidently clas-
sified instances for bootstrapping, as in standard
bootstrapping and balanced bootstrapping, is sim-
ple, but results in choosing mostly generalists, and
is too conservative. We design a scoring function
1525
on instances, which has high value when the in-
stance is informative and sufficiently likely to be
correctly labeled in order to identify correctly la-
beled bridges.
Intuitively, informativeness of an instance can
be measured by the prediction results of the ideal
classifier IS for the source domain and the ideal
classifier IT for the target domain. If IS and IT
are both probabilistic classifiers, IS should return
a noninformative distribution while IT should re-
turn an informative one. The ideal classifier for the
source domain is approximated with a source clas-
sifier SC trained on D
S
, while the ideal classifier
for the target domain is approximated by training a
classifier, TC, on target domain instances labeled
by the source classifier.
We also try to ensure that instances that are se-
lected are correctly classified. As the label used
is provided by the target classifier, we estimate
the precision of the target classification. The final
ranking function is constructed by combining this
estimate with the informativeness of the instance.
We show the algorithm for the instance selec-
tion in Algorithm 2. The notations used follow
those used in Algorithm 1. For simplicity, we as-
sume that w
S
t
= w
T
t
= 1 for all t. We expect
TC to be a reasonable classifier on D
T
due to the
presence of generalists and bridges. Note that the
target classifier is constructed by randomly split-
ting D
T
into two partitions, training a classifier
on each partition and using the prediction of the
trained classifier on the partition it is not trained
on. This is because classifiers tend to fit the data
that they have been trained on too well making the
probability estimates on their training data unreli-
able. Also, a random partition is used to ensure
that the data in each partition is representative of
D
u
.
3.3 The scoring function: score(p
(s)
, p
(t)
)
The scoring function score(p
(s)
, p
(t)
) in Algo-
rithm 2 is simply implemented as the product of
two components: a measure of the informative-
ness and the probability that SC?s label is correct.
We show how the intuitive ideas (described above)
behind these two components are formalized.
Informativeness of a distribution p on a set of
discrete labels Y is measured by its entropy h(p)
defined by
h(p) = ?
?
y?Y
p(y) log p(y).
Algorithm 2 Algorithm for selecting instances for
bootstrapping at iteration t
Input: Labeled source domain data D
S
, target do-
main training data T
t
, remaining data R
t
, the clas-
sifier SC
t
trained on D
S
? T
t
, and a scoring func-
tion score(p
(s)
, p
(t)
)
Output: k instances for bootstrapping.
1. Label R
t
with SC
t
, and to each instance x
i
?
R
t
, SC
t
outputs a distribution p
(s)
i
(y
i
) over
its labels.
2. Randomly split R
t
into two partitions, R
0
t
and R
1
t
with their labels assigned by SC
t
.
3. Train each target classifier, TC
x
t
with the data
R
x
t
, for x = {0, 1}.
4. Label R
(1?x)
t
with the classifier TC
x
t
, which
to each instance x
i
? R
t
, outputs a distribu-
tion p
(t)
i
(y
i
) over its labels.
5. Score each instance from x
i
? R
t
with the
function score(p
(s)
i
, p
(t)
i
).
6. Select top k instances from R
t
with the high-
est scores.
h(p) is nonnegative; h(p) = 0 if and only if p
has probability 1 on one of the labels; h(p) attains
its maximum value when the distribution p is uni-
form over all labels. Hence, an instance is clas-
sified with high confidence when the distribution
over its labels has low entropy.
We measure the informativeness of an instance
using h(p
(s)
)? h(p
(t)
), where p
(s)
and p
(t)
are as
in Algorithm 2. We argue that a larger value of this
expression implies that the instance is more likely
to be a bridge instance. This expression has a high
value when the source classifier is uncertain, and
the target classifier is certain. Uncertain classifi-
cation by the source classifier indicates that the in-
stance is unlikely to be a generalist. Moreover, if
the target classifier is certain on x
i
, it means that
instances similar to the instance x
i
are consistently
labeled with the same label by the source classifier
SC
t
, indicating that it is likely to be a bridge in-
stance.
The probability that TC?s label is correct can-
not be estimated directly because we do not have
labeled target domain data. Instead, we use the
source domain to give an estimate. We do this with
a simple pre-processing step: we split the data D
S
into two partitions of equal size, train a classifier
on each partition, and test each classifier on the
1526
other partition. We then measure the resulting ac-
curacy given each label:
?(y) =
# correctly labeled instances of label y
# total instances of label y
.
Summarizing the above discussion, the scoring
function is as shown below.
score(p
(s)
, p
(t)
) = ?(y
?
)
[
h(p
(s)
)?h(p
(t)
)
]
,
where y
?
= argmax
y?Y
p
(s)
(y)
The scoring function has a high value when the
information content of the example is high and the
label has high precision.
3.4 The termination criterion
Intuitively, our algorithm terminates when there
are not enough informative instances. Formally,
we define the termination criterion as follows: we
terminate the bootstrapping process when, there
exists an instance x
i
in the top k instances satis-
fying the following condition:
1. h(p
(s)
i
) < h(p
(t)
i
), or
2. max
y?Y
p
(s)
i
(y) > max
y?Y
p
(t)
i
(y)
The second case is used to check for instances
where the classifier SC
t
is more confident than
the target classifiers TC
x
t
, on their respective pre-
dicted labels. This shows that the instance x
i
is
more of a generalist than a bridge.
4 NER task and implementation
The algorithm described in Section 3 is not spe-
cific to any particular application. In this paper,
we apply it to the problem of named entity recog-
nition (NER). In this section, we describe the NER
classifier and the features used in our experiments.
4.1 NER features
We used the features generated by the CRF pack-
age (Finkel et al, 2005). These features include
the word string feature, the case feature for the cur-
rent word, the context words for the current word
and their cases, the presence in dictionaries for the
current word, the position of the current word in
the sentence, prefix and suffix of the current word
as well as the case information of the multiple oc-
currences of the current word. We use the same
set of features for all classifiers used in the boot-
strapping process, and for all baselines used in the
experimental section.
4.2 Machine learning algorithms
A base machine learning algorithm is required in
bootstrapping approaches. We describe the two
machine learning algorithms used in this paper.
We chose these algorithms for their good perfor-
mance on the NER task.
Maximum entropy classification (MaxEnt):
The MaxEnt approach, or logistic regression, is
one of the most competitive methods for named
entity recognition (Tjong and Meulder, 2003).
MaxEnt is a discriminative method that learns a
distribution, p(y
i
|x
i
), over the labels, y
i
, given
the vector of features, x
i
. We used the imple-
mentation of MaxEnt classifier described in (Man-
ning and Klein, 2003). For NER, each instance
represents a single word token within a sentence,
with the feature vector x
i
derived from the sen-
tence as described in the previous section. Max-
Ent is not designed for sequence classification. To
deal with sequences, each name-class (e.g. PER-
SON) is divided into sub-classes: first token (e.g.
PERSON-begin), unique token (e.g. PERSON-
unique), or subsequent tokens (e.g. PERSON-
continue) in the name-class. To ensure that the
results returned by MaxEnt is coherent, we de-
fine deterministic transition probabilities that dis-
allow transitions such as one from PERSON-begin
to LOCATION-continue. A Viterbi parse is used
to find the valid sequence of name-classes with the
highest probability.
Support vector machines (SVM): The basic
idea behind SVM for binary classification prob-
lems is to consider the data points in their fea-
ture space, and to separate the two classes with a
hyper-plane, by maximizing the shortest distance
between the data points and the hyper-plane. If
there exists no hyperplane that can split the two la-
bels, the soft margin version of SVM will choose
a hyperplane that splits the examples as cleanly as
possible, while still maximizing the distance to the
nearest cleanly split examples (Joachims, 2002).
We used the SVM
light
package for our experi-
ments (Joachims, 2002). For the multi-label NER
classification with N classes, we learn N SVM
classifiers, and use a softmax function to obtain
the distribution. Formally, denoting by s(y) the
confidence returned by the classifier for each label
y ? Y , the probability of the label y
i
is given by
p(y
i
|x
i
) =
exp(s(y
i
))
?
y?Y
exp(s(y))
1527
Similarly to MaxEnt, we subdivide name-classes
into begin, continue, and unique sub-classes, and
use a Viterbi parse for the sequence of highest
probability. The SVM
light
package also imple-
ments a transductive version of the SVM algo-
rithm. We also compare our approach with the
transductive SVM (Joachims, 1999) in our experi-
mental results.
5 Experimental results
In this paper, we use the annotated data provided
by the Automatic Content Extraction (ACE) pro-
gram. The ACE data set is annotated for an Entity
Detection task, and the annotation consists of the
labeling of entity names (e.g. Powell) and men-
tions for each entity (e.g. pronouns such as he).
In this paper, we are interested in the problem of
recognition of the proper names (the named entity
recognition task), and hence use only entities la-
beled with the type NAM (LDC, 2005). Entities
are classified into seven types: Person entities are
humans mentioned in a document; Organization
entities are limited to established associations of
people; Geo-political entities are geographical ar-
eas defined by political and/or social groups; Lo-
cation entities are geographical items like land-
masses and bodies of water; Facility entities re-
fer to buildings and real estate improvements; Ve-
hicle entities are devices used for transportation;
and Weapon entities are devices used for harming
or destruction.
We compare performances of a few algorithms:
MaxEnt classifier (MaxEnt); MaxEnt classifier
with standard bootstrapping (MaxEnt-SB); bal-
anced bootstrapping based on MaxEnt classi-
fier (MaxEnt-BB); MaxEnt with DAB (MaxEnt-
DAB); SVM classifier (SVM); transductive SVM
classifier (SVM-Trans); and DAB based on SVM
classifier (SVM-DAB). No regularization is used
for MaxEnt classifiers. SVM classifiers use a
value of 10 for parameter C (trade-off between
training error and margin). Bootstrapping based
algorithms are run for 30 iterations and 100 in-
stances are selected in every iteration.
The evaluation measure used is the F-measure.
F-measure is the harmonic mean of precision and
recall, and is commonly used to evaluate NER
systems. We use the scorer for CONLL 2003
shared task (Tjong and Meulder, 2003) where the
F-measure is computed by averaging F-measures
for name-classes, weighted by the number of oc-
Code Source Num docs
NW Newswire 81
BC Broadcast conversation 52
WL Weblog 114
CTS Conversational Telephone Speech 34
Table 1: The sources, and the number of docu-
ments in each source, in the ACE 2005 data set.
currences.
5.1 Cross-source transfer
The ACE 2005 data set consists of articles drawn
from a variety of sources. We use the four cate-
gories shown in Table 1. Each category is consid-
ered to be a domain, and we consider each pair of
categories as the source and the target domain in
turn.
Figure 1 compares the performance of MaxEnt-
SB, MaxEnt-BB and MaxEnt-DAB over multiple
iterations. Figure 2 compares the performance
of SVM, SVM-Trans and SVM-DAB. Each line
in the figures represents the average F-measure
across all the domains over many iterations. When
the termination condition is met for one domain,
its F-measure remains at the value of the final iter-
ation.
Despite a large number of iterations, both stan-
dard and balanced bootstrapping fail to improve
performance. Supervised learning performance on
each domain is shown in Table 3 (by 2-fold cross-
validation with random ordering) as a reference.
In Table 5, we compare the F-measures obtained
by different algorithms at the last iteration they
were run. We will discuss more on this in Sec-
tion 5.3.
5.2 Cross-topic transfer
This data set is constructed from 175 articles from
the ACE 2005 corpus. The data set is used to eval-
uate transfer across topics. We manually classify
the articles into 4 categories: military operations
(MO), political relationship or politicians (POL),
terrorism-related (TER), and those which are not
in the above categories (OTH). A detailed break-
down of the number of documents in the each
topic is given in Table 2.
Supervised learning performance on each do-
main is shown in Table 4 (by 2-fold cross-
validation with random ordering) as a reference.
Experimental results on cross-topic evaluation are
shown in Table 6. Figure 3 compares the perfor-
mance of MaxEnt-SB, MaxEnt-BB and MaxEnt-
1528
57.558.0
58.559.0
59.560.0
60.561.0
61.562.0
 0  5  10  15  20  25  30  35
F-mea
sure
Number of iterations
MaxEnt-DABMaxEnt-SBMaxEnt-BB
Figure 1: Average performance on the cross-
source transfer using MaxEnt classifier.
35.0
40.0
45.0
50.0
55.0
60.0
65.0
70.0
 1  2  3  4  5  6
F-mea
sure
Number of iterations
SVM-DABSVMSVM-Trans
Figure 2: Average performance on the cross-
source transfer using SVM classifier.
66.266.4
66.666.8
67.067.2
67.467.6
67.868.0
68.2
 0  5  10  15  20  25  30  35
F-mea
sure
Number of iterations
MaxEnt-DABMaxEnt-SBMaxEnt-BB
Figure 3: Average performance on the cross-topic
transfer using MaxEnt classifier.
56.058.0
60.062.0
64.066.0
68.070.0
72.0
 1  2  3  4
F-mea
sure
Number of iterations
SVM-DABSVMSVM-Trans
Figure 4: Average performance on the cross-topic
transfer using SVM classifier.
Topic Topic description # docs
MO Military operations 92
POL Political relationships 40
TER Terrorist-related 28
OTH None of the above 15
Table 2: The topics, their descriptions, and the
number of training and test documents in each
topic.
Domain MaxEnt SVM
NW 82.47 82.32
BC 78.21 77.91
WL 71.41 71.84
CTS 93.90 94.01
Table 3: F-measure of supervised learning on the
cross-source target domains.
DAB over multiple iterations. Figure 4 compares
the performance of SVM, SVM-Trans and SVM-
DAB. Similar to cross-source transfer, standard
and balanced bootstrapping perform badly. This
will be discussed in Section 5.3.
Domain MaxEnt SVM
MO 80.52 80.6
POL 77.99 79.05
TER 81.74 82.12
OTH 71.33 72.08
Table 4: F-measure of supervised learning on the
cross-topic target domains.
5.3 Discussion
We show in our experiments that DAB outper-
forms standard and balanced bootstrapping, as
well as the transductive SVM. We have also shown
DAB to be robust across two state-of-the-art clas-
sifiers, MaxEnt and SVM. Balanced bootstrapping
has been shown to be more effective for domain
adaptation than standard bootstrapping (Jiang and
Zhai, 2007) for named entity classification on a
subset of the dataset used here. In contrast, we
found that both methods perform poorly on do-
main adaptation for NER. In named entity clas-
sification, the names have already been segmented
out and only need to be classified with the appro-
priate class. However, for NER, the names also
1529
Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB
BC CTS 74.26 74.19 74.16 81.03 72.47 43.27 75.43
BC NW 64.81 64.76 64.80 66.20 64.08 43.01 64.39
BC WL 47.81 47.80 47.76 49.52 47.98 36.58 47.93
CTS BC 46.19 46.12 46.40 54.62 46.02 40.44 49.64
CTS NW 54.25 54.15 54.26 53.07 55.63 23.61 58.99
CTS WL 40.42 40.43 40.72 41.27 39.96 29.05 42.04
NW BC 59.90 59.83 59.80 60.55 59.89 45.71 58.42
NW CTS 66.64 66.48 66.59 66.73 68.28 28.80 73.47
NW WL 52.52 52.53 52.47 53.44 52.19 36.39 52.30
WL BC 58.58 58.79 58.65 56.00 58.43 52.64 58.64
WL CTS 64.63 63.89 64.50 80.45 65.96 45.04 81.04
WL NW 67.79 67.72 67.92 68.46 68.38 43.40 69.33
Average 58.15 58.06 58.17 60.95 58.27 39.00 60.97
Table 5: F-measure of the cross-source transfer.
Train Test MaxEnt MaxEnt-SB MaxEnt-BB MaxEnt-DAB SVM SVM-Trans SVM-DAB
MO OTH 81.70 81.48 81.57 81.95 81.78 75.68 81.94
MO POL 73.21 73.11 73.28 74.97 72.56 58.13 72.66
MO TER 68.13 68.07 68.24 69.89 69.40 65.02 69.38
OTH MO 63.30 63.80 63.94 63.91 64.18 61.03 65.45
OTH POL 67.96 68.05 67.86 69.13 68.29 56.50 70.67
OTH TER 45.34 44.82 45.30 51.06 45.71 48.77 52.87
POL MO 62.14 62.12 61.95 61.94 61.98 51.67 62.32
POL OTH 77.91 77.72 77.79 76.58 78.11 65.71 78.13
POL TER 66.55 66.38 66.08 66.38 66.44 51.29 67.24
TER MO 58.35 58.62 58.02 57.29 58.30 49.80 58.14
TER OTH 66.83 67.61 66.83 68.97 66.28 58.25 68.12
TER POL 67.34 66.94 67.16 72.00 67.54 50.55 70.65
Average 66.56 66.56 66.50 67.84 66.71 57.70 68.13
Table 6: F-measure of the cross-topic transfer.
need to be separated from not-a-name instances.
We find that the addition of not-a-name instances
changes the problem - the not-a-names form most
of the instances classified with high confidence.
As a result, we find that both standard and bal-
anced bootstrapping fail to improve performance:
the selection of the most confident instances no
longer provide sufficient new information to im-
prove performance.
We also find that transductive SVM performs
poorly on this task. This is because it assumes
that the unlabeled data comes from the same dis-
tribution as the labeled data. In general, apply-
ing semi-supervised learning methods directly to
[S+T-] type domain adaptation problems do not
work and appropriate modifications need to be
made to the methods.
The ACE 2005 data set alo contains a set of
ariticles from the broadcast news (BN) source
which is written entirely in lower case. This makes
NER much more difficult. However, when BN is
the source domain, the capitalization information
can be discovered by DAB. Figures 5 and 6 show
the average performance when BN is used as the
source domain and all other domains in Table 1 as
the target domains.
The source domain classifier tends to have high
precision and low recall, DAB results in an in-
crease in recall, with a small decrease in precision.
Testing the significance of the F-measure is not
trivial because the named entities wrongly labeled
by two classifiers are not directly comparable. We
tested the labeling disagreements instead, using a
McNemar paired test. The significance test is per-
formed on the improvement of MaxEnt-DAB over
MaxEnt and SVM-DAB over SVM. In most of
the domains for the cross-source transfer, the im-
provements are significant at a significance level
of 0.05, using MaxEnt classifier. The exceptional
train-test pairs are NW-WL and WL-BC. In the
case of WL-BC, this means the slight decrement in
performance is not statistically significant. Similar
result is achieved for the cross-source transfer us-
ing SVM classifier. In the cross-topic transfer, the
source domain and the target domain are not very
different. When we have a large amount of train-
ing data and little testing data, the gain of DAB
can be not statistically significant, as in the case
when we train with MO and POL domains.
1530
20.0
25.0
30.0
35.0
40.0
45.0
50.0
 1  2  3  4  5  6  7  8  9
F-mea
sure
Number of iterations
MaxEnt-DABMaxEnt-SBMaxEnt-BB
Figure 5: Performance on recovering capitaliza-
tion using MaxEnt classifier.
28.030.0
32.034.0
36.038.0
40.042.0
44.046.0
 1  2  3  4
F-mea
sure
Number of iterations
SVM-DABSVMSVM-Trans
Figure 6: Performance on recovering capitaliza-
tion using SVM classifier.
6 Conclusion
We proposed a bootstrapping approach for domain
adaptation, and we applied it to the named entity
recognition task. Our approach leverages on in-
stances that serve as bridges between the source
and target domain. Empirically, our method out-
performs baseline approaches including super-
vised, transductive and standard bootstrapping ap-
proaches. It also outperforms balanced bootstrap-
ping, an approach designed for domain adaptation
(Jiang and Zhai, 2007).
References
Steffen Bickel, Michael Br?uckner, and Tobias Scheffer.
2007. Discriminative learning for differing training
and test distributions. In ICML ?07: Proceedings of
the 24th international conference on Machine learn-
ing, pages 81?88, New York, NY, USA. ACM Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney, Aus-
tralia.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49?56, Prague, Czech Republic,
June. Association for Computational Linguistics.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Advances in Structured
Learning for Text and Speech Processing Workshop.
Hal Daum?e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Compu-
tational Linguistics (ACL), Prague, Czech Republic.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Main Conference, New York
City, USA. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 264?271,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML ?99: Proceedings of the Sixteenth Interna-
tional Conference on Machine Learning, pages 200?
209, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
T. Joachims. 2002. Learning to Classify Text Using
Support Vector Machines ? Methods, Theory, and
Algorithms. Kluwer/Springer.
Linguistic Data Consortium LDC. 2005. ACE
(Automatic Content Extraction) English Annotation
Guidelines for Entities.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
1531
Human Language Technology, pages 8?8, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based ac-
tive learning for named entity recognition. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 589?596, Barcelona, Spain, July.
Erik Tjong and Fien De Meulder. 2003. Intro-
duction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of Conference on Computational Natural Lan-
guage Learning.
1532
Proceedings of the 43rd Annual Meeting of the ACL, pages 34?41,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Learning Semantic Classes for Word Sense Disambiguation
Upali S. Kohomban Wee Sun Lee
Department of Computer Science
National University of Singapore
Singapore, 117584
{upalisat,leews}@comp.nus.edu.sg
Abstract
Word Sense Disambiguation suffers from
a long-standing problem of knowledge ac-
quisition bottleneck. Although state of the
art supervised systems report good accu-
racies for selected words, they have not
been shown to be promising in terms of
scalability. In this paper, we present an ap-
proach for learning coarser and more gen-
eral set of concepts from a sense tagged
corpus, in order to alleviate the knowl-
edge acquisition bottleneck. We show that
these general concepts can be transformed
to fine grained word senses using simple
heuristics, and applying the technique for
recent SENSEVAL data sets shows that our
approach can yield state of the art perfor-
mance.
1 Introduction
Word Sense Disambiguation (WSD) is the task of
determining the meaning of a word in a given con-
text. This task has a long history in natural language
processing, and is considered to be an intermediate
task, success of which is considered to be important
for other tasks such as Machine Translation, Lan-
guage Understanding, and Information Retrieval.
Despite a long history of attempts to solve WSD
problem by empirical means, there is not any clear
consensus on what it takes to build a high perfor-
mance implementation of WSD. Algorithms based
on Supervised Learning, in general, show better per-
formance compared to unsupervised systems. But
they suffer from a serious drawback: the difficulty
of acquiring considerable amounts of training data,
also known as knowledge acquisition bottleneck. In
the typical setting, supervised learning needs train-
ing data created for each and every polysemous
word; Ng (1997) estimates an effort of 16 person-
years for acquiring training data for 3,200 significant
words in English. Mihalcea and Chklovski (2003)
provide a similar estimate of an 80 person-year ef-
fort for creating manually labelled training data for
about 20,000 words in a common English dictionary.
Two basic approaches have been tried as solu-
tions to the lack of training data, namely unsu-
pervised systems and semi-supervised bootstrapping
techniques. Unsupervised systems mostly work
on knowledge-based techniques, exploiting sense
knowledge encoded in machine-readable dictionary
entries, taxonomical hierarchies such as WORD-
NET (Fellbaum, 1998), and so on. Most of the
bootstrapping techniques start from a few ?seed? la-
belled examples, classify some unlabelled instances
using this knowledge, and iteratively expand their
knowledge using information available within newly
labelled data. Some others employ hierarchical rel-
atives such as hypernyms and hyponyms.
In this work, we present another practical alterna-
tive: we reduce the WSD problem to a one of finding
generic semantic class of a given word instance. We
show that learning such classes can help relieve the
problem of knowledge acquisition bottleneck.
1.1 Learning senses as concepts
As the semantic classes we propose learning, we
use WORDNET lexicographer file identifiers corre-
34
sponding to each fine-grained sense. By learning
these generic classes, we show that we can reuse
training data, without having to rely on specific
training data for each word. This can be done be-
cause the semantic classes are common to words
unlike senses; for learning the properties of a given
class, we can use the data from various words. For
instance, the noun crane falls into two semantic
classes ANIMAL and ARTEFACT. We can expect the
words such as pelican and eagle (in the bird sense)
to have similar usage patterns to those of ANIMAL
sense of crane, and to provide common training ex-
amples for that particular class.
For learning these classes, we can make use of any
training example labelled with WORDNET senses
for supervised WSD, as we describe in section 3.1.
Once the classification is done for an instance, the
resulting semantic classes can be transformed into
finer grained senses using some heuristical mapping,
as we show in the next sub section. This would not
guarantee a perfect conversion because such a map-
ping can miss some finer senses, but as we show in
what follows, this problem in itself does not prevent
us from attaining good performance in a practical
WSD setting.
1.2 Information loss in coarse grained senses
As an empirical verification of the hypothesis that
we can still build effective fine-grained sense dis-
ambiguators despite the loss of information, we an-
alyzed the performance of a hypothetical coarse
grained classifier that can perform at 100% accu-
racy. As the general set of classes, we used WORD-
NET unique beginners, of which there are 25 for
nouns, and 15 for verbs.
To simulate this classifier on SENSEVAL English
all-words tasks? data (Edmonds and Cotton, 2001;
Snyder and Palmer, 2004), we mapped the fine-
grained senses from official answer keys to their
respective beginners. There is an information loss
in this mapping, because each unique beginner can
typically include more than one sense. To see how
this ?classifier? fares in a fine-grained task, we can
map the ?answers? back to WORDNET fine-grained
senses by picking up the sense with the lowest sense
number that falls within each unique beginner. In
principal, this is the most likely sense within the
class, because WORDNET senses are said to be



	



   

Figure 1: Performance of a hypothetical coarse-
grained classifier, output mapped to fine-grained
senses, on SENSEVAL English all-words tasks.
ordered in descending order of frequency. Since
this sense is not necessarily the same as the origi-
nal sense of the instance, the accuracy of the fine-
grained answers will be below 100%.
Figure 1 shows the performance of this trans-
formed fine-grained classifier (CG) for nouns and
verbs with SENSEVAL-2 and 3 English all words
task data (marked as S2 and S3 respectively),
along with the baseline WORDNET first sense (BL),
and the best-performer classifiers at each SENSE-
VAL excercise (CL), SMUaw (Mihalcea, 2002) and
GAMBL-AW (Decadt et al, 2004) respectively.
There is a considerable difference in terms of im-
provement over baseline, between the state-of-the-
art systems and the hypothetical optimal coarse-
grained system. This shows us that there is an im-
provement in performance that we can attain over
the state-of-the-art, if we can create a classifier for
even a very coarse level of senses, with sufficiently
high accuracy. We believe that the chances for such
a high accuracy in a coarse-grained sense classifier
is better, for several reasons:
? previously reported good performance for
coarse grained systems (Yarowsky, 1992)
? better availability of data, due to the possibil-
ity of reusing data created for different words.
For instance, labelled data for the noun ?crane?
is not found in SEMCOR corpus at all, but
there are more than 1000 sample instances for
the concept ANIMAL, and more than 9000 for
ARTEFACT.
35
? higher inter-annotator agreement levels and
lower corpus/genre dependencies in train-
ing/testing data due to coarser senses.
1.3 Overall approach
Basically, we assume that we can learn the ?con-
cepts?, in terms of WORDNET unique beginners, us-
ing a set of data labelled with these concepts, re-
gardless of the actual word that is labelled. Hence,
we can use a generic data set that is large enough,
where various words provide training examples for
these concepts, instead of relying upon data from the
examples of the same word that is being classified.
Unfortunately, simply labelling each instance
with its semantic class and then using standard su-
pervised learning algorithms did not work well. This
is probably because the effectiveness of the feature
patterns often depend on the actual word being dis-
ambiguated and not just its semantic class. For ex-
ample, the phrase ?run the newspaper? effectively
indicates that ?newspaper? belongs to the seman-
tic class GROUP. But ?run the tape? indicates that
?tape? belongs to the semantic class ARTEFACT. The
collocation ?run the? is effective for indicating the
GROUP sense only for ?newspaper? and closely re-
lated words such as ?department? or ?school?.
In this experiment, we use a k-nearest neighbor
classifier. In order to allow training examples of
different words from the same semantic class to
effectively provide information for each other, we
modify the distance between instances in a way
that makes the distance between instances of simi-
lar words smaller. This is described in Section 3.
The rest of the paper is organized as follows: In
section 2, we discuss several related work. We pro-
ceed on to a detailed description of our system in
section 3, and discuss the empirical results in section
4, showing that our representation can yield state of
the art performance.
2 Related Work
Using generic classes as word senses has been
done several times in WSD, in various contexts.
Resnik (1997) described a method to acquire a set
of conceptual classes for word senses, employing
selectional preferences, based on the idea that cer-
tain linguistic predicates constraint the semantic in-
terpretation of underlying words into certain classes.
The method he proposed could acquire these con-
straints from a raw corpus automatically.
Classification proposed by Levin (1993) for Eng-
lish verbs remains a matter of interest. Although
these classes are based on syntactic properties unlike
those in WORDNET, it has been shown that they can
be used in automatic classifications (Stevenson and
Merlo, 2000). Korhonen (2002) proposed a method
for mapping WORDNET entries into Levin classes.
WSD System presented by Crestan et al (2001)
in SENSEVAL-2 classified words into WORD-
NET unique beginners. However, their approach
did not use the fact that the primes are common for
words, and training data can hence be reused.
Yarowsky (1992) used Roget?s Thesaurus cate-
gories as classes for word senses. These classes dif-
fer from those mentioned above, by the fact that they
are based on topical context rather than syntax or
grammar.
3 Basic Design of the System
The system consists of three classifiers, built using
local context, part of speech and syntax-based rela-
tionships respectively, and combined with the most-
frequent sense classifier by using weighted major-
ity voting. Our experiments (section 4.3) show that
building separate classifiers from different subsets
of features and combining them works better than
building one classifier by concatenating the features
together.
For training and testing, we used publicly avail-
able data sets, namely SEMCOR corpus (Miller et
al., 1993) and SENSEVAL English all-words task
data. In order to evaluate the systems performance
in vivo, we mapped the outputs of our classifier to
the answers given in the key. Although we face a
penalty here due to the loss of granularity, this ap-
proach allows a direct comparison of actual usability
of our system.
3.1 Data
As training corpus, we used Brown-1 and Brown-
2 parts of SEMCOR corpus; these parts have all of
their open-class words tagged with corresponding
WORDNET senses. A part of the training corpus was
set aside as the development corpus. This part was
selected by randomly selecting a portion of multi-
36
class words (600 instances for each part of speech)
from the training data set. As labels, the seman-
tic class (lexicographic file number) was extracted
from the sense key of each instance. Testing data
sets from SENSEVAL-2 and SENSEVAL-3 English
all-words tasks were used as testing corpora.
3.2 Features
The feature set we selected was fairly simple; As
we understood from our initial experiments, wide-
window context features and topical context were
not of much use for learning semantic classes from
a multi-word training data set. Instead of general-
izing, wider context windows add to noise, as seen
from validation experiments with held-out data.
Following are the features we used:
3.2.1 Local context
This is a window of n words to the left, and n
words to the right, where n ? {1, 2, 3} is a parame-
ter we selected via cross validation.1
Punctuation marks were removed and all words
were converted into lower case. The feature vec-
tor was calculated the same way for both nouns and
verbs. The window did not exceed the boundaries
of a sentence; when there were not enough words to
either side of the word within the window, the value
NULL was used to fill the remaining positions.
For instance, for the noun ?companion? in sen-
tence (given with POS tags)
?Henry/NNP peered/VBD doubtfully/RB
at/IN his/PRP$ drinking/NN compan-
ion/NN through/IN bleary/JJ ,/, tear-
filled/JJ eyes/NNS ./.?
the local context feature vector is [at,
his, drinking, through, bleary,
tear-filled], for window size n = 3. Notice
that we did not consider the hyphenated words as
two words, when the data files had them annotated
as a single token.
3.2.2 Part of speech
This consists of parts of speech for a window of
n words to both sides of word (excluding the word
1Validation results showed that a window of two words to
both sides yields the best performance for both local context and
POS features. n = 2 is the size we used in actual evaluation.
Feature Example Value
nouns
Subject - verb [art] represents a culture represent
Verb - object He sells his [art] sell
Adjectival modifiers the ancient [art] of runes ancient
Prepositional connectors academy of folk [art] academy of
Post-nominal modifiers the [art] of fishing of fishing
verbs
Subject - verb He [sells] his art he
Verb - object He [sells] his art art
Infinitive connector He will [sell] his art he
Adverbial modifier He can [paint] well well
Words in split infinitives to boldly [go] boldly
Table 1: Syntactic relations used as features. The
target word is shown inside [brackets]
itself), with quotation signs and punctuation marks
ignored. For SEMCOR files, existing parts of speech
were used; for SENSEVAL data files, parts of speech
from the accompanying Penn-Treebank parsed data
files were aligned with the XML data files. The
value vector is calculated the same way as the lo-
cal context, with the same constraint on sentence
boundaries, replacing vacancies with NULL.
As an example, for the sentence we used in the
previous example, the part-of-speech vector with
context size n = 3 for the verb peered is [NULL,
NULL, NNP, RB, IN, PRP$].
3.2.3 Syntactic relations with the word
The words that hold several kinds of syntactic re-
lations with the word under consideration were se-
lected. We used Link Grammar parser due to Sleator
and Temperley (1991) because of the information-
rich parse results produced by it.
Sentences in SEMCOR corpus files and the SEN-
SEVAL files were parsed with Link parser, and words
were aligned with links. A given instance of a word
can have more than one syntactic features present.
Each of these features was considered as a binary
feature, and a vector of binary values was con-
structed, of which each element denoted a unique
feature found in the test set of the word.
Each syntactic pattern feature falls into either of
two types collocation or relation:
Collocation features Collocation features are
such features that connect the word under consid-
eration to another word, with a preposition or an in-
finitive in between ? for instance, the phrase ?art
of change-ringing? for the word art. For these fea-
tures, the feature value consists of two words, which
are connected to the given word either from left or
37
from right, in a given order. For the above example,
the feature value is [?.of.change-ringing],
where ? denotes the placeholder for word under
consideration.
Relational features Relational features represent
more direct grammatical relationships, such as
subject-verb or noun-adjective, the word under con-
sideration has with surrounding words. When
encoding the feature value, we specified the re-
lation type and the value of the feature in the
given instance. For instance, in the phrase ?Henry
peered doubtfully?, the adverbial modifier feature
for the verb ?peered? is encoded as [adverb-mod
doubtfully].
A description of the relations for each part of
speech is given in the table 1.
3.3 Classifier and instance weighting
The classifier we used was TiMBL, a memory based
learner due to Daelemans et al (2003). One reason
for this choice was that memory based learning has
shown to perform well in previous word sense dis-
ambiguation tasks, including some best performers
in SENSEVAL, such as (Hoste et al, 2001; Decadt
et al, 2004; Mihalcea and Faruque, 2004). Another
reason is that TiMBL supported exemplar weights, a
necessary feature for our system for the reasons we
describe in the next section.
One of the salient features of our system is that it
does not consider every example to be equally im-
portant. Due to the fact that training instances from
different instances can provide confusing examples,
as shown in section 1.3, such an approach cannot be
trusted to give good performance; we verified this
by our own findings through empirical evaluations
as shown in section 4.2.
3.3.1 Weighting instances with similarity
We use a similarity based measure to assign
weights to training examples. In the method we use,
these weights are used to adjust the distances be-
tween the test instance and the example instances.
The distances are adjusted according to the formula
?E(X,Y ) =
?(X,Y )
ewX + 
,
where ?E(X,Y ) is the adjusted distance between
instance Y and example X , ?(X,Y ) is the original
distance, ewX is the exemplar weight of instance X .
The small constant  is added to avoid division by
zero.
There are various schemes used to measure inter-
sense similarity. Our experiments showed that the
measure defined by Jiang and Conrath (1997) (JCn)
yields best results. Results for various weighting
schemes are discussed in section 4.2.
3.3.2 Instance weighting explained
The exemplar weights were derived from the fol-
lowing method:
1. pick a labelled example e, and extract its sense
se and semantic class ce.
2. if the class ce is a candidate for the current test
word w, i.e. w has any senses that fall into
ce, find out the most frequent sense of w, scew ,
within ce. We define the most frequent sense
within a class as the sense that has the lowest
WORDNET sense number within that class. If
none of the senses of w fall into ce, we ignore
that example.
3. calculate the relatedness measure between se
and scew , using whatever the similarity metric
being considered. This is the exemplar weight
for example e.
In the implementation, we used freely available
WordNet::Similarity package (Pedersen et
al., 2004). 2
3.4 Classifier optimization
A part of SEMCOR corpus was used as a validation
set (see section 3.1). The rest was used as training
data in validation phase. In the preliminary experi-
ments, it was seen that the generally recommended
classifier options yield good enough performance,
although variations of switches could improve per-
formance slightly in certain cases. Classifier op-
tions were selected by a search over the available
option space for only three basic classifier parame-
ters, namely, number of nearest neighbors, distance
metric and feature weighting scheme.
2WordNet::Similarity is a perl package available
freely under GNU General Public Licence. http://wn-
similarity.sourceforge.net.
38
Classifier Senseval-2 Senseval-3
Baseline 0.617 0.627
POS 0.616 0.614
Local context 0.627 0.633
Synt. Pat 0.620 0.612
Concatenated 0.609 0.611
Combined 0.631 0.643
Table 2: Results of baseline, individual, and com-
bined classifiers: recall measures for nouns and
verbs combined.
4 Results
In what follows, we present the results of our ex-
periments in various test cases.3 We combined the
three classifiers and the WORDNET first-sense clas-
sifier through simple majority voting. For evaluating
the systems with SENSEVAL data sets, we mapped
the outputs of our classifiers to WORDNET senses
by picking the most-frequent sense (the one with the
lowest sense number) within each of the class. This
mapping was used in all tests. For all evaluations,
we used SENSEVAL official scorer.
We could use the setting only for nouns and verbs,
because the similarity measures we used were not
defined for adjectives or adverbs, due to the fact that
hypernyms are not defined for these two parts of
speech. So we list the initial results only for nouns
and verbs.
4.1 Individual classifiers vs. combination
We evaluated the results of the individual classifiers
before combination. Only local context classifier
could outperform the baseline in general, although
there is a slight improvement with the syntactic pat-
tern classifier on SENSEVAL-2 data.
The results are given in the table 2, together
with the results of voted combination, and baseline
WORDNET first sense. Classifier shown as ?con-
catenated? is a single classifier trained from all of
these feature vectors concatenated to make a sin-
gle vector. Concatenating features this way does not
seem to improve performance. Although exact rea-
sons for this are not clear, this is consistent with pre-
3Note that the experiments and results are reported for SEN-
SEVAL data for comparison purposes, and were not involved in
parameter optimization, which was done with the development
sample.
Senseval-2 Senseval-3
No similarity used 0.608 0.599
Resnik 0.540 0.522
JCn 0.631 0.643
Table 3: Effect of different similarity schemes on
recall, combined results for nouns and verbs
Senseval-2 Senseval-3
SM 0.631 0.643
GW 0.634 0.649
LW 0.641 0.650
Table 4: Improvement of performance with classifier
weighting. Combined results for nouns and verbs
with voting schemes Simple Majority (SM), Global
classifier weights (GW) and local weights (LW).
vious observations (Hoste et al, 2001; Decadt et al,
2004) that combining classifiers, each using differ-
ent features, can yield good performance.
4.2 Effect of similarity measure
Table 3 shows the effect of JCn and Resnik simi-
larity measures, along with no similarity weighting,
for the combined classifier. It is clear that proper
similarity measure has a major impact on the perfor-
mance, with Resnik measure performing worse than
the baseline.
4.3 Optimizing the voting process
Several voting schemes were tried for combining
classifiers. Simple majority voting improves perfor-
mance over baseline. However, previously reported
results such as (Hoste et al, 2001) and (Decadt et al,
2004) have shown that optimizing the voting process
helps improve the results. We used a variation of
Weighted Majority Algorithm (Littlestone and War-
muth, 1994). The original algorithm was formulated
for binary classification tasks; however, our use of it
for multi-class case proved to be successful.
We used the held-out development data set for ad-
justing classifier weights. Originally, all classifiers
have the same weight of 1. With each test instance,
the classifier builds the final output considering the
weights. If this output turns out to be wrong, the
classifiers that contributed to the wrong answer get
their weights reduced by some factor. We could ad-
39
Senseval-2 Senseval-3
System 0.777 0.806
Baseline 0.756 0.783
Table 5: Coarse grained results
just the weights locally or globally; In global setting,
the weights were adjusted using a random sample
of held-out data, which contained different words.
These weights were used for classifying all words
in the actual test set. In local setting, each classifier
weight setting was optimized for individual words
that were present in test sets, by picking up random
samples of the same word from SEMCOR .4 Table 4
shows the improvements with each setting.
Coarse grained (at semantic-class level) results
for the same system are shown in table 5. Baseline
figures reported are for the most-frequent class.
4.4 Final results on SENSEVAL data
Here, we list the performance of the system with ad-
jectives and adverbs added for the ease of compar-
ison. Due to the facts mentioned at the beginning
of this section, our system was not applicable for
these parts of speech, and we classified all instances
of these two POS types with their most frequent
sense. We also identified the multi-word phrases
from the test documents. These phrases generally
have a unique sense in WORDNET ; we marked
all of them with their first sense without classify-
ing them. All the multiple-class instances of nouns
and verbs were classified and converted to WORD-
NET senses by the method described above, with lo-
cally optimized classifier voting.
The results of the systems are shown in tables 7
and 8. Our system?s results in both cases are listed
as Simil-Prime, along with the baseline WORD-
NET first sense (including multi-word phrases and
?U? answers), and the two best performers? results
reported.5 These results compare favorably with the
official results reported in both tasks.
4Words for which there were no samples in SEMCOR were
classified using a weight of 1 for all classifiers.
5The differences of the baseline figures from the previously
reported figures are clearly due to different handling of multi-
word phrases, hyphenated words, and unknown words in each
system. We observed by analyzing the answer keys that even
better baseline figures are technically possible, with better tech-
niques to identify these special cases.
Senseval-2 Senseval-3
Micro Average < 0.0001 < 0.0001
Macro Average 0.0073 0.0252
Table 6: One tailed paired t-test significance levels
of results: P (T 6 t)
System Recall
SMUaw (Mihalcea, 2002) 0.690
Simil-Prime 0.664
Baseline (WORDNET first sense) 0.648
CNTS-Antwerp (Hoste et al, 2001) 0.636
Table 7: Results for SENSEVAL-2 English all words
data for all parts of speech and fine grained scoring.
Significance of results To verify the significance
of these results, we used one-tailed paired t-test, us-
ing results of baseline WORDNET first sense and
our system as pairs. Tests were done both at micro-
average level and macro-average level, (considering
test data set as a whole and considering per-word av-
erage). Null hypothesis was that there is no signif-
icant improvement over the baseline. Both settings
yield good significance levels, as shown in table 6.
5 Conclusion and Future Work
We analyzed the problem of Knowledge Acquisition
Bottleneck in WSD, proposed using a general set of
semantic classes as a trade-off, and discussed why
such a system is promising. Our formulation al-
lowed us to use training examples from words dif-
ferent from the actual word being classified. This
makes the available labelled data reusable for differ-
ent words, relieving the above problem. In order to
facilitate learning, we introduced a technique based
on word sense similarity.
The generic classes we learned can be mapped to
System Recall
Simil-Prime 0.661
GAMBL-AW-S (Decadt et al, 2004) 0.652
SenseLearner (Mihalcea and Faruque, 2004) 0.646
Baseline (WORDNET first sense) 0.642
Table 8: Results for SENSEVAL-3 English all words
data for all parts of speech and fine grained scoring.
40
finer grained senses with simple heuristics. Through
empirical findings, we showed that our system can
attain state of the art performance, when applied to
standard fine-grained WSD evaluation tasks.
In the future, we hope to improve on these results:
Instead of using WORDNET unique beginners, using
more natural semantic classes based on word usage
would possibly improve the accuracy, and finding
such classes would be a worthwhile area of research.
As seen from our results, selecting correct similarity
measure has an impact on the final outcome. We
hope to work on similarity measures that are more
applicable in our task.
6 Acknowledgements
Authors wish to thank the three anonymous review-
ers for their helpful suggestions and comments.
References
E. Crestan, M. El-Be`ze, and C. De Loupy. 2001. Improv-
ing wsd with multi-level view of context monitored by
similarity measure. In Proceeding of SENSEVAL-2:
Second International Workshop on Evaluating Word
Sense Disambiguation Systems, Toulouse, France.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg Memory
Based Learner, version 5.0, reference guide. Technical
report, ILK 03-10.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and
Antal Van den Bosch. 2004. GAMBL, genetic
algorithm optimization of memory-based wsd. In
Senseval-3: Third Intl. Workshop on the Evaluation of
Systems for the Semantic Analysis of Text.
P. Edmonds and S. Cotton. 2001. Senseval-2: Overview.
In Proc. of the Second Intl. Workshop on Evaluating
Word Sense Disambiguation Systems (Senseval-2).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. The MIT Press, Cambridge, MA.
Ve?ronique Hoste, Anne Kool, and Walter Daelmans.
2001. Classifier optimization and combination in Eng-
lish all words task. In Proceeding of SENSEVAL-2:
Second International Workshop on Evaluating Word
Sense Disambiguation Systems.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of International Conference on Research in Com-
putational Linguistics.
Anna Korhonen. 2002. Assigning verbs to semantic
classes via wordnet. In Proceedings of the COLING
Workshop on Building and Using Semantic Networks.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. University of Chicago Press, Chicago, IL.
N Littlestone and M.K. Warmuth. 1994. The weighted
majority algorithm. Information and Computation,
108(2):212?261.
Rada Mihalcea and Tim Chklovski. 2003. Open Mind
Word Expert: Creating large annotated data collec-
tions with web users? help. In Proceedings of the
EACL 2003 Workshop on Linguistically Annotated
Corpora.
Rada Mihalcea and Ehsanul Faruque. 2004. Sense-
learner: Minimally supervised word sense disam-
biguation for all words in open text. In Senseval-3:
Third Intl. Workshop on the Evaluation of Systems for
the Semantic Analysis of Text.
Rada Mihalcea. 2002. Bootstrapping large sense tagged
corpora. In Proc. of the 3rd Intl. Conference on Lan-
guages Resources and Evaluations.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proc. of the 3rd DARPA
Workshop on Human Language Technology.
Hwee Tou Ng. 1997. Getting serious about word sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?, pages 1?7.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence (AAAI-04).
P. Resnik. 1997. Selectional preference and sense dis-
ambiguation. In Proc. of ACL Siglex Workshop on
Tagging Text with Lexical Semantics, Why, What and
How?
D. Sleator and D. Temperley. 1991. Parsing English with
a Link Grammar. Technical report, Carnegie Mellon
University Computer Science CMU-CS-91-196.
B. Snyder and M. Palmer. 2004. The English all-words
task. In Senseval-3: Third Intl. Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text.
Suzanne Stevenson and Paola Merlo. 2000. Automatic
lexical acquisition based on statistical distributions. In
Proc. of the 17th conf. on Computational linguistics.
David Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Roget?s categories trained on
large corpora. In Proceedings of COLING-92, pages
454?460.
41
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 249?252,
Prague, June 2007. c?2007 Association for Computational Linguistics
NUS-ML: Improving Word Sense Disambiguation Using Topic Features
Jun Fu Cai, Wee Sun Lee
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{caijunfu, leews}@comp.nus.edu.sg
Yee Whye Teh
Gatsby Computational Neuroscience Unit
University College London
17 Queen Square, London WC1N 3AR, UK
ywteh@gatsby.ucl.ac.uk
Abstract
We participated in SemEval-1 English
coarse-grained all-words task (task 7), En-
glish fine-grained all-words task (task 17,
subtask 3) and English coarse-grained lex-
ical sample task (task 17, subtask 1). The
same method with different labeled data is
used for the tasks; SemCor is the labeled
corpus used to train our system for the all-
words tasks while the labeled corpus that
is provided is used for the lexical sam-
ple task. The knowledge sources include
part-of-speech of neighboring words, single
words in the surrounding context, local col-
locations, and syntactic patterns. In addi-
tion, we constructed a topic feature, targeted
to capture the global context information,
using the latent dirichlet alocation (LDA)
algorithm with unlabeled corpus. A modi-
fied na??ve Bayes classifier is constructed to
incorporate all the features. We achieved
81.6%, 57.6%, 88.7% for coarse-grained all-
words task, fine-grained all-words task and
coarse-grained lexical sample task respec-
tively.
1 Introduction
Supervised corpus-based approach has been the
most successful in WSD to date. However, this ap-
proach faces severe data scarcity problem, resulting
features being sparsely represented in the training
data. This problem is especially prominent for the
bag-of-words feature. A direct consequence is that
the global context information, which the bag-of-
words feature is supposed to capture, may be poorly
represented.
Our system tries to address this problem by
clustering features to relieve the scarcity problem,
specifically on the bag-of-words feature. In the pro-
cess, we construct topic features, trained using the
latent dirichlet alocation (LDA) algorithm. We train
the topic model (Blei et al, 2003) on unlabeled data,
clustering the words occurring in the corpus to a pre-
defined number of topics. We then use the resulting
topic model to tag the bag-of-words in the labeled
corpus with topic distributions.
We incorporate the distributions, called the topic
features, using a simple Bayesian network, modified
from na??ve Bayes model, alongside other features
and train the model on the labeled corpus.
2 Feature Construction
2.1 Baseline Features
For both the lexical sample and all-words tasks, we
use the following standard baseline features.
POS Tags For each word instance w, we include
POS tags for P words prior to as well as after w
within the same sentence boundary. We also include
the POS tag of w. If there are fewer than P words
prior or after w in the same sentence, we denote the
corresponding feature as NIL.
Local Collocations We adopt the same 11 col-
location features as (Lee and Ng, 2002), namely
C?1,?1, C1,1, C?2,?2, C2,2, C?2,?1, C?1,1, C1,2,
C?3,?1, C?2,1, C?1,2, and C1,3.
249
Bag-of-Words For each training or testing word,
w, we get G words prior to as well as after w, within
the same document. These features are position in-
sensitive. The words we extract are converted back
to their morphological root forms.
Syntactic Relations We adopt the same syntactic
relations as (Lee and Ng, 2002). For easy reference,
we summarize the features into Table 1.
POS of w Features
Noun Parent headword h
POS of h
Relative position of h to w
Verb Left nearest child word of w, l
Right nearest child word of w, r
POS of l
POS of r
POS of w
Voice of w
Adjective Parent headword h
POS of h
Table 1: Syntactic Relations Features
The exact values of P and G for each task are set
according to validation result.
2.2 Latent Dirichlet Allocation
We present here the latent dirichlet alocation algo-
rithm and its inference procedures, adapted from the
original paper (Blei et al, 2003).
LDA is a probabilistic model for collections of
discrete data and has been used in document mod-
eling and text classification. It can be represented
as a three level hierarchical Bayesian model, shown
graphically in Figure 1. Given a corpus consisting of
M documents, LDA models each document using a
mixture over K topics, which are in turn character-
ized as distributions over words.
In the generative process of LDA, for each doc-
ument d we first draw the mixing proportion over
topics ?d from a Dirichlet prior with parameters ?.
Next, for each of the Nd words wdn in document d, a
topic zdn is first drawn from a multinomial distribu-
tion with parameters ?d. Finally wdn is drawn from
the topic specific distribution over words. The prob-
ability of a word token w taking on value i given
that topic z = j was chosen is parameterized using
?
wz??
N
M
Figure 1: Graphical Model for LDA
a matrix ? with ?ij = p(w = i|z = j). Integrating
out ?d?s and zdn?s, the probability p(D|?, ?) of the
corpus is thus:
M?
d=1
?
p(?d|?)
(
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?)
)
d?d
In variational inference, the latent variables ?d
and zdn are assumed independent and updates to
the variational posteriors for ?d and zdn are derived
(Blei et al, 2003). It can be shown that the varia-
tional posterior for ?d is a Dirichlet distribution, say
with variational parameters ?d, which we shall use
in the following to construct topic features.
2.3 Topic Features
We first select an unlabeled corpus, such as 20
Newsgroups, and extract individual words from it
(excluding stopwords). We choose the number of
topics, K, for the unlabeled corpus and we apply the
LDA algorithm to obtain the ? parameters, where ?
represents the probability of a word w = i given a
topic z = j, p(w = i|z = j) = ?ij .
The model essentially clusters words that oc-
curred in the unlabeled corpus according to K top-
ics. The conditional probability p(w = i|z = j) =
?ij is later used to tag the words in the unseen test
example with the probability of each topic.
We also use the document-specific ?d parameters.
Specifically, we need to run the inference algorithm
on the labeled corpus to get ?d for each document d
in the corpus. The ?d parameter provides an approx-
imation to the probability of selecting topic i in the
document:
p(zi|?d) =
?di
?
K ?dk
. (1)
250
3 Classifier Construction
We construct a variant of the na??ve Bayes network
as shown in Figure 2. Here, w refers to the word.
s refers to the sense of the word. In training, s is
observed while in testing, it is not. The features f1
to fn are baseline features mentioned in Section 2.1
(including bag-of-words) while z refers to the la-
tent topic that we set for clustering unlabeled corpus.
The bag-of-words b are extracted from the neigh-
bours of w and there are L of them. Note that L can
be different from G, which is the number of bag-of-
words in baseline features. Both will be determined
by the validation result.
? ? ?
? ?? ?
baselinefeatures
w
s
fnf1
b
z
L
Figure 2: Graphical Model with LDA feature
The log-likelihood of an instance, `(w, s, F, b)
where F denotes the set of baseline features, can be
written as
= logp(w) + logp(s|w) +
?
F
log(p(f |s))
+
?
L
log
(
?
K
p(zk|s)p(bl|zk)
)
.
The log p(w) term is constant and thus can be
ignored. The first portion is normal na??ve Bayes.
And second portion represents the additional LDA
plate. We decouple the training process into separate
stages. We first extract baseline features from the
task training data, and estimate, using normal na??ve
Bayes, p(s|w) and p(f |s) for all w, s and f .
Next, the parameters associated with p(b|z) are
estimated using LDA from unlabeled data, which is
?. To estimate p(z|s), we perform LDA inference
on the training corpus in order to obtain ?d for each
document d. We then use the ?d and ? to obtain
p(z|b) for each word using
p(zi|bl, ?d) =
p(bl|zi)p(zi|?d)
?
K p(bl|zk)p(zk|?d)
,
where equation (1) is used for estimation of p(zi|?d).
This effectively transforms b to a topical distri-
bution which we call a soft tag where each soft
tag is probability distribution t1, . . . , tK on topics.
We then use this topical distribution for estimating
p(z|s). Let si be the observed sense of instance i
and tij1 , . . . , t
ij
K be the soft tag of the j-th bag-of-
word feature of instance i. We estimate p(z|s) as
p(zjk|s) =
?
si=s t
ij
k
?
si=s
?
k? t
ij
k?
(2)
This approach requires us to do LDA inference on
the corpus formed by the labeled training data, but
not the testing data. This is because we need ? to
get transformed topical distribution in order to learn
p(z|s) in the training. In the testing, we only apply
the learnt parameters to the model.
4 Experimental Setup
We describe here the experimental setup on the En-
glish lexical sample task and all-words task. Note
that we do not distinguish the two all-words tasks as
the same parameters will be applied.
For lexical sample task, we use 5-fold cross val-
idation on the training data provided to determine
our parameters. For all-words task, we use SemCor
as our training data and validate on Senseval-2 and
Senseval-3 all-words test data.
We use MXPOST tagger (Adwait, 1996) for POS
tagging, Charniak parser (Charniak, 2000) for ex-
tracting syntactic relations, and David Blei?s version
of LDA1 for LDA training and inference. All default
parameters are used unless mentioned otherwise.
For the all-word tasks, we use sense 1 as back-off
for words that have not appeared in SemCor. We use
the same fine-grained system for both the coarse and
fine-grained all-words tasks. We make predictions
1http://www.cs.princeton.edu/?blei/lda-c/
251
for all words for all the systems - precision, recall
and accuracy scores are all the same.
Baseline features For lexical sample task, we
choose P = 3 and G = 3. For all-words task, we
choose P = 3 and G = 1. (G = 1 means only the
nearest word prior and after the test word.)
Smoothing For all standard baseline features, we
use Laplace smoothing but for the soft tag (equation
(2)), we use a smoothing parameter value of 2 for
all-words task and 0.1 for lexical sample task.
Unlabeled Corpus Selection The unlabeled cor-
pus we select from for LDA training include 20
Newsgroups, Reuters, SemCor, Senseval-2 lexical
sample data, Senseval-3 lexical sample data and
SemEval-1 lexical sample data. Although the last
four are labeled corpora, we only need the words
from these corpora and thus they can be regarded as
unlabeled too. For lexical sample data, we define the
whole passage for each training and testing instance
as one document.
For lexical sample task, we use all the unlabeled
corpus mentioned with K = 60 and L = 18. For
all-words task, we use a corpora consisting only 20
Newsgroups and SemCor with K = 40 and L = 14.
Validation Result Table 2 shows the results we
get on the validation sets. We give both the system
accuracy (named as Soft Tag) and the na??ve Bayes
result with only standard features as baseline.
Validation Set Soft Tag NB baseline
SE-2 All-words 66.3 63.7
SE-3 All-words 66.1 64.6
Lexical Sample 89.3 87.9
Table 2: Validation set results (best configuration).
5 Official Results
We now present the official results on all three tasks
we participated in, summarized in Table 3.
The system ranked first, fourth and second in
the lexical sample task, fine-grained all-words task
and coarse-grained all-words task respectively. For
coarse-grained all-words task, we obtained 86.1,
88.3, 81.4, 76.7 and 79.1 for each document, from
d001 to d005.
Task Precision/Recall
Lexical sample(Task 17) 88.7
Fine-grained all-words(Task 17) 57.6
Course-grained all-words(Task 7) 81.6
Table 3: Official Results
5.1 Analysis of Results
For the lexical sample task, we compare the re-
sults to that of our na??ve Bayes baseline and Sup-
port Vector Machine (SVM) (Vapnik, 1995) base-
line. Our SVM classifier (using SVMlight) follows
that of (Lee and Ng, 2002), which ranked the third
in Senseval-3 English lexical sample task. We also
analyse the result according to the test instance?s
part-of-speech and find that the improvements are
consistent for both noun and verb.
System Noun Verb Total
Soft Tag 92.7 84.2 88.7
NB baseline 91.7 83.5 87.8
SVM baseline 91.6 83.1 87.6
Table 4: Analysis on different POS on English lexi-
cal sample task
Our coarse-grained all-words task result outper-
formed the first sense baseline score of 0.7889 by
about 2.7%.
References
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP.
D. M. Blei and A. Y. Ng and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research.
A. Ratnaparkhi 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP.
E. Charniak 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
V. N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
252
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 400?409,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Natural Language Generation with Tree Conditional Random Fields
Wei Lu
1
, Hwee Tou Ng
1,2
, Wee Sun Lee
1,2
1
Singapore-MIT Alliance
2
Department of Computer Science
National University of Singapore
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
Abstract
This paper presents an effective method
for generating natural language sentences
from their underlying meaning represen-
tations. The method is built on top of
a hybrid tree representation that jointly
encodes both the meaning representation
as well as the natural language in a tree
structure. By using a tree conditional
random field on top of the hybrid tree
representation, we are able to explicitly
model phrase-level dependencies amongst
neighboring natural language phrases and
meaning representation components in a
simple and natural way. We show that
the additional dependencies captured by
the tree conditional random field allows it
to perform better than directly inverting a
previously developed hybrid tree semantic
parser. Furthermore, we demonstrate that
the model performs better than a previ-
ous state-of-the-art natural language gen-
eration model. Experiments are performed
on two benchmark corpora with standard
automatic evaluation metrics.
1 Introduction
One of the ultimate goals in the field of natural lan-
guage processing (NLP) is to enable computers to
converse with humans through human languages.
To achieve this goal, two important issues need
to be studied. First, it is important for comput-
ers to capture the meaning of a natural language
sentence in a meaning representation. Second,
computers should be able to produce a human-
understandable natural language sentence from its
meaning representation. These two tasks are re-
ferred to as semantic parsing and natural language
generation (NLG), respectively.
In this paper, we use corpus-based statistical
methods for constructing a natural language gener-
ation system. Given a set of pairs, where each pair
consists of a natural language (NL) sentence and
its formal meaning representation (MR), a learn-
ing method induces an algorithm that can be used
for performing language generation from other
previously unseen meaning representations.
A crucial question in any natural language pro-
cessing system is the representation used. Mean-
ing representations can be in the form of a tree
structure. In Lu et al (2008), we introduced a
hybrid tree framework together with a probabilis-
tic generative model to tackle semantic parsing,
where tree structured meaning representations are
used. The hybrid tree gives a natural joint tree rep-
resentation of a natural language sentence and its
meaning representation.
A joint generative model for natural language
and its meaning representation, such as that used
in Lu et al (2008) has several advantages over var-
ious previous approaches designed for semantic
parsing. First, unlike most previous approaches,
the generative approach models a simultaneous
generation process for both NL and MR. One el-
egant property of such a joint generative model
is that it allows the modeling of both semantic
parsing and natural language generation within the
same process. Second, the generative process pro-
ceeds as a recursive top-down Markov process in
a way that takes advantage of the tree structure
of the MR. The hybrid tree generative model pro-
posed in Lu et al (2008) was shown to give state-
of-the-art accuracy in semantic parsing on bench-
mark corpora.
While semantic parsing with hybrid trees has
been studied in Lu et al (2008), its inverse task
? NLG with hybrid trees ? has not yet been ex-
plored. We believe that the properties that make
the hybrid trees effective for semantic parsing also
make them effective for NLG. In this paper, we de-
velop systems for the generation task by building
400
on top of the generative model introduced in Lu et
al. (2008) (referred to as the LNLZ08 system).
We first present a baseline model by directly
?inverting? the LNLZ08 system, where an NL sen-
tence is generated word by word. We call this
model the direct inversion model. This model is
unable to model some long range global depen-
dencies over the entire NL sentence to be gener-
ated. To tackle several weaknesses exhibited by
the baseline model, we next introduce an alterna-
tive, novel model that performs generation at the
phrase level. Motivated by conditional random
fields (CRF) (Lafferty et al, 2001), a different pa-
rameterization of the conditional probability of the
hybrid tree that enables the model to encode some
longer range dependencies amongst phrases and
MRs is used. This novel model is referred to as
the tree CRF-based model.
Evaluation results for both models are pre-
sented, through which we demonstrate that the tree
CRF-based model performs better than the direct
inversion model. We also compare the tree CRF-
based model against the previous state-of-the-art
model of Wong and Mooney (2007). Further-
more, we evaluate our model on a dataset anno-
tated with several natural languages other than En-
glish (Japanese, Spanish, and Turkish). Evalua-
tion results show that our proposed tree CRF-based
model outperforms the previous model.
2 Related Work
There have been substantial earlier research ef-
forts on investigating methods for transforming
MR to their corresponding NL sentences. Most
of the recent systems tackled the problem through
the architecture of chart generation introduced by
Kay (1996). Examples of such systems include
the chart generator for Head-Driven Phrase Struc-
ture Grammar (HPSG) (Carroll et al, 1999; Car-
roll and Oepen, 2005; Nakanishi et al, 2005), and
more recently for Combinatory Categorial Gram-
mar (CCG) (White and Baldridge, 2003; White,
2004). However, most of these systems only fo-
cused on surface realization (inflection and order-
ing of NL words) and ignored lexical selection
(learning the mappings from MR domain concepts
to NL words).
The recent work by Wong and Mooney (2007)
explored methods for generation by inverting a
system originally designed for semantic pars-
ing. They introduced a system named WASP
?1
that employed techniques from statistical ma-
chine translation using Synchronous Context-Free
Grammar (SCFG) (Aho and Ullman, 1972). The
system took in a linearized MR tree as input, and
translated it into a natural language sentence as
output. Unlike most previous systems, their sys-
tem integrated both lexical selection and surface
realization in a single framework. The perfor-
mance of the system was enhanced by incorpo-
rating models borrowed from PHARAOH (Koehn,
2004). Experiments show that this new hybrid
system named WASP
?1
++ gives state-of-the-art
accuracies and outperforms the direct translation
model obtained from PHARAOH, when evaluated
on two corpora. We will compare our system?s
performance against that of WASP
?1
++ in Sec-
tion 5.
3 The Hybrid Tree Framework and the
LNLZ08 System
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : river(all) RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
what is the longest river that
does not run through texas
Figure 1: An example MR paired with its NL sen-
tence.
Following most previous works in this
area (Kate et al, 2005; Ge and Mooney, 2005;
Kate and Mooney, 2006; Wong and Mooney,
2006; Lu et al, 2008), we consider MRs in the
form of tree structures. An example MR and
its corresponding natural language sentence are
shown in Figure 1. The MR is a tree consisting
of nodes called MR productions. For example,
the node ?QUERY : answer(RIVER)? is one MR
production. Each MR production consists of a
semantic category (?QUERY?), a function symbol
(?answer?) which can be optionally omitted, as
well as an argument list which possibly contains
401
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
texas
run through
that does notRIVER : river(all)
river
the longest
what is
Figure 2: One possible hybrid tree T
1
child semantic categories (?RIVER?).
Now we give a brief overview of the hybrid tree
framework and the LNLZ08 system that was pre-
sented in Lu et al (2008). The training corpus re-
quired by the LNLZ08 system contains example
pairs d
(i)
= (
?
m
(i)
,
?
w
(i)
) for i = 1 . . . N , where
each
?
m
(i)
is an MR, and each
?
w
(i)
is an NL sen-
tence. The system makes the assumption that the
entire training corpus is generated from an under-
lying generative model, which is specified by the
parameter set ?.
The parameter set ? includes the following: the
MR model parameter ?(m
j
|m
i
, arg
k
) which mod-
els the generation of an MR production m
j
from
its parent MR production m
i
as its k-th child, the
emission parameter ?(t|m
i
,?) that is responsible
for generation of an NL word or a semantic cate-
gory t from the MR production m
i
(the parent of
t) under the context ? (such as the token to the left
of the current token), and the pattern parameter
?(r|m
i
), which models the selection of a hybrid
pattern r that defines globally how the NL words
and semantic categories are interleaved given a
parent MR production m
i
. All these parameters
are estimated from the corpus during the training
phase. The list of possible hybrid patterns is given
in Table 1 (at most two child semantic categories
are allowed ? MR productions with more child se-
mantic categories are transformed into those with
two).
In the table, m refers to the MR production, the
symbol w denotes an NL word sequence and is
optional if it appears inside []. The symbol Y and
Z refer to the first and second semantic category
under the MR production m respectively.
# RHS Hybrid Pattern # Patterns
0 m ? w 1
1 m ? [w]Y[w] 4
2
m ? [w]Y[w]Z[w] 8
m ? [w]Z[w]Y[w] 8
Table 1: The list of possible hybrid patterns, [] de-
notes optional
The generative process recursively creates MR
productions as well as NL words at each gen-
eration step in a top-down manner. This pro-
cess results in a hybrid tree for each MR-NL
pair. The list of children under each MR pro-
duction in the hybrid tree forms a hybrid se-
quence. One example hybrid tree for the MR-
NL pair given in Figure 1 is shown in Figure 2.
In this hybrid tree T
1
, the list of children under
the production RIVER : longest(RIVER) forms
the hybrid sequence ?the longest RIVER :
exclude(RIVER
1
RIVER
2
)?. The yield of the hy-
brid tree is exactly the NL sentence. The MR can
also be recovered from the hybrid tree by record-
ing all the internal nodes of the tree, subject to the
reordering operation required by the hybrid pat-
tern.
To illustrate, consider the generation of the hy-
brid tree T
1
shown in Figure 2. The model first
generates an MR production from its parent MR
production (empty as the MR production is the
root in the MR). Next, it selects a hybrid pattern
m ? wY from the predefined list of hybrid pat-
terns, which puts a constraint on the set of all al-
lowable hybrid sequences that can be generated:
the hybrid sequence must be an NL word sequence
402
followed by a semantic category. Finally, actual
NL words and semantic categories are generated
from the parent MR production. Now the genera-
tion for one level is complete, and the above pro-
cess repeats at the newly generated MR produc-
tions, until the complete NL sentence and MR are
both generated.
Mathematically, the above generative process
yields the following formula that models the joint
probability for the MR-NL pair, assuming the con-
text ? for the emission parameter is the preceding
word or semantic category (i.e., the bigram model
is assumed, as discussed in Lu et al (2008)):
p
(
T
1
(
?
w,
?
m)
)
= ?(QUERY : answer(RIVER)|?, arg
1
)
??(m ? wY|QUERY : answer(RIVER))
??(what|QUERY : answer(RIVER),BEGIN)
??(is|QUERY : answer(RIVER),what)
??(RIVER|QUERY : answer(RIVER),is)
??(END|QUERY : answer(RIVER),RIVER)
??(RIVER : longest(RIVER)|
QUERY : answer(RIVER), arg
1
)? . . . (1)
where T
1
(
?
w,
?
m) denotes the hybrid tree T
1
which
contains the NL sentence
?
w and MR
?
m.
For each MR-NL pair in the training set, there
can be potentially many possible hybrid trees asso-
ciated with the pair. However, the correct hybrid
tree is completely unknown during training. The
correct hybrid tree is therefore treated as a hidden
variable. An efficient inside-outside style algo-
rithm (Baker, 1979) coupled with further dynamic
programming techniques is used for efficient pa-
rameter estimation.
During the testing phase, the system makes use
of the learned model parameters to determine the
most probable hybrid tree given a new natural lan-
guage sentence. The MR contained in that hybrid
tree is the output of the system. Dynamic pro-
gramming techniques similar to those of training
are also employed for efficient decoding.
The generative model used in the LNLZ08 sys-
tem has a natural symmetry, allowing for easy
transformation from NL to MR, as well as from
MR to NL. This provides the starting point for our
work in ?inverting? the LNLZ08 system to gener-
ate natural language sentences from the underly-
ing meaning representations.
4 Generation with Hybrid Trees
The task of generating NL sentences from MRs
can be defined as follows. Given a training cor-
pus consisting of MRs paired with their NL sen-
tences, one needs to develop algorithms that learn
how to effectively ?paraphrase? MRs with natu-
ral language sentences. During testing, the sys-
tem should be able to output the most probable NL
?paraphrase? for a given new MR.
The LNLZ08 system models p(T (
?
w,
?
m)), the
joint generative process for the hybrid tree con-
taining both NL and MR. This term can be rewrit-
ten in the following way:
p(T (
?
w,
?
m)) = p(
?
m)? p (T (
?
w,
?
m)|
?
m) (2)
In other words, we reach an alternative view of
the joint generative process as follows. We choose
to generate the complete MR
?
m first. Given
?
m, we
generate hybrid sequences below each of its MR
production, which gives us a complete hybrid tree
T (
?
w,
?
m). The NL sentence
?
w can be constructed
from this hybrid tree exactly.
We define an operation yield(T ) which returns
the NL sentence as the yield of the hybrid tree T .
Given an MR
?
m, we find the most probable NL
sentence
?
w
?
as follows:
?
w
?
= yield
(
argmax
T
p(T |
?
m)
)
(3)
In other words, we first find the most probable
hybrid tree T that contains the provided MR
?
m.
Next we return the yield of T as the most probable
NL sentence.
Different assumptions can be made in the pro-
cess of finding the most probable hybrid tree. We
first describe a simple model which is a direct in-
version of the LNLZ08 system. This model, as a
baseline model, generates a complete NL sentence
word by word. Next, a more sophisticated model
that exploits NL phrase-level dependencies is built
that tackles some weaknesses of the simple base-
line model.
4.1 Direct Inversion Model
Assume that a pre-order traversal of the
MR
?
m gives us the list of MR productions
m
1
,m
2
, . . . ,m
S
, where S is the number of MR
productions in
?
m. Based on the independence
assumption made by the LNLZ08 system, each
MR production independently generates a hybrid
403
sequence. Denote the hybrid sequence gener-
ated under the MR production m
s
as h
s
, for
s = 1, . . . , S. We call the list of hybrid sequences
h = ?h
1
, h
2
, . . . , h
S
? a hybrid sequence list
associated with this particular MR. Thus, our goal
is to find the optimal hybrid sequence list h
?
for
the given MR
?
m, which is formulated as follows:
h
?
= ?h
?
1
, . . . , h
?
S
? = argmax
h
1
,...,h
S
S
?
s=1
p(h
s
|m
s
) (4)
The optimal hybrid sequence list defines the op-
timal hybrid tree whose yield gives the optimal NL
sentence.
Due to the strong independence assumption in-
troduced by the LNLZ08 system, the hybrid tree
generation process is in fact highly decompos-
able. Optimization of the hybrid sequence list
?h
1
, . . . , h
S
? can be performed individually since
they are independent of one another. Thus, math-
ematically, for s = 1, . . . , S, we have:
h
?
s
= argmax
h
s
p(h
s
|m
s
) (5)
The LNLZ08 system presented three models for
the task of transforming NL to MR. In this in-
verse task, for generation of a hybrid sequence,
we choose to use the bigram model (model II). We
choose this model mainly due to its stronger abil-
ity in modeling dependencies between adjacent
NL words, which we believe to be quite important
in this NL generation task. With the bigram model
assumption, the optimal hybrid sequence that can
be generated from each MR production is defined
as follows:
h
?
s
= argmax
h
s
p(h
s
|m
s
)
= argmax
h
s
{
?(r|m
s
)?
|h
s
|+1
?
j=1
?(t
j
|m
s
, t
j?1
)
}
(6)
where t
i
is either an NL word or a semantic cat-
egory with t
0
? BEGIN and t
|h
s
|+1
? END, and
r is the hybrid pattern that matches the hybrid se-
quence h
s
, which is equivalent to t
1
, . . . , t
|h
s
|
.
Equivalently, we can view the problem in the
log-space:
h
?
s
= argmin
h
s
{
? log ?(r|m
s
)
+
|h
s
|+1
?
j=1
? log ?(t
j
|m
s
, t
j?1
)
}
(7)
Note the term ? log ?(r|m
s
) is a constant for
a particular MR production m
s
and a particu-
lar hybrid pattern r. This search problem can
be equivalently cast as the shortest path problem
which can be solved efficiently with Dijkstra?s al-
gorithm (Cormen et al, 2001). We define a set
of states. Each state represents a single NL word
or a semantic category, including the special sym-
bols BEGIN and END. A directed path between
two different states t
u
and t
v
is associated with
a distance measure ? log ?(t
v
|m
s
, t
u
), which is
non-negative. The task now is to find the short-
est path between BEGIN and END
1
. The sequence
of words appearing in this path is simply the most
probable hybrid sequence under this MR produc-
tion m
s
. We build this model by directly inverting
the LNLZ08 system, and this model is therefore
referred to as the direct inversion model.
A major weakness of this baseline model is that
it encodes strong independence assumptions dur-
ing the hybrid tree generation process. Though
shown to be effective in the task of transform-
ing NL to MR, such independence assumptions
may introduce difficulties in this NLG task. For
example, consider the MR shown in Figure 1.
The generation steps of the hybrid sequences
from the two adjacent MR productions QUERY :
answer(RIVER) and RIVER : longest(RIVER)
are completely independent of each other. This
may harm the fluency of the generated NL sen-
tence, especially when a transition from one hy-
brid sequence to another is required. In fact, due
to such an independence assumption, the model
always generates the same hybrid sequence from
the same MR production, regardless of its context
such as parent or child MR productions. Such a
limitation points to the importance of better uti-
lizing the tree structure of the MR for this NLG
task. Furthermore, due to the bigram assumption,
the model is unable to capture longer range depen-
dencies amongst the words or semantic categories
in each hybrid sequence.
To tackle the above issues, we explore ways of
relaxing various assumptions, which leads to an
1
In addition, we should make sure that the generated hy-
brid sequence t
0
. . . t
|h
s
|+1
is a valid hybrid sequence that
comply with the hybrid pattern r. For example, the MR
production STATE : loc 1(RIVER) can generate the follow-
ing hybrid sequence ?BEGIN have RIVER END? but not
this hybrid sequence ?BEGIN have END?. This can be
achieved by finding the shortest path from BEGIN to RIVER,
which then gets concatenated to the shortest path from RIVER
to END.
404
QUERY : answer(RIVER)
RIVER : longest(RIVER)
RIVER : exclude(RIVER
1
RIVER
2
)
RIVER : river(all) RIVER : traverse(STATE)
STATE : stateid(STATENAME)
STATENAME : texas
what is RIVER
1
the longest RIVER
1
RIVER
1
that does not RIVER
2
river run through STATE
1
STATENAME
1
texas
Figure 3: An MR (left) and its associated hybrid sequences (right)
alternative model as discussed next.
4.2 Tree CRF-Based Model
Based on the belief that using known phrases usu-
ally leads to better fluency in the NLG task (Wong
and Mooney, 2007), we explore methods for gen-
erating an NL sentence at phrase level rather than
at word level. This is done by generating hybrid
sequences as complete objects, rather than sequen-
tially one word or semantic category at a time,
from MR productions.
We assume that each MR production can gen-
erate a complete hybrid sequence below it from a
finite set of possible hybrid sequences. Each such
hybrid sequence is called a candidate hybrid se-
quence associated with that particular MR produc-
tion. Given a set of candidate hybrid sequences as-
sociated with each MR production, the generation
task is to find the optimal hybrid sequence list h
?
for a given MR
?
m:
h
?
= argmax
h
p(h|
?
m) (8)
Figure 3 shows a complete MR, as well as a
possible tree that contains hybrid sequences as-
sociated with the MR productions. For exam-
ple, in the figure the MR production RIVER :
traverse(STATE) is associated with the hybrid se-
quence run through STATE
1
. Each MR pro-
duction can be associated with potentially many
different hybrid sequences. The task is to deter-
mine the most probable list of hybrid sequences as
the ones appearing on the right of Figure 3, one for
each MR production.
To make better use of the tree structure of MR,
we take the approach of modeling the conditional
distribution using a log-linear model. Following
the conditional random fields (CRF) framework
(Lafferty et al, 2001), we can define the probabil-
ity of the hybrid sequence list given the complete
MR
?
m, as follows:
p(h|
?
m) =
1
Z(
?
m)
exp
(
?
i?V
?
k
?
k
g
k
(h
i
,
?
m, i)
+
?
(i,j)?E
?
k
?
k
f
k
(h
i
, h
j
,
?
m, i, j)
)
(9)
where V is the set of all the vertices in the tree, and
E is the set of the edges in the tree, consisting of
parent-child pairs. The function Z(
?
m) is the nor-
malization function. Note that the dependencies
among the features here form a tree, unlike the se-
quence models used in Lafferty et al (2001). The
function f
k
(h
i
, h
j
,
?
m, i, j) is a feature function of
the entire MR tree
?
m and the hybrid sequences at
vertex i and j. These features are usually referred
to as the edge features in the CRF framework. The
function g
k
(h
i
,
?
m, i) is a feature function of the
hybrid sequence at vertex i and the entire MR tree.
These features are usually referred to as the vertex
features. The parameters ?
k
and ?
k
are learned
from the training data.
In this task, we are given only MR-NL pairs
and do not have the hybrid tree corresponding to
each MR as training data. Now we describe how
the set of candidate hybrid sequences for each MR
production is obtained as well as how the train-
ing data for this model is constructed. After the
joint generative model is learned as done in Lu et
al. (2008), we first use a Viterbi algorithm to find
the optimal hybrid tree for each MR-NL pair in
the training set. From each optimal hybrid tree,
we extract the hybrid sequence h
i
below each MR
production m
i
. Using this process on the train-
ing MR-NL pairs, we can obtain a set of candidate
405
hybrid sequences that can be associated with each
MR production. The optimal hybrid tree generated
by the Viterbi algorithm in this way is considered
the ?correct? hybrid tree for theMR-NL pair and is
used as training data. While this does not provide
hand-labeled training data, we believe the hybrid
trees generated this way form a high quality train-
ing set as both the MR and NL are available when
Viterbi decoding is performed, guaranteeing that
the generated hybrid tree has the correct yield.
There exist several advantages of such a model
over the simple generative model. First, this model
allows features that specifically model the depen-
dencies between neighboring hybrid sequences in
the tree to be used. In addition, the model can effi-
ciently capture long range dependencies between
MR productions and hybrid sequences since each
hybrid sequence is allowed to depend on the entire
MR tree.
For features, we employ four types of simple
features, as presented below. Note that the first
three types of features are vertex features, and the
last are edge features. Examples are given based
on Figure 3. All the features are indicator func-
tions, i.e., a feature takes value 1 if a certain com-
bination is present, and 0 otherwise. The last three
features explicitly encode information from the
tree structure of MR.
Hybrid sequence features : one hybrid sequence
together with the associated MR production.
For example:
g
1
: ?run through STATE
1
,
RIVER : traverse(STATE)? ;
Two-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
and the parent MR production. For example:
g
2
: ?run through STATE
1
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
)? ;
Three-level hybrid sequence features : one hy-
brid sequence, its associated MR production,
the parent MR production, and the grandpar-
ent MR production. For example:
g
3
: ?run through STATE
1
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
),
RIVER : longest(RIVER)? ;
Adjacent hybrid sequence features : two adja-
cent hybrid sequences, together with their as-
sociated MR productions. For example:
f
1
: ?run through STATE
1
,
RIVER
1
that does not RIVER
2
,
RIVER : traverse(STATE),
RIVER : exclude(RIVER
1
,RIVER
2
)? .
For training, we use the feature forest model
(Miyao and Tsujii, 2008), which was originally
designed as an efficient algorithm for solving max-
imum entropy models for data with complex struc-
tures. The model enables efficient training over
packed trees that potentially represent exponen-
tial number of trees. The tree conditional random
fields model can be effectively represented using
the feature forest model. The model has also been
successfully applied to the HPSG parsing task.
To train the model, we run the Viterbi algorithm
on the trained LNLZ08 model and perform convex
optimization using the feature forest model. The
LNLZ08 model is trained using an EM algorithm
with time complexity O(MN
3
D) per EM itera-
tion, where M and N are respectively the maxi-
mum number of MR productions and NL words
for each MR-NL pair, and D is the number of
training instances. The time complexity of the
Viterbi algorithm is also O(MN
3
D). For training
the feature forest, we use the Amis toolkit (Miyao
and Tsujii, 2002) which utilizes the GIS algorithm.
The time complexity for each iteration of the GIS
algorithm is O(MK
2
D), where K is the maxi-
mum number of candidate hybrid sequences asso-
ciated with each MR production. Finally, the time
complexity for generating a natural language sen-
tence from a particular MR is O(MK
2
).
5 Experiments
In this section, we present the results of our sys-
tems when evaluated on two standard benchmark
corpora. The first corpus is GEOQUERY, which
contains Prolog-based MRs that can be used to
query a US geographic database (Kate et al,
2005). Our task for this domain is to generate
NL sentences from the formal queries. The second
corpus is ROBOCUP. This domain contains MRs
which are instructions written in a formal language
called CLANG. Our task for this domain is to gen-
erate NL sentences from the coaching advice writ-
ten in CLANG.
406
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
Direct inversion model 0.3973 5.5466 0.5468 6.6738
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
Table 2: Results of automatic evaluation of both models (bold type indicates the best performing system).
GEOQUERY (880) ROBOCUP (300)
BLEU NIST BLEU NIST
WASP
?1
++ 0.5370 6.4808 0.6022 6.8976
Tree CRF-based model 0.5733 6.7459 0.6220 6.9845
Table 3: Results of automatic evaluation of our tree CRF-based model and WASP
?1
++.
English Japanese Spanish Turkish
BLEU NIST BLEU NIST BLEU NIST BLEU NIST
WASP
?1
++ 0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283
Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033
Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages.
The GEOQUERY domain contains 880 in-
stances, while the ROBOCUP domain contains 300
instances. The average NL sentence length for the
two corpora are 7.57 and 22.52 respectively. Fol-
lowing the evaluation methodology of Wong and
Mooney (2007), we performed 4 runs of the stan-
dard 10-fold cross validation and report the aver-
aged performance in this section using the stan-
dard automatic evaluation metric BLEU (Papineni
et al, 2002) and NIST (Doddington, 2002)
2
. The
BLEU and NIST scores of the WASP
?1
++ sys-
tem reported in this section are obtained from
the published paper of Wong and Mooney (2007).
Note that to make our experimental results directly
comparable to Wong and Mooney (2007), we used
the identical training and test data splits for the 4
runs of 10-fold cross validation used by Wong and
Mooney (2007) on both corpora.
Our system has the advantage of always pro-
ducing an NL sentence given any input MR, even
if there exist unseen MR productions in the input
MR. We can achieve this by simply skipping those
unseen MR productions during the generation pro-
cess. However, in order to make a fair comparison
against WASP
?1
++, which can only generate NL
sentences for 97% of the input MRs, we also do
not generate any NL sentence in the case of ob-
serving an unseen MR production. All the evalu-
ations discussed in this section follow this evalu-
2
We used the official evaluation script (version 11b) pro-
vided by http://www.nist.gov/.
ation methodology, but we notice that empirically
our system is able to achieve higher BLEU/NIST
scores if we allow generation for those MRs that
include unseen MR productions.
5.1 Comparison between the two models
We compare the performance of our two models
in Table 2. From the table, we observe that the
tree CRF-based model outperforms the direct in-
version model on both domains. This validates
our earlier belief that some long range dependen-
cies are important for the generation task. In ad-
dition, while the direct inversion model performs
reasonably well on the ROBOCUP domain, it per-
forms substantially worse on the GEOQUERY do-
main where the sentence length is shorter. We note
that the evaluation metrics are strongly correlated
with the cumulative matching n-grams between
the output and the reference sentence (n ranges
from 1 to 4 for BLEU, and 1 to 5 for NIST). The
direct inversion model fails to capture the transi-
tional behavior from one phrase to another, which
makes it more vulnerable to n-gram mismatch, es-
pecially when evaluated on the GEOQUERY cor-
pus where phrase-to-phrase transitions are more
frequent. On the other hand, the tree CRF-based
model does not suffer from this problem, mainly
due to its ability to model such dependencies be-
tween neighboring phrases. Sample outputs from
the two models are shown in Figure 4.
407
Reference: what is the largest state bordering texas
Direct inversion model: what the largest states border texas
Tree CRF-based model: what is the largest state that borders texas
Reference: if DR2C7 is true then players 2 , 3 , 7 and 8
should pass to player 4
Direct inversion model: if DR2C7 , then players 2 , 3 7 and 8 should
ball to player 4
Tree CRF-based model: if the condition DR2C7 is true then players 2 ,
3 , 7 and 8 should pass to player 4
Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain
(bottom) respectively.
5.2 Comparison with previous model
We also compare the performance of our tree CRF-
based model against the previous state-of-the-art
system WASP
?1
++ in Table 3. Our tree CRF-based
model achieves better performance on both cor-
pora. We are unable to carry out statistical sig-
nificance tests since the detailed BLEU and NIST
scores of the cross validation runs of WASP
?1
++
as reported in the published paper of Wong and
Mooney (2007) are not available.
The results confirm our earlier discussions: the
dependencies between the generated NL words
are important and need to be properly modeled.
The WASP
?1
++ system uses a log-linear model
which incorporates two major techniques to at-
tempt to model such dependencies. First, a back-
off language model is used to capture dependen-
cies at adjacent word level. Second, a technique
that merges smaller translation rules into a single
rigid rule is used to capture dependencies at phrase
level (Wong, 2007). In contrast, the proposed tree
CRF-based model is able to explicitly and flexibly
exploit phrase-level features that model dependen-
cies between adjacent phrases. In fact, with the
hybrid tree framework, the better treatment of the
tree structure of MR enables us to model some cru-
cial dependencies between the complete MR tree
and generated NL phrases. We believe that this
property plays an important role in improving the
quality of the generated sentences in terms of flu-
ency, which is assessed by the evaluation metrics.
Furthermore, WASP
?1
++ employs minimum
error rate training (Och, 2003) to directly optimize
the evaluation metrics. We have not done so but
still obtain better performance. In future, we plan
to explore ways to directly optimize the evaluation
metrics in our system.
5.3 Experiments on different languages
Following the work of Wong and Mooney (2007),
we also evaluated our system?s performance on
a subset of the GEOQUERY corpus with 250 in-
stances, where sentences of 4 natural languages
(English, Japanese, Spanish, and Turkish) are
available. The evaluation results are shown in Ta-
ble 4. Our tree CRF-based model achieves better
performance on this task compared to WASP
?1
++.
We are again unable to conduct statistical signifi-
cance tests for the same reason reported earlier.
6 Conclusions
In this paper, we presented two novel models for
the task of generating natural language sentences
from given meaning representations, under a hy-
brid tree framework. We first built a simple di-
rect inversion model as a baseline. Next, to ad-
dress the limitations associated with the direct in-
version model, a tree CRF-based model was in-
troduced. We evaluated both models on standard
benchmark corpora. Evaluation results show that
the tree CRF-based model performs better than the
direct inversion model, and that the tree CRF-based
model also outperforms WASP
?1
++, which was a
previous state-of-the-art system reported in the lit-
erature.
Acknowledgments
The authors would like to thank Seung-Hoon Na
for his suggestions on the presentation of this pa-
per, Yuk Wah Wong for answering various ques-
tions related to the WASP
?1
++ system, and the
anonymous reviewers for their thoughtful com-
ments on this work.
408
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Clis, NJ.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547?550, Boston, MA, June.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP 2005), pages 165?176.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation (EWNLG 1999), pages 86?95.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms (Second Edition). MIT Press.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology Research (HLT 2002), pages 138?145.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the 9th Conference on
Computational Natural Language Learning (CoNLL
2005), pages 9?16.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL 2006), pages 913?920.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the 20th National
Conference on Artificial Intelligence (AAAI 2005),
pages 1062?1068.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL 1996), pages 200?
204.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas (AMTA 2004), pages 115?124.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 783?792.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the 2nd International Conference on Human
Language Technology Research (HLT 2002), pages
292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technologies
(IWPT 2005), volume 5, pages 93?102.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2002), pages 311?318.
Michael White and Jason Baldridge. 2003. Adapting
chart realization to CCG. In Proceedings of the 9th
European Workshop on Natural Language Genera-
tion (EWNLG 2003), pages 119?126.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proceeding of the 3rd International Confer-
ence on Natural Language Generation (INLG 2004),
pages 182?191.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL 2006), pages 439?446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Generation by inverting a semantic parser that uses
statistical machine translation. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics (NAACL/HLT 2007),
pages 172?179.
Yuk Wah Wong. 2007. Learning for Semantic Parsing
and Natural Language Generation Using Statistical
Machine Translation Techniques. Ph.D. thesis, The
University of Texas at Austin.
409
