Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 120?127,
New York, June 2006. c?2006 Association for Computational Linguistics
An Empirical Study of the Behavior of Active Learning for Word Sense 
Disambiguation 
1 Jinying Chen, 1 Andrew Schein, 1 Lyle Ungar, 2 Martha Palmer 
1 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
{jinying,ais,ungar}@cis.upenn.edu 
2 Linguistic Department  
University of Colorado 
Boulder, CO, 80309 
Martha.Palmer@colorado.edu 
Abstract 
This paper shows that two uncertainty-
based active learning methods, combined 
with a maximum entropy model, work 
well on learning English verb senses. 
Data analysis on the learning process, 
based on both instance and feature levels, 
suggests that a careful treatment of feature 
extraction is important for the active 
learning to be useful for WSD. The 
overfitting phenomena that occurred 
during the active learning process are 
identified as classic overfitting in machine 
learning based on the data analysis. 
1 Introduction 
Corpus-based methods for word sense 
disambiguation (WSD) have gained popularity in 
recent years. As evidenced by the SENSEVAL 
exercises (http://www.senseval.org), machine 
learning models supervised by sense-tagged 
training corpora tend to perform better on the 
lexical sample tasks than unsupervised methods. 
However, WSD tasks typically have very limited 
amounts of training data due to the fact that 
creating large-scale high-quality sense-tagged 
corpora is difficult and time-consuming. Therefore, 
the lack of sufficient labeled training data has 
become a major hurdle to improving the 
performance of supervised WSD.  
A promising method for solving this problem 
could be the use of active learning. Researchers 
use active learning methods to minimize the 
labeling of examples by human annotators. A 
decrease in overall labeling occurs because active 
learners (the machine learning models used in 
active learning) pick more informative examples 
for the target word (a word whose senses need to 
be learned) than those that would be picked 
randomly. Active learning requires human labeling 
of the newly selected training data to ensure high 
quality. 
We focus here on pool-based active learning 
where there is an abundant supply of unlabeled 
data, but where the labeling process is expensive.  
In NLP problems such as text classification (Lewis 
and Gale, 1994; McCallum and Nigam, 1998), 
statistical parsing (Tang et al, 2002), information 
extraction (Thompson et al, 1999), and named 
entity recognition (Shen et al, 2004), pool-based 
active learning has produced promising results.  
This paper presents our experiments in applying 
two active learning methods, a min-margin based 
method and a Shannon-entropy based one, to the 
task of the disambiguation of English verb senses. 
The contribution of our work is not only in 
demonstrating that these methods work well for the 
active learning of coarse-grained verb senses, but 
also analyzing the behavior of the active learning 
process on two levels: the instance level and the 
feature level. The analysis suggests that a careful 
treatment of feature design and feature generation 
is important for a successful application of active 
learning to WSD. We also accounted for the 
overfitting phenomena that occurred in the learning 
process based on our data analysis.  
The rest of the paper is organized as follows. In 
Section 2, we introduce two uncertainty sampling 
methods used in our active learning experiments 
and review related work in using active learning 
for WSD. We then present our active learning 
experiments on coarse-grained English verb senses 
in Section 3 and analyze the active learning 
120
process in Section 4. Section 5 presents 
conclusions of our study.        
2 Active Learning Algorithms 
The methods evaluated in this work fit into a 
common framework described by Algorithm 1 (see 
Table 1). The key difference between alternative 
active learning methods is how they assess the 
value of labeling individual examples, i.e., the 
methods they use for ranking and selecting the 
candidate examples for labeling. The framework is 
wide open to the type of ranking rule employed. 
Usually, the ranking rule incorporates the model 
trained on the currently labeled data.  This is the 
reason for the requirement of a partial training set 
when the algorithm begins. 
                                Algorithm 1 
Require: initial training set, pool of unlabeled examples 
  Repeat 
Select T random examples from pool 
      Rank T examples according to active learning rule 
     Present the top-ranked example to oracle for labeling 
     Augment the training set with the new example 
  Until Training set reaches desirable size 
Table 1. A Generalized Active Learning Loop 
 
In our experiments we look at two variants of 
the uncertainty sampling heuristic: entropy 
sampling and margin sampling. Uncertainty 
sampling is a term invented by Lewis and Gale 
(Lewis and Gale, 1994) to describe a heuristic 
where a probabilistic classifier picks examples for 
which the model?s current predictions are least 
certain. The intuitive justification for this approach 
is that regions where the model is uncertain 
indicate a decision boundary, and clarifying the 
position of decision boundaries is the goal of 
learning classifiers. Schein (2005) demonstrates 
the two methods run quickly and compete 
favorably against alternatives when combined with 
the logistic regression classifier. 
2.1 Entropy Sampling 
A key question is how to measure uncertainty.  
Different methods of measuring uncertainty will 
lead to different variants of uncertainty sampling.  
We will look at two such measures.  As a 
convenient notation we use q (a vector) to 
represent the trained model?s predictions, with cq  
equal to the predicted probability of class c .  One 
method is to pick the example whose prediction 
vector q displays the greatest Shannon entropy: 
??
c
cc qq log    (1) 
Such a rule means ranking candidate examples 
in Algorithm 1 by Equation 1.  
2.2 Margin Sampling 
An alternative method picks the example with the 
smallest margin: the difference between the largest 
two values in the vector q (Abe and Mamitsuka, 
1998). In other words, if c and 'c are the two most 
likely categories for example nx , the margin is 
measured as follows: 
)|'Pr()|Pr( nnn xcxcM ?=   (2) 
In this case Algorithm 1 would rank examples 
by increasing values of margin, with the smallest 
value at the top of the ranking. 
Using either method of uncertainty sampling, 
the computational cost of picking an example from 
T candidates is: O(TD) where D is the number of 
model parameters.   
2.3 Related Work 
To our best knowledge, there have been very few 
attempts to apply active learning to WSD in the 
literature (Fujii and Inui, 1999; Chklovski and 
Mihalcea, 2002; Dang, 2004). Fujii and Inui (1999) 
developed an example sampling method for their 
example-based WSD system in the active learning 
of verb senses in a pool-based setting. Unlike the 
uncertainty sampling methods (such as the two 
methods we used), their method did not select 
examples for which the system had the minimal 
certainty. Rather, it selected the examples such that 
after training using those examples the system 
would be most certain about its predictions on the 
rest of the unlabeled examples in the next iteration. 
This sample selection criterion was enforced by 
calculating a training utility function. The method 
performed well on the active learning of Japanese 
verb senses. However, the efficient computation of 
the training utility function relied on the nature of 
the example-based learning method, which made 
their example sampling method difficult to export 
to other types of machine learning models. 
Open Mind Word Expert (Chklovski and 
Mihalcea, 2002) was a real application of active 
learning for WSD. It collected sense-annotated 
examples from the general public through the Web 
to create the training data for the SENSEVAL-3 
lexical sample tasks. The system used the 
121
disagreement of two classifiers (which employed 
different sets of features) on sense labels to 
evaluate the difficulty of the unlabeled examples 
and ask the web users to tag the difficult examples 
it selected. There was no formal evaluation for this 
active learning system.  
Dang (2004) used an uncertainty sampling 
method to get additional training data for her WSD 
system. At each iteration the system selected a 
small set of examples for which it had the lowest 
confidence and asked the human annotators to tag 
these examples. The experimental results on 5 
English verbs with fine-grained senses (from 
WordNet 1.7) were a little surprising in that active 
learning performed no better than random 
sampling. The proposed explanation was that the 
quality of the manually sense-tagged data was 
limited by an inconsistent or unclear sense 
inventory for the fine-grained senses. 
3 Active Learning Experiments 
3.1 Experimental Setting 
We experimented with the two uncertainty 
sampling methods on 5 English verbs that had 
coarse-grained senses (see Table 2), as described 
below. By using coarse-grained senses, we limit 
the impact of noisy data due to unclear sense 
boundaries and therefore can get a clearer 
observation of the effects of the active learning 
methods themselves.  
verb # of 
sen. 
baseline 
acc. (%) 
Size of data for 
active learning 
Size of 
test data  
Add 3 91.4 400 100 
Do 7 76.9 500 200 
Feel 3 83.6 400 90 
See 7 59.7 500 200 
Work 9 68.3 400 150 
Table 2. The number of senses, the baseline 
accuracy, the number of instances used for active 
learning and for held-out evaluation for each verb 
 
The coarse-grained senses are produced by 
grouping together the original WordNet senses 
using syntactic and semantic criteria (Palmer et al, 
2006). Double-blind tagging is applied to 50 
instances of the target word. If the ITA < 90%, the 
sense entry is revised by adding examples and 
explanations of distinguishing criteria. 
Table 2 summarizes the statistics of the data. 
The baseline accuracy was computed by using the 
?most frequent sense? heuristic to assign sense 
labels to verb instances (examples). The data used 
in active learning (Column 4 in Table 2) include 
two parts: an initial labeled training set and a pool 
of unlabeled training data. We experimented with 
sizes 20, 50 and 100 for the initial training set. The 
pool of unlabeled data had actually been annotated 
in advance, as in most pool-based active learning 
experiments. Each time an example was selected 
from the pool by the active learner, its label was 
returned to the learner. This simulates the process 
of asking human annotators to tag the selected 
unlabeled example at each time. The advantage of 
using such a simulation is that we can experiment 
with different settings (different sizes of the initial 
training set and different sampling methods).  
The data sets used for active learning and for 
held-out evaluation were randomly sampled from a 
large data pool for each round of the active 
learning experiment. We ran ten rounds of the 
experiments for each verb and averaged the 
learning curves for the ten rounds. 
In the experiments, we used random sampling 
(picking up an unlabeled example randomly at 
each time) as a lower bound. Another control 
(ultimate-maxent) was the learner?s performance 
on the test set when it was trained on a set of 
labeled data that were randomly sampled from a 
large data pool and equaled the amount of data 
used in the whole active learning process (e.g., 400 
training data for the verb add).  
The machine learning model we used for active 
learning was a regularized maximum entropy 
(MaxEnt) model (McCallum, 2002). The features 
used for disambiguating the verb senses included 
topical, collocation, syntactic (e.g., the subject, 
object, and preposition phrases taken by a target 
verb), and semantic (e.g., the WordNet synsets and 
hypernyms of the head nouns of a verb?s NP 
arguments) features (Chen and Palmer, 2005). 
3.2 Experimental Results 
Due to space limits, Figure 1 only shows the 
learning curves for 4 verbs do, feel, see, and work 
(size of the initial training set = 20). The curve for 
the verb add is similar to that for feel. These curves 
clearly show that the two uncertainty sampling 
methods, the entropy-based (called entropy-maxent 
in the figure) and the margin-based (called 
min_margin-maxent), work very well for active 
learning of the senses of these verbs. 
 
122
Figure 1 Active learning for four verbs  
Both methods outperformed the random 
sampling method in that they reached the upper-
bound accuracy earlier and had smoother learning 
curves. For the four verbs add, do, feel and see, 
their learning curves reached the upper bound at 
about 200~300 iterations, which means 1/2 or 1/3 
of the annotation effort can be saved for these 
verbs by using active learning, while still achieving 
the same level of performance as supervised WSD 
without using active learning. Given the large-
scale annotation effort currently underway in the 
OntoNotes project (Hovy et al, 2006), this could 
provide considerable savings in annotation effort 
and speed up the process of providing sufficient 
data for a large vocabulary. The OntoNotes project 
has now provided coarse-grained entries for over 
350 verbs, with corresponding double?blind 
annotation and adjudication in progress.  As this 
adjudicated data becomes available, we will be 
able to train our system accordingly. Preliminary 
results for 22 of these coarse-grained verbs (with 
an average grouping polysemy of 4.5) give us an 
average accuracy of 86.3%. This will also provide 
opportunities for more experiments with active 
learning, where there are enough instances.  Active 
learning could also be beneficial in porting these 
supervised taggers to new genres with different 
sense distributions. 
We also experimented with different sizes of 
the initial training set (20, 50 and 100) and found 
no significant differences in the performance at 
different settings. That means, for these 5 verbs, 
only 20 labeled training instances will be enough 
to initiate an efficient active learning process.        
From Figure 1, we can see that the two 
uncertainty sampling methods generally perform 
equally well except that for the verb do, the min-
margin method is slightly better than the entropy 
method at the beginning of active learning. This 
may not be so surprising, considering that the two 
methods are equal for two-class classification tasks 
(see Equations 1 and 2 for their definition) and the 
verbs used in our experiments have coarse-grained 
senses and often have only 2 or 3 major senses.   
An interesting phenomenon observed from 
these learning curves is that for the two verbs add 
and feel, the active learner reached the upper 
bound very soon (at about 100 iterations) and then 
even breached the upper bound. However, when 
the training set was extended, the learner?s 
performance dropped and eventually returned to 
123
the same level of the upper bound. We discuss the 
phenomenon below.  
4 Analysis of the Learning Process 
In addition to verifying the usefulness of active 
learning for WSD, we are also interested in a 
deeper analysis of the learning process. For 
example, why does the active learner?s 
performance drop sometimes during the learning 
process? What are the characteristics of beneficial 
features that help to boost the learner?s accuracy? 
How do we account for the overfitting phenomena 
that occurred during the active learning for the 
verbs add and feel? We analyzed the effect of both 
instances and features throughout the course of 
active learning using min-margin-based sampling.  
4.1 Instance-level Analysis  
Intuitively, if the learner?s performance drops after 
a new example is added to the training set, it is 
likely that something has gone wrong with the new 
example. To find out such bad examples, we 
define a measure credit_inst for instance i as: 
??
=
+
=
?
m
r
ll
n
l
AccAcclisel
m 1
1
1
)(),(1   (3) 
where Accl and Accl+1 are the classification 
accuracies of the active learner at the lth and 
(l+1)th iterations. n is the total number of 
iterations of active learning and m is the number of 
rounds of active learning (m=10 in our case). 
),( lisel is 1 iff instance i is selected by the active 
learner at the lth iteration and is 0 if otherwise. 
An example is a bad example if and only if it 
satisfies the following conditions: 
a)  its credit_inst value is negative 
b) it increases the learner?s performance, if it 
does, less often than it decreases the 
performance in the 10 rounds. 
We ranked the bad examples by their 
credit_inst values and their frequency of 
decreasing the learner?s performance in the 10 
rounds. Table 3 shows the top five bad examples 
for feel and work. There are several reasons why 
the bad examples may hurt the learner?s 
performance. Column 3 of Table 3 proposes 
reasons for many of our bad examples. We 
categorized these reasons into three major types. 
I. The major senses of a target verb depend 
heavily on the semantic categories of its NP 
arguments but WordNet sometimes fails to provide 
the appropriate semantic categories (features) for 
the head nouns of these NP arguments. For 
example, feel in the board apparently felt no 
pressure has Sense 1 (experience). In Sense 1, feel 
typically takes an animate subject. However, 
board, the head word of the verb?s subject in the 
above sentence has no animate meanings defined 
in WordNet. Even worse, the major meaning of 
board, i.e., artifact, is typical for the subject of feel 
in Sense 2 (touch, grope). Similar semantic type 
mismatches hold for the last four bad examples of 
the verb work in Table 3.  
II. The contexts of the target verb are difficult 
for our feature exaction module to analyze. For 
example, the antecedent for the pronoun subject 
they in the first example of work in Table 3 should 
be ringers, an agent subject that is typical for 
Sense 1 (exert oneself in an activity). However, the 
feature exaction module found the wrong 
antecedent changes that is an unlikely fit for the 
intended verb sense. In the fourth example for feel, 
the feature extraction module cannot handle the 
expletive ?it? (a dummy subject) in ?it was felt 
that?, therefore, it cannot identify the typical 
syntactic pattern for Sense 3 (find, conclude), i.e., 
subject+feel+relative clause. 
III. Sometimes, deep semantic and discourse 
analyses are needed to get the correct meaning of 
the target verb. For example, in the third example 
of feel, ??, he or she feels age creeping up?, it is 
difficult to tell whether the verb has Sense 1 
(experience) or Sense 3 (find) without an 
understanding of the meaning of the relative clause 
and without looking at a broader discourse context. 
The syntactic pattern identified by our feature 
extraction module, subject+feel+relative clause, 
favors Sense 3 (find), which leads to an inaccurate 
interpretation for this case. 
Recall that the motivation behind uncertainty 
samplers is to find examples near decision 
boundaries and use them to clarify the position of 
these boundaries. Active learning often does find 
informative examples, either ones from the less 
common senses or ones close to the boundary 
between the different senses. However, active 
learning also identifies example sentences that are 
difficult to analyze. The failure of our feature 
extraction module, the lack of appropriate semantic 
categories for certain NP arguments in WordNet, 
the lack of deep analysis (semantic and discourse 
analysis) of the context of the target verb can all 
124
         Table 3 Data analysis of the top-ranked bad examples found for two verbs 
produce misleading features. Therefore, in order to 
make active learning useful for its applications, 
both identifying difficult examples and getting 
good features for these examples are equally 
important. In other words, a careful treatment of 
feature design and feature generation is necessary 
for a successful application of active learning. 
There is a positive side to identifying such 
?bad? examples; one can have human annotators 
look at the features generated from the sentences 
(as we did above), and use this to improve the data 
or the classifier. Note that this is exactly what we 
did above: the identification of bad sentences was 
automatic, and they could then be reannotated or 
removed from the training set or the feature 
extraction module needs to be refined to generate 
informative features for these sentences. 
Not all sentences have obvious interpretations; 
hence the two question marks in Table 3. An 
example can be bad for many reasons: conflicting 
features (indicative of different senses), misleading 
features (indicative of non-intended senses), or just 
containing random features that are incorrectly 
incorporated into the model. We will return to this 
point in our discussion of the overfitting 
phenomena for active learning in Section 4.3. 
4.2 Feature-level Analysis 
The purpose of our feature-level analysis is to 
identify informative features for verb senses. The 
learning curve of the active learner may provide 
some clues. The basic idea is, if the learner?s 
performance increases after adding a new example, 
it is likely that the good example contains good 
features that contribute to the clarification of sense 
boundaries. However, the feature-level analysis is 
much less straightforward than the instance-level 
analysis since we cannot simply say the features 
that are active (present) in this good example are 
all good. Rather, an example often contains both 
good and bad features, and many other features 
that are somehow neutral or uninformative. The 
interaction or balance between these features 
determines the final outcome. On the other hand, a 
statistics based analysis may help us to find 
features that tend to be good or bad. For this 
analysis, we define a measure credit_feat for 
feature i as: 
feel Proposed reasons for bad examples Senses 
Some days the coaches make you feel as though you 
are part of a large herd of animals . 
? S1: experience 
And , with no other offers on the table , the board 
apparently felt no pressure to act on it.  
subject: board, no ?animate? meaning in 
WordNet  
S1: experience 
Sometimes a burst of aggressiveness will sweep over a 
man -- or his wife -- because he or she feels age 
creeping up.  
syntactic pattern: sbj+feel+relative clause 
headed by that, a typical pattern for Sense 
3 (find) rather than Sense 1 (experience)  
S1: experience 
At this stage it was felt I was perhaps more pertinent as 
chief. executive . 
syntactic pattern: sbj+feel+relative clause, 
typical for Sense 3 (find) but has not been 
detected by the feature exaction module 
S3: find, conclude
I felt better Tuesday evening when I woke up. ? S1: experience 
Work    
When their changes are completed, and after they have 
worked up a sweat, ringers often ?? 
subject: they, the feature exaction module 
found the wrong antecedent (changes 
rather than ringers) for they 
S1: exert oneself 
in an activity 
Others grab books, records , photo albums , sofas and 
chairs , working frantically in the fear that an 
aftershock will jolt the house again . 
subject: others (means people here), no 
definition in WordNet 
S1: exert oneself 
in an activity 
Security Pacific 's factoring business works with 
companies in the apparel, textile and food industries ?
subject: business, no ?animate? meaning 
in WordNet 
S1: exert oneself 
in an activity 
? ; blacks could work there , but they had to leave at 
night . 
subject: blacks, no ?animate? meaning in 
WordNet 
S1: exert oneself 
in an activity 
? has been replaced by alginates (gelatin-like material 
) that work quickly and accurately and with least 
discomfort to a child . 
subject: alginates, unknown by WordNet S2: perform, 
function, behave 
125
??
=
+
=
?
m
r l
ll
n
l act
AccAccliactive
m 1
1
1
1)(),(1         (4) 
where ),( liactive is 1 iff feature i is active in the 
example selected by the active learner at the lth 
iteration and is 0 if otherwise. actl is the total 
number of active features in the example selected 
at the lth iteration. n and m have the same 
definition as in Equation 3.  
A feature is regarded as good if its credit_feat 
value is positive. We ranked the good features by 
their credit_feat values.  By looking at the top-
ranked good features for the verb work (due to 
space limitations, we omit the table data), we 
identify two types of typically good features.  
The first type of good feature occurs frequently 
in the data and has a frequency distribution over 
the senses similar to the data distribution over the 
senses. Such features include those denoting that 
the target verb takes a subject (subj), is not used in 
a passive mode (morph_normal), does not take a 
direct object (intransitive), occurs in present tense 
(word_work, pos_vb, word_works, pos_vbz), and 
semantic features denoting an abstract subject 
(subjsyn_16993 1) or an entity subject (subjsyn_ 
1742), etc. We call such features background 
features. They help the machine learning model 
learn the appropriate sense distribution of the data. 
In other words, a learning model only using such 
features will be equal to the ?most frequent sense? 
heuristic used in WSD.  
Another type of good feature occurs less 
frequently and has a frequency distribution over 
senses that mismatches with the sense distribution 
of the data. Such features include those denoting 
that the target verb takes an inanimate subject 
(subj_it), takes a particle out (prt_out), is followed 
directly by the word out (word+1_out), or occurs at 
the end of the sentence. Such features are 
indicative of less frequent verb senses  that still 
occur fairly frequently in the data. For example, 
taking an inanimate subject (subj_it) is a strong 
clue for Sense 2 (perform, function, behave) of the 
verb work. Occurring at the end of the sentence is 
also indicative of Sense 2 since when work is used 
in Sense 1 (exert oneself in an activity), it tends to 
take adjuncts to modify the activity as in He is 
working hard to bring up his grade. 
                                                          
1 Those features are from the WordNet. The numbers are 
WordNet ids of synsets and hypernyms. 
There are some features that don?t fall into the 
above two categories, such as the topical feature 
tp_know and the collocation feature pos-2_nn. 
There are no obvious reasons why they are good 
for the learning process, although it is possible that 
the combination of two or more such features 
could make a clear sense distinction. However, this 
hypothesis cannot be verified by our current 
statistics-based analysis. It is also worth noting that 
our current feature analysis is post-experimental 
(i.e., based on the results). In the future, we will try 
automatic feature selection methods that can be 
used in the training phase to select useful features 
and/or their combinations.  
We have similar results for the feature analysis 
of the other four verbs. 
4.3 Account for the Overfitting Phenomena 
Recall that in the instance-level analysis in Section 
4.1, we found that some examples hurt the learning 
performance during active learning but for no 
obvious reasons (the two examples marked by ? in 
Table 3). We found that these two examples 
occurred in the overfitting region for feel. By 
looking at the bad examples (using the same 
definition for bad example as in Section 4.1) that 
occurred in the overfitting region for both feel and 
add, we identified two major properties of these 
examples. First, most of them occurred only once 
as bad examples (19 out 23 for add and 40 out of 
63 for feel). Second, many of the examples had no 
obvious reasons for their badness. 
Based on the above observations, we believe 
that the overfitting phenomena that occurred for 
the two verbs during active learning is typical of 
classic overfitting, which is consistent with a 
"death by a thousand mosquito bites" of rare bad 
features, and consistent with there often being (to 
mix a metaphor) no "smoking gun" of a bad 
feature/instance that is added in, especially in the 
region far away from the starting point of active 
learning. 
5 Conclusions 
We have shown that active learning can lead to 
substantial reductions (often by half) in the number 
of observations that need to be labeled to achieve a 
given accuracy in word sense disambiguation, 
compared to labeling randomly selected instances. 
In a follow-up experiment, we also compared a 
larger number of different active learning methods. 
126
The results suggest that for tasks like word sense 
disambiguation where maximum entropy methods 
are used as the base learning models, the minimum 
margin active criterion for active learning gives 
superior results to more comprehensive 
competitors including bagging and two variants of 
query by committee (Schein, 2005). By also taking 
into account the high running efficiency of the 
min-margin method, it is a very promising active 
learning method for WSD. 
We did an analysis on the learning process on 
two levels: instance-level and feature-level. The 
analysis suggests that a careful treatment of feature 
design and feature generation is very important for 
the active learner to take advantage of the difficult 
examples it finds during the learning process. The 
feature-level analysis identifies some 
characteristics of good features. It is worth noting 
that the good features identified are not particularly 
tied to active learning, and could also be obtained 
by a more standard feature selection method rather 
than by looking at how the features provide 
benefits as they are added in.   
For a couple of the verbs examined, we found 
that active learning gives higher prediction 
accuracy midway through the training than one 
gets after training on the entire corpus.  Analysis 
suggests that this is not due to bad examples being 
added to the training set. It appears that the widely 
used maximum entropy model with Gaussian 
priors is overfitting: the model by including too 
many features and thus fitting noise as well as 
signal.  Using different strengths of the Gaussian 
prior does not solve the problem. If a very strong 
prior is used, then poorer accuracy is obtained. We 
believe that using appropriate feature selection 
would cause the phenomenon to vanish. 
Acknowledgements 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not 
necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA.  
References 
Naoki Abe and Hiroshi Mamitsuka. 1998. Query 
learning strategies using boosting and bagging. In 
Proc. of ICML1998, pages 1?10. 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features, In Proc. of IJCNLP2005, Oct., 
Jeju, Republic of Korea. 
Tim Chklovski and Rada Mihalcea, Building a Sense 
Tagged Corpus with Open Mind Word Expert, in 
Proceedings of the ACL 2002 Workshop on "Word 
Sense Disambiguation: Recent Successes and Future 
Directions", Philadelphia, July 2002. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  PhD 
Thesis. University of Pennsylvania. 
Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui, 
Hozumi Tanaka. 1998. Selective sampling for 
example-based word sense disambiguation, 
Computational Linguistics, v.24 n.4, p.573-597, Dec.  
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance 
Ramshaw and Ralph Weischedel. OntoNotes: The 
90% Solution. Accepted by HLT-NAACL06. Short 
paper. 
David D. Lewis and William A. Gale. 1994. A 
sequential algorithm for training text classifiers. In W. 
Bruce Croft and Cornelis J. van Rijsbergen, editors, 
Proceedings of SIGIR-94, Dublin, IE. 
Andrew K. McCallum. 2002. MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet. 
Andew McCallum and Kamal Nigam. 1998. Employing 
EM in pool-based active learning for text 
classification. In Proc. of ICML ?98. 
Martha Palmer, Hoa Trang Dang and Christiane 
Fellbaum. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually and 
automatically. Natural Language Engineering. 
Andrew I. Schein. 2005. Active Learning for Logistic 
Regression. Ph.D. Thesis. Univ. of Pennsylvania. 
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou and Chew 
Lim Tan. 2004 Multi-criteria-based active learning 
for named entity recognition, In Proc. of ACL04, 
Barcelona, Spain. 
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proc. of ACL 2002. 
Cynthia A. Thompson, Mary Elaine Califf, and 
Raymond J. Mooney. 1999. Active learning for 
natural language parsing and information extraction. 
In Proc. of ICML-99. 
127
Integrated Annotation for Biomedical Information Extraction
Seth Kulick and Ann Bies and Mark Liberman and Mark Mandel
and Ryan McDonald and Martha Palmer and Andrew Schein and Lyle Ungar
University of Pennsylvania
Philadelphia, PA 19104
 
skulick,bies,myl  @linc.cis.upenn.edu,
mamandel@unagi.cis.upenn.edu,
 
ryantm,mpalmer,ais,ungar  @cis.upenn.edu
Scott Winters and Pete White
Division of Oncology,
Children?s Hospital of Philadelphia
Philadelphia, Pa 19104
 
winters,white  @genome.chop.edu
Abstract
We describe an approach to two areas of
biomedical information extraction, drug devel-
opment and cancer genomics. We have devel-
oped a framework which includes corpus anno-
tation integrated at multiple levels: a Treebank
containing syntactic structure, a Propbank con-
taining predicate-argument structure, and an-
notation of entities and relations among the en-
tities. Crucial to this approach is the proper
characterization of entities as relation compo-
nents, which allows the integration of the entity
annotation with the syntactic structure while
retaining the capacity to annotate and extract
more complex events. We are training statis-
tical taggers using this annotation for such ex-
traction as well as using them for improving the
annotation process.
1 Introduction
Work over the last few years in literature data mining
for biology has progressed from linguistically unsophisti-
cated models to the adaptation of Natural Language Pro-
cessing (NLP) techniques that use full parsers (Park et
al., 2001; Yakushiji et al, 2001) and coreference to ex-
tract relations that span multiple sentences (Pustejovsky
et al, 2002; Hahn et al, 2002) (For an overview, see
(Hirschman et al, 2002)). In this work we describe an ap-
proach to two areas of biomedical information extraction,
drug development and cancer genomics, that is based on
developing a corpus that integrates different levels of se-
mantic and syntactic annotation. This corpus will be a
resource for training machine learning algorithms useful
for information extraction and retrieval and other data-
mining applications. We are currently annotating only
abstracts, although in the future we plan to expand this to
full-text articles. We also plan to make publicly available
the corpus and associated statistical taggers.
We are collaborating with researchers in the Division
of Oncology at The Children?s Hospital of Philadelphia,
with the goal of automatically mining the corpus of can-
cer literature for those associations that link specified
variations in individual genes with known malignancies.
In particular we are interested in extracting three entities
(Gene, Variation Event, and Malignancy) in the follow-
ing relationship: Gene X with genomic Variation Event
Y is correlated with Malignancy Z. For example, WT1 is
deleted in Wilms Tumor #5. Such statements found in the
literature represent individual gene-variation-malignancy
observables. A collection of such observables serves
two important functions. First, it summarizes known
relationships between genes, variation events, and ma-
lignancies in the cancer literature. As such, it can be
used to augment information available from curated pub-
lic databases, as well as serve as an independent test for
accuracy and completeness of such repositories. Second,
it allows inferences to be made about gene, variation, and
malignancy associations that may not be explicitly stated
in the literature, both at the fact and entity instance lev-
els. Such inferences provide testable hypotheses and thus
future research targets.
The other major area of focus, in collaboration with
researchers in the Knowledge Integration and Discov-
ery Systems group at GlaxoSmithKline (GSK), is the ex-
traction of information about enzymes, focusing initially
on compounds that affect the activity of the cytochrome
P450 (CYP) family of proteins. For example, the goal is
to see a phrase like
Amiodarone weakly inhibited CYP2C9,
CYP2D6, and CYP3A4-mediated activities
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 61-68.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
with Ki values of 45.1?271.6 
and extract the facts
amiodarone inhibits CYP2C9 with
Ki=45.1-271.6
amiodarone inhibits CYP2D6 with
Ki=45.1-271.6
amiodarone inhibits CYP3A4 with
Ki=45.1-271.6
Previous work at GSK has used search algorithms that
are based on pattern matching rules filling template slots.
The rules rely on identifying the relevant passages by first
identifying compound names and then associating them
with a limited number of relational terms such as inhibit
or inactivate. This is similar to other work in biomedical
extraction projects (Hirschman et al, 2002).
Creating good pattern-action rules for an IE problem is
far from simple. There are many complexities in the dif-
ferent ways that a relation can be expressed in language,
such as syntactic alternations and the heavy use of co-
ordination. While sufficiently complex patterns can deal
with these issues, it requires a good amount of time and
effort to build such hand-crafted rules, particularly since
such rules are developed for each specific problem. A
corpus that is annotated with sufficient syntactic and se-
mantic structure offers the promise of training taggers for
quicker and easier information extraction.
The corpus that we are developing for the two differ-
ent application demands consists of three levels of anno-
tation: the entities and relations among the entities for the
oncology or CYP domain, syntactic structure (Treebank),
and predicate-argument structure (Propbank). This is a
novel approach from the point-of-view of NLP since pre-
vious efforts at Treebanking and Propbanking have been
independent of the special status of any entities, and pre-
vious efforts at entity annotation have been independent
of corresponding layers of syntactic and semantic struc-
ture. The decomposition of larger entities into compo-
nents of a relation, worthwhile by itself on conceptual
grounds for entity definition, also allows the component
entities to be mapped to the syntactic structure. These
entities can be viewed as semantic types associated with
syntactic constituents, and so our expectation is that au-
tomated analyses of these related levels will interact in a
mutually reinforcing and beneficial way for development
of statistical taggers. Development of such statistical tag-
gers is proceeding in parallel with the annotation effort,
and these taggers help in the annotation process, as well
as being steps towards automatic extraction.
In this paper we focus on the aspects of this project
that have been developed and are in production, while
also trying to give enough of the overall vision to place
the work that has been done in context. Section 2 dis-
cusses some of the main issues around the development
of the guidelines for entity annotation, for both the on-
cology and inhibition domains. Section 3 first discusses
the overall plan for the different levels of annotation, and
then focuses on the integration of the two levels currently
in production, entity annotation and syntactic structure.
Section 4 describes the flow of the annotation process,
including the development of the statistical taggers men-
tioned above. Section 5 is the conclusion.
2 Guidelines for Entity Annotation
Annotation has been proceeding for both the oncology
and the inhibition domains. Here we give a summary of
the main features of the annotation guidelines that have
been developed. We have been influenced in this by pre-
vious work in annotation for biomedical information ex-
traction (Ohta et al, 2002; Gaizauskas et al, 2003). How-
ever, we differ in the domains we are annotating and the
design philosophy for the entity guidelines. For exam-
ple, we have been concentrating on explicit concepts for
entities like genes rather than developing a wide-range
ontology for the various physical instantiations.
2.1 Oncology Domain
Gene Entity For the sake of this project the defini-
tion for ?Gene Entity? has two significant characteristics.
First, ?Gene? refers to a composite entity as opposed to
the strict biological definition. As has been noted by oth-
ers, there are often ambiguities in the usage of the en-
tity names. For example, it is sometimes unclear as to
whether it is the gene or protein being referenced, or the
same name might refer to the gene or the protein at dif-
ferent locations in the same document. Our approach to
this problem is influenced by the named entity annota-
tion in the Automatic Content Extraction (ACE) project
(Consortium, 2002), in which ?geopolitical? entities can
have different roles, such as ?location? or ?organization?.
Analogously, we consider a ?gene? to be a composite en-
tity that can have different roles throughout a document.
Standardization of ?Gene? references between different
texts and between gene synonyms is handled by exter-
nally referencing each instance to a standard ontology
(Ashburner et al, 2000).
In the context of this project, ?Gene? refers to a con-
ceptual entity as opposed to the specific manifestation of
a gene (i.e. an allele or nucleotide sequence). Therefore,
we consider genes to be abstract concepts identifying ge-
nomic regions often associated with a function, such as
MYC or TrkB; we do not consider actual instances of
such genes within the gene-entity domain. Since we are
interested in the association between Gene-entities and
malignancies, for this project genes are of interest to us
when they have an associated variation event. Therefore,
the combination of Gene entities and Variation events
provides us with an evoked entity representing the spe-
cific instance of a gene.
Variation Events as Relations Variations comprise a
relationship between the following entities: Type (e.g.
point mutation, translocation, or inversion), Location
(e.g. codon 14, 1p36.1, or base pair 278), Original-State
(e.g. Alanine), and Altered-State (e.g. Thymine). These
four components represent the key elements necessary
to describe any genomic variation event. Variations are
often underspecified in the literature, frequently having
only two or three of these specifications. Characterizing
individual variations as a relation among such compo-
nents provides us with a great deal of flexibility: 1) it al-
lows us to capture the complete variation event even when
specific components are broadly spaced in the text, often
spanning multiple sentences or even paragraphs; 2) it pro-
vides us with a convenient means of tracking anaphora
between detailed descriptions (e.g. a point mutation at
codon 14 and summary references (e.g. this variation);
and 3) it provides a single structure capable of capturing
the breadth of variation specifications (e.g. A-  T point
mutation at base pair 47, A48-  G or t(11;14)(q13;32)).
Malignancy The guidelines for malignancy annotation
are under development. We are planning to define it in a
manner analogous to variation, whereby a Malignancy is
composed of various attribute types (such as developmen-
tal stage, behavior, topographic site, and morphology).
2.2 CYP Domain
In the CYP Inhibition annotation task we are tagging
three types of entities:
1. CYP450 enzymes (cyp)
2. other substances (subst)
3. quantitative measurements (quant)
Each category has its own questions and uncertain-
ties. Names like CYP2C19 and cytochrome P450 en-
zymes proclaim their membership, but there are many
aliases and synonyms that do not proclaim themselves,
such as 17,20-lyase. We are compiling a list of such
names.
Other substances is a potentially huge and vaguely-
delimited set, which in the current corpus includes grape-
fruit juice and red wine as well as more obviously bio-
chemical entities like polyunsaturated fatty acids and ery-
thromycin. The quantitative measurements we are di-
rectly interested in are those directly related to inhibition,
such as IC50 and K(i). We tag the name of the measure-
ment, the numerical value, and the unit. For example, in
the phrase ...was inhibited by troleandomycin (ED50 = 1
microM), ED50 is the name, 1 the value, and microM the
unit. We are also tagging other measurements, since it
is easy to do and may provide valuable information for
future IE work.
3 Integrated Annotation
As has been noted in the literature on biomedical IE (e.g.,
(Pustejovsky et al, 2002; Yakushiji et al, 2001)), the
same relation can take a number of syntactic forms. For
example, the family of words based on inhibit occurs
commonly in MEDLINE abstracts about CYP enzymes
(as in the example in the introduction) in patterns like A
inhibited B, A inhibited the catalytic activity of B, inhibi-
tion of B by A, etc.
Such alternations have led to the use of pattern-
matching rules (often hand-written) to match all the rele-
vant configurations and fill in template slots based on the
resulting pattern matches. As discussed in the introduc-
tion, dealing with such complications in patterns can take
much time and effort.
Our approach instead is to build an annotated corpus
in which the predicate-argument information is annotated
on top of the parsing annotations in the Treebank, the re-
sulting corpus being called a ?proposition bank? or Prop-
bank. This newly annotated corpus is then used for train-
ing processors that will automatically extract such struc-
tures from new examples.
In a Propbank for biomedical text, the types of in-
hibit examples listed above would consistently have their
compounds labeled as Arg0 and their enzymes labeled as
Arg1, for nominalized forms such as A is an inhibitor of
B, A caused inhibition of B, inhibition of B by A, as well
the standard A inhibits B. We would also be able to la-
bel adjuncts consistently, such as the with prepositional
phrase in CYP3A4 activity was decreased by L, S and F
with IC(50) values of about 200 mM. In accordance with
other Calibratable verbs such as rise, fall, decline, etc.,
this phrase would be labeled as an Arg2-EXTENT, re-
gardless of its syntactic role.
A Propbank has been built on top of the Penn Tree-
bank, and has been used to train ?semantic taggers?, for
extracting argument roles for the predicates of interest,
regardless of the particular syntactic context.1
Such semantic taggers have been developed by using
machine learning techniques trained on the Penn Prop-
bank (Surdeanu et al, 2003; Gildea and Palmer, 2002;
Kingsbury and Palmer, 2002). However, the Penn Tree-
bank and Propbank involve the annotation of Wall Street
Journal text. This text, being a financial domain, differs
in significant ways from the biomedical text, and so it is
1The Penn Propbank is complemented by NYU?s Nom-
bank project (Meyers, October 2003), which includes tagging
of nominal predicate structure. This is particular relevant for
the biomedical domain, given the heavy use of nominals such
mutation and inhibition.
necessary for this approach to have a corpus of biomed-
ical texts such as MEDLINE articles annotated for both
syntactic structure (Treebanking) and shallow semantic
structure (Propbanking).
In this project, the syntactic and semantic annotation is
being done on a corpus which is also being annotated for
entities, as described in Section 2. Since semantic tag-
gers of the sort described above result in semantic roles
assigned to syntactic tree constituents, it is desirable to
have the entities correspond to syntactic constituents so
that the semantic roles are assigned to entities. The en-
tity information can function as type information and be
taken advantage of by learning algorithms to help charac-
terize the properties of the terms filling specified roles in
a given predicate.
This integration of these three different annotation lev-
els, including the entities, is being done for the first time2,
and we discuss here three main challenges to this corre-
spondence between entities and constituents: (1) entities
that are large enough to cut across multiple constituents,
(2) entities within prenominal modifiers, and (3) coordi-
nation.3
Relations and Large Entities One major area of con-
cern is the possibility of entities that contain more than
one syntactic constituent and do not match any node in
the syntax tree. For example, as discussed in Section 2, a
variation event includes material on a variation?s type, lo-
cation, and state, and can cut not only across constituents,
but even sentences and paragraphs. A simple example is
point mutations at codon 12, containing both the nominal
(the type of mutation) and following NP (the location).
Note that while in isolation this could also be considered
one syntactic constituent, the NP and PP together, the ac-
tual context is ...point mutations at codon 12 in duode-
nal lavage fluid.... Since all PPs are attached at the same
level, at codon 12 and in duodenal lavage fluid are sis-
ters, and so there is no constituent consisting of just point
mutations at codon 12.
Casting the variation event as a relation between dif-
ferent component entities allows the component entities
to correspond to tree constituents, while retaining the ca-
pacity to annotate and search for more complex events.
In this case, one component entity point mutations cor-
2An influential precursor to this integration is the system de-
scribed in (Miller et al, 1996). Our work is in much the same
spirit, although the representation of the predicate-argument
structure via Propbank and the linkage to the entities is quite
different, as well as of course the domain of annotation.
3There are cases where the entities are so minimal that they
are contained within a NP, not including the determiner, such as
CpG site in the NP a CpG site. entities. We are not as concerned
about these cases since we expect that such entity information
properly contained within a base NP can be associated with the
full base NP.
responds to a (base) NP node, and at codon 12 is corre-
sponds to the PP node that is the NP?s sister. At the same
time, the relation annotation contains the information re-
lating these two constituents.
Similarly, while the malignancy entity definition is cur-
rently under development, as mentioned in Section 2.1, a
guiding principle is that it will also be treated as a relation
and broken down into component entities. While this also
has conceptual benefits for the annotation guidelines, it
has the fortunate effect of making such otherwise syntax-
unfriendly malignancies as colorectal adenomas contain-
ing early cancer and acute myelomonocytic leukemia in
remission amenable for mapping the component parts to
syntactic nodes.
Entities within Prenominal Modifiers While we are
for the most part following the Penn Treebank guide-
lines (Bies et al, 1995), we are modifying them in two
important aspects. One concerns the prenominal mod-
ifiers, which in the Penn Treebank were left flat, with
no structure, but in this biomedical domain contain much
of the information - e.g., cancer-associated autoimmune
antigen. Not only would this have had no annotation
for structure, but even more bizarrely, cancer-associated
would have been a single token in the Penn Treebank,
thus making it impossible to capture the information as
to what is associated with what. We have developed new
guidelines to assign structure to prenominal entities such
as breast cancer, as well as changed the tokenization
guidelines to break up tokens such as cancer-associated.
Coordination We have also modified the treebank an-
notation to account for the well-known problem of enti-
ties that are discontinuous within a coordination structure
- e.g., K- and H-ras, where the entities are K-ras and H-
ras. Our annotation tool allows for discontinuous entities,
so that both K-ras and H-ras are annotated as genes.
Under standard Penn Treebank guidelines for tokeniza-
tion and syntactic structure, this would receive the flat
structure
NP
K- and H-ras
in which there is no way to directly associate the entity
K-ras with a constituent node.
We have modified the treebank guidelines so that K-ras
and H-ras are both constituents, with the ras part of K-ras
represented with an empty category co-indexed with ras
in H-ras:4.
4This is related to the approach to coordination in the GE-
NIA project.
NP
NP
K - NX-1
*P*
and NP
H - NX-1
ras
4 Annotation Process
We are currently annotating MEDLINE abstracts for both
the oncology and CYP domains. The flowchart for the
annotation process is shown in Figure 1. Tokenization,
POS-tagging, entity annotation (both domains), and tree-
banking are in full production. Propbank annotation and
the merging of the entities and treebanking remain to be
integrated into the current workflow. The table in Fig-
ure 2 shows the number of abstracts completed for each
annotation area.
The annotation sequence begins with tokenization and
part-of-speech annotating. While both aspects are simi-
lar to those used for the Penn Treebank, there are some
differences, partly alluded to in Section 3. Tokens are
somewhat more fine-grained than in the Penn Treebank,
so that H-ras, e.g., would consist of three tokens: H, -,
and ras.
Tokenized and part-of-speech annotated files are then
sent to the entity annotators, either for oncology or CYP,
depending on which domain the abstract has been chosen
for. The entities described in Section 2 are annotated at
this step. We are using WordFreak, a Java-based linguis-
tic annotation tool5, for annotation of tokenization, POS,
and entities. Figure 3 is a screen shot of the oncology do-
main annotation, here showing a variation relation being
created out of component entities for type and location.
In parallel with the entity annotation, a file is tree-
banked - i.e., annotated for its syntactic structure. Note
that this is done independently of the entity annotation.
This is because the treebanking guidelines are relatively
stable (once they were adjusted for the biomedical do-
main as described in Section 3), while the entity defini-
tions can require a significant period of study before sta-
bilizing, and with the parallel treatment the treebanking
can proceed without waiting for the entity annotation.
However, this does mean that to produce the desired
integrated annotation, the entity and treebanking annota-
tions need to be merged into one representation. The con-
sideration of the issues described in Section 3 has been
carried out for the purpose of allowing this integration
of the treebanking and entity annotation. This has been
completed for some pilot documents, but the full merging
remains to be integrated into the workflow system.
5http://www.sf.net/projects/wordfreak
As mentioned in the introduction, statistical taggers
are being developed in parallel with the annotation effort.
While such taggers are part of the final goal of the project,
providing the building blocks for extracting entities and
relations, they are also useful in the annotation process
itself, so that the annotators only need to perform correc-
tion of automatically tagged data, instead of starting from
scratch.
Until recently (Feb. 10), the part-of-speech annotation
was done by hand-correcting the results of tagging the
data with a part-of-speech tagger trained on a modified
form of the Penn Treebank.6 The tagger is a maximum-
entropy model utilizing the opennlp package available
at http://www.sf.net/projects/opennlp . It
has now been retrained using 315 files (122 from the
oncology domain, 193 from the cyp domain). Figure 4
shows the improvement of the new vs. the old POS tag-
ger on the same 294 files that have been hand-corrected.
These results are based on testing files that have already
been tokenized, and thus are an evaluation only of the
POS tagger and not the tokenizer. While not directly
comparable to results such as (Tateisi and Tsujii, 2004),
due to the different tag sets and tokenization, they are in
the same general range.7
The oncology and cyp entity annotation, as well as the
treebanking are still being done fully manually, although
that will change in the near future. Initial results for a tag-
ger to identify the various components of a variation re-
lation are promising, although not yet integrated into an-
notation process. The tagger is based on the implementa-
tion of Conditional Random Fields (Lafferty et al, 2001)
in the Mallet toolkit (McCallum, 2002). Briefly, Condi-
tional Random Fields are log-linear models that rely on
weighted features to make predictions on the input. Fea-
tures used by our system include standard pattern match-
ing and word features as well as some expert-created reg-
ular expression features8. Using 10-fold cross-validation
on 264 labelled abstracts containing 551 types, 1064 lo-
6Roughly, Penn Treebank tokens were split at hyphens, with
the individual components then sent through a Penn Treebank-
trained POS tagger, to create training data for another POS tag-
ger. For example (JJ York-based) is treated as (NNP
York) (HYPH -) (JJ based). While this works rea-
sonably well for tokenization, the POS tagger suffered severely
from being trained on a corpus with such different properties.
7The tokenizer has also been retrained and the new tokenizer
is being used for annotation, although although we do not have
the evaluation results here.
8e.g., chr|chromosome [1-9]|1[0-9]|2[0-
2]|X|Y p|q
Merged Entity/
Treebank Annotation
Tokenization
Entity Annotation
POS Annotation
Treebank/Propbank
Annotation
Figure 1: Annotation Flow
Annotation Task Start Date Annotated Documents
Part-of-Speech Tagging 8/22/03 422
Entity Tagging 9/12/03 414
Treebanking 1/8/04 127
Figure 2: Current Annotation Production Results
Figure 3: Relation Annotation in WordFreak
Tagger Training Material Token Instances
Old Sections 00-15 Penn Treebank 773832
New 315 abstracts 103159
Tagger Overall Accuracy Number Token Instances Accuracy on Accuracy on
Unseen in Training Data Unseen Seen
Old 88.53% 14542 58.80% 95.53%
New 97.33% 4096 85.05% 98.02%
(Testing Material: 294 abstracts from the oncology domain, with 76324 token instances.)
Figure 4: Evaluation of Part-of-Speech Taggers
cations and 557 states, we obtained the following results:
Entity Precision Recall F-measure
Type 0.80 0.72 0.76
Location 0.85 0.73 0.79
State 0.90 0.80 0.85
Overall 0.86 0.75 0.80
An entity is considered correctly identified if and only
if it matches the human labeling by both category (type,
location or state) and span (from position a to position b).
At this stage we have not distinguished between initial
and final states.
While it is difficult to compare taggers that tag
different types of entities (e.g., (Friedman et al, 2001;
Gaizauskas et al, 2003)), CRFs have been utilized for
state-of-the-art results in NP-chunking and gene and
protein tagging (Sha and Pereira, 2003; McDonald
and Pereira, 2004) Currently, we are beginning to
investigate methods to identify relations over the varia-
tion components that are extracted using the entity tagger.
5 Conclusion
We have described here an integrated annotation ap-
proach for two areas of biomedical information extrac-
tion. We discussed several issues that have arisen for this
integration of annotation layers. Much effort has been
spent on the entity definitions and how they relate to the
higher-level concepts which are desired for extraction.
There are promising initial results for training taggers to
extract these entities.
Next steps in the project include: (1) continued anno-
tation of the layers we are currently doing, (2) integra-
tion of the level of predicate-argument annotation, and
(3) further development of the statistical taggers, includ-
ing taggers for identifying relations over their component
entities.
Acknowledgements
The project described in this paper is based at the In-
stitute for Research in Cognitive Science at the Uni-
versity of Pennsylvania and is supported by grant EIA-
0205448 from the National Science Foundation?s Infor-
mation Technology Research (ITR) program.
We would like to thank Aravind Joshi, Jeremy
Lacivita, Paula Matuszek, Tom Morton, and Fernando
Pereira for their comments.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, M.A. Harris, D.P. Hill, L. Issel-Tarver,
A. Kasarskis, S. Lewis, J.C. Matese, J.E. Richardson,
M. Ringwald, G.M. Rubin, and G. Sherlock. 2000.
Gene ontology: Tool for the unification of biology.
Nature Genetics, 25(1):25?29.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
Style, Penn Treebank Project. Tech report MS-CIS-
95-06, University of Pennsylvania, Philadelphia, PA.
Linguistic Data Consortium. 2002. Entity de-
tection and tracking - phase 1 - EDT and
metonymy annotation guidelines version 2.5
20021205. http://www.ldc.upenn.edu/Projects/ACE
/PHASE2/Annotation/.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. Genies: a
natural-language processing system for the extraction
of molecular pathways from journal articles. ISMB
(Supplement of Bioinformatics), pages 74?82.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Bioinformatics applications of information
extraction from journal articles. Journal of Bioinfor-
matics, 19(1):135?143.
Daniel Gildea and Martha Palmer. 2002. The Necessity
of Syntactic Parsing for Predicate Argument Recogni-
tion. In Proc. of ACL-2002.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: The
MEDSYNDIKATE text mining system. In Proceed-
ings of the Pacific Rim Symposium on Biocomputing,
pages 338?349.
Lynette Hirschman, Jong C. Park, Junichi Tsuji, Limsoon
Wong, and Cathy H. Wu. 2002. Accomplishments and
challenges in literature data mining for biology. Bioin-
formatics Review, 18(12):1553?1561.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Identify-
ing gene and protein mentions in text using conditional
random fields. In A Critical Assessment of Text Min-
ing Methods in Molecular Biology workshop. To be
presented.
Adam Meyers. October, 2003. Nombank. Talk at Auto-
matic Content Extraction (ACE) PI Meeting, Alexan-
dria, VA.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to
natural language interfaces. In Aravind Joshi and
Martha Palmer, editors, Proceedings of the Thirty-
Fourth Annual Meeting of the Association for Compu-
tational Linguistics, pages 55?61, San Francisco. Mor-
gan Kaufmann Publishers.
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, and Jun?ici
Tsuji. 2002. The GENIA corpus: An annotated corpus
in molecular biology domain. In Proceedings of the
10th International Conference on Intelligent Systems
for Molecular Biology.
J. Park, H. Kim, and J. Kim. 2001. Bidirectional in-
cremental parsing for automatic pathway identification
with combinatory categorial grammar. In Proceedings
of the Pacific Rim Symposium on Biocomputing, pages
396?407.
J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust
relational parsing over biomedical literature: Extract-
ing inhibit relations. In Proceedings of the Pacific Rim
Symposium on Biocomputing, pages 362?373.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceeds of Human
Language Technology-NAACL 2003.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-speech
annotation of biology research abstracts. In Proceed-
ings of LREC04. To be presented.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Proceedings of the Pacific Rim Symposium
on Biocomputing, pages 408?419.
