Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 96?99,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UBIU: A Language-Independent System for Coreference Resolution
Desislava Zhekova
University of Bremen
zhekova@uni-bremen.de
Sandra K?ubler
Indiana University
skuebler@indiana.edu
Abstract
We present UBIU, a language indepen-
dent system for detecting full coreference
chains, composed of named entities, pro-
nouns, and full noun phrases which makes
use of memory based learning and a fea-
ture model following Rahman and Ng
(2009). UBIU is evaluated on the task
?Coreference Resolution in Multiple Lan-
guages? (SemEval Task 1 (Recasens et al,
2010)) in the context of the 5th Interna-
tional Workshop on Semantic Evaluation.
1 Introduction
Coreference resolution is a field in which major
progress has been made in the last decade. Af-
ter a concentration on rule-based systems (cf. e.g.
(Mitkov, 1998; Poesio et al, 2002; Markert and
Nissim, 2005)), machine learning methods were
embraced (cf. e.g. (Soon et al, 2001; Ng and
Cardie, 2002)). However, machine learning based
coreference resolution is only possible for a very
small number of languages. In order to make such
resources available for a wider range of languages,
language independent systems are often regarded
as a partial solution. To this day, there have been
only a few systems reported that work on multiple
languages (Mitkov, 1999; Harabagiu and Maio-
rano, 2000; Luo and Zitouni, 2005). However, all
of those systems were geared towards predefined
language sets.
In this paper, we present a language indepen-
dent system that does require syntactic resources
for each language but does not require any effort
for adapting the system to a new language, except
for minimal effort required to adapt the feature ex-
tractor to the new language. The system was com-
pletely developed within 4 months, and will be ex-
tended to new languages in the future.
2 UBIU: System Structure
The UBIU system aims at being a language-
independent system in that it uses a combination
of machine learning, in the form of memory-based
learning (MBL) in the implementation of TiMBL
(Daelemans et al, 2007), and language indepen-
dent features. MBL uses a similarity metric to find
the k nearest neighbors in the training data in order
to classify a new example, and it has been shown
to work well for NLP problems (Daelemans and
van den Bosch, 2005). Similar to the approach
by Rahman and Ng (2009), classification in UBUI
is based on mention pairs (having been shown to
work well for German (Wunsch, 2009)) and uses
as features standard types of linguistic annotation
that are available for a wide range of languages
and are provided by the task.
Figure 1 shows an overview of the system. In
preprocessing, we slightly change the formatting
of the data in order to make it suitable for the next
step in which language dependent feature extrac-
tion modules are used, fromwhich the training and
test sets for the classification are extracted. Our
approach is untypical in that it first extracts the
heads of possible antecedents during feature ex-
traction. The full yield of an antecedent in the test
set is determined after classification in a separate
module. During postprocessing, final decisions
are made concerning which of the mention pairs
are considered for the final coreference chains.
In the following sections, we will describe fea-
ture extraction, classification, markable extraction,
and postprocessing in more detail.
2.1 Feature Extraction
The language dependent modules contain finite
state expressions that detect the heads based on the
linguistic annotations. Such a language module re-
quires a development time of approximately 1 per-
son hour in order to adapt the regular expressions
96
Figure 1: Overview of the system.
to the given language data (different POS tagsets,
differences in the provided annotations). This is
the only language dependent part of the system.
We decided to separate the task of finding heads
of markables, which then serve as the basis for the
generation of the feature vectors, from the identi-
fication of the scope of a markable. For the En-
glish sentence ?Any details or speculation on who
specifically, we don?t know that at this point.?, we
first detect the heads of possible antecedents, for
example ?details?. However, the decision on the
scope of the markable, i.e. the decision between
?details? or ?Any details or speculation on who
specifically? is made in the postprocessing phase.
One major task of the language modules is the
check for cyclic dependencies. Our system re-
lies on the assumption that cyclic dependencies do
not occur, which is a standard assumption in de-
pendency parsing (K?ubler et al, 2009). However,
since some of the data sets in the multilingual task
contained cycles, we integrated a module in the
preprocessing step that takes care of such cycles.
After the identification of the heads of mark-
ables, the actual feature extraction is performed.
The features that were used for training a classifier
(see Table 1) were selected from the feature pool
# Feature Description
1 m
j
- the antecedent
2 m
k
- the mention to be resolved
3 Y ifm
j
is pron.; else N
4 Y ifm
j
is subject; else N
5 Y ifm
j
is a nested NP; else N
6 number - Sg. or Pl.
7 gender - F(emale), M(ale), N(euter), U(nknown)
8 Y ifm
k
is a pronoun; else N
9 Y ifm
k
is a nested NP; else N
10 semantic class ? extracted from the NEs in the data
11 the nominative case ofm
k
if pron.; else NA
12 C if the mentions are the same string; else I
13 C if one mention is a substring of the other; else I
14 C if both mentions are pron. and same string; else I
15 C if both mentions are both non-pron. and same
string; else I
16 C if both m. are pron. and either same pron. or diff.
w.r.t. case; NA if at least one is not pron.; else I
17 C if the mentions agree in number; I if not; NA if the
number for one or both is unknown
18 C if both m. are pron. I if neither
19 C if both m. are proper nouns; I if neither; else NA
20 C if the m. have same sem. class; I if not; NA if the
sem. class for one or both m. is unknown
21 sentence distance between the mentions
22 concat. values for f. 6 form
j
andm
k
23 concat. values for f. 7 form
j
andm
k
24 concat. values for f. 3 form
j
andm
k
25 concat. values for f. 5 form
j
andm
k
26 concat. values for f. 10 form
j
andm
k
27 concat. values for f. 11 form
j
andm
k
Table 1: The pool of features for all languages.
presented by Rahman and Ng (2009). Note that
not all features could be used for all languages.
We extracted all the features in Table 1 if the cor-
responding type of annotation was available; oth-
erwise, a null value was assigned.
A good example for the latter concerns the gen-
der information represented by feature 7 (for pos-
sible feature values cf. Table 1). Let us consider
the following two entries - the first from the Ger-
man data set and the second from English:
1. Regierung Regierung Regierung NN NN
cas=d|num=sg|gend=fem cas=d|num=sg|gend=fem 31
31 PN PN . . .
2. law law NN NN NN NN 2 2 PMOD PMOD . . .
Extracting the value from entry 1, where
gend=fem, is straightforward; the value being F.
However, there is no gender information provided
in the English data (entry 2). As a result, the value
for feature 7 is U for the closed task.
2.2 Classifier Training
Based on the features extracted with the feature
extractors described above, we trained TiMBL.
Then we performed a non-exhaustive parameter
97
optimization across all languages. Since a full op-
timization strategy would lead to an unmanageable
number of system runs, we concentrated on vary-
ing k, the number of nearest neighbors considered
in classification, and on the distance metric.
Furthermore, the optimization is focused on
language independence. Hence, we did not op-
timize each classifier separately but selected pa-
rameters that lead to best average results across
all languages of the shared task. In our opinion,
this ensures an acceptable performance for new
languages without further adaptation. The optimal
settings for all the given languages were k=3 with
the Overlap distance and gain ratio weighting.
2.3 Markable Extraction
The markable extractor makes use of the depen-
dency relation labels. Each syntactic head together
with all its dependents is identified as a separate
markable. This approach is very sensitive to incor-
rect annotations and to dependency cycles in the
data set. It is also sensitive to differences between
the syntactic annotation and markables. In the
Dutch data, for example, markables for named en-
tities (NE) often exclude the determiner, a nominal
dependent in the dependency annotation. Thus,
the markable extractor suggests the whole phrase
as a markable, rather than just the NE.
During the development phase, we determined
experimentally that the recognition of markables
is one of the most important steps in order to
achieve high accuracy in coreference resolution:
We conducted an ablation study on the training
data set. We used the train data as training set and
the devel data as testing set and investigated three
different settings:
1. Gold standard setting: Uses gold markable
annotations as well as gold linguistic anno-
tations (upper bound).
2. Gold linguistic setting: Uses automatically
determined markables and gold linguistic an-
notations.
3. Regular setting: Uses automatically deter-
mined markables and automatic linguistic in-
formation.
Note that we did not include all six languages:
we excluded Italian and Dutch because there is
no gold-standard linguistic annotation provided.
The results of the experiment are shown in Table
2. From those results, we can conclude that the
S Lang. IM CEAF MUC B
3
BLANC
1
Spanish 85.8 52.3 12.8 60.0 56.9
Catalan 85.5 56.0 11.6 59.4 51.9
English 96.1 68.7 17.9 74.9 52.7
German 93.6 70.0 19.7 73.4 64.5
2
Spanish 61.0 41.5 11.3 42.4 48.7
Catalan 60.8 40.5 9.6 41.4 48.3
English 72.1 54.1 11.6 57.3 50.3
German 57.7 45.5 12.2 45.7 44.3
3
Spanish 61.2 41.8 10.3 42.3 48.5
Catalan 61.3 40.9 11.3 41.9 48.5
English 71.9 54.7 13.3 57.4 50.3
German 57.5 45.4 12.0 45.6 44.2
Table 2: Experiment results (as F1 scores) where
IM is identification of mentions and S - Setting.
figures in Setting 2 and 3 are very similar. This
means that the deterioration from gold to automat-
ically annotated linguistic information is barely
visible in the coreference results. This is a great
advantage, since gold-standard data has always
proved to be very expensive and difficult or im-
possible to obtain. The information that proved to
be extremely important for the performance of the
system is the one providing the boundaries of the
markables. As shown in Table 2, the latter leads to
an improvement of about 20%, which is observ-
able in the difference in the figures of Setting 1
and 2. The results for the different languages show
that it is more important to improve markable de-
tection than the linguistic information.
2.4 Postprocessing
In Section 2.1, we described that we decided to
separate the task of finding heads of markables
from the identification of the scope of a markable.
Thus, in the postprocessing step, we perform the
latter (by the Markables Extractor module) as well
as reformat the data for evaluation.
Another very important step during postpro-
cessing is the selection of possible antecedents. In
cases where more than one mention pair is classi-
fied as coreferent, only the pair with highest con-
fidence by TiMBL is selected. Since nouns can
be discourse-new, they do not necessarily have a
coreferent antecedent; pronouns however, require
an antecedent. Thus, in cases where all possible
antecedents for a given pronoun are classified as
not coreferent, we select the closest subject as an-
tecedent; or if this heuristic is not successful, the
antecedent that has been classified as not corefer-
ent with the lowest confidence score (i.e. the high-
est distance) by TiMBL.
98
Lang. S IM CEAF MUC B
3
BLANC
Catalan G 84.4 52.3 11.7 58.8 52.2
R 59.6 38.4 8.6 40.9 47.8
English G 95.9 65.7 20.5 74.8 54.0
R 74.2 53.6 14.2 58.7 51.0
German G 94.0 68.2 21.9 75.7 64.5
R 57.6 44.8 10.4 46.6 48.0
Spanish G 83.6 51.7 12.7 58.3 54.3
R 60.0 39.4 10.0 41.6 48.4
Italian R 40.6 32.9 3.6 34.8 37.2
Dutch R 34.7 17.0 8.3 17.0 32.3
Table 3: Final system results (as F1 scores) where
IM is identification of mentions and S - Setting.
For more details cf. (Recasens et al, 2010).
3 Results
UBIU participated in the closed task (i.e. only in-
formation provided in the data sets could be used),
in the gold and regular setting. It was one of two
systems that submitted results for all languages,
which we count as preliminary confirmation that
our system is language independent. The final re-
sults of UBIU are shown in Table 3. The figures
for the identification of mentions show that this is
an area in which the system needs to be improved.
The errors in the gold setting result from an in-
compatibility of our two-stage markable annota-
tion with the gold setting. We are planning to use
a classifier for mention identification in the future.
The results for coreference detection show that
English has a higher accuracy than all the other
languages. We assume that this is a consequence
of using a feature set that was developed for En-
glish (Rahman and Ng, 2009). This also means
that an optimization of the feature set for individ-
ual languages should result in improved system
performance.
4 Conclusion and Future Work
We have presented UBIU, a coreference resolution
system that is language independent (given differ-
ent linguistic annotations for languages). UBIU
is easy to maintain, and it allows the inclusion of
new languages with minimal effort.
For the future, we are planning to improve the
system while strictly adhering to the language in-
dependence. We are planning to separate pronoun
and definite noun classification, with the possibil-
ity of using different feature sets. We will also
investigate language independent features and im-
plement a markable classifier and a negative in-
stance sampling module.
References
Walter Daelemans and Antal van den Bosch. 2005.
Memory Based Language Processing. Cambridge
University Press.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2007. TiMBL: Tilburg mem-
ory based learner ? version 6.1 ? reference guide.
Technical Report ILK 07-07, Induction of Linguis-
tic Knowledge, Computational Linguistics, Tilburg
University.
Sanda M. Harabagiu and Steven J. Maiorano. 2000.
Multilingual coreference resolution. In Proceedings
of ANLP 2000, Seattle, WA.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan Claypool.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proceedings of HLT/EMNLP 2005, Van-
couver, Canada.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3).
Ruslan Mitkov. 1998. Robust pronoun resolu-
tion with limited knowledge. In Proceedings of
ACL/COLING 1998, Montreal, Canada.
Ruslan Mitkov. 1999. Multilingual anaphora resolu-
tion. Machine Translation, 14(3-4):281?299.
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of ACL 2002, pages 104?111,
Philadelphia, PA.
Massimo Poesio, Tomonori Ishikawa, Sabine
Schulte im Walde, and Renata Vieira. 2002.
Acquiring lexical knowledge for anaphora resolu-
tion. In Proceedings of LREC 2002, Las Palmas,
Gran Canaria.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP 2009, Singapore.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M.Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Holger Wunsch. 2009. Rule-Based and Memory-
Based Pronoun Resolution for German: A Compar-
ison and Assessment of Data Sources. Ph.D. thesis,
Universit?at T?ubingen.
99
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 112?116,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
UBIU: A Robust System for Resolving Unrestricted Coreference
Desislava Zhekova
University of Bremen
zhekova@uni-bremen.de
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Abstract
In this paper, we discuss the application
of UBIU to the CONLL-2011 shared task
on ?Modeling Unrestricted Coreference? in
OntoNotes. The shared task concentrates on
the detection of coreference not only in noun
phrases but also involving verbs. The infor-
mation provided for the closed track included
WordNet as well as corpus generated number
and gender information. Our system shows
no improvement when using WordNet infor-
mation, and the number information proved
less reliable than the information in the part
of speech tags.
1 Introduction
Coreference Resolution is the process of identify-
ing the linguistic expressions in a discourse that re-
fer to the same real world entity and to divide those
expressions into equivalence classes that represent
each discourse entity. For this task, a deeper knowl-
edge of the discourse is often required. However,
such knowledge is difficult to acquire. For this rea-
son, many systems use superficial information such
as string match. The CoNLL shared task on ?Model-
ing Unrestricted Coreference in OntoNotes? (Prad-
han et al, 2011) presents challenges that go be-
yond previous definitions of the task. On the one
hand, mention extraction is part of the task while
many previous approaches assumed gold standard
mentions. On the other hand, coreference is not
restricted to noun phrases, verbs are also included.
Thus, in Sales of passenger cars grew 22%. The
strong growth followed year-to-year increases., the
verb grew has an identity relation with the noun
phrase The strong growth.
The system that we used for the shared task is
the memory-based machine learning system UBIU
(Zhekova and Ku?bler, 2010). We describe the most
important components of the system in section 2.
The system was originally developed for robust,
multilingual coreference resolution, and thus had to
be adapted to this shared task. We investigate the
quality of our mention extraction in section 2.1 and
the quality of the features used in the classifier in
section 2.2. In section 3, we present UBIU?s results
on the development set, and in section 4, UBIU?s
final results in the shared task.
2 UBIU
UBIU (Zhekova and Ku?bler, 2010) was developed
as a multilingual coreference resolution system. A
robust approach is necessary to make the system ap-
plicable for a variety of languages. For this rea-
son, we use a machine learning approach to clas-
sify mention pairs. We use TiMBL (Daelemans et
al., 2007), a memory-based learner (MBL) that la-
bels the feature vectors from the test set based on
the k nearest neighbors in the training instances.
We chose TiMBL since MBL has been shown to
work well with small training sets. A non-exhaustive
parameter optimization on the development set led
us to use the IB1 algorithm, similarity is computed
based on weighted overlap, the relevance weights
are computed using gain ratio and the number of
nearest neighbors is set to k = 3 (for a description
of the algorithm and parameters cf. (Daelemans et
al., 2007)). The classifier is preceded by a mention
extractor, which identifies possible mentions, and a
feature extractor. The latter creates a feature vec-
tor for each possible pair of a potentially coreferring
112
mention and all possible antecedents in a context of
3 sentences. Another important step is to separate
singleton mentions from coreferent ones since only
the latter are annotated in OntoNotes. Our markable
extractor overgenerates in that it extracts all possi-
ble mentions, and only after classification, the sys-
tem can decide which mentions are singletons. We
investigate the performance of the mention and fea-
ture extraction modules in more detail below.
2.1 Mention Extraction
UBIU?s mention extractor uses part-of-speech
(POS), syntactic, and lemma information provided
in the OntoNotes data set to detect mentions. The
module defines a mention for each noun phrase,
based on syntactic information, as well as for all
possessive pronouns and all proper nouns, based on
their POS tags. Since for the shared task, verbs are
also potentially coreferent, we included a mention
for each of the verbs with a predicate lemma. An ex-
ample of the output of the mention extraction mod-
ule is shown in table 1. Each mention is numbered
with an individual number and thus still represents a
distinct entity. Since singleton mentions are not an-
notated in the OntoNotes data set, mentions without
coreference relations after classification need to be
removed from the answer set, which can only be per-
formed after coreference resolution when all coref-
erent pairs are identified. For this reason, the mark-
able extractor is bound to overgenerate. The latter
can clearly be seen when the mention extraction out-
put is compared to the provided gold mentions (cf.
the last column in table 1).
We conducted a simple experiment on the devel-
opment data in order to gain insight into the per-
formance of the mention extraction module. Using
the scorer provided by the shared task, we evaluated
the output of the module, without performing coref-
erence resolution and without removing singleton
mentions. This led to a recall of 96.55 % and a preci-
sion of 18.55%, resulting in an F-score of 31.12. The
high recall shows that the system is very reliable in
finding mentions with the correct boundaries. How-
ever, since we do not remove any singletons, UBIU
overgenerates and thus the system identified a con-
siderable number of singletons, too. Nevertheless,
the fact that UBIU identified 96.55% of all mentions
shows that the performance of the mention extrac-
# Word POS Parse bit ME output Gold
0 Devastating VBG (TOP(NP(NP* (1)|(2|(3 -
1 Critique NN *) 3) -
2 of IN (PP* - -
3 the DT (NP* (4 (32
4 Arab JJ * - -
5 World NN *)) 4) 32)
6 by IN (PP* - -
7 One CD (NP(NP*) (5|(6) -
8 of IN (PP* - -
9 Its PRP$ (NP* (7)|(8 (32)
10 Own JJ *)))))) 8)|5)|2) -
Table 1: The output of the mention extractor for a sample
sentence.
tion module is close to optimal.
2.2 Feature Extraction
Feature extraction is the second important subtask
for the UBIU pipeline. Since mentions are repre-
sented by their syntactic head, the feature extractor
uses a heuristic that selects the rightmost noun in a
noun phrase. However, since postmodifying prepo-
sitional phrases may be present in the mention, the
noun may not be followed by a preposition. For each
mention, a feature vector is created for all of its pre-
ceding mentions in a window of 3 sentences. Af-
ter classification, a filter can optionally be applied
to filter out mention pairs that disagree in number,
and another filter deletes all mentions that were not
assigned an antecedent in classification. Note that
the number information was derived from the POS
tags and not from the number/gender data provided
by the shared task since the POS information proved
more reliable in our system.
Initially, UBIU was developed to use a wide set of
features (Zhekova and Ku?bler, 2010), which consti-
tutes a subset of the features described by Rahman
and Ng (2009). For the CONLL-2011 shared task,
we investigated the importance of various additional
features that can be included in the feature set used
by the memory-based classifier. Thus, we conducted
experiments with a base set and an extended feature
set, which makes use of lexical semantic features.
Base Feature Set Since the original feature set in
Zhekova and Ku?bler (2010) contained information
that is not easily accessible in the OntoNotes data
set (such as grammatical functions), we had to re-
strict the feature set to information that can be de-
rived solely from POS annotations. Further infor-
113
# Feature Description
1 mj - the antecedent
2 mk - the mention to be resolved
3 Y ifmj is a pronoun; else N
4 number - S(ingular) or P(lural)
5 Y ifmk is a pronoun; else N
6 C if the mentions are the same string; else I
7 C if one mention is a substring of the other; else I
8 C if both mentions are pronominal and are the same
string; else I
9 C if the two mentions are both non-pronominal and
are the same string; else I
10 C if both mentions are pronominal and are either the
same pronoun or different only w.r.t. case;
NA if at least one of them is not pronominal; else I
11 C if the mentions agree in number; I if they disagree;
NA if the number for one or
both mentions cannot be determined
12 C if both mentions are pronouns; I if neither are
pronouns; else NA
13 C if both mentions are proper nouns; I if neither are
proper nouns; else NA
14 sentence distance between the mentions
Table 2: The pool of features used in the base feature set.
mation as sentence distance, word overlap etc. was
included as well. The list of used features is shown
in table 2.
Extended Feature Set Since WordNet informa-
tion was provided for the closed setting of the
CONLL-2011 shared task, we also used an ex-
tended feature set, including all features from the
base set alng with additional features derived from
WordNet. The latter features are shown in table 3.
2.3 Singletons
In section 2.1, we explained that singletons need to
be removed after classification. However, this leads
to a drastic decrease in system performance for two
reasons. First, if a system does not identify a coref-
erence link, the singleton mentions will be removed
from the coreference chains, and consequently, the
system is penalized for the missing link as well as
for the missing mentions. If singletons are included,
the system will still receive partial credit for them
from all metrics but MUC. For this reason, we in-
vestigated filtered and non-filtered results in combi-
nation with the base and the extended feature sets.
3 Results on the Development Set
The results of our experiment on the development
set are shown in table 4. Since the official scores
of the shared task are based on an average of MUC,
# Feature Description
15 C if both are nouns andmk is hyponym ofmj ; I if both
are nouns butmk is not a hyponym ofmj ; NA otherwise
16 C if both are nouns andmj is hyponym ofmk; I if both
are nouns butmj is not a hyponym ofmk; NA otherwise
17 C if both are nouns andmk is a partial holonym ofmj ;
I if both are nouns butmk is not a partial holonym ofmj ;
NA otherwise
18 C if both are nouns andmj is a partial holonym ofmk;
I if both are nouns butmj is not a partial holonym ofmk;
NA otherwise
19 C if both are nouns andmk is a partial meronym ofmj ;
I if both are nouns butmk is not a partial meronym ofmj ;
NA otherwise
20 C if both are nouns andmj is a partial meronym ofmk;
I if both are nouns butmj is not a partial meronym ofmk;
NA otherwise
21 C if both are verbs andmk entailsmj ; I if both are
verbs butmk does not entailmj ; NA otherwise
22 C if both are verbs andmj entailsmk; I if both are
verbs butmj does not entailmk; NA otherwise
23 C if both are verbs andmk is a hypernym ofmj ;
I if both are verbs butmk is not a hypernym ofmj ;
NA otherwise
24 C if both are verbs andmj is a hypernym ofmk;
I if both are verbs butmj is not a hypernym ofmk;
NA otherwise
25 C if both are verbs andmk is a troponym ofmj ;
I if both are verbs butmk is not a troponym ofmj ;
NA otherwise
26 C if both are verbs andmj is a troponym ofmk;
I if both are verbs butmj is not a troponym ofmk;
NA otherwise
Table 3: The features extracted from WordNet.
B3, and CEAFE, we report these measures and their
average. All the results in this section are based on
automatically annotated linguistic information. The
first part of the table shows the results for the base
feature set (UBIUB), the second part for the ex-
tended feature set (UBIUE). We also report results
if we keep all singletons (& Sing.) and if we filter
out coreferent pairs that do not agree in number (&
Filt.). The results show that keeping the singletons
results in lower accuracies on the mention and the
coreference level. Only recall on the mention level
profits from the presence of singletons. Filtering for
number agreement with the base set has a detrimen-
tal effect on mention recall but increases mention
precision so that there is an increase in F-score of
1%. However, on the coreference level, the effect is
negligible. For the extended feature set, filtering re-
sults in a decrease of approximately 2.0% in mention
precision, which also translates into lower corefer-
ence scores. We also conducted an experiment in
which we filter before classification (& Filt. BC),
following a more standard approach. The reasoning
114
IM MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
UBIUB 62.71 38.66 47.83 30.59 24.65 27.30 67.06 62.65 64.78 34.19 40.16 36.94 43.01
UBIUB & Sing. 95.11 18.27 30.66 30.59 24.58 27.26 67.10 62.56 64.75 34.14 40.18 36.92 42.97
UBIUB & Filt. 61.30 40.58 48.83 29.10 25.77 27.33 64.88 64.63 64.76 35.38 38.74 36.98 43.02
UBIUB & Filt. BC 61.33 40.49 48.77 28.96 25.54 27.14 64.95 64.48 64.71 35.23 38.71 36.89 42.91
UBIUE 62.72 39.09 48.16 30.63 24.94 27.49 66.72 62.76 64.68 34.19 39.90 36.82 43.00
UBIUE & Sing. 95.11 18.27 30.66 29.87 20.96 24.64 69.13 57.71 62.91 32.28 42.24 36.59 41.38
UBIUE & Filt. 63.01 36.62 46.32 28.65 21.05 24.27 68.10 58.72 63.06 32.91 41.53 36.72 41.35
Gold ME 100 100 100 38.83 82.97 52.90 39.99 92.33 55.81 66.73 26.75 38.19 48.97
Table 4: UBIU system results on the development set.
is that the training set for the classifier is biased to-
wards not assuming coreference since the majority
of mention pairs does not have a coreference rela-
tion. Thus filtering out non-agreeing mention pairs
before classification reduces not only the number of
test mention pairs to be classified but also the num-
ber of training pairs. However, in our system, this
approach leads to minimally lower results, which is
why we decided not to pursue this route. We also
experimented with instance sampling in order to re-
duce the bias towards non-coreference in the training
set. This also did not improve results.
Contrary to our expectation, using ontological in-
formation does not improve results. Only on the
mention level, we see a minimal gain in precision.
But this does not translate into any improvement on
the coreference level. Using filtering in combination
with the extended feature set results in a more pro-
nounced deterioration than with the base set.
The last row of table 4 (Gold ME) shows re-
sults when the system has access to the gold stan-
dard mentions. The MUC and B3 results show that
the classifier reaches an extremely high precision
(82.97% and 92.33%), from which we conclude that
the coreference links that our system finds are re-
liable, but it is also too conservative in assuming
coreference relations. For the future, we need to
investigate undersampling the negative examples in
the training set and more efficient methods for filter-
ing out singletons.
4 Final Results
In the following, we present the UBIU system re-
sults in two separate settings: using the test set with
automatically extracted mentions (section 4.1) and
using a test set with gold standard mentions, includ-
ing singletons (section 4.2). An overview of all sys-
tems participating in the CONLL-2011 shared task
and their results is provided by Pradhan et al (2011).
4.1 Automatic Mention Identification
The final results of UBIU for the test set without
gold standard mentions are shown in the first part
of table 5. They are separated into results for the
coreference resolution module based on automati-
cally annotated linguistic information and the gold
annotations. Again, we report results for both the
base feature set (UBIUB) and the extended feature
set usingWordNet features (UBIUE). A comparison
of the system results on the test and the development
set in the UBIUB setting shows that the average F-
score is considerably lower for the test set, 40.46 vs.
43.01 although the quality of the mentions remains
constant with an F-score of 48.14 on the test set and
47.83 on the development set.
The results based on the two data sets show that
UBIU?s performance improves when the system has
access to gold standard linguistic annotations. How-
ever, the difference between the results is in the area
of 2%. The improvement is due to gains of 3-5%
in precision for MUC and B3, which are counter-
acted by smaller losses in recall. In contrast, CEAFE
shows a loss in precision and a similar gain in recall,
resulting in a minimal increase in F-score.
A comparison of the results for the experiments
with the base set as opposed to the extended set in
5 shows that the extended feature set using Word-
Net information is detrimental to the final results av-
eraged over all metrics while it led to a slight im-
provement on the mention level. Our assumption
is that while in general, the ontological information
is useful, the additional information may be a mix-
ture of relevant and irrelevant information. Mihalcea
(2002) showed for word sense disambiguation that
115
IM MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
Automatic Mention Identification
auto
UBIUB 67.27 37.48 48.14 28.75 20.61 24.01 67.17 56.81 61.55 31.67 41.22 35.82 40.46
UBIUE 67.49 37.60 48.29 28.87 20.66 24.08 67.14 56.67 61.46 31.57 41.21 35.75 40.43
gold
UBIUB 65.92 40.56 50.22 31.05 25.57 28.04 64.94 62.23 63.56 33.53 39.08 36.09 42.56
UBIUE 66.11 40.37 50.13 30.84 25.14 27.70 65.07 61.83 63.41 33.23 39.05 35.91 42.34
Gold Mention Boundaries
auto
UBIUB 67.57 58.66 62.80 34.14 40.43 37.02 54.24 71.09 61.53 39.65 33.73 36.45 45.00
UBIUE 69.19 57.27 62.67 33.48 37.15 35.22 55.47 68.23 61.20 38.29 34.65 36.38 44.27
gold
UBIUB 67.64 58.75 62.88 34.37 40.68 37.26 54.28 71.18 61.59 39.69 33.76 36.49 45.11
UBIUE 67.72 58.66 62.87 34.18 40.40 37.03 54.30 71.04 61.55 39.64 33.78 36.47 45.02
Table 5: Final system results for the coreference resolution module on automatically extracted mentions on the gold
standard mentions for the base and extended feature sets.
memory-based learning is extremely sensitive to ir-
relevant features. For the future, we are planning
to investigate this problem by applying forward-
backward feature selection, as proposed by Mihal-
cea (2002) and Dinu and Ku?bler (2007).
4.2 Gold Mention Boundaries
UBIU was also evaluated in the experimental set-
ting in which gold mention boundaries were pro-
vided in the test set, including for singletons. The
results of the setting using both feature sets are re-
ported in the second part of table 5. The results show
that overall the use of gold standard mentions re-
sults in an increase of the average F-score of approx.
4.5%. Where mention quality and MUC are con-
cerned, gold standard mentions have a significant
positive influence on the average F-score. For B3
and CEAFE, however, there is no significant change
in scores. The increase in performance is most no-
ticeable in mention identification, for which the F-
score increases from 48.14 to 62.80. But this im-
provement has a smaller effect on the overall coref-
erence system performance leading to a 5% increase
of results. In contrast to the gold mention results in
the development set, we see lower precision values
in the test set. This is due to the fact that the test set
contains singletons. Detecting singletons reliably is
a difficult problem that needs further investigation.
5 Conclusion and Future Work
In the current paper, we presented the results of
UBIU in the CONLL-2011 shared task. We showed
that for a robust system for coreference resolution
such as UBIU, automatically annotated linguistic
data is sufficient for mention-pair based coreference
resolution. We also showed that ontological infor-
mation as well as filtering non-agreeing mention
pairs leads to an insignificant improvement of the
overall coreference system performance. The treat-
ment of singletons in the data remains a topic that
requires further investigation.
References
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2007. TiMBL: Tilburg memory
based learner ? version 6.1 ? reference guide. Techni-
cal Report ILK 07-07, Induction of Linguistic Knowl-
edge, Computational Linguistics, Tilburg University.
Georgiana Dinu and Sandra Ku?bler. 2007. Sometimes
less is more: Romanian word sense disambiguation
revisited. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing, RANLP 2007, Borovets, Bulgaria.
Rada Mihalcea. 2002. Instance based learning with
automatic feature selection applied to word sense
disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics,
COLING?02, Taipeh, Taiwan.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL), Portland, Oregon.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP
2009, Singapore.
Desislava Zhekova and Sandra Ku?bler. 2010. UBIU: A
language-independent system for coreference resolu-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation (SemEval), pages 96?99, Up-
psala, Sweden.
116
Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 18?27,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Speech and Gesture Interaction in an Ambient Assisted Living Lab 
   Dimitra Anastasiou SFB/TR8 Spatial Cognition, Languages Science, University of Bremen, Germany anastasiou@uni-bremen.de 
Cui Jian SFB/TR8 Spatial Cognition,  University of Bremen, Germany ken@informatik.uni-bremen.de 
Desislava Zhekova Department of Linguistics,  Indiana University, USA dzhekova@ indiana.edu 
    Abstract In this paper we describe our recent and future research on multimodal interaction in an Ambient Assisted Living Lab. Our work combines two interaction modes, speech and gesture, for multiple device control in Ambient Assisted Living environments. We conducted a user study concerning multimodal interaction between participants and an intelligent wheelchair in a smart home environment. Important empirical data were collected through the user study, which encouraged further developments on our multi-modal interactive system for Ambient Assisted Living environments. 1 Introduction Multimodal interaction has been gaining more and more importance in various application systems and domains. On one hand, it is considered as an encouraging way to improve the effectiveness and efficiency of interaction in general, and on the other hand, to increase user satisfaction in a more natural and intuitive manner (Jaimes & Sebe, 2007; Oviatt, 1999). Meanwhile, the domain of Ambient Assisted Living (AAL), although very significantly and increasingly well researched in recent literature (Steg et al, 2006; Fuchsberger, 2008; Wichert & Eberhardt, 2011), has not been enriched with profuse advanced multimodal technologies so far. 
Therefore, generally as well as particularly in assistive environments, multimodal applications are not only preferred, but also often the only solution for people who cannot master their everyday tasks by themselves. These multimodal applications are showing their necessities of compensating for the various visual, perceptual, sensory, cognitive, and motoric impairments of senior and/or disabled people. We focus on speech and gesture as two intuitive modalities which can be combined to compensate for physical and/or cognitive limitations; speech interaction for those who have motor disabilities and gesture for those with speech impairments. A Wizard-of-Oz (WoZ)-controlled user study concerning multimodal interaction between participants and an intelligent wheelchair took place in our AAL lab. A drawback in the design of gesture-based user interfaces today is the lack of experience and empirical data about which gestures are required for which activities. Our goal in the presented user study is to collect empirical speech and gesture data of natural dialogue in Human-Robot Interaction (HRI). This paper is laid out as follows: in section 2 we present the most relevant work on speech interaction in assistive environments (2.1), spatial gestures in assistive environments (2.2) and speech-gesture interaction (2.3). Section 3 gives an introduction to our Ambient Assisted Living Lab and the intelligent wheelchair. Section 4 describes the current speech interaction with the wheelchair in our assistive environment/smart home. Section 5 
18
reports on a user study focusing on the speech-gesture interaction within this environment. Conclusion and future work follow in section 6. 2 Related Work One of the main goals of AAL is to alleviate and compensate for the disabilities of its inhabitants. The latter are often predisposed to constant and permanent increase of their inability to orally express themselves and/or adequately employ elementary motoric functions. Thus, efficient and dynamic interaction of the AAL modalities should be targeted in order to balance the lack of either eloquence and/or movement. In case further deficiencies are present, additional modalities that can account for those deficiencies should be considered. For example, if a person experiences any kind of speech disorder, the speech modality can be exchanged with a typed-text interface. Yet, in the following subsections we will concentrate on related work about two common interaction modalities, speech and gesture, and the way they can interact with each other.  2.1 Speech interaction in assistive environ-ments As one of the most important interaction modalities in assistive environments, there has been a significant amount of research on speech interaction regarding different kinds of motivation and technology-dependent approaches. Some studies are focusing on gathering objective and subjective evidence for motivating and supporting further development on speech interaction. Takahashi et al (2003) collected dialogue examples and conducted a recognition experiment for the collected speech; Ivanecky et al (2011) found that the set of the commands for the house control is relatively small (usually around 50). At the same time, other research concerning general-purpose speech-enabled dialogue systems has also been reported. Goetze et al (2010) described technologies for acoustic user interaction in AAL scenarios, where they designed and evaluated a multimedia reminding and calendar system. The authors carried out an automatic speech recognition (ASR) performance study having as training set both male and female speakers of different age and hearing loss. The 
results showed that the ASR performance was lower for older persons and for female. Moreover, Becker et al (2009) carried out experiments in an assistive environment using voice recognition and pointed out that ?the speech interface is the easiest way for the user to interact with the computer-based service system?.  Furthermore, much effort has been put into considering the special requirements of assistive environments and developing the accordingly adapted interactive systems. Krajewski et al (2008) described an acoustic framework for detecting accident-prone fatigue states according to prosody, articulation and speech quality related speech characteristics for speech-based human computer interaction (HCI). Moreover, Jian et al (2012) studied, implemented, and evaluated the speech interface of a multimodal interactive guidance system based on the most common elderly-centered characteristics during interaction within assistive environments.   2.2 Spatial gestures in assistive environments We coin the term ?spatial? to describe gestures that often iconically represent spatial concepts (Rauscher et al, 1996). Alibali (2005: 307) states: ?gestures contribute to effective communication of spatial information?. She added that ?speakers tend to produce gestures when they produce linguistic units that contain spatial information, and they gesture more when talking about spatial topics than when talking about abstract or verbal ones?. Kopp (2005) has shown that gestures have sufficient specificity to be communicative of spatial infor-mation. Spatial gestures belong to representational gestures, which according to McNeill?s (1992) taxonomy can be deictic, iconic, or metaphoric. Kita (2009: 145) stated: ?representational gestures (...) that express spatial contents (?) reflect the cognitive differences in how direction, relative location and different axes in space are conceptualized and processed?. As far as spatial gestures in assistive environments is concerned, Nazemi et al (2011) conducted an experiment, where test subjects in middle age were asked to make gestures with the WiiMote to scroll, zoom, renew, and navigate in a relational database. The results showed that in complex tasks, participants employed more and various gestures. Ne?elrath et al (2011) designed a gesture-based system for context-sensitive interaction with a 
19
smart kitchen. Users had to solve interaction tasks by controlling appliances in a smart home. Recently Marinc et al (2012) presented a demonstrator that uses Kinect to recognize pointing gestures for device selection and control. When a device is selected, a graphical user interface (GUI) is shown on a screen to inform the user that the interaction has started. A hand movement to the left stops the HCI. 2.3 Speech-gesture interaction  Concerning speech accompaniment of gestures, Chovil (1992), among others, stated that speakers frequently use gesture to supplement speech. McNeill (1992) pointed out that speech and gesture must cooperate to express a person?s meaning and Goldin-Meadow (2003) stated that speech-associated gestures often convey information that complements the information conveyed in the talk they accompany and, in this sense, are meaningful. Similarly, Kendon (2004) suggests that gestures enrich the speech, helping the interlocutor to easily express concepts that will otherwise be complex to explain through speech only. McNeill (2000) points out that gestures and the synchronous speech are semantically and pragmatically co-expressive. Specifically concerning the relationship between spatial gestures and speech, Kita (2000) stated that a possible function of gesture is that gesture may help speakers to package spatial information into units suitable for verbal output. Moreover, Hostetter & Alibali (2005) regarded individual differences in gesture and found that the gesture rate was highest among individuals who had a combination of high spatial skill and low verbal skill. Furthermore, research has been carried out towards a grammar of gesture, in other words the relationship of gestures within a multimodal grammar. Fricke (2009) claimed that in German spoken language, co-speech gestures can be structurally integrated as constituents of nominal phrases, and can semantically modify the nucleus of the nominal phrases. Hahn & Rieser (2010) looked at the types of gestures co-occurring with noun phrases and their function, semantic values, and how these values interface with a natural language expression. In addition, the employment of gesture to improve the semantic analysis of the dialogues in AAL has 
gained considerable attention in the research community in the last decade. In particular the effect of gesture on the improvement of co-reference resolution (the process of determining if two phrases in a discourse refer to the same real-world entity) has been examined in a variety of studies. Eisenstein and Davis (2006) consider various gesture features and delineate their importance for the co-reference process. Chen et. al. (2011) show that when the pronominal mentions are typed and simultaneously a pointing gesture is used, the co-reference performance improves for personal and deictic pronouns. Co-reference in spoken dialogues has proven to be much more different than the one we encounter in written texts. As Strube and M?ller (2003) point out, a big number of the pronouns used in spoken dialogue have non-noun phrase (NP) antecedents or no antecedents at all, which can prove to be a challenge for the semantic analysis of dialogue in AAL. The TRAINS93 corpus study of Byron and Allen (1998) shows that about 50% of the pronouns that are used in the corpus have antecedents that are non-NP-phrases. Thus, co-reference resolution for dialogue can highly benefit from the additional information that various modalities and more specifically gesture can provide.  3 Our Ambient Assisted Living Lab and the Intelligent Wheelchair The Bremen Ambient Assisted Living Lab (BAALL) comprises all necessary conditions for trial living intended for two persons. This lab is a smart home suitable for the elderly and people with disabilities. It has an area of 60m2 and contains all standard living areas, i.e. kitchen, bathroom, bedroom, and living room. It has intelligent adaptable household appliances and furniture for compensating for special limitations, e.g. separate kitchen cabinets can be moved up and down. The lab looks like a normal apartment and the technological infrastructure is discreet, if visible at all. In the lab mobility assistance is provided through an Intelligent Wheelchair as well as an Intelligent Walker. For our studies we use the autonomous wheelchair/robot which is equipped with two laser range-sensors, wheel encoders, and an onboard computer; the wheelchair has a spoken dialogue 
20
interface that allows to navigate to predefined destinations and to control devices in the lab.  The goal of the smart environment with mobility assistants and smart furniture is to evaluate new ambient assisted living technologies regarding their everyday usability. Users can interact through various interaction modes, such as a head joystick, a touch screen, and natural language dialogue. In this paper we focus on the natural language dialogue and on contact-free, touchless, and not pen-based gestures in interaction with the wheelchair and smart furniture. Figure 1 shows a smart appliance, i.e. the kitchen cabinet, which is moving down, so that it can be reachable for the wheelchair user.    
 Figure 1. Kitchen cabinets moving down  4 Speech Interaction in our Lab Since the users of an AAL environment are typically untrained persons, elderly persons or persons possibly with physical or cognitive deficits, the user-centered analysis and adaptation of specific AAL-related application scenarios are necessary for developing a speech-enabled interactive dialogue system for our environment. In the following subsections we first describe the speech-related functionalities for the targeted users in our smart home (4.1) and then report on our recent work at the grammar level for improving the common problems caused by the automatic speech recognition (4.2). 4.1 Speech-related functionalities According to the various assistance possibilities currently provided in our AAL environment, each 
of the speech supportive functionalities can be classified based on the following three levels: ? An explicit elementary action on behalf of a simple dialogic utterance is used to ask for a specific assisting service to control each device in the AAL environment, such as ?turn on the kitchen light?, ?close the door of the bathroom? or ?drive me to my bed?, etc. ? An implicit composite action, which can be uttered by simple or longer sentences, is used to converse with the dialogue system to trigger a set of explicit elementary actions regarding a predefined yet dynamically adaptable planning component. A typical utterance of such is ?where is my pizza??, which can then result in a sequence of actions including driving the user to the kitchen, opening all the doors on the path, showing the location of the pizza either orally or using other already implemented hardware supports (e.g. blinking light). ? A context sensitive negotiating action, which can be uttered during a clarification situation, should be used on the top of the explicit and implicit actions according to the situated context. Our AAL environment is in fact a multi-agent environment, which involves necessary dialogic interaction with other agents and their activities with respect to the possible temporal and spatial conflicts. For example, if a user wants to bake a pizza and the system detects that the oven is being used, the system would inform the user about it, then the user should be allowed to say ?then take me to the oven when it?s available?. In order to support the above three speech-enabled dialogic activities, a general dialogue system framework, the Diaspace Adaptive Information State Interaction Executive (DAISIE, cf. Ross & Bateman, 2009), is investigated and accordingly being extended. DAISIE is a tightly coupled information state-based (see Larsson & Traum, 2000) dialogue backbone that fuses a formal language based dialogue controller, which provides a complex yet easily reusable plug-in mechanism for domain specific applications. Figure 2 depicts the general architecture of DAISIE.  
21
 Figure 2. DAISIE with its plug-in components  According to the requirements of an operational dialogue system, the DAISIE architecture consists of a set of principle processing components, which are classified into the following three functionality groups:  ? The DAISIE Plug-ins include a range of common language technology resources, such as speech recognizers/synthesizers, language parsers/generators, etc. ? The DAISIE Basic System integrates all the DAISIE plug-ins with the information state structure, knowledge monument component and the formal language based dialogue controller into a basic functional dialogue system. ? The DAISIE Application Framework specifies the application dependent components and provides a direct interface between the concrete domain application and the DAISIE basic system.  An instantiation of DAISIE is being developed and tested, which includes the implementation ranging over all levels of linguistic and conceptual representation and reasoning, as well as the adaptation of the current hybrid unified dialogue model (cf. Shi et al, 2011) to the AAL environment application regarding the listed three speech-related activities and possibly further modalities, such as gesture (see section 5). 4.2 Foot-syllable Grammar  Reliable speech recognition is a key factor for the success of a dialogue system in our AAL 
environment. Currently, the expression stratum of the language system is modeled with two components in the DAISIE Framework: a speech recognizer for understanding spoken text and speech synthesizer for producing it. We use VoCon1 as our speech recognizer, which takes a restriction grammar to know which commands the AAL and the wheelchair can undertake.  A foot-syllable restriction grammar was developed for optimized performance (Couto Vale & Mast, 2012). This grammar has a three-level structure starting at the lowest level with the syllable (S), an intermediate structure named foot (F), and the clause (C). A foot is a rhythmic unit in the compositional hierarchy of spoken language, which contains syllables as its parts and which is part of a curve (Halliday & Matthiessen, 2004). In German, it is composed by one stressed syllable and its adjacent unstressed ones. Below we present a segment of the grammar:  Foot-Syllable Grammar <Foot1>     : <IndIBegin> <kYStrong> <CEWeak> ; <IndIBegin> : <InWeak> <dIWeak> | <IWeak> <nIWeak> ; <kYStrong>  : 'kY !pronounce("'kY") ; <CEWeak>    : CE !pronounce("CE") | C$ !pronounce ("C$") ; <IWeak>     : I  !pronounce("I")  | $  !pronounce ("$")  ; <InWeak>    : In !pronounce("In") | $n !pronounce ("$n") ; <nIWeak>    : nI !pronounce("nI") | n$ !pronounce ("n$") ; <dIWeak>    : dI !pronounce("dI") | d$ !pronounce ("d$") ;  The foot-syllable grammar (FS) was contrasted with a foot-word (FW) and a phrase-word grammar (PW) in speech recognition effectiveness. The foot helped enforce corpus-based restrictions on syntactic structures and the syllable gave fine control over phonological variation. We conducted an evaluation study and our results have shown that, for complex highly flexible natural language dialogue situations such as human-robot interaction in AAL, a restriction grammar such as our foot-syllable grammar outperforms the other two approaches: 51,81% (FS) of correctly recognized utterances versus 26,67% (FW) and 6,67% (PW). We argue that using phonological units, such as syllables and foot units, provides a better way to achieve high recognition performance than phrases and words in both development cost and effectiveness. 
                                                      1 http://www.nuance.com/for-business/by-product/automotive-products-services/vocon3200/index.htm, 19/03/2012  
22
5 Speech-Gesture User Study A user study was conducted in our lab in November-December 2011. This user study included a real-life everyday scenario of a human user using a wheelchair to navigate in their environment by means of speech and gesture. The goal was to observe whether people would gesture and how, and what they would say if they used a wheelchair in their domotic environment. The study took place in BAALL and 20 German participants (students) took part in the study (mean age 25). Older users were not considered as participants in this study, as various tests, such as OsteoArthritis screening, neuropsychological tests, memory tests etc. would have to be taken in order to make sure that the elderly are physically able of performing gestures. Furthermore, it is difficult to bring seniors to the lab due to their physical condition. Elderly users might also be digitally intimidated by such technology. Although the tested group and the prospective user group are divergent, our user study primarily focuses on the collection of empirical gesture-speech data through the interaction of participants with technical devices in a smart home and thus does not distinguish between participants based on their age. The participants were asked to act as if they were dependent on the wheelchair called Rolland. They had to navigate with Rolland to carry out daily activities (wash their teeth, eat something, read a book). They were informed in advance about the goal of the study, i.e. the collection of speech-gesture data and the video recording. The participants used a Bluetooth head-set and their activities were recorded by two digital IP cameras placed in BAALL, and also an SLR camera on Rolland?s back. Through audio and video streaming an experimenter (WoZ) selected through a GUI the destination point of Rolland. It is important to note again that we are interested into collecting various empirical spoken commands and gestures produced by the participants in their interaction with the wheelchair during the experimental run. Thus both the wheelchair navigation and Rolland's speech feedback were WoZ-controlled. During most of the tasks the user was sitting on the wheelchair, but in one task the wheelchair drove autonomously without the user, as differences in gesture may change based on the 
recipient (see discussion in Rim? & Schiaratura, 1991). Technical problems appeared in 8 sessions out of 20, when Rolland did not drive to the desired destination. The reasons for this are outside the scope of our research and of this paper. We evaluated 12 sessions regarding speech, but all 20 sessions regarding gesture. As far as the results of this study are concerned, we collected 317 spoken commands in total. Many different language variants were uttered in order to carry out the same task. For example, four distinct utterances which were produced when participants were sitting on the bed and asked Rolland to come to them follow: i) ?Rolland, komm her?  (Rolland, come here) ii) ?Rolland, <break9secs> Rolland, <break3secs> komm her?  (Rolland, Rolland, come here) iii) ?Rolland, komm bitte zum Bett, hier wo ich sitze?  (Rolland, please come to the bed, here where I  am sitting)  iv)  ?Rolland, fahr zum Bett?  (Rolland, drive to the bed)  Thus many context-sensitive utterances appeared in the collected data; for example, in the first two utterances above the participants did not use the name of a landmark (bed) in their command. Moreover, in the second example above we see that the participant waited for a backchannel feedback from the wheelchair and then uttered the actual command (come here).  From the study also the attitude, e.g. politeness, and expectations of the participants against the robot were measured. The style, volume of utterance, waiting time for the wheelchair to react as well as the sentence structure and lexical content were measurement factors. For example, male participants used more direct style with imperative sentences than female and included the name of their wheelchair in their command. Concerning gestural frequency during the user study, in 7 sessions out of 20 participants employed at least one gesture during a session. In 6 sessions participants used deictic/pointing gestures and in 1 an iconic gesture (rubbing hands under the tap to represent washing hands). In 2 of the 7 sessions participants gestured more than once, while in the remaining 5 sessions, they gestured 
23
once. The participants gestured mostly when something happened out of order, e.g. the wheelchair drove to a wrong place or stopped too far from the participant. Particularly in the bathroom, the wheelchair could not drive very close to the washbasin (predefined destination) and thus many participants gestured so that the wheelchair moves closer.  Two exceptions on gestural types and frequency were the following: in one case a male participant used often iconic gestures ?for fun?, e.g. representing that he holds a gun. Another participant (female) gestured constantly using pointing gestures during all activities that she carried out. These cases can be attributed to personal influences, e.g. the user?s personality (see Rehm et al, 2008). The gestures are annotated with the tool ANVIL (Kipp et al 2007), a free video annotation tool that offers multi-layered annotation. The gesture anno-tation conventions for gesture, form, space, and trajectory, which are based on the practice of N. Furuyama (see McNeill, 2005: 273-278), are followed. As far as co-reference is concerned, in all 317 commands there were several instances that the participants employed in their utterances. Yet, all of them were either references to the participant him/herself or to the wheelchair:  i) ?Rolland, drehe dich bitte um?  (Rolland, turn around please) ii) ?Fahr mich bitte zum Badezimmer?  (Drive me to the bathroom please)  For those cases, the participants did not use gestures. We assume that the rare use of co-reference in our user study is due to the fact that participants almost never had to refer to the same entity again. Once a command was uttered, the WoZ executed the required action and the participants could move further to the next task they had in their agendas. Thus, once an entity (i.e. the sofa, the bathroom, the kitchen) has been introduced, the participants never needed to refer back to that same entity again. Last but not least, nobody of the participants realized that the experiment was WoZ, believing that the wheelchair moved based on their own commands. 
6 Conclusion and Future Work Our lab is equipped with intelligent adaptable household appliances and furniture for compensating for special limitations; this lab can be used as an experiment area for many user studies with different purposes.  In the presented user study we collected empirical speech and gesture data of natural dialogue in HRI. By making the speech-gesture interaction between users and robot more natural, intuitive, effective, efficient, and user-friendly, assistive environments will become more appropriate in the real world used by seniors and/or seniors to be, people actively planning their future. A planned second user study handles selection and control of objects in a smart environment. In this study objects such as television, lights, electronic sliding doors, etc. will be remotely controlled by the experimenter (WoZ). The participants will be requested to select objects and control their position and level (higher, louder, etc.). A condition tested here will be the presence/lack of ambiguity. Participants will be asked to ?open a door? or ?turn on a light? having many doors and lights available in the lab. In addition, the wheelchair will be intentionally driven by an experimenter to a wrong destination or stopped on its way to a destination point. This adjustment has been made considering the results in the conducted study that participants gestured more when something went wrong. A third study is planned in order to identify which spatial gestures are universal and which are locale-dependent. Within the field of localization, locale is a combination of language and culture. The criteria of locale selection are countries with i) big geographic distance, ii) strong cultural differences, iii) diversity of gestures based on literature evidence, and iv) typologically different languages. This study is necessary to investigate the differences in speech-gesture interaction between the German and other locales.  Last but not least, a small scale follow-up study with elderly people will take place in a nursing home. There the elderly could be requested to perform gestures that have already been collected in our previous studies in order to evaluate them depending on their skills and preferences. The collected data from the user studies stored in a corpus will be examined based on the speech-
24
gesture alignment concerning their semantics, their temporal arrangement, and their coordinated organization in the phrasal structure. Later an extension for the semantics of gesture types will be added to the Generalized Upper Model (GUM) in order to anchor spatial gestures into a semantic spatial representation. GUM (Bateman et al, 2010) is a linguistically motivated ontology for the semantics of spatial language of German and English. New GUM categories will be created for gestures, when the linguistic ones are not applicable and/or sufficient. Acknowledgments We gratefully acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) through the Collaborative Research Center SFB/TR 8 Spatial Cognition. We also thank Daniel Vale, Bernd Gersdorf, Thora Tenbrink, Carsten Gendorf, and Vivien Mast for their help with the user study. References  M. Alibali. 2005. Gesture in spatial cognition: expressing, communicating, and thinking about spatial information. Spatial Cognition and Computation, 5(4):307-331. J. Bateman, J. Hois, R. Ross, and T. Tenbrink. 2010. A linguistic ontology of space for natural language processing. Artificial Intelligence, 174(14): 1027-1071. E. Becker, Z. Le, K. Park, Y. Lin and F. Makedon. 2009. Event-based experiments in an assistive environment using wireless sensor networks and voice recognition. Proceedings of the International conference on Pervasive technologies for assistive environments (PETRA). D.K. Byron and J.F. Allen. 1998. Resolving demonstrative pronouns in the TRAINS93 corpus. New Approaches to Discourse Anaphora: Proceedings of the 2nd Colloquium on Discourse Anaphora and Anaphor Resolution (DAARC2), 68-81. L. Chen, A. Wang and B. Di Eugenio, B. 2011. Improving pronominal and deictic co-reference resolution with multi-modal features. Proceedings of the SIGDIAL Conference, 307-311. 
N. Chovil. 1992. Discourse-oriented facial displays in conversation. Research on Language and Social Interaction, 25:163-194. D. Couto Vale and V. Mast. 2012. Customizing Speech Recognizers for Situated Dialogue Systems. Proceedings of the 15th International Conference on Text, Speech and Dialogue.  J. Eisenstein and R. Davis. 2006. Gesture improves coreference resolution. Proceedings of the Human Language Technology Conference of the NAACL, 37-40. E. Fricke. 2009. Multimodal attribution: How gestures are syntactically integrated into spoken language. Proceedings of GESPIN: Gesture and Speech in Interaction. V. Fuchsberger. 2008. Ambient assisted living: elderly people?s needs and how to face them. Proceedings of the 1st ACM International Workshop on Semantic Ambient Media Experiences, 21-24. S. Goetze, N. Moritz,  J.E. Appell, M. Meis, C. Bartsch and J. Bitzer. 2010. Acoustic user interfaces for ambient-assisted living technologies. Inform Health Soc Care, 35(3-4):125-143. S. Goldin-Meadow. 2003. Hearing gesture: How our hands help us think. Cambridge, MA: Harvard University Press. F. Hahn and H. Rieser. 2010. Explaining speech gesture alignment in MM dialogue using gesture typology. P. Lupowski and M. Purver (Eds.), Proceedings of the 14th Workshop on the Semantics and Pragmatics of Dialogue (SemDial), 99-111. F.M.A.K. Halliday and C.M.I.M. Matthiessen 2004. An introduction to functional grammar. 3rd Edition. Edward Arnold, London.  A. Hostetter and M. Alibali. 2005. Raise your hand if you?re spatial?Relations between verbal and spatial skills and gesture production. Gesture, 7(1): 73-95. J. Ivanecky, S. Mehlhase and M. Mieskes, M. 2011. An Intelligent House Control Using Speech Recognition with Integrated Localization. R. Wichert and B. Eberhardt, B. (Eds.), 4. AAL Kongress. Berlin, Germany. 
25
A. Jaimes and N. Sebe. 2007. Multimodal human-computer interaction: A Survey, Computational Vision and Image Understanding. Elsevier Science Inc., New York, USA, 116-134. C. Jian, F. Schafmeister, C. Rachuy, N. Sasse, H. Shi, H. Schmidt and N.v. Steinb?chel. 2012. Evaluating a Spoken Language Interface of a Multimodal Interactive Guidance System for Elderly Persons. Proceedings of the International Conference on Health Informatics.  A. Kendon. 2004. Gesture: Visible action as utterance. Cambridge: Cambridge University Press. S. Kita. 2000. How representational gestures help speaking. McNeill, D. (Ed.), Language and gesture, Cambridge, UK: Cambridge University Press, 162-185. S. Kita. 2009. Cross-cultural variation of speech-accompanying gesture: A review. Language and Cognitive Processes, 24(2): 145-167. M. Kipp, M. Neff and I. Albrecht. 2007. An annotation scheme for conversational gestures: How to economically capture timing and form. Language Resources and Evaluation Journal, 41: 325-339. S. Kopp. 2005. The spatial specificity of iconic gestures. Proceedings of the 7th International Conference of the German Cognitive Science Society, 112-117. J. Krajewski, R., Wieland and A. Batliner. 2008. An acoustic Framework for detecting Fatique in Speech based Human Computer Interaction. Proceedings of the 11th International Conference on Computers Help People with Special Needs, 54-61. S. Larsson and D. Traum. 2000. Information State and Dialogue Management in the TRINDI Dialogue Move Engine Toolkit. Natural Language Engineering. Special Issue on Best Practice in Spoken Language Dialogue Systems Engineering, 323-340.  A. Marinc, C. Stockl?w and S. Tazari, S. 2012. 3D Interaktion in AAL Umgebungen basierend auf Ontologien. Proceedings of AAL Kongress. 
D. McNeill. 1992. Hand and Mind: What Gestures reveal about Thought. University of Chicago Press. D. McNeill. 2000. Introduction. McNeill, D. (Ed.), Language and gesture. Cambridge: Cambridge University Press. D. McNeill. 2005. Gestures and Thought. University of Chicago Press.  K. Nazemi, D. Burkhardt, C. Stab, M. Breyer, R. Wichert and D.W. Fellner. 2011. Natural gesture interaction with accelerometer-based devices in ambient assisted environments. R. Wichert and B. Eberhardt, B. (Eds.), 4. AAL-Kongress, Springer, 75-84. R. Ne?elrath, C. Lu, C.H. Schulz, J., Frey, and J. Alexandersson. 2011. A gesture based system for context-sensitive interaction with smart homes. R. Wichert and B. Eberhardt, B. (Eds.), 4. AAL-Kongress, 209-222. S. T. Oviatt. 1999. Ten myths of multimodal interaction. Communications of the ACM. ACM New York, USA, 42(11): 74-81. F.H. Rauscher, R.M. Krauss and Y. Chen. 1996. Gesture, speech, and lexical access: The role of lexical movements in speech production. Psychological Science, 7: 226-230. M. Rehm, N. Bee and E., Andr?. 2008. Wave like an Egyptian: accelerometer-based gesture recognition for culture specific interactions. Proceedings of the 2nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction, 1:13-22. B. Rim? and L. Schiaratura. 1991. Gesture and speech. Fundamentals of nonverbal behavior. Studies in emotion & social interaction, 239-281. J. R. Ross and J. Bateman. 2009. Daisie: Information State Dialogues for Situated Systems. Proceedings of Text, Speech and Dialogue, 5729/2009, 379-386. H. Shi, C. Jian and C. Rachuy. 2011. Evaluation of a Unified Dialogue Model for Human-Computer Interaction. International Journal of Computational Linguistics and Applications, 2. H. Steg, H. Strese, C. Loroff, J. Hull and S. Schmidt. 2006. Europe is facing a demographic 
26
challenge ambient assisted living offers solutions. VDI/VDE/IT, Berlin, Germany. M. Strube and C. M?ller. 2003. A machine learning approach to pronoun resolution in spoken dialogue. Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, 1:168-175.   S. Takahashi, T. Morimoto, S. Maeda and N. Tsuruta. 2003. Dialogue experiment for elderly people in home health care system. Proceedings of the 6th International Conference on Text, Speech and Dialogue, 418-423. R. Wichert and B. Eberhardt, B. (Eds.). 2011. Ambient Assisted Living. 4. AAL-Kongress, Springer.   
27
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 88?94,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
UBIU for Multilingual Coreference Resolution in OntoNotes
Desislava Zhekova Sandra Ku?bler Joshua Bonner Marwa Ragheb Yu-Yin Hsu
Indiana University
Bloomington, IN, USA
{dzhekova, skuebler, jebonner, mragheb, hsuy}@indiana.edu
Abstract
The current work presents the participa-
tion of UBIU (Zhekova and Ku?bler, 2010)
in the CoNLL-2012 Shared Task: Model-
ing Multilingual Unrestricted Coreference in
OntoNotes (Pradhan et al, 2012). Our system
deals with all three languages: Arabic, Chi-
nese and English. The system results show
that UBIU works reliably across all three lan-
guages, reaching an average score of 40.57 for
Arabic, 46.12 for Chinese, and 48.70 for En-
glish. For Arabic and Chinese, the system pro-
duces high precision, while for English, preci-
sion and recall are balanced, which leads to
the highest results across languages.
1 Introduction
Multilingual coreference resolution has been gain-
ing considerable interest among researchers in re-
cent years. Yet, only a very small number of sys-
tems target coreference resolution (CR) for more
than one language (Mitkov, 1999; Harabagiu and
Maiorano, 2000; Luo and Zitouni, 2005). A first
attempt at gaining insight into the comparability of
systems on different languages was accomplished in
the SemEval-2010 Task 1: Coreference Resolution
in Multiple Languages (Recasens et al, 2010). Six
systems participated in that task, UBIU (Zhekova
and Ku?bler, 2010) among them. However, since sys-
tems participated across the various languages rather
irregularly, Recasens et al (2010) reported that the
data points were too few to allow for a proper com-
parison between different approaches. Further sig-
nificant issues concerned system portability across
the various languages and the respective language
tuning, the influence of the quantity and quality of
diverse linguistic annotations as well as the perfor-
mance and behavior of various evaluation metrics.
The CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes (Pradhan et al,
2011) targeted unrestricted CR, which aims at iden-
tifying nominal coreference but also event corefer-
ence, within an English data set from the OntoNotes
corpus. Not surprisingly, attempting to include such
event mentions had a detrimental effect on over-
all accuracy, and the best performing systems (e.g.,
(Lee et al, 2011)) did not attempt event anaphora.
The current shared task extends the task definition to
three different languages (Arabic, Chinese and En-
glish), which can prove challenging for rule-based
approaches such as the best performing system from
2011 (Lee et al, 2011).
In the current paper, we present UBIU, a memory-
based coreference resolution system, and its re-
sults in the CoNLL-2012 Shared Task. We give an
overview of UBIU in Section 2. In Section 3, we
present the system results, after which Section 4 lays
out some conclusive remarks.
2 UBIU
UBIU (Zhekova and Ku?bler, 2010) is a corefer-
ence resolution system designed specifically for a
multilingual setting. As shown by Recasens et al
(2010), multilingual coreference resolution can be
approached by various machine learning methods
since machine learning provides a possibility for ro-
bust abstraction over the variation of language phe-
nomena and specificity. Therefore, UBIU employs88
a machine learning approach, memory-based learn-
ing (MBL) since it has proven to be a good so-
lution to various natural language processing tasks
(Daelemans and van den Bosch, 2005). We em-
ploy TiMBL (Daelemans et al, 2010), which uses
k nearest neighbour classification to assign class la-
bels to the targeted instances. The classifier set-
tings we used were determined by a non-exhaustive
search over the development data and are as follows:
the IB1 algorithm, similarity is computed based on
weighted overlap, gain ratio is used for the relevance
weights and the number of nearest neighbors is set to
k=3 (cf. (Daelemans et al, 2010) for an explanation
of the system parameters).
In UBIU, we use a pairwise mention model (Soon
et al, 2001; Broscheit et al, 2010) since this model
has proven more robust towards multiple languages
(Wunsch, 2009) than more elaborate ones. We con-
centrate on nominal coreference resolution, i.e. we
ignore the more unrestricted cases of event corefer-
ence. Below, we describe the modules used in UBIU
in more detail.
2.1 Preprocessing
The preprocessing module oversees the proper for-
matting of the data for all modules applied in later
stages during coreference resolution. During pre-
processing, we use the speaker information, if pro-
vided, and replace all 1st person singular pronouns
from the token position with the information pro-
vided in the speaker column and adjust the POS tag
correspondingly.
2.2 Mention Detection
Mention detection is the process of detecting the
phrases that are potentially coreferent and are thus
considered candidates for the coreference process.
Mention detection in UBIU is based on the parse and
named entity information provided by the shared
task. This step is crucial for the overall system per-
formance, and we aim for high recall at this stage.
Singleton mentions that are added in this step can
be filtered out in later stages. However, if we fail
to detect a mention in this stage, it cannot be added
later. We predict a mention for each noun phrase and
named entity provided in the data. Additionally, we
extract mentions for possessive pronouns in English
as only those did not correspond to a noun phrase in
MD
R P F1
Arabic 97.13 19.06 31.87
Chinese 98.33 31.64 47.88
English 96.73 30.75 46.67
Table 1: Mention detection (development set).
the syntactic structure provided by the task. In Ara-
bic and Chinese, possessives are already marked as
noun phrases.
The system results on mention detection on the
development set are listed in Table 1. The results
show that we reach very high recall but low preci-
sion, as intended. The majority of the errors are due
to discrepancies between noun phrases and named
entities on the one hand and mentions on the other.
Furthermore, since we do not target event corefer-
ence, we do not add mentions for the verbs in the
data, which leads to a reduction of recall.
In all further system modules, we represent a
mention by its head, which is extracted via heuris-
tic methods. For Arabic, we select the first noun or
pronoun while for Chinese and English, we extract
the the pronoun or the last noun of a mention unless
it is a common title. Additionally, we filter out men-
tions that correspond to types of named entities that
in a majority of the cases in the training data are not
coreferent (i.e. cardinals, ordinals, etc.).
One problem with representing mentions mostly
by their head is that it is difficult to decide between
the different mention spans of a head. Since auto-
matic mentions are considered correct only if they
match the exact span of a gold mention, we include
all identified mention spans for every extracted head
for classification, which can lead to losses in evalu-
ation. For example, consider the instance from the
development set in (1): the noun phrase the Avenue
of Stars is coreferent and thus marked as a gold men-
tion (key 7). UBIU extracts two different spans for
the same head Avenue: the Avenue (MD 3) and the
Avenue of Stars (MD 5).
(1)
token POS parse key MD output
the DT (NP(NP* (7 (3|(5 (9
Avenue NNP *) - 3) 9)
of IN (PP* - - -
Stars NNPS (NP*))) 7) (4)|5) -
Both mention spans are passed to the coreference
resolver, together with additional features (i.e. men-89
MD MUC B3 CEAFE Average
F1 F1 F1 F1 F1
long 100.0 100.0 100.0 100.0 100.0
short 50.00 0 66.66 66.66 44.44
Table 2: The scores for the short example in (1).
tion length, head modification, etc.) that will allow
the resolver to distinguish between the spans. The
classifier decides that the shorter mention is coref-
erent and that the longer mention is a singleton. In
order to show the effect of this decision, we assume
that there is one coreferent mention to key 7. We
consider the two possible spans and show the re-
spective scores in Table 2. The evaluation in Table 2
shows that providing the correct coreference link but
the wrong, short mention span, the Avenue, has con-
siderable effects to the overall performance. First,
as defined by the task, the mention is ignored by all
evaluation metrics leading to a decrease in mention
detection and coreference performance. Moreover,
the fact that this mention is ignored means that the
second mention becomes a singleton and is not con-
sidered by MUC either, leading to an F1 score of 0.
This example shows the importance of selecting the
correct mention span.
2.3 Singleton Classification
A singleton is a mention which corefers with no
other mention, either because it does not refer to any
entity or because it refers to an entity with no other
mentions in the discourse. Because singletons com-
prise the majority of mentions in a discourse, their
presence can have a substantial effect on the perfor-
mance of machine learning approaches to CR, both
because they complicate the learning task and be-
cause they heavily skew the proportion in the train-
ing data towards negative instances, which can bias
the learner towards assuming no coreference relation
between pairs of mentions. For this reason, informa-
tion concerning singletons needs to be incorporated
into the CR process so that such mentions can be
eliminated from consideration.
Boyd et al (2005), Ng and Cardie (2002), and
Evans (2001) experimented with machine learning
approaches to detect and/or eliminate singletons,
finding that such a module provides an improve-
ment in CR performance provided that the classifier
# Feature Description
1 the depth of the mention in the syntax tree
2 the length of the mention
3 the head token of the mention
4 the POS tag of the head
5 the NE of the head
6 the NE of the mention
7 PR if the head is premodified, PO if it is not; UN otherwise
8 D if the head is in a definite mention; I otherwise
9 the predicate argument corresponding to the mention
10 left context token on position token -3
11 left context token on position token -2
12 left context token on position token -1
13 left context POS tag of token on position token -3
14 left context POS tag of token on position token -2
15 left context POS tag of token on position token -1
10 right context token on position token +1
11 right context token on position token +2
12 right context token on position token +3
13 right context POS tag of token on position token +1
14 right context POS tag of token on position token +2
15 right context POS tag of token on position token +3
16 the syntactic label of the mother node
17 the syntactic label of the grandmother node
18 a concatenation of the labels of the preceding nodes
19 C if the mention is in a PP; else I
Table 3: The features used by the singleton classifier.
does not eliminate non-singletons too frequently. Ng
(2004) additionally compared various feature- and
constraint-based approaches to incorporating single-
ton information into the CR pipeline. Feature-based
approaches integrate information from the single-
ton classifier as features while constraint-based ap-
proaches filter singletons from the mention set. Fol-
lowing these works, we include a k nearest neigh-
bor classifier for singleton mentions in UBIU with
19 commonly-used features described below. How-
ever, unlike Ng (2004), we use a combination of the
feature- and constraint-based approaches to incorpo-
rate the classifier?s results.
Each training/testing instance represents a noun
phrase or a named entity from the data together with
features describing this phrase in its discourse. The
list of features is shown in Table 3. The instances
that are classified by the learner as singletons with
a distance to their nearest neighbor below a thresh-
old (i.e., half the average distance observed in the
training data) are filtered from the mention set, and
are thus not considered in the pairwise coreference
classification. For the remainder of the mentions, the
class that the singletons classifier has assigned to the
instance is used as a feature in the coreference clas-
sifier. Experiments on the development set showed90
MD MUC B3 CEAFE Average
F1 F1 F1 F1 F1
Arabic
+SC 58.36 34.75 58.26 37.39 43.47
-SC 56.12 34.96 58.52 36.05 43.18
Chinese
+SC 52.30 42.70 61.11 32.86 45.56
-SC 50.40 41.19 60.96 32.47 44.87
English
+SC 67.38 53.20 59.23 34.90 49.11
-SC 65.55 51.57 59.18 34.38 48.38
Table 4: Evaluation of using (+SC) or not (-SC) the sin-
gleton classifier in UBIU on the development set.
that the most important features across all languages
are the POS tag of the head word, definiteness, and
the mother node in the syntactic representation. In-
formation about head modification is helpful for En-
glish and Arabic, but not for Chinese.
The results of using the singleton classifier in
UBIU on the development set are shown in Table 4.
They show a moderate improvement for all evalu-
ation metrics and all languages, with the exception
of MUC and B3 for Arabic. The most noticeable
improvement can be observed in mention detection,
which gains approx. 2% in all languages. A man-
ual inspection of the development data shows that
the version using the singleton classifier extracts a
slightly higher number of coreferent mentions than
the version without. However, the reduction of men-
tions that are never coreferent, which was the main
goal of the singleton classifier, is also present in the
version without the classifier, so that the results of
the classifier only have a minimal influence on the
final results.
2.4 Coreference Classification
Coreference classification is the process in which
all identified mentions are paired up and features
are extracted to build feature vectors that represent
the mention pairs in their context. Each mention
is represented in the feature vector by its syntactic
head. The vectors for the pairs are then used by the
memory-based learner TiMBL.
As anaphoric mentions, we consider all definite
phrases; we then create a pair for each anaphor with
each mention preceding it within a window of 10
(English, Chinese) or 7 (Arabic) sentences. We con-
sider a shorter window of sentences for Arabic be-
cause of its NP-rich syntactic structure and its longer
sentences, which leads to an increased number of
possible mention pairs. The set of features that we
use, listed in Table 5, is an extension of the set by
Rahman and Ng (2009). Before classification, we
apply a morphological filter, which excludes vectors
that disagree in number or gender (applied only if
the respective information is provided or can be de-
duced from the data).
Both the anaphor and the antecedent carry a la-
bel assigned to them by the singletons classifier.
Yet, we consider as anaphoric only the heads of
definite mentions. Including a feature representing
the class assigned by the singletons classifier for
each anaphor triggers a conservative learner behav-
ior, i.e., fewer positive classes are assigned. Thus, to
account for this behavior, we ignore those labels for
the anaphor and include only one feature (no. 25 in
Table 5) in the vector for the antecedent.
2.5 Postprocessing
In postprocessing, we create the equivalence classes
of mentions that were classified as coreferent and
# Feature Description
1 mj - the antecedent
2 mk - the mention (further m.) to be resolved
3 C if mj is a pronoun; else I
4 C if mk is a pronoun; else I
5 the concatenated values of feature 3 and feature 4
6 C if the m. are the same string; else I
7 C if one m. is a substring of the other; else I
8 C if both m. are pronominal and are the same string; else I
9 C if both are non-pronominal and are the same string; else I
10 C if both are pronouns; I if neither is a pronoun; else U
11 C if both are proper nouns; I if neither is; else U
12 C if both m. have the same speaker; I if they do not
13 C if both m. are the same named entity; I if they are not and
U if they are not assigned a NE
14 token distance between mj and mk
15 sentence distance between mj and mk
16 normalised levenstein distance for both m.
17 PR if mj is premodified, PO if it is not; UN otherwise
18 PR if mk is premodified, PO if it is not; UN otherwise
19 the concatenated values for feature 17 and 18
20 D if mj is in a definite m.; I otherwise
21 C if mj is within the subject; I-within an object; U otherwise
22 C if mk is within the subject; I-within an object; U otherwise
23 C if neither is embedded in a PP; I otherwise
24 C if neither is embedded in a NP; I otherwise
25 C if mj has been classified as singleton; I otherwise
26 C if both are within ARG0-ARG4; I-within ARGM; else U
27 C if mj is within ARG0-ARG4; I-within ARGM; else U
28 C if mk is within ARG0-ARG4; I-within ARGM; else U
29 concatenated values for features 27 and 28
30 the predicate argument label for mj
31 the predicate argument label for mk
32 C if both m. agree in number; else I
33 C if both m. agree in gender; else I
Table 5: The features used by the coreference classifier.91
MD MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
Automatic Mention Detection
auto
Arabic 27.54 80.34 41.02 19.64 62.13 29.85 41.91 90.72 57.33 56.79 24.81 34.53 40.57
Chinese 35.12 72.52 47.32 31.19 57.97 40.56 49.49 77.65 60.45 45.92 25.24 32.58 44.53
English 65.78 68.49 67.11 54.28 52.79 53.52 62.26 54.90 58.35 33.52 34.96 34.22 48.70
gold
Arabic 28.00 82.21 41.78 15.47 45.92 23.15 39.22 84.86 53.65 55.10 24.22 33.65 36.82
Chinese 37.84 74.84 50.27 33.95 60.29 43.44 50.95 77.28 61.41 46.68 26.13 33.50 46.12
English 66.05 69.62 67.79 54.45 53.59 54.02 61.66 55.62 58.48 33.82 34.65 34.23 48.91
Gold Mention Boundaries
auto
Arabic 27.48 75.53 40.29 18.75 56.47 28.16 42.67 89.25 57.74 55.53 25.36 34.82 40.24
Chinese 36.97 73.98 49.30 32.09 58.30 41.39 49.43 77.38 60.32 46.35 25.71 33.07 44.93
English 66.45 70.91 68.61 54.96 54.67 54.82 61.85 55.60 58.56 34.38 34.67 34.53 49.30
gold
Arabic 28.06 82.39 41.87 15.56 46.18 23.28 39.23 84.95 53.67 55.10 24.20 33.63 36.86
Chinese 37.89 74.79 50.30 33.93 60.19 43.39 50.87 77.27 61.35 46.62 26.13 33.49 46.08
English 65.82 71.72 68.65 54.68 55.51 55.09 61.22 56.59 58.82 34.85 34.04 34.44 49.45
Gold Mentions
auto
Arabic 100 100 100 42.48 80.36 55.58 50.87 89.69 64.92 71.96 34.52 46.66 55.72
Chinese 100 100 100 42.02 79.57 55.00 50.22 80.81 61.94 60.27 27.08 37.37 51.44
English 100 100 100 68.38 78.11 72.92 63.04 58.60 60.74 52.64 37.10 43.53 59.06
gold
Arabic 100 100 100 45.58 73.27 56.20 52.27 82.35 63.95 70.17 37.54 48.91 56.35
Chinese 100 100 100 44.12 80.89 57.10 51.79 80.53 63.04 60.37 27.69 37.96 52.70
English 100 100 100 68.54 78.10 73.01 63.14 58.63 60.80 52.84 37.44 43.83 59.21
Table 6: UBIU system performance in the shared task.
insert the appropriate class/entity IDs in the data,
removing mentions that constitute a class on their
own ? singletons. We bind all pronouns (except the
ones that were labeled as singletons by the singleton
classifier) that were not assigned an antecedent to
the last seen subject and if such is not present to the
last seen mention. We consider all positively classi-
fied instances in the clustering process.
3 Evaluation
The results of the final system evaluation are pre-
sented in Table 6. Comparing the results for mention
detection (MD) on the development set (see Table 1,
which shows MD before the resolution step) and the
final test set (Table 6, showing MD after resolution
and the deletion of singletons), we encounter a rever-
sal of precision and recall tendencies (even though
the results are not fully comparable since they are
based on different data sets). This is due to the fact
that during mention detection, we aim for high re-
call, and after coreference resolution, all mentions
identified as singletons by the system are excluded
from the answer set. Thus mentions that are coref-
erent in the key set but wrongly classified in the an-
swer set are removed, leading to a decrease in re-
call. With regard to MD precision, a considerable
increase is recorded, showing that the majority of
the mentions that the system indicates as coreferent
have the correct mention spans. Additionally, the
problem of selecting the correct span (as described
in Section 2) is another factor that has a considerable
effect on precision at that stage ? mentions that were
accurately attached to the correct coreference chain
are not considered if their span is not identical to the
span of their counterparts in the key set.
Automatic Mention Detection In the first part in
Table 6, we show the system scores for UBIU?s per-
formance when no mention information is provided
in the data. We report both gold (using gold linguis-
tic annotations) and auto (using automatically an-
notated data) settings. A comparison of the results
shows that there are only minor differences between
them with gold outperforming auto apart from Ara-
bic for which there is a drop of 3.75 points in the
gold setting. However, the small difference between
all results shows that the quality of the automatic an-
notation is good enough for a CR system and that
further improvements in the quality of the linguistic
information will not necessarily improve CR.
If we compare results across languages, we see
that Arabic has the lowest results. One of the rea-
sons for this decreased performance can be found in
the NP-rich syntactic structure of Arabic. This leads
to a high number of identified mentions and in com-
bination with the longer sentence length to a higher92
number of training/test instances. Another reason
for the drop in performance for Arabic can be found
in the lack of annotations expected by our system
(named entities and predicted arguments) that were
not provided by the task due to time constraints and
the accuracy of the annotations. Further, Arabic is
a morphologically rich language for which only the
simplified standard POS tags were provided and not
the gold standard ones that contain much richer and
thus more helpful morphology information.
The results for Chinese and English are relatively
close. We can also see that the CEAFE results are
extremely close, with a difference of less than 1%.
MUC, in contrast, shows the largest differences with
more than 30% between Arabic and English in the
gold setting. It is also noteworthy that the results for
English show a balance between precision and recall
while both Arabic and Chinese favor precision over
recall in terms of mention detection, MUC, and B3.
The reasons for this difference between languages
need to be investigated further.
Gold Mention Boundaries The results for this set
of experiments is based on a version of the test set
that contains the gold boundaries of all mentions, in-
cluding singletons. Thus, we use these gold men-
tion boundaries instead of the ones generated by our
system. These experiments give us an insight on
how well UBIU performs on selecting the correct
boundaries. Since we do not expect the system?s
selection to be perfect, we would expect to see im-
proved system performance given the correct bound-
aries. The results are shown in the second part of
Table 6. As for using automatically generated men-
tions the tendencies in scores between gold and auto
linguistic annotations are kept. A further compari-
son of the overall results between the two settings
also shows only minor changes. The only exception
is the auto setting for Arabic, for which we see drop
in MD precision of approximately 5%. This also re-
sults in lower MUC and B3 precision and CEAFE
recall. The reasons for this drop in performance
need to be investigated further. The fact that most
results for both auto and gold settings change only
sightly shows that having information about the cor-
rect mention boundaries is not very helpful. Thus,
the system seems to have reached its optimal per-
formance on selecting mention boundaries given the
information that it has.
Gold Mentions The last set of experiments is
based on a version of the test set that contains the
gold mentions, i.e., all mentions that are coreferent,
but without any information about the identity of the
coreference chains. The results of this set of exper-
iments gives us information about the quality of the
coreference classifier. The results are shown in the
third part of Table 6. Using gold parses leads to
only minor improvement of the overall system per-
formance, yet, in that case all languages, including
Arabic, show consistent increase of results. Alto-
gether, there is a major improvement of the scores in
MD, MUC, and CEAFE . The B
3 scores only show
minor improvements, resulting from a slight drop in
precision across languages. The results also show
considerably higher precision than recall for MUC
and B3, and higher recall for CEAFE . This means
that the coreference decisions that the system makes
are highly reliable but that it still has a preference
for treating coreferent mentions as singletons.
A comparison across languages shows that pro-
viding gold mentions has a considerable positive ef-
fect on the system performance for Arabic since for
that setting Chinese leads to lower overall scores.
We assume that this is again due to the NP-rich syn-
tactic structure of Arabic and the fact that provid-
ing the mentions decreases drastically the number of
mentions the system works with and has to choose
from during the resolution process.
4 Conclusion and Future Work
We presented the UBIU system for coreference res-
olution in a multilingual setting. The system per-
formed reliably across all three languages of the
CoNLL 2012 shared task. For the future, we are
planning an in-depth investigation of the perfor-
mance of the mention detection module and the sin-
gleton classifier, as well as in investigation into more
complex models for coreference classification than
the mention pair model.
Acknowledgments
This work is based on research supported by the US
Office of Naval Research (ONR) Grant #N00014-
10-1-0140. We would also like to thank Kiran Ku-
mar for his help with tuning the system.93
References
Adriane Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: A machine
learning approach incorporating linguistically moti-
vated patterns. In Proceedings of the ACL Workshop
on Feature Engineering for Machine Learning in Nat-
ural Language Processing, FeatureEng ?05, pages 40?
47, Ann Arbor, MI.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A Multilingual Anaphora Resolution
System. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval), pages 104?
107, Uppsala, Sweden.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2010. TiMBL: Tilburg Memory
Based Learner, version 6.3,reference guide. Techni-
cal Report ILK 10-01, Induction of Linguistic Knowl-
edge, Computational Linguistics, Tilburg University.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45 ? 57.
Sanda M. Harabagiu and Steven J. Maiorano. 2000.
Multilingual coreference resolution. In Proceedings
of ANLP 2000, Seattle, WA.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, OR.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
Lingual Coreference Resolution with Syntactic Fea-
tures. In Proceedings of HLT/EMNLP 2005, Vancou-
ver, Canada.
Ruslan Mitkov. 1999. Multilingual anaphora resolution.
Machine Translation, 14(3-4):281?299.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings COLING ?02,
pages 1?7, Taipei, Taiwan.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?04, Barcelona, Spain.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011, Portland, OR.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977, Singapore.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Holger Wunsch. 2009. Rule-Based and Memory-Based
Pronoun Resolution for German: A Comparison and
Assessment of Data Sources. Ph.D. thesis, Universita?t
Tu?bingen.
Desislava Zhekova and Sandra Ku?bler. 2010. UBIU: A
language-independent system for coreference resolu-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 96?99, Uppsala,
Sweden.
94
