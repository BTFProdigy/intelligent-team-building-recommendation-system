Language Model Adaptation for Statistical Machine Translation 
with Structured Query Models 
Bing Zhao    Matthias Eck     Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, matteck, vogel+}@cs.cmu.edu 
 
 
Abstract 
We explore unsupervised language model 
adaptation techniques for Statistical Machine 
Translation.  The hypotheses from the 
machine translation output are converted into 
queries at different levels of representation 
power and used to extract similar sentences 
from very large monolingual text collection.  
Specific language models are then build from 
the retrieved data and interpolated with a 
general background model.  Experiments 
show significant improvements when 
translating with these adapted language 
models. 
1 Introduction 
Language models (LM) are applied in many 
natural language processing applications, such as 
speech recognition and machine translation, to 
encapsulate syntactic, semantic and pragmatic 
information.  For systems which learn from given 
data we frequently observe a severe drop in 
performance when moving to a new genre or new 
domain.  In speech recognition a number of 
adaptation techniques have been developed to cope 
with this situation.  In statistical machine 
translation we have a similar situation, i.e. estimate 
the model parameter from some data, and use the 
system to translate sentences which may not be 
well covered by the training data.  Therefore, the 
potential of adaptation techniques needs to be 
explored for machine translation applications. 
Statistical machine translation is based on the 
noisy channel model, where the translation 
hypothesis is searched over the space defined by a 
translation model and a target language (Brown et 
al, 1993).  Statistical machine translation can be 
formulated as follows: 
)()|(maxarg)|(maxarg* tPtsPstPt
tt
?==  
where t is the target sentence, and s is the source 
sentence. P(t) is the target language model and 
P(s|t) is the translation model.  The argmax 
operation is the search, which is done by the 
decoder. 
In the current study we modify the target 
language model P(t), to represent the test data 
better, and thereby improve the translation quality.  
(Janiszek, et al 2001) list the following approaches 
to language model adaptation: 
? Linear interpolation of a general and a domain 
specific model (Seymore, Rosenfeld, 1997). 
? Back off of domain specific probabilities with 
those of a specific model (Besling, Meier, 
1995). 
? Retrieval of documents pertinent to the new 
domain and training a language model on-line 
with those data (Iyer, Ostendorf, 1999, 
Mahajan et. al. 1999). 
? Maximum entropy, minimum discrimination 
adaptation (Chen, et. al., 1998). 
? Adaptation by linear transformation of vectors 
of bigram counts in a reduced space (DeMori, 
Federico, 1999). 
? Smoothing and adaptation in a dual space via 
latent semantic analysis, modeling long-term 
semantic dependencies, and trigger 
combinations.  (J. Bellegarda, 2000). 
Our approach can be characterized as 
unsupervised data augmentation by retrieval of 
relevant documents from large monolingual 
corpora, and interpolation of the specific language 
model, build from the retrieved data, with a 
background language model.  To be more specific, 
the following steps are carried out to do the 
language model adaptation.  First, a baseline 
statistical machine translation system, using a large 
general language model, is applied to generate 
initial translations.  Then these translations 
hypotheses are reformulated as queries to retrieve 
similar sentences from a very large text collection.  
A small domain specific language model is build 
using the retrieved sentences and linearly 
interpolated with the background language model.  
This new interpolated language model in applied in 
a second decoding run to produce the final 
translations.  
There are a number of interesting questions 
pertaining to this approach: 
? Which information can and should used to 
generate the queries: the first-best translation 
only, or also translation alternatives. 
? How should we construct the queries, just as 
simple bag-of-words, or can we incorporate more 
structure to make them more powerful. 
? How many documents should be retrieved to 
build the specific language models, and on what 
granularity should this be done, i.e. what is a 
document in the information retrieval process. 
 
The paper is structured as follows:  section 2 
outlines the sentence retrieval approach, and three 
bag-of-words query models are designed and 
explored; structured query models are introduced 
in section 3.  In section 4 we present translation 
experiments are presented for the different query.  
Finally, summary is given in section 5. 
2 LM Adaptation via Sentence Retrieval 
Our language model adaptation is an unsupervised 
data augmentation approach guided by query 
models.  Given a baseline statistical machine 
translation system, the language model adaptation 
is done in several steps shown as follows: 
 
? Generate a set of initial translation 
hypotheses H = {h1 ?hn} for source 
sentences s, using either the baseline MT 
system with the background language 
model or only the translation model 
? Use H  to build query 
? Use query to retrieve relevant sentences 
from the large corpus  
? Build specific language models from 
retrieved sentences 
? Interpolate the specific language model 
with the background language 
? Re-translate sentences s with adapted 
language model 
 
Figure-1: Adaptation Algorithm 
 
The specific language model )|( hwP iA  and the 
general background model )|( hwP iB  are combined 
using linear interpolation: 
)|()1()|()|(? hwPhwPhwP iAiBi ?? ?+=  (1)
The interpolation factor ?  can be simply 
estimated using cross validation or a grid search. 
As an alternative to using translations for the 
baseline system, we will also describe an approach, 
which uses partial translations of the source 
sentence, using the translation model only.  In this 
case, no full translation needs to be carried out in 
the first step; only information from the translation 
model is used.  
Our approach focuses on query model building, 
using different levels of knowledge representations 
from the hypothesis set or from the translation 
model itself.  The quality of the query models is 
crucial to the adapted language model?s 
performance.  Three bag-of-words query models 
are proposed and explained in the following 
sections. 
2.1 Sentence Retrieval Process 
In our sentence retrieval process, the standard tf/idf 
(term frequency and inverse document frequency) 
term weighting scheme is used.  The queries are 
built from the translation hypotheses.  We follow 
(Eck, et al, 2004) in considering each sentence in 
the monolingual corpus as a document, as they 
have shown that this gives better results compared 
to retrieving entire news stories. 
Both the query and the sentences in the text 
corpus are converted into vectors by assigning a 
term weight to each word.  Then the cosine 
similarity is calculated proportional to the inner 
product of the two vectors.  All sentences are 
ranked according to their similarity with the query, 
and the most similar sentences are used as the data 
for building the specific language model.  In our 
experiments we use different numbers of similar 
sentences, ranting from one to several thousand. 
2.2 Bag-of-words Query Models 
Different query models are designed to guide the 
data augmentation efficiently.  We first define  
?bag-of-words? models, based on different levels 
of knowledge collected from the hypotheses of the 
statistical machine translation engine. 
2.2.1 First-best Hypothesis as a Query Model 
The first-best hypothesis is the Viterbi path in the 
search space returned from the statistical machine 
translation decoder.  It is the optimal hypothesis 
the statistical machine translation system can 
generate using the given translation and language 
model, and restricted by the applied pruning 
strategy.  Ignoring word order, the hypothesis is 
converted into a bag-of-words representation, 
which is then used as a query: 
}|),{(),,( 1211 TiiilT VwfwwwwQ ?== L  
where iw is a word in the vocabulary 1TV of the Top-
1 hypothesis. if  is the frequency of iw ?s 
occurrence in the hypothesis.  
The first-best hypothesis is the actual translation 
we want to improve, and usually it captures 
enough correct word translations to secure a sound 
adaptation process.  But it can miss some 
informative translation words, which could lead to 
better-adapted language models.  
2.2.2  N-Best Hypothesis List as a Query Model 
Similar to the first-best hypothesis, the n-best 
hypothesis list is converted into a bag-of-words 
representation.  Words which occurred in several 
translation hypotheses are simply repeated in the 
bag-of-words representations.  
}|),{(
),,;;,,( ,2,1,,12,11,1 1
TNiii
lNNNlTN
Vwfw
wwwwwwQ
N
?=
= LLL   
where TNV  is the combined vocabulary from all n-
best hypotheses and if  is the frequency of iw ?s 
occurrence in the n-best hypothesis list. 
TNQ  has several good characteristics:  First it 
contains translation candidates, and thus is more 
informative than 1TQ .  In addition, the confidently 
translated words usually occur in every hypothesis 
in the n-best list, therefore have a stronger impact 
on the retrieval result due to the higher term 
frequency (tf) in the query.  Thirdly, most of the 
hypotheses are only different from each other in 
one word or two.  This means, there is not so much 
noise and variance introduced in this query model. 
2.2.3 Translation Model as a Query Model 
To fully leverage the available knowledge from the 
translation system, the translation model can be 
used to guide the language model adaptation 
process.  As introduced in section 1, the translation 
model represents the full knowledge of translating 
words, as it encodes all possible translations 
candidates for a given source sentence.  Thus the 
query model based on the translation model, has 
potential advantages over both 1TQ  and TNQ . 
To utilize the translation model, all the n-grams 
from the source sentence are extracted, and the 
corresponding candidate translations are collected 
from the translation model.  These are then 
converted into a bag-of-words representation as 
follows: 
}|),{(
),,;;,,( ,2,1,,2,1, 1111
TMiii
nsssnsssTM
Vwfw
wwwwwwQ
IIII
?=
= LLL   
where is  is a source n-gram, and I is the number of 
n-grams in the source sentence.  jsiw ,  is a candidate 
target word as translation of is .  Thus the 
translation model is converted into a collection of 
target words as a bag-of-word query model. 
There is no decoding process involved to build 
TMQ .  This means TMQ  does not incorporate any 
background language model information at all, 
while both 1TQ  and TNQ  implicitly use the 
background language model to prune the words in 
the query.  Thus TMQ  is a generalization, and 1TQ  
and TNQ  are pruned versions.  This also means TMQ  
is subject to more noise. 
3 Structured Query Models 
Word proximity and word order is closely related 
to syntactic and semantic characteristics.  
However, it is not modeled in the query models 
presented so far, which are simple bag-of-words 
representations.  Incorporating syntactic and 
semantic information into the query models can 
potentially improve the effectiveness of LM 
adaptation. 
The word-proximity and word ordering 
information can be easily extracted from the first-
best hypothesis, the n-best hypothesis list, and the 
translation lattice built from the translation model.  
After extraction of the information, structured 
query models are proposed using the structured 
query language, described in the Section 3.1. 
3.1 Structured Query Language 
This query language essentially enables the use of 
proximity operators (ordered and unordered 
windows) in queries, so that it is possible to model 
the syntactic and semantic information encoded in 
phrases, n-grams, and co-occurred word pairs.  
The InQuery implementation (Lemur 2003) is 
applied.  So far 16 operators are defined in 
InQuery to model word proximity (ordered, 
unordered, phrase level, and passage level).  Four 
of these operators are used specially for our 
language model adaptation: 
Sum Operator: #sum( 1t ? nt ) 
The terms or nodes ( 1t ? nt ) are treated as 
having equal influence on the final retrieval result.  
The belief values provided by the arguments of the 
sum are averaged to produce the belief value of the 
#sum node. 
Weighted Sum Operator: #wsum( 11 : tw , ?) 
The terms or nodes ( 1t ? nt ) contribute 
unequally to the final result according to the 
weight ( iw ) associated with each it .  
Ordered Distance Operator: #N( 1t ? nt ) 
The terms must be found within N words of 
each other in the text in order to contribute to the 
document's belief value.  An n-gram phrase can be 
modeled as an ordered distance operator with N=n. 
Unordered Distance Operator: #uwN( 1t ? nt ) 
The terms contained must be found in any order 
within a window of N words in order for this 
operator to contribute to the belief value of the 
document. 
3.2 Structured Query Models 
Given the representation power of the structured 
query language, the Top-1 hypothesis, Top-N Best 
hypothesis list, and the translation lattice can be 
converted into three Structured Query Models 
respectively. 
For first-best and n-best hypotheses, we collect 
related target n-grams of a given source word 
according to the alignments generated in the 
Viterbi decoding process.  While for the translation 
lattice, similar to the construction of TMQ , we 
collect all the source n-grams, and translate them 
into target n-grams.  In either case, we get a set of 
target n-grams for each source word. The 
structured query model for the whole source 
sentence is a collection of such subsets of target n-
grams. 
},,,{
21 Isssst
tttQ
vLvv=  
is
t
v
is a set of target n-grams for the source word is : 
}}{;},{;},{{ 311211 LLL
v
gramiiigramiigramis ttttttt i ?+??+?=  
In our experiments, we consider up to trigram for 
better retrieval efficiency, but higher order n-grams 
could be used as will.  The second simplification is 
that every source word is equally important, thus 
each n-gram subset 
is
t
v
will have an equal 
contribution to the final retrieval results.  The last 
simplification is each n-gram within the set of 
is
t
v
 
has an equal weight, i.e. we do not use the 
translation probabilities of the translation model.  
If the system is a phrase-based translation system, 
we can encode the phrases using the ordered 
distance operator (#N) with N equals to the number 
of the words of that phrase, which is denoted as the 
#phrase operator in InQuery implementation.  The 
2-grams and 3-grams can be encoded using this 
operator too. 
Thus our final structured query model is a sum 
operator over a set of nodes.  Each node 
corresponds to a source word.  Usually each source 
word has a number of translation candidates 
(unigrams or phrases).  Each node is a weighted 
sum over all translation candidates weighted by 
their frequency in the hypothesis set.  An example 
is shown below, where #phrase indicates the use of 
the ordered distance operator with varying n: 
 
#q=#sum( #wsum(2 eu  2 #phrase(european union) ) 
   #wsum(12 #phrase(the united states) 
1 american 1 #phrase(an american) ) 
   #wsum(4 are 1 is ) 
   #wsum(8 markets  3 market)) 
   #wsum(7 #phrase(the main)  5 primary ) ); 
4 Experiments 
Experiments are carried out on a standard 
statistical machine translation task defined in the 
NIST evaluation in June 2002.  There are 878 test 
sentences in Chinese, and each sentence has four 
human translations as references.  NIST score 
(NIST 2002) and Bleu score (Papineni et. al. 2002) 
of mteval version 9 are reported to evaluate the 
translation quality. 
4.1  Baseline Translation System 
Our baseline system (Vogel et al, 2003) gives 
scores of 7.80 NIST and 0.1952 Bleu for Top-1 
hypothesis, which is comparable to the best results 
reported on this task.  
For the baseline system, we built a translation 
model using 284K parallel sentence pairs, and a 
trigram language model from a 160 million words 
general English news text collection.  This LM is 
the background model to be adapted.  
With the baseline system, the n-best hypotheses 
list and the translation lattice are extracted to build 
the query models.  Experiments are carried out on 
the adapted language model using the three bag-of-
words query models: 1TQ , TNQ  and TMQ , and the 
corresponding structured query models. 
4.2 Data: GigaWord Corpora 
The so-called GigaWord corpora (LDC, 2003) are 
very large English news text collections.  There are 
four distinct international sources of English 
newswire: 
 
AFE Agence France Press English Service 
APW Associated Press Worldstream English Service 
NYT The New York Times Newswire Service 
XIE The Xinhua News Agency English Service 
 
Table-1 shows the size of each part in word counts. 
 
AFE APW NYT XIE 
170,969K 539,665K 914,159K 131,711K 
Table-1: Number of words in the different 
GigaWord corpora 
 
As the Lemur toolkit could not handle the two 
large corpora (APW and NYT) we used only 200 
million words from each of these two corpora. 
In the preprocessing all words are lowercased 
and punctuation is separated.  There is no explicit 
removal of stop words as they usually fade out by 
tf.idf weights, and our experiments showed not 
positive effects when removing stop words. 
4.3 Bag-of-Words Query Models 
Table-2 shows the size of 1TQ , TNQ  and TMQ  in 
terms of number of tokens in the 878 queries: 
 
 1TQ  TNQ  TMQ  
|| Q  25,861 231,834 3,412,512 
Table-2: Query size in number of tokens 
 
As words occurring several times are reduced to 
word-frequency pairs, the size of the queries 
generated from the 100-best translation lists is only 
9 times as big as the queries generated from the 
first-best translations.  The queries generated from 
the translation model contain many more 
translation alternatives, summing up to almost 3.4 
million tokens.  Using the lattices the whole 
information of the translation model is kept.  
4.3.1 Results for Query 1TQ  
In the first experiment we used the first-best 
translations to generate the queries.  For each of 
the 4 corpora different numbers of similar 
sentences (1, 10, 100, and 1000) were retrieved to 
build specific language models.  Figure-2 shows 
the language model adaptation after tuning the 
interpolation factor ?  by a grid search over [0,1]. 
Typically ?  is around 0.80. 
 
1-Best/NIST Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
1-Best/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-2: NIST and Bleu scores 
1TQ  
 
We see that each corpus gives an improvement 
over the baseline.  The best NIST score is 7.94, 
and the best Bleu score is 0.2018.  Both best scores 
are realized using top 100 relevant sentences 
corpus per source sentence mined from the AFE. 
4.3.2 Results for Query TNQ  
Figure-3 shows the results for the query model TNQ .  
The best results are 7.99 NIST score, and 0.2022 
Bleu score.  These improvements are statistically 
significant.  Both scores are achieved at the same 
settings as those in 1TQ , i.e. using top 100 retrieved 
relevant sentences mined from the AFE corpus. 
 
100-Best/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
100-Best/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-3: NIST and Bleu scores from TNQ  
 
Using the translation alternatives to retrieve the 
data for language model adaptation gives an 
improvement over using the first-best translation 
only for query construction.  Using only one 
translation hypothesis to build an adapted language 
model has the tendency to reinforce that 
translation. 
4.3.3 Results for Query TMQ  
The third bag-of-words query model uses all 
translation alternatives for source words and source 
phrases.  Figure-4 shows the results of this query 
model TMQ .  The best results are 7.91 NIST score 
and 0.1995 Bleu.  For this query model best results 
were achieved using the top 1000 relevant 
sentences mined from the AFE corpus per source 
sentence. 
The improvement is not as much as the other 
two query models.  The reason is probably that all 
translation alternatives, even wrong translations 
resulting from errors in the word and phrase 
alignment, contribute alike to retrieve similar 
sentences.  Thereby, an adapted language model is 
built, which reinforces not only good translations, 
but also bad translations. 
All the three query models showed 
improvements over the baseline system in terms of 
NIST and Bleu scores.  The best bag-of-words 
query model is TNQ  built from the N-Best list.  It 
provides a good balance between incorporating 
translation alternatives in the language model 
adaptation process and not reinforcing wrong 
translations. 
 
Lattice/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Lattice/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-4: NIST and Bleu scores from TMQ  
 
4.4 Structured Query Models 
The next series of experiments was done to 
study if using word order information in 
constructing the queries could help to generate 
more effective adapted language models.  By using 
the structured query language we converted the 
same first-best hypothesis, the 100-best list, and 
the translation lattice into structured query models.  
Results are reported for the AFE corpus only, as 
this corpus gave best translation scores. 
Figure-5 shows the results for all three structured 
query models, built from the first-best hypothesis 
(?1-Best?), the 100 best hypotheses list (?100-
Best?), and translation lattice (?TM-Lattice?).  
Using these query models, different numbers of 
most similar sentences, ranging from 100 to 4000, 
where retrieved from the AFE corpus.  The given 
baseline results are the best results achieved from 
the corresponding bag-of-words query models. 
Consistent improvements were observed on 
NIST and Bleu scores.  Again, optimal 
interpolation factors to interpolate the specific 
language models with the background language 
model were used, which typically were in the 
range of [0.6, 0.7].  Structured query models give 
most improvements when using more sentences for 
language model adaptation.  The effect is more 
pronounced for Bleu then for NIST score. 
 
Structured query/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
8.0500
8.1000
8.1500
Baseline Top100 Top500 Top1000 Top2000 Top4000
1-Best
100-Best
TM-Lattice
Structured query/BLEU-Scores
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
0.2060
0.2080
Baseline Top100 Top500 Top1000 Top2000 Top4000
1-Best
100-Best
TM-Lattice
 
Figure-5: NIST and Bleu scores from the  
structured query models 
 
The really interesting result is that the structured 
query model TMQ gives now the best translation 
results.  Adding word order information to the 
queries obviously helps to reduce the noise in the 
retrieved data by selecting sentences, which are 
closer to the good translations,  
The best results using the adapted language 
models are NIST score 8.12 for using the 2000 
most similar sentences, whereas Bleu score goes 
up to 0.2068 when using 4000 sentences for 
language model adaptation. 
4.5 Example 
Table-3 shows translation examples for the 17th 
Chinese sentence in the test set. We applied the 
baseline system (Base), the bag-of-word query 
model (Hyp1), and the structured query model 
(Hyp2) using AFE corpus. 
 
Ref The police has already blockade the scene of the explosion. 
Base At present, the police had cordoned off the explosion. 
Hyp1 At present, police have sealed off the explosion.  
Hyp2 Currently, police have blockade on the scene of the explosion. 
Table-3 Translation examples 
 4.6 Oracle Experiment 
Finally, we run an oracle experiments to see 
how much improvement could be achieved if we 
only selected better data for the specific language 
models. We converted the four available reference 
translations into structured query models and 
retrieved the top 4000 relevant sentences from 
AFE corpus for each source sentence.  Using these 
language models, interpolated with the background 
language model gave a NIST score of 8.67, and a 
Bleu score of 0.2228.  This result indicates that 
there is room for further improvements using this 
language model adaptation technique. 
The oracle experiment suggests that better initial 
translations lead to better language models and 
thereby better 2nd iteration translations.  This lead 
to the question if we can iterate the retrieval 
process several times to get further improvement, 
or if the observed improvement results form using 
for (good) translations, which have more diversity 
than the translations in an n-best list. 
On the other side the oracle experiment also 
shows that the optimally expected improvement is 
limited by the translation model and decoding 
algorithm used in the current SMT system. 
 
5 Summary 
In this paper, we studied language model 
adaptation for statistical machine translation.  
Extracting sentences most similar to the initial 
translations, building specific language models for 
each sentence to be translated, and interpolating 
those with the background language models gives 
significant improvement in translation quality.  
Using structured query models, which capture 
word order information, leads to better results that 
plain bag of words models. 
The results obtained suggest a number of 
extensions of this work:  The first question is if 
more data to retrieve similar sentences from will 
result in even better translation quality.  A second 
interesting question is if the translation 
probabilities can be incorporated into the queries.  
This might be especially useful for structured 
query models generated from the translation 
lattices.  
References  
J. Bellegarda. 2000, Exploiting Latent Semantic 
Information in Statistical Language Modeling. In 
Proceedings of the IEEE, 88(8), pp. 1279-1296. 
S. Besling and H.G. Meier 1995. Language Model 
Speaker Adaptation, Eurospeech 1995, Madrid, 
Spain. 
Peter F Brown., Stephen A Della Pietra., Vincent J. 
Della Pietra and Mercer Robert L., 1993.  The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2), pp. 263?311.  
S.F Chen., K. Seymore, and R. Rosenfeld 1998. Topic 
Adaptation for Language Modeling using 
Unnormalized Exponential Models. IEEE 
International Conference on Acoustics, Speech and 
Signal Processing 1998, Seattle WA.  
Renato DeMori and Marcello Federico 1999. Language 
Model Adaptation, In Computational Models of 
Speech Pattern Processing, Keith Pointing (ed.), 
NATO ASI Series, Springer Verlag.  
Matthias Eck, Stephan Vogel, and Alex Waibel, 2004. 
Language Model Adaptation for Statistical 
Machine Translation based on Information Retrieval, 
International Conference on Language Resources and 
Evaluation, Lisbon, Portugal. 
R. Iyer and M. Ostendorf, 1999. Modeling Long 
Distance Dependence in Language: Topic Mixtures 
vs. Dynamic Cache Models, IEEE Transactions on 
Speech and Audio Processing, SAP-7(1): pp. 30-39. 
David Janiszek, Renato DeMori and Frederic Bechet, 
2001.  Data Augmentation and Language Model 
adaptation, IEEE International Conference on 
Acoustics, Speech and Signal Processing 2001, Salt 
Lake City, UT.  
LDC, Gigaword Corpora. http://wave.ldc.upenn.edu/ 
Catalog/CatalogEntry.jsp?catalogId=LDC2003T05 
Lemur, The Lemur Toolkit for Language Modeling and 
Information Retrieval, http://www.cs.cmu.edu/~ 
lemur/ 
Milind Mahajan, Doug Beeferman and X.D. Huang, 
1999. Improved Topic-Dependent Language 
Modeling Using Information Retrieval Techniques, 
IEEE International Conference on Acoustics, Speech 
and Signal Processing 1999, Phoenix, AZ. 
NIST Report: 2002, Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics.  http://www.nist.gov/speech/tests/mt/doc/ 
ngram-study.pdf . 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu, 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proc of the 
40th Annual Meeting of the Association for 
Computational Linguistics. 2002, Philadelphia, PA. 
Kristie Seymore and Ronald Rosenfeld, 1997. Using 
Story Topics for Language Model Adaptation. In 
Proc. Eurospeech 1997, Rhodes, Greece.  
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, 
Ashish Venogupal, Bing Zhao, Alex Waibel, 2003. 
The CMU Statistical Translation System, Proceedings 
of MT-Summit IX, 2003, New Orleans, LA. 
 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 177?184, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Inner-Outer Bracket Models for Word Alignment
using Hidden Blocks
Bing Zhao
School of Computer Science
Carnegie Mellon University
{bzhao}@cs.cmu.edu
Niyu Ge and Kishore Papineni
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
{niyuge, papineni}@us.ibm.com
Abstract
Most statistical translation systems are
based on phrase translation pairs, or
?blocks?, which are obtained mainly from
word alignment. We use blocks to infer
better word alignment and improved word
alignment which, in turn, leads to better
inference of blocks. We propose two new
probabilistic models based on the inner-
outer segmentations and use EM algorithms
for estimating the models? parameters. The
first model recovers IBM Model-1 as a spe-
cial case. Both models outperform bi-
directional IBM Model-4 in terms of word
alignment accuracy by 10% absolute on the
F-measure. Using blocks obtained from
the models in actual translation systems
yields statistically significant improvements
in Chinese-English SMT evaluation.
1 Introduction
Today?s statistical machine translation systems rely
on high quality phrase translation pairs to acquire
state-of-the-art performance, see (Koehn et al, 2003;
Zens and Ney, 2004; Och and Ney, 2003). Here,
phrase pairs, or ?blocks? are obtained automati-
cally from parallel sentence pairs via the underlying
word alignments. Word alignments traditionally are
based on IBM Models 1-5 (Brown et al, 1993) or on
HMMs (Vogel et al, 1996). Automatic word align-
ment is challenging in that its accuracy is not yet
close to inter-annotator agreement in some language
pairs: for Chinese-English, inter-annotator agree-
ment exceeds 90 on F-measure whereas IBM Model-
4 or HMM accuracy is typically below 80s. HMMs
assume that words ?close-in-source? are aligned to
words ?close-in-target?. While this locality assump-
tion is generally sound, HMMs do have limitations:
the self-transition probability of a state (word) is the
only control on the duration in the state, the length
of the phrase aligned to the word. Also there is no
natural way to control repeated non-contiguous vis-
its to a state. Despite these problems, HMMs remain
attractive for their speed and reasonable accuracy.
We propose a new method for localizing word
alignments. We use blocks to achieve locality in the
following manner: a block in a sentence pair is a
source phrase aligned to a target phrase. We assume
that words inside the source phrase cannot align to
words outside the target phrase and that words out-
side the source phrase cannot align to words inside
the target phrase. Furthermore, a block divides the
sentence pair into two smaller regions: the inner
part of the block, which corresponds to the source
and target phrase in the block, and the outer part of
the block, which corresponds to the remaining source
and target words in the parallel sentence pair. The
two regions are non-overlapping; and each of them is
shorter than the original parallel sentence pair. The
regions are thus easier to align than the original sen-
tence pairs (e.g., using IBM Model-1). While the
model uses a single block to split the sentence pair
into two independent regions, it is not clear which
block we should select for this purpose. Therefore,
we treat the splitting block as a hidden variable.
This proposed approach is far simpler than treat-
ing the entire sentence as a sequence of non-
overlapping phrases (or chunks) and considering such
sequential segmentation either explicitly or implic-
itly. For example, (Marcu and Wong, 2002) for a
joint phrase based model, (Huang et al, 2003) for
a translation memory system; and (Watanabe et
al., 2003) for a complex model of insertion, deletion
and head-word driven chunk reordering. Other ap-
proaches including (Watanabe et al, 2002) treat ex-
tracted phrase-pairs as new parallel data with limited
success. Typically, they share a similar architecture
of phrase level segmentation, reordering, translation
as in (Och and Ney, 2002; Koehn and Knight, 2002;
Yamada and Knight, 2001). The phrase level inter-
action has to be taken care of for the non-overlapping
sequential segmentation in a complicated way. Our
models model such interactions in a soft way. The
hidden blocks are allowed to overlap with each other,
177
while each block induced two non-overlapping re-
gions, i.e. the model brackets the sentence pair
into two independent parts which are generated syn-
chronously. In this respect, it resembles bilingual
bracketing (Wu, 1997), but our model has more lex-
ical items in the blocks with many-to-many word
alignment freedom in both inner and outer parts.
We present our localization constraints using
blocks for word alignment in Section 2; we detail our
two new probabilistic models and their EM train-
ing algorithms in Section 3; our baseline system, a
maximum-posterior inference for word alignment, is
explained in Section 4; experimental results of align-
ments and translations are in Section 5; and Section
6 contains discussion and conclusions.
2 Segmentation by a Block
We use the following notation in the remainder of
this paper: e and f denote the English and foreign
sentences with sentence lengthes of I and J , respec-
tively. ei is an English word at position i in e; fj is
a foreign word at position j in f . a is the alignment
vector with aj mapping the position of the English
word eaj to which fj connects. Therefore, we have
the standard limitation that one foreign word can-
not be connected to more than one English word. A
block ?[] is defined as a pair of brackets as follows:
?[] = (?e, ?f ) = ([il, ir], [jl, jr]), (1)
where ?e = [il, ir] is a bracket in English sentence de-
fined by a pair of indices: the left position il and the
right position ir, corresponding to a English phrase
eiril . Similar notations are for ?f = [jl, jr], which isone possible projection of ?e in f . The subscript l and
r are abbreviations of left and right, respectively.
?e segments e into two parts: (?e, e) = (?e?, ?e/?).The inner part ?e? = {ei, i ? [il, ir]} and the outer
part ?e/? = {ei, i /? [il, ir]}; ?f segments f similarly.
Thus, the block ?[] splits the parallel sentence pair
into two non-overlapping regions: the Inner ?[]? and
Outer ?[]/? parts (see Figure 1). With this segmen-tation, we assume the words in the inner part are
aligned to inner part only: ?[]? = ?e? ? ?f? : {ei, i ?
[il, ir]} ? {fj , j ? [jl, jr]}; and words in the outer
part are aligned to outer part only: ?[]/? = ?e/? ? ?f/? :{ei, i /? [il, ir]} ? {fj , j /? [jl, jr]}. We do not allow
alignments to cross block boundaries. Words inside
a block ?[] can be aligned using a variety of models
(IBM models 1-5, HMM, etc). We choose Model1 for
simplicity. If the block boundaries are accurate, we
can expect high quality word alignment. This is our
proposed new localization method.
Outer
Inner
li ri
rj
lj
e?
f?
Figure 1: Segmentation by a Block
3 Inner-Outer Bracket Models
We treat the constraining block as a hidden variable
in a generative model shown in Eqn. 2.
P (f |e) =
?
{?[]}
P (f , ?[]|e)
=
?
{?e}
?
{?f}
P (f , ?f |?e, e)P (?e|e), (2)
where ?[] = (?e, ?f ) is the hidden block. In the gen-
erative process, the model first generates a bracket
?e for e with a monolingual bracketing model of
P (?e|e). It then uses the segmentation of the En-
glish (?e, e) to generate the projected bracket ?f of f
using a generative translation model P (f , ?f |?e, e) =
P (?f/?, ?f?|?e/?, ?e?) ? the key model to implement ourproposed inner-outer constraints. With the hidden
block ?[] inferred, the model then generates word
alignments within the inner and outer parts sepa-
rately. We present two generating processes for the
inner and outer parts induced by ?[] and correspond-
ing two models of P (f , ?f |?e, e). These models are
described in the following secions.
3.1 Inner-Outer Bracket Model-A
The first model assumes that the inner part and the
outer part are generated independently. By the for-
mal equivalence of (f, ?f ) with (?f?, ?f/?), Eqn. 2 canbe approximated as:
P (f |e)?
?
{?e}
?
{?f}
P (?f?|?e?)P (?f/?|?e/?)P (?e|e)P (?f |?e),
(3)
where P (?f?|?e?) and P (?f/?|?e/?) are two independentgenerative models for inner and outer parts, respec-
178
tively and are futher decompsed into:
P (?f?|?e?) =
?
{aj??e?}
?
fj??f?
P (fj |eaj )P (eaj |?e?)
P (?f/?|?e/?) =
?
{aj??e/?}
?
fj??f/?
P (fj |eaj )P (eaj |?e/?), (4)
where {aJ1 } is the word alignment vector. Given the
block segmentation and word alignment, the genera-
tive process first randomly selects a ei according to
either P (ei|?e?) or P (ei|?e/?); and then generates fj in-dexed by word alignment aj with i = aj according to
a word level lexicon P (fj |eaj ). This generative pro-
cess using the two models of P (?f?|?e?) and P (?f/?|?e/?)must satisfy the constraints of segmentations induced
by the hidden block ?[] = (?e, ?f ). The English
words ?e? inside the block can only generate the words
in ?f? and nothing else; likewise ?e/? only generates
?f/?. Overall, the combination of P (?f?|?e?)P (?f/?|?e/?)in Eqn. 3 collaborates each other quite well in prac-
tice. For a particular observation ?f?, if ?e? is too
small (i.e., missing translations), P (?f?|?e?) will suf-
fer; and if ?e? is too big (i.e., robbing useful words
from ?e/?), P (?f/?|?e/?) will suffer. Therefore, our pro-posed model in Eqn. 3 combines the two costs and
requires both inner and outer parts to be explained
well at the same time.
Because the model in Eqn. 3 is essentially a two-
level (?[] and a) mixture model similar to IBM Mod-
els, the EM algorithm is quite straight forward as
in IBM models. Shown in the following are several
key E-step computations of the posteriors. The M-
step (optimization) is simply the normalization of
the fractional counts collected using the posteriors
through the inference results from E-step:
P?[]?(aj |?
f
?, ?e?) =
P (fj |eaj )?
ek??e? P (fj |ek)
P?[]/?(aj |?
f
/?, ?e/?) =
P (fj |eaj )?
ek??e/? P (fj |ek)
(5)
The posterior probability of P (aJ1 |f , ?f , ?e, e) =?J
j=1 P (aj |f , ?f , ?e, e), where P (aj |f , ?f , ?e, e) is ei-
ther P?[]?(aj |?
f
?, ?e?) when (fj , eaj ) ? ?[]?, or oth-
erwise P?[]/?(aj |?
f
/?, ?e/?) when (fj , eaj ) ? ?[]/?. As-
suming P (?e|e) to be a uniform distribution, the
posterior of selecting a hidden block given ob-
servations: P (?[] = (?e, ?f )|e, f) is proportional
to block level relative frequency Prel(?f?|?e?) up-
dated in each iteration; and can be smoothed
with P (?f |?e, f , e) = P (?f?|?e?)P (?f/?|?e/?)/
?
{??f}
P (??f? |?e?)P (?
?f
/? |?e/?) assuming Model-1 alignment inthe inner and outer parts independently to reduce
the risks of data sparseness in estimations.
In principle, ?e can be a bracket of any length
not exceeding the sentence length. If we restrict the
bracket length to that of the sentence length, we re-
cover IBM Model-1. Figure 2 summarizes the gener-
ation process for Inner-Outer Bracket Model-A.
f1 f2  f3  f4
e1 e2  e3
[e1] e2  e3 e1 [e2] e3 [e1 e2] e3 e1 [e2 e3]
?.
f1 f4
e1 e3
f2  f3
e2
f1 f3 f4
e1 e3
f2
e2
? ?
]3,2[=f? ]2,2[=f? [.,.]=f?
]2,2[=e?]1,1[=e? ]2,1[=e? ]3,2[=e?
innerouter innerouter
Figure 2: Illustration of generative Bracket Model-A
3.2 Inner-Outer Bracket Model-B
A block ?[] invokes both the inner and outer gener-
ations simultaneously in Bracket Model A (BM-A).
However, the generative process is usually more ef-
fective in the inner part as ?[] is generally small and
accurate. We can build a model focusing on gener-
ating only the inner part with careful inferences to
avoid errors from noisy blocks. To ensure that all
fJ1 are generated, we need to propose enough blocks
to cover each observation fj . This constraint can be
met by treating the whole sentence pair as one block.
The generative process is as follows: First the
model generates an English bracket ?e as before. The
model then generates a projection ?f in f to local-
ize all aj ?s for the given ?e according to P (?f |?e, e).
?e and ?f forms a hidden block ?[]. Given ?[], the
model then generates only the inner part fj ? ?f? via
P (f |?f , ?e, e) ' P (?f?|?f , ?e, e). Eqn. 6 summarizes
this by rewriting P (f , ?f |?e, e):
P (f , ?f |?e, e) = P (f |?f , ?e, e)P (?f |?e, e) (6)
= P (f |?f , ?e, e)P ([jl, jr]|?e, e)
' P (?f?|?f , ?e, e)P ([jl, jr]|?e, e).
P (?f?|?f , ?e, e) is a bracket level emission proba-
bilistic model which generates a bag of contiguous
words fj ? ?f? under the constraints from the given
hidden block ?[] = (?f , ?e). The model is simplified
in Eqn. 7 with the assumption of bag-of-words? inde-
pendence within the bracket ?f :
P (?f?|?f , ?e, e) =?
aJ1
?
j??f? P (fj |eaj )P (eaj |?
f , ?e, e). (7)
179
180
puting a pair (j, t)?:
(j, t)? = argmax
(j,t)
P (fj |et), (11)
that is, the point at which the posterior is maximum.
The pair (j, t) defines a word pair (fj , et) which is
then aligned. The procedure continues to find the
next maximum in the posterior matrix. Contrast
this with Viterbi alignment where one computes
f?T1 = argmax
{fT1 }
P (f1, f2, ? ? ? , fT |eT1 ), (12)
We observe, in parallel corpora, that when one
word translates into multiple words in another lan-
guage, it usually translates into a contiguous se-
quence of words. Therefore, we impose a conti-
guity constraint on word alignments. When one
word fj aligns to multiple English words, the En-
glish words must be contiguous in e and vice versa.
The algorithm to find word alignments using max-
posterior with contiguity constraint is illustrated in
Algorithm 1.
Algorithm 1 A maximum-posterior algorithm with
contiguity constraint
1: while (j, t) = (j, t)? (as computed in Eqn. 11)
do
2: if (fj , et) is not yet algned then
3: align(fj , et);
4: else if (et is contiguous to what fj is aligned)
or (fj is contiguous to what et is aligned) then
5: align(fj , et);
6: end if
7: end while
The algorithm terminates when there isn?t any
?next? posterior maximum to be found. By defi-
nition, there are at most JxT ?next? maximums in
the posterior matrix. And because of the contiguity
constraint, not all (fj , et) pairs are valid alignments.
The algorithm is sure to terminate. The algorithm
is, in a sense, directionless, for one fj can align to
multiple et?s and vise versa as long as the multiple
connections are contiguous. Viterbi, however, is di-
rectional in which one state can emit multiple obser-
vations but one observation can only come from one
state.
5 Experiments
We evaluate the performances of our proposed mod-
els in terms of word alignment accuracy and trans-
lation quality. For word alignment, we have 260
hand-aligned sentence pairs with a total of 4676 word
pair links. The 260 sentence pairs are randomly
selected from the CTTP1 corpus. They were then
word aligned by eight bilingual speakers. In this set,
we have one-to-one, one-to-many and many-to-many
alignment links. If a link has one target functional
word, it is considered to be a functional link (Ex-
amples of funbctional words are prepositions, deter-
miners, etc. There are in total 87 such functional
words in our experiments). We report the overall F-
measures as well as F-measures for both content and
functional word links. Our significance test shows
an overall interval of ?1.56% F-measure at a 95%
confidence level.
For training data, the small training set has 5000
sentence pairs selected from XinHua news stories
with a total of 131K English words and 125K Chi-
nese words. The large training set has 181K sentence
pairs (5k+176K); and the additional 176K sentence
pairs are from FBIS and Sinorama, which has in to-
tal 6.7 million English words and 5.8 million Chinese
words.
5.1 Baseline Systems
The baseline is our implementation of HMM with
the maximum-posterior algorithm introduced in sec-
tion 4. The HMMs are trained unidirectionally. IBM
Model-4 is trained with GIZA++ using the best re-
ported settings in (Och and Ney, 2003). A few pa-
rameters, especially the maximum fertility, are tuned
for GIZA++?s optimal performance. We collect bi-
directional (bi) refined word alignment by growing
the intersection of Chinese-to-English (CE) align-
ments and English-to-Chinese (EC) alignments with
the neighboring unaligned word pairs which appear
in the union similar to the ?final-and? approaches
(Koehn, 2003; Och and Ney, 2003; Tillmann, 2003).
Table 1 summarizes our baseline with different set-
tings. Table 1 shows that HMM EC-P gives the
F-measure(%) Func Cont Both
Small
HMM EC-P 54.69 69.99 64.78
HMM EC-V 31.38 53.56 55.59
HMM CE-P 51.44 69.35 62.69
HMM CE-V 31.43 63.84 55.45
Large
HMM EC-P 60.08 78.01 71.92
HMM EC-V 32.80 74.10 64.26
HMM CE-P 58.45 79.44 71.84
HMM CE-V 35.41 79.12 68.33
Small GIZA MH-bi 45.63 69.48 60.08GIZA M4-bi 48.80 73.68 63.75
Large GIZA MH-bi 49.13 76.51 65.67GIZA M4-bi 52.88 81.76 70.24
- Fully-Align 2 5.10 15.84 9.28
Table 1: Baseline: V: Viterbi; P: Max-Posterior
1LDC2002E17
181
best baseline, better than bidirectional refined word
alignments from GIZA M4 and the HMM Viterbi
aligners.
5.2 Inner-Outer Bracket Models
We trained HMM lexicon P (f |e) to initialize the
inner-outer Bracket models. Afterwards, up to 15?
20 EM iterations are carried out. Iteration starts
from the fully aligned2 sentence pairs, which give an
F-measure of 9.28% at iteration one.
5.2.1 Small Data Track
Figure 4 shows the performance of Model-A (BM-
A) trained on the small data set. For each English
bracket, Top-1 means only the fractional counts from
the Top-1 projection are collected, Top-all means
counts from all possible projections are collected. In-
side means the fractional counts are collected from
the inner part of the block only; and outside means
they are collected from the outer parts only. Using
the Top-1 projection from the inner parts of the block
(top-1-inside) gives the best performance: an F-
measure of 72.29%, or a 7.5% absolute improvement
over the best baseline at iteration 5. Figure 5 shows
BM-A with different settings on small data set
62
64
66
68
70
72
74
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16EM  Iterations
F-m
ea
su
re top-1 inside
top-all inside
top all
top-1
top-1 outside
top-all outside
Figure 4: BM-A with different settings on small data
the performance of Inner-Outer Bracket Model-B
(BM-B) over EM iterations. smoothing means when
collecting the fractional counts, we reweigh the up-
dated fractional count by 0.95 and give the remain-
ing 0.05 weight to original fractional count from the
links, which were aligned in the previous iteration.
w/null means we applied the proposed Null word
model in section 3.3 to infer null links. We also pre-
defined a list of 15 English function words, for which
there might be no corresponding Chinese words as
translations. These 15 English words are ?a, an, the,
of, to, for, by, up, be, been, being, does, do, did, -?.
In the drop-null experiments, the links containing
these predefined function words are simply dropped
2Every possible word pair is aligned
in the final word alignment (this means they are left
unaligned).
BM-B with different settings on small data set
65
67
69
71
73
75
77
1 2 3 4 5 6 7 8EM  Iterations
F-m
ea
su
re
top-1 smooth dropnulltop-1 smooth w/nulltop-1 smoothtop 1top all
Figure 5: BM-B with different settings on small data
Empirically we found that doing more than 5 it-
erations lead to overfitting. The peak performance
in our model is usually achieved around iteration
4?5. At iteration 5, setting ?BM-B Top-1? gives an
F-measure of 73.93% which is better than BM-A?s
best performance (72.29%). This is because Model
B leverages a local search for less noisy blocks and
hence the inner part is more accurately generated
(which in turn means the outer part is also more
accurate). From this point on, all of our experi-
ments are using Model B. With smoothing, BM-B
improves to 74.46%. After applying the null word
model, we get 75.20%. By simply dropping links
containing the 15 English functional words, we get
76.24%, which is significantly better than our best
baseline obtained from even the large training set
(HMM EC-P: 71.92%).
BM-B with different settings on large data set
69
71
73
75
77
79
81
83
1 2 3 4 5 6 7 8EM  Iterations
F-m
ea
su
re
top-1 smooth dropnull
top-1 smooth w/null
top-1 smooth
Figure 6: BM-B with different settings on large data
5.2.2 Large Data Track
Figure 6 shows performance pictures of model
BM-B on the large training set. Without dropping
English functional words, the best performance is
182
80.38% at iteration 4 using the Top-1 projection to-
gether with the null word models. By additionally
dropping the links containing the 15 functional En-
glish words, we get 81.47%. These results are all
significantly better than our strongest baseline sys-
tem: 71.92% F-measure using HMM EC-P (70.24%
using bidirectional Model-4 for comparisons).
On this data set, we experimented with different
maximum bracket length limits, from one word (un-
igram) to nine-gram. Results show that a maximum
bracket length of four is already optimal (79.3% with
top-1 projection), increased from 62.4% when maxi-
mum length is limited to one. No improvements are
observed using longer than five-gram.
5.3 Evaluate Blocks in the EM Iterations
Our intuition was that good blocks can improve word
alignment and, in turn, good word alignment can
lead to better block selection. The experimental re-
sults above support the first claim. Now we consider
the second claim that good word alignment leads to
better block selection.
Given reference human word alignment, we extract
reference blocks up to five-gram phrases on Chinese.
The block extraction procedure is based on the pro-
cedures in (Tillmann, 2003).
During EM, we output all the hidden blocks actu-
ally inferred at each iteration, then we evaluate the
precision, recall and F-measure of the hidden blocks
according to the extracted reference blocks. The re-
sults are shown in Figure 7. Because we extract all
10%
15%
20%
25%
30%
35%
40%
45%
F-m
ea
su
res
1 2 3 4 5 6 7 8EM  Iterations
A Direct Eval of blocks' accuracy in 'BM-B top-1 smooth w/null'
F-measure
Recall
Precision
Figure 7: A Direct Eval. of Blocks in BM-B
possible n-grams at each position in e, the precision
is low and the recall is relatively high as shown by
Figure 7. It also shows that blocks do improve, pre-
sumably benefiting from better word alignments.
Table 2 summarizes word alignment performances
of Inner-Outer BM-B in different settings. Overall,
without the handcrafted function word list, BM-B
gives about 8% absolute improvement in F-measure
on the large training set and 9% for the small set
F-measure(%) Func Cont Both
Small
Baseline 54.69 69.99 64.78
BM-B-drop 62.76 82.99 76.24
BM-B w/null 61.24 82.54 75.19
BM-B smooth 59.61 82.99 74.46
Large
Baseline 60.08 78.01 71.92
BM-B-drop 63.95 90.09 81.47
BM-B w/null 62.24 89.99 80.38
BM-B smooth 60.49 90.09 79.31
Table 2: BM-B with different settings
with a confidence interval of ?1.56%.
5.4 Translation Quality Evaluations
We also carried out the translation experiments using
the best settings for Inner-Outer BM-B (i.e. BM-B-
drop) on the TIDES Chinese-English 2003 test set.
We trained our models on 354,252 test-specific sen-
tence pairs drawn from LDC-supplied parallel cor-
pora. On this training data, we ran 5 iterations of
EM using BM-B to infer word alignments. A mono-
tone decoder similar to (Tillmann and Ney, 2003)
with a trigram language model3 is set up for trans-
lations. We report case sensitive Bleu (Papineni et
al., 2002) scoreBleuC for all experiments. The base-
line system (HMM ) used phrase pairs built from the
HMM-EC-P maximum posterior word alignment and
the corresponding lexicons. The baseline BleuC score
is 0.2276 ? 0.015. If we use the phrase pairs built
from the bracket model instead (but keep the HMM
trained lexicons), we get case sensitive BleuC score
0.2526. The improvement is statistically significant.
If on the other hand, we use baseline phrase pairs
with bracket model lexicons, we get a BleuC score
0.2325, which is only a marginal improvement. If we
use both phrase pairs and lexicons from the bracket
model, we get a case sensitive BleuC score 0.2750,
which is a statistically significant improvement. The
results are summarized in Table 3.
Settings BleuC
Baseline (HMM phrases and lexicon) 0.2276
Bracket phrases and HMM lexicon 0.2526
Bracket lexicon and HMM phrases 0.2325
Bracket (phrases and lexicon) 0.2750
Table 3: Improved case sensitive BleuC using BM-B
Overall, using Model-B, we improve translation
quality from 0.2276 to 0.2750 in case sensitive BleuC
score.
3Trained on 1-billion-word ViaVoice English data; the
same data is used to build our True Caser.
183
6 Conclusion
Our main contributions are two novel Inner-Outer
Bracket models based on segmentations induced by
hidden blocks. Modeling the Inner-Outer hidden seg-
mentations, we get significantly improved word align-
ments for both the small training set and the large
training set over the widely-practiced bidirectional
IBM Model-4 alignment. We also show significant
improvements in translation quality using our pro-
posed bracket models. Robustness to noisy blocks
merits further investigation.
7 Acknowledgement
This work is supported by DARPA under contract
number N66001-99-28916.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. In Computational Linguis-
tics, volume 19(2), pages 263?331.
Niyu Ge. 2004. A maximum posterior method
for word alignment. In Presentation given at
DARPA/TIDES MT workshop.
J.X. Huang, W.Wang, and M. Zhou. 2003. A unified
statistical model for generalized translation mem-
ory system. In Machine Translation Summit IX,
pages 173?180, New Orleans, USA, September 23-
27.
Philipp Koehn and Kevin Knight. 2002. Chunkmt:
Statistical machine translation with richer linguis-
tic knowledge. Draft, Unpublished.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based machine transla-
tion. In Proc. of HLT-NAACL 2003, pages 48?54,
Edmonton, Canada, May-June.
Philipp Koehn. 2003. Noun phrase translation. In
Ph.D. Thesis, University of Southern California,
ISI.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 133?139, Philadelphia, PA, July 6-7.
Franz J. Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for
statistical machine translation. In Proceedings of
the 40th Annual Meeting of ACL, pages 440?447.
Franz J. Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. In Computational Linguistics, volume 29,
pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
of the 40th Annual Conf. of the ACL (ACL 02),
pages 311?318, Philadelphia, PA, July.
Christoph Tillmann and Hermann Ney. 2003. Word
reordering and a dp beam search algorithm for
statistical machine translation. In Computational
Linguistics, volume 29(1), pages 97?133.
Christoph Tillmann. 2003. A projection extension
algorithm for statistical machine translation. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing.
Kristina Toutanova, H. Tolga Ilhan, and Christo-
pher D. Manning. 2002. Extensions to hmm-based
statistical word alignment models. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA, July 6-7.
S. Vogel, Hermann Ney, and C. Tillmann. 1996.
Hmm based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Com-
putational Lingustics, (Coling?96), pages 836?841,
Copenhagen, Denmark.
Taro Watanabe, Kenji Imamura, and Eiichiro
Sumita. 2002. Statistical machine translation
based on hierarchical phrases. In 9th International
Conference on Theoretical and Methodological Is-
sues, pages 188?198, Keihanna, Japan, March.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G.
Okuno. 2003. Chunk-based statistical transla-
tion. In In 41st Annual Meeting of the ACL (ACL
2003), pages 303?310, Sapporo, Japan.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. In Computational Linguistics, volume 23(3),
pages 377?403.
K. Yamada and Kevin. Knight. 2001. Syntax-based
statistical translation model. In Proceedings of the
Conference of the Association for Computational
Linguistics (ACL-2001).
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference (HLT-NAACL)s, pages 257?264, Boston,
MA, May.
184
79
80
81
82
83
84
85
86
Proceedings of NAACL HLT 2007, pages 364?371,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Log-linear Block Transliteration Model based on Bi-Stream HMMs
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel
{bzhao, nbach, ianlane, vogel}@cs.cmu.edu
Language Technologies Institute
School of Computer Science, Carnegie Mellon University
Abstract
We propose a novel HMM-based framework to
accurately transliterate unseen named entities.
The framework leverages features in letter-
alignment and letter n-gram pairs learned from
available bilingual dictionaries. Letter-classes,
such as vowels/non-vowels, are integrated to
further improve transliteration accuracy. The
proposed transliteration system is applied to
out-of-vocabulary named-entities in statistical
machine translation (SMT), and a significant
improvement over traditional transliteration ap-
proach is obtained. Furthermore, by incor-
porating an automatic spell-checker based on
statistics collected from web search engines,
transliteration accuracy is further improved.
The proposed system is implemented within
our SMT system and applied to a real transla-
tion scenario from Arabic to English.
1 Introduction
Cross-lingual natural language applications, such as in-
formation retrieval, question answering, and machine
translation for web-documents (e.g. Google translation),
are becoming increasingly important. However, current
state-of-the-art statistical machine translation (SMT) sys-
tems cannot yet translate named-entities which are not
seen during training. New named-entities, such as per-
son, organization, and location names are continually
emerging on the World-Wide-Web. To realize effective
cross-lingual natural language applications, handling out-
of-vocabulary named-entities is becoming more crucial.
Named entities (NEs) can be translated via transliter-
ation: mapping symbols from one writing system to an-
other. Letters of the source language are typically trans-
formed into the target language with similar pronunci-
ation. Transliteration between languages which share
similar alphabets and sound systems is usually not dif-
ficult, because the majority of letters remain the same.
However, the task is significantly more difficult when the
language pairs are considerably different, for example,
English-Arabic, English-Chinese, and English-Japanese.
In this paper, we focus on forward transliteration from
Arabic to English.
The work in (Arbabi et al, 1994), to our knowledge, is
the first work on machine transliteration of Arabic names
into English, French, and Spanish. The idea is to vow-
elize Arabic names by adding appropriate vowels and uti-
lizing a phonetic look-up table to provide the spelling in
the target language. Their framework is strictly applica-
ble within standard Arabic morphological rules. Knight
and Graehl (1997) introduced finite state transducers that
implement back-transliteration from Japanese to English,
which was then extended to Arabic-English in (Stalls and
Knight, 1998). Al-Onaizan and Knight (2002) translit-
erated named entities in Arabic text to English by com-
bining phonetic-based and spelling-based models, and re-
ranking candidates with full-name web counts, named en-
tities co-reference, and contextual web counts. Huang
(2005) proposed a specific model for Chinese-English
name transliteration with clusterings of names? origins,
and appropriate hypotheses are generated given the ori-
gins. All of these approaches, however, are not based
on a SMT-framework. Technologies developed for SMT
are borrowed in Virga and Khudanpur (2003) and Ab-
dulJaleel and Larkey (2003). Standard SMT alignment
models (Brown et al, 1993) are used to align letter-pairs
within named entity pairs for transliteration. Their ap-
proach are generative models for letter-to-letter transla-
tions, and the letter-alignment is augmented with heuris-
tics. Letter-level contextual information is shown to be
very helpful for transliteration. Oh and Choi (2002)
used conversion units for English-Korean Transliteration;
Goto et al (2003) used conversion units, mapping En-
glish letter-sequence into Japanese Katakana character
string. Li et al (2004) presented a framework allowing
direct orthographical mapping of transliteration units be-
tween English and Chinese, and an extended model is
presented in Ekbal et al (2006).
We propose a block-level transliteration framework, as
shown in Figure 1, to model letter-level context infor-
mation for transliteration at two levels. First, we pro-
pose a bi-stream HMM incorporating letter-clusters to
better model the vowel and non-vowel transliterations
with position-information, i.e., initial and final, to im-
prove the letter-level alignment accuracy. Second, based
on the letter-alignment, we propose letter n-gram (letter-
sequence) alignment models (block) to automatically
learn the mappings from source letter n-grams to target
letter n-grams. A few features specific for transliterations
are explored, and a log-linear model is used to combine
364
Figure 1: Transliteration System Structure. The upper-part is
the two-directional Bi-Stream HMM for letter-alignment; the
lower-part is a log-linear model for combining different feature
functions for block-level transliteration.
these features to learn block-level transliteration-pairs
from training data. The proposed transliteration frame-
work obtained significant improvements over a strong
baseline transliteration approach similar to AbdulJaleel
and Larkey (2003) and Virga and Khudanpur (2003).
The remainder of this paper is organized as follows.
In Section 2, we formulate the transliteration as a general
translation problem; in Section 4, we propose a log-linear
alignment model with a local search algorithm to model
the letter n-gram translation pairs; in Section 5, exper-
iments are presented. Conclusions and discussions are
given in Section 6.
2 Transliteration as Translation
Transliteration can be viewed as a special case of transla-
tion. In this approach, source and target NEs are split into
letter sequences, and each sequence is treated as a pseudo
sentence. The appealing reason of formulating transliter-
ation in this way is to utilize advanced alignment models,
which share ideas applied also within phrase-based sta-
tistical machine translation (Koehn, 2004).
To apply this approach to transliteration, however,
some unique aspects should be considered. First, letters
should be generated from left to right, without any re-
ordering. Thus, the transliteration models can only exe-
cute forward sequential jumps. Second, for unvowelized
languages such as Arabic, a single Arabic letter typically
maps to less than four English letters. Thus, the fertility
for each letter should be recognized to ensure reasonable
length relevance. Third, the position of the letter within
a NE is important. For example, in Arabic, letters such
as ?al? at the beginning of the NE can only be translated
into ?the? or ?al?. Therefore position information should
be considered within the alignment models.
Incorporating the above considerations, transliteration
can be formulated as a noisy channel model. Let fJ1 =
f1f2...fJ denote the source NE with J letters, eI1 =
e1e2...eI be an English transliteration candidate with I
letters. According to Bayesian decision rule:
e?I1=argmax
{eI1}
P (eI1|fJ1 )= argmax
{eI1}
P (fJ1 |eI1)P (eI1), (1)
where P (fJ1 |eI1) is the letter translation model and P (eI1)
is the English letter sequence model corresponding to
the monolingual language models in SMT. In this noisy-
channel scheme, P (fJ1 |eI1) is the key component for
transliteration, in which the transliteration between eI1
and fJ1 can be modeled at either letter-to-letter level, or
letter n-gram transliteration level (block-level).
Our transliteration models are illustrated in Figure 1.
We propose a Bi-Stream HMM of P (fJ1 |eI1) to infer
letter-to-letter alignments in two directions: Arabic-to-
English (F-to-E) and English-to-Arabic (E-to-F), shown
in the upper-part in Figure 1; refined alignment is then
obtained. We propose a log-linear model to extract block-
level transliterations with additional informative features,
as illustrated in the lower-part of Figure 1.
3 Bi-Stream HMMs for Transliteration
Standard IBM translation models (Brown et al, 1993)
can be used to obtain letter-to-letter translations. How-
ever, these models are not directly suitable, because
letter-alignment within NEs is strictly left-to-right. This
sequential property is well suited to HMMs (Vogel et al,
1996), in which the jumps from the current aligned posi-
tion can only be forward.
3.1 Bi-Stream HMMs
We propose a bi-stream HMM for letter-alignment within
NE pairs. For the source NE fJ1 and a target NE eI1, a bi-
stream HMM is defined as follows:
p(fJ1 |eI1)=
?
aJ1
J?
j=1
p(fj |eaj )p(cfj |ceaj )p(aj |aj?1), (2)
where aj maps fj to the English letter eaj at the position
aj in the English named entity. p(aj |aj?1) is the transi-
tion probability distribution assuming first-order Markov
dependency; p(fj |eaj ) is a letter-to-letter translation lex-
icon; cfj is the letter cluster of fj and p(cfj |ceaj ) is a
cluster level translation lexicon. As mentioned in the
above, the vowel/non-vowel linguistic features can be uti-
lized to cluster the letters. The letters from the same clus-
ter tend to share the similar letter transliteration forms.
p(cfj |ceaj ) enables to leverage such letter-correlation in
the transliteration process.
The HMM in Eqn. 2 generates two streams of observa-
tions: the letters together with the letters? classes follow-
ing the distribution of p(fj |eaj ) and p(cfj |ceaj ) at each
365
Figure 2: Block of letters for transliteration. A block is defined
by the left- and right- boundaries in the NE-pair.
state, respectively. To be in accordance with the mono-
tone nature of the NE?s alignment mentioned before, we
enforce the following constraints in Eqn. 3, so that the
transition can only jump forward or stay at the same state:
aj?aj?1?0 ?j ? [1, J ]. (3)
Since the two streams are conditionally independent
given the current state, the extended EM is straight-
forward, with only small modifications of the standard
forward-backward algorithm (Zhao et al, 2005), for pa-
rameter estimation.
3.2 Designing Letter-Classes
Pronunciation is typically highly structured. For in-
stance, in English the pronunciation structure of ?cvc?
(consonant-vowel-consonant) is common. By incorpo-
rating letter classes into the proposed two-stream HMM,
the models? expressiveness and robustness can be im-
proved. In this work, we focus on transliteration of Ara-
bic NEs into English. We define six non-overlapping
letter classes: vowel, consonant, initial, final, noclass,
and unknown. Initial and final classes represent semantic
markers at the beginning or end of NEs such as ?Al? and
?wAl? (in romanization form). Noclass signifies letters
which can be pronounced as both a vowel and a conso-
nant depending on context, for example, the English let-
ter ?y?. The unknown class is reserved for punctuations
and letters that we do not have enough linguistic clues for
mapping them to phonemes.
4 Transliteration Blocks
To further leverage the information from the letter-
context beyond the letter-classes incorporated in our bi-
stream HMM in Eqn. 2, we define letter n-grams, which
consist of n consecutive letters, as the basic transliter-
ation unit. A block is defined as a pair of such letter
n-grams which are transliterations of each other. Dur-
ing decoding of unseen NEs, transliteration is performed
block-by-block, rather than letter-by-letter. The goal of
transliteration model is to learn high-quality translitera-
tion blocks from the training data in a unsupervised fash-
ion.
Specifically, a block X can be represented by its left
and right boundaries in the source and target NEs shown
in Figure 2:
X = (f j+lj , ei+ki ), (4)
where f j+lj is the source letter-ngram with (l+1) letters
in source language, and its projection of ei+ki in the En-
glish NE with left boundary at the position of i, and right
boundary at (i+ k).
We formulate the block extraction as a local search
problem following the work in Zhao and Waibel (2005):
given a source letter n-gram f j+lj , search for the pro-
jected boundaries of candidate target letter n-gram ei+ki
according to a weighted combination of the diverse fea-
tures in a log-linear model detailed in ?4.3. The log-linear
model serves as a performance measure to guide the local
search, which, in our setup, is randomized hill-climbing,
to extract bilingual letter n-gram transliteration pairs.
4.1 Features for Block Transliteration
Three features: fertility, distortion, and lexical transla-
tion are investigated for inferring transliteration blocks
from the NE pairs. Each feature corresponds to one as-
pect of the block within the context of a given NE pair.
4.1.1 Letter n-gram Fertility
The fertility P (?|e) of a target letter e specifies the
probability of generating ? source letters for translitera-
tion. The fertilities can be easily read-off from the letter-
alignment, i.e., the output from the Bi-stream HMM.
Given letter fertility model P (?|ei), a target letter n-gram
eI1, and a source n-gram fJ1 of length J , we compute a
probability of letter n-gram length relevance: P (J |eI1)
via a dynamic programming.
The probability of generating J letters by the English
letter n-gram eI1 is defined:
P (J |eI1) = max{?I1,J=?Ii=1 ?i}
I?
i=1
P (?i|ei). (5)
The recursively updated cost ?[j, i] in dynamic program-
ming is defined as follows:
?[j, i] = max
?
???
???
?[j, i? 1] + logPNull(0|ei)
?[j ? 1, i? 1] + logP?(1|ei)
?[j ? 2, i? 1] + logP?(2|ei)
?[j ? 3, i? 1] + logP?(3|ei)
, (6)
where PNull(0|ei) is the probability of generating a Null
letter from ei; P?(k=1|ei) is the letter-fertility model of
generating one source letter from ei; ?[j, i] is the cost
366
so far for generating j letters from i consecutive English
letters (letter n-gram) ei1 : e1, ? ? ? , ei.
After computing the cost of ?[J, I], the probability
P (J |eI1) is computed for generating the length of the
source NE fJ1 from the English NE eI1 shown in Eqn. 5.
With this letter n-gram fertility model, for every block,
we can compute a fertility score to estimate how relevant
the lengths of the transliteration-pairs are.
4.1.2 Distortion of Centers
When aligning blocks of letters within transliteration
pairs, we expect most of them are close to the diagonal
due to the monotone alignment nature. Thus, a simple
position metric is proposed for each block considering
the relative positions within NE-pairs.
The center ?fj+lj of the source phrase f
j+l
j with a
length of (l + 1) is simply a normalized relative position
in the source entity defined as follows:
?fj+lj =
1
l + 1
j?=j+l?
j?=j
j?
l + 1 . (7)
For the center of English letter-phrase ei+ki , we first
define the expected corresponding relative center for ev-
ery source letter fj? using the lexicalized position score
as follows:
?ei+ki (fj?) =
1
k + 1 ?
?(i+k)
i?=i i? ? P (fj? |ei?)?(i+k)
i?=i P (fj? |ei?)
, (8)
where P (fj? |ei) is the letter translation lexicon estimated
in IBM Models 1?5. i is the position index, which
is weighted by the letter-level translation probabilities;
the term of
?i+k
i?=i P (fj? |ei?) provides a normalization so
that the expected center is within the range of the target
length. The expected center for ei+ki is simply the aver-
age of the ?ei+ki (fj?):
?ei+ki =
1
l + 1
j+l?
j?=j
?ei+ki (fj?) (9)
Given the estimated centers of ?fj+lj and ?ei+ki , we
can compute how close they are via the probability of
P (?fj+lj |?ei+ki ). In our case, because of the mono-
tone alignment nature of transliteration pairs, a simple
gaussian model is employed to enforce that the point
(?ei+ki ,?fj+lj ) is not far away from the diagonal.
4.1.3 Letter Lexical Transliteration
Similar to IBM Model-1 (Brown et al, 1993), we use
a ?bag-of-letter? generative model within a block to ap-
proximate the lexical transliteration equivalence:
P (f j+lj |ei+ki )=
j+l?
j?=j
i+k?
i?=i
P (fj? |ei?)P (ei? |ei+ki ), (10)
where P (ei? |ei+ki ) ' 1/(k+1) is approximated by a bag-
of-word unigram. Since named entities are usually rela-
tively short, this approximation works reasonably well in
practice.
4.2 Extended Feature Functions
Because of the underlying nature of the noisy-channel
model in our proposed transliteration approach in Section
2, the three base feature functions are extended to cover
the directions both from target-to-source and source-to-
target. Therefore, we have in total six feature functions
for inferring transliteration blocks from a named entity
pair.
Besides the above six feature functions, we also com-
pute the average letter-alignment links per block. We
count the number of letter-alignment links within the
block, and normalize the number by the length of the
source letter-ngram. Note that, we can refine the letter-
alignment by growing the intersections of the two di-
rection letter-alignments from Bi-stream HMM via ad-
ditional aligned letter-pairs seen in the union of the two.
In a way, this approach is similar to those of refining the
word-level alignment for SMT in (Och and Ney, 2003).
This step is shown in the upper-part in Figure 1.
Overall, our proposed feature functions cover rela-
tively different aspects for transliteration blocks: the
block level length relevance probability in Eqn. 5, lexical
translation equivalence, and positions? distortion from a
gaussian distribution in Eqn. 8, in both directions; and
the average number of letter-alignment links within the
block. Also, these feature functions are positive and
bounded within [0, 1]. Therefore, it is suitable to apply a
log-linear model (in ?4.3) to combine the weighted indi-
vidual strengths from the proposed feature functions for
better modeling the quality of the candidate translitera-
tion blocks. This log-linear model will serve as a per-
formance measure in a local-search in ?4.4 for inferring
transliteration blocks.
4.3 Log-Linear Transliteration Model
We propose a log-linear model to combine the seven fea-
ture functions in ?4.1 with proper weights as in Eqn. 11:
Pr(X|e, f)= exp(
?M
m=1 ?m?m(X, e, f))?
{X?} exp(
?M
m=1 ?m?m(X ?, e, f))
,
(11)
where ?m(X, e, f) are the real-valued bounded feature
functions corresponding to the seven models introduced
in ?4.1. The log-linear model?s parameters are the
weights {?m} associated with each feature function.
With hand-labeled data, {?m} can be learnt via gen-
eralized iterative scaling algorithm (GIS) (Darroch and
Ratcliff, 1972) or improved iterative scaling (IIS) (Berger
367
et al, 1996). However, as these algorithms are computa-
tionally expensive, we apply an alternative approach us-
ing a simplex down-hill algorithm to optimize the weights
toward better F-measure of block transliterations. Each
feature function corresponds to one dimension in the sim-
plex, and the local optimum only happens at a vertex of
the simplex. Simplex-downhill has several advantages:
it is an efficient approach for optimizing multi-variables
given some performance measure. We compute the F-
measure against a gold-standard block set extracted from
hand-labeled letter-alignment.
To build gold-standard blocks from hand-labeled
letter-alignment, we propose the block transliteration co-
herence in a two-stage fashion. First is the forward pro-
jection: for each candidate source letter-ngram f j+nj ,
search for its left-most el and right-most er projected
positions in the target NE according to the given letter-
alignment. Second is the backward projection: for the
target letter-gram erl , search for its left-most fl? and right-
most fr? projected positions in the source NE. Now if
l??j and r??j+n, i.e. frl is contained within the source
letter-ngram f j+nj , then this block X = (f j+nj , erl ) is de-
fined as coherent for the aligned pairs: (f j+nj , erl ) . We
accept coherent X as gold-standard blocks. This block
transliteration coherence is generally sound for extracting
the gold-blocks mostly because of the the monotone left-
to-right nature of the letter-alignment for transliteration.
A related coherence assumption can be found in (Fox,
2002), where their assumption on phrase-pairs for sta-
tistical machine translation is shown to be somewhat re-
strictive for SMT. This is mainly because the word align-
ment is often non-monotone, especially for langauge-
pairs from different families such as Arabic-English and
Chinese-English.
4.4 Aligning Letter-Blocks: a Local Search
Aligning the blocks within NE pairs can be formulated
as a local search given the heuristic function defined in
Eqn. 11. To be more specific: given a Arabic letter-ngram
f j+lj , our algorithm searches for the best translation can-
didate ei+ki in the target named entities. In our implemen-
tation, we use stochastic hill-climbing with Eqn. 11 as the
performance measure. Down-hill moves are accepted to
allow one or two left and right null letters to be attached
to ei+ki to expand the table of transliteration-blocks.
To make the local search more effective, we normal-
ize the letter translation lexicon p(f |e) within the parallel
entity pair as in:
P? (f |e) = P (f |e)?J
j?=1 P (fj? |e)
. (12)
In this way, the distribution of P? (f |e) is sharper and more
focused in the context of an entity pair.
Overall, given the parallel NE pairs, we can train the
letter level translation models in both directions via the
Bi-stream HMM in Eqn. 2. From the letter-alignment,
we can build the letter translation lexicons and fertility
tables. With these tables, the base feature functions are
then computed for each candidate block, and the features
are combined in the log-linear model in Eqn. 11. Given
a named-entity pair in the training data, we rank all the
transliteration blocks by the scores using the log-linear
model. This step is shown in the lower-part in Figure 1.
4.5 Decoding Unseen NEs
The decoding of NEs is an extension to the noisy-channel
scheme in Eqn. 1. In our configurations for NE translit-
eration, the extracted transliteration blocks are used. Our
letter ngram is a standard letter-ngram model trained us-
ing the SriLM toolkit (Stolcke, 2002). To transliterate the
unseen NEs, the decoder (Hewavitharana et al, 2005) is
configured for monotone decoding. It loads the transliter-
ation blocks and the letter-ngram LM, and it decodes the
unseen Arabic named entities with block-based translit-
eration from left to right.
5 Experiments
5.1 The Data
We have 74,887 bilingual geographic names from
LDC2005G01-NGA, 11,212 bilingual person names
from LDC2005G021, and about 6,000 bilingual names
extracted from the BAMA2 dictionary. In total, there are
92,099 NE pairs. We split them into three parts: 91,459
pairs as the training dataset, 100 pairs as the development
dataset, and 540 unique NE pairs as the held-out dataset.
An additional test set is collected from the TIDES 2003
Arabic-English machine translation evaluation test set.
The 663 sentences contain 286 unique words, which were
not covered by the available training data. From this set
of untranslated words, we manually labeled the entities of
persons, locations and organizations, giving a total of 97
unique un-translated NEs. The BAMA toolkit was used
to romanize the Arabic words. Some names from this test
set are shown in Figure 1.
These untranslated NEs make up only a very small
fraction of all words in the test set. Therefore, having
correct transliterations would give only small improve-
ments in terms of BLEU (Papineni et al, 2002) and NIST
scores. However, successfully translating these unknown
NEs is very crucial for cross-lingual distillation tasks or
question-answering based on the MT-output.
1The corpus is provided as FOUO (for official use only) in
the DARPA-GALE project
2LDC2004L02: Buckwalter Arabic Morphological Ana-
lyzer version 2.0
368
Table 1: Test Set Examples.
To evaluate the transliteration performance, we use
edit-distance between the hypothesis against a reference
set. This is to count the number of insertions, dele-
tions, and substitutions required to correct the hypoth-
esis to match the given reference. An edit-distance of
zero is a perfect match. However, NEs typically have
more than one correct variant. For example, the Arabic
name ?mHmd? (in romanized form) can be transliterated
as Muhammad or Mohammed; both are considered as
correct transliterations. Ideally, we want to have all vari-
ants as reference transliterations. To enable our translit-
eration evaluation to be more informative given only one
reference, edit-distance of one between hypothesis and
reference is considered to be an acceptable match.
5.2 Comparison of Transliteration Models
We compare the performance of three systems within our
proposed framework in Figure.1: the baseline Block sys-
tem, a system in which we use a log-linear combination
of alignment features as described in ?4.3, we call the the
L-Block system, and finally a system, which also uses
the bi-stream HMM alignment model as described in ?3.
This last system will be denoted LCBE system.
The baseline is based on the refined letter-alignment
from the two directions of IBM-Model-4, trained with a
scheme of 15h545 using GIZA++ (Och and Ney, 2004).
The final alignment was obtained by growing the inter-
sections between Arabic-to-English (AE) and English-
to-Arabic (EA) alignments with additional aligned letter-
pairs seen in the union. This is to compensate for the
inherent asymmetry in alignment models. Blocks (letter-
ngram pairs) were collected directly from the refined
letter-alignment, using the same algorithm as described
in ?4.3 for extracting gold-standard letter blocks. There is
no length restrictions to the letter-ngram extracted in our
system. All the blocks were then scored using relative
frequencies and lexical scores in both directions, similar
to the scoring of phrase-pairs in SMT (Koehn, 2004).
In the L-Block system additional feature functions as
defined in ?4.1 were computed on top of the letter-level
alignment obtained from the baseline system. A log-
linear model combining these features was learned with
the gold-blocks described in ?4.3. Transliteration blocks
were extracted using the local-search ?4.4. The other
Table 2: Transliteration accuracy for different translitera-
tion models.
System Accuracy
Baseline 39.18%
L-Block 41.24%
LCBE 46.39%
components remained the same as in the baseline system.
The LCBE system is an extension to both the baseline
and the L-Block system. The key difference in LCBE
is that our proposed bi-stream HMM in Eqn. 2 was ap-
plied in both directions with extended letter-classes. The
resulting combined alignment was used together with all
features of the L-Block system to guide the local-search
for extracting the blocks. The same procedure of decod-
ing was then carried out for the unseen NEs using the
extracted blocks.
To build the letter language model for the decoding
process, we first split the English entities into charac-
ters; additional position indicators ? begin? and ? end?
were added to the begin and end position of the named-
entity; ? middle? was added between the first name and
last name. A letter-trigram language model with SRI LM
toolkit (Stolcke, 2002) was then built using the target side
(English) of NE pairs tagged with the above position in-
formation.
Table 2 shows that the baseline system gives an accu-
racy of 39.18%, while the extended systems L-Block and
LCBE give 41.24% and 46.39%, respectively. These re-
sults show that the additional features besides the letter-
alignment are helpful. The L-Block system, which uses
these features, outperforms the baseline system signifi-
cantly by 2.1% absolute in accuracy. The results also
show that the bi-stream HMM alignment, which uses not
only the letters but also the letter-classes, leads to signif-
icant improvement. It outperforms the L-Block system,
which does not leverage the letter-classes and monotone
alignment, by 4.15% absolute.
5.3 Incorporation of Spell Checking
Our spelling-checker is based on the suggested word-
forms from web search engines for ambiguous candi-
dates. We collected web statistics frequency for both the
proposed transliteration candidates from our system, and
also the suggested candidates from web-search engines.
All the candidates were re-ranked by their frequencies.
Figure 3 shows the performances on the held-out set,
using system LCBE augmented with a spell-checker
(LCBE+Spell), with varying sizes of N-best hypotheses
lists. The held-out set contains 540 unique named entity
pairs. We show accuracy when exact match is requested
and when an edit distances of one is allowed.
369
Figure 3: Transliteration accuracy of LCBE and LCBE+Spell
models for 540 named entity pairs in the held-out set.
Figure 4: Transliteration accuracy of N-best hypotheses for
LCBE and LCBE+Spell models it the MT-03 test set.
Figure 4 shows the performances in the unseen test set
of LCBE and LCBE+Spell, with varying sizes of N-best
hypotheses lists. LCBE+Spell reaches 52% accuracy in
1-best hypothesis. In the 5-best and 10-best cases, the ac-
curacies of LCBE+Spell system archive the highest per-
formances with 66% and 72.16% respectively. The spell-
checker increases the 1-best accuracy by 11.12% and the
10-best accuracy by 7.69%. All these improvements are
statistically significant. These results are also comparable
to other state-of-the-art statistical Arabic name transliter-
ation systems such as (Al-Onaizan and Knight, 2002).
5.4 Comparison with the Google Web Translation
We finally compared our best system with the
state-of-the-art Arabic-English Google Web Translation
(Google). Table 3 shows transliteration examples from
our best system in comparison with Google (as in June
20, 2006)3. The Google system achieved 45.36% accu-
racy for the 1-best hypothesis, which is comparable to
the results when using the LCBE transliteration system,
while LCBE+Spell archived 52%.
3http://www.google.com/translate t
Table 3: Transliteration examples between LCBE+Spell
and Google web translation.
6 Conclusions and Discussions
In this paper we proposed a novel transliteration model.
Viewing transliteration as a translation task we adopt
alignment and decoding techniques used in a phrase-
based statistical machine translation system to work on
letter sequences instead of word sequences. To improve
the performance we extended the HMM alignment model
into a bi-stream HMM alignment by incorporating letter-
classes into the alignment process. We also showed that a
block-extraction approach, which uses a log-linear com-
bination of multiple alignment features, can give signif-
icant improvements in transliteration accuracy. Finally,
spell-checking based on work occurrence statistics ob-
tained from the web gave an additional boost in translit-
eration accuracy.
The goal for this work is to improve the quality of ma-
chine translation, esp. when used in cross-lingual infor-
mation retrieval and distillation tasks, by incorporating
the proposed framework to handle unknown words. Fig-
ure 5 gives an example of the difference named entity
transliteration can make. Shown are the original SMT
system output, the translation when the proposed translit-
eration models are used to translate the unknown named-
entities, and the reference translation. A comparison of
the two SMT outputs indicates that integrating the pro-
posed transliteration model into our machine translation
system can significantly improve translation utility.
Acknowledgment
This work was partially supported by grants from
DARPA (GALE project) and NFS (Str-Dust project).
References
Nasreen AbdulJaleel and Leah Larkey. 2003. Statistical
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of the 12th International
Conference on Information and Knowledge Management,
New Orleans, LA, USA, November.
370
Figure 5: Incorporation of the transliteration model to our
SMT System.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of ACL
Workshop on Computational Approaches to Semitic Lan-
guages, Philadelphia, PA, USA.
Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, and
Elizabeth Bart. 1994. Algorithms for Arabic name translit-
eration. In IBM Journal of Research and Development,
volume 38(2), pages 183?193.
Adam L. Berger, Vincent Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Linguistics,
volume 22 of 1, pages 39?71, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. In Annals of Mathematical
Statistics, volume 43, pages 1470?1480.
Asif Ekbal, S. Naskar, and S. Bandyopadhyay. 2006. A modi-
fied joint source channel model for machine transliteration.
In Proceedings of COLING/ACL, pages 191?198, Australia.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 304?311,
Philadelphia, PA, July 6-7.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context informa-
tion based on the maximum entropy method. In Proceedings
of MT-Summit IX, New Orleans, Louisiana, USA.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand,
Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel.
2005. The CMU statistical machine translation system
for IWSLT2005. In The 2005 International Workshop on
Spoken Language Translation.
Fei Huang. 2005. Cluster-specific name transliteration. In
Proceedings of the HLT-EMNLP 2005, Vancouver, BC,
Canada, October.
Kevin Knight and Jonathan Graehl. 1997. Machine transliter-
ation. In Proceedings of the Conference of the Association
for Computational Linguistics (ACL), Madrid, Spain.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based smt. In Proceedings of the Conference of
the Association for Machine Translation in the Americans
(AMTA), Washington DC, USA.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-
channel model for machine transliteration. In Proceedings
of 42nd ACL, pages 159?166, Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 1:29, pages 19?51.
Franz J. Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. In Computa-
tional Linguistics, volume 30, pages 417?449.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean
transliteration model using pronunciation and contextual
rules. In Proceedings of COLING-2002, pages 1?7, Taipei,
Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Bonnie Stalls and Kevin Knight. 1998. Translating names
and technical terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational Approaches to
Semitic Languages, Montreal, Quebec, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration
of proper names in cross-lingual information retrieval. In
Proceedings of the ACL Workshop on Multi-lingual Named
Entity Recognition, Edmonton, Canada.
Stephan. Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Computational
Lingustics, (COLING-1996), pages 836?841, Copenhagen,
Denmark.
Bing Zhao and Alex Waibel. 2005. Learning a log-linear
model with bilingual phrase-pair features for statistical
machine translation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, Jeju Island,
Korean, October.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual
word spectral clustering for statistical machine translation.
In Proceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 25?32, Ann Arbor, Michigan, June.
371
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969?976,
Sydney, July 2006. c?2006 Association for Computational Linguistics
BiTAM: Bilingual Topic AdMixture Models for Word Alignment
Bing Zhao? and Eric P. Xing??
{bzhao,epxing}@cs.cmu.edu
Language Technologies Institute? and Machine Learning Department?
School of Computer Science, Carnegie Mellon University
Abstract
We propose a novel bilingual topical ad-
mixture (BiTAM) formalism for word
alignment in statistical machine transla-
tion. Under this formalism, the paral-
lel sentence-pairs within a document-pair
are assumed to constitute a mixture of
hidden topics; each word-pair follows a
topic-specific bilingual translation model.
Three BiTAM models are proposed to cap-
ture topic sharing at different levels of lin-
guistic granularity (i.e., at the sentence or
word levels). These models enable word-
alignment process to leverage topical con-
tents of document-pairs. Efficient vari-
ational approximation algorithms are de-
signed for inference and parameter esti-
mation. With the inferred latent topics,
BiTAM models facilitate coherent pairing
of bilingual linguistic entities that share
common topical aspects. Our preliminary
experiments show that the proposed mod-
els improve word alignment accuracy, and
lead to better translation quality.
1 Introduction
Parallel data has been treated as sets of unre-
lated sentence-pairs in state-of-the-art statistical
machine translation (SMT) models. Most current
approaches emphasize within-sentence dependen-
cies such as the distortion in (Brown et al, 1993),
the dependency of alignment in HMM (Vogel et
al., 1996), and syntax mappings in (Yamada and
Knight, 2001). Beyond the sentence-level, corpus-
level word-correlation and contextual-level topical
information may help to disambiguate translation
candidates and word-alignment choices. For ex-
ample, the most frequent source words (e.g., func-
tional words) are likely to be translated into words
which are also frequent on the target side; words of
the same topic generally bear correlations and sim-
ilar translations. Extended contextual information
is especially useful when translation models are
vague due to their reliance solely on word-pair co-
occurrence statistics. For example, the word shot
in ?It was a nice shot.? should be translated dif-
ferently depending on the context of the sentence:
a goal in the context of sports, or a photo within
the context of sightseeing. Nida (1964) stated
that sentence-pairs are tied by the logic-flow in a
document-pair; in other words, the document-pair
should be word-aligned as one entity instead of be-
ing uncorrelated instances. In this paper, we pro-
pose a probabilistic admixture model to capture
latent topics underlying the context of document-
pairs. With such topical information, the trans-
lation models are expected to be sharper and the
word-alignment process less ambiguous.
Previous works on topical translation models
concern mainly explicit logical representations of
semantics for machine translation. This include
knowledge-based (Nyberg and Mitamura, 1992)
and interlingua-based (Dorr and Habash, 2002)
approaches. These approaches can be expen-
sive, and they do not emphasize stochastic trans-
lation aspects. Recent investigations along this
line includes using word-disambiguation schemes
(Carpua and Wu, 2005) and non-overlapping bilin-
gual word-clusters (Wang et al, 1996; Och, 1999;
Zhao et al, 2005) with particular translation mod-
els, which showed various degrees of success. We
propose a new statistical formalism: Bilingual
Topic AdMixture model, or BiTAM, to facilitate
topic-based word alignment in SMT.
Variants of admixture models have appeared in
population genetics (Pritchard et al, 2000) and
text modeling (Blei et al, 2003). Statistically, an
object is said to be derived from an admixture if it
consists of a bag of elements, each sampled inde-
pendently or coupled in some way, from a mixture
model. In a typical SMT setting, each document-
pair corresponds to an object; depending on a
chosen modeling granularity, all sentence-pairs or
word-pairs in the document-pair correspond to the
elements constituting the object. Correspondingly,
a latent topic is sampled for each pair from a prior
topic distribution to induce topic-specific transla-
tions; and the resulting sentence-pairs and word-
pairs are marginally dependent. Generatively, this
admixture formalism enables word translations to
be instantiated by topic-specific bilingual models
969
and/or monolingual models, depending on their
contexts. In this paper we investigate three in-
stances of the BiTAM model, They are data-driven
and do not need hand-crafted knowledge engineer-
ing.
The remainder of the paper is as follows: in sec-
tion 2, we introduce notations and baselines; in
section 3, we propose the topic admixture models;
in section 4, we present the learning and inference
algorithms; and in section 5 we show experiments
of our models. We conclude with a brief discus-
sion in section 6.
2 Notations and Baseline
In statistical machine translation, one typically
uses parallel data to identify entities such as
?word-pair?, ?sentence-pair?, and ?document-
pair?. Formally, we define the following terms1:
? A word-pair (fj , ei) is the basic unit for word
alignment, where fj is a French word and ei
is an English word; j and i are the position
indices in the corresponding French sentence
f and English sentence e.
? A sentence-pair (f , e) contains the source
sentence f of a sentence length of J ; a target
sentence e of length I . The two sentences f
and e are translations of each other.
? A document-pair (F,E) refers to two doc-
uments which are translations of each other.
Assuming sentences are one-to-one corre-
spondent, a document-pair has a sequence of
N parallel sentence-pairs {(fn, en)}, where
(fn, en) is the n?th parallel sentence-pair.
? A parallel corpus C is a collection of M par-
allel document-pairs: {(Fd,Ed)}.
2.1 Baseline: IBM Model-1
The translation process can be viewed as opera-
tions of word substitutions, permutations, and in-
sertions/deletions (Brown et al, 1993) in noisy-
channel modeling scheme at parallel sentence-pair
level. The translation lexicon p(f |e) is the key
component in this generative process. An efficient
way to learn p(f |e) is IBM-1:
p(f |e) =
J?
j=1
I?
i=1
p(fj |ei) ? p(ei|e). (1)
1We follow the notations in (Brown et al, 1993) for
English-French, i.e., e ? f , although our models are tested,
in this paper, for English-Chinese. We use the end-user ter-
minology for source and target languages.
IBM-1 has global optimum; it is efficient and eas-
ily scalable to large training data; it is one of the
most informative components for re-ranking trans-
lations (Och et al, 2004). We start from IBM-1 as
our baseline model, while higher-order alignment
models can be embedded similarly within the pro-
posed framework.
3 Bilingual Topic AdMixture Model
Now we describe the BiTAM formalism that
captures the latent topical structure and gener-
alizes word alignments and translations beyond
sentence-level via topic sharing across sentence-
pairs:
E?=argmax
{E}
p(F|E)p(E), (2)
where p(F|E) is a document-level translation
model, generating the document F as one entity.
In a BiTAM model, a document-pair (F,E) is
treated as an admixture of topics, which is induced
by random draws of a topic, from a pool of topics,
for each sentence-pair. A unique normalized and
real-valued vector ?, referred to as a topic-weight
vector, which captures contributions of different
topics, are instantiated for each document-pair, so
that the sentence-pairs with their alignments are
generated from topics mixed according to these
common proportions. Marginally, a sentence-
pair is word-aligned according to a unique bilin-
gual model governed by the hidden topical assign-
ments. Therefore, the sentence-level translations
are coupled, rather than being independent as as-
sumed in the IBM models and their extensions.
Because of this coupling of sentence-pairs (via
topic sharing across sentence-pairs according to
a common topic-weight vector), BiTAM is likely
to improve the coherency of translations by treat-
ing the document as a whole entity, instead of un-
correlated segments that have to be independently
aligned and then assembled. There are at least
two levels at which the hidden topics can be sam-
pled for a document-pair, namely: the sentence-
pair and the word-pair levels. We propose three
variants of the BiTAM model to capture the latent
topics of bilingual documents at different levels.
3.1 BiTAM-1: The Frameworks
In the first BiTAM model, we assume that topics
are sampled at the sentence-level. Each document-
pair is represented as a random mixture of la-
tent topics. Each topic, topic-k, is presented by a
topic-specific word-translation table: Bk, which is
970
f
a
J
I
N
M
e
B? z?
?
J B
I
f
e
a
? ? z
M
N
a
J
I
N
M
e
B? z? f
(a) (b) (c)
Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and
a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate
(M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each
sentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair. (a) BiTAM-1 samples
one topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e.,
p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair.
a translation lexicon: Bi,j,k=p(f=fj |e=ei, z=k),
where z is an indicator variable to denote the
choice of a topic. Given a specific topic-weight
vector ?d for a document-pair, each sentence-pair
draws its conditionally independent topics from a
mixture of topics. This generative process, for a
document-pair (Fd,Ed), is summarized as below:
1. Sample sentence-number N from a Poisson(?).
2. Sample topic-weight vector ?d from a Dirichlet(?).
3. For each sentence-pair (fn, en) in the d?th doc-pair ,
(a) Sample sentence-length Jn from Poisson(?);
(b) Sample a topic zdn from a Multinomial(?d);
(c) Sample ej from a monolingual model p(ej);
(d) Sample each word alignment link aj from a uni-
form model p(aj) (or an HMM);
(e) Sample each fj according to a topic-specific
translation lexicon p(fj |e, aj , zn,B).
We assume that, in our model, there are K pos-
sible topics that a document-pair can bear. For
each document-pair, a K-dimensional Dirichlet
random variable ?d, referred to as the topic-weight
vector of the document, can take values in the
(K?1)-simplex following a probability density:
p(?|?) = ?(
?K
k=1 ?k)?K
k=1 ?(?k)
??1?11 ? ? ? ??K?1K , (3)
where the hyperparameter ? is a K-dimension
vector with each component ?k>0, and ?(x)
is the Gamma function. The alignment is
represented by a J-dimension vector a =
{a1, a2, ? ? ? , aJ}; for each French word fj at the
position j, an position variable aj maps it to an
English word eaj at the position aj in English sen-
tence. The word level translation lexicon probabil-
ities are topic-specific, and they are parameterized
by the matrix B = {Bk}.
For simplicity, in our current models we omit
the modelings of the sentence-number N and the
sentence-length Jn, and focus only on the bilin-
gual translation model. Figure 1 (a) shows the
graphical model representation for the BiTAM
generative scheme discussed so far. Note that, the
sentence-pairs are now connected by the node ?d.
Therefore, marginally, the sentence-pairs are not
independent of each other as in traditional SMT
models, instead they are conditionally indepen-
dent given the topic-weight vector ?d. Specifi-
cally, BiTAM-1 assumes that each sentence-pair
has one single topic. Thus, the word-pairs within
this sentence-pair are conditionally independent of
each other given the hidden topic index z of the
sentence-pair.
The last two sub-steps (3.d and 3.e) in the
BiTam sampling scheme define a translation
model, in which an alignment link aj is proposed
and an observation of fj is generated according
to the proposed distributions. We simplify align-
ment model of a, as in IBM-1, by assuming that
aj is sampled uniformly at random. Given the pa-
rameters ?, B, and the English part E, the joint
conditional distribution of the topic-weight vector
?, the topic indicators z, the alignment vectors A,
and the document F can be written as:
p(F,A, ?, z|E, ?,B)=
p(? |?)
N?
n=1
p(zn|?)p(fn,an|en, ?, Bzn),
(4)
where N is the number of the sentence-pair.
Marginalizing out ? and z, we can obtain the
marginal conditional probability of generating F
from E for each document-pair:
p(F,A|E, ?,Bzn) =
?
p(?|?)
( N?
n=1
?
zn
p(zn|?)p(fn,an|en, Bzn)
)
d?, (5)
where p(fn,an|en, Bzn) is a topic-specific
sentence-level translation model. For simplicity,
we assume that the French words fj?s are condi-
tionally independent of each other; the alignment
971
variables aj?s are independent of other variables
and are uniformly distributed a priori. Therefore,
the distribution for each sentence-pair is:
p(fn,an|en, Bzn) = p(fn|en,an, Bzn)p(an|en, Bzn)
= 1IJnn
Jn?
j=1
p(fnj |eanj , Bzn). (6)
Thus, the conditional likelihood for the entire
parallel corpus is given by taking the product
of the marginal probabilities of each individual
document-pair in Eqn. 5.
3.2 BiTAM-2: Monolingual Admixture
In general, the monolingual model for English
can also be a rich topic-mixture. This is real-
ized by using the same topic-weight vector ?d and
the same topic indicator zdn sampled according
to ?d, as described in ?3.1, to introduce not only
topic-dependent translation lexicon, but also topic-
dependent monolingual model of the source lan-
guage, English in this case, for generating each
sentence-pair (Figure 1 (b)). Now e is generated
from a topic-based language model ?, instead of a
uniform distribution in BiTAM-1. We refer to this
model as BiTAM-2.
Unlike BiTAM-1, where the information ob-
served in ei is indirectly passed to z via the node
of fj and the hidden variable aj , in BiTAM-2, the
topics of corresponding English and French sen-
tences are also strictly aligned so that the informa-
tion observed in ei can be directly passed to z, in
the hope of finding more accurate topics. The top-
ics are inferred more directly from the observed
bilingual data, and as a result, improve alignment.
3.3 BiTAM-3: Word-level Admixture
It is straightforward to extend the sentence-level
BiTAM-1 to a word-level admixture model, by
sampling topic indicator zn,j for each word-pair
(fj , eaj ) in the n?th sentence-pair, rather than
once for all (words) in the sentence (Figure 1 (c)).
This gives rise to our BiTAM-3. The conditional
likelihood functions can be obtained by extending
the formulas in ?3.1 to move the variable zn,j in-
side the same loop over each of the fn,j .
3.4 Incorporation of Word ?Null?
Similar to IBM models, ?Null? word is used for
the source words which have no translation coun-
terparts in the target language. For example, Chi-
nese words ?de? () , ?ba? (r) and ?bei?
(Word Alignment Based on Bilingual Bracketing
Bing Zhao
Language Technologies Institute
Carnegie Mellon University
bzhao@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
vogel+@cs.cmu.edu
Abstract
In this paper, an improved word alignment
based on bilingual bracketing is described. The
explored approaches include using Model-1
conditional probability, a boosting strategy for
lexicon probabilities based on importance sam-
pling, applying Parts of Speech to discriminate
English words and incorporating information
of English base noun phrase. The results of
the shared task on French-English, Romanian-
English and Chinese-English word alignments
are presented and discussed.
1 Introduction
Bilingual parsing based word alignment is promising
but still difficult. The goal is to extract structure in-
formation from parallel sentences, and thereby improve
word/phrase alignment via bilingual constraint transfer.
This approach can be generalized to the automatic acqui-
sition of a translation lexicon and phrase translations esp.
for languages for which resources are relatively scarce
compared with English.
The parallel sentences in building Statistical Machine
Translation (SMT) systems are mostly unrestricted text
where full parsing often fails, and robustness with respect
to the inherent noise of the parallel data is important.
Bilingual Bracketing [Wu 1997] is one of the bilingual
shallow parsing approaches studied for Chinese-English
word alignment. It uses a translation lexicon within a
probabilistic context free grammar (PCFG) as a genera-
tive model to analyze the parallel sentences with weak
order constraints. This provides a framework to incorpo-
rate knowledge from the English side such as POS, phrase
structure and potentially more detailed parsing results.
In this paper, we use a simplified bilingual bracket-
ing grammar together with a statistical translation lexicon
such as the Model-1 lexicon [Brown 1993] to do the bilin-
gual bracketing. A boosting strategy is studied and ap-
plied to the statistical lexicon training. English POS and
Base Noun Phrase (NP) detection are used to further im-
prove the alignment performance. Word alignments and
phrase alignments are extracted from the parsing results
as post processing. The settings of different translation
lexicons within the bilingual bracketing framework are
studied and experiments on word-alignment are carried
out on Chinese-English, French-English, and Romanian-
English language pairs.
The paper is structured as follows: in section 2, the
simplified bilingual bracketing used in our system is de-
scribed; in section 3, the boosting strategy based on im-
portance sampling for IBM Model-1 lexicon is intro-
duced; in section 4, English POS and English Base Noun
Phrase are used to constrain the alignments ; in section
5, the experimental results are shown; summary and con-
clusions are given in section 6.
2 Bilingual Bracketing
In [Wu 1997], the Bilingual Bracketing PCFG was intro-
duced, which can be simplified as the following produc-
tion rules:
A ? [AA] (1)
A ? < AA > (2)
A ? f/e (3)
A ? f/null (4)
A ? null/e (5)
Where f and e are words in the target vocabulary Vf and
source vocabulary Ve respectively. A is the alignment
of texts. There are two operators for bracketing: direct
bracketing denoted by [ ], and inverse bracketing, de-
noted by <>. The A-productions are divided into two
classes: syntactic {(1),(2)}and lexical rules {(3),(4),(5)}.
Each A-production rule has a probability.
In our algorithm, we use the same PCFG. However,
instead of estimating the probabilities for the production
rules via EM as described in [Wu 1997], we assign the
probabilities to the rules using the Model-1 statistical
translation lexicon [Brown et al 1993].
Because the syntactic A-production rules do not com-
pete with the lexical rules, we can set them some default
values. Also we make no assumptions which bracketing
direction is more likely to occur, thus the probabilities
for [ ] and <> are set to be equal. As for the lexical
rules, we experimented with the conditional probabilities
p(e|f), p(f |e) and the interpolation of p(f |e, epos) and
p(f |e) (described in section 4.1). As for these probabil-
ities of aligning a word to the null word or to unknown
words, they are set to be 1e-7, which is the default small
value used in training Model-1.
The word alignment can then be done via maximizing
the likelihood of matched words subject to the bracketing
grammar using dynamic programming.
The result of the parsing gives bracketing for both in-
put sentences as well as bracket algnments indicating the
corresponding brackets between the sentence pairs. The
bracket algnment includes a word alignment as a by-
product. One example for French-English (the test set
sentence pair #18) is shown as below:
[[it1 is2 ] [quite3 [understandable4 .5 ]]]
[[ce1 est2 ] [tout3 [[?4 [fait5 comprihensible6 ] ] .7]]]
[[it1/ce1 is2/est2 ] [quite3/tout3 [[e/?4 [e/fait5
understandable4/comprihensible6 ] ] .5/.7]]]
3 Boosting Strategy of Model-1 Lexicon
The probabilities for the lexical rules are Model-1 condi-
tional probabilities p(f |e), which can be estimated using
available toolkits such as [Franz 2000].
This strategy is a three-pass training of Model-1, which
was shown to be effective in our Chinese-English align-
ment experiments. The first two passes are carried out to
get Viterbi word alignments based on Model-1?s param-
eters in both directions: from source to target and then
vice versa. An intersection of the two Viterbi word align-
ments is then calculated. The highly frequent word-pairs
in the intersection set are considered to be important sam-
ples supporting the alignment of that word-pair. This ap-
proach, which is similar to importance sampling, can be
summarized as follows:
Denote a sample as a co-occurred word-pair as
x = (ei, fj) with its observed frequency: C(x) =
freq(ei, fj); Denote I(x) = freq(ei, fj) as the fre-
quency of that word-pair x observed in the intersection
of the two Viterbi alignments.
? Build I(x) = freq(ei, fj) from the intersection of
alignments in two directions.
? Generate x = (ei, fj) and its C(x) = freq(ei, fj)
observed from a given parallel corpus;
? Generate random variable u from uniform [0,1] dis-
tribution independent of x;
? If I(x)M ?C(x) ? u, then accept x, where M is a finite
known constant M > 0;
? Re-weight sample x: Cb(x) = C(x)?(1+?), ? > 0)
The modified counts (weighted samples) are re-
normalized to get a proper probability distribution, which
is used in the next iteration of EM training. The constant
M is a threshold to remove the potential noise from the
intersection set. M ?s value is related to the size of the
training corpus, the larger its size, the larger M should
be. ? is chosen as a small positive value. The overall idea
is to collect those word-pairs which are reliable and give
an additional pseudo count to them.
4 Incorporating English Grammatical
Constraints
There are several POS taggers, base noun phrase detec-
tors and parsers available for English. Both the shallow
and full parsing information of English sentences can be
used as constraints in Bilingual Bracketing. Here, we
explored utilizing English POS and English base noun
phrase boundaries.
4.1 Incorporating English POS
The correctly aligned words from two languages are very
likely to have the same POS. For example, a Chinese
noun is very likely to be aligned with a English noun.
While the English POS tagging is often reliable and ac-
curate, the POS tagging for other languages is usually not
easily acquired nor accurate enough. Modelling only the
English POS in word alignment is usually a practical way.
Given POS information for only the English side, we
can discriminate English words and thus disambiguate
the translation lexicon. We tagged each English word
in the parallel corpus, so that each English word is as-
sociated with its POS denoted as epos. The English
word and its POS were concatenated into one pseudo
word. For example: beginning/NN and beginning/VBG
are two pseudo words which occurred in our training
corpus. Then the Model-1 training was carried out on
this concatenated parallel corpus to get estimations of
p(f |e, epos).
One potential problem is the estimation of p(f |e, epos).
When we concatenated the word with its POS, we im-
plicitly increased the vocabulary size. For example, for
French-English training set, the English vocabulary in-
creased from 57703 to 65549. This may not cause a prob-
lem when the training data?s size is large. But for small
parallel corpora, some correct word-pair?s p(f |e, epos)
will be underestimated due to the sparse data, and some
word-pairs become unknown in p(f |e, epos). So in our
system, we actually interpolated p(f |e, epos) with p(f |e)
as a mixture model for robustness:
P (A ? f/e|A) = ??P (f |e)+(1??)?P (f |e, epos) (6)
Where ? can be estimated by EM for this two-mixture
model on the training data, or a grid search via cross-
validation.
4.2 Incorporating English Base Noun Boundaries
The English sentence is bracketed according to the syn-
tactic A-production rules. This bracketing can break an
English noun phrase into separated pieces, which are
not in accordance with results from standard base noun
phrase detectors. Though the word-alignments may still
be correct, but for the phrase level alignment, it is not
desired.
One solution is to constrain the syntactic A-production
rules to penalize bracketing English noun phrases into
separated pieces. The phrase boundaries can be obtained
by using a base noun phrase detection toolkit [Ramshaw
1995], and the boundaries are loaded into the bracketing
program. During the dynamic programming, before ap-
plying a syntactic A-production rule, the program checks
if the brackets defined by the syntactic rule violate the
noun phrase boundaries. If so, an additional penalty is
attached to this rule.
5 Experiments
All the settings described so far are based on our pre-
vious experiments on Chinese-English (CE) alignment.
These settings are then used directly without any ad-
justment of the parameters for the French-English (FE)
and Romanian-English (RE) word alignment tasks. In
this section, we will first describe our experiments on
Chinese-English alignment, and then the results for the
shared task on French-English and Romanian-English.
For Chinese-English alignment, 365 sentence-pairs are
randomly sampled from the Chinese Tree-bank provided
by the Linguistic Data Consortium. Three persons man-
ually aligned the word-pairs independently, and the con-
sistent alignments from all of them were used as the ref-
erence alignments. There are totally 4094 word-pairs in
the reference set. Our way of alignment is very similar
to the ?SURE? (S) alignment defined in the shared task.
The training data we used is 16K parallel sentence-pairs
from Hong-Kong news data. The English POS tagger we
used is Brill?s POS tagger [Brill 1994]. The base noun
detector is [Ramshaw 1995]. The alignment is evaluated
in terms of precision, recall, F-measure and alignment er-
ror rate (AER) defined in the shared task. The results are
shown in Table-1:
Table-1. Chinese-English Word-Alignment
CE precision recall F-measure AER
No-Boost 50.88 58.77 54.54 45.46
Boosted 52.19 60.33 55.96 44.04
+POS 54.77 63.34 58.71 41.29
+NP 55.16 63.75 59.14 40.86
Table-1 shows the effectiveness of using each setting
on this small size training data. Here the boosted model
gives a noticeable improvement over the baseline. How-
ever, our observations on the trial/test data showed very
similar results for boosted and non-boosted models, so
we present only the non-boosted results(standard Model-
1) for the shared task of EF and RE word alignment.
Adding POS further improved the performance signif-
icantly. The AER drops from 44.04 to 41.29. Adding
additional base noun phrase boundaries did not give as
much improvement as we hoped. There is only slight
improvement in terms of AER and F-measure. One rea-
son is that noun phrase boundaries is more directly re-
lated to phrase alignment than word-alignment. A close
examination showed that with wrong phrase-alignment,
word-alignment can still be correct. Another reason is
that using the noun phrase boundaries this way may not
be powerful enough to leverage the English structure in-
formation in Bilingual Bracketing. More suitable ways
could be bilingual chunk parsing, and refining the brack-
eting grammar as described in [Wu 1997].
In the shared task experiments, we restricted the train-
ing data to sentences upto 60 words. The statistics for the
training sets are shown in Table-2. (French/Romanian are
source and English is target language).
Table-2.Training Set Statistics
French-English Romanian-English
Sent-pairs 1028382 45456
Src Voc 79601 45880
Tgt Voc 57703 26904
There are 447 test sentence pairs for English-French
and 248 test sentence pairs for Romanian-English. After
the bilingual bracketing, we extracted only the explicit
word alignment from lexical rules: A ? e/f , where nei-
ther e nor f is the null(empty) word. These explicit word
alignments are more directly related to the translation
quality in our SMT system than the null-word alignments.
Also the explicit word alignments is in accordance with
the ?SURE? (S) alignment defined in the shared tasks.
However the Bilingual Bracketing system is not adapted
to the ?PROBABLE? (P) alignment because of the inher-
ent one-to-one mapping. All the AERs in the following
tables are calculated based solely on S alignment without
any null alignments collected from the bracketing results.
Table-3. Limited Resource French-English
FE precision recall F-measure AER
p(f |e) 49.85 79.45 61.26 23.87
p(e|f) 51.46 82.42 63.36 20.95
inter 63.03 74.59 68.32 19.26
Table-4. Unlimited Resource French-English
FE precision recall F-measure AER
p(f |e) 50.21 80.36 61.80 23.07
p(e|f) 51.91 83.26 63.95 19.96
inter 66.34 74.86 70.34 17.77
For the limited resource task, we trained Model-1 lex-
icons in both directions: from source to target denoted as
p(f |e) and from target to source denoted as p(e|f). These
two lexicons are then plugged into the Bilingual Brack-
eting algorithm separately to get two sets of bilingual
bracketing word alignments. The intersection of these
two sets of word alignments is then collected. The result-
ing AERs are shown in Table-3 and Table-5 respectively.
For the unlimited resource task, we again tagged the
English sentences and base noun phrase boundaries as
mentioned before. Then corresponding Model-1 lexicon
was trained and Bilingual Bracketing carried out. Using
the same strategies as in the limited resource task, we got
the results shown in Table-4 and Table-6.
The table above show that adding English POS and
base noun detection gave a consistent improvement for
all conditions in the French-to-English alignment. The
intersection of the two alignments greatly improves the
precision, paired with a reduction in recall, still resulting
in an overall improvement in F-measure and AER.
For the Romanian-English alignment the POS tagging
and noun phrase boundaries did not help. On the small
corpus the increase in vocabulary resulted in addition un-
known words in the test sentences which introduces ad-
ditional alignment errors.
Comparing the results of the French-English and
Romanian-English alignment tasks we see a striking dif-
ference in precision and recall. Whereas the French-
English alignment has a low precision and a high recall
its the opposite for the Romanian-English alignment. The
cause lays in different styles for the manual alignments.
The French-English reference set contains both S and P
alignments, whereas the Romanian-English reference set
was annotated with only S alignments. As a result, there
are on average only 0.5 S alignments per word in the FE
reference set, but 1.5 S alignments per word in the RE
test set.
6 Summary
In this paper we presented our word alignment system
based on bilingual bracketing. We introduced a technique
Table-5. Limited Resource Romanian-English
RE precision recall F-measure AER
p(r|e) 70.65 55.75 62.32 37.66
p(e|r) 71.39 55.00 62.13 37.87
inter 85.48 48.64 62.01 37.99
Table-6. Unlimited Resource Romanian-English
RE precision recall F-measure AER
p(r|e) 69.63 54.65 61.24 38.76
p(e|r) 70.36 55.50 62.05 37.95
inter 82.09 48.73 61.15 38.85
to boost lexical probabilities for more reliable word pairs
in the statistical lexicon. In addition, we investigated the
effects of using POS and noun phrase detection on the
English side of the bilingual corpus as constraints for the
alignment. We applied these techniques to the French-
English and Romanian-English alignment tasks, and in
addition to Chinese-English alignment. For Chinese-
English and French-English alignments these additional
knowledge sources resulted in improvements in align-
ment quality. Best results were obtained by using the
intersection of the source to target and target to source
bilingual bracketing alignments. The results show very
different behavior of the alignment system on the French-
English and Romanian-English tasks which is due to dif-
ferent characteristics of the manually aligned test data.
This indicates that establishing a good golden standard
for word alignment evaluation is still an open issue.
References
Brown, P. F. and Della Pietra, S. A. and Della Pietra, V. J.
and Mercer, R. L. 1993. The Mathematics of Statisti-
cal Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19-2, pp 263-311.
Erik Brill. 1994. Some advances in rule-based part of
speech tagging. Proceedings of the Twelfth National
Conference on Artificial Intelligence (AAAI-94), Seat-
tle, Wa., 1994.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. Proceedings of ACL-00,
pp. 440-447, Hongkong, China.
Lance Ramshaw and Mitchell Marcus 1995. Text Chunk-
ing Using Transformation-Based Learning. Proceed-
ings of the Third ACL Workshop on Very Large Cor-
pora, MIT, June, 1995.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics 23(3):377-404, Sep. 1997.
Efficient Optimization for Bilingual Sentence Alignment  
Based on Linear Regression 
 
Bing Zhao 
 
Language Technologies 
Institute 
Carnegie Mellon University 
bzhao@cs.cmu.edu 
Klaus Zechner 
 
Educational Testing Service 
Rosedale Road, Princeton, 
NJ 08541 
kzechner@ets.org 
Stephan Vogel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
vogel+@cs.cmu.edu 
Alex Waibel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
ahw@cs.cmu.edu 
 
Abstract 
This paper presents a study on optimizing sen-
tence pair alignment scores of a bilingual sen-
tence alignment module. Five candidate 
scores based on perplexity and sentence 
length are introduced and tested. Then a linear 
regression model based on those candidates is 
proposed and trained to predict sentence pairs? 
alignment quality scores solicited from human 
subjects. Experiments are carried out on data 
automatically collected from Internet. The 
correlation between the scores generated by 
the linear regression model and the scores 
from human subjects is in the range of the in-
ter-subject agreement score correlations. Pear-
son's correlation ranges from 0.53 up to 0.72 
in our experiments.  
1 Introduction 
In many instances, multilingual natural language 
systems like machine translation systems are developed 
and trained on parallel corpora.  When faced with a dif-
ferent, unseen text genre, however, translation perform-
ance usually drops noticeably.  One way to remedy this 
situation is to adapt and retrain the system parameters 
based on bilingual data from the same source or at least 
a closely related source.  A bilingual sentence alignment 
program (Gale and Church, 1991, and Brown et al, 
1991) is the crucial part in this adaptation procedure, in 
that it collects bilingual document pairs from the Inter-
net, and identifies sentence pairs, which should have a 
high likelihood of being correct translations of each 
other.  The set of identified bilingual parallel sentence 
pairs is then added to the training set for parameter re-
estimation. 
As is well known, text mined from the Internet is 
very noisy.  Even after careful html parsing and filtering 
for text size and language, the text from comparable 
html-page pairs still contains mismatches of content or 
non-parallel junk text, and the sentence order can be too 
different to be aligned.  Together with a large mismatch 
of vocabulary, the aligned sentence pairs, which are 
extracted from these collected comparable html-page 
pairs, contain a number of low translation quality 
alignments.  These need to be removed before the re-
training of the MT system. 
In this paper, we present an approach to automati-
cally optimizing the alignment scores of such a bilingual 
sentence alignment program.  The alignment score is a 
combination (by linear regression) of two word transla-
tion lexicon scores and three sentence length scores and 
predicts the translation quality scores from a set of hu-
man annotators.  We also present experiments analyzing 
how many different human scorers are needed for good 
prediction and also how many sentence pairs should be 
scored per human annotator. 
The paper is structured as follows: in section 2, the 
text mining system is briefly described.  In section 3, 
five sentence alignment models based on lexical infor-
mation and sentence length are explained. In section 4, a 
regression model is proposed to combine the five mod-
els to get further improvement in predicting alignment 
quality.  We describe alignment experiments in section 
5, focusing on the correlation between the alignment 
scores predicted by the sentence alignment models and 
by humans.  Conclusions are given in section 6. 
2 System of Mining Parallel Text 
One crucial component of statistical machine trans-
lation (SMT) system is the parallel text mining from 
Internet. Several processing modules are applied to col-
lect, extract, convert, and clean the text from Internet.  
The components in our system include: 
? A web crawler, which collects potential parallel 
html documents based on link information follow-
ing (Philip Resnik 1999); 
? A bilingual html parser (based on flex for effi-
ciency), which is designed for both Chinese and 
English html documents.  The paragraphs? bounda-
ries within the html structure are kept.  
? A character encoding detector, which judges if the 
Chinese html document is GB2312 encoding or 
BIG5 encoding.  
? An encoding converter, which converts the BIG5 
documents to GB2312 encoding.  
? A language identifier to ensure that source and tar-
get documents are both of the proper language. 
(Noord?s Implementation).  
? A Chinese word segmenter, which parses the Chi-
nese strings into Chinese words.   
? A document alignment program, which judges if 
the document pair is close translation candidates, 
and filters out those non-translation pairs. 
? A sentence boundary detector, which is based on 
punctuation and capitalized characters; 
? And the key component, a sentence alignment pro-
gram, which aligns and extracts potential parallel 
sentence pairs from the candidate document pairs. 
   
After sentence alignment, each candidate of a par-
allel sentence pair is then re-scored by the regression 
models (to be described in section 5). These scores are 
used to judge the quality of the aligned sentences.  Thus 
one can select the aligned sentence pairs, which have 
high alignment quality scores, to re-estimate the sys-
tem?s parameters.  
2.1 Sentence Alignment 
Our sentence alignment program uses IBM Model-1 
based perplexity (section 2.2) to calculate the similarity 
of each sentence pair. Dynamic programming is applied 
to find Viterbi path for sentence alignments of the bilin-
gual comparable document pair. In our dynamic pro-
gramming implementation, we allow for seven 
alignment types between English and Chinese sentences: 
 
? 1:1 ? exact match, where one sentence is the trans-
lation of the other one; 
? 2:2 ? the break point between two sentences in the 
source document is different from the segmentation 
in the target document.  E.g. part of sentence one in 
the source might be translated as part of the second 
sentence in the target; 
? 2:1, 1:2, and 3:1 ? these cases are similar to the 
case before: they handle differences in how a text is 
split into sentences. The case 1:3 has not been used 
in the final configuration of the system, as this type 
did not occur in any significant number; 
? 1:0 (deletion) and (0:1) insertion ? a sentence in the 
source document is missing in the translation or 
vice versa. 
 
The deletion and insertion types are discarded, and 
the remaining types are extracted to be used as potential 
parallel data. In general, one Chinese sentence corre-
sponds to several English sentences. In (Bing and 
Stephan, 2002), experiments on a 10-year XinHua news 
story collection from the Linguistic Data Consortium 
(LDC) show that alignment types like (2:1) and (3:1) 
are common, and this 7-type alignment is shown to be 
reliable for English-Chinese sentence alignment.  How-
ever, only a small part of the whole 10-year collection 
was pre-aligned (Xiaoyi, 1999) and extracted for sen-
tence alignment.   
The picture can be very different when directly min-
ing the data from Internet. Due to the mismatch between 
the training data and the data collected from Internet, 
the vocabulary coverage can be very low; the data is 
very noisy; and the data aligned is not strictly parallel. 
The percentage of alignment types of insertion (0:1) and 
deletion (1:0) become very high as shown in section 5. 
The aligned sentence pairs are subject to many align-
ment errors. The alignment errors are not desired in the 
re-training of the system, and need to be removed.  
Though the sentence alignment outputs a score from 
Viterbi path for each of the aligned sentence pairs, this 
score is only a rough estimation of the alignment quality. 
A more reliable re-scoring of the data is desirable to 
estimate the alignment quality as a post processing step 
to filter out the errors and noise from the aligned data.  
2.2 Statistical Translation Lexicon 
We use a statistical translation lexicon known as IBM 
Model-1 in (Brown et al, 1993) for both efficiency and 
simplicity.  
In our approach, Model-1 is the conditional probabil-
ity that a word f in the source language is translated 
given word e in the target language, t(f|e). This prob-
ability can be reliably estimated using the expectation-
maximization (EM) algorithm (Cavnar, W. B. and J. M. 
Trenkle, 1994). 
Given training data consisting of parallel sen-
tences: }..1),,{( )()( Sief ii = , our Model-1 training for 
t(f|e) is as follows: 
?
=
?
=
S
s
ss
e efefceft
1
)()(1 ),;|()|( ?  
Where 1?
e? is a normalization factor such that 
0.1)|( =
?
j
j eft  
),;|( )()( ss efefc denotes the expected number of times 
that word e connects to word f.  
??
?
==
=
=
l
i
i
m
j
jl
k
k
ss eeff
eft
eft
efefc
11
1
)()( ),(),(
)|(
)|(),;|( ??  
With the conditional probability t(f|e), the probability 
for an alignment of foreign string F given English string 
E is in (1): 
??
= =
+
=
m
j
n
i
ijm eftlEFP 1 0
)|()1(
1)|(  (1) 
The probability of alignment F given E: )|( EFP is 
shown to achieve the global maximum under this EM 
framework as stated in (Brown et al,1993).  
In our approach, equation (1) is further normalized 
so that the probability for different lengths of F is com-
parable at the word level: 
m
m
j
n
i
ijm eftlEFP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (2) 
 
The alignment models described in (Brown et al, 
1993) are all based on the notion that an alignment 
aligns each source word to exactly one target word.  
This makes this type of alignment models asymmetric.  
Thus by using the conditional probability t(e|f) trans-
lation lexicon trained from English (source) to Chinese 
(target), different aspects of the bilingual lexical 
information can be captured. A similar probability to (2) 
can be defined based on this reverse translation lexicon: 
n
m
i
n
j
jim fetlFEP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (3) 
 
Starting from the Hong Kong news corpora provided 
by LDC, we trained the translation lexicons to be used 
in the parallel sentence alignment.  Each sentence pair 
has a perplexity, which is calculated using the minus log 
of the probability eg. equation (2).  
3 Alignment Models 
The alignment model is aimed at automatically pre-
dicting the alignment scores of a bilingual sentence 
alignment program. By scoring the alignment quality of 
the sentence pairs, we can filter out those mis-aligned 
sentence pairs, and save our SMT system from being 
corrupted by mis-aligned data. 
3.1 Lexicon Based Models 
It is necessary to include lexical features in the 
aligned quality evaluation. One way is to use the trans-
lation lexicon based perplexity as in our sentence 
alignment program.  
For each of the aligned sentence pairs, the sentence 
alignment generated a score, which is solely based on 
equation (2). Using this score only, we can do a simple 
filtering by setting a threshold of perplexity. The sen-
tence pairs which have a higher perplexity than the 
threshold will be removed. However the perplexity 
based on (2) is definitely not discriminative enough to 
evaluate the quality of aligned sentence pairs.  
In our experiment, it showed that perplexity (3) has 
more discriminative power in judging the quality of the 
aligned sentence pairs for Chinese-English sentence 
alignment. It is also possible that equation (2) is more 
suitable for other language pairs.  Both (2) and (3) are 
applied in our sentence alignment quality judgment, 
which is to be explained in section 4.  
3.2 Sentence Length Models 
As was shown in the sentence alignment literature 
(Church, K.W. 1993), the sentence length ratio is also a 
very good indication of the alignment of a sentence pair 
for languages from a similar family such as French and 
English.  For language pairs from very different families 
such as Chinese and English, the sentence length ratio is 
also a good indication of alignment quality as shown in 
our experiments.  
For the language pair of Chinese and English, the 
sentence length can be defined in several different ways.  
3.2.1 Sentence Length 
In general, a Chinese sentence does not have word 
boundary information; so one way to define Chinese 
sentence length is to count the number of bytes of the 
sentence. Another way is to first segment the Chinese 
sentence into words (section 3.2.2) and count how many 
words are in the sentence. For English sentences, we 
can similarly define the length in bytes and in words.  
The length ratio is assumed to be a Gaussian distri-
bution. The mean and variance are calculated from the 
parallel training corpus, which, in our case, is the Hong 
Kong parallel corpus with 290K parallel sentence pairs.  
3.2.2 A Chinese Word Segmenter 
The word segmenter for Chinese is to parse the Chi-
nese string into words. Different word segmenters can 
generate different numbers of words for the same Chi-
nese sentence.  
There are many word segmenters publicly available. 
In our experiments, we applied a two-pass strategy to 
segment the word according to the dictionary of the 
LDC bilingual dictionary of Chinese-English. The two-
pass started first from left to right, and then from right 
back to left, to calculate the maximum word frequency 
and select one best path to segment the words.  
In general, the sentence length is not sensitive to the 
segmenters used. But for reliability, we want each seg-
mented word can have an English translation, thus we 
used the LDC bilingual dictionary as a reference word 
list for segmentation.  
3.2.3 Sentence Length Model 
Assume the alignment probability of ),|( tsAP  is 
only related to the length of source sentence s and target 
sentence t: 
|))||,(|(~
|)||(|~
||)||,|||||(|~
),||||(|),|(
tsP
tsP
tstsP
tstsPtsAP
?=
?=
?=
?=
 
where || s and || t are the sentence lengths of s and t.  
The difference of the length |)||,(| ts? is assumed 
to be a Gaussian distribution (Church, K.W. 1993) and 
can be normalized as follows: 
)1,0(~
)1|(|
||||
2
N
s
cst
?
?
+
?
=  (4) 
where c is a constant indicating the mean length ratios 
between source and target sentences and 2? is the vari-
ance of the length ratios.  
In our case, we applied three length models de-
scribed in the following Table 1: 
 
Table 1. Three Length Models description 
L-1 Both English and Chinese sentence are meas-
ured in bytes 
L-2 Both English and Chinese sentence are meas-
ured in words 
L-3 English sentence is measured in words and 
Chinese sentence is measured in bytes 
 
The means and 2? of the length ratios for each of the 
length models are calculated from Hong Kong news 
parallel corpus. The statistics of the three sentence 
length models are shown in Table 2. 
 
Table 2. Sentence length ratio statistics 
  L-1  L-2 L-3: 
Mean 1.59 1.01 0.33 
Var 3.82 0.79 0.71 
 
In general, the smaller the variance, the better the 
sentence length model can be. From Table 2 we observe 
that the bytes based length ratio model has significantly 
larger variance (3.82) than the other two models (L-2: 
0.79, L-3: 0.71).  This means L1 is not as reliable as L2 
and L3. Both L2 and L3 have similar variance, which 
indicates measuring English sentences in words will 
entail smaller variance in length model; measuring Chi-
nese sentences in bytes or words entails only a slight 
difference in variance. This also indicates that the length 
model is not so sensitive to the Chinese word segmenter 
applied. L-1, L-2 and L-3 capture the length relationship 
of parallel sentence in different views. Their modeling 
power has overlap, but they also compensate each other 
in capturing the parallel characteristics of good transla-
tion quality. A combination of these models can poten-
tially bring further improvement, which is shown in our 
experiment in section 6.  
4 Regression Model 
Rather than doing a binary decision (classification) that 
the aligned sentence pair is either good or not, the re-
gression can give a confidence score indicating how 
good the alignment can be, thus offering more flexibil-
ity in decisions.   Predicting the alignment quality using 
the candidate models is considered as a regression prob-
lem in that different scores are combined together.   
There are many ways such as genetic programming, 
to combine the candidate models, and regression is one 
of the straight forward and efficient ones.  So in this 
work, we explored linear regression. 
4.1 Candidate Models 
We have five candidate models described in section 
3. They are: PP1, the perplexity based on the word pair 
conditional probability p(f|e) in equation (2); PP2, the 
perplexity based on the reverse word pair conditional 
probability p(e|f) in equation (3); L-1, Length ratio 
model measured in bytes (mean=1.59, var=3.82); L-2, 
length ratio model measured in words (mean=1.01, 
var=0.79); L-3, length ratio model, where the English 
sentence is measured in words and the Chinese sentence 
is measured in bytes (mean=0.33, var=0.71).  These five 
models capture different aspects of the aligned quality 
of the sentence pair. The idea is to combine these five 
models together to get better prediction of the aligned 
quality. 
Linear regression is applied to combine these five 
models. It is trained from the observation of the five 
models together with the label of human judgment on a 
training set. 
4.2 Regression Model Training 
The linear regression model tries to discover the 
equation for a line that most nearly fits the given data 
(Trevor Hastie et al 2001). That linear equation is then 
used to predict values for the data.  
Now given human subject judgment of the aligned 
translation quality of sentence pairs, we can train a re-
gression model based on the five models we described 
in section 4.1 under the objective of least square errors.  
The human evaluation is measures translation qual-
ity of aligned pairs on a discrete 6-point scale between 1 
(very bad) and 5 (perfect translation). The score 0 was 
used for alignments that were not genuine translation 
e.g., both sentences were from the same language. We 
will use n for the number of total sentence pairs labeled 
by humans and used in training.  
Let A= [PP1, PP2, L-1, L-2, L-3] be the machine-
generated scores for each of the sentence pairs. In our 
case, A is a 5?n  matrix.  
Let H= [Human-Judgment-Score] be the human 
evaluation of the sentence pairs on a 6-point scale. In 
our case, H is a 1?n  matrix. 
In linear regression modeling, a linear transforma-
tion matrix W should satisfy the least square error crite-
rion: 
||}{||min* HAWW
w
?=  (5) 
where W is in fact a 5x1 weight matrix. The equation 
can be solved as:  
HAAAW TT 1* )( ?=  (6) 
The inverse of matrix AAT  is usually calculated using 
singular vector decomposition (SVD). After W is calcu-
lated, the predicted score from the regression model is: 
*
' AWH =  (7) 
where 'H  is the final predicted alignment quality score 
of the regression model. We can also view 'H  as a 
weighted sum of the five models shown in section 4.1. 
The calculation of 'H  reduces to a linear weighted 
summation, which is very efficient to compute.  
5 Experiments 
1500 pairs of comparable html document pairs were 
obtained from bilingual web pages crawled from Inter-
net. After preprocessing, filtering, and sentence align-
ment, the alignment types were distributed as shown in 
Table 3. Ignoring the alignment type of insertion (0:1) 
and deletion (1:0), we extracted around 5941 parallel 
sentences.  
 
Table 3. Alignment types? distribution of mined 
data from noisy web data crawled 
 1:0 0:1 1:1 2:1 1:2 2:2 3:1 
% 23.7 41.9 29.4 1.99 0.01 0.02 2.79 
 
From Table 3, we see the data is very noisy, con-
taining a large portion of insertions (23.7%) and dele-
tions (41.9%).  This is very different from the LDC 
XinHua pre-aligned collection provided by LDC, which 
is relatively clean.  
For this set of English-Chinese bilingual sentences, 
we randomly selected 200 sentence pairs, focusing on 
Viterbi alignment scores below 12.0 from sentence 
alignment, which was an empirically determined 
threshold (The alignment scores here were purely re-
flecting the Model-1 parameters using equation (2)).  
Three human subjects then had to score the 'translation 
quality' of every sentence pair, using a 6 point scale 
described in section 4.2. We further excluded very short 
sentences from consideration and evaluated 168 remain-
ing sentences. 
Pearson R correlation is applied to calculate the mag-
nitude of the association between two variables (human-
human or human-machine in our case) that are on an 
interval or ratio scale. The correlation coefficients 
(Pearson R) between human subjects were in Table 4 
(all are statistically significant): 
 
Table 4. Correlation between Human Subjects 
 H2 H3 
H1 0.786 0.615 
H2 ---- 0.568 
 
Overall, more than 2/3 of the human scores are identical 
or differ by only 1 (between subjects). 
For the automatic score prediction, the five compo-
nent scores described in section 4.1 are used, which are 
then combined using a standard Linear Regression as 
described in section 4.2. Table 5 shows the correlation 
between alignment scores based on Model X and human 
subjects' predicted quality scores: 
 
Table 5. Correlation between optimization models 
and human subjects 
Model human-1 human -2 human -3 
PP-1 .57 .53 .32 
PP-2 .60 .58 .46 
L-1 .42 .41 .30 
L-2 .46 .41 .40 
L-3 .40 .38 .29 
Na?ve .58 .56 .38 
Regression  .72 .68 .53 
 
The data we used in our training of the lexicon is Hong 
Kong news parallel data from LDC. There are 290K 
parallel sentence pairs, with 7 million words of English 
and 7.3 million Chinese words after segmentation. The 
IBM Model-1 for PP-1 and PP-2 are both trained using 
5 EM iterations. The other three length models are also 
calculated from the same 290K sentence pairs. Punctua-
tion is removed before the calculation of all automatic 
score prediction models. 
The regression model here is the standard linear re-
gression using the observations from three human sub-
jects as described in section 4.1. The average 
performance of the regression model is shown in the 
bottom line of the above Table 5. The average correla-
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Average of 3-human judgement 5-scale score (5-best, 0-non)
Pearson r Correlation
tion varies from 0.53 upto 0.72, which shows that the 
regression model has a very strong positive correlation 
with the human judgment.  
Also from Table 5, we see both lexicon based mod-
els: PP-1 and PP-2 are better than the length models in 
term of correlation with human scorer. Model PP-2 has 
the largest correlation, and is slightly better than PP-1. 
PP-2 is based on the conditional probability of p(e|f), 
which models the generation of an English word from a 
Chinese word. The vocabulary size of Chinese is usu-
ally smaller than English vocabulary size, so this model 
can be more reliably estimated than the reverse direction 
of p(f|e). This explains why PP-2 is slightly better than 
PP-1.  
For sentence length models, we see L-2, for which 
the lengths of both the English sentence and the Chinese 
sentence are measured in words, has the best perform-
ance among the three settings of a sentence length 
model. This indicates that the length model measured in 
words is more reliable.  
Also shown in Table 5, the na?ve interpolation of 
these different models, i.e. just using each model with 
equal weight, resulted in lower correlation than the best 
single alignment model. 
We also performed correlation experiments with 
varied numbers of training sentences from either Hu-
man-1/Human-2/Human-3 or from all of the three hu-
man subjects.  We picked the first 30/60/90/120 labeled 
sentence pairs for training and saved the last 48 sen-
tence pairs for testing.  The average performance of the 
regression model is as follows: 
 
Table 6. Correlation between different training set 
sizes and human scorers. 
Training  
set size 
Human-1 Human -2 Human ?3 
30 .686 .639 .447 
60 .750 .707 .452 
90 .765 .721 .456 
120 .760 .721 .464 
 
The average correlation of the regression models 
showed here increased noticeably when the training set 
was increased from 30 sentence pairs to 90 sentence 
pairs. More sentence pairs caused no or only marginal 
improvements (esp. for the third human subject).  
Figure 1 shows a scatter plot, which illustrates a 
good correlation (here: Pearson R=0.74) between our 
regression model predictors and the human scorers. 
6 Conclusion 
In this paper, we have demonstrated ways to effi-
ciently optimize a sentence alignment module, such that 
it is able to select aligned sentence pairs of high transla-
tion quality automatically. This procedure of alignment 
score optimization requires (a) a small number of hu-
man subjects who annotate a set of about 100 sentence 
pairs each for translation quality; and (b) a set of align-
ment scores, based on perplexity and sentence length 
ratio, to be able to learn to predict the human scores. 
Based on the learned predictions, by means of linear 
regression, the alignment program can choose the best  
sentence pair candidates to be included in the training 
data for the SMT system re-estimation. 
Our experiments showed that, for Chinese-English 
language pair, perplexity based on the reverse word pair 
conditional probability p(e|f) (PP-2) gives the most reli-
able prediction among the five models proposed in this 
paper; the regression model, which combines those five 
models, give the best correlation between human score 
and automatic predictions. Our approach needs only a 
fairly limited number of human labeled sentences pairs, 
and is an efficient optimization of the sentence 
alignment system. 
 
Figure 1. Correlation between regression model and 
human scorers, Pearson R=0.74. 
 
References 
Bing Zhao, Stephan Vogel. 2002. Adaptive Parallel 
Sentences Mining from Web Bilingual News Collec-
tion. IEEE International Conference on Data Mining 
(ICDM 02) , pp. 745-748. Japan. 
Brown, P., Lai, J. C., and Mercer, R. 1991. Aligning 
Sentences in Parallel Corpora. In Proceedings of 
ACL-91, Berkeley CA. 1991 
Cavnar, W. B. and J. M. Trenkle. 1994.  N-Gram-Based 
Text Categorization.  Proceedings of Third Annual 
Symposium on Document Analysis and Information 
Retrieval, Las Vegas, NV, UNLV Publica-
tions/Reprographics, pp. 161-175, 11-13. 
Stanley Chen. 1993. Aligning sentences in Bilingual 
corpora using lexical information. In proceedings of 
the 31st Annual Conference of the Association for 
computational linguistics, pages 9-16, Columbus, 
Ohio, June 1993 
Church, K. W. 1993. Char_align: A Program for Align-
ing Parallel Texts at the Character Level. Proceed-
ings of ACL-93, Columbus OH. 
Gale, W. A. and Church, K. W.  1991. A Program for 
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-91, Berkeley CA. 1991. 
Melamed, I.D. 1996. A Geometric Approach to Map-
ping Bitext Correspondence. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA. 1996 
Noord?s Implementation of Textcat: http://odur.let. 
rug.nl/~vannoord/TextCat/index.html 
Peter F. Brown, Stephan A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer.  1993. The 
Mathmatics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, vol 
19, no.2 , pp.263-311. 
Philip Resnik. 1999. Mining the Web for Bilingual Text. 
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL'99), University of Maryland, 
College Park, Maryland. 
Trevor Hastie, Robert Tibshirani, Jerome Friedman. 
2001. The Elements of Statistical Learning: Data 
Mining, Inference and Prediction. Springer Publisher. 
Xiaoyi Ma, Mark Y. Liberman, ?BITS: A Method for 
Bilingual Text Search over the Web?. Machine 
Translation Summit VII, 1999 
 
Phrase Pair Rescoring with Term Weightings for  
Statistical Machine Translation 
Bing Zhao   Stephan Vogel   Alex Waibel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, vogel+, ahw}@cs.cmu.edu 
 
Abstract 
We propose to score phrase translation 
pairs for statistical machine translation 
using term weight based models.  These 
models employ tf.idf to encode the 
weights of content and non-content words 
in phrase translation pairs.  The transla-
tion probability is then modeled by simi-
larity functions defined in a vector space.  
Two similarity functions are compared.  
Using these models in a statistical ma-
chine translation task shows significant 
improvements. 
1 Introduction 
Words can be classified as content and func-
tional words.  Content words like verbs and 
proper nouns are more informative than function 
words like "to'' and "the''.  In machine translation, 
intuitively, the informative content words should 
be emphasized more for better adequacy of the 
translation quality.  However, the standard statis-
tical translation approach does not take account 
how informative and thereby, how important a 
word is, in its translation model.  One reason is 
the difficulty to measure how informative a word 
is. Another problem is to integrate it naturally 
into the existing statistical machine translation 
framework, which typically is built on word 
alignment models, like the well-known IBM 
alignment models (Brown et al1993).  
In recent years there has been a strong ten-
dency to incorporate phrasal translation into sta-
tistical machine translation.  It directly translates 
an n-gram from the source language into an m-
gram in the target language.  The advantages are 
obvious:  It has built-in local context modeling, 
and provides reliable local word reordering.  It 
has multi-word translations, and models a word?s 
conditional fertility given a local context.  It cap-
tures idiomatic phrase translations and can be 
easily enriched with bilingual dictionaries. In 
addition, it can compensate for the segmentation 
errors made during preprocessing, i.e. word seg-
mentation errors of Chinese.  The advantage of 
using phrase-based translation in a statistical 
framework has been shown in many studies such 
as (Koehn et al 2003; Vogel et al 2003; Zens et 
al. 2002; Marcu and Wong, 2002).  However, the 
phrase translation pairs are typically extracted 
from a parallel corpus based on the Viterbi align-
ment of some word alignment models.  The leads 
to the question what probability should be as-
signed to those phrase translations.  Different 
approaches have been suggested as using relative 
frequencies (Zens et al 2002), calculate prob-
abilities based on a statistical word-to-word dic-
tionary (Vogel et al 2003) or use a linear 
interpolation of these scores (Koehn et al 2003).  
In this paper we investigate a different ap-
proach with takes the information content of 
words better into account.  Term weighting based 
vector models are proposed to encode the transla-
tion quality.  The advantage is that term weights, 
such as tf.idf, are useful to model the informa-
tiveness of words.  Highly informative content 
words usually have high tf.idf scores.  In informa-
tion retrieval this has been successfully applied to 
capture the relevance of a document to a query, 
by representing both query and documents as 
term weight vectors and use for example the 
cosine distance to calculate the similarity be-
tween query vector and document vector.  The 
idea now is to consider the source phrase as a 
?query?, and the different target phrases ex-
tracted from the bilingual corpus as translation 
candidates as a relevant ?documents?.  The co-
sine distance is then a natural choice to model the 
translation probability.  
Our approach is to apply term weighting 
schemes to transform source and target phrases 
into term vectors.  Usually content words in both 
source and target languages will be emphasized 
by large term weights.  Thus, good phrase trans-
lation pairs will share similar contours, or, to ex-
press it in a different way, will be close to each 
other in the term weight vector space.  A similar-
ity function is then defined to approximate trans-
lation probability in the vector space. 
The paper is structured as follows:  in Section 
2, our phrase-based statistical machine translation 
system is introduced; in Section 3, a phrase trans-
lation score function based on word translation 
probabilities is explained, as this will be used as a 
baseline system;  in Section 4, a vector model 
based on tf.idf is proposed together with two 
similarity functions;  in Section 5, length regu-
larization and smoothing schemes are explained 
briefly;  in Section 6, the translation experiments 
are presented; and Section 7 concludes with a 
discussion.  
2 Phrase-based Machine Translation 
In this section, the phrase-based machine transla-
tion system used in the experiments is briefly 
described: the phrase based translation models 
and the decoding algorithm, which allows for 
local word reordering.  
2.1 Translation Model 
The phrase-based statistical translation systems 
use not only word-to-word translation, extracted 
from bilingual data, but also phrase-to phrase 
translations. . Different types of extraction ap-
proaches have been described in the literature: 
syntax-based, word-alignment-based, and genu-
ine phrase alignment models.  The syntax-based 
approach has the advantage to model the gram-
mar structures using models of more or less 
structural richness, such as the syntax-based 
alignment model in (Yamada and Knight, 2001) 
or the Bilingual Bracketing in (Wu, 1997).  Popu-
lar word-alignment-based approaches usually 
rely on initial word alignments from the IBM and 
HMM alignment models (Och and Ney, 2000), 
from which the phrase pairs are then extracted.  
(Marcu and Wong 2002) and (Zhang et al 2003) 
do not rely on word alignment but model directly 
the phrase alignment. 
Because all statistical machine translation sys-
tems search for a globally optimal translation 
using the language and translation model, a trans-
lation probability has to be assigned to each 
phrase translation pair.  This score should be 
meaningful in that better translations have a 
higher probability assigned to them, and balanced 
with respect to word translations.  Bad phrase 
translations should not win over better word for 
word translations, only because they are phrases. 
Our focus here is not phrase extraction, but 
how to estimate a reasonable probability (or 
score) to better represent the translation quality 
of the extracted phrase pairs.  One major problem 
is that most phrase pairs are seen only several 
times, even in a very large corpus.  A reliable and 
effective estimation approach is explained in sec-
tion 3, and the proposed models are introduced in 
section 4.   
In our system, a collection of phrase transla-
tions is called a transducer.  Different phrase ex-
traction methods result in different transducers.  
A manual dictionary can be added to the system 
as just another transducer.  Typically, one source 
phrase is aligned with several candidate target 
phrases, with a score attached to each candidate 
representing the translation quality.  
2.2 Decoding Algorithm 
Given a set of transducers as the translation 
model (i.e. phrase translation pairs together with 
the scores of their translation quality), decoding 
is divided into several steps. 
The first step is to build a lattice by applying 
the transducers to the input source sentence.  We 
start from a lattice, which has as its only path the 
source sentence.  Then for each word or sequence 
of words in the source sentence for which we 
have an entry in the transducer new edges are 
generated and inserted into the lattice, spanning 
over the source phrase.  One new edge is created 
for each translation candidate, and the translation 
score is assigned to this edge.  The resulting lat-
tice has then all the information available from 
the translation model. 
The second step is search for a best path 
through this lattice, but not only based on the 
translation model scores but applying also the 
language model.  We start with an initial special 
sentence begin hypothesis at the first node in the 
lattice.  Hypotheses are then expanded over the 
edges, applying the language model to the partial 
translations attached to the edges.  The following 
algorithm summarizes the decoding process 
when not considering word reordering:  
 
Current node n, previous node n?; edge e 
Language model state L, L? 
Hypothesis h, h? 
Foreach node n in the lattice 
  Foreach incoming edge e in n 
      phrase = word sequence at e 
      n?     = FromNode(e) 
      foreach L in n? 
         foreach h with LMstate L 
           LMcost = 0.0 
           foreach word w in phrase 
             LMcost += -log p(w|L) 
             L? = NewState(L,w) 
             L  = L? 
           end 
           Cost= LMcost+TMcost(e) 
           TotalCost=TotalCost(h)+Cost 
          h? = (L,e,h,TotalCost) 
          store h?in Hypotheses(n,L) 
 
The updated hypothesis h? at the current node 
stores the pointer to the previous hypothesis and 
the edge (labeled with the target phrase) over 
which it was expanded.  Thus, at the final step, 
one can trace back to get the path associated with 
the minimum cost, i.e. the best hypothesis. 
Other operators such as local word reordering 
are incorporated into this dynamic programming 
search (Vogel, 2003). 
3 Phrase Pair Translation Probability  
As stated in the previous section, one of the 
major problems is how to assign a reasonable 
probability for the extracted phrase pair to repre-
sent the translation quality. 
Most of the phrase pairs are seen only once or 
twice in the training data.  This is especially true 
for longer phrases.  Therefore, phrase pair co-
occurrence counts collected from the training 
corpus are not reliable and have little discrimina-
tive power.  In (Vogel et al 2003) a different es-
timation approach was proposed.  Similar as in 
the IBM models, it is assumed that each source 
word si in the source phrase ),,( 21 Issss L
v =  is 
aligned to every target word tj in the target phrase 
),,( 21 Jtttt L
v =  with probability )|Pr( ij st .  The 
total phrase translation probability is then calcu-
lated according to the following generative 
model:  
? ?
= =
=
I
i
J
j
ji tsts
1 1
))|Pr(()|Pr(
vv  (1) 
 
This is essentially the lexical probability as 
calculated in the IBM1 alignment model, without 
considering position alignment probabilities.  
Any statistical translation can be used in (1) to 
calculate the phrase translation probability.  
However, in our experiment we typically see now 
significant difference in translation results when 
using lexicons trained from different alignment 
models. 
Also Equation (1) was confirmed to be robust 
and effective in parallel sentence mining from a 
very large and noisy comparable corpus (Zhao 
and Vogel, 2002).  
Equation (1) does not explicitly discriminate 
content words from non-content words.  As non-
content words such as high frequency functional 
words tend to occur in nearly every parallel sen-
tence pair, they co-occur with most of the source 
words in the vocabulary with non-trivial transla-
tion probabilities.  This noise propagates via (1) 
into the phrase translations probabilities, increas-
ing the chance that non-optimal phrase transla-
tion candidates get high probabilities and better 
translations are often not in the top ranks.  
We propose a vector model to better distin-
guish between content words and non-content 
words with the goal to emphasize content words 
in the translation.  This model will be used to 
rescore the phrase translation pairs, and to get a 
normalized score representing the translation 
probability.  
4 Vector Model for Phrase Translation 
Probability 
Term weighting models such as tf.idf are ap-
plied successfully in information retrieval.  The 
duality of term frequency (tf) and inverse docu-
ment frequency (idf), document space and collec-
tion space respectively, can smoothly predict the 
probability of terms being informative (Roelleke, 
2003).  Naturally, tf.idf is suitable to model con-
tent words as these words in general have large 
tf.idf weights. 
4.1 Phrase Pair as Bag-of-Words 
Our translation model: (transducer, as defined 
in 2.1), is a collection of phrase translation pairs 
together with scores representing the translation 
quality.  Each phrase translation pair, which can 
be represented as a triple },{ pts vv? , is now con-
verted into a ?Bag-of-Words? D consisting of a 
collection of both source and target words ap-
pearing in the phrase pair, as shown in (2):  
},,,,,{},{ 2121 JI tttsssDpts LL
vv =??  (2) 
 
Given each phrase pair as one document, the 
whole transducer is a collection of such docu-
ments.  We can calculate tf.idf for each is  and jt , 
and represent source and target phrases by vec-
tors of sv
v  and tv
v   as in Equation (3):  
},,,{
21 Issss
wwwv Lv =  
},,,{
21 Jtttt
wwwv Lv =  (3) 
where
is
w and
jt
w are tf.idf for is or jt respectively.  
This vector representation can be justified by 
word co-occurrence considerations.  As the 
phrase translation pairs are extracted from paral-
lel sentences, the source words is  and target 
words jt  in the source and target phrases must 
co-occur in the training data.  The co-occurring 
words should share similar term frequency and 
document frequency statistics.  Therefore, the 
vectors  sv
v and tv
v  have similar term weight con-
tours corresponding to the co-occurring word 
pairs.  So the vector representations of a phrase 
translation pair can reflect the translation quality.  
In addition, the content words and non-content 
words are modeled explicitly by using term 
weights.  An over-simplified example would be 
that a rare word in the source language usually 
translates into a rare word in the target language. 
4.2 Term Weighting Schemes 
Given the transducer, it is straightforward to 
calculate term weights for source and target 
words.  There are several versions of tf.idf.  The 
smooth ones are preferred, because phrase trans-
lation pairs are rare events collected from train-
ing data.  
The idf model selected is as in Equation (4): 
)
5.0
5.0log( +
+?=
df
dfNidf  (4)
where N is the total number of documents in the 
transducer, i.e. the total number of translation 
pairs, and df is the document frequency, i.e. in 
how many phrase pairs a given word occurs.  The 
constant of 0.5 acts as smoothing. 
Because most of the phrases are short, such as 
2 to 8 words, the term frequency in the bag of 
words representation is usually 1, and some times 
2.  This, in general, does not bring much dis-
crimination in representing translation quality.  
The following version of tf is chosen, so that 
longer target phrases with more words than aver-
age will be slightly down-weighted: 
)(/)(5.15.0
'
vavglenvlentf
tftf vv?++=  (5)
where tf is the term frequency, )(vlen v  is the 
length in words of the phrase vv , and )(vavglen v  is 
the average length of source or target phrase cal-
culated from the transducer.  Again, the values of 
0.5 and 1.5 are constants used in IR tasks acting 
as smoothing.  
Thus after a transducer is extracted from a par-
allel corpus, tf and df are counted from the collec-
tion of the ?bag-of-words'' phrase alignment 
representations.  For each word in the phrase pair 
translation its tf.idf weight is assigned and the 
source and target phrase are transformed into 
vectors as shown in Equation (3).  These vectors 
reserve the translation quality information and 
also model the content and non-content words by 
the term weighting model of tf.idf. 
4.3 Vector Space Alignment 
Given the vector representations in Equation 
(3), a similarity between the two vectors can not 
directly be calculated.  The dimensions I and J 
are not guaranteed to be the same.  The goal is to 
transform the source vector into a vector having 
the same dimensions as the target vector, i.e. to 
map the source vector into the space of the target 
vector, so that a similarity distance can be calcu-
lated.  Using the same reasoning as used to moti-
vate Equation (1), it is assumed that every source 
word is  contributes some probability mass to 
each target word jt .  That is to say, given a term 
weight for jt , all source term weights are aligned 
to it with some probability.  So we can calculate 
a transformed vector from the source vectors by 
calculating weights jtaw  using a translation lexi-
con )|Pr( st  as in Equation (6): 
?
=
?=
I
i
sij
t
a i
j wstw
1
)|Pr(  (6) 
 
Now the target vector and the mapped vector 
av
v  have the same dimensions as shown in (7):  
},,,{ 21 Jta
t
a
t
aa wwwv L
v =  
},,,{
21 Jtttt
wwwv Lv =  (7) 
 
4.4 Similarity Functions 
As explained in section 4.1, intuitively, if sv  
and t
v  is a good translation pair, then the corre-
sponding vectors of av
v  and tv
v  should be similar 
to each other in the vector space.   
Cosine distance 
The standard cosine distance is defined as the 
inner product of the two vectors av
v  and tv
v  nor-
malized by their norms.  Based on Equation (6), 
it is easy to derive the similarity as follows:  
)()(
)|(
)|(1
1),(),(
1
2
1
2
1 1
1 1
1
cos
??
? ?
? ?
?
==
= =
= =
=
=
=
==
J
j
t
a
J
j
t
J
j
I
i
sijt
J
j
I
i
sijt
t
t
a
J
j
t
t
a
t
t
at
t
a
t
t
a
t
t
a
j
j
ij
ij
j
j
wsqrtwsqrt
wstPw
wstPw
vv
ww
vvvv
vvvvd
 
(8)
where I and J are the length of the source and 
target phrases; 
is
w  and 
jt
w  are term weights for 
source word and target words;  jtaw  is the trans-
formed weight mapped from all source words to 
the target dimension at word jt .   
BM25 distance 
TREC tests show that bm25 (Robertson and 
Walker, 1997) is one of the best-known distance 
schemes.  This distance metric is given in Equa-
tion (9). The constants of 31 ,, kbk are set to be 1, 1 
and 1000 respectively.  
)(
)1(
)(
)1(
3
3
1
1
25 j
j
j
j
t
a
t
a
J
j t
t
bm wk
wk
wK
wk
wd +
+
+
+=?
=
 
)5.0/()5.0( ++?==
jjj ttt
dfdfNidfw  
))(/)1((1 lavgJbkK +?=  
(9)
where avg(l) is the average target phrase length 
in words given the same source phrase. 
Our experiments confirmed the bm25 distance 
is slightly better than the cosine distance, though 
the difference is not really significant.  One ad-
vantage of bm25 distance is that the set of free 
parameters 31 ,, kbk can be tuned to get better per-
formance e.g. via n-fold cross validation.  
4.5 Integrated Translation Score 
Our goal is to rescore the phrase translation 
pairs by using additional evidence of the transla-
tion quality in the vector space.   
The vector based scores (8) & (9) provide a 
distinct view of the translation quality in the vec-
tor space.  Equation (1) provides a evidence of 
the translation quality based on the word align-
ment probability, and can be assumed to be dif-
ferent from the evidences in vector space.  Thus, 
a natural way of integrating them together is a 
geometric interpolation shown in (10) or equiva-
lently a linear interpolation in the log domain.  
)|(Pr),( 1int tsstdd vec
vvvv ?? ??=  (10)
where ),( stdvec
vv is the score from the cosine or 
bm25 vector distance, normalized within [0, 1], 
like a probability. 
0.1),( =?
t
vec stdv
vv   
The parameter ? can be tuned using held-out 
data.  In our cross validation experiments 5.0=?  
gave the best performance in most cases.  There-
fore, Equation (10) can be simplified into: 
)|Pr(),(int tsstdd vec
vvvv ?=  (11)
 
The phrase translation score functions in (1) 
and (11) are non-symmetric.  This is because the 
statistical lexicon Pr(s|t) is non-symmetric.  One 
can easily re-write all the distances by using 
Pr(t|s).  But in our experiments this reverse di-
rection of using Pr(t|s) gives trivially difference.  
So in all the experimental results reported in this 
paper, the distances defined in (1) and (11) are 
used. 
5 Length Regularization  
Phrase pair extraction does not work perfectly 
and sometimes a short source phrase is aligned to 
a long target phrase or vice versa.  Length regu-
larization can be applied to penalize too long or 
too short candidate translations.  Similar to the 
sentence alignment work in (Gale and Church, 
1991), the phrase length ratio is assumed to be a 
Gaussian distribution as given in Equation (12):  
)))(/)((5.0exp(),( 2
2
?
????? sltlstl
vvvv  (12)
where l(t) is the target sentence  length.  Mean ?  
and variance ?  can be estimated using a parallel 
corpus using a Maximum Likelihood criteria. 
The regularized score is the product of (11) and 
(12).  
6 Experiments  
Experiments were carried out on the so-called 
large data track Chinese-English TIDES transla-
tion task, using the June 2002 test data.  The 
training data used to train the statistical lexicon 
and to extract the phrase translation pairs was 
selected from a 120 million word parallel corpus 
in such a way as to cover the phrases in test sen-
tences.  The restricted training corpus contained 
then approximately 10 million words..  A trigram 
model was built on 20 million words of general 
newswire text, using the SRILM toolkit (Stolcke, 
2002).  Decoding was carried out as described in 
section 2.2.  The test data consists of 878 Chinese 
sentences or 24,337 words after word segmenta-
tion.  There are four human translations per Chi-
nese sentence as references.  Both NIST score 
and Bleu score (in percentage) are reported for 
adequacy and fluency aspects of the translation 
quality. 
6.1 Transducers 
Four transducers were used in our experi-
ments: LDC, BiBr, HMM, and ISA.  
LDC was built from the LDC Chinese-English 
dictionary in two steps: first, morphological 
variations are created.  For nouns and noun 
phrases plural forms and entries with definite and 
indefinite determiners were generated.  For verbs 
additional word forms with -s -ed and -ing were 
generated, and the infinitive form with 'to'.  Sec-
ond, a large monolingual English corpus was 
used to filter out the new word forms.  If they did 
not appear in the corpus, the new entries were not 
added to the transducer (Vogel, 2004). 
BiBr extracts sub-tree mappings from Bilin-
gual Bracketing alignments (Wu, 1997);  HMM 
extracts partial path mappings from the Viterbi 
path in the Hidden Markov Model alignments 
(Vogel et. al., 1996).  ISA is an integrated seg-
mentation and alignment for phrases (Zhang et.al, 
2003), which is an extension of (Marcu and 
Wong, 2002).  
 LDC BiBr HMM ISA 
)(KN  425K 137K 349K 263K 
)/( srctgt llavg  1.80 1.11 1.09 1.20 
Table-1 statistics of transducers 
 
Table-1 shows some statistics of the four 
transducers extracted for the translation task. N  
is the total number of phrase pairs in the trans-
ducer.  LDC is the largest one having 425K en-
tries, as the other transducers are restricted to 
?useful? entries, i.e. those translation pairs where 
the source phrase matches a sequence of words in 
one of the test sentence.  Notice that the LDC 
dictionary has a large number of long transla-
tions, leading to a high source to target length 
ratio. 
6.2 Cosine vs BM25 
The normalized cosine and bm25 distances de-
fined in (8) and (9) respectively, are plugged into 
(11) to calculate the translation probabilities.  
Initial experiments are reported on the LDC 
transducer, which gives already a good transla-
tion, and therefore allows for fast and yet mean-
ingful experimentation.  
Four baselines (Uniform, Base-m1, Base-m4, 
and Base-m4S) are presented in Table-2.   
 
NIST Bleu 
Uniform 6.69 13.82 
Base-m1 7.08 14.84 
Base-m4 7.04 14.91 
Base-m4S 6.91 14.44 
cosine 7.17 15.30 
bm25 7.19 15.51 
bm25-len 7.21 15.64 
Table-2 Comparisons of different score functions 
 
In the first uniform probabilities are assigned 
to each phrase pair in the transducer.  The second 
one (Base-m1) is using Equation (1) with a statis-
tical lexicon trained using IBM Model-1, and 
Base-m4 is using the lexicon from IBM Model-4.  
Base-m4S is using IBM Model-4, but we skipped 
194 high frequency English stop words in the 
calculation of Equation (1). 
Table-2 shows that the translation score de-
fined by Equation (1) is much better than a uni-
form model, as expected.  Base-m4 is slightly 
worse than Base-m1.on NIST score, but slightly 
better using the Bleu metric.  Both differences 
are not statistically significant.  The result for 
Base-m4S shows that skipping English stop 
words in Equation (1) gives a disadvantage.  One 
reason is that skipping ignores too much non-
trivial statistics from parallel corpus especially 
for short phrases.  These high frequency words 
actually account already for more than 40% of 
the tokens in the corpus.  
Using the vector model, both with the cosine 
cosd  and the bm25 25bmd  distance, is significantly 
better than Base-m1 and Base-m4 models, which 
confirms our intuition of the vector model as an 
additional useful evidence for translation quality. 
The length regularization (12) helps only slightly 
for LDC.  Since bm25?s parameters could be 
tuned for potentially better performance, we se-
lected bm25 with length regularization as the 
model tested in further experiments.  
A full-loaded system is tested using the 
LM020 with and without word-reordering in de-
coding.  The results are presented in Table-3.  
Table-3 shows consistent improvements on all 
configurations: the individual transducers, com-
binations of transducers, and different decoder 
settings of word-reordering. Because each phrase 
pair is treated as a ?bag-of-words?, the grammar 
structure is not well represented in the vector 
model.  Thus our model is more tuned towards 
the adequacy aspect, corresponding to NIST 
score improvement. 
Because the transducers of BiBr, HMM, and 
ISA are extracted from the same training data, 
they have significant overlaps with each other.  
This is why we observe only small improvements 
when adding more transducers.   
The final NIST score of the full system is 8.24, 
and the Bleu score is 22.37.  This corresponds to 
3.1% and 11.8% relative improvements over the 
baseline.  These improvements are statistically 
significant according to a previous study (Zhang 
et.al., 2004), which shows that a 2% improve-
ment in NIST score and a 5% improvement in 
Bleu score is significant for our translation sys-
tem on the June 2002 test data. 
6.3 Mean Reciprocal Rank 
To further investigate the effects of the rescor-
ing function in (11), Mean Reciprocal Rank 
(MRR) experiments were carried out.  MRR for a 
labeled set is the mean of the reciprocal rank of 
the individual phrase pair, at which the best can-
didate translation is found (Kantor and Voorhees, 
1996).  
Totally 9,641 phrase pairs were selected con-
taining 216 distinct source phrases.  Each source 
phrase was labeled with its best translation can-
didate without ambiguity.  The rank of the la-
beled candidate is calculated according to 
translation scores. The results are shown in Ta-
ble-4. 
 baseline cosine bm25 
MRR 0.40 0.58 0.75 
Table-4 Mean Reciprocal Rank 
 
The rescore functions improve the MRR from 
0.40 to 0.58 using cosine distance, and to 0.75 
using bm25.  This confirms our intuitions that 
good translation candidates move up in the rank 
after the rescoring.  
Decoder settings without word reordering with word reordering 
baseline bm25 baseline bm25 Scores (%) NIST Bleu NIST Bleu NIST Bleu NIST Bleu 
LDC 7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98 
LDC+ISA 7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93 
LDC+ISA+HMM 7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60 
LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37 
Table-3 Translation using bm25 rescore function with different decoder settings 
 
7 Conclusion and Discussion  
In this work, we proposed a way of using term 
weight based models in a vector space as addi-
tional evidences for translation quality, and inte-
grated the model into an existing phrase-based 
statistical machine translation system.  The mod-
el shows significant improvements when using it 
to score a manual dictionary as well as when us-
ing different phrase transducers or a combination 
of all available translation information.  Addi-
tional experiments also confirmed the effective-
ness of the proposed model in terms of of 
improved Mean Reciprocal Rank of good transla-
tions. 
Our future work is to explore alternatives such 
as the reranking work in (Collins, 2002) and in-
clude more knowledge such as syntax informa-
tion in rescoring the phrase translation pairs.  
References 
A. Stolcke. 2002. SRILM -- An Extensible Language 
Modeling Toolkit. In 2002 Proc. Intl. Conf. on 
Spoken Language Processing, Denver. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation, Computational Linguistics, 
vol. 19, no. 2, pp. 263?311. 
Michael Collins. 2000. Discriminative Reranking for 
Natural Language Parsing. Proc. 17th International 
Conf. on Machine Learning. pp. 175-182. 
William A. Gale and Kenneth W. Church. 1991. A 
Program for Aligning Sentences in Bilingual Cor-
pora.  In Computational Linguistics, vol.19 pp. 75-
102.  
Paul B. Kantor, Ellen Voorhees. 1996. Report on the 
TREC-5 Confusion Track. The Fifth Text Retrieval 
Conference. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. Pro-
ceedings of HLT-NAACL. Edmonton, Canada.  
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. Proceedings of EMNLP-2002, 
Philadelphia, PA. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. Proceedings of ACL-
00, pp. 440-447, Hongkong, China. 
Thomas Roelleke. 2003. A Frequency-based and a 
Poisson-based Definition of the Probability of Be-
ing Informative. Proceedings of the 26th annual in-
ternational ACM SIGIR. pp. 227-234.  
S.E. Robertson, and S. Walker. 1997. On relevance 
weights with little relevance information. In 1997 
Proceedings of ACM SIGIR. pp. 16-24. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora, 
Computational Linguistics 23(3): pp.377-404.  
K. Yamada and K. Knight. 2001. A Syntax-Based 
Statistical Translation Model. Proceedings of the 
39th Annual Meeting of the Association for Compu-
tational Linguistics. pp. 523-529. Toulouse, France.  
Richard Zens, Franz Josef Och and Hermann Ney. 
2002. Phrase-Based Statistical Machine Transla-
tion. Proceedings of the 25th Annual German Con-
ference on AI: Advances in Artificial Intelligence. 
pp. 18-22. 
Stephan Vogel, Hermann Ney, Christian Tillmann. 
1996. HMM-based word alignment in statistical 
translation. In: COLING '96: The 16th Int. Conf. on 
Computational Linguistics, Copenhagen, Denmark 
(1996) pp. 836-841. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, Alex Waibel. 
2003. The CMU Statistical Translation System, 
Proceedings of MT-Summit IX. New Orleans, LA. 
Stephan Vogel. 2003. SMT decoder dissected: word 
reordering, In 2003 Proceedings of Natural Lan-
guage Processing and Knowledge Engineering, 
(NLP-KE'03) Beijing, China.  
Stephan Vogel. 2004. Augmenting Manual Dictionar-
ies for Statistical Machine Translation Systems, In 
2003 Proceedings of LREC, Lisbon, Portugal.  pp. 
1593-1596. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST Scores : How much Im-
provement Do We Need to Have a Better System? 
In 2004 Proceedings of LREC, Lisbon, Portugal. 
pp. 2051-2054. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2003. "In-
tegrated Phrase Segmentation and Alignment Algo-
rithm for Statistical Machine Translation," in the 
Proceedings of NLP-KE'03, Beijing, China. 
Bing Zhao, Stephan Vogel. 2002. Adaptative Parallel 
Sentences Mining from web bilingual news collec-
tion. In 2002 IEEE International Conference on 
Data Mining, Maebashi City, Japan.  
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25?32,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Bilingual Word Spectral Clustering for Statistical Machine Translation
Bing Zhao? Eric P. Xing? ? Alex Waibel?
?Language Technologies Institute
?Center for Automated Learning and Discovery
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213
{bzhao,epxing,ahw}@cs.cmu.edu
Abstract
In this paper, a variant of a spectral clus-
tering algorithm is proposed for bilingual
word clustering. The proposed algorithm
generates the two sets of clusters for both
languages efficiently with high seman-
tic correlation within monolingual clus-
ters, and high translation quality across
the clusters between two languages. Each
cluster level translation is considered as
a bilingual concept, which generalizes
words in bilingual clusters. This scheme
improves the robustness for statistical ma-
chine translation models. Two HMM-
based translation models are tested to use
these bilingual clusters. Improved per-
plexity, word alignment accuracy, and
translation quality are observed in our ex-
periments.
1 Introduction
Statistical natural language processing usually suf-
fers from the sparse data problem. Comparing to
the available monolingual data, we have much less
training data especially for statistical machine trans-
lation (SMT). For example, in language modelling,
there are more than 1.7 billion words corpora avail-
able: English Gigaword by (Graff, 2003). However,
for machine translation tasks, there are typically less
than 10 million words of training data.
Bilingual word clustering is a process of form-
ing corresponding word clusters suitable for ma-
chine translation. Previous work from (Wang et al,
1996) showed improvements in perplexity-oriented
measures using mixture-based translation lexicon
(Brown et al, 1993). A later study by (Och,
1999) showed improvements on perplexity of bilin-
gual corpus, and word translation accuracy using a
template-based translation model. Both approaches
are optimizing the maximum likelihood of parallel
corpus, in which a data point is a sentence pair: an
English sentence and its translation in another lan-
guage such as French. These algorithms are es-
sentially the same as monolingual word clusterings
(Kneser and Ney, 1993)?an iterative local search.
In each iteration, a two-level loop over every possi-
ble word-cluster assignment is tested for better like-
lihood change. This kind of approach has two draw-
backs: first it is easily to get stuck in local op-
tima; second, the clustering of English and the other
language are basically two separated optimization
processes, and cluster-level translation is modelled
loosely. These drawbacks make their approaches
generally not very effective in improving translation
models.
In this paper, we propose a variant of the spec-
tral clustering algorithm (Ng et al, 2001) for bilin-
gual word clustering. Given parallel corpus, first, the
word?s bilingual context is used directly as features
- for instance, each English word is represented by
its bilingual word translation candidates. Second,
latent eigenstructure analysis is carried out in this
bilingual feature space, which leads to clusters of
words with similar translations. Essentially an affin-
ity matrix is computed using these cross-lingual fea-
tures. It is then decomposed into two sub-spaces,
which are meaningful for translation tasks: the left
subspace corresponds to the representation of words
in English vocabulary, and the right sub-space cor-
responds to words in French. Each eigenvector is
considered as one bilingual concept, and the bilin-
gual clusters are considered to be its realizations in
two languages. Finally, a general K-means cluster-
25
ing algorithm is used to find out word clusters in the
two sub-spaces.
The remainder of the paper is structured as fol-
lows: in section 2, concepts of translation models
are introduced together with two extended HMMs;
in section 3, our proposed bilingual word cluster-
ing algorithm is explained in detail, and the related
works are analyzed; in section 4, evaluation metrics
are defined and the experimental results are given;
in section 5, the discussions and conclusions.
2 Statistical Machine Translation
The task of translation is to translate one sentence
in some source language F into a target language E.
For example, given a French sentence with J words
denoted as fJ1 = f1f2...fJ , an SMT system auto-
matically translates it into an English sentence with
I words denoted by eI1 = e1e2...eI . The SMT sys-
tem first proposes multiple English hypotheses in its
model space. Among all the hypotheses, the system
selects the one with the highest conditional proba-
bility according to Bayes?s decision rule:
e?I1 = argmax
{eI1}
P (eI1|fJ1 ) = argmax
{eI1}
P (fJ1 |eI1)P (eI1),
(1)
where P (fJ1 |eI1) is called translation model, and
P (eI1) is called language model. The translation
model is the key component, which is the focus in
this paper.
2.1 HMM-based Translation Model
HMM is one of the effective translation models (Vo-
gel et al, 1996), which is easily scalable to very
large training corpus.
To model word-to-word translation, we introduce
the mapping j ? aj , which assigns a French word
fj in position j to a English word ei in position
i = aj denoted as eaj . Each French word fj is
an observation, and it is generated by a HMM state
defined as [eaj , aj], where the alignment aj for po-
sition j is considered to have a dependency on the
previous alignment aj?1. Thus the first-order HMM
is defined as follows:
P (fJ1 |eI1) =
?
aJ1
J?
j=1
P (fj |eaj )P (aj |aj?1), (2)
where P (aj |aj?1) is the transition probability. This
model captures the assumption that words close in
the source sentence are aligned to words close in
the target sentence. An additional pseudo word of
?NULL? is used as the beginning of English sen-
tence for HMM to start with. The (Och and Ney,
2003) model includes other refinements such as spe-
cial treatment of a jump to a Null word, and a uni-
form smoothing prior. The HMM with these refine-
ments is used as our baseline. Motivated by the work
in both (Och and Ney, 2000) and (Toutanova et al,
2002), we propose the two following simplest ver-
sions of extended HMMs to utilize bilingual word
clusters.
2.2 Extensions to HMM with word clusters
Let F denote the cluster mapping fj ? F(fj), which
assigns French word fj to its cluster ID Fj = F(fj).
Similarly E maps English word ei to its cluster ID
of Ei = E(ei). In this paper, we assume each word
belongs to one cluster only.
With bilingual word clusters, we can extend the
HMM model in Eqn. 1 in the following two ways:
P (fJ1 |eI1) =
?
aJ1
?J
j=1 P (fj |eaj )?
P (aj |aj?1,E(eaj?1),F(fj?1)),
(3)
where E(eaj?1) and F(fj?1) are non overlapping
word clusters (Eaj?1 , Fj?1)for English and French
respectively.
Another explicit way of utilizing bilingual word
clusters can be considered as a two-stream HMM as
follows:
P (fJ1 , F J1 |eI1, EI1) =?
aJ1
?J
j=1 P (fj |eaj )P (Fj |Eaj )P (aj |aj?1).
(4)
This model introduces the translation of bilingual
word clusters directly as an extra factor to Eqn. 2.
Intuitively, the role of this factor is to boost the trans-
lation probabilities for words sharing the same con-
cept. This is a more expressive model because it
models both word and the cluster level translation
equivalence. Also, compared with the model in Eqn.
3, this model is easier to train, as it uses a two-
dimension table instead of a four-dimension table.
However, we do not want this P (Fj |Eaj ) to dom-
inate the HMM transition structure, and the obser-
26
vation probability of P (fj |eaj ) during the EM itera-
tions. Thus a uniform prior P (Fj) = 1/|F | is intro-
duced as a smoothing factor for P (Fj |Eaj ):
P (Fj |Eaj ) = ?P (Fj |Eaj ) + (1? ?)P (Fj), (5)
where |F | is the total number of word clusters in
French (we use the same number of clusters for both
languages). ? can be chosen to get optimal perfor-
mance on a development set. In our case, we fix it to
be 0.5 in all our experiments.
3 Bilingual Word Clustering
In bilingual word clustering, the task is to build word
clusters F and E to form partitions of the vocabular-
ies of the two languages respectively. The two par-
titions for the vocabularies of F and E are aimed to
be suitable for machine translation in the sense that
the cluster/partition level translation equivalence is
reliable and focused to handle data sparseness; the
translation model using these clusters explains the
parallel corpus {(fJ1 , eI1)} better in terms of perplex-
ity or joint likelihood.
3.1 From Monolingual to Bilingual
To infer bilingual word clusters of (F,E), one can
optimize the joint probability of the parallel corpus
{(fJ1 , eI1)} using the clusters as follows:
(F?, E?) = argmax
(F,E)
P (fJ1 , eI1|F,E)
= argmax
(F,E)
P (eI1|E)P (fJ1 |eI1, F, E).(6)
Eqn. 6 separates the optimization process into two
parts: the monolingual part for E, and the bilingual
part for F given fixed E. The monolingual part is
considered as a prior probability:P (eI1|E), and E can
be inferred using corpus bigram statistics in the fol-
lowing equation:
E? = argmax
{E}
P (eI1|E)
= argmax
{E}
I?
i=1
P (Ei|Ei?1)P (ei|Ei). (7)
We need to fix the number of clusters beforehand,
otherwise the optimum is reached when each word
is a class of its own. There exists efficient leave-one-
out style algorithm (Kneser and Ney, 1993), which
can automatically determine the number of clusters.
For the bilingual part P (fJ1 |eI1, F, E), we can
slightly modify the same algorithm as in (Kneser
and Ney, 1993). Given the word alignment {aJ1}
between fJ1 and eI1 collected from the Viterbi path
in HMM-based translation model, we can infer F? as
follows:
F? = argmax
{F}
P (fJ1 |eI1, F,E)
= argmax
{F}
J?
j=1
P (Fj |Eaj )P (fj |Fj). (8)
Overall, this bilingual word clustering algorithm is
essentially a two-step approach. In the first step, E
is inferred by optimizing the monolingual likelihood
of English data, and secondly F is inferred by op-
timizing the bilingual part without changing E. In
this way, the algorithm is easy to implement without
much change from the monolingual correspondent.
This approach was shown to give the best results
in (Och, 1999). We use it as our baseline to compare
with.
3.2 Bilingual Word Spectral Clustering
Instead of using word alignment to bridge the par-
allel sentence pair, and optimize the likelihood in
two separate steps, we develop an alignment-free al-
gorithm using a variant of spectral clustering algo-
rithm. The goal is to build high cluster-level trans-
lation quality suitable for translation modelling, and
at the same time maintain high intra-cluster similar-
ity , and low inter-cluster similarity for monolingual
clusters.
3.2.1 Notations
We define the vocabulary VF as the French vo-
cabulary with a size of |VF |; VE as the English vo-
cabulary with size of |VE |. A co-occurrence matrix
C{F,E} is built with |VF | rows and |VE | columns;
each element represents the co-occurrence counts of
the corresponding French word fj and English word
ei. In this way, each French word forms a row vec-
tor with a dimension of |VE |, and each dimensional-
ity is a co-occurring English word. The elements in
the vector are the co-occurrence counts. We can also
27
view each column as a vector for English word, and
we?ll have similar interpretations as above.
3.2.2 Algorithm
With C{F,E}, we can infer two affinity matrixes
as follows:
AE = CT{F,E}C{F,E}
AF = C{F,E}CT{F,E},
where AE is an |VE | ? |VE | affinity matrix for En-
glish words, with rows and columns representing
English words and each element the inner product
between two English words column vectors. Corre-
spondingly, AF is an affinity matrix of size |VF | ?
|VF | for French words with similar definitions. Both
AE and AF are symmetric and non-negative. Now
we can compute the eigenstructure for both AE and
AF . In fact, the eigen vectors of the two are corre-
spondingly the right and left sub-spaces of the orig-
inal co-occurrence matrix of C{F,E} respectively.
This can be computed using singular value decom-
position (SVD): C{F,E} = USV T , AE = V S2V T ,
and AF = US2UT , where U is the left sub-space,
and V the right sub-space of the co-occurrence ma-
trix C{F,E}. S is a diagonal matrix, with the singular
values ranked from large to small along the diagonal.
Obviously, the left sub-space U is the eigenstructure
for AF ; the right sub-space V is the eigenstructure
for AE .
By choosing the top K singular values (the square
root of the eigen values for both AE and AF ), the
sub-spaces will be reduced to: U|VF |?K and V|VE |?K
respectively. Based on these subspaces, we can carry
out K-means or other clustering algorithms to in-
fer word clusters for both languages. Our algorithm
goes as follows:
? Initialize bilingual co-occurrence matrix
C{F,E} with rows representing French words,
and columns English words. Cji is the co-
occurrence raw counts of French word fj and
English word ei;
? Form the affinity matrix AE = CT{F,E}C{F,E}
and AF = CT{F,E}C{F,E}. Kernels can also be
applied here such as AE = exp(
C{F,E}CT{F,E}
?2 )
for English words. Set AEii = 0 and AF ii = 0,
and normalize each row to be unit length;
? Compute the eigen structure of the normalized
matrix AE , and find the k largest eigen vectors:
v1, v2, ..., vk; Similarly, find the k largest eigen
vectors of AF : u1, u2, ..., uk;
? Stack the k eigenvectors of v1, v2, ..., vk in
the columns of YE , and stack the eigenvectors
u1, u2, ..., uk in the columns for YF ; Normalize
rows of both YE and YF to have unit length. YE
is size of |VE | ? k and YF is size of |VF | ? k;
? Treat each row of YE as a point in R|VE |?k, and
cluster them into K English word clusters us-
ing K-means. Treat each row of YF as a point in
R|VF |?k, and cluster them into K French word
clusters.
? Finally, assign original word ei to cluster Ek
if row i of the matrix YE is clustered as Ek;
similar assignments are for French words.
Here AE and AF are affinity matrixes of pair-wise
inner products between the monolingual words. The
more similar the two words, the larger the value.
In our implementations, we did not apply a kernel
function like the algorithm in (Ng et al, 2001). But
the kernel function such as the exponential func-
tion mentioned above can be applied here to control
how rapidly the similarity falls, using some carefully
chosen scaling parameter.
3.2.3 Related Clustering Algorithms
The above algorithm is very close to the variants
of a big family of the spectral clustering algorithms
introduced in (Meila and Shi, 2000) and studied in
(Ng et al, 2001). Spectral clustering refers to a class
of techniques which rely on the eigenstructure of
a similarity matrix to partition points into disjoint
clusters with high intra-cluster similarity and low
inter-cluster similarity. It?s shown to be computing
the k-way normalized cut: K ? trY TD? 12AD? 12Y
for any matrix Y ? RM?N . A is the affinity matrix,
and Y in our algorithm corresponds to the subspaces
of U and V .
Experimentally, it has been observed that using
more eigenvectors and directly computing a k-way
partitioning usually gives better performance. In our
implementations, we used the top 500 eigen vectors
to construct the subspaces of U and V for K-means
clustering.
28
3.2.4 K-means
The K-means here can be considered as a post-
processing step in our proposed bilingual word clus-
tering. For initial centroids, we first compute the
center of the whole data set. The farthest centroid
from the center is then chosen to be the first initial
centroid; and after that, the other K-1 centroids are
chosen one by one to well separate all the previous
chosen centroids.
The stopping criterion is: if the maximal change
of the clusters? centroids is less than the threshold of
1e-3 between two iterations, the clustering algorithm
then stops.
4 Experiments
To test our algorithm, we applied it to the TIDES
Chinese-English small data track evaluation test set.
After preprocessing, such as English tokenization,
Chinese word segmentation, and parallel sentence
splitting, there are in total 4172 parallel sentence
pairs for training. We manually labeled word align-
ments for 627 test sentence pairs randomly sampled
from the dry-run test data in 2001, which has four
human translations for each Chinese sentence. The
preprocessing for the test data is different from the
above, as it is designed for humans to label word
alignments correctly by removing ambiguities from
tokenization and word segmentation as much as pos-
sible. The data statistics are shown in Table 1.
English Chinese
Train
Sent. Pairs 4172
Words 133598 105331
Voc Size 8359 7984
Test
Sent. Pairs 627
Words 25500 19726
Voc Size 4084 4827
Unseen Voc Size 1278 1888
Alignment Links 14769
Table 1: Training and Test data statistics
4.1 Building Co-occurrence Matrix
Bilingual word co-occurrence counts are collected
from the training data for constructing the matrix
of C{F,E}. Raw counts are collected without word
alignment between the parallel sentences. Practi-
cally, we can use word alignment as used in (Och,
1999). Given an initial word alignment inferred by
HMM, the counts are collected from the aligned
word pair. If the counts are L-1 normalized, then
the co-occurrence matrix is essentially the bilingual
word-to-word translation lexicon such as P (fj |eaj ).
We can remove very small entries (P (f |e) ? 1e?7),
so that the matrix of C{F,E} is more sparse for eigen-
structure computation. The proposed algorithm is
then carried out to generate the bilingual word clus-
ters for both English and Chinese.
Figure 1 shows the ranked Eigen values for the
co-occurrence matrix of C{F,E}.
0 100 200 300 400 500 600 700 800 900 10000.5
1
1.5
2
2.5
3
3.5 Eigen values of affinity matrices
Top 1000 Eigen Values
Eige
n Va
lues
(a) co?occur counts from init word alignment(b) raw co?occur counts from data
Figure 1: Top-1000 Eigen Values of Co-occurrence
Matrix
It is clear, that using the initial HMM word align-
ment for co-occurrence matrix makes a difference.
The top Eigen value using word alignment in plot a.
(the deep blue curve) is 3.1946. The two plateaus
indicate how many top K eigen vectors to choose to
reduce the feature space. The first one indicates that
K is in the range of 50 to 120, and the second plateau
indicates K is in the range of 500 to 800. Plot b. is
inferred from the raw co-occurrence counts with the
top eigen value of 2.7148. There is no clear plateau,
which indicates that the feature space is less struc-
tured than the one built with initial word alignment.
We find 500 top eigen vectors are good enough
for bilingual clustering in terms of efficiency and ef-
fectiveness.
29
4.2 Clustering Results
Clusters built via the two described methods are
compared. The first method bil1 is the two-step op-
timization approach: first optimizing the monolin-
gual clusters for target language (English), and af-
terwards optimizing clusters for the source language
(Chinese). The second method bil2 is our proposed
algorithm to compute the eigenstructure of the co-
occurrence matrix, which builds the left and right
subspaces, and finds clusters in such spaces. Top
500 eigen vectors are used to construct these sub-
spaces. For both methods, 1000 clusters are inferred
for English and Chinese respectively. The number
of clusters is chosen in a way that the final word
alignment accuracy was optimal. Table 2 provides
the clustering examples using the two algorithms.
settings cluster examples
mono-E1 entirely,mainly,merely
mono-E2
10th,13th,14th,16th,17th,18th,19th
20th,21st,23rd,24th,26th
mono-E3 drink,anglophobia,carota,giant,gymnasium
bil1-C3 ?,d,?,?,??,yQ,y
bil2-E1 alcoholic cognac distilled drinkscotch spirits whiskey
bil2-C1 ??,?,,??,2,?y,
?h,7,??},6,?,,k
bil2-E2 evrec harmony luxury people sedan sedanstour tourism tourist toward travel
bil2-C2 ??,s?,?,?(,ff?,u?,
@q,@?,|,|?,-|
Table 2: Bilingual Cluster Examples
The monolingual word clusters often contain
words with similar syntax functions. This hap-
pens with esp. frequent words (eg. mono-E1 and
mono-E2). The algorithm tends to put rare words
such as ?carota, anglophobia? into a very big cluster
(eg. mono-E3). In addition, the words within these
monolingual clusters rarely share similar transla-
tions such as the typical cluster of ?week, month,
year?. This indicates that the corresponding Chi-
nese clusters inferred by optimizing Eqn. 7 are not
close in terms of translational similarity. Overall, the
method of bil1 does not give us a good translational
correspondence between clusters of two languages.
The English cluster of mono-E3 and its best aligned
candidate of bil1-C3 are not well correlated either.
Our proposed bilingual cluster algorithm bil2
generates the clusters with stronger semantic mean-
ing within a cluster. The cluster of bil2-E1 relates
to the concept of ?wine? in English. The mono-
lingual word clustering tends to scatter those words
into several big noisy clusters. This cluster also has a
good translational correspondent in bil2-C1 in Chi-
nese. The clusters of bil2-E2 and bil2-C2 are also
correlated very well. We noticed that the Chinese
clusters are slightly more noisy than their English
corresponding ones. This comes from the noise in
the parallel corpus, and sometimes from ambiguities
of the word segmentation in the preprocessing steps.
To measure the quality of the bilingual clusters,
we can use the following two kind of metrics:
? Average ?-mirror (Wang et al, 1996): The ?-
mirror of a class Ei is the set of clusters in
Chinese which have a translation probability
greater than ?. In our case, ? is 0.05, the same
value used in (Och, 1999).
? Perplexity: The perplexity is defined as pro-
portional to the negative log likelihood of the
HMM model Viterbi alignment path for each
sentence pair. We use the bilingual word clus-
ters in two extended HMM models, and mea-
sure the perplexities of the unseen test data af-
ter seven forward-backward training iterations.
The two perplexities are defined as PP1 =
exp(??Jj=1 log(P (fj |eaj )P (aj |aj?1, Eaj?1 ,
Fj?1))/J) and PP2 = exp(?J?1
?J
j=1 log(
P (fj |eaj )P (aj |aj?1)P (Fj?1|Eaj?1))) for the
two extended HMM models in Eqn 3 and 4.
Both metrics measure the extent to which the trans-
lation probability is spread out. The smaller the bet-
ter. The following table summarizes the results on
?-mirror and perplexity using different methods on
the unseen test data.
algorithms ?-mirror HMM-1 Perp HMM-2 Perp
baseline - 1717.82
bil1 3.97 1810.55 352.28
bil2 2.54 1610.86 343.64
The baseline uses no word clusters. bil1 and bil2
are defined as above. It is clear that our proposed
method gives overall lower perplexity: 1611 from
the baseline of 1717 using the extended HMM-1.
If we use HMM-2, the perplexity goes down even
more using bilingual clusters: 352.28 using bil1, and
343.64 using bil2. As stated, the four-dimensional
30
table of P (aj |aj?1, E(eaj?1), F (fj?1)) is easily
subject to overfitting, and usually gives worse per-
plexities.
Average ?-mirror for the two-step bilingual clus-
tering algorithm is 3.97, and for spectral cluster-
ing algorithm is 2.54. This means our proposed al-
gorithm generates more focused clusters of transla-
tional equivalence. Figure 2 shows the histogram for
the cluster pairs (Fj , Ei), of which the cluster level
translation probabilities P (Fj |Ei) ? [0.05, 1]. The
interval [0.05, 1] is divided into 10 bins, with first bin
[0.05, 0.1], and 9 bins divides[0.1, 1] equally. The
percentage for clusters pairs with P (Fj |Ei) falling
in each bin is drawn.
Histogram of (F,E) pairs with P(F|E) > 0.05 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ten bins for P(F|E) ranging from [0.05, 1.0] 
spec-bi-clustering
two-step-bi-clustering
Figure 2: Histogram of cluster pairs (Fj , Ei)
Our algorithm generates much better aligned clus-
ter pairs than the two-step optimization algorithm.
There are 120 cluster pairs aligned with P (Fj |Ei) ?
0.9 using clusters from our algorithm, while there
are only 8 such cluster pairs using the two-step ap-
proach. Figure 3 compares the ?-mirror at different
numbers of clusters using the two approaches. Our
algorithm has a much better ?-mirror than the two-
step approach over different number of clusters.
Overall, the extended HMM-2 is better than
HMM-1 in terms of perplexity, and is easier to train.
4.3 Applications in Word Alignment
We also applied our bilingual word clustering in a
word alignment setting. The training data is the
TIDES small data track. The word alignments are
manually labeled for 627 sentences sampled from
the dryrun test data in 2001. In this manually
aligned data, we include one-to-one, one-to-many,
and many-to-many word alignments. Figure 4 sum-
marizes the word alignment accuracy for different
e-mirror over different settings
00.5
11.5
22.5
33.5
44.5
0 200 400 600 800 1000 1200 1400 1600 1800 2000
number of clusters
e-m
irro
r
BIL2: Co-occur raw countsBIL2: Co-occur counts from init word-alignBIL1: Two-step optimization
Figure 3: ?-mirror with different settings
methods. The baseline is the standard HMM trans-
lation model defined in Eqn. 2; the HMM1 is de-
fined in Eqn 3, and HMM2 is defined in Eqn 4. The
algorithm is applying our proposed bilingual word
clustering algorithm to infer 1000 clusters for both
languages. As expected, Figure 4 shows that using
F-measure of word alignment
38.00%
39.00%
40.00%
41.00%
42.00%
43.00%
44.00%
45.00%
1 2 3 4 5 6 7HMM Viterbi Iterations
F-m
ea
su
re
Baseline HMM
Extended HMM-1
Extended HMM-2
Figure 4: Word Alignment Over Iterations
word clusters is helpful for word alignment. HMM2
gives the best performance in terms of F-measure of
word alignment. One quarter of the words in the test
vocabulary are unseen as shown in Table 1. These
unseen words related alignment links (4778 out of
14769) will be left unaligned by translation models.
Thus the oracle (best possible) recall we could get
is 67.65%. Our standard t-test showed that signifi-
cant interval is 0.82% at the 95% confidence level.
The improvement at the last iteration of HMM is
marginally significant.
4.4 Applications in Phrase-based Translations
Our pilot word alignment on unseen data showed
improvements. However, we find it more effective
in our phrase extraction, in which three key scores
31
are computed: phrase level fertilities, distortions,
and lexicon scores. These scores are used in a lo-
cal greedy search to extract phrase pairs (Zhao and
Vogel, 2005). This phrase extraction is more sen-
sitive to the differences in P (fj |ei) than the HMM
Viterbi word aligner.
The evaluation conditions are defined in NIST
2003 Small track. Around 247K test set (919 Chi-
nese sentences) specific phrase pairs are extracted
with up to 7-gram in source phrase. A trigram
language model is trained using Gigaword XinHua
news part. With a monotone phrase-based decoder,
the translation results are reported in Table 3. The
Eval. Baseline Bil1 Bil2
NIST 6.417 6.507 6.582
BLEU 0.1558 0.1575 0.1644
Table 3: NIST?03 C-E Small Data Track Evaluation
baseline is using the lexicon P (fj |ei) trained from
standard HMM in Eqn. 2, which gives a BLEU
score of 0.1558 +/- 0.0113. Bil1 and Bil2 are using
P (fj |ei) from HMM in Eqn. 4 with 1000 bilingual
word clusters inferred from the two-step algorithm
and the proposed one respectively. Using the clus-
ters from the two-step algorithm gives a BLEU score
of 0.1575, which is close to the baseline. Using clus-
ters from our algorithm, we observe more improve-
ments with BLEU score of 0.1644 and a NIST score
of 6.582.
5 Discussions and Conclusions
In this paper, a new approach for bilingual word
clustering using eigenstructure in bilingual feature
space is proposed. Eigenvectors from this feature
space are considered as bilingual concepts. Bilin-
gual clusters from the subspaces expanded by these
concepts are inferred with high semantic correla-
tions within each cluster, and high translation quali-
ties across clusters from the two languages.
Our empirical study also showed effectiveness of
using bilingual word clusters in extended HMMs for
statistical machine translation. The K-means based
clustering algorithm can be easily extended to do hi-
erarchical clustering. However, extensions of trans-
lation models are needed to leverage the hierarchical
clusters appropriately.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263?331.
David Graff. 2003. Ldc gigaword corpora: English gi-
gaword (ldc catalog no: Ldc2003t05). In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
R. Kneser and Hermann Ney. 1993. Improved clus-
tering techniques for class-based statistical language
modelling. In European Conference on Speech Com-
munication and Technology, pages 973?976.
Marina Meila and Jianbo Shi. 2000. Learning segmenta-
tion by random walks. In Advances in Neural Informa-
tion Processing Systems. (NIPS2000), pages 873?879.
A. Ng, M. Jordan, and Y. Weiss. 2001. On spectral
clustering: Analysis and an algorithm. In Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2001.
Franz J. Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation.
In COLING?00: The 18th Int. Conf. on Computational
Linguistics, pages 1086?1090, Saarbrucken, Germany,
July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19?51.
Franz J. Och. 1999. An efficient method for determin-
ing bilingal word classes. In Ninth Conf. of the Europ.
Chapter of the Association for Computational Linguis-
tics (EACL?99), pages 71?76.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proc. of the Conference on
Empirical Methods in Natural Language Processing.
S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm
based word alignment in statistical machine transla-
tion. In Proc. The 16th Int. Conf. on Computational
Lingustics, (Coling?96), pages 836?841.
Yeyi Wang, John Lafferty, and Alex Waibel. 1996.
Word clustering with parallel spoken language cor-
pora. In proceedings of the 4th International Con-
ference on Spoken Language Processing (ICSLP?96),
pages 2364?2367.
Bing Zhao and Stephan Vogel. 2005. A generalized
alignment-free phrase extraction algorithm. In ACL
2005 Workshop: Building and Using Parallel Cor-
pora: Data-driven Machine Translation and Beyond,
Ann Arbor, Michigan.
32
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 141?144,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
A Generalized Alignment-Free Phrase Extraction
Bing Zhao
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA-15213
bzhao@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA-15213
vogel+@cs.cmu.edu
Abstract
In this paper, we present a phrase ex-
traction algorithm using a translation lex-
icon, a fertility model, and a simple dis-
tortion model. Except these models, we
do not need explicit word alignments for
phrase extraction. For each phrase pair (a
block), a bilingual lexicon based score is
computed to estimate the translation qual-
ity between the source and target phrase
pairs; a fertility score is computed to es-
timate how good the lengths are matched
between phrase pairs; a center distortion
score is computed to estimate the relative
position divergence between the phrase
pairs. We presented the results and our
experience in the shared tasks on French-
English.
1 Introduction
Phrase extraction becomes a key component in to-
day?s state-of-the-art statistical machine translation
systems. With a longer context than unigram, phrase
translation models have flexibilities of modelling lo-
cal word-reordering, and are less sensitive to the er-
rors made from preprocessing steps including word
segmentations and tokenization. However, most of
the phrase extraction algorithms rely on good word
alignments. A widely practiced approach explained
in details in (Koehn, 2004), (Och and Ney, 2003)
and (Tillmann, 2003) is to get word alignments from
two directions: source to target and target to source;
the intersection or union operation is applied to get
refined word alignment with pre-designed heuristics
fixing the unaligned words. With this refined word
alignment, the phrase extraction for a given source
phrase is essentially to extract the target candidate
phrases in the target sentence by searching the left
and right projected boundaries.
In (Vogel et al, 2004), they treat phrase align-
ment as a sentence splitting problem: given a source
phrase, find the boundaries of the target phrase such
that the overall sentence alignment lexicon probabil-
ity is optimal. We generalize it in various ways, esp.
by using a fertility model to get a better estimation of
phrase lengths, and a phrase level distortion model.
In our proposed algorithm, we do not need ex-
plicit word alignment for phrase extraction. Thereby
it avoids the burden of testing and comparing differ-
ent heuristics especially for some language specific
ones. On the other hand, the algorithm has such flex-
ibilities that one can incorporate word alignment and
heuristics in several possible stages within this pro-
posed framework to further improve the quality of
phrase pairs. In this way, our proposed algorithm
is more generalized than the usual word alignment
based phrase extraction algorithms.
The paper is structured as follows: in section 2,
The concept of blocks is explained; in section 3, a
dynamic programming approach is model the width
of the block; in section 4, a simple center distortion
of the block; in section 5, the lexicon model; the
complete algorithm is in section 6; in section 7, our
experience and results using the proposed approach.
2 Blocks
We consider each phrase pair as a block within a
given parallel sentence pair, as shown in Figure 1.
The y-axis is the source sentence, indexed word
by word from bottom to top; the x-axis is the target
sentence, indexed word by word from left to right.
The block is defined by the source phrase and its pro-
jection. The source phrase is bounded by the start
and the end positions in the source sentence. The
projection of the source phrase is defined as the left
and right boundaries in the target sentence. Usually,
the boundaries can be inferred according to word
alignment as the left most and right most aligned
positions from the words in the source phrase. In
141
Start
End
Right boundaryLeft boundary
Width
src center
tgt center
Figure 1: Blocks with ?width? and ?centers?
this paper, we provide another view of the block,
which is defined by the centers of source and target
phrases, and the width of the target phrase.
Phrase extraction algorithms in general search
for the left and right projected boundaries of each
source phrase according to some score metric com-
puted for the given parallel sentence pairs. We
present here three models: a phrase level fertility
model score for phrase pairs? length mismatch, a
simple center-based distortion model score for the
divergence of phrase pairs? relative positions, and
a phrase level translation score to approximate the
phrase pairs? translational equivalence. Given a
source phrase, we can search for the best possible
block with the highest combined scores from the
three models.
3 Length Model: Dynamic Programming
Given the word fertility definitions in IBM Mod-
els (Brown et al, 1993), we can compute a prob-
ability to predict phrase length: given the candi-
date target phrase (English) eI1, and a source phrase
(French) of length J , the model gives the estima-
tion of P (J |eI1) via a dynamic programming algo-
rithm using the source word fertilities. Figure 2
shows an example fertility trellis of an English tri-
gram. Each edge between two nodes represents one
English word ei. The arc between two nodes rep-
resents one candidate non-zero fertility for ei. The
fertility of zero (i.e. generating a NULL word) cor-
responds to the direct edge between two nodes, and
in this way, the NULL word is naturally incorpo-
rated into this model?s representation. Each arc is
e1 e2 e3
1
3
2
0 0
2
0
e1 e2 e3
??
?.
1
2
3
4
3
1
3
1
2
Figure 2: An example of fertility trellis for dynamic
programming
associated with a English word fertility probability
P (?i|ei). A path ?I1 through the trellis represents
the number of French words ?i generated by each
English word ei. Thus, the probability of generating
J words from the English phrase along the Viterbi
path is:
P (J |eI1) = max
{?I1,J=
?I
i=1 ?i}
I
?
i=1
P (?i|ei) (1)
The Viterbi path is inferred via dynamic program-
ming in the trellis of the lower panel in Figure 2:
?[j, i] = max
?
?
?
?
?
?
?
?[j, i ? 1] + log PNULL(0|ei)
?[j ? 1, i ? 1] + log P?(1|ei)
?[j ? 2, i ? 1] + log P?(2|ei)
?[j ? 3, i ? 1] + log P?(3|ei)
where PNULL(0|ei) is the probability of generating
a NULL word from ei; P?(k = 1|ei) is the usual
word fertility model of generating one French word
from the word ei; ?[j, i] is the cost so far for gener-
ating j words from i English words ei1 : e1, ? ? ? , ei.
After computing the cost of ?[J, I], we can trace
back the Viterbi path, along which the probability
P (J |eI1) of generating J French words from the En-
glish phrase eI1 as shown in Eqn. 1.
142
With this phrase length model, for every candidate
block, we can compute a phrase level fertility score
to estimate to how good the phrase pairs are match
in their lengthes.
4 Distortion of Centers
The centers of source and target phrases are both il-
lustrated in Figure 1. We compute a simple distor-
tion score to estimate how far away the two centers
are in a parallel sentence pair in a sense the block is
close to the diagonal.
In our algorithm, the source center fj+lj of the
phrase f j+lj with length l +1 is simply a normalized
relative position defined as follows:
fj+lj =
1
|F |
j?=j+l
?
j?=j
j?
l + 1 (2)
where |F | is the French sentence length.
For the center of English phrase ei+ki in the target
sentence, we first define the expected corresponding
relative center for every French word fj? using the
lexicalized position score as follows:
ei+ki (fj?) =
1
|E| ?
?(i+k)
i?=i i? ? P (fj? |ei?)
?(i+k)
i?=i P (fj? |ei?)
(3)
where |E| is the English sentence length. P (fj? |ei)
is the word translation lexicon estimated in IBM
Models. i is the position index, which is weighted
by the word level translation probabilities; the term
of ?Ii=1 P (fj? |ei) provides a normalization so that
the expected center is within the range of target sen-
tence length. The expected center for ei+ki is simply
a average of ei+ki (fj?):
ei+ki =
1
l + 1
j+l
?
j?=j
ei+ki (fj?) (4)
This is a general framework, and one can certainly
plug in other kinds of score schemes or even word
alignments to get better estimations.
Given the estimated centers of fj+lj and
ei+ki , we can compute how close they are bythe probability of P (ei+ki |fj+lj ). To estimate
P (ei+ki |fj+lj ), one can start with a flat gaussian
model to enforce the point of (ei+ki ,fj+lj ) not toofar off the diagonal and build an initial list of phrase
pairs, and then compute the histogram to approxi-
mate P (ei+ki |fj+lj ).
5 Lexicon Model
Similar to (Vogel et al, 2004), we compute for each
candidate block a score within a given sentence pair
using a word level lexicon P (f |e) as follows:
P (f j+lj |ei+ki ) =
?
j??[j,j+l]
?
i??[i,i+k]
P (fj? |ei?)
k + 1
?
?
j? /?[j,j+l]
?
i? /?[i,i+k]
P (fj? |ei?)
|E| ? k ? 1
6 Algorithm
Our phrase extraction is described in Algorithm
1. The input parameters are essentially from IBM
Model-4: the word level lexicon P (f |e), the English
word level fertility P?(?e = k|e), and the center
based distortion P (ei+ki |fj+lj ).
Overall, for each source phrase f j+lj , the algo-
rithm first estimates its normalized relative center
in the source sentence, its projected relative cen-
ter in the target sentence. The scores of the phrase
length, center-based distortion, and a lexicon based
score are computed for each candidate block A lo-
cal greedy search is carried out for the best scored
phrase pair (f j+lj , ei+ki ).
In our submitted system, we computed the
following seven base scores for phrase pairs:
Pef (f j+lj |ei+ki ), Pfe(ei+ki |f j+lj ), sharing similar
function form in Eqn. 5.
Pef (f j+lj |ei+ki ) =
?
j?
?
i?
P (fj? |ei?)P (ei? |ei+ki )
=
?
j?
?
i?
P (fj? |ei?)
k + 1 (5)
We compute phrase level relative frequency in both
directions: Prf (f j+lj |ei+ki ) and Prf (ei+ki |f j+lj ). We
compute two other lexicon scores which were also
used in (Vogel et al, 2004): S1(f j+lj |ei+ki ) and
S2(ei+ki |f
j+l
j ) using the similar function in Eqn. 6:
S(f j+lj |ei+ki ) =
?
j?
?
i?
P (fj? |ei?) (6)
143
In addition, we put the phrase level fertility score
computed in section 3 via dynamic programming to
be as one additional score for decoding.
Algorithm 1 A Generalized Alignment-free Phrase
Extraction
1: Input: Pre-trained models: P?(?e = k|e) ,
P (E |F ) , and P (f |e).
2: Output: PhraseSet: Phrase pair collections.
3: Loop over the next sentence pair
4: for j : 0 ? |F | ? 1,
5: for l : 0 ? MaxLength,
6: foreach f j+lj
7: compute f and E
8: left = E ? |E|-MaxLength,
9: right= E ? |E|+MaxLength,
10: for i : left ? right,
11: for k : 0 ? right,
12: compute e of ei+ki ,
13: score the phrase pair (f j+lj , ei+ki ), where
score = P (e|f )P (l|ei+ki )P (f j+lj |ei+ki )
14: add top-n {(f j+lj , ei+ki )} into PhraseSet.
7 Experimental Results
Our system is based on the IBM Model-4 param-
eters. We train IBM Model 4 with a scheme of
1720h73043 using GIZA++ (Och and Ney, 2003).
The maximum fertility for an English word is 3. All
the data is used as given, i.e. we do not have any
preprocessing of the English-French data. The word
alignment provided in the workshop is not used in
our evaluations. The language model is provided
by the workshop, and we do not use other language
models.
The French phrases up to 8-gram in the devel-
opment and test sets are extracted with top-3 can-
didate English phrases. There are in total 2.6 mil-
lion phrase pairs 1 extracted for both development
set and the unseen test set. We did minimal tuning
of the parameters in the pharaoh decoder (Koehn,
2004) settings, simply to balance the length penalty
for Bleu score. Most of the weights are left as they
are given: [ttable-limit]=20, [ttable-threshold]=0.01,
1Our phrase table is to be released to public in this workshop
[stack]=100, [beam-threshold]=0.01, [distortion-
limit]=4, [weight-d]=0.5, [weight-l]=1.0, [weight-
w]=-0.5. Table 1 shows the algorithm?s performance
on several settings for the seven basic scores pro-
vided in section 6.
settings Dev.Bleu Tst.Bleu
s1 27.44 27.65
s2 27.62 28.25
Table 1: Pharaoh Decoder Settings
In Table 1, setting s1 was our submission
without using the inverse relative frequency of
Prf (ei+ki |f
j+l
j ). s2 is using all the seven scores.
8 Discussions
In this paper, we propose a generalized phrase ex-
traction algorithm towards word alignment-free uti-
lizing the fertility model to predict the width of the
block, a distortion model to predict how close the
centers of source and target phrases are, and a lex-
icon model for translational equivalence. The algo-
rithm is a general framework, in which one could
plug in other scores and word alignment to get bet-
ter results.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263?331.
Philip Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based smt. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americans (AMTA).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19?51.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
Stephan Vogel, Sanjika Hewavitharana, Muntsin Kolss,
and Alex Waibel. 2004. The ISL statistical translation
system for spoken language translation. In Proc. of the
International Workshop on Spoken Language Transla-
tion, pages 65?72, Kyoto, Japan.
144
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 572?581,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Generalizing Local and Non-Local Word-Reordering Patterns for
Syntax-Based Machine Translation
Bing Zhao
IBM T.J. Watson Research
Yorktown Heights, NY-10598
zhaob@us.ibm.com
Yaser Al-onaizan
IBM T.J. Watson Research
Yorktown Heights, NY-10598
onaizan@us.ibm.com
Abstract
Syntactic word reordering is essential for
translations across different grammar struc-
tures between syntactically distant language-
pairs. In this paper, we propose to em-
bed local and non-local word reordering de-
cisions in a synchronous context free gram-
mar, and leverages the grammar in a chart-
based decoder. Local word-reordering is ef-
fectively encoded in Hiero-like rules; whereas
non-local word-reordering, which allows for
long-range movements of syntactic chunks,
is represented in tree-based reordering rules,
which contain variables correspond to source-
side syntactic constituents. We demonstrate
how these rules are learned from parallel cor-
pora. Our proposed shallow Tree-to-String
rules show significant improvements in trans-
lation quality across different test sets.
1 Introduction
One of the main issues that a translator (human or
machine) must address during the translation pro-
cess is how to match the different word orders be-
tween the source language and the target language.
Different language-pairs require different levels of
word reordering. For example, when we translate
between English and Spanish (or other Romance
languages), most of the word reordering needed
is local because of the shared syntactical features
(e.g., Spanish noun modifier constructs are written
in English as modifier noun). However, for syn-
tactically distant language-pairs such as Chinese-
English, long-range reordering is required where
whole phrases are moved across the sentence.
The idea of ?syntactic cohesion? (Fox, 2002) is
characterized by its simplicity, which has attracted
researchers for years. Previous works include sev-
eral approaches of incorporating syntactic informa-
tion to preprocess the source sentences to make them
more like the target language in structure. Xia and
McCord (2004) (Niessen and Ney, 2004; Collins et
al., 2005) described approaches applied to language-
pairs such as French-English and German-English.
Later, Wang et al (2007) presented specific rules
to pre-order long-range movements of words, and
improved the translations for Chinese-to-English.
Overall, these works are similar, in that they design
a few language-specific and linguistically motivated
reordering rules, which are generally simple. The
eleven rules described in Wang et al (2007) are ap-
pealing, as they have rather simple structure, mod-
eling only NP, VP and LCP via one-level sub-tree
structure with two children, in the source parse-tree
(a special case of ITG (Wu, 1997)). It effectively en-
hances the quality of the phrase-based translation of
Chinese-to-English. One major weakness is that the
reordering decisions were done in the preprocessing
step, therefore rendering the decoding process un-
able to recover the reordering errors from the rules if
incorrectly applied to. Also the reordering decisions
are made without the benefits of additional models
(e.g., the language models) that are typically used
during decoding.
Another method to address the re-ordering prob-
lem in translation is the Hiero model proposed by
Chiang (2005), in which a probabilistic synchronous
context free grammar (PSCFG) was applied to guide
the decoding. Hiero rules generalize phrase-pairs
572
by introducing a single generic nonterminal (i.e., a
variable) [X]. The combination of variables and lex-
icalized words in a Hiero rule nicely captures local
word and phrase reordering (modeling an implicit
reordering window of max-phrase length). These
rules are then applied in a CYK-style decoder. In
Hiero rules, any nested phrase-pair can be general-
ized as variables [X]. This usually leads to too many
redundant translations, which worsens the spurious
ambiguities (Chiang, 2005) problems for both de-
coding and optimization (i.e., parameter tuning). We
found thatvariables (nonterminal [X]) in Hiero rules
offer a generalization too coarse to improve the ef-
fectiveness of hierarchical models? performance.
We propose to enrich the variables in Hiero rules
with additional source syntactic reordering informa-
tion, in the form of shallow Tree-to-String syntactic
structures. The syntactic information is represented
by flat one-level sub-tree structures, with Hiero-like
nonterminal variables at the leaf nodes. The syntac-
tic rules, proposed in this paper, are composed of
(possibly lexicalized) source treelets and target sur-
face strings, with one or more variables that help
capture local-reordering similar to the Hiero rules.
Variables in a given rule are derived not only from
the embedded aligned blocks (phrase-pairs), but also
from the aligned source syntactic constituents. The
aligned constituents, as in our empirical observa-
tions for Chinese-English, tend to move together in
translations. The decoder is guided by these rules to
reduce spurious derivations; the rules also constrain
the exploration of the search space toward better
translation quality and sometime improved speed by
breaking long sentences into pieces. Overall, what
we want is to enable the long-range reordering deci-
sions to be local in a chart-based decoder.
To be more specific, we think the simple shal-
low syntactic structure is powerful enough for cap-
turing the major structure-reordering patterns, such
as NP, VP and LCP structures. We also use sim-
ple frequency-based feature functions, similar to the
blocks used in phrase-based decoder, to further im-
prove the rules? representation power. Overall, this
enables us to avoid either a complex decoding pro-
cess to generate the source parse tree, or difficult
combinatorial optimizations for the feature func-
tions associated with rules.
In Marton and Resnik (2008), hiero variables
were disambiguated with additional binary feature
functions, with their weights optimized in standard
MER training. The combinatorial effects of the
added feature functions can make the feature se-
lection and optimization of the weights rather dif-
ficult. Since the grammar is essentially the same
as the Hiero ones, a standard CYK decoder can be
simply applied in their work. Word reordering can
also be addressed via distortion models. Work in
(Al-Onaizan and Kishore, 2006; Xiong et al, 2006;
Zens et al, 2004; Kumar and Byrne, 2005; Tillmann
and Zhang, 2005) modeled the limited information
available at phrase-boundaries. Syntax-based ap-
proaches such as (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al, 2006) heavily rely on
the parse-tree to constrain the search space by as-
suming a strong mapping of structures across distant
language-pairs. Their algorithms are also subject to
parsers? performances to a larger extent, and have
high complexity and less scalability in reality. In Liu
et al (2007), multi-level tree-structured rules were
designed, which made the decoding process very
complex, and auxiliary rules have to be designed
and incorporated to shrink multiple source nonter-
minals into one target nonterminal. From our em-
pirical observations, most of the time, however, the
multi-level tree-structure is broken in the translation
process, and POS tags are frequently distorted. In-
deed, strictly following the source parse tree is usu-
ally not necessary, and maybe too expensive for the
translation process.
The remainder of this paper is structured as fol-
lows: in section ? 2, we define the notations in our
synchronous context free grammar, in section ? 3,
the rule extractions are illustrated in details, in sec-
tion ? 4, the decoding process of applying these rules
is described. Experiments in ? 5 were carried out
using GALE Dev07 datasets. Improved translation
qualities were obtained by applying the proposed
Tree-to-String rules. Conclusions and discussions
are given in ? 6.
2 Shallow Tree-to-String Rules
Our proposed rules are in the form of probabilis-
tic synchronous context free grammar (PSCFG). We
adopt the notations used in (Chiang, 2005). Let N
be a set of nonterminals, a rule has the following
573
form:
X ?< `; ?;?;?; w? >, (1)
where X abstracts nonterminal symbols in N ; ? ?
[N,VS ]+ is a sequence of one or more source 1
words (as in the vocabulary of VS) and nonterminal
symbols in N ; ? ? [N,VT ]+ is a sequence of one
or more target words (in VT ) and nonterminals in N
. ? is the one-to-one alignment of the nonterminals
between ? and ?; w? contains non-negative weights
associated with each rule; ` is a label-symbol speci-
fying the root node of the source span covering ?. In
our grammar, ` is one of the labels (e.g., NP) defined
in the source treebank tagset (in our case UPenn
Chinese tagset) indicating that the source span ? is
rooted at `. Additionally, a NULL tag ? in ` denotes
a flat structure of ?, in which no constituent structure
was found to cover the span, and we need to back
off to the normal Hiero-style rules. Our nonterminal
symbols include the labels and the POS tags in the
source parse trees.
In the following, we will illustrate the Tree-to-
String rules we are proposing. At the same time, we
will describe the extraction algorithm, with which
we derive our rules from the word-aligned source-
parsed parallel text. Our nonterminal set N is a re-
duced set of the treebank tagset (Xue et al, 2005). It
consists of 17 unique labels.
The rules we extract belong to one of the follow-
ing categories:
? ? contains only words, and ` is NULL; this cor-
responds to the general blocks used in phrase-
based decoder (Och and Ney, 2004);
? ? contains words and variables of [X,0] and
[X,1], and ` is NULL; this corresponds to the
Hiero rules as in Chiang (2005);
? ? contains words and variables in the form
of [X,TAG2], in which TAG is from the LDC
tagset; this defines a well formed subtree, in
which at least one child (constituent) is aligned
to continuous target ngrams. If ? contains only
variables from LDC tag set, this indicates all
the constituents (children) in the subtree are
aligned. This is a superset of rules generalizing
1we use end-user terminologies for source and target.
2we index the tags for multiple occurrences in one rule
those in Wang et al (2007). If ? contains vari-
ables from POS tags, this essentially produces
a superset of the monolingual side POS-based
reordering rules explored in Tillmann (2008).
We focus on the third category ? a syntactic label
` over the span of ?, indicating the covered source
words consist of a linguistically well-defined phrase.
` together with ? define a tree-like structure: the root
node is `, and the aligned children are nonterminals
in ?. The structure information is encoded in (`,
?) pair-wise connections, and the variables keep the
generalizations over atomic translation-pairs similar
to Hiero models. When the rule is applied during
decoding time, the labels, the tree-structure and the
lexical items need to be all matched.
3 Learning and Applying Rules
A parser is assumed for the source language in the
parallel data. In our case, a Chinese parser is applied
for training and test data. A word alignment model is
used to align the source words with the target words.
3.1 Extractions
Our rule extraction is a three-step process. First, tra-
ditional blocks (phrase-pairs) extraction is carried
out. Secondly, Tree-to-String rules, are then ex-
tracted from the aligned blocks, of which the source
side is covered by a complete subtree, with different
permutations of the embedded aligned constituents,
or partially lexicalized constituents. Otherwise, the
Hiero-like rules will be extracted when there is no
sub-tree structure identified, in our final step. Fre-
quencies of extracted rules were counted to compute
feature functions.
Figure 1-(a) shows that a subtree (with root at
VP) is aligned to the English string. Considering the
huge quantity of all the permutations of the aligned
constituents under the tree, only part of the Tree-to-
String rules extracted are shown in Figure 1-(c). The
variables incorporate linguistic information in the
assigned tag by the parser. When there is no aligned
constituent for further generalization, the variables,
defined in our grammar, back off to the Hiero-like
ones without any label-identity information. One
such example is in the rule ?? [X,0]? [X,VP] ?
[X,VP] before the [X,0]?, in which the Hiero-style
574
March      before   the    sunrise 
??? ?
??
VP
PP VP
??? ? before the sunrise 
?? March
?? sunrise
[X,PP] [X,VP]  [X,VP] [X,PP] 
[X,PP]?? March [X,PP] 
??? ? [X,VP] [X,VP] before the sunrise 
??? ??? March before the sunrise ? [X,0]  ?[X,VP] [X,VP ] before the [X,0] 
(a) Parse-Tree Alignment (b) Blocks Alignment (c) Tree-to-String rules with root of VP
Figure 1: Example rules extracted. (a) the aligned source parse tree with target string; (b) general blocks alignment;
(c) Tree-to-String rules, with root of VP. The tree structure is aligned with target strings
This The casecases triggered trigger an enormous tremendous a huge
shockshocked  shocks
inIn the locallocally In the local
This case The case This case was
a great shock great shocks a huge shock
locally in the local local
?? ? ?? ?? ?? ??
inthis case the locals triggered enormous shock
triggered a huge
? ? ? ?? ?? ?? ??
IP
NP VP
DP NP
DT NN
PP
P
VP
NP
NN
VV NP
ADJP
JJ
NP
NN
Translations of ???????????:
triggered a huge shock in the locallocally triggered an enormous shock
Figure 2: Subtree of ?VP(PP,VP)? triggered a reordering pattern of swapping the order of the two children PP and VP
in the source parse tree. This will move the translation ?in the local? after the translation of ?triggered a huge shock?,
to form the preferred translation in the highlighted cell: ?triggered a huge shock in the local?.
variable [X,0] and the label-based variable [X,VP]
co-exist in our proposed rule.
We illustrate several special cases of our extracted
Tree-to-String rules in the following. We index the
variables with their positions to indicate the align-
ment ?, and skip the feature function w? to simplify
the notations.
X ?< [X, IP ]; [X,NP0] [X,V P0]; (2)
[X,NP0] is [X,V P0] > .
The rule in Eqn. 2 shows that a source tree rooted
at IP, with two children of NP and VP generalized
into variables [X,NP] and [X,VP]; they are rewritten
into ?[X,NP] is [X,VP]?, with the spontaneous word
is inserted. Such rules are not allowed in Hiero-style
models, as there is no lexical item between the two
variables (Chiang, 2005) in the source side. This
rule will generate a spontaneous word ?is? from the
given subtree structure. Usually, it is very hard to
align the spontaneous word correctly, and the rules
we proposed indicate that spontaneous words are
generated directly from the source sub-tree struc-
ture, and they might not necessarily get algned to
some particular source words.
A second example is shown in Eqn. 3, which is
similar to the Hiero rules:
X ?< ?; [X, 0] zhiyi; (3)
one of the [X, 0] > .
The rule in Eqn. 3 shows that when there is
no linguistically-motivated root covering the span,
([X,NULL] is then assigned), we simply back
off to the Hiero rules. In this case, the source
span of [X, 0] zhiyi is rewritten into the target
?one of the [X, 0]?, without considering the map-
575
ping of the root of the span. In this way, the repre-
sentation power is kept in the variables in our rules,
even if the source subtree is aligned to a discontin-
uous sequence on the target side. This is important
for Chinese-to-English, because the grammar struc-
ture is so different that more than 40% of the subtree
structures were not kept during the translation in our
study on hand-aligned data. Following strictly the
source side syntax will derail from these informative
translation patterns.
X ?< [X,NP ]; [X,NN1][X,NN2][X,NN3];
[X,NN3][X,NN1][X,NN2] > . (4)
Eqn. 4. is a POS-based rule ? a special case in
our proposed rules. This rule shows the reorder-
ing patterns for three adjacent NN?s. POS based
rules can be very informative for some language-
pairs such as Arabic-to-English, where the ADJ is
usually moved before NN during the translations.
As also shown in Eqn. 4 for POS sequences, in the
UPenn treebank-style parse trees, a root usually have
more than two variables. Our rule set for subtree,
therefore, contain more than two variables: ?X ?<
[X, IP ]; [X,ADV P0][X,NP0][X,V P0]; [X,NP0]
[X,ADV P0][X,V P0] >?. A CYK-style decoder
has to rely on binarization to preprocess the
grammar as did in (Zhang et al, 2006) to handle
multi-nonterminal rules. We adopt the so-called
dotted-rule or dotted-production, similar to the
Early-style algorithm (Earley, 1970), to handle the
multi-nonterminal rules in our chart-based decoder.
3.2 Feature Functions
As used in most of the SMT decoders for a phrase-
pair, a set of standard feature functions are applied
in our decoder, including IBM Model-1 like scores
in both directions, relative frequencies in both direc-
tions. In addition to these features, a counter is as-
sociated to each rule to collect how many rules were
applied so far to generate a hypothesis. The stan-
dard Minimum Error Rate training (Och, 2003) was
applied to tune the weights for all feature types.
The number of extracted rules from the GALE
data is generally large. We pruned the rules accord-
ing to their frequencies, and only keep at most the
top-50 frequent candidates for each source side.
4 Chart-based Decoder
Given the source sentence, with constituent parse-
trees, the decoder is to find the best derivation D?
which yield the English string e?:
e? = argmax
D?
{?(D)?(e)?(f |e)}, (5)
where ?(D) is the cost for each of the derivations
that lead to e from a given source-parsed f ; ?(e)
is for cost functions from the standard n-gram lan-
guage models; ?(f |e) is the cost for the standard
translation models, including general blocks. We
separate the costs for normal blocks and the general-
ized rules explicitly here, because the blocks contain
stronger lexical evidences observed directly from
data, and we assign them with less cost penalties
via a different weight factor visible for optimization,
and prefer the lexical match over the derived paths
during the decoding.
Our decoder is a chart-based parser with beam-
search for each cell in a chart. Because the tree-
structure can have more than two children, there-
fore, the Tree-to-String rules extracted usually con-
tain more than two variables. Slightly different from
the decoder in (Chiang, 2005), we implemented
the dotted-rule in Early-style parser to handle rules
containing more than two variables. Our cube-
expansion, implemented the cube-pruning in Chiang
(2007), and integrated piece-wise cost computations
for language models via LM states. The intermedi-
ate hypotheses were merged (recombined) accord-
ing to their LM states and other cost model states.
We use MER (Och, 2003) to tune the decoder?s pa-
rameters using a development data set.
Figure 2 shows an example of a tree-based rule
fired at the subtree of VP covering the highlighted
cell. When a rule is applied at a certain cell in the
chart, the covered source ngram should match not
only the lexical items in the rules, but also the tree-
structures as well. The two children under the sub-
tree root VP are PP (?????: in the local) and VP
(????????: triggered a huge shock ). This
rule triggered a swap of these children to generate
the correct word order in the translation: ?triggered
a huge shock in the local?.
576
5 Experiments
Our training data consists of two corpora: the GALE
Chinese-English parallel corpus and the LDC hand-
aligned corpus1. The Chinese side of these two cor-
pora were parsed using a constituency parser (Luo,
2003). The average labeled F-measure of the parser
is 81.4%.
Parallel sentences were first word-aligned using
a MaxEnt aligner (Ittycheriah and Roukos, 2005).
Then, phrase-pairs that overlap with our develop-
ment and test set were extracted from the word
alignments (from both hand alignments and auto-
matically aligned GALE corpora) based on the pro-
jection principle (Tillmann, 2003). Besides the regu-
lar phrase-pairs, we also extracted the Tree-to-String
rules from the two corpora. The detailed statistics
are shown in Table 1. Our re-implementation of Hi-
ero system is the baseline. We integrated the eleven
reordering rules described in (Wang et al, 2007),
in our chart-based decoder. In addition, we report
the results of using the Tree-to-String rules extracted
from the hand-aligned training data and the automat-
ically aligned training data. We also report the result
of our translation quality in terms of both BLEU (Pa-
pineni et al, 2002) and TER (Snover et al, 2006)
against four human reference translations.
5.1 The Data
Table 1 shows the statistics of our training, develop-
ment and test data. As our word aligner (Ittycheriah
and Roukos, 2005) can introduce errors in extracting
Tree-to-String rules, we use a small hand-aligned
data set ?CE16K?, which consists of 16K sentence-
pairs, to get relatively clean rules, free from align-
ment errors. A much larger GALE data set, which
consists of 10 million sentence-pairs, is used to in-
vestigate the scalability of our proposed approach.
Table 1: Training and Test Data
Train/test sentences src words tgt words
CE16K 16379 380103 477801
GALE 10.5M 274M 310M
MT03 919 24099 -
Dev07 2303 61881 -
1LDC2006E93
The NIST 2003 MT Evaluation (MT03) is used
as our development data set to tune the decoder?s
parameters toward better BLEU score. The text part
of GALE 2007 Chinese-to-English Development set
(GALE DEV07) is used as our test set. MT03 con-
sists of 919 sentences, whereas GALE DEV07 con-
sists of 2303 sentences under two genres: NewsWire
and WebLog. Both have four human reference trans-
lations.
5.2 Details of Extracted Rules
From the hand-aligned data, the rules we extracted
fall into three categories: regular blocks (phrase-
pairs), Hiero-like rules, and Tree-to-String rules.
The statistics of the extracted rules are shown in Ta-
ble 2
Table 2: Rules extracted from hand-aligned data
Types Frequency
Block 846965
Hiero 508999
Tree-to-String 409767
Total 1765731
We focus on Tree-to-String rules. Table 3 shows
the detailed statistics of the Tree-to-String rules ex-
tracted from the Chinese-to-English hand-aligned
training data. The following section provides a de-
tailed analysis of the most frequent subtrees ob-
served in our training data.
5.2.1 Frequent Subtrees: NP, VP, and DNP
The majority of Tree-to-String rules we extracted
are rooted at the following labels: NP (46%),
VP(22.8%), DNP (2.23%), and QP(2.94%).
Wang et al (2007) covers only subtrees of NP,
VP, and LCP, which are a subset of our proposed
Tree-to-String rules here. They apply these rules as
a pre-processing step to reorder the input sentences
with hard decisions. Our proposed Tree-to-String
rules, on the contrary, are applied during the de-
coding process which allows for considering many
possible competing reordering options for the given
sentences, and the decoder will choose the best one
according to the cost functions.
Table 4 shows the statistics of reordering rules
for subtrees rooted at VP. The statistics suggest that
577
Table 5: Hiero, Tree-Based (eleven rules in Wang et al (2007)), and Tree-to-String Rules with ?DE?
Ruleset Root Src Tgt Frequency
Hiero
NULL [X,0] ? [X,1] [X,0] ?s [X,1] 347
NULL [X,0] ? [X,1] [X,1] of [X,0] 306
NULL [X,0] ? [X,1] [X,0] of [X,1] 174
Tree-Based
NP DNP(NP) NP NP DNP(NP) -
NP DNP(PP) NP NP DNP(PP) -
NP DNP(LCP) NP NP DNP(LCP) -
Tree-to-String
[X,DNP] [X,NP] [X,DEG] [X,NP] [X,DEG] 580
[X,DNP] [X,NP] [X,DEG] [X,DEG] [X,NP] 2163
[X,DNP] [X,NP] [X,DEG] [X,NP] , [X,DEG] 4
Table 3: Distributions of the NP, VP, QP, LCP rules
Root Frequency Percentage (%)
NP 189616 46.2
VP 93535 22.8
IP 68341 16.6
PP 18519 4.51
DNP 9141 2.23
QP 12064 2.94
LCP 4127 1.00
CP 2994 0.73
PRN 2810 0.68
DP 1415 0.34
Others 6879 1.67
Total 409767 -
Table 4: Distribution of the reordering rules for subtrees
rooted at VP: [X,VP]; [X,PP] [X,VP]; statistics are col-
lected from GALE training data
Root Target Frequency
VP
[X,PP] [X,VP] 126310
[X,VP] [X,PP] 22144
[X,PP] , [X,VP] 1524
[X,PP] that [X,VP] 1098
[X,PP] and [X,VP] 831
it is impossible to come up with a reordering rule
that is always applicable. For instance, (Wang et
al., 2007) will always swap the children of the sub-
tree VP(PP,VP). However, the statistics shown in Ta-
ble 4 suggest that might not be best way. In fact,
due to parser?s performance and word alignment ac-
curacies, the statistics we collected from the GALE
dataset, containing 10 million sentence-pairs, show
that the children in the subtree VP(PP,VP) is trans-
lated monotonically 126310 times, while reordered
of only 22144 times. However, the hand-aligned
data support the swap for 1245 times, and monotoni-
cally for only 168 times. Part of this disagreement is
due to the word segmentation errors, incorrect word
alignments and unreliable parsing results.
Another observations through our extracted Tree-
to-String rules is on the controlled insertion of the
target spontaneous2 (function) words. Instead of hy-
pothesizing spontaneous words based only on the
language model or only on observing in phrase-
pairs, we make use of the Tree-to-String rules to get
suggestion on the insertion of spontaneous words.
In this way, we can make sure that the spontaneous
words are generated from the structure information,
as opposed to those from a pure hypothesis. The ad-
vantage of this method is shown in Table 4. For in-
stance, the word ?that? and the punctuation ?,? were
generated in the target side of the rule. This proves
that our model can provide a more principled way to
generate spontaneous words needed for fluent trans-
lations.
5.2.2 DEG and DEC
An interesting linguistic phenomenon that we in-
vestigated is the Chinese word DE ???. ??? is an
informative lexical clue that indicates the need for
long range phrasal movements. Table 5 shows a few
2Target spontaneous words are function words that do not
have specific lexical source informants and are needed to make
the target translation fluent.
578
high-frequent reordering rules that contain the Chi-
nese word ?DE?.
The three type of rules handle ?DE? differently. A
major difference is the structure in the source side.
Hiero rules do not consider any structure, and ap-
ply the rule of ?[X,0] ? [X,1]?. Tree-based rules,
as described in Wang et al (2007) do not handle
? directly; they are often implicitly taken care of
when reordering DNPs instead. Our proposed Tree-
to-String rules model ? directly in a subtree con-
taining DEG/DEC, which triggers word reordering
within the structure. Our rule set includes all the
above three rule-types with the associated frequen-
cies, this enriched the reordering choices to be cho-
sen by the chart-based decoder, guided by the statis-
tics collected from the data and the language model
costs.
5.3 Evaluation
We tuned the decoding parameters using the MT03
data set, and applied the updated parameters to the
GALE evaluation set. The eleven rules of VP, NP,
and LCP (tree-based) improved the Hiero baseline3
from 32.43 to 33.02 on BLEU. The reason, the tree-
reordering does not gain much over Hiero baseline,
is probably that the reordering patterns covered by
tree-reordering rules, are potentially handled in the
standard Hiero grammar.
A small but noticeable further improvement over
tree-based rules, from 33.02 to 33.26, was ob-
tained on applying Tree-to-String rules extracted
from hand-aligned dataset. We think that the Tree-
based rules covers major reordering patterns for
Chinese-English, and our hand-aligned dataset is
also too small to capture representative statistics and
more reordering patterns. A close check at the rules
we learned from the hand-aligned data shows that
the tree-based rules are simply the subset of the
rules extracted. The Tree-to-String grammar im-
proved the Hiero baseline from 32.43 to 33.26 on
BLEU; considering the effects from the tree-based
rules only, the additional information improved the
BLEU scores from 33.02 to 33.26. Similar pictures
of improvements were observed for the two unseen
tests of newswire and weblog in GALE data.
When applying the rules extracted from the much
3Hiero results are from our own re-implementation.
larger GALE training set with about ten million
sentence-pairs, we achieved significant improve-
ments from both genres (newswire and web data).
The improvements are significant in both BLEU
and TER. BLEU improved from 32.44 to 33.51 on
newswire, and from 25.88 to 27.91 on web data.
Similar improvements were found in TER as shown
in the table. The gain came mostly from the richer
extracted rule set, which not only presents robust
statistics for reordering patterns, but also offers more
target spontaneous words generated from the syntac-
tic structures. Since the top-frequent rules extracted
are NP, VP, and IP as shown in Table 3, our proposed
rules will be able to win the correct word order with
reliable statistics, as long as the parser shows accept-
able performances on these structures. This is espe-
cially important for weblog data, where the parser?s
overall accuracy potentially might not be very good.
Table 7 shows the translations from different
grammars for the same source sentence. Both Tree-
based and Tree-to-String methods get the correct re-
ordering, while the latter can suggest insertions of
target spontaneous words like ?a? to allow the trans-
lation to run more fluently.
6 Conclusion and Discussions
In this paper, we proposed our approach to model
both local and non-local word-reordering in one
probabilistic synchronous CFG. Our current model
incorporates source-side syntactic information, to
model the observations that the source syntactic con-
stituent tends to move together during translations.
The proposed rule set generalizes over the variables
in Hiero-rules, and we also showed the special cases
of the Tree-based rules and the POS-based rules.
Since the proposed rules has at most one-level tree
structure, they can be easily applied in a chart-based
decoder. We analyzed the statistics of our rules,
qualitatively and quantitatively. Next, we compared
our work with other research, especially with the
work in Wang et al (2007). Finally, we reported
our empirical results on Chinese-English transla-
tions. Our Tree-to-String rules showed significant
improvements over the Hiero baseline on the GALE
DEV07 test set.
Given the low accuracy of the parsers, and the po-
tential errors from Chinese word-segmentations, and
579
Table 6: Hiero, Tree-Based (NP, VP, LCP), and Tree-to-String rules extracted from hand-aligned data (H) or from
GALE training data (G)
Setup MT03 GALE07-NewsWire GALE07-WeblogBLEUr4n4 TER BLEUr4n4 TER BLEUr4n4 TER
Hiero 32.43 59.75 31.68 61.45 25.99 65.65
Tree-based 33.02 59.84 32.22 61.46 25.67 65.64
Tree-to-String (H) 33.26 61.04 32.44 61.36 25.88 65.54
Tree-to-String (G) 35.51 57.28 33.51 59.71 27.91 62.88
Table 7: Hiero, Tree-Based (NP, VP, LCP), Tree-to-String Translations
Src-Sent ????????????
Hiero in this case local triggered shock .
Tree-Based the case triggered uproar in the local.
Tree-to-String the case triggered a huge uproar in the local .
word-alignments, our rules learned are still noisy.
Exploring better cost functions associate each rule
might lead to further improvement. Because of
the relative high accuracy of English parsers, many
works such as Zollmann and Venugopal (2006) and
Shen et al (2008) emphasize on using syntax in tar-
get languages, to directly influence the fluency as-
pect of the translation output. In future, we plan to
incorporate features from target-side syntactic infor-
mation, and connect them with the source informa-
tion explored in this paper, to model long-distance
reordering for better translation quality.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their comments to improve this pa-
per. This work was supported by DARPA GALE
program under the contract number HR0011-06-2-
0001.
References
Yaser Al-Onaizan and Papineni. Kishore. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of ACL-COLING, pages 529?536.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. In Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. In Communications of the ACM., volume 13,
pages 94?102.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing,
pages 304?311, Philadelphia, PA, July 6-7.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. NAACL-HLT.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT/EMNLP.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In HLT/EMNLP 2005, Vancouver, B.C., Canada.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In ACL-Coling.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In 45th
Annual Meeting of the Association for Computational
Linguistics.
Xiaoqiang Luo. 2003. A maximum entropy chinese
character-based parser. In Proc. of ACL.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In ACL.
580
Sonja Niessen and Hermann Ney. 2004. Statistical
machine translation with scarce resources using mor-
phosyntactic information. In Computational Linguis-
tics.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
In Computational Linguistics, volume 30, pages 417?
449.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics, Japan, Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pages 311?318, Philadelphia, PA,
July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
557?564, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Christoph Tillmann. 2008. A rule-driven dynamic pro-
gramming decoder for statistical mt. In HLT Second
Workshop on Syntax and Structure in Statistical Trans-
lation.
Chao Wang, Michael Collins, and Phillip Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. In
Computational Linguistics, volume 23(3), pages 377?
403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In the 20th International Conference on
Computational Linguistics (COLING 2004), Geneva,
Switzerland, Aug 22-29.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In ACL-Coling.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
K. Yamada and Kevin. Knight. 2001. Syntax-based Sta-
tistical Translation Model. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL-2001).
Richard Zens, E. Matusov, and Hermmann Ney. 2004.
Improved word alignment using a symmetric lexicon
model. In Proceedings of the 20th International Con-
ference on Computational Linguistics (CoLing 2004),
pages 36?42, Geneva, Switzerland, Auguest.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the HLT-NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. of NAACL 2006 - Workshop on statistical ma-
chine translation.
581
Proceedings of NAACL HLT 2009: Short Papers, pages 21?24,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Simplex Armijo Downhill Algorithm for
Optimizing Statistical Machine Translation Decoding Parameters
Bing Zhao
IBM T.J. Watson Research
zhaob@us.ibm.com
Shengyuan Chen
IBM T.J. Watson Research
sychen@us.ibm.com
Abstract
We propose a variation of simplex-downhill algo-
rithm specifically customized for optimizing param-
eters in statistical machine translation (SMT) de-
coder for better end-user automatic evaluation met-
ric scores for translations, such as versions of BLEU,
TER and mixtures of them. Traditional simplex-
downhill has the advantage of derivative-free com-
putations of objective functions, yet still gives satis-
factory searching directions in most scenarios. This
is suitable for optimizing translation metrics as they
are not differentiable in nature. On the other hand,
Armijo algorithm usually performs line search ef-
ficiently given a searching direction. It is a deep
hidden fact that an efficient line search method
will change the iterations of simplex, and hence
the searching trajectories. We propose to embed
the Armijo inexact line search within the simplex-
downhill algorithm. We show, in our experiments,
the proposed algorithm improves over the widely-
applied Minimum Error Rate training algorithm for
optimizing machine translation parameters.
1 Introduction
A simple log-linear form is used in SMT systems to
combine feature functions designed for identifying good
translations, with proper weights. However, we often ob-
serve that tuning the weight associated with each feature
function is indeed not easy. Starting from a N-Best list
generated from a translation decoder, an optimizer, such
as Minimum Error Rate (MER) (Och, 2003) training, pro-
poses directions to search for a better weight-vector ? to
combine feature functions. With a given ?, the N-Best
list is re-ranked, and newly selected top-1 hypothesis will
be used to compute the final MT evaluation metric score.
Due to limited variations in the N-Best list, the nature of
ranking, and more importantly, the non-differentiable ob-
jective functions used for MT (such as BLEU (Papineni et
al., 2002)), one often found only local optimal solutions
to ?, with no clue to walk out of the riddles.
Automatic evaluation metrics of translations known so
far are designed to simulate human judgments of trans-
lation qualities especially in the aspects of fluency and
adequacy; they are not differentiable in nature. Simplex-
downhill algorithm (Nelder and Mead, 1965) does not
require the objective function to be differentiable, and
this is well-suited for optimizing such automatic met-
rics. MER searches each dimension independently in a
greedy fashion, while simplex algorithms consider the
movement of all the dimensions at the same time via
three basic operations: reflection, expansion and contrac-
tion, to shrink the simplex iteratively to some local op-
timal. Practically, as also shown in our experiments, we
observe simplex-downhill usually gives better solutions
over MER with random restarts for both, and reaches
the solutions much faster in most of the cases. How-
ever, simplex-downhill algorithm is an unconstrained al-
gorithm, which does not leverage any domain knowledge
in machine translation. Indeed, the objective function
used in SMT is shown to be a piece-wise linear prob-
lem in (Papineni et al, 1998), and this motivated us to
embed an inexact line search with Armijo rules (Armijo,
1966) within a simplex to guide the directions for itera-
tive expansion, reflection and contraction operations. Our
proposed modification to the simplex algorithm is an em-
bedded backtracking line search, and the algorithm?s con-
vergence (McKinnon, 1999) still holds, though it is con-
figured specially here for optimizing automatic machine
translation evaluation metrics.
The remainder of the paper is structured as follow: we
briefly introduce the optimization problem in section 2;
in section 3, our proposed simplex Armijo downhill al-
gorithm is explained in details; experiments comparing
relevant algorithms are in section 4; the conclusions and
discussions are given in section 5.
2 Notations
Let {(ei,k, c?i,k, Si,k), k ? [1,K]} be the K-Best list
for a given input source sentence fi in a development
dataset containing N sentences. ei,k is a English hy-
pothesis at the rank of k; c?i,k is a cost vector ? a
vector of feature function values, with M dimensions:
c?i,k = (ci,k,1, ci,k,2 . . . ci,k,M ); Si,k is a sentence-level
translation metric general counter (e.g. ngram hits for
BLEU, or specific types of errors counted in TER, etc.)
for the hypothesis. Let ?? be the weight-vector, so that the
cost of ei,k is an inner product: C(ei,k) = ?? ? c?i,k. The
optimization process is then defined as below:
k?(wrt i) = argmin
k
?? ? c?i,k (1)
??? = argmin
??
Eval(
N?
i=1
Si,k?), (2)
21
where Eval is an evaluation Error metric for MT, presum-
ing the smaller the better internal to an optimizer; in our
case, we decompose BLEU, TER (Snover et al, 2006)
and (TER-BLEU)/2.0 into corresponding specific coun-
ters for each sentence, cache the intermediate counts in
Si,k, and compute final corpus-level scores using the sum
of all counters; Eqn. 1 is simply a ranking process, with
regard to the source sentence i, to select the top-1 hypoth-
esis, indexed by k? with the lowest cost C(ei,k?) given
current ??; Eqn. 2 is a scoring process of computing the fi-
nal corpus-level MT metrics via the intermediate counters
collected from each top1 hypothesis selected in Eqn. 1.
Iteratively, the optimizer picks up an initial guess of ??
using current K-Best list, and reaches a solution ???, and
then updates the event space with new K-Best list gener-
ated using a decoder with ???; it iterates until there is little
change to final scores (a local optimal ??? is reached).
3 Simplex Armijo Downhill
We integrate the Armijo line search into the simplex-
downhill algorithm in Algorithm 1. We take the reflec-
tion, expansion and contractions steps1 from the simplex-
downhill algorithm to find a ?? to form a direction ?? ?
?M+1 as the input to the Armijo algorithm, which in
turn updates ?? to ?+ as the input for the next iteration
of simplex-downhill algorithm. The combined algorithm
iterates until the simplex shrink sufficiently within a pre-
defined threshold. Via Armijo algorithm, we avoid the
expensive shrink step, and slightly speed up the search-
ing process of simplex-downhill algorithm. Also, the
simplex-downhill algorithm usually provides a descend
direction to start the Armijo algorithm efficiently. Both
algorithms are well known to converge. Moreover, the
new algorithm changes the searching path of the tradi-
tional simplex-downhill algorithm, and usually leads to
better local minimal solutions.
To be more specific, Algorithm 1 clearly conducts an
iterative search in the while loop from line 3 to line 28
until the stopping criteria on line 3 is satisfied. Within
the loop, the algorithm can be logically divided into two
major parts: from line 4 to line 24, it does the simplex-
downhill algorithm; the rest does the Armijo search. The
simplex-downhill algorithm looks for a lower point by
trying the reflection (line 6), expansion (line 10) and con-
traction (line 17) points in the order showed in the al-
gorithm, which turned out to be very efficient. In rare
cases, especially for many dimensions (for instance, 10
to 30 dimensions, as in typical statistical machine trans-
lation decoders) none of these three points are not lower
enough (line 21), we adapt other means to select lower
points. We avoid the traditional expensive shrink pro-
1These three basic operations are generally based on heuristics in
the traditional simplex-downhill algorithm.
Algorithm 1 Simplex Armijo Downhill Algorithm
1: ? ? 1, ? ? 2, ? ? 0.5, ? = ? ? 0.9, ? ? 1.0 ?
10?6
2: initilize (?1, ? ? ? , ?M+1)
3: while ?M+1i,j=1 ??i ? ?j?2 ? ? do
4: sort ?i ascend
5: ?o ? 1N
?M
i=1 ?i,
6: ?r ? ?o + ?(?o ? ?M+1)
7: if S(?1) ? S(?r) ? S(?M ) then
8: ?? ? ?r
9: else if S(?r) < S(?1) then
10: ?e ? ?o + ?(?o ? ?M+1)
11: if S(?e) < S(?r) then
12: ?? ? ?e
13: else
14: ?? ? ?r
15: end if
16: else if S(?r) > S(?M ) then
17: ?c ? ?M+1 + ?(?o ? ?M+1)
18: if S(?c) < S(?r) then
19: ?? ? ?c
20: else
21: try points on two additional lines for ??
22: end if
23: end if
24: d ? ?? ? ?M+1
25: ?? ? maxk=0,1,??? ,40{?k|S(?M+1 + ?kd) ?
S(?M+1) ? ???d?2?k}
26: ?+ = ?M+1 + ?? ? d
27: replace ?M+1 with ?+
28: end while
cedure, which is not favorable for our machine transla-
tion problem neither. Instead we try points on different
search lines. Specifically, we test two additional points
on the line through the highest point and the lowest point,
and on the line through the reflection point and the low-
est point. It worth pointing out that there are many vari-
ants of simplex-downhill algorithm 2, and the implemen-
tation described above showed that the algorithm can suc-
cessfully select a lower ?? in many of our translation test
cases to enable the simplex move to a better region of lo-
cal optimals in the high-dimension space. Our proposed
embedded Armijo algorithm, in the second part of the
loop (line 25), continues to refine the search processes.
By backtracking on the segment from ?? to ?M+1, the
Armijo algorithm does bring even lower points in our
many test cases. With the new lower ?? found by the
Armijo algorithm, the simplex-downhill algorithm starts
over again. The parameters in line 1 we used are com-
2One of such effective tricks for the baseline simplex algorithms
can be found here: http://paula.univ.gda.pl/?dokgrk/simplex.html (link
tested to be valid as of 04/03/2009)
22
Optimizing (1-TER)
0.410.412
0.4140.416
0.4180.42
0.4220.424
0.4260.428
0.43
1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163Random Seeds
(1-TE
R)
MERSimplexSimplex-Arm
(a) Optimizing Toward Metric (1-TER)
Optimizing IBM BLEU
0.375
0.377
0.379
0.381
0.383
0.385
0.387
0.389
0.391
0.393
1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163Random Seeds
IBM B
LEU
MERSIMPLEXSIMPLEX-ARM
(b) Optimizing Toward Metric IBM BLEUOptimizing NIST-BLEU
0.388
0.393
0.398
0.403
0.408
0.413
1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163Random Seeds
NIST
-BLE
U
MERSimplex-ArmSimplex
(c) Optimizing Toward Metric NIST BLEU
Optimizing (1-(TER-BLEU))/2.0
0.4
0.402
0.404
0.406
0.408
0.41
0.412
0.414
0.416
0.418
1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163Random Seeds
(1-(TE
R-BL
EU))/
2.0
MERSimplex-ArmijoSimplex
(d) Optimizing Toward Metric (1-(TER-NISTBLEU))/2
Figure 1: On devset, comparing MER, Simplex Downhill, and Simplex Armijo Downhill Algorithms on different Translation
Metrics including TER, IBM BLEU, NIST BLEU, and the combination of TER & NISTBLEU. Empirically, we found optimizing
toward (TER-NISTBLEU)/2 gave more reliable solutions on unseen test data. All optimizations are with internal random restarts,
and were run from the same 164 random seeds with multiple iterations until convergence. Simplex Armijo downhill algorithm is
often better than Simplex-downhill algorithm, and is also much better than MER algorithm.
mon ones from literatures and can be tuned further. We
find that the combination not only accelerates the search-
ing process to reach similar solutions to the baseline sim-
plex algorithm, but also changes the searching trajectory
significantly, leading to even better solutions for machine
translation test cases as shown in our experiments.
4 Experiments
Our experiments were carried out on Chinese-English
using our syntax-based decoder (Zhao and Al-Onaizan,
2008), a chart-based decoder with tree-to-string 3 gram-
mar, in GALE P3/P3.5 evaluations. There were 10 fea-
ture functions computed for each hypothesis, and N-best
list size is up to 2,000 per sentence.
Given a weight-vector ??0, our decoder outputs N-Best
unique hypotheses for each input source sentence; the
event space is then built, and the optimizer is called with
3Source shallow constituency tree to target-string rules with vari-
ables, forming a probabilistic synchronous context free grammar.
a number of random restarts. We used 164 seeds4 with
a small perturbation of three random dimensions in ??0.
The best ??1 is selected under a given optimizing metric,
and is fed back to the decoder to re-generate a new N-Best
list. Event space is enriched by merging the newly gen-
erated N-Best list, and the optimization runs again. This
process is iteratively carried out until there are no more
improvements observed on a development data set.
We select three different metrics: NIST BLEU, IBM
BLEU, TER, and a combination of (TER-NISTBLEU)/2
as our optimization goal. On the devset with four refer-
ences using MT06-NIST text part data, we carried out the
optimizations as shown in Figure 1. Over these 164 ran-
dom restarts in each of the optimizers over the four con-
figurations shown in Figure 1, we found most of the time
simplex algorithms perform better than MER in these
configurations. Simplex algorithm considers to move all
the dimensions at the same time, instead of fixing other
4There are 41 servers used in our experiments, four CPUs each.
23
Table 1: Comparing different optimization algorithms on the held-out speech data, measured on document-average TER, IBMBLEU
and (TER-IBMBLEU)/2.0, which were used in GALE P3/3.5 Chinese-English evaluations in Rosetta consortium.
Setup Broadcast News & Conversation DataBLEUr4n4 TER (TER-BLEUr4n4)/2
MER 37.36 51.12 6.88
Simplex-Downhill 37.71 50.10 6.19
Simplex Armijo Downhill 38.15 49.92 5.89
dimensions and carrying out a greedy search for one di-
mension as in MER. With Armijo line search embedded
in the simplex-downhill algorithm, the algorithm has a
better chance to walk out of the local optimal, via chang-
ing the shrinking trajectory of the simplex using a line
search to identify the best steps to move. Shown in Fig-
ure 1, the solutions from simplex Armijo downhill out-
performed the other two under four different optimiza-
tion metrics for most of the time. Empirically, we found
optimizing toward (TER-NISTBLEU)/2 gives marginally
better results on final TER and IBM BLEU.
On our devset, we also observed that whenever opti-
mizing toward TER (or mixture of TER & BLEU), MER
does not seem to move much, as shown in Figure 1-(a)
and Figure 1-(d). However, on BLEU (NIST or IBM ver-
sion), MER does move reasonably with random restarts.
Comparing TER with BLEU, we think the ?shift? counter
in TER is a confusing factor to the optimizer, and cannot
be computed accurately in the current TER implementa-
tions. Also, our random perturbations to the seeds used
in restarts might be relatively weaker for MER compar-
ing to our simplex algorithms, though they use exactly the
same random seeds. Another fact we found is optimizing
toward corpus-level (TER-NISTBLEU)/2 seems to give
better performances on most of our unseen datasets, and
we choose this as optimization goal to illustrate the algo-
rithms? performances on our unseen testset.
Our test set is the held-out speech part data5. We
optimize toward corpus-level (TER-NISTBLEU)/2 using
devset, and apply the weight-vector on testset to evalu-
ate TER, IBMBLEUr4n4, and a simple combination of
(TER-IBMBLEU)/2.0 to compare different algorithms?
strengths6. Shown in Table 1, simplex Armijo downhill
performs the best (though not statistically significant),
and the improvements are consistent in multiple runs in
our observations. Also, given limited resources, such as
number of machines and fixed time schedule, both sim-
plex algorithms can run with more random restarts than
MER, and can potentially reach better solutions.
5Transcriptions of broadcast news and broadcast conversion in
MT06; there are 565 sentences, or 11,691 words after segmentation.
6We choose document-average metrics to show here simply because
they were chosen/required in our GALE P3/P3.5 evaluations for both
Arabic-English and Chinese-English individual systems and syscombs.
5 Conclusions and Discussions
We proposed a simplex Armijo downhill algorithm
for improved optimization solutions over the standard
simplex-downhill and the widely-applied MER. The
Armijo algorithm changes the trajectories for the simplex
to shrink to a local optimal, and empowers the algorithm a
better chance to walk out of the riddled error surface com-
puted by automatic MT evaluation metrics. We showed,
empirically, such utilities under several evaluation met-
rics including BLEU, TER, and a mixture of them. In the
future, we plan to integrate domain specific heuristics via
approximated derivatives of evaluation metrics or mix-
ture of them to guide the optimizers move toward better
solutions for simplex-downhill algorithms.
References
L. Armijo. 1966. Minimization of functions having lipschitz
continuous first partial derivatives. Pacific Journal of mathe-
matics, 6:1?3.
K.I.M. McKinnon. 1999. Convergence of the nelder-mead sim-
plex method to a non-stationary point. SIAM J Optimization,
9:148?158.
J.A. Nelder and R. Mead. 1965. A simplex method for function
minimization. The Computer Journal, 7:308?313.
Franz J. Och. 2003. Minimum error rate training for statistical
machine translation. In Proc. of the 41st Annual Meeting of
the Association for Computational Linguistics, Japan, Sap-
poro, July.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maxi-
mum likelihood and discriminative training of direct transla-
tion models. In Proceedings of the 1998 IEEE International
Conference on Acoustics, Speech & Signal Processing, vol-
ume 1, pages 189?192, Seattle, May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In AMTA.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing local
and non-local word-reordering patterns for syntax-based ma-
chine translation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).
24
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 626?634,
Beijing, August 2010
Constituent Reordering and Syntax Models for English-to-
Japanese Statistical Machine Translation
Young-Suk Lee
IBM Research
ysuklee@us.ibm.com
Bing Zhao
IBM Research
zhaob@us.ibm.com
Xiaoqiang Luo
IBM Research
xiaoluo@us.ibm.com
Abstract
We present a constituent parsing-based
reordering technique that improves the
performance of the state-of-the-art Eng-
lish-to-Japanese phrase translation sys-
tem that includes distortion models by
4.76 BLEU points. The phrase transla-
tion model with reordering applied at the
pre-processing stage outperforms a syn-
tax-based translation system that incor-
porates a phrase translation model, a hi-
erarchical phrase-based translation
model and a tree-to-string grammar. We
also show that combining constituent re-
ordering and  the syntax model improves
the translation quality by additional  0.84
BLEU points.
1 Introduction
Since the seminal work by (Wu, 1997) and (Ya-
mada and Knight, 2001), there have been great
advances in syntax-based statistical machine
translation to accurately model the word order
distortion between the source and the target lan-
guages.
Compared with the IBM source-channel mod-
els (Brown et al, 1994) and the phrase transla-
tion models (Koehn et al, 2003), (Och and Ney,
2004) which are good at capturing local reorder-
ing within empirical phrases, syntax-based mod-
els have been effective in  capturing the long-
range reordering between language pairs with
very different word orders like Japanese-English
(Yamada and Knight, 2001), Chinese-English
(Chiang, 2005) and Urdu-English (Zollmann et
al. 2008), (Callison-Burch et al 2010).
 However, (Xu et al, 2009) show that apply-
ing dependency parsing-based reordering as pre-
processing (pre-ordering hereafter) to phrase
translation models produces translation qualities
significantly better than a hierarchical phrase-
based  translation model (Hiero hereafter) im-
plemented in (Zollman and Venugopal, 2006)
for English-to-Japanese translation. They also
report that the two models result in comparable
translation qualities for English-to-
Korean/Hindi/Turkish/Urdu, underpinning the
limitations of syntax-based models for handling
long-range reordering exhibited by the strictly
head-final Subject-Object-Verb (SOV) order
languages like Japanese and the largely head-
initial Subject-Verb-Object (SVO) order lan-
guages like English.
In this paper,  we present a novel constituent
parsing-based reordering technique that uses
manually written context free (CFG hereafter)
and context sensitive grammar (CSG hereafter)
rules. The technique improves the performance
of the state-of-the-art English-to-Japanese
phrase translation system that includes distortion
models by 4.76 BLEU points. The phrase trans-
lation model with constituent pre-ordering con-
sistently outperforms a syntax-based translation
system that integrates features from a phrase
translation model, Hiero and a tree-to-string
grammar. We also achieve an additional 0.84
BLEU point improvement by  applying an ex-
tended set of  reordering rules that incorporate
new rules learned from the syntax model for
decoding.
The rest of the paper is organized as follows.
In Section 2, we summarize  previous work re-
lated to this paper. In Section 3, we give an
overview of the syntax model with which we
compare the performance of a phrase translation
626
model with pre-ordering. We also discuss a
chart-based decoder used in all of our experi-
ments. In Section 4, we describe the constituent
parsing-based reordering rules. We show the
impact of pre-ordering on a phrase translation
model and compare its performance with the
syntax model. In Section 5, we discuss experi-
mental results from the combination of syntax
model and pre-ordering.  Finally in Section 6,
we discuss future work.
2 Related Work
Along the traditions of unsupervised learning by
(Wu, 1997), (Chiang, 2005) presents a model
that uses hierarchical phrases, Hiero.   The
model is a synchronous context free grammar
learned from a parallel corpus without any lin-
guistic annotations and is applied to Chinese-to-
English translation. (Galley and Manning, 2008)
propose a hierarchical phrase reordering model
that uses shift-reduce parsing.
In line with the syntax-based model of (Ya-
mada and Knight, 2001) that transforms a source
language parse tree into a target language string
for Japanese-English translation, linguistically
motivated syntactic features have been directly
incorporated into both modeling and decoding.
(Liu, et. al. 2006), (Zhao and Al-Onaizan, 2008)
propose a  source tree to target string grammar
(tree-to-string grammar hereafter) in order to
utilize the source language parsing information
for translation. (Liu, et. al. 2007) propose
packed forest to allow ambiguities in the source
structure for the tree-to-string grammar.  (Ding
and Palmer, 2005) and (Zhang et. al., 2006) pro-
pose a tree-to-tree grammar, which generates the
target tree structure from the high-precision
source syntax.  (Shen, et. al., 2008) propose a
string to dependency tree grammar to use the
target syntax when the target is English for
which parsing is more accurate than other lan-
guages.  (Marcu et al, 2006) introduce a syntax
model that uses syntactified target language
phrases. (Chang and Toutanova, 2007) propose a
global discriminative statistical word order
model that combines syntactic and surface
movement information, which improves  the
translation quality by 2.4 BLEU points in Eng-
lish-to-Japanese translation. (Zollmann, et. al.,
2008) compare various translation models and
report that the syntax augmented model works
better for Chinese-to-English and Urdu-to-
English, but not for Arabic-to-English transla-
tion. (Carreras and Collins, 2009) propose a
highly flexible reordering operations during tree
adjoining grammar parsing for German-English
translation. (Callison-Burch et al, 2010) report a
dramatic impact of syntactic translation models
on Urdu-to-English translation.
Besides the approaches which integrate  the
syntactic features into translation models, there
are approaches showing improvements via pre-
ordering for model training and decoding. (Xia
and McCord, 2004), (Collins et al, 2005) and
(Wang, et. al. 2007) apply pre-ordering to the
training data according to language-pair specific
reordering rules to improve the translation quali-
ties of French-English, German-English and
Chinese-English, respectively. (Habash, 2007)
uses syntactic preprocessing for Arabic-to-
English translation. (Xu et al, 2009) use a de-
pendency parsing-based pre-ordering to improve
translation qualities of English to five SOV lan-
guages including Japanese.
The current work is related to (Xu et al,
2009) in terms of the language pair and transla-
tion models explored. However, we use con-
stituent parsing with hierarchical rules, while
(Xu et al, 2009) use dependency parsing with
precedence rules. The two approaches have dif-
ferent rule coverage and result in different word
orders especially for phrases headed by verbs
and prepositions. We also present techniques for
combining the syntax model with tree-to-string
grammar and pre-ordering for additional per-
formance improvement. The total  improvement
by the current techniques over the state-of-the-
art phrase translation model is  5.6 BLEU points,
which is an improvement gap not attested else-
where with reordering approaches.
3 Syntax Model and Chart-Based De-
coder
In this section, we give an overview of  the syn-
tax model incorporating a tree-to-string gram-
mar.  We will compare  the syntax model per-
formance with  a phrase translation model that
uses the pre-ordering technique we propose in
Section 4. We also describe the chart-based de-
coder that we use in all of the experiments re-
ported in this paper.
627
3.1 Tree-to-String Grammar
Tree-to-string grammar utilizes the source lan-
guage parse to model reordering probabilities
from a source tree to the target string (Liu et. al.,
2006), (Liu et. al., 2007), (Zhao and Al-
Onaizan, 2008) so that long distance word reor-
dering becomes local in the parse tree.
Reordering patterns of the source language
syntax and their probabilities are automatically
learned from the word-aligned source-parsed
parallel data and incorporated as a tree-to-string
grammar for decoding.  Source side parsing and
the resulting reordering patterns bound the
search space. Parsing also assigns linguistic la-
bels to the chunk, e.g. NP-SBJ, and allows sta-
tistics to be clustered reasonably.   Each syn-
chronous context free grammar (SCFG) rewrit-
ing rule rewrites a source treelet into a target
string, with both sides containing hiero-style
variables.  For instance, the rule [X, VP] [X,
VB] [X,NP] ? [X, NP] [X, VB] rewrites a VP
with two constituents VB and NP  into an NP
VB order in the target, shown below.
The tree-to-string grammar introduces possible
search space to generate an accurate word order,
which is refined on the basis of supports from
other models. However, if the correct word or-
der cannot be generated by the tree-to-string
grammar, the system can resort to rules from
Hiero or a phrase translation model for extended
rule coverage.
3.2 Chart-based Decoder
We use a  chart-based decoder ? a template de-
coder that generalizes over various decoding
schemes in terms of the dot-product in Earley-
style parsing (Earley, 1970) ? to support various
decoding schemes such as phrase, Hiero
(Chiang, 2005), Tree-to-String, and the mixture
of all of the above.
This framework allows one to strictly com-
pare different decoding schemes using the same
feature and parameter setups. For the experi-
mental results in Sections 4 & 5, we applied (1)
phrase decoding for the baseline phrase transla-
tion system that includes distortion models, (2)
Hiero decoding for the Hiero system that incor-
porates a phrase translation model, and (3)
Tree-to-string decoding for the syntax-based
systems that incorporate features  from phrase
translation, Hiero and tree-to-string grammar
models.
The decoder seeks the best hypothesis *e  ac-
cording to the Bayesian decision rule (1):
)1()()(minarg*
},{
dee
Dde
?? ??
?
d is one derivation path, rewriting the source
tree into the target string via the probabilistic
synchronous context free tree-to-string grammar
(PSCFG). )(e? is the cost functions computed
from general n-gram language models. In this
work, we use two sets of interpolated 5-gram
language models. )(d? is a vector of cost func-
tions defined on the derivation sequence. We
have integrated  18 cost functions ranging  from
the basic relative frequencies and IBM model-1
scores to counters for different types of rules
including blocks, glue, Hiero, and tree-to-string
grammar rules.  Additional cost functions are
also integrated into the decoder, including meas-
uring the function/content-word mismatch be-
tween source and target, similar to (Chiang et.
al., 2009) and length distribution for non-
terminals in (Shen et. al., 2009).
4 Parsing and Reordering Rules
We apply a set of manually acquired reordering
rules to the parsing output from a constituent
parser to pre-order the data for model training
and decoding.
4.1 Parsing with Functional Tags
We use a maximum entropy English parser (Rat-
naparkhi, 1999) trained on OntoNotes (Hovy,
2006) data. OntoNotes data include most of the
Wall Street Journal data in Penn Treebank
(Marcus et al, 1993) and additional data from
broadcast conversation, broadcast news and web
log.
S
NP-SBJ
X1
X2
VP
VB
X3
NP
X1 X3 X2
Src treelet
Tgt string
628
Figure 1. Parse Tree and Word Alignment before Reordering
Figure 2. Parse Tree and Word Alignment after Reordering
The parser is trained with all of the functional
and part-of-speech (POS)  tags in the original
distribution: total 59 POS tags and 364 phrase
labels.
We use functional tags since reordering de-
cisions for machine translation are highly in-
fluenced by the function of a phrase, as will be
shown later in this section. An example parse
tree with functional tags is given at the top half
of  Figure 1. NP-SBJ indicates a subject noun
phrase, SBAR-ADV, an adverbial clause.
4.2 Structural Divergence between Eng-
lish and Japanese
Japanese is a strictly head-final language, i.e.
the head is located at the end of  a phrase.
This leads to  a high degree of distortions with
English, which is largely head initial.
SBAR-ADV
S
VP
VBN
IN
NP-SBJ
PRP
VP
VP
NP VB
NP VP
DT NNS VBN
PP
NP
DT NN
IN
MD
NN
NP-SBJ
PRP
VP
MD VP
VB NP
NP VP
DT NNS VBN PP
S
IN NP
DT NN
SBAR-ADV
IN S
VP
VBN
you           must       undo   the        changes     made      by        that     installation         if        needed
??? ??? , ?? ?????? ? ?? ?? ? ?? ?? ??? ?? ??
needed if you sbj  the changes that  installation by  made undo     must
S
??? ??? , ?? ?????? ? ?? ?? ? ?? ?? ??? ?? ??
629
The word order contrast between the two
languages is illustrated by the human word
alignment at the bottom half of Figure 1. All
instances of word alignments are non-
monotonic except for the sequence that installa-
tion, which is monotonically aligned to the
Japanese morpheme sequence ??
??????.  Note that there are no word
boundaries in Japanese written text, and we ap-
ply Japanese morpheme segmentation to obtain
morpheme sequences in the figure. All of the
Japanese examples in this paper are presented
with morpheme segmentation.
The manual reordering rules are written by a
person who is proficient with English and Japa-
nese/Korean grammars, mostly on the basis of
perusing parsed English texts.
4.3 CFG Reordering Rules
Our reordering rules are mostly CFG rules and
divided into head and modifier  rules.
Head reordering rules in Table 1 move verbs
and prepositions from the phrase initial to the
phrase final positions (Rules 1-11). Reordering
of the head phrase in an adverbial clause also
belongs to this group (Rules 12-14). The label
sequences in Before RO and After RO are the
immediate children of the Parent Node before
and after reordering. VBX stands for VB, VBZ,
VBP, VBD, VBN and VBG. XP+ stands for one
or more POS and/or phrase labels such as MD,
VBX, NP, PP, VP, etc.  In 2 & 4, RB is  the tag
for negation not. In 5, RP is the tag for a verb
particle.
Modifier reordering rules in Table 2 move
modified phrases from the phrase initial to the
phrase final positions within an NP (Rules 1-3).
They also include placement of NP, PP, ADVP
within a VP (Rules 4 & 5).  The subscripts in a
rule, e.g. PP1 and PP2 in Rule 3, indicate the
distinctness of each phrase sharing the same
label.
4.4 CSG Reordering Rules
Some reordering rules cannot be captured by
CFG rules, and we resort to CSG rules.1
1 These CSG rules apply to trees of depth two or more, and
the applications are dependent on surrounding contexts.
Therefore,  they are different from CFG rules which apply
only to trees of depth one, and TSG (tree substitution
grammar) rules for which variables are independently
substituted by substitution. The readers are referred to
Parent Node Before RO After RO
1 VP MD VP VP MD
2 VP MD RB VP VP MD RB
3 VP VBX XP+ XP+ VBX
4 VP VBX RB XP+ XP+ VBX RB
5 VP VBX RP XP+ XP+ VBX RP
6 ADJP-PRD JJ XP+ XP+ JJ
7 PP IN NP NP IN
8 PP IN S S IN
9 SBAR-TMP IN S S IN
10 SBAR-ADV IN S S IN
11 SBAR-PRP IN S S IN
12 SBAR-TMP WHADVP S S WHADVP
13 SBAR-ADV WHADVP S S WHADVP
14 SBAR-PRP WHADVP S S WHADVP
Table 1. Head Reordering Rules
Parent
Node
Before RO After RO
1 NP NP SBAR SBAR NP
2 NP NP PP PP NP
3 NP NP PP1 PP2 PP1 PP2 NP
4 VP VBX NP PP PP NP VBX
5 VP VBX NP ADVP-
TMP PP
PP NP ADVP-
TMP VBX
Table 2. Modifier Reordering Rules
For instance, in the parse tree and word
alignment in Figure 1,  the last two English
words if needed under SBAR-ADV is aligned to
the first  two Japanese words ??? ???.
In order to change the English order to the cor-
responding Japanese order, SBAR-ADV domi-
nated by the VP should move across the VP to
sentence initial position, as shown in the top
half of Figure 2,  requiring a CSG rule.
The adverbial clause reordering in Figure 2 is
denoted as Rule 1 in Table 3, which lists two
other CSG rules, Rule 2 & 3.2  The subscripts in
Table 3 are interpreted in the same way as those
in Table 2.
(Joshi and Schabes, 1997) for formal definitions of various
grammar formalisms.
2
 Rule 3 is applied after all CFG rules, see Section 4.6.
Therefore, VBX?s are located at the end of each corre-
sponding VP.
630
Before  RO ? After RO
1 (S XP1+ (VP XP2+ SBAR-ADV ))? (S SBAR-ADV XP1 + (VP XP2+ ))
2 (S XP1+ (VP (XP2+ SBAR-ADV )))? (S XP1+ SBAR-ADV (VP (XP2 + )))
3 (VP1 ADVP-MNR (VP2 XP+ VBX2 ) VBX1)?(VP1 (VP2 XP+ ADVP-MNR VBX2) VBX1)
Table 3. CSG Reordering Rules
ADVP-MNR stands for a manner adverbial
phrase such as explicitly in the following: The
software version has been explicitly verified as
working. Rule 3 in Table 3 indicates that a
ADVP-MNR has to immediately precede a verb
in Japanese, resulting in the substring ?...as
working explicitly verified...? after reordering.
Note that functional tags allow us to write re-
ordering rules specific to  semantic phrases. For
instance, in Rule 1, SBAR-ADV under VP
moves to the sentence initial position under S,
but an SBAR without any functional tags do
not. It typically stays within a VP as the com-
plement of the verb.
4.5 Subject Marker Insertion
Japanese extensively uses case particles that
denote the role of the preceding noun phrase,
for example,  as subject, object, etc.  We insert
sbj, denoting the subject marker, at the end of a
subject noun phrase NP-SBJ. The inserted sub-
ject marker sbj mostly gets translated into the
subject particle? or? in Japanese.3
4.6 Reordering Rule Application
The rules are applied categorically, sequentially
and recursively. CSG Rules 1 and 2 in Table 3
are applied before all of the CFG rules. Among
CFG rules, the modifier rules in Table 2 are
applied before the head rules in Table 1. CSG
Rule 3 in Table 3 is applied last,  followed by
the subject marker insertion operation.
CFG head and modifier rules are applied re-
cursively.  The top half of Figure 2 is the parse
tree obtained by applying reordering rules to the
parse tree in Figure 1. After reordering, the
word alignment becomes mostly monotonic, as
shown at the bottom half of Figure 2.
3 The subject marker insertion is analogous to the insertion
operation  in (Yamada and Knight, 2001), which covers a
wide range of insertion of case particles and verb inflec-
tions in general.
4.7 Experimental Results
All systems are trained on a parallel corpus,
primarily from the Information Technology (IT)
domain and evaluated on the data from the same
domain. The training data statistics is in Table 4
and the evaluation data statistics is in Table 5.
Japanese tokens are morphemes and English
tokens are punctuation tokenized words.
Corpus Stats English Japanese
sentence count 3,358,635 3,358,635
token count 57,231,649 68,725,865
vocabulary size 242,712 348,221
    Table 4. Training Corpus Statistics
Data Sets Sentence Count Token Count
Tuning 600 11,761
DevTest 437 8,158
Eval 600 11,463
Table 5. Evaluation Data Statistics
We measure the translation quality with IBM
BLEU (Papineni et al, 2002) up to 4 grams,
using 2 reference translations, BLEUr2n4. For
BLEU score computation, we character-
segment Kanji and Kana sequences in the refer-
ence and the machine translation output.   Vari-
ous system performances are shown in Table 6.
Models Tuning DevTest Eval
Phrase (BL) 0.5102 0.5330 0.5486
Hiero 0.5385 0.5574 0.5724
Syntax 0.5561 0.5777 0.5863
Phrase+RO1 0.5681 0.5793 0.5962
Table 6. Model Performances (BLEUr2n4)
Phrase (BL) is the baseline phrase translation
system that  incorporates lexical distortion
models (Al-Onaizan and Papineni, 2006).
Hiero is the hierarchical phrase-based system
(Chiang, 2006) that incorporates the phrase
translation model. Syntax is the syntax model
described in Section 3, which incorporates the
phrase translation, Hiero and tree-to-string
grammar models. Phrase+RO1 is the phrase
translation model with pre-ordering  for system
training and decoding,  using the rules described
in this section. Phrase+RO1 improves the trans-
lation quality of the baseline model by 4.76
BLEU points and outperforms the syntax model
by over 0.9 BLEU points.
631
5 Constituent Reordering and Syntax
Model Combined
Translation qualities of systems that combine
the syntax model and pre-ordering are shown in
Table 7. Syntax+RO1 indicates the  syntax
model with pre-ordering discussed in Section 4.
Syntax+RO2 indicates the syntax model with a
more extensive pre-ordering for decoding dis-
cussed below .
Models Tuning DevTest Eval
Phrase+RO1 0.5681 0.5793 0.5962
Syntax+RO1 0.5742 0.5802 0.6003
Syntax+RO2 0.5769 0.5880 0.6046
Table 7. Syntax Model with Pre-ordering
Analyses of the syntax model in Table 6 re-
vealed that automatically learned rules by the
tree-to-string grammar include new rules not
covered by the manually written rules,  some of
which are shown in Table 8.
Parent  Node Before  RO After RO
ADJP-PRD RB JJ PP PP RB JJ
ADVP-TMP RB PP PP RB
ADVP ADVP PP PP ADVP
NP NP VP VP NP
Table 8. New CFG rules automatically learned
by Tree-to-String grammar
We augment the manual rules with the new
automatically learned  rules. We call this ex-
tended set of reordering rules RO2. We use the
manual reordering rules RO1 for model train-
ing, but use the extended rules RO2 for decod-
ing. And we obtain the translation output Syn-
tax+RO2 in Table 7.  Syntax+RO2 outperforms
Phrase+RO1 by 0.84 BLEU points, and Syn-
tax+RO1 by 0.43 BLEU points.
In Table 9, we show the ratio of each rule
type preserved in the derivation of one-best
translation output of the following two models:
Syntax  and Syntax+RO2.  In the table,
?Blocks? indicate phrases from the phrase trans-
lation model and ?Glue Rules? denote the de-
fault grammar rule for monotone decoding.
The syntax model without pre-ordering (Syn-
tax) heavily utilizes the Hiero and tree-to-string
grammar rules, whereas the syntax model with
pre-ordering (Syntax+RO2) mostly depends on
monotone decoding with blocks and glue rules.
Rule Type Syntax Syntax+RO2
Blocks 46.3% 51.2%
Glue Rules  6.0% 37.3%
Hiero Rules 18.3%   1.3%
Tree-to-String 29.4% 10.2%
Table 9. Ratio of each rule type preserved in the
translation derivation of Syntax and Syn-
tax+RO2
6 Summary and Future Research
We have proposed a constituent pre-ordering
technique for English-to-Japanese translation.
The technique improves the performance of the
state-of-the-art phrase translation models by
4.76 BLEU points and outperforms a syntax-
based translation system that incorporates a
phrase translation model, Hiero and a tree-to-
string grammar. We have also shown that com-
bining constituent pre-ordering and  the syntax
model improves the translation quality by addi-
tional  0.84 BLEU points.
While achieving solid performance im-
provement over the existing translation models
for English-to-Japanese translation, our work
has revealed some limitations of syntax models
both in terms of grammar representations and
modeling.  Whereas many syntax models are
based on CFG rules for probability acquisition,
the current research shows that there are various
types of reordering that require the generative
capacity beyond CFG.  While most of the reor-
dering rules for changing the English order to
the Japanese order (and vice versa) should ap-
ply categorically,4 often the probabilities of
tree-to-string grammar rules are not high
enough to survive in the translation derivations.
As for the reordering rules that require the
generative capacity beyond CFG, we may
model mildly context sensitive grammars such
as tree adjoining grammars (Joshi and Schabes,
1997), as in (Carreras and Collins, 2009). The
4 Assuming that the parses are correct, the head reordering
rules in Table 1 have to apply categorically to change the
English order into the Japanese order because English is
head initial and Japanese is head final without any excep-
tions. Similarly, most of the modifier reordering rules in
Table 2 have to apply categorically because most modifi-
ers follow the modified head phrase in English, e.g. a rela-
tive clause modifier follows the head noun phrase, a
prepositional phrase modifier follows the head noun
phrase, etc., whereas modifier phrases precede the modi-
fied head phrases in Japanese.
632
extended domain of locality of  tree adjoining
grammars should suffice to capture non-CFG
reordering rules for many language pairs. Alter-
natively, we can adopt enriched feature repre-
sentations so that  a tree of depth one can actu-
ally convey information on a tree of several
depths, such as parent annotation of (Klein and
Manning, 2003).
Regarding the issue of modeling, we can in-
troduce a rich set of features, as in (Ittycheriah
and Roukos, 2007), the weights of which are
trained to ensure that the tree-to-string grammar
rules generating the accurate target orders are
assigned probabilities high enough not to get
pruned out  in the translation derivation.
Acknowledgement
We would like to acknowledge IBM RTTS
(Realtime Translation Systems) team for tech-
nical discussions on the topic and the provision
of linguistic resources. We also would like to
thank IBM SMT (Statistical Machine Transla-
tion) team for various software tools and the
anonymous reviewers for their helpful com-
ments .
References
Y. Al-Onaizan and K. Papineni. 2006.  Distortion
models for statistical machine translation. Pro-
ceedings of ACL-COLING. Pages 529-536.
C. Baker, S. Bethard, M. Bloodgood, R. Brown, C.
Callison-Burch, G. Coppersmith, B. Dorr, W. Fi-
lardo, K. Giles, A. Irvine, M. Kayser, L. Levin, J.
Martineau, J. Mayfield, S. Miller, A. Phillips, A.
Philpot, C. Piatko, L. Schwartz, D. Zajic. 2010.
Semantically Informed Machine Translation
(SIMT). Final Report of the 2009 Summer Camp
for Applied Language Exploration.
P. Brown, V. Della Pietra, S. Della Pietra, and R.
Mercer. 1993. The mathematics of statistical ma-
chine translation: parameter estimation, Computa-
tional Linguistics, 19(2):263?311.
X. Carreras and M. Collins. 2009. Non-projective
parsing for statistical machine translation. Pro-
ceedings of the 2009 EMNLP. Pages 200-209.
P. Chang and C. Toutanova. 2007.  A Discriminative
Syntactic Word Order Model for Machine Trans-
lation. Proceedings of ACL. Pages 9-16.
D. Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. Pro-
ceedings of ACL. Pages 263-270.
D. Chiang, W. Wang and  Kevin Knight. 2009.
11,001 new features for statistical machine trans-
lation. Proceedings of HLT-NAACL. Pages 218-
226.
M. Collins, P. Koehn, I. Kucerova. 2005. Clause
Restructuring for Statistical Machine Translation.
Proceedings of  ACL. Pages 531-540.
Y. Ding and M. Palmer. 2005. Machine translation
using probabilistic synchronous dependency in-
sertion grammars. Proceedings of ACL. Pages
541-548.
J. Earley. 1970. An efficient context-free parsing
algorithm. Communications of the ACM. Vol. 13.
Pages 94?102.
M. Galley and C. Manning. 2008. A Simple and Ef-
fective Hierarchical Phrase Reordering Model.
Proceedings of EMNLP.
N. Habash. 2007. Syntactic Preprocessing for Statis-
tical Machine Translation. Proceedings of the
Machine Translation Summit.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. Proceedings of HLT. Pages 57-60.
A. Ittycheriah and S. Roukos. 2007. Direct Transla-
tion Model 2. Proceedings of HLT-NAACL.
A. Joshi and Y. Schabes. 1997.  Tree-adjoining
grammars. In G. Rozenberg and K. Salomaa, edi-
tors, Handbook of Formal Languages, volume 3.
Springer.
D. Klein and C. Manning. 2003.  Accurate Unlexi-
calized Parsing. Proceedings of 41st ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation, Proceedings of
HLT?NAACL. Pages 48?54.
Y. Liu, Q. Liu and S. Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. Proceedings of ACL-COLING.
Y. Liu, Y. Huang, Q. Liu, and S. Lin. 2007. Forest-
to-string statistical translation rules. Proceedings
of the 45th ACL.
D. Marcu, W. Wang, A. Echihabi and K. Knight.
2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. Proceed-
ings of EMNLP. Pages 44-52.
M. Marcus, B. Santorini and M.  Marcinkiewicz.
1993. Building a Large Annotated Corpus of Eng-
633
lish: the Penn Treebank. Computational Linguis-
tics,  19(2):  313-330.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Com-
putational Linguistics: Vol. 30.  Pages 417? 449.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. Proceddings  of ACL. Pages
311?318.
A. Ratnaparkhi. 1999. Learning to Parse Natural
Language with Maximum Entropy Models. Ma-
chine Learning: Vol. 34. Pages 151-178.
L. Shen, J. Xu and R. Weischedel. 2008. A new
string-to-dependency machine translation algo-
rithm with a target dependency language model.
Proceedings of ACL.
L. Shen, J. Xu, B. Zhang, S. Matsoukas and Ralph
Weischedel. 2009. Effective Use of Linguistic
and Contextual Information for Statistical Ma-
chine Translation. Proceedings of EMNLP.
C. Wang, M. Collins, P. Koehn. 2007. Chinese Syn-
tactic Reordering for Statistical Machine Transla-
tion. Proceedings of EMNLP-CoNLL.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3): 377-404.
F. Xia and M. McCord. 2004. Improving a Statistical
MT System with Automatically Learned Rewrite
Patterns. Proceedings of COLING.
P. Xu, J. Kang, M. Ringgaard, F. Och. 2009. Using a
dependency parser to improve SMT for subject-
verb-object languages. Proceedings of HLT-
NAACL.
K. Yamada and K. Knight.  2001. A Syntax-based
Statistical Translation Model. Proceedings of the
39th ACL. Pages 523-530.
H. Zhang, L. Huang, D. Gildea and K. Knight.
2006.  Synchronous binarization for machine
translation. Proceedings of the HLT-NAACL.
Pages 256-263.
B. Zhao and Y. Al-onaizan. 2008. Generalizing Lo-
cal and Non-Local Word-Reordering Patterns for
Syntax-Based Machine Translation. Proceedings
of EMNLP. Pages 572-581.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing.
Proceedings of NAACL 2006 -Workshop on sta-
tistical machine  translation.
A. Zollmann, A. Venugopal, F. Och and J. Ponte.
2008. A Systematic Comparison of Phrase-Based,
Hierarchical and Syntax-Augmented MT. Pro-
ceedings of COLING.
634
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 846?855,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning to Transform and Select Elementary Trees for Improved
Syntax-based Machine Translations
Bing Zhao?, and Young-Suk Lee?, and Xiaoqiang Luo?, and Liu Li?
IBM T.J. Watson Research? and Carnegie Mellon University?
{zhaob, ysuklee, xiaoluo}@us.ibm.com and liul@andrew.cmu.edu
Abstract
We propose a novel technique of learning how to
transform the source parse trees to improve the trans-
lation qualities of syntax-based translation mod-
els using synchronous context-free grammars. We
transform the source tree phrasal structure into a
set of simpler structures, expose such decisions to
the decoding process, and find the least expensive
transformation operation to better model word re-
ordering. In particular, we integrate synchronous bi-
narizations, verb regrouping, removal of redundant
parse nodes, and incorporate a few important fea-
tures such as translation boundaries. We learn the
structural preferences from the data in a generative
framework. The syntax-based translation system in-
tegrating the proposed techniques outperforms the
best Arabic-English unconstrained system in NIST-
08 evaluations by 1.3 absolute BLEU, which is sta-
tistically significant.
1 Introduction
Most syntax-based machine translation models with syn-
chronous context free grammar (SCFG) have been re-
lying on the off-the-shelf monolingual parse structures
to learn the translation equivalences for string-to-tree,
tree-to-string or tree-to-tree grammars. However, state-
of-the-art monolingual parsers are not necessarily well
suited for machine translation in terms of both labels
and chunks/brackets. For instance, in Arabic-to-English
translation, we find only 45.5% of Arabic NP-SBJ struc-
tures are mapped to the English NP-SBJ with machine
alignment and parse trees, and only 60.1% of NP-SBJs
are mapped with human alignment and parse trees as in
? 2. The chunking is of more concern; at best only 57.4%
source chunking decisions are translated contiguously on
the target side. To translate the rest of the chunks one
has to frequently break the original structures. The main
issue lies in the strong assumption behind SCFG-style
nonterminals ? each nonterminal (or variable) assumes a
source chunk should be rewritten into a contiguous chunk
in the target. Without integrating techniques to mod-
ify the parse structures, the SCFGs are not to be effec-
tive even for translating NP-SBJ in linguistically distant
language-pairs such as Arabic-English.
Such problems have been noted in previous literature.
Zollmann and Venugopal (2006) and Marcu et al (2006)
used broken syntactic fragments to augment their gram-
mars to increase the rule coverage; while we learn opti-
mal tree fragments transformed from the original ones via
a generative framework, they enumerate the fragments
available from the original trees without learning pro-
cess. Mi and Huang (2008) introduced parse forests to
blur the chunking decisions to a certain degree, to ex-
pand search space and reduce parsing errors from 1-best
trees (Mi et al, 2008); others tried to use the parse trees
as soft constraints on top of unlabeled grammar such as
Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang
et al, 2010; Shen et al, 2010) without sufficiently lever-
aging rich tree context. Recent works tried more com-
plex approaches to integrate both parsing and decoding
in one single search space as in (Liu and Liu, 2010), at
the cost of huge search space. In (Zhang et al, 2009),
combinations of tree forest and tree-sequence (Zhang et
al., 2008) based approaches were carried out by adding
pseudo nodes and hyper edges into the forest. Overall,
the forest-based translation can reduce the risks from up-
stream parsing errors and expand the search space, but
it cannot sufficiently address the syntactic divergences
between various language-pairs. The tree sequence ap-
proach adds pseudo nodes and hyper edges to the forest,
which makes the forest even denser and harder for nav-
igation and search. As trees thrive in the search space,
especially with the pseudo nodes and edges being added
to the already dense forest, it is becoming harder to wade
through the deep forest for the best derivation path out.
We propose to simplify suitable subtrees to a reason-
able level, at which the correct reordering can be easily
identified. The transformed structure should be frequent
enough to have rich statistics for learning a model. In-
stead of creating pseudo nodes and edges and make the
forest dense, we transform a tree with a few simple oper-
ators; only meaningful frontier nodes, context nodes and
edges are kept to induce the correct reordering; such oper-
ations also enable the model to share the statistics among
all similar subtrees.
On the basis of our study on investigating the language
divergence between Arabic-English with human aligned
and parsed data, we integrate several simple statistical op-
erations, to transform parse trees adaptively to serve the
846
translation purpose better. For each source span in the
given sentence, a subgraph, corresponding to an elemen-
tary tree (in Eqn. 1), is proposed for PSCFG translation;
we apply a few operators to transform the subgraph into
some frequent subgraphs seen in the whole training data,
and thus introduce alternative similar translational equiv-
alences to explain the same source span with enriched
statistics and features. For instance, if we regroup two
adjacent nodes IV and NP-SBJ in the tree, we can ob-
tain the correct reordering pattern for verb-subject order,
which is not easily available otherwise. By finding a set
of similar elementary trees derived from the original ele-
mentary trees, statistics can be shared for robust learning.
We also investigate the features using the context be-
yond the phrasal subtree. This is to further disambiguate
the transformed subgraphs so that informative neighbor-
ing nodes and edges can influence the reordering prefer-
ences for each of the transformed trees. For instance, at
the beginning and end of a sentence, we do not expect
dramatic long distance reordering to happen; or under
SBAR context, the clause may prefer monotonic reorder-
ing for verb and subject. Such boundary features were
treated as hard constraints in previous literature in terms
of re-labeling (Huang and Knight, 2006) or re-structuring
(Wang et al, 2010). The boundary cases were not ad-
dressed in the previous literature for trees, and here we
include them in our feature sets for learning a MaxEnt
model to predict the transformations. We integrate the
neighboring context of the subgraph in our transforma-
tion preference predictions, and this improve translation
qualities further.
The rest of the paper is organized as follows: in sec-
tion 2, we analyze the projectable structures using hu-
man aligned and parsed data, to identify the problems for
SCFG in general; in section 3, our proposed approach
is explained in detail, including the statistical operators
using a MaxEnt model; in section 4, we illustrate the in-
tegration of the proposed approach in our decoder; in sec-
tion 5, we present experimental results; in section 6, we
conclude with discussions and future work.
2 The Projectable Structures
A context-free style nonterminal in PSCFG rules means
the source span governed by the nonterminal should be
translated into a contiguous target chunk. A ?projectable?
phrase-structure means that it is translated into a con-
tiguous span on the target side, and thus can be gener-
alized into a nonterminal in our PSCFG rule. We carried
out a controlled study on the projectable structures using
human annotated parse trees and word alignment for 5k
Arabic-English sentence-pairs.
In Table 1, the unlabeled F-measures with machine
alignment and parse trees show that, for only 48.71% of
the time, the boundaries introduced by the source parses
Alignment Parse Labels Accuracy
H H
NP-SBJ 0.6011
PP 0.3436
NP 0.4832
unlabel 0.5739
M H
NP-SBJ 0.5356
PP 0.2765
NP 0.3959
unlabel 0.5305
M M
NP-SBJ 0.4555
PP 0.1935
NP 0.3556
unlabel 0.4871
Table 1: The labeled and unlabeled F-measures for projecting
the source nodes onto the target side via alignments and parse
trees; unlabeled F-measures show the bracketing accuracies for
translating a source span contiguously. H: human, M: machine.
are real translation boundaries that can be explained by a
nonterminal in PSCFG rule. Even for human parse and
alignment, the unlabeled F-measures are still as low as
57.39%. Such statistics show that we should not blindly
learn tree-to-string grammar; additional transformations
to manipulate the bracketing boundaries and labels ac-
cordingly have to be implemented to guarantee the reli-
ability of source-tree based syntax translation grammars.
The transformations could be as simple as merging two
adjacent nonterminals into one bracket to accommodate
non-contiguity on the target side, or lexicalizing those
words which have fork-style, many-to-many alignment,
or unaligned content words to enable the rest of the span
to be generalized into nonterminals. We illustrate several
cases using the tree in Figure 1.
NP?SBJ
the millde east crisisupthat make
mn
PREP PRON
+hA Azmp
NOUN
Al$rq
ADJ
AlAwsT
PRON IV
Alty ttAlf
NOUN
WHNP VP PP?CLR
SBAR
S
NP
Figure 1: Non-projectable structures in an SBAR tree with
human parses and alignment; there are non-projectable struc-
tures: the deleted nonterminals PRON (+hA), the many-to-
many alignment for IV(ttAlf) PREP(mn), fork-style alignment
for NOUN (Azmp).
In Figure 1, several non-projectable nodes were illus-
847
trated: the deleted nonterminals PRON (+hA), the many-
to-many alignment for IV(ttAlf) PREP(mn), fork-style
alignment for NOUN (Azmp). Intuitively, it would be
good to glue the nodes NOUN(Al$rq) ADJ(AlAwsT) un-
der the node of NP, because it is more frequent for moving
ADJ before NOUN in our training data. It should be eas-
ier to model the swapping of (NOUN ADJ) using the tree
(NP NOUN, ADJ) instead of the original bigger tree of
(NP-SBJ Azmp, NOUN, ADJ) with one lexicalized node.
Approaches in tree-sequence based grammar (Zhang et
al., 2009) tried to address the bracketing problem by us-
ing arbitrary pseudo nodes to weave a new ?tree? back
into the forest for further grammar extractions. Such ap-
proach may improve grammar coverage, but the pseudo
node labels would be arguably a worse choice to split
the already sparse data. Some of the interior nodes con-
necting the frontier nodes might be very informative for
modeling reordering. Also, due to the introduced pseudo
nodes, it would need exponentially many nonterminals to
keep track of the matching tree-structures for translations.
The created pseudo node could easily block the informa-
tive neighbor nodes associated with the subgraph which
could change the reordering nature. For instance, IV and
NP-SBJ tends to swap at the beginning of a sentence, but
it may prefer monotone if they share a common parent of
SBAR for a subclause. In this case, it is unnecessary to
create a pseudo node ?IV+SBJ? to block useful factors.
We propose to navigate through the forest, via simpli-
fying trees by grouping the nodes, cutting the branches,
and attaching connected neighboring informative nodes
to further disambiguate the derivation path. We apply ex-
plicit translation motivated operators, on a given mono-
lingual elementary tree, to transform it into similar but
simpler trees, and expose such statistical preferences to
the decoding process to select the best rewriting rule
from the enriched grammar rule sets, for generating tar-
get strings.
3 Elementary Trees to String Grammar
We propose to use variations of an elementary tree, which
is a connected subgraph fitted in the original monolingual
parse tree. The subgraph is connected so that the frontiers
(two or more) are connected by their immediate common
parent. Let ? be a source elementary tree:
? =< `; vf , vi, E >, (1)
where vf is a set of frontier nodes which contain nonter-
minals or words; vi are the interior nodes with source la-
bels/symbols; E is the set of edges connecting the nodes
v = vf+vi into a connected subgraph fitted in the source
parse tree; ` is the immediate common parent of the fron-
tier nodes vf . Our proposed grammar rule is formulated
as follows:
< ?;?;?; m?; t? >, (2)
where ? is the target string, containing the terminals
and/or nonterminals in a target language; ? is the one-
to-one alignment of the nonterminals between ? and ?; t?
contains possible sequence of transform operations (to be
explained later in this section) associated with each rule;
m? is a function of enumerating the neighborhood of the
source elementary tree ?, and certain tree context (nodes
and edges) can be used to further disambiguate the re-
ordering or the given lexical choices. The interior nodes
of ?.vi, however, are not necessarily informative for the
reordering decisions, like the unary nodes WHNP,VP, and
PP-CLR in Figure 1; while the frontier nodes ?.vf are the
ones directly executing the reordering decisions. We can
selectively cut off the interior nodes, which have no or
only weak causal relations to the reordering decisions.
This will enable the frequency or derived probabilities
for executing the reordering to be more focused. We call
such transformation operators t?. We specified a few op-
erators for transforming an elementary tree ?, including
flattening tree operators such as removing interior nodes
in vi, or grouping the children via binarizations.
Let?s use the trigram ?Alty ttAlf mn? in Figure 1 as
an example, the immediate common parent for the span
is SBAR: ?.` = SBAR; the interior nodes are ?.vi =
{WHNP VP S PP-CLR}; the frontier nodes are ?.vf =
(x:PRON x:IV x:PREP). The edges ?.E (as highlighted
in Figure 1) connect ?.vi and ?.vf into a subgraph for the
given source ngram.
For any source span, we look up one elementary tree ?
covering the span, then we select an operator t? ? T , to
explore a set of similar elementary trees t?(?, m?) = {??}
as simplified alternatives for translating that source tree
(span) ? into an optimal target string ?? accordingly. Our
generative model is summarized in Eqn. 3:
?? = argmax
t??T ;???t?(?,m?)
pa(??|??)?
pb(??|t?, ?, m?)?
pc(t?|?, m?). (3)
In our generative scheme, for a given elementary tree
?, we sample an operator (or a combination of operations)
t? with the probability of pc(t?|?); with operation t?, we
transform ? into a set of simplified versions ?? ? t?(?, m?)
with the probability of pb(??|t?, ?); finally we select the
transformed version ?? to generate the target string ??
with a probability of pa(??|??). Note here, ?? and ? share
the same immediate common parent `, but not necessar-
ily the frontier, or interior, or even neighbors. The frontier
nodes can be merged, lexicalized, or even deleted in the
tree-to-string rule associated with ??, as long as the align-
ment for the nonterminals are book-kept in the deriva-
tions. To simplify the model, one can choose the operator
t? to be only one level, and the model using a single oper-
ator t? is to be deterministic. Thus, the final set of models
848
to learn are pa(??|??) for rule alignment, and the pref-
erence model pb(??|t?, ?, m?), and the operator proposal
model pc(t?|?, m?), which in our case is a maximum en-
tropy model? the key model in our proposed approach
in this paper for transforming the original elementary tree
into similar trees for evaluating the reordering probabili-
ties.
Eqn. 3 significantly enriches reordering powers for
syntax-based machine translation. This is because it uses
all similar set of elementary trees to generate the best tar-
get strings. In the next section, we?ll first define the op-
erators conceptually, and then explain how we learn each
of the models.
3.1 Model pa(??|??)
A log linear model is applied here to approximate
pa(??|??) ? exp(?? ?ff) via weighted combination (??) of
feature functions ff(??, ??), including relative frequen-
cies in both directions, and IBM Model-1 scores in both
directions as ?? and ?? have lexical items within them.
We also employed a few binary features listed in the fol-
lowing table.
?? is observed less than 2 times
(??, ??) deletes a src content word
(??, ??) deletes a src function word
(??, ??) over generates a tgt content word
(??, ??) over generates a tgt function word
Table 2: Additional 5 Binary Features for pa(??|??)
3.2 Model pb(??|t?, ?, m?)
pb(??|t?, ?, m?) is our preference model. For instance us-
ing the operator t? of cutting an unary interior node in
?.vi, if ?.vi has more than one unary interior node, like
the SBAR tree in Figure 1, having three unary interior
node: WHNP, VP and PP-CLR, pb(??|t?, ?, m?) specifies
which one should have more probabilities to be cut. In
our case, to make model simple, we simply choose his-
togram/frequency for modeling the choices.
3.3 Model pc(t?|?, m?)
pc(t?|?, m?) is our operator proposal model. It ranks
the operators which are valid to be applied for the
given source tree ? together with its neighborhood m?.
Here, in our approach, we applied a Maximum Entropy
model, which is also employed to train our Arabic parser:
pc(t?|?, m?) ? exp ?? ? ff(t?, ?, m?). The feature sets we
use here are almost the same set we used to train our Ara-
bic parser; the only difference is the future space here is
operator categories, and we check bag-of-nodes for inte-
rior nodes and frontier nodes. The key feature categories
we used are listed as in the Table 3. The headtable used
in our training is manually built for Arabic.
bag-of-nodes ?.vi
bag-of-nodes and ngram of ?.vf
chunk-level features: left-child, right-child, etc.
lexical features: unigram and bigram
pos features: unigram and bigram
contextual features: surrounding words
Table 3: Feature Features for learning pc(t?|?, m?)
3.4 t?: Tree Transformation Function
Obvious systematic linguistic divergences between
language-pairs could be handled by some simple oper-
ators such as using binarization to re-group contiguously
aligned children. Here, we start from the human aligned
and parsed data as used in section 2 to explore potential
useful operators.
3.4.1 Binarizations
One of the simplest way for transforming a tree is via bi-
narization. Monolingual binarization chooses to re-group
children into smaller subtree with a suitable label for the
newly created root. We choose a function mapping to se-
lect the top-frequent label as the root for the grouped chil-
dren; if such label is not found we simply use the label of
the immediate common parent for ?. In decoding time,
we need to select trees from all possible binarizations,
while in the training time, we restrict the choices allowed
with the alignment constraint, that every grouped chil-
dren should be aligned contiguously on the target side.
Our goal is to simulate the synchronous binarization as
much as we can. In this paper, we applied the four ba-
sic operators for binarizing a tree: left-most, right-most
and additionally head-out left and head-out right for more
than three children. Two examples are given in Table 4,
in which we used LDC style representation for the trees.
With the proper binarization, the structure becomes
rich in sub-structures which allow certain reordering to
happen more likely than others. For instance for the sub-
tree (VP PV NP-SBJ), one would apply stronger statistics
from training data to support the swap of NP-SBJ and PV
for translation.
3.4.2 Regrouping verbs
Verbs are keys for reordering especially for Araic-English
with VSO translated into SVO. However, if the verb and
its relevant arguments for reordering are at different lev-
els in the tree, the reordering is difficult to model as more
interior nodes combinations will distract the distributions
and make the model less focused. We provide the fol-
lowing two operations specific for verb in VP trees as in
Table 5.
3.4.3 Removing interior nodes and edges
For reordering patterns, keeping the deep tree structure
might not be the best choice. Sometimes it is not even
849
Binarization Operations Examples
right-most (NP Xnoun Xadj1 Xadj2) 7? (NP Xnoun (ADJP Xadj1 Xadj2))left-most (VP Xpv XNP-SBJ XSBAR) 7? (VP (VP Xpv XNP-SBJ) XSBAR)
Table 4: Operators for binarizing the trees
Operators for regroup verbs Examples
regroup verb (V P1 Xv (V P2 Y )) 7? (V P1 (V P2 Xv Y ))
regroup verb and remove the top level VP (R (V P1 Xv (R2 Y ))) 7? (R (R2 XvY ))
Table 5: Operators for manipulating the trees
possible due to the many-to-many alignment, insertions
and deletions of terminals. So, we introduce the oper-
ators to remove the interior nodes ?.vi selectively; this
way, we can flatten the tree, remove irrelevant nodes and
edges, and can use more frequent observations of simpli-
fied structures to capture the reordering patterns. We use
two operators as shown in Table 6.
The second operator deletes all the interior nodes, la-
bels and edges; thus reordering will become a Hiero-alike
(Chiang, 2007) unlabeled rule, and additionally a spe-
cial glue rule: X1X2 ? X1X2. This operator is neces-
sary, we need a scheme to automatically back off to the
meaningful glue or Hiero-alike rules, which may lead to a
cheaper derivation path for constructing a partial hypoth-
esis, at the decoding time.
NP*
PREP
to ignite the situation
AlAwDAE
DET+NOUN
AlAnfjAr
DET+NOUN
dfE
NOUN
NP
NP
PP*
NP
Aly
Figure 2: A NP tree with an ?inside-out? alignment. The nodes
?NP*? and ?PP*? are not suitable for generalizing into NTs
used in PSCFG rules.
As shown in Table 1, NP brackets has only 35.56% of
time to be translated contiguously as an NP in machine
aligned & parsed data. The NP tree in Figure 2 happens to
be an ?inside-out? style alignment, and context free gram-
mar such as ITG (Wu, 1997) can not explain this structure
well without necessary lexicalization. Actually, the Ara-
bic tokens of ?dfE Aly AlAnfjAr? form a combination
and is turned into English word ?ignite? in an idiomatic
way. With lexicalization, a Hiero style rule ?dfE X Aly
AlAnfjAr 7? to ignite X? is potentially a better alterna-
tive for translating the NP tree. Our operators allow us
to back off to such Hiero-style rules to construct deriva-
tions, which share the immediate common parent NP, as
defined for the elementary tree, for the given source span.
3.5 m?: Neighboring Function
For a given elementary tree, we use function m? to check
the context beyond the subgraph. This includes looking
the nodes and edges connected to the subgraph. Similar
to the features used in (Dyer et al, 2009), we check the
following three cases.
3.5.1 Sentence boundaries
When the tree ? frontier sets contain the left-most token,
right-most token, or both sides, we will add to the neigh-
boring nodes the corresponding decoration tags L (left),
R (right), and B (both), respectively. These decorations
are important especially when the reordering patterns for
the same trees are depending on the context. For instance,
at the beginning or end of a sentence, we do not expect
dramatic reordering ? moving a token too far away in the
middle of the sentences.
3.5.2 SBAR/IP/PP/FRAG boundaries
We check siblings of the root for ? for a few special la-
bels, including SBAR, IP, PP, and FRAG. These labels
indicate a partial sentence or clause, and the reordering
patterns may get different distributions due to the posi-
tion relative to these nodes. For instance, the PV and SBJ
nodes under SBAR tends to have more monotone prefer-
ence for word reordering (Carpuat et al, 2010). We mark
the boundaries with position markers such as L-PP, to in-
dicate having a left sibling PP, R-IP for having a right
sibling IP, and C-SBAR to indicate the elementary tree is
a child of SBAR. These labels are selected mainly based
on our linguistic intuitions and errors in our translation
system. A data-driven approach might be more promis-
ing for identifying useful markups w.r.t specific reorder-
ing patterns.
3.5.3 Translation boundaries
In the Figure 2, there are two special nodes under NP:
NP* and PP*. These two nodes are aligned in a ?inside-
out? fashion, and none of them can be generalized into
a nonterminal to be rewritten in a PSCFG rule. In other
words, the phrasal brackets induced from NP* and PP*
850
operators for removing nodes/edges Examples
remove unary nodes (R Xt1(R1 (R2 Xt2))) 7? (R Xt1(R2 Xt2)))
remove all labels (R (R1 Xt1(R2 Xt2))) 7? (R Xt2Xt1)
Table 6: Operators for simplifying the trees
are not translation boundaries, and to avoid translation
errors we should identify them by applying a PSCFG
rule on top of them. During training, we label nodes
with translation boundaries, as one additional function
tag; during decoding, we employ the MaxEnt model to
predict the translation boundary label probability for each
span associated with a subgraph ?, and discourage deriva-
tions accordingly for using nonterminals over the non-
translation boundary span. The translation boundaries
over elementary trees have much richer representation
power. The previous works as in Xiong et al (2010),
defined translation boundaries on phrase-decoder style
derivation trees due to the nature of their shift-reduce al-
gorithm, which is a special case in our model.
4 Decoding
Decoding using the proposed elementary tree to string
grammar naturally resembles bottom up chart parsing al-
gorithms. The key difference is at the grammar querying
step. Given a grammar G, and the input source parse tree
pi from a monolingual parser, we first construct the ele-
mentary tree for a source span, and then retrieve all the
relevant subgraphs seen in the given grammar through
the proposed operators. This step is called populating,
using the proposed operators to find all relevant elemen-
tary trees ? which may have contributed to explain the
source span, and put them in the corresponding cells in
the chart. There would have been exponential number of
relevant elementary trees to search if we do not have any
restrictions in the populating step; we restrict the maxi-
mum number of interior nodes |?.vi| to be 3, and the size
of frontier nodes |?.vf | to be less than 6; additional prun-
ing for less frequent elementary trees is carried out.
After populating the elementary trees, we construct
the partial hypotheses bottom up, by rewriting the fron-
tier nodes of each elementary tree with the probabili-
ties(costs) for ? ? ?? as in Eqn. 3. Our decoder (Zhao
and Al-Onaizan, 2008) is a template-based chart decoder
in C++. It generalizes over the dotted-product operator in
Earley style parser, to allow us to leverage many opera-
tors t? ? T as above-mentioned, such as binarizations, at
different levels for constructing partial hypothesis.
5 Experiments
In our experiments, we built our system using most of the
parallel training data available to us: 250M Arabic run-
ning tokens, corresponding to the ?unconstrained? condi-
tion in NIST-MT08. We chose the testsets of newswire
and weblog genres from MT08 and DEV101. In partic-
ular, we choose MT08 to enable the comparison of our
results to the reported results in NIST evaluations. Our
training and test data is summarized in Table 5. For test-
ings, we have 129,908 tokens in our testsets. For lan-
guage models (LM), we used 6-gram LM trained with
10.3 billion English tokens, and also a shrinkage-based
LM (Chen, 2009) ? ?ModelM? (Chen and Chu, 2010;
Emami et al, 2010) with 150 word-clusters learnt from
2.1 million tokens.
From the parallel data, we extract phrase pairs(blocks)
and elementary trees to string grammar in various con-
figurations: basic tree-to-string rules (Tr2str), elementary
tree-to-string rules with boundaries t?(elm2str+m?), and
with both t? and m? (elm2str+t? + m?). This is to evalu-
ate the operators? effects at different levels for decoding.
To learn our MaxEnt models defined in ? 3.3, we collect
the events during extracting elm2str grammar in training
time, and learn the model using improved iterative scal-
ing. We use the same training data as that used in training
our Arabic parser. There are 16 thousand human parse
trees with human alignment; additional 1 thousand hu-
man parse and aligned sent-pairs are used as unseen test
set to verify our MaxEnt models and parsers. For our
Arabic parser, we have a labeled F-measure of 78.4%,
and POS tag accuracy 94.9%. In particular, we?ll evaluate
model pc(t?|?, m?) in Eqn. 3 for predicting the translation
boundaries in ? 3.5.3 for projectable spans as detailed in
? 5.1.
Our decoder (Zhao and Al-Onaizan, 2008) supports
grammars including monotone, ITG, Hiero, tree-to-
string, string-to-tree, and several mixtures of them (Lee
et al, 2010). We used 19 feature functions, mainly from
those used in phrase-based decoder like Moses (Koehn
et al, 2007), including two language models (one for a
6-gram LM, one for ModelM, one brevity penalty, IBM
Model-1 (Brown et al, 1993) style alignment probabil-
ities in both directions, relative frequency in both direc-
tions, word/rule counts, content/function word mismatch,
together with features on tr2str rule probabilities. We
use BLEU (Papineni et al, 2002) and TER (Snover et
al., 2006) to evaluate translation qualities. Our base-
line used basic elementary tree to string grammar without
any manipulations and boundary markers in the model,
1DEV10 are unseen testsets used in our GALE project. It was se-
lected from recently released LDC data LDC2010E43.v3.
851
Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB
# Sents 8,032,837 813 547 1089 1059
# Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088
Table 7: Training and test data; using all training parallel training data for 4 test sets
and we achieved a BLEUr4n4 55.01 for MT08-NW, or
a cased BLEU of 53.31, which is close to the best offi-
cially reported result 53.85 for unconstrained systems.2
We expose the statistical decisions in Eqn. 3 as the rule
probability as one of the 19 dimensions, and use Sim-
plex Downhill algorithm with Armijo line search (Zhao
and Chen, 2009) to optimize the weight vector for de-
coding. The algorithm moves all dimensions at the same
time, and empirically achieved more stable results than
MER(Och, 2003) in many of our experiments.
5.1 Predicting Projectable Structures
The projectable structure is important for our proposed
elementary tree to string grammar (elm2str). When a
span is predicted not to be a translation boundary, we
want the decoder to prefer alternative derivations out-
side of the immediate elementary tree, or more aggres-
sive manipulation of the trees, such as deleting inte-
rior nodes, to explore unlabeled grammar such as Hi-
ero style rules, with proper costs. We test separately
on predicting the projectable structures, like predicting
function tags in ? 3.5.3, for each node in syntactic parse
tree. We use one thousand test sentences with two con-
ditions: human parses and machine parses. There are
totally 40,674 nodes excluding the sentence-level node.
The results are shown in Table 8. It showed our Max-
Ent model is very accurate using human trees: 94.5% of
accuracy, and about 84.7% of accuracy for using the ma-
chine parsed trees. Our accuracies are higher compared
with the 71+% accuracies reported in (Xiong et al, 2010)
for their phrasal decoder.
Setups Accuracy
Human Parses 94.5%
Machine Parses 84.7%
Table 8: Accuracies of predicting projectable structures
We zoom in the translation boundaries for MT08-NW,
in which we studied a few important frequent labels in-
cluding VP and NP-SBJ as in Table 9. According to our
MaxEnt model, 20% of times we should discourage a VP
tree to be translated contiguously; such VP trees have an
average span length of 16.9 tokens in MT08-NW. Simi-
lar statistics are 15.9% for S-tree with an average span of
13.8 tokens.
2See link: http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08
official results v0.html
Labels total NonProj Percent Avg.len
VP* 4479 920 20.5% 16.9
NP* 14164 825 5.8% 8.12
S* 3123 495 15.9% 13.8
NP-SBJ* 1284 53 4.12% 11.9
Table 9: The predicted projectable structures in MT08-NW
Using the predicted projectable structures for elm2str
grammar, together with the probability defined in Eqn. 3
as additional cost, the translation results in Table 11 show
it helps BLEU by 0.29 BLEU points (56.13 v.s. 55.84).
The boundary decisions penalize the derivation paths us-
ing nonterminals for non-projectable spans for partial hy-
pothesis construction.
Setups TER BLEUr4n4
Baseline 39.87 55.01
right-binz (rbz) 39.10 55.19
left-binz (lbz) 39.67 55.31
Head-out-left (hlbz) 39.56 55.50
Head-out-right (hrbz) 39.52 55.53
+all binzation (abz) 39.42 55.60
+regroup-verb 39.29 55.72
+deleting interior nodes ?.vi 38.98 55.84
Table 10: TER and BLEU for MT08-NW, using only t?(?)
5.2 Integrating t? and m?
We carried out a series of experiments to explore the im-
pacts using t? and m? for elm2str grammar. We start from
transforming the trees via simple operator t?(?), and then
expand the function with more tree context to include the
neighboring functions: t?(?, m?).
Setups TER BLEUr4n4
Baseline w/ t? 38.98 55.84
+ TM Boundaries 38.89 56.13
+ SENT Bound 38.63 56.46
all t?(?, m?) 38.61 56.87
Table 11: TER and BLEU for MT08-NW, using t?(?, m?).
Experiments in Table 10 focus on testing operators es-
pecially binarizations for transforming the trees. In Ta-
ble 10, the four possible binarization methods all improve
852
Data MT08-NW MT08-WB Dev10-NW Dev10-WB
Tr2Str 55.01 39.19 37.33 41.77
elm2str+t? 55.84 39.43 38.02 42.70
elm2str+m? 55.57 39.60 37.67 42.54
elm2str+t?(?, m?) 56.87 39.82 38.62 42.75
Table 12: BLEU scores on various test sets; comparing elementary tree-to-string grammar (tr2str), transformation of the trees
(elm2str+t?), using the neighboring function for boundaries ( elm2str+m?), and combination of all together ( elm2str+t?(?, m?)).
MT08-NW and MT08-WB have four references; Dev10-WB has three references, and Dev10-NW has one reference. BLEUn4
were reported.
over the baseline from +0.18 (via right-most binarization)
to +0.52 (via head-out-right) BLEU points. When we
combine all binarizations (abz), we did not see additive
gains over the best individual case ? hrbz. Because during
our decoding time, we do not frequently see large number
of children (maximum at 6), and for smaller trees (with
three or four children), these operators will largely gen-
erate same transformed trees, and that explains the differ-
ences from these individual binarization are small. For
other languages, these binarization choices might give
larger differences. Additionally, regrouping the verbs is
marginally helpful for BLEU and TER. Upon close ex-
aminations, we found it is usually beneficial to group
verb (PV or IV) with its neighboring nodes for expressing
phrases like ?have to do? and ?will not only?. Deleting
the interior nodes helps on shrinking the trees, so that we
can translate it with more statistics and confidences. It
helps more on TER than BLEU for MT08-NW.
Table 11 extends Table 10 with neighboring function
to further disambiguate the reordering rule using the tree
context. Besides the translation boundary, the reorder-
ing decisions should be different with regard to the posi-
tions of the elementary tree relative to the sentence. At
the sentence-beginning one might expect more for mono-
tone decoding, while in the middle of the sentence, one
might expect more reorderings. Table 11 shows when we
add such boundary markups in our rules, an improvement
of 0.33 BLEU points were obtained (56.46 v.s. 56.13)
on top of the already improved setups. A close check
up showed that the sentence-begin/end markups signifi-
cantly reduced the leading ?and? (from Arabic word w#)
in the decoding output. Also, the verb subject order un-
der SBAR seems to be more like monotone with a lead-
ing pronoun, rather than the general strong reordering of
moving verb after subject. Overall, our results showed
that such boundary conditions are helpful for executing
the correct reorderings. We conclude the investigation
with full function t?(?, m?), which leads to a BLEUr4n4 of
56.87 (cased BLEUr4n4c 55.16), a significant improve-
ment of 1.77 BLEU point over a already strong baseline.
We apply the setups for several other NW and WEB
datasets to further verify the improvement. Shown in Ta-
ble 12, we apply separately the operators for t? and m? first,
then combine them as the final results. Varied improve-
ments were observed for different genres. On DEV10-
NW, we observed 1.29 BLEU points improvement, and
about 0.63 and 0.98 improved BLEU points for MT08-
WB and DEV10-WB, respectively. The improvements
for newwire are statistically significant. The improve-
ments for weblog are, however, only marginally better.
One possible reason is the parser quality for web genre is
reliable, as our training data is all in newswire. Regarding
to the individual operators proposed in this paper, we ob-
served consistent improvements of applying them across
all the datasets. The generative model in Eqn. 3 leverages
the operators further by selecting the best transformed
tree form for executing the reorderings.
5.3 A Translation Example
To illustrate the advantages of the proposed grammar, we
use a testing case with long distance word reordering and
the source side parse trees. We compare the translation
from a strong phrasal decoder (DTM2) (Ittycheriah and
Roukos, 2007), which is one of the top systems in NIST-
08 evaluation for Arabic-English. The translations from
both decoders with the same training data (LM+TM) are
in Table 13. The highlighted parts in Figure 3 show that,
the rules on partial trees are effectively selected and ap-
plied for capturing long-distance word reordering, which
is otherwise rather difficult to get correct in a phrasal sys-
tem even with a MaxEnt reordering model.
6 Discussions and Conclusions
We proposed a framework to learn models to predict
how to transform an elementary tree into its simplified
forms for better executing the word reorderings. Two
types of operators were explored, including (a) trans-
forming the trees via binarizations, grouping or deleting
interior nodes to change the structures; and (b) neighbor-
ing boundary context to further disambiguate the reorder-
ing decisions. Significant improvements were observed
on top of a strong baseline system, and consistent im-
provements were observed across genres; we achieved a
cased BLEU of 55.16 for MT08-NW, which is signifi-
cantly better than the officially reported results in NIST
MT08 Arabic-English evaluations.
853
Src Sent qAl AlAmyr EbdAlrHmn bn EbdAlEzyz nA}b wzyr AldfAE AlsEwdy AlsAbq fy tSryH SHAfy An +hmtfA}l b# qdrp Almmlkp Ely AyjAd Hl l# Alm$klp .
Phrasal Decoder prince abdul rahman bin abdul aziz , deputy minister of defense former saudi said in a press statementthat he was optimistic about the kingdom ?s ability to find a solution to the problem .
Elm2Str+t?(?, m?) former saudi deputy defense minister prince abdul rahman bin abdul aziz said in a press statementthat he was optimistic of the kingdom ?s ability to find a solution to the problem .
Table 13: A translation example, comparing with phrasal decoder.
Figure 3: A testing case: illustrating the derivations from chart decoder. The left panel is source parse tree for the Arabic sentence
? the input to our decoder; the right panel is the English translation together with the simplified derivation tree and alignment
from our decoder output. Each ?X? is a nonterminal in the grammar rule; a ?Block? means a phrase pair is applied to rewrite a
nonterminal; ?Glue? and ?Hiero? means the unlabeled rules were chosen to explain the span as explained in ? 3.4.3 ; ?Tree? means
a labeled rule is applied for the span. For instance, for the source span [1,10], a rule is applied on a partial tree with PV and NP-SBJ;
for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs.
Within the proposed framework, we also presented
several special cases including the translation boundaries
for nonterminals in SCFG for translation. We achieved
a high accuracy of 84.7% for predicting such bound-
aries using MaxEnt model on machine parse trees. Fu-
ture works aim at transforming such non-projectable trees
into projectable form (Eisner, 2003), driven by translation
rules from aligned data(Burkett et al, 2010), and infor-
mative features form both the source 3 and the target sides
(Shen et al, 2008) to enable the system to leverage more
3The BLEU score on MT08-NW has been improved to 57.55 since
the acceptance of this paper, using the proposed technique but with our
GALE P5 data pipeline and setups.
isomorphic trees, and avoid potential detour errors. We
are exploring the incremental decoding framework, like
(Huang and Mi, 2010), to improve pruning and speed.
Acknowledgments
This work was partially supported by the Defense Ad-
vanced Research Projects Agency under contract No.
HR0011-08-C-0110. The views and findings contained in
this material are those of the authors and do not necessar-
ily reflect the position or policy of the U.S. government
and no official endorsement should be inferred.
We are also very grateful to the three anonymous re-
viewers for their suggestions and comments.
854
References
Peter F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estimation. In
Computational Linguistics, volume 19(2), pages 263?331.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammars. In
Proceedings of HLT-NAACL, pages 127?135, Los Angeles,
California, June. Association for Computational Linguistics.
Marine Carpuat, Yuval Marton, and Nizar Habash. 2010.
Reordering matrix post-verbal subjects for arabic-to-english
smt. In 17th Confrence sur le Traitement Automatique des
Langues Naturelles, Montral, Canada, July.
Stanley F. Chen and Stephen M. Chu. 2010. Enhanced word
classing for model m. In Proceedings of Interspeech.
Stanley F. Chen. 2009. Shrinking exponential language mod-
els. In Proceedings of NAACL HLT,, pages 468?476.
David Chiang. 2007. Hierarchical phrase-based translation. In
Computational Linguistics, volume 33(2), pages 201?228.
David Chiang. 2010. Learning to translate with source and
target syntax. In Proc. ACL, pages 1443?1452.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip Resnik.
2009. The University of Maryland statistical machine trans-
lation system for the Fourth Workshop on Machine Trans-
lation. In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation, pages 145?149, Athens, Greece,
March.
Jason Eisner. 2003. Learning Non-Isomorphic tree mappings
for Machine Translation. In Proc. ACL-2003, pages 205?
208.
Ahmad Emami, Stanley F. Chen, Abe Ittycheriah, Hagen
Soltau, and Bing Zhao. 2010. Decoding with shrinkage-
based language models. In Proceedings of Interspeech.
Bryant Huang and Kevin Knight. 2006. Relabeling syntax
trees to improve syntax-based machine translation quality. In
Proc. NAACL-HLT, pages 240?247.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings of
EMNLP, pages 273?283, Cambridge, MA, October. Asso-
ciation for Computational Linguistics.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010.
Soft syntactic constraints for hierarchical phrase-based trans-
lation using latent syntactic distributions. In Proceedings of
the 2010 EMNLP, pages 138?147.
Abraham Ittycheriah and Salim Roukos. 2007. Direct transla-
tion model 2. In Proc of HLT-07, pages 57?64.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In ACL, pages 177?180.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Con-
stituent reordering and syntax models for english-to-japanese
statistical machine translation. In Proceedings of Coling-
2010, pages 626?634, Beijing, China, August.
Yang Liu and Qun Liu. 2010. Joint parsing and translation.
In Proceedings of COLING 2010,, pages 707?715, Beijing,
China, August.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. Spmt: Statistical machine translation with
syntactified target language phraases. In Proceedings of
EMNLP-2006, pages 44?52.
Yuval Marton and Philip Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation. In Pro-
ceedings of ACL-08: HLT, pages 1003?1011.
Haitao Mi and Liang Huang. 2008. Forest-based translation
rule extraction. In Proceedings of EMNLP 2008, pages 206?
214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based
translation. In In Proceedings of ACL-HLT, pages 192?199.
Franz Josef Och. 2003. Minimum error rate training in Statis-
tical Machine Translation. In ACL-2003, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. of the ACL-02), pages 311?318,
Philadelphia, PA, July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new
string-to-dependency machine translation algorithm with a
target dependency language model. In Proceedings of ACL-
08: HLT, pages 577?585, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu, and
Ralph Weischedel. 2010. Statistical machine translation
with a factorized grammar. In Proceedings of the 2010
EMNLP, pages 616?625, Cambridge, MA, October. Asso-
ciation for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In AMTA.
W. Wang, J. May, K. Knight, and D. Marcu. 2010. Re-
structuring, re-labeling, and re-aligning for syntax-based sta-
tistical machine translation. In Computational Linguistics,
volume 36(2), pages 247?277.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. In Computational
Linguistics, volume 23(3), pages 377?403.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding. In
NAACL-HLT 2010, pages 136?144.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim
Tan, and Sheng Li. 2008. A tree sequence alignment-based
tree-to-tree translation model. In ACL-HLT, pages 559?567.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim
Tan. 2009. Forest-based tree sequence to string translation
model. In Proc. of ACL 2009, pages 172?180.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing local
and non-local word-reordering patterns for syntax-based ma-
chine translation. In Proceedings of EMNLP, pages 572?
581, Honolulu, Hawaii, October.
Bing Zhao and Shengyuan Chen. 2009. A simplex armijo
downhill algorithm for optimizing statistical machine trans-
lation decoding parameters. In Proceedings of HLT-NAACL,
pages 21?24, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc. of
NAACL 2006 - Workshop on SMT, pages 138?141.
855
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1230?1238,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Statistical Tree Annotator and Its Applications
Xiaoqiang Luo and Bing Zhao
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
{xiaoluo,zhaob}@us.ibm.com
Abstract
In many natural language applications, there
is a need to enrich syntactical parse trees. We
present a statistical tree annotator augmenting
nodes with additional information. The anno-
tator is generic and can be applied to a va-
riety of applications. We report 3 such ap-
plications in this paper: predicting function
tags; predicting null elements; and predicting
whether a tree constituent is projectable in ma-
chine translation. Our function tag prediction
system outperforms significantly published re-
sults.
1 Introduction
Syntactic parsing has made tremendous progress in
the past 2 decades (Magerman, 1994; Ratnaparkhi,
1997; Collins, 1997; Charniak, 2000; Klein and
Manning, 2003; Carreras et al, 2008), and accu-
rate syntactic parsing is often assumed when devel-
oping other natural language applications. On the
other hand, there are plenty of language applications
where basic syntactic information is insufficient. For
instance, in question answering, it is highly desir-
able to have the semantic information of a syntactic
constituent, e.g., a noun-phrase (NP) is a person or
an organization; an adverbial phrase is locative or
temporal. As syntactic information has been widely
used in machine translation systems (Yamada and
Knight, 2001; Xiong et al, 2010; Shen et al, 2008;
Chiang, 2010; Shen et al, 2010), an interesting
question is to predict whether or not a syntactic con-
stituent is projectable1 across a language pair.
1A constituent in the source language is projectable if it can
be aligned to a contiguous span in the target language.
Such problems can be abstracted as adding addi-
tional annotations to an existing tree structure. For
example, the English Penn treebank (Marcus et al,
1993) contains function tags and many carry seman-
tic information. To add semantic information to the
basic syntactic trees, a logical step is to predict these
function tags after syntactic parsing. For the prob-
lem of predicting projectable syntactic constituent,
one can use a sentence alignment tool and syntac-
tic trees on source sentences to create training data
by annotating a tree node as projectable or not. A
generic tree annotator can also open the door of solv-
ing other natural language problems so long as the
problem can be cast as annotating tree nodes. As
one such example, we will present how to predict
empty elements for the Chinese language.
Some of the above-mentioned problems have
been studied before: predicting function tags were
studied in (Blaheta and Charniak, 2000; Blaheta,
2003; Lintean and Rus, 2007a), and results of pre-
dicting and recovering empty elements can be found
in (Dienes et al, 2003; Schmid, 2006; Campbell,
2004). In this work, we will show that these seem-
ingly unrelated problems can be treated uniformly
as adding annotations to an existing tree structure,
which is the first goal of this work. Second, the
proposed generic tree annotator can also be used
to solve new problems: we will show how it can
be used to predict projectable syntactic constituents.
Third, the uniform treatment not only simplifies the
model building process, but also affords us to con-
centrate on discovering most useful features for a
particular application which often leads to improved
performances, e.g, we find some features are very
effective in predicting function tags and our system
1230
has significant lower error rate than (Blaheta and
Charniak, 2000; Lintean and Rus, 2007a).
The rest of the paper is organized as follows. Sec-
tion 2 describes our tree annotator, which is a con-
ditional log-linear model. Section 3 describes the
features used in our system. Next, three applications
of the proposed tree annotator are presented in Sec-
tion 4: predicting English function tags, predicting
Chinese empty elements and predicting Arabic pro-
jectable constituents. Section 5 compares our work
with some related prior arts.
2 A MaxEnt Tree Annotator Model
The input to the tree annotator is a tree T . While
T can be of any type, we concentrate on the syntac-
tic parse tree in this paper. The non-terminal nodes,
N = {n : n ? T} of T are associated with an
order by which they are visited so that they can be
indexed as n1, n2, ? ? ? , n|T |, where |T | is the num-
ber of non-terminal nodes in T . As an example,
Figure 1 shows a syntactic parse tree with the pre-
fix order (i.e., the number at the up-right corner of
each non-terminal node), where child nodes are vis-
ited recursively from left to right before the parent
node is visited. Thus, the NP-SBJ node is visited
first, followed by the NP spanning duo action,
followed by the PP-CLR node etc.
With a prescribed tree visit order, our tree annota-
tor model predicts a symbol li, where li takes value
from a predefined finite set L, for each non-terminal
node ni in a sequential fashion:
P (l1, ? ? ? , l|T ||T )
=
|T |
?
i=1
P (li|l1, ? ? ? , li?1, T ) (1)
The visit order is important since it determines what
are in the conditioning of Eq. (1).
P (li|l1, ? ? ? , li?1, T ) in this work is a conditional
log linear (or MaxEnt) model (Berger et al, 1996):
P (li|l1, ? ? ? , li?1, T )
= exp
(
?
k ?kgk(li?11 , T, li)
)
Z(li?11 , T )
(2)
where
Z(li?11 , T ) =
?
x?L
exp
(
?
k
?kgk(li?11 , T, x)
)
3
VBZ TO NN NNJJ
Newsnight     returns        to           duo       action       tonight 
NP
VP
S
NP?TMP
2
4
5
6
NP?SBJ
1
PP?CLR
NNP
Figure 1: A sample tree: the number on the upright corner
of each non-terminal node is the visit order.
is the normalizing factor to ensure that
P (li|l1, ? ? ? , li?1, T ) in Equation (2) is a prob-
ability and {gk(li?11 , T, li)} are feature functions.
There are efficient training algorithms to find op-
timal weights relative to a labeled training data set
once the feature functions {gk(li?11 , T, li)} are se-
lected (Berger et al, 1996; Goodman, 2002; Malouf,
2002). In our work, we use the SCGIS training al-
gorithm (Goodman, 2002), and the features used in
our systems are detailed in the next section.
Once a model is trained, at testing time it is ap-
plied to input tree nodes by the same order. Figure 1
highlights the prediction of the function tag for node
3(i.e., PP-CLR-node in the thickened box) after 2
shaded nodes (NP-SBJ node and NP node) are pre-
dicted. Note that by this time the predicted values
are available to the system, while unvisited nodes
(nodes in dashed boxes in Figure 1) can not provide
such information.
3 Features
The features used in our systems are tabulated in Ta-
ble 1. Numbers in the first column are the feature in-
dices. The second column contains a brief descrip-
tion of each feature, and the third column contains
the feature value when the feature at the same row
is applied to the PP-node of Figure 1 for the task of
predicting function tags.
Feature 1 through 8 are non-lexical features in that
all of them are computed based on the labels or POS
tags of neighboring nodes (e.g., Feature 4 computes
the label or POS tag of the right most child), or the
structure information (e.g., Feature 5 computes the
number of child nodes).
1231
Feature 9 and 10 are computed from past pre-
dicted values. When predicting the function tag for
the PP-node in Figure 1, there is no predicted value
for its left-sibling and any of its child node. That?s
why both feature values are NONE, a special sym-
bol signifying that a node does not carry any func-
tion tag. If we were to predict the function tag for
the VP-node, the value of Feature 9 would be SBJ,
while Feature 10 will be instantiated twice with one
value being CLR, another being TMP.
No. Description Value
1 current node label PP
2 parent node label VP
3 left-most child label/tag TO
4 right-most child label/tag NP
5 number of child nodes 2
6 CFG rule PP->TO NP
7 label/tag of left sibling VBZ
8 label/tag of right sibling NP
9 predicted value of left-sibling NONE
10 predicted value of child nodes NONE
11 left-most internal word to
12 right-most internal word action
13 left neighboring external word returns
14 right neighboring external word tonight
15 head word of current node to
16 head word of parent node returns
17 is current node the head child false
18 label/tag of head child TO
19 predicted value of the head child NONE
Table 1: Feature functions: the 2nd column contains the
descriptions of each feature, and the 3rd column the fea-
ture value when it is applied to the PP-node in Figure 1.
Feature 11 to 19 are lexical features or computed
from head nodes. Feature 11 and 12 compute the
node-internal boundary words, while Feature 13 and
14 compute the immediate node-external boundary
words. Feature 15 to 19 rely on the head informa-
tion. For instance, Feature 15 computes the head
word of the current node, which is to for the PP-
node in Figure 1. Feature 16 computes the same for
the parent node. Feature 17 tests if the current node
is the head of its parent. Feature 18 and 19 compute
the label or POS tag and the predicted value of the
head child, respectively.
Besides the basic feature presented in Table 1, we
also use conjunction features. For instance, applying
the conjunction of Feature 1 and 18 to the PP-node
in Figure 1 would yield a feature instance that cap-
tures the fact that the current node is a PP node and
its head child?s POS tag is TO.
4 Applications and Results
A wide variety of language problems can be treated
as or cast into a tree annotating problem. In this
section, we present three applications of the statisti-
cal tree annotator. The first application is to predict
function tags of an input syntactic parse tree; the sec-
ond one is to predict Chinese empty elements; and
the third one is to predict whether a syntactic con-
stituent of a source sentence is projectable, meaning
if the constituent will have a contiguous translation
on the target language.
4.1 Predicting Function Tags
In the English Penn Treebank (Marcus et al, 1993)
and more recent OntoNotes data (Hovy et al,
2006), some tree nodes are assigned a function tag,
which is of one of the four types: grammatical,
form/function, topicalization and miscellaneous. Ta-
ble 2 contains a list of function tags used in the
English Penn Treebank (Bies et al, 1995). The
?Grammatical? row contains function tags marking
the grammatical role of a constituent, e.g., DTV for
dative objects, LGS for logical subjects etc. Many
tags in the ?Form/function? row carry semantic in-
formation, e.g., LOC is for locative expressions, and
TMP for temporal expressions.
Type Function Tags
Grammatical (52.2%) DTV LGS PRD
PUT SBJ VOC
Form/function (36.2%) ADV BNF DIR
EXT LOC MNR
NOM PRP TMP
Topicalization (2.2%) TPC
Miscellaneous (9.4%) CLF CLR HLN TTL
Table 2: Four types of function tags and their relative
frequency
4.1.1 Comparison with Prior Arts
In order to have a direct comparison with (Blaheta
and Charniak, 2000; Lintean and Rus, 2007a), we
use the same English Penn Treebank (Marcus et al,
1993) and partition the data set identically: Section
1232
2-21 of Wall Street Journal (WSJ) data for training
and Section 23 as the test set. We use all features in
Table 1 and build four models, each of which pre-
dicting one type of function tags. The results are
tabulated in Table 3.
As can be seen, our system performs much better
than both (Blaheta and Charniak, 2000) and (Lin-
tean and Rus, 2007a). For two major categories,
namely grammatical and form/function which ac-
count for 96.84% non-null function tags in the test
set, our system achieves a relative error reduction of
77.1% (from (Blaheta and Charniak, 2000)?s 1.09%
to 0.25%) and 46.9%(from (Blaheta and Charniak,
2000)?s 2.90% to 1.54%) , respectively. The per-
formance improvements result from a clean learn-
ing framework and some new features we intro-
duced: e.g., the node-external features, i.e., Feature
13 and 14 in Table 1, can capture long-range statis-
tical dependencies in the conditional model (2) and
are proved very useful (cf. Section 4.1.2). As far as
we can tell, they are not used in previous work.
Type Blaheta00 Lintean07 Ours
Grammar 98.91% 98.45% 99.75%
Form/Func 97.10% 95.15% 98.46%
topic 99.92% 99.87% 99.98%
Misc 98.65% 98.54% 99.41%
Table 3: Function tag prediction accuracies on gold parse
trees: breakdown by types of function tags. The 2nd col-
umn is due to (Blaheta and Charniak, 2000) and 3rd col-
umn due to (Lintean and Rus, 2007a). Our results on the
4th column compare favorably with theirs.
4.1.2 Relative Contributions of Features
Since the English WSJ data set contains newswire
text, the most recent OntoNotes (Hovy et al, 2006)
contains text from a more diversified genres such
as broadcast news and broadcast conversation, we
decide to test our system on this data set as well.
WSJ Section 24 is used for development and Sec-
tion 23 for test, and the rest is used as the training
data. Note that some WSJ files were not included in
the OntoNotes release and Section 23 in OntoNotes
contains only 1640 sentences. The OntoNotes data
statistics is tabulated in Table 4. Less than 2% of
nodes with non-empty function tags were assigned
multiple function tags. To simplify the system build-
ing, we take the first tag in training and testing and
report the aggregated accuracy only in this section.
#-sents #-nodes #-funcNodes
training 71,186 1,242,747 280,755
test 1,640 31,117 6,778
Table 4: Statistics of OntoNotes: #-sents ? number
of sentences; #-nodes ? number of non-terminal nodes;
#-funcNodes ? number of nodes containing non-empty
function tags.
We use this data set to test relative contributions
of different feature groups by incrementally adding
features into the system, and the results are reported
in Table 5. The dummy baseline is predicting the
most likely prior ? the empty function tag, which
indicates that there are 78.21% of nodes without a
function tag. The next line reflects the performance
of a system with non-lexical features only (Feature
1 to 8 in Table 1), and the result is fairly poor with
an accuracy 91.51%. The past predictions (Feature
8 and 9) helps a bit by improving the accuracy to
92.04%. Node internal lexical features (Feature 11
and 12) are extremely useful: it added more than 3
points to the accuracy. So does the node external lex-
ical features (Feature 13 and 14) which added an ad-
ditional 1.52 points. Features computed from head
words (Feature 15 to 19) carry information comple-
mentary to the lexical features and it helps quite a
bit by improving the accuracy by 0.64%. When all
features are used, the system reached an accuracy of
97.34%.
From these results, we can conclude that, unlike
syntactic parsing (Bikel, 2004), lexical information
is extremely important for predicting and recover-
ing function tags. This is not surprising since many
function tags carry semantic information, and more
often than not, the ambiguity can only be resolved
by lexical information. E.g., whether a PP is locative
or temporal PP is heavily influenced by the lexical
choice of the NP argument.
4.2 Predicting Chinese Empty Elements
As is well known, Chinese is a pro-drop language.
This and its lack of subordinate conjunction com-
plementizers lead to the ubiquitous use of empty el-
ements in the Chinese treebank (Xue et al, 2005).
Predicting or recovering these empty elements is
therefore important for the Chinese language pro-
1233
Feature Set Accuracy
prior (guess NONE) 78.21%
Non-lexical labels only 91.52%
+past prediction 92.04%
+node-internal lexical 95.17%
+node-external lexical 96.70%
+head word 97.34%
Table 5: Effects of feature sets: the second row contains
the baseline result when always predicting NONE; Row 3
through 8 contain results by incrementally adding feature
sets.
cessing. Recently, Chung and Gildea (2010) has
found it useful to recover empty elements in ma-
chine translation.
Since empty elements do not have any surface
string representation, we tackle the problem by at-
taching a pseudo function tag to an empty element?s
lowest non-empty parent and then removing the sub-
tree spanning it. Figure 2 contains an example
tree before and after removing the empty element
*pro* and annotating the non-empty parent with
a pseudo function tag NoneL. The transformation
procedure is summarized in Algorithm 1.
In particular, line 2 of Algorithm 1 find the lowest
parent of an empty element that spans at least one
non-trace word. In the example in Figure 2, it would
find the top IP-node. Since *pro* is the left-most
child, line 4 of Algorithm 1 adds the pseudo function
tag NoneL to the top IP-node. Line 9 then removes
its NP child node and all lower children (i.e., shaded
subtree in Figure 2(1)), resulting in the tree in Fig-
ure 2(2).
Line 4 to 8 of Algorithm 1 indicate that there are
3 types of pseudo function tags: NoneL, NoneM,
and NoneR, encoding a trace found in the left, mid-
dle or right position of its lowest non-empty parent.
It?s trivial to recover a trace?s position in a sentence
from NoneL, and NoneR, but it may be ambiguous
for NoneM. The problem could be solved either us-
ing heuristics to determine the position of a middle
empty element, or encoding the positional informa-
tion in the pseudo function tag. Since here we just
want to show that predicting empty elements can be
cast as a tree annotation problem, we leave this op-
tion to future research.
With this transform, the problem of predicting
a trace is cast into predicting the corresponding
JJ
NN NNNN
NP
NP
VP
VP
(1) Original tree with a trace (the left?most child of the top IP?node)
NP
NP
VP
VP
NN NN NN
AD VE JJ VV
IP
IP?NoneL
         ran2hou4  you3  zhuan3men2  dui4wu3  jin4xing2  jian1du1  jian3cha2
(2) After removing trace and its parent node (shaded subtree in (1))
NP
NONE AD
IP
IP
VVVE
*pro*    ran2hou4  you3  zhuan3men2  dui4wu3  jin4xing2  jian1du1  jian3cha2
Figure 2: Transform of traces in a Chinese parse tree by
adding pseudo function tags.
Algorithm 1 Procedure to remove empty elements
and add pseudo function tags.
Input: An input tree
Output: a tree after removing traces (and their
empty parents) and adding pseudo function tags to
its lowest non-empty parent node
1:Foreach trace t
2: Find its lowest ancestor node p spanning at least
one non-trace word
3: if t is p?s left-most child
4: add pseudo tag NoneL to p
5: else if t is p?s right-most child
6: add pseudo tag NoneR to p
7: else
8: add pseudo tag NoneM to p
9: Remove p?s child spanning the trace t and all its
children
1234
pseudo function tag and the statistical tree annota-
tor can thus be used to solve this problem.
4.2.1 Results
We use Chinese Treebank v6.0 (Xue et al, 2005)
and the broadcast conversation data from CTB
v7.0 2. The data set is partitioned into training, de-
velopment and blind test as shown in Table 6. The
partition is created so that different genres are well
represented in different subsets. The training, de-
velopment and test set have 32925, 3297 and 3033
sentences, respectively.
Subset File IDs
Training
0001-0325, 0400-0454, 0600-0840
0500-0542, 2000-3000, 0590-0596
1001-1120, cctv,cnn,msnbc, phoenix 00-06
Dev 0841-0885, 0543-0548, 3001-30751121-1135, phoenix 07-09
Test 0900-0931,0549-0554, 3076-31451136-1151, phoenix 10-11
Table 6: Data partition for CTB6 and CTB 7?s broadcast
conversation portion
We then apply Algorithm 1 to transform trees and
predict pseudo function tags. Out of 1,100,506 non-
terminal nodes in the training data, 80,212 of them
contain pseudo function tags. There are 94 nodes
containing 2 pseudo function tags. The vast major-
ity of pseudo tags ? more then 99.7% ? are attached
to either IP, CP, or VP: 50971, 20113, 8900, respec-
tively.
We used all features in Table 1 and achieved an
accuracy of 99.70% on the development data, and
99.71% on the test data on gold trees.
To understand why the accuracies are so high, we
look into the 5 most frequent labels carrying pseudo
tags in the development set, and tabulate their per-
formance in Table 7. The 2nd column contains the
number of nodes in the reference; the 3rd column the
number of nodes of system output; the 4th column
the number of nodes with correct prediction; and the
5th column F-measure for each label.
From Table 7, it is clear that CP-NoneL and
IP-NoneL are easy to predict. This is not sur-
prising, given that the Chinese language lacks of
2Many files are missing in LDC?s early 2010 release of CTB
7.0, but broadcast conversation portion is new and is used in our
system.
Label numRef numSys numCorr F1
CP-NoneL 1723 1724 1715 0.995
IP-NoneL 3874 3875 3844 0.992
VP-NoneR 660 633 597 0.923
IP-NoneM 440 432 408 0.936
VP-NoneL 135 107 105 0.868
Table 7: 5 most frequent labels carrying pseudo tags and
their performances
complementizers for subordinate clauses. In other
words, left-most empty elements under CP are al-
most unambiguous: if a CP node has an immediate
IP child, it almost always has a left-most empty el-
ement; similarly, if an IP node has a VP node as
the left-most child (i.e., without a subject), it almost
always should have a left empty element (e.g., mark-
ing the dropped pro). Another way to interpret these
results is as follows: when developing the Chinese
treebank, there is really no point to annotate left-
most traces for CP and IP when tree structures are
available.
On the other hand, predicting the left-most empty
elements for VP is a lot harder: the F-measure is
only 86.8% for VP-NoneL. Predicting the right-
most empty elements under VP and middle empty
elements under IP is somewhat easier: VP-NoneR
and IP-NoneM?s F-measures are 92.3% and 93.6%,
respectively.
4.3 Predicting Projectable Constituents
The third application is predicting projectable con-
stituents for machine translation. State-of-the-art
machine translation systems (Yamada and Knight,
2001; Xiong et al, 2010; Shen et al, 2008; Chi-
ang, 2010; Shen et al, 2010) rely heavily on syn-
tactic analysis. Projectable structures are impor-
tant in that it is assumed in CFG-style translation
rules that a source span can be translated contigu-
ously. Clearly, not all source constituents can be
translated this way, but if we can predict whether
a non-terminal source node is projectable, we can
avoid translation errors by bypassing or discourag-
ing the derivation paths relying on non-projectable
constituents, or using phrase-based approaches for
non-projectable constituents.
We start from LDC?s bilingual Arabic-English
treebank with source human parse trees and align-
ments, and mark source constituents as either pro-
1235
NOUN
b# sbb "
" 
l# Alms&wl
the Iraqi official ?s sudden obligations ".
tAr}pAltzAmAt
PREP
Because of "
NOUN
S
PP#
NP#1
NP#2
NP
PP
NP
AlErAqy .
PUNC PREP DET+NOUN DET+ADJADJ PUNCPUNC
Figure 3: An example to show how a source tree is annotated with its alignment with the target sentence.
jectable or non-projectable. The binary annotations
can again be treated as pseudo function tags and the
proposed tree annotator can be readily applied to this
problem.
As an example, the top half of Figure 3 con-
tains an Arabic sentence with its parse tree; the bot-
tom is its English translation with the human word-
alignment. There are three non-projectable con-
stituents marked with ?#?: the top PP# spanning
the whole sentence except the final stop, and NP#1
and NP#2. The PP# node is not projectable due
to an inserted stop from outside; NP#1 is not pro-
jectable because it is involved in a 2-to-2 alignment
with the token b# outside NP#1; NP#2 is aligned
to a span the Iraqi official ?s sudden
obligations ., in which Iraqi official
breaks the contiguity of the translation. It is clear
that a CFG-like grammar will not be able to gener-
ate the translation for NP#2.
The LDC?s Arabic-English bilingual treebank
does not mark if a source node is projectable or
not, but the information can be computed from word
alignment. In our experiments, we processed 16,125
sentence pairs with human source trees for training,
and 1,151 sentence pairs for testing. The statistics
of the training and test data can be found in Table 8,
where the number of sentences, the number of non-
terminal nodes and the number of non-projectable
nodes are listed in Column 2 through 4, respectively.
Data Set #Sents #nodes #NonProj
Training 16,125 558,365 121,201
Test 1,151 40,674 8,671
Table 8: Statistics of the data for predicting projectable
constituents
We get a 94.6% accuracy for predicting pro-
jectable constituents on the gold trees, and an 84.7%
F-measure on the machine-generated parse trees.
This component has been integrated into our ma-
chine translation system (Zhao et al, 2011).
5 Related Work
Blaheta and Charniak (2000) used a feature tree
model to predict function tags. The work was
later extended to use the voted perceptron (Blaheta,
2003). There are considerable overlap in terms of
features used in (Blaheta and Charniak, 2000; Bla-
heta, 2003) and our system: for example, the label of
current node, parent node and sibling nodes. How-
ever, there are some features that are unique in our
work, e.g., lexical features at a constituent bound-
aries (node-internal and node-external words). Table
2 of (Blaheta and Charniak, 2000) contains the ac-
1236
curacies for 4 types of function tags, and our results
in Table 3 compare favorably with those in (Blaheta
and Charniak, 2000). Lintean and Rus (2007a; Lin-
tean and Rus (2007b) also studied the function tag-
ging problem and applied naive Bayes and decision
tree to it. Their accuracy results are worse than
(Blaheta and Charniak, 2000). Neither (Blaheta and
Charniak, 2000) nor (Lintean and Rus, 2007a; Lin-
tean and Rus, 2007b) reported the relative usefulness
of different features, while we found that the lexical
features are extremely useful.
Campbell (2004) and Schmid (2006) studied the
problem of predicting and recovering empty cate-
gories, but they used very different approaches: in
(Campbell, 2004), a rule-based approach is used
while (Schmid, 2006) used a non-lexical PCFG sim-
ilar to (Klein and Manning, 2003). Chung and
Gildea (2010) studied the effects of empty cate-
gories on machine translation and they found that
even with noisy machine predictions, empty cate-
gories still helped machine translation. In this paper,
we showed that empty categories can be encoded as
pseudo function tags and thus predicting and recov-
ering empty categories can be cast as a tree anno-
tating problem. Our results also shed light on some
empty categories can almost be determined unam-
biguously, given a gold tree structure, which sug-
gests that these empty elements do not need to be
annotated.
Gabbard et al (2006) modified Collins? parser to
output function tags. Since their results for predict-
ing function tags are on system parses, they are not
comparable with ours. (Gabbard et al, 2006) also
contains a second stage employing multiple clas-
sifiers to recover empty categories and resolve co-
indexations between an empty element and its an-
tecedent.
As for predicting projectable constituent, it is re-
lated to the work described in (Xiong et al, 2010),
where they were predicting translation boundaries.
A major difference is that (Xiong et al, 2010) de-
fines projectable spans on a left-branching deriva-
tion tree solely for their phrase decoder and models,
while translation boundaries in our work are defined
from source parse trees. Our work uses more re-
sources, but the prediction accuracy is higher (mod-
ulated on a different test data): we get a F-measure
84.7%, in contrast with (Xiong et al, 2010)?s 71%.
6 Conclusions and Future Work
We proposed a generic statistical tree annotator in
the paper. We have shown that a variety of natural
language problems can be tackled with the proposed
tree annotator, from predicting function tags, pre-
dicting empty categories, to predicting projectable
syntactic constituents for machine translation. Our
results of predicting function tags compare favor-
ably with published results on the same data set, pos-
sibly due to new features employed in the system.
We showed that empty categories can be represented
as pseudo function tags, and thus predicting empty
categories can be solved with the proposed tree an-
notator. The same technique can be used to predict
projectable syntactic constituents for machine trans-
lation.
There are several directions to expand the work
described in this paper. First, the results for predict-
ing function tags and Chinese empty elements were
obtained on human-annotated trees and it would be
interesting to do it on parse trees generated by sys-
tem. Second, predicting projectable constituents is
for improving machine translation and we are inte-
grating the component into a syntax-based machine
translation system.
Acknowledgments
This work was partially supported by the Defense
Advanced Research Projects Agency under contract
No. HR0011-08-C-0110. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position or policy
of the U.S. government and no official endorsement
should be inferred.
We are also grateful to three anonymous reviewers
for their suggestions and comments for improving
the paper.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71, March.
Ann Bies, Mark Ferguson, and karen Katz. 1995. Brack-
eting guidelines for treebank II-style penn treebank
project. Technical report, Linguistic Data Consortium.
Daniel M. Bikel. 2004. A distributional analysis of a
lexicalized statistical parsing model. In Dekang Lin
1237
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 182?189, Barcelona, Spain, July. Association
for Computational Linguistics.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 234?240.
Don Blaheta. 2003. Function Tagging. Ph.D. thesis,
Brown University.
Richard Campbell. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 645?652,
Barcelona, Spain, July.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL, Seattle.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, pages 1443?1452.
Tagyoung Chung and Daniel Gildea. 2010. Effects of
empty categories on machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 636?645, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. Annual Meet-
ing of ACL, pages 16?23.
Peter Dienes, P Eter Dienes, and Amit Dubey. 2003. An-
tecedent recovery: Experiments with a trace tagger. In
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 33?40.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick. 2006.
Fully parsing the Penn Treebank. In Proceedings of
Human Language Technology Conference of the North
Amer- ican Chapter of the Association of Computa-
tional Linguistics.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In Pro. of the 40th ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60, New York
City, USA, June. Association for Computational Lin-
guistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 423?430.
Mihai Lintean and V. Rus. 2007a. Large scale exper-
iments with function tagging. In Proceedings of the
International Conference on Knowledge Engineering,
pages 1?7.
Mihai Lintean and V. Rus. 2007b. Naive Bayes and deci-
sion trees for function tagging. In Proceedings of the
International Conference of the FLAIRS-2007.
David M. Magerman. 1994. Natural Language Parsing
As Statistical Pattern Recognition. Ph.D. thesis, Stan-
ford University.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In the Sixth
Conference on Natural Language Learning (CoNLL-
2002), pages 49?55.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
treebank. Computational Linguistics, 19(2):313?330.
Adwait Ratnaparkhi. 1997. A Linear Observed Time
Statistical Parser Based on Maximum Entropy Mod-
els. In Second Conference on Empirical Methods in
Natural Language Processing, pages 1 ? 10.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized PCFGs and slash features. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 177?184, Sydney, Australia, July. Association
for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu,
and Ralph Weischedel. 2010. Statistical machine
translation with a factorized grammar. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 616?625, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In NAACL-HLT 2010.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. Annual Meeting
of the Association for Computational Linguistics.
Bing Zhao, , Young-Suk Lee, Xiaoqiang Luo, and Liu
Li. 2011. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In Proc. of ACL.
1238
