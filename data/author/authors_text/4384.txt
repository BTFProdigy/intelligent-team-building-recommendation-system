Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 9?16, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Data-driven Approaches for Information Structure Identification
Oana Postolache,
Ivana Kruijff-Korbayova?
University of Saarland,
Saarbru?cken, Germany
{oana,korbay}@coli.uni-saarland.de
Geert-Jan M. Kruijff
German Research Center for
Artificial Intelligence (DFKI GmbH)
Saarbru?cken, Germany
gj@dfki.de
Abstract
This paper investigates automatic identi-
fication of Information Structure (IS) in
texts. The experiments use the Prague
Dependency Treebank which is annotated
with IS following the Praguian approach
of Topic Focus Articulation. We auto-
matically detect t(opic) and f(ocus), us-
ing node attributes from the treebank as
basic features and derived features in-
spired by the annotation guidelines. We
present the performance of decision trees
(C4.5), maximum entropy, and rule in-
duction (RIPPER) classifiers on all tec-
togrammatical nodes. We compare the re-
sults against a baseline system that always
assigns f(ocus) and against a rule-based
system. The best system achieves an ac-
curacy of 90.69%, which is a 44.73% im-
provement over the baseline (62.66%).
1 Introduction
Information Structure (IS) is a partitioning of the
content of a sentence according to its relation to
the discourse context. There are numerous theo-
retical approaches describing IS and its semantics
(Halliday, 1967; Sgall, 1967; Vallduv??, 1990; Steed-
man, 2000) and the terminology used is diverse ?
see (Kruijff-Korbayova? and Steedman, 2003) for an
overview. However, all theories consider at least one
of the following two distinctions: (i) a Topic/Focus1
distinction that divides the linguistic meaning of the
sentence into parts that link the sentence content
? We use the Praguian terminology for this distinction.
to the discourse context, and other parts that ad-
vance the discourse, i.e., add or modify informa-
tion; and (ii) a background/kontrast2 distinction be-
tween parts of the utterance which contribute to dis-
tinguishing its actual content from alternatives the
context makes available.
Information Structure is an important factor in de-
termining the felicity of a sentence in a given con-
text. Applications in which IS is crucial are text-
to-speech systems, where IS helps to improve the
quality of the speech output (Prevost and Steedman,
1994; Kruijff-Korbayova? et al, 2003; Moore et al,
2004), and machine translation, where IS improves
target word order, especially that of free word order
languages (Stys and Zemke, 1995).
Existing theories, however, state their principles
using carefully selected illustrative examples. Be-
cause of this, they fail to adequately explain how
different linguistic dimensions cooperate to realize
Information Structure.
In this paper we describe data-driven, machine
learning approaches for automatic identification of
Information Structure; we describe what aspects of
IS we deal with and report results of the performance
of our systems and make an error analysis. For our
experiments, we use the Prague Dependency Tree-
bank (PDT) (Hajic?, 1998). PDT follows the theory
of Topic-Focus Articulation (Hajic?ova? et al, 1998)
and to date is the only corpus annotated with IS.
Each node of the underlying structure of sentences
in PDT is annotated with a TFA value: t(opic), dif-
ferentiated in contrastive and non-contrastive, and
f(ocus). Our system identifies these two TFA val-
ues automatically. We trained three different clas-
? The notion ?kontrast? with a ?k? has been introduced in (Vall-
duv?? and Vilkuna, 1998) to replace what Steedman calls ?fo-
cus?, and to avoid confusion with other definitions of focus.
9
sifiers, C4.5, RIPPER and MaxEnt using basic fea-
tures from the treebank and derived features inspired
by the annotation guidelines. We evaluated the per-
formance of the classifiers against a baseline sys-
tem that simulates the preprocessing procedure that
preceded the manual annotation of PDT, by always
assigning f(ocus), and against a rule-based system
which we implemented following the annotation in-
structions. Our best system achieves a 90.69% accu-
racy, which is a 44.73% improvement over the base-
line (62.66%).
The organization of the paper is as follows.
Section 2 describes the Prague Dependency Tree-
bank and the Praguian approach of Topic-Focus Ar-
ticulation, from two perspectives: of the theoreti-
cal definition and of the annotation guidelines that
have been followed to annotate the PDT. Section 3
presents our experiments, the data settings, results
and error analysis. The paper closes with conclu-
sions and issues for future research (Section 4).
2 Prague Dependency Treebank
The Prague Dependency Treebank (PDT) consists of
newspaper articles from the Czech National Corpus
( ?Cerma?k, 1997) and includes three layers of annota-
tion:
1. The morphological layer gives a full mor-
phemic analysis in which 13 categories are
marked for all sentence tokens (including punc-
tuation marks).
2. The analytical layer, on which the ?surface?
syntax (Hajic?, 1998) is annotated, contains an-
alytical tree structures, in which every token
from the surface shape of the sentence has a
corresponding node labeled with main syntac-
tic functions like SUBJ, PRED, OBJ, ADV.
3. The tectogrammatical layer renders the deep
(underlying) structure of the sentence (Sgall et
al., 1986; Hajic?ova? et al, 1998). Tectogram-
matical tree structures (TGTSs) contain nodes
corresponding only to the autosemantic words
of the sentence (e.g., no preposition nodes) and
to deletions on the surface level; the condi-
tion of projectivity is obeyed, i.e., no cross-
ing edges are allowed; each node of the tree is
assigned a functor such as ACTOR, PATIENT,
ADDRESSEE, ORIGIN, EFFECT, the repertoire
of which is very rich; elementary coreference
links are annotated for pronouns.
2.1 Topic-Focus Articulation (TFA)
The tectogrammatical level of the PDT was moti-
vated by the ever increasing need for large corpora to
include not only morphological and syntactic infor-
mation but also semantic and discourse-related phe-
nomena. Thus, the tectogrammatical trees have been
enriched with features indicating the information
structure of sentences which is a means of showing
their contextual potential.
In the Praguian approach to IS, the content of the
sentence is divided into two parts: the Topic is ?what
the sentence is about? and the Focus represents the
information asserted about the Topic. A prototypical
declarative sentence asserts that its Focus holds (or
does not hold) about its Topic: Focus(Topic) or not-
Focus(Topic).
The TFA definition uses the distinction between
Context-Bound (CB) and Non-Bound (NB) parts of
the sentence. To distinguish which items are CB and
which are NB, the question test is applied, (i.e., the
question for which a given sentence is the appropri-
ate answer is considered). In this framework, weak
and zero pronouns and those items in the answer
which reproduce expressions present in the question
(or associated to those present) are CB. Other items
are NB.
In example (1), (b) is the sentence under investi-
gation, in which CB and NB items are marked. Sen-
tence (a) is the context in which the sentence (b) is
uttered, and sentence (c) is the question for which
the sentence (b) is an appropriate answer:
(1) (a) Tom and Mary both came to John?s party.
(b) John
CB
invited
CB
only
NB
her
NB
.
(c) Whom did John invite?
It should be noted that the CB/NB distinction is
not equivalent to the given/new distinction, as the
pronoun ?her? is NB although the cognitive entity,
Mary, has already been mentioned in the discourse
(therefore is given).
The following rules determine which lexical items
(CB or NB) belong to the Topic or to the Focus of the
sentence (Hajic?ova? et al, 1998; Hajic?ova? and Sgall,
2001):
10
1. The main verb and any of its direct dependents
belong to the Focus if they are NB;
2. Every item that does not depend directly on the
main verb and is subordinated to a Focus el-
ement belongs to the Focus (where ?subordi-
nated to? is defined as the irreflexive transitive
closure of ?depend on?);
3. If the main verb and all its dependents are CB,
then those dependents di of the verb which
have subordinated items sm that are NB are
called ?proxi foci?; the items sm together with
all items subordinated to them belong to the Fo-
cus (i,m > 1);
4. Every item not belonging to the Focus accord-
ing to 1 ? 3 belongs to the Topic.
Applying these rules for the sentence (b) in exam-
ple (1) we find the Topic and the Focus of the sen-
tence: [John invited]Topic [only her]Focus.
It is worth mentioning that although most of the
time, CB items belong to the Topic and NB items
belong to the Focus (as it happens in our exam-
ple too), there may be cases when the Focus con-
tains some NB items and/or the Topic contains some
CB items. Figure 1 shows such configurations: in
the top-left corner the tectogrammatical representa-
tion of sentence (1) (b) is presented together with
its Topic-Focus partitioning. The other three con-
figurations are other possible tectogrammatical trees
with their Topic-Focus partitionings; the top-right
one corresponds to the example (2), the bottom-left
to (3), and bottom-right to (4).
(2) Q: Which teacher did Tom meet?
A: Tom
CB
met
CB
the teacher
CB
of chemistry
NB
.
(3) Q: What did he think about the teachers?
A: He
CB
liked
NB
the teacher
CB
of chemistry
NB
.
(4) Q: What did the teachers do?
A: The teacher
CB
of chemistry
NB
met
NB
his
CB
pupils
NB
.
2.2 TFA annotation
Within PDT, the TFA attribute has been annotated
for all nodes (including the restored ones) from the
tectogrammatical level. Instructions for the assign-
ment of the TFA attribute have been specified in
Figure 1: Topic-Focus partitionings of tectogram-
matical trees.
(Bura?n?ova? et al, 2000) and are summarized in Ta-
ble 1. These instructions are based on the surface
word order, the position of the sentence stress (into-
nation center ? IC)3 and the canonical order of the
dependents.
The TFA attribute has three values:
1. t ? for non-contrastive CB items;
2. f ? for NB items;
3. c ? for contrastive CB items.
In this paper, we do not distinguish between con-
trastive and non-contrastive items, considering both
of them as being just t. In the PDT annotation, the
notation t (from topic) and f (from focus) was chosen
to be used because, as we mentioned earlier, in the
most common cases and in prototypical sentences,
t-items belong to the Topic and f-items to the Focus.
Prior the manual annotation, the PDT corpus was
preprocessed to mark all nodes with the TFA at-
tribute of f, as it is the most common value. Then
the annotators corrected the value according to the
guidelines in Table 1.
Figure 2 illustrates the tectogramatical tree struc-
ture of the following sentence:
(5) Sebeve?dom??m
self-confidence
votroku?
bastards
to
it
ale
but
neotr?a?slo.
not shake
?But it did not shake the self-confidence of those bas-
tards?.
? In the PDT the intonation center is not annotated. However,
the annotators were instructed to use their judgement where
the IC would be if they uttered the sentence.
11
1. The bearer of the IC (typically, the rightmost child of the verb) f
2. If IC is not on the rightmost child, everything after IC t
3. A left-side child of the verb (unless it carries IC) t
4. The verb and the right children of the verb before the f-node (cf. 1) that are canon-
ically ordered
f
5. Embedded attributes (unless repeated or restored) f
6. Restored nodes t
7. Indexical expressions (ja? I, ty you, te?d now, tady here), weak pronouns, pronominal
expressions with a general meaning (ne?kdo somebody, jednou once) (unless they
carry IC)
t
8. Strong forms of pronouns not preceded by a preposition (unless they carry IC) t
Table 1: Annotation guidelines; IC = Intonation Center.
Each node is labeled with the corresponding word?s
lemma, the TFA attribute, and the functor attribute.
For example, votroku? has lemma votrok, the TFA at-
tribute f, and the functor APP (appurtenance).
Figure 2: Tectogramatical tree annotated with t/f.
In order to measure the consistency of the annota-
tion, Interannotator Agreement has been measured
(Vesela? et al, 2004).4 During the annotation pro-
cess, there were four phases in which parallel anno-
tations have been performed; a sample of data was
chosen and annotated in parallel by three annotators.
AGREEMENT 1 2 3 4 AVG
t/c/f 81.32 81.89 76.21 89.57 82.24
t/f 85.42 83.94 84.18 92.15 86.42
Table 2: Interannotator Agreement for TFA assign-
ment in PDT 2.0.
The agreement for each of the four phases, as well
as an average agreement, is shown in Table 2. The
second row of the table displays the percentage of
nodes for which all three annotators assigned the
? In their paper the authors don?t give Kappa values, nor the
complete information needed to compute a Kappa statistics
ourselves.
same TFA value (be it t, c or f). Because in our
experiments we do not differentiate between t and c,
considering both as t, we computed, in the last row
of the table, the agreement between the three anno-
tators after replacing the TFA value c with t.5
3 Identification of topic and focus
In this section we present data-driven, machine
learning approaches for automatic identification of
Information Structure. For each tectogrammatical
node we detect the TFA value t(opic) or f(ocus) (that
is CB or NB). With these values one can apply the
rules presented in Subsection 2.1 in order to find the
Topic-Focus partitioning of each sentence.
3.1 Experimental settings
Our experiments use the tectogrammatical trees
from The Prague Dependency Treebank 2.0.6 Statis-
tics of the experimental data are shown in Table 3.
Our goal is to automatically label the tectogram-
matical nodes with topic or focus. We built ma-
chine learning models based on three different well
known techniques, decision trees (C4.5), rule induc-
tion (RIPPER) and maximum entropy (MaxEnt), in
order to find out which approach is the most suitable
for our task. For C4.5 and RIPPER we use the Weka
implementations (Witten and Frank, 2000) and for
MaxEnt we use the openNLP package.7
? In (Vesela? et al, 2004), the number of cases when the anno-
tators disagreed when labeling t or c is reported; this allowed
us to compute the t/f agreement, by disregarding this number.
? We are grateful to the researchers at the Charles University in
Prague for providing us the data before the PDT 2.0 official
release.
? http://maxent.sourceforge.net/
12
PDT DATA TRAIN DEV EVAL TOTAL
#files 2,53680%
316
10%
316
10%
3,168
100%
#sentences 38,73778.3%
5,228
10.6%
5,477
11.1%
49,442
100%
#tokens 652,70078.3%
87,988
10.6%
92,669
11.1%
833,356
100%
#tecto-nodes 494,75978.3%
66,711
10.5%
70,323
11.2%
631,793
100%
Table 3: PDT data: Statistics for the training, devel-
opment and evaluation sets.
All our models use the same set of 35 features (pre-
sented in detail in Appendix A), divided in two
types:
1. Basic features, consisting of attributes of the
tectogrammatical nodes whose values were
taken directly from the treebank annotation.
We used a total of 25 basic features, that may
have between 2 and 61 values.
2. Derived features, inspired by the annotation
guidelines. The derived features are computed
using the dependency information from the tec-
togrammatical level of the treebank and the
surface order of the words corresponding to
the nodes.8 We also used lists of forms of
Czech pronouns that are used as weak pro-
nouns, indexical expressions, pronouns with
general meaning, or strong pronouns. All the
derived features have boolean values.
3.2 Results
The classifiers were trained on 494,759 instances
(78.3%) (cf. Table 3) (tectogrammatical nodes) from
the training set. The performance of the classifiers
was evaluated on 70,323 instances (11.2%) from the
evaluation set. We compared our models against a
baseline system that assigns focus to all nodes (as it
is the most common value) and against a determinis-
tic, rule-based system, that implements the instruc-
tions from the annotation guidelines.
Table 4 shows the percentages of correctly classi-
fied instances for our models. We also performed a
? In the tectogramatical level in the PDT, the order of the nodes
has been changed during the annotation process of the TFA
attribute, so that all t items precede all f items. Our fea-
tures use the surface order of the words corresponding to the
nodes.
10-fold cross validation, which for C4.5 gives accu-
racy of 90.62%.
BASELINE RULE-BASED C4.5 RIPPER MAXENT
62.66 58.92 90.69 88.46? 88.97
Table 4: Correctly classified instances (the numbers
are given as percentages). ?The RIPPER classifier
was trained with only 40% of the training data.
The baseline value is considerably high due to the
topic/focus distribution in the test set (a similar dis-
tribution characterizes the training set as well). The
rule-based system performs very poorly, although it
follows the guidelines according to which the data
was annotated. This anomaly is due to the fact that
the intonation center of the sentence, which plays a
very important role in the annotation, is not marked
in the corpus, thus the rule-based system doesn?t
have access to this information.
The results show that all three models perform
much better than the baseline and the rule-based sys-
tem. We used the ?? test to examine if the dif-
ference between the three classifiers is statistically
significant. The C4.5 model significantly outper-
forms the MaxEnt model (?? = 113.9, p < 0.001)
and the MaxEnt model significantly outperforms the
RIPPER model although with a lower level of confi-
dence (?? = 9.1, p < 0.01).
The top of the decision tree generated by C4.5 in
the training phase looks like this:
coref = true
| is_member = true
| | POS = ...
| is_member = false
| | is_rightmost = ...
coref = false
| is_generated = true
| | nodetype = ...
| is_generated = false
| | iterativeness = ...
It is worth mentioning that the RIPPER classifier
was built with only 40% of the training set (with
more data, the system crashes due to insufficient
memory). Interestingly and quite surprisingly, the
values of all three classifiers are actually greater than
the interannotator agreement which has an average
of 86.42%.
What is the cause of the classifiers? success? How
come that they perform better than the annotators
themselves? Is it because they take advantage of a
13
large amount of training data? To answer this ques-
tion we have computed the learning curves. They
are shown in the figure 3, which shows that, actu-
ally, after using only 1% of the training data (4,947
instances), the classifiers already perform very well,
and adding more training data improves the results
only slightly. On the other hand, for RIPPER,
adding more data causes a decrease in performance,
and as we mentioned earlier, even an impossibility
of building a classifier.
 0.82
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0  10  20  30  40  50  60  70  80  90
Co
rre
ct
ly 
Cl
as
sif
ie
d 
In
st
an
ce
s
% of Training Data
Figure 3: Learning curves for C4.5 (+),
RIPPER(?), MaxEnt(?) and a na??ve predictor
(2) (introduced in Section 3.3).
3.3 Error Analysis
If errors don?t come from the lack of training data,
then where do they come from? To answer this ques-
tion we performed an error analysis. For each in-
stance (tectogrammatical node), we considered its
context as being the set of values for the features pre-
sented in Appendix A. Table 5 displays in the second
column the number of all contexts. The last three
columns divide the contexts in three groups:
1. Only t ? all instances having these contexts are
assigned t;
2. Only f ? all instances having these contexts
are assigned f;
3. Ambiguous ? some instances that have these
contexts are assigned t and some other are as-
signed f.
The last row of the table shows the number of in-
stances for each type of context, in the training data.
All Only t Only f Ambiguous
#contexts 27,901 9,901 13,009 4,991
#instances 494,759100%
94,056
19.01%
42,048
8.49%
358,655
72.49%
Table 5: Contexts & Instances in the training set.
Table 5 shows that the source of ambiguity (and
therefore of errors) stays in 4,991 contexts that cor-
respond to nodes that have been assigned both t and
f. Moreover these contexts yield the largest amount
of instances (72.49%). We investigated further these
ambiguous contexts and we counted how many of
them correspond to a set of nodes that are mostly as-
signed t (#t > #f), respectively f (#t < #f), and how
many are highly ambiguous (half of the correspond-
ing instances are assigned t and the other half f (#t =
#f)). The numbers, shown in Table 6, suggest that in
the training data there are 41,851 instances (8.45%)
(the sum of highlighted numbers in the third row of
the Table 6) that are exceptions, meaning they have
contexts that usually correspond to instances that are
assigned the other TFA value. There are two ex-
planations for these exceptions: either they are part
of the annotators disagreement, or they have some
characteristics that our set of features fail to capture.
#t > #f #t = #f #t < #f
#ambiguous
contexts 998 833 3,155
#instances
t=50,722
f=4,854
all=55,576
11.23%
t=602
f=602
all=1,204
0.24%
t=35,793
f=266,082
all=301,875
61.01%
Table 6: Ambiguous contexts in the training data.
The error analysis led us to the idea of implementing
a na??ve predictor. This predictor trains on the train-
ing set, and divides the contexts into five groups. Ta-
ble 7 describes these five types of contexts and dis-
plays the TFA value assigned by the na??ve predictor
for each type.
If an instance has a context of type #t = #f, we
decide to assign f because this is the most common
value. Also, for the same reason, new contexts in
the test set that don?t appear in the training set are
assigned f.
The performance of the na??ve predictor on the
evaluation set is 89.88% (correctly classified in-
stances), a value which is significantly higher than
14
Context Type In the training set, instances with
a context of this type are:
Predicted
TFA value
Only t all t t
Only f all f f
#t > #f more t than f t
#t = #f half t, half f f
#t < #f more f than t f
unseen not seen f
Table 7: Na??ve Predictor: its TFA prediction for
each type of context.
the one obtained by the MaxEnt and RIPPER clas-
sifiers (?? = 30.7, p < 0.001 and respectively ??
= 73.3, p < 0.001), and comparable with the C4.5
value, although the C4.5 classifier still performs sig-
nificantly better (?? = 26.3, p < 0.001).
To find out whether the na??ve predictor would im-
prove if we added more data, we computed the learn-
ing curve, shown in Figure 3. Although the curve
is slightly more abrupt than the ones of the other
classifiers, we do not have enough evidence to be-
lieve that more data in the training set would bring
a significant improvement. We calculated the num-
ber of new contexts in the development set, and al-
though the number is high (2,043 contexts), they
correspond to only 2,125 instances. This suggests
that the new contexts that may appear are very rare,
therefore they cannot yield a big improvement.
4 Conclusions
In this paper we investigated the problem of learn-
ing Information Structure from annotated data. The
contribution of this research is to show for the first
time that IS can be successfuly recovered using
mostly syntactic features. We used the Prague De-
pendency Treebank which is annotated with Infor-
mation Structure following the Praguian theory of
Topic Focus Articulation. The results show that we
can reliably identify t(opic) and f(ocus) with over
90% accuracy while the baseline is at 62%.
Issues for further research include, on the one
hand, a deeper investigation of the Topic-Focus Ar-
ticulation in the Prague Dependency Treebank of
Czech, by improving the feature set, considering
also the distinction between contrastive and non-
contrastive t items and, most importantly, by inves-
tigating how we can use the t/f annotation in PDT
(and respectively our results) in order to detect the
Topic/Focus partitioning of the whole sentence.
We also want to benefit from our experience with
the Czech data in order to create an English corpus
annotated with Information Structure. We have al-
ready started to exploit a parallel English-Czech cor-
pus, in order to transfer to the English version the
topic/focus labels identified by our systems.
References
Eva Bura?n?ova?, Eva Hajic?ova?, and Petr Sgall. 2000. Tagging of very large corpora:
Topic-Focus Articulation. In Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING 2000), pages 139?144.
Jan Hajic?. 1998. Building a syntactically annotated corpus: The Prague Depen-
dency Treebank. In Eva Hajic?ova?, editor, Issues of valency and Meaning.
Studies in Honor of Jarmila Panevova?. Karolinum, Prague.
Eva Hajic?ova? and Petr Sgall. 2001. Topic-focus and salience. In Proceedings
of the 39th Annual Meeting of the Association for Computational Linguistics
(ACL 2001), pages 268?273, Toulose, France.
Eva Hajic?ova?, Barbara Partee, and Petr Sgall. 1998. Topic-focus articulation,
tripartite structures, and semantic content. In Studies in Linguistics and Phi-
losophy, number 71. Dordrecht: Kluwer.
M. Halliday. 1967. Notes on transitivity and theme in english, part ii. Journal of
Linguistic, (3):199?244.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003. Discourse and Information
Structure. Journal of Logic, Language and Information, (12):249?259.
Ivana Kruijff-Korbayova?, Stina Erricson, Kepa J. Rodr??gues, and Elena
Karagjosova. 2003. Producing Contextually Appropriate Intonation in an
Information-State Based Dialog System. In Proceeding of European Chapter
of the Association for Computational Linguistics, Budapest, Hungary.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and Michael White. 2004.
Generating Tailored, Comparative Description in Spoken Dialogue. In Pro-
ceedings of the Seventeenth International Florida Artificial Intelligence Re-
search Sociey Conference.
Scott Prevost and Mark Steedman. 1994. Information Based Intonation Synthe-
sis. In Proceedings of the ARPA Workshop on Human Language Technology,
Princeton, USA.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986. The Meaning of the Sen-
tence in Its Semantic and Pragmatic Aspects. Reidel, Dordrecht.
Petr Sgall. 1967. Functional sentence perspective in a generative description.
Prague Studies in Mathematical Linguistics, (2):203?225.
Mark Steedman. 2000. Information Structure and the syntax-phonology inter-
face. Linguistic Inquiry, (34):649?689.
Malgorzata Stys and Stefan Zemke. 1995. Incorporating Discourse Aspects in
English-Polish MT: Towards Robust Implementation. In Recent Advances in
NLP, Velingrad, Bulgaria.
Enrich Vallduv?? and Maria Vilkuna. 1998. On rheme and kontrast. In P. Culicover
and L. McNally, editors, Syntax and Semantics Vol 29: The Limits of Syntax.
Academic Press, San Diego.
Enrich Vallduv??. 1990. The information component. Ph.D. thesis, University of
Pennsylvania.
Frantis?ek ?Cerma?k. 1997. Czech National Corpus: A Case in Many Contexts.
International Journal of Corpus Linguistics, (2):181?197.
Kater?ina Vesela?, Jir??? Havelka, and Eva Hajic?ova. 2004. Annotators? Agreement:
The Case of Topic-Focus Articulation. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC 2004).
Ian H. Witten and Eibe Frank. 2000. Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kaufmann, San Francisco.
15
Appendix A
In this appendix we provide a full list of the feature names and the values they take (a feature for MaxEnt being a
combination of the name, value and the prediction).
BASIC FEATURE POSSIBLE VALUES
nodetype complex, atom, dphr, list, qcomplex
is generated true, false
functor ACT, LOC, DENOM, APP, PAT, DIR1, MAT, RSTR, THL, TWHEN, REG,
CPHR, COMPL, MEANS, ADDR, CRIT, TFHL, BEN, ORIG, DIR3, TTILL,
TSIN, MANN, EFF, ID, CAUS, CPR, DPHR, AIM, EXT, ACMP, THO, DIR2,
RESTR, TPAR, PAR, COND, CNCS, DIFF, SUBS, AUTH, INTT, VOCAT,
TOWH, ATT, RHEM, TFRWH, INTF, RESL, PREC, PRED, PARTL, HER,
MOD, CONTRD
coref true, false
afun Pred, Pnom, AuxV, Sb, Obj, Atr, Adv, AtrAdv, AdvAtr, Coord, AtrObj, ObjAtr,
AtrAtr, AuxT, AuxR, AuxP, Apos, ExD, AuxC, Atv, AtvV, AuxO, AuxZ, AuxY,
AuxG, AuxK, NA
POS N, A, R, V, D, C, P, J, T, Z, I, NA
SUBPOS NN, AA, NA, RR, VB, Db, Vp, C=, Dg, PD, Vf, J, J
?
, P7, P4, PS, Cl, TT, RV, PP,
P8, Vs, Cr, AG, Cn, PL, PZ, Vc, AU, PH, Z:, PW, AC, NX, Ca, PQ, P5, PJ, Cv,
PK, PE, P1, Vi, P9, A2, CC, P6, Cy, C?, RF, Co, Ve, II, Cd, Ch, J*, AM, Cw,
AO, Vt, Vm
is member true, false
is parenthesis true, false
sempos n.denot, n.denot.neg, n.pron.def.demon, n.pron.def.pers, n.pron.indef,
n.quant.def, adj.denot, adj.pron.def.demon, adj.pron.indef, adj.quant.def,
adj.quant.indef, adj.quant.grad, adv.denot.grad.nneg, adv.denot.ngrad.nneg,
adv.denot.grad.neg, adv.denot.ngrad.neg, adv.pron.def, adv.pron.indef, v, NA
number sg, pl, inher, nr, NA
gender anim, inan, fem, neut, inher, nr, NA
person 1, 2, 3, inher, NA
degcmp pos, comp, acomp, sup, nr, NA
verbmod ind, imp, cdn, nr, NA
aspect proc, cpl, nr, NA
tense sim, ant, post, nil, NA
numertype basic, set, kind, ord, frac, NA
indeftype relat, indef1, indef2, indef3, indef4, indef5, indef6, inter, negat, total1, total2,
NA
negation neg0, neg1, NA
politeness polite, basic, inher, NA
deontmod deb, hrt, vol, poss, perm, fac, decl, NA
dispmod disp1, disp0, nil, NA
resultative res1, res0, NA
iterativeness it1, it0, NA
DERIVED FEATURE POSSIBLE VALUES
is rightmost true, false
is rightside from verb true, false
is leftside dependent true, false
is embedded attribute true, false
has repeated lemma true, false
is in canonical order true, false
is weak pronoun true, false
is indexical expression true, false
is pronoun with general meaning true, false
is strong pronoun with no prep true, false
16
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1041?1048,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incremental generation of spatial referring expressions
in situated dialog?
John D. Kelleher
Dublin Institute of Technology
Dublin, Ireland
john.kelleher@comp.dit.ie
Geert-Jan M. Kruijff
DFKI GmbH
Saarbru?cken, Germany
gj@dfki.de
Abstract
This paper presents an approach to incrementally
generating locative expressions. It addresses the is-
sue of combinatorial explosion inherent in the con-
struction of relational context models by: (a) con-
textually defining the set of objects in the context
that may function as a landmark, and (b) sequenc-
ing the order in which spatial relations are consid-
ered using a cognitively motivated hierarchy of re-
lations, and visual and discourse salience.
1 Introduction
Our long-term goal is to develop conversational
robots with whom we can interact through natural,
fluent, visually situated dialog. An inherent as-
pect of visually situated dialog is reference to ob-
jects located in the physical environment (Moratz
and Tenbrink, 2006). In this paper, we present a
computational approach to the generation of spa-
tial locative expressions in such situated contexts.
The simplest form of locative expression is a
prepositional phrase, modifying a noun phrase to
locate an object. (1) illustrates the type of locative
we focus on generating. In this paper we use the
term target (T) to refer to the object that is being
located by a spatial expression and the term land-
mark (L) to refer to the object relative to which
the target?s location is described.
(1) a. the book [T] on the table [L]
Generating locative expressions is part of the
general field of generating referring expressions
(GRE). Most GRE algorithms deal with the same
problem: given a domain description and a target
object, generate a description of the target object
that distinguishes it from the other objects in the
domain. We use distractor objects to indicate the
?The research reported here was supported by the CoSy
project, EU FP6 IST ?Cognitive Systems? FP6-004250-IP.
objects in the context excluding the target that at
a given point in processing fulfill the description
of the target object that has been generated. The
description generated is said to be distinguishing
if the set of distractor objects is empty.
Several GRE algorithms have addressed the is-
sue of generating locative expressions (Dale and
Haddock, 1991; Horacek, 1997; Gardent, 2002;
Krahmer and Theune, 2002; Varges, 2004). How-
ever, all these algorithms assume the GRE compo-
nent has access to a predefined scene model. For
a conversational robot operating in dynamic envi-
ronments this assumption is unrealistic. If a robot
wishes to generate a contextually appropriate ref-
erence it cannot assume the availability of a fixed
scene model, rather it must dynamically construct
one. However, constructing a model containing all
the relationships between all the entities in the do-
main is prone to combinatorial explosion, both in
terms of the number objects in the context (the lo-
cation of each object in the scene must be checked
against all the other objects in the scene) and num-
ber of inter-object spatial relations (as a greater
number of spatial relations will require a greater
number of comparisons between each pair of ob-
jects).1 Also, the context free a priori construction
of such an exhaustive scene model is cognitively
implausible. Psychological research indicates that
spatial relations are not preattentively perceptually
available (Treisman and Gormican, 1988), their
perception requires attention (Logan, 1994; Lo-
gan, 1995). Subjects appear to construct contex-
tually dependent reduced relational scene models,
not exhaustive context free models.
Contributions We present an approach to in-
1In English, the vast majority of spatial locatives are bi-
nary, some notable exceptions include: between, amongst etc.
However, we will not deal with these exceptions in this paper.
1041
crementally generating locative expressions. It ad-
dresses the issue of combinatorial explosion in-
herent in relational scene model construction by
incrementally creating a series of reduced scene
models. Within each scene model only one spatial
relation is considered and only a subset of objects
are considered as candidate landmarks. This re-
duces both the number of relations that must be
computed over each object pair and the number of
object pairs. The decision as to which relations
should be included in each scene model is guided
by a cognitively motivated hierarchy of spatial re-
lations. The set of candidate landmarks in a given
scene is dependent on the set of objects in the
scene that fulfil the description of the target object
and the relation that is being considered.
Overview ?2 presents some relevant back-
ground data. ?3 presents our GRE approach. ?4
illustrates the framework on a worked example
and expands on some of the issues relevant to the
framework. We end with conclusions.
2 Data
If we consider that English has more than eighty
spatial prepositions (omitting compounds such as
right next to) (Landau, 1996), the combinatorial
aspect of relational scene model construction be-
comes apparent. It should be noted that for our
purposes, the situation is somewhat easier because
a distinction can be made between static and dy-
namic prepositions: static prepositions primarily2
denote the location of an object, dynamic preposi-
tions primarily denote the path of an object (Jack-
endoff, 1983; Herskovits, 1986), see (2). How-
ever, even focusing just on the set of static prepo-
sitions does not remove the combinatorial issues
effecting the construction of a scene model.
(2) a. the tree is behind [static] the house
b. the man walked across [dyn.] the road
In general, static prepositions can be divided
into two sets: topological and projective. Topo-
logical prepositions are the category of preposi-
tions referring to a region that is proximal to the
landmark; e.g., at, near, etc. Often, the distinc-
tions between the semantics of the different topo-
logical prepositions is based on pragmatic con-
traints, e.g. the use of at licences the target to be
2Static prepositions can be used in dynamic contexts, e.g.
the man ran behind the house, and dynamic prepositions can
be used in static ones, e.g. the tree lay across the road.
in contact with the landmark, whereas the use of
near does not. Projective prepositions describe a
region projected from the landmark in a particular
direction; e.g., to the right of, to the left of. The
specification of the direction is dependent on the
frame of reference being used (Herskovits, 1986).
Static prepositions have both qualitative and
quantitative semantic properties. The qualitative
aspect is evident when they are used to denote an
object by contrasting its location with that of the
distractor objects. Using Figure 1 as visual con-
text, the locative expression the circle on the left
of the square illustrates the contrastive semantics
of a projective preposition, as only one of the cir-
cles in the scene is located in that region. Taking
Figure 2, the locative expression the circle near
the black square shows the contrastive semantics
of a topological preposition. Again, of the two cir-
cles in the scene only one of them may be appro-
priately described as being near the black square,
the other circle is more appropriately described as
being near the white square. The quantitative as-
pect is evident when a static preposition denotes
an object using a relative scale. In Figure 3 the
locative the circle to the right of the square shows
the relative semantics of a projective preposition.
Although both the circles are located to the right of
the square we can distinguish them based on their
location in the region. Figure 3 also illustrates the
relative semantics of a topological preposition Fig-
ure 3. We can apply a description like the circle
near the square to either circle if none other were
present. However, if both are present we can inter-
pret the reference based on relative proximity to
the landmark the square.
Figure 1: Visual context illustrating contrastive se-
mantics of projective prepositions
Figure 2: Visual context illustrating contrastive se-
mantics of topological prepositions
Figure 3: Visual context illustrating relative se-
mantics of topological and projective prepositions
1042
3 Approach
We base our GRE approach on an extension of
the incremental algorithm (Dale and Reiter, 1995).
The motivation for basing our approach on this al-
gorithm is its polynomial complexity. The algo-
rithm iterates through the properties of the target
and for each property computes the set of distrac-
tor objects for which (a) the conjunction of the
properties selected so far, and (b) the current prop-
erty hold. A property is added to the list of se-
lected properties if it reduces the size of the dis-
tractor object set. The algorithm succeeds when
all the distractors have been ruled out, it fails if all
the properties have been processed and there are
still some distractor objects. The algorithm can
be refined by ordering the checking of properties
according to fixed preferences, e.g. first a taxo-
nomic description of the target, second an absolute
property such as colour, third a relative property
such as size. (Dale and Reiter, 1995) also stipulate
that the type description of the target should be in-
cluded in the description even if its inclusion does
not make the target distinguishable.
We extend the original incremental algorithm
in two ways. First we integrate a model of ob-
ject salience by modifying the condition under
which a description is deemed to be distinguish-
ing: it is, if all the distractors have been ruled out
or if the salience of the target object is greater
than the highest salience score ascribed to any
of the current distractors. This is motivated by
the observation that people can easily resolve un-
derdetermined references using salience (Duwe
and Strohner, 1997). We model the influence
of visual and discourse salience using a function
salience(L), Equation 1. The function returns
a value between 0 and 1 to represent the relative
salience of a landmark L in the scene. The relative
salience of an object is the average of its visual
salience (Svis ) and discourse salience (Sdisc),
salience(L) = (Svis(L) + Sdisc(L))/2 (1)
Visual salience Svis is computed using the algo-
rithm of (Kelleher and van Genabith, 2004). Com-
puting a relative salience for each object in a scene
is based on its perceivable size and its centrality
relative to the viewer focus of attention, return-
ing scores in the range of 0 to 1. The discourse
salience (Sdisc) of an object is computed based
on recency of mention (Hajicova?, 1993) except
we represent the maximum overall salience in the
scene as 1, and use 0 to indicate that the landmark
is not salient in the current context. Algorithm 1
gives the basic algorithm with salience.
Algorithm 1 The Basic Incremental Algorithm
Require: T = target object; D = set of distractor objects.
Initialise: P = {type, colour, size}; DESC = {}
for i = 0 to |P | do
if T salience() >MAXDISTRACTORSALIENCE then
Distinguishing description generated
if type(x) !? DESC then
DESC = DESC ? type(x)
end if
return DESC
else
D? = {x : x ? D,P i(x) = P i(T )}
if |D?| < |D| then
DESC = DESC ? P i(T )
D = {x : x ? D,P i(x) = P i(T )}
end if
end if
end for
Failed to generate distinguishing description
return DESC
Secondly, we extend the incremental algorithm
in how we construct the context model used by
the algorithm. The context model determines to
a large degree the output of the incremental al-
gorithm. However, Dale and Reiter do not de-
fine how this set should be constructed, they only
write: ?[w]e define the context set to be the set of
entities that the hearer is currently assumed to be
attending to? (Dale and Reiter, 1995, pg. 236).
Before applying the incremental algorithm we
must construct a context model in which we can
check whether or not the description generated
distinguishes the target object. To constrain the
combinatorial explosion in relational scene model
construction we construct a series of reduced
scene models, rather than one complex exhaus-
tive model. This construction is driven by a hi-
erarchy of spatial relations and the partitioning of
the context model into objects that may and may
not function as landmarks. These two components
are developed below. ?3.1 discusses a hierarchy of
spatial relations, and ?3.2 presents a classification
of landmarks and uses these groupings to create a
definition of a distinguishing locative description.
In ?3.3 we give the generation algorithm integrat-
ing these components.
3.1 Cognitive Ordering of Contexts
Psychological research indicates that spatial re-
lations are not preattentively perceptually avail-
able (Treisman and Gormican, 1988). Rather,
their perception requires attention (Logan, 1994;
1043
Logan, 1995). These findings point to subjects
constructing contextually dependent reduced rela-
tional scene models, rather than an exhaustive con-
text free model. Mimicking this, we have devel-
oped an approach to context model construction
that constrains the combinatorial explosion inher-
ent in the construction of relational context mod-
els by incrementally building a series of reduced
context models. Each context model focuses on
a different spatial relation. The ordering of the
spatial relations is based on the cognitive load of
interpreting the relation. Below we motivate and
develop the ordering of relations used.
We can reasonably asssume that it takes less
effort to describe one object than two. Follow-
ing the Principle of Minimal Cooperative Effort
(Clark and Wilkes-Gibbs, 1986), one should only
use a locative expression when there is no distin-
guishing description of the target object using a
simple feature based approach. Also, the Princi-
ple of Sensitivity (Dale and Reiter, 1995) states
that when producing a referring expression, one
should prefer features the hearer is known to be
able to interpret and see. This points to a prefer-
ence, due to cognitive load, for descriptions that
identify an object using purely physical and easily
perceivable features ahead of descriptions that use
spatial expressions. Experimental results support
this (van der Sluis and Krahmer, 2004).
Similarly, we can distinguish between the cog-
nitive loads of processing different forms of spa-
tial relations. In comparing the cognitive load as-
sociated with different spatial relations it is im-
portant to recognize that they are represented and
processed at several levels of abstraction. For ex-
ample, the geometric level, where metric prop-
erties are dealt with, the functional level, where
the specific properties of spatial entities deriving
from their functions in space are considered, and
the pragmatic level, which gathers the underly-
ing principles that people use in order to discard
wrong relations or to deduce more information
(Edwards and Moulin, 1998). Our discussion is
grounded at the geometric level.
Focusing on static prepositions, we assume
topological prepositions have a lower percep-
tual load than projective ones, as perceiving
two objects being close to each other is eas-
ier than the processing required to handle frame
of reference ambiguity (Carlson-Radvansky and
Irwin, 1994; Carlson-Radvansky and Logan,
1997). Figure 4 lists the preferences, further
Figure 4: Cognitive load
discerning objects
type as the easi-
est to process, be-
fore absolute grad-
able predicates (e.g.
color), which is still
easier than relative
gradable predicates
(e.g. size) (Dale and Reiter, 1995).
We can refine the topological versus projective
preference further if we consider their contrastive
and relative uses of these relations (?2). Perceiv-
ing and interpreting a contrastive use of a spatial
relation is computationally easier than judging a
relative use. Finally, within projective preposi-
tions, psycholinguistic data indicates a perceptu-
ally based ordering of the relations: above/below
are easier to percieve and interpret than in front
of /behind which in turn are easier than to the right
of /to the left of (Bryant et al, 1992; Gapp, 1995).
In sum, we propose the following ordering: topo-
logical contrastive < topological relative < pro-
jective constrastive < projective relative.
For each level of this hierarchy we require a
computational model of the semantics of the rela-
tion at that level that accomodates both contrastive
and relative representations. In ?2 we noted that
the distinctions between the semantics of the dif-
ferent topological prepositions is often based on
functional and pragmatic issues.3 Currently, how-
ever, more psycholinguistic data is required to dis-
tinguish the cognitive load associated with the dif-
ferent topological prepositions. We use the model
of topological proximity developed in (Kelleher et
al., 2006) to model all the relations at this level.
Using this model we can define the extent of a re-
gion proximal to an object. If the target or one of
the distractor objects is the only object within the
region of proximity around a given landmark this
is taken to model a contrastive use of a topologi-
cal relation relative to that landmark. If the land-
mark?s region of proximity contains more than one
object from the target and distractor object set then
it is a relative use of a topological relation. We
handle the issue of frame of reference ambiguity
and model the semantics of projective prepostions
using the framework developed in (Kelleher et al,
2006). Here again, the contrastive-relative distinc-
3See inter alia (Talmy, 1983; Herskovits, 1986; Vande-
loise, 1991; Fillmore, 1997; Garrod et al, 1999) for more
discussion on these differences
1044
tion is dependent on the number of objects within
the region of space defined by the preposition.
3.2 Landmarks and Descriptions
If we want to use a locative expression, we must
choose another object in the scene to function as
landmark. An implicit assumption in selecting a
landmark is that the hearer can easily identify and
locate the object within the context. A landmark
can be: the speaker (3)a, the hearer (3)b, the scene
(3)c, an object in the scene (3)d, or a group of ob-
jects in the scene (3)e.4
(3) a. the ball on my right [speaker]
b. the ball to your left [hearer]
c. the ball on the right [scene]
d. the ball to the left of the box [an object
in the scene]
e. the ball in the middle [group of ob-
jects]
Currently, we need new empirical research to
see if there is a preference order between these
landmark categories. Intuitively, in most situa-
tions, either of the interlocutors are ideal land-
marks because the speaker can naturally assume
that the hearer is aware of the speaker?s location
and their own. Focusing on instances where an
object in the scene is used as a landmark, several
authors (Talmy, 1983; Landau, 1996; Gapp, 1995)
have noted a target-landmark asymmetry: gener-
ally, the landmark object is more permanently lo-
cated, larger, and taken to have greater geometric
complexity. These characteristics are indicative of
salient objects and empirical results support this
correlation between object salience and landmark
selection (Beun and Cremers, 1998). However, the
salience of an object is intrinsically linked to the
context it is embedded in. For example, in Figure
5 the ball has a relatively high salience, because
it is a singleton, despite the fact that it is smaller
and geometrically less complex than the other fig-
ures. Moreover, in this scene it is the only object
that can function as a landmark without recourse
to using the scene itself or a grouping of objects.
Clearly, deciding which objects in a given con-
text are suitable to function as landmarks is a com-
plex and contextually dependent process. Some
of the factors effecting this decision are object
4See (Gorniak and Roy, 2004) for further discussion on
the use of spatial extrema of the scene and groups of objects
in the scene as landmarks
Figure 5: Landmark salience
salience and the functional relationships between
objects. However, one basic constraint on land-
mark selection is that the landmark should be dis-
tinguishable from the target. For example, given
the context in Figure 5 and all other factors be-
ing equal, using a locative such as the man to the
left of the man would be much less helpful than
using the man to the right of the ball. Following
this observation, we treat an object as a candidate
landmark if the following conditions are met: (1)
the object is not the target, and (2) it is not in the
distractor set either.
Furthermore, a target landmark is a member
of the candidate landmark set that stands in re-
lation to the target. A distractor landmark is a
member of the candidate landmark set that stands
in the considered relation to a distractor object. We
then define a distinguishing locative description
as a locative description where there is target land-
mark that can be distinguished from all the mem-
bers of the set of distractor landmarks under the
relation used in the locative.
3.3 Algorithm
We first try to generate a distinguishing descrip-
tion using Algorithm 1. If this fails, we divide the
context into three components: the target, the dis-
tractor objects, and the set of candidate landmarks.
We then iterate through the set of candidate land-
marks (using a salience ordering if there is more
than one, cf. Equation 1) and try to create a distin-
guishing locative description. The salience order-
ing of the landmarks is inspired by (Conklin and
McDonald, 1982) who found that the higher the
salience of an object the more likely it appears in
the description of the scene it was embedded in.
For each candidate landmark we iterate through
the hierarchy of relations, checking for each re-
lation whether the candidate can function as a tar-
get landmark under that relation. If so we create
a context model that defines the set of target and
distractor landmarks. We create a distinguishing
locative description by using the basic incremental
algorithm to distinguish the target landmark from
the distractor landmarks. If we succeed in generat-
ing a distinguishing locative description we return
1045
the description and stop.
Algorithm 2 The Locative Incremental Algorithm
DESC = Basic-Incremental-Algorithm(T,D)
if DESC != Distinguishing then
create CL the set of candidate landmarks
CL = {x : x != T,DESC(x) = false}
for i = 0 to |CL| by salience(CL) do
for j = 0 to |R| do
if Rj (T,CLi)=true then
TL = {CLi}
DL = {z : z ? CL,Rj (D, z) = true}
LANDDESC = Basic-Incremental-
Algorithm(TL, DL)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Rj ,LANDDESC}
end if
end if
end for
end for
end if
FAIL
If we cannot create a distinguishing locative de-
scription we face two choices: (1) iterate on to the
next relation in the hierarchy, (2) create an embed-
ded locative description distinguishing the land-
mark. We adopt (1) over (2), preferring the dog
to the right of the car over the dog near the car
to the right of the house. However, we can gener-
ate these longer embedded descriptions if needed,
by replacing the call to the basic incremental algo-
rithm for the landmark object with a call to the
whole locative expression generation algorithm,
using the target landmark as the target object and
the set of distractor landmarks as the distractors.
An important point in this context is the issue
of infinite regression (Dale and Haddock, 1991).
A compositional GRE system may in certain con-
texts generate an infinite description, trying to dis-
tinguish the landmark in terms of the target, and
the target in terms of the landmark, cf. (4). But,
this infinite recursion can only occur if the con-
text is not modified between calls to the algorithm.
This issue does not affect Algorithm 2 as each call
to the algorithm results in the domain being parti-
tioned into those objects we can and cannot use as
landmarks. This not only reduces the number of
object pairs that relations must be computed for,
but also means that we need to create a distin-
guishing description for a landmark on a context
that is a strict subset of the context the target de-
scription was generated in. This way the algorithm
cannot distinguish a landmark using its target.
(4) the bowl on the table supporting the bowl
on the table supporting the bowl ...
3.4 Complexity
The computational complexity of the incremental
algorithm is O(nd*nl ), with nd the number of dis-
tractors, and nl the number of attributes in the final
referring description (Dale and Reiter, 1995). This
complexity is independent of the number of at-
tributes to be considered. Algorithm 2 is bound by
the same complexity. For the average case, how-
ever, we see the following. For one, with every
increase in nl , we see a strict decrease in nd : the
more attributes we need, the fewer distractors we
strictly have due to the partitioning into distrac-
tor and target landmarks. On the other hand, we
have the dynamic construction of a context model.
This latter factor is not considered in (Dale and
Reiter, 1995), meaning we would have to multiply
O(nd*nl ) with a constant Kctxt for context con-
struction. Depending on the size of this constant,
we may see an advantage of our algorithm in that
we only consider a single spatial relation each time
we construct a context model, we avoid an expo-
nential number of comparisons: we need to make
at most nd * (nd ? 1) comparisons (and only nd
if relations are symmetric).
4 Discussion
We examplify the approach on the visual scene on
the left of Figure 6. This context consists of two
red boxes R1 and R2 and two blue balls B1 and
B2. Imagine that we want to refer to B1. We be-
gin by calling Algorithm 2. This in turn calls Al-
gorithm 1, returning the property ball. This is not
sufficient to create a distinguishing description as
B2 is also a ball. In this context the set of can-
didate landmarks equals {R1,R2}. We take R1 as
first candidate landmark, and check for topologi-
cal proximity in the scene as modeled in (Kelle-
her et al, 2006). The image on the right of Fig-
ure 6 illustrates the resulting scene analysis: the
green region on the left defines the area deemed to
be proximal to R1, and the yellow region on the
right defines the area proximal to R2. Clearly, B1
is in the area proximal to R1, making R1 a tar-
get landmark. As none of the distractors (i.e., B2)
are located in a region that is proximal to a can-
didate landmark there are no distractor landmarks.
As a result when the basic incremental algorithm
is called to create a distinguishing description for
the target landmark R1 it will return box and this
will be deemed to be a distinguishing locative de-
scription. The overall algorithm will then return
1046
Figure 6: A visual scene and the topological anal-
sis of R1 and R2
the vector {ball, proximal, box} which would re-
sult in the realiser generating a reference of the
form: the ball near the box.5
The relational hierarchy used by the frame-
work has some commonalities with the relational
subsumption hierarchy proposed in (Krahmer and
Theune, 2002). However, there are two important
differences between them. First, an implication of
the subsumption hierarchy proposed in (Krahmer
and Theune, 2002) is that the semantics of the rela-
tions at lower levels in the hierarchy are subsumed
by the semantics of their parent relations. For ex-
ample, in the portion of the subsumption hierarchy
illustrated in (Krahmer and Theune, 2002) the re-
lation next to subsumes the relations left of and
right of. By contrast, the relational hierarchy de-
veloped here is based solely on the relative cogni-
tive load associated with the semantics of the spa-
tial relations and makes not claims as to the se-
mantic relationships between the semantics of the
spatial relations. Secondly, (Krahmer and Theune,
2002) do not use their relational hierarchy to guide
the construction of domain models.
By providing a basic contextual definition of
a landmark we are able to partition the context
in an appropriate manner. This partitioning has
two advantages. One, it reduces the complexity
of the context model construction, as the relation-
ships between the target and the distractor objects
or between the distractor objects themselves do
not need to be computed. Two, the context used
during the generation of a landmark description
is always a subset of the context used for a tar-
get (as the target, its distractors and the other ob-
jects in the domain that do not stand in relation
to the target or distractors under the relation being
considered are excluded). As a result the frame-
work avoids the issue of infinite recusion. Further-
more, the target-landmark relationship is automat-
5For more examples, see the videos available at
http://www.dfki.de/cosy/media/.
ically included as a property of the landmark as its
feature based description need only distinguish it
from objects that stand in relation to one of the dis-
tractor objects under the same spatial relationship.
In future work we will focus on extending the
framework to handle some of the issues effect-
ing the incremental algorithm, see (van Deemter,
2001). For example, generating locative descrip-
tions containing negated relations, conjunctions of
relations and involving sets of objects (sets of tar-
gets and landmarks).
5 Conclusions
We have argued that an if a conversational robot
functioning in dynamic partially known environ-
ments needs to generate contextually appropriate
locative expressions it must be able to construct
a context model that explicitly marks the spatial
relations between objects in the scene. However,
the construction of such a model is prone to the
issue of combinatorial explosion both in terms of
the number objects in the context (the location of
each object in the scene must be checked against
all the other objects in the scene) and number of
inter-object spatial relations (as a greater number
of spatial relations will require a greater number
of comparisons between each pair of objects.
We have presented a framework that addresses
this issue by: (a) contextually defining the set of
objects in the context that may function as a land-
mark, and (b) sequencing the order in which spa-
tial relations are considered using a cognitively
motivated hierarchy of relations. Defining the set
of objects in the scene that may function as a land-
mark reduces the number of object pairs that a spa-
tial relation must be computed over. Sequencing
the consideration of spatial relations means that
in each context model only one relation needs to
be checked and in some instances the agent need
not compute some of the spatial relations, as it
may have succeeded in generating a distinguishing
locative using a relation earlier in the sequence.
A further advantage of our approach stems from
the partitioning of the context into those objects
that may function as a landmark and those that
may not. As a result of this partitioning the al-
gorithm avoids the issue of infinite recursion, as
the partitioning of the context stops the algorithm
from distinguishing a landmark using its target.
We have employed the approach in a system for
Human-Robot Interaction, in the setting of object
1047
manipulation in natural scenes. For more detail,
see (Kruijff et al, 2006a; Kruijff et al, 2006b).
References
R.J. Beun and A. Cremers. 1998. Object reference in a
shared domain of conversation. Pragmatics and Cogni-
tion, 6(1/2):121?152.
D.J. Bryant, B. Tversky, and N. Franklin. 1992. Internal
and external spatial frameworks representing described
scenes. Journal of Memory and Language, 31:74?98.
L.A. Carlson-Radvansky and D. Irwin. 1994. Reference
frame activation during spatial term assignment. Journal
of Memory and Language, 33:646?671.
L.A. Carlson-Radvansky and G.D. Logan. 1997. The influ-
ence of reference frame selection on spatial template con-
struction. Journal of Memory and Language, 37:411?437.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collab-
orative process. Cognition, 22:1?39.
E. Jeffrey Conklin and David D. McDonald. 1982. Salience:
the key to the selection problem in natural language gen-
eration. In ACL Proceedings, 20th Annual Meeting, pages
129?135.
R. Dale and N. Haddock. 1991. Generating referring ex-
pressions involving relations. In Proceeding of the Fifth
Conference of the European ACL, pages 161?166, Berlin,
April.
R. Dale and E. Reiter. 1995. Computational interpretations
of the Gricean maxims in the generation of referring ex-
pressions. Cognitive Science, 19(2):233?263.
I. Duwe and H. Strohner. 1997. Towards a cognitive model
of linguistic reference. Report: 97/1 - Situierte Ku?nstliche
Kommunikatoren 97/1, Univerista?t Bielefeld.
G. Edwards and B. Moulin. 1998. Towards the simula-
tion of spatial mental images using the vorono?? model. In
P. Oliver and K.P. Gapp, editors, Representation and pro-
cessing of spatial expressions, pages 163?184. Lawrence
Erlbaum Associates.
C. Fillmore. 1997. Lecture on Deixis. CSLI Publications.
K.P. Gapp. 1995. Angle, distance, shape, and their relation-
ship to projective relations. In Proceedings of the 17th
Conference of the Cognitive Science Society.
C Gardent. 2002. Generating minimal definite descrip-
tions. In Proceedings of the 40th International Confernce
of the Association of Computational Linguistics (ACL-02),
pages 96?103.
S. Garrod, G. Ferrier, and S. Campbell. 1999. In and on:
investigating the functional geometry of spatial preposi-
tions. Cognition, 72:167?189.
P. Gorniak and D. Roy. 2004. Grounded semantic compo-
sition for visual scenes. Journal of Artificial Intelligence
Research, 21:429?470.
E. Hajicova?. 1993. Issues of sentence structure and discourse
patterns. In Theoretical and Computational Linguistics,
volume 2, Charles University, Prague.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
H. Horacek. 1997. An algorithm for generating referential
descriptions with flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association for Computational
Linguistics, Madrid.
R. Jackendoff. 1983. Semantics and Cognition. Current
Studies in Linguistics. The MIT Press.
J. Kelleher and J. van Genabith. 2004. A false colouring real
time visual salency algorithm for reference resolution in
simulated 3d environments. AI Review, 21(3-4):253?267.
J.D. Kelleher, G.J.M. Kruijff, and F. Costello. 2006. Prox-
imity in context: An empirically grounded computational
model of proximity for processing topological spatial ex-
pressions. In Proceedings ACL/COLING 2006.
E. Krahmer and M. Theune. 2002. Efficient context-sensitive
generation of referring expressions. In K. van Deemter
and R. Kibble, editors, Information Sharing: Reference
and Presupposition in Language Generation and Interpre-
tation. CLSI Publications, Standford.
G.J.M. Kruijff, J.D. Kelleher, G. Berginc, and A. Leonardis.
2006a. Structural descriptions in human-assisted robot vi-
sual learning. In Proceedings of the 1st Annual Confer-
ence on Human-Robot Interaction (HRI?06).
G.J.M. Kruijff, J.D. Kelleher, and Nick Hawes. 2006b. Infor-
mation fusion for visual reference resolution in dynamic
situated dialogue. In E. Andre?, L. Dybkjaer, W. Minker,
H.Neumann, and M. Weber, editors, Perception and Inter-
active Technologies (PIT 2006). Springer Verlag.
B Landau. 1996. Multiple geometric representations of ob-
jects in language and language learners. In P Bloom,
M. Peterson, L Nadel, and M. Garrett, editors, Language
and Space, pages 317?363. MIT Press, Cambridge.
G. D. Logan. 1994. Spatial attention and the apprehension
of spatial realtions. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036.
G.D. Logan. 1995. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533.
R. Moratz and T. Tenbrink. 2006. Spatial reference in
linguistic human-robot interaction: Iterative, empirically
supported development of a model of projective relations.
Spatial Cognition and Computation.
L. Talmy. 1983. How language structures space. In H.L.
Pick, editor, Spatial orientation. Theory, research and ap-
plication, pages 225?282. Plenum Press.
A. Treisman and S. Gormican. 1988. Feature analysis in
early vision: Evidence from search assymetries. Psycho-
logical Review, 95:15?48.
K. van Deemter. 2001. Generating referring expressions:
Beyond the incremental algorithm. In 4th Int. Conf. on
Computational Semantics (IWCS-4), Tilburg.
I van der Sluis and E Krahmer. 2004. The influence of target
size and distance on the production of speech and gesture
in multimodal referring expressions. In Proceedings of
International Conference on Spoken Language Processing
(ICSLP04).
C. Vandeloise. 1991. Spatial Prepositions: A Case Study
From French. The University of Chicago Press.
S. Varges. 2004. Overgenerating referring expressions in-
volving relations and booleans. In Proceedings of the 3rd
International Conference on Natural Language Genera-
tion, University of Brighton.
1048
A context-dependent algorithm for generating
locative expressions in physically situated environments
John D. Kelleher & Geert-Jan M. Kruijff
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
Saarbru?cken, Germany
{kelleher,gj}@dfki.de
Abstract
This paper presents a framework for generating
locative expressions. The framework addresses
the issue of combinatorial explosion inherent in
the construction of relational context models by:
(a) contextually defining the set of objects in the
context that may function as a landmark, and (b)
sequencing the order in which spatial relations are
considered using a cognitively motivated hierar-
chy of relations.
1 Introduction
Our long-term goal is to develop embodied conversational
robots that are capable of natural, fluent visually situated di-
alog with one or more interlocutors. An inherent aspect of
visually situated dialog is reference to objects located in the
physical environment. In this paper, we present a computa-
tional framework for the generation of a spatial locative ex-
pressions in such contexts.
In the simplest form of locative expression, a prepositional
phrase modifies a noun phrase to explicitly specify the loca-
tion of the object. (1) is an example of the type of locative
we focus on generating. In this example, the book is the sub-
ject of the expression and the table is the object. Following
[Langacker, 1987], we use the terms trajector and landmark
to respectively denote the subject and the object of a locative
expression: the location of the trajector is specified relative to
the landmark by the semantics of the preposition.
(1) a. the book [subject] on the table [object]
Generating locative expressions is part of the general field
of generating referring expressions (GRE). Most GRE algo-
rithms deal with the same problem: given a domain descrip-
tion and a target object generate a description of the target
object that distinguishes it from the other objects in the do-
main. The term distractor objects is used to describe the
objects in the context excluding the trajector that at a given
point in processing fulfil the description of the target object
that has been generated. The description generated is said to
be distinguishing when the set of distractor objects is empty.
Several GRE algorithms have addressed the issue of gen-
erating locative expressions [Dale and Haddock, 1991; Ho-
racek, 1997; Gardent, 2002; Krahmer and Theune, 2002;
Varges, 2004]. However, all these algorithms assume the
GRE component has access to a predefined scene model.
For an embodied conversational robot functioning in dynamic
partially known environments this assumption is a serious
drawback. If an agent wishes to generate a contextually ap-
propriate reference it cannot assume the availability of a do-
main model, rather it must dynamically construct one. How-
ever, constructing a model containing all the relationships be-
tween all the entities in the domain is prone to combinatorial
explosion, both in terms of the number of objects in the con-
text (the location of each object in the scene must be checked
against all the other objects in the scene) and number of inter-
object spatial relations (as a greater number of spatial rela-
tions will require a greater number of comparisons between
each pair of objects.1 Moreover, the context free a priori con-
struction of such an exhaustive scene model is cognitively im-
plausible. Psychological research indicates that spatial rela-
tions are not preattentively perceptually available [Treisman
and Gormican, 1988]. Rather, their perception requires atten-
tion [Logan, 1994; 1995]. These findings point to subjects
constructing contextually dependent reduced relational scene
models, rather than an exhaustive context free model.
Contributions In this paper we present a framework for
generating locative expressions. This framework addresses
the issue of combinatorial explosion inherent in relational
scene model construction by incrementally creating a series
of reduced scene models. Within each scene model only one
spatial relations is considered and only a subset of objects
are considered as candidate landmarks. This reduces both the
number of relations that must be computed over each object
pair and the number of object pairs. The decision as to which
relations should be included in each scene model is guided
by a cognitively motivated hierarchy of spatial relations. The
set of candidate landmarks in a given scene is dependent on
the set of objects in the scene that fulfil the description of the
1In English, the vast majority of spatial locatives are binary, some
notable exceptions include: between, amongst etc. However, we will
not deal with these exceptions in this paper
target object and the relation that is being considered.
Overview In ?2 we present some background data relevant
to our discussion. In ?3 we present our GRE framework. In
?4 we illustrate the framework with a worked example and
expand on some of the issues relevant to the framework. We
end with conclusions.
2 Data
When one considers that the English lexicon of spatial prepo-
sitions numbers above eighty members (not considering com-
pounds such as right next to) [Landau, 1996], the combina-
torial aspect of relational scene model construction becomes
apparent. It should be noted that for our purposes, the sit-
uation is somewhat ameliorated by the fact that a distinc-
tion can be made between static and dynamic prepositions:
static prepositions primarily2 denote the location of an object,
dynamic prepositions primarily denote the path of an object
[Jackendoff, 1983; Herskovits, 1986], see (2). However, even
focusing exclusively on the set of static prepositions does not
remove the combinatorial issues effecting the construction of
a scene model.
(2) a. the tree is behind [static] the house
b. the man walked across [dynamic] the road
In general, the set of static prepositions can be decomposed
into two sets called topological and projective. Topological
prepositions are the category of prepositions referring to a
region that is proximal to the landmark; e.g., at, near, etc.
Often, the distinctions between the semantics of the differ-
ent topological prepositions is based on pragmatic contraints,
for example the use of at licences the trajector to be in con-
tact with the landmark, by contrast the use of near does not.
Projective prepositions describe a region projected from the
landmark in a particular direction, the specification of the di-
rection is dependent on the frame of reference being used;
e.g., to the right of, to the left of, etc.
The semantics of static prepositions exhibit both qualita-
tive and quantitative properties. The qualitative aspect of their
semantics is evident when they are used to denote an object
by contrasting its location with the distractor objects location.
Taking Figure 1 as a visual context the locative expression the
circle on the left of the square exhibits the contrastive seman-
tics of a projective preposition. Only one of the circles in the
scene is located in the region to the right of the square. Tak-
ing Figure 2 as a visual context the locative expression the
circle near the black square illustrates the contrastive seman-
tics of a topological preposition. Again, of the two circles in
the scene only one of them may be appropriately described
as being near the black square, the other circle is more ap-
propriately described as being near the white square. The
quantitative aspect of the semantics of static prepositions is
evident when they denote an object using a relative scale. In
the context provided by Figure 3 the locative the circle to the
right of the square exhibits the relative semantics of a projec-
tive preposition. Although both the circles are located to the
2Static prepositions can be used in dynamic contexts, e.g. the
man ran behind the house, and dynamic prepositions can be used in
static ones, e.g. the tree lay across the road.
right of the square it is possible to adjudicate between them
based on their location in the region. The relative semantics
of a topological preposition can also be illustrated using Fig-
ure 3. A description such as the circle near the square could
be applied to either circle if the other circle was not present.
However, when both are present it is possible to interpret the
reference based on their relative proximity to the landmark
the square.
Figure 1: Visual context used to illustrate the contrastive se-
mantics of projective prepositions.
Figure 2: Visual context used to illustrate the contrastive se-
mantics of topological prepositions.
Figure 3: Visual context used to illustrate the relative seman-
tics of topological and projective prepositions.
3 Approach
The approach we adopt to generating locative expressions in-
volves extending the incremental algorithm [Dale and Reiter,
1995]. The motivation for this is the polynomial complexity
of the incremental algorithm. The incremental algorithm iter-
ates through the properties of the target and for each property
computes the set of distractor objects for which (a) the con-
junction of the properties selected so far, and (b) the current
property hold. A property is added to the list of selected prop-
erties if it reduces the size of the distractor object set. The al-
gorithm succeeds when all the distractors have been ruled out,
it fails if all the properties have been processed and there are
still some distractor objects. The algorithm can be refined by
ordering the checking of properties according to fixed prefer-
ences, e.g. first a taxonomic description of the target, second
an absolute property such as colour, third a relative property
such as size. [Dale and Reiter, 1995] also stipulate that the
type description of the target should be included in the de-
scription even if its inclusion does not distinguish the target
from any of the distractors, see Algorithm 1.
However, before applying the incremental algorithm we
must construct a context model within which we can check
whether or not the description generated distinguishes the tar-
get object. In order to constrain the combinatorial issues in-
herent in relational scene model construction we construct
a series of reduced scene models, rather than constructing
one complex exhaustive model. This construction process is
driven by a hierarchy of spatial relations and the partition-
ing of the context model into objects that may and may not
function as landmarks. These two components are developed
Algorithm 1 The Basic Incremental Algorithm
Require: T = target object; D = set of distractor objects.
Initialise: P = {type, colour, size}; DESC = {}
for i = 0 to |P | do
if |D| 6= 0 then
D? = {x : x ? D,P i(x) = P i(T )}
if |D?| < |D| then
DESC = DESC ? P i(T )
D = {x : x ? D,P i(x) = P i(T )}
end if
else
Distinguishing description generated
if type(x) 6? DESC then
DESC = DESC ? type(x)
end if
return DESC
end if
end for
Failed to generate distinguishing description
return DESC
in the next two sections. In ?3.1 we develop the hierarchy
of spatial relations and in ?3.2 we develop a classification of
landmarks and use these groupings to create a definition of
a distinguishing locative description. In ?3.3 we present the
generation algorithm that integrates these components.
3.1 Cognitive Ordering of Contexts
Psychological research indicates that spatial relations are not
preattentively perceptually available [Treisman and Gormi-
can, 1988]. Rather, their perception requires attention [Lo-
gan, 1994; 1995]. These findings point to subjects construct-
ing contextually dependent reduced relational scene models,
rather than an exhaustive context free model. Mimicking this,
we have developed an approach to context model construc-
tion that attempts to constrain the combinatorial explosion
inherent in the construction of relational context models by
incrementally constructing a series of reduced context mod-
els. Each context model focuses on a different spatial relation.
The ordering of the spatial relations is based on the cognitive
load of interpreting the relation. In this section, we motivate
and develop the ordering of relations used.
It seems reasonable to asssume that it takes less effort to
describe one object than two. Consequently, following the
Principle of Minimal Cooperative Effort [Clark and Wilkes-
Gibbs, 1986], a speaker should only use a locative expression
when they cannot create a distinguishing description of the
target object using a simple feature based approach. More-
over, the Principle of Sensitivity [Dale and Reiter, 1995]
states that when producing a referring expression, the speaker
should prefer features which the hearer is known to be able
to interpret and perceive. This points to a preference, due
to cognitive load, towards descriptions that distinguish an ob-
ject using purely physical and easily perceivable features over
descriptions that use spatial expressions. Psycholinguistic
results support this preference [van der Sluis and Krahmer,
2004].
Similarly, we can distinguish between the cognitive loads
of processing different forms of spatial relations. In com-
paring the cognitive load associated with different spatial re-
lations it is important to recognize that they are represented
and processed at several levels of abstraction. For example,
the geometric level, where metric properties are dealt with,
the functional level, where the specific properties of spatial
entities deriving from their functions in space are considered,
and the pragmatic level, which gathers the underlying prin-
ciples that people use in order to discard wrong relations or to
deduce more information [Edwards and Moulin, 1998]. Our
discussion is grounded at the geometric level of representa-
tion and processing.
Focusing on static prepositions, it is reasonable to
Figure 4: Cognitive load of
reference forms
propose that topologi-
cal prepositions have a
lower perceptual load
than projective prepo-
sitions, due to the
relative ease of per-
ceiving two objects
that are close to each
other and the complex
processing required to
handle frame of refer-
ence ambiguity [Carlson-Radvansky and Irwin, 1994;
Carlson-Radvansky and Logan, 1997]. Figure 4 lists these
preferences, with further distinctions among features: objects
type is the easiest to process, before absolute gradable
predicates (e.g. color), which is still easier than relative
gradable predicates (e.g. size) [Dale and Reiter, 1995].
This topological versus projective preference can be fur-
ther refined if we consider the contrastive and relative uses
of these relations noted in ?2. Perceiving and interpreting a
constrastive use of a spatial relation is computationally easier
than judging a relative use. Finally, within the set of projec-
tive prepositions, psycholinguistic data indicates a perceptu-
ally based ordering of the relations: above/below are easier
to percieve and interpret than in front of /behind which in turn
are easier than to the right of /to the left of [Bryant et al,
1992; Gapp, 1995].
In sum, we would like to propose the following ordering
of spatial relations:
1. topological contrastive
2. topological relative
3. projective constrastive [above/below, front/back/,
right/left]
4. projective relative [above/below, front/back, right/left]
For each level of this hierarchy we require a computational
model of the semantics of the relation at that level that acco-
modates both contrastive and relative representations. In ?2
we noted that the distinctions between the semantics of the
different topological prepositions is often based on functional
and pragmatic issues.3 Currently, however, more psycholin-
guistic data is required to distinguish the cognitive load asso-
ciated with the different topological prepositions. We use the
3See inter alia [Talmy, 1983; Herskovits, 1986; Vandeloise,
1991; Fillmore, 1997; Garrod et al, 1999] for more discussion on
these differences
model of topological proximity developed in [Kelleher and
Kruijff, 2005] to model all the relations at this level. Using
this model we can define the extent of a region proximal to
an object. If the trajector or one of the distractor objects is
the only object within the region of proximity around a given
landmark this is taken to model a contrastive use of a topo-
logical relation relative to that landmark. If the landmark?s
region of proximity contains more than one object from the
trajector and distractor object set then it is a relative use of
a topological relation. We handle the issue of frame of ref-
erence ambiguity and model the semantics of projective pre-
postions using the framework developed in [Kelleher and van
Genabith, 2005]. Here again, the contrastive-relative distinc-
tion is dependent on the number of objects within the region
of space defined by the preposition.
3.2 Landmarks and Distinguishing Descriptions
In order to use a locative expression an object in the context
must be selected to function as the landmark. An implicit
assumption in selecting an object to function as a landmark is
that the hearer can easily identify and locate the object within
the context. A landmark can be: the speaker (3)a, the hearer
(3)b, the scene (3)c, an object in the scene (3)d, or a group of
objects in the scene (3)e.4
(3) a. the ball on my right [speaker]
b. the ball to your left [hearer]
c. the ball on the right [scene]
d. the ball to the left of the box [an object in the
scene]
e. the ball in the middle [group of objects]
Currently, new empirical research is required to see if
there is a preference order between these landmark cate-
gories. Intuitively, in most situations, either of the inter-
locutors are ideal landmarks because the speaker can natu-
rally assume that the hearer is aware of the speaker?s location
and their own. Focusing on instances where an object in the
scene is used as a landmark, several authors [Talmy, 1983;
Landau, 1996; Gapp, 1995] have noted a trajector-landmark
asymmetry: generally, the landmark object is more perma-
nently located, larger, and taken to have greater geometric
complexity. These characteristics are indicative of salient ob-
jects and empirical results support this correlation between
object salience and landmark selection [Beun and Cremers,
1998]. However, the salience of an object is intrinsically
linked to the context it is embedded in. For example, in the
context provided by Figure 5 the ball has a relatively high
salience, because it is a singleton, despite the fact that it is
smaller and geometrically less complex than the other fig-
ures. Moreover, in this context, the ball is the only object in
the scene that can function as a landmark without recourse to
using the scene itself or a grouping of objects in the scene.
Clearly, deciding which objects in a given context are suit-
able to function as landmarks is a complex and contextually
dependent process. Some of the factors effecting this decision
4See [Gorniak and Roy, 2004] for further discussion on the use
of spatial extrema of the scene and groups of objects in the scene as
landmarks
Figure 5: Visual context used to illustrate the relative seman-
tics of topological and projective prepositions.
are object salience and the functional relationships between
objects. However, one basic constraint on landmark selection
is that the landmark should be distinguishable from the trajec-
tor. For example, given the context in Figure 5 and all other
factors being equal, using a locative such as the man to the left
of the man would be much less helpful than using the man to
the right of the ball. Following this observation, we treat an
object as a candidate landmark if the trajector object can be
distinguished from it using the basic incremental algorithm,
Algorithm 1.5 Furthermore, a trajector landmark is a mem-
ber of the candidate landmark set that stands in relation to
the trajector and a distractor landmark is a member of the
candidate landmark set that stands in relation to a distractor
object under the relation being considered. Using these cat-
egories of landmark we can define a distinguishing locative
description as a locative description where there is trajector
landmark that can be distinguished from all the members of
the set of distractor landmarks under the relation used in the
locative.
We can illustrate these different categories of landmark
using Figure 6 as the visual context. In this context, if
W1 is taken as the target object, the distractor set equals
{T1,B1,W2,B2}. Running the basic incremental algorithm
would generate the description white block. This distin-
guishes W1 from T1, B1 and B2 but not from W2. Conse-
quently, the set of candidate landmarks equals {T1,B1,B2}.
If we now create a context model for the relation near the set
of trajector landmarks would be {T1,B1} and the set of dis-
tractor landmarks would be {B1,B2}. Obviously, B1 cannot
be distinguished from all the distractor landmarks as it cannot
be distinguished from itself. As a result, B1 cannot function
as the landmark for a distinguishing locative description for
W1 using the relation near. However, T1 can be distinguished
from the distractor landmarks B1 and B2 by its type, triangle.
So the white block near the triangle would be considered a
distinguishing description.
Figure 6: Visual context used to illustrate the different cate-
gories of landmark.
5As noted by one of our reviewers, one unwanted effect of this
definition of a landmark is that it precludes the generation of descrip-
tions that use a landmark that are themselves distinguished using a
locative expression. For example, the block to the right of the block
which has a ball on it.
3.3 Algorithm
The basic approach is to try to generate a distinguishing de-
scription using the standard incremental algorithm. If this
fails, we divide the context into three components:
the trajector: the target object,
the distractor objects: the objects that match the descrip-
tion generated for the target object by the standard in-
cremental algorithm,
the set of candidate landmarks: the objects that do not
match the description generated for the target object by
the standard incremental algorithm.
We then begin to iterate through the hierarchy of relations
and for each relation we create a context model that defines
the set of trajector and distractor landmarks. Once a context
model has been created we iterate through the trajector land-
marks (using a salience ordering if there is more than one)6
and try to create a distinguishing locative description. A dis-
tinguishing locative description is created by using the basic
incremental algorithm to distinguish the trajector landmark
from the distractor landmarks. If we succeed in generating a
distinguishing locative description we return the description
and stop processing. Algorithm 2 lists the steps in the algo-
rithm.
Algorithm 2 The Locative Algorithm
Require: T = target object; D = set of distractor objects; R = hier-
archy of relations.
DESC = Basic-Incremental-Algorithm(T,D)
if DESC 6= Distinguishing then
create CL the set of candidate landmarks
CL = {x : x 6= T,DESC(x) = false}
for i = 0 to |R| do
create a context model for relation Ri consisting of TL the
set of trajector landmarks and DL the set of distractor land-
marks
DL = {z : z ? CL,Ri(D, z) = true}
TL = {y : y ? CL, y 6? DL,Ri(T, y) = true}
for j = 0 to |TL| by salience(TL) do
LANDDESC = Basic-Incremental-Algorithm(TLj , DL)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Ri,LANDDESC}
end if
end for
end for
end if
FAIL
If we cannot create a distinguishing locative description we
are faced with a choice of: (1) iterate on to the next relation
6We model both visual and linguistic salience. Visual salience
is computed using a modified version of the visual saliency algo-
rithm described in [Kelleher and van Genabith, 2004]. Discourse
salience is computed based on recency of mention as defined in [Ha-
jicova?, 1993] except we represent the maximum overall salience in
the scene as 1, and use 0 to indicate object is not salient. We in-
tegrate these two components by summing them and dividing the
result by 2.
in the hierarchy, (2) create an embedded locative description
that distinguishes the landmark. Currently, we prefer option
(1) over (2), preferring the dog to the right of the car over
the dog near the car to the right of the house. However, the
algorithm can generate these longer embedded descriptions if
needed. This is done by replacing the call to the basic incre-
mental algorithm for the trajector landmark object with a call
to the whole locative expression generation algorithm, with
the trajector landmark as the target object and the set of dis-
tractor landmarks as the distractor objects. Algorithm 3 lists
the steps in the recursive version of the algorithm.
Algorithm 3 The Recursive Locative Algorithm
Require: T = target object; D = set of distractor objects; R = hier-
archy of relations.
DESC = Basic-Incremental-Algorithm(T,D)
if DESC 6= Distinguishing then
create CL the set of candidate landmarks
CL = {x : x 6= T,DESC(x) = false}
for i = 0 to |R| do
create a context model for relation Ri consisting of TL the
set of trajector landmarks and DL the set of distractor land-
marks
DL = {z : z ? CL,Ri(D, z) = true}
TL = {y : y ? CL, y 6? DL,Ri(T, y) = true}
for j = 0 to |TL| by salience(TL) do
LANDDESC =
Recursive-Locative-Algorithm(T=TLj ,D=DL,R)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Ri,LANDDESC}
end if
end for
end for
end if
FAIL
For both versions of the locative algorithm an important
consideration is the issue of infinite regression. As noted by
[Dale and Haddock, 1991] a compositional GRE system may,
in certain contexts, generate an infinite description by trying
to distinguish the landmark in terms of the trajector and the
trajector in terms of the landmark, see (4). However, this in-
finite recursion can only occur if the context is not modified
between calls to the algorithm. This issue does not effect Al-
gorithm 2 because each call to the algorithm results in the
domain being partitioned into those objects that can and can-
not be used as landmarks. One effect of this partitioning is a
reduction in the number of object pairs that relations must be
computed for. However, and more importantly for this dis-
cussion, another consequence of this partitioning is that the
process of creating a distinguishing description for a land-
mark is carried out in a context that is a subset of the context
the trajector description was generated in. The distractor set
used during the generation of a landmark description is the
set of distractor landmarks. This minimally excludes the tra-
jector object, since by definition the landmark objects cannot
fulfill the description of the trajector generated by the basic
incremental algorithm. This naturally removes the possibility
for the algorithm to distinguish a landmark using its trajector.
Figure 7: A visual scene and the topological analsis of R1
and R2
(4) the bowl on the table supporting the bowl on the table
supporting the bowl ...
4 Discussion
We can illustrate the framework using the visual context pro-
vided by the scene on the left of Figure 7. This context con-
sists of two red boxes R1 and R2 and two blue balls B1 and
B2. Imagine that we want to refer to B1. We begin by call-
ing the locative incremental algorithm, Algorithm 2. This
in turn calls the basic incremental algorithm, Algorithm 1,
which will return the property ball. However, this is not suf-
ficient to create a distinguishing description as B2 is also a
ball. In this context the set of candidate landmarks equals
{R1,R2} and the first relation in the hierarchy is topological
proximity, which we model using the algorithm developed
in [Kelleher and Kruijff, 2005]. The image on the right of
Figure 7 illustrates the analysis of the scene using this frame-
work: the green region on the left defines the area deemed to
be proximal to R1, and the yellow region on the right defines
the area deemed to be proximal to R2. It is evident that B1
is in the area proximal to R1, consequently R1 is classified as
a trajector landmark. As none of the distractors (i.e., B2) are
located in a region that is proximal to a candidate landmark
there are no distractor landmarks. As a result when the basic
incremental algorithm is called to create a distinguishing de-
scription for the trajector landmark R1 it will return box and
this will be deemed to be a distinguishing locative descrip-
tion. The overall algorithm will then return the vector {ball,
proximal, box} which would result in the realiser generating
a reference of the form: the ball near the box.
The relational hierarchy used by the framework has some
commonalities with the relational subsumption hierarchy pro-
posed in [Krahmer and Theune, 2002]. However, there are
two important differences between them. First, an implica-
tion of the subsumption hierarchy proposed in [Krahmer and
Theune, 2002] is that the semantics of the relations at lower
levels in the hierarchy are subsumed by the semantics of their
parent relations. For example, in the portion of the subsump-
tion hierarchy illustrated in [Krahmer and Theune, 2002] the
relation next to subsumes the relations left of and right of.
By contrast, the relational hierarchy developed here is based
solely on the relative cognitive load associated with the se-
mantics of the spatial relations and makes no claims as to the
semantic relationships between the semantics of the spatial
relations. Secondly, [Krahmer and Theune, 2002] do not use
their relational hierarchy to guide the construction of domain
models.
By providing a basic contextual definition of a landmark
we are able to partition the context in an appropriate manner.
This partitioning has two advantages:
1. it reduces the complexity of the context model con-
struction, as the relationships between the trajector and
the distractor objects or between the distractor objects
themselves do not need to be computed;
2. the context used during the generation of a landmark
description is always a subset of the context used for
a trajector (as the trajector, its distractors and the other
objects in the domain that do not stand in relation to
the trajector or distractors under the relation being con-
sidered are excluded). As a result the framework avoids
the issue of infinite recusion. Furthermore, the trajector-
landmark relationship is automatically included as a
property of the landmark as its feature based descrip-
tion need only distinguish it from objects that stand in
relation to one of the distractor objects under the same
spatial relationship.
.
In future work we will focus on extending the framework
to handle some of the issues effecting the incremental algo-
rithm, see [van Deemter, 2001]. For example, generating
locative descriptions containing negated relations, conjunc-
tions of relations and involving sets of objects (sets of trajec-
tors and landmarks).
5 Conclusions
In this paper we have argued that an if an embodied conver-
sational agent functioning in dynamic partially known envi-
ronments wishes to generate contextually appropriate locative
expressions it must be able to construct a context model that
explicitly marks the spatial relations between objects in the
scene. However, the construction of such a model is prone
to the issue of combinatorial explosion both in terms of the
number of objects in the context (the location of each object
in the scene must be checked against all the other objects in
the scene) and number of inter-object spatial relations (as a
greater number of spatial relations will require a greater num-
ber of comparisons between each pair of objects.
We have presented a framework that address this issue by:
(a) contextually defining the set of objects in the context that
may function as a landmark, and (b) sequencing the order
in which spatial relations are considered using a cognitively
motivated hierarchy of relations. Defining the set of objects in
the scene that may function as a landmark reduces the number
of object pairs that a spatial relation must be computed over.
Sequencing the consideration of spatial relations means that
in each context model only one relation needs to be checked
and in some instances the agent need not compute some of the
spatial relations, as it may have succeeded in generating a dis-
tinguishing locative using a relation earlier in the sequence.
A further advantage of our approach stems from the parti-
tioning of the context into those objects that may function as
a landmark and those that may not. As a result of this parti-
tioning the algorithm avoids the issue of infinite recursion, as
the partitioning of the context stops the algorithm from dis-
tinguishing a landmark using its trajector.
References
[Beun and Cremers, 1998] R.J. Beun and A. Cremers. Object ref-
erence in a shared domain of conversation. Pragmatics and Cog-
nition, 6(1/2):121?152, 1998.
[Bryant et al, 1992] D.J. Bryant, B. Tversky, and N. Franklin. In-
ternal and external spatial frameworks representing described
scenes. Journal of Memory and Language, 31:74?98, 1992.
[Carlson-Radvansky and Irwin, 1994] L.A. Carlson-Radvansky
and D. Irwin. Reference frame activation during spatial term
assignment. Journal of Memory and Language, 33:646?671,
1994.
[Carlson-Radvansky and Logan, 1997] L.A. Carlson-Radvansky
and G.D. Logan. The influence of reference frame selection on
spatial template construction. Journal of Memory and Language,
37:411?437, 1997.
[Clark and Wilkes-Gibbs, 1986] H. Clark and D. Wilkes-Gibbs.
Referring as a collaborative process. Cognition, 22:1?39, 1986.
[Dale and Haddock, 1991] R. Dale and N. Haddock. Generating
referring expressions involving relations. In Proceeding of the
Fifth Conference of the European ACL, pages 161?166, Berlin,
April 1991.
[Dale and Reiter, 1995] R. Dale and E. Reiter. Computational inter-
pretations of the Gricean maxims in the generation of referring
expressions. Cognitive Science, 19(2):233?263, 1995.
[Edwards and Moulin, 1998] G. Edwards and B. Moulin. Towards
the simulation of spatial mental images using the vorono?? model.
In P. Oliver and K.P. Gapp, editors, Representation and process-
ing of spatial expressions, pages 163?184. Lawrence Erlbaum
Associates., 1998.
[Fillmore, 1997] C. Fillmore. Lecture on Deixis. CSLI Publica-
tions, 1997.
[Gapp, 1995] K.P. Gapp. Angle, distance, shape, and their relation-
ship to projective relations. In Proceedings of the 17th Confer-
ence of the Cognitive Science Society, 1995.
[Gardent, 2002] C Gardent. Generating minimal definite descrip-
tions. In Proceedings of the 40th International Confernce of the
Association of Computational Linguistics (ACL-02), pages 96?
103, 2002.
[Garrod et al, 1999] S. Garrod, G. Ferrier, and S. Campbell. In and
on: investigating the functional geometry of spatial prepositions.
Cognition, 72:167?189, 1999.
[Gorniak and Roy, 2004] P. Gorniak and D. Roy. Grounded seman-
tic composition for visual scenes. Journal of Artificial Intelli-
gence Research, 21:429?470, 2004.
[Hajicova?, 1993] E. Hajicova?. Issues of sentence structure and dis-
course patterns. In Theoretical and Computational Linguistics,
volume 2, Charles University, Prague, 1993.
[Herskovits, 1986] A Herskovits. Language and spatial cognition:
An interdisciplinary study of prepositions in English. Studies
in Natural Language Processing. Cambridge University Press,
1986.
[Horacek, 1997] H. Horacek. An algorithm for generating referen-
tial descriptions with flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association for Computational Lin-
guistics, Madrid, 1997.
[Jackendoff, 1983] R. Jackendoff. Semantics and Cognition. Cur-
rent Studies in Linguistics. The MIT Press, 1983.
[Kelleher and Kruijff, 2005] J. Kelleher and G.J. Kruijff. A
context-dependent model of proximity is physically situated en-
vironments. In Proceedings of the 2nd ACL-SIGSEM Workshop
on The Linguistic Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and Applications, 2005.
[Kelleher and van Genabith, 2004] J. Kelleher and J. van Genabith.
A false colouring real time visual salency algorithm for refer-
ence resolution in simulated 3d environments. AI Review, 21(3-
4):253?267, 2004.
[Kelleher and van Genabith, 2005] J. Kelleher and J. van Genabith.
In press: A computational model of the referential semantics of
projective prepositions. In P. Saint-Dizier, editor, Dimensions
of the Syntax and Semantics of Prepositions. Kluwer Academic
Publishers, Dordrecht, The Netherlands, 2005.
[Krahmer and Theune, 2002] E. Krahmer and M. Theune. Efficient
context-sensitive generation of referring expressions. In K. van
Deemter and R. Kibble, editors, Information Sharing: Reference
and Presupposition in Language Generation and Interpretation.
CLSI Publications, Standford, 2002.
[Landau, 1996] B Landau. Multiple geometric representations of
objects in language and language learners. In P Bloom, M. Pe-
terson, L Nadel, and M. Garrett, editors, Language and Space,
pages 317?363. MIT Press, Cambridge, 1996.
[Langacker, 1987] R.W. Langacker. Foundations of Cognitive
Grammar: Theoretical Prerequisites, volume 1. Standford Uni-
versity Press, 1987.
[Logan, 1994] Gordon D. Logan. Spatial attention and the appre-
hension of spatial realtions. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036, 1994.
[Logan, 1995] G.D. Logan. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533, 1995.
[Talmy, 1983] L. Talmy. How language structures space. In H.L.
Pick, editor, Spatial orientation. Theory, research and applica-
tion, pages 225?282. Plenum Press, 1983.
[Treisman and Gormican, 1988] A. Treisman and S. Gormican.
Feature analysis in early vision: Evidence from search assyme-
tries. Psychological Review, 95:15?48, 1988.
[van Deemter, 2001] K. van Deemter. Generating referring expres-
sions: Beyond the incremental algorithm. In 4th Int. Conf. on
Computational Semantics (IWCS-4), Tilburg, 2001.
[van der Sluis and Krahmer, 2004] I van der Sluis and E Krahmer.
The influence of target size and distance on the production of
speech and gesture in multimodal referring expressions. In Pro-
ceedings of International Conference on Spoken Language Pro-
cessing (ICSLP04), 2004.
[Vandeloise, 1991] C. Vandeloise. Spatial Prepositions: A Case
Study From French. The University of Chicago Press, 1991.
[Varges, 2004] S. Varges. Overgenerating referring expressions in-
volving relations and booleans. In Proceedings of the 3rd Inter-
national Conference on Natural Language Generation, Univer-
sity of Brighton, 2004.
Context-sensitive utterance planning for CCG
Geert-Jan M. Kruijff
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI GmbH)
Saarbru?cken, Germany
?gj@dfki.de??
Abstract
The paper presents an approach to utterance plan-
ning, which can dynamically use context informa-
tion about the environment in which a dialogue
is situated. The approach is functional in nature,
using systemic networks to specify its planning
grammar. The planner takes a description of a
communicative goal as input, and produces one
or more logical forms that can express that goal
in a contextually appropriate way. Both the goal
and the resulting logical forms are expressed in a
single formalism as ontologically rich, relational
structures. To realize the logical forms, OpenCCG
is used. The paper focuses primarily on the im-
plementation, but also discusses how the planning
grammar can be based on the grammar used in
OpenCCG, and trained on (parseable) data.
1 Introduction
Conversational robots often need to carry out a dialogue with
other agents while being situated in a dynamic environment.
This poses an interesting challenge: For the robot to con-
verse in a natural manner with other interlocutors its commu-
nication needs to be contextually appropriate, but referential
contexts may naturally change in such a setting. The robot
thus must be actively aware of the environment, and use this
awareness when producing utterances.
Here, we present an approach to utterance planning where
we can use context information to dynamically guide deci-
sions we need to make during planning. These decisions are
paradigmatic in nature, and get us from a logical form stating
a communicative intention, to a logical form (or a set thereof)
expressing the intention in a contextually appropriate way.
We specify a planning grammar as a systemic network, in
the tradition of generation systems for systemic functional
grammar [Mathiessen, 1983; Bateman, 1997]. We process
using an agenda/chart-based algorithm, (meaning there is no
?determinicity? assumption). The utterance planner itself
is embedded in a distributed architecture that makes it pos-
sible to access the various models of the situated environ-
?This research is supported by the EU FP6 IST IP ?Cognitive
Systems for Cognitive Assistants? (CoSy), FP6-004250-IP
ment that the robot maintains. This way we can dynam-
ically use contextual information during the planning pro-
cess. The logical forms we operate on are all specified in a
single formalism, namely Hybrid Logic Dependency Seman-
tics (HLDS), meaning we have a representational continuum
between discourse-level and utterance-level representations
[Kruijff, 2001; Baldridge and Kruijff, 2002], and can guide
utterance planning through content decisions made at higher
levels. The logical form we obtain from the utterance planner
serves as input to a separate OpenCCG realizer [White and
Baldridge, 2003; White, 2004].
The resulting approach is related to [Stone and Doran,
1997; Cassell et al, 2000]. We adopt their idea of an utter-
ance as a description, generated from a communicative goal,
and also use an ?ontologically promiscuous? formalism for
representing meaning [Hobbs, 1985]. We differ in that we
separate out the realizer, though minimize the need for back-
tracking in the planner by allowing for multiple, alternative
logical forms to be sent to the realizer, cf. [Foster and White,
2004]. Also, to establish contextual status of an entity, we can
in principle use any type of model that the robot maintains of
the environment, as long as we have an ontology on which we
can establish a common ground in interpretation.
Our approach places us squarely in the full generation
camp, but there is a continuum: Using the approach to in-
cluding canned text as proposed in [Foster and White, 2004],
we can freely position the actual planner between full gen-
eration and pre-baked generation. We can use the flexibility
of full generation where necessary, notably to achieve con-
textual appropriateness, but if desired we can use more direct
methods to specify content. Our approach owes its perspec-
tive to systemic approaches, particularly KPML [Bateman,
1997]. Where we differ is in the creation of, and relation be-
tween, the resources we use in the parser, the realizer, and
the utterance planner: We use one and the same grammar
for both parsing and realization (though with different algo-
rithms), and we can derive the systemic network for utterance
planning from this grammar (?4) to ensure that we have a
single formulation of the robot?s linguistic knowledge, in the
form of a CCG grammar. We also point out (?4), how we can
in principle train the planner, like e.g. [Stent et al, 2004].
Overview In ?2 we briefly discuss HLDS, and the overall
architecture in which we employ the utterance planner. ?3
presents the planner, focusing on the basic structure of the
planning grammar, and context sensitivity. ?4 discusses how
we can base the planning grammar on the specification of the
grammar we employ for parsing and realization, and how we
can in principle train the planning grammar given a corpus of
(analyzable) utterances.
2 Background
2.1 Hybrid Logic Dependency Semantics
Hybrid Logic Dependency Semantics (HLDS; [Kruijff, 2001;
Baldridge and Kruijff, 2002]) is an ?ontologically promiscu-
ous? [Hobbs, 1985] framework for representing the propo-
sitional content (or meaning) of an expression as an onto-
logically richly sorted, relational structure. The relational
structure connects different bits of meaning using (directed)
labelled edges. The labels on these edges indicate how the
meaning of the dependent contributes to the meaning of the
whole, rooted at the head node that governs the dependent.
This view on the representation of meaning can be traced
back to various theories of valency in dependency gram-
mar and related work on theta-frames. Under this view we
obtain relatively flat representations. These flat represen-
tations are nowadays used in various grammar frameworks,
and are closely related to the conceptual structures found in
AI knowledge representations. [Baldridge and Kruijff, 2002;
White and Baldridge, 2003] show how HLDS representations
can be built compositionally with CCG using unification, and
compare HLDS to other semantic formalisms like Minimal
Recursion Semantics [Copestake et al, 1997].
Formally, we represent the meaning of an expression us-
ing hybrid logic [Blackburn, 2000; Areces, 2000]. Being a
type of modal logic, hybrid logic is ideally suited to capture
relational structures. Furthermore, it adopts an approach to
sorting that enables us to represent meaning as an ontologi-
cally richly sorted structure. This works out as follows.
Hybrid logic is a modal logic, but a modal logic with a
twist. Modal logics are interpreted on models consisting of
states and accessibility relations between these states. How-
ever, we cannot reference these states directly in the language
of a modal logic itself. This is problematic. Modal logic is
often used to model temporal structure, e.g. using Prior?s Past
and Future operators. Unfortunately, all we can express is that
something happened at some point in the past, or will happen
at some point in the future. We cannot specify that point in a
formula, which is counter-intuitive; cf. [Blackburn, 2000].
Hybrid logic addresses this issue by introducing nominals
into the language. A nominal is a type of formula, which is
interpreted as a unique reference to a state in the underlying
model theory of the logic. Nominals are formulas, and hence
equal citizens in the language next to e.g. propositions. There
are several operators that range over nominals, the most ubiq-
uitous for our current purposes being the ?@? operator: @n?
means that ?at the state referred to by n, formula ? holds?.
We use the standard modal operators to model relations:
@n?R?m means that there is a relation R between the nomi-
nals n and m. Particularly important for our purposes is that
we can sort the nominals further, to indicate the ontological
sort or category of the proposition that holds at the state re-
ferred to by the nominal. For example, @{k :person}Kathy rep-
Figure 1: Conceptual architecture
resents the fact that Kathy is a person; note that we can thus
use nominals as (neo-Davidsonian style) discourse referents.
We obtain a relational structure by relating nominals
through modal relations, e.g. (1).
(1) a. Kathy saw Eli.
b. @{s:observing}(see & ?Tense?past
& ?Actor?(k : person & Kathy)
& ?Patient?(e : person & Eli))
Here, s is the nominal (or discourse referent) for the event
see, which we interpret as an observational process in past
tense. Related to the event s are two dependents: we have an
Actor, a person with discourse referent k, being Kathy (the
one doing the seeing); and a Patient, another person but with
discourse referent e, being Eli (the one being seen).
We can flatten the representation in (1b) by rewriting it into
a conjunction of elementary predications, akin to MRS terms:
(2) @{s:observing}(see)
& @{s:observing}?Tense?past
& @{s:observing}?Actor?(k : person)
& @{k :person}(Kathy)
& @{s:observing}?Patient?(e : person)
& @{e:person}(Eli)
The flattened representation in (2) illustrates that
we have basically three types of elementary predica-
tions: lexical predications ? @{k :person}(Kathy), features
? @{s:observing}?Tense?past, and dependency relations ?
@{s:observing}?Actor?(k : person).
2.2 System architecture
Figure 1 describes the conceptual architecture that underlies
our system. As a cognitively motivated architecture, it speci-
fies the underlying infrastructure for the communication abil-
ities for an intelligent embodied agent.
The distributed nature of the architecture is inspired by
the general tendency to see cognition as a network of con-
current, situated processes, e.g. [Minsky, 1986; Langley and
Laird, 2002]. Distributed information processing facilitates
the concurrent maintenance of several models of the environ-
ment, possibly using different means for representation and
interpretation. Furthermore, we can adopt a localised ap-
proach to the processing and fusion of information stemming
from different modalities (local cross-modal fusion), guided
by passive attention mechanisms [Chum and Wolfe, 2001]
and active attention mechanisms through a short-term work-
ing memory with activated concepts and their associations.
The architecture is layered in that we distinguish different
levels of information processing. The levels basically corre-
spond to the reactive (or perceptual), deliberative, and meta-
level processes in the cognitive architecture presented in [Slo-
man, 2001]. Like cognitive robotics [Reiter, 2001] we use
logic as a representational medium at the deliberative level,
but with an explicit relation to lower-level perceptual pro-
cesses [Shanahan, 2000; Shanahan and Witkowski, 2001].
We use context models to represent the deliberative in-
terpretation of the situation relative to a particular modality,
on the basis of which future states can be anticipated and
planned. Each context model maintains a (model-specific, lo-
cal) salience measure over the information in the model, to in-
dicate what is currently activated. Examples of context mod-
els are the dialogue context model, capturing the dialogue his-
tory which serves as the background against which new dia-
logue moves are interpreted and planned, or the action context
which keeps track of the current status of tasks and the overall
action plan. Figure 1 shows these context models as modal-
ity specific context models. It also includes a belief context.
The belief context model captures global cross-modal fusion,
achieved by fusing information across the different modal-
ities at least at the level of token identification. We estab-
lish a common ground for interpretation across modalities
by relating each layer to a set of ontologies that model cat-
egories on which the events, states, and entities at that layer
can be interpreted, following recent work in information fu-
sion [Wache et al, 2001] and dialogue systems [Gurevych et
al., 2003]. There are different levels of granularity for the
common ground we may be able to establish, due to the po-
tential for hybridity across the different local representations
used in the architecture. On the low end of the scale we have
type identity, to type/token identity, to the high end where
we have fully shared representations. The granularity of the
common ground we are able to establish determines to what
extend information can be fused.
For the purposes of this paper we use the example imple-
mented architecture, shown in Figure 2. The goal of this
instance is to enable a robot to conduct a simple dialogue
about a dynamic, visual scene. We have implemented the
distributed infrastructure using the Open Agent Architecture
[Cheyer and Martin, 2001]; the different boxes in Figure 2 are
processes implemented as OAA agents.
On the interpretation side, we have several layers of pro-
cessing for a speech dimension, and for a vision dimension.
We process the acoustic signal using Sphinx4 [Walker et al,
2004] with a domain-specific, English language model, and
then parse the recognized string using the OpenCCG parser
for combinatory categorial grammar [Baldridge, 2002]. The
parser yields a logical form of the meaning of the string, rep-
resented as an HLDS logical form. This logical form is inter-
preted further in a dialogue process, which maintains a model
of the dialogue history. In parallel to the speech dimension,
Figure 2: Example implemented architecture
we also have processes that interpret the visual scene. We
use a visual recognition and classification algorithm based on
OpenCV 1 that produces a representation of an object in terms
of its type, physical properties, and position. We interpret
these representations on a model of the visual scene, captur-
ing proximal and projective spatial relations between objects
[Kelleher and Kruijff, 2005b].
For production, the architecture in Figure 2 includes only
spoken language as an output modality. The dialogue planner
constructs an HLDS logical form that specifies the communi-
cation goal reflecting how the belief context could be updated
with the information coming from the acoustic and visual di-
mensions. This logical form is taken by the utterance planner,
which expands this logical form to a full logical form that
OpenCCG can realize as a well-formed string [White, 2004].
Finally, we use FreeTTS2 for speech synthesis.
3 Utterance planning
Following the systemic tradition, we formulate a planning
grammar as a network of systems. A system represents a
paradigmatic choice, i.e. a choice about an aspect of the
meaning to be specified by the logical form we are planning.
We specify the decision process involved in this choice as a
decision tree or chooser, associated with the system. In the
chooser, we can pose several inquiries about the logical form
and the contextual status of discourse referents, to guide the
decision process. On the basis of the choice we make, the
system performs one or more operations on the logical form,
to expand it; we thus reflect grammatical features directly as
content in the logical form.
A system consists of an entry condition, actions associated
with the different choices the associated chooser can make,
and an output. Both the entry condition and the output of the
system take the shape of an HLDS logical form, and an indi-
cation of the locus within that logical form. As a result, the
combination of locus and output logical form of one system
may be the entry condition for another system. It is in this
way that we obtain a network of systems.
1We would like to thank Somboon Hongeng from Birmingham
University for the implementation of this module.
2http://freetts.sf.net
<system id=??evidencing-modality?? region=???? metafunction=??ideational??>
<chooser id=??c-evidencing-mod??/>
<conditions>
<condition features=??@type:process??/>
</conditions>
<actions>
<action choice=??vision??>
<assign-type type=??observing??/>
<add-proposition propositions=??@see??/>
<add-relation mode=??Actor?? nomvar=??sp?? type=??speaker??/>
<identify-nomvar mode=??Patient?? nomvar=??obj??/>
<move-locus nomvar=??obj??/>
</action>
:
</actions>
</system>
Figure 3: Example of a system
Figure 3 provides an example specification of a system,
called evidencing-modality. The point of the system is to
specify the kind of mental process the current locus in the
logical form should express to refer to the modality in which
an entity can be grounded. The chooser associated with this
system is c-evidencing-mod; cf. Figure 5 and below.
One of the possible answers of this chooser is vision, i.e.
the visually situated context is the ?strongest? modality in
which we can ground the entity that is part of the commu-
nicative goal. This results in the system performing several
actions on the logical form:
1. assign-type specifies the type of the locus as observing
2. add-proposition adds the proposition see to the nomi-
nal of the locus
3. add-relation adds a relation of type Actor between the
locus and a nominal sp of type speaker
4. identify-nomvar identifies the nominal to which the Pa-
tient relation points, and then moves the locus to this
nominal (identified by variable name obj ).3
We have two more operations that a system can specify,
besides the above ones. The operation add-feature adds a
feature and a value to the nominal of the current locus. This
operation together with the operations assign-type, add-
proposition and add-relation gives us a basic inventory for
extending a logical form through substitution:
? add-feature:
@{n:nomv}?=?@{n:nomv}?&@{n:nomv}?Feat?(value).
? add-proposition:
@{n:nomv}? =? @{n:nomv}(? & prop)
? add-relation:
@{n:nomv}? =? @{n:nomv}? & @{n:nomv}?Rel?n? :
nomv?.
? assign-type:
@{n:nomv}? =? @{n:type}?
3We can define for a variable whether it is to have system-local
scope, or global scope. This way, we can reference other parts of a
logical form, outside the scope of the subtree that is currently in the
locus.
Furthermore, we have an operation adjoin-lf. With this
operation we can extend the current logical form by adjoining
another logical form into it. We can explain adjunction using
the illustration in Figure 4.
Figure 4: Adjunction
We start with a logical
form which contains the
greyed subtree rooted by a
nominal n? of type t? (1).
Next, we remove that sub-
tree, leaving an argument
position for a nominal of
type t? in the logical form
rooted by n : t, (2). We
now insert a new subtree,
rooted by a nominal n?? of type t?, which itself also contains
an argument position of type t? into which we can slot the
greyed subtree of n? : t?. The adjunction operator, the above
substitution operators, and the identify-nomvar, give us a
complete inventory of operations for defining logical forms
as directed graphs in HLDS.
The main way we bring in context-sensitivity is through
the types of inquiries we can pose in a chooser. In the archi-
tecture, the utterance planner runs as an agent with access to
the various short-term and long-term models that the robot
maintains for the environment it is situated in; cf. Figure
2. Each of these models is equipped with attentional mech-
anisms, which model current attentional prominence (short-
term working memory) or salience (longer-term memories,
like models of the discursive or visual context).
Using the inquiries built into the utterance planner, we can
query the architecture for the contextual status of an entity,
and what the strongest evidencing modality is in which we
can ground the entity.4 Based on the results of these inquiries,
we can decide how to reflect the contextual status of an en-
tity or an event in the logical form. Contextual appropriate-
ness can thereby take various forms, not just in terms of us-
ing information structure to reflect attentional status, but also
4We assume that the visual context is the strongest modality,
followed by the discursive context, and then the (personal) belief
context.
<chooser id=??c-evidencing-mod?? region=???? metafunction=??textual??>
<dectree>
<choicenode answer=??*TOP*??>
<inquiry id=??fetch-evid-modality?? type=??string??
answerset=??@vision @dialogue @beliefs??>
<f-mod-maxevid/>
</inquiry>
<choicenode answer=??vision??>
<result val=??vision??/>
</choicenode>
:
</choicenode>
</dectree>
</chooser>
Figure 5: Example of a chooser
by appealing to the appropriate (and maximally informative)
modal context to refer.
Another way we bring in context-sensitivity is through the
inclusion of algorithms for generating referring expressions.
One of the actions a system can perform is to call a dedicated
GRE algorithm, to plan a contextually appropriate referring
expression for an entity. Currently, the planner has access
to an extension of the incremental Dale & Reiter GRE al-
gorithm, which is able to generate a referring expression for
an entity on the basis of its physical properties as well as its
spatial relations to other entities in the visually situated con-
text. The algorithm returns a full logical form for the mean-
ing of the referring expressions. This algorithm is described
in [Kelleher and Kruijff, 2005a].
Example. To illustrate the way logical forms are planned,
consider the network in Figure 6. The network provides an
illustration of how we could plan simple types of grounding
feedback, for example to a statement about the visual scene
like ?The red ball is near the blue box.?
Depending on whether the robot is able to verify the
statement against its models of the dialogue history and
the visual scene, the dialogue planner generates a simple
communicative goal providing feedback to the statement.
For example, if the robot is able to resolve the referents
both in the dialogue and the visual contexts, then the di-
alogue planner will send an acknowledgment to the utter-
ance planner: @{d:disc?vantagepoint}?Acknowledgment?(p1 :
process) & @{p1 :process}?Patient?(o1 : phys ? obj). Here,
o1 is an identifier for the red ball in the belief context, where
we fuse the information about identifiers in the dialogue and
visual context.
The utterance planner makes d the locus, and enters system
(1). Here, a chooser inquires after the dialogue move to deter-
mine the polarity of the utterance. Because we need to pro-
duce an acknowledgment, the polarity is to be positive. The
utterance planner now extends the logical form to express the
polarity through ?yes? and a positive state: We add yes to the
nominal d, and adjoin a positive state construction between d
and the process p1. We now get the following logical form:
(3) @{d:disc?vantagepoint}(yes)
& @{d:disc?vantagepoint}?Acknowl.?(p1 : process)
& @{s:state}(do)
& @{s:state}?Scope?(p1 : process))
& @{p1 :process}?Patient?(o1 : phys? obj)
Finally, we move the locus to the process p1, identified by
the variable px in the system.
The type of p1, process, satisfies the entry condition for
system (2). In this system, we inquire after the strongest ev-
idencing modality for the entity referenced in the commu-
nicative goal. Because the strongest modality is the visual
context, we turn the process into a mental process of type
?observing?. Although the entity can also be grounded in the
dialogue and belief contexts, it would be less appropriate to
say ?Yes I do understand ...? or ?Yes I do believe ..? than it
would be ?Yes I do see ...?. The remaining actions in the sys-
tem add the proposition see to p1, and add an Actor relation
to the speaker:
(4) @{d:disc?vantagepoint}(yes)
& @{d:disc?vantagepoint}?Acknowl.?(p1 : process)
& @{s:state}(do)
& @{s:state}?Scope?(p1 : process))
& @{p1 :process}(see)
& @{p1 :process}?Actor?(sp : speaker)
& @{p1 :process}?Patient?(o1 : phys? obj)
We next move the locus to Patient, i.e. the entity to be
acknowledged. The type of the entity satisfies the entry con-
dition for system (4). There we trigger the generation of a
contextually appropriate referring expression, calling a GRE
algorithm with the identifier o1.
(5) @{d:disc?vantagepoint}(yes)
& @{d:disc?vantagepoint}?Acknowl.?(p1 : process)
& @{s:state}(do)
& @{s:state}?Scope?(p1 : process))
& @{p1 :process}(see)
& @{p1 :process}?Actor?(sp : speaker)
& @{p1 :process}?Patient?(o1 : phys ? obj)
& @{o1 :phys?obj}(ball)
& @{o1 :phys?obj}?Delimitation?(unique)
&@{o1 :phys?obj}?Quantification?(specific singular)
Figure 6: Simple network for planning grounding feedback
& @{o1 :phys?obj}?Property?(c1 : color)
& @{c1 :color}(red)
Finally, the agenda manager moves the locus automatically
to the nominal sp. Applying system (3), we introduce the full
specification of the speaker, to yield the logical form that is
outputted by the utterance planner:
(6) @{d:disc?vantagepoint}(yes)
& @{d:disc?vantagepoint}?Acknowl.?(p1 : process)
& @{s:state}(do)
& @{s:state}?Scope?(p1 : process))
& @{p1 :process}(see)
& @{p1 :process}?Actor?(sp : speaker)
& @{sp:speaker}(I)
& @{sp:speaker}?Number?(singular)
& @{p1 :process}?Patient?(o1 : phys ? obj)
& @{o1 :phys?obj}(ball)
& @{o1 :phys?obj}?Delimitation?(unique)
&@{o1 :phys?obj}?Quantification?(specific singular)
& @{o1 :phys?obj}?Property?(c1 : color)
& @{c1 :color}(red)
3.1 Coverage
One of the topics under investigation in the CoSy project
is how a robot can learn through language, acquiring more
knowledge of its environment through interaction with a hu-
man tutor. As such, we are currently developing grammars for
the utterance planner, so as to be able to handle clarification
dialogues, verbalization of what the robot does or does not
know, and synchronization of different modalities (sequenc-
ing, concurrency) through which the robot can communicate.
The example below illustrates such a dialogue.
(7) H: ?In front of you you see a desk.?
R: looks in front of it, acquires a visual recognition model
of the object it has in its field of vision
R: turns to the tutor
R: nods, and says ?Thank you for showing me a desk.?
R: ?Is a desk a kind of table??
H: ?Yes, that is correct.?
4 Practical planning grammars
In this section we describe ongoing research on constructing
utterance planning grammars for practical systems.
4.1 Derivation from a CCG grammar
In a dialogue system, there are usually various grammars:
for speech recognition, parsing recognized strings, generat-
ing strings from logical forms, an utterance planning gram-
mar; and so on. Ideally, one grammar would be enough ?
and should, if we want to maintain a single source of linguis-
tic knowledge across the different levels of interpretation and
production of natural language. The OpenCCG system al-
ready facilitates the use of a single grammar for both parsing
and realization. Here, we discuss how we could derive the
planning grammar from the signature of the CCG grammar,
to ensure that we can realize what we can plan.
We start from two observations. First, we can break up the
meaning of an expression, down to the level of a single word,
in terms of a conjunction of elementary predications; cf. ?2.1.
Second, we can organize the computational CCG lexicon as a
collection of (monotonic) inheritance hierarchies [Baldridge,
2002]. Taken together, this means that we can describe the hi-
erarchies in terms of how elementary predications are added,
when descending down a hierarchy. More precisely, because
we can distinguish between elementary predications that add
propositions, features, or relations, we can describe the hier-
archies in terms of what types of structure are being added.
Hierarchies are over lexical families. Given a lexical fam-
ily f i , we can define the signature of its meaning in terms
of what it (a) inherits from a super-family f j , and (b) con-
tributes itself: ?(f i) = LF f i + ?(f j : f i v f j ). Be-
cause a contribution to logical form is essentially the speci-
fication of the type of the root nominal and a conjunction of
elementary predications epi , LF f i is nom : type plus a set
of conjuncts conj(f i) = {ep1 , ..., epn}. We can separate
the conjuncts further in terms of relations, propositions, and
features: conj(f i) = props(f i){ep1 , .., epk} ? rels(f i) =
{epl , ..., epm} ? feats(f i) = {epn , ..., epo}.
Based on the signatures, we define the construction of sys-
tems and outline their associated choosers. Staying with a
functional perspective, we should not map lexical families di-
rectly onto systems. Systems define paradigmatic choices,
whereas lexical families reflect an aggregation of several such
choices: they reflect transitivity through their valency and the
associated structure of their categories, whereas other mean-
ingful dimensions are associated with their features.
We focus first on transitivity. This is expressed by the type
of the nominal, and the elementary predications in props and
rels. Given a subtree in the inheritance hierarchy, being a
family f i and its n immediate children f j , we can define
a system ? for the transitivity region in the planning net-
work as follows. The entry condition to the system is defined
by the logical form that f i yields, modulo its features i.e.
entry(?) = ?(f i)? feats(f i). The chooser needs to make
(n ? 1) decisions, to cover the different possibilities; we de-
rive the associated actions (add-proposition, add-relation,
assign-type) directly from the type for f j , props(f j ) and
rels(f j ). This procedure yields a shallow network, in which
systems do not take into account any similarities between the
children f j of f i . If we want this, we need to find a com-
mon structure between the children. This is again an inheri-
tance hierarchy, over contributions to logical form in terms of
props(f j ) ? rels(f j ). We then define system for each node
in this hierarchy, using the above procedure.
We can thus obtain the systems for the transitivity region
of the planning grammar, based on how logical forms in the
lexical inheritance hierarchy are expanded by assigning more
specific types, and adding propositions and relations. To or-
ganize regions around features (and their values), we suggest
to let this organization follow from the organization of fea-
tures and values in the type-hierarchy, which we can specify
for a CCG grammar [Erkan, 2003]. Given a class of features,
and a lexical inheritance hierarchy, we can define systems on
the basis of how these features are set when we descend down
the hierarchy. Given an inheritance hierarchy, we annotate
each node for a family f j with those features in feats(f j )
that are in the class we consider. The resulting structure may
be sparse, as not every family needs to add features; hence,
we can flatten this structure by removing nodes that do not
add any features. From this structure, we can create systems
in essentially the same way as we did above. For each subtree
in the structure, we create a system that has as entry condi-
tion the type for the root f i plus its features in the current
class. The chooser needs to make decisions about the speci-
fication of the features introduced or further specified by the
children f j of f i . For both introduction and specification
the associated action is add-feature; if a feature is specified,
the chooser can already specify a choice point based on the
inquiry after the presence and (underspecified) value of the
feature.
The resulting utterance planning grammar is based on a
network in which the entry conditions ensure that the way
logical forms are expanded conforms to the way lexical fam-
ilies are formulated in the grammar. The systems are dis-
tributed across different regions, modelling different dimen-
sions of paradigmatic choices that the systems in these re-
gions make, whereby the organization into regions is driven
by the relational structures defined through the lexical fami-
lies (i.e. transitivity) and the type hierarchies over features.
Current research focuses on how we can use XSLT to
transform the XSL-based specification of a CCG grammar
into the different structures from which we derive the sys-
temic network. We then use the resulting XML-based struc-
tures together with the type hierarchies for the grammar as in-
put to the above construction procedures. The systems we can
thus obtain still lack the decisions to be made in the choosers;
we are developing a debugger/editor for the sentence planner
to help specifying these.
4.2 Trainability
Relatively recently, the issue of trainability of utterance plan-
ners has arisen in the context of practical dialogue systems.
Using training, we can automatically adapt and optimize the
choices of a planner to the domain in which the planner needs
to be applied. This has the potential of yielding a significantly
faster planner; cf. e.g. [Stent et al, 2004].
For training the planner we discuss in this paper, we are not
only interested in ensuring that we obtain a logical form that
is appropriate given the communicative goal to be expressed
(or, in a more structured way, comparable to the rhetorical
structures considered in [Stent et al, 2004]); the logical form
also needs to be appropriate given ?the? context.
This poses an interesting challenge, because it means that
we need to train the planner on data that is rated not only
for its structural appropriateness, but also for contextual ap-
propriateness. By data we understand a domain-specific cor-
pus of parseable expressions, annotated with context features
such as the salience of an entity or event across different
modalities. We are exploring how we can train the planner
over the logical forms underlying the syntactic analyses of
the expressions, i.e. training is not on the surface forms.
The basic idea we consider is the following. Given the
logical form for an expression, written as elementary predi-
cations, we can reconstruct the path through the systemic net-
work that gives rise to this logical form. A path consists of
the systems that need to be entered, and the decisions that
the associated choosers need to make. Training then comes
down to learning, for each system, an n-ary classifier that
takes context features and the logical form for the current lo-
cus, to output the choice that the chooser should make.
5 Conclusions
In this paper we discussed the implementation of an utterance
planner which is part of a larger communication subsystem
for a conversational robot. One of the challenges for such a
robot is to produce contextually appropriate utterances in a
dynamic context. We presented an approach that can dynam-
ically include information about the situated context while
planning an utterance. We use systemic networks to guide
the paradigmatic choices the planner needs to make, and we
discussed (briefly) how these networks could be trained, and
derived from the CCG grammar that specifies the linguistic
knowledge of the robot. The planner is implemented in Java,
and has a (tccg-style) debugger enabling one to trace, and
interact with, the decisions the planner makes, and to realize
the resulting logical forms using OpenCCG.
References
[Areces, 2000] Carlos Areces. Logic Engineering. The Case of De-
scription and Hybrid Logics. Phd thesis, University of Amster-
dam, Amsterdam, the Netherlands, 2000.
[Baldridge and Kruijff, 2002] Jason Baldridge and Geert-Jan M.
Kruijff. Coupling CCG and hybrid logic dependency semantics.
In Proceedings of ACL 2002, Philadelphia, Pennsylvania, 2002.
[Baldridge, 2002] Jason Baldridge. Lexically Specified Deriva-
tional Control in Combinatory Categorial Grammar. PhD thesis,
University of Edinburgh, 2002.
[Bateman, 1997] John A. Bateman. Enabling technology for multi-
lingual natural language generation: the KPML development en-
vironment. Journal of Natural Language Engineering, 3(1):15?
55, 1997.
[Blackburn, 2000] Patrick Blackburn. Representation, reasoning,
and relational structures: a hybrid logic manifesto. Journal of
the Interest Group in Pure Logic, 8(3):339?365, 2000.
[Cassell et al, 2000] Justine Cassell, Matthew Stone, and Hao Yan.
Coordination and context-dependence in the generation of em-
bodied conversation. In Proceedings of INLG-2000, pages pages
171?178, 2000.
[Cheyer and Martin, 2001] Adam Cheyer and David Martin. The
open agent architecture. Journal of Autonomous Agents and
Multi-Agent Systems, 4(1):143?148, March 2001.
[Chum and Wolfe, 2001] M. Chum and J. Wolfe. Visual attention.
In E. Bruce Goldstein, editor, Blackwell Handbook of Perception,
Handbooks of Experimental Psychology, chapter 9, pages 272?
310. Blackwell, 2001.
[Copestake et al, 1997] Ann Copestake, Dan Flickinger, and
Ivan A. Sag. Minimal recursion semantics. an introduction. Un-
published Manuscript. CSLI/Stanford University, 1997.
[Erkan, 2003] Gu?nes? Erkan. A type system for CCG. Master?s
thesis, Middle East Technical University, Ankara, Turkey, 2003.
[Foster and White, 2004] Mary Ellen Foster and Michael White.
Techniques for text planning with XSLT. In Proceeedings of
NLPXML-2004, Barcelona, Spain, 2004.
[Gurevych et al, 2003] Iryna Gurevych, Robert Porzel, Elena
Slinko, Norbert Pfleger, Jan Alexandersson, and Stefan Merten.
Less is more: Using a single knowledge representation in dia-
logue systems. In Proceedings of the HLT-NAACL WS on Text
Meaning, Edmonton, Canada, 2003.
[Hobbs, 1985] Jerry R. Hobbs. Ontological promiscuity. In Pro-
ceedings of ACL 1985, 1985.
[Kelleher and Kruijff, 2005a] John D. Kelleher and Geert-Jan M.
Kruijff. A context-dependent algorithm for generating locative
expressions in physically situated environments. In Proceedings
of ENLG-05, Aberdeen, Scotland, 2005.
[Kelleher and Kruijff, 2005b] John D. Kelleher and Geert-Jan M.
Kruijff. A context-dependent model of proximity in physically
situated environments. In Proceedings of the ACL-SIGSEM
workshop The Linguistic Dimension of Prepositions, Colchester,
England, 2005.
[Kruijff, 2001] Geert-Jan M. Kruijff. A Categorial-Modal Logical
Architecture of Informativity: Dependency Grammar Logic &
Information Structure. PhD thesis, Charles University, Prague,
Czech Republic, 2001.
[Langley and Laird, 2002] Pat Langley and John E. Laird. Cogni-
tive architectures: Research issues and challenges. Technical re-
port, Institute for the Study of Learning and Expertise, Palo Alto,
CA, 2002.
[Mathiessen, 1983] Christian M.I.M. Mathiessen. Systemic gram-
mar in computation: the Nigel case. In Proceedings of EACL
1983, 1983.
[Minsky, 1986] Marvin L. Minsky. The Society of Mind. Simon and
Schuster, New York, NY, 1986.
[Reiter, 2001] Raymond Reiter. Knowledge in Action: Logical
Foundations for Specifying and Implementing Dynamical Sys-
tems. The MIT Press, Cambridge MA, 2001.
[Shanahan and Witkowski, 2001] Murray P. Shanahan and Michael
Witkowski. High-level robot control through logic. In Intelligent
Agents VII, pages 104?121. Springer-Verlag, Berlin, Germany,
2001.
[Shanahan, 2000] Murray P. Shanahan. Reinventing Shakey. In
Jack Minker, editor, Logic-Based Artificial Intelligence, pages
233?253. Kluwer Academic Publishers, Dordrecht, the Nether-
lands, 2000.
[Sloman, 2001] Aaron Sloman. Beyond shallow models of emo-
tion. Cognitive Processing, 2(1):177?198, 2001.
[Stent et al, 2004] Amanda Stent, Rashmi Prasad, and Marilyn
Walker. Trainable sentence planning for complex information
presentation in spoken dialog systems. In Proceedings of ACL
2004, Barcelona, Spain, 2004.
[Stone and Doran, 1997] Matthew Stone and Christine Doran. Sen-
tence planning as description using tree-adjoining grammar. In
Proceedings of ACL 1997, pages 198?205, 1997.
[Wache et al, 2001] H. Wache, T. Vo?gele, U. Visser, H. Stucken-
schmidt, G. Schuster, H. Neumann, and S. Hu?bner. Ontology-
based integration of information - a survey of existing ap-
proaches. In Proceedings of IJCAI 2001 Workshop ?Ontologies
and Information Sharing?, Seattle WA, 2001.
[Walker et al, 2004] Willie Walker, Paul Lamere, Philip Kwok,
Bhiksha Raj, Rita Singh, Evandro Gouvea, Peter Wolf, and Joe
Woelfel. Sphinx-4: A flexible open source framework for speech
recognition. Technical report, SUN Microsystems Inc., 2004.
Technical Report TR2004-0811.
[White and Baldridge, 2003] Michael White and Jason Baldridge.
Adapting chart realization to CCG. In Proceedings of ENLG-03,
Budapest, Hungary, 2003.
[White, 2004] Michael White. Efficient realizations of coordinate
structures in combinatory categorial grammar. Research on Lan-
guage and Computation, 2004.
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 58?65,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
An Integrated Approach to Robust Processing
of Situated Spoken Dialogue
Pierre Lison
Language Technology Lab,
DFKI GmbH,
Saarbru?cken, Germany
pierre.lison@dfki.de
Geert-Jan M. Kruijff
Language Technology Lab,
DFKI GmbH,
Saarbru?cken, Germany
gj@dfki.de
Abstract
Spoken dialogue is notoriously hard to
process with standard NLP technologies.
Natural spoken dialogue is replete with
disfluent, partial, elided or ungrammatical
utterances, all of which are very hard to
accommodate in a dialogue system. Fur-
thermore, speech recognition is known to
be a highly error-prone task, especially for
complex, open-ended discourse domains.
The combination of these two problems
? ill-formed and/or misrecognised speech
inputs ? raises a major challenge to the de-
velopment of robust dialogue systems.
We present an integrated approach for ad-
dressing these two issues, based on a in-
cremental parser for Combinatory Cate-
gorial Grammar. The parser takes word
lattices as input and is able to handle ill-
formed and misrecognised utterances by
selectively relaxing its set of grammati-
cal rules. The choice of the most rele-
vant interpretation is then realised via a
discriminative model augmented with con-
textual information. The approach is fully
implemented in a dialogue system for au-
tonomous robots. Evaluation results on a
Wizard of Oz test suite demonstrate very
significant improvements in accuracy and
robustness compared to the baseline.
1 Introduction
Spoken dialogue is often considered to be one of
the most natural means of interaction between a
human and a robot. It is, however, notoriously
hard to process with standard language process-
ing technologies. Dialogue utterances are often in-
complete or ungrammatical, and may contain nu-
merous disfluencies like fillers (err, uh, mm), rep-
etitions, self-corrections, etc. Rather than getting
crisp-and-clear commands such as ?Put the red
ball inside the box!?, it is more likely the robot
will hear such kind of utterance: ?right, now, could
you, uh, put the red ball, yeah, inside the ba/ box!?.
This is natural behaviour in human-human interac-
tion (Ferna?ndez and Ginzburg, 2002) and can also
be observed in several domain-specific corpora for
human-robot interaction (Topp et al, 2006).
Moreover, even in the (rare) case where the ut-
terance is perfectly well-formed and does not con-
tain any kind of disfluencies, the dialogue sys-
tem still needs to accomodate the various speech
recognition errors thay may arise. This problem
is particularly acute for robots operating in real-
world noisy environments and deal with utterances
pertaining to complex, open-ended domains.
The paper presents a new approach to address
these two difficult issues. Our starting point is the
work done by Zettlemoyer and Collins on parsing
using relaxed CCG grammars (Zettlemoyer and
Collins, 2007) (ZC07). In order to account for
natural spoken language phenomena (more flex-
ible word order, missing words, etc.), they aug-
ment their grammar framework with a small set
of non-standard combinatory rules, leading to a
relaxation of the grammatical constraints. A dis-
criminative model over the parses is coupled with
the parser, and is responsible for selecting the most
likely interpretation(s) among the possible ones.
In this paper, we extend their approach in two
important ways. First, ZC07 focused on the treat-
ment of ill-formed input, and ignored the speech
recognition issues. Our system, to the contrary,
is able to deal with both ill-formed and misrec-
ognized input, in an integrated fashion. This is
done by augmenting the set of non-standard com-
binators with new rules specifically tailored to deal
with speech recognition errors.
Second, the only features used by ZC07 are syn-
tactic features (see 3.4 for details). We signifi-
cantly extend the range of features included in the
58
discriminative model, by incorporating not only
syntactic, but also acoustic, semantic and contex-
tual information into the model.
An overview of the paper is as follows. We first
describe in Section 2 the cognitive architecture in
which our system has been integrated. We then
discuss the approach in detail in Section 3. Fi-
nally, we present in Section 4 the quantitative eval-
uations on a WOZ test suite, and conclude.
2 Architecture
The approach we present in this paper is fully im-
plemented and integrated into a cognitive architec-
ture for autonomous robots. A recent version of
this system is described in (Hawes et al, 2007). It
is capable of building up visuo-spatial models of
a dynamic local scene, continuously plan and exe-
cute manipulation actions on objects within that
scene. The robot can discuss objects and their
material- and spatial properties for the purpose of
visual learning and manipulation tasks.
Figure 1: Architecture schema of the communica-
tion subsystem (only for comprehension).
Figure 2 illustrates the architecture schema for
the communication subsystem incorporated in the
cognitive architecture (only the comprehension
part is shown).
Starting with ASR, we process the audio signal
to establish a word lattice containing statistically
ranked hypotheses about word sequences. Subse-
quently, parsing constructs grammatical analyses
for the given word lattice. A grammatical analy-
sis constructs both a syntactic analysis of the ut-
terance, and a representation of its meaning. The
analysis is based on an incremental chart parser1
for Combinatory Categorial Grammar (Steedman
and Baldridge, 2009). These meaning represen-
tations are ontologically richly sorted, relational
1Built on top of the OpenCCG NLP library:
http://openccg.sf.net
structures, formulated in a (propositional) descrip-
tion logic, more precisely in the HLDS formal-
ism (Baldridge and Kruijff, 2002). The parser
compacts all meaning representations into a sin-
gle packed logical form (Carroll and Oepen, 2005;
Kruijff et al, 2007). A packed LF represents con-
tent similar across the different analyses as a single
graph, using over- and underspecification of how
different nodes can be connected to capture lexical
and syntactic forms of ambiguity.
At the level of dialogue interpretation, a packed
logical form is resolved against a SDRS-like di-
alogue model (Asher and Lascarides, 2003) to
establish contextual co-reference and dialogue
moves.
Linguistic interpretations must finally be associ-
ated with extra-linguistic knowledge about the en-
vironment ? dialogue comprehension hence needs
to connect with other subarchitectures like vision,
spatial reasoning or planning. We realise this
information binding between different modalities
via a specific module, called the ?binder?, which is
responsible for the ontology-based mediation ac-
cross modalities (Jacobsson et al, 2008).
2.1 Context-sensitivity
The combinatorial nature of language provides
virtually unlimited ways in which we can commu-
nicate meaning. This, of course, raises the ques-
tion of how precisely an utterance should then be
understood as it is being heard. Empirical stud-
ies have investigated what information humans use
when comprehending spoken utterances. An im-
portant observation is that interpretation in con-
text plays a crucial role in the comprehension of
utterance as it unfolds (Knoeferle and Crocker,
2006). During utterance comprehension, humans
combine linguistic information with scene under-
standing and ?world knowledge?.
Figure 2: Context-sensitivity in processing situ-
ated dialogue understanding
Several approaches in situated dialogue for
human-robot interaction have made similar obser-
59
vations (Roy, 2005; Roy and Mukherjee, 2005;
Brick and Scheutz, 2007; Kruijff et al, 2007): A
robot?s understanding can be improved by relating
utterances to the situated context. As we will see
in the next section, by incorporating contextual in-
formation into our model, our approach to robust
processing of spoken dialogue seeks to exploit this
important insight.
3 Approach
3.1 Grammar relaxation
Our approach to robust processing of spoken di-
alogue rests on the idea of grammar relaxation:
the grammatical constraints specified in the gram-
mar are ?relaxed? to handle slightly ill-formed or
misrecognised utterances.
Practically, the grammar relaxation is done
via the introduction of non-standard CCG rules
(Zettlemoyer and Collins, 2007). In Combinatory
Categorial Grammar, the rules are used to assem-
ble categories to form larger pieces of syntactic
and semantic structure. The standard rules are ap-
plication (<,>), composition (B), and type rais-
ing (T) (Steedman and Baldridge, 2009).
Several types of non-standard rules have been
introduced. We describe here the two most impor-
tant ones: the discourse-level composition rules,
and the ASR correction rules. We invite the reader
to consult (Lison, 2008) for more details on the
complete set of grammar relaxation rules.
3.1.1 Discourse-level composition rules
In natural spoken dialogue, we may encounter ut-
terances containing several independent ?chunks?
without any explicit separation (or only a short
pause or a slight change in intonation), such as
(1) ?yes take the ball no the other one on your
left right and now put it in the box.?
Even if retrieving a fully structured parse for
this utterance is difficult to achieve, it would be
useful to have access to a list of smaller ?discourse
units?. Syntactically speaking, a discourse unit
can be any type of saturated atomic categories -
from a simple discourse marker to a full sentence.
The type raising rule Tdu allows the conversion
of atomic categories into discourse units:
A : @if ? du : @if (Tdu)
where A represents an arbitrary saturated
atomic category (s, np, pp, etc.).
The rule>C is responsible for the integration of
two discourse units into a single structure:
du : @if, du : @jg ?
du : @{d:d-units}(list?
(?FIRST? i ? f)?
(?NEXT? j ? g)) (>C)
3.1.2 ASR error correction rules
Speech recognition is a highly error-prone task. It
is however possible to partially alleviate this prob-
lem by inserting new error-correction rules (more
precisely, new lexical entries) for the most fre-
quently misrecognised words.
If we notice e.g. that the ASR system frequently
substitutes the word ?wrong? for the word ?round?
during the recognition (because of their phonolog-
ical proximity), we can introduce a new lexical en-
try in the lexicon in order to correct this error:
round ` adj : @attitude(wrong) (2)
A set of thirteen new lexical entries of this type
have been added to our lexicon to account for the
most frequent recognition errors.
3.2 Parse selection
Using more powerful grammar rules to relax the
grammatical analysis tends to increase the number
of parses. We hence need a a mechanism to dis-
criminate among the possible parses. The task of
selecting the most likely interpretation among a set
of possible ones is called parse selection. Once all
the possible parses for a given utterance are com-
puted, they are subsequently filtered or selected
in order to retain only the most likely interpreta-
tion(s). This is done via a (discriminative) statisti-
cal model covering a large number of features.
Formally, the task is defined as a function F :
X ? Y where the domain X is the set of possible
inputs (in our case, X is the set of possible word
lattices), and Y the set of parses. We assume:
1. A function GEN(x) which enumerates all
possible parses for an input x. In our case,
this function simply represents the set of
parses of x which are admissible according
to the CCG grammar.
2. A d-dimensional feature vector f(x, y) ?
<d, representing specific features of the pair
(x, y). It can include various acoustic, syn-
tactic, semantic or contextual features which
can be relevant in discriminating the parses.
60
3. A parameter vector w ? <d.
The function F , mapping a word lattice to its
most likely parse, is then defined as:
F (x) = argmax
y?GEN(x)
wT ? f(x, y) (3)
where wT ? f(x, y) is the inner product
?d
s=1ws fs(x, y), and can be seen as a measure
of the ?quality? of the parse. Given the parameters
w, the optimal parse of a given utterance x can be
therefore easily determined by enumerating all the
parses generated by the grammar, extracting their
features, computing the inner product wT ?f(x, y),
and selecting the parse with the highest score.
The task of parse selection is an example of
structured classification problem, which is the
problem of predicting an output y from an input
x, where the output y has a rich internal structure.
In the specific case of parse selection, x is a word
lattice, and y a logical form.
3.3 Learning
3.3.1 Training data
In order to estimate the parameters w, we need a
set of training examples. Unfortunately, no corpus
of situated dialogue adapted to our task domain is
available to this day, let alne semantically anno-
tated. The collection of in-domain data via Wizard
of Oz experiments being a very costly and time-
consuming process, we followed the approach ad-
vocated in (Weilhammer et al, 2006) and gener-
ated a corpus from a hand-written task grammar.
To this end, we first collected a small set of
WoZ data, totalling about a thousand utterances.
This set is too small to be directly used as a cor-
pus for statistical training, but sufficient to cap-
ture the most frequent linguistic constructions in
this particular context. Based on it, we designed
a domain-specific CFG grammar covering most of
the utterances. Each rule is associated to a seman-
tic HLDS representation. Weights are automati-
cally assigned to each grammar rule by parsing our
corpus, hence leading to a small stochastic CFG
grammar augmented with semantic information.
Once the grammar is specified, it is randomly
traversed a large number of times, resulting in a
larger set (about 25.000) of utterances along with
their semantic representations. Since we are inter-
ested in handling errors arising from speech recog-
nition, we also need to ?simulate? the most fre-
quent recognition errors. To this end, we synthe-
sise each string generated by the domain-specific
CFG grammar, using a text-to-speech engine2,
feed the audio stream to the speech recogniser,
and retrieve the recognition result. Via this tech-
nique, we are able to easily collect a large amount
of training data3.
3.3.2 Perceptron learning
The algorithm we use to estimate the parameters
w using the training data is a perceptron. The al-
gorithm is fully online - it visits each example in
turn and updates w if necessary. Albeit simple,
the algorithm has proven to be very efficient and
accurate for the task of parse selection (Collins
and Roark, 2004; Collins, 2004; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007).
The pseudo-code for the online learning algo-
rithm is detailed in [Algorithm 1].
It works as follows: the parameters w are first
initialised to some arbitrary values. Then, for
each pair (xi, zi) in the training set, the algorithm
searchs for the parse y? with the highest score ac-
cording to the current model. If this parse happens
to match the best parse which generates zi (which
we shall denote y?), we move to the next example.
Else, we perform a simple perceptron update on
the parameters:
w = w + f(xi, y?)? f(xi, y?) (4)
The iteration on the training set is repeated T
times, or until convergence.
The most expensive step in this algorithm is
the calculation of y? = argmaxy?GEN(xi) w
T ?
f(xi, y) - this is the decoding problem.
It is possible to prove that, provided the train-
ing set (xi, zi) is separable with margin ? > 0, the
algorithm is assured to converge after a finite num-
ber of iterations to a model with zero training er-
rors (Collins and Roark, 2004). See also (Collins,
2004) for convergence theorems and proofs.
3.4 Features
As we have seen, the parse selection operates by
enumerating the possible parses and selecting the
2We used MARY (http://mary.dfki.de) for the
text-to-speech engine.
3Because of its relatively artificial character, the quality
of such training data is naturally lower than what could be
obtained with a genuine corpus. But, as the experimental re-
sults will show, it remains sufficient to train the perceptron
for the parse selection task, and achieve significant improve-
ments in accuracy and robustness. In a near future, we plan
to progressively replace this generated training data by a real
spoken dialogue corpus adapted to our task domain.
61
Algorithm 1 Online perceptron learning
Require: - set of n training examples {(xi, zi) : i = 1...n}
- T : number of iterations over the training set
- GEN(x): function enumerating possible parses
for an input x, according to the CCG grammar.
- GEN(x, z): function enumerating possible parses
for an input x and which have semantics z,
according to the CCG grammar.
- L(y) maps a parse tree y to its logical form.
- Initial parameter vector w0
% Initialise
w? w0
% Loop T times on the training examples
for t = 1...T do
for i = 1...n do
% Compute best parse according to current model
Let y? = argmaxy?GEN(xi) w
T ? f(xi, y)
% If the decoded parse 6= expected parse, update the
parameters
if L(y?) 6= zi then
% Search the best parse for utterance xi with se-
mantics zi
Let y? = argmaxy?GEN(xi,zi) w
T ? f(xi, y)
% Update parameter vector w
Set w = w + f(xi, y?)? f(xi, y?)
end if
end for
end for
return parameter vector w
one with the highest score according to the linear
model parametrised by w.
The accuracy of our method crucially relies on
the selection of ?good? features f(x, y) for our
model - that is, features which help discriminat-
ing the parses. They must also be relatively cheap
to compute. In our model, the features are of four
types: semantic features, syntactic features, con-
textual features, and speech recognition features.
3.4.1 Semantic features
What are the substructures of a logical form which
may be relevant to discriminate the parses? We de-
fine features on the following information sources:
1. Nominals: for each possible pair
?prop, sort?, we include a feature fi in
f(x, y) counting the number of nominals
with ontological sort sort and proposition
prop in the logical form.
2. Ontological sorts: occurrences of specific
ontological sorts in the logical form.
Figure 3: graphical representation of the HLDS
logical form for ?I want you to take the mug?.
3. Dependency relations: following (Clark and
Curran, 2003), we also model the depen-
dency structure of the logical form. Each
dependency relation is defined as a triple
?sorta, sortb, label?, where sorta denotes
the sort of the incoming nominal, sortb the
sort of the outgoing nominal, and label is the
relation label.
4. Sequences of dependency relations: number
of occurrences of particular sequences (ie. bi-
gram counts) of dependency relations.
The features on nominals and ontological sorts
aim at modeling (aspects of) lexical semantics -
e.g. which meanings are the most frequent for a
given word -, whereas the features on relations and
sequence of relations focus on sentential seman-
tics - which dependencies are the most frequent.
These features therefore help us handle lexical and
syntactic ambiguities.
3.4.2 Syntactic features
By ?syntactic features?, we mean features associ-
ated to the derivational history of a specific parse.
The main use of these features is to penalise to a
correct extent the application of the non-standard
rules introduced into the grammar.
To this end, we include in the feature vector
f(x, y) a new feature for each non-standard rule,
which counts the number of times the rule was ap-
plied in the parse.
62
pick
s/particle/np
cup
up corr
particle
s/np
>
the
np/n
ball
n
np >
s >
Figure 4: CCG derivation of ?pick cup the ball?.
In the derivation shown in the figure 4, the rule
corr (correction of a speech recognition error) is
applied once, so the corresponding feature value is
set to 1. The feature values for the remaining rules
are set to 0, since they are absent from the parse.
These syntactic features can be seen as a penalty
given to the parses using these non-standard rules,
thereby giving a preference to the ?normal? parses
over them. This mechanism ensures that the gram-
mar relaxation is only applied ?as a last resort?
when the usual grammatical analysis fails to pro-
vide a full parse. Of course, depending on the
relative frequency of occurrence of these rules in
the training corpus, some of them will be more
strongly penalised than others.
3.4.3 Contextual features
As we have already outlined in the background
section, one striking characteristic of spoken dia-
logue is the importance of context. Understanding
the visual and discourse contexts is crucial to re-
solve potential ambiguities and compute the most
likely interpretation(s) of a given utterance.
The feature vector f(x, y) therefore includes
various features related to the context:
1. Activated words: our dialogue system main-
tains in its working memory a list of contex-
tually activated words (cfr. (Lison and Krui-
jff, 2008)). This list is continuously updated
as the dialogue and the environment evolves.
For each context-dependent word, we include
one feature counting the number of times it
appears in the utterance string.
2. Expected dialogue moves: for each possible
dialogue move, we include one feature indi-
cating if the dialogue move is consistent with
the current discourse model. These features
ensure for instance that the dialogue move
following a QuestionYN is a Accept, Re-
ject or another question (e.g. for clarification
requests), but almost never an Opening.
3. Expected syntactic categories: for each
atomic syntactic category in the CCG gram-
mar, we include one feature indicating if the
category is consistent with the current dis-
course model. These features can be used to
handle sentence fragments.
3.4.4 Speech recognition features
Finally, the feature vector f(x, y) also includes
features related to the speech recognition. The
ASR module outputs a set of (partial) recognition
hypotheses, packed in a word lattice. One exam-
ple of such a structure is given in Figure 5. Each
recognition hypothesis is provided with an asso-
ciated confidence score, and we want to favour
the hypotheses with high confidence scores, which
are, according to the statistical models incorpo-
rated in the ASR, more likely to reflect what was
uttered.
To this end, we introduce three features: the
acoustic confidence score (confidence score pro-
vided by the statistical models included in the
ASR), the semantic confidence score (based on a
?concept model? also provided by the ASR), and
the ASR ranking (hypothesis rank in the word lat-
tice, from best to worst).
Figure 5: Example of word lattice
4 Experimental evaluation
We performed a quantitative evaluation of our ap-
proach, using its implementation in a fully inte-
grated system (cf. Section 2). To set up the ex-
periments for the evaluation, we have gathered a
corpus of human-robot spoken dialogue for our
task-domain, which we segmented and annotated
manually with their expected semantic interpreta-
tion. The data set contains 195 individual utter-
ances along with their complete logical forms.
4.1 Results
Three types of quantitative results are extracted
from the evaluation results: exact-match, partial-
match, and word error rate. Tables 1, 2 and 3 illus-
trate the results, broken down by use of grammar
relaxation, use of parse selection, and number of
recognition hypotheses considered.
63
Size of word lattice
(number of NBests)
Grammar
relaxation
Parse
selection Precision Recall F1-value
(Baseline) 1 No No 40.9 45.2 43.0
. 1 No Yes 59.0 54.3 56.6
. 1 Yes Yes 52.7 70.8 60.4
. 3 Yes Yes 55.3 82.9 66.3
. 5 Yes Yes 55.6 84.0 66.9
(Full approach) 10 Yes Yes 55.6 84.9 67.2
Table 1: Exact-match accuracy results (in percents).
Size of word lattice
(number of NBests)
Grammar
relaxation
Parse
selection Precision Recall F1-value
(Baseline) 1 No No 86.2 56.2 68.0
. 1 No Yes 87.4 56.6 68.7
. 1 Yes Yes 88.1 76.2 81.7
. 3 Yes Yes 87.6 85.2 86.4
. 5 Yes Yes 87.6 86.0 86.8
(Full approach) 10 Yes Yes 87.7 87.0 87.3
Table 2: Partial-match accuracy results (in percents).
Each line in the tables corresponds to a possible
configuration. Tables 1 and 2 give the precision,
recall and F1 value for each configuration (respec-
tively for the exact- and partial-match), and Table
3 gives the Word Error Rate [WER].
The first line corresponds to the baseline: no
grammar relaxation, no parse selection, and use of
the first NBest recognition hypothesis. The last
line corresponds to the results with the full ap-
proach: grammar relaxation, parse selection, and
use of 10 recognition hypotheses.
Size of word
lattice (NBests)
Grammar
relaxation
Parse
selection WER
1 No No 20.5
1 Yes Yes 19.4
3 Yes Yes 16.5
5 Yes Yes 15.7
10 Yes Yes 15.7
Table 3: Word error rate (in percents).
4.2 Comparison with baseline
Here are the comparative results we obtained:
? Regarding the exact-match results between
the baseline and our approach (grammar re-
laxation and parse selection with all fea-
tures activated for NBest 10), the F1-measure
climbs from 43.0 % to 67.2 %, which means
a relative difference of 56.3 %.
? For the partial-match, the F1-measure goes
from 68.0 % for the baseline to 87.3 % for
our approach ? a relative increase of 28.4 %.
? We obverse a significant decrease in WER:
we go from 20.5 % for the baseline to 15.7 %
with our approach. The difference is statisti-
cally significant (p-value for t-tests is 0.036),
and the relative decrease of 23.4 %.
5 Conclusions
We presented an integrated approach to the pro-
cessing of (situated) spoken dialogue, suited to
the specific needs and challenges encountered in
human-robot interaction.
In order to handle disfluent, partial, ill-formed
or misrecognized utterances, the grammar used by
the parser is ?relaxed? via the introduction of a
set of non-standard combinators which allow for
the insertion/deletion of specific words, the com-
bination of discourse fragments or the correction
of speech recognition errors.
The relaxed parser yields a (potentially large)
set of parses, which are then packed and retrieved
by the parse selection module. The parse selec-
tion is based on a discriminative model exploring a
set of relevant semantic, syntactic, contextual and
acoustic features extracted for each parse. The pa-
rameters of this model are estimated against an au-
tomatically generated corpus of ?utterance, logical
form? pairs. The learning algorithm is an percep-
tron, a simple albeit efficient technique for param-
eter estimation.
As forthcoming work, we shall examine the po-
tential extension of our approach in new direc-
tions, such as the exploitation of parse selection
for incremental scoring/pruning of the parse chart,
64
the introduction of more refined contextual fea-
tures, or the use of more sophisticated learning al-
gorithms, such as Support Vector Machines.
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press.
J. Baldridge and G.-J. M. Kruijff. 2002. Coupling
CCG and hybrid logic dependency semantics. In
ACL?02: Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics,
pages 319?326, Philadelphia, PA. Association for
Computational Linguistics.
T. Brick and M. Scheutz. 2007. Incremental natu-
ral language processing for HRI. In Proceeding of
the ACM/IEEE international conference on Human-
Robot Interaction (HRI?07), pages 263 ? 270.
J. Carroll and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
In Proceedings of the International Joint Confer-
ence on Natural Language Processing (IJCNLP?05),
pages 165?176.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage ccg parsing. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 97?104, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In ACL
?04: Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics, page
111, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Michael Collins. 2004. Parameter estimation for
statistical parsing models: theory and practice of
distribution-free methods. In New developments in
parsing technology, pages 19?55. Kluwer Academic
Publishers.
R. Ferna?ndez and J. Ginzburg. 2002. A corpus study
of non-sentential utterances in dialogue. Traitement
Automatique des Langues, 43(2):12?43.
N. Hawes, A. Sloman, J. Wyatt, M. Zillich, H. Jacob-
sson, G.J. M. Kruijff, M. Brenner, G. Berginc, and
D. Skocaj. 2007. Towards an integrated robot with
multiple cognitive functions. In AAAI, pages 1548?
1553. AAAI Press.
Henrik Jacobsson, Nick Hawes, Geert-Jan Kruijff, and
Jeremy Wyatt. 2008. Crossmodal content bind-
ing in information-processing architectures. In Pro-
ceedings of the 3rd ACM/IEEE International Con-
ference on Human-Robot Interaction (HRI), Amster-
dam, The Netherlands, March 12?15.
P. Knoeferle and M.C. Crocker. 2006. The coordinated
interplay of scene, utterance, and world knowledge:
evidence from eye tracking. Cognitive Science.
G.J.M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,
and N.A. Hawes. 2007. Incremental, multi-level
processing for comprehending situated dialogue in
human-robot interaction. In Language and Robots:
Proceedings from the Symposium (LangRo?2007),
pages 55?64, Aveiro, Portugal, December.
Pierre Lison and Geert-Jan M. Kruijff. 2008. Salience-
driven contextual priming of speech recognition for
human-robot interaction. In Proceedings of the 18th
European Conference on Artificial Intelligence, Pa-
tras (Greece).
Pierre Lison. 2008. Robust processing of situated spo-
ken dialogue. Master?s thesis, Universita?t des Saar-
landes, Saarbru?cken.
D. Roy and N. Mukherjee. 2005. Towards situated
speech understanding: visual context priming of
language models. Computer Speech & Language,
19(2):227?248, April.
D.K. Roy. 2005. Semiotic schemas: A framework for
grounding language in action and perception. Artifi-
cial Intelligence, 167(1-2):170?205.
Mark Steedman and Jason Baldridge. 2009. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Bo?rjars, editors, Nontransformational Syntax: A
Guide to Current Models. Blackwell, Oxford.
E. A. Topp, H. Hu?ttenrauch, H.I. Christensen, and
K. Severinson Eklundh. 2006. Bringing together
human and robotic environment representations ?
a pilot study. In Proc. of the IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems
(IROS), Beijing, China, October.
Karl Weilhammer, Matthew N. Stuttle, and Steve
Young. 2006. Bootstrapping language models
for dialogue systems. In Proceedings of INTER-
SPEECH 2006, Pittsburgh, PA.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI ?05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence, July
2005, pages 658?666.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 678?687.
65
Proceedings of the 12th European Workshop on Natural Language Generation, pages 126?129,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Situated Context Model for
Resolution and Generation of Referring Expressions
Hendrik Zender and Geert-Jan M. Kruijff and Ivana Kruijff-Korbayova?
Language Technology Lab, German Research Center for Artificial Intelligence (DFKI)
Saarbru?cken, Germany
{zender, gj, ivana.kruijff}@dfki.de
Abstract
The background for this paper is the aim
to build robotic assistants that can ?natu-
rally? interact with humans. One prereq-
uisite for this is that the robot can cor-
rectly identify objects or places a user
refers to, and produce comprehensible ref-
erences itself. As robots typically act
in environments that are larger than what
is immediately perceivable, the problem
arises how to identify the appropriate con-
text, against which to resolve or produce
a referring expression (RE). Existing al-
gorithms for generating REs generally by-
pass this problem by assuming a given
context. In this paper, we explicitly ad-
dress this problem, proposing a method for
context determination in large-scale space.
We show how it can be applied both for re-
solving and producing REs.
1 Introduction
The past years have seen an extraordinary increase
in research on robotic assistants that help users
perform daily chores. Autonomous vacuum clean-
ers have already found their way into people?s
homes, but it will still take a while before fully
conversational robot ?gophers? will assist people
in more demanding everyday tasks. Imagine a
robot that can deliver objects, and give directions
to visitors on a university campus. This robot must
be able to verbalize its knowledge in a way that is
understandable by humans.
A conversational robot will inevitably face sit-
uations in which it needs to refer to an entity (an
object, a locality, or even an event) that is located
somewhere outside the current scene, as Figure 1
illustrates. There are conceivably many ways in
which a robot might refer to things in the world,
but many such expressions are unsuitable in most
Where is the 
IT Help desk?
It is on the 
1st floor in 
building 3b.
it is at
<45.56, -3.92, 10.45>
Where i  the 
IT hel  desk?
It is on the 1st 
floor in building 
3B.
It is at
Figure 1: Situated dialogue with a service robot
human-robot dialogues. Consider the following
set of examples:
1. ?position P = ?45.56,?3.92, 10.45??
2. ?Peter?s office no. 200 at the end of the cor-
ridor on the third floor of the Acme Corp.
building 3 in the Acme Corp. complex, 47
Evergreen Terrace, Calisota, Earth, (...)?
3. ?the area?
These REs are valid descriptions of their respec-
tive referents. Still they fail to achieve their com-
municative goal, which is to specify the right
amount of information that the hearer needs to
uniquely identify the referent. The next REs might
serve as more appropriate variants of the previous
examples (in certain contexts! ):
1. ?the IT help desk?
2. ?Peter?s office?
3. ?the large hall on the first floor?
The first example highlights a requirement on the
knowledge representation to which an algorithm
for generating referring expressions (GRE) has ac-
cess. Although the robot needs a robot-centric rep-
resentation of its surrounding space that allows it
to safely perform actions and navigate its world,
it should use human-centric qualitative descrip-
tions when talking about things in the world. We
126
do not address this issue here, but refer the inter-
ested reader to our recent work on multi-layered
spatial maps for robots, bridging the gap between
robot-centric and human-centric spatial represen-
tations (Zender et al, 2008).
The other examples point out another impor-
tant consideration: howmuch information does the
human need to single out the intended referent
among the possible entities that the robot could be
referring to? According to the seminal work on
GRE by Dale and Reiter (1995), one needs to dis-
tinguish whether the intended referent is already
in the hearer?s focus of attention or not. This focus
of attention can consist of a local visual scene (vi-
sual context) or a shared workspace (spatial con-
text), but also contains recently mentioned entities
(dialogue context). If the referent is already part
of the current context, the GRE task merely con-
sists of singling it out among the other members
of the context, which act as distractors. In this
case the generated RE contains discriminatory in-
formation, e.g. ?the red ball? if several kinds of ob-
jects with different colors are in the context. If, on
the other hand, the referent is not in the hearer?s fo-
cus of attention, an RE needs to contain what Dale
and Reiter call navigational, or attention-directing
information. The example they give is ?the black
power supply in the equipment rack,? where ?the
equipment rack? is supposed to direct the hearers
attention to the rack and its contents.
In the following we propose an approach for
context determination and extension that allows a
mobile robot to produce and interpret REs to enti-
ties outside the current visual context.
2 Background
Most GRE approaches are applied to very lim-
ited, visual scenes ? so-called small-scale space.
The domain of such systems is usually a small vi-
sual scene, e.g. a number of objects, such as cups
and tables, located in the same room), or other
closed-context scenarios (Dale and Reiter, 1995;
Horacek, 1997; Krahmer and Theune, 2002). Re-
cently, Kelleher and Kruijff (2006) have presented
an incremental GRE algorithm for situated di-
alogue with a robot about a table-top setting,
i.e. also about small-scale space. In all these cases,
the context set is assumed to be identical to the
visual scene that is shared between the interlocu-
tors. The intended referent is thus already in the
hearer?s focus of attention.
In contrast, robots typically act in large-scale
space, i.e. space ?larger than what can be per-
ceived at once? (Kuipers, 1977). They need the
ability to understand and produce references to
things that are beyond the current visual and spa-
tial context. In any situated dialogue that involves
entities beyond the current focus of attention, the
task of extending the context becomes key.
Paraboni et al (2007) present an algorithm for
context determination in hierarchically ordered
domains, e.g. a university campus or a document
structure. Their approach is mainly targeted at
producing textual references to entities in written
documents (e.g. figures, tables in book chapters).
Consequently they do not address the challenges
that arise in physically and perceptually situated
dialogues. Still, the approach presents a num-
ber of good contributions towards GRE for situ-
ated dialogue in large-scale space. An appropriate
context, as a subset of the full domain, is deter-
mined through Ancestral Search. This search for
the intended referent is rooted in the ?position of
the speaker and the hearer in the domain? (repre-
sented as d), a crucial first step towards situated-
ness. Their approach suffers from the shortcom-
ing that spatial relationships are treated as one-
place attributes by their GRE algorithm. For ex-
ample they transform the spatial containment re-
lation that holds between a room entity and a
building entity (?the library in the Cockroft build-
ing?) into a property of the room entity (BUILDING
NAME = COCKROFT) and not a two-place relation
(in(library,Cockroft)). Thus they avoid
recursive calls to the algorithm, which would be
needed if the intended referent is related to another
entity that needs to be properly referred to.
However, according to Dale and Reiter (1995),
these related entities do not necessarily serve as
discriminatory information. At least in large-scale
space, in contrast to a document structure that is
conceivably transparent to a reader, they function
as attention-directing elements that are introduced
to build up common ground by incrementally ex-
tending the hearer?s focus of attention. Moreover,
representing some spatial relations as two-place
predicates between two entities and some as one-
place predicates is an arbitrary decision.
We present an approach for context determina-
tion (or extension), that imposes less restrictions
on its knowledge base, and which can be used as a
sub-routine in existing GRE algorithms.
127
3 Situated Dialogue in Large-Scale Space
Imagine the situation in Figure 1 did not take place
somewhere on campus, but rather inside building
3B. Certainly the robot would not have said ?the
IT help desk is on the 1st floor in building 3B.?
To avoid confusing the human, an utterance like
?the IT help desk is on the 1st floor? would have
been appropriate. Likewise, if the IT help desk
happened to be located on another site of the uni-
versity, the robot would have had to identify its lo-
cation as being ?on the 1st floor in building 3B on
the new campus.? The hierarchical representation
of space that people are known to assume (Cohn
and Hazarika, 2001), reflects upon the choice of
an appropriate context when producing REs.
In the above example the physical and spatial
situatedness of the dialogue participants play an
important role in determining which related parts
of space come into consideration as potential dis-
tractors. Another important observation concerns
the verbal behavior of humans when talking about
remote objects and places during a complex dia-
logue (i.e. more than just a question and a reply).
Consider the following example dialogue:
Person A: ?Where is the exit??
Person B: ?You first go down this corridor.
Then you turn right. After a few steps you
will see the big glass doors.?
Person A: ?And the bus station? Is it to the
left??
The dialogue illustrates how utterances become
grounded in previously introduced discourse ref-
erents, both temporally and spatially. Initially,
the physical surroundings of the dialogue partners
form the context for anchoring references. As a di-
alogue unfolds, this point can conceptually move
to other locations that have been explicitly intro-
duced. Discourse markers denoting spatial or tem-
poral cohesion (e.g. ?then? or ?there?) can make
this move to a new anchor explicit, leading to a
?mental tour? through large-scale space.
We propose a general principle of Topological
Abstraction (TA) for context extension which is
rooted in what we will call the Referential Anchor
a.1 TA is designed for a multiple abstraction hier-
archy (e.g. represented as a lattice structure rather
than a simple tree). The Referential Anchor a, cor-
responding to the current focus of attention, forms
the nucleus of the context. In the simple case, a
1similar to Ancestral Search (Paraboni et al, 2007)
loc1 loc2 loc3
room1 room2
floor1_1 floor1_2
building1
loc4 (a) loc5 loc7 loc8loc6
room3 room4 room5 (r)
floor2_1 floor2_2
building2
1
2
3
4
Figure 2: Incremental TA in large-scale space
corresponds to the hearer?s physical location. As
illustrated above, a can also move along the ?spa-
tial progression? of the most salient discourse en-
tity during a dialogue. If the intended referent is
outside the current context, TA extends the context
by incrementally ascending the spatial abstraction
hierarchy until the intended referent is an element
of the resulting sub-hierarchy, as illustrated in Fig-
ure 2. Below we describe two instantiations of the
TA principle, a TA algorithm for reference gener-
ation (TAA1) and TAA2 for reference resolution.
Context Determination for GRE TAA1 con-
structs a set of entities dominated by the Referen-
tial Anchor a (and a itself). If this set contains the
intended referent r, it is taken as the current utter-
ance context set. Else TAA1 moves up one level
of abstraction and adds the set of all child nodes to
the context set. This loop continues until r is in the
context set. At that point TAA1 stops and returns
the constructed context set (cf. Algorithm 1).
TAA1 is formulated to be neutral to the kind of
GRE algorithm that it is used for. It can be used
with the original Incremental Algorithm (Dale and
Reiter, 1995), augmented by a recursive call if a
relation to another entity is selected as a discrim-
inatory feature. It could in principle also be used
with the standard approach to GRE involving re-
lations (Dale and Haddock, 1991), but we agree
with Paraboni et al (2007) that the mutually qual-
ified references that it can produce2 are not easily
resolvable if they pertain to circumstances where
a confirmatory search is costly (such as in large-
scale space). More recent approaches to avoid-
ing infinite loops when using relations in GRE
make use of a graph-based knowledge represen-
tation (Krahmer et al, 2003; Croitoru and van
Deemter, 2007). TAA1 is compatible with these
approaches, as well as with the salience based ap-
proach of (Krahmer and Theune, 2002).
2An example for such a phenomenon is the expression
?the ball on the table? in a context with several tables and
several balls, but of which only one is on a table. Humans
find such REs natural and easy to resolve in visual scenes.
128
Algorithm 1 TAA1 (for reference generation)
Require: a = referential anchor; r = intended referent
Initialize context: C = {}
C = C ? topologicalChildren(a) ? {a}
if r ? C then
return C
else
Initialize: SUPERNODES = {a}
for each n ? SUPERNODES do
for each p ? topologicalParents(n) do
SUPERNODES = SUPERNODES ? {p}
C = C ? topologicalChildren(p)
end for
if r ? C then
return C
end if
end for
return failure
end if
Algorithm 2 TAA2 (for reference resolution)
Require: a = ref. anchor; desc(x) = description of referent
Initialize context: C = {}
Initialize possible referents: R = {}
C = C ? topologicalChildren(a) ? {a}
R = desc(x) ? C
if R 6= {} then
return R
else
Initialize: SUPERNODES = {a}
for each n ? SUPERNODES do
for each p ? topologicalParents(n) do
SUPERNODES = SUPERNODES ? {p}
C = C ? topologicalChildren(p)
end for
R = desc(x) ? C
if R 6= {} then
return R
end if
end for
return failure
end if
Resolving References to Elsewhere Analogous
to the GRE task, a conversational robot must be
able to understand verbal descriptions by its users.
In order to avoid overgenerating possible refer-
ents, we propose TAA2 (cf. Algorithm 2) which
tries to select an appropriate referent from a rel-
evant subset of the full knowledge base. It is ini-
tialized with a given semantic representation of the
referential expression, desc(x), in a format com-
patible with the knowledge base. Then, an appro-
priate entity satisfying this description is searched
for in the knowledge base. Similarly to TAA1,
the description is first matched against the current
context set C consisting of a and its child nodes. If
this set does not contain any instances that match
desc(x), TAA2 increases the context set alng the
spatial abstraction axis until at least one possible
referent can be identified within the context.
4 Conclusions and Future Work
We have presented two algorithms for context de-
termination that can be used both for resolving and
generating REs in large-scale space.
We are currently planning a user study to evalu-
ate the performance of the TA algorithms. Another
important item for future work is the exact nature
of the spatial progression, modeled by ?moving?
the referential anchor, in a situated dialogue.
Acknowledgments
This work was supported by the EU FP7 ICT
Project ?CogX? (FP7-ICT-215181).
References
A. G. Cohn and S. M. Hazarika. 2001. Qualitative
spatial representation and reasoning: An overview.
Fundamenta Informaticae, 46:1?29.
M. Croitoru and K. van Deemter. 2007. A conceptual
graph approach to the generation of referring expres-
sions. In Proc. IJCAI-2007, Hyderabad, India.
R. Dale and N. Haddock. 1991. Generating referring
expressions involving relations. In Proc. of the 5th
Meeting of the EACL, Berlin, Germany, April.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean Maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
H. Horacek. 1997. An algorithm for generating ref-
erential descriptions with flexible interfaces. In
Proc. of the 35th Annual Meeting of the ACL and
8th Conf. of the EACL, Madrid, Spain.
J. Kelleher and G.-J. Kruijff. 2006. Incremental gener-
ation of spatial referring expressions in situated di-
alogue. In In Proc. Coling-ACL 06, Sydney, Aus-
tralia.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R.Kibble, editors, Information
Sharing: Givenness and Newness in Language Pro-
cessing. CSLI Publications, Stanford, CA, USA.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1).
B. Kuipers. 1977. Representing Knowledge of Large-
scale Space. Ph.D. thesis, Massachusetts Institute of
Technology, Cambridge, MA, USA.
I. Paraboni, K. van Deemter, and J. Masthoff. 2007.
Generating referring expressions: Making refer-
ents easy to identify. Computational Linguistics,
33(2):229?254, June.
H. Zender, O. Mart??nez Mozos, P. Jensfelt, G.-J. Krui-
jff, and W. Burgard. 2008. Conceptual spatial rep-
resentations for indoor mobile robots. Robotics and
Autonomous Systems, 56(6):493?502, June.
129
Generalizing Dimensionality in Combinatory Categorial Grammar
Geert-Jan M. Kruijff
Computational Linguistics
Saarland University
Saarbru?cken, Germany
gj@coli.uni-sb.de
Jason Baldridge
ICCS, Division of Informatics
University of Edinburgh
Edinburgh, Scotland
jbaldrid@inf.ed.ac.uk
Abstract
We extend Combinatory Categorial Grammar
(CCG) with a generalized notion of multi-
dimensional sign, inspired by the types of rep-
resentations found in constraint-based frame-
works like HPSG or LFG. The generalized
sign allows multiple levels to share information,
but only in a resource-bounded way through
a very restricted indexation mechanism. This
improves representational perspicuity without
increasing parsing complexity, in contrast to
full-blown unification used in HPSG and LFG.
Well-formedness of a linguistic expressions re-
mains entirely determined by the CCG deriva-
tion. We show how the multidimensionality
and perspicuity of the generalized signs lead to
a simplification of previous CCG accounts of
how word order and prosody can realize infor-
mation structure.
1 Introduction
The information conveyed by linguistic utterances
is diverse, detailed, and complex. To properly ana-
lyze what is communicated by an utterance, this in-
formation must be encoded and interpreted at many
levels. The literature contains various proposals for
dealing with many of these levels in the description
of natural language grammar.
Since information flows between different levels
of analysis, it is common for linguistic formalisms
to bundle them together and provide some means
for communication between them. Categorial gram-
mars, for example, normally employ a Saussurian
sign that relates a surface string with its syntactic
category and the meaning it expresses. Syntactic
analysis is entirely driven by the categories, and
when information from other levels is used to affect
the derivational possibilities, it is typically loaded as
extra information on the categories.
Head-driven Phrase Structure Grammar (HPSG)
(Pollard and Sag, 1993) and Lexical Functional
Grammar (LFG) (Kaplan and Bresnan, 1982) also
use complex signs. However, these signs are mono-
lithic structures which permit information to be
freely shared across all dimensions: any given di-
mension can place restrictions on another. For ex-
ample, variables resolved during the construction of
the logical form can block a syntactic analysis. This
provides a clean, unified formal system for dealing
with the different levels, but it also can adversely af-
fect the complexity of parsing grammars written in
these frameworks (Maxwell and Kaplan, 1993).
We thus find two competing perspectives on com-
munication between levels in a sign. In this paper,
we propose a generalization of linguistic signs for
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000b). This generalization enables different
levels of linguistic information to be represented but
limits their interaction in a resource-bounded man-
ner, following White (2004). This provides a clean
separation of the levels and allows them to be de-
signed and utilized in a more modular fashion. Most
importantly, it allows us to retain the parsing com-
plexity of CCG while gaining the representational
advantages of the HPSG and LFG paradigms.
To illustrate the approach, we use it to model var-
ious aspects of the realization of information struc-
ture, an inherent aspect of the (linguistic) meaning
of an utterance. Speakers use information struc-
ture to present some parts of that meaning as de-
pending on the preceding discourse context and oth-
ers as affecting the context by adding new content.
Languages may realize information structure us-
ing different, often interacting means, such as word
order, prosody, (marked) syntactic constructions,
or morphological marking (Vallduv?? and Engdahl,
1996; Kruijff, 2002). The literature presents vari-
ous proposals for how information structure can be
captured in categorial grammar (Steedman, 2000a;
Hoffman, 1995; Kruijff, 2001). Here, we model the
essential aspects of these accounts in a more per-
spicuous manner by using our generalized signs.
The main outcomes of the proposal are three-
fold: (1) CCG gains a more flexible and general
kind of sign; (2) these signs contain multiple levels
that interact in a modular fashion and are built via
CCG derivations without increasing parsing com-
plexity; and (3) we use these signs to simplify pre-
vious CCG?s accounts of the effects of word order
and prosody on information structure.
2 Combinatory Categorial Grammar
In this section, we give an overview of syntactic
combination and semantic construction in CCG. We
use CCG?s multi-modal extension (Baldridge and
Kruijff, 2003), which enriches the inventory of slash
types. This formalization renders constraints on
rules unnecessary and supports a universal set of
rules for all grammars.
2.1 Categories and combination
Nearly all syntactic behavior in CCG is encoded in
categories. They may be atoms, like np, or func-
tions which specify the direction in which they seek
their arguments, like (s\np)/np. The latter is the
category for English transitive verbs; it first seeks
its object to its right and then its subject to its left.
Categories combine through a small set of univer-
sal combinatory rules. The simplest are application
rules which allow a function category to consume
its argument either on its right (>) or on its left (<):
(>) X/?Y Y ? X
(<) Y X\?Y ? X
Four further rules allow functions to compose
with other functions:
(>B) X/Y Y/Z ? X/Z
(<B) Y\Z X\Y ? X\Z
(>B?) X/?Y Y\?Z ? X\?Z
(<B?) Y/?Z X\?Y ? X/?Z
The modalities ?,  and ? on the slashes enforce
different kinds of combinatorial potential on cate-
gories. For a category to serve as input to a rule,
it must contain a slash which is compatible with
that specified by the rule. The modalities work
as follows. ? is the most restricted modality, al-
lowing combination only by the application rules
(> and <).  allows combination with the appli-
cation rules and the order-preserving composition
rules (>B and <B). ? allows limited permutation
via the crossed composition rules (>B? and <B?)
as well as the application rules. Additionally, a per-
missive modality ? allows combination by all rules
in the system. However, we suppress the ? modal-
ity on slashes to avoid clutter. An undecorated slash
may thus combine by all rules.
There are two further rules of type-raising that
turn an argument category into a function over func-
tions that seek that argument:
(>T) X ? Y/i(Y\iX)
(<T) X ? Y\i(Y/iX)
The variable modality i on the output categories
constrains both slashes to have the same modality.
These rules support the following incremental
derivation for Marcel proved completeness:
(1) Marcel proved completeness
np (s\np)/np np
>T
s/(s\np)
>B
s/np
>s
This derivation does not display the effect of us-
ing modalities in CCG; see Baldridge (2002) and
Baldridge and Kruijff (2003) for detailed linguistic
justification for this modalized formulation of CCG.
2.2 Hybrid Logic Dependency Semantics
Many different kinds of semantic representations
and ways of building them with CCG exist. We
use Hybrid Logic Dependency Semantics (HLDS)
(Kruijff, 2001), a framework that utilizes hybrid
logic (Blackburn, 2000) to realize a dependency-
based perspective on meaning.
Hybrid logic provides a language for represent-
ing relational structures that overcomes standard
modal logic?s inability to directly reference states
in a model. This is achieved via nominals, a kind of
basic formula which explicitly names states. Like
propositions, nominals are first-class citizens of the
object language, so formulas can be formed us-
ing propositions, nominals, standard boolean oper-
ators, and the satisfaction operator ?@?. A formula
@i(p? ?F?(j ? q)) indicates that the formulas p and
?F?(j ? q) hold at the state named by i and that the
state j is reachable via the modal relation F.
In HLDS, hybrid logic is used as a language
for describing semantic interpretations as follows.
Each semantic head is associated with a nominal
that identifies its discourse referent and heads are
connected to their dependents via dependency rela-
tions, which are modeled as modal relations. As an
example, the sentence Marcel proved completeness
receives the representation in (2).
(2) @e(prove ? ?TENSE?past
??ACT?(m?Marcel)??PAT?(c?comp.))
In this example, e is a nominal that labels the predi-
cations and relations for the head prove, and m and
c label those for Marcel and completeness, respec-
tively. The relations ACT and PAT represent the de-
pendency roles Actor and Patient, respectively.
By using the @ operator, hierarchical terms such
as (2) can be flattened to an equivalent conjunction
of fixed-size elementary predications (EPs):
(3) @eprove ? @e?TENSE?past ? @e?ACT?m
? @e?PAT?c ? @mMarcel ? @ccomp.
2.3 Semantic Construction
Baldridge and Kruijff (2002) show how HLDS
representations can be built via CCG derivations.
White (2004) improves HLDS construction by op-
erating on flattened representations such as (3) and
using a simple semantic index feature in the syntax.
We adopt this latter approach, described below.
EPs are paired with syntactic categories in the
lexicon as shown in (4)?(6) below. Each atomic cat-
egory has an index feature, shown as a subscript,
which makes a nominal available for capturing syn-
tactically induced dependencies.
(4) prove ` (se\npx)/npy :
@eprove ? @e?TENSE?past
? @e?ACT?x ? @e?PAT?y
(5) Marcel ` npm : @mMarcel
(6) completeness ` npc : @ccompleteness
Applications of the combinatory rules co-index
the appropriate nominals via unification on the cat-
egories. EPs are then conjoined to form the result-
ing interpretation. For example, in derivation (1),
(5) type-raises and composes with (4) to yield (7).
The index x is syntactically unified with m, and this
resolution is reflected in the new conjoined logical
form. (7) can then apply to (6) to yield (8), which
has the same conjunction of predications as (3).
(7) Marcel proved ` se/npy :
@eprove ? @e?TENSE?past
? @e?ACT?m ? @e?PAT?y ? @mMarcel
(8) Marcel proved completeness ` se :
@eprove ? @e?TENSE?past ? @e?ACT?m
?@e?PAT?c?@mMarcel ?@ccompleteness
Since the EPs are always conjoined by the com-
binatory rules, semantic construction is guaranteed
to be monotonic. No semantic information can be
dropped during the course of a derivation. This pro-
vides a clean way of establishing semantic depen-
dencies as informed by the syntactic derivation. In
the next section, we extend this paradigm for use
with any number of representational levels.
3 Generalized dimensionality
To support a more modular and perspicuous encod-
ing of multiple levels of analysis, we generalize the
notion of sign commonly used in CCG. The ap-
proach is inspired on the one hand by earlier work
by Steedman (2000a) and Hoffman (1995), and on
the other by the signs found in constraint-based ap-
proaches to grammar. The principle idea is to ex-
tend White?s (2004) approach to semantic construc-
tion (see ?2.3). There, categories and the mean-
ing they help express are connected through co-
indexation. Here, we allow for information in any
(finite) number of levels to be related in this way.
A sign is an n-tuple of terms that represent in-
formation at n distinct dimensions. Each dimension
represents a level of linguistic information such as
prosody, meaning, or syntactic category. As a repre-
sentation, we assume that we have for each dimen-
sion a language that defines well-formed representa-
tions, and a set of operations which can create new
representations from a set of given representations.1
For example, we have by definition a dimension
for syntactic categories. The language for this di-
mension is defined by the rules for category con-
struction: given a set of atomic categories A, C is a
category iff (i) C ? A or (ii) C is of the form A\mB
or A/mB with A,B categories and m ? {?,  ?, ?}.
The set of combinatory rules defines the possible
operations on categories.
This syntactic category dimension drives the
grammatical analysis, thus guiding the composition
of signs. When two categories are combined via
a rule, the appropriate indices are unified. It is
through this unification of indices that information
can be passed between signs. At a given dimen-
sion, the co-indexed information coming from the
two signs we combine must be unifiable.
With these signs, dimensions interact in a more
limited way than in HPSG or LFG. Constraints (re-
solved through unification) may only be applied
if they are invoked through co-indexation on cat-
egories. This provides a bound on the number of
indices and the number of unifications to be made.
As such, full recursion and complex unification as in
attribute-value matrices with re-entrancy is avoided.
The approach incorporates various ideas from
constraint-based approaches, but remains based on
a derivational perspective on grammatical analysis
and derivational control, unlike e.g Categorial Uni-
fication Grammar. Furthermore, the ability for di-
mensions to interact through shared indices brings
several advantages: (1) ?parallel derivations? (Hoff-
man, 1995) are unnecessary; (2) non-isomorphic,
functional structures across different dimensions
can be employed; and (3) there is no longer a need
to load all the necessary information into syntactic
categories (as with Kruijff (2001)).
1In the context of this paper we assume operations are mul-
tiplicative. Also, note that dimensions may differ in what lan-
guages and operations they use.
4 Examples
In this section, we illustrate our approach on several
examples involving information structure. We use
signs that include the following dimensions.
Phonemic representation: word sequences, composi-
tion of sequences is through concatenation
Prosody: sequences of tunes from the inventory of
(Pierrehumbert and Hirschberg, 1990), composi-
tion through concatenation
Syntactic category: well-formed categories, combina-
tory rules (see ?2)
Information structure: hybrid logic formulas of the
form @d [in]r, with r a discourse referent that has
informativity in (theme ?, or rheme ?) relative to
the current point in the discourse d (Kruijff, 2003).
Predicate-argument structure: hybrid logic formulas
of the form as discussed in ?2.3.
Example (9) illustrates a sign with these dimen-
sions. The word-form Marcel bears an H* accent,
and acts as a type-raised category that seeks a verb
missing its subject. The H* accent indicates that the
discourse referent m introduces new information at
the current point in the discourse d: i.e. the meaning
@mmarcel should end up as part of the rheme (?) of
the utterance, @d [?]m.
(9) Marcel
H*
sh/(sh\npm)
@d [?]m
@mmarcel
If a sign does not specify any information at a
particular dimension, this is indicated by > (or an
empty line if no confusion can arise).
4.1 Topicalization
We start with a simple example of topicalization in
English. In topicalized constructions, a thematic ob-
ject is fronted before the subject. Given the question
Did Marcel prove soundness and completeness?,
(10) is a possible response using topicalization:
(10) Completeness, Marcel proved, and sound-
ness, he conjectured.
We can capture the syntactic and information
structure effects of such sentences by assigning the
following kind of sign to (topicalized) noun phrases:
(11) completeness
>
si/(si/npc)
@d [?]c
@ccompleteness
This category enables the derivation in Figure 1.
The type-raised subject composes with the verb, and
the result is consumed by the topicalizing category.
The information structure specification stated in the
sign in (11) is passed through to the final sign.
The topicalization of the object in (10) only indi-
cates the informativity of the discourse referent re-
alized by the object. It does not yield any indica-
tions about the informativity of other constituents;
hence the informativity for the predicate and the Ac-
tor is left unspecified. In English, the informativity
of these discourse referents can be indicated directly
with the use of prosody, to which we now turn.
4.2 Prosody & information structure
Steedman (2000a) presents a detailed, CCG-based
account of how prosody is used in English as a
means to realize information structure. In the
model, pitch accents and boundary tones have an ef-
fect on both the syntactic category of the expression
they mark, and the meaning of that expression.
Steedman distinguishes pitch accents as markers
of either the theme (?) or of the rheme (?): L+H*
and L*+H are ?-markers; H*, L*, H*+L and H+L*
are ?-markers. Since pitch accents mark individual
words, not (necessarily) larger phrases, Steedman
uses the ?/?-marking to spread informativity over
the domain and the range of function categories.
Identical markings on different parts of a function
category not only act as features, but also as occur-
rences of a singular variable. The value of the mark-
ing on the domain can thus get passed down (?pro-
jected?) to markings on categories in the range.
Constituents bearing no tune have an ?-marking,
which can be unified with either ?, ? or ?. Phrases
with such markings are ?incomplete? until they
combine with a boundary tone. Boundary tones
have the effect of mapping phrasal tones into
intonational phrase boundaries. To make these
boundaries explicit and enforce such ?complete?
prosodic phrases to only combine with other com-
plete prosodic phrases, Steedman introduces two
further types of marking ? ? and ? ? on categories.
The ? markings only unify with other ? or ? mark-
ings on categories, not with ?, ? or ?. These mark-
ings are only introduced to provide derivational con-
trol and are not reflected in the underlying meaning
(which only reflects ?, ? or ?).
Figure 2 recasts the above as an abstract speci-
fication of which different types of prosodic con-
stituents can, or cannot, be combined.2 Steedman?s
2There is one exception we should note: two intermediate
phrases can combine if a second one has a downstepped accent.
We deal with this exception at the end of the section.
completeness Marcel proved
si/(si/npc) sj /(sj \npm) (sp\npx )/npy
@d [?]c
@ccompleteness @mMarcel @pprove ? @p?ACT?x ? @p?PAT?y
>B
sp/npy
@pprove ? @p?ACT?m ? @p?PAT?y ? @mMarcel
>sp
@d [?]c
@pprove ? @p?ACT?m ? @p?PAT?c ? @mMarcel ? @ccompleteness
Figure 1: Derivation for topicalization.
system can be implemented using just one feature
pros which takes the values ip for intermediate
phrases, cp for complete phrases, and up for un-
marked phrases. We write spros=ip , or simply sip if
no confusion can arise.
Figure 2: Abstract specification of derivational con-
trol in prosody
First consider the top half of Figure 2. If a con-
stituent is marked with either a ?- or ?-tune, the
atomic result category of the (possibly complex)
category is marked with ip. Prosodically unmarked
constituents are marked as up. The lexical entries
in (12) illustrates this idea.3
(12) MARCEL proved COMPLETENESS
H* L+H*
sip/(sup\np) (sup\np)/np sip$\(sup$/np)
This can proceed in two ways. Either the marked
MARCEL and the unmarked proved combine to pro-
duce an intermediate phrase (13), or proved and the
marked COMPLETENESS combine (14).
(13) MARCEL proved COMPLETENESS
H* L+H*
sip/(sup\np) (sup\np)/np sip$\(sup$/np)
>
sip/np
3The $?s in the category for COMPLETENESS are standard
CCG schematizations: s$ indicates all functions into s, such as
s\np and (s\np)/np. See Steedman (2000b) for details.
(14) MARCEL proved COMPLETENESS
H* L+H*
sip/(sup\np) (sup\np)/np sip$\(sup$/np)
<
sip\np
For the remainder of this paper, we will suppress up
marking and write sup simply as s.
Examples (13) and (14) show that prosodically
marked and unmarked phrases can combine. How-
ever, both of these partial derivations produce cate-
gories that cannot be combined further. For exam-
ple, in (14), sip/(s\np) cannot combine with sip\np
to yield a larger intermediate phrase. This properly
captures the top half of Figure 2.
To obtain a complete analysis for (12), bound-
ary tones are needed to complete the intermediate
phrases tones. For example, consider (15) (based
on example (70) in Steedman (2000a)):
(15) MARCEL proved COMPLETENESS
H* L L+H* LH%
To capture the bottom-half of Figure 2, the bound-
ary tones L and LH% need categories which cre-
ate complete phrases out of those for MARCEL and
proved COMPLETENESS, and thereafter allow them
to combine. Figure 3 shows the appropriate cate-
gories and complete analysis.
We noted earlier that downstepped phrasal tunes
form an exception to the rule that intermediate
phrases cannot combine. To enable this, we not
only should mark the result category with ip (tune),
but also any leftward argument(s) should have ip
(downstep). Thus, the effect of (lexically) combin-
ing a downstep tune with an unmarked category is
specified by the following template: add marking
xip$\yip to an unmarked category of the form x$\y.
The derivation in Figure 5 illustrates this idea on ex-
ample (64) from (Steedman, 2000a).
To relate prosody to information structure, we ex-
tend the strategy used for constructing logical forms
described in ?2.3, in which a simple index feature
MARCEL proved COMPLETENESS
H* L L+H* LH%
sip/(s\np) (scp/scp$)\?(sip/s$) (s\np)/np sip$\(s$/np) scp$\?sip$
< <
scp/(scp\np) sip\np
<
scp\np
>scp
Figure 3: Derivation including tunes and boundary tones; (70) from (Steedman, 2000a)
Marcel PROVED COMPLETENESS
L+H* LH% H* LL%
np (sip:p\npx )/npy scp$\?sip$ sip\(s/npc) (scp\scp$)\?(sip\s$)
@d [?]p @d [?]c
@mMarcel @pprove ? @p?ACT?x ? @p?PAT?y @ccompleteness
>T <
sip/(sip\np) scp\(scp/npc)
@d [?]c
@mMarcel @ccompleteness
>B
sip/np
@d [?]p
@pprove ? @p?ACT?m ? @p?PAT?y ? @mMarcel
<
scp/npy
@d [?]p
@pprove ? @p?ACT?m ? @p?PAT?y ? @mMarcel
<scp
@d [?]p ? @d [?]c
@pprove ? @p?ACT?m ? @p?PAT?c ? @mMarcel ? @ccompleteness
Figure 4: Information structure for derivation for (67)-(68) from (Steedman, 2000a)
on atomic categories makes a nominal (discourse
referent) available. We represent information struc-
ture as a formula @d [i]r at a dimension separate
from the syntactic category. The nominal r stands
for the discourse referent, which has informativity
i with respect to the current point in the discourse
d (Kruijff, 2003). Following Steedman, we distin-
guish two levels of informativity, namely ? (theme)
and ? (rheme).
We start with a minimal assignment of informa-
tivity: a theme-tune on a constituent sets the infor-
mativity of the discourse referent r realized by the
constituent to ? and a rheme-tune sets it to ?. This
is a minimal assignment in the sense that we do not
project informativity; instead, we only set informa-
tivity for those discourse referents whose realization
shows explicit clues as to their information status.
The derivation in Figure 4 illustrates this idea and
shows the construction of both logical form and in-
formation structure.
Indices can also impose constraints on the infor-
mativity of arguments. For example, in the down-
step example (Figure 5), the discourse referents cor-
responding to ANNA and SAYS are both part of the
theme. We specify this with the constituent that has
received the downstepped tune. The referent of the
subject of SAYS (indexed x) must be in the theme
along with the referent s for SAYS. This is satisfied
in the derivation: a unifies with x, and we can unify
the statements about a?s informativity coming from
ANNA (@d [?]a) and SAYS (@d [?]x with x replaced
by a in the >B step).
5 Conclusions
In this paper, we generalize the traditional Saus-
surian sign in CCG with an n-dimensional linguis-
tic sign. The dimensions in the generalized linguis-
tic sign can be related through indexation. Index-
ation places constraints on signs by requiring that
co-indexed material is unifiable, on a per-dimension
basis. Consequently, we do not need to overload the
syntactic category with information from different
dimensions.
The resulting sign structure resembles the signs
found in constraint-based grammar formalisms.
There is, however, an important difference. Infor-
mation at various dimensions can be related through
co-indexation, but dimensions cannot be directly
ANNA SAYS he proved COMPLETENESS
L+H* !L+H* LH%
npip:a (sip:s\npip:x )/sy s/(s\np) (sp\np)/np
@d [?]a @d [?]s ? @d [?]x @d [?](pron) @d [i]p
>T
sip/(sip\npip)
@d [?]a
>B
sip/s
@d [?]s ? @d [?]a
>B
sip/(s\np)
@d [?]s ? @d [?]a ? @d [?](pron)
>B
sip/np
@d [?]s ? @d [?]a ? @d [?](pron) ? @d [i]p
Figure 5: Information structure for derivation for (64) from (Steedman, 2000a)
referenced. As analysis remains driven only by in-
ference over categories, only those constraints trig-
gered by indexation on the categories are imposed.
We do not allow for re-entrancy.
It is possible to conceive of a scenario in which
the various levels can contribute toward determin-
ing the well-formedness of an expression. For ex-
ample, we may wish to evaluate the current informa-
tion structure against a discourse model, and reject
the analysis if we find it is unsatisfiable. If such a
move is made, then the complexity will be bounded
by the complexity of the dimension for which it is
most difficult to determine satisfiability.
Acknowledgments
Thanks to Ralph Debusmann, Alexander Koller,
Mark Steedman, and Mike White for discussion.
Geert-Jan Kruijff?s work is supported by the DFG
SFB 378 Resource-Sensitive Cognitive Processes,
Project NEGRA EM 6.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. of 40th Annual Meeting of the ACL, pages 319?
326, Philadelphia, Pennsylvania.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proc. of
10th Annual Meeting of the EACL, Budapest.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Patrick Blackburn. 2000. Representation, reasoning,
and relational structures: a hybrid logic manifesto.
Journal of the Interest Group in Pure Logic, 8(3):339?
365.
Beryl Hoffman. 1995. Integrating ?free? word order
syntax and information structure. In Proc. of 7th An-
nual Meeting of the EACL, Dublin.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In The Mental Representation
of Grammatical Relations, pages 173?281. The MIT
Press, Cambridge Massachusetts.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logi-
cal Architecture of Informativity: Dependency Gram-
mar Logic & Information Structure. Ph.D. thesis,
Charles University, Prague, Czech Republic.
Geert-Jan M. Kruijff. 2002. Formulating a category of
informativity. In Hilde Hasselgard, Stig Johansson,
Bergljot Behrens, and Cathrine Fabricius-Hansen, ed-
itors, Information Structure in a Cross-Linguistic Per-
spective, pages 129?146. Rodopi, Amsterdam.
Geert-Jan M. Kruijff. 2003. Binding across boundaries.
In Geert-Jan M. Kruijff and Richard T. Oehrle, editors,
Resource Sensitivity, Binding, and Anaphora. Kluwer
Academic Publishers, Dordrecht.
John T. III Maxwell and Ronald M. Kaplan. 1993. The
interface between phrasal and functional constraints.
Computational Linguistics, 19(4):571?590.
Janet Pierrehumbert and Julia Hirschberg. 1990. The
meaning of intonational contours in the interpretation
of discourse. In J. Morgan P. Cohen and M. Pollack,
editors, Intentions in Communication. The MIT Press,
Cambridge Massachusetts.
Carl Pollard and Ivan A. Sag. 1993. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago IL.
Mark Steedman. 2000a. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
Mark Steedman. 2000b. The Syntactic Process. The
MIT Press, Cambridge, MA.
Enric Vallduv?? and Elisabet Engdahl. 1996. The linguis-
tic realization of information packaging. Linguistics,
34:459?519.
Michael White. 2004. Efficient realization of coordinate
structures in Combinatory Categorial Grammar. Re-
search on Language and Computation. To appear.
Talking Robots With LEGO MindStorms
Alexander Koller
Saarland University
Saarbru?cken, Germany
koller@coli.uni-sb.de
Geert-Jan M. Kruijff
Saarland University
Saarbru?cken, Germany
gj@coli.uni-sb.de
Abstract
This paper shows how talking robots can be built
from off-the-shelf components, based on the Lego
MindStorms robotics platform. We present four
robots that students created as final projects in a
seminar we supervised. Because Lego robots are so
affordable, we argue that it is now feasible for any
dialogue researcher to tackle the interesting chal-
lenges at the robot-dialogue interface.
1 Introduction
Ever since Karel ?Capek introduced the word ?robot?
in his 1921 novel Rossum?s Universal Robots and
the subsequent popularisation through Issac Asi-
mov?s books, the idea of building autonomous
robots has captured people?s imagination. The cre-
ation of an intelligent, talking robot has been the ul-
timate dream of Artificial Intelligence from the very
start.
Yet, although there has been a tremendous
amount of AI research on topics such as control
and navigation for robots, the issue of integrat-
ing dialogue capabilities into a robot has only re-
cently started to receive attention. Early successes
were booked with Flakey (Konolige et al, 1993),
a voice-controlled robot which roamed the corri-
dors of SRI. Since then, the field of socially in-
teractive robots has established itself (see (Fong et
al., 2003)). Often-cited examples of such inter-
active robots that have a capability of communi-
cating in natural language are the humanoid robot
ROBOVIE (Kanda et al, 2002) and robotic mu-
seum tour guides like RHINO (Burgard et al, 1999)
(Deutsches Museum Bonn), its successor MINERVA
touring the Smithsonian in Washington (Thrun et
al., 2000), and ROBOX at the Swiss National Ex-
hibition Expo02 (Siegwart and et al 2003). How-
ever, dialogue systems used in robotics appear to
be mostly restricted to relatively simple finite-state,
query/response interaction. The only robots in-
volving dialogue systems that are state-of-the-art in
computational linguistics (and that we are aware of)
are those presented by Lemon et al (2001), Sidner
et al (2003) and Bos et al (2003), who equipped
a mobile robot with an information state based dia-
logue system.
There are two obvious reasons for this gap be-
tween research on dialogue systems in robotics on
the one hand, and computational linguistics on the
other hand. One is that the sheer cost involved
in buying or building a robot makes traditional
robotics research available to only a handful of re-
search sites. Another is that building a talking robot
combines the challenges presented by robotics and
natural language processing, which are further ex-
acerbated by the interactions of the two sides.
In this paper, we address at least the first prob-
lem by demonstrating how to build talking robots
from affordable, commercial off-the-shelf (COTS)
components. We present an approach, tested in a
seminar taught at the Saarland University in Win-
ter 2002/2003, in which we combine the Lego
MindStorms system with COTS software for speech
recognition/synthesis and dialogue modeling.
The Lego MindStorms1 system extends the tra-
ditional Lego bricks with a central control unit (the
RCX), as well as motors and various kinds of sen-
sors. It provides a severely limited computational
platform from a traditional robotics point of view,
but comes at a price of a few hundred, rather than
tens of thousands of Euros per kit. Because Mind-
Storms robots can be flexibly connected to a dia-
logue system running on a PC, this means that af-
fordable robots are now available to dialogue re-
searchers.
We present four systems that were built by teams
of three students each under our supervision, and
use off-the-shelf components such as the Mind-
Storms kits, a dialogue system, and a speech recog-
niser and synthesis system, in addition to commu-
nications software that we ourselves wrote to link
all the components together. It turns out that using
1LEGO and LEGO MindStorms are trademarks of the
LEGO Company.
this accessible technology, it is possible to create
basic but interesting talking robots in limited time
(7 weeks). This is relevant not only for future re-
search, but can also serve as a teaching device that
has shown to be extremely motivating for the stu-
dents. MindStorms are a staple in robotics educa-
tion (Yu, 2003; Gerovich et al, 2003; Lund, 1999),
but to our knowledge, they have never been used as
part of a language technology curriculum.
The paper is structured as follows. We first
present the basic setup of the MindStorms system
and the software architecture. Then we present the
four talking robots built by our students in some de-
tail. Finally, we discuss the most important chal-
lenges that had to be overcome in building them.
We conclude by speculating on further work in Sec-
tion 5.
2 Architecture
Lego MindStorms robots are built around a pro-
grammable microcontroller, the RCX. This unit,
which looks like an oversized yellow Lego brick,
has three ports each to attach sensors and motors,
an infrared sender/receiver for communication with
the PC, and 32 KB memory to store the operating
system, a programme, and data.
Figure 1: Architecture of a talking Lego robot.
Our architecture for talking robots (Fig. 1) con-
sists of four main modules: a dialogue system, a
speech client with speech recognition and synthesis
capabilities, a module for infrared communication
between the PC and the RCX, and the programme
that runs on the RCX itself. Each student team had
to specify a dialogue, a speech recognition gram-
mar, and the messages exchanged between PC and
RCX, as well as the RCX control programme. All
other components were off-the-shelf systems that
were combined into a larger system by us.
The centrepiece of the setup is the dialogue
system. We used the DiaWiz system by CLT
Figure 2: The dialogue system.
Sprachtechnologie GmbH2, a proprietary frame-
work for defining finite-state dialogues (McTear,
2002). It has a graphical interface (Fig. 2) that al-
lows the user to draw the dialogue states (shown
as rectangles in the picture) and connect them via
edges. The dialogue system connects to an arbitrary
number of ?clients? via sockets. It can send mes-
sages to and receive messages from clients in each
dialogue state, and thus handles the entire dialogue
management. While it was particularly convenient
for us to use the CLT system, it could probably re-
placed without much effort by a VoiceXML-based
dialogue manager.
The client that interacts most directly with the
user is a module for speech recognition and synthe-
sis. It parses spoken input by means of a recogni-
tion grammar written in the Java Speech Grammar
Format, 3 and sends an extremely shallow semantic
representation of the best recognition result to the
dialogue manager as a feature structure. The out-
put side can be configured to either use a speech
synthesiser, or play back recorded WAV files. Our
implementation assumes only that the recognition
and synthesis engines are compliant with the Java
Speech API 4.
The IR communication module has the task of
converting between high-level messages that the di-
2http://www.clt-st.de
3http://java.sun.com/products/java-
media/speech/forDevelopers/JSGF/
4http://java.sun.com/products/java-media/speech/
Figure 3: A robot playing chess.
alogue manager and the RCX programme exchange
and their low-level representations that are actually
sent over the IR link, in such a way that the user
need not think about the particular low-level details.
The RCX programme itself is again implemented in
Java, using the Lejos system (Bagnall, 2002). Such
a programme is typically small (to fit into the mem-
ory of the microcontroller), and reacts concurrently
to events such as changes in sensor values and mes-
sages received over the infrared link, mostly by con-
trolling the motors and sending messages back to
the PC.
3 Some Robots
3.1 Playing Chess
The first talking robot we present plays chess
against the user (Fig. 3). It moves chess pieces on
a board by means of a magnetic arm, which it can
move up and down in order to grab and release a
piece, and can place the arm under a certain posi-
tion by driving back and forth on wheels, and to the
right and left on a gear rod.
The dialogue between the human player and the
robot is centred around the chess game: The human
speaks the move he wants to make, and the robot
confirms the intended move, and announces check
and checkmate. In order to perform the moves for
the robot, the dialogue manager connects to a spe-
cialised client which encapsulates the GNU Chess
system.5 In addition to computing the moves that
the robot will perform, the chess programme is also
used in disambiguating elliptical player inputs.
Figure 4 shows the part of the chess dialogue
model that accepts a move as a spoken command
from the player. The Input node near the top waits
for the speech recognition client to report that it
5http://www.gnu.org/software/chess/chess.html
Figure 4: A small part of the Chess dialogue.
<cmd> = [<move>] <piece> <to> <squareTo>
| ...
<squareTo> = <colTo> <rowTo>
<colTo> = [a wie] anton {colTo:a} |
[b wie] berta {colTo:b} | ...
<rowTo> = eins {rowTo:1} |
zwei {rowTo:2} | ...
Figure 5: A small part of the Chess grammar.
understood a player utterance as a command. An
excerpt from the recogniser grammar is shown in
Fig. 5: The grammar is a context-free grammar in
JSGF format, whose production rules are annotated
with tags (in curly brackets) representing a very
shallow semantics. The tags for all production rules
used in a parse tree are collected into a table.
The dialogue manager then branches depend-
ing on the type of the command given by the
user. If the command specified the piece and target
square, e.g. ?move the pawn to e4?, the recogniser
will return a representation like {piece="pawn"
colTo="e" rowTo="4"}, and the dialogue will
continue in the centre branch. The user can also
specify the source and target square.
If the player confirms that the move command
was recognised correctly, the manager sends the
move description to the chess client (the ?send
move? input nodes near the bottom), which can dis-
ambiguate the move description if necessary, e.g.
by expanding moves of type ?move the pawn to
e4? to moves of type ?move from e2 to e4?. Note
that the reference ?the pawn? may not be globally
unique, but if there is only one possible referent that
could perform the requested move, the chess client
resolves this automatically.
The client then sends a message to the RCX,
which moves the piece using the robot arm. It up-
dates its internal data structures, as well as the GNU
Chess representations, computes a move for itself,
and sends this move as another message to the RCX.
While the dialogue system as it stands already of-
fers some degree of flexibility with regard to move
phrasings, there is still plenty of open room for im-
provements. One is to use even more context infor-
mation, in order to understand commands like ?take
it with the rook?. Another is to incorporate recent
work on improving recognition results in the chess
domain by certain plausibility inferences (Gabsdil,
2004).
3.2 Playing a Shell Game
Figure 6 introduces Luigi Legonelli. The robot rep-
resents a charismatic Italian shell-game player, and
engages a human player in style: Luigi speaks Ger-
man with a heavy Italian accent, lets the human
player win the first round, and then tries to pull sev-
eral tricks either to cheat or to keep the player inter-
ested in the game.
Figure 6: A robot playing a shell game.
Luigi?s Italian accent was obtained by feeding
transliterated German sentences to a speech synthe-
sizer with an Italian voice. Although the resulting
accent sounded authentic, listeners who were unfa-
miliar with the accent had trouble understanding it.
For demonstration purposes we therefore decided to
use recorded speech instead. To this end, the Italian
student on the team lent his voice for the different
sentences uttered by Luigi.
The core of Luigi?s dialogue model reflects the
progress of game play in a shell game. At the start,
Luigi and the player settle on a bet (between 1 and
10 euros), and Luigi shows under which shell the
coin is. Then, Luigi manipulates the shells (see
also below), moving them (and the coin) around the
board, and finally asks the player under which shell
the player believes the coin is. Upon the player?s
guess Luigi lifts the shell indicated by the player,
and either loudly exclaims the unfairness of life (if
he has lost) or kindly inquires after the player?s
visual capacities (in case the player has guessed
wrong). At the end of the turn, Luigi asks the player
whether he wants to play again. If the player would
like to stop, Luigi tries to persuade the player to
stay; only if the player is persistent, Luigi will end
the game and beat a hasty retreat.
(1) rob ?Ciao, my name is Luigi Legonelli.
Do you feel like a little game??
usr ?Yes ... ?
rob ?The rules are easy. I move da cuppa,
you know, cuppa? You look, say where
coin is. How much money you bet??
usr ?10 Euros.?
rob (Luigi moves the cups/shells)
rob ?So, where is the coin? What do you
think, where?s the coin??
usr ?Cup 1?
rob ?Mamma mia! You have won! Who
told you, where is coin?! Another
game? Another game!?
usr ?No.?
rob ?Come! Play another game!?
usr ?No.?
rob ?Okay, ciao signorina! Police, much
police! Bye bye!?
The shells used in the game are small cups with a
metal top (a nail), which enables Luigi to pick them
up using a ?hand? constructed around a magnet.
The magnet has a downward oriented, U-shaped
construction that enables Luigi to pick up two cups
at the same time. Cups then get moved around
the board by rotating the magnet. By magnetizing
the nail at the top of the cup, not only the cup but
also the coin (touched by the tip of the nail) can be
moved. When asked to show whether the coin is un-
der a particular shell, one of Luigi?s tricks is to keep
the nail magnetized when lifting a cup ? thus also
lifting the coin, giving off the impression that there
was no coin under the shell.
The Italian accent, the android shape of the robot,
and the ?authentic? behavior of Luigi all contributed
to players genuinely getting engaged in the game.
After the first turn, having won, most players ac-
knowledged that this is an amusing Lego construc-
tion; when they were tricked at the end of the sec-
ond turn, they expressed disbelief; and when we
showed them that Luigi had deliberately cheated
them, astonishment. At that point, Luigi had ceased
to be simply an amusing Lego construction and had
achieved its goal as an entertainment robot that can
immerse people into its game.
3.3 Exploring a pyramid
The robot in Figure 7, dubbed ?Indy?, is inspired
by the various robots that have been used to explore
the Great Pyramids in Egypt (e.g. Pyramid Rover6,
UPUAUT7). It has a digital videocamera (webcam)
and a lamp mounted on it, and continually transmits
images from inside the pyramid. The user, watch-
ing the images of the videocamera on a computer
screen, can control the robot?s movements and the
angle of the camera by voice.
Figure 7: A robot exploring a pyramid.
Human-robot interaction is crucial to the explo-
ration task, as neither user nor robot has a com-
plete picture of the environment. The robot is aware
of the environment through an (all-round) array of
touch-sensors, enabling it to detect e.g. openings in
walls; the user receives a more detailed picture, but
6http://www.newscientist.com/news/news.jsp?id=ns99992805
7http://www.cheops.org
only of the environment straight ahead of the robot
(due to the frontal orientation of the camera).
The dialogue model for Indy defines the possible
interaction that enables Indy and the user to jointly
explore the environment. The user can initiate a di-
alogue to control the camera and its orientation (by
letting the robot turn on the spot, in a particular di-
rection), or to instruct the robot to make particular
movements (i.e. turn left or right, stop).
3.4 Traversing a labyrinth
A variation on the theme of human-robot interaction
in navigation is the robot in Figure 8. Here, the user
needs to guide a robot through a labyrinth, specified
by thick black lines on a white background. The
task that the robot and the human must solve col-
laboratively is to pick up objects randomly strewn
about the maze. The robot is able to follow the black
lines lines (the ?path?) by means of an array of three
light sensors at its front.
Figure 8: A robot traversing a labyrinth.
Both the user and the robot can take the initia-
tive in the dialogue. The robot, capable of spotting
crossings (and the possibilities to go straight, left
and/or right), can initiate a dialogue asking for di-
rections if the user had not instructed the robot be-
forehand; see Example 2.
(2) rob (The robot arrives at a crossing; it
recognizes the possibility to go either
straight or left; there are no current in-
structions)
rob ?I can go left or straight ahead; which
way should I go??
usr ?Please go right.?
rob ?I cannot go right here.
usr ?Please go straight.?
rob ?Okay.?
The user can give the robot two different types of
directions: in-situ directions (as illustrated in Ex-
ample 2) or deictic directions (see Example 3 be-
low). This differentiates the labyrinth robot from
the pyramid robot described in ?3.3, as the latter
could only handle in-situ directions.
(3) usr ?Please turn left at the next crossing.?
rob ?Okay?
rob (The robot arrives at a crossing; it
recognizes the possibility to go either
straight or left; it was told to go left at
the next crossing)
rob (The robot recognizes it can go left
and does so, as instructed)
4 Discussion
The first lesson we can learn from the work de-
scribed above is that affordable COTS products in
dialogue and robotics have advanced to the point
that it is feasible to build simple but interesting talk-
ing robots with limited effort. The Lego Mind-
Storms platform, combined with the Lejos system,
turned out to be a flexible and affordable robotics
framework. More ?professional? robots have the
distinct advantage of more interesting sensors and
more powerful on-board computing equipment, and
are generally more physically robust, but Lego
MindStorms is more than suitable for robotics ex-
perimentation under controlled circumstances.
Each of the robots was designed, built, and pro-
grammed within twenty person-weeks, after an ini-
tial work phase in which we created the basic in-
frastructure shown in Figure 1. One prerequisite of
this rather efficient development process was that
the entire software was built on the Java platform,
and was kept highly modular. Speech software ad-
hering to the Java Speech API is becoming avail-
able, and plugging e.g. a different JSAPI-compliant
speech recogniser into our system is now a matter
of changing a line in a configuration file.
However, building talking robots is still a chal-
lenge that combines the particular problems of dia-
logue systems and robotics, both of which introduce
situations of incomplete information. The dialogue
side has to robustly cope with speech recognition er-
rors, and our setup inherits all limitations inherent in
finite-state dialogue; applications having to do e.g.
with information seeking dialogue would be better
served with a more complex dialogue model. On
the other hand, a robot lives in the real world, and
has to deal with imprecisions in measuring its po-
sition, unexpected obstacles, communications with
the PC breaking off, and extremely limited sensory
information about its surroundings.
5 Conclusion
The robots we developed together with our stu-
dents were toy robots, looked like toy robots, and
could (given the limited resources) only deal with
toy examples. However, they confirmed that there
are affordable COTS components on the market
with which we can, even in a limited amount of
time, build engaging talking robots that capture the
essence of various (potential) real-life applications.
The chess and shell game players could be used as
entertainment robots. The labyrinth and pyramid
robots could be extended into tackling real-world
exploration or rescue tasks, in which robots search
for disaster victims in environments that are too
dangerous for rescuers to venture into.8 Dialogue
capabilities are useful in such applications not just
to communicate with the human operator, but also
possibly with disaster victims, to check their condi-
tion.
Moreover, despite the small scale of these robots,
they show genuine issues that could provide in-
teresting lines of research at the interface between
robotics and computational linguistics, and in com-
putational linguistics as such. Each of our robots
could be improved dramatically on the dialogue side
in many ways. As we have demonstrated that the
equipment for building talking robots is affordable
today, we invite all dialogue researchers to join us
in making such improvements, and in investigat-
ing the specific challenges that the combination of
robotics and dialogue bring about. For instance, a
robot moves and acts in the real world (rather than
a carefully controlled computer system), and suffers
from uncertainty about its surroundings. This limits
the ways in which the dialogue designer can use vi-
sual context information to help with reference res-
olution.
Robots, being embodied agents, present a host
of new challenges beyond the challenges we face
in computational linguistics. The interpretation of
language needs to be grounded in a way that is
both based in perception, and on conceptual struc-
tures to allow for generalization over experiences.
Naturally, this problem extends to the acquisition
of language, where approaches such as (Nicolescu
and Mataric?, 2001; Carbonetto and Freitos, 2003;
Oates, 2003) have focused on basing understanding
entirely in sensory data.
Another interesting issue concerns the interpreta-
tion of deictic references. Research in multi-modal
8See also http://www.rescuesystem.org/robocuprescue/
interfaces has addressed the issue of deictic refer-
ence, notably in systems that allow for pen-input
(see (Oviatt, 2001)). Embodied agents raise the
complexity of the issues by offering a broader range
of sensory input that needs to be combined (cross-
modally) in order to establish possible referents.
Acknowledgments. The authors would like to
thank LEGO and CLT Sprachtechnologie for pro-
viding free components from which to build our
robot systems. We are deeply indebted to our stu-
dents, who put tremendous effort into designing and
building the presented robots. Further information
about the student projects (including a movie) is
available at the course website, http://www.coli.uni-
sb.de/cl/courses/lego-02.
References
Brian Bagnall. 2002. Core Lego Mindstorms Pro-
gramming. Prentice Hall, Upper Saddle River
NJ.
Johan Bos, Ewan Klein, and Tetsushi Oka. 2003.
Meaningful conversation with a mobile robot. In
Proceedings of the 10th EACL, Budapest.
W. Burgard, A.B. Cremers, D. Fox, D. Ha?hnel,
G. Lakemeyer, D. Schulz, W. Steiner, and
S. Thrun. 1999. Experiences with an interactive
museum tour-guide robot. Artificial Intelligence,
114(1-2):3?55.
Peter Carbonetto and Nando de Freitos. 2003. Why
can?t Jose? talk? the problem of learning se-
mantic associations in a robot environment. In
Proceedings of the HLT-NAACL 2003 Workshop
on Learning Word Meaning from Non-Linguistic
Data, pages 54?61, Edmonton, Canada.
Terrence W Fong, Illah Nourbakhsh, and Kerstin
Dautenhahn. 2003. A survey of socially interac-
tive robots. Robotics and Autonomous Systems,
42:143?166.
Malte Gabsdil. 2004. Combining acoustic confi-
dences and pragmatic plausibility for classifying
spoken chess move instructions. In Proceedings
of the 5th SIGdial Workshop on Discourse and
Dialogue.
Oleg Gerovich, Randal P. Goldberg, and Ian D.
Donn. 2003. From science projects to the en-
gineering bench. IEEE Robotics & Automation
Magazine, 10(3):9?12.
Takayuki Kanda, Hiroshi Ishiguro, Tetsuo Ono, Mi-
chita Imai, and Ryohei Nakatsu. 2002. Develop-
ment and evaluation of an interactive humanoid
robot ?robovie?. In Proceedings of the IEEE In-
ternational Conference on Robotics and Automa-
tion (ICRA 2002), pages 1848?1855.
Kurt Konolige, Karen Myers, Enrique Ruspini,
and Alessandro Saffiotti. 1993. Flakey in ac-
tion: The 1992 aaai robot competition. Techni-
cal Report 528, AI Center, SRI International, 333
Ravenswood Ave., Menlo Park, CA 94025, Apr.
Oliver Lemon, Anne Bracy, Alexander Gruenstein,
and Stanley Peters. 2001. A multi-modal dia-
logue system for human-robot conversation. In
Proceedings NAACL 2001.
Henrik Hautop Lund. 1999. AI in children?s play
with LEGO robots. In Proceedings of AAAI 1999
Spring Symposium Series, Menlo Park. AAAI
Press.
Michael McTear. 2002. Spoken dialogue technol-
ogy: enabling the conversational user interface.
ACM Computing Surveys, 34(1):90?169.
Monica N. Nicolescu and Maja J. Mataric?. 2001.
Learning and interacting in human-robot do-
mains. IEEE Transactions on Systems, Man and
Cybernetics, 31.
Tim Oates. 2003. Grounding word meanings
in sensor data: Dealing with referential un-
certainty. In Proceedings of the HLT-NAACL
2003 Workshop on Learning Word Meaning from
Non-Linguistic Data, pages 62?69, Edmonton,
Canada.
Sharon L. Oviatt. 2001. Advances in the robust
processing of multimodal speech and pen sys-
tems. In P. C. Yuen, Y.Y. Tang, and P.S. Wang,
editors, Multimodal InterfacesB for Human Ma-
chine Communication, Series on Machine Per-
ception and Artificial Intelligence, pages 203?
218. World Scientific Publisher, London, United
Kingdom.
Candace L. Sidner, Christopher Lee, and Neal Lesh.
2003. Engagement by looking: Behaviors for
robots when collaborating with people. In Pro-
ceedings of the 7th workshop on the semantics
and pragmatics of dialogue (DIABRUCK).
R. Siegwart and et al 2003. Robox at expo.02:
A large scale installation of personal robots.
Robotics and Autonomous Systems, 42:203?222.
S. Thrun, M. Beetz, M. Bennewitz, W. Burgard,
A.B. Cremers, F. Dellaert, D. Fox, D. Ha?hnel,
C. Rosenberg, N. Roy, J. Schulte, and D. Schulz.
2000. Probabilistic algorithms and the interactive
museum tour-guide robot minerva. International
Journal of Robotics Research, 19(11):972?999.
Xudong Yu. 2003. Robotics in education: New
platforms and environments. IEEE Robotics &
Automation Magazine, 10(3):3.
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 507 ? 518, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Exploring Syntactic Relation Patterns  
for Question Answering 
Dan Shen1,2, Geert-Jan M. Kruijff 1, and Dietrich Klakow2 
1
 Department of Computational Linguistics, Saarland University, 
Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany 
{dshen, gj}@coli.uni-sb.de 
2
 Lehrstuhl Sprach Signal Verarbeitung,Saarland University, 
Building 17, Postfach 15 11 50, 66041 Saarbruecken, Germany 
{dietrich.klakow}@lsv.uni-saarland.de 
Abstract. In this paper, we explore the syntactic relation patterns for open-
domain factoid question answering.  We propose a pattern extraction method to 
extract the various relations between the proper answers and different types of 
question words, including target words, head words, subject words and verbs, 
from syntactic trees.  We further propose a QA-specific tree kernel to partially 
match the syntactic relation patterns.  It makes the more tolerant matching be-
tween two patterns and helps to solve the data sparseness problem.  Lastly, we 
incorporate the patterns into a Maximum Entropy Model to rank the answer 
candidates.  The experiment on TREC questions shows that the syntactic rela-
tion patterns help to improve the performance by 6.91 MRR based on the com-
mon features. 
1   Introduction 
Question answering is to find answers for open-domain natural language questions in 
a large document collection.  A typical QA system usually consists of three basic 
modules: 1. Question Processing (QP) Module, which finds some useful information 
from questions, such as expected answer type and key words; 2. Information Retrieval 
(IR) Module, which searches a document collection to retrieve a set of relevant sen-
tences using the key words; 3. Answer Extraction (AE) Module, which analyzes the 
relevant sentences using the information provided by the QP module and identify the 
proper answer.  In this paper, we will focus on the AE module. 
In order to find the answers, some evidences, such as expected answer types and 
surface text patterns, are extracted from answer sentences and incorporated in the AE 
module using a pipelined structure, a scoring function or some statistical-based meth-
ods.  However, the evidences extracted from plain texts are not sufficient to identify a 
proper answer.  For examples, for ?Q1910: What are pennies made of??, the expected 
answer type is unknown; for ?Q21: Who was the first American in space??, the sur-
face patterns may not detect the long-distance relations between the question key 
phrase ?the first American in space? and the answer ?Alan Shepard? in ?? that car-
ried Alan Shepard on a 15 - minute suborbital flight in 1961 , making him the first 
508 D. Shen, G.-J.M. Kruijff, and D. Klakow 
American in space.?  To solve these problems, more evidences need to be extracted 
from the more complex data representations, such as parse trees. 
In this paper, we explore the syntactic relation patterns (SRP) for the AE module.  
An SRP is defined as a kind of relation between a question word and an answer can-
didate in the syntactic tree.  Different from the textual patterns, the SRPs capture the 
relations based on the sentence syntactic structure rather than the sentence surface.  
Therefore, they may get the deeper understanding of the relations and capture the long 
range dependency between words regardless of their ordering and distance in the 
surface text.  Based on the observation of the task, we find that the syntactic relations 
between different types of question words and answers vary a lot with each other.  We 
classify the question words into four classes, including target words, head words, 
subject phrases and verbs, and generate the SRPs for them respectively.  Firstly, we 
generate the SRPs from the training data and score them based on the support and 
confidence measures.  Next, we propose a QA-specific tree kernel to calculate the 
similarity between two SRPs in order to match the patterns from the unseen data into 
the pattern set.  The tree kernel makes the partial matching between two patterns and 
helps to solve the data sparseness problem.  Lastly, we incorporate the SRPs into a 
Maximum Entropy Model along with some common features to classify the answer 
candidates.  The experiment on TREC questions shows that the syntactic relation 
patterns improve the performance by 6.91 MRR based on the common features. 
Although several syntactic relations, such as subject-verb and verb-object, have 
been also considered in some other systems, they are basically extracted using a small 
number of hand-built rules.  As a result, they are limited and costly.  In our task, we 
automatically extract the various relations between different question words and an-
swers and more tolerantly match the relation patterns using the tree kernel. 
2   Related Work 
The relations between answers and question words have been explored by many suc-
cessful QA systems based on certain sentence representations, such as word sequence, 
logic form, parse tree, etc. 
In the simplest case, a sentence is represented as a sequence of words.  It is as-
sumed that, for certain type of questions, the proper answers always have certain 
surface relations with the question words.  For example, ?Q: When was X born??, the 
proper answers often have such relation ?<X> ( <Answer>--? with the question 
phrase X .  [14] first used a predefined pattern set in QA and achieved a good per-
formance at TREC10.  [13] further developed a bootstrapping method to learn the 
surface patterns automatically.  When testing, most of them make the partial matching 
using regular expression.  However, such surface patterns strongly depend on the 
word ordering and distance in the text and are too specific to the question type. 
LCC [9] explored the syntactic relations, such as subject, object, prepositional at-
tachment and adjectival/adverbial adjuncts, based on the logic form transformation.  
Furthermore they used a logic prover to justify the answer candidates.  The prover is 
accurate but costly. 
Most of the QA systems explored the syntactic relations on the parse tree.  Since 
such relations do not depend on the word ordering and distance in the sentence, they 
may cope with the various surface expressions of the sentence.  ISI [7] extracted the 
 Exploring Syntactic Relation Patterns for Question Answering 509 
relations, such as ?subject-verb? and ?verb-object?, in the answer sentence tree and 
compared with those in the question tree.  IBM?s Maximum Entropy-based model 
[10] integrated a rich feature set, including words co-occurrence scores, named entity, 
dependency relations, etc.  For the dependency relations, they considered some prede-
fined relations in trees by partial matching.  BBN [15] also considered the verb-
argument relations. 
However, most of the current QA systems only focus on certain relation types, 
such as verb-argument relations, and extract them from the syntactic tree using some 
heuristic rules.  Therefore, extracting such relations is limited in a very local context 
of the answer node, such as its parent or sibling nodes, and does not involve long 
range dependencies.  Furthermore, most of the current systems only concern the rela-
tions to certain type of question words, such as verb.  In fact, different types of ques-
tion words may have different indicative relations with the proper answers.  In this 
paper, we will automatically extract more comprehensive syntactic relation patterns 
for all types of question words, partially match them using a QA-specific tree kernel 
and evaluate their contributions by integrating them into a Maximum Entropy Model. 
3   Syntactic Relation Pattern Generating 
In this section, we will discuss how to extract the syntactic relation patterns.  Firstly, 
we briefly introduce the question processing module which provides some necessary 
information to the answer extraction module.  Secondly, we generate the dependency 
tree of the answer sentence and map the question words into the tree using a Modified 
Edit Distance (MED) algorithm.  Thirdly, we define and extract the syntactic relation 
patterns in the mapped dependency tree.  Lastly, we score and filter the patterns. 
3.1   Question Processing Module 
The key words are extracted from the questions.  Considering that different key words 
may have different syntactic relations with the answers, we divide the key words into 
the following four types: 
1. Target Words, which are extracted from what / which questions.  Such words indi-
cate the expected answer types, such as ?party? in ?Q1967: What party led ???. 
2. Head Words, which are extracted from how questions.  Such words indicate the 
expected answer heads, such as ?dog? in the ?Q210: How many dogs pull ??? 
3. Subject Phrases, which are extracted from all types of questions.  They are the base 
noun phrases of the questions except the target words and the head words. 
4. Verbs, which are the main verbs extracted from non-definition questions. 
The key words described above are identified and classified based on the question 
parse tree.  We employ the Collins Parser [2] to parse the questions and the answer 
sentences. 
3.2   Question Key Words Mapping 
From this section, we start to introduce the AE module.  Firstly, the answer sentences 
are tagged with named entities and parsed.  Secondly, the parse trees are transformed 
510 D. Shen, G.-J.M. Kruijff, and D. Klakow 
to the dependency trees based on a set of rules.  To simplify a dependency tree, some 
special rules are used to remove the non-useful nodes and dependency information.  
The rules include 
1. Since the question key words are always NPs and verbs, only the syntactic rela-
tions between NP and NP / NP and verb are considered. 
2. The original form of Base Noun Phrase (BNP) is kept and the dependency relations 
within the BNPs are not considered, such as adjective-noun.  A base noun phrase is 
defined as the smallest noun phrase in which there are no noun phrases embedded. 
An example of the dependency tree is shown in Figure 1.  We regard all BNP 
nodes and leaf nodes as answer candidates. 
 
Fig. 1. Dependency tree and Tagged dependency tree 
Next, we map the question key words into the simplified dependency trees.  We 
propose a weighted edit distance (WED) algorithm, which is to find the similarity 
between two phrases by computing the minimal cost of operations needed to transform 
one phrase into the other, where an operation is an insertion, deletion, or substitution. 
Different from the commonly-used edit distance algorithm [11], the WED defines 
the more flexible cost function which incorporates the morphological and semantic 
alternations of the words.  The morphological alternations indicate the inflections of 
noun/verb.  For example, for Q2149: How many Olympic gold medals did Carl Lewis 
win?   We map the verb win to the nominal winner in the answer sentence ?Carl 
Lewis, winner of nine Olympic gold medals, thinks that ??.  The morphological alter-
nations are found based on a stemming algorithm and the ?derivationally related 
forms? in WordNet [8].  The semantic alternations consider the synonyms of the 
words.  Some types of the semantic relations in WordNet enable the retrieval of syno-
nyms, such as hypernym, hyponym, etc.  For example, for Q212: Who invented the 
electric guitar?  We may map the verb invent to its direct hypernym create in answer 
sentences.  Based on the observation of the task, we set the substitution costs of the 
alternations as follows: Identical words have cost 0; Words with the same morpho-
logical root have cost 0.2; Words with the hypernym or hyponym relations have cost 
tagged dependency tree dependency tree 
live 
BNP 
NER_PER 
Ellington 
BNP 
NER_LOC 
BNP 
Washington his early NNP 20s
NER_DAT 
VER
VER: the verb of the question 
SUB: the subject words of the question 
TGT_HYP: the hypernym of the target word of the question  
live 
BNP 
NER_PER 
SUB 
Ellington 
BNP 
NER_LOC 
TGT_HYP 
BNP 
Washington his early NNP 20s 
NER_DAT 
Q1916: What city did Duke Ellington live in?  
A: Ellington lived in Washington until his early 20s. 
 Exploring Syntactic Relation Patterns for Question Answering 511 
0.4; Words in the same SynSet have cost 0.6; Words with subsequence relations have 
cost 0.8; otherwise, words have cost 1.  Figure 1 also shows an example of the tagged 
dependency tree. 
3.3   Syntactic Relation Pattern Extraction 
A syntactic relation pattern is defined as the smallest subtree which covers an answer 
candidate node and one question key word node in the dependency tree.  To capture 
different relations between answer candidates and different types of question words, 
we generate four pattern sets, called PSet_target, PSet_head, PSet_subject and 
PSet_verb, for the answer candidates.  The patterns are extracted from the training 
data.  Some pattern examples are shown in Table 1.  For a question Q, there are a set 
of relevant sentences SentSet.  The extraction process is as follows: 
1. for each question Q in the training data 
2. question processing model extract the key words of Q 
3. for each sentence s in SentSet 
a) parse s  
b) map the question key words into the parse tree 
c) tag all BNP nodes in the parse tree as answer candidates. 
d) for each answer candidate (ac) node 
 for each question word (qw) node 
      extract the syntactic relation pattern (srp) for ac and qw  
 add srp to PSet_target, PSet_head, PSet_subject or 
PSet_verb based on the types of qw. 
Table 1. Examples of the patterns in the four pattern sets 
PatternSet  Patterns Sup. Conf. 
(NPB~AC~TGT) 0.55 0.22 
(NPB~AC~null (NPB~null~TGT)) 0.08 0.06 PSet_target 
(NPB~null~null (NPB~AC~null) (NPB~null~TGT)) 0.02 0.09 
PSet_head (NPB~null~null (CD~AC~null) (NPB~null~HEAD)) 0.59 0.67 
(VP~null~null (NPB~null~SUB) (NPB~null~null 
(NPB~AC~null))) 
0.04 0.33 
PSet_subject 
(NPB~null~null (NPB~null~SUB) (NPB~AC~null)) 0.02 0.18 
PSet_verb (VP~null~VERB (NPB~AC~null)) 0.18 0.16 
3.4   Syntactic Relation Pattern Scoring 
The patterns extracted in section 3.3 are scored by support and confidence measures.  
Support and confidence measures are most commonly used to evaluate the association 
rules in the data mining area.  The support of a rule is the proportion of times the rule 
applies.  The confidence of a rule is the proportion of times the rule is correct.  In our 
task, we score a pattern by measuring the strength of the association rule from the 
pattern to the proper answer (the pattern is matched => the answer is correct). Let pi 
be any pattern in the pattern set PSet , 
512 D. Shen, G.-J.M. Kruijff, and D. Klakow 
the number of  in which  is correct
support( )
the size of 
p acipi PSet
=  
the number of  in which  is correct
confidence( )
the number of  
p acipi pi
=  
We score the patterns in the PSet_target, PSet_head, PSet_subject and PSet_verb 
respectively.  If the support value is less than the threshold supt or the confidence 
value is less than the threshold conft , the pattern is removed from the set.  In the ex-
periment, we set supt 0.01 and conft  0.5.  Table 1 lists the support and confidence of 
the patterns. 
4   Syntactic Relation Pattern Matching 
Since we build the pattern sets based on the training data in the current experiment, 
the pattern sets may not be large enough to cover all of the unseen cases.  If we make 
the exact match between two patterns, we will suffer from the data sparseness prob-
lem.  So a partial matching method is required.  In this section, we will propose a QA-
specific tree kernel to match the patterns. 
A kernel function 1 2( , ) : [0, ]K x x ? ?X X R , is a similarity measure between 
two objects 1x and 2x with some constraints.  It is the most important component of 
kernel methods [16].  Tree kernels are the structure-driven kernels used to calculate 
the similarity between two trees.  They have been successfully accepted in the natural 
language processing applications, such as parsing [4], part of speech tagging and 
named entity extraction [3], and information extraction [5, 17].  To our knowledge, 
tree kernels have not been explored in answer extraction. 
Suppose that a pattern is defined as a tree T with nodes 0 1{ , , ..., }nt t t  and each node 
it is attached with a set of attributes 0 1{ , , ..., }ma a a , which represent the local charac-
teristics of ti .  In our task, the set of the attributes include Type attributes, Ortho-
graphic attributes and Relation Role attributes, as shown in Table 2.  Figure 2 shows 
an example of the pattern tree T_ac#target. 
The core idea of the tree kernel ( , )1 2K T T  is that the similarity between two trees 
T1 and T2 is the sum of the similarity between their subtrees.  It can be calculated by 
dynamic programming and can capture the long-range relations between two nodes.  
The kernel we use is similar to [17] except that we define a task-specific matching 
function and similarity function, which are two primitive functions to calculate the 
similarity between two nodes in terms of their attributes.  
Matching function 
1 if . .  and . .   
( , )
0 otherwise                                           
i j i j
i j
t type t type t role t role
m t t
= =
=
???  
 Exploring Syntactic Relation Patterns for Question Answering 513 
Similarity function 
0{ ,..., }
( , ) ( . , . )i j i j
ma a a
s t t f t a t a
?
= ?  
where, ( . , . )i jf t a t a  is a compatibility function between two feature values 
. .
( . , . )
1   if 
0   otherwise
i j
i j
t a t a
f t a t a =
=???  
Table 2. Attributes of the nodes 
Attributes Examples 
POS tag CD, NNP, NN? Type 
syntactic tag NP, VP, ? 
Is Digit? DIG, DIGALL 
Is Capitalized? CAP, CAPALL 
Orthographic  
length of phrase LNG1, LNG2#3, LNGgt3 
Role1 Is answer candidate? true, false 
Role2 Is question key words? true, false 
 
Fig. 2. An example of the pattern tree T_ac#target 
5   ME-Based Answer Extraction 
In addition to the syntactic relation patterns, many other evidences, such as named 
entity tags, may help to detect the proper answers.  Therefore, we use maximum en-
tropy to integrate the syntactic relation patterns and the common features. 
5.1   Maximum Entropy Model 
[1] gave a good description of the core idea of maximum entropy model.  In our task, 
we use the maximum entropy model to rank the answer candidates for a question, 
T_ac#target 
Q1897: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort Worth Inter-
national Airport was 81 degrees. 
t4 t3 t2 
T: BNP 
O: null  
R1: true 
R2: false 
t1 
Dallas-Fort 
T: NNP 
O: CAPALL  
R1: false 
R2: false 
International 
T: JJ 
O: CAPALL  
R1: false 
R2: false 
Airport 
T: NNP 
O: CAPALL  
R1: false 
R2: true 
t0 
Worth 
T: NNP 
O: CAPALL 
R1: false 
R2: false 
 
514 D. Shen, G.-J.M. Kruijff, and D. Klakow 
which is similar to [12].  Given a question q and a set of possible answer candi-
dates 1 2{ , ... }nac ac ac , the model outputs the answer 1 2{ , ... }nac ac ac ac? with the 
maximal probability from the answer candidate set.  We define M feature func-
tions 1 2( ,{ , ... }, ),  m=1,...,Mm nf ac ac ac ac q .  The probability is modeled as  
1 2
1
1 2
1 2
' 1
exp[ ( ,{ , ... }, ))]
( | { , ... }, )
exp[ ( ',{ , ... }, )]
M
m m n
m
n M
m m n
ac m
f ac ac ac ac q
P ac ac ac ac q
f ac ac ac ac q
?
?
=
=
?
=
? ?
 
where, (m=1,...,M)
m
? are the model parameters, which are trained with General-
ized Iterative Scaling [6].  A Gaussian Prior is used to smooth the ME model. 
Table 3. Examples of the common features 
Features Examples Explanation 
NE#DAT_QT_DAT ac is NE (DATE) and qtarget is DATE NE  
NE#PER_QW_WHO ac is NE (PERSON) and qword is WHO 
SSEQ_Q ac is a subsequence of question 
CAP_QT_LOC ac is capitalized and qtarget is LOCATION 
Ortho-
graphic  
LNGlt3_QT_PER the length of ac ? 3 and qtarget is PERSON 
CD_QT_NUM syn. tag of ac is CD and qtarget is NUM Syntactic 
Tag  NNP_QT_PER syn. tag of ac is NNP and qtarget is PERSON 
Triggers TRG_HOW_DIST ac matches the trigger words for HOW questions which 
ask for distance  
5.2   Features 
For the baseline maximum entropy model, we use four types of common features: 
1. Named Entity Features: For certain question target, if the answer candidate is 
tagged as certain type of named entity, one feature fires. 
2. Orthographic Features: They capture the surface format of the answer candi-
dates, such as capitalizations, digits and lengths, etc.  
3. Syntactic Tag Features: For certain question target, if the word in the answer 
candidate belongs to a certain syntactic / POS type, one feature fires. 
4. Triggers: For some how questions, there are always some trigger words which are 
indicative for the answers.  For example, for ?Q2156: How fast does Randy John-
son throw??, the word ?mph? may help to identify the answer ?98-mph? in ?John-
son throws a 98-mph fastball.? 
Table 3 shows some examples of the common features.  All of the features are the 
binary features.  In addition, many other features, such as the answer candidate fre-
quency, can be extracted based on the IR output and are thought as the indicative 
evidences for the answer extraction [10].  However, in this paper, we are to evaluate 
the answer extraction module independently, so we do not incorporate such features 
in the current model. 
 Exploring Syntactic Relation Patterns for Question Answering 515 
In order to evaluate the effectiveness of the automatically generated syntactic rela-
tion patterns, we also manually build some heuristic rules to extract the relation fea-
tures from the trees and incorporate them into the baseline model.  The baseline 
model uses 20 rules.  Some examples of the hand-extracted relation features are 
listed as follows, 
z If the ac node is the same of the qtarget node, one feature fires. 
z If the ac node is the sibling of the qtarget node, one feature fires. 
z If the ac node is the child of the qsubject node, one feature fires. 
z ? 
Next, we will discuss the use of the syntactic relation features.  Firstly, for each 
answer candidate, we extract the syntactic relations between it and all mapped ques-
tion key words in the sentence tree.  Then for each extracted relation, we match it in 
the pattern set PSet_target, PSet_head, PSet_subject or PSet_verb.  A tree kernel 
discussed in Section 4 is used to calculate the similarity between two patterns.  Fi-
nally, if the maximal similarity is above a threshold ? , the pattern with the maximal 
similarity is chosen and the corresponding feature fires.  The experiments will evalu-
ate the performance and the coverage of the pattern sets based on different ?  values. 
6   Experiment 
We apply the AE module to the TREC QA task.  Since this paper focuses on the AE 
module alone, we only present those sentences containing the proper answers to the 
AE module based on the assumption that the IR module has got 100% precision.  The 
AE module is to identify the proper answers from the given sentence collection. 
We use the questions of TREC8, 9, 2001 and 2002 for training and the questions of 
TREC2003 for testing.  The following steps are used to generate the data: 
1. Retrieve the relevant documents for each question based on the TREC judgments. 
2. Extract the sentences, which match both the proper answer and at least one ques-
tion key word, from these documents.   
3. Tag the proper answer in the sentences based on the TREC answer patterns. 
In TREC 2003, there are 413 factoid questions in which 51 questions (NIL ques-
tions) are not returned with the proper answers by TREC.  According to our data 
generation process, we cannot provide data for those NIL questions because we can-
not get the sentence collections.  Therefore, the AE module will fail on all of the NIL 
questions and the number of the valid questions should be 362 (413 ? 51).  In the 
experiment, we still test the module on the whole question set (413 questions) to keep 
consistent with the other?s work.  The training set contains 1252 questions.  The per-
formance of our system is evaluated using the mean reciprocal rank (MRR).  Fur-
thermore, we also list the percentages of the correct answers respectively in terms of 
the top 5 answers and the top 1 answer returned.  No post-processes are used to adjust 
the answers in the experiments. 
In order to evaluate the effectiveness of the syntactic relation patterns in the answer 
extraction, we compare the modules based on different feature sets.  The first ME 
module ME1 uses the common features including NE features, Orthographic features, 
516 D. Shen, G.-J.M. Kruijff, and D. Klakow 
Syntactic Tag features and Triggers.  The second ME module ME2 uses the common 
features and some hand-extracted relation features, described in Section 5.2.  The 
third module ME3 uses the common features and the syntactic relation patterns which 
are automatically extracted and partial matched with the methods proposed in Section 
3 and 4.  Table 4 shows the overall performance of the modules.  Both ME2 and ME3 
outperform ME1 by 3.15 MRR and 6.91 MRR respectively.  This may indicate that 
the syntactic relations between the question words and the answers are useful for the 
answer extraction.  Furthermore, ME3 got the higher performance (+3.76 MRR) than 
ME2.  The probable reason may be that the relations extracted by some heuristic rules 
in ME2 are limited in the very local contexts of the nodes and they may not be suffi-
cient.  On the contrary, the pattern extraction methods we proposed can explore the 
larger relation space in the dependency trees. 
Table 4. Overall performance 
 ME1 ME2 ME3 
Top1  44.06 47.70 51.81 
Top5 53.27 55.45 58.85 
MRR 47.75 50.90 54.66 
Table 5. Performances for two pattern matching methods 
PartialMatch  ExactMatch 
( ? =1) ? =0.8 ? =0.6 ? =0.4 ? =0.2 ? =0 
Top1 50.12 51.33 51.81 51.57 50.12 50.12 
Top5 57.87 58.37 58.85 58.60 57.16 57.16 
MRR 53.18 54.18 54.66 54.41 52.97 52.97 
Furthermore, we evaluate the effectiveness of the pattern matching method in Sec-
tion 4.  We compare two pattern matching methods: the exact matching (ExactMatch) 
and the partial matching (PartialMatch) using the tree kernel.  Table 5 shows the 
performances for the two pattern matching methods.  For PartialMatch, we also 
evaluate the effect of the parameter ?  (described in Section 5.2) on the performance.  
In Table 5, the best PartialMatch ( ?  = 0.6) outperforms ExactMatch by 1.48 MRR.  
Since the pattern sets extracted from the training data is not large enough to cover the 
unseen cases, ExactMatch may have too low coverage and suffer with the data sparse-
ness problem when testing, especially for PSet_subject (24.32% coverage using Ex-
actMatch vs. 49.94% coverage using PartialMatch).  In addition, even the model with 
ExactMatch is better than ME2 (common features + hand-extracted relations) by 2.28 
MRR.  It indicates that the relation patterns explored with the method proposed in 
Section 3 are more effective than the relations extracted by the heuristic rules. 
Table 6 shows the size of the pattern sets PSet_target, PSet_head, PSet_subject 
and PSet_verb and their coverage for the test data based on different ?  values.  
PSet_verb gets the low coverage (<5% coverage).  The probable reason is that the 
verbs in the answer sentences are often different from those in the questions, therefore 
only a few question verbs can be matched in the answer sentences.  PSet_head also 
gets the relatively low coverage since the head words are only exacted from how 
questions and there are only 49/413 how questions with head words in the test data.  
 Exploring Syntactic Relation Patterns for Question Answering 517 
Table 6. Size and coverage of the pattern sets 
coverage (*%)  size 
? =1 ? =0.8 ? =0.6 ? =0.4 ? =0.2 ? =0 
PSet_target 45 49.85 53.73 57.01 58.14 58.46 58.46 
PSet_head 42 5.82 6.48 6.69 6.80 6.80 6.80 
PSet_subject 123 24.32 44.82 49.94 51.29 51.84 51.84 
PSet_verb 125 2.21 3.49 3.58 3.58 3.58 3.58 
We further evaluate the contributions of different types of patterns.  We respec-
tively combine the pattern features in different pattern set and the common features.  
Some findings can be concluded from Table 7: All of the patterns have the positive 
effects based on the common features, which indicates that all of the four types of the 
relations are helpful for answer extraction.  Furthermore, P_target (+4.21 MRR) and 
P_subject (+2.47 MRR) are more beneficial than P_head (+1.25 MRR) and P_verb 
(+0.19 MRR).  This may be explained that the target and subject patterns may have 
the effect on the more test data than the head and verb patterns since PSet_target and 
PSet_subject have the higher coverage for the test data than PSet_head and 
PSet_verb, as shown in Table 6.  
Table 7. Performance on feature combination 
Combination of features MRR 
common features 47.75 
common features + P_target 51.96 
common features + P_head 49.00 
common features + P_subject 50.22 
common features + P_verb 47.94 
7   Conclusion 
In this paper, we study the syntactic relation patterns for question answering.  We 
extract the various syntactic relations between the answers and different types of 
question words, including target words, head words, subject words and verbs and 
score the extracted relations based on support and confidence measures.  We further 
propose a QA-specific tree kernel to partially match the relation patterns from the 
unseen data to the pattern sets.  Lastly, we incorporate the patterns and some com-
mon features into a Maximum Entropy Model to rank the answer candidates.  The 
experiment shows that the syntactic relation patterns improve the performance by 
6.91 MRR based on the common features. Moreover, the contributions of the pat-
tern matching methods are evaluated.  The results show that the tree kernel-based 
partial matching outperforms the exact matching by 1.48 MRR.  In the future, we 
are to further explore the syntactic relations using the web data rather than the  
training data. 
518 D. Shen, G.-J.M. Kruijff, and D. Klakow 
References 
1. Berger, A., Della Pietra, S., Della Pietra, V.: A maximum entropy approach to natural lan-
guage processing. Computational Linguistics (1996), vol. 22, no. 1, pp. 39-71 
2. Collins, M.: A New Statistical Parser Based on Bigram Lexical Dependencies. In: Pro-
ceedings of ACL-96 (1996) 184-191 
3. Collins, M.: New Ranking Algorithms for Parsing and Tagging: Kernel over Discrete 
Structures, and the Voted Perceptron. In: Proceeings of ACL-2002 (2002). 
4. Collins, M., Duffy, N.: Convolution Kernels for Natural Language. Advances in Neural 
Information Processing Systems 14, Cambridge, MA.  MIT Press (2002) 
5. Culotta, A., Sorensen, J.: Dependency Tree Kernels for Relation Extraction. In: Proceed-
ings of ACL-2004 (2004) 
6. Darroch, J., Ratcliff, D.: Generalized iterative scaling for log-linear models. The annuals 
of Mathematical Statistics (1972), vol. 43, pp. 1470-1480 
7. Echihabi, A., Hermjakob, U., Hovy, E., Marcu, D., Melz, E., Ravichandran, D.: Multiple-
Engine Question Answering in TextMap. In: Proceedings of the TREC-2003 Conference, 
NIST (2003) 
8. Fellbaum, C.: WordNet - An Electronic Lexical Database. MIT Press, Cambridge, MA 
(1998) 
9. Harabagiu, S., Moldovan, D., Clark, C., Bowden, M., Williams, J., Bensley, J.: Answer 
Mining by Combining Extraction Techniques with Abductive Reasoning. In: Proceedings 
of the TREC-2003 Conference, NIST (2003) 
10. Ittycheriah, A., Roukos, S.: IBM's Statistical Question Answering System - TREC 11. In: 
Proceedings of the TREC-2002 Conference, NIST (2002) 
11. Levenshtein, V. I.: Binary Codes Capable of Correcting Deletions, Insertions and Rever-
sals. Doklady Akademii Nauk SSSR 163(4) (1965) 845-848 
12. Ravichandran, D., Hovy, E., Och, F. J.: Statistical QA - Classifier vs. Re-ranker: What's 
the difference? In: Proceedings of Workshop on Multilingual Summarization and Question 
Answering, ACL (2003) 
13. Ravichandran, D., Hovy, E.: Learning Surface Text Patterns for a Question Answering 
System. In: Proceedings of ACL-2002 (2002) 41-47 
14. Soubbotin, M. M., Soubbotin, S. M.: Patterns of Potential Answer Expressions as Clues to 
the Right Answer. In: Proceedings of the TREC-10 Conference, NIST (2001) 
15. Xu, J., Licuanan, A., May, J., Miller, S., Weischedel, R.: TREC 2002 QA at BBN: Answer 
Selection and Confidence Estimation. In: Proceedings of the TREC-2002 Conference, 
NIST (2002) 
16. Vapnik, V.: Statistical Learning Theory, John Wiley, NY, (1998) 732. 
17. Zelenko, D., Aone, C., Richardella, A.: Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research (2003) 1083-1106. 
Coupling CCG and Hybrid Logic Dependency Semantics
Jason Baldridge
ICCS
Division of Informatics
2 Buccleuch Place
University of Edinburgh
Edinburgh, UK, EH8 9LW
jmb@cogsci.ed.ac.uk
Geert-Jan M. Kruijff
Universita?t des Saarlandes
Computational Linguistics
Lehrstuhl Uszkoreit
Building 17, Postfach 15 11 50
66041 Saarbru?cken, Germany
gj@CoLi.Uni-SB.DE
Abstract
Categorial grammar has traditionally used
the ?-calculus to represent meaning. We
present an alternative, dependency-based
perspective on linguistic meaning and sit-
uate it in the computational setting. This
perspective is formalized in terms of hy-
brid logic and has a rich yet perspicuous
propositional ontology that enables a wide
variety of semantic phenomena to be rep-
resented in a single meaning formalism.
Finally, we show how we can couple this
formalization to Combinatory Categorial
Grammar to produce interpretations com-
positionally.
1 Introduction
The ?-calculus has enjoyed many years as the stan-
dard semantic encoding for categorial grammars and
other grammatical frameworks, but recent work has
highlighted its inadequacies for both linguistic and
computational concerns of representing natural lan-
guage semantics (Copestake et al, 1999; Kruijff,
2001). The latter couples a resource-sensitive cate-
gorial proof theory (Moortgat, 1997) to hybrid logic
(Blackburn, 2000) to formalize a dependency-based
perspective on meaning, which we call here Hybrid
Logic Dependency Semantics (HLDS). In this pa-
per, we situate HLDS in the computational context
by explicating its properties as a framework for com-
putational semantics and linking it to Combinatory
Categorial Grammar (CCG).
The structure of the paper is as follows. In x2,
we briefly introduce CCG and how it links syntax
and semantics, and then discuss semantic represen-
tations that use indexes to identify subparts of logi-
cal forms. x3 introduces HLDS and evaluates it with
respect to the criteria of other computational seman-
tics frameworks. x4 shows how we can build HLDS
terms using CCG with unification and x5 shows how
intonation and information structure can be incorpo-
rated into the approach.
2 Indexed semantic representations
Traditionally, categorial grammar has captured
meaning using a (simply typed) ?-calculus, build-
ing semantic structure in parallel to the categorial in-
ference (Morrill, 1994; Moortgat, 1997; Steedman,
2000b). For example, a (simplified) CCG lexical en-
try for a verb such as wrote is given in (1).
(1) wrote ` (snn)=n : ?x:?y:write(y;x)
Rules of combination are defined to operate on both
categories and ?-terms simultaneously. For exam-
ple, the rules allow the following derivation for Ed
wrote books.
(2) Ed wrote books
n:Ed (snn)=n:?x:?y:write(y;x) n:books
>
snn : ?y:write(y;books)
<
s : write(Ed;books)
Derivations like (2) give rise to the usual sort
of predicate-argument structure whereby the order
in which the arguments appear (and are bound by
the ??s) is essentially constitutive of their meaning.
Thus, the first argument could be taken to corre-
spond to the writer, whereas the second argument
corresponds to what is being written.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 319-326.
                         Proceedings of the 40th Annual Meeting of the Association for
One deficiency of ?-calculus meaning representa-
tions is that they usually have to be type-raised to
the worst case to fully model quantification, and this
can reverberate and increase the complexity of syn-
tactic categories since a verb like wrote will need to
be able to take arguments with the types of general-
ized quantifiers. The approach we advocate in this
paper does not suffer from this problem.
For CCG, the use of the ?-terms is simply a con-
venient device to bind arguments when presenting
derivations (Steedman, 2000b). In implementations,
a more common strategy is to compute semantic rep-
resentations via unification, a tactic explicitly em-
ployed in Unification Categorial Grammar (UCG)
(Zeevat, 1988). Using a unification paradigm in
which atomic categories are bundles of syntactic and
semantic information, we can use an entry such as
(3) for wrote in place of (1). In the unification set-
ting, (3) permits a derivation analogous to (2).
(3) wrote ` (s : write(y;x)nn:y)=n:x
For creating predicate-argument structures of this
kind, strategies using either ?-terms or unification
to bind arguments are essentially notational vari-
ants. However, UCG goes beyond simple predicate-
argument structures to instead use a semantic repre-
sentation language called Indexed Language (InL).
The idea of using indexes stems from Davidson
(event variables), and are a commonly used mech-
anism in unification-based frameworks and theories
for discourse representation. InL attaches one to ev-
ery formula representing its discourse referent. This
results in a representation such as (4) for the sen-
tence Ed came to the party.
(4) [e][party(x);past(e); to(e;x);come(e;Ed)]
InL thus flattens logical forms to some extent, using
the indexes to spread a given entity or event through
multiple predications. The use of indexes is crucial
for UCG?s account of modifiers, and as we will see
later, we exploit such referents to achieve similar
ends when coupling HLDS and CCG.
Minimal Recursion Semantics (MRS) (Copestake
et al, 1999; Copestake et al, 2001) is a frame-
work for computational semantics that is designed
to simplify the work of algorithms which produce
or use semantic representations. MRS provides the
means to represent interpretations with a flat, un-
derspecified semantics using terms of the predicate
calculus and generalized quantifiers. Flattening is
achieved by using an indexation scheme involving
labels that tag particular groups of elementary pred-
ications (EPs) and handles (here, h1;h2; :::) that ref-
erence those EPs. Underspecification is achieved
by using unresolved handles as the arguments for
scope-bearing elements and declaring constraints
(with the =q operator) on how those handles can be
resolved. Different scopes can be reconstructed by
equating unresolved handles with the labels of the
other EPs obeying the =q constraints. For example,
(5) would be given as the representation for every
dog chases some white cat.
(5) hh0;fh1:every(x;h2;h3);h4:dog(x);
h11:cat(y);h8:some(y;h9;h10);
h11:white(y);h7:chase(x;y)g;
fh0=qh7; h2=qh4; h9=qh11gi
Copestake et al argue that these flat representa-
tions facilitate a number of computational tasks, in-
cluding machine translation and generation, without
sacrificing linguistic expressivity. Also, flatness per-
mits semantic equivalences to be checked more eas-
ily than in structures with deeper embedding, and
underspecification simplifies the work of the parser
since it does not have to compute every possible
reading for scope-bearing elements.
3 Hybrid Logic Dependency Semantics
Kruijff (2001) proposes an alternative way to rep-
resenting linguistically realized meaning: namely,
as terms of hybrid modal logic (Blackburn, 2000)
explicitly encoding the dependency relations be-
tween heads and dependents, spatio-temporal struc-
ture, contextual reference, and information struc-
ture. We call this unified perspective combining
many levels of meaning Hybrid Logic Dependency
Semantics (HLDS). We begin by discussing how hy-
brid logic extends modal logic, then look at the rep-
resentation of linguistic meaning via hybrid logic
terms.
3.1 Hybrid Logic
Though modal logic provides a powerful tool for
encoding relational structures and their properties,
it contains a surprising inherent asymmetry: states
(?worlds?) are at the heart of the model theory for
modal logic, but there are no means to directly
reference specific states using the object language.
This inability to state where exactly a proposition
holds makes modal logic an inadequate representa-
tion framework for practical applications like knowl-
edge representation (Areces, 2000) or temporal rea-
soning (Blackburn, 1994). Because of this, compu-
tational work in knowledge representation has usu-
ally involved re-engineering first-order logic to suit
the task, e.g., the use of metapredicates such as Hold
of Kowalski and Allen. Unfortunately, such logics
are often undecidable.
Hybrid logic extends standard modal logic while
retaining decidability and favorable complexity
(Areces, 2000) (cf. (Areces et al, 1999) for a com-
plexity roadmap). The strategy is to add nominals,
a new sort of basic formula with which we can ex-
plicitly name states in the object language. Next to
propositions, nominals are first-class citizens of the
object language: formulas can be formed using both
sorts, standard boolean operators, and the satisfac-
tion operator ?@?. A formula @i p states that the
formula p holds at the state named by i.1 (There
are more powerful quantifiers ranging over nomi-
nals, such as #, but we do not consider them here.)
With nominals we obtain the possibility to explic-
itly refer to the state at which a proposition holds. As
Blackburn (1994) argues, this is essential for cap-
turing our intuitions about temporal reference. A
standard modal temporal logic with the modalities
F and P (future and past, respectively) cannot cor-
rectly represent an utterance such as Ed finished the
book because it is unable to refer to the specific time
at which the event occurred. The addition of nomi-
nals makes this possible, as shown in (6), where the
nominal i represents the Reichenbachian event time.
(6) hPi(i^Ed-finish-book)
Furthermore, many temporal properties can be de-
fined in terms of pure formulas which use nominals
and contain no propositional variables. For example,
the following term defines the fact that the relations
for F and P are mutually converse:
1A few notes on our conventions: p;q;r are variables over
any hybrid logic formula; i; j;k are variables over nominals; di
and hi denote nominals (for dependent and head, respectively).
(7) @i[F]hPii ^ @i[P]hFii
It is also possible to encode a variety of other rep-
resentations in terms of hybrid logics. For example,
nominals correspond to tags in attribute-value matri-
ces (AVMs), so the hybrid logic formula in (8) cor-
responds to the AVM in (9).
(8) hSUBJi(i^hAGRisingular ^hPREDidog)
^ hCOMPihSUBJii
(9) 2
6
6
6
4
SUBJ 1
"
AGR singular
PRED dog
#
COMP
h
SUBJ 1
i
3
7
7
7
5
A crucial aspect of hybrid logic is that nominals
are at the heart of a sorting strategy. Different sorts
of nominals can be introduced to build up a rich
sortal ontology without losing the perspicuity of a
propositional setting. Additionally, we can reason
about sorts because nominals are part and parcel of
the object language. We can extend the language of
hybrid logic with fSort:Nominalg to facilitate the ex-
plicit statement of what sort a nominal is in the lan-
guage and carry this modification into one of the ex-
isting tableaux methods for hybrid logic to reason ef-
fectively with this information. This makes it possi-
ble to capture the rich ontologies of lexical databases
like WordNet in a clear and concise fashion which
would be onerous to represent in first-order logic.
3.2 Encoding linguistic meaning
Hybrid logic enables us to logically capture two es-
sential aspects of meaning in a clean and compact
way, namely ontological richness and the possibility
to refer. Logically, we can represent an expression?s
linguistically realized meaning as a conjunction of
modalized terms, anchored by the nominal that iden-
tifies the head?s proposition:
(10) @h(proposition
V
h?ii(di ^depi))
Dependency relations are modeled as modal rela-
tions h?ii, and with each dependent we associate
a nominal di, representing its discourse referent.
Technically, (10) states that each nominal di names
the state where a dependent expressed as a proposi-
tion depi should be evaluated and is a ?i successor
of h, the nominal identifying the head. As an exam-
ple, the sentence Ed wrote a long book in London
receives the represention in (11).
(11) @h1(write^hACTi(d0^Ed)
^hPATi(d5^book^hGRi(d7^long))
^hLOCi(d9^London))
The modal relations ACT, PAT, LOC, and GR stand
for the dependency relations Actor, Patient, Loca-
tive, and General Relationship, respectively. See
Kruijff (2001) for the model-theoretic interpretation
of expressions such as (11).
Contextual reference can be modeled as a state-
ment that from the current state (anaphor) there
should be an accessible antecedent state at which
particular conditions hold. Thus, assuming an ac-
cessibility relation XS, we can model the meaning
of the pronoun he as in (12).
(12) @ihXSi( j ^male)
During discourse interpretation, this statement is
evaluated against the discourse model. The pronoun
is resolvable only if a state where male holds is XS-
accessible in the discourse model. Different acces-
sibility relations can be modeled, e.g. to distinguish
a local context (for resolving reflexive anaphors like
himself ) from a global context (Kruijff, 2001).
Finally, the rich temporal ontology underlying
models of tense and aspect such as Moens and
Steedman (1988) can be captured using the sorting
strategy. Earlier work like Blackburn and Lascarides
(1992) already explored such ideas. HLDS employs
hybrid logic to integrate Moens and Steedman?s no-
tion of the event nucleus directly into meaning rep-
resentations. The event nucleus is a tripartite struc-
ture reflecting the underlying semantics of a type of
event. The event is related to a preparation (an ac-
tivity bringing the event about) and a consequent (a
state ensuing to the event), which we encode as the
modal relations PREP and CONS, respectively. Dif-
ferent kinds of states and events are modeled as dif-
ferent sorts of nominals, shown in (13) using the no-
tation introduced above.
(13) @
fActivity:e1ghPREPifAchievement:e2g
^@
fAchievement:e2ghCONSifState:e3g
To tie (13) in with a representation like (11), we
equate the nominal of the head with one of the nom-
inals in the event nucleus (E)a and state its temporal
relation (e.g. hPi). Given the event nucleus in (13),
the representation in (11) becomes (14), where the
event is thus located at a specific time in the past.
(14) @h1(E
(13) ^write^hACTi(d0^Ed)
^hPATi(d5^book^hGRi(d7^long))
^hLOCi(d9^London))
^@h1fAchievement:e2g^hPifAchievement:e2g
Hybrid logic?s flexibility makes it amenable to
representing a wide variety of semantic phenomena
in a propositional setting, and it can furthermore be
used to formulate a discourse theory (Kruijff and
Kruijff-Korbayova?, 2001).
3.3 Comparison to MRS
Here we consider the properties of HLDS with
respect to the four main criteria laid out by
Copestake et al (1999) which a computational se-
mantics framework must meet: expressive adequacy,
grammatical compatibility, computational tractabil-
ity, and underspecifiability.
Expressive adequacy refers to a framework?s abil-
ity to correctly express linguistic meaning. HLDS
was designed not only with this in mind, but as its
central tenet. In addition to providing the means
to represent the usual predicate-valency relations,
it explicitly marks the named dependency relations
between predicates and their arguments and modi-
fiers. These different dependency relations are not
just labels: they all have unique semantic imports
which project new relations in the context of differ-
ent heads. HLDS also tackles the representation of
tense and aspect, contextual reference, and informa-
tion structure, as well as their interaction with dis-
course.
The criterion of grammatical compatibility re-
quires that a framework be linkable to other kinds of
grammatical information. Kruijff (2001) shows that
HLDS can be coupled to a rich grammatical frame-
work, and in x4 we demonstrate that it can be tied to
CCG, a much lower power formalism than that as-
sumed by Kruijff. It should furthermore be straight-
forward to use our approach to hook HLDS up to
other unification-based frameworks.
The definition of computational tractability states
that it must be possible to check semantic equiva-
lence of different formulas straightforwardly. Like
MRS, HLDS provides the means to view linguis-
tic meaning in a flattened format and thereby ease
the checking of equivalence. For example, (15) de-
scribes the same relational structure as (11).
(15) @h1(write^hACTid0 ^hPATid5 ^hLOCid9)
^@d0Ed^@d5book^@d9London
^@d7 long^@d5hGRid7
This example clarifies how the use of nominals is
related to the indexes of UCG?s InL and the labels
of MRS. However, there is an important difference:
nominals are full citizens of the object language with
semantic import and are not simply a device for
spreading meaning across several elementary predi-
cations. They simultaneously represent tags on sub-
parts of a logical form and discourse referents on
which relations are predicated. Because it is possi-
ble to view an HLDS term as a flat conjunction of
the heads and dependents inside it, the benefits de-
scribed by Copestake et al with respect to MRS?s
flatness thus hold for HLDS as well.
Computational tractability also requires that it
is straightforward to express relationships between
representations. This can be done in the object lan-
guage of HLDS as hybrid logic implicational state-
ments which can be used with proof methods to dis-
cover deeper relationships. Kruijff?s model connect-
ing linguistic meaning to a discourse context is one
example of this.
Underspecifiability means that semantic represen-
tations should provide means to leave some semantic
distinctions unresolved whilst allowing partial terms
to be flexibly and monotonically resolved. (5) shows
how MRS leaves quantifier scope underspecified,
and such formulas can be transparently encoded in
HLDS. Consider (16), where the relations RESTR
and BODY represent the restriction and body argu-
ments of the generalized quantifiers, respectively.
(16) @h7(chase^hACTih4 ^hPATih11)
^@h1(every^hRESTRii^hBODYi j)
^@h8(some^hRESTRik^hBODYil)
^@h4dog^@h11cat^@h11hGRi(h12^white)
^@ihQEQih4 ^@khQEQih11
MRS-style underspecification is thus replicated by
declaring new nominals and modeling =q as a modal
relation between nominals. When constructing the
fully-scoped structures generated by an underspeci-
fied one, the =q constraints must be obeyed accord-
ing to the qeq condition of Copestake etal. Because
HLDS is couched directly in terms of hybrid logic,
we can concisely declare the qeq condition as the
following implication:
(17) @ihQEQi j ! @i j_ (@ihBODYik^@khQEQi j)
Alternatively, it would in principle be possible to
adopt a truly modal solution to the representation
of quantifiers. Following Alechina (1995), (general-
ized) quantification can be modeled as modal opera-
tors. The complexity of generalized quantification is
then pushed into the model theory instead of forcing
the representation to carry the burden.
4 CCG Coupled to HLDS
In Dependency Grammar Logic (DGL),
Kruijff (2001) couples HLDS to a resource-
sensitive categorial proof theory (CTL) (Moortgat,
1997). Though DGL demonstrates a procedure for
building HLDS terms from linguistic expressions,
there are several problems we can overcome by
switching to CCG. First, parsing with CCG gram-
mars for substantial fragments is generally more
efficient than with CTL grammars with similar
coverage. Also, a wide-coverage statistical parser
which produces syntactic dependency structures
for English is available for CCG (Clark et al,
2002). Second, syntactic features (modeled by
unary modalities) in CTL have no intuitive semantic
reflection, whereas CCG can relate syntactic and
semantic features perspicuously using unification.
Finally, CCG has a detailed syntactic account of the
realization of information structure in English.
To link syntax and semantics in derivations, ev-
ery logical form in DGL expresses a nominal iden-
tifying its head in the format @i p. This handles de-
pendents in a linguistically motivated way through
a linking theory: given the form of a dependent, its
(possible) role is established, after which its mean-
ing states that it seeks a head that can take such a
role. However, to subsequently bind that dependent
into the verb?s argument slot requires logical axioms
about the nature of various dependents. This not
only requires extra reduction steps to arrive at the
desired logical form, but could also lead to problems
depending on the underlying theory of roles.
We present an alternative approach to binding de-
pendents, which overcomes these problems without
abandoning the linguistic motivation. Because we
work in a lexicalist setting, we can compile the ef-
fects of the linguistic linking theory directly into cat-
egory assignments.
The first difference in our proposal is that argu-
ments express only their own nominal, not the nom-
inal of a head as well. For example, proper nouns
receive categories such as (18).
(18) Ed ` n : @d1 Ed
This entry highlights our relaxation of the strict con-
nection between syntactic and semantic types tradi-
tionally assumed in categorial grammars, a move in
line with the MRS approach.
In contrast with DGL, the semantic portion of a
syntactic argument in our system does not declare
the role it is to take and does not identify the head
it is to be part of. Instead it identifies only its own
referent. Without using additional inference steps,
this is transmuted via unification into a form similar
to DGL?s in the result category. (19) is an example
of the kind of head category needed.
(19) sleeps ` s : @h2(sleep^hACTi(i^p))nn : @ip
To derive Ed sleeps, (18) and (19) combine via back-
ward application to produce (20), the same term as
that built in DGL using one step instead of several.
(20) @h2(sleep^hACTi(d1^Ed))
To produce HLDS terms that are fully compati-
ble with the way that Kruijff and Kruijff-Korbayova?
(2001) model discourse, we need to mark the infor-
mativity of dependents as contextually bound (CB)
and contextually nonbound (NB). In DGL, these ap-
pear as modalities in logical forms that are used to
create a topic-focus articulation that is merged with
the discourse context. For example, the sentence he
wrote a book would receive the following (simpli-
fied) interpretation:
(21) @h1([NB]write^ [NB]hPATi(d5^book)
^ [CB]hACTi(d6 ^hXSi(d3^male)))
DGL uses feature-resolving unary modalities
(Moortgat, 1997) to instantiate the values of in-
formativity. In unification-based approaches such
as CCG, the transferal of feature information into
semantic representations is standard practice. We
thus employ the feature inf and mark informativity
in logical forms with values resolved syntactically.
(22) Ed ` ninf=CB : @d1 Ed
(23) sleeps ` s : @h2([NB]sleep^ [q]hACTi(i^p))nninf=q:@ip
Combining these entries using backward application
gives the following result for Ed sleeps:
(24) s : @h2([NB]sleep^ [CB]hACTi(d1^Ed))
A major benefit of having nominals in our rep-
resentations comes with adjuncts. With HLDS, we
consider the prepositional verbal modifier in the sen-
tence Ed sleeps in the bed as an optional Locative
dependent of sleeps. To implement this, we fol-
low DGL in identifying the discourse referent of the
head with that of the adjunct. However, unlike DGL,
this is compiled into the category for the adjunct.
(25) in ` (s : @i(p^ [r]hLOCi(j^q))ns:@ip)=ninf=r:@jq
To derive the sentence Ed sleeps in the bed (see
Figure 1), we then need the following further entries:
(26) the ` ninf=CB:p=ninf=NB:p
(27) bed ` ninf=NB : @d3 bed
This approach thus allows adjuncts to insert their
semantic import into the meaning of the head, mak-
ing use of nominals in a manner similar to the use of
indexes in Unification Categorial Grammar.
5 Intonation and Information Structure
Information Structure (IS) in English is in part deter-
mined by intonation. For example, given the ques-
tion in (28), an appropriate response would be (29).2
(28) I know what Ed READ. But what did Ed
WRITE?
(29) (Ed WROTE) (A BOOK).
L+H* LH% H* LL%
Steedman (2000a) incorporates intonation into
CCG syntactic analyses to determine the contribu-
tion of different constituents to IS. Steedman calls
segments such as Ed wrote of (29) the theme of the
sentence, and a book the rheme. The former indi-
cates the part of the utterance that connects it with
the preceding discourse, whereas the latter provides
information that moves the discourse forward.
In the context of Discourse Representation The-
ory, Kruijff-Korbayova? (1998) represents IS by
splitting DRT structures into a topic/focus articula-
tion of the form TOPIC ./ FOCUS . We represent
2Following Pierrehumbert?s notation, the intonational con-
tour L+H* indicates a low-rising pitch accent, H* a sharply-
rising pitch accent, and both LH% and LL% are boundary tones.
Ed sleeps (= (24)) in the bed
s : @h2([NB]sleep^ [CB]hACTi(d1^Ed)) s : @i(p^ [r]hLOCi(j^q))ns:@ip)=ninf=r:@jq ninf=CB:s=ninf=NB:s ninf=NB:@d3 bed
>
ninf=CB : @d3 bed
>
s : @i(p^ [CB]hLOCi(d3^bed))ns:@ip
<
s : @h2([NB]sleep^ [CB]hACTi(d1^Ed)^ [CB]hLOCi(d3^bed))
Figure 1: Derivation of Ed sleeps in the bed.
this in HLDS as a term incorporating the ./ opera-
tor. Equating topic and focus with Steedman?s theme
and rheme, we encode the interpretation of (29) as:
(30) @h7([CB]write^ [CB]hACTi(d1^Ed)
./ [NB]hPATi(d4^book))
DGL builds such structures by using a rewriting sys-
tem to produce terms with topic/focus articulation
from the terms produced by the syntax.
Steedman uses the pitch accents to produce lexi-
cal entries with values for the INFORMATION fea-
ture, which we call here sinf . L+H* and H* set
the value of this feature as ? (for theme) or ?
(for rheme), respectively. He also employs cate-
gories for the boundary tones that carry blocking
values for sinf which stop incomplete intonational
phrases from combining with others, thereby avoid-
ing derivations for utterances with nonsensical into-
nation contours.
Our approach is to incorporate the syntactic as-
pects of Steedman?s analysis with DGL?s rewriting
system for using informativity to partition senten-
tial meaning. In addition to using the syntactic fea-
ture sinf , we allow intonation marking to instantiate
the values of the semantic informativity feature inf .
Thus, we have the following sort of entry:
(31) WROTE (L+H*) `
ssinf=?:?nninf=w;sinf=?:@ip=ninf=x;sinf=?:@jq
?=@h2([CB]write^[w]hACTi(i^p)^[x]hPATi( j^q))
We therefore straightforwardly reap the syntactic
benefits of Steedman?s intonation analysis, while IS
itself is determined via DGL?s logical form rewrit-
ing system operating on the modal indications of
informativity produced during the derivation. The
articulation of IS can thus be performed uniformly
across languages, which use a variety of strategies
including intonation, morphology, and word order
variation to mark the informativity of different el-
ements. The resulting logical form plugs directly
into DGL?s architecture for incorporating sentence
meaning with the discourse.
6 Conclusions and Future Work
Since it is couched in hybrid logic, HLDS is ide-
ally suited to be logically engineered to the task at
hand. Hybrid logic can be made to do exactly what
we want, answering to the linguistic intuitions we
want to formalize without yielding its core assets ? a
rich propositional ontology, decidability, and favor-
able computational complexity.
Various aspects of meaning, like dependency re-
lations, contextual reference, tense and aspect, and
information structure can be perspicuously encoded
with HLDS, and the resulting representations can
be built compositionally using CCG. CCG has close
affinities with dependency grammar, and it provides
a competitive and explanatorily adequate basis for
a variety of phenomena ranging from coordination
and unbounded dependencies to information struc-
ture. Nonetheless, the approach we describe could
in principle be fit into other unification-based frame-
works like Head-Driven Phrase Structure Grammar.
Hybrid logic?s utility does not stop with senten-
tial meaning. It can also be used to model dis-
course interpretation and is closely related to log-
ics for knowledge representation. This way we can
cover the track from grammar to discourse with a
single meaning formalism. We do not need to trans-
late or make simplifying assumptions for different
processing modules to communicate, and we can
freely include and use information across different
levels of meaning.
We have implemented a (preliminary) Java pack-
age for creating and manipulating hybrid logic terms
and connected it to Grok, a CCG parsing system.3
The use of HLDS has made it possible to improve
3The software is available at http://opennlp.sf.net
and http://grok.sf.net under an open source license.
the representation of the lexicon. Hybrid logic nom-
inals provide a convenient and intuitive manner of
localizing parts of a semantic structure, which has
made it possible to greatly simplify the use of inher-
itance in the lexicon. Logical forms are created as
an accumulation of different levels in the hierarchy
including morphological information. This is partic-
ularly important since the system does not otherwise
support typed feature structures with inheritance.
Hybrid logics provide a perspicuous logical lan-
guage for representing structures in temporal logic,
description logic, AVMs, and indeed any relational
structure. Terms of HLDS can thus be marshalled
into terms of these other representations with the
potential of taking advantage of tools developed for
them or providing input to modules expecting them.
In future work, we intend to combine techniques
for building wide-coverage statistical parsers for
CCG (Hockenmaier and Steedman, 2002; Clark et
al., 2002) with corpora that have explicitly marked
semantic dependency relations (such as the Prague
Dependency Treebank and NEGRA) to produce
HLDS terms as the parse output.
Acknowledgements
We would like to thank Patrick Blackburn, Johan Bos, Nissim
Francez, Alex Lascarides, Mark Steedman, Bonnie Webber and
the ACL reviewers for helpful comments on earlier versions of
this paper. All errors are, of course, our own. Jason Baldridge?s
work is supported in part by Overseas Research Student Award
ORS/98014014. Geert-Jan Kruijff?s work is supported by the
DFG Sonderforschungsbereich 378 Resource-Sensitive Cogni-
tive Processes, Project NEGRA EM6.
References
Natasha Alechina. 1995. Modal Quantifiers. Ph.D. thesis, Uni-
versity of Amsterdam, Amsterdam, The Netherlands.
Carlos Areces, Patrick Blackburn, and Maarten Marx. 1999. A
road-map on complexity for hybrid logics. In J. Flum and
M. Rodr??guez-Artalejo, editors, Computer Science Logic,
number 1683 in Lecture Notes in Computer Science, pages
307?321. Springer-Verlag.
Carlos Areces. 2000. Logic Engineering. The Case of Descrip-
tion and Hybrid Logics. Ph.D. thesis, University of Amster-
dam, Amsterdam, The Netherlands.
Patrick Blackburn and Alex Lascarides. 1992. Sorts and oper-
ators for temporal semantics. In Proc. of the Fourth Sympo-
sium on Logic and Language, Budapest, Hungary.
Patrick Blackburn. 1994. Tense, temporal reference and tense
logic. Journal of Semantics, 11:83?101.
Patrick Blackburn. 2000. Representation, reasoning, and rela-
tional structures: a hybrid logic manifesto. Logic Journal of
the IGPL, 8(3):339?625.
Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002.
Building deep dependency structures using a wide-coverage
CCG parser. In Proc. of the 40th Annual Meeting of the As-
sociation of Computational Linguistics, Philadelphia, PA.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard.
1999. Minimal recursion semantics: An introduction. ms,
www-csli.stanford.edu/?aac/newmrs.ps.
Ann Copestake, Alex Lascarides, and Dan Flickinger. 2001.
An algebra for semantic construction in constraint-based
grammars. In Proc. of the 39th Annual Meeting of the
Association of Computational Linguistics, pages 132?139,
Toulouse, France.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with combinatory categorial
grammar. In Proc. of the 40th Annual Meeting of the As-
sociation of Computational Linguistics, Philadelphia, PA.
Geert-Jan M. Kruijff and Ivana Kruijff-Korbayova?. 2001. A
hybrid logic formalization of information structure sensitive
discourse interpretation. In Proc. of the Fourth Workshop
on Text, Speech and Dialogue, volume 2166 of LNCS/LNAI,
pages 31?38. Springer-Verlag.
Ivana Kruijff-Korbayova?. 1998. The Dynamic Potential of
Topic and Focus: A Praguian Approach to Discourse Repre-
sentation Theory. Ph.D. thesis, Charles University, Prague,
Czech Republic.
Geert-Jan M. Kruijff. 2001. A Categorial Modal Architec-
ture of Informativity: Dependency Grammar Logic & Infor-
mation Structure. Ph.D. thesis, Charles University, Prague,
Czech Republic.
Marc Moens and Mark Steedman. 1988. Temporal ontology
and temporal reference. Computational Linguistics, 14:15?
28.
Michael Moortgat. 1997. Categorial type logics. In Johan van
Benthem and Alice ter Meulen, editors, Handbook of Logic
and Language. Elsevier Science B.V.
Glyn V. Morrill. 1994. Type Logical Grammar: Categorial
Logic of Signs. Kluwer Academic Publishers, Dordrecht,
Boston, London.
Mark Steedman. 2000a. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 34:649?689.
Mark Steedman. 2000b. The Syntactic Process. The MIT
Press, Cambridge Mass.
Henk Zeevat. 1988. Combining categorial grammar and unifi-
cation. In Uwe Reyle and Christian Rohrer, editors, Natural
Language Parsing and Linguistic Theories, pages 202?229.
Reidel, Dordrecht.
Linear order as higher-level decision:
Information Structure in strategic and tactical generation
Geert-Jan M. Kruijff,
Ivana Kruijff-Korbayova?
Computational Linguistics
University of the Saarland
Saarbru?cken, Germany
 
gj,korbay  @coli.uni-sb.de 
John Bateman
Applied Linguistics
University of Bremen
Bremen, Germany
 
bateman@uni-bremen.de 
Elke Teich
Applied Linguistics
University of the Saarland
Saarbru?cken, Germany
 
E.Teich@mx.uni-saarland.de 
Abstract
We propose a multilingual approach to
characterizing word order at the clause
level as a means to realize informa-
tion structure. We illustrate the prob-
lem with three languages which differ
in the degree of word order freedom
they exhibit: Czech, a free word or-
der language in which word order vari-
ation is pragmatically determined; En-
glish, a fixed word order language in
which word order is primarily gram-
matically determined; and German, a
language which is between Czech and
English on the scale of word order free-
dom. Our work is theoretically rooted
in previous work on information struc-
turing and word order in the Prague
School framework as well as on the
systemic-functional notion of Theme.
The approach we present has been im-
plemented in KPML.
1 Introduction
The aim of this paper is to describe an architecture
that addresses how information structure can be
integrated into strategic and tactical generation.
We focus primarily here on the tactical aspect of
how word order (henceforth: WO) may function
as a means of realizing information structure. The
approach we take is multilingually applicable. It
is implemented in KPML (Bateman, 1997b; Bate-
man, 1997a) and has been tested for Czech, Bul-
garian and Russian as three Slavonic languages
with different WO properties, as well as for En-
glish. The algorithm itself is not KPML-specific:
it combines the idea of WO constraints posed by
the grammar, with a complementary mechanism
of default ordering based on information struc-
ture. The algorithm could thus be applied in other
systems wich allow multiple sources of ordering
constraints.
Information structure is a means that a speaker
employs to indicate that some parts of a sen-
tence meaning are context-dependent (?given?),
and that others are context-affecting (?new?). In-
formation structure is therefore an inherent aspect
of sentence meaning, and it contributes in an im-
portant way to the overall coherence of a text.
While it is commonly accepted that information
structuring is a major source of constraints for the
organization of a given content in a particular lin-
ear order in many languages, there is very little
work in Natural Language Generation that explic-
itly models this relation.
From a practical perspective, in the most com-
monly employed generation systems such as
KPML, FUF (Elhadad, 1993; Elhadad and Robin,
1997) or REALPRO (Lavoie and Rambow, 1997),
linear ordering comes as a by-product of other
grammatical choices. This is fine for tactical
generation components and it is sufficient for
languages with grammatically determined WO
(?fixed? WO languages), such as English or Chi-
nese. However, most languages have some WO
variability and this variation usually reflects infor-
mation structure. When languages in which linear
order is primarily pragmatically determined are
involved, such as the Slavonic languages we have
dealt with, a number of problems become imme-
diately apparent.
A comprehensive account of WO variation for
natural language generation that is reusable across
languages is thus required. Such an account needs
to represent linearization as an explicit decision-
making process that involves both the representa-
tion of the language-specific linear ordering pos-
sibilites and the representation of the language-
specific (and possibly cross-linguistically valid)
motivations for particular linearizations. Again,
while the former is catered for in most tactical
generation systems, only selected aspects of the
latter have been dealt with and only for selected
languages (e.g., (Hoffman, 1994; Hoffman, 1995;
Hakkani et al, 1996)).
For example, (Hoffman, 1994) proposes a
treatment of WO in Turkish using a categorial
grammar framework (CCG, (Steedman, 2000))
and relating this to Steedman?s (earlier) account
of information structure (Steedman, 1991). How-
ever, the most important issue, that of providing
an integrated account of how information struc-
ture guides the choice of (or, is realized by) linear
ordering, is left unsolved (Kruijff, 2001).
Given that in many languages, information
structure is the major driving force for WO vari-
ation, it is indeed the most straighforward idea to
couple an account of information structure with
the choice of linear ordering. However, for mul-
tilingual application, the particular challenge is to
develop a solution that can be applied, no mat-
ter at which point on the free-to-fixed WO cline a
language is located.
The approach to WO proposed in this paper is a
move in exactly this direction. We start in  2 with
presenting data from Czech, German and English
that motivate the perspective we take on informa-
tion structure, and its role in generating coher-
ent discourse. In  3 we introduce the linguistic
notions employed in the present account. In  4
we discuss how information structure fits into a
general system architecture, and we discuss the
implementation of the strategic generation com-
ponent on the basis of KPML. We continue with
an elaboration of the role of information structure
in tactical generation, presenting an algorithm for
generating contextually appropriate linearization,
given a sentence?s information structure, and il-
lustrate its implementation on Czech and English
examples (  5). We conclude the paper with a
summary (  6).
2 Linguistic motivation
There are a number of factors commonly ac-
knowledged to play an important role in express-
ing a given content in a specific linear form.
The inventory of these factors contains at least
the following: information structure, syntactic
structure, intonation, rhythm and style. Cross-
linguistically, these factors may be involved in
constraining linear ordering to varying degrees.
English, for instance, is an example of a lan-
guage in which WO is rather rigid, i.e., strongly
constrained by syntactic structure. In such lan-
guages, differences in information structure are
often reflected by varying the intonation pat-
tern or by the choice of particular types of
grammatical constructions, such as clefting and
pseudo-clefting, or definiteness/indefiniteness of
the nominal group. Czech, in contrast, which has
a rich case system and no definite or indefinite
article, belongs to the so-called ?free word order?
languages, where the same effects are achieved by
varying WO. Finally, German lies between En-
glish and Czech in the spectrum between fixed
and free WO. We illustrate the general point that
WO selections are related to information structure
by appropriateness judgements of some examples
of instructions in Czech, German and English.1
(1) Otevr?eme
open-1PL
pr???kazem
command-INS
Open
Open
soubor.
file-ACC
Sie
You
o?ffnen
open
eine
a
Datei
file
mit
with
dem
the
Befehl
command
Open.
Open.
Open a file with the Open command.
The ordering in (1) is neutral in that no partic-
ular contextual constraints hold with respect to
the newsworthiness of any of the elements ex-
pressed in this clause. This kind of ordering can
1The English examples use imperative mood, while the
Czech and the German examples use indicative mood as
the most common way of conveying instructions of the dis-
cussed type. Alternatively, both Czech and German can use
also imperatives or infinitives for instructions, but these are
considered less polite than the indicative versions. Last but
not least, instructions can also be formulated in indicative
mood with passive voice in both Czech and German.
be elicited by the question What should we do?.2
We follow Prague School accounts (Firbas, 1992;
Sgall et al, 1986) in calling this neutral ordering
the systemic ordering (cf. also  5). Alternatively,
(1) could be used in a context characterized by
the question What should we open by the Open
command?, when the Open command is not be-
ing contrasted with some other entity.
(2) Otevr?eme
open-1PL
soubor
file-ACC
pr???kazem
command-INS
Open.
Open
Sie
you
o?ffnen
open
die
the
Datei
file
mit
with
dem
the
Befehl
command
Open.
Open.
?Open the file with the Open command.?
(3) Soubor
file-ACC
otevr?eme
open-1PL
pr???kazem
command-INS
Open.
Open
Die
the
Datei
file
o?ffnen
open
Sie
you
mit
with
dem
the
Befehl
command
Open.
Open.
?Open the file with the Open command.?
The word order variants illustrated in (2) and (3)
are appropriate when some file is active in the
context (Chafe, 1976), for instance when the user
is working with a file. In (2), the action of open-
ing is also active; in (3) it can, but does not have
to be active, too. The contexts in which (2) and
(3) can be appropriately used can be character-
ized by the questions What should we do with the
file? or How should we open the file?. Unlike
(2), example (3) can be used if file is contrasted
with another entity. In German, this contrast is
required, whereas in Czech it is optional. In En-
glish, intonation could mark whether contrast is
required.
(4) Pr???kazem
command-INS
Open
Open
otevr?eme
open-1PL
soubor.
file-ACC
Mit
with
dem
the
Befehl
command
Open
Open
o?ffnen
open
Sie
you
eine
a
Datei.
file.
With the Open command, open a file.
(5) Pr???kazem
command-INS
Open
Open
soubor
fileACC
otevr?eme.
open-1PL
Mit
with
dem
the
Befehl
command
Open
Open
o?ffnen
open
Sie
you
die
a
Datei.
file.
With the Open command, open the file.
2We use questions for presentational purposes to indicate
which contexts would be appropriate for uttering sentences
with particular WO variants. Such question-answer pairs are
known as question tests (Sgall et al, 1986).
The contexts in which (4) can be used are char-
acterized by What should we do with the Open
command?. While (4) does not refer to a spe-
cific file, in (5) an activated file is presumed. (5)
is appropriate in contexts characterized by What
should we do to the file with the Open command?.
It is also possible to use (4) in a context charac-
terized by What should we do?, and (5) in a con-
text characterized by What should we do to the
file?, if it is presumed that we are talking about
using various commands (or various means or in-
struments) to do various things. In the latter type
of context, the Open command does not have to
be activated.
(6) Soubor
file-ACC
pr???kazem
command-INS
Open
Open
otevr?ete.
open-I2PL
Die
the
Datei
file
o?ffnen
open
Sie
you
mit
with
dem
the
Befehl
command
Open.
Open
Open the file with the Open command.
Example (6) is like (5) in that it is appropriate
when both a file and the Open command are acti-
vated. The contexts in which (6) can be appropri-
ately used can be characterized by What should
we do to the file with the Open command?. Un-
like (5), (6) can also be used when file is con-
trasted with another entity. In German, there is
no difference in word order between (6) and (3)
(they differ only in intonation). This is a result of
the strong ordering constraint in German to place
the finite verb as second (in independent, declara-
tive clauses). In Czech verb secondness also plays
a role, but it is much weaker.
Analogous judgements concerning contextual
appropriateness apply to WO variants in differ-
ent mood and/or voice (when available in the in-
dividual languages). The orders in which the verb
is first do not presume the activation of either a
file or a command. The orders in which ?file?
precedes the verb appear to presume an active
file, the orders in which ?command? precedes the
verb appear to presume the activation of a com-
mand. When both ?file? and ?command? precede
the verb, the activation of both a file and a com-
mand appears to be presumed.
These judgements show that differences in WO
(in languages with a more flexible WO then En-
glish, e.g., Czech and German) very often corre-
spond to differences in how the speaker presents
the information status of the entities and pro-
cesses that are referred to in a text, in particu-
lar, whether they are assumed to be already fa-
miliar or not, and whether they are assumed to
be activated in the context. Note that in English,
the same distinction is expressed by the use of a
definite vs. an indefinite nominal expression, i.e.
?a  the file?.
To summarize: Since sentences which differ
only in WO (and not in the syntactic realizations
of clause elements) are not freely interchangable
in a given context, we have to be able to gen-
erate contextually appropriate WOs. In order to
achieve this, we need to be able to capture not
only the structural restrictions specific to individ-
ual languages, but also the restrictions reflecting
the information status of the entities (and pro-
cesses) being referred to.
3 Underlying notions
In order to provide constraints for WO decisions
within our generation architecture, we require
mechanisms through which particular patterns of
information structuring can constrain the choice
among the WO variants available. These patterns
are provided by our text planning component. We
have found two complementary approaches to the
relationship between aspects of information struc-
turing and WO to be ripe for application in the
generation of extended texts; these approaches are
briefly introduced below.
In order to clarify the complementary nature of
the approaches that we have adopted, it is neces-
sary first to distinguish between two dimensions
of organization that are often confused or whose
difference is contested: in his Systemic Func-
tional Grammar (SFG), (Halliday, 1970; Hall-
iday, 1985) distinguishes between the thematic
structure of a clause and its information struc-
ture: Whereas the Theme is ?the starting point
for the message, it is the ground from which the
clause is taking off? (Halliday, 1985, 38), infor-
mation structure concerns the distinction between
the Given as ?what is presented as being already
known to the listener? (Halliday, 1985, 59), and
the New as ?what the listener is being invited to
attend to as new, or unexpected, or important?
(ibid).
3.1 Information structure and ordering
In Halliday?s original approach (Halliday, 1967),
the basic assumption for English and also for
other languages is that ordering, apart from being
grammatically constrained, is iconic with respect
to ?newsworthiness?. So on a scale from Given
to New information, the ?newer? elements would
come towards the end of the information unit, the
?newest? element bearing the nuclear stress. This
approach relies on the possibility of giving a com-
plete ordering of all clause elements with respect
to their newsworthiness.
The notion of ordering by newsworthiness in
Halliday?s approach is parallel to the notion of
communicative dynamism (CD) introduced in the
early works of Firbas (for a recent formulation
see (Firbas, 1992)) and used also within the Func-
tional Generative Description (FGD, (Sgall et al,
1986)). Also from the viewpoint of CD, the pro-
totypical ordering of clause elements from left
to right respects newsworthiness: In prototypical
cases, WO corresponds to CD. However, textu-
ally motivated thematization or grammatical con-
straints may force WO to diverge from CD.
The FGD approach differs from Halliday?s in
that, in addition to CD, it works with a de-
fault (canonical) ordering, called systemic order-
ing (SO). SO is the language specific canonical
ordering of clause elements (complements and
adjuncts), as well as of elements of lower syntac-
tic levels, with respect to one another.
For the current purposes we concentrate on the
SO for a subset of the clause elements that are dis-
cerned in FGD. We use the following SOs for the
Slavonic languages and for English and German:3
SO for Czech, Russian, Bulgarian:
Actor  TemporalLocative  Purpose  Space-
Locative  Means  Addressee  Patient 
Source  Destination
SO for English: Actor  Addressee  Pa-
tient  SpaceLocative  TemporalLocative 
Means  Source  Destination  Purpose-
dependent
SO for German: Actor  TemporalLocative
 SpaceLocative  Means  Addressee  Pa-
tient  Source  Destination  Purpose
3The labels we use for the various types of elements are
a mixture of FGD and SFG terminology.
The SO for the Slavonic languages is based on
the one for Czech (Sgall et al, 1986); the only
difference is that we have placed Patient before
Source (?from where?). We follow (Sgall et al,
1986) in considering the SOs for the main types
of complementations in Russian and Bulgarian to
be similar to the Czech one, though there can be
slight differences (cf. the observations reported in
(Adonova et al 1999)). The SO for English com-
bines the suggestions made by (Sgall et al, 1986)
and the ordering defaults of the NIGEL grammar
of English (cf. Section 5.2). The SO for German
is based on (Heidolph et al, 1981, p.704).
The informational status of elements is estab-
lished through deviation of CD from the SO. This
leads us to the distinction FGD makes between
contextually bound (CB) and contextually non-
bound (NB) items in a sentence (Sgall et al,
1986). A CB item is assumed to convey some
content that bears on the preceding discourse con-
text. It may refer to an entity already explic-
itly referred to in the discourse, or an ?implicitly
evoked? entity. At each level of syntactic struc-
ture, CB items are ranked lower than NB items in
the CD ordering. The motivation behind and the
meaning of the CB/NB distinction in FGD cor-
responds to those underlying the Given/New di-
chotomy in SFG.
Contextual boundness can be used to constrain
WO (at the clause level) as follows:
 The CB elements (if there are any) typically
precede the NB elements.
 The mutual ordering of multiple CB items in
a clause corresponds to communicative dy-
namism, and the mutual ordering of mul-
tiple NB items in a clause follows the SO
(with the exceptions required by grammati-
cally constrained ordering as described be-
low). The default for communicative dy-
namism is SO.
 The main verb of a clause is ordered at the
boundary between the CB elements and the
NB elements, unless the grammar specifies
otherwise (verb secondness).
It is the above abstract ordering principles that
underly the algorithm we present in  5.
3.2 Thematic structure
In all languages we looked at so far, there are also
orders we cannot explain solely on the basis of
the CB/NB distinction along with SO and gram-
matical constraints. On the one hand, it has been
claimed that the ordering of CB elements follows
CD rather than SO, and that CD is determined
by contextual factors (Sgall et al, 1986). On the
other hand, cases where an NB element appears at
the beginning of a clause are far from rare. While
we currently do not have more to add to the for-
mer issue, the latter can be readily addressed us-
ing the notion of Theme. For illustration, consider
(8) in Czech, German and English, appearing in a
context where it is preceded only by (7).
(7) First open the Multiline styles dialog box using one
of the following methods.
(8) Z
From Data
menu
menu
Data
choose 	

vybereme
Style.
Style.
Im
In
Menu?
menu
Data
Data
wa?hlen
choose
Sie
you
Style.
Style.
In the Data menu, choose Style.
The preceding context does not refer to the ?Data
menu? or make it active in any way. Working
only with the notion of information structure dis-
cerning CB (Given) and NB (New) elements, one
is thus unable to explain this ordering. On the
other hand, the notion of thematic structure as
a reflection of a global text organization strategy
makes such explanation possible. In Halliday?s
approach, Theme has a particular textual function,
that of signposting the intended development or
?scaffolding? that a writer employs for structuring
an extended text. In software instruction manuals,
for example, we encounter regular thematization
of (i) the location where actions are performed,
(ii) the particular action that the user is instructed
to perform, or (iii) the goal that the user wants to
achieve (cf. (Kruijff-Korbayova? et al, in prep) for
a more detailed discussion).
4 Information structure and strategic
planning
In this section we briefly describe how we in-
tegrate information structure into strategic gen-
eration, i.e. text- and sentence-planning. The
Figure 1: A text plan. In our system, a text plan organizes content into a linear fashion, showing
where (and how) content might be aggregated syntactically (e.g. conjunction) or discursively (e.g.
RST-relations). In the example above, the text plan specifies a text consisting of an overall goal (the
title) and five substeps to resolve that goal (the tasks). The first task is a simple one, the second task
is a complex formed around an RST-purpose relation, after which follows a conjunction of tasks. (The
CONJOINED-INSTRUCTION-TASKS nodes indicate that the left-daughter node (a task) and the task
dominated by the immediate non-terminal node above a CONJOINED-INSTRUCTION-TASKS node,
are to be related by a conjunction.) The content to be realized is identified by the leaves of the text
plan. Whenever a leaf is introduced in the text plan, the discourse model is updated with the content?s
(A-box) concepts. The sentence planner decends through the text plan depth-first. Thereby it gathers
the leaves? content into sentence-specifications, following any indications of aggregation. It makes use
of the discourse model to specify whether content should be realized as contextually bound (or not).
principle idea is that during text-planning, a dis-
course model is built that is then used in sentence-
planning to determine a sentence?s information
structure.
We have developed a system using KPML. In
KPML, generation resources are divided into in-
teracting modules called regions. For the purpose
of text-planning we have constructed a region that
defines an additional level of linguistic resources
for the level of genre. The region facilitates the
composition of text structures in a way that is very
similar to the way the lexico-grammar builds up
grammatical structures. This enables us to have a
close interaction between global level text gener-
ation and lexico-grammatical expression, with the
possibility to accommodate and propagate con-
straints on output realization. While constructing
a text plan, the text planner constructs a (rudimen-
tary) discourse model that keeps track of the dis-
course entities introduced.
Text planning results in a text plan and a dis-
course model that serve as input to the sentence
planner. The text plan is a hierarchical structure,
organizing the content into a more linear fashion
(see Figure 3.2). The sentence planner creates
the input to the tactical generation phase as for-
mulas of the Sentence planning Language (SPL,
(Kasper, 1989)). The SPL formulas express the
bits of content identified by the text plan?s leaves,
and can also group one or more leaves together
(aggregation) depending on decisions taken by
the text planner concerning discourse relations.
Most importantly, during this phase of planning
what content is to be realized by a sentence, the
underlying information structure of that content
is determined: Whenever the sentence planner
encounters a piece of content that the discourse
model notes as previously used, it marks the cor-
responding item in the SPL formula as contextu-
ally bound (note that we are hereby making a sim-
plifying assumption that in the current version of
the sentence planner we equate contextual bound-
ness with previous mention).
The text planner can also choose a particular
textual organization and determine the element
which should become the Theme. If no particu-
lar element is chosen as the Theme, the grammar
chooses some element as the default Theme. This
can be the Subject (as in English), the least com-
municatively dynamic element (as in Czech); the
choice of the default Theme in German is freer
than in English, but more restricted than in Czech
(cf. (Steiner and Ramm, 1995) for a discussion).
The Theme is then placed at the beginning of the
clause, although not necessarily at the very first
position, as this might be occupied, e.g., by a con-
nective. The placement of the Theme is also re-
solved by the grammar.
5 Realizing information structure
through linearization
It is in the setting described in  4 that the issue of
generating contextually appropriate sentences re-
ally arises. In this section we describe the word
ordering algorithm (  5.1) and its application to
Czech and English (  5.2).
5.1 Flexible word order algorithm
As discussed, constraints from various sources
need to be combined in order to determine gram-
matically well-formed and contextually appropri-
ate WO. Contextual boundness is used to con-
strain WO at the clause level as specified above.
We combine the following two phases in which
information structure (CB/NB) is taken into ac-
count during tactical generation:
 information structure can determine partic-
ular realization choices made in the gram-
mar; for example, when inserting and plac-
ing the particle of a phrasal verb, when in-
serting and ordering the Source and Destina-
tion for a motion process;
 information structure can determine the or-
dering of elements whose placement has not
been sufficiently constrained by the gram-
mar.
For a multilingual resource, this allows each
language to establish its own balance between the
two phases. To show our approach in a nutshell,
we present an abstract WO algorithm in Figure 2.
Given:
a set GC of ordering constraints
imposed by the grammar
a list L1 of constituents
that are to be ordered,
a list D giving ordering of CB
constituents (default is SO)
Create two lists LC and LN of de-
fault orders:
Create empty lists LC (for CB items)
and LN (for NB items)
Repeat for each element E in L1
if E is CB,
then add E into LC,
else add E into LN.
Order all elements in LC
according to D
Order all elements in LN
according to SO
if the Verb is yet unordered then
Order the Verb at
the beginning of LN
Order the elements of L1
if GC is not empty then
use the contraints in GC, and
if the contraints in GC are
insufficient,
apply first the default
orders in LC and then those in LN
Figure 2: Abstract ordering algorithm
The ordering constraints posed by the gram-
mar have the highest priority. Note that this in-
cludes the ordering of the textually determined
Theme. Then, elements which are not ordered by
the grammar are subject to the ordering according
to information structure, i.e. systemic ordering in
combination with the CB/NB distinction. The or-
dering of the NB elements (i) is restricted by the
syntactic structure or (ii) follows SO. The order-
ing of the CB elements can be (i) specified on the
basis of the context, (ii) restricted by the syntactic
structure, or (iii) follow SO.
The ordering algorithm as such is not language
specific, and could be usefully applied in the gen-
eration of any language. What differs across lan-
guages is first of all the extent to which the gram-
mar of a particular language constrains ordering,
i.e. which elements are subject to ordering re-
quirements posed by the syntactic structure, and
which elements can be ordered according to infor-
mation structure. Also, it is desirable (and our al-
gorithm allows it) to specify different systemic or-
derings for different languages. And, even within
a single language, our algorithm allows the spec-
ification of different systemic orderings in differ-
ent grammatical contexts (just by adding a real-
ization statement that (partially) defines the SO
during strategic generation).
The algorithm is applicable in platforms other
than KPML. In the first place, any grammar
can modify its decisions to take information
structure into account. In addition, those tacti-
cal generators allows multiple sources of order-
ing constraints, e.g., a combination of grammar-
determined choices and defaults, as long as such
that the default ordering based on information
structure can be applied.
5.2 Algorithm application
The algorithm described above has been imple-
mented and used for generation of Czech and En-
glish instructional texts. The Czech grammar re-
sources used in tactical generation have been built
up along with Bulgarian and Russian grammar
resources as described in (Kruijff et al, 2000),
reusing the NIGEL grammar for English. The
original NIGEL grammar itself already combines
the specification of ordering constraints in the
grammar with the application of defaults. If an or-
dering is underspecified by the grammar, the de-
faults are applied. The defaults are ?static?, i.e.
specified once and for all. The algorithm we have
described replaces these ?static? defaults with a
?dynamic? construction of ordering constraints.
Two separate sets of ?dynamic? defaults are com-
puted on the basis of the SO for the CB and the
NB elements in each sentence/clause.
We use the SOs for Czech and English
specified above (cf.  3.1). For each ele-
ment in the input SPL we specify whether it
is CB (:contextual-boundness yes) or
NB (:contextual-boundness no); in ad-
dition, we can specify the textual Theme in the
SPL (theme <id>). The SPL in Figure 3 illus-
trates this.
Note that the information structure distinction
between CB vs. NB elements on the one hand,
and the informational status of referents as iden-
tifiable vs. non-identifiable on the other hand, are
orthogonal. Whereas CB/NB has to do with the
(R / RST-purpose
:speechact assertion
:DOMAIN (ch/DM::choose
:actor (a1/DM::user
:identifiability-q identifiable
:contextual-boundness yes)
:actee (a2/object :name gui-open
:identifiability-q identifiable
:contextual-boundness no)
:instrumental (mea/DM::mouse
:identifiability-q identifiable
:contextual-boundness no)
:spatial-locating (loc/DM::menu
:identifiability-q identifiable
:contextual-boundness yes
:class-ascription (label/object
:name gui-file))
:RANGE (open/DM::open
:contextual-boundness no
:actee (f/DM::file
:contextual-boundness no)))
:theme open)
Generated output:
Pro
for
otevr?en??
opening-GEN
souboru
file-GEN
uz?ivatel
user-NOM
v
in
menu
menu-LOC
vybere
choose-3SG
mys???
mouse-INS
Open.
Open
To open a file, the user chooses Open in the menu with the
mouse.
Figure 3: Sample input SPL for English and
Czech and generated outputs
speaker?s presenting an element as either bearing
on the context or context-affecting, identifiability
reflects whether the speaker assumes the hearer
to pick out the intended referent. These two di-
mensions are independent, though correlated (cf.
the discussion of activation vs. identifiability in
(Lambrecht, 1994)). What is encountered most
often is the correlation of CB with identifiable
and NB with non-identifiable. The correlation of
NB with identifiable corresponds is found, e.g., in
cases of ?reintroducing? an element talked about
before, or in cases like There is a square and a
circle. Delete the circle. ?in the second sentence,
the same ordering would be used also in German
(Lo?schen Sie den Kreis) and in Czech (Vymaz?te
kruh.).
What is hard to find is the correlation of CB
with non-identifiable, but it is the way we would
analyze a dollar bill in example (9) (Gregory
Ward, p.c.)4
(9) (What do you do if you see money laying on the
ground?)
Dolarovou
Dollar
bankovku
note
bych
would 	
zvedla.
pick-up 	
Eine
a
Dollarnote
dollarnote
wu?rde
would
ich
I
aufheben.
pick-up
A dollar bill I would pick up.
The CB/NB assignments can be varied to ob-
tain different WO variants. The examples below
show some of the CB/NB assignment combina-
tions and the outputs generated using the Czech
and English grammars.
(10) user
Actor-NB
Uz?ivatel
choose
(Finite-Verb)
vybere
Open
Purpose-NB
pro
menu
SpaceLoc.-NB
otevr?en??
mouse
Means-NB
souboru
open file
Patient-NB
v
The user chooses Open in the menu with the mouse
to open a file.
(11) user
Actor-CB
Uz?ivatel
choose
v
Open
SpaceLoc.-CB
menu
menu
(Finite-Verb)
vybere
mouse
Purpose-NB
pro
open file
Means-NB
otevr?en??
Patient-NB
souboru mys???
The user chooses Open in the menu with the mouse
to open a file.
(12) user
Purpose-CB
Pro
choose
otevr?en??
Open
Actor-CB
souboru
menu
SpaceLoc.-CB
uz?ivatel
mouse
Means-CB
v
open file
(Finite-Verb)
menu
Patient-NB
mys??? vybere
To open a file the user chooses Open in the menu
with the mouse.
As mentioned above, we preserve the notion of
textual Theme. An SPL can contain a specifica-
tion of a Theme, and the corresponding element
is then ordered at the front of the sentence, as de-
termined by the grammar. The WO of the rest of
the sentence is determined as described.
4Regarding intonation: in English, there are two into-
nation phrases, the first containing dollar bill with a L+H*
pitch accent on dollar, and the second with a H* pitch accent
on pick up. In Czech and German it seems that a contrastive
pitch accent on dolarovou bankovku is optional, and the rest
can have neutral intonation with nuclear stress on the last
word.
6 Summary and conclusions
We have presented a flexible word ordering al-
gorithm for natural language generation. The
novel contribution consists in offering one way
of implementing information structure as the ma-
jor source of constraints on word order varia-
tion for languages with pragmatically-determined
word order. Apart from that, the special feature of
the word order algorithm proposed is that it can
also be applied to languages with grammatically-
determined word order. We have illustrated the
application of the algorithm for Czech and En-
glish, Czech being a language in which word or-
der is primarily pragmatically determined and En-
glish being a grammatically-determined word or-
der language. We have thus provided evidence
that the algorithm can flexibly be applied to ?free?
word order languages as well as ?fixed? word or-
der languages.
From a linguistic theoretical point of view, the
most important precondition for achieving this
has been to take seriously the linguistic observa-
tion that in many languages information structure
is the driving force for word order variation. For
the modeling of information structure for strate-
gic generation, we have drawn upon two well es-
tablished linguistic frameworks, in both of which
the discourse-linguistic and pragmatic constraints
on grammatical realization are a focal interest, the
Prague School and Systemic Functional Linguis-
tics. From a technical point of view, we have
based the implementation on the KPML system,
integrating the proposed word order algorithm
with existing multilingual grammatical resources
and re-using KPML?s mechanisms for word or-
der realization as well as its systemic-functionally
based notion of Theme. The algorithm is not
KPML-specific, though, and could be applied in
other frameworks as well, especially if they allow
the combination of linearization constraints com-
ing from different sources.
Acknowledgements
The work presented here folows up on our
earlier work carried out partially within AG-
ILE (Automatic Generation of Instructions
in Languages of Eastern Europe), a project
funded by the European Community within
the INCO-COPERNICUS programme (grant
No. PL96114). We would also like to thank
the anonymous reviewers of this workshop for
valuable comments.
References
John A. Bateman. 1997a. Enabling technology for multilin-
gual natural language generation: The kpml development
environment. Natural Language Engineering, 3:15 ? 55.
John A. Bateman. 1997b. KPML Development Environ-
ment: multilingual linguistic resource development and
sentence generation. Darmstadt, Germany, March. (Re-
lease 1.0).
Wallae Chafe. 1976. Givenness, contrastiveness, definite-
ness, subjects, topics and point of view. Subject and
Topic. Charles Li (ed.). New York: Academic Press. p.
25 ? 56.
Michael Elhadad and Jacques Robin. 1997. Surge: A com-
prehensive plug-in syntactic realisation component for
text generation. Technical report, Department of Com-
puter Science, Ben Gurion University, Beer Shava, Israel.
Michael Elhadad. 1993. Fuf: The universal unifier user
manual 5.2. Technical report, Department of Computer
Science, Ben Gurion University, Beer Shava, Israel.
Jan Firbas. 1992. Functional Sentence Perspective in Writ-
ten and Spoken Communication. Studies in English Lan-
guage. Cambridge University Press, Cambridge.
Dilek Zeynep Hakkani, Kemal Oflazer, and Ilyas Cicekli.
1996. Tactical generation in a free constituent order lan-
guage. In Proceedings of the International Workshop on
Natural Language Generation, Herstmonceux, Sussex,
UK.
Michael A. K. Halliday. 1967. Notes on transitivity and
theme in English ? parts 1 and 2. Journal of Linguistics,
3(1 and 2):37?81 and 199?244.
Michael A.K. Halliday. 1970. A Course in Spoken English:
Intonation. Oxford Uniersity Press, Oxford.
Michael A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London, U.K.
K. Heidolph, W. Fla?mig, and W. Motsch. 1981. Grundzu?ge
einer deutschen Grammatik. Akademie-Verlag.
Beryl Hoffman. 1994. Generating context-appropriate
word orders in turkish. In Proceedings of the Internati-
nal Workshop on Natural Language Generation, Kenneb-
unkport, Maine.
Beryl Hoffman. 1995. Integrating ?free? word order syntax
and information structure. In Proceedings of the Euro-
pean Chapter of the Association for computational Lin-
guistics (EACL), Dublin, Ireland.
Robert T. Kasper. 1989. A flexible interface for linking
applications to PENMAN?s sentence generator. In Pro-
ceedings of the DARPA Workshop on Speech and Natural
Language.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logi-
cal Architecture of Informativity: Dependency Grammar
Logic & Information Structure. Ph.D. thesis, Faculty
of Mathematics and Physics, Charles University, Prague,
Czech Republic, April.
Geert-Jan M. Kruijff, Elke Teich, John Bateman, Ivana
Kruijff-Korbayova?, Hana Skoumalova?, Serge Sharoff,
Lena Sokolova, Tony Hartley, Kamy Staykova and Jir???
Hana. 2000. Multilingual generation for three slavic lan-
guages. In Proceedings COLING 2000.
Ivana Kruijff-Korbayova?, John Bateman, and Geert-Jan M.
Kruijff. in prep. Generation of contextually appropriate
word order. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing, Lecture Notes. CSLI.
Knud Lambrecht. 1994. Information Structure and Sen-
tence Form. Cambridge Studies in Linguistics. Cam-
bridge University Press.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation. In Proceedings of
the Fifth Conference on Applied Natural Language Pro-
cessing (ANLP), Washington DC.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel Publishing Company, Dor-
drecht, Boston, London.
Mark J. Steedman. 1991. Structure and intonation. Lan-
guage, 68:260 ? 296.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge Massachusetts.
Erich Steiner and Wiebke Ramm. 1995. On Theme as a
grammatical notion for German. Functions of Language,
2(1):57?93.
Discourse-Level Annotation
for Investigating Information Structure
Ivana Kruijff-Korbayova? and Geert-Jan M. Kruijff
Computational Linguistics, Saarland University, Saarbru?cken, Germany
{korbay,gj}@coli.uni-sb.de
Abstract
We present discourse-level annotation of newspa-
per texts in German and English, as part of an
ongoing project aimed at investigating information
structure from a cross-linguistic perspective. Rather
than annotating some specific notion of information
structure, we propose a theory-neutral annotation
of basic features at the levels of syntax, prosody
and discourse, using treebank data as a starting
point. Our discourse-level annotation scheme cov-
ers properties of discourse referents (e.g., semantic
sort, delimitation, quantification, familiarity status)
and anaphoric links (coreference and bridging). We
illustrate what investigations this data serves and
discuss some integration issues involved in combin-
ing different levels of stand-off annotations, created
by using different tools.
1 Introduction
The goal of this paper is to present a discourse-
level annotation scheme developed for the pur-
pose of investigating information distribution in
text from a cross-linguistic perspective, with a
particular focus on the interplay of various fac-
tors pertaining to the realization of information
structure. Information Structure (IS) concerns
utterance-internal structural and semantic proper-
ties reflecting the speaker?s/writer?s communica-
tive intentions and the relation of the utterance
to the discourse context, in terms of the dis-
course status of the content, the actual and at-
tributed attentional states of the discourse partici-
pants, and the participants? prior and changing atti-
tudes (knowledge, beliefs, intentions, expectations,
etc.) (Kruijff-Korbayova? and Steedman, 2003). In
many (if not all) languages, differences in IS moti-
vate variations in surface realization of utterances,
such as syntactic structure, word order and intona-
tion. But languages differ in the extent to which
they employ various combinations of IS-realization
means (Vallduv?? and Engdahl, 1996; Kruijff, 2001).
Modeling these phenomena and their interaction re-
quires understanding IS and its role in discourse.
IS is therefore an important aspect of meaning at
the interface between utterance and discourse, which
computational models of discourse processing should
take into account. Unfortunately, there exists no
theory that provides a comprehensive picture of IS,
explaining its realization cross-linguistically, its rep-
resentation at the level of linguistic meaning, and its
interpretation in context. Employing corpora can
help to deepen our intuitive understanding of IS, in
order to construct explanatorily more adequate the-
ories.
While the phenomena involved in discourse and
IS are themselves complex and not yet fully un-
derstood, studying and modeling their interaction
is made difficult by proliferating and often under-
formalized terminologies, especially for IS (cf. the
diverging dichotomies, e.g., Theme-Rheme, Topic-
Comment, Topic-Focus, Background-Focus, Given-
New, Contextually Bound-Nonbound). What is
needed is further systematization of terminologies,
formalization and computational modeling, and em-
pirical and corpus-based studies.
The goal of the MULI (MUltilingual Informa-
tion structure) project is to contribute to this effort
by empirically analyzing IS in German and English
newspaper texts. For this, we designed annotation
schemes for enriching existing linguistically inter-
preted language resources with information at the
levels of syntax, discourse semantics and prosody.
The MULI corpus consists of extracts
from the Tiger treebank for German
(Brants et al, to appear)1 and the Penn treebank
for English (Marcus et al, 1994)2. It comprises
250 sentences in German (app. 3,500 tokens) and
320 sentences in English (app. 7,000 tokens). The
MULI corpus has been created by extracting a
continuous stretch of 21 relatively short texts from
the Tiger treebank, and a set of 10 texts from the
Penn Treebank. The selection was made so that
the texts would be comparable in genre (financial
news/announcements).
The morphological, part-of-speech and syntactic
information encoded in the treebanks can be re-
used for our purposes. We add annotations of
syntactically marked constructions, prosodic fea-
tures and discourse semantics. Our approach
to annotation at the levels of syntax, prosody
and discourse is outlined in (Bauman et al, 2004a;
Bauman et al, 2004b). In this paper, we provide
1http://www.coli.uni-sb.de/cl/projects/tiger/
2http://www.cis.upenn.edu/~treebank/home.html
more details about the discourse-level annotation.
In ?2 we overview the methodological concerns
and desiderata we adhere to in designing our anno-
tation schemes. In ?3 we present the discourse-level
annotation scheme in detail. In ?4 we illustrate the
multi-level investigation perspective. ?5 we briefly
describe the annotation tools we use. In ?6 we con-
clude and sketch future work.
2 Methodology
Text samples of varying origin, genre, language and
size have been previously annotated with theory-
specific notions of IS by various authors. Such data
are typically not publicly available, and even if they
can be obtained, it is very hard if not impossible
to compare and reuse different annotations. More
promising in this respect are annotations that in-
clude or add some aspect(s) of IS to an existing
corpus or treebank. The most systematic effort of
this kind that we are familiar with is the Topic-
Focus annotation in the Prague Dependency Tree-
bank (Bura?n?ova? et al, 2000).
In contrast to other projects in which IS is
annotated and investigated, we do not annotate
theory-biased abstract categories like Topic-Focus
or Theme-Rheme. Since we are particularly inter-
ested in the correlations and co-occurrences of fea-
tures on different linguistic levels that can be inter-
preted as indicators of the abstract IS categories,
we needed an annotation scheme to be as theory-
neutral as possible: It should allow for a descrip-
tion of the phenomena, from which ?any? theory-
specific explanatory mechanisms can subsequently
be derived (Skut et al, 1997). We therefore con-
centrate instead on features pertaining, on the one
hand, to the surface realization of linguistic expres-
sions (the levels of syntax and prosody), and, on
the other hand, to the semantic character of the dis-
course referents (the discourse level).
In designing our annotation schemes, we fol-
lowed the guidelines of the Text Encoding Ini-
tiative3 and the Discourse Resource Initiative
(Carletta et al, 1997). In line with these standards,
we define for each annotation level (i) the markable
expressions, (ii) the attributes of markables, and (iii)
the links between markables (if any).
Syntax The Tiger treebank and the Penn tree-
bank we use as the starting point already con-
tain syntactic information. The additional syntac-
tic features annotated in the MULI project per-
tain to clauses as markable units, and encode the
presence of structures with noncanonical word order
that typically serve to put the focus on certain syn-
tactic elements. We include cleft, pseudo-cleft, re-
versed pseudo-cleft, extraposition, fronting and ex-
pletives, as well as voice distinctions (active, medio-
passive and passive). We annotate these features
explicitly (when not already present in the tree-
3http://www.tei-c.org/
bank annotation), to be able to correlate them di-
rectly with features at other levels. The annotation
scheme draws on accounts of the analysed features in
(Eisenberg, 1994) and (Weinrich, 1993) for German
and in (Quirk et al, 1985) and (Biber et al, 1999)
for English.
Prosody For the prosodic annotation, we
recorded one German and one English native
speaker reading aloud the texts of the MULI
corpus.4,5 The recordings were digitised and
annotated using the EMU Speech Database
System ((Cassidy and Harrington, 2001b);
http://emu.sourceforge.net/).
The markables at the prosody level are into-
nation phrases, intermediate phrases and words.
Their attributes encode the position and strength
of phrase breaks, and the position and type of
pitch accents and boundary tones, following the
conventions of ToBI (Tones and Break Indices
(Beckmann and Hirschberg, 1994)) for English and
GToBI6 (Grice et al, in press) for German, which
are regarded as standards for describing the into-
nation of these languages within the framework of
autosegmental-metrical phonology.
Discourse At the discourse level, we define as
markable those linguistic expressions that introduce
or access discourse entities (i.e., discourse referents
in the sense used in DRT and alike) (Webber, 1983;
Kamp and Reyle, 1993). Currently we consider
primarily the discourse entities introduced by
?nominal-like? expressions (Passoneau, 1996). We
include other kinds of expressions as markable only
when they participate in an anaphoric relation
with a ?nominal-like? expression. For example, a
sentence is a markable when it serves as an an-
tecedent of a discourse-deictic anaphoric expres-
sion (Webber, 1991); the main verb of a sentence
is a markable when the subject of the sentence
is a ?zero-anaphor?, etc. Our annotation instruc-
tions for identifying markables are an amalgamation
and extension of those of the MUC-7 Coreference
Task Definition7, the DRAMA annotation manual
(Passoneau, 1996), and (Wind, 2002).
The attributes of markables in our discourse-
level annotation scheme are designed to capture
a range of properties that semantically character-
ize the discourse entities evoked by linguistic ex-
4We are aware that using recorded speech is not ideal. We
nevertheless decided for this approach, as we wanted to work
on top of existing treebanks. As far as we are aware, there
does not exist a treebank for any of the publicly available
speech corpora.
5Since prosodic annotation is very time-consuming, we had
to concentrate mainly on one language. Thus, we analysed
all German texts and restricted ourselves to some English
examples. Since individual speaking preferences may vary
from speaker to speaker, we will have to record additional
speakers in order to be able to come up with generalizable
results.
6http://www.coli.uni-sb.de/phonetik/projects/Tobi/gtobi.html
7http://www.itl.nist.gov/iaui/894.02/related_projects/
muc/proceedings/co_task.html
pressions. Thereby we differ from most existing
discourse-level annotation efforts, which concentrate
on the linguistic expressions and on identifying
anaphoric relations between them (i.e., identifying
anaphors and their antecedents). A notable ex-
ception is the GNOME project annotation scheme
(Poesio et al, 1999): In GNOME, the aim was to
annotate a corpus with information relevant for noun
phrase generation. This included syntactic, seman-
tic and discourse attributes of nominal expressions.
The semantic attributes include, among others, an-
imacy, ontological status, countability, quantifica-
tion and generic vs. specific reference, which reflect
similar distinctions as we make in our annotation
scheme.
Besides the semantic properties that characterize
discourse entities individually, our annotation
scheme of course also covers referential rela-
tions between discourse entities, including both
identity and bridging. We build on and ex-
tend the MUC-7 coreference specification and
the coreference/bridging classifications described
in (Passoneau, 1996), (Carletta et al, 1997),
(Poesio, 2000) and (Mu?ller and Strube, 2001). We
represent anaphoric relations between linguistic ex-
pressions through links between the corresponding
markables. The type of relation is annotated as
an attribute of the markable corresponding to the
anaphor.
3 Discourse-Level Annotation
Information structure theories describe the phenom-
ena at hand at a surface level, at a semantic level,
or at both levels simultaneously, i.e., an expres-
sion belongs to some IS partition, in virtue of some
information-status of the corresponding discourse
entity. For the investigation of IS at the (discourse)
semantic level, we thus need more information about
the character of the discourse entities introduced by
linguistic expressions. We therefore annotated ex-
pressions with their discourse referents and their fol-
lowing properties:
Semantic type/sort reflects ontological charac-
ter of a discourse entity: object, property, even-
tuality or textual entity. Since the primary fo-
cus of our current annotation are discourse enti-
ties evoked by nominal-like expressions, most of
them denote objects. Objects are further classi-
fied according to semantic sorts: human/person, of-
fice/profession, organization, animal, plant, physical
object, quantity/amount, date/time, location/place,
group/collection, abstract entity, other. Proper-
ties are classified into either temporal or perma-
nent. Eventuality has sub-classes phase (habit
or state) and process (activity, accomplishment,
achievement). Textual entities are for now not fur-
ther classified.
Denotation characteristics of a discourse en-
tity are captured by a combination of attributes,
inspired by (Hlavsa, 1975). First, we distinguish
between denotational (extensional, referential) and
non-denotational (intensional, attributive) uses of
linguistic expressions. Denotationally used expres-
sions pick out (specify) some instance(s) of the des-
ignated concept(s). The instance(s) can be uniquely
specified (=identifiable to the hearer), or specific
but not identifiable, or even unspecific (arbitrary,
generic ? so any instance will do). Generic refer-
ences are seen as denoting types. An expression is
used non-denotationally when it attribute or qual-
ifies, i.e., evokes the characteristic properties of a
concept, without actually instantiating it. A typical
example of a non-denotationally used expression is
a predicative NP, as in ?He was a painter?.
The annotation of a group of denotation proper-
ties is motivated by the need to have a language-
independent characterization of the referents as
such, rather then the properties of the referring ex-
pression, such as (in)definiteness. The latter is a
surface reflex of a combination of denotation char-
acteristics, and sometimes may not even be overtly
indicated by articles or other determiners.
For the denotationally used expressions, we then
analyze what part of the domain designated by
the expression is actualy included in the extension.
These aspects are annotated in the determination,
delimitation and quantification attributes.
Determination characterizes the specificity of
the denoted concept instance. Unique determina-
tion means that the entity is uniquely specified, i.e.,
the hearer can (or is assumed to be able to) iden-
tify the entity/instance intended by the speaker.
There may be just one such entity, e.g., as with
proper names, or there are possibly more entities
that satisfy the description, but the speaker means
a particular one and assumes that the hearer can
identify it. Anaphoric pronouns are also typically
used as unique denotators. Finally, an entity can
be uniquely specified through a relation to another
entity, or through a relation between expressions in
the text. In (Hlavsa, 1975) this is called relational
uniqueness ; it seems to correspond to Loebner?s no-
tion of NPs as functions, used in the GNOME an-
notation scheme.
Existential determination is assigned to entities
that are not uniquely specified, that is, the speaker
does not assume the hearer to be able to identify a
particular entity, but in principle the speaker would
be able to identify one. Maybe such unique identifi-
cation by the hearer is not important for the inter-
action, it is enough to take ?some instance?.
Variable determination is assigned when an ex-
pression not only does not uniquely specify an en-
tity, but a particular entity cannot in principle be
identified, rather, the speaker means an arbitrary
(?any?) instance. Typical examples are generics, or
references to type.
Delimitation characterizes the extent of the de-
noted concept instance with respect to the domain
designated by the expression. The posible values are
total and partial, indicating the entire domain desig-
nated by the expression is included in the extension,
or only a part.
Quantification captures the countability of the
denotated concept instance, and if countable, the
quantity of the individual objects included in the
extension:
 uncountable is assigned when it is impossible
to decompose the extension into countable dis-
tinguishable individual objects, e.g., with mass
nouns;
 specific-single means quantity of one, e.g., ?one
x?, ?the other x?;
 specific-multiple means a concrete quantity
larger than one, e.g., ?two x?, ?both x?, ?a
dozen?;
 unspecific-multiple means an unspecified num-
ber larger than one, e.g., ?some x?, ?many x?,
?most x?.
Familiarity Status is a notion that most ap-
proaches to IS use as one dimension or level
of the IS-partitioning, for example Given/New
in (Halliday, 1985), Background/Focus in
(Steedman, 2000), or as the basis for deriving
a higher level of partitioning (Sgall et al, 1986).
It is therefore important to capture it in our anno-
tation as an independent feature, so that we can cor-
relate it with other features at the discourse level and
at other levels. We apply the familiarity status tax-
onomy from (Prince, 1981), distinguishing between
new, unused, inferrable, textually and situationally
evoked entities. We are aware that operationalizing
Prince?s taxonomy is a tough issue. For the time be-
ing, our annotation guidelines give intuitive descrip-
tions of the different statuses, roughly as follows:
 brand new : create a new discourse referent for
a previously unknown object;
 unused : create a new discourse referent for a
known object;
 inferable: create a new discourse referent for an
inferable object;
 evoked (textually or situationally): access an
available discourse referent.
Annotators? uncertainty or discrepancies between
annotators help us to identify problematic cases, and
to revise the guidelines where necessary.8
Linguistic form encodes the syntactic category
of the markable expression. This is not an attribute
encoding a semantic property of a discourse entity.
We have found it useful to distinguish the following
categories:
8Our reason for applying the familiarity taxonomy from
(Prince, 1981) is that it addresses the status of discourse en-
tities as such, not other referential properties. For example,
the givenness hierarchy in (Gundel et al, 1993) interleaves in-
formation status with uniqueness and specificity.
 nominal group is a ?normal? NP with a head
noun;
 pronominal subsumes expressions headed by a
personal, demonstrative, interrogative or rela-
tive pronoun;
 possessive covers possessive premodifiers (typ-
ically a possessive pronoun, e.g., ?our view?,
or possessive adjective, e.g., ?the Treasury?s
threat? or in German ?newyorker Burse?;
 pronominal adverb in German, e.g. ?daraus?
(from that);
 apposition and coordination;
 clitic is used for clitics and in those cases when
an expression contains a clitic affix (though
not frequent in English and German newspaper
text);
 ellipsis is used for elliptical (reduced) expres-
sions, which function as nominal-like groups,
but contain no nominal head (e.g., ?the first?);
in case a discourse entity is evoked by a zero ar-
gument, e.g., in case of subject- or object pro-
drop, a markable is created on a surrogate non-
nominal expression, labeled as zero-arg; finally,
clause or text are used for markables which are
clause and simple sentences, or text segments,
respectively (note that these are only mark-
able, when they serve as antecedents to nominal
anaphors).
These categories classify the linguistic forms of ex-
pressions independently of the categories employed
in the syntactic-level annotation. There are also
technical reasons for introducing a form-feature, e.g.,
when some other expression serves as a markable to
annotate the attributes of the discourse entity cor-
responding to a ?zero-anaphor? or to a clitic affix.
Referential link encodes the type of relation
between the discourse entity corresponding to an
anaphoric expression, and the one corresponding to
the (most likely) antecedent. The referential links we
distinguish are identity (representing coreference)
and bridging, further classified into set-membership,
set-containment, part-whole composition, property-
attribution, generalized possession, causal link and
lexical-argument-filling.
The attributes of information status and referen-
tial link are related, but we include them both, be-
cause the former is a property of a discourse entity,
while the latter directly reflects anaphoricity as a
property of an expression (the size of it ranging, ul-
timately, from a word to a segment of a discourse).
The relation between anaphoricity and IS is not a
straightforward one, and needs further investigation,
enabled by an annotation like ours.
4 Multi-level Investigation of IS
We illustrate the different levels of annotation and
analysis with an example sequence taken from our
English corpus (Figure 1). We considered the syn-
tactic annotation as a suitable starting point for the
analysis. Where relevant features are detected, we
compare the annotation at other levels.
(1) In the 1987 crash, remember, the market
was shaken by a Danny Rostenkowski pro-
posal to tax takeovers out of existence. (2)
Even more important, in our view, was the
Treasury?s threat to thrash the dollar. (3) The
Treasury is doing the same thing today; (4)
thankfully, the dollar is not under 1987-style
pressure.
Figure 1: Example from the English corpus
Of the four clauses in the example sequence, three
show noncanonical word orders. In (1), the temporal
adjunct is fronted, followed by the main predicate
remember (in imperative mood). Additionally, (1)
contains a passive construction bringing the patient
in subject position. In (2), subject complement and
adjunct (marking stance) are fronted. In (4), an
adjunct (againmarking stance) is fronted.
The discourse entity (DE) introduced in the
fronted temporal phrase the 1987 crash in (1) is ex-
tensional, abstract, unique, specific singular, and has
the information status of unused (also indicated by
remember). The DE introduced in the unmarked
subject position is extensional, abstract, unique,
specific singular, but has the status of inferrable:
the market can be seen as a bridging anaphor to the
crash, by means of an argument filling (crash of the
market). The DEs introduced by the sentence-final
expressions in (1) and (2) are also extensional, ab-
stract, unique, specific singular, and both have the
information status of new.9 What appears sentence-
final in (1) and (2) are thus two negative things that
happened during the 1987 crash. The fronted ex-
pression(s) in (2) are not annotated as a DE. The
DEs in the unmarked subject positions in (3) and (4)
both have the information status of textually evoked,
as both expressions are coreferential anaphors to
parts of the Treasury?s threat to thrash the dollar.
While the DE referred to by the Treasury is an ex-
tensional, office, unique, specific singular, that of the
dollar is intensional, abstract, unique, uncountable.
The expression the same thing in (3) is anaphoric to
the Treasury?s threat . . . in (2), but it introduces
a new DE of the same type; its information status
is that of inferrable. Finally, the DE introduced in
the sentence-final expression 1987-style pressure in
(4) is intensional, abstract, existential, uncountable,
and also has the information status of inferable; it
is however hard to code it as a bridging anaphor,
because it is not clear what relation it would have
to what antecedent: if anything, then a Danny Ros-
tenkowski proposal . . . in (1).
The prosodic analysis shows that the fronted
phrase in (2) is not only syntactically but also
9We assume a layman reader. For an economy expert,
these entities may have the status of unused.
prosodically prominent (cf. Figure 2): Two peak ac-
cents on even and more highlight these words (with
the more pronounced accent on more expressing a
contrast), whereas the word important is deaccented,
since the concept of ?importance? is inferable from
the context. Furthermore, the adjective construc-
tion forms a phrase of its own, delimited by an in-
tonation phrase boundary, which is in turn signalled
by a falling-rising contour plus a short pause. The
following parenthesis in our view also constitutes a
single intonation phrase. Here again, our is assigned
a contrastive accent, while view is unaccented.
All remaining content words of the clause re-
ceive accents. However, the most ?newsworthy?
word, threat, is the only one marked by a ris-
ing pitch accent (L+H*), indicating its higher de-
gree of importance for the speaker. This interpre-
tation is further supported by the insertion of a
phrase break directly after this word. Finally, the
high-downstepped nuclear accent (H+!H*) on dollar
marks this item as being accessible by speaker and
hearer (Pierrehumbert and Hirschberg, 1990).
5 Technical Realization
Above we presented a multi-level view on IS anno-
tation, where each layer is to be annotated indepen-
dently, to enable us to investigate interactions across
the different levels. Such investigations involve ei-
ther exploration of the integrated data (i.e., simul-
taneous viewing of the different levels and searching
across levels) or integrated processing, e.g., in order
to discover or test correlations across levels. There
are two crucial technical requirements that must be
satisfied to make this possible: (i) stand-off anno-
tation at each level and (ii) alignment of base data
across the levels. Without the first, we would not be
able to keep the levels separate and perform annota-
tion at each level independently, without the latter
we would not be able to align the separate levels.
We have chosen XML for the representation and
maintenance of annotations. Each level of anno-
tation is represented as a separate XML file, re-
ferring to (sequences of) tokens in a common base
file containing the actual text data. We keep inde-
pendent levels of annotation separate, even if they
can in principle be merged into a single hierarchy.
Parallel aligned texts (e.g., the written and spo-
ken versions of our corpus) are also represented via
shared token IDs. A related issue is that of annota-
tion tools. We are not using one generic tool for
all levels for the simple reason that we have not
found a tool that would support the needs of all
levels and still be efficient (Bauman et al, 2004b;
Mu?ller and Strube, 2001). Therefore, we prefer to
use tools specifically designed for the task at hand.
We describe the tools of our choice below.
Prosodic Level The speech data was anno-
tated with the EMU Speech Database System10
(Cassidy and Harrington, 2001a), which produces
10http://emu.sourceforge.net/
Figure 2: Prosodic annotation of example sentence (2) in EMU
several files in which time stamps are associated with
the respective annotated labels.
Syntactic Level For the syntactic annotation, we
used the XML editor XML-Spy11. The annotation
scheme is defined in a DTD, which is used to check
the well-formedness and the validity of the annota-
tion.
Discourse Level The discourse-level annotation
is done with the MMAX annotation tool developed
at EML, Heidelberg (Mu?ller and Strube, 2003).
MMAX is a light-weight tool written in Java that
runs under both Windows and Unix/Linux. It sup-
ports multilevel annotation of XML-encoded data
using annotation schemes defined as DTDs. MMAX
implements the above-mentioned general concepts of
markables with attributes and standing in link rela-
tions to one another. To exploit and reuse annotated
data in the MMAX format, there is the MMAX
XML Discourse API.
Integration The tools inevitably employ differ-
ent data formats: on the prosodic level data is stored
in the EMU data format, on the syntactic level in
Tiger XML and on the discourse level in MMAX
XML format.
The EMU files have to be converted into stand-off
XML format. To be able to align the prosodic an-
notation with the syntax and the discourse level, we
chose the word as common basic unit. This poses
several problems. First, punctuation marks count
as separate words, but are not realised in spoken
language. To be able to correlate prosodic phras-
ing and punctuation marks, we store the punctua-
tion marks as attributes of the respective preceding
word. Second, pauses occur very often in speech, but
as they are not part of the written texts, they do not
count as words. Because they are an important fea-
ture for phrasing and rhythm, we also code them
as attributes of the preceding word. Third, in some
cases a single word carries more than one accent, e.g.
11http://www.xmlspy.com/
long compounds (Getra?nkedosenhersteller), or num-
bers. In these cases, it would be interesting to know
which part(s) of the word get accented, which re-
quires some way of annotating parts of words (e.g.,
syllables). Finally, for some multi-word units, e.g.
18,50 Mark, the spoken realisation (achtzehn Mark
fu?nfzig) cannot be aligned with the orthographic
form, because spoken and orthographic form differ
in number and order of words.
6 Conclusions and Perspectives
We presented the details of the discourse-level anno-
tation scheme that we developed within the MULI
project. This project is a pilot project: As such, the
annotation has so far been restricted to a relatively
small amount of data, since the experimental design
of the study required testing of tools as well as man-
ual annotation. We plan to extend the size of the
corpus by manual and semi-automatic annotation in
a follow-up project.
The challenge in the MULI project has been to
define theory-neutral and language-independent an-
notation schemes for annotating linguistic data with
information that pertains to the realisation and in-
terpretation of information structure. An important
characteristic of the MULI corpus, arising from its
theory-neutrality, is that it is descriptive. The cor-
pus annotation is not based on explanatory mecha-
nisms: We have to derive such explanations from the
data. (See (Skut et al, 1997) for related methodol-
ogy pertaining to syntactic treebanks.)
The MULI corpus facilitates linguistic investiga-
tion of how phenomena at different annotation levels
interact. For example, how do syntactic structure
and intonation interact to realize information struc-
ture? Or, how does information structure interact
with anaphoric relationships? Such linguistic inves-
tigations can help to extend existing accounts of in-
formation structure, and can also be used to verify
(or falsify) predictions made by such accounts. The
corpus also makes it possible to construct computa-
tional models from the corpus data.
Theory-neutrality enhances reusability of linguis-
tic resources, because it facilitates the integration
with other, theory-neutral resources. To some ex-
tent we have already explored this in MULI, com-
bining e.g. Tiger annotation with discourse-level
annotation. Another possibility to explore is the to
integrate MULI annotation with, e.g., the SALSA
corpus (Erk et al, 2003), which provides more de-
tailed semantico-pragmatic information in the style
of FrameNet.
Our initial investigation also reveals where addi-
tional annotation would be needed. For instance,
the text example discussed above constitutes a con-
cession scheme, which we cannot identify without
annotating discourse/rhetorical relations. This in
turn requires extending the annotation scheme to
non-nominal markables.
Acknowledgements
We would like to thank Saarland University for fund-
ing the MULI pilot project. Thanks also to Stella
Neumann, Erich Steiner, Elke Teich, Stefan Bau-
mann, Caren Brinckmann, Silvia Hansen-Schirra
and Hans Uszkoreit for discussions.
References
S. Bauman, C. Brinckmann, S. Hansen-Schirra, G.-J.
Kruijff, I. Kruijff-Korbayova?, S. Neumann, and E. Te-
ich. 2004a. Multi-dimensional annotation of linguis-
tic corpora for investigating information structure. In
Proc. of the Workshop on Frontiers in Corpus Anno-
tation, held at the NAACL-HLT 2004 Conference.
S. Bauman, C. Brinckmann, S. Hansen-Schirra, G.-J.
Kruijff, I. Kruijff-Korbayova?, S. Neumann, E. Te-
ich, E. Steiner, and H. Uszkoreit. 2004b. The muli
project: Annotation and analysis of information struc-
ture in German and English. In Proc. of the LREC
2004 Conference.
M. E. Beckmann and J. Hirschberg. 1994. The ToBI an-
notation conventions. Ms. and accompanying speech
materials, Ohio State University.
D. Biber, S. Johansson, G. Leech, S. Conrad, and E.
Finegan. 1999. The Longman Grammar of Spoken
and Written English. Longman, Harlow.
S. Bird and M. Liberman. 2001. A formal framework for
linguistic annotation. Speech Communication, 33(1-
2):23?60.
S. Brants, S. Dipper, P. Eisenberg, S. Hansen, E. Ko?nig,
W. Lezius, C. Rohrer, G. Smith, and H. Uszkoreit. to
appear. TIGER: Linguistic interpretation of a Ger-
man corpus. Journal of Language and Computation
(JLAC), Special Issue.
E. Bura?n?ova?, E. Hajic?ova?, and P. Sgall. 2000. Tagging of
very large corpora: Topic-focus articulation. In Proc.
of the 18th Conference on Computational Linguistics
(COLING?2000), July 31 - August 4 2000, pages 139?
144. Universita?t des Saarlandes, Saarbru?cken, Ger-
many.
J. Carletta, N. Dahlba?ck, N. Reithinger, and M. A.
Walker. 1997. Standards for dialogue coding in natu-
ral language processing. Report on the dagstuhl sem-
inar, Discourse Resource Initiative, February 3?7.
S. Cassidy and J. Harrington. 2001a. Multi-level anno-
tation in the emu speech database management sys-
tem. Speech Communication, 33(1-2):61?78.
S. Cassidy and J. Harrington. 2001b. Multi-level an-
notation in the EMU speech database management
system. Speech Communication, 33(1-2):61?78.
P. Eisenberg. 1994. Grundriss der deutschen Gram-
matik, 3. Aufl. Metzler, Stuttgart, Weimar.
K. Erk, A. Kowalski, S. Pado?, and M. Pinkal. 2003. To-
wards a resource for lexical semantics: A large german
corpus with extensive semantic annotation. In Proc.
of ACL 2003, Sapporo, Japan.
M. Grice, S. Baumann, and R. Benzmu?ller. in press.
German intonation in autosegmental-metrical phonol-
ogy. In Sun-Ah Jun, editor, Prosodic Typology:
Through Intonational Phonology and Transcription.
OUP.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, (69):274?307.
M. A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London, U.K.
Z. Hlavsa. 1975. Denotace objektu a jej?? prostr?edky v
souc?asne? c?es?tine? [Denotating of objects and its means
in contemporary Czech], volume 10 of Studie a pra?ce
lingvisticke? [Linguistic studies and works]. Academia.
N. Ide, P. Bonhomme, and L. Romary. 2000. Xces: An
xml-based standard for linguistic corpora. pages 825?
830, Athens, Greece.
H. Kamp and U. Reyle. 1993. From discourse to logic.
Kluwer Academic Publishers, Dordrecht, the Nether-
lands.
Geert-Jan M. Kruijff 2001. A Categorial-Modal Logical
Architecture of Informativity: Dependency Grammar
Logic & Information Structure, Faculty of Mathemat-
ics and Physics, Charles University. Prague, Czech Re-
public.
I. Kruijff-Korbayova? and M. Steedman. 2003. Discourse
and information structure. Journal of Logic, Lan-
guage and Information: Special Issue on Discourse
and Information Structure, 12(3):249?259.
M. Marcus, G. Kim, M. Ann Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger? 1994. The Penn treebank: Annotating predi-
cate argument structure. In Proc. of the Human Lan-
guage Technology Workshop, San Francisco, Morgan
Kaufmann.
D. McKelvie, A. Isard, A. Mengel, M.B. Moller,
M. Grosse, and M. Klein. 2001. The MATE work-
bench ? an annotation tool for XML coded speech
corpora. Speech Communication, 33(1-2):97?112.
C. Mu?ller and M. Strube. 2001. Annotating anaphoric
and bridging relations with MMAX. In Proc. of the
2nd SIGdial Workshop on Discourse and Dialogue,
pages 90?95, Aalborg, Denmark, 1?2 September.
http://www.eml.villa-bosch.de/english/Research/NLP/sigdial
C. Mu?ller and M. Strube. 2003. Multi-level an-
notation in mmax. In Proc. of the 4th SIG-
dial Workshop on Discourse and Dialogue, Sap-
poro, Japan, 4-5 July. http://www.eml.villa-
bosch.de/english/Research/NLP/Publications.
R. Passoneau. 1996. Instructions for applying dis-
course reference annotation for multiple applications
(DRAMA). draft, December 20.
J. Pierrehumbert and J. Hirschberg. 1990. The meaning
of intonational contours in the interpretation of dis-
course. In P.R. Cohen, J. Morgan, and M.E. Pollack,
editors, Intentions in Communication, pages 271?311.
MIT press.
Massimo Poesio, Renate Henschel, Janet Hitzeman,
Rodger Kibble, Shane Montague, and Kees van
Deemter 1999. Towards An Annotation Scheme For
Noun Phrase Generation In Proc. of the EACL Work-
shop on Linguistically Interpreted Corpora. Bergen,
Norway.
Massimo Poesio 2000. The GNOME An-
notation Scheme Manual Available online
http://www.hcrc.ed.ac.uk/~gnome/anno manual.html
E. Prince. 1981. Toward a taxonomy of given-new infor-
mation. In P. Cole, editor, Radical Pragmatics, pages
223?256. Academic Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartik. 1985.
A comprehensive grammar of the English language.
Longman, London.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The mean-
ing of the sentence in its semantic and pragmatic as-
pects. Reidel, Dordrecht, The Netherlands.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Applied Natural Language Processing 1997, pages
88?95.
M. Steedman. 2000. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
E. Teich, S. Hansen, and P. Fankhauser. 2001. Repre-
senting and querying multi-layer annotated corpora.
pages 228?237, Philadelphia.
E. Vallduv?? and E. Engdahl. 1996. The linguistic reali-
sation of information packaging. Linguistics, 34:459?
519.
B. L. Webber. 1983. So what can we talk about now?
M.I.T. Press.
B. L. Webber. 1991. Structure and ostension in the in-
terpretation of discourse deixis. Language and Cogni-
tive Processes, 6(2):107?135.
H. Weinrich. 1993. Textgrammatik der deutschen
Sprache. Dudenverlag, Mannheim u.a.
L. Wind. 2002. Manual zur Annotation von anapho-
rischen und Bridging-relationen. European Media
Laboratory GmbH, August 9.
Extensible Dependency Grammar: A New Methodology
Ralph Debusmann
Programming Systems Lab
Saarland University
Postfach 15 11 50
66041 Saarbru?cken
Germany
rade@ps.uni-sb.de
Denys Duchier
?Equipe Calligramme
LORIA ? UMR 7503
Campus Scientifique, B. P. 239
54506 Vand?uvre le`s Nancy CEDEX
France
duchier@loria.fr
Geert-Jan M. Kruijff
Computational Linguistics
Saarland University
Postfach 15 11 50
66041 Saarbru?cken
Germany
gj@coli.uni-sb.de
Abstract
This paper introduces the new grammar formalism
of Extensible Dependency Grammar (XDG), and
emphasizes the benefits of its methodology of ex-
plaining complex phenomena by interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. This has the potential to increase
modularity with respect to linguistic description and
grammar engineering, and to facilitate concurrent
processing and the treatment of ambiguity.
1 Introduction
We introduce the new grammar formalism of Exten-
sible Dependency Grammar (XDG). In XDG, com-
plex phenomena arise out of the interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. In this paper, we point out how
this novel methodology positions XDG in between
multi-stratal approaches like LFG (Bresnan and Ka-
plan, 1982) and MTT (Mel?c?uk, 1988), see also
(Kahane, 2002), and mono-stratal ones like HPSG
(Pollard and Sag, 1994), attempting to combine
their benefits and avoid their problems.
It is the division of linguistic analyses into dif-
ferent dimensions which makes XDG multi-stratal.
On the other, XDG is mono-stratal in that its princi-
ples interact to constrain all dimensions simultane-
ously. XDG combines the benefits of these two po-
sitions, and attempts to circumvent their problems.
From multi-stratal approaches, XDG adopts a high
degree of modularity, both with respect to linguis-
tic description as well as for grammar engineering.
This also facilitates the statement of cross-linguistic
generalizations. XDG avoids the problem of placing
too high a burden on the interfaces, and allows in-
teractions between all and not only adjacent dimen-
sions. From mono-stratal approaches, XDG adopts
a high degree of integration, facilitating concurrent
processing and the treatment of ambiguity. At the
same time, XDG does not lose its modularity.
XDG is a descendant of Topological Depen-
dency Grammar (TDG) (Duchier and Debusmann,
2001), pushing the underlying methodology further
by generalizing it in two aspects:
? number of dimensions: two in TDG (ID and
LP), arbitrary many in XDG
? set of principles: fixed in TDG, extensible
principle library in XDG
The structure of this paper is as follows: In ?2, we
introduce XDG and the XDG solver used for pars-
ing and generation. In ?3, we introduce a number
of XDG principles informally, before making use of
them in an idealized example grammar in ?4. In ?5
we argue why XDG has the potential to be an im-
provement over multi-stratal and mono-stratal ap-
proaches, before we conclude in ?6.
2 Extensible Dependency Grammar
In this section, we introduce XDG formally and
mention briefly the constraint-based XDG solver for
parsing and generation.
2.1 Formalization
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principles, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments
called lexical entries. An analysis on dimension
D is a triple (V,E,F) of a set V of nodes, a set
E ? V ?V ?Lab of directed labeled edges, and an
assignment F : V ? (Fea ? Val) of lexical entries
to nodes. V and E form a graph. We write AnaD for
the set of all possible analyses on dimension D. The
principles characterize subsets of AnaD. We assume
that the elements of Pri are finite representations of
such subsets.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Ana1 ? ?? ? ?
Anan where all dimensions share the same set of
nodes V . We call a dimension of a grammar gram-
mar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex ? Lex1??? ??Lexn con-
strains all dimensions at once, thereby synchroniz-
ing them. An XDG analysis is licensed by Lex iff
(F1(v), . . . ,Fn(v)) ? Lex for every node v ?V .
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are licensed
by Lex, and consistent with Inp and Pri. The input
constraints determine whether XDG solving is to be
used for parsing or generation. For parsing, they
specify a sequence of words, and for generation, a
multiset of semantic literals.
2.2 Solver
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of integers,
where well-formed analyses correspond to the solu-
tions of the CSP (Duchier, 2003). We have imple-
mented an XDG solver using the Mozart-Oz pro-
gramming system.
XDG solving operates on all dimensions concur-
rently. This means that the solver can infer informa-
tion about one dimension from information on an-
other, if there is either a multi-dimensional principle
linking the two dimensions, or by the synchroniza-
tion induced by the lexical entries. For instance, not
only can syntactic information trigger inferences in
syntax, but also vice versa.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an
NP-complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential. The average-case complexity
of many smaller-scale grammars that we have ex-
perimented with seems polynomial, but it remains
to be seen whether we can scale this up to large-
scale grammars.
3 Principles
The well-formedness conditions of XDG analy-
ses are stipulated by principles. Principles are
parametrizable, e.g. by the dimensions on which
they are applied, or by lexical features. They can
be lexicalized or non-lexicalized, and can be one-
dimensional or multi-dimensional. Principles are
taken from an extensible principle library, and we
introduce some of the most important principles in
the following.
3.1 Tree principle
tree(i) The analysis on dimension i must be a tree.
The tree principle is non-lexicalized and
parametrized by the dimension i.
3.2 Dag principle
dag(i) The analysis on dimension i must be a di-
rected acyclic graph.
The dag principle is non-lexicalized and
parametrized by the dimension i.
3.3 Valency principle
valency(i, ini,outi) All nodes on dimension i must
satisfy their in and out specifications.
The valency principle is lexicalized and serves
to lexically describe dependency graphs. It is
parametrized by the dimension i, the in specification
ini and the out specification outi. For each node, ini
stipulates the licensed incoming edges, and outi the
licensed outgoing edges.
In the example grammar lexicon part in Figure 1
below, the in specification is inID and outID is the
out specification on the ID dimension. For the com-
mon noun Roman, the in specification licenses zero
or one incoming edges labeled subj, and zero or one
incoming edges labeled obj ({subj?,obj?}), i.e. it
can be either a subject or an object. The out specifi-
cation requires precisely one outgoing edge labeled
det ({det!}), i.e. it requires a determiner.
3.4 Government principle
government(i,casesi,governi) All edges in dimen-
sion i must satisfy the government specification of
the mother.
The government principle is lexicalized. Its pur-
pose is to constrain the case feature of a depen-
dent.1 It is parametrized by the dimension i, the
cases specification casesi and the government spec-
ification govern. cases assigns to each word a set of
possible cases, and govern a mapping from labels to
sets of cases.
In Figure 1, the cases specification for the deter-
miner den is {acc} (i.e. it can only be accusative).
By its government specification, the finite verb ver-
sucht requires its subject to exhibit nominative case
(subj 7? {nom}).
3.5 Agreement principle
agreement(i,casesi,agreei) All edges in dimension
i must satisfy the agreement specification of the
mother.
1We restrict ourselves to the case feature only for simplicity.
In a fully-fledged grammar, the government principle would be
used to constrain also other morphological aspects like number,
person and gender.
The agreement principle is lexicalized. Its pur-
pose is to enforce the case agreement of a daugh-
ter.2 It is parametrized by dimension i, the lexical
cases specification casesi, assigning to each word a
set of possible cases, and the agreement specifica-
tion agreei, assigning to each word a set of labels.
As an example, in Figure 1, the agreement spec-
ification for the common noun Roman is {det}, i.e.
the case of the common noun must agree with its
determiner.
3.6 Order principle.
order(i,oni,?i) On dimension i, 1) each node must
satisfy its node labels specification, 2) the order of
the daughters of each node must be compatible with
?i, and 3) the node itself must be ordered correctly
with respect to its daughters (using its node label).
The order principle is lexicalized. It is
parametrized by the dimension i, the node labels
specification oni mapping each node to set of labels
from Labi, and the total order ?i on Labi.
Assuming the node labels specification given in
Figure 2, and the total order in (5), the tree in (11)
satisfies the order principle.3 For instance for the
node versucht: 1) The node label of versucht is lbf,
satisfying the node labels specification. 2) The order
of the daughters Roman (under the edge labeled vf),
Peter (mf) and lesen (rbf) is compatible with the
total order prescribing vf ? mf ? rbf. 3) The node
versucht itself is ordered correctly with respect to its
daughters (the total order prescribes vf ? lbf ?mf).
3.7 Projectivity principle
projectivity(i) The analysis on dimension i must be
projective.
The projectivity principle is non-lexicalized. Its
purpose is to exclude non-projective analyses.4 It is
parametrized by dimension i.
3.8 Climbing principle
climbing(i, j) The graph on dimension i must be
flatter than the graph on dimension j.
The climbing principle is non-lexicalized and
two-dimensional. It is parametrized by the two di-
mensions i and j.
For instance, the tree in (11) is flatter than the
corresponding tree in (10). This concept was intro-
duced as lifting in (Kahane et al, 1998).
2Again, we restrict ourselves to case for simplicity.
3The node labels are defined in (2) below.
4The projectivity principle of course only makes sense in
combination with the order principle.
3.9 Linking principle
linking(i, j, linki, j) All edges on dimension i must
satisfy the linking specification of the mother.
The linking principle is lexicalized and two-
dimensional. It is parametrized by the two dimen-
sions i and j, and by the linking specification linki, j ,
mapping labels from Labi to sets of labels from
Lab j . Its purpose is to specify how dependents on
dimension i are realized by (or linked to) dependents
on dimension j.
In the lexicon part in Figure 3, the linking spec-
ification for the transitive verb lesen requires that
its agent on the PA dimension must be realized by a
subject (ag 7? {subj}), and the patient by an object
(pat 7? {obj}).
The linking principle is oriented. Symmetric
linking could be gained simply by using the linking
principle twice (in both directions).
4 Example grammar
In this section, we elucidate XDG with an example
grammar fragment for German. With it, we demon-
strate three aspects of the methodology of XDG:
? How complex phenomena such as topicaliza-
tion and control arise by the interaction of sim-
ple principles on different dimensions of lin-
guistic description.
? How the high degree of integration helps to re-
duce ambiguity.
? How the high degree of modularity facilitates
the statement of cross-linguistic generaliza-
tions.
Note that this grammar fragment is an idealized ex-
ample, and does not make any claims about XDG as
a grammar theory. Its purpose is solely to substan-
tiate our points about XDG as a framework. More-
over, the grammar is fully lexicalized for simplicity.
However, XDG of course allows the grammar writer
to formulate lexical abstractions using inheritance
(like in HPSG) or crossings (Candito, 1996).
4.1 Dimensions
The grammar fragment make use of two dimen-
sions: Immediate Dominance (ID) and Linear
Precedence (LP). The models on the ID dimension
are unordered, syntactic dependency trees whose
edge labels correspond to syntactic functions like
subject and object. On the LP dimension, the mod-
els are ordered, projective topological dependency
trees whose edge labels are topological fields like
Vorfeld and Mittelfeld.
4.2 Labels
The set LabID of labels on the ID dimension is:
LabID = {det,subj,obj,vinf ,part} (1)
These correspond resp. to determiner, subject, ob-
ject, infinitive verbal complement, and particle.
The set LabLP of labels on the LP dimension is:
LabLP = {detf,nounf,vf, lbf,mf,partf, rbf} (2)
Corresponding resp. to determiner field, noun field,
Vorfeld, left bracket field, Mittelfeld, particle field,
and right bracket field.
4.3 Principles
On the ID dimension, we make use of the following
one-dimensional principles:
tree(ID)
valency(ID, inID,outID)
government(ID,casesID,governID)
agreement(ID,casesID,agreeID)
(3)
The LP dimension uses the following principles:
tree(LP)
valency(LP, inLP,outLP)
order(LP,onLP,?LP)
projectivity(LP)
(4)
where the total order ?LP is defined as:
detf ? nounf ? vf ? lbf ? mf ? partf ? rbf (5)
We make use of the following multi-dimensional
principles:
climbing(LP, ID)
linking(LP, ID) (6)
4.4 Lexicon
We split the lexicon into two parts. The ID and LP
parts are displayed resp. in Figure 15 and Figure 2.
The LP part includes also the linking specification
for the LP,ID-application of the linking principle.6
4.5 Government and agreement
Our first example is the following sentence:
Peter versucht einen Roman zu lesen.
Peter tries aacc novel to read.
Peter tries to read a novel.
(7)
We display the ID analysis of the sentence below:
.
Peter versucht einen Roman zu lesen
subj vinf
partobj
det
(8)
5Here, stands for ?don?t care?, this means e.g. for the verb
versucht that it has unspecified case.
6We do not make use of the linking specification for the
German grammar fragment (the mappings are all empty), but
we will do so as we switch to Dutch in ?4.8 below.
Here, Peter is the subject of versucht. lesen is the in-
finitival verbal complement of versucht, zu the parti-
cle of lesen, and Roman the object of lesen. Finally,
einen is the determiner of Roman.
Under our example grammar, the sentence is un-
ambiguous, i.e. the given ID tree is the only possible
one. Other ID trees are ruled out by the interaction
of the principles on the ID dimension. For instance,
the government and agreement principles conspire
to rule out the reading where Roman is the subject of
versucht (and Peter the object). How? By the agree-
ment principle, Roman must be accusative, since it
agrees with its accusative determiner einen. By the
government principle, the subject of versucht must
be nominative, and the object of lesen accusative.
Thus Roman, by virtue of being accusative, cannot
become the subject of versucht. The only other op-
tion for it is to become the object of lesen. Conse-
quently, Peter, which is unspecified for case, must
become the subject of versuchen (versuchen must
have a subject by the valency principle).
4.6 Topicalization
Our second example is a case of topicalization,
where the object has moved into the Vorfeld, to the
left of the finite verb:
Einen Roman versucht Peter zu lesen. (9)
Here is the ID tree and the LP tree analysis:
.
Einen Roman versucht Peter zu lesen
subj vinf
partobj
det
(10)
.
Einen Roman versucht Peter zu lesen
detf
nounf
lbf
nounf
partf
rbf
vf
detf
mf rbf
part
f
(11)
The ID tree analysis is the same as before, except
that the words are shown in different positions. In
the LP tree, Roman is in the Vorfeld of versucht, Pe-
ter in the Mittelfeld, and lesen in the right bracket
field. versucht itself is (by its node label) in the left
bracket field. Moreover, Einen is in the determiner
field of Roman, and zu in the particle field of lesen.
Again, this is an example demonstrating how
complex phenomena (here: topicalization) are ex-
plained by the interaction of simple principles. Top-
icalization does not have to explicitly taken care of,
it is rather a consequence of the interacting princi-
ples. Here, the valency, projectivity and climbing
inID outID casesID governID agreeID
den {det?} {} {acc} {} {}
Roman {subj?,obj?} {det!} {nom,dat,acc} {} {det}
Peter {subj?,obj?} {} {nom,dat,acc} {} {}
versucht {} {subj!,vinf!} {subj 7? {nom}} {}
zu {part?} {} {} {}
lesen {vinf?} {obj!} {obj 7? {acc}} {}
Figure 1: Lexicon for the example grammar fragment, ID part
inLP outLP onLP linkLP,ID
den {detf?} {} {detf} {}
Roman {vf?,mf?} {detf!} {nounf} {}
Peter {vf?,mf?} {} {nounf} {}
versucht {} {vf?,mf?, rbf?} {lbf} {}
zu {partf?} {} {partf} {}
lesen {rbf?} {} {rbf} {}
Figure 2: Lexicon for the example grammar fragment, LP part
principles conspire to bring about the ?climbing up?
of the NP Einen Roman from being the daughter of
lesen in the ID tree to being the daughter of versucht
in the LP tree: The out specification of lesen does
not license any outgoing edge. Hence, Roman must
become the daughter of another node. The only pos-
sibility is versucht. The determiner Einen must then
also ?climb up? because Roman is its only possi-
ble mother. The result is an LP tree which is flat-
ter with respect to the ID tree. The LP tree is also
projective. If it were not be flatter, then it would
be non-projective, and ruled out by the projectivity
principle.
4.7 Negative example
Our third example is a negative example, i.e. an un-
grammatical sentence:
?Peter einen Roman versucht zu lesen. (12)
This example is perfectly legal on the unordered ID
dimension, but has no model on the LP dimension.
Why? Because by its LP out specification, the finite
verb versucht allows only one dependent to the left
of it (in its Vorfeld), and here we have two. The
interesting aspect of this example is that although
we can find a well-formed ID tree for it, this ID tree
is never actually generated. The interactions of the
principles, viz. here of the principles on the LP di-
mension, rule out the sentence before any full ID
analysis has been found.
4.8 From German to Dutch
For the fourth example, we switch from German to
Dutch. We will show how to use the lexicon to con-
cisely capture an important cross-linguistic general-
ization. We keep the same grammar as before, but
with two changes, arising from the lesser degree of
inflection and the higher reliance on word order in
Dutch:
? The determiner een is not case-marked but
can be either nominative, dative or accusative:
casesID = {nom,dat,acc}.
? The Vorfeld of the finite verb probeert cannot
be occupied by an object (but only by an ob-
ject): linkLP,ID = {vf 7? {subj}}.7
Now to the example, a Dutch translation of (7):
Peter probeert een roman te lezen.
Peter tries a novel to read.
Peter tries to read a novel.
(13)
We get only one analysis on the ID dimension,
where Peter is the subject and roman the object.
An analysis where Peter is the object of lezen and
roman the subject of probeert is impossible, as in
the German example. The difference is, however,
how this analysis is excluded. In German, the ac-
cusative inflection of the determiner einen triggered
the agreement and the government principle to rule
it out. In Dutch, the determiner is not inflected.
The unwanted analysis is excluded on the grounds
of word order instead: By the linking principle, the
Vorfeld of probeert must be filled by a subject, and
not by an object. That means that Peter in the Vor-
feld (to the left of probeert) must be a subject, and
consequently, the only other choice for roman is that
it becomes the object of lezen.
4.9 Predicate-Argument Structure
Going towards semantics, we extend the grammar
with another dimension, Predicate-Argument Struc-
ture (PA), where the models are not trees but di-
rected acyclic graphs (dags), to model re-entrancies
7Of course, this is an idealized assumption. In fact, given
the right stress, the Dutch Vorfeld can be filled by objects.
e.g. caused by control constructions. Thanks to the
modularity of XDG, the PA part of the grammar is
the same for German and Dutch.
The set LabPA of labels on the PA dimension is:
LabPA = {ag,pat,prop} (14)
Corresponding resp. to agent, patient and proposi-
tion.
The PA dimension uses the following one-
dimensional principles:
dag(PA)
valency(PA, inPA,outPA) (15)
Note that we re-use the valency principle again, as
we did on the ID and LP dimensions.
And also the following multi-dimensional princi-
ples:
climbing(ID, PA)
linking(PA, ID) (16)
Here, we re-use the climbing and linking princi-
ples. That is, we state that the ID tree is flatter
than the corresponding PA dag. This captures rais-
ing and control, where arguments of embedded infi-
nite verbs can ?climb up? and become arguments of
a raising or control verb, in the same way as syntac-
tic arguments can ?climb up? from ID to LP. We use
the linking principle to specify how semantic argu-
ments are to be realized syntactically (e.g. the agent
as a subject etc.). We display the PA part of the lex-
icon in Figure 3.8
Here is an example PA dag analysis of example
sentence (7):
.
Peter versucht einen Roman zu lesen
ag
prop
patag
(17)
Here, Peter is the agent of versucht, and also the
agent of lesen. Furthermore, lesen is a proposition
dependent of versucht, and Roman is the patient of
lesen.
Notice that the PA dag is indeed a dag and not a
tree since Peter has two incoming edges: It is simul-
taneously the agent of versucht and of lesen. This
is enforced by by the valency principle: Both ver-
sucht and lesen require an agent. Peter is the only
word which can be the agent of both, because it is a
subject and the agents of versucht and lesen must
be subjects by the linking principle. The climb-
ing principle ensures that predicate arguments can
8Notice that we specify linking lexically, allowing us to
capture deviations from the typical linking patterns. Still, we
can also accommodate linking generalizations using lexical ab-
stractions.
be ?raised? on the ID structure with respect to the
PA structure. Again, this example demonstrates that
XDG is able to reduce a complex phenomenon such
as control to the interaction of per se fairly simple
principles such as valency, climbing and linking.
5 Comparison
This section includes a more in-depth comparison
of XDG with purely multi- and mono-stratal ap-
proaches.
Contrary to multi-stratal approaches like LFG or
MTT, XDG is more integrated. For one, it places
a lighter burden the interfaces between the dimen-
sions. In LFG for instance, the ? -mapping from c-
structure to f-structure is rather specific, and has to
be specifically adapted to new c-structures, e.g. in
order to handle a new construction with a different
word order. That is, not only the grammar rules for
the c-structure need to be adapted, but also the inter-
face between c- and f-structure. In XDG, complex
phenomena arise out of the interaction of simple,
maximally general principles. To accommodate the
new construction, the grammar would ideally only
need to be adapted on the word order dimension.
Furthermore, XDG allows interactions of rela-
tional constraints between all dimensions, not only
between adjacent ones (like c- and f-structure),
and in all directions. For one, this gets us bi-
directionality for free. Secondly, the interactions
of XDG have the potential to help greatly in reduc-
ing ambiguity. In multi-stratal approaches, ambigu-
ity must be duplicated throughout the system. E.g.
suppose there are two candidate c-structures in LFG
parsing, but one is ill-formed semantically. Then
they can only be ruled out after duplicating the am-
biguity on the f-structure, and then filtering out the
ill-formed structure on the semantic ? -structure. In
XDG on the other hand, the semantic principles can
rule out the ill-formed analysis much earlier, typ-
ically on the basis of a partial syntactic analysis.
Thus, ill-formed analyses are never duplicated.
Contrary to mono-stratal ones, XDG is more
modular. For one, as (Oliva et al, 1999) note,
mono-stratal approaches like HPSG usually give
precedence to the syntactic tree structure, while
putting the description of other aspects of the anal-
ysis on the secondary level only, by means of fea-
tures spread over the nodes of the tree. As a result,
it becomes a hard task to modularize grammars. Be-
cause syntax is privileged, the phenomena ascribing
to semantics cannot be described independently, and
whenever the syntax part of the grammar changes,
the semantics part needs to be adapted. In XDG, no
dimension is privileged to another. Semantic phe-
inPA outPA linkPA,ID
den {} {} {}
Roman {ag?,pat?} {} {}
Peter {ag?,pat?} {} {}
versucht {} {ag!,prop!} {ag 7? {subj},prop 7? {vinf}}
zu {} {} {}
lesen {prop?} {ag!,pat!} {ag 7? {subj},pat 7? {obj}}
Figure 3: Lexicon of the example grammar fragment, PA part
nomena can be described much more independently
from syntax. This facilitates grammar engineering,
and also the statement of cross-linguistic general-
izations. Assuming that the semantics part of a
grammar stay invariant for most natural languages,
in order to accommodate a new language, ideally
only the syntactic parts would need to be changed.
6 Conclusion
In this paper, we introduced the XDG grammar
framework, and emphasized that its new methodol-
ogy places it in between the extremes of multi- and
mono-stratal approaches. By means of an idealized
example grammar, we demonstrated how complex
phenomena are explained as arising from the in-
teraction of simple principles on numerous dimen-
sions of linguistic description. On the one hand, this
methodology has the potential to modularize lin-
guistic description and grammar engineering, and
to facilitate the statement of linguistic generaliza-
tions. On the other hand, as XDG is a inherently
concurrent architecture, inferences from any dimen-
sion can help reduce the ambiguity on others.
XDG is a new grammar formalism, and still has
many open issues. Firstly, we need to continue work
on XDG as a framework. Here, one important goal
is to find out what criteria we can give to restrict the
principles. Secondly, we need to evolve the XDG
grammar theory, and in particular the XDG syntax-
semantics interface. Thirdly, for practical use, we
need to improve our knowledge about XDG solv-
ing (i.e. parsing and generation). So far, our only
good results are for smaller-scale handwritten gram-
mars, and we have not good results yet for larger-
scale grammars induced from treebanks (NEGRA,
PDT) or converted from other grammar formalisms
(XTAG). Finally, we need to incorporate statistics
into the picture, e.g. to guide the search for solu-
tions, in the vein of (Dienes et al, 2003).
References
Joan Bresnan and Ronald Kaplan. 1982. Lexical-
functional grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 173?281. The MIT Press, Cam-
bridge/USA.
Marie-He`le?ne Candito. 1996. A principle-based hi-
erarchical representation of LTAG. In Proceed-
ings of COLING 1996, Kopenhagen/DEN.
Peter Dienes, Alexander Koller, and Marco
Kuhlmann. 2003. Statistical A* Dependency
Parsing. In Prospects and Advances in the Syn-
tax/Semantics Interface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001.
Topological dependency trees: A constraint-
based account of linear precedence. In Proceed-
ings of ACL 2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled
trees under lexicalized constraints and principles.
Research on Language and Computation, 1(3?
4):307?336.
Sylvain Kahane, Alexis Nasr, and Owen Ram-
bow. 1998. Pseudo-projectivity: a polynomi-
ally parsable non-projective dependency gram-
mar. In 36th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 1998),
Montre?al/CAN.
Sylvain Kahane. 2002. Grammaire d?Unification
Sens-Texte: Vers un mode`le mathe?matique ar-
ticule? de la langue. Universite? Paris 7. Docu-
ment de synthe`se de l?habilitation a` diriger les
recherches.
Alexander Koller and Kristina Striegnitz. 2002.
Generation as dependency parsing. In Proceed-
ings of ACL 2002, Philadelphia/USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Karel Oliva, M. Andrew Moshier, and Sabine
Lehmann. 1999. Grammar engineering for the
next millennium. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium
1999 ?Closing the Millennium?, Beijing/CHI.
Tsinghua University Press.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago/USA.
  	
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 65?72,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Studying Feature Generation from Various Data Representations for 
Answer Extraction 
 
Dan Shen?? Geert-Jan M. Kruijff? Dietrich Klakow? 
? Department of Computational Linguistics 
Saarland University 
Building 17,Postfach 15 11 50 
66041 Saarbruecken, Germany 
? Lehrstuhl Sprach Signal Verarbeitung 
Saarland University 
Building 17, Postfach 15 11 50 
66041 Saarbruecken, Germany 
{dshen,gj}@coli.uni-sb.de 
{dietrich.klakow}@lsv.uni-saarland.de 
 
 
Abstract 
In this paper, we study how to generate 
features from various data representations, 
such as surface texts and parse trees, for 
answer extraction.  Besides the features 
generated from the surface texts, we 
mainly discuss the feature generation in 
the parse trees.  We propose and compare 
three methods, including feature vector, 
string kernel and tree kernel, to represent 
the syntactic features in Support Vector 
Machines.  The experiment on the TREC 
question answering task shows that the 
features generated from the more struc-
tured data representations significantly 
improve the performance based on the 
features generated from the surface texts.  
Furthermore, the contribution of the indi-
vidual feature will be discussed in detail. 
1 Introduction 
Open domain question answering (QA), as defined 
by the TREC competitions (Voorhees, 2003), 
represents an advanced application of natural lan-
guage processing (NLP).  It aims to find exact an-
swers to open-domain natural language questions 
in a large document collection.  For example: 
Q2131: Who is the mayor of San Francisco? 
Answer: Willie Brown 
A typical QA system usually consists of three 
basic modules: 1. Question Processing (QP) Mod-
ule, which finds some useful information from the 
questions, such as expected answer type and key 
words.  2. Information Retrieval (IR) Module, 
which searches a document collection to retrieve a 
set of relevant sentences using the question key 
words.  3. Answer Extraction (AE) Module, which 
analyzes the relevant sentences using the informa-
tion provided by the QP module and identify the 
answer phrase. 
In recent years, QA systems trend to be more 
and more complex, since many other NLP tech-
niques, such as named entity recognition, parsing, 
semantic analysis, reasoning, and external re-
sources, such as WordNet, web, databases, are in-
corporated.  The various techniques and resources 
may provide the indicative evidences to find the 
correct answers.  These evidences are further com-
bined by using a pipeline structure, a scoring func-
tion or a machine learning method. 
In the machine learning framework, it is critical 
but not trivial to generate the features from the 
various resources which may be represented as 
surface texts, syntactic structures and logic forms, 
etc.  The complexity of feature generation strongly 
depends on the complexity of data representation.  
Many previous QA systems (Echihabi et al, 2003; 
Ravichandran, et al, 2003; Ittycheriah and Roukos, 
2002; Ittycheriah, 2001; Xu et al, 2002) have well 
studied the features in the surface texts.  In this 
paper, we will use the answer extraction module of 
QA as a case study to further explore how to gen-
erate the features for the more complex sentence 
representations, such as parse tree.  Since parsing 
gives the deeper understanding of the sentence, the 
features generated from the parse tree are expected 
to improve the performance based on the features 
generated from the surface text.  The answer ex-
65
traction module is built using Support Vector Ma-
chines (SVM).  We propose three methods to rep-
resent the features in the parse tree: 1. features are 
designed by domain experts, extracted from the 
parse tree and represented as a feature vector; 2. 
the parse tree is transformed to a node sequence 
and a string kernel is employed; 3. the parse tree is 
retained as the original representation and a tree 
kernel is employed. 
Although many textual features have been used 
in the others? AE modules, it is not clear that how 
much contribution the individual feature makes.  In 
this paper, we will discuss the effectiveness of 
each individual textual feature in detail.  We fur-
ther evaluate the effectiveness of the syntactic fea-
tures we proposed.  Our experiments using TREC 
questions show that the syntactic features improve 
the performance by 7.57 MRR based on the textual 
features.  It indicates that the new features based 
on a deeper language understanding are necessary 
to push further the machine learning-based QA 
technology.  Furthermore, the three representations 
of the syntactic features are compared.  We find 
that keeping the original data representation by 
using the data-specific kernel function in SVM 
may capture the more comprehensive evidences 
than the predefined features.  Although the features 
we generated are specific to the answer extraction 
task, the comparison between the different feature 
representations may be helpful to explore the syn-
tactic features for the other NLP applications. 
2 Related Work 
In the machine learning framework, it is crucial to 
capture the useful evidences for the task and inte-
grate them effectively in the model.  Many re-
searchers have explored the rich textual features 
for the answer extraction. 
IBM (Ittycheriah and Roukos, 2002; Ittycheriah, 
2001) used a Maximum Entropy model to integrate 
the rich features, including query expansion fea-
tures, focus matching features, answer candidate 
co-occurrence features, certain word frequency 
features, named entity features, dependency rela-
tion features, linguistic motivated features and sur-
face patterns.  ISI?s (Echihabi et al 2003; Echihabi 
and Marcu, 2003) statistical-based AE module im-
plemented a noisy-channel model to explain how a 
given sentence tagged with an answer can be re-
written into a question through a sequence of sto-
chastic operations.  (Ravichandran et al, 2003) 
compared two maximum entropy-based QA sys-
tems, which view the AE as a classification prob-
lem and a re-ranking problem respectively, based 
on the word frequency features, expected answer 
class features, question word absent features and 
word match features.  BBN (Xu et al 2002) used a 
HMM-based IR system to score the answer candi-
dates based on the answer contexts.  They further 
re-ranked the scored answer candidates using the 
constraint features, such as whether a numerical 
answer quantifies the correct noun, whether the 
answer is of the correct location sub-type and 
whether the answer satisfies the verb arguments of 
the questions.  (Suzuki et al 2002) explored the 
answer extraction using SVM. 
However, in the previous statistical-based AE 
modules, most of the features were extracted from 
the surface texts which are mainly based on the 
key words/phrases matching and the key word fre-
quency statistics.  These features only capture the 
surface-based information for the proper answers 
and may not provide the deeper understanding of 
the sentences.  In addition, the contribution of the 
individual feature has not been evaluated by them.  
As for the features extracted from the structured 
texts, such as parse trees, only a few works ex-
plored some predefined syntactic relation features 
by partial matching.  In this paper, we will explore 
the syntactic features in the parse trees and com-
pare the different feature representations in SVM.  
Moreover, the contributions of the different fea-
tures will be discussed in detail. 
3 Answer Extraction 
Given a question Q and a set of relevant sentences 
SentSet which is returned by the IR module, we 
consider all of the base noun phrases and the words 
in the base noun phrases as answer candidates aci.  
For example, for the question ?Q1956: What coun-
try is the largest in square miles??, we extract the 
answer candidates { Russia, largest country, larg-
est, country, world, Canada, No.2.} in the sentence 
?I recently read that Russia is the largest country 
in the world, with Canada No. 2.?  The goal of the 
AE module is to choose the most probable answer 
from a set of answer candidates 1 2{ , ,... }mac ac ac  
for the question Q. 
We regard the answer extraction as a classifica-
tion problem, which classify each question and 
66
answer candidate pair <Q, aci> into the positive 
class (the correct answer) and the negative class 
(the incorrect answer), based on some features.  
The predication for each <Q, aci> is made inde-
pendently by the classifier, then, the ac with the 
most confident positive prediction is chosen as the 
answer for Q.  SVM have shown the excellent per-
formance for the binary classification, therefore, 
we employ it to classify the answer candidates.   
Answer extraction is not a trivial task, since it 
involves several components each of which is 
fairly sophisticated, including named entity recog-
nition, syntactic / semantic parsing, question analy-
sis, etc.  These components may provide some 
indicative evidences for the proper answers.  Be-
fore generating the features, we process the sen-
tences as follows: 
1. tag the answer sentences with named entities. 
2. parse the question and the answer sentences us-
ing the Collins? parser (Collin, 1996). 
3. extract the key words from the questions, such 
as the target words, query words and verbs. 
In the following sections, we will briefly intro-
duce the machine learning algorithm.  Then, we 
will discuss the features in detail, including the 
motivations and representations of the features. 
4 Support Vector Machines  
Support Vector Machines (SVM) (Vapnik, 1995) 
have strong theoretical motivation in statistical 
learning theory and achieve excellent generaliza-
tion performance in many language processing 
applications, such as text classification (Joachims, 
1998). 
SVM constructs a binary classifier that predict 
whether an instance x ( n?w R ) is positive 
( ( ) 1f =x ) or negative ( ( ) 1f = ?x ), where, an 
instance may be represented as a feature vector or 
a structure like sequence of characters or tree.  In 
the simplest case (linearly separable instances), the 
decision f( ) sgn( b )? +x = w x is made based 
on a separating hyperplane 0b? + =w x  ( n?w R , 
b?R ).  All instances lying on one side of the hy-
perplane are classified to a positive class, while 
others are classified to a negative class. 
Given a set of labeled training instances 
( ) ( ) ( ){ }1 1 2 2, , , ,..., ,m mD y y y= x x x , where ni ?x R  
and { }1, 1iy = ? , SVM is to find the optimal hy-
perplane that separates the positive and negative 
training instances with a maximal margin.  The 
margin is defined as the distance from the separat-
ing hyperplane to the closest positive (negative) 
training instances.  SVM is trained by solving a 
dual quadratic programming problem. 
Practically, the instances are non-linearly sepa-
rable.  For this case, we need project the instances 
in the original space Rn to a higher dimensional 
space RN based on the kernel function 
1 2 1 2( , ) ( ), ( )K =<? ? >x x x x ,where, ( ): n N? ?x R R  is 
a project function of the instance.  By this means, a 
linear separation will be made in the new space.  
Corresponding to the original space Rn, a non-
linear separating surface is found.  The kernel 
function has to be defined based on the Mercer?s 
condition.  Generally, the following kernel func-
tions are widely used. 
Polynomial kernel: ( , ) ( 1) pi j i jk = ? +x x x x  
Gaussian RBF kernel:  
2 22( , ) i j-i jk e
??= x xx x  
5 Textual Features 
Since the features extracted from the surface texts 
have been well explored by many QA systems 
(Echihabi et al, 2003; Ravichandran, et al, 2003; 
Ittycheriah and Roukos, 2002; Ittycheriah, 2001; 
Xu et al, 2002), we will not focus on the textual 
feature generation in this paper.  Only four types of 
the basic features are used: 
1. Syntactic Tag Features: the features capture 
the syntactic/POS information of the words in 
the answer candidates.  For the certain ques-
tion, such as ?Q1903: How many time zones 
are there in the world??, if the answer candi-
date consists of the words with the syntactic 
tags ?CD NN?, it is more likely to be the 
proper answer. 
2. Orthographic Features: the features capture 
the surface format of the answer candidates, 
such as capitalization, digits and lengths, etc.  
These features are motivated by the observa-
tions, such as, the length  of the answers are 
often less than 3 words for the factoid ques-
tions; the answers may not be the subse-
quences of the questions; the answers often 
contain digits for the certain questions. 
3. Named Entity Features: the features capture 
the named entity information of the answer 
67
candidates.  They are very effective for the 
who, when and where questions, such as, For 
?Q1950: Who created the literary character 
Phineas Fogg??, the answer ?Jules Verne? is 
tagged as a PERSON name in the sentences 
?Jules Verne 's Phileas Fogg made literary 
history when he traveled around the world in 
80 days in 1873.?.  For the certain question tar-
get, if the answer candidate is tagged as the 
certain type of named entity, one feature fires. 
4. Triggers: some trigger words are collected for 
the certain questions.  For examples, for 
?Q2156: How fast does Randy Johnson 
throw??, the trigger word ?mph? for the ques-
tion words ?how fast? may help to identify the 
answer ?98-mph? in ?Johnson throws a 98-
mph fastball?. 
6 Syntactic Features 
In this section, we will discuss the feature genera-
tion in the parse trees.  Since parsing outputs the 
highly structured data representation of the sen-
tence, the features generated from the parse trees 
may provide the more linguistic-motivated expla-
nation for the proper answers.  However, it is not 
trivial to find the informative evidences from a 
parse tree. 
The motivation of the syntactic features in our 
task is that the proper answers often have the cer-
tain syntactic relations with the question key words.  
Table 1 shows some examples of the typical syn-
tactic relations between the proper answers (a) and 
the question target words (qtarget).  Furthermore, 
the syntactic relations between the answers and the 
different types of question key words vary a lot.  
Therefore, we capture the relation features for the 
different types of question words respectively.  The 
question words are divided into four types: 
z Target word, which indicates the expected an-
swer type, such as ?city? in ?Q: What city is 
Disneyland in??. 
z Head word, which is extracted from how ques-
tions and indicates the expected answer head, 
such as ?dog? in ?Q210: How many dogs 
pull ??? 
z Subject words, which are the base noun phrases 
of the question except the target word and the 
head word. 
z Verb, which is the main verb of the question. 
To our knowledge, the syntactic relation fea-
tures between the answers and the question key 
words haven?t been explored in the previous ma-
chine learning-based QA systems.  Next, we will 
propose three methods to represent the syntactic 
relation features in SVM. 
6.1 Feature Vector 
It is the commonly used feature representation in 
most of the machine learning algorithms.  We pre-
define a set of syntactic relation features, which is 
an enumeration of some useful evidences of the 
answer candidates (ac) and the question key words 
in the parse trees.  20 syntactic features are manu-
ally designed in the task.  Some examples of the 
features are listed as follows, 
z if the ac node is the same of the qtarget node, 
one feature fires. 
z if the ac node is the sibling of the qtarget node, 
one feature fires. 
z if the ac node the child of the qsubject node, 
one feature fires. 
The limitation of the manually designed features is 
that they only capture the evidences in the local 
context of the answer candidates and the question 
key words.  However, some question words, such 
as subject words, often have the long range syntac-
1. a node is the same as the qtarget node and qtarget is the hypernym of a. 
Q: What city is Disneyland in? 
S: Not bad for a struggling actor who was working at Tokyo Disneyland a few years ago. 
2. a node is the parent of qtarget node. 
Q: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort Worth International Airport was 81 degrees. 
3. a node is the sibling of the qtarget node. 
Q: What book did Rachel Carson write in 1962? 
S: In her 1962 book Silent Spring, Rachel Carson, a marine biologist, chronicled DDT 's poisonous effects, ?. 
Table 1: Examples of the typical relations between answer and question target word.  In Q, the italic word is 
question target word.  In S, the italic word is the question target word which is mapped in the answer sentence; 
the underlined word is the proper answer for the question Q. 
68
Figure 1: An example of the path from the answer 
candidate node to the question subject word node 
tic relations with the answers.  To overcome the 
limitation, we will propose some special kernels 
which may keep the original data representation 
instead of explicitly enumerate the features, to ex-
plore a much larger feature space. 
6.2 String Kernel 
The second method represents the syntactic rela-
tion as a linked node sequence and incorporates a 
string kernel in SVM to handle the sequence. 
We extract a path from the node of the answer 
candidate to the node of the question key word in 
the parse tree.  The path is represented as a node 
sequence linked by symbols indicating upward or 
downward movement through the tree. For exam-
ple, in Figure 1, the path from the answer candi-
date node ?211,456 miles? to the question subject 
word node ?the moon? is 
? NPB ADVP VP S NPB? ? ? ? ?, where ? ? ? and 
? ? ? indicate upward movement and downward 
movement in the parse tree.  By this means, we 
represent the object from the original parse tree to 
the node sequence.  Each character of the sequence 
is a syntactic/POS tag of the node.  Next, a string 
kernel will be adapted to our task to calculate the 
similarity between two node sequences. 
 
 
 
 
 
(Haussler, 1999) first described a convolution 
kernel over the strings.  (Lodhi et al, 2000) applied 
the string kernel to the text classification.  (Leslie 
et al, 2002) further proposed a spectrum kernel, 
which is simpler and more efficient than the previ-
ous string kernels, for protein classification prob-
lem.  In their tasks, the string kernels achieved the 
better performance compared with the human-
defined features. 
The string kernel is to calculate the similarity 
between two strings.  It is based on the observation 
that the more common substrings the strings have, 
the more similar they are.  The string kernel we 
used is similar to (Leslie et al, 2002).  It is defined 
as the sum of the weighted common substrings.  
The substring is weighted by an exponentially de-
caying factor ? (set 0.5 in the experiment) of its 
length k.  For efficiency, we only consider the sub-
strings which length are less than 3.  Different 
from (Leslie et al, 2002), the characters (syntac-
tic/POS tag) of the string are linked with each 
other.  Therefore, the matching between two sub-
strings will consider the linking information.  Two 
identical substrings will not only have the same 
syntactic tag sequences but also have the same 
linking symbols.  For example, for the node se-
quences NP VP VP S NP? ? ? ?  and NP NP VP NP? ? ? , 
there is a matched substring (k = 2): NP VP? . 
6.3 Tree Kernel 
The third method keeps the original representation 
of the syntactic relation in the parse tree and incor-
porates a tree kernel in SVM. 
Tree kernels are the structure-driven kernels to 
calculate the similarity between two trees.  They 
have been successfully accepted in the NLP appli-
cations.  (Collins and Duffy, 2002) defined a ker-
nel on parse tree and used it to improve parsing.  
(Collins, 2002) extended the approach to POS tag-
ging and named entity recognition.  (Zelenko et al, 
2003; Culotta and Sorensen, 2004) further ex-
plored tree kernels for relation extraction. 
We define an object (a relation tree) as the 
smallest tree which covers one answer candidate 
node and one question key word node.  Suppose 
that a relation tree T has nodes 0 1{ , , ..., }nt t t  and 
each node it is attached with a set of attrib-
utes 0 1{ , , ..., }ma a a , which represents the local char-
acteristics of ti .  In our task, the set of the 
attributes includes Type attributes, Orthographic 
attributes and Relation Role attributes, as shown in 
Table 2.  The core idea of the tree kernel ( , )1 2K T T  
is that the similarity between two trees T1 and T2 is 
PUNC
. away 221,456 miles 
S 
PP NPB VP 
VBZ ADVP
NPB RB 
the moon 
is 
Q1980: How far is the moon from Earth in miles? 
S: At its perigee, the closest approach to Earth , the 
moon is 221,456 miles away. 
?? 
69
T1_ac#target 
T2_ac#target 
Q1897: What is the name of the airport in Dallas Ft. Worth? 
S: Wednesday morning, the low temperature at the Dallas-Fort 
Worth International Airport was 81 degrees. 
t4t3 t2
T: BNP 
O: null  
R1: true 
R2: false 
t1
Dallas-Fort
T: NNP 
O: CAPALL 
R1: false 
R2: false
International 
T: JJ 
O: CAPALL  
R1: false 
R2: false 
Airport 
T: NNP 
O: CAPALL 
R1: false 
R2: true
Q35: What is the name of the highest mountain in Africa? 
S: Mount Kilimanjaro, at 19,342 feet, is Africa's highest moun-
tain, and is 5,000 feet higher than ?. 
Mount 
T: NNP 
O: CAPALL 
R1: false 
R2: true
Kilimanjaro
T: NNP 
O: CAPALL 
R1: false 
R2: false 
T: BNP 
O: null  
R1: true 
R2: false 
t0 
w0 
w1 w2 
Worth 
T: NNP 
O: CAPALL 
R1: false 
R2: false
the sum of the similarity between their subtrees.  It 
is calculated by dynamic programming and cap-
tures the long-range syntactic relations between 
two nodes.  The kernel we use is similar to (Ze-
lenko et al, 2003) except that we define a task-
specific matching function and similarity function, 
which are two primitive functions to calculate the 
similarity between two nodes in terms of their at-
tributes. 
Matching function 
1 if . .  and . .   
( , )
0 otherwise                                           
i j i j
i j
t type t type t role t role
m t t
= == ???  
Similarity function 
0{ ,..., }
( , ) ( . , . )i j i j
ma a a
s t t f t a t a
?
= ?  
where, ( . , . )i jf t a t a  is a compatibility function be-
tween two feature values 
. .
( . , . )
1   if 
0   otherwise
i j
i j
t a t a
f t a t a =
=???  
Figure 2 shows two examples of the relation tree 
T1_ac#targetword and T2_ac#targetword.  The 
kernel we used matches the following pairs of the 
nodes <t0, w0>, <t1, w2>, <t2, w2> and <t4, w1>. 
 
Attributes Examples 
POS tag CD, NNP, NN?Type 
syntactic tag NP, VP, ? 
Is Digit? DIG, DIGALL 
Is Capitalized? CAP, CAPALL 
Ortho-
graphic  
length of phrase LNG1, LNG2#3, 
LNGgt3 
Role1 Is answer candidate? true, false 
Role2 Is question key words? true, false 
Table 2: Attributes of the nodes 
7 Experiments 
We apply the AE module to the TREC QA task.  
To evaluate the features in the AE module inde-
pendently, we suppose that the IR module has got 
100% precision and only passes those sentences 
containing the proper answers to the AE module.  
The AE module is to identify the proper answers 
from the given sentence collection. 
We use the questions of TREC8, 9, 2001 and 
2002 for training and the questions of TREC2003 
for testing.  The following steps are used to gener-
ate the data: 
1. Retrieve the relevant documents for each ques-
tion based on the TREC judgments. 
2. Extract the sentences, which match both the 
proper answer and at least one question key word, 
from these documents. 
3. Tag the proper answer in the sentences based on 
the TREC answer patterns 
 
Figure 2: Two objects representing the relations be-
tween answer candidates and target words. 
 
In TREC 2003, there are 413 factoid questions 
in which 51 questions (NIL questions) are not re-
turned with the proper answers by TREC.  Accord-
ing to our data generation process, we cannot 
provide data for those NIL questions because we 
cannot get the sentence collections.  Therefore, the 
AE module will fail on all of the NIL questions 
and the number of the valid questions should be 
362 (413 ? 51).  In the experiment, we still test the 
module on the whole question set (413 questions) 
to keep consistent with the other?s work.  The 
training set contains 1252 questions.  The perform-
ance of our system is evaluated using the mean 
reciprocal rank (MRR).  Furthermore, we also list 
the percentages of the correct answers respectively 
70
in terms of the top 5 answers and the top 1 answer 
returned.  We employ the SVMLight (Joachims, 
1999) to incorporate the features and classify the 
answer candidates.  No post-processes are used to 
adjust the answers in the experiments. 
Firstly, we evaluate the effectiveness of the tex-
tual features, described in Section 5.  We incorpo-
rate them into SVM using the three kernel 
functions: linear kernel, polynomial kernel and 
RBF kernel, which are introduced in Section 4.  
Table 3 shows the performance for the different 
kernels.  The RBF kernel (46.24 MRR) signifi-
cantly outperforms the linear kernel (33.72 MRR) 
and the polynomial kernel (40.45 MRR).  There-
fore, we will use the RBF kernel in the rest ex-
periments. 
 Top1 Top5 MRR 
linear 31.28 37.91 33.72 
polynomial 37.91 44.55 40.45 
RBF 42.67 51.58 46.24 
Table 3: Performance for kernels 
 
In order to evaluate the contribution of the indi-
vidual feature, we test out module using different 
feature combinations, as shown in Table 4.  Sev-
eral findings are concluded: 
1. With only the syntactic tag features Fsyn., the 
module achieves a basic level MRR of 31.38.  The 
questions ?Q1903: How many time zones are there 
in the world?? is correctly answered from the sen-
tence ?The world is divided into 24 time zones.?.  
2. The orthographic features Forth. show the posi-
tive effect with 7.12 MRR improvement based on 
Fsyn..  They help to find the proper answer ?Grover 
Cleveland? for the question ?Q2049: What presi-
dent served 2 nonconsecutive terms?? from the 
sentence ?Grover Cleveland is the forgotten two-
term American president.?, while Fsyn. wrongly 
identify ?president? as the answer. 
3. The named entity features Fne are also benefi-
cial as they make the 4.46 MRR increase based on 
Fsyn.+Forth.   For the question ?Q2076: What com-
pany owns the soft drink brand "Gatorade"??, Fne 
find the proper answer ?Quaker Oats? in the sen-
tence ?Marineau , 53 , had distinguished himself 
by turning the sports drink Gatorade into a mass 
consumer brand while an executive at Quaker Oats 
During his 18-month??, while Fsyn.+Forth. return 
the wrong answer ?Marineau?. 
4. The trigger features Ftrg lead to an improve-
ment of 3.28 MRR based on Fsyn.+Forth+Fne.  They 
correctly answer more questions.  For the question 
?Q1937: How fast can a nuclear submarine 
travel??, Ftrg return the proper answer ?25 knots? 
from the sentence ?The submarine , 360 feet 
( 109.8 meters ) long , has 129 crew members and 
travels at 25 knots.?, but the previous features fail 
on it. 
Fsyn Forth. Fne Ftrg Top1 Top5 MRR
?    26.50 38.92 31.38
? ?   34.69 43.61 38.50
? ? ?  39.85 47.82 42.96
? ? ? ? 42.67 51.58 46.24
Table 4: Performance for feature combinations 
 
Next, we will evaluate the effectiveness of the syn-
tactic features, described in Section 6.  Table 5 
compares the three feature representation methods, 
FeatureVector, StringKernel and TreeKernel.   
z FeatureVector (Section 6.1).  We predefine 
some features in the syntactic tree and present 
them as a feature vector.  The syntactic fea-
tures are added with the textual features and 
the RBF kernel is used to cope with them. 
z StringKernel (Section 6.2).  No features are 
predefined.  We transform the syntactic rela-
tions between answer candidates and question 
key words to node sequences and a string ker-
nel is proposed to cope with the sequences.  
Then we add the string kernel for the syntactic 
relations and the RBF kernel for the textual 
features. 
z TreeKernel (Section 6.3).  No features are 
predefined.  We keep the original representa-
tions of the syntactic relations and propose a 
tree kernel to cope with the relation trees.  
Then we add the tree kernel and the RBF ker-
nel. 
 Top1 Top2 MRR
Fsyn.+Forth.+Fne+Ftrg 42.67 51.58 46.24
FeatureVector 46.19 53.69 49.28
StringKernel 48.99 55.83 52.29
TreeKernel 50.41 57.46 53.81
Table 5: Performance for syntactic feature repre-
sentations 
 
Table 5 shows the performances of FeatureVec-
tor, StringKernel and TreeKernel.  All of them im-
prove the performance based on the textual 
features (Fsyn.+Forth.+Fne+Ftrg) by 3.04 MRR, 6.05 
MRR and 7.57 MRR respectively.  The probable 
reason may be that the features generated from the 
structured data representation may capture the 
71
more linguistic-motivated evidences for the proper 
answers.  For example, the syntactic features help 
to find the answer ?nitrogen? for the question 
?Q2139: What gas is 78 percent of the earth 's at-
mosphere?? in the sentence ?One thing they have-
n't found in the moon's atmosphere so far is 
nitrogen, the gas that makes up more than three-
quarters of the Earth's atmosphere.?, while the 
textual features fail on it.  Furthermore, the String-
Kernel (+3.01MRR) and TreeKernel (+4.53MRR) 
achieve the higher performance than FeatureVec-
tor, which may be explained that keeping the 
original data representations by incorporating the 
data-specific kernels in SVM may capture the 
more comprehensive evidences than the predefined 
features.  Moreover, TreeKernel slightly outper-
forms StringKernel by 1.52 MRR.  The reason may 
be that when we transform the representation of the 
syntactic relation from the tree to the node se-
quence, some information may be lost, such as the 
sibling node of the answer candidates.  Sometimes 
the information is useful to find the proper answers. 
8 Conclusion  
In this paper, we study the feature generation based 
on the various data representations, such as surface 
text and parse tree, for the answer extraction.  We 
generate the syntactic tag features, orthographic 
features, named entity features and trigger features 
from the surface texts.  We further explore the fea-
ture generation from the parse trees which provide 
the more linguistic-motivated evidences for the 
task.  We propose three methods, including feature 
vector, string kernel and tree kernel, to represent 
the syntactic features in Support Vector Machines.  
The experiment on the TREC question answering 
task shows that the syntactic features significantly 
improve the performance by 7.57MRR based on 
the textual features.  Furthermore, keeping the 
original data representation using a data-specific 
kernel achieves the better performance than the 
explicitly enumerated features in SVM. 
References  
M. Collins.  1996.  A New Statistical Parser Based on 
Bigram Lexical Dependencies.  In Proceedings of 
ACL-96, pages 184-191. 
M. Collins. 2002.  New Ranking Algorithms for Parsing 
and Tagging: Kernel over Discrete Structures, and 
the Voted Perceptron.  In Proceedings of ACL-2002. 
M. Collins and N. Duffy.  2002.  Convolution Kernels 
for Natural Language.  Advances in Neural Informa-
tion Processing Systems 14, Cambridge, MA.  MIT 
Press. 
A. Culotta and J. Sorensen.  2004.  Dependency Tree 
Kernels for Relation Extraction.  In Proceedings of 
ACL-2004. 
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. 
Melz, D. Ravichandran.  2003.  Multiple-Engine 
Question Answering in TextMap.  In Proceedings of 
the TREC-2003 Conference, NIST. 
A. Echihabi, D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proceedings of the 
ACL-2003. 
D. Haussler. 1999.  Convolution Kernels on Discrete 
Structures.  Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
A. Ittycheriah and S. Roukos.  2002.  IBM?s Statistical 
Question Answering System ? TREC 11.  In Pro-
ceedings of the TREC-2002 Conference, NIST. 
A. Ittycheriah. 2001. Trainable Question Answering 
System.  Ph.D. Dissertation, Rutgers, The State Uni-
versity of New Jersey, New Brunswick, NJ. 
T. Joachims.  1999.  Making large-Scale SVM Learn-
ing Practical.  Advances in Kernel Methods - Sup-
port Vector Learning, MIT-Press, 1999. 
T. Joachims.  1998.  Text Categorization with Support 
Vector Machines: Learning with Many Relevant Fea-
tures.  In Proceedings of the European Conference on 
Machine Learning, Springer. 
C. Leslie, E. Eskin and W. S. Noble.  2002.  The spec-
trum kernel: A string kernel for SVM protein classi-
fication.  Proceedings of the Pacific Biocomputing 
Symposium. 
H. Lodhi, J. S. Taylor, N. Cristianini and C. J. C. H. 
Watkins.  2000.  Text Classification using String 
Kernels.  In NIPS, pages 563-569. 
D. Ravichandran, E. Hovy and F. J. Och.  2003.  Statis-
tical QA ? Classifier vs. Re-ranker: What?s the dif-
ference?  In Proceedings of Workshop on Mulingual 
Summarization and Question Answering, ACL 2003. 
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. SVM Answer 
Selection for Open-domain Question Answering. In 
Proc. of COLING 2002, pages 974?980. 
V. N. Vapnik.  1998.  Statistical Learning Theory.  
Springer. 
E.M. Voorhees.  2003. Overview of the TREC 2003 
Question Answering Track.  In Proceedings of the 
TREC-2003 Conference, NIST. 
J. Xu, A. Licuanan, J. May, S. Miller and R. Weischedel.  
2002.  TREC 2002 QA at BBN: Answer Selection 
and Confidence Estimation.  In Proceedings of the 
TREC-2002 Conference, NIST. 
D. Zelenko, C. Aone and A. Richardella.  2003.  Kernel 
Methods for Relation Extraction.  Journal of Ma-
chine Learning Research, pages 1083-1106. 
72
Mul t i l i ngua l i ty  in a Text  Generat ion  System 
For Three  Slavic Languages  
Geert-Jan Kruijff a, Elke Teich t', John Bateman ~, Ivana Kruijit;Korbayovfi", 
Hana Skoumalovg ~,Serge Sharoff 'l, Lena Sokolova d, Tony Hartley ~, 
Kamenka Staykova/, Ji~'~ Hana" 
?Charles University, Prague; ~University of the Saarland; ~University of Bremen; 
aRRIAI, Moscow; ?University of Brighton; /IIT, BAS, Sofia 
http://www.itri.brighton.ac.uk/projects/agile/ 
Abstract 
This paper describes a lnultilingual text generation 
system in the domain of CAD/CAM software in-- 
structions tbr Bulgarian, Czech and l:\[ussian. Start- 
ing from a language-independent semantic represen- 
tation, the system drafts natural, continuous text 
as typically found in software inammls. The core 
modules for strategic and tactical gene,'ation are im- 
plemented using the KPML platform for linguistic 
resource development and generation. Prominent 
characteristics of the approach implemented a.re a 
treatment of multilinguality that makes maximal use 
of the cominonalities between languages while also 
accounting for their differences and a common repre- 
sentational strategy for both text planning and sen- 
tence generation. 
1 In t roduct ion  
This paper describes the Agile system I tbr the 
multilingual generation of instructional texts as 
found in soft;ware user-manuals in Bulgarian, 
Czech and Russian. The current prototype fo- 
cuses on the automatic drafting of CAD/CAM 
software documentation; routine passages as 
found in the AutoCAD user-manual have been 
taken as target texts. The application sce- 
nario of the Agile system is as follows. First, 
a user constructs, with the help of a GUI, 
language-independent task models that spec- 
ify the contents of the documentation to be 
generated. The user additionally specifies the 
language (currently Bulgarian, Czech or Rus- 
sian) and the register of the text to be gen- 
erated. The Agile system then produces con- 
tinuous instructional texts realizing the speci- 
fied content and conforming to the style of soft- 
ware user-manuals. The texts produced are 
1EU Inco-Copernicus project PL961004: 'Automatic 
Generation of Instructional Texts in the Languages of 
Eastern Europe' 
intended to serve as drafts for final revision; 
this ~drafting' scenario is therefbre analogous to 
that first explored within the Drafter project. 
Within the Agile project, however, we have ex- 
plored a more thoroughly nmltilingual architec- 
ture, making substantial use of existing linguis- 
tic resources and components. 
The design of the Agile system overall re, sts 
on the following three assumI)tions. 
First, the input of the system should be spec- 
ified irrespective of any particular output lan- 
guage. This means that the user must be able to 
express the content that she wants the texts to 
convey, irrespective of what natural language(s) 
she masters and in what language(s) the out- 
put text shouM be realized. Such language- 
independent content specification can take the 
form of some knowledge representation pertain- 
ing to the application domain. 
Second, the texts generated as the outtmt of 
the system should be well-formulated with re- 
spect to the expectations of natiw. ? speakers of 
each particular language covered by the system. 
Since differences among languages may appear 
at any level, language-sensitive d cisions about 
the realization of the specified content must be 
possible throughout he generation process. 
And third, the notion of multilinguality em- 
ployed in the system should be recursive, in 
the sense that the modules responsible tbr the 
generation should themselves be multilingual. 
The text generation tasks which are common 
to the languages under consideration should be 
pertbrmed only once. Ideally, there should be 
one process of generation yielding output in 
multiple languages rather than a sequence of 
monolingual processes. This view of 'intrin- 
sic multilinguality' builds on the approach set 
out in Bateman et al (1999). Each module of 
the system is fnlly multilingual in that it simul- 
474 
taneously enables both integration of linguistic 
resources, defining commonalities bel;ween lan- 
guages, and resource integrity, in |;bat the in- 
dividuality of each of the language-speeitic re- 
sources of a multilingnM ensemble is always pre- 
served. 
We consider these assuml)l;ions an(l the view 
of multilinguality entailed by |;hem to be cru- 
cial for the design of efli;ctive multilingual text 
generation systems. The results so far a(:hicved 
by the Agile system SUl)port this and also ofl'er 
a ~soli(l experiential basis tbr the develot)mcnt of 
fllrther multilingnal generation systems. 
The overall operation of 1;t1(; Agile sysl;em is 
as tbllows. Al/tcr the us(u' has Sl)ecilied some 
inl;en(led text (;OlltenI; (described in Section 2) 
via the Agile GUI, the system i)ro(:eeds to gen- 
eral;e the texts required. To do this, a text 
t)lammr (Section 3) first assigns parts of the, 
task model to text elements and arranges l;h(;m 
in a hierarchical fashion a text t)lan. Then, a 
sentence plammr organizes I;he content of the 
text elements into sentence-sized elml~ks and 
ere~,tes the corresponding input fin' l;he tacti- 
ca,1 generator, expressed in standard sentence 
l)lamfing language (SPI,) lbrmulae. Finally, 1;11(; 
tactical g(meral;or generates t;he linguistic real- 
izations corresponding 1;o these Sl)l~s the text 
(Sect;ion 4). In the stage of the l)rojccI; rt}l)orte(l 
here, we, conceal;rated i)arl;icularly on \])roccdu- 
ral texts. These otlhr sl;el)-by-st;e t) des(:rit)t;ions 
of how to perlbrm domain tasks using the given 
software tools. A simplified version of one such 
procedural text is given (tbr English) in Fig- 
ure 1. This architectm:e mirrors the reference 
architecture for generation diseusse(t in I/,eiter 
8z Dale (1.997). The modules of the system are 
1)ipelined so that a continuous text is generated 
realizing the intended meaning of the inlmt se- 
mantic representation without backtracking or 
revision. 
Several important properties have ('haracter- 
ized the method of development leading to the 
Agile system. These are to a large extent re- 
sponsible for the eflhetiveness of the system. 
These include: 
Re-use  and  adaptat ion  o f  ava i lab le  re-  
sources .  We have re-used snt)stantial bodics 
of e, xisting linguistic resources at all levels rel- 
evant for the system; this t)laye(l a (:rueial role 
in achieving the Sol)histieatcd generation capa- 
7b d~nw a polylinc 
First start the PLINE command using one of these meth- 
ods: 
Windows From the Polylinc tlyout on the, l)raw tool~ 
lmr, choose Polylinc. 
DOS and UNIX  lqom the Draw menu, choose Poly- 
line. 
1.. Spccit~y the start point of the polyline. 
2. S1)ecil~y tim next point of the 1)olylinc. 
3. Press ll,cturn t;o end the polyline. 
Figure l: Example "To draw a polyline" 
bilities now displayed by the system in each of 
its languages of expertise prior to the project 
t\]m'l.'e were 11o substantial ~mtomatic generation 
systenls fi)r any of the languages covered. The 
core modules for strategic and ta(:ti('al gener- 
ation were all imt)lemcnted using the Kernel- 
Penman Multilingual system (KPML: ef Bate- 
man et al, \]999) a Common l,isp base(t gram- 
mar development environment, in addition, 
we adopted the Pemnan Upt)er Model as used 
within Pemnan/KPMl~ as the basis tbr our 
linguistic semantics; a more rcstri(:ted domain 
model (DM) rclewmt o the CAD/CAM-domain 
was &',lined as a st)e('ialization of l;he UM con- 
(:epts. The I)M was iuspired by the domain 
me(tel of the Drafter l)rojet:t, but l)res(ml;s a 
g(m(',ralizati()n ()f the latter in that it allows for 
eml)(;d(ling t:asks and illsLrut'|;ions t:o any arlfi- 
l;rm:y re(:ursive depth (i.e., more complex l;cxt; 
plans). Ah'eady existing lexical resom:ces and 
morphological modules availabh; to the 1)ro.j(',ct 
were re-used tbr Bulgarian, Czech and l~.ussian: 
the Czech and Bulgarian components were mo(t- 
ules written in C (e.g., IIaji(: L; Hla(lk~, 1997, 
tbr Czech) that were interfimed with KPMI, us- 
ing a standard set of API-methods (of. Bate- 
man & Sharoff, 1998). Finally, because no 
grammars uitable for generation in Bulgarian, 
Czech and l/.ussia,n existed, a grammar tbr En- 
glish (NIGEL: Mann & Matthiessen, 1985) was 
re-used to lmild them; tbr the theoretical basis 
of this technique see Teich (1995). 
Combinat ion  o f  two  methods  o f  resources  
deve lopment .  Two methods  were com- 
bined to enable us to develop basic general- 
language grammars and sublanguage grammars 
fin: CAD/CAM instructional texts at; |;11(; same 
time. One nmthod is the system-oriented one 
aimed at lmildiug a computational resource 
475 
with a view of the whole language system: this 
is a method strongly supported by the KPML 
development environment. The other method 
is instance-oriented, and is guided by a detailed 
register analysis. The latter method was partic- 
ularly important given the Agile goal of being 
able to generate texts belonging to rather di- 
verse text types- -  e.g., impersonal vs. personal; 
procedural, flmetional descriptions, overviews 
etc. 
Cross-linguistic resource-sharing. A cross- 
linguistic approach to linguistic specifications 
and implementation was taken by maximizing 
resource sharing, i.e. taking into account sim- 
ilarities and differences among the treated lan- 
guages o that development tasks have been dis- 
tributed across different languages and re-used 
wherever possible. 
2 Language- independent  Content 
Specif icat ions 
The content constructed by a user via the Ag- 
ile GUI is specified in terms of Assertion-bozes 
or A-boxes. These A-boxes are considered to 
be entirely neutral with respect o the language 
that will be used to express the A-box's con- 
tent. Thus individual A-boxes can be used for 
generating multiple languages. A-boxes spec- 
i(y content by instantiating concepts from ~,he 
DM or UM, and placing these concepts in rela- 
tion to one another by means of configurational 
concepts. The configurational concepts define 
adnfissible ways in which content can be struc- 
tured. Figure 2 gives the configurational con- 
cepts distinguished within Agile. 
Procedure A procedure has three slots: 
(i) GOAL (obligatory,filled by a USER-AcTION), 
(ii) METIIODS (optional, filled by a METHOD-LIsT), 
(iii) SIDE-EPFECT (optional, filled by a USER- 
EVENT). 
Method A method has three slots: 
(i) CONSTRAINT (optionM, filled by an OPERATING- 
SYSTEM), 
(ii) PaEeONDITION (optional, filled by a PROCE- 
DURE), 
(iii) SUUSTEPS (obligatory, filled by a PI~OCEDUI/E- 
LIST). 
Method-List A METIIOD-LIST is a list of h/IETIIOD'S. 
Procedure-List A PROCEI)URE-LIST is a list of 
PROCEDURE:S. 
Figure 2: Configurational concepts 
Configurational concepts are devoid of actual 
content. Tile content is provided by inst, antia- 
tions of concepts that represent various user ac- 
tions, interface events, and interface modalities 
and functions. Taken together, these instanti- 
ations provide the basic propositional content 
tbr instructional texts and are taken as input 
tbr the text planning process. 
3 Strategic Generat ion: From 
Content  Specif icat ions to Sentence 
Plans 
To realize an A-box as a text, we go through suc- 
cessive stages of text planning, sentence plan- 
ning, and lexico-grammatical generation (cf 
also Reiter & Dale, 1997). At each stage there 
is an increase in sensitivity to, or dependency 
on, the target language in which output will 
be generated. Although the text planner itself 
is language-independent, the text; plamfing re- 
sources may (lifter fl'om language to language 
as much as is required. This is exactly analo- 
gous to the situation we find within the individ- 
ual language grammars as represented within 
KPML: we therefore represent the text planning 
resources in the same fashion. For the text type 
and languages of concern here, however, w~ria- 
lion across languages at the text planning stage 
turned out to be minimal. 
The organization of an A-box is used to guide 
the text planning process. Itere, we draw a dis- 
tinction between text structure elements (TSEs) 
as the elements from which a (task-oriented) 
text, is built ut), and text templates', which con- 
dition the way TSEs are to be realized linguis- 
tically. We locus on the relation between con- 
cepts on the one hand, and TSEs on the other. 
We are specifically interested in the configura- 
tional concepts that are used to configure the 
content specified in an A-box because we want 
to maintain a close connection between how the 
content can be defined in an A-box and how 
that content is to be spelled out in text. 
3.1 Structuring and Styling 
A text structure element is a predefined com- 
ponent that needs to be filled by one or more 
specific parts of the user's content definition. 
Using the reader-oriented terminology common 
in technical authoring guides, we distinguish 
a small (recursively defined) set of text TSEs; 
these are listed in Figure 3. 
476 
Task-Docmnent  A TASK-\])OCUMFNT has tWO slots: 
(i) TASK-TFI'I,E (ol)ligatory), 
(ii) TASK-INSTI{U(ITIONS (obligatory), being a list 
of" at least one ~\[NSTRUCTION. 
Instruction An INSTRUCTION has three slots: 
(i) TASKS (obligatory), being a list of at least one 
TASK~ 
(ii) CONSTRAINT (optional), 
(iii) Pm,ZCONDITION (optional). 
Task  A TASK has two slots: 
(i) INSTRUCTIONS (ol)tional), 
(ii) SII)I';-EI,'I"I,:C'I' (ol)tional). 
Figure 3: Text Structure Elements (TSEs) 
The TSEs are placed in correspondence with 
the configurational concet)ts of the DM (cf. Fig- 
ure 2); this enat)les us to lmild a text stru('ture 
l;hat folh)ws the structuring of the content in an 
A-1)ox (cf. Figure 4). 
Orthogonal to the notion of text structure l- 
ement is the notion of text temt)late. Whereas 
TSEs capture what needs to be realized, the 
text template (:al)tures how that content is to 
1)e realized. Thus, a feint)late defines a style 
for expressing the content. Am we discuss be- 
low, we define text templates in terms of con- 
straints on the realization of si)e(:iti(" (in(tivid- 
ual) TSEs. D)r examt)le, whereas in Bulgarian 
and Czech headings (to which the '\]'ASK-TITLE 
element corresponds: of. Figure 4) are usually 
realized as nominal groups, in the Russian Au- 
toCAD ulallnal headings are realized as nonii- 
nile purpose clauses as they are ill English. 
3.2 Tex~ P lann ing  g~ Sentence  P lann ing  
The major component of the text pbmner is 
fi)rnmd by a systemic network fi)r text struc- 
turing; this network, called the text structur- 
ing region, defines an additional level of linguis- 
tic resources for the level of genre. This region 
constructs text structures in a way that is very 
similar to the way in which the systemic net- 
works of the grammars of the tactical genera- 
|or build up grammatical structures. In fact, 
by using KPML to implement his means for 
text structuring, the interaction between global 
level text generation (strategic generation) and 
lexico-grammatical expression (tactical genera- 
tion) is greatly facilitated. Moreover, this al)- 
t)roach has the advantage |;tint constraints on 
output realization can 1)e easily accmnulated 
and propagated: for example, the text plan- 
ner can iml)ose constraints on the output lexico- 
grammatical realization of particular text t)lan 
elements, such am the realization of text head- 
ings by a nominalization ill Czech and Bulgar- 
|an or by an infinite purpose clause in Rus- 
sian. This is one contribution to overcoming the 
notorious generation gap prol)leln caused when 
a text planning module lacks control over the 
line-grained istinctions that m'e available in a 
grmmnar. Ill our case, both text plamfing and 
sentence planning are integrated into one and 
the same system and are distinguished by strat- 
ification. 
TASK-TITLE ~-} GOAl, of topmost  PROCEDURE 
TASK-INSTRUCTIONS ~-} METIIODS of PROCEDUI/E 
Sll)E-EIq,'ECT ~ SIDhl-EFFI~CT of PROCEDUII.I\] 
TASK /-~ GOAL of PROCEI)IHtI,; 
(-JONSTRA1NT <-} CONSTRAINT of ~41,VI'IIO1) 
PRECONI)ITION ~ PIH?COND1TION of ~,4ETI1OI) 
1NSTIIUCTI(IN-TAsKS 1--} SUBSTH)S of a METIIOD 
INST1HJCTION +5 MI,TI'IIOD 
Figure 4: Mapping TSEs and configurational 
concepts defined in the DM 
Following on from the orthogomflity of text 
t/;mplates and text structure elements, the text 
structuring region consists of two parts. One 
1)arl; deals wil;h interpreting the A-box in terms 
of TSEs: traversing l;he network of this part of 
the region produces a text structure for the A- 
b/lx contbrufing to the definitions above. The 
second part of the region imposes constraints 
on the realization of the TSEs introduced by 
the first part. Divers(; constraints can be ira- 
posed depending on the user's choice of style, 
e.g., personal (featuring ppredominantly imper- 
atives) vs. impersonal (tbaturing indicatives). 
Tile result of text plmming is a text plan. 
This can be thought of as a hierarchical struc- 
ture (built by TSEs) with lilts of A-box content 
at; its leaves together with additional constraints 
imposed by the text planning process: e.g., that 
the Title segment of the document should not be 
realized as a full (:lause but; rather as a nominal 
phrase or a lmrt)osive det)endent clause. The 
text plan may also include constraints on pre- 
ferred layout of the docmnent elements: this 
ilflbrmation is passed on via HTML annotations. 
The sentence plmmer then takes this text plan 
as intmt, and creates SPL tbrmulae to express 
477 
the content identified by the text plan's leaves. 
The resulting SPLs can also group one or more 
leaves together (aggregation) det)ending on de- 
cisions taken by the text planner concerning dis- 
course relations. Furthennore, constraints on 
realization that were introduced by the text- 
planner are also included into the SPLs at this 
stage. 
Of particular interest multilingually is the 
way concepts may require different kinds of re- 
alizations ill different languages. For example, 
languages need not of course realize concepts 
as single words: in Czech the concept Mcn,t 
gets realized as "menu" but the interface modal- 
ity Dialogboz is realized as a multiword expres- 
sion "dialogovd okno" (whose compofients i.e., 
an adjective and a nominal head may undergo 
various grammatical operations independently). 
The Agile system sentence plammr handles uch 
cases by inserting SPL fbrms corresponding to 
the literal semantics of the complex expressions 
required; these are then expressed via the tac- 
tical generator in the usual way. The result- 
ing SPL formulas thus represent the language- 
specitic semantics of the sentences to be gener- 
ated. Otherwise, if a concept maps to a single 
word, the sentence planner leaves the fnrther 
specification of how the concept should be re- 
alized to the lexico-grammar nd its concept- 
to-word mapI)ings. More extensive diflb.rences 
between languages are handled by conditional- 
izing the text and sentence planner resources 
fltrther according to language. 
4 Tactical Generat ion:  From 
Sentence P lans  to Sentences 
The tactical generation component hat colt- 
structs sentences (and other grammatical units) 
fl'om the SPL tbrmulae specified in the text 
plan relies on linguistic resources tbr Bulgarian, 
Czech and Russian. The necessary grammars 
and lexicons have been constrncted employing 
the methods described in Section 1. As ,toted 
there, the crucial characteristic of this model 
of nmltilingual representation is that it allows 
tbr the representation f both, commonalities and 
differences between languages, as required to 
cover the observable ontrastive-linguistic phe- 
nomena. This can be applied even among typo- 
logically rather distant languages. 
We first illustrate this with respect o some 
of the contrastive-linguistic t)henomena that are 
covered by this model employing exami)les ti'om 
English, Bulgarian, Czech and Russian. We 
then show the organization of the lexicons and 
briefly describe lexical dloice. 
4.1 Semantic and grammatical 
cross-linguistic variation 
One. of the tenets of our model of cross-linguistic 
variation is that languages have a rather high 
degree of similarity semantically attd tend to 
differ syntactically. We can thus expect o have 
identical SPL expressions for Bulgarian, Czech 
and Russian in many cases, although these may 
be realized by diverging syntactic structures. 
However, we also allow for the case in which 
there is no commonality at; this level and even 
the SPL expressions diverge. 2 Example 1 illus- 
trates the latter case (high semantic divergence, 
plus grammatical divergence), and example 2 
the former (semantic ommonality, plus gram- 
matical divergence). 
Example 1: English and Russian spa- 
tial PPs .  The major lexico-grammatical d i f  
ference l)etween English and Russian preposi- 
tional phrases is that the relation expressed by 
the PP is realized by the choice of the prepo- 
sition in English, whereas in Russian, it; is in 
addition realized by case-government. In the 
are.a of spatial PPs, the choice of a particular 
preI)osition in English corresl)onds to a distinc- 
tion in the dimensionality of the object that re- 
alizes the range of the relation expressed by the 
PP. For both PPs expressing a location and PPs 
expressing movement, English distinguishes be- 
tween three-dimensional objects (in, into), one- 
or-two-dimensional objects (on, onto) and zero- 
dimensional objects (at, to). 
In Russian, in contrast, zero-or-three dimen- 
sional objects (preposition: v) are opposed 
to one-or-two-dimensional objects (preposition: 
ha). A fnrther difference between the expres- 
sion of static location vs. movement is expressed 
by case selection: na/v+locative case expresses 
static location, v/na+accusative case expresses 
inovement (entering or reaching an object) and 
the preposition k+dative case expresses move- 
inent towards an object (,lot quite reaching or 
2This distinguishes our approach fl'om interlingua- 
based systems, which typically require a common seman- 
tic (or conceptual) input. 
478 
entering it). In the {-onverse relation, motion 
away from an object, s is sele, eted tbr move- 
ment from within an oh.joel;, and ot fbr move- 
men| away from the vicinity of an ot).jeet. Her(;, 
both prel)ositions govern genitive case. The di- 
mensionality of the object is only relevant for 
the distinction between v/na and s/ot, 1)ut not 
for h. Since the concel)tualizations of spatial re- 
lations are ditf'erent across \]'3nglish and Russian, 
the input SPL expressions diverge, as shown in 
Figure 5); rather than using domain model con- 
cepts, these SPL ext)ressions restrict hemselves 
to Ut)pe, r Model concepts in order to highlight 
the cross-linguistic contrast. This examl)le illus- 
trates well how it is (}ften ne{:e, ssary t{} 'semanti- 
{:ize,' eve, nts differently in (tilt'ere|d; languages in 
order 1;o achieve the most natural results. Not;{; 
that Cze, ch is here very similar to l/nssian. 
a. SPL Russian 
(example 
:name DO-Textl-Ku 
:targetform "Pomestite fragment v bufer." 
:logicalform 
(s / dispositive-material-action 
:lex pomestitj 
:speech-act-id command 
:actee (a / object :lex fragment) 
:destination (d / THREE-D-0BJECT 
:lex bufer))) 
1}. SPL Rn: English 
(example  
:name D0-Textl-En 
:targetform "Put the selection on the clipboard." 
:iogicalform 
(s / dispositive-material-action 
:lex put 
:speech-act-id command 
:actee (a / object :lex selection) 
:destination (d / ONE-0R-TW0-D-0BJECT 
:lex clipboard))) 
Figure 5: SPI, ext}ressions 
Example 2: English, Bulgar ian and Czech 
headers in CAD/CAM texts. Grammatical 
ullits (1) (4) below show all ex~?tIllt, e of ,:r,,ss- 
linguistic commonality at the level of sen|anti{: 
int}ut and divergence at the le, vel of grammar. 
These units all time|ion as selfsutficient Task- 
titles tbr the deseril}tions of particular actions 
that can be t)erformed with the given s{}t'tware. 
(1) En: T{} draw a polyline 
(2) BU: qepTaene na IlOJII4MI4IIFIH 
Drawing- of polylineqNDEF 
NOMINAL  
(3) Cz: Kreslenl kf'ivky 
drawing-NOMINAL \]ine-GEN 
(d) \]/,ll: LIwo6I,I Hal)I4COBaTI, IIO,KHJIIIIIHIO 
in-order draw-INF l)olyline-AcC 
There are two major dit  re,,,ces (:,) (4) 
that need to 1)e accounte, d for: (i) they exhibit 
divergent grammatieal  ranks in that (1) and 
(4) are clauses (uontinite), while (2) and (3) are 
nomil,al groul,s (nominalizations); and ( i i )they 
show divergent syntact ic realizations: (2) 
and (3) ditl'er in that in Bulgarian, wlfich does 
not have (:as(',, the relation 1)etween the syntactic 
head Met)q_'aelte (ch, crtacnc) and the modifier lie- 
:mamma (polilinia) is (;xt)ressed by a t)re, position 
na (ha), whereas in Cze, ch, which has cast, this 
relation is expressed by genitive case, (k?ivky). 
\])espite these (litferen(:es, only the first diver- 
gen(:e has any (;onsequen(:{;s for the S\])L ext)res- 
sions rcquir(;d; I;hc l)asie semantic ommona\]- 
ity among (1)(4)  is 1)reserve, d. This is shown 
in Figm:e 6 t)y me, ans of the standard linguis- 
tic conditionalization 1)rovided 1)y KPML l'or all 
levels of linguistic des(:ription. The COll(tition- 
alization shows that both the English (1.) and 
the Russian (4) ar(' nontinite clauses while, the 
\]hdgarian (2) and the Czech (3) are nominM- 
izations. These S\])l, ext)ressions also show the 
use of (lom~dn ('onc(;1)ts as i)rodu('e(l by the text 
tfl~mner rathe, r than Ut)lmr model concepts as in 
the SPLs in Figure, 5. 
(example 
:name DO-Textl 
:logicalform 
(s / DM::draw 
:en :ru :PROPOSAL-Q & PROPOSAL 
:bu :cz :EXIST-SPEECHACT-Q & NOSPEECHACT 
:actee (d / DM::polyline))) 
Figure 6: Multilingual SPL e, xpression tbr the 
header examlfles 
The second differen('e is handled by the gener- 
ation grmmnars internally. Here, Bulgarian and 
Czech share the basic tractional-grammatical 
description of t)ostmotlifie, rs tbr nomilmlizati(ms 
(Figm:e 7). The ditl'erence in structure only 
479 
shows in syntagmatic realization and is separate 
from the functional description: For Bulgarian, 
the postmodifier marker Ha (ha: %f') is inserted, 
and tbr Czech, the nominal group realizing the 
Postmodifier is attr ibuted genitive ease. a
(gate 
:name MEDIUM-QUALIFIER 
:inputs processual-mediated 
:outputs 
((i.0 medium-qualifier 
(:bu :cz preselect Medium nominal-group) 
(:cz preselect Medium noun-gen-case) 
(:bu insert Mediumqualifiermarker) 
(:bu lexify Mediumqualifiermarker na))) 
:region QUALIFICATION) 
Figure 7: Shared system tbr Bulgarian and 
Czech 
4.2 Lexical choice and lexicons 
The lexical items tbr each language are selected 
from the lexicon via the domain model. A DM 
concept is annotated with one or more lexical 
items from each language. If there is more than 
one item per language, the choice is constrained 
by features imposed by the gralnmar. 
For example the concept DN::draw is anno- 
tated with two lexical items which are the im- 
perfective and perfective forms of the verb draw 
in Czech, Bulgarian and Russian. If the gram- 
mar selects imperfective aspect, tim first is cho- 
sen; if the grammar selects perfective aspect, 
the second is chosen. This mechanism is used 
also fbr the choice between a verb and its nom- 
inalization, among others. With the help of the 
lexicon, the inflectional properties collected tbr 
a particular lexical item during generation are 
translated into a format suitable tbr external 
morphological modules, which are then called. 
The result of the external module, the inflected 
tbrm, is passed back to the KPML system and 
inserted into the grammatical structure. 
5 Eva luat ion  and Conc lus ions  
A first round of evaluation has been carried 
out on the Agile prototype. This directly as- 
sessed the ability of users to control multilin- 
3This description is also valid for Russian, which has a 
nominal group structure similar to Czech. The 13ulgarian 
one is more like English. 
gual generation in tile three languages, as well 
as the design and robustness of the system eom- 
1}onents. Groups of users were given a brief 
training period and then asked to construct 
A-boxes expressing iven content. Texts were 
cross-generated: i.e., the languages were w~ried 
across the A-boxes independently of the native 
languages of the subjects who created them. Er- 
rors were then classified and recommendations 
for the next and final Agile prototype collected. 
The generated texts were then evaluated by ex- 
pert technical authors. They were generally 
judged to be of a broadly similar quality to 
the texts originating from manuals, and both 
kinds of texts received similar criticism. The 
main source of criticism and errors was the de- 
sign of the GUI which is now being improved 
for the final prototype. The overall design of 
the system has theretbre shown itself to offer an 
etfective approach tbr multilingual generation. 
We are now extending the system to cover a 
broader ange of text types as well as the further 
grammatical and semantic variation required by 
the evaluators as well as by the additional text 
types. 
Re ferences  
Bateman, J. A., Matthiessen, C. M. I. M., & Zeng, L. 
(1999). Multilingual natural anguage generation 
for multilingual software: a flmctional inguistic 
approach. Applied Artificial hdelligencc, 13(6), 
607-639. 
Bateman, J. A. & Sharoff, S. (1998). Mult, ilingual 
grammars and multilingual lexicons for nmltilin- 
gual text; generation. In Mnltilinguality in the Icz- 
icon II, ECAI'98 Workshop 13, (pp. 1-8). 
Hajie, J. 8; Hladk?, B. (1997). Probabilistic and 
rule-based tagger of an inflective language a 
comparison. In Proceedings of ANLP'97, (pp. 
111-118). 
Mmm, W. C. & Matthiessen, C. M. I. M. (1985). 
Demonstration of the Nigel text generation com- 
puter progrmn. In J. D. Benson 8: W. S. Greaves 
(Eds.), Systemic Perspectives on Discourse, Vol- 
ume 1 (pp. 50-83). Ablex. 
Reiter, E. & Dale, R. (1997). Building applied natu- 
ral language generation systems. Journal of Nat- 
ural Language Engineering, 3, 57-87. 
Tcich, E. (1995). Towards a methodology for the 
construction of multilingual resources tbr multi- 
lingual text generation. In Proceedings of the I J- 
CAI'95 workshop on multilingual generation, (pp. 
136-148). 
480 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 745?752,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Proximity in Context: an empirically grounded computational model of
proximity for processing topological spatial expressions?
John D. Kelleher
Dublin Institute of Technology
Dublin, Ireland
john.kelleher@comp.dit.ie
Geert-Jan M. Kruijff
DFKI GmbH
Saarbruc?ken, Germany
gj@dfki.de
Fintan J. Costello
University College Dublin
Dublin, Ireland
fintan.costello@ucd.ie
Abstract
The paper presents a new model for context-
dependent interpretation of linguistic expressions
about spatial proximity between objects in a nat-
ural scene. The paper discusses novel psycholin-
guistic experimental data that tests and verifies the
model. The model has been implemented, and en-
ables a conversational robot to identify objects in a
scene through topological spatial relations (e.g. ?X
near Y?). The model can help motivate the choice
between topological and projective prepositions.
1 Introduction
Our long-term goal is to develop conversational
robots with which we can have natural, fluent sit-
uated dialog. An inherent aspect of such situated
dialog is reference to aspects of the physical envi-
ronment in which the agents are situated. In this
paper, we present a computational model which
provides a context-dependent analysis of the envi-
ronment in terms of spatial proximity. We show
how we can use this model to ground spatial lan-
guage that uses topological prepositions (?the ball
near the box?) to identify objects in a scene.
Proximity is ubiquitous in situated dialog, but
there are deeper ?cognitive? reasons for why we
need a context-dependent model of proximity to
facilitate fluent dialog with a conversational robot.
This has to do with the cognitive load that process-
ing proximity expressions imposes. Consider the
examples in (1). Psycholinguistic data indicates
that a spatial proximity expression (1b) presents a
heavier cognitive load than a referring expression
identifying an object purely on physical features
(1a) yet is easier to process than a projective ex-
pression (1c) (van der Sluis and Krahmer, 2004).
?The research reported here was supported by the CoSy
project, EU FP6 IST ?Cognitive Systems? FP6-004250-IP.
(1) a. the blue ball
b. the ball near the box
c. the ball to the right of the box
One explanation for this preference is that
feature-based descriptions are easier to resolve
perceptually, with a further distinction among fea-
tures as given in Figure 1, cf. (Dale and Reiter,
1995). On the other hand, the interpretation and
realization of spatial expressions requires effort
and attention (Logan, 1994; Logan, 1995).
Figure 1: Cognitive load
Similarly we
can distinguish be-
tween the cognitive
loads of processing
different forms of
spatial relations.
Focusing on static
prepositions, topo-
logical prepositions
have a lower cognitive load than projective
prepositions. Topological prepositions (e.g.
?at?, ?near?) describe proximity to an object.
Projective prepositions (e.g. ?above?) describe a
region in a particular direction from the object.
Projective prepositions impose a higher cognitive
load because we need to consider different spatial
frames of reference (Krahmer and Theune, 1999;
Moratz and Tenbrink, 2006). Now, if we want
a robot to interact with other agents in a way
that obeys the Principle of Minimal Cooperative
Effort (Clark and Wilkes-Gibbs, 1986), it should
adopt the simplest means to (spatially) refer to an
object. However, research on spatial language in
human-robot interaction has primarily focused on
the use of projective prepositions.
We currently lack a comprehensive model for
topological prepositions. Without such a model,
745
a robot cannot interpret spatial proximity expres-
sions nor motivate their contextually and pragmat-
ically appropriate use. In this paper, we present
a model that addresses this problem. The model
uses energy functions, modulated by visual and
discourse salience, to model how spatial templates
associated with other landmarks may interfere to
establish what are contextually appropriate ways
to locate a target relative to these landmarks. The
model enables grounding of spatial expressions
using spatial proximity to refer to objects in the
environment. We focus on expressions using topo-
logical prepositions such as ?near? or ?at?.
Terminology. We use the term target (T) to
refer to the object that is being located by a spa-
tial expression, and landmark (L) to refer to the
object relative to which the target?s location is de-
scribed: ?[The man]T near [the table]L.? A dis-
tractor is any object in the visual context that is
neither landmark nor target.
Overview ?2 presents contextual effects we can
observe in grounding spatial expressions, includ-
ing the effect of interference on whether two ob-
jects may be considered proximal. ?3 discusses a
model that accounts for all these effects, and ?4 de-
scribes an experiment to test the model. ?5 shows
how we use the model in linguistic interpretation.
2 Data
Below we discuss previous psycholinguistic expe-
rients, focusing on how contextual factors such as
distance, size, and salience may affect proximity.
We also present novel examples, showing that the
location of other objects in a scene may interfere
with the acceptability of a proximal description to
locate a target relative to a landmark. These exam-
ples motivate the model in ?3.
 
 
  
1.74 1.90 2.84 3.16 2.34 1.81 2.13 
2.61 3.84 4.66 4.97 4.90 3.56 3.26 
4.06 5.56 7.55 7.97 7.29 4.80 3.91 
3.47 4.81 6.94 7.56 7.31 5.59 3.63 
4.47 5.91 8.52 O 7.90 6.13 4.46 
3.25 4.03 4.50 4.78 4.41 3.47 3.10 
1.84 2.23 2.03 3.06 2.53 2.13 2.00 
Figure 2: 7-by-7 cell grid with mean goodness ratings for
the relation the X is near O as a function of the position oc-
cupied by X.
Spatial reasoning is a complex activity that in-
volves at least two levels of processing: a geomet-
ric level where metric, topological, and projective
properties are handled, (Herskovits, 1986); and a
functional level where the normal function of an
entity affects the spatial relationships attributed to
it in a context, cf. (Coventry and Garrod, 2004).
We focus on geometric factors.
Although a lot of experimental work has been
done on spatial reasoning and language (cf.
(Coventry and Garrod, 2004)), only Logan and
Sadler (1996) examined topological prepositions
in a context where functional factors were ex-
cluded. They introduced the notion of a spatial
template. The template is centred on the land-
mark and identifies for each point in its space the
acceptability of the spatial relationship between
the landmark and the target appearing at that point
being described by the preposition. Logan &
Sadler examined various spatial prepositions this
way. In their experiments, a human subject was
shown sentences of the form ?the X is [relation]
the O?, each with a picture of a spatial configura-
tion of an O in the center of an invisible 7-by-7
cell grid, and an X in one of the 48 surrounding
positions. The subject then had to rate how well
the sentence described the picture, on a scale from
1(bad) to 9(good). Figure 2 gives the mean good-
ness rating for the relation ?near to? as a function
of the position occupied by X (Logan and Sadler,
1996). It is clear from Figure 2 that ratings dimin-
ish as the distance between X and O increases, but
also that even at the extremes of the grid the rat-
ings were still above 1 (min. rating).
Besides distance there are also other factors that
determine the applicability of a proximal relation.
For example, given prototypical size, the region
denoted by ?near the building? is larger than that
of ?near the apple? (Gapp, 1994). Moreover, an
object?s salience influences the determination of
the proximal region associated with it (Regier and
Carlson, 2001; Roy, 2002).
Finally, the two scenes in Figure 3 show inter-
ference as a contextual factor. For the scene on the
left we can use ?the blue box is near the black box?
to describe object (c). This seems inappropriate in
the scene on the right. Placing an object (d) beside
(b) appears to interfere with the appropriateness
of using a proximal relation to locate (c) relative
to (b), even though the absolute distance between
(c) and (b) has not changed.
Thus, there is empirical evidence for several
746
Figure 3: Proximity and distance
contextual factors determining the applicability of
a proximal description. We argued that the loca-
tion of other distractor objects in context may also
interfere with this applicability. The model in ?3
captures all these factors, and is evaluated in ?4.
3 Computational Model
Below we describe a model of relative proximity
that uses (1) the distance between objects, (2) the
size and salience of the landmark object, and (3)
the location of other objects in the scene. Our
model is based on first computing absolute prox-
imity between each point and each landmark in a
scene, and then combining or overlaying the re-
sulting absolute proximity fields to compute the
relative proximity of each point to each landmark.
3.1 Computing absolute proximity fields
We first compute for each landmark an absolute
proximity field giving each point?s proximity to
that landmark, independent of proximity to any
other landmark. We compute fields on the pro-
jection of the scene onto the 2D-plane, a 2D-array
ARRAY of points. At each point P in ARRAY ,
the absolute proximity for landmark L is
proxabs = (1 ? distnormalised(L,P,ARRAY ))
? salience(L).
(1)
In this equation the absolute proximity for a
point P and a landmark L is a function of both
the distance between the point and the location of
the landmark, and the salience of the landmark.
To represent distance we use a normalised
distance function distnormalised (L, P, ARRAY ),
which returns a value between 0 and 1.1 The
smaller the distance between L and P , the higher
the absolute proximity value returned, i.e. the
more acceptable it is to say that P is close to L. In
this way, this component of the absolute proximity
field captures the gradual gradation in applicabil-
ity evident in Logan and Sadler (1996).
1We normalise by computing the distance between the
two points, and then dividing this distance it by the maximum
distance between point L and any point in the scene.
We model the influence of visual and dis-
course salience on absolute proximity as a func-
tion salience(L), returning a value between 0 and
1 that represents the relative salience of the land-
mark L in the scene (2). The relative salience of
an object is the average of its visual salience (Svis )
and discourse salience (Sdisc),
salience(L) = (Svis(L) + Sdisc(L))/2 (2)
Visual salience Svis is computed using the algo-
rithm of Kelleher and van Genabith (2004). Com-
puting a relative salience for each object in a scene
is based on its perceivable size and its centrality
relative to the viewer?s focus of attention. The al-
gorithm returns scores in the range of 0 to 1. As
the algorithm captures object size we can model
the effect of landmark size on proximity through
the salience component of absolute proximity. The
discourse salience (Sdisc) of an object is computed
based on recency of mention (Hajicova?, 1993) ex-
cept we represent the maximum overall salience in
the scene as 1, and use 0 to indicate that the land-
mark is not salient in the current context. 
 
0
0.1
0.2
0.3
0.4
0.5
0.?
0.?
0.?
0.?
1
?-3?-3? ?-2?-2? ?-1?-1? L ?1?1? ?2?2? ?3?3?
point location
pro
xim
ity 
rati
n?
???ol?te proximity to L? ?alien?e 1
???ol?te proximity to L? ?alien?e 0.?
???ol?te proximity to L? ?alien?e 0.5
 
Figure 4: Absolute proximity ratings for landmark L cen-
tered in a 2D plane, points ranging from plane?s upper-left
corner (<-3,-3>) to lower right corner(<3,3>).
Figure 4 shows computed absolute proximity
with salience values of 1, 0.6, and 0.5, for points
from the upper-left to the lower-right of a 2D
plane, with the landmark at the center of that
plane. The graph shows how salience influences
absolute proximity in our model: for a landmark
with high salience, points far from the landmark
can still have high absolute proximity to it.
3.2 Computing relative proximity fields
Once we have constructed absolute proximity
fields for the landmarks in a scene, our next step
is to overlay these fields to produce a measure of
747
relative proximity to each landmark at each point.
For this we first select a landmark, and then iter-
ate over each point in the scene comparing the ab-
solute proximity of the selected landmark at that
point with the absolute proximity of all other land-
marks at that point. The relative proximity of a
selected landmark at a point is equal to the abso-
lute proximity field for that landmark at that point,
minus the highest absolute proximity field for any
other landmark at that point (see Equation 3).
proxrel(P,L) = proxabs(P,L)? MAX
?LX #=L
proxabs(P,LX )
(3)
The idea here is that the other landmark with the
highest absolute proximity is acting in competi-
tion with the selected landmark. If that other land-
mark?s absolute proximity is higher than the ab-
solute proximity of the selected landmark, the se-
lected landmark?s relative proximity for the point
will be negative. If the competing landmark?s ab-
solute proximity is slightly lower than the abso-
lute proximity of the selected landmark, the se-
lected landmark?s relative proximity for the point
will be positive, but low. Only when the compet-
ing landmark?s absolute proximity is significantly
lower than the absolute proximity of the selected
landmark will the selected landmark have a high
relative proximity for the point in question.
In (3) the proximity of a given point to a se-
lected landmark rises as that point?s distance from
the landmark decreases (the closer the point is to
the landmark, the higher its proximity score for the
landmark will be), but falls as that point?s distance
from some other landmark decreases (the closer
the point is to some other landmark, the lower its
proximity score for the selected landmark will be).
Figure 5 shows the relative proximity fields of two
landmarks, L1 and L2, computed using (3), in a
1-dimensional (linear) space. The two landmarks
have different degrees of salience: a salience of
0.5 for L1 and of 0.6 for L2 (represented by the
different sizes of the landmarks). In this figure,
any point where the relative proximity for one par-
ticular landmark is above the zero line represents
a point which is proximal to that landmark, rather
than to the other landmark. The extent to which
that point is above zero represents its degree of
proximity to that landmark. The overall proximal
area for a given landmark is the overall area for
which its relative proximity field is above zero.
The left and right borders of the figure represent
the boundaries (walls) of the area.
Figure 5 illustrates three main points. First, the
overall size of a landmark?s proximal area is a
function of the landmark?s position relative to the
other landmark and to the boundaries. For exam-
ple, landmark L2 has a large open space between
it and the right boundary: Most of this space falls
into the proximal area for that landmark. Land-
mark L1 falls into quite a narrow space between
the left boundary and L2. L1 thus has a much
smaller proximal area in the figure than L2. Sec-
ond, the relative proximity field for some land-
mark is a function of that landmark?s salience.
This can be seen in Figure 5 by considering the
space between the two landmarks. In that space
the width of the proximal area for L2 is greater
than that of L1, because L2 is more salient.
The third point concerns areas of ambiguous
proximity in Figure 5: areas in which neither of
the landmarks have a significantly higher relative
proximity than the other. There are two such areas
in the Figure. The first is between the two land-
marks, in the region where one relative proxim-
ity field line crosses the other. These points are
ambiguous in terms of relative proximity because
these points are equidistant from those two land-
marks. The second ambiguous area is at the ex-
treme right of the space shown in Figure 5. This
area is ambiguous because this area is distant from
both landmarks: points in this area would not be
judged proximal to either landmark. The ques-
tion of ambiguity in relative proximity judgments
is considered in more detail in ?5. 
 
????
????
????
????
????
?
???
???
???
???
???
?? ??
point lo?ation?
rela
tive
 pro
xim
ity
???????? ????? ????? ????? ?? ??
???????? ????? ??? ?? ????? ?? ??
 
Figure 5: Graph of relative proximity fields for two land-
marks L1 and L2. Relative proximity fields were computed
with salience scores of 0.5 for L1 and 0.6 for L2.
4 Experiment
Below we describe an experiment which tests our
approach (?3) to relative proximity by examining
748
the changes in people?s judgements of the appro-
priateness of the expression near being used to de-
scribe the relationship between a target and land-
mark object in an image where a second, distractor
landmark is present. All objects in these images
were coloured shapes, a circle, triangle or square.
4.1 Material and Procedure
All images used in this experiment contained a
central landmark object and a target object, usu-
ally with a third distractor object. The landmark
was always placed in the middle of a 7-by-7 grid.
Images were divided into 8 groups of 6 images
each. Each image in a group contained the target
object placed in one of 6 different cells on the grid,
numbered from 1 to 6. Figure 6 shows how we
number these target positions according to their
nearness to the landmark. 
 
  
       
       
       
       
       
       
       
1 2 
4 5 a 
6 
g L c 
e 
b 
d f 
3 
Figure 6: Relative locations of landmark (L) target posi-
tions (1..6) and distractor landmark positions (a..g) in images
used in the experiment.
Groups are organised according to the presence
and position of a distractor object. In group a the
distractor is directly above the landmark, in group
b the distractor is rotated 45 degrees clockwise
from the vertical, in group c it is directly to the
right of the landmark, in d it is rotated 135 de-
grees clockwise from the vertical, and so on. The
distractor object is always the same distance from
the central landmark. In addition to the distractor
groups a,b,c,d,e,f and g, there is an eighth group,
group x, in which no distractor object occurs.
In the experiment, each image was displayed
with a sentence of the form The is near the ,
with a description of the target and landmark re-
spectively. The sentence was presented under the
image. 12 participants took part in this experi-
ment. Participants were asked to rate the accept-
ability of the sentence as a description of the im-
age using a 10-point scale, with zero denoting not
acceptable at all; four or five denoting moderately
acceptable; and nine perfectly acceptable.
4.2 Results and Discussion
We assess participants? responses by comparing
their average proximity judgments with those pre-
dicted by the absolute proximity equation (Equa-
tion 1), and by the relative proximity equation
(Equation 3). For both equations we assume
that all objects have a salience score of 1. With
salience equal to 1, the absolute proximity equa-
tion relates proximity between target and land-
mark objects to the distance between those two ob-
jects, so that the closer the target is to the landmark
the higher its proximity will be. With salience
equal to 1, the relative proximity equation re-
lates proximity to both distance between target and
landmark and distance between target and distrac-
tor, so that the proximity of a given target object
to a landmark rises as that target?s distance from
the landmark decreases but falls as the target?s dis-
tance from some other distractor object decreases.
Figure 7 shows graphs comparing participants?
proximity ratings with the proximity scores com-
puted by Equation 1 (the absolute proximity equa-
tion), and by Equation 3 (the relative proximity
equation), for the images in group x and in the
other 7 groups. In the first graph there is no dif-
ference between the proximity scores computed
by the two equations, since, when there is no dis-
tractor object present the relative proximity equa-
tion reduces to the absolute proximity equation.
The correlation between both computed proximity
scores and participants? average proximity scores
for this group is quite high (r = 0.95). For the re-
maining 7 groups the proximity value computed
from Equation 1 gives a fair match to people?s
proximity judgements for target objects (the aver-
age correlation across these seven groups in Fig-
ure 7 is around r = 0.93). However, relative
proximity score as computed in Equation 3 signifi-
cantly improves the correlation in each graph, giv-
ing an average correlation across the seven groups
of around r = 0.99 (all correlations in Figure 7
are significant p < 0.01).
Given that the correlations for both Equation 1
and Equation 3 are high we examined whether the
results returned by Equation 3 were reliably closer
to human judgements than those from Equation 1.
For the 42 images where a distractor object was
present we recorded which equation gave a result
that was closer to participants? normalised aver-
749
??
?
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
????????????????????????????
? ??? ? ?????? ???????
????????????????????????????
? ??? ? ?????? ???????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
????????? ???????????
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ???????? ??? ??
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
? ??? ??? ???? ?????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
??????? ?????????????
????????????????????????????
??????? ???????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
? ??? ??? ???? ?????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
????????????????????
?????????????????
Figure 7: comparison between normalised proximity scores observed and computed for each group.
age for that image. In 28 cases Equation 3 was
closer, while in 14 Equation 1 was closer (a 2:1
advantage for Equation 3, significant in a sign test:
n+ = 28, n? = 14, Z = 2.2, p < 0.05). We con-
clude that proximity judgements for objects in our
experiment are best represented by relative prox-
imity as computed in Equation 3. These results
support our ?relative? model of proximity.2
It is interesting to note that Equation 3 over-
estimates proximity in the cases (a, b and g)
2Note that, in order to display the relationship between
proximity values given by participants, computed in Equa-
tion 1, and computed in Equation 3, the values displayed in
Figure 7 are normalised so that proximity values have a mean
of 0 and a standard deviation of 1. This normalisation simply
means that all values fall in the same region of the scale, and
can be easily compared visually.
where the distractor object is closest to the targets
and slightly underestimates proximity in all other
cases. We will investigate this in future work.
5 Expressing spatial proximity
We use the model of ?3 to interpret spatial ref-
erences to objects. A fundamental requirement
for processing situated dialogue is that linguistic
meaning provides enough information to establish
the visual grounding of spatial expressions: How
can the robot relate the meaning of a spatial ex-
pression to a scene it visually perceives, so it can
locate the objects which the expression applies to?
Approaches agree here on the need for ontolog-
ically rich representations, but differ in how these
are to be visually grounded. Oates et al (2000)
750
and Roy (2002) use machine learning to obtain
a statistical mapping between visual and linguis-
tic features. Gorniak and Roy (2004) use manu-
ally constructed mappings between linguistic con-
structions, and probabilistic functions which eval-
uate whether an object can act as referent, whereas
DeVault and Stone (2004) use symbolic constraint
resolution. Our approach to visual grounding of
language is similar to the latter two approaches.
We use a Combinatory Categorial Grammar
(CCG) (Baldridge and Kruijff, 2003) to describe
the relation between the syntactic structure of
an utterance and its meaning. We model mean-
ing as an ontologically richly sorted, relational
structure, using a description logic-like framework
(Baldridge and Kruijff, 2002). We use OpenCCG
for parsing and realization.3
(2) the box near the ball
@{b:phys?obj}(box
& ?Delimitation?unique
& ?Number?singular
& ?Quantification?specific singular)
& @{b:phys?obj}?Location?(r : region & near
& ?Proximity?proximal
& ?Positioning?static)
& @{r :region}?FromWhere?(b1 : phys ? obj
& ball
& ?Delimitation?unique
& ?Number?singular
& ?Quantification?specific singular)
Example (2) shows the meaning representation
for ?the box near the ball?. It consists of sev-
eral, related elementary predicates (EPs). One
type of EP represents a discourse referent as a
proposition with a handle: @{b:phys?obj}(box)
means that the referent b is a physical object,
namely a box. Another type of EP states de-
pendencies between referents as modal relations,
e.g. @{b:phys?obj}?Location?(r : region & near)
means that discourse referent b (the box) is located
in a region r that is near to a landmark. We repre-
sent regions explicitly to enable later reference to
the region using deictic reference (e.g. ?there?).
Within each EP we can have semantic features,
e.g. the region r characterizes a static location of b
and expresses proximity to a landmark. Example
(2) gives a ball in the context as the landmark.
We use the sorting information in the utter-
ance?s meaning (e.g. phys-obj, region) for further
3http://www.sf.net/openccg/
interpretation using ontology-based spatial rea-
soning. This yields several inferences that need to
hold for the scene, like DeVault and Stone (2004).
Where we differ is in how we check whether these
inferences hold. Like Gorniak and Roy (2004), we
map these conditions onto the energy landscape
computed by the proximity field functions. This
enables us to take into account inhibition effects
arising in the actual situated context, unlike Gor-
niak & Roy or DeVault & Stone.
We convert relative proximity fields into prox-
imal regions anchored to landmarks to contextu-
ally interpret linguistic meaning. We must decide
whether a landmark?s relative proximity score at
a given point indicates that it is ?near? or ?close
to? or ?at? or ?beside? the landmark. For this we
iterate over each point in the scene, and compare
the relative proximity scores of the different land-
marks at each point. If the primary landmark?s
(i.e., the landmark with the highest relative prox-
imity at the point) relative proximity exceeds the
next highest relative proximity score by more than
a predefined confidence interval the point is in the
vague region anchored around the primary land-
mark. Otherwise, we take it as ambiguous and not
in the proximal region that is being interpreted.
The motivation for the confidence interval is to
capture situations where the difference in relative
proximity scores between the primary landmark
and one or more landmarks at a given point is rel-
atively small. Figure 8 illustrates the parsing of a
scene into the regions ?near? two landmarks. The
relative proximity fields of the two landmarks are
identical to those in Figure 5, using a confidence
interval of 0.1. Ambiguous points are where the
proximity ambiguity series is plotted at 0.5. The
regions ?near? each landmark are those areas of
the graph where each landmark?s relative proxim-
ity series is the highest plot on the graph.
Figure 8 illustrates an important aspect of our
model: the comparison of relative proximity fields
naturally defines the extent of vague proximal re-
gions. For example, see the region right of L2 in
Figure 8. The extent of L2?s proximal region in
this direction is bounded by the interference ef-
fect of L1?s relative proximity field. Because the
landmarks? relative proximity scores converge, the
area on the far right of the image is ambiguous
with respect to which landmark it is proximal to.
In effect, the model captures the fact that the area
is relatively distant from both landmarks. Follow-
751
Figure 8: Graph of ambiguous regions overlaid on relative
proximity fields for landmarks L1 and L2, with confidence
interval=0.1 and different salience scores for L1 (0.5) and L2
(0.6). Locations of landmarks are marked on the X-axis.
ing the cognitive load model (?1), objects located
in this region should be described with a projective
relation such as ?to the right of L2? rather than a
proximal relation like ?near L2?, see Kelleher and
Kruijff (2006).
6 Conclusions
We addressed the issue of how we can provide
a context-dependent interpretation of spatial ex-
pressions that identify objects based on proxim-
ity in a visual scene. We discussed available
psycholinguistic data to substantiate the useful-
ness of having such a model for interpreting and
generating fluent situated dialogue between a hu-
man and a robot, and that we need a context-
dependent representation of what is (situationally)
appropriate to consider proximal to a landmark.
Context-dependence thereby involves salience of
landmarks as well as inhibition effects between
landmarks. We presented a model in which we
can address these issues, and we exemplified how
logical forms representing the meaning of spa-
tial proximity expressions can be grounded in this
model. We tested and verified the model using a
psycholinguistic experiment. Future work will ex-
amine whether the model can be used to describe
the semantics of nouns (such as corner) that ex-
press vague spatial extent, and how the model re-
lates to the functional aspects of spatial reasoning.
References
J. Baldridge and G.J.M. Kruijff. 2002. Coupling CCG and
hybrid logic dependency semantics. In Proceedings of
ACL 2002, Philadelphia, Pennsylvania.
J. Baldridge and G.J.M. Kruijff. 2003. Multi-modal combi-
natory categorial grammar. In Proceedings of EACL 2003,
Budapest, Hungary.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collab-
orative process. Cognition, 22:1?39.
K.R. Coventry and S. Garrod. 2004. Saying, Seeing and
Acting. The Psychological Semantics of Spatial Preposi-
tions. Essays in Cognitive Psychology Series. Lawrence
Erlbaum Associates.
R. Dale and E. Reiter. 1995. Computatinal interpretations of
the gricean maxims in the generation of referring expres-
sions. Cognitive Science, 18:233?263.
D. DeVault and M. Stone. 2004. Interpreting vague utter-
ances in context. In Proceedings of COLING 2004, vol-
ume 2, pages 1247?1253, Geneva, Switzerland.
K.P. Gapp. 1994. Basic meanings of spatial relations: Com-
putation and evaluation in 3d space. In Proceedings of
AAAI-94, pages 1393?1398.
P. Gorniak and D. Roy. 2004. Grounded semantic compo-
sition for visual scenes. Journal of Artificial Intelligence
Research, 21:429?470.
E. Hajicova?. 1993. Issues of Sentence Structure and Dis-
course Patterns, volume 2 of Theoretical and Computa-
tional Linguistics. Charles University Press.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
J.D. Kelleher and G.J. Kruijff. 2006. Incremental genera-
tion of spatial referring expressions in situated dialog. In
Proceedings ACL/COLING ?06, Sydney, Australia.
J. Kelleher and J. van Genabith. 2004. Visual salience and
reference resolution in simulated 3d environments. AI Re-
view, 21(3-4):253?267.
E. Krahmer and M. Theune. 1999. Efficient generation of
descriptions in context. In R. Kibble and K. van Deemter,
editors, Workshop on the Generation of Nominals, ESS-
LLI?99, Utrecht, The Netherlands.
G.D. Logan and D.D. Sadler. 1996. A computational analy-
sis of the apprehension of spatial relations. In M. Bloom,
P.and Peterson, L. Nadell, and M. Garrett, editors, Lan-
guage and Space, pages 493?529. MIT Press.
G.D. Logan. 1994. Spatial attention and the apprehension
of spatial relations. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036.
G.D. Logan. 1995. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533.
R. Moratz and T. Tenbrink. 2006. Spatial reference in
linguistic human-robot interaction: Iterative, empirically
supported development of a model of projective relations.
Spatial Cognition and Computation.
T. Oates, Z. Eyler-Walker, and P.R. Cohen. 2000. Toward
natural language interfaces for robotic agents: Ground-
ing linguistic meaning in sensors. In Proceedings of the
Fourth International Conference on Autonomous Agents,
pages 227?228.
T Regier and L. Carlson. 2001. Grounding spatial language
in perception: An empirical and computational investi-
gation. Journal of Experimental Psychology: General,
130(2):273?298.
D.K. Roy. 2002. Learning words and syntax for a scene
description task. Computer Speech and Language, 16(3).
I.F. van der Sluis and E.J. Krahmer. 2004. The influence of
target size and distance on the production of speech and
gesture in multimodal referring expressions. In R. Kibble
and K. van Deemter, editors, ICSLP04.
752
Anchor-Progression in Spatially Situated Discourse:
a Production Experiment
Hendrik Zender and Christopher Koppermann and Fai Greeve and Geert-Jan M. Kruijff
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
Saarbru?cken, Germany
zender@dfki.de
Abstract
The paper presents two models for produc-
ing and understanding situationally appro-
priate referring expressions (REs) during
a discourse about large-scale space. The
models are evaluated against an empirical
production experiment.
1 Introduction and Background
For situated interaction, an intelligent system
needs methods for relating entities in the world,
its representation of the world, and the natural lan-
guage references exchanged with its user. Hu-
man natural language processing and algorithmic
approaches alike have been extensively studied
for application domains restricted to small visual
scenes and other small-scale surroundings. Still,
rather little research has addressed the specific is-
sues involved in establishing reference to entities
outside the currently visible scene. The challenge
that we address here is how the focus of attention
can shift over the course of a discourse if the do-
main is larger than the currently visible scene.
The generation of referring expressions (GRE)
has been viewed as an isolated problem, focussing
on efficient algorithms for determining which in-
formation from the domain must be incorporated
in a noun phrase (NP) such that this NP allows
the hearer to optimally understand which referent
is meant. The domains of such approaches usu-
ally consist of small, static domains or simple vi-
sual scenes. In their seminal work Dale and Reiter
(1995) present the Incremental Algorithm (IA) for
GRE. Recent extensions address some of its short-
comings, such as negated and disjoined properties
(van Deemter, 2002) and an account of salience for
generating contextually appropriate shorter REs
(Krahmer and Theune, 2002). Other, alternative
GRE algorithms exist (Horacek, 1997; Bateman,
1999; Krahmer et al, 2003). However, all these al-
gorithms rely on a given domain of discourse con-
stituting the current context (or focus of attention).
The task of the GRE algorithm is then to single out
the intended referent against the other members of
the context, which act as potential distractors. As
long as the domains are such closed-context sce-
narios, the intended referent is always in the cur-
rent focus. We address the challenge of producing
and understanding of references to entities that are
outside the current focus of attention, because they
have not been mentioned yet and are beyond the
currently observable scene.
Our approach relies on the dichotomy between
small-scale space and large-scale space for hu-
man spatial cognition. Large-scale space is ?a
space which cannot be perceived at once; its global
structure must be derived from local observations
over time? (Kuipers, 1977). In everyday situa-
tions, an office environment, one?s house, or a uni-
versity campus are large-scale spaces. A table-top
or a part of an office are examples of small-scale
space. Despite large-scale space being not fully
observable, people can nevertheless have a rea-
sonably complete mental representation of, e.g.,
their domestic or work environments in their cog-
nitive maps. Details might be missing, and peo-
ple might be uncertain about particular things and
states of affairs that are known to change fre-
quently. Still, people regularly engage in a con-
versation about such an environment, making suc-
cessful references to spatially located entities.
It is generally assumed that humans adopt a par-
tially hierarchical representation of spatial orga-
nization (Stevens and Coupe, 1978; McNamara,
1986). The basic units of such a representation
are topological regions (i.e., more or less clearly
bounded spatial areas) (Hirtle and Jonides, 1985).
Paraboni et al (2007) are among the few to ad-
dress the issue of generating references to entities
outside the immediate environment, and present
an algorithm for context determination in hierar-
...
...
... ...
...
office1 office4 office1
floor1 floor2
building 1A building 3B
old campus
kitchen office2 helpdesk office3office5
floor1 floor2 floor1
building 2C building 3B
new campus
Dienstag, 14. April 2009
(a) Example for a hierarchical representation of space.
(b) Illustration of the TA principle: starting from the atten-
tional anchor (a), the smallest sub-hierarchy containing both
a and the intended referent (r) is formed incrementally.
Figure 1: TA in a spatial hierarchy.
chically ordered domains. However, since it is
mainly targeted at producing textual references to
entities in written documents (e.g., figures and ta-
bles in book chapters), they do not address the
challenges of physical and perceptual situated-
ness. Large-scale space can be viewed as a hier-
archically ordered domain. To keep track of the
referential context in such a domain, in our previ-
ous work we propose the principle of topological
abstraction (TA, summarized in Fig. 1) for context
extension (Zender et al, 2009a), similar to Ances-
tral Search (Paraboni et al, 2007). In (Zender et
al., 2009b), we describe the integration of the ap-
proach in an NLP system for situated human-robot
dialogues and present two algorithms instantiating
the TA principle for GRE and resolving referring
expressions (RRE), respectively. It relies on two
parameters: the location of the intended referent
r, and the attentional anchor a. As discussed in
our previous works, for single utterances the an-
chor is the physical position where it is made (i.e.,
the utterance situation (Devlin, 2006)). Below, we
propose models for attentional anchor-progression
for longer discourses about large-scale space, and
evaluate them against real-world data.
2 The Models
In order to account for the determination of the
attentional anchor a, we propose a model called
anchor-progression A. The model assumes that
each exophoric reference1 serves as attentional
anchor for the subsequent reference. It is based
on observations on ?principles for anchoring re-
source situations? by Poesio (1993), where the ex-
pression of movement in the domain determines
1This excludes pronouns as well as other descriptions that
pick up an existing referent from the linguistic context.
the updated current mutual focus of attention. a
and r are then passed to the TA algorithm. Taking
into account the verbal behavior observed in our
experiment, we also propose a refined model of
anchor-resetting R, where for each new turn (e.g.,
a new instruction), the anchor is re-set to the utter-
ance situation. R leads to the inclusion of naviga-
tional information for each first RE in a turn, thus
reassuring the hearer of the focus of attention.
3 The Experiment
We are interested in the way the disambiguation
strategies change when producing REs during a
discourse about large-scale space versus discourse
about small-scale space. In our experiment, we
gathered a corpus of spoken instructions in two
different situations: small-scale space (SSS) and
large-scale space (LSS). We use the data to evalu-
ate the utility of the A and R models. We specifi-
cally evaluate them against the traditional (global)
model G in which the indented referent must be
singled out from all entities in the domain.
The cover story for the experiment was to
record spoken instructions to help improve a
speech recognition system for robots. The partici-
pants were asked to imagine an intelligent service
robot capable of understanding natural language
and familiar with its environment. The task of the
participants was to instruct the robot to clean up
a working space, i.e., a table-top (SSS) and an in-
door environment (LSS) by placing target objects
(cookies or balls) in boxes of the same color. The
use of color terms to identify objects was discour-
aged by telling the participants that the robot is un-
able to perceive color. The stimuli consisted of 8
corresponding scenes of the table-top and the do-
mestic setting (cf. Fig. 2). In order to preclude the
specific phenomena of collaborative, task-oriented
dialogue (cf., e.g., (Garrod and Pickering, 2004)),
the participants had to instruct an imaginary recip-
ient of orders. The choice of a robot was made to
rule out potential social implications when imag-
ining, e.g., talking to a child, a butler, or a friend.
The SSS scenes show a bird?s-eye view of the
table including the robot?s position (similar to (Fu-
nakoshi et al, 2004)). The way the objects are ar-
ranged allows to refer to their location with respect
to the corners of the table, with plates as additional
landmarks. The LSS scenes depict an indoor envi-
ronment with a corridor and, parallel to SSS, four
rooms with tables as landmarks. The scenes show
Table 1: Example from the small-scale (1?2) and large-scale space (3?4) scenes in Fig. 2.
1. nimm [das pla?tzchen unten links]mG,A , leg es [in die schachtel unten rechts auf dem teller]oG,A
?take the cookie on the bottom left, put it into the bottom right box on the plate?
2. nimm [das pla?tzchen unten rechts]mG,oA , leg es [in die schachtel oben links auf dem teller]mG,A
?take the cookie on the bottom right, put it into the top left box on the plate?
3. geh [ins wohnzimmer]mG,A,R und nimm [den ball]uG,mA,R und bring ihn [ins arbeitszimmer]mG,A,R , leg ihn [in die
kiste auf dem tisch]uG,oA,R
?go to the living room and take the ball and bring it to the study; put it into the box on the table?
4. und nimm [den ball]uG,R,mA und bring ihn [in die ku?che]mG,A,R und leg ihn [in die kiste auf dem boden]uG,mA,R
?and take the ball and bring it to the kitchen and put it into the box on the floor?
(a) Small-scale space: squares represent small boxes,
stars cookies, and white circles plates.
ArbeitszimmerK?che
Wohnzimmer Bad
(b) Large-scale space: squares represent boxes placed on the
floor or on a table, circles represent balls, rooms are labeled.
Figure 2: Two stimuli scenes from the experiment.
the robot and the participant in the corridor.
In order to gather more comparable data we
opted for a within-participants approach. Each
person participated in the SSS treatment and in the
LSS treatment. To counterbalance potential carry-
over effects, half of the participants were shown
the treatments in inverse order, and the sequence
of the 8 scenes in each treatment was varied in a
principled way. In order to make the participants
produce multi-utterance discourses, they were re-
quired to refer to all target object pairs. The exact
wording of their instructions was up to them.
Participants were placed in front of a screen and
a microphone into which they spoke their orders
to the imaginary robot, followed by a self-paced
keyword after which the experimenter showed the
next scene. The experiment was conducted in Ger-
man and consisted of a pilot study (10 partici-
pants) and the main part (19 female and 14 male
students, aged 19?53, German native speakers).
The data of three participants who did not behave
according to the instructions was discarded. The
individual sessions took 20?35 min., and the par-
ticipants were paid for their efforts.
Using the UAM CorpusTool software, tran-
scriptions of the recorded spoken instructions
were annotated for occurrences of the linguistic
phenomenon we are interested in, i.e., REs. Sam-
ples were cross-checked by a second annotator.
REs were marked as shallow ?refex? segments,
i.e., complex NPs were not decomposed into their
constituents. Only definite NPs representing ex-
ophoric REs (cf. Sec. 2) qualify as ?refex? seg-
ments. If a turn contained an indefinite NP, the
whole turn was discarded. The ?refex? segments
were coded according to the amount of informa-
tion they contain, and under which disambigua-
tion model M ? {G,A,R} (R only for LSS)
they succeed in singling out the described refer-
ent. Following Engelhardt et al (2006), we dis-
tinguish three types of semantic specificity. A RE
is an over-description with respect to M (overM )
if it contains redundant information, and it is an
under-description (underM ) if it is ambiguous ac-
cording to M . Minimal descriptions (minM ) con-
tain just enough information to uniquely identify
the referent. Table 1 shows annotated examples.
4 Results
The collected corpus consists of 30 annotated ses-
sions with 2 treatments comprising 8 scenes with
4 turns. In total, it contains 4,589 annotated REs,
out of which only 83 are errors. Except for the
error rate calculation, we only consider non-error
?refex? segments as the universe. The SSS treat-
Table 2: Mean frequencies (with standard deviation in italics) of minimal (min), over-descriptions
(over), and under-descriptions (under) with respect to the models (A, R, G) in both treatments.
overG overA overR minG minA minR underG underA underR
small-scale 13.94% 34.45% 78.90% 60.11% 7.16% 5.43%
space 15.85% 14.37% 17.66% 13.13% 12.07% 10.50%
large-scale 6.81% 34.75% 20.06 % 68.04% 64.55% 76.73% 25.16% 0.69% 3.21%
space 7.53% 12.13% 10.10% 17.87% 13.13% 10.66% 19.48% 1.72% 5.06%
ment contains 1,902 ?refex?, with a mean number
of 63.4 and a std. dev. ?=1.98 per participant. This
corresponds to the expected number of 64 REs to
be uttered: 8 scenes ? 4 target object pairs. The
LSS treatment contains 2,604 ?refex? with an aver-
age of 86.8 correct REs (?=18.19) per participant.
As can be seen in Table 1 (3?4), this difference
is due to the participants? referring to intermediate
waypoints in addition to the target objects. Table 2
summarizes the analysis of the annotated data.
Overall, the participants had no difficulties with
the experiment. The mean error rates are low in
both treatments: 1.78% (?=3.36%) in SSS, and
1.80% (?=2.98%) in LSS. A paired sample t-
test of both scores for each participant shows that
there is no significant difference between the error
rates in the treatments (p=0.985), supporting the
claim that both treatments were of equal difficulty.
Moreover, a MANOVA shows no significant effect
of treatment-order for the verbal behavior under
study, ruling out potential carry-over effects.
Production experiments always exhibit a con-
siderable variation between participants. When
modeling natural language processing systems,
one needs to take this into account. A GRE com-
ponent should produce REs that are easy to un-
derstand, i.e., ambiguities should be avoided and
over-descriptions should occur sparingly. A GRE
algorithm will always try to produce minimal de-
scriptions. The generation of an under-description
means a failure to construct an identifying RE,
while over-descriptions are usually the result of
a globally ?bad? incremental construction of the
generated REs (as is the case, e.g., in the IA). An
RRE component, on the other hand, should be able
to identify as many referents as possible by treat-
ing as few as possible REs as under-descriptions.
The analysis of the SSS data with respect to
G establishes the baseline for a comparison with
other experiments and GRE approaches. 13.9% of
the REs contain redundant information (overG),
compared to 21% in (Viethen and Dale, 2006). In
contrast, however, our SSS scenes did not provide
the possibility for producing more-than-minimal
REs for every target object, which might account
for the difference. underG REs occur with a fre-
quency of 7.2% in the SSS data. Because under-
descriptions result in the the hearer being unable to
reliably resolve the reference, this means that the
robot in our experiment cannot fulfill its task. This
might explain the difference to the 16% observed
in the task-independent study by Viethen and Dale
(2006). The significantly (p<0.001) higher mean
frequency of minG than minA underpins that G
is an accurate model for the verbal behavior in
SSS. However, G does not fit the LSS data well.
An RRE algorithm with model G would fail to
resolve the intended referent in 1 out of 4 cases
(cf. underG in LSS). With only 0.7% underA
REs on average, A models the LSS data signifi-
cantly better (p<0.001). Still, there is is a high
rate of overA REs. In comparison, R yields a
significantly (p<0.001) lower amount of overR.
The mean frequency of underR is significantly
(p=0.010) higher than for underA, but still below
underG in the SSS data. With a mean frequency
of 76.7% minR, R models the data better than
both G and A. For the REs in LSS minR is in
the same range as minG for the REs in SSS.
5 Conclusions
Overall, the data exhibit a high mean frequency of
over-descriptions. However, since this means that
the human-produced REs contain more informa-
tion than minimally necessary, this does not nega-
tively affect the performance of an RRE algorithm.
For a GRE algorithm, however, a more cautious
approach might be desirable. In situated discourse
about LSS, we thus suggest that A is suitable for
the RRE task because it yields the least amount
of unresolvable under-descriptions. For the GRE
task R is more appropriate. It strikes a balance
between producing short descriptions and supple-
menting navigational information.
Acknowledgments
This work was supported by the EU Project CogX
(FP7-ICT-215181). Thanks to Mick O?Donnell
for his support with the UAM CorpusTool.
References
John A. Bateman. 1999. Using aggregation for select-
ing content when generating referring expressions.
In Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics (ACL?99), pages 127?134, Mor-
ristown, NJ, USA.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean Maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Keith Devlin. 2006. Situation theory and situation se-
mantics. In Dov M. Gabbay and John Woods, edi-
tors, Logic and the Modalities in the Twentieth Cen-
tury, volume 7 of Handbook of the History of Logic,
pages 601?664. Elsevier.
Paul E. Engelhardt, Karl G.D. Bailey, and Fernanda
Ferreira. 2006. Do speakers and listeners observe
the Gricean Maxim of Quantity? Journal of Mem-
ory and Language, 54(4):554?573.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generation of
relative referring expressions based on perceptual
grouping. In COLING ?04: Proceedings of the 20th
international conference on Computational Linguis-
tics, Morristown, NJ, USA.
Simon Garrod and Martin J. Pickering. 2004. Why is
conversation so easy? Trends in Cognitive Sciences,
8(1):8?11, January.
Stephen C. Hirtle and John Jonides. 1985. Evidence
for hierarchies in cognitive maps. Memory and Cog-
nition, 13:208?217.
Helmut Horacek. 1997. An algorithm for generating
referential descriptions with flexible interfaces. In
Proceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (ACL-97), pages
206?213, Morristown, NJ, USA.
Emiel Krahmer and Marie?t Theune. 2002. Effi-
cient context-sensitive generation of referring ex-
pressions. In Kees van Deemter and R. Kibble, ed-
itors, Information Sharing: Givenness and Newness
in Language Processing, pages 223?264. CSLI Pub-
lications, Stanford, CA, USA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Benjamin Kuipers. 1977. Representing Knowledge of
Large-scale Space. PhD thesis, MIT-AI TR-418,
Massachusetts Institute of Technology, Cambridge,
MA, USA, May.
Timothy P. McNamara. 1986. Mental representations
of spatial relations. Cognitive Psychology, 18:87?
121.
Ivandre? Paraboni, Kees van Deemter, and Judith Mas-
thoff. 2007. Generating referring expressions:
Making referents easy to identify. Computational
Linguistics, 33(2):229?254, June.
Massimo Poesio. 1993. A situation-theoretic formal-
ization of definite description interpretation in plan
elaboration dialogues. In Peter Aczel, David Israel,
Yasuhiro Katagiri, and Stanley Peters, editors, Sit-
uation Theory and its Applications Volume 3, CSLI
Lecture Notes No. 37, pages 339?374. Center for the
Study of Language and Information, Menlo Park,
CA, USA.
Albert Stevens and Patty Coupe. 1978. Distortions
in judged spatial relations. Cognitive Psychology,
10:422?437.
Kees van Deemter. 2002. Generating referring expres-
sions: boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
Jette Viethen and Robert Dale. 2006. Algorithms
for generating referring expressions: Do they do
what people do? In Proceedings of the 4th Inter-
national Natural Language Generation Conference
(INLG 2006), pages 63?70, Sydney, Australia.
Hendrik Zender, Geert-Jan M. Kruijff, and Ivana
Kruijff-Korbayova?. 2009a. A situated context
model for resolution and generation of referring ex-
pressions. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 126?129, Athens, Greece, March.
Hendrik Zender, Geert-Jan M. Kruijff, and Ivana
Kruijff-Korbayova?. 2009b. Situated resolution
and generation of spatial referring expressions for
robotic assistants. In Proceedings of the Twenty-
First International Joint Conference on Artificial In-
telligence (IJCAI-09), pages 1604?1609, Pasadena,
CA, USA, July.
