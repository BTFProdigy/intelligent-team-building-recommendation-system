Proceedings of NAACL HLT 2007, Companion Volume, pages 129?132,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ADVANCES IN THE CMU/INTERACT ARABIC GALE TRANSCRIPTION SYSTEM   
 
Mohamed Noamany, Thomas Schaaf*, Tanja Schultz 
 
InterACT, Language Technologies Institute, Carnegie Mellon University  
Pittsburgh, PA 15213 
{mfn,tschaaf,tanja@cs.cmu.edu} 
 
                                                 
* Now with Toshiba Research Europe Ltd, Cambridge, United Kingdom 
ABSTRACT 
This paper describes the CMU/InterACT effort in 
developing an Arabic Automatic Speech Recognition 
(ASR) system for broadcast news and conversations 
within the GALE 2006 evaluation. Through the span of 
9 month in preparation for this evaluation we improved 
our system by 40% relative compared to our legacy 
system. These improvements have been achieved by 
various steps, such as developing a vowelized system, 
combining this system with a non-vowelized one, 
harvesting transcripts of TV shows from the web for 
slightly supervised training of acoustic models, as well 
as language model adaptation, and finally fine-tuning 
the overall ASR system. 
Index Terms? Speech recognition, Vowelization, 
GALE, Arabic, Slightly supervised training, web data. 
 
1. INTRODUCTION 
The goal of the GALE (Global Autonomous Language 
Exploitation) program is to develop and apply computer 
software technologies to absorb, analyze and interpret 
huge volumes of speech and text in multiple languages 
and make them available in English. In a long run this 
requires to combine techniques from text 
summarization, information retrieval, machine 
translation, and automatic speech recognition.  NIST 
will perform regular evaluations and the first evaluation 
took place recently. This paper describes improvements 
in the CMU Modern Standard Arabic (MSA) system 
through the span of 9 months in preparation for this 
evaluation.  
One of the language characteristics and challenges of 
Arabic is that some vowels are omitted in the written 
form. These vowels carry grammatical case information 
and may change the meaning of a word. Modeling the 
vowels in the pronunciation dictionary was found to 
give improvements over un-vowelized pronunciations 
[4].  In this paper we achieved another significant 
improvement by combining a vowelized with a non-
vowelized system. Furthermore, we got gains by 
collecting and utilizing web transcripts from TV show, 
which include broadcast conversations.  
 
2. SYSTEM DESCRIPTION 
Our MSA speech recognition system is based on the 
Janus Recognition Toolkit JRTk [9] and the IBIS 
decoder [10].  
Before decoding the audio, an automatic segmentation 
step and a speaker clustering step is performed. The 
segmentation step aims at excluding those segments that 
contain no speech, such as music or background noise.  
The remaining segments are clustered into speaker 
clusters such that all adaptation and normalization steps 
can be processed on clusters as batches. 
 
From the incoming 16 kHz audio signal we extract for 
each segment power spectral features using a FFT with 
a 10ms frame-shift and a 16ms Hamming window. From 
these we compute 13 Mel-Frequency Cepstral 
Coefficients (MFCC) per frame and perform a cepstral 
mean as well as variance normalization on a cluster 
basis. To incorporate dynamic features we concatenate 
15 adjacent MFCC frames (?7) and project these 195 
dimensional features into a 42 dimensional space using 
a transform found by linear discriminate analysis 
(LDA). We use the context-dependent codebooks as 
classes for finding the LDA transform [2]. On top of the 
LDA we apply a single maximum likelihood trained 
Semi-Tied-Covariance (STC) matrix. 
 
The general decoding setup employs a first pass in 
which a speaker independent acoustic model without 
vocal tract length normalization (VTLN) and no 
adaptation is used. The hypotheses of a cluster from the 
first pass are then used to estimate the VTLN warping 
factors to warp the power spectrum using the maximum 
likelihood approach described in [8]. After the VTLN 
factors are found, the same hypotheses are considered to 
estimate a feature space adaptation (FSA) using a 
constrained MLLR (CMLLR) transform. Then a model 
space adaptation is performed using maximum 
likelihood linear regression with multiple regression 
classes. The regression classes are found through 
clustering of the Gaussians in the acoustic model. The 
second pass decoding uses a speaker adaptive trained 
129
acoustic model, in which the adaptation was performed 
using a single CMLLR transform per speaker. 
For the non-vowelized system, we applied a grapheme-
to-phoneme approach to automatically generate the 
pronunciation dictionary. For the vowelized system we 
used the same phoneme set as in the non-vowelized 
system but extended it with the 3 short vowels, which 
do not appear in the writing system. Both systems are 2-
pass system as described above and employ Cepstral 
Mean Normalization (CMN), MLLR, Semi-tied 
covariance (STC), and Feature space adaptation (FSA).  
 
For the development of context dependent acoustic 
models we applied an entropy-based polyphone 
decision tree clustering process using context questions 
of maximum width ?2, resulting in shared quin-phones. 
In addition we included word-boundary tags into the 
pronunciation dictionary, which can be asked for in the 
decision tree can ask for word-boundary tags. The non-
vowelized system uses 4000 phonetically-tied quin-
phones with a total of 305,000 Gaussians. The non-
vowelized system has 5000 codebooks with a total of 
308,000 Gaussians.  
In total we used 190 hours for acoustic training. 
These consist of 40 hours Broadcast news (BN) from 
manually transcribed FBIS data, 50 hours BN LDC-
TDT4 selected from 85 hours using a slightly 
supervised approach as described in [3], and 30 hours 
Broadcast conversation (BC) recorded from Al-jazeera 
TV, and 70 hours (40hrs BN, 30hrs BC) from LDC-
GALE data. For quality reasons we removed some of 
the most recent GALE data from acoustic model 
training.  
4. LANGUAGE MODELING 
The Arabic Giga word corpus distributed by LDC is 
currently the major Arabic text resource for language 
modeling. Since this corpus only covers broadcast news, 
we spidered the web to cover broadcast conversational 
data. We found transcripts for Arabic talk shows on the 
Al-jazeera web site www.al-jazeera.net and collected all 
data available from 1998 to 2005. We excluded all 
material from 2006 to comply the evaluation rules 
which prohibit the use of any data starting February 
2006. In addition to the mentioned data we collected 
BN data from the following source: Al-Akhbar 
(Egyptian daily newspaper 08/2000 to 12/2005) and 
Akhbar Elyom (Egyptian weekly newspaper 08/2000 to 
12/2005). Furthermore, we used unsupervised training 
transcripts from 750 hours BN created and shared by 
IBM. 
 
For language modeling building we used the SRILM 
tool kit from SRI [5]. Since we have 2 kinds of data, 
Broadcast News and Conversation, we built various 
individual 4-grams language models. 11 models were 
then interpolated to create one language model. The 
interpolation weights were selected based on a held out 
data set from BN and BC sources. We found that the 
data from Al-jazeera (both BN & BC) has the highest 
weight comparing to other sources. The resulting final 
language model uses a total number of n-grams is 126M 
and a vocabulary of 219k words. The perplexity of the 
language model is 212 on a test set containing BC and 
BN data. 
5. TV WEB TRANSCRIPTS 
Most of our acoustic and language model training data 
comes from broadcast news. However, since GALE 
targets broadcast news as well as conversations we 
looked for an effective method to increase the training 
data for Arabic BC. We made use of the fact that some 
Arabic TV stations place transcripts for their program 
on the web. These transcripts lack time stamp but 
include acceptable quality of the transcription. 
However, one challenge is that the transcriptions are not 
complete in that they do not include transcripts of 
commercials or any news break that may interrupt the 
show.  In total we recorded 50 hours of Broadcast 
conversation shows from Al-jazeera and used them in 
our acoustic model and language model training by 
performing the following procedures:  
? We manually selected shows from Al-jazeera TV 
? We used a scheduler to automatically start the 
recording of the selected shows.  
? We spidered the web to collect corresponding show 
transcripts from their web site www.aljazeera.net.  
? We automatically processed the transcripts to 
convert the html files to text, convert numbers to 
words and remove any non-Arabic words in the 
shows.  
? We added these shows to our LM data with high 
weight, built a biased LM, and used this LM to 
decode the recorded shows.  
? We aligned the reference (transcripts without time 
stamps) with the decoder output that may contain 
speech recognition errors.  
? We selected only the portions that are correct; we 
did not select any portion with number of words 
less than 3 correct consecutive words.  
? Based on the above criteria we finally selected 30 
hours out of the total 40 hours recordings. 
? We clustered utterances based on BIC criteria 
approach described in [7]. 
  
As a result, we managed to project the time stamp in the 
original transcript such that it can be used for training. 
Using these 30 hours of data resulted in a 7% relative 
improvement on RT04. Since RT04 is broadcast news, 
we expect even higher gains on broadcast 
conversational data. It is worth mentioning that we 
130
applied the same slightly supervised approach to the 
TDT4 data which is a low quality quick transcription. 
We selected 50 out of 80 hours and achieved an 
improvement of 12% relative. The gain was higher 
since at the time of these experiments we had only 40 
hours of training from FBIS data, therefore more than 
doubled the amount of training data by adding TDT4. 
 
6. NON-VOWELIZED SYSTEM 
Arabic spelling is mostly phonemic; there is a close 
letter-to-sound correspondence. We used a grapheme-
to-phoneme approach similar to [1]. Our phoneme set 
contains 37 phonemes plus three special phonemes for 
silence, non-speech events, and non-verbal effects, such 
as hesitation. 
 We preprocessed the text by mapping the 3 shapes of 
the grapheme for glottal stops to one shape at the 
beginning of the word since these are frequently miss-
transcribed. This preprocessing step leads to 20% 
reduction in perplexity of our language model and 0.9% 
improvements in the final WER performance on RT04. 
Preprocessing of this kind appears to be appropriate 
since the target of the project is not transcription but 
speech translation and the translation community 
applies the same pre-processing. We used a vocabulary 
of 220K words selected by including all words 
appearing in the acoustic transcripts and the most 
frequent words occurring in the LM. The OOV rate is 
1.7% on RT04. Table 1 shows the performance of our 
Speaker-Independent (SI) and Speaker-Adaptive (SA) 
non-vowelized system on the RT04 set. 
 
   Table 1: Non-vowelized System Results 
            System     WER on RT04 (%) 
Non-Vowelized         (SI)           25.3 
Non-Vowelized         (SA)           20.8 
 
7. VOWELIZED SYSTEM 
Written MSA lacks vowels, thus native speakers add 
them during reading. Vowels are written only in 
children books or traditional religious books. To restore 
vowels for a 129K vocabulary [4], we performed the 
following steps:   
? Buckwalter morphological analyzer (BMA) (found 
106K out of 129K entries). 
? If a word is not vowelized by the analyzer, we 
check for its vowelization in the LDC Arabic Tree-
Bank (additional 5k entries found). 
? If the word did not appear in any of those, we used 
the written non-vowelized word form.  
In total 11k entries could not be resolved by either the 
BMA or the Treebank. 
This vowelization step resulted in 559,035 
pronunciations for the 129k words in our vocabulary, 
i.e. we have on average 5 pronunciations per word. To 
reduce the number of pronunciation variants we 
performed a forced alignment and excluded 
pronunciations which did not occur in the training 
corpus. This results in 407,754 pronunciations, which is 
a relative reduction of about 27%. For system training 
we used the same vocabulary and applied the same 
training procedure as in the non-vowelized system for 
acoustic model training.  
As Table 2 shows, we achieved a very good gain of 
1.3% absolute on the SI pass and 1.5% on the SA pass, 
both benchmarked on RT04 (compare Table 1). We 
envision to seeing even higher improvements after 
estimating and applying probability priors to multiple 
pronunciation and after vowelizing the remainder 11k 
words that had not been covered by BMA or the Tree-
Bank.  
 
   Table2: Vowelized System Results 
           System  WER on RT04 (%) 
Vowelized         (SI)        24.0 
Vowelized         (SA)       19.3 
 
8. COMBINING VOWELIZED & NON-
VOWELIZED SYSTEM 
After seeing significant improvements by vowelization, 
we investigated the performance gain through cross-
adapting the vowelized system with the non-vowelized 
system. The vowelized system cross adapted with the 
SA non-vowelized gave us 1.3 over the vowelized 
system  adapted on the SI vowelized system. We used a 
3-pass decoding strategy, in which the first pass uses the 
speaker independent (SI) vowelized system, the second 
pass uses the speaker adaptive (SA) non-vowelized 
system, and the third, final pass, uses the speaker 
adaptive vowelized system.  Some challenges for the 
cross-adaptations had to be overcomed, for instance to 
cross adapt the non-vowelized system on the vowelized 
system, we had to remove the vowels to have a non-
vowelized transcript. Since the phoneme set of the non-
vowelized system is a subset of the phoneme set of the 
vowelized system, we could simply exclude the vowel 
phonemes from the vowelized system.  Furthermore, the 
search vocabulary is the same and so is the language 
model.  
The main changes are the pronunciation dictionary and 
the decision tree. We tried different combination 
schemes, e.g. by starting with the non-vowelized 
system, then the vowelized, and then the non-vowelized 
but found that none outperforms the combination 
reported here in terms of WER. In addition starting with 
the non-vowelized SI pass is much faster than the 
vowelized SI system (4.5RT compared to 9RT). 
131
 Table 3: Non-vowelized & vowelized System 
Combination 
            System    WER on RT04 (%) 
Vowelized         (SI)           24.0 
Non-Vowelized (SA)           19.9 
Vowelized         (SA)           18.3 
 
9.  ACOUSTIC MODEL PARAMETER TUNING 
We started our legacy system with 40 hours and until it 
reached 90 hours we were using the same number of 
codebooks (3000) and same number of Gaussians (64) 
per codebook. With the increase of training data from 
90 hours to 190 hours we investigated the effect of 
increasing the number of codebooks and Gaussians. 
Also, we were using merge and split training (MAS) 
and STC only for the adapted pass; we furthermore 
investigated the effect of using it for the SI pass. We 
found that using MAS & STC on the SI pass gave us a 
gain of 5% relative on the SI pass. In addition we found 
that the ideal number of codebooks is 5000 for the non-
vowelized system resulting in a gain of 5.3% relative on 
the SI pass.  We expect to see further gains on the SA 
pass. Table 4 summarizes the system performance using 
different parameter sizes and training schemes.  
 
Table 4: System Performance vs.Model Size  
#codebooks MAS #Gausians Voc System WER(%) 
3K - 64K 129 Non-
vow(NV) 
29.6 
3K Mas 64K 129      NV 28.3 
5K Mas 64K 129      NV 27.9 
5K Mas 100K 129      NV 27.6 
5K Mas 100K 200 nv+tv 
TRANS 
26.3 
3K Mas 100K 200 vow+ 
tvTRANS 
24.0 
 
10. SYSTEM EVOLUTION 
Table 5 shows the gains we achieved at major milestone 
stages while building the system. The key improvements 
are due to adding data collected from the web, 
Vowelization, and combining the vowelized and non-
vowelized systems. Tuning the acoustic models 
parameters gave us a good gain and finally the 
interpolation of different language model for different 
sources gave additional improvements. The real-time 
behavior of the system improved from 20RT to 10 RT 
while loosing only 0.2% which is in acceptable trade-
off. Recently, we gained 3.5% relative applying 
discriminative training (MMIE).  
 
11. CONCLUSION 
We presented the CMU 2006 GALE ASR Arabic 
system. It can be seen that we achieved 40% 
improvements over our legacy system. 
Table 5: System Progress WER (%) 
LEGACY SYSTEM  32.7 
STC+VTLN 30.1 
SPEED FROM 20RT TO 10RT 30.3 
FROM 3 TO 4GM+BETTER 
SEGMENTATION 
28.4 
TDT4 TRANSCIPTS SELECTION 
REFINEMENT 
26.3 
CLUSTERING REFINEMENT & 
RETRAINING 
25.5 
MORE LM DATA +INTERPOLATING 11 LMS 24.2 
ADDITION Q3 OF LDC DATA 23.6 
ACOUSTIC MODEL PARAMETER TUNING 20.7 
MMIE 20.0 
COMBINED SYSTEMS (VOW+NON-VOW) 18.3 
 
We combined a vowelized and a non-vowelized system 
and achieved 4.0% relative over the vowelized system. 
Also, we managed to use TV web transcript as a method 
to cover the shortage of training data specially the 
broadcast conversation. Currently, we are exploring 
more on the vowelized system by adding weights to 
different multiple pronunciations and adding 
vowelization to words not covered by the morphological 
analyzer or the tree-bank. 
12. ACKNOWLEGMENTS 
We also would like to thank Qin Jin for applying her 
automatic clustering techniques to the web data.  
 
13. REFERENCES 
[1] J. Billa, M. Noamany, A. Srivastava,  D. Liu,  R. Stone, J. 
Xu,  J. Makhoul, and F. Kubala, ?Audio Indexing of 
Arabic Broadcast News?,   International Conference on 
Acoustics, Speech, and Signal Processing  (ICASSP), 
May 2002. 
[2] H. Yu, Y-C. Tam, T. Schaaf, S. St?ker, Q. Jin, M. 
Noamany, and T. Schultz ?The ISL RT04 Mandarin 
Broadcast News Evaluation System?, EARS Rich 
Transcription workshop, Palisades, NY, 2004. 
[3] L. Nguyen et al, ?Light supervision in acoustic model 
training,? ICASSP , Montreal, QC, Canada, May 2004. 
 [4] M. Afify et al,"Recent progress in Arabic broadcast news 
transcription at BBN", In INTERSPEECH-2005. 
[5] A. Stolcke SRILM- An Extensible Language Modeling 
ToolKit  ICSLP. 2002, Denver, Colorado. 
 [6] T.Buckwalter, ?Issues in Arabic Orthography and 
morphology Analysis?, COLING 2004, Geneva, 2004. 
[7] Q. Jin, T. Schultz, ?Speaker segmentation and clustering 
in meetings?, ICSLP, 2004. 
[8] P. Zhan, M. Westphal, "Speaker Normalization Based On 
Frequency Warping", ICASSP 1997, Munich, Germany. 
[9] M. Finke, et al, "The Karlsruhe Verbmobil Speech 
Recognition Engine," International Conference on 
Acoustics, Speech, and Signal Processing, ICASSP,1997. 
[10] H. Soltau, et al, "A One Pass-Decoder Based On 
Polymorphic Linguistic Context",  ASRU 2001, Trento, 
Italy, 2001 
132
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77?80,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recent Improvements in the
CMU Large Scale Chinese-English SMT System
Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao,
Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu
Abstract
In this paper we describe recent improvements
to components and methods used in our statis-
tical machine translation system for Chinese-
English used in the January 2008 GALE eval-
uation. Main improvements are results of
consistent data processing, larger statistical
models and a POS-based word reordering ap-
proach.
1 Introduction
Building a full scale Statistical Machine Transla-
tion (SMT) system involves many preparation and
training steps and it consists of several components,
each of which contribute to the overall system per-
formance. Between 2007 and 2008 our system im-
proved by 5 points in BLEU from 26.60 to 31.85
for the unseen MT06 test set, which can be mainly
attributed to two major points.
The fast growth of computing resources over
the years make it possible to use larger and larger
amounts of data in training. In Section 3 we show
how parallelizing model training can reduce training
time by an order of magnitude and how using larger
training data as well as more extensive models im-
prove translation quality.
Word reordering is still a difficult problem in
SMT. In Section 4 we apply a Part Of Speech (POS)
based syntactic reordering model successfully to our
large Chinese system.
1.1 Decoder
Our translation system is based on the CMU
SMT decoder as described in (Hewavitharana et
al., 2005). Our decoder is a phrase-based beam
search decoder, which combines multiple models
e.g. phrase tables, several language models, a dis-
tortion model ect. in a log-linear fashion. In order
to find an optimal set of weights, we use MER train-
ing as described in (Venugopal et al, 2005), which
uses rescoring of the top n hypotheses to maximize
an evaluation metric like BLEU or TER.
1.2 Evaluation
In this paper we report results using the BLEU met-
ric (Papineni et al, 2002), however as the evaluation
criterion in GALE is HTER (Snover et al, 2006), we
also report in TER (Snover et al, 2005).
We used the test sets from the NIST MT evalua-
tions from the years 2003 and 2006 as development
and unseen test data.
1.3 Training Data
In translation model training we used the Chinese-
English bilingual corpora relevant to GALE avail-
able through the LDC1. After sentence alignment
these sources add up to 10.7 million sentences with
301 million running words on the English side. Our
preprocessing steps include tokenization on the En-
glish side and for Chinese: automatic word segmen-
tation using the revised version of the Stanford Chi-
nese Word Segmenter2 (Tseng et al, 2005) from
2007, replacement of traditional by simplified Chi-
nese characters and 2-byte to 1-byte ASCII charac-
ter normalization. After data cleaning steps like e.g.
removal of sentence pairs with very unbalanced sen-
1http://projects.ldc.upenn.edu/gale/data/catalog.html
2http://nlp.stanford.edu/software/segmenter.shtml
77
tence length etc., we used the remaining 10 million
sentences with 260 million words (English) in trans-
lation model training (260M system).
2 Number Tagging
Systematic tagging and pre-translation of numbers
had shown significant improvements for our Arabic-
English system, so we investigated this for Chinese-
English. The baseline for these experiments was a
smaller system with 67 million words (67M) bilin-
gual training data (English) and a 500 million word
3-gram LM with a BLEU score of 27.61 on MT06.
First we pre-translated all numbers in the testdata
only, thus forcing the decoder to treat the numbers as
unknown words. Probably because the system could
not match longer phrases across the pre-translated
numbers, the overall translation quality degraded by
1.6 BLEU to 26.05 (see Table 1).
We then tagged all numbers in the training corpus,
replaced them with a placeholder tag and re-trained
the translation model. This reduced the vocabu-
lary and enabled the decoder to generalize longer
phrases across numbers. This strategy did not lead to
the expected result, the BLEU score for MT06 only
reached 25.97 BLEU.
System MT03 MT06
67M baseline 31.45/60.93 27.61/62.18
test data tagged ? 26.06/63.36
training data tagged 29.07/62.52 25.97/63.39
Table 1: Number tagging experiments, BLEU/TER
Analysing this in more detail, we found, the rea-
son for this degradation in translation quality could
be the unbalanced occurrence of number tags in the
training data. From the bilingual sentence pairs,
which contain number tags, 66.52% do not contain
the same number of tags on the Chinese and the En-
glish side. As a consequence 52% of the phrase pairs
in the phrase table, which contain number tags had
to be removed, because the tags were unbalanced.
This hurts system performance considerably.
3 Scaling up to Large Data
3.1 Language Model
Due to the availability of more computing resources,
we were able to extend the language model history
from 4- to 5-gram, which improved translation qual-
ity from 29.49 BLEU to 30.22 BLEU for our large
scale 260M system (see Table 2). This shows, that
longer LM histories help if we are able to use enough
data in model training.
System MT03 MT06
260M, 4gram 31.20/61.00 29.49/61.00
260M, 5gram 32.20/60.59 30.22/60.81
Table 2: 4- and 5-gram LM,260M system, BLEU/TER
The language model was trained on the sources
from the English Gigaword Corpus V3, which con-
tains several newspapers for the years between 1994
to 2006. We also included the English side of the
bilingual training data, resulting in a total of 2.7 bil-
lion running words after tokenization.
We trained separate open vocabulary language
models for each source and interpolated them using
the SRI Language Modeling Toolkit (Stolcke, 2002).
Table 3 shows the interpolation weights for the dif-
ferent sources. Apart from the English part of the
bilingual data, the newswire data from the Chinese
Xinhua News Agency and the Agence France Press
have the largest weights. This reflects the makeup of
the test data, which comes in large parts from these
sources. Other sources, as for example the UN par-
lamentary speeches or the New York Times, differ
significantly in style and vocabulary from the test
data and therefore get small weights.
xin 0.30 cna 0.06 nyt 0.03
bil 0.26 un 0.07 ltw 0.01
afp 0.21 apw 0.05
Table 3: LM interpolation weights per source
3.2 Speeding up Model Training
To accelerate the training of word alignment
models we implemented a distributed version of
GIZA++ (Och and Ney, 2003), based on the latest
version of GIZA++ and a parallel version developed
at Peking University (Lin et al, 2006). We divide the
bilingual training data in equal parts and distribute it
over several processing nodes, which perform align-
ment independently. In each iteration the nodes read
the model from the previous step and output all nec-
essary counts from the data for the models, e.g. the
78
co-occurrence or fertility model. A master process
collects the counts from the nodes, normalizes them
and outputs the intermediate model for each itera-
tion.
This distributed GIZA++ version finished training
the word alignment up to IBM Model 4 for both lan-
guage directions on the full bilingual corpus (260
million words, English) in 39 hours. On average
about 11 CPUs were running concurrently. In com-
parison the standard GIZA++ implementation fin-
ished the same training in 169 hours running on 2
CPUs, one for each language direction.
We used the Pharaoh/Moses package (Koehn et
al., 2007) to extract and score phrase pairs using the
grow-diag-final extraction method.
3.3 Translation Model
We trained two systems, one on the full data and one
without the out-of-domain corpora: UN parlament,
HK hansard and HK law parallel texts. These parla-
mentary sessions and law texts are very different in
genre and style from the MT test data, which con-
sists mainly of newspaper texts and in recent years
also of weblogs, broadcast news and broadcast con-
versation. The in-domain training data had 3.8 mil-
lion sentences and 67 million words (English). The
67 million word system reached a BLEU score of
29.65 on the unseeen MT06 testset. Even though the
full 260M system was trained on almost four times
as many running words, the baseline score for MT06
only increased by 0.6 to 30.22 BLEU (see Table 4).
System MT03 MT06
67M in-domain 32.42/60.26 29.65/61.22
260M full 32.20/60.59 30.22/60.81
Table 4: In-domain only or all training data, BLEU/TER
The 67M system could not translate 752 Chinese
words out of 38937, the number of unknown words
decreased to 564 for the 260M system. To increase
the unigram coverage of the phrase table, we added
the lexicon entries that were not in the phrase table
as one-word translations. This lowered the number
of unknown words further to 410, but did not effect
the translation score.
4 POS-based Reordering
As Chinese and English have very different word
order, reordering over a rather limited distance dur-
ing decoding is not sufficient. Also using a simple
distance based distortion probability leaves it essen-
tially to the language model to select among dif-
ferent reorderings. An alternative is to apply auto-
matically learned reordering rules to the test sen-
tences before decoding (Crego and Marino, 2006).
We create a word lattice, which encodes many re-
orderings and allows long distance reordering. This
keeps the translation process in the decoder mono-
tone and makes it significantly faster compared to
allowing long distance reordering at decoding time.
4.1 Learning Reordering Rules
We tag both language sides of the bilingual corpus
with POS information using the Stanford Parser3
and extract POS based reordering patterns from
word alignment information. We use the context in
which a reordering pattern is seen in the training data
as an additional feature. Context refers to the words
or tags to the left or to the right of the sequence for
which a reordering pattern is extracted.
Relative frequencies are computed for every rule
that has been seen more than n times in the training
corpus (we observed good results for n > 5).
For the Chinese system we used only 350k bilin-
gual sentence pairs to extract rules with length of
up to 15. We did not reorder the training corpus
to retrain the translation model on modified Chinese
word order.
4.2 Applying Reordering Rules
To avoid hard decisions, we build a lattice struc-
ture for each source sentence as input for our de-
coder, which contains reordering alternatives consis-
tent with the previously extracted rules.
Longer reordering patterns are applied first.
Thereby shorter patterns can match along new paths,
creating short distance reordering on top of long dis-
tance reordering. Every outgoing edge of a node is
scored with the relative frequency of the pattern used
on the following sub path (For details see (Rottmann
and Vogel, 2007)). These model scores give this re-
3http://nlp.stanford.edu/software/lex-parser.shtml
79
ordering approach an advantage over a simple jump
model with a sliding window.
System MT03 MT06
260M, standard 32.20/60.59 30.22/60.81
260M, lattice 33.53/59.74 31.74/59.59
Table 5: Reordering lattice decoding in BLEU/TER
The system with reordering lattice input outper-
forms the system with a reordering window of 4
words by 1.5 BLEU (see Table 5).
5 Summary
The recent improvements to our Chinese-English
SMT system (see Fig. 1) can be mainly attributed to
a POS based word reordering method and the possi-
bility to work with larger statistical models.
We used the lattice translation functionality of our
decoder to translate reordering lattices. They are
built using reordering rules extracted from tagged
and aligned parallel data. There is further potential
for improvement in this approach, as we did not yet
reorder the training corpus and retrain the translation
model on modified Chinese word order.Improvements in BLEU
242526
272829
303132
33
2007 67M+3gr 260M+3gr 260M+4gr 260M+5gr 260M+RO
Figure 1: Improvements for MT06 in BLEU
We modified GIZA++ to run in parallel, which en-
abled us to include especially longer sentences into
translation model training. We also extended our de-
coder to use 5-gram language models and were able
to train an interpolated LM from all sources of the
English GigaWord Corpus.
Acknowledgments
This work was partly funded by DARPA under
the project GALE (Grant number #HR0011-06-2-
0001).
References
Josep M. Crego and Jose B. Marino. 2006. Reordering
Experiments for N-Gram-Based SMT. Spoken Lan-
guage Technology Workshop, Palm Beach, Aruba.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-
brand, Matthias Eck, Chiori Hori, Stephan Vogel and
Alex Waibel. 2005. The CMU Statistical Machine
Translation System for IWSLT 2005. IWSLT 2005,
Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL 2007,
Demonstration Session, Prague, Czech Republic.
Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 MT Evaluation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Poukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. ACL 2002, Philadel-
phia, USA.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
based Distortion Model. TMI-2007: 11th Interna-
tional Conference on Theoretical and Methodological
Issues in MT, Skvde, Sweden.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula and Ralph Weischedel.
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. LAMP-TR-126, University
of Maryland, College Park and BBN Technologies.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. 7th Conference of AMTA, Cambridge, Mas-
sachusetts, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. ICSLP, Denver, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A Con-
ditional Random Field Word Segmenter. Fourth
SIGHAN Workshop on Chinese Language Processing.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. ACL 2005,
WPT-05, Ann Arbor, MI
80
