Proceedings of the 12th Conference of the European Chapter of the ACL, pages 549?557,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Bilingually Motivated Domain-Adapted Word Segmentation
for Statistical Machine Translation
Yanjun Ma Andy Way
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
{yma, away}@computing.dcu.ie
Abstract
We introduce a word segmentation ap-
proach to languages where word bound-
aries are not orthographically marked,
with application to Phrase-Based Statis-
tical Machine Translation (PB-SMT). In-
stead of using manually segmented mono-
lingual domain-specific corpora to train
segmenters, we make use of bilingual cor-
pora and statistical word alignment tech-
niques. First of all, our approach is
adapted for the specific translation task at
hand by taking the corresponding source
(target) language into account. Secondly,
this approach does not rely on manu-
ally segmented training data so that it
can be automatically adapted for differ-
ent domains. We evaluate the perfor-
mance of our segmentation approach on
PB-SMT tasks from two domains and
demonstrate that our approach scores con-
sistently among the best results across dif-
ferent data conditions.
1 Introduction
State-of-the-art Statistical Machine Translation
(SMT) requires a certain amount of bilingual cor-
pora as training data in order to achieve compet-
itive results. The only assumption of most cur-
rent statistical models (Brown et al, 1993; Vogel
et al, 1996; Deng and Byrne, 2005) is that the
aligned sentences in such corpora should be seg-
mented into sequences of tokens that are meant to
be words. Therefore, for languages where word
boundaries are not orthographically marked, tools
which segment a sentence into words are required.
However, this segmentation is normally performed
as a preprocessing step using various word seg-
menters. Moreover, most of these segmenters are
usually trained on a manually segmented domain-
specific corpus, which is not adapted for the spe-
cific translation task at hand given that the manual
segmentation is performed in a monolingual con-
text. Consequently, such segmenters cannot pro-
duce consistently good results when used across
different domains.
A substantial amount of research has been car-
ried out to address the problems of word segmen-
tation. However, most research focuses on com-
bining various segmenters either in SMT training
or decoding (Dyer et al, 2008; Zhang et al, 2008).
One important yet often neglected fact is that the
optimal segmentation of the source (target) lan-
guage is dependent on the target (source) language
itself, its domain and its genre. Segmentation con-
sidered to be ?good? from a monolingual point
of view may be unadapted for training alignment
models or PB-SMT decoding (Ma et al, 2007).
The resulting segmentation will consequently in-
fluence the performance of an SMT system.
In this paper, we propose a bilingually moti-
vated automatically domain-adapted approach for
SMT. We utilise a small bilingual corpus with
the relevant language segmented into basic writ-
ing units (e.g. characters for Chinese or kana for
Japanese). Our approach consists of using the
output from an existing statistical word aligner
to obtain a set of candidate ?words?. We evalu-
ate the reliability of these candidates using sim-
ple metrics based on co-occurrence frequencies,
similar to those used in associative approaches to
word alignment (Melamed, 2000). We then mod-
ify the segmentation of the respective sentences
in the parallel corpus according to these candi-
date words; these modified sentences are then
given back to the word aligner, which produces
new alignments. We evaluate the validity of our
approach by measuring the influence of the seg-
mentation process on Chinese-to-English Machine
Translation (MT) tasks in two different domains.
The remainder of this paper is organised as fol-
549
lows. In section 2, we study the influence of
word segmentation on PB-SMT across different
domains. Section 3 describes the working mecha-
nism of our bilingually motivated word segmenta-
tion approach. In section 4, we illustrate the adap-
tation of our decoder to this segmentation scheme.
The experiments conducted in two different do-
mains are reported in Section 5 and 6. We discuss
related work in section 7. Section 8 concludes and
gives avenues for future work.
2 The Influence of Word Segmentation
on SMT: A Pilot Investigation
The monolingual word segmentation step in tra-
ditional SMT systems has a substantial impact on
the performance of such systems. A considerable
amount of recent research has focused on the in-
fluence of word segmentation on SMT (Ma et al,
2007; Chang et al, 2008; Zhang et al, 2008);
however, most explorations focused on the impact
of various segmentation guidelines and the mech-
anisms of the segmenters themselves. A current
research interest concerns consistency of perfor-
mance across different domains. From our ex-
periments, we show that monolingual segmenters
cannot produce consistently good results when ap-
plied to a new domain.
Our pilot investigation into the influence of
word segmentation on SMT involves three off-
the-shelf Chinese word segmenters including
ICTCLAS (ICT) Olympic version1, LDC seg-
menter2 and Stanford segmenter version 2006-05-
113. Both ICTCLAS and Stanford segmenters
utilise machine learning techniques, with Hidden
Markov Models for ICT (Zhang et al, 2003) and
conditional random fields for the Stanford seg-
menter (Tseng et al, 2005). Both segmenta-
tion models were trained on news domain data
with named entity recognition functionality. The
LDC segmenter is dictionary-based with word fre-
quency information to help disambiguation, both
of which are collected from data in the news do-
main. We used Chinese character-based and man-
ual segmentations as contrastive segmentations.
The experiments were carried out on a range of
data sizes from news and dialogue domains using
a state-of-the-art Phrase-Based SMT (PB-SMT)
1http://ictclas.org/index.html
2http://www.ldc.upenn.edu/Projects/
Chinese
3http://nlp.stanford.edu/software/
segmenter.shtml
system?Moses (Koehn et al, 2007). The perfor-
mance of PB-SMT system is measured with BLEU
score (Papineni et al, 2002).
We firstly measure the influence of word seg-
mentation on in-domain data with respect to the
three above mentioned segmenters, namely UN
data from the NIST 2006 evaluation campaign. As
can be seen from Table 1, using monolingual seg-
menters achieves consistently better SMT perfor-
mance than character-based segmentation (CS) on
different data sizes, which means character-based
segmentation is not good enough for this domain
where the vocabulary tends to be large. We can
also observe that the ICT and Stanford segmenter
consistently outperform the LDC segmenter. Even
using 3M sentence pairs for training, the differ-
ences between them are still statistically signifi-
cant (p < 0.05) using approximate randomisation
(Noreen, 1989) for significance testing.
40K 160K 640K 3M
CS 8.33 12.47 14.40 17.80
ICT 10.17 14.85 17.20 20.50
LDC 9.37 13.88 15.86 19.59
Stanford 10.45 15.26 16.94 20.64
Table 1: Word segmentation on NIST data sets
However, when tested on out-of-domain data,
i.e. IWSLT data in the dialogue domain, the re-
sults seem to be more difficult to predict. We
trained the system on different sizes of data and
evaluated the system on two test sets: IWSLT
2006 and 2007. From Table 2, we can see that on
the IWSLT 2006 test sets, LDC achieves consis-
tently good results and the Stanford segmenter is
the worst.4 Furthermore, the character-based seg-
mentation also achieves competitive results. On
IWSLT 2007, all monolingual segmenters outper-
form character-based segmentation and the LDC
segmenter is only slightly better than the other seg-
menters.
From the experiments reported above, we
can reach the following conclusions. First of
all, character-based segmentation cannot achieve
state-of-the-art results in most experimental SMT
settings. This also motivates the necessity to
work on better segmentation strategies. Second,
monolingual segmenters cannot achieve consis-
4Interestingly, the developers themselves also note the
sensitivity of the Stanford segmenter and incorporate exter-
nal lexical information to address such problems (Chang et
al., 2008).
550
40K 160K
IWSLT06 CS 19.31 23.06
Manual 19.94 -
ICT 20.34 23.36
LDC 20.37 24.34
Stanford 18.25 21.40
IWSLT07 CS 29.59 30.25
Manual 33.85 -
ICT 31.18 33.38
LDC 31.74 33.44
Stanford 30.97 33.41
Table 2: Word segmentation on IWSLT data sets
tently good results when used in another domain.
In the following sections, we propose a bilingually
motivated segmentation approach which can be
automatically derived from a small representative
data set and the experiments show that we can con-
sistently obtain state-of-the-art results in different
domains.
3 Bilingually Motivated Word
Segmentation
3.1 Notation
While in this paper, we focus on Chinese?English,
the method proposed is applicable to other lan-
guage pairs. The notation, however, assumes
Chinese?English MT. Given a Chinese sentence
cJ1 consisting of J characters {c1, . . . , cJ} and
an English sentence eI1 consisting of I words
{e1, . . . , eI}, AC?E will denote a Chinese-to-
English word alignment between cJ1 and eI1. Since
we are primarily interested in 1-to-n alignments,
AC?E can be represented as a set of pairs ai =
?Ci, ei? denoting a link between one single En-
glish word ei and a few Chinese characters Ci.The
set Ci is empty if the word ei is not aligned to any
character in cJ1 .
3.2 Candidate Extraction
In the following, we assume the availability of an
automatic word aligner that can output alignments
AC?E for any sentence pair (cJ1 , eI1) in a paral-
lel corpus. We also assume that AC?E contain
1-to-n alignments. Our method for Chinese word
segmentation is as follows: whenever a single En-
glish word is aligned with several consecutive Chi-
nese characters, they are considered candidates for
grouping. Formally, given an alignment AC?E
between cJ1 and eI1, if ai = ?Ci, ei? ? AC?E ,
with Ci = {ci1 , . . . , cim} and ?k ? J1,m ? 1K,
ik+1 ? ik = 1, then the alignment ai between ei
and the sequence of words Ci is considered a can-
didate word. Some examples of such 1-to-n align-
ments between Chinese and English we can derive
automatically are displayed in Figure 1.5
Figure 1: Example of 1-to-n word alignments be-
tween English words and Chinese characters
3.3 Candidate Reliability Estimation
Of course, the process described above is error-
prone, especially on a small amount of training
data. If we want to change the input segmentation
to give to the word aligner, we need to make sure
that we are not making harmful modifications. We
thus additionally evaluate the reliability of the can-
didates we extract and filter them before inclusion
in our bilingual dictionary. To perform this filter-
ing, we use two simple statistical measures. In the
following, ai = ?Ci, ei? denotes a candidate.
The first measure we consider is co-occurrence
frequency (COOC(Ci, ei)), i.e. the number of
times Ci and ei co-occur in the bilingual corpus.
This very simple measure is frequently used in as-
sociative approaches (Melamed, 2000). The sec-
ond measure is the alignment confidence (Ma et
al., 2007), defined as
AC(ai) =
C(ai)
COOC(Ci, ei)
,
where C(ai) denotes the number of alignments
proposed by the word aligner that are identical to
ai. In other words, AC(ai) measures how often
the aligner aligns Ci and ei when they co-occur.
We also impose that |Ci | ? k, where k is a fixed
integer that may depend on the language pair (be-
tween 3 and 5 in practice). The rationale behind
this is that it is very rare to get reliable alignments
between one word and k consecutive words when
k is high.
5While in this paper we are primarily concerned with lan-
guages where the word boundaries are not orthographically
marked, this approach, however, can also be applied to lan-
guages marked with word boundaries to construct bilingually
motivated ?words?.
551
The candidates are included in our bilingual dic-
tionary if and only if their measures are above
some fixed thresholds tCOOC and tAC , which al-
low for the control of the size of the dictionary and
the quality of its contents. Some other measures
(including the Dice coefficient) could be consid-
ered; however, it has to be noted that we are more
interested here in the filtering than in the discov-
ery of alignments per se, since our method builds
upon an existing aligner. Moreover, we will see
that even these simple measures can lead to an im-
provement in the alignment process in an MT con-
text.
3.4 Bootstrapped word segmentation
Once the candidates are extracted, we perform
word segmentation using the bilingual dictionar-
ies constructed using the method described above;
this provides us with an updated training corpus,
in which some character sequences have been re-
placed by a single token. This update is totally
naive: if an entry ai = ?Ci, ei? is present in the
dictionary and matches one sentence pair (cJ1 , eI1)
(i.e. Ci and ei are respectively contained in cJ1 and
eI1), then we replace the sequence of characters Ci
with a single token which becomes a new lexical
unit.6 Note that this replacement occurs even if
no alignment was found between Ci and ei for the
pair (cJ1 , eI1). This is motivated by the fact that the
filtering described above is quite conservative; we
trust the entry ai to be correct.
This process can be applied several times: once
we have grouped some characters together, they
become the new basic unit to consider, and we can
re-run the same method to get additional group-
ings. However, we have not seen in practice much
benefit from running it more than twice (few new
candidates are extracted after two iterations).
4 Word Lattice Decoding
4.1 Word Lattices
In the decoding stage, the various segmentation
alternatives can be encoded into a compact rep-
resentation of word lattices. A word lattice G =
?V,E? is a directed acyclic graph that formally is
a weighted finite state automaton. In the case of
word segmentation, each edge is a candidate word
associated with its weights. A straightforward es-
6In case of overlap between several groups of words to
replace, we select the one with the highest confidence (ac-
cording to tAC).
timation of the weights is to distribute the proba-
bility mass for each node uniformly to each out-
going edge. The single node having no outgoing
edges is designated the ?end node?. An example
of word lattices for a Chinese sentence is shown in
Figure 2.
4.2 Word Lattice Generation
Previous research on generating word lattices re-
lies on multiple monolingual segmenters (Xu et
al., 2005; Dyer et al, 2008). One advantage of
our approach is that the bilingually motivated seg-
mentation process facilitates word lattice genera-
tion without relying on other segmenters. As de-
scribed in section 3.4, the update of the training
corpus based on the constructed bilingual dictio-
nary requires that the sentence pair meets the bilin-
gual constraints. Such a segmentation process in
the training stage facilitates the utilisation of word
lattice decoding.
4.3 Phrase-Based Word Lattice Decoding
Given a Chinese input sentence cJ1 consisting of J
characters, the traditional approach is to determine
the best word segmentation and perform decoding
afterwards. In such a case, we first seek a single
best segmentation:
f?K1 = arg max
fK1 ,K
{Pr(fK1 |cJ1 )}
Then in the decoding stage, we seek:
e?I1 = arg max
eI1,I
{Pr(eI1|f?K1 )}
In such a scenario, some segmentations which are
potentially optimal for the translation may be lost.
This motivates the need for word lattice decoding.
The search process can be rewritten as:
e?I1 = arg max
eI1,I
{max
fK1 ,K
Pr(eI1, f
K
1 |cJ1 )}
= arg max
eI1,I
{max
fK1 ,K
Pr(eI1)Pr(f
K
1 |eI1, cJ1 )}
= arg max
eI1,I
{max
fK1 ,K
Pr(eI1)Pr(f
K
1 |eI1)Pr(fK1 |cJ1 )}
Given the fact that the number of segmentations
fK1 grows exponentially with respect to the num-
ber of characters K , it is impractical to firstly enu-
merate all possible fK1 and then to decode. How-
ever, it is possible to enumerate all the alternative
segmentations for a substring of cJ1 , making the
utilisation of word lattices tractable in PB-SMT.
552
Figure 2: Example of a word lattice
5 Experimental Setting
5.1 Evaluation
The intrinsic quality of word segmentation is nor-
mally evaluated against a manually segmented
gold-standard corpus using F-score. While this
approach can give a direct evaluation of the qual-
ity of the word segmentation, it is faced with sev-
eral limitations. First of all, it is really difficult to
build a reliable and objective gold-standard given
the fact that there is only 70% agreement between
native speakers on this task (Sproat et al, 1996).
Second, an increase in F-score does not necessar-
ily imply an improvement in translation quality. It
has been shown that F-score has a very weak cor-
relation with SMT translation quality in terms of
BLEU score (Zhang et al, 2008). Consequently,
we chose to extrinsically evaluate the performance
of our approach via the Chinese?English transla-
tion task, i.e. we measure the influence of the
segmentation process on the final translation out-
put. The quality of the translation output is mainly
evaluated using BLEU, with NIST (Doddington,
2002) and METEOR (Banerjee and Lavie, 2005)
as complementary metrics.
5.2 Data
The data we used in our experiments are from
two different domains, namely news and travel di-
alogues. For the news domain, we trained our
system using a portion of UN data for NIST
2006 evaluation campaign. The system was de-
veloped on LDC Multiple-Translation Chinese
(MTC) Corpus and tested on MTC part 2, which
was also used as a test set for NIST 2002 evalua-
tion campaign.
For the dialogue data, we used the Chinese?
English datasets provided within the IWSLT 2007
evaluation campaign. Specifically, we used the
standard training data, to which we added devset1
and devset2. Devset4 was used to tune the param-
eters and the performance of the system was tested
on both IWSLT 2006 and 2007 test sets. We used
both test sets because they are quite different in
terms of sentence length and vocabulary size. To
test the scalability of our approach, we used HIT
corpus provided within IWSLT 2008 evaluation
campaign. The various statistics for the corpora
are shown in Table 3.
5.3 Baseline System
We conducted experiments using different seg-
menters with a standard log-linear PB-SMT
model: GIZA++ implementation of IBM word
alignment model 4 (Och and Ney, 2003), the
refinement and phrase-extraction heuristics de-
scribed in (Koehn et al, 2003), minimum-error-
rate training (Och, 2003), a 5-gram language
model with Kneser-Ney smoothing trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al, 2007; Dyer
et al, 2008) to translate both single best segmen-
tation and word lattices.
6 Experiments
6.1 Results
The initial word alignments are obtained using
the baseline configuration described above by seg-
menting the Chinese sentences into characters.
From these we build a bilingual 1-to-n dictionary,
and the training corpus is updated by grouping the
characters in the dictionaries into a single word,
using the method presented in section 3.4. As pre-
viously mentioned, this process can be repeated
several times. We then extract aligned phrases us-
ing the same procedure as for the baseline sys-
tem; the only difference is the basic unit we are
considering. Once the phrases are extracted, we
perform the estimation of weights for the fea-
tures of the log-linear model. We then use a
simple dictionary-based maximum matching algo-
rithm to obtain a single-best segmentation for the
Chinese sentences in the development set so that
553
Train Dev. Eval.
Zh En Zh En Zh En
Dialogue Sentences 40,958 489 (7 ref.) 489 (6 ref.)/489 (7 ref.)
Running words 488,303 385,065 8,141 46,904 8,793/4,377 51,500/23,181
Vocabulary size 2,742 9,718 835 1,786 936/772 2,016/1,339
News Sentences 40,000 993 (9 ref.) 878 (4 ref.)
Running words 1,412,395 956,023 41,466 267,222 38,700 105,530
Vocabulary size 6057 20,068 1,983 10,665 1,907 7,388
Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)
minimum-error-rate training can be performed.7
Finally, in the decoding stage, we use the same
segmentation algorithm to obtain the single-best
segmentation on the test set, and word lattices can
also be generated using the bilingual dictionary.
The various parameters of the method (k, tCOOC ,
tAC , cf. section 3) were optimised on the develop-
ment set. One iteration of character grouping on
the NIST task was found to be enough; the optimal
set of values was found to be k = 3, tAC = 0.0
and tCOOC = 0, meaning that all the entries in the
bilingually dictionary are kept. On IWSLT data,
we found that two iterations of character grouping
were needed: the optimal set of values was found
to be k = 3, tAC = 0.3, tCOOC = 8 for the first
iteration, and tAC = 0.2, tCOOC = 15 for the
second.
As can be seen from Table 4, our bilingually
motivated segmenter (BS) achieved statistically
significantly better results than character-based
segmentation when enhanced with word lattice de-
coding.8 Compared to the best in-domain seg-
menter, namely the Stanford segmenter on this
particular task, our approach is inferior accord-
ing to BLEU and NIST. We firstly attribute this
to the small amount of training data, from which
a high quality bilingual dictionary cannot be ob-
tained due to data sparseness problems. We also
attribute this to the vast amount of named entity
terms in the test sets, which is extremely difficult
for our approach.9 We expect to see better re-
sults when a larger amount of data is used and the
segmenter is enhanced with a named entity recog-
niser. On IWSLT data (cf. Tables 5 and 6), our
7In order to save computational time, we used the same
set of parameters obtained above to decode both the single-
best segmentation and the word lattice.
8Note the BLEU scores are particularly low due to the
number of references used (4 references), in addition to the
small amount of training data available.
9As we previously point out, both ICT and Stanford seg-
menters are equipped with named entity recognition func-
tionality. This may risk causing data sparseness problems on
small training data. However, this is beneficial in the transla-
tion process compared to character-based segmentation.
approach yielded a consistently good performance
on both translation tasks compared to the best in-
domain segmenter?the LDC segmenter. More-
over, the good performance is confirmed by all
three evaluation measures.
BLEU NIST METEOR
CS 8.43 4.6272 0.3778
Stanford 10.45 5.0675 0.3699
BS-SingleBest 7.98 4.4374 0.3510
BS-WordLattice 9.04 4.6667 0.3834
Table 4: BS on NIST task
BLEU NIST METEOR
CS 0.1931 6.1816 0.4998
LDC 0.2037 6.2089 0.4984
BS-SingleBest 0.1865 5.7816 0.4602
BS-WordLattice 0.2041 6.2874 0.5124
Table 5: BS on IWSLT 2006 task
BLEU NIST METEOR
CS 0.2959 6.1216 0.5216
LDC 0.3174 6.2464 0.5403
BS-SingleBest 0.3023 6.0476 0.5125
BS-WordLattice 0.3171 6.3518 0.5603
Table 6: BS on IWSLT 2007 task
6.2 Parameter Search Graph
The reliability estimation process is computation-
ally intensive. However, this can be easily paral-
lelised. From our experiments, we observed that
the translation results are very sensitive to the pa-
rameters and this search process is essential to
achieve good results. Figure 3 is the search graph
on the IWSLT data set in the first iteration step.
From this graph, we can see that filtering of the
bilingual dictionary is essential in order to achieve
better performance.
554
Figure 3: The search graph on development set of
IWSLT task
6.3 Vocabulary Size
Our bilingually motivated segmentation approach
has to overcome another challenge in order to
produce competitive results, i.e. data sparseness.
Given that our segmentation is based on bilingual
dictionaries, the segmentation process can signif-
icantly increase the size of the vocabulary, which
could potentially lead to a data sparseness prob-
lem when the size of the training data is small. Ta-
bles 7 and 8 list the statistics of the Chinese side
of the training data, including the total vocabulary
(Voc), number of character vocabulary (Char.voc)
in Voc, and the running words (Run.words) when
different word segmentations were used. From Ta-
ble 7, we can see that our approach suffered from
data sparseness on the NIST task, i.e. a large
vocabulary was generated, of which a consider-
able amount of characters still remain as separate
words. On the IWSLT task, since the dictionary
generation process is more conservative, we main-
tained a reasonable vocabulary size, which con-
tributed to the final good performance.
Voc. Char.voc Run. Words
CS 6,057 6,057 1,412,395
ICT 16,775 1,703 870,181
LDC 16,100 2,106 881,861
Stanford 22,433 1,701 880,301
BS 18,111 2,803 927,182
Table 7: Vocabulary size of NIST task (40K)
6.4 Scalability
The experimental results reported above are based
on a small training corpus containing roughly
40,000 sentence pairs. We are particularly inter-
ested in the performance of our segmentation ap-
Voc. Char.voc Run. Words
CS 2,742 2,742 488,303
ICT 11,441 1,629 358,504
LDC 9,293 1,963 364,253
Stanford 18,676 981 348,251
BS 3,828 2,740 402,845
Table 8: Vocabulary size of IWSLT task (40K)
proach when it is scaled up to larger amounts of
data. Given that the optimisation of the bilingual
dictionary is computationally intensive, it is im-
practical to directly extract candidate words and
estimate their reliability. As an alternative, we can
use the obtained bilingual dictionary optimised on
the small corpus to perform segmentation on the
larger corpus. We expect competitive results when
the small corpus is a representative sample of the
larger corpus and large enough to produce reliable
bilingual dictionaries without suffering severely
from data sparseness.
As we can see from Table 9, our segmenta-
tion approach achieved consistent results on both
IWSLT 2006 and 2007 test sets. On the NIST task
(cf. Table 10), our approach outperforms the basic
character-based segmentation; however, it is still
inferior compared to the other in-domain mono-
lingual segmenters due to the low quality of the
bilingual dictionary induced (cf. section 6.1).
IWSLT06 IWSLT07
CS 23.06 30.25
ICT 23.36 33.38
LDC 24.34 33.44
Stanford 21.40 33.41
BS-SingleBest 22.45 30.76
BS-WordLattice 24.18 32.99
Table 9: Scale-up to 160K on IWSLT data sets
160K 640K
CS 12.47 14.40
ICT 14.85 17.20
LDC 13.88 15.86
Stanford 15.26 16.94
BS-SingleBest 12.58 14.11
BS-WordLattice 13.74 15.33
Table 10: Scalability of BS on NIST task
555
6.5 Using different word aligners
The above experiments rely on GIZA++ to per-
form word alignment. We next show that our ap-
proach is not dependent on the word aligner given
that we have a conservative reliability estimation
procedure. Table 11 shows the results obtained on
the IWSLT data set using the MTTK alignment
tool (Deng and Byrne, 2005; Deng and Byrne,
2006).
IWSLT06 IWSLT07
CS 21.04 31.41
ICT 20.48 31.11
LDC 20.79 30.51
Stanford 17.84 29.35
BS-SingleBest 19.22 29.75
BS-WordLattice 21.76 31.75
Table 11: BS on IWSLT data sets using MTTK
7 Related Work
(Xu et al, 2004) were the first to question the use
of word segmentation in SMT and showed that the
segmentation proposed by word alignments can be
used in SMT to achieve competitive results com-
pared to using monolingual segmenters. Our ap-
proach differs from theirs in two aspects. Firstly,
(Xu et al, 2004) use word aligners to reconstruct
a (monolingual) Chinese dictionary and reuse this
dictionary to segment Chinese sentences as other
monolingual segmenters. Our approach features
the use of a bilingual dictionary and conducts a
different segmentation. In addition, we add a pro-
cess which optimises the bilingual dictionary ac-
cording to translation quality. (Ma et al, 2007)
proposed an approach to improve word alignment
by optimising the segmentation of both source and
target languages. However, the reported experi-
ments still rely on some monolingual segmenters
and the issue of scalability is not addressed. Our
research focuses on avoiding the use of monolin-
gual segmenters in order to improve the robustness
of segmenters across different domains.
(Xu et al, 2005) were the first to propose the
use of word lattice decoding in PB-SMT, in order
to address the problems of segmentation. (Dyer
et al, 2008) extended this approach to hierarchi-
cal SMT systems and other language pairs. How-
ever, both of these methods require some mono-
lingual segmentation in order to generate word lat-
tices. Our approach facilitates word lattice gener-
ation given that our segmentation is driven by the
bilingual dictionary.
8 Conclusions and Future Work
In this paper, we introduced a bilingually moti-
vated word segmentation approach for SMT. The
assumption behind this motivation is that the lan-
guage to be segmented can be tokenised into ba-
sic writing units. Firstly, we extract 1-to-n word
alignments using statistical word aligners to con-
struct a bilingual dictionary in which each entry
indicates a correspondence between one English
word and n Chinese characters. This dictionary is
then filtered using a few simple association mea-
sures and the final bilingual dictionary is deployed
for word segmentation. To overcome the segmen-
tation problem in the decoding stage, we deployed
word lattice decoding.
We evaluated our approach on translation tasks
from two different domains and demonstrate that
our approach is (i) not as sensitive as monolingual
segmenters, and (ii) that the SMT system using
our word segmentation can achieve state-of-the-art
performance. Moreover, our approach can easily
be scaled up to larger data sets and achieves com-
petitive results if the small data used is a represen-
tative sample.
As for future work, firstly we plan to integrate
some named entity recognisers into our approach.
We also plan to try our approach in more do-
mains and on other language pairs (e.g. Japanese?
English). Finally, we intend to explore the corre-
lation between vocabulary size and the amount of
training data needed in order to achieve good re-
sults using our approach.
Acknowledgments
This work is supported by Science Foundation Ire-
land (O5/IN/1732) and the Irish Centre for High-
End Computing.10 We would like to thank the re-
viewers for their insightful comments.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, MI.
10http://www.ichec.ie/
556
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232, Columbus, OH.
Yonggang Deng and William Byrne. 2005. HMM
word and phrase alignment for statistical machine
translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
169?176, Vancouver, BC, Canada.
Yonggang Deng and William Byrne. 2006. MTTK:
An alignment toolkit for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, pages 265?268,
New York City, NY.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145, San Francisco,
CA.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1012?1020, Colum-
bus, OH.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 48?54, Edmonton, AL,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 304?
311, Prague, Czech Republic.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Richard W Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
Andrea Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904, Denver, CO.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 168?
171, Jeju Island, Republic of Korea.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics, pages
836?841, Copenhagen, Denmark.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In ACL SIGHAN Workshop
2004, pages 122?128, Barcelona, Spain.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings
of the International Workshop on Spoken Language
Translation, pages 141?147, Pittsburgh, PA.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical ana-
lyzer ICTCLAS. In Proceedings of Second SIGHAN
Workshop on Chinese Language Processing, pages
184?187, Sappora, Japan.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216?223, Columbus, OH.
557
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304?311,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bootstrapping Word Alignment via Word Packing
Yanjun Ma, Nicolas Stroppa, Andy Way
School of Computing
Dublin City University
Glasnevin, Dublin 9, Ireland
{yma,nstroppa,away}@computing.dcu.ie
Abstract
We introduce a simple method to pack words
for statistical word alignment. Our goal is to
simplify the task of automatic word align-
ment by packing several consecutive words
together when we believe they correspond
to a single word in the opposite language.
This is done using the word aligner itself,
i.e. by bootstrapping on its output. We
evaluate the performance of our approach
on a Chinese-to-English machine translation
task, and report a 12.2% relative increase in
BLEU score over a state-of-the art phrase-
based SMT system.
1 Introduction
Automatic word alignment can be defined as the
problem of determining a translational correspon-
dence at word level given a parallel corpus of aligned
sentences. Most current statistical models (Brown
et al, 1993; Vogel et al, 1996; Deng and Byrne,
2005) treat the aligned sentences in the corpus as se-
quences of tokens that are meant to be words; the
goal of the alignment process is to find links be-
tween source and target words. Before applying
such aligners, we thus need to segment the sentences
into words ? a task which can be quite hard for lan-
guages such as Chinese for which word boundaries
are not orthographically marked. More importantly,
however, this segmentation is often performed in a
monolingual context, which makes the word align-
ment task more difficult since different languages
may realize the same concept using varying num-
bers of words (see e.g. (Wu, 1997)). Moreover, a
segmentation considered to be ?good? from a mono-
lingual point of view may be unadapted for training
alignment models.
Although some statistical alignment models al-
low for 1-to-n word alignments for those reasons,
they rarely question the monolingual tokenization
and the basic unit of the alignment process remains
the word. In this paper, we focus on 1-to-n align-
ments with the goal of simplifying the task of auto-
matic word aligners by packing several consecutive
words together when we believe they correspond to a
single word in the opposite language; by identifying
enough such cases, we reduce the number of 1-to-n
alignments, thus making the task of word alignment
both easier and more natural.
Our approach consists of using the output from
an existing statistical word aligner to obtain a set of
candidates for word packing. We evaluate the re-
liability of these candidates, using simple metrics
based on co-occurence frequencies, similar to those
used in associative approaches to word alignment
(Kitamura and Matsumoto, 1996; Melamed, 2000;
Tiedemann, 2003). We then modify the segmenta-
tion of the sentences in the parallel corpus accord-
ing to this packing of words; these modified sen-
tences are then given back to the word aligner, which
produces new alignments. We evaluate the validity
of our approach by measuring the influence of the
alignment process on a Chinese-to-English Machine
Translation (MT) task.
The remainder of this paper is organized as fol-
lows. In Section 2, we study the case of 1-to-
n word alignment. Section 3 introduces an auto-
matic method to pack together groups of consecutive
304
1: 0 1: 1 1: 2 1: 3 1:n (n > 3)
IWSLT Chinese?English 21.64 63.76 9.49 3.36 1.75
IWSLT English?Chinese 29.77 57.47 10.03 1.65 1.08
IWSLT Italian?English 13.71 72.87 9.77 3.23 0.42
IWSLT English?Italian 20.45 71.08 7.02 0.9 0.55
Europarl Dutch?English 24.71 67.04 5.35 1.4 1.5
Europarl English?Dutch 23.76 69.07 4.85 1.2 1.12
Table 1: Distribution of alignment types for different language pairs (%)
words based on the output from a word aligner. In
Section 4, the experimental setting is described. In
Section 5, we evaluate the influence of our method
on the alignment process on a Chinese to English
MT task, and experimental results are presented.
Section 6 concludes the paper and gives avenues for
future work.
2 The Case of 1-to-n Alignment
The same concept can be expressed in different lan-
guages using varying numbers of words; for exam-
ple, a single Chinese word may surface as a com-
pound or a collocation in English. This is fre-
quent for languages as different as Chinese and En-
glish. To quickly (and approximately) evaluate this
phenomenon, we trained the statistical IBM word-
alignment model 4 (Brown et al, 1993),1 using the
GIZA++ software (Och and Ney, 2003) for the fol-
lowing language pairs: Chinese?English, Italian?
English, and Dutch?English, using the IWSLT-2006
corpus (Takezawa et al, 2002; Paul, 2006) for the
first two language pairs, and the Europarl corpus
(Koehn, 2005) for the last one. These asymmet-
ric models produce 1-to-n alignments, with n ? 0,
in both directions. Here, it is important to mention
that the segmentation of sentences is performed to-
tally independently of the bilingual alignment pro-
cess, i.e. it is done in a monolingual context. For Eu-
ropean languages, we apply the maximum-entropy
based tokenizer of OpenNLP2; the Chinese sen-
tences were human segmented (Paul, 2006).
In Table 1, we report the frequencies of the dif-
ferent types of alignments for the various languages
and directions. As expected, the number of 1:n
1More specifically, we performed 5 iterations of Model 1, 5
iterations of HMM, 5 iterations of Model 3, and 5 iterations of
Model 4.
2http://opennlp.sourceforge.net/.
alignments with n 6= 1 is high for Chinese?English
(' 40%), and significantly higher than for the Eu-
ropean languages. The case of 1-to-n alignments is,
therefore, obviously an important issue when deal-
ing with Chinese?English word alignment.3
2.1 The Treatment of 1-to-n Alignments
Fertility-based models such as IBM models 3, 4, and
5 allow for alignments between one word and sev-
eral words (1-to-n or 1:n alignments in what fol-
lows), in particular for the reasons specified above.
They can be seen as extensions of the simpler IBM
models 1 and 2 (Brown et al, 1993). Similarly,
Deng and Byrne (2005) propose an HMM frame-
work capable of dealing with 1-to-n alignment,
which is an extension of the original model of (Vogel
et al, 1996).
However, these models rarely question the mono-
lingual tokenization, i.e. the basic unit of the align-
ment process is the word.4 One alternative to ex-
tending the expressivity of one model (and usually
its complexity) is to focus on the input representa-
tion; in particular, we argue that the alignment pro-
cess can benefit from a simplification of the input,
which consists of trying to reduce the number of
1-to-n alignments to consider. Note that the need
to consider segmentation and alignment at the same
time is also mentioned in (Tiedemann, 2003), and
related issues are reported in (Wu, 1997).
2.2 Notation
While in this paper, we focus on Chinese?English,
the method proposed is applicable to any language
3Note that a 1: 0 alignment may denote a failure to capture
a 1:n alignment with n > 1.
4Interestingly, this is actually even the case for approaches
that directly model alignments between phrases (Marcu and
Wong, 2002; Birch et al, 2006).
305
pair ? even for closely related languages, we ex-
pect improvements to be seen. The notation how-
ever assume Chinese?English MT. Given a Chi-
nese sentence cJ1 consisting of J words {c1, . . . , cJ}
and an English sentence eI1 consisting of I words
{e1, . . . , eI}, AC?E (resp. AE?C) will denote a
Chinese-to-English (resp. an English-to-Chinese)
word alignment between cJ1 and eI1. Since we are
primarily interested in 1-to-n alignments, AC?E
can be represented as a set of pairs aj = ?cj , Ej?
denoting a link between one single Chinese word
cj and a few English words Ej (and similarly for
AE?C). The set Ej is empty if the word cj is not
aligned to any word in eI1.
3 Automatic Word Repacking
Our approach consists of packing consecutive words
together when we believe they correspond to a sin-
gle word in the other language. This bilingually
motivated packing of words changes the basic unit
of the alignment process, and simplifies the task of
automatic word alignment. We thus minimize the
number of 1-to-n alignments in order to obtain more
comparable segmentations in the two languages. In
this section, we present an automatic method that
builds upon the output from an existing automatic
word aligner. More specifically, we (i) use a word
aligner to obtain 1-to-n alignments, (ii) extract can-
didates for word packing, (iii) estimate the reliability
of these candidates, (iv) replace the groups of words
to pack by a single token in the parallel corpus, and
(v) re-iterate the alignment process using the up-
dated corpus. The first three steps are performed
in both directions, and produce two bilingual dic-
tionaries (source-target and target-source) of groups
of words to pack.
3.1 Candidate Extraction
In the following, we assume the availability of an
automatic word aligner that can output alignments
AC?E and AE?C for any sentence pair (cJ1 , eI1)
in a parallel corpus. We also assume that AC?E
and AE?C contain 1:n alignments. Our method for
repacking words is very simple: whenever a single
word is aligned with several consecutive words, they
are considered candidates for repacking. Formally,
given an alignment AC?E between cJ1 and eI1, if
aj = ?cj , Ej? ? AC?E , with Ej = {ej1 , . . . , ejm}
and ?k ? J1,m? 1K, jk+1 ? jk = 1, then the align-
ment aj between cj and the sequence of words Ej
is considered a candidate for word repacking. The
same goes for AE?C . Some examples of such 1-
to-n alignments between Chinese and English (in
both directions) we can derive automatically are dis-
played in Figure 1.
????: white wine
????: department store
??: excuse me
??: call the police
?: cup of
??: have to
closest: ? ?
fifteen: ? ?
fine: ? ?
flight: ? ?? 
get: ? ?
here:  ? ??
Figure 1: Example of 1-to-n word alignments be-
tween Chinese and English
3.2 Candidate Reliability Estimation
Of course, the process described above is error-
prone and if we want to change the input to give to
the word aligner, we need to make sure that we are
not making harmful modifications.5 We thus addi-
tionally evaluate the reliability of the candidates we
extract and filter them before inclusion in our bilin-
gual dictionary. To perform this filtering, we use
two simple statistical measures. In the following,
aj = ?cj , Ej? denotes a candidate.
The first measure we consider is co-occurrence
frequency (COOC(cj , Ej)), i.e. the number of
times cj and Ej co-occur in the bilingual corpus.
This very simple measure is frequently used in as-
sociative approaches (Melamed, 1997; Tiedemann,
2003). The second measure is the alignment confi-
dence, defined as
AC(aj) =
C(aj)
COOC(cj , Ej)
,
where C(aj) denotes the number of alignments pro-
posed by the word aligner that are identical to aj .
In other words, AC(aj) measures how often the
5Consequently, if we compare our approach to the problem
of collocation identification, we may say that we are more in-
terested in precision than recall (Smadja et al, 1996). However,
note that our goal is not recognizing specific sequences of words
such as compounds or collocations; it is making (bilingually
motivated) changes that simplify the alignment process.
306
aligner aligns cj and Ej when they co-occur. We
also impose that |Ej | ? k, where k is a fixed inte-
ger that may depend on the language pair (between
3 and 5 in practice). The rationale behind this is that
it is very rare to get reliable alignment between one
word and k consecutive words when k is high.
The candidates are included in our bilingual dic-
tionary if and only if their measures are above some
fixed thresholds tcooc and tac, which allow for the
control of the size of the dictionary and the quality
of its contents. Some other measures (including the
Dice coefficient) could be considered; however, it
has to be noted that we are more interested here in
the filtering than in the discovery of alignment, since
our method builds upon an existing aligner. More-
over, we will see that even these simple measures
can lead to an improvement of the alignment pro-
cess in a MT context (cf. Section 5).
3.3 Bootstrapped Word Repacking
Once the candidates are extracted, we repack the
words in the bilingual dictionaries constructed using
the method described above; this provides us with
an updated training corpus, in which some word se-
quences have been replaced by a single token. This
update is totally naive: if an entry aj = ?cj , Ej? is
present in the dictionary and matches one sentence
pair (cJ1 , eI1) (i.e. cj and Ej are respectively con-
tained in cJ1 and eI1), then we replace the sequence
of words Ej with a single token which becomes a
new lexical unit.6 Note that this replacement occurs
even if no alignment was found between cj and Ej
for the pair (cJ1 , eI1). This is motivated by the fact
that the filtering described above is quite conserva-
tive; we trust the entry ai to be correct. This update
is performed in both directions. It is then possible to
run the word aligner using the updated (simplified)
parallel corpus, in order to get new alignments. By
performing a deterministic word packing, we avoid
the computation of the fertility parameters associ-
ated with fertility-based models.
Word packing can be applied several times: once
we have grouped some words together, they become
the new basic unit to consider, and we can re-run
the same method to get additional groupings. How-
6In case of overlap between several groups of words to re-
place, we select the one with highest confidence (according to
tac).
ever, we have not seen in practice much benefit from
running it more than twice (few new candidates are
extracted after two iterations).
It is also important to note that this process is
bilingually motivated and strongly depends on the
language pair. For example, white wine, excuse me,
call the police, and cup of (cf. Figure 1) translate re-
spectively as vin blanc, excusez-moi, appellez la po-
lice, and tasse de in French. Those groupings would
not be found for a language pair such as French?
English, which is consistent with the fact that they
are less useful for French?English than for Chinese?
English in a MT perspective.
3.4 Using Manually Developed Dictionaries
We wanted to compare this automatic approach to
manually developed resources. For this purpose,
we used a dictionary built by the MT group of
Harbin Institute of Technology, as a preprocessing
step to Chinese?English word alignment, and moti-
vated by several years of Chinese?English MT prac-
tice. Some examples extracted from this resource
are displayed in Figure 2.
?: whiti en 
??:?dp aw wr
??: aiim arw
??: ea str aw rs
?: pn nrra  pn
?: orrx pw
Figure 2: Examples of entries from the manually de-
veloped dictionary
4 Experimental Setting
4.1 Evaluation
The intrinsic quality of word alignment can be as-
sessed using the Alignment Error Rate (AER) met-
ric (Och and Ney, 2003), that compares a system?s
alignment output to a set of gold-standard align-
ment. While this method gives a direct evaluation of
the quality of word alignment, it is faced with sev-
eral limitations. First, it is really difficult to build
a reliable and objective gold-standard set, especially
for languages as different as Chinese and English.
Second, an increase in AER does not necessarily im-
ply an improvement in translation quality (Liang et
al., 2006) and vice-versa (Vilar et al, 2006). The
307
relationship between word alignments and their im-
pact on MT is also investigated in (Ayan and Dorr,
2006; Lopez and Resnik, 2006; Fraser and Marcu,
2006). Consequently, we chose to extrinsically eval-
uate the performance of our approach via the transla-
tion task, i.e. we measure the influence of the align-
ment process on the final translation output. The
quality of the translation output is evaluated using
BLEU (Papineni et al, 2002).
4.2 Data
The experiments were carried out using the
Chinese?English datasets provided within the
IWSLT 2006 evaluation campaign (Paul, 2006), ex-
tracted from the Basic Travel Expression Corpus
(BTEC) (Takezawa et al, 2002). This multilingual
speech corpus contains sentences similar to those
that are usually found in phrase-books for tourists
going abroad. Training was performed using the de-
fault training set, to which we added the sets de-
vset1, devset2, and devset3.7 The English side of
the test set was not available at the time we con-
ducted our experiments, so we split the development
set (devset 4) into two parts: one was kept for testing
(200 aligned sentences) with the rest (289 aligned
sentences) used for development purposes.
As a pre-processing step, the English sentences
were tokenized using the maximum-entropy based
tokenizer of the OpenNLP toolkit, and case infor-
mation was removed. For Chinese, the data pro-
vided were tokenized according to the output format
of ASR systems, and human-corrected (Paul, 2006).
Since segmentations are human-corrected, we are
sure that they are good from a monolingual point of
view. Table 2 contains the various corpus statistics.
4.3 Baseline
We use a standard log-linear phrase-based statistical
machine translation system as a baseline: GIZA++
implementation of IBM word alignment model 4
(Brown et al, 1993; Och and Ney, 2003),8 the re-
finement and phrase-extraction heuristics described
in (Koehn et al, 2003), minimum-error-rate training
7More specifically, we choose the first English reference
from the 7 references and the Chinese sentence to construct new
sentence pairs.
8Training is performed using the same number of iterations
as in Section 2.
Chinese English
Train Sentences 41,465
Running words 361,780 375,938
Vocabulary size 11,427 9,851
Dev. Sentences 289 (7 refs.)
Running words 3,350 26,223
Vocabulary size 897 1,331
Eval. Sentences 200 (7 refs.)
Running words 1,864 14,437
Vocabulary size 569 1,081
Table 2: Chinese?English corpus statistics
(Och, 2003) using Phramer (Olteanu et al, 2006),
a 3-gram language model with Kneser-Ney smooth-
ing trained with SRILM (Stolcke, 2002) on the En-
glish side of the training data and Pharaoh (Koehn,
2004) with default settings to decode. The log-linear
model is also based on standard features: condi-
tional probabilities and lexical smoothing of phrases
in both directions, and phrase penalty (Zens and
Ney, 2004).
5 Experimental Results
5.1 Results
The initial word alignments are obtained using the
baseline configuration described above. From these,
we build two bilingual 1-to-n dictionaries (one for
each direction), and the training corpus is updated
by repacking the words in the dictionaries, using the
method presented in Section 2. As previously men-
tioned, this process can be repeated several times; at
each step, we can also choose to exploit only one of
the two available dictionaries, if so desired. We then
extract aligned phrases using the same procedure as
for the baseline system; the only difference is the ba-
sic unit we are considering. Once the phrases are ex-
tracted, we perform the estimation of the features of
the log-linear model and unpack the grouped words
to recover the initial words. Finally, minimum-error-
rate training and decoding are performed.
The various parameters of the method (k, tcooc,
tac, cf. Section 2) have been optimized on the devel-
opment set. We found out that it was enough to per-
form two iterations of repacking: the optimal set of
values was found to be k = 3, tac = 0.5, tcooc = 20
for the first iteration, and tcooc = 10 for the second
308
BLEU[%]
Baseline 15.14
n=1. with C-E dict. 15.92
n=1. with E-C dict. 15.77
n=1. with both 16.59
n=2. with C-E dict. 16.99
n=2. with E-C dict. 16.59
n=2. with both 16.88
Table 3: Influence of word repacking on Chinese-to-
English MT
iteration, for both directions.9 In Table 3, we report
the results obtained on the test set, where n denotes
the iteration. We first considered the inclusion of
only the Chinese?English dictionary, then only the
English?Chinese dictionary, and then both.
After the first step, we can already see an im-
provement over the baseline when considering one
of the two dictionaries. When using both, we ob-
serve an increase of 1.45 BLEU points, which cor-
responds to a 9.6% relative increase. Moreover, we
can gain from performing another step. However,
the inclusion of the English?Chinese dictionary is
harmful in this case, probably because 1-to-n align-
ments are less frequent for this direction, and have
been captured during the first step. By including the
Chinese?English dictionary only, we can achieve an
increase of 1.85 absolute BLEU points (12.2% rela-
tive) over the initial baseline.10
Quality of the Dictionaries To assess the qual-
ity of the extraction procedure, we simply manu-
ally evaluated the ratio of incorrect entries in the
dictionaries. After one step of word packing, the
Chinese?English and the English?Chinese dictio-
naries respectively contain 7.4% and 13.5% incor-
rect entries. After two steps of packing, they only
contain 5.9% and 10.3% incorrect entries.
5.2 Alignment Types
Intuitively, the word alignments obtained after word
packing are more likely to be 1-to-1 than before. In-
9The parameters k, tac, and tcooc are optimized for each
step, and the alignment obtained using the best set of parameters
for a given step are used as input for the following step.
10Note that this setting (using both dictionaries for the first
step and only the Chinese dictionary for the second step) is also
the best setting on the development set.
deed, the word sequences in one language that usu-
ally align to one single word in the other language
have been grouped together to form one single to-
ken. Table 4 shows the detail of the distribution of
alignment types after one and two steps of automatic
repacking. In particular, we can observe that the 1: 1
1: 0 1: 1 1: 2 1: 3 1:n
(n > 3)
C-E Base. 21.64 63.76 9.49 3.36 1.75
n=1 19.69 69.43 6.32 2.79 1.78
n=2 19.67 71.57 4.87 2.12 1.76
E-C Base. 29.77 57.47 10.03 1.65 1.08
n=1 26.59 61.95 8.82 1.55 1.09
n=2 25.10 62.73 9.38 1.68 1.12
Table 4: Distribution of alignment types (%)
alignments are more frequent after the application
of repacking: the ratio of this type of alignment has
increased by 7.81% for Chinese?English and 5.26%
for English?Chinese.
5.3 Influence of Word Segmentation
To test the influence of the initial word segmenta-
tion on the process of word packing, we considered
an additional segmentation configuration, based on
an automatic segmenter combining rule-based and
statistical techniques (Zhao et al, 2001).
BLEU[%]
Original segmentation 15.14
Original segmentation + Word packing 16.99
Automatic segmentation 14.91
Automatic segmentation + Word packing 17.51
Table 5: Influence of Chinese segmentation
The results obtained are displayed in Table 5. As
expected, the automatic segmenter leads to slightly
lower results than the human-corrected segmenta-
tion. However, the proposed method seems to be
beneficial irrespective of the choice of segmentation.
Indeed, we can also observe an improvement in the
new setting: 2.6 points absolute increase in BLEU
(17.4% relative).11
11We could actually consider an extreme case, which would
consist of splitting the sentences into characters, i.e. each char-
acter would be blindly treated as one word. The segmentation
309
5.4 Exploiting Manually Developed Resources
We also compared our technique for automatic pack-
ing of words with the exploitation of manually
developed resources. More specifically, we used
a 1-to-n Chinese?English bilingual dictionary, de-
scribed in Section 3.4, and used it in place of the
automatically acquired dictionary. Words are thus
grouped according to this dictionary, and we then
apply the same word aligner as for previous experi-
ments. In this case, since we are not bootstrapping
from the output of a word aligner, this can actually
be seen as a pre-processing step prior to alignment.
These resources follow more or less the same for-
mat as the output of the word segmenter mentioned
in Section 5.1.2 (Zhao et al, 2001), so the experi-
ments are carried out using this segmentation.
BLEU[%]
Baseline 14.91
Automatic word packing 17.51
Packing with ?manual? dictionary 16.15
Table 6: Exploiting manually developed resources
The results obtained are displayed in Table 6.We
can observe that the use of the manually developed
dictionary provides us with an improvement in trans-
lation quality: 1.24 BLEU points absolute (8.3% rel-
ative). However, there does not seem to be a clear
gain when compared with the automatic method.
Even if those manual resources were extended, we
do not believe the improvement is sufficient enough
to justify this additional effort.
6 Conclusion and Future Work
In this paper, we have introduced a simple yet effec-
tive method to pack words together in order to give
a different and simplified input to automatic word
aligners. We use a bootstrap approach in which we
first extract 1-to-n word alignments using an exist-
ing word aligner, and then estimate the confidence
of those alignments to decide whether or not the n
words have to be grouped; if so, this group is con-
would thus be completely driven by the bilingual alignment pro-
cess (see also (Wu, 1997; Tiedemann, 2003) for related consid-
erations). In this case, our approach would be similar to the
approach of (Xu et al, 2004), except for the estimation of can-
didates.
sidered a new basic unit to consider. We can finally
re-apply the word aligner to the updated sentences.
We have evaluated the performance of our ap-
proach by measuring the influence of this process
on a Chinese-to-English MT task, based on the
IWSLT 2006 evaluation campaign. We report a
12.2% relative increase in BLEU score over a stan-
dard phrase-based SMT system. We have verified
that this process actually reduces the number of 1:n
alignments with n 6= 1, and that it is rather indepen-
dent from the (Chinese) segmentation strategy.
As for future work, we first plan to consider dif-
ferent confidence measures for the filtering of the
alignment candidates. We also want to bootstrap on
different word aligners; in particular, one possibility
is to use the flexible HMM word-to-phrase model of
Deng and Byrne (2005) in place of IBM model 4.
Finally, we would like to apply this method to other
corpora and language pairs.
Acknowledgment
This work is supported by Science Foundation Ire-
land (grant number OS/IN/1732). Prof. Tiejun Zhao
and Dr. Muyun Yang from the MT group of Harbin
Institute of Technology, and Yajuan Lv from the In-
stitute of Computing Technology, Chinese Academy
of Sciences, are kindly acknowledged for provid-
ing us with the Chinese segmenter and the manually
developed bilingual dictionary used in our experi-
ments.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond aer: An extensive analysis of word alignments
and their impact on mt. In Proceedings of COLING-
ACL 2006, pages 9?16, Sydney, Australia.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings of AMTA 2006, pages 10?18, Boston, MA.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of HLT-EMNLP 2005, pages
169?176, Vancouver, Canada.
310
Alexander Fraser and Daniel Marcu. 2006. Measuring
word alignment quality for statistical machine transla-
tion. Technical Report ISI-TR-616, ISI/University of
Southern California.
Mihoko Kitamura and Yuji Matsumoto. 1996. Auto-
matic extraction of word sequence correspondences in
parallel corpora. In Proceedings of the 4th Workshop
on Very Large Corpora, pages 79?87, Copenhagen,
Denmark.
Philip Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 48?54, Edmonton, Canada.
Philip Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004, pages 115?124,
Washington, District of Columbia.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL
2006, pages 104?111, New York, NY.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the link?
In Proceedings of AMTA 2006, pages 90?99, Cam-
bridge, MA.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP 2002, pages 133?139,
Morristown, NJ.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of EMNLP 1997, pages 97?108, Somerset,
New Jersey.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
Marian Olteanu, Chris Davis, Ionut Volosen, and Dan
Moldovan. 2006. Phramer - an open source statis-
tical phrase-based translator. In Proceedings of the
NAACL 2006 Workshop on Statistical Machine Trans-
lation, pages 146?149, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadelphia, PA.
Michael Paul. 2006. Overview of the IWSLT 2006 Eval-
uation Campaign. In Proceedings of IWSLT 2006,
pages 1?15, Kyoto, Japan.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1?38.
Andrea Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings of LREC 2002,
pages 147?152, Las Palmas, Spain.
Jo?rg Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of EACL 2003, pages 339?346,
Budapest, Hungary.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ?improve? our alignments? In
Proceedings of IWSLT 2006, pages 205?212, Kyoto,
Japan.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING 1996, pages 836?
841, Copenhagen, Denmark.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122?128, Barcelona, Spain.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proceedings of HLT-NAACL 2004, pages 257?264,
Boston, MA.
Tiejun Zhao, Yajuan Lu?, and Hao Yu. 2001. Increas-
ing accuracy of chinese segmentation with strategy of
multi-step processing. Journal of Chinese Information
Processing, 15(1):13?18.
311
Proceedings of the Third Workshop on Statistical Machine Translation, pages 171?174,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
MATREX: the DCU MT System for WMT 2008
John Tinsley, Yanjun Ma, Sylwia Ozdowska, Andy Way
National Centre for Language Technology
Dublin City University
Dublin 9, Ireland
{jtinsley, yma, sozdowska, away}@computing.dcu.ie
Abstract
In this paper, we give a description of the ma-
chine translation system developed at DCU
that was used for our participation in the eval-
uation campaign of the Third Workshop on
Statistical Machine Translation at ACL 2008.
We describe the modular design of our data-
driven MT system with particular focus on
the components used in this participation. We
also describe some of the significant modules
which were unused in this task.
We participated in the EuroParl task for the
following translation directions: Spanish?
English and French?English, in which we em-
ployed our hybrid EBMT-SMT architecture to
translate. We also participated in the Czech?
English News and News Commentary tasks
which represented a previously untested lan-
guage pair for our system. We report results
on the provided development and test sets.
1 Introduction
In this paper, we present the Data-Driven MT sys-
tems developed at DCU, MATREX (Machine Trans-
lation using Examples). This system is a hybrid sys-
tem which exploits EBMT and SMT techniques to
build a combined translation model.
We participated in both the French?English and
Spanish?English EuroParl tasks. In these two tasks,
we monolingually chunk both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004). We then align these chunks
using a dynamic programming, edit-distance-style
algorithm and combine them with phrase-based
SMT-style chunks into a single translation model.
We also participated in the Czech?English News
Commentary and News tasks. This language pair
represents a new challenge for our system and pro-
vides a good test of its flexibility.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the chunking and chunk
alignment strategies used for the shared task. In Sec-
tion 3, we outline the complete system setup for the
shared task, and in Section 4 we give some results
and discussion thereof.
2 The MATREX System
The MATREX system is a modular hybrid data-
driven MT system, built following established De-
sign Patterns, which exploits aspects of both the
EBMT and SMT paradigms. It consists of a num-
ber of extendible and re-implementable modules, the
most significant of which are:
? Word Alignment Module: outputs a set of word
alignments given a parallel corpus,
? Chunking Module: outputs a set of chunks
given an input corpus,
? Chunk Alignment Module: outputs aligned
chunk pairs given source and target chunks ex-
tracted from comparable corpora,
? Decoder: returns optimal translation given a
set of aligned sentence, chunk/phrase and word
pairs.
In some cases, these modules may comprise
wrappers around pre-existing software. For exam-
ple, our system configuration for the shared task
incorporates a wrapper around GIZA++ (Och and
Ney, 2003) for word alignment and a wrapper
around Moses (Koehn et al, 2007) for decoding. It
171
should be noted, however, that the complete system
is not limited to using only these specific module
choices. The following subsections describe those
modules unique to our system.
2.1 Marker-Based Chunking
The chunking module used for the shared task is
based on the Marker Hypothesis, a psycholinguistic
constraint which posits that all languages are marked
for surface syntax by a specific closed set of lex-
emes or morphemes which signify context. Using a
set of closed-class (or ?marker?) words for a particu-
lar language, such as determiners, prepositions, con-
junctions and pronouns, sentences are segmented
into chunks. A chunk is created at each new occur-
rence of a marker word with the restriction that each
chunk must contain at least one content (or non-
marker) word. An example of this chunking strategy
for English and Spanish is given in Figure 1.
2.2 Chunk Alignment
In order to align the chunks obtained by the chunk-
ing procedures described in Section 2.1, we make
use of an ?edit-distance-style? dynamic program-
ming alignment algorithm.
In the following, a denotes an alignment between
a target sequence e consisting of I chunks and a
source sequence f consisting of J chunks. Given
these sequences of chunks, we are looking for the
most likely alignment a?:
a? = argmax
a
P(a|e, f) = argmax
a
P(a, e|f).
We first consider alignments such as those ob-
tained by an edit-distance algorithm, i.e.
a = (t1, s1)(t2, s2) . . . (tn, sn),
with ?k ? J1, nK, tk ? J0, IK and sk ? J0, JK, and
?k < k?:
tk ? tk? or tk? = 0,
sk ? sk? or sk? = 0,
where tk = 0 (resp. sk = 0) denotes a non-aligned
target (resp. source) chunk.
We then assume the following model:
P(a, e|f) = ?kP(tk, sk, e|f) = ?kP(etk |fsk),
where P(e0|fj) (resp. P(ei|f0)) denotes an ?inser-
tion? (resp. ?deletion?) probability.
Assuming that the parameters P(etk |fsk) are
known, the most likely alignment is computed by
a simple dynamic-programming algorithm.1
Instead of using an Expectation-Maximization al-
gorithm to estimate these parameters, as commonly
done when performing word alignment (Brown
et al, 1993; Och and Ney, 2003), we directly com-
pute these parameters by relying on the information
contained within the chunks. The conditional prob-
ability P(etk |fsk) can be computed in several ways.
In our experiments, we have considered three main
sources of knowledge: (i) word-to-word translation
probabilities, (ii) word-to-word cognates, and (iii)
chunk labels. These sources of knowledge are com-
bined in a log-linear framework. The weights of
the log-linear model are not optimised; we experi-
mented with different sets of parameters and did not
find any significant difference as long as the weights
stay in the interval [0.5 ? 1.5]. Outside this inter-
val, the quality of the model decreases. More details
about the combination of knowledge sources can be
found in (Stroppa and Way, 2006).
2.3 Unused Modules
There are numerous other features available in our
system which, due to time constraints, were not ex-
ploited for the purposes of the shared task. They
include:
? Word packing (Ma et al, 2007): a bilingually
motivated packing of words that changes the
basic unit of the alignment process in order to
simplify word alignment.
? Supertagging (Hassan et al, 2007b): incorpo-
rating lexical syntactic descriptions, in the form
of supertags, to the language model and target
side of the translation model in order to better
inform decoding.
? Source-context features (Stroppa et al, 2007):
use memory-based classification to incorporate
context-informed features on the source side of
the translation model.
? Treebank-based phrase extraction (Tinsley
et al, 2007): extract word and phrase align-
ments based on linguistically informed sub-
sentential alignment of the parallel data.
1This algorithm is actually a classical edit-distance al-
gorithm in which distances are replaced by opposite-log-
conditional probabilities.
172
English: [I voted] [in favour] [of the strategy presented] [by the council] [concerning relations] [with
Mediterranean countries]
Spanish: [He votado] [a favor] [de la estrategia presentada] [por el consejo] [relativa las relaciones]
[con los pa??ses mediterrane?os]
Figure 1: English and Spanish Marker-Based chunking
Filter criteria es?en fr?en cz?en
Initial Total 1258778 1288074 1096941
Blank Lines 5632 4200 2
Length 6794 8361 2922
Fertility 120 82 1672
Final Total 1246234 1275432 1092345
Table 1: Summary of pre-processing on training data.
3 Shared Task Setup
The following section describes the system setup
using the Spanish?English and French?English Eu-
roParl, and Czech?English CzEng training data.
3.1 Pre-processing
For all tasks we initially tokenised the data (Czech
data was already tokenised) and removed blank
lines. We then filtered out sentence pairs based on
length (>100 words) and fertility (9:1 word ratio).
Finally we lowercased the data. Details of this pre-
processing are given in Table 1.
3.2 System Configuration
As mentioned in Section 2, our word alignment
module employs a wrapper around GIZA++.
We built a 5-gram language model based the tar-
get side of the training data. This was done using
the SRI Language Modelling toolkit (Stolcke, 2002)
employing linear interpolation and modified Kneser-
Ney discounting (Chen and Goodman, 1996).
Our phrase-table comprised a combination of
marker-based chunk pairs2, extracted as described
in Sections 2.1 and 2.2, and word-alignment-based
phrase pairs extracted using the ?grow-diag-final?
method of Koehn et al (2003), with a maximum
phrase length of 7 words. Phrase translation proba-
bilities were estimated by relative frequency over all
phrase pairs and were combined with other features,
2This module was omitted from the Czech?English system
as we have yet to verify whether marker-based chunking is ap-
propriate for Czech.
System BLEU (-EBMT) BLEU (+EBMT)
es?en 0.3283 0.3287
fr?en 0.2768 0.2770
cz?en 0.2235 -
Table 2: Summary of results on developments sets de-
vtest2006 for EuroParl tasks and nc-test2007 for cz?en
tasks.
System BLEU (-EBMT) BLEU (+EBMT)
es?en 0.3274 0.3285
fr?en 0.3163 0.3174
cz?en (news) 0.1458 -
cz?en (nc) 0.2217 -
Table 3: Summary of results on 2008 test data.
such as a reordering model, in a log-linear combina-
tion of functions.
We tuned our system on the development set de-
vtest2006 for the EuroParl tasks and on nc-test2007
for Czech?English, using minimum error-rate train-
ing (Och, 2003) to optimise BLEU score.
Finally, we carried out decoding using a wrapper
around the Moses decoder.
3.3 Post-processing
Case restoration was carried out by training the sys-
tem outlined above - without the EBMT chunk ex-
traction - to translate from the lowercased version
of the applicable target language training data to the
truecased version. We have previously shown this
approach to be very effective for both case and punc-
tuation restoration (Hassan et al, 2007a). The trans-
lations were then detokenised.
4 Results
The system output is evaluated with respect to
BLEU score. Results on the development sets and
test sets for each task are given in Tables 2 and 3
respectively, where ?-EBMT? indicates that EBMT
chunk modules were not used, and ?+EBMT? indi-
cates that they were used.
173
4.1 Discussion
Those configurations which incorporated the EBMT
chunks improved slightly over those which did not.
Groves (2007) has shown previously that combin-
ing EBMT and SMT translation models can lead to
considerable improvement over the baseline systems
from which they are derived. The results achieved
here lead us to believe that on such a large scale
there may be a more effective way to incorporate the
EBMT chunks.
Previous work has shown the EBMT chunks to
have higher precision than their SMT counterparts,
but they lack sufficient recall when used in isola-
tion (Groves, 2007). We believe that increasing their
influence in the translation model may lead to im-
proved translation accuracy. One experiment to this
effect would be to add the EBMT chunks as a sep-
arate phrase table in the log-linear model and allow
the decoder to chose when to use them.
Finally, we intend to exploit the unused modules
of the system in future experiments to investigate
their effects on the tasks presented here.
Acknowledgments
This work is supported by Science Foundation Ireland
(grant nos. 05/RF/CMS064 and OS/IN/1732). Thanks
also to the reviewers for their insightful comments and
suggestions.
References
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mercer,
R. L. (1993). The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Chen, S. F. and Goodman, J. (1996). An Empirical Study
of Smoothing Techniques for Language Modeling. In
Proceedings of the Thirty-Fourth Annual Meeting of
the Association for Computational Linguistics, pages
310?318, San Francisco, CA.
Gough, N. and Way, A. (2004). Robust Large-Scale
EBMT with Marker-Based Segmentation. In Proceed-
ings of the 10th International Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI-04), pages 95?104, Baltimore, MD.
Groves, D. (2007). Hybrid Data-Driven Models of Ma-
chine Translation. PhD thesis, Dublin City University,
Dublin, Ireland.
Hassan, H., Ma, Y., and Way, A. (2007a). MATREX: the
DCU Machine Translation System for IWSLT 2007. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 69?75, Trento, Italy.
Hassan, H., Sima?an, K., and Way, A. (2007b). Su-
pertagged Phrase-based Statistical Machine Transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL?07),
pages 288?295, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,
C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and
Herbst, E. (2007). Moses: Open Source Toolkit for
Statistical Machine Translation. In Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague, Czech
Republic.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology (NAACL ?03), pages 48?54, Ed-
monton, Canada.
Ma, Y., Stroppa, N., and Way, A. (2007). Boostrap-
ping Word Alignment via Word Packing. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL?07), pages 304?311,
Prague, Czech Republic.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan., Sapporo,
Japan.
Och, F. J. and Ney, H. (2003). A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
Stolcke, A. (2002). SRILM - An Extensible Language
Modeling Toolkit. In Proceedings of the Interna-
tional Conference Spoken Language Processing, Den-
ver, CO.
Stroppa, N., van den Bosch, A., and Way, A. (2007).
Exploiting Source Similarity for SMT using Context-
Informed Features. In Proceedings of the 11th Interna-
tional Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-07), pages 231?
240, Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the DCU ma-
chine translation system for IWSLT 2006. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation, pages 31?36, Kyoto, Japan.
Tinsley, J., Hearne, M., and Way, A. (2007). Exploiting
Parallel Treebanks to Improve Phrase-Based Statisti-
cal Machine Translation. In Proceedings of the Sixth
International Workshop on Treebanks and Linguistic
Theories (TLT-07), pages 175?187, Bergen, Norway.
174
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69?77,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Word Alignment Using Syntactic Dependencies
Yanjun Ma1 Sylwia Ozdowska1 Yanli Sun2 Andy Way1
1 School of Computing, Dublin City University, Dublin, Ireland
{yma,sozdowska,away}@computing.dcu.ie
2 School of Applied Language and Intercultural Studies,
Dublin City University, Dublin, Ireland
yanli.sun2@mail.dcu.ie
Abstract
We introduce a word alignment framework
that facilitates the incorporation of syntax en-
coded in bilingual dependency tree pairs. Our
model consists of two sub-models: an anchor
word alignment model which aims to find a set
of high-precision anchor links and a syntax-
enhanced word alignment model which fo-
cuses on aligning the remaining words relying
on dependency information invoked by the ac-
quired anchor links. We show that our syntax-
enhanced word alignment approach leads to a
10.32% and 5.57% relative decrease in align-
ment error rate compared to a generative word
alignment model and a syntax-proof discrim-
inative word alignment model respectively.
Furthermore, our approach is evaluated ex-
trinsically using a phrase-based statistical ma-
chine translation system. The results show
that SMT systems based on our word align-
ment approach tend to generate shorter out-
puts. Without length penalty, using our word
alignments yields statistically significant im-
provement in Chinese?English machine trans-
lation in comparison with the baseline word
alignment.
1 Introduction
Automatic word alignment can be defined as the
problem of determining translational correspon-
dences at word level given a parallel corpus of
aligned sentences. Bilingual word alignment is a
fundamental component of most approaches to sta-
tistical machine translation (SMT). Dominant ap-
proaches to word alignment can be classified into
two main schools: generative and discriminative
word alignment models.
Generative word alignment models, initially de-
veloped at IBM (Brown et al, 1993), and then
augmented by an HMM-based model (Vogel et al,
1996), have provided powerful modeling capability
for word alignment. However, it is very difficult to
incorporate new features into these models. Dis-
criminative word alignment models, based on dis-
criminative training of a set of features (Liu et al,
2005; Moore, 2005), on the other hand, are more
flexible to incorporate new features, and feature se-
lection is essential to the performance of the system.
Syntactic annotation of bilingual corpora, which
can be obtained more efficiently and accurately with
the advances in monolingual language processing,
is a potential information source for word align-
ment tasks. For example, Part-of-Speech (POS) tags
of source and target words can be used to tackle
the data sparseness problem in discriminative word
alignment (Liu et al, 2005; Blunsom and Cohn,
2006). Shallow parsing has also been used to pro-
vide relevant information for alignment (Ren et al,
2007; Sun et al, 2000). Deeper syntax, e.g. phrase
or dependency structures, has been shown useful in
generative models (Wang and Zhou, 2004; Lopez
and Resnik, 2005), heuristic-based models (Ayan et
al., 2004; Ozdowska, 2004) and even for syntac-
tically motivated models such as ITG (Wu, 1997;
Cherry and Lin, 2006).
In this paper, we introduce an approach to im-
prove word alignment by incorporating syntactic de-
pendencies. Our approach is motivated by the fact
that words tend to be dependent on each other. If
69
we can first obtain a set of reliable anchor links, we
could take advantage of the syntactic dependencies
relating unaligned words to aligned anchor words to
expand the alignment. Figure 1 gives an illustrating
example. Note that the link (2, 4) can be easily iden-
tified, but the link involving the fourth Chinese word
(a function word denoting ?time?) (4, 4) is hard. In
such cases, we can make use of the dependency re-
lationship (?tclause?) between c2 and c4 to help the
alignment process. Given such an observation, our
model is composed of two related alignment models.
The first one is an anchor alignment model which is
used to find a set of anchor links; the other one is a
syntax-enhanced alignment model aiming to process
the words left unaligned after anchoring.
Figure 1: How syntactic dependencies can help word
alignment: an example
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce our syntax-
enhanced discriminative word alignment approach.
The feature functions used are described in Sec-
tion 3. Experimental setting and results are pre-
sented in Section 4 and 5 respectively. In Section 6,
we compare our approach with other related word
alignment approaches. Section 7 concludes the pa-
per and gives avenues for future work.
2 Word Alignment Model
2.1 Notation
While in this paper we focus on Chinese?English,
the method proposed is applicable to any language
pair. The notation will assume Chinese?English
word alignment and Chinese?English MT. Here we
adopt a notation similar to (Brown et al, 1993).
Given a Chinese sentence cJ1 consisting of J words
{c1, ..., cJ} and an English sentence eI1 consisting of
I words e1, ..., eI , we define the alignment A be-
tween cJ1 and eI1 as a subset of the Cartesian product
of the word positions:
A ? {(j, i) : j = 1, ..., J ; i = 1, ..., I}
Our alignment representation is restricted so that
each source word can only be aligned to one tar-
get word. The alignment A consists of associations
j ? i = aj from a source position j to a target po-
sition i = aj . The ?null? alignment aj = 0 with the
?empty? word e0 is used to account for source words
that are not aligned to any target word.
We use A? to denote a subset of A. The indices of
the K source words involved in A? are represented
as ?K1 and the corresponding target indices for ?k
are represented as a?k . The unaligned source words
are represented as ??.
2.2 General Model
Given a source sentence cJ1 and target sentence eI1,
we seek to find the optimum alignment A? such that:
A? = argmax
A
P (A|cJ1 , eI1) (1)
We use a model (2) that directly models the link-
age between source and target words similarly to (It-
tycheriah and Roukos, 2005). We decompose this
model into an anchor alignment model (3) and a
syntax-enhanced model (4) by distinguishing the an-
chor alignment from the non-anchor alignment.
p(A|cJ1 , eI1) =
J
?
j=0
p(aj |cJ1 , eI1, aj?11 ) (2)
= 1Z ? p?(A?|c
J
1 , eI1) ? (3)
?
j???
p(aj|cJ1 , eI1, aj?11 , A?) (4)
2.3 Anchor Alignment Model
The anchor alignment model p?(A?) aims to find a
set of high precision links. Various approaches can
be used for this purpose. In this paper we adopted
the following two approaches.
2.3.1 Heuristics-based Approach
The problem of word alignment is regarded as a
process of word linkage disambiguation, i.e. choos-
ing the correct links between words from all com-
peting hypothesis (Melamed, 2000; Deng and Gao,
2007).
70
We constrain the link probabilities in such a way
that:
?i? ? {1, ..., I}, i? 6= i : p((j, i))p((j, i?)) > ?1 (5)
?j? ? {1, ..., J}, j? 6= j : p((j, i))p((j?, i)) > ?2 (6)
Condition (5) implies that for the source word cj ,
the link with the target word ei is more probable
(with reliability threshold ?1) than the link with any
other target word. Condition (6) guarantees that for
the target word ei, cj is the only most probable (with
threshold ?2) source word to be linked to.
2.3.2 Intersected Generative Word Alignment
Models
We can use the asymmetric IBM models for bidi-
rectional word alignment and get the intersection.
2.4 Syntax-Enhanced Word Alignment Model
The syntax-enhanced model is used to model the
alignment of the words left unaligned after anchor-
ing. We directly model the linkage between source
and target words using a discriminative word align-
ment framework where various features can be in-
corporated. Given a source word cj and the target
sentence eI1, we search for the alignment aj such
that:
a?j = argmax
aj
{p?M1 (aj |c
J
1 , eI1, a
j?1
1 , A?)} (7)
= argmax
aj
{?Mm=1 ?mhm(cJ1 , eI1, a
j
1, A?, Tc, Te)}
In this decision rule, we assume that a set of highly
reliable anchor alignments A? has been obtained,
and Tc (resp. Te) is used to denote the dependency
structure for source (resp. target) language. In such
a framework, various machine learning techniques
can be used for parameter estimation.
3 Feature Function for Syntax-Enhanced
Model
The various features used in our syntax-enhanced
model can be classified into three groups: statistics-
based features, syntactic features and relative distor-
tion features.
3.1 Statistics-based Features
3.1.1 IBM model 1 score
IBM model 1 is a position-independent word
alignment model which is often used to boot-
strap parameters for more complex models. Model
1 models the conditional distribution and uses a
uniform distribution for the dependencies between
source word positions and target word positions.
Pr(cJ1 , aJ1 |eI1) =
p(J |I)
(I + 1)J
J
?
j=1
p(cj |eaj ) (8)
3.1.2 Log-likelihood ratio
The log-likelihood ratio statistic has been found to
be accurate for modeling the associations between
rare events (Dunning, 1993). It has also been suc-
cessfully used to measure the associations between
word pairs (Melamed, 2000; Moore, 2005). Given
the following contingency table:
cj ?cj
ei a b
?ei c d
the log-likelihood ratio can be defined as:
G2(cj , ei) = ?2log
B(a|a + b, p1)B(c|c + d, p2)
B(a|a+ b, p)B(c|c + d, p)
where B(k|n, p) = (nk )pk(1 ? p)n?k are binomial
probabilities. The probability parameters can be ob-
tained using maximum likelihood estimates:
p1 =
a
a+ b , p2 =
c
c+ d (9)
p = a+ ca + b+ c+ d (10)
3.1.3 POS translation probability
The POS tags can provide effective information
for addressing the data sparseness problem using the
lexical features (Liu et al, 2005; Blunsom and Cohn,
2006). The POS translation probability can be easily
obtained using maximum likelihood estimation from
an annotated corpus:
Pr(Tc|Te) =
COL(Tc, Te)
COF (Te)
(11)
71
where Tc is a Chinese word?s POS tag and Te is an
English word?s POS tag. COL(Tc, Te) is the count
of Tc and Te being linked to each other in the corpus,
and COF (Te) is the frequency of Te in the corpus.
3.2 Syntactic Features
The dependency relation Re (resp. Rc) between two
English (resp. Chinese) words ei and ei? (resp. cj
and cj?) in the dependency tree of the English sen-
tence eI1 (resp. Chinese sentence cJ1 ) can be repre-
sented as a triple <ei, Re, ei?>(resp. <cj , Rc, ej?>).
Given cJ1 , eI1 and their syntactic dependency trees
TcJ1 , TeI1 , if ei is aligned to cj and ei? aligned to
cj? , according to the dependency correspondence as-
sumption (Hwa et al, 2002), there exists a triple
<cj , Rc, cj?>.
While we are not aiming to justify the feasibil-
ity of the dependency correspondence assumption
by proving to what extent Re = Rc under the con-
dition described above, we do believe that cj and cj?
are likely to be dependent on each other. Given the
anchor alignment A?, a candidate link (j, i) and the
dependency trees, we can design four classes of fea-
ture functions.
3.2.1 Agreement features
The agreement features can be further classi-
fied into dependency agreement features and depen-
dency label agreement features. Given a candidate
link (j, i) and the anchor alignment A?, the depen-
dency agreement (DA) feature function is defined as
follows:
hDA?1 =
?
?
?
?
?
1 if ? <cj, Rc, cj?>, <ei, Re, ei?>
and (j?, i?) ? A?,
0 otherwise.
(12)
By changing the dependency direction between the
words cj and cj? , we can derive another dependency
agreement feature:
hDA?2 =
?
?
?
?
?
1 if ? <cj? , Rc, cj>, <ei? , Re, ei>
and (j?, i?) ? A?,
0 otherwise.
(13)
We can define the dependency label agreement fea-
ture1 as follows:
hDLA?1 =
?
?
?
?
?
1 if ? <cj , Rc, cj?>, <ei, Re, ei?>
and (j?, i?) ? A?,Rc = Re,
0 otherwise.
(14)
Similarly we can obtain hDLA?2 by changing the
dependency direction.
3.2.2 Source word dependency features
Given a candidate link (j, i) and anchor alignment
A?, source language dependency features are used
to capture the dependency label between a source
word cj and a source anchor word ck ? ?. For
example, a feature function relating to dependency
type ?PRD? can be defined as:
hsrc?1?PRD =
?
?
?
?
?
1 if ? <cj, Rc, cj?>
and Rc =?PRD?,
0 otherwise.
(15)
By changing the direction we can obtain
hsrc?2?PRD.
3.2.3 Target word dependency features
Target word dependency features can be defined
in a similar way as source word dependency fea-
tures.
3.2.4 Target anchor feature
The target anchor feature defines whether the tar-
get word ei is an anchor word.
hsrc?1?PRD =
{
1 if i ? a?,
0 otherwise.
(16)
3.3 Relative distortion feature
We can design features encoding the relative dis-
tortion information which can be used to evaluate
a candidate link by computing its relative position
change with respect to the anchor alignment. The
relative position change of a candidate link l = (j, i)
is formally defined as follows:
1Note that we used the same dependency parser for source
and target language parsing.
72
D(l) = min(|dL|, |dR|) (17)
dL = (j ? jL) ? (i? iL) (18)
dR = (j ? jR)? (i? iR) (19)
where (iL, jL) is the leftmost anchor link of l,
(iR, jR) is the rightmost anchor link of l. The less
the relative position changes, the more likely the
candidate link is. With a set of anchor alignments,
we can obtain the distribution of the relative posi-
tion changes from an annotated corpus using maxi-
mum likelihood estimation. In our experiments, we
used the following four probabilities: p(D = 0),
p(D = 1, 2), p(D = 3, 4) and p(D > 4).
4 Experimental Setting
4.1 Data
The experiments were carried out using the
Chinese?English datasets provided within the
IWSLT 2007 evaluation campaign (Fordyce, 2007),
extracted from the Basic Travel Expression Corpus
(BTEC) (Takezawa et al, 2002). This multilingual
speech corpus contains sentences similar to those
that are usually found in phrase-books for tourists
going abroad.
We tagged all the sentences in the training and de-
vset3 using a maximum entropy-based POS tagger?
MXPOST (Ratnaparkhi, 1996), trained on the Penn
English and Chinese Treebanks. Both Chinese and
English sentences are parsed using the Malt depen-
dency parser (Nivre et al, 2007), which achieved
84% and 88% labelled attachment scores for Chi-
nese and English respectively.
4.1.1 Word Alignment
We manually annotated word alignments on de-
vset3. Since manual word alignment is an ambigu-
ous task, we also explicitly allow for ambiguous
alignments, i.e. the links are marked as sure (S) or
possible (P) (Och and Ney, 2003). IWSLT devset3
consists of 502 sentence pairs after cleaning. We
used the first 300 sentence pairs for training, the fol-
lowing 50 sentence pairs as validation set and the
last 152 sentence pairs for testing.
4.1.2 Machine Translation
Training was performed using the default training
set (39,952 sentence pairs), to which we added the
set devset1 (506 sentence pairs).2 We used devset2
(506 sentence pairs, 16 references) to tune various
parameters in the MT system and IWSLT 2007 test
set (489 sentence pairs, 6 references) for testing.
4.2 Alignment Training and Search
In our experiments, we treated anchor alignment and
syntax-enhanced alignment as separate processes in
a pipeline. The anchor alignments are kept fixed so
that the parameters in the syntax-enhanced model
can be optimized.3 We used the support vector ma-
chine (SVM) toolkit?SVM light4 to optimize the
parameters in (7). Our model is constrained in such
a way that each source word can only be aligned to
one target word. Therefore, in training, we trans-
form each possible link involving the words left un-
aligned after anchoring into an event. In testing, the
source words are consumed in sequence and the tar-
get words serve as states. The SVM dual variable
was used to measure the reliability of each candidate
link and the alignment link for each word is made
independently, which makes the alignment search
much easier. A threshold t was set as the minimal
reliability score for each link. t is optimized accord-
ing to alignment error rate (21) on the validation set.
4.3 Baselines
4.3.1 Word Alignment
We used the GIZA++ implementation of IBM
word alignment model 4 (Brown et al, 1993; Och
and Ney, 2003) for word alignment, and the heuris-
tics described in (Och and Ney, 2003) to derive the
intersection and refined alignment.
4.3.2 Machine Translation
We use a standard log-linear phrase-based SMT
(PB-SMT) model as a baseline: GIZA++ implemen-
tation of IBM word alignment model 4,5 the refine-
2More specifically, we chose the first English reference from
the 16 references and the Chinese sentence to construct new
sentence pairs.
3Note our anchor alignment does not achieve 100% preci-
sion. Since we performed precision-oriented alignment for the
anchor alignment model, the errors in anchor alignment will not
bring much noise into the syntax-enhanced model.
4http://svmlight.joachims.org/
5More specifically, we performed 5 iterations of Model 1, 5
iterations of HMM, 3 iterations of Model 3, and 3 iterations of
Model 4.
73
ment and phrase-extraction heuristics described in
(Koehn et al, 2003), minimum-error-rate training
(Och, 2003), a trigram language model with Kneser-
Ney smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and Moses
(Koehn et al, 2007) to decode.
4.4 Evaluation
We evaluate the intrinsic quality of predicted align-
ment A with precision, recall and alignment error
rate (AER). Slightly differently from (Och and Ney,
2003), we use possible alignments in computing re-
call.
recall = |A ? P ||P | , precision =
|A ? P |
|A| (20)
AER(S,P ;A) = 1? |A ? S|+ |A ? P ||A| + |S| (21)
We also extrinsically measure the word alignment
quality via a Chinese?English translation task. The
translation output is measured using BLEU (Pap-
ineni et al, 2002).
5 Experimental Results
5.1 Word Alignment
We performed word alignment bidirectionally using
our approach to obtain the union and compared our
results with two strong baselines based on generative
word alignment models. The results are shown in
Table 1. We can see that both the syntax-enhanced
model based on HMM intersection anchors (Syntax-
HMM) and on IBM model 4 anchors (Syntax-Model
4) are better than the pure generative word alignment
models. Our approach is superior in precision with
a disadvantage in recall. The best result achieved
10.32% relative decrease in AER compared to the
baseline when we use IBM model 4 intersection to
obtain the set of anchor alignments.
model precision recall f-score AER
HMM refined 0.8043 0.7592 0.7811 0.2059
Syntax-HMM 0.8744 0.7304 0.7959 0.1845
Model 4 refined 0.7941 0.7987 0.7964 0.1929
Syntax-Model 4 0.8566 0.7685 0.8102 0.1730
Table 1: Comparing syntax-enhanced approach with gen-
erative word alignment
5.1.1 The Influence of Anchor Alignment
Quality
As we can see in Table 2, our precision-oriented
approach to acquire anchor alignments was accom-
plished quite well. All four different anchor align-
ment models achieved high precision. However, the
recall differs dramatically, with model 4 achieving
the highest recall and the heuristics-based approach
receiving the lowest. To investigate the influence
anchor model precision recall f-measure AER
Heuristics 0.9774 0.4047 0.5724 0.3947
Model 1 0.9509 0.5011 0.6563 0.3157
HMM 0.9802 0.5327 0.6903 0.2809
Model 4 0.9777 0.5677 0.7179 0.2533
Table 2: Performance of anchor alignment
of the anchor alignment model, we first obtained
the intersection of the words left unaligned after an-
choring using each of the anchor alignment models.
We evaluate the alignment of these words against
the gold-standard alignments involving these words.
The influence of anchor alignment on the perfor-
mance of the syntax-enhanced model can be seen
in Table 3. The performance of the syntax-enhanced
model is closely related to that of the anchor align-
ment method. As can be seen from Table 2 and
3, HMM anchoring achieves the best precision and
so does the syntax-enhanced alignment; IBM model
4 achieves the best recall and so does the syntax-
enhanced alignment. Finally, the best alignment per-
formances are obtained with IBM model 4 anchor-
ing, with the difference in recall between HMM and
IBM model 4 anchoring being more significant than
the difference in precision.
anchor model precision recall f-score AER
Heuristics 0.4505 0.3270 0.3790 0.6210
Model 1 0.5538 0.3894 0.4573 0.5427
HMM 0.5932 0.3611 0.4489 0.5511
Model 4 0.5660 0.4216 0.4832 0.5168
Table 3: Influence of anchor alignment in syntax-
enhanced model
5.1.2 The Influence of Syntactic Dependencies
on Word Alignment
The influence of incorporating syntactic depen-
dencies into the word alignment process is shown
74
in Table 4. Syntax plays a positive role in all differ-
ent anchor alignment configurations. The influence
grows proportionally to the strength of the anchor
alignment model. With the Model 4 intersection
used as the set of anchor alignments, adding syn-
tactic dependency features into the syntax-enhanced
alignment model yields a 5.57% relative decrease in
AER.
model precision recall f-score AER
Heuristics
no syntax 0.8362 0.6751 0.7470 0.2302
w. syntax 0.8376 0.6894 0.7563 0.2240
Model 1
no syntax 0.8759 0.6902 0.7720 0.2045
w. syntax 0.8542 0.7160 0.7790 0.2011
HMM
no syntax 0.8655 0.7168 0.7841 0.1952
w. syntax 0.8744 0.7304 0.7959 0.1845
Model 4
no syntax 0.8697 0.7340 0.7961 0.1832
w. syntax 0.8566 0.7685 0.8102 0.1730
Table 4: Influence of syntactic dependencies on word
alignment
5.1.3 Contribution of Different Feature Classes
We interpret the contribution of each feature in
terms of feature weights in SVM model training.
The weights for the most discriminative features in
each feature class in Chinese?English word align-
ment (using HMM intersection as anchor align-
ment) are shown in Table 5. As we can see, all
statistics-based features are informative. Two target
dependency features are informative: ?PRD? denot-
ing ?predicative? dependency, and ?AMOD? denot-
ing ?adjective/adverb modifier? dependency.
weight
Model 1 Score 0.1416
POS 0.0540
Log-likelihood Ratio 0.0856
relative distortion 0.0606
DA-1 0.0227
DLA-2 0.0927
tgt-1-PRD 0.0961
tgt-2-AMOD 0.0621
Table 5: Weights of some informative features
5.2 Machine Translation
Research has shown that an increase in AER does
not necessarily imply an improvement in translation
quality (Liang et al, 2006) and vice-versa (Vilar et
al., 2006). Hereafter, we used a Chinese?English
MT task to extrinsically evaluate the quality of our
word alignment.
Table 6 shows the influence of our word align-
ment approach on MT quality.6 On development set,
we achieved statistically significant improvement
using both our syntax-enhanced models?Syntax-
HMM (p<0.002) and Syntax-Model 4 (p<0.008).
On the test set, we observed that the MT output
based on our alignment model tends to be shorter
than the reference translations and the BLEU score
is considerably penalized. If we ignore the length
penalty (?BP? in Table 6) in significance testing, the
improvement on test set is also statistically signif-
icant: p<0.04 for both Syntax-HMM and Syntax-
Model 4. However, an indepth manual analysis
needs to be carried out in order to determine the ex-
act nature of the shorter sentences derived.
dev. set test set
Baseline 0.5412 0.3510 (BP=0.96)
Syntax-HMM 0.6015 0.3409 (BP=0.86)
Syntax-Model 4 0.5834 0.3585 (BP=0.91)
Table 6: The Influence of Word Alignment on MT
6 Comparison with Previous Work
Our syntax-enhanced model is a discriminative word
alignment model. Certain generative word align-
ment models (e.g. HMM or IBM 4) also take
the first-order dependencies into account. How-
ever, long distance dependencies between words are
hard to incorporate into these models because of
the explosive number of parameters. On the other
hand, like existing discriminative models, our ap-
proach uses a set of informative features based on
co-occurrence statistics, e.g. log-likelihood ratio
and DICE score. The advantage of our approach is
the mechanism by which syntactic features may be
incorporated.
6Note that the only difference between our MT system and
the baseline PB-SMT system is the word alignment component.
75
Some previous research also tried to make use
of syntax in word alignment. (Wang and Zhou,
2004) investigated the benefit of monolingual pars-
ing for alignment. They learned a generalized word
association measure (crosslingual word similarities)
based on monolingual dependency structures and
improved alignment performances over IBM model
2 and certain heuristic-based models. (Cherry and
Lin, 2006) used dependency structures as soft con-
straints to improve word alignment in an ITG frame-
work. Compared to these models, our approach di-
rectly takes advantage of dependency relations as
they are transformed into feature functions incorpo-
rated into a discriminative word alignment frame-
work.
7 Conclusion and Future Work
In this paper, we proposed a model that can facili-
tate the incorporation of syntax into word alignment
and measured the combination of a set of syntactic
features. Experimental results have shown that syn-
tax is useful in word alignment and especially effec-
tive in improving the recall. We have also observed
that in our word alignment framework, the two sub-
models are closely related and the quality of the an-
chor alignment model plays an important role in the
system performance.
The promising results will lead us to improve our
model in the following aspects. First, the two sub-
models in our approach are two separate processes
performed in pipeline. We plan to jointly optimize
the two models in one go. Second, some of our
experiments used complex IBM models, e.g. IBM
Model 4, to obtain anchor alignment. We plan to
boostrap the alignment using simple heuristics with-
out relying on complex IBM models. Third, the
alignment searching process assumed the alignment
link for each word is made independently. A feasible
markovian assumption will be tested for searching.
Fourth, a comparison with traditional discriminative
word alignment models is also necessary to justify
the merits of our approach. Finally, we also plan to
adapt our approach to larger data sets and more lan-
guage pairs.
Acknowledgments
This work is supported by Science Foundation Ire-
land (grant number OS/IN/1732). Prof. Rebecca
Hwa from University of Pittsburgh and Dr. Yang
Liu from the Institute of Computing Technology,
Chinese Academy of Sciences, are kindly acknowl-
edged for providing us with their word alignment
guidelines. We would also like to thank the anony-
mous reviewers for their insightful comments.
References
Necip Fazil Ayan, Bonnie Dorr, and Nizar Habash.
2004. Multi-align: Combining linguistic and statis-
tical techniques to improve alignments for adaptable
mt. In Proceedings of the 6th Conference of the AMTA
(AMTA-2004), pages 17?26, Washington DC.
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 65?72, Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 105?112, Sydney, Australia.
Yonggang Deng and Yuqing Gao. 2007. Guiding sta-
tistical word alignment models with prior knowledge.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 1?8,
Prague, Czech Republic.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Cameron Shaw Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1?12, Trento, Italy.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspondence
using annotation projection. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 392?399, Philadelphia, PA.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of Human Language
76
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 89?
96, Vancouver, British Columbia, Canada.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 104?111, New York, NY.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 459?466, Ann Arbor, MI.
Adam Lopez and Philip Resnik. 2005. Improved HMM
alignment models for languages with scarce resources.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 83?86, Ann Arbor, Michi-
gan, June.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 81?88, Vancouver, British Columbia, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan.
Sylwia Ozdowska. 2004. Identifying correspondences
between words: an approach based on a bilingual syn-
tactic analysis of French/English parallel corpora. In
Proceedings of the COLING?04 Workshop on Multi-
lingual Linguistic Resources, pages 49?56, Geneva,
Switzerland.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Eric Brill and Ken-
neth Church, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 133?142, Somerset, NJ.
Dengjun Ren, Hua Wu, and Haifeng Wang. 2007. Im-
proving statistical word alignment with various clues.
In Machine Translation Summit XI, pages 391?397,
Copenhagen, Denmark.
Andrea Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Le Sun, Youbing Jin, Lin Du, and Yufang Sun. 2000.
Word alignment of English-Chinese bilingual corpus
based on chunks. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical Methods in Natural
Language Processing and very large corpora, pages
110?116.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world.
In Proceedings of Third International Conference on
Language Resources and Evaluation 2002, pages 147?
152, Las Palmas, Spain.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ?improve? our alignments? In
Proceedings of the International Workshop on Spoken
Language Translation, pages 205?212, Kyoto, Japan.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th International Con-
ference on Computational Linguistics, pages 836?841,
Copenhagen, Denmark.
Wei Wang and Ming Zhou. 2004. Improving word align-
ment models using structured monolingual corpora.
In Dekang Lin and Dekai Wu, editors, Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing, pages 198?205, Barcelona, Spain.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
77
Coling 2010: Poster Volume, pages 374?382,
Beijing, August 2010
Integrating N-best SMT Outputs into a TM System
Yifan He Yanjun Ma Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,away,josef}@computing.dcu.ie
Abstract
In this paper, we propose a novel frame-
work to enrich Translation Memory (TM)
systems with Statistical Machine Trans-
lation (SMT) outputs using ranking. In
order to offer the human translators mul-
tiple choices, instead of only using the
top SMT output and top TM hit, we
merge the N-best output from the SMT
system and the k-best hits with highest
fuzzy match scores from the TM sys-
tem. The merged list is then ranked ac-
cording to the prospective post-editing ef-
fort and provided to the translators to aid
their work. Experiments show that our
ranked output achieve 0.8747 precision at
top 1 and 0.8134 precision at top 5. Our
framework facilitates a tight integration
between SMT and TM, where full advan-
tage is taken of TM while high quality
SMT output is availed of to improve the
productivity of human translators.
1 Introduction
Translation Memories (TM) are databases that
store translated segments. They are often used to
assist translators and post-editors in a Computer
Assisted Translation (CAT) environment by re-
turning the most similar translated segments. Pro-
fessional post-editors and translators have long
been relying on TMs to avoid duplication of work
in translation.
With the rapid development in statistical ma-
chine translation (SMT), MT systems are begin-
ning to generate acceptable translations, espe-
cially in domains where abundant parallel corpora
exist. It is thus natural to ask if these translations
can be utilized in some way to enhance TMs.
However advances in MT are being adopted
only slowly and sometimes somewhat reluctantly
in professional localization and post-editing envi-
ronments because of 1) the usefulness of the TM,
2) the investment and effort the company has put
into TMs, and 3) the lack of robust SMT confi-
dence estimation measures which are as reliable
as fuzzy match scores (cf. Section 4.1.2) used in
TMs. Currently the localization industry relies on
TM fuzzy match scores to obtain both a good ap-
proximation of post-editing effort and an estima-
tion of the overall translation cost.
In a forthcoming paper, we propose a trans-
lation recommendation model to better integrate
MT outputs into a TM system. Using a binary
classifier, we only recommend an MT output to
the TM-user when the classifier is highly confi-
dent that it is better than the TM output. In this
framework, post-editors continue to work with the
TM while benefiting from (better) SMT outputs;
the assets in TMs are not wasted and TM fuzzy
match scores can still be used to estimate (the up-
per bound of) post-editing labor.
In the previous work, the binary predictor
works on the 1-best output of the MT and TM sys-
tems, presenting either the one or the other to the
post-editor. In this paper, we develop the idea fur-
ther by moving from binary prediction to ranking.
We use a ranking model to merge the k-best lists
of the two systems, and produce a ranked merged
374
list for post-editing. As the list is an enriched ver-
sion of the TM?s k-best list, the TM related assets
are better preserved and the cost estimation is still
valid as an upper bound.
More specifically, we recast SMT-TM integra-
tion as a ranking problem, where we apply the
Ranking SVM technique to produce a ranked list
of translations combining the k-best lists of both
the MT and the TM systems. We use features in-
dependent of the MT and TM systems for rank-
ing, so that outputs from MT and TM can have
the same set of features. Ideally the transla-
tions should be ranked by their associated post-
editing efforts, but given the very limited amounts
of human annotated data, we use an automatic
MT evaluation metric, TER (Snover et al, 2006),
which is specifically designed to simulate post-
editing effort to train and test our ranking model.
The rest of the paper is organized as follows:
we first briefly introduce related research in Sec-
tion 2, and review Ranking SVMs in Section 3.
The formulation of the problem and experiments
with the ranking models are presented in Sections
4 and 5. We analyze the post-editing effort ap-
proximated by the TER metric in Section 6. Sec-
tion 7 concludes and points out avenues for future
research.
2 Related Work
There has been some work to help TM users to
apply MT outputs more smoothly. One strand is
to improve the MT confidence measures to bet-
ter predict post-editing effort in order to obtain a
quality estimation that has the potential to replace
the fuzzy match score in the TM. To the best of
our knowledge, the first paper in this area is (Spe-
cia et al, 2009a), which uses regression on both
the automatic scores and scores assigned by post-
editors. The method is improved in (Specia et
al., 2009b), which applies Inductive Confidence
Machines and a larger set of features to model
post-editors? judgment of the translation quality
between ?good? and ?bad?, or among three levels
of post-editing effort.
Another strand is to integrate high confidence
MT outputs into the TM, so that the ?good? TM
entries will remain untouched. In our forthcoming
paper, we recommend SMT outputs to a TM user
when a binary classifier predicts that SMT outputs
are more suitable for post-editing for a particular
sentence.
The research presented here continues the line
of research in the second strand. The difference
is that we do not limit ourselves to the 1-best out-
put but try to produce a k-best output in a rank-
ing model. The ranking scheme also enables us
to show all TM hits to the user, and thus further
protects the TM assets.
There has also been work to improve SMT us-
ing the knowledge from the TM. In (Simard and
Isabelle, 2009), the SMT system can produce a
better translation when there is an exact or close
match in the corresponding TM. They use regres-
sion Support Vector Machines to model the qual-
ity of the TM segments. This is also related to
our work in spirit, but our work is in the opposite
direction, i.e. using SMT to enrich TM.
Moreover, our ranking model is related to
reranking (Shen et al, 2004) in SMT as well.
However, our method does not focus on produc-
ing better 1-best translation output for an SMT
system, but on improving the overall quality of the
k-best list that TM systems present to post-editors.
Some features in our work are also different in na-
ture to those used in MT reranking. For instance
we cannot use N-best posterior scores as they do
not make sense for the TM outputs.
3 The Support Vector Machines
3.1 The SVM Classifier
Classical SVMs (Cortes and Vapnik, 1995) are
binary classifiers that classify an input instance
based on decision rules which minimize the reg-
ularized error function in (Eq. 1):
min
w,b,?
1
2w
Tw + C
l?
i=1
?i
subject to: yi(wT xi + b) > 1 ? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {1,?1} are l training in-
stances. w is the weight vector, ? is the relaxation
variable and C > 0 is the penalty parameter.
3.2 Ranking SVM for SMT-TM Integration
The SVM classification algorithm is extended to
the ranking case in (Joachims, 2002). For a cer-
375
tain group of instances, the Ranking SVM aims
at producing a ranking r that has the maximum
Kendall?s ? coefficient with the the gold standard
ranking r?.
Kendall?s ? measures the relevance of two rank-
ings: ?(ra, rb) = P?QP+Q , where P and Q arethe amount of concordant and discordant pairs in
ra and rb. In practice, this is done by building
constraints to minimize the discordant pairs Q.
Following the basic idea, we show how Ranking
SVM can be applied to MT-TM integration as fol-
lows.
Assume that for each source sentence s, we
have a set of outputs from MT, M and a set of
outputs from TM, T. If we have a ranking r(s)
over translation outputs M?T where for each
translation output d ? M?T, (di, dj) ? r(s) iff
di <r(s) dj , we can rewrite the ranking constraints
as optimization constraints in an SVM, as in Eq.
(2).
min
w,b,?
1
2w
Tw + C? ?
subject to:
?(di, dj) ? r(s1) : w(?(s1, di)? ?(s1, dj)) > 1 ? ?i,j,1
...
?(di, dj) ? r(sn) : w(?(sn, di)? ?(sn, dj)) > 1? ?i,j,n
?i,j,k > 0
(2)
where ?(sn, di) is a feature vector of translation
output di given source sentence sn. The Ranking
SVM minimizes the discordant number of rank-
ings with the gold standard according to Kendall?s
? .
When the instances are not linearly separable,
we use a mapping function ? to map the features
xi (?(sn, di) in the case of ranking) to high di-
mensional space, and solve the SVMwith a kernel
function K in where K(xi, xj) = ?(xi)T?(xj).
We perform our experiments with the Radial
Basis Function (RBF) kernel, as in Eq. (3).
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (3)
4 The Ranking-based Integration Model
In this section we present the Ranking-based
SMT-TM integration model in detail. We first in-
troduce the k-best lists in MT (called N-best list)
and TM systems (called m-best list in this section)
and then move on to the problem formulation and
the feature set.
4.1 K-Best Lists in SMT and TM
4.1.1 The SMT N-best List
The N-best list of the SMT system is generated
during decoding according to the internal feature
scores. The features include language and transla-
tion model probabilities, reordering model scores
and a word penalty.
4.1.2 The TM M-Best List and the Fuzzy
Match Score
The m-best list of the TM system is gener-
ated in descending fuzzy match score. The fuzzy
match score (Sikes, 2007) uses the similarity of
the source sentences to predict a level to which a
translation is reusable or editable.
The calculation of fuzzy match scores is one of
the core technologies in TM systems and varies
among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Lev-
enshtein, 1966) between the source and TM en-
try, normalized by the length of the source as in
Eq. (4), as most of the current implementations
are based on edit distance while allowing some
additional flexible matching.
FuzzyMatch(t) = min
e
EditDistance(s, e)
Len(s) (4)
where s is the source side of the TM hit t, and e
is the source side of an entry in the TM.
4.2 Problem Formulation
Ranking lists is a well-researched problem in
the information retrieval community, and Ranking
SVMs (Joachims, 2002), which optimizes on the
ranking correlation ? have already been applied
successfully in machine translation evaluation (Ye
et al, 2007). We apply the same method here to
rerank a merged list of MT and TM outputs.
Formally given an MT-produced N-best list
M = {m1,m2, ...,mn}, a TM-produced m-best
list T = {t1, t2, ..., tm} for a input sentence s,
we define the gold standard using the TER met-
ric (Snover et al, 2006): for each d ? M?T,
(di, dj) ? r(s) iff TER(di) < TER(dj). We
train and test a Ranking SVM using cross vali-
dation on a data set created according to this cri-
terion. Ideally the gold standard would be cre-
ated by human annotators. We choose to use TER
376
as large-scale annotation is not yet available for
this task. Furthermore, TER has a high correla-
tion with the HTER score (Snover et al, 2006),
which is the TER score using the post-edited MT
output as a reference, and is used as an estimation
of post-editing effort.
4.3 The Feature Set
When building features for the Ranking SVM, we
are limited to features that are independent of the
MT and TM system. We experiment with system-
independent fluency and fidelity features below,
which capture translation fluency and adequacy,
respectively.
4.3.1 Fluency Features
Source-side Language Model Scores. We
compute the LM probability and perplexity of the
input source sentence on a language model trained
on the source-side training data of the SMT sys-
tem, which is also the TM database. The inputs
that have lower perplexity on this language model
are more similar to the data set on which the SMT
system is built.
Target-side LanguageModel Scores. We com-
pute the LM probability and perplexity as a mea-
sure of the fluency of the translation.
4.3.2 Fidelity Features
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM performs well
enough, these two sentences should be the same
or very similar. Therefore the fuzzy match score
here gives an estimation of the confidence level of
the output.
The IBMModel 1 Score. We compute the IBM
Model 1 score in both directions to measure the
correspondence between the source and target, as
it serves as a rough estimation of how good a
translation it is on the word level.
5 Experiments
5.1 Experimental Settings
5.1.1 Data
Our raw data set is an English?French trans-
lation memory with technical translation from a
multi-national IT security company, consisting of
51K sentence pairs. We randomly select 43K to
train an SMT system and translate the English side
of the remaining 8K sentence pairs, which is used
to run cross validation. Note that the 8K sentence
pairs are from the same TM, so that we are able to
create a gold standard by ranking the TER scores
of the MT and TM outputs.
Duplicated sentences are removed from the
data set, as those will lead to an exact match in
the TM system and will not be translated by trans-
lators. The average sentence length of the training
set is 13.5 words and the size of the training set
is comparable to the (larger) translation memories
used in the industry.
5.1.2 SMT and TM systems
We use a standard log-linear PB-SMT
model (Och and Ney, 2002): GIZA++ imple-
mentation of IBM word alignment model 4, the
phrase-extraction heuristics described in (Koehn
et al, 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
Moses (Koehn et al, 2007) to decode. We train a
system in the opposite direction using the same
data to produce the pseudo-source sentences.
We merge distinct 5-best lists from MT and TM
systems to produce a new ranking. To create the
distinct list for the SMT system, we search over
a 100-best list and keep the top-5 distinct out-
puts. Our data set consists of mainly short sen-
tences, leading to many duplications in the N-best
output of the SMT decoder. In such cases, top-
5 distinct outputs are good representations of the
SMT?s output.
5.2 Training, Tuning and Testing the
Ranking SVM
We run training and prediction of the Ranking
SVM in 4-fold cross validation. We use the
377
SVMlight1 toolkit to perform training and testing.
When using the Ranking SVM with the RBF
kernel, we have two free parameters to tune on:
the cost parameter C in Eq. (1) and the radius
parameter ? in Eq. (3). We optimize C and
? using a brute-force grid search before running
cross-validation and maximize precision at top-5,
with an inner 3-fold cross validation on the (outer)
Fold-1 training set. We search within the range
[2?6, 29], the step size is 2 on the exponent.
5.3 The Gold Standard
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























  
  
  0%
  20%
  40%
  60%
  80%
  100%
Top1 Top3 Top5
Go
ld S
tan
dar
d %
TM
MT
Figure 1: MT and TM?s percentage in gold stan-
dard
Figure 1 shows the composition of translations
in the gold standard. Each source sentence is asso-
ciated with a list of translations from two sources,
i.e. MT output and TM matches. This list of
translations is ranked from best to worst accord-
ing TER scores. The figure shows that over 80%
of the translations are from the MT system if we
only consider the top-1 translation. As the num-
ber of top translations we consider increases, more
TM matches can be seen. On the one hand, this
does show a large gap in quality between MT out-
put and TM matches; on the other hand, however,
it also reveals that we will have to ensure two ob-
jectives in ranking: the first is to rank the 80%
MT translations higher and the second is to keep
the 20% ?good? TM hits in the Top-5. We design
our evaluation metrics accordingly.
5.4 Evaluation Metrics
The aim of this research is to provide post-editors
with translations that in many cases are easier to
1http://svmlight.joachims.org/
edit than the original TM output. As we formulate
this as a ranking problem, it is natural to measure
the quality of the ranking output by the number
of better translations that are ranked high. Some-
times the top TM output is the easiest to edit; in
such a case we need to ensure that this translation
has a high rank, otherwise the system performance
will degrade.
Based on this observation, we introduce the
idea of relevant translations, and our evaluation
metrics: PREC@k and HIT@k.
Relevant Translations. We borrow the idea
of relevence from the IR community to define
the idea of translations worth ranking high. For
a source sentence s which has a top TM hit t,
we define an MT/TM output m as relevant, if
TER(m) ? TER(t). According to the defini-
tion, relevant translations should need no more
post-edits than the original top hit from the TM
system. Clearly the top TM hit is always relevant.
PREC@k. We calculate the precision
(PREC@k) of the ranking for evaluation. As-
suming that there are n relevant translations in
the top k list for a source sentence s, we have
PREC@k= n/k for s. We test PREC@k, for
k = 1...10, in order to evaluate the overall quality
of the ranking.
HIT@k. We also estimate the probability of
having one of the relevant translations in the top
k, denoted as HIT@k. For a source sentence s,
HIT@k equals to 1 if there is at least one relevant
translation in top k, and 0 otherwise. This mea-
sures the quality of the best translation in top k,
which is the translation the post-editor will find
and work on if she reads till the kth place in the
list. HIT@k equals to 1.0 at the end of the list.
We report the mean PREC@k and HIT@k for
all s with the 0.95 confidence interval.
5.5 Experimental Results
In Table 1 we report PREC@k and HIT@k
for k = 1..10. The ranking receives 0.8747
PREC@1, which means that most of the top
ranked translations have at least the same quality
as the top TM output. We notice that precision re-
mains above 0.8 till k = 5, leading us to conclude
that most of the relevant translations are ranked in
the top-5 positions in the list.
378
Table 1: PREC@k and HIT@k of Ranking
PREC % HIT %
k=1 87.47?1.60 87.47?1.60
k=2 85.42?1.07 93.36?0.53
k=3 84.13?0.94 95.74?0.61
k=4 82.79?0.57 97.08?0.26
k=5 81.34?0.51 98.04?0.23
k=6 79.26?0.59 99.41?0.25
k=7 74.99?0.53 99.66?0.29
k=8 70.87?0.59 99.84?0.10
k=9 67.23?0.48 99.94?0.08
k=10 64.00?0.46 100.0?0.00
Using the HIT@k scores we can further con-
firm this argument. The HIT@k score grows
steadily from 0.8747 to 0.9941 for k = 1...6, so
most often there will be at least one relevant trans-
lation in top-6 for the post-editor to work with.
After that room for improvement becomes very
small.
In sum, both of the PREC@k scores and the
HIT@k scores show that the ranking model effec-
tively integrates the two translation sources (MT
and TM) into one merged k-best list, and ranks
the relevant translations higher.
Table 2: PREC@k - MT and TM Systems
MT % TM %
k=1 85.87?1.32 100.0?0.00
k=2 82.52?1.60 73.58?1.04
k=3 80.05?1.11 62.45?1.14
k=4 77.92?0.95 56.11?1.11
k=5 76.22?0.87 51.78?0.78
To measure whether the ranking model is ef-
fective compared to pure MT or TM outputs, we
report the PREC@k of those outputs in Table 2.
The k-best output used in this table is ranked by
the MT or TM system, without being ranked by
our model. We see the ranked outputs consistently
outperform the MT outputs for all k = 1...5 w.r.t.
precision at a significant level, indicating that our
system preserves some high quality hits from the
TM.
The TM outputs alone are generally of much
lower quality than the MT and Ranked outputs, as
is shown by the precision scores for k = 2...5. But
TM translations obtain 1.0 PREC@1 according to
the definition of the PREC calculation. Note that
it does not mean that those outputs will need less
post-editing (cf. Section 6.1), but rather indicates
that each one of these outputs meet the lowest ac-
ceptable criterion to be relevant.
6 Analysis of Post-Editing Effort
A natural question follows the PREC and HIT
numbers: after reading the ranked k-best list, will
the post-editors edit less than they would have to if
they did not have access to the list? This question
would be best answered by human post-editors in
a large-scale experimental setting. As we have not
yet conducted a manual post-editing experiment,
we try to measure the post-editing effort implied
by our model with the edit statistics captured by
the TER metric, sorted into four types: Insertion,
Substitution, Deletion and Shift. We report the av-
erage number of edits incurred along with the 0.95
confidence interval.
6.1 Top-1 Edit Statistics
We report the results on the 1-best output of TM,
MT and our ranking system in Table 3.
In the single best results, it is easy to see that
the 1-best output from the MT system requires
the least post-editing effort. This is not surpris-
ing given the distribution of the gold standard in
Section 5.3, where most MT outputs are of better
quality than the TM hits.
Moreover, since TM translations are generally
of much lower quality as is indicated by the num-
bers in Table 3 (e.g. 2x as many substitutions
and 3x as many deletions compared to MT), un-
justly including very few of them in the ranking
output will increase loss in the edit statistics. This
explains why the ranking model has better rank-
ing precision in Tables 1 and 2, but seems to in-
cur more edit efforts. However, in practice post-
editors can neglect an obvious ?bad? translation
very quickly.
6.2 Top-k Edit Statistics
We report edit statistics of the Top-3 and Top-5
outputs in Tables 4 and 5, respectively. For each
system we report two sets of statistics: the Best-
statistics calculated on the best output (according
379
Table 3: Edit Statistics on Ranked MT and TM Outputs - Single Best
Insertion Substitution Deletion Shift
TM-Top1 0.7554 ? 0.0376 4.2461 ? 0.0960 2.9173 ? 0.1027 1.1275 ? 0.0509
MT-Top1 0.9959 ? 0.0385 2.2793 ? 0.0628 0.8940 ? 0.0353 1.2821 ? 0.0575
Rank-Top1 1.0674 ? 0.0414 2.6990 ? 0.0699 1.1246 ? 0.0412 1.2800 ? 0.0570
to TER score) in the list, and the Mean- statistics
calculated on the whole Top-k list.
The Mean- numbers allow us to have a general
overview of the ranking quality, but it is strongly
influenced by the poor TM hits that can easily be
neglected in practice. To control the impact of
those TM hits, we rely on the Best- numbers to es-
timate the edits performed on the translations that
are more likely to be used by post-editors.
In Table 4, the ranking output?s edit statistics
is closer to the MT output than the Top-1 case
in Table 3. Table 5 continues this tendency, in
which the Best-in-Top5 Ranking output requires
marginally less Substitution and Deletion opera-
tions and significantly less Insertion and Shift op-
erations (starred) than its MT counterpart. This
shows that when more of the list is explored, the
advantage of the ranking model ? utilizing mul-
tiple translation sources ? begins to compensate
for the possible large number of edits required by
poor TM hits and finally leads to reduced post-
editing effort.
There are several explanations to why the rel-
ative performance of the ranking model improves
when k increases, as compared to other models.
The most obvious explanation is that a single poor
translation is less likely to hurt edit statistics on
a k-best list with large k, if most of the transla-
tions in the k-best list are of good quality. We see
from Tables 1 and 2 that the ranking output is of
better quality than the MT and TM outputs w.r.t.
precision. For a larger k, the small number of in-
correctly ranked translations are less likely to be
chosen as the Best- translation and hold back the
Best- numbers.
A further reason is related to our ranking model
which optimizes on Kendall?s ? score. Accord-
ingly the output might not be optimal when we
evaluate the Top-1 output, but will behave better
when we evaluate on the list. This is also in ac-
cordance with our aim, which is to enrich the TM
with MT outputs and help the post-editor, instead
of choosing the translation for the post-editor.
6.3 Comparing the MT, TM and Ranking
Outputs
One of the interesting findings from Tables 3 and
4 is that according to the TER edit statistics, the
MT outputs generally need a smaller number of
edits than the TM and Ranking outputs. This cer-
tainly confirms the necessity to integrate MT into
today?s TM systems.
However, this fact should not lead to the con-
clusion that TMs should be replaced by MT com-
pletely. First of all, all of our experiments exclude
exact TM matches, as those translations will sim-
ply be reused and not translated. While this is a
realistic setting in the translation industry, it re-
moves all sentences for which the TM works best
from our evaluations.
Furthermore, Table 5 shows that the Best-in-
Top5 Ranking output performs better than the MT
outputs, hence there are TM outputs that lead to
smaller number of edits. As k increases, the rank-
ing model is able to better utilize these outputs.
Finally, in this task we concentrate on rank-
ing useful translations higher, but we are not in-
terested in how useless translations are ranked.
Ranking SVM optimizes on the ranking of the
whole list, which is slightly different from what
we actually require. One option is to use other
optimization techniques that can make use of this
property to get better Top-k edit statistics for a
smaller k. Another option is obviously to perform
regression directly on the number of edits instead
of modeling on the ranking. We plan to explore
these ideas in future work.
7 Conclusions and Future Work
In this paper we present a novel ranking-based
model to integrate SMT into a TM system, in or-
der to facilitate the work of post-editors. In such
380
Table 4: Edit Statistics on Ranked MT and TM Outputs - Top 3
Insertion Substitution Deletion Shift
TM-Best-in-Top3 0.4241 ? 0.0250 3.7395 ? 0.0887 2.9561 ? 0.0966 0.9738 ? 0.0505
TM-Mean-Top3 0.6718 ? 0.0200 5.1428 ? 0.0559 3.6192 ? 0.0649 1.3233 ? 0.0310
MT-Best?in-Top3 0.7696 ? 0.0351 1.9210 ? 0.0610 0.7706 ? 0.0332 1.0842 ? 0.0545
MT-Mean-Top3 1.1296 ? 0.0229 2.4405 ? 0.0368 0.9341 ? 0.0209 1.3797 ? 0.0344
Rank-Best-in-Top3 0.8170 ? 0.0355 2.0744 ? 0.0608 0.8410 ? 0.0338 1.0399 ? 0.0529
Rank-Mean-Top3 1.0942 ? 0.0234 2.7437 ? 0.0392 1.0786 ? 0.0231 1.3309 ? 0.0334
Table 5: Edit Statistics on Ranked MT and TM Outputs
Insertion Substitution Deletion Shift
TM-Best-in-Top5 0.4239 ? 0.0250 3.7319 ? 0.0885 2.9552 ? 0.0967 0.9673 ? 0.0504
TM-Mean-Top5 0.6143 ? 0.0147 5.5092 ? 0.0473 3.9451 ? 0.0521 1.3737 ? 0.0240
MT-Best-in-Top5 0.7690 ? 0.0351 1.9163 ? 0.0610 0.7685 ? 0.0332 1.0811 ? 0.0544
MT-Mean-Top5 1.1912 ? 0.0182 2.5326 ? 0.0291 0.9487 ? 0.0165 1.4305 ? 0.0272
Rank-Best-in-Top5 0.7246 ? 0.0338* 1.8887 ? 0.0598 0.7562 ? 0.0327 0.9705 ? 0.0515*
Rank-Mean-Top5 1.1173 ? 0.0181 2.8777 ? 0.0312 1.1585 ? 0.0200 1.3675 ? 0.0260
a model, the user of the TM will be presented
with an augmented k-best list, consisting of trans-
lations from both the TM and theMT systems, and
ranked according to ascending prospective post-
editing effort.
From the post-editors? point of view, the TM
remains intact. And unlike in the binary transla-
tion recommendation, where only one translation
recommendation is provided, the ranking model
offers k-best post-editing candidates, enabling the
user to use more resources when translating. As
we do not actually throw away any translation pro-
duced from the TM, the assets represented by the
TM are preserved and the related estimation of the
upper bound cost is still valid.
We extract system independent features from
theMT and TM outputs and use Ranking SVMs to
train the ranking model, which outperforms both
the TM?s and MT?s k-best list w.r.t. precision at k,
for all ks.
We also analyze the edit statistics of the inte-
grated k-best output using the TER edit statistics.
Our ranking model results in slightly increased
number of edits compared to the MT output (ap-
parently held back by a small number of poor TM
outputs that are ranked high) for a smaller k, but
requires less edits than both the MT and the TM
output for a larger k.
This work can be extended in a number of ways.
Most importantly, We plan to conduct a user study
to validate the effectiveness of the method and
to gather HTER scores to train a better ranking
model. Furthermore, we will try to experiment
with learning models that can further reduce the
number of edit operations on the top ranked trans-
lations. We also plan to improve the adaptability
of this method and apply it beyond a specific do-
main and language pair.
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We
thank Symantec for providing the TM database
and the anonymous reviewers for their insightful
comments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Joachims, Thorsten. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 133?142, New York, NY, USA.
381
Koehn, Philipp., Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
(NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions (ACL-2007), pages 177?
180, Prague, Czech Republic.
Levenshtein, Vladimir Iosifovich. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10(8):707?710.
Och, Franz Josef and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 295?302,
Philadelphia, PA, USA.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics (ACL-2003), pages 160?167,
Morristown, NJ, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine trans-
lation. In HLT-NAACL 2004: Main Proceedings,
pages 177?184, Boston, Massachusetts, USA. As-
sociation for Computational Linguistics.
Sikes, Richard. 2007. Fuzzy matching in theory and
practice. Multilingual, 18(6):39 ? 43.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120 ? 127, Ottawa, Ontario, Canada.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas (AMTA-2006), pages 223?231,
Cambridge, MA, USA.
Specia, Lucia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009a. Esti-
mating the sentence-level quality of machine trans-
lation systems. In Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Specia, Lucia, Craig Saunders, Marco Turchi, Zhuo-
ran Wang, and John Shawe-Taylor. 2009b. Improv-
ing the confidence of machine translation quality
estimates. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 136 ?
143, Ottawa, Ontario, Canada.
Stolcke, Andreas. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, volume 2, pages 901?904, Denver, CO,
USA.
Ye, Yang, Ming Zhou, and Chin-Yew Lin. 2007.
Sentence level machine translation evaluation as a
ranking. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 240?247,
Prague, Czech Republic.
382
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622?630,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bridging SMT and TM with Translation Recommendation
Yifan He Yanjun Ma Josef van Genabith Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,josef,away}@computing.dcu.ie
Abstract
We propose a translation recommendation
framework to integrate Statistical Machine
Translation (SMT) output with Transla-
tion Memory (TM) systems. The frame-
work recommends SMT outputs to a TM
user when it predicts that SMT outputs are
more suitable for post-editing than the hits
provided by the TM. We describe an im-
plementation of this framework using an
SVM binary classifier. We exploit meth-
ods to fine-tune the classifier and inves-
tigate a variety of features of different
types. We rely on automatic MT evalua-
tion metrics to approximate human judge-
ments in our experiments. Experimental
results show that our system can achieve
0.85 precision at 0.89 recall, excluding ex-
act matches. Furthermore, it is possible for
the end-user to achieve a desired balance
between precision and recall by adjusting
confidence levels.
1 Introduction
Recent years have witnessed rapid developments
in statistical machine translation (SMT), with con-
siderable improvements in translation quality. For
certain language pairs and applications, automated
translations are now beginning to be considered
acceptable, especially in domains where abundant
parallel corpora exist.
However, these advances are being adopted
only slowly and somewhat reluctantly in profes-
sional localization and post-editing environments.
Post-editors have long relied on translation memo-
ries (TMs) as the main technology assisting trans-
lation, and are understandably reluctant to give
them up. There are several simple reasons for
this: 1) TMs are useful; 2) TMs represent con-
siderable effort and investment by a company or
(even more so) an individual translator; 3) the
fuzzy match score used in TMs offers a good ap-
proximation of post-editing effort, which is useful
both for translators and translation cost estimation
and, 4) current SMT translation confidence esti-
mation measures are not as robust as TM fuzzy
match scores and professional translators are thus
not ready to replace fuzzy match scores with SMT
internal quality measures.
There has been some research to address this is-
sue, see e.g. (Specia et al, 2009a) and (Specia et
al., 2009b). However, to date most of the research
has focused on better confidence measures for MT,
e.g. based on training regression models to per-
form confidence estimation on scores assigned by
post-editors (cf. Section 2).
In this paper, we try to address the problem
from a different perspective. Given that most post-
editing work is (still) based on TM output, we pro-
pose to recommend MT outputs which are better
than TM hits to post-editors. In this framework,
post-editors still work with the TM while benefit-
ing from (better) SMT outputs; the assets in TMs
are not wasted and TM fuzzy match scores can
still be used to estimate (the upper bound of) post-
editing labor.
There are three specific goals we need to
achieve within this framework. Firstly, the rec-
ommendation should have high precision, other-
wise it would be confusing for post-editors and
may negatively affect the lower bound of the post-
editing effort. Secondly, although we have full
access to the SMT system used in this paper,
our method should be able to generalize to cases
where SMT is treated as a black-box, which is of-
622
ten the case in the translation industry. Finally,
post-editors should be able to easily adjust the rec-
ommendation threshold to particular requirements
without having to retrain the model.
In our framework, we recast translation recom-
mendation as a binary classification (rather than
regression) problem using SVMs, perform RBF
kernel parameter optimization, employ posterior
probability-based confidence estimation to sup-
port user-based tuning for precision and recall, ex-
periment with feature sets involvingMT-, TM- and
system-independent features, and use automatic
MT evaluation metrics to simulate post-editing ef-
fort.
The rest of the paper is organized as follows: we
first briefly introduce related research in Section 2,
and review the classification SVMs in Section 3.
We formulate the classification model in Section 4
and present experiments in Section 5. In Section
6, we analyze the post-editing effort approximated
by the TER metric (Snover et al, 2006). Section
7 concludes the paper and points out avenues for
future research.
2 Related Work
Previous research relating to this work mainly fo-
cuses on predicting the MT quality.
The first strand is confidence estimation for MT,
initiated by (Ueffing et al, 2003), in which pos-
terior probabilities on the word graph or N-best
list are used to estimate the quality of MT out-
puts. The idea is explored more comprehensively
in (Blatz et al, 2004). These estimations are often
used to rerank the MT output and to optimize it
directly. Extensions of this strand are presented
in (Quirk, 2004) and (Ueffing and Ney, 2005).
The former experimented with confidence esti-
mation with several different learning algorithms;
the latter uses word-level confidence measures to
determine whether a particular translation choice
should be accepted or rejected in an interactive
translation system.
The second strand of research focuses on com-
bining TM information with an SMT system, so
that the SMT system can produce better target lan-
guage output when there is an exact or close match
in the TM (Simard and Isabelle, 2009). This line
of research is shown to help the performance of
MT, but is less relevant to our task in this paper.
A third strand of research tries to incorporate
confidence measures into a post-editing environ-
ment. To the best of our knowledge, the first paper
in this area is (Specia et al, 2009a). Instead of
modeling on translation quality (often measured
by automatic evaluation scores), this research uses
regression on both the automatic scores and scores
assigned by post-editors. The method is improved
in (Specia et al, 2009b), which applies Inductive
Confidence Machines and a larger set of features
to model post-editors? judgement of the translation
quality between ?good? and ?bad?, or among three
levels of post-editing effort.
Our research is more similar in spirit to the third
strand. However, we use outputs and features from
the TM explicitly; therefore instead of having to
solve a regression problem, we only have to solve
a much easier binary prediction problem which
can be integrated into TMs in a straightforward
manner. Because of this, the precision and recall
scores reported in this paper are not directly com-
parable to those in (Specia et al, 2009b) as the lat-
ter are computed on a pure SMT system without a
TM in the background.
3 Support Vector Machines for
Translation Quality Estimation
SVMs (Cortes and Vapnik, 1995) are binary clas-
sifiers that classify an input instance based on de-
cision rules which minimize the regularized error
function in (1):
min
w,b,?
1
2w
Tw + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training
instances that are mapped by the function ? to a
higher dimensional space. w is the weight vec-
tor, ? is the relaxation variable and C > 0 is the
penalty parameter.
Solving SVMs is viable using the ?kernel
trick?: finding a kernel function K in (1) with
K(xi, xj) = ?(xi)T?(xj). We perform our ex-
periments with the Radial Basis Function (RBF)
kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we
have two free parameters to tune on: the cost pa-
rameter C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimized by a brute-force grid
623
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
4 Translation Recommendation as
Binary Classification
We use an SVM binary classifier to predict the rel-
ative quality of the SMT output to make a recom-
mendation. The SVM classifier uses features from
the SMT system, the TM and additional linguis-
tic features to estimate whether the SMT output is
better than the hit from the TM.
4.1 Problem Formulation
As we treat translation recommendation as a bi-
nary classification problem, we have a pair of out-
puts from TM and MT for each sentence. Ideally
the classifier will recommend the output that needs
less post-editing effort. As large-scale annotated
data is not yet available for this task, we use auto-
matic TER scores (Snover et al, 2006) as the mea-
sure for the required post-editing effort. In the fu-
ture, we hope to train our system on HTER (TER
with human targeted references) scores (Snover et
al., 2006) once the necessary human annotations
are in place. In the meantime we use TER, as TER
is shown to have high correlation with HTER.
We label the training examples as in (3):
y =
{
+1 if TER(MT) < TER(TM)
?1 if TER(MT) ? TER(TM) (3)
Each instance is associated with a set of features
from both the MT and TM outputs, which are dis-
cussed in more detail in Section 4.3.
4.2 Recommendation Confidence Estimation
In classical settings involving SVMs, confidence
levels are represented as margins of binary predic-
tions. However, these margins provide little in-
sight for our application because the numbers are
only meaningful when compared to each other.
What is more preferable is a probabilistic confi-
dence score (e.g. 90% confidence) which is better
understood by post-editors and translators.
We use the techniques proposed by (Platt, 1999)
and improved by (Lin et al, 2007) to obtain the
posterior probability of a classification, which is
used as the confidence score in our system.
Platt?s method estimates the posterior probabil-
ity with a sigmod function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B) (4)
where f = f(x) is the decision function of the
estimated SVM. A and B are parameters that min-
imize the cross-entropy error function F on the
training data, as in Eq. (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{N++1
N++2
if yi = +1
1
N?+2
if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label
yi. These numbers are obtained using an internal
cross-validation on the training set.
4.3 The Feature Set
We use three types of features in classification: the
MT system features, the TM feature and system-
independent features.
4.3.1 The MT System Features
These features include those typically used in
SMT, namely the phrase-translation model scores,
the language model probability, the distance-based
reordering score, the lexicalized reordering model
scores, and the word penalty.
4.3.2 The TM Feature
The TM feature is the fuzzy match (Sikes, 2007)
cost of the TM hit. The calculation of fuzzy match
score itself is one of the core technologies in TM
systems and varies among different vendors. We
compute fuzzy match cost as the minimum Edit
Distance (Levenshtein, 1966) between the source
and TM entry, normalized by the length of the
source as in (6), as most of the current implemen-
tations are based on edit distance while allowing
some additional flexible matching.
hfm(t) = min
e
EditDistance(s, e)
Len(s) (6)
where s is the source side of t, the sentence to
translate, and e is the source side of an entry in the
TM. For fuzzy match scores F , this fuzzy match
cost hfm roughly corresponds to 1?F . The differ-
ence in calculation does not influence classifica-
tion, and allows direct comparison between a pure
TM system and a translation recommendation sys-
tem in Section 5.4.2.
624
4.3.3 System-Independent Features
We use several features that are independent of
the translation system, which are useful when a
third-party translation service is used or the MT
system is simply treated as a black-box. These
features are source and target side LM scores,
pseudo source fuzzy match scores and IBM model
1 scores.
Source-Side Language Model Score and Per-
plexity. We compute the language model (LM)
score and perplexity of the input source sentence
on a LM trained on the source-side training data of
the SMT system. The inputs that have lower per-
plexity or higher LM score are more similar to the
dataset on which the SMT system is built.
Target-Side Language Model Perplexity. We
compute the LM probability and perplexity of the
target side as a measure of fluency. Language
model perplexity of the MT outputs are calculated,
and LM probability is already part of the MT sys-
tems scores. LM scores on TM outputs are also
computed, though they are not as informative as
scores on the MT side, since TM outputs should
be grammatically perfect.
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM system performs
well enough, these two sentences should be the
same or very similar. Therefore, the fuzzy match
score here gives an estimation of the confidence
level of the output. We compute this score for both
the MT output and the TM hit.
The IBM Model 1 Score. The fuzzy match
score does not measure whether the hit could be
a correct translation, i.e. it does not take into ac-
count the correspondence between the source and
target, but rather only the source-side information.
For the TM hit, the IBM Model 1 score (Brown
et al, 1993) serves as a rough estimation of how
good a translation it is on the word level; for the
MT output, on the other hand, it is a black-box
feature to estimate translation quality when the in-
formation from the translation model is not avail-
able. We compute bidirectional (source-to-target
and target-to-source) model 1 scores on both TM
and MT outputs.
5 Experiments
5.1 Experimental Settings
Our raw data set is an English?French translation
memory with technical translation from Syman-
tec, consisting of 51K sentence pairs. We ran-
domly selected 43K to train an SMT system and
translated the English side of the remaining 8K
sentence pairs. The average sentence length of
the training set is 13.5 words and the size of the
training set is comparable to the (larger) TMs used
in the industry. Note that we remove the exact
matches in the TM from our dataset, because ex-
act matches will be reused and not presented to the
post-editor in a typical TM setting.
As for the SMT system, we use a stan-
dard log-linear PB-SMT model (Och and Ney,
2002): GIZA++ implementation of IBM word
alignment model 4,1 the refinement and phrase-
extraction heuristics described in (Koehn et
al., 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al, 2007) to
decode. We train a system in the opposite direc-
tion using the same data to produce the pseudo-
source sentences.
We train the SVM classifier using the lib-
SVM (Chang and Lin, 2001) toolkit. The SVM-
training and testing is performed on the remaining
8K sentences with 4-fold cross validation. We also
report 95% confidence intervals.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 4-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameterC in (1) we search in the range
[2?5, 215], and for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the expo-
nent.
5.2 The Evaluation Metrics
We measure the quality of the classification by
precision and recall. Let A be the set of recom-
mended MT outputs, and B be the set of MT out-
puts that have lower TER than TM hits. We stan-
dardly define precision P , recall R and F-value as
in (7):
1More specifically, we performed 5 iterations of Model 1,
5 iterations of HMM, 3 iterations of Model 3, and 3 iterations
of Model 4.
625
P = |A
?
B|
|A| , R =
|A
?
B|
|B| and F =
2PR
P + R (7)
5.3 Recommendation Results
In Table 1, we report recommendation perfor-
mance using MT and TM system features (SYS),
system features plus system-independent features
(ALL:SYS+SI), and system-independent features
only (SI).
Table 1: Recommendation Results
Precision Recall F-Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
SI 82.56?1.46 95.83?0.52 88.70?.65
ALL 83.45?1.33 95.56?1.33 89.09?.24
From Table 1, we observe that MT and TM
system-internal features are very useful for pro-
ducing a stable (as indicated by the smaller con-
fidence interval) recommendation system (SYS).
Interestingly, only using some simple system-
external features as described in Section 4.3.3 can
also yield a system with reasonably good per-
formance (SI). We expect that the performance
can be further boosted by adding more syntactic
and semantic features. Combining all the system-
internal and -external features leads to limited
gains in Precision and F-score compared to using
only system-internal features (SYS) only. This in-
dicates that at the default confidence level, current
system-external (resp. system-internal) features
can only play a limited role in informing the sys-
tem when current system-internal (resp. system-
external) features are available. We show in Sec-
tion 5.4.2 that combing both system-internal and -
external features can yield higher, more stable pre-
cision when adjusting the confidence levels of the
classifier. Additionally, the performance of system
SI is promising given the fact that we are using
only a limited number of simple features, which
demonstrates a good prospect of applying our rec-
ommendation system to MT systems where we do
not have access to their internal features.
5.4 Further Improving Recommendation
Precision
Table 1 shows that classification recall is very
high, which suggests that precision can still be im-
proved, even though the F-score is not low. Con-
sidering that TM is the dominant technology used
by post-editors, a recommendation to replace the
hit from the TM would require more confidence,
i.e. higher precision. Ideally our aim is to obtain
a level of 0.9 precision at the cost of some recall,
if necessary. We propose two methods to achieve
this goal.
5.4.1 Classifier Margins
We experiment with different margins on the train-
ing data to tune precision and recall in order to
obtain a desired balance. In the basic case, the
training example would be marked as in (3). If we
label both the training and test sets with this rule,
the accuracy of the prediction will be maximized.
We try to achieve higher precision by enforc-
ing a larger bias towards negative examples in the
training set so that some borderline positive in-
stances would actually be labeled as negative, and
the classifier would have higher precision in the
prediction stage as in (8).
y =
{
+1 if TER(SMT) + b < TER(TM)
?1 if TER(SMT) + b > TER(TM)
(8)
We experiment with b in [0, 0.25] usingMT sys-
tem features and TM features. Results are reported
in Table 2.
Table 2: Classifier margins
Precision Recall
TER+0 83.45?1.33 95.56?1.33
TER+0.05 82.41?1.23 94.41?1.01
TER+0.10 84.53?0.98 88.81?0.89
TER+0.15 85.24?0.91 87.08?2.38
TER+0.20 87.59?0.57 75.86?2.70
TER+0.25 89.29?0.93 66.67?2.53
The highest accuracy and F-value is achieved
by TER + 0, as all other settings are trained
on biased margins. Except for a small drop in
TER+0.05, other configurations all obtain higher
precision than TER+ 0. We note that we can ob-
tain 0.85 precision without a big sacrifice in recall
with b=0.15, but for larger improvements on pre-
cision, recall will drop more rapidly.
When we use b beyond 0.25, the margin be-
comes less reliable, as the number of positive
examples becomes too small. In particular, this
causes the SVM parameters we tune on in the first
fold to become less applicable to the other folds.
This is one limitation of using biased margins to
626
obtain high precision. The method presented in
Section 5.4.2 is less influenced by this limitation.
5.4.2 Adjusting Confidence Levels
An alternative to using a biased margin is to output
a confidence score during prediction and to thresh-
old on the confidence score. It is also possible to
add this method to the SVM model trained with a
biased margin.
We use the SVM confidence estimation tech-
niques in Section 4.2 to obtain the confidence
level of the recommendation, and change the con-
fidence threshold for recommendation when nec-
essary. This also allows us to compare directly
against a simple baseline inspired by TM users. In
a TM environment, some users simply ignore TM
hits below a certain fuzzy match score F (usually
from 0.7 to 0.8). This fuzzy match score reflects
the confidence of recommending the TM hits. To
obtain the confidence of recommending an SMT
output, our baseline (FM) uses fuzzy match costs
hFM ? 1?F (cf. Section 4.3.2) for the TM hits as
the level of confidence. In other words, the higher
the fuzzy match cost of the TM hit is (lower fuzzy
match score), the higher the confidence of recom-
mending the SMT output. We compare this base-
line with the three settings in Section 5.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pre
cis
ion
Confidence
SISysAllFM
Figure 1: Precision Changes with Confidence
Level
Figure 1 shows that the precision curve of FM
is low and flat when the fuzzy match costs are
low (from 0 to 0.6), indicating that it is unwise to
recommend an SMT output when the TM hit has
a low fuzzy match cost (corresponding to higher
fuzzy match score, from 0.4 to 1). We also observe
that the precision of the recommendation receives
a boost when the fuzzy match costs for the TM
hits are above 0.7 (fuzzy match score lower than
0.3), indicating that SMT output should be recom-
mended when the TM hit has a high fuzzy match
cost (low fuzzy match score). With this boost, the
precision of the baseline system can reach 0.85,
demonstrating that a proper thresholding of fuzzy
match scores can be used effectively to discrimi-
nate the recommendation of the TM hit from the
recommendation of the SMT output.
However, using the TM information only does
not always find the easiest-to-edit translation. For
example, an excellent SMT output should be rec-
ommended even if there exists a good TM hit (e.g.
fuzzy match score is 0.7 or more). On the other
hand, a misleading SMT output should not be rec-
ommended if there exists a poor but useful TM
match (e.g. fuzzy match score is 0.2).
Our system is able to tackle these complica-
tions as it incorporates features from the MT and
the TM systems simultaneously. Figure 1 shows
that both the SYS and the ALL setting consistently
outperform FM, indicating that our classification
scheme can better integrate the MT output into the
TM system than this naive baseline.
The SI feature set does not perform well when
the confidence level is set above 0.85 (cf. the de-
scending tail of the SI curve in Figure 1). This
might indicate that this feature set is not reliable
enough to extract the best translations. How-
ever, when the requirement on precision is not that
high, and the MT-internal features are not avail-
able, it would still be desirable to obtain transla-
tion recommendations with these black-box fea-
tures. The difference between SYS and ALL is
generally small, but ALL performs steadily better
in [0.5, 0,8].
Table 3: Recall at Fixed Precision
Recall
SYS @85PREC 88.12?1.32
SYS @90PREC 52.73?2.31
SI @85PREC 87.33?1.53
ALL @85PREC 88.57?1.95
ALL @90PREC 51.92?4.28
5.5 Precision Constraints
In Table 3 we also present the recall scores at 0.85
and 0.9 precision for SYS, SI and ALL models to
demonstrate our system?s performance when there
is a hard constraint on precision. Note that our
system will return the TM entry when there is an
exact match, so the overall precision of the system
627
is above the precision score we set here in a ma-
ture TM environment, as a significant portion of
the material to be translated will have a complete
match in the TM system.
In Table 3 for MODEL@K, the recall scores are
achieved when the prediction precision is better
than K with 0.95 confidence. For each model, pre-
cision at 0.85 can be obtained without a very big
loss on recall. However, if we want to demand
further recommendation precision (more conser-
vative in recommending SMT output), the recall
level will begin to drop more quickly. If we use
only system-independent features (SI), we cannot
achieve as high precision as with other models
even if we sacrifice more recall.
Based on these results, the users of the TM sys-
tem can choose between precision and recall ac-
cording to their own needs. As the threshold does
not involve training of the SMT system or the
SVM classifier, the user is able to determine this
trade-off at runtime.
Table 4: Contribution of Features
Precision Recall F Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
+M1 82.87?1.26 96.23?0.53 89.05?.52
+LM 82.82?1.16 96.20?1.14 89.01?.23
+PS 83.21?1.33 96.61?0.44 89.41?.84
5.6 Contribution of Features
In Section 4.3.3 we suggested three sets of
system-independent features: features based on
the source- and target-side language model (LM),
the IBMModel 1 (M1) and the fuzzy match scores
on pseudo-source (PS). We compare the contribu-
tion of these features in Table 4.
In sum, all the three sets of system-independent
features improve the precision and F-scores of the
MT and TM system features. The improvement
is not significant, but improvement on every set of
system-independent features gives some credit to
the capability of SI features, as does the fact that
SI features perform close to SYS features in Table
1.
6 Analysis of Post-Editing Effort
A natural question on the integration models is
whether the classification reduces the effort of the
translators and post-editors: after reading these
recommendations, will they translate/edit less than
they would otherwise have to? Ideally this ques-
tion would be answered by human post-editors in
a large-scale experimental setting. As we have
not yet conducted a manual post-editing experi-
ment, we conduct two sets of analyses, trying to
show which type of edits will be required for dif-
ferent recommendation confidence levels. We also
present possible methods for human evaluation at
the end of this section.
6.1 Edit Statistics
We provide the statistics of the number of edits
for each sentence with 0.95 confidence intervals,
sorted by TER edit types. Statistics of positive in-
stances in classification (i.e. the instances in which
MT output is recommended over the TM hit) are
given in Table 5.
When an MT output is recommended, its TM
counterpart will require a larger average number
of total edits than the MT output, as we expect. If
we drill down, however, we also observe that many
of the saved edits come from the Substitution cat-
egory, which is the most costly operation from the
post-editing perspective. In this case, the recom-
mended MT output actually saves more effort for
the editors than what is shown by the TER score.
It reflects the fact that TM outputs are not actual
translations, and might need heavier editing.
Table 6 shows the statistics of negative instances
in classification (i.e. the instances in which MT
output is not recommended over the TM hit). In
this case, the MT output requires considerably
more edits than the TM hits in terms of all four
TER edit types, i.e. insertion, substitution, dele-
tion and shift. This reflects the fact that some high
quality TM matches can be very useful as a trans-
lation.
6.2 Edit Statistics on Recommendations of
Higher Confidence
We present the edit statistics of recommendations
with higher confidence in Table 7. Comparing Ta-
bles 5 and 7, we see that if recommended with
higher confidence, the MT output will need sub-
stantially less edits than the TM output: e.g. 3.28
fewer substitutions on average.
From the characteristics of the high confidence
recommendations, we suspect that these mainly
comprise harder to translate (i.e. different from
the SMT training set/TM database) sentences, as
indicated by the slightly increased edit operations
628
Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 0.9849 ? 0.0408 2.2881 ? 0.0672 0.8686 ? 0.0370 1.2500 ? 0.0598
TM 0.7762 ? 0.0408 4.5841 ? 0.1036 3.1567 ? 0.1120 1.2096 ? 0.0554
Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 1.0830 ? 0.1167 2.2885 ? 0.1376 1.0964 ? 0.1137 1.5381 ? 0.1962
TM 0.7554 ? 0.0376 1.5527 ? 0.1584 1.0090 ? 0.1850 0.4731 ? 0.1083
Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85
Insertion Substitution Deletion Shift
MT 1.1665 ? 0.0615 2.7334 ? 0.0969 1.0277 ? 0.0544 1.5549 ? 0.0899
TM 0.8894 ? 0.0594 6.0085 ? 0.1501 4.1770 ? 0.1719 1.6727 ? 0.0846
on the MT side. TM produces much worse edit-
candidates for such sentences, as indicated by
the numbers in Table 7, since TM does not have
the ability to automatically reconstruct an output
through the combination of several segments.
6.3 Plan for Human Evaluation
Evaluation with human post-editors is crucial to
validate and improve translation recommendation.
There are two possible avenues to pursue:
? Test our system on professional post-editors.
By providing them with the TM output, the
MT output and the one recommended to edit,
we can measure the true accuracy of our
recommendation, as well as the post-editing
time we save for the post-editors;
? Apply the presented method on open do-
main data and evaluate it using crowd-
sourcing. It has been shown that crowd-
sourcing tools, such as the Amazon Me-
chanical Turk (Callison-Burch, 2009), can
help developers to obtain good human judge-
ments on MT output quality both cheaply and
quickly. Given that our problem is related to
MT quality estimation in nature, it can poten-
tially benefit from such tools as well.
7 Conclusions and Future Work
In this paper we present a classification model to
integrate SMT into a TM system, in order to facili-
tate the work of post-editors. Insodoing we handle
the problem of MT quality estimation as binary
prediction instead of regression. From the post-
editors? perspective, they can continue to work in
their familiar TM environment, use the same cost-
estimation methods, and at the same time bene-
fit from the power of state-of-the-art MT. We use
SVMs to make these predictions, and use grid
search to find better RBF kernel parameters.
We explore features from inside the MT sys-
tem, from the TM, as well as features that make
no assumption on the translation model for the bi-
nary classification. With these features we make
glass-box and black-box predictions. Experiments
show that the models can achieve 0.85 precision at
a level of 0.89 recall, and even higher precision if
we sacrifice more recall. With this guarantee on
precision, our method can be used in a TM envi-
ronment without changing the upper-bound of the
related cost estimation.
Finally, we analyze the characteristics of the in-
tegrated outputs. We present results to show that,
if measured by number, type and content of ed-
its in TER, the recommended sentences produced
by the classification model would bring about less
post-editing effort than the TM outputs.
This work can be extended in the following
ways. Most importantly, it is useful to test the
model in user studies, as proposed in Section 6.3.
A user study can serve two purposes: 1) it can
validate the effectiveness of the method by mea-
suring the amount of edit effort it saves; and 2)
the byproduct of the user study ? post-edited sen-
tences ? can be used to generate HTER scores
to train a better recommendation model. Further-
more, we want to experiment and improve on the
adaptability of this method, as the current experi-
ment is on a specific domain and language pair.
629
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank Symantec for providing the TM database and the
anonymous reviewers for their insightful comments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for ma-
chine translation. In The 20th International Conference
on Computational Linguistics (Coling-2004), pages 315 ?
321, Geneva, Switzerland.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263 ? 311.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechani-
cal Turk. In The 2009 Conference on Empirical Methods
in Natural Language Processing (EMNLP-2009), pages
286 ? 295, Singapore.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Corinna Cortes and Vladimir Vapnik. 1995. Support-vector
networks. Machine learning, 20(3):273 ? 297.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In The 1995 International
Conference on Acoustics, Speech, and Signal Processing
(ICASSP-95), pages 181 ? 184, Detroit, MI.
Philipp. Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In The 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy (NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In The 45th Annual Meet-
ing of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Ses-
sions (ACL-2007), pages 177 ? 180, Prague, Czech Re-
public.
Vladimir Iosifovich Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10(8):707 ? 710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vector
machines. Machine Learning, 68(3):267 ? 276.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics (ACL-
2002), pages 295 ? 302, Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In The 41st Annual Meet-
ing on Association for Computational Linguistics (ACL-
2003), pages 160 ? 167.
John C. Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood meth-
ods. Advances in Large Margin Classifiers, pages 61 ? 74.
Christopher B. Quirk. 2004. Training a sentence-level ma-
chine translation confidence measure. In The Fourth In-
ternational Conference on Language Resources and Eval-
uation (LREC-2004), pages 825 ? 828, Lisbon, Portugal.
Richard Sikes. 2007. Fuzzy matching in theory and practice.
Multilingual, 18(6):39 ? 43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation en-
vironment. In The Twelfth Machine Translation Sum-
mit (MT Summit XII), pages 120 ? 127, Ottawa, Ontario,
Canada.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In The 2006
conference of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223 ? 231, Cambridge,
MA.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009a. Estimating the
sentence-level quality of machine translation systems. In
The 13th Annual Conference of the European Association
for Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang,
and John Shawe-Taylor. 2009b. Improving the confidence
of machine translation quality estimates. In The Twelfth
Machine Translation Summit (MT Summit XII), pages 136
? 143, Ottawa, Ontario, Canada.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In The Seventh International Confer-
ence on Spoken Language Processing, volume 2, pages
901 ? 904, Denver, CO.
Nicola Ueffing and Hermann Ney. 2005. Application
of word-level confidence measures in interactive statisti-
cal machine translation. In The Ninth Annual Confer-
ence of the European Association for Machine Translation
(EAMT-2005), pages 262 ? 270, Budapest, Hungary.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation.
In The Ninth Machine Translation Summit (MT Summit
IX), pages 394 ? 401, New Orleans, LA.
630
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239?1248,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Consistent Translation using Discriminative Learning:
A Translation Memory-inspired Approach?
Yanjun Ma? Yifan He? Andy Way? Josef van Genabith?
? Baidu Inc., Beijing, China
yma@baidu.com
?Centre for Next Generation Localisation
School of Computing, Dublin City University
{yhe,away,josef}@computing.dcu.ie
Abstract
We present a discriminative learning method
to improve the consistency of translations in
phrase-based Statistical Machine Translation
(SMT) systems. Our method is inspired by
Translation Memory (TM) systems which are
widely used by human translators in industrial
settings. We constrain the translation of an in-
put sentence using the most similar ?transla-
tion example? retrieved from the TM. Differ-
ently from previous research which used sim-
ple fuzzy match thresholds, these constraints
are imposed using discriminative learning to
optimise the translation performance. We ob-
serve that using this method can benefit the
SMT system by not only producing consis-
tent translations, but also improved translation
outputs. We report a 0.9 point improvement
in terms of BLEU score on English?Chinese
technical documents.
1 Introduction
Translation consistency is an important factor
for large-scale translation, especially for domain-
specific translations in an industrial environment.
For example, in the translation of technical docu-
ments, lexical as well as structural consistency is es-
sential to produce a fluent target-language sentence.
Moreover, even in the case of translation errors, con-
sistency in the errors (e.g. repetitive error patterns)
are easier to diagnose and subsequently correct by
translators.
?This work was done while the first author was in the Cen-
tre for Next Generation Localisation at Dublin City University.
In phrase-based SMT, translation models and lan-
guage models are automatically learned and/or gen-
eralised from the training data, and a translation is
produced by maximising a weighted combination of
these models. Given that global contextual informa-
tion is not normally incorporated, and that training
data is usually noisy in nature, there is no guaran-
tee that an SMT system can produce translations in
a consistent manner.
On the other hand, TM systems ? widely used by
translators in industrial environments for enterprise
localisation by translators ? can shed some light on
mitigating this limitation. TM systems can assist
translators by retrieving and displaying previously
translated similar ?example? sentences (displayed as
source-target pairs, widely called ?fuzzy matches? in
the localisation industry (Sikes, 2007)). In TM sys-
tems, fuzzy matches are retrieved by calculating the
similarity or the so-called ?fuzzy match score? (rang-
ing from 0 to 1 with 0 indicating no matches and 1
indicating a full match) between the input sentence
and sentences in the source side of the translation
memory.
When presented with fuzzy matches, translators
can then avail of useful chunks in previous transla-
tions while composing the translation of a new sen-
tence. Most translators only consider a few sen-
tences that are most similar to the current input sen-
tence; this process can inherently improve the con-
sistency of translation, given that the new transla-
tions produced by translators are likely to be similar
to the target side of the fuzzy match they have con-
sulted.
Previous research as discussed in detail in Sec-
1239
tion 2 has focused on using fuzzy match score as
a threshold when using the target side of the fuzzy
matches to constrain the translation of the input
sentence. In our approach, we use a more fine-
grained discriminative learning method to determine
whether the target side of the fuzzy matches should
be used as a constraint in translating the input sen-
tence. We demonstrate that our method can consis-
tently improve translation quality.
The rest of the paper is organized as follows:
we begin by briefly introducing related research in
Section 2. We present our discriminative learning
method for consistent translation in Section 3 and
our feature design in Section 4. We report the exper-
imental results in Section 5 and conclude the paper
and point out avenues for future research in Section
6.
2 Related Research
Despite the fact that TM and MT integration has
long existed as a major challenge in the localisation
industry, it has only recently received attention in
main-stream MT research. One can loosely combine
TM and MT at sentence (called segments in TMs)
level by choosing one of them (or both) to recom-
mend to the translators using automatic classifiers
(He et al, 2010), or simply using fuzzy match score
or MT confidence measures (Specia et al, 2009).
One can also tightly integrate TM with MT at the
sub-sentence level. The basic idea is as follows:
given a source sentence to translate, we firstly use
a TM system to retrieve the most similar ?example?
source sentences together with their translations. If
matched chunks between input sentence and fuzzy
matches can be detected, we can directly re-use the
corresponding parts of the translation in the fuzzy
matches, and use an MT system to translate the re-
maining chunks.
As a matter of fact, implementing this idea is
pretty straightforward: a TM system can easily de-
tect the word alignment between the input sentence
and the source side of the fuzzy match by retracing
the paths used in calculating the fuzzy match score.
To obtain the translation for the matched chunks, we
just require the word alignment between source and
target TM matches, which can be addressed using
state-of-the-art word alignment techniques. More
importantly, albeit not explicitly spelled out in pre-
vious work, this method can potentially increase the
consistency of translation, as the translation of new
input sentences is closely informed and guided (or
constrained) by previously translated sentences.
There are several different ways of using the
translation information derived from fuzzy matches,
with the following two being the most widely
adopted: 1) to add these translations into a phrase
table as in (Bic?ici and Dymetman, 2008; Simard and
Isabelle, 2009), or 2) to mark up the input sentence
using the relevant chunk translations in the fuzzy
match, and to use an MT system to translate the parts
that are not marked up, as in (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and van Gen-
abith, 2010). It is worth mentioning that translation
consistency was not explicitly regarded as their pri-
mary motivation in this previous work. Our research
follows the direction of the second strand given that
consistency can no longer be guaranteed by con-
structing another phrase table.
However, to categorically reuse the translations
of matched chunks without any differentiation could
generate inferior translations given the fact that the
context of these matched chunks in the input sen-
tence could be completely different from the source
side of the fuzzy match. To address this problem,
both (Koehn and Senellart, 2010) and (Zhechev and
van Genabith, 2010) used fuzzy match score as a
threshold to determine whether to reuse the transla-
tions of the matched chunks. For example, (Koehn
and Senellart, 2010) showed that reusing these trans-
lations as large rules in a hierarchical system (Chi-
ang, 2005) can be beneficial when the fuzzy match
score is above 70%, while (Zhechev and van Gen-
abith, 2010) reported that it is only beneficial to a
phrase-based system when the fuzzy match score is
above 90%.
Despite being an informative measure, using
fuzzy match score as a threshold has a number of
limitations. Given the fact that fuzzy match score
is normally calculated based on Edit Distance (Lev-
enshtein, 1966), a low score does not necessarily
imply that the fuzzy match is harmful when used
to constrain an input sentence. For example, in
longer sentences where fuzzy match scores tend to
be low, some chunks and the corresponding trans-
lations within the sentences can still be useful. On
1240
the other hand, a high score cannot fully guarantee
the usefulness of a particular translation. We address
this problem using discriminative learning.
3 Constrained Translation with
Discriminative Learning
3.1 Formulation of the Problem
Given a sentence e to translate, we retrieve the most
similar sentence e? from the translation memory as-
sociated with target translation f ?. The m com-
mon ?phrases? e?m1 between e and e? can be iden-
tified. Given the word alignment information be-
tween e? and f ?, one can easily obtain the corre-
sponding translations f? ?m1 for each of the phrases in
e?m1 . This process can derive a number of ?phrase
pairs? < e?m, f? ?m >, which can be used to specify
the translations of the matched phrases in the input
sentence. The remaining words without specified
translations will be translated by an MT system.
For example, given an input sentence e1e2 ? ? ?
eiei+1 ? ? ? eI , and a phrase pair < e?, f? ? >, e? =
eiei+1, f? ? = f ?jf
?
j+1 derived from the fuzzy match,
we can mark up the input sentence as:
e1e2 ? ? ? <tm=?f ?jf ?j+1?> eiei+1 < /tm> ? ? ? eI .
Our method to constrain the translations using
TM fuzzy matches is similar to (Koehn and Senel-
lart, 2010), except that the word alignment between
e? and f ? is the intersection of bidirectional GIZA++
(Och and Ney, 2003) posterior alignments. We use
the intersected word alignment to minimise the noise
introduced by word alignment of only one direction
in marking up the input sentence.
3.2 Discriminative Learning
Whether the translation information from the fuzzy
matches should be used or not (i.e. whether the input
sentence should be marked up) is determined using
a discriminative learning procedure. The translation
information refers to the ?phrase pairs? derived us-
ing the method described in Section 3.1. We cast
this problem as a binary classification problem.
3.2.1 Support Vector Machines
SVMs (Cortes and Vapnik, 1995) are binary classi-
fiers that classify an input instance based on decision
rules which minimise the regularised error function
in (1):
min
w,b,?
1
2
wT w + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training in-
stances that are mapped by the function ? to a higher
dimensional space. w is the weight vector, ? is the
relaxation variable and C > 0 is the penalty param-
eter.
Solving SVMs is viable using a kernel function
K in (1) with K(xi, xj) = ?(xi)T?(xj). We per-
form our experiments with the Radial Basis Func-
tion (RBF) kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we have
two free parameters to tune on: the cost parameter
C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimised by a brute-force grid
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
The SVM classifier will thus be able to predict
the usefulness of the TM fuzzy match, and deter-
mine whether the input sentence should be marked
up using relevant phrase pairs derived from the fuzzy
match before sending it to the SMT system for trans-
lation. The classifier uses features such as the fuzzy
match score, the phrase and lexical translation prob-
abilities of these relevant phrase pairs, and addi-
tional syntactic dependency features. Ideally the
classifier will decide to mark up the input sentence
if the translations of the marked phrases are accurate
when taken contextual information into account. As
large-scale manually annotated data is not available
for this task, we use automatic TER scores (Snover
et al, 2006) as the measure for training data annota-
tion.
We label the training examples as in (3):
y =
{
+1 if TER(w. markup) < TER(w/o markup)
?1 if TER(w/o markup) ? TER(w. markup)
(3)
Each instance is associated with a set of features
which are discussed in more detail in Section 4.
1241
3.2.2 Classification Confidence Estimation
We use the techniques proposed by (Platt, 1999) and
improved by (Lin et al, 2007) to convert classifica-
tion margin to posterior probability, so that we can
easily threshold our classifier (cf. Section 5.4.2).
Platt?s method estimates the posterior probability
with a sigmoid function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B)
(4)
where f = f(x) is the decision function of the esti-
mated SVM. A and B are parameters that minimise
the cross-entropy error function F on the training
data, as in (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1 ? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{
N++1
N++2 if yi = +1
1
N?+2 if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label yi.
These numbers are obtained using an internal cross-
validation on the training set.
4 Feature Set
The features used to train the discriminative classi-
fier, all on the sentence level, are described in the
following sections.
4.1 The TM Feature
The TM feature is the fuzzy match score, which in-
dicates the overall similarity between the input sen-
tence and the source side of the TM output. If the
input sentence is similar to the source side of the
matching segment, it is more likely that the match-
ing segment can be used to mark up the input sen-
tence.
The calculation of the fuzzy match score itself is
one of the core technologies in TM systems, and
varies among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Leven-
shtein, 1966) between the source and TM entry, nor-
malised by the length of the source as in (6), as
most of the current implementations are based on
edit distance while allowing some additional flexi-
ble matching.
hfm(e) = min
s
EditDistance(e, s)
Len(e)
(6)
where e is the sentence to translate, and s is the
source side of an entry in the TM. For fuzzy match
scores F , hfm roughly corresponds to 1? F .
4.2 Translation Features
We use four features related to translation probabil-
ities, i.e. the phrase translation and lexical probabil-
ities for the phrase pairs < e?m, f? ?m > derived us-
ing the method in Section 3.1. Specifically, we use
the phrase translation probabilities p(f? ?m|e?m) and
p(e?m|f? ?m), as well as the lexical translation prob-
abilities plex(f? ?m|e?m) and plex(e?m|f? ?m) as calcu-
lated in (Koehn et al, 2003). In cases where mul-
tiple phrase pairs are used to mark up one single
input sentence e, we use a unified score for each
of the four features, which is an average over the
corresponding feature in each phrase pair. The intu-
ition behind these features is as follows: phrase pairs
< e?m, f? ?m > derived from the fuzzy match should
also be reliable with respect to statistically produced
models.
We also have a count feature, i.e. the number of
phrases used to mark up the input sentence, and a
binary feature, i.e. whether the phrase table contains
at least one phrase pair < e?m, f? ?m > that is used to
mark up the input sentence.
4.3 Dependency Features
Given the phrase pairs < e?m, f? ?m > derived from
the fuzzy match, and used to translate the corre-
sponding chunks of the input sentence (cf. Sec-
tion 3.1), these translations are more likely to be co-
herent in the context of the particular input sentence
if the matched parts on the input side are syntacti-
cally and semantically related.
For matched phrases e?m between the input sen-
tence and the source side of the fuzzy match, we de-
fine the contextual information of the input side us-
ing dependency relations between words em in e?m
and the remaining words ej in the input sentence e.
We use the Stanford parser to obtain the depen-
dency structure of the input sentence. We add
a pseudo-label SYS PUNCT to punctuation marks,
whose governor and dependent are both the punc-
tuation mark. The dependency features designed to
capture the context of the matched input phrases e?m
are as follows:
1242
Coverage features measure the coverage of de-
pendency labels on the input sentence in order to
obtain a bigger picture of the matched parts in the
input. For each dependency label L, we consider its
head or modifier as covered if the corresponding in-
put word em is covered by a matched phrase e?m.
Our coverage features are the frequencies of gov-
ernor and dependent coverage calculated separately
for each dependency label.
Position features identify whether the head and
the tail of a sentence are matched, as these are the
cases in which the matched translation is not af-
fected by the preceding words (when it is the head)
or following words (when it is the tail), and is there-
fore more reliable. The feature is set to 1 if this hap-
pens, and to 0 otherwise. We distinguish among the
possible dependency labels, the head or the tail of
the sentence, and whether the aligned word is the
governor or the dependent. As a result, each per-
mutation of these possibilities constitutes a distinct
binary feature.
The consistency feature is a single feature which
determines whether matched phrases e?m belong to
a consistent dependency structure, instead of being
distributed discontinuously around in the input sen-
tence. We assume that a consistent structure is less
influenced by its surrounding context. We set this
feature to 1 if every word in e?m is dependent on an-
other word in e?m, and to 0 otherwise.
5 Experiments
5.1 Experimental Setup
Our data set is an English?Chinese translation mem-
ory with technical translation from Symantec, con-
sisting of 87K sentence pairs. The average sentence
length of the English training set is 13.3 words and
the size of the training set is comparable to the larger
TMs used in the industry. Detailed corpus statistics
about the training, development and test sets for the
SMT system are shown in Table 1.
The composition of test subsets based on fuzzy
match scores is shown in Table 2. We can see that
sentences in the test sets are longer than those in the
training data, implying a relatively difficult trans-
lation task. We train the SVM classifier using the
libSVM (Chang and Lin, 2001) toolkit. The SVM-
Train Develop Test
SENTENCES 86,602 762 943
ENG. TOKENS 1,148,126 13,955 20,786
ENG. VOC. 13,074 3,212 3,115
CHI. TOKENS 1,171,322 10,791 16,375
CHI. VOC. 12,823 3,212 1,431
Table 1: Corpus Statistics
Scores Sentences Words W/S
(0.9, 1.0) 80 1526 19.0750
(0.8, 0.9] 96 1430 14.8958
(0.7, 0.8] 110 1596 14.5091
(0.6, 0.7] 74 1031 13.9324
(0.5, 0.6] 104 1811 17.4135
(0, 0.5] 479 8972 18.7307
Table 2: Composition of test subsets based on fuzzy
match scores
training and validation is on the same training sen-
tences1 as the SMT system with 5-fold cross valida-
tion.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 5-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameter C in (1), we search in the range
[2?5, 215], while for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the exponent.
We conducted experiments using a standard log-
linear PB-SMT model: GIZA++ implementation of
IBM word alignment model 4 (Och and Ney, 2003),
the refinement and phrase-extraction heuristics de-
scribed in (Koehn et al, 2003), minimum-error-
rate training (Och, 2003), a 5-gram language model
with Kneser-Ney smoothing (Kneser and Ney, 1995)
trained with SRILM (Stolcke, 2002) on the Chinese
side of the training data, and Moses (Koehn et al,
2007) which is capable of handling user-specified
translations for some portions of the input during de-
coding. The maximum phrase length is set to 7.
5.2 Evaluation
The performance of the phrase-based SMT system
is measured by BLEU score (Papineni et al, 2002)
and TER (Snover et al, 2006). Significance test-
1We have around 87K sentence pairs in our training data.
However, for 67.5% of the input sentences, our MT system pro-
duces the same translation irrespective of whether the input sen-
tence is marked up or not.
1243
ing is carried out using approximate randomisation
(Noreen, 1989) with a 95% confidence level.
We also measure the quality of the classification
by precision and recall. Let A be the set of pre-
dicted markup input sentences, and B be the set
of input sentences where the markup version has a
lower TER score than the plain version. We stan-
dardly define precision P and recall R as in (7):
P =
|A?B|
|A| , R =
|A?B|
|B| (7)
5.3 Cross-fold translation
In order to obtain training samples for the classifier,
we need to label each sentence in the SMT training
data as to whether marking up the sentence can pro-
duce better translations. To achieve this, we translate
both the marked-up versions and plain versions of
the sentence and compare the two translations using
the sentence-level evaluation metric TER.
We do not make use of additional training data to
translate the sentences for SMT training, but instead
use cross-fold translation. We create a new training
corpus T by keeping 95% of the sentences in the
original training corpus, and creating a new test cor-
pus H by using the remaining 5% of the sentences.
Using this scheme we make 20 different pairs of cor-
pora (Ti,Hi) in such a way that each sentence from
the original training corpus is in exactly one Hi for
some 1 ? i ? 20. We train 20 different systems
using each Ti, and use each system to translate the
corresponding Hi as well as the marked-up version
of Hi using the procedure described in Section 3.1.
The development set is kept the same for all systems.
5.4 Experimental Results
5.4.1 Translation Results
Table 3 contains the translation results of the SMT
system when we use discriminative learning to mark
up the input sentence (MARKUP-DL). The first row
(BASELINE) is the result of translating plain test
sets without any markup, while the second row is
the result when all the test sentences are marked
up. We also report the oracle scores, i.e. the up-
perbound of using our discriminative learning ap-
proach. As we can see from this table, we obtain sig-
nificantly inferior results compared to the the Base-
line system if we categorically mark up all the in-
TER BLEU
BASELINE 39.82 45.80
MARKUP 41.62 44.41
MARKUP-DL 39.61 46.46
ORACLE 37.27 48.32
Table 3: Performance of Discriminative Learning (%)
put sentences using phrase pairs derived from fuzzy
matches. This is reflected by an absolute 1.4 point
drop in BLEU score and a 1.8 point increase in TER.
On the other hand, both the oracle BLEU and TER
scores represent as much as a 2.5 point improve-
ment over the baseline. Our discriminative learning
method (MARKUP-DL), which automatically clas-
sifies whether an input sentence should be marked
up, leads to an increase of 0.7 absolute BLEU points
over the BASELINE, which is statistically signifi-
cant. We also observe a slight decrease in TER com-
pared to the BASELINE. Despite there being much
room for further improvement when compared to the
Oracle score, the discriminative learning method ap-
pears to be effective not only in maintaining transla-
tion consistency, but also a statistically significant
improvement in translation quality.
5.4.2 Classification Confidence Thresholding
To further analyse our discriminative learning ap-
proach, we report the classification results on the test
set using the SVM classifier. We also investigate the
use of classification confidence, as described in Sec-
tion 3.2.2, as a threshold to boost classification pre-
cision if required. Table 4 shows the classification
and translation results when we use different con-
fidence thresholds. The default classification con-
fidence is 0.50, and the corresponding translation
results were described in Section 5.4.1. We inves-
tigate the impact of increasing classification confi-
dence on the performance of the classifier and the
translation results. As can be seen from Table 4,
increasing the classification confidence up to 0.70
leads to a steady increase in classification precision
with a corresponding sacrifice in recall. The fluc-
tuation in classification performance has an impact
on the translation results as measured by BLEU and
TER. We can see that the best BLEU as well as TER
scores are achieved when we set the classification
confidence to 0.60, representing a modest improve-
1244
Classification Confidence
0.50 0.55 0.60 0.65 0.70 0.75 0.80
BLEU 46.46 46.65 46.69 46.59 46.34 46.06 46.00
TER 39.61 39.46 39.32 39.36 39.52 39.71 39.71
P 60.00 68.67 70.31 74.47 72.97 64.28 88.89
R 32.14 29.08 22.96 17.86 13.78 9.18 4.08
Table 4: The impact of classification confidence thresholding
ment over the default setting (0.50). Despite the
higher precision when the confidence is set to 0.7,
the dramatic decrease in recall cannot be compen-
sated for by the increase in precision.
We can also observe from Table 4 that the recall
is quite low across the board, and the classification
results become unstable when we further increase
the level of confidence to above 0.70. This indicates
the degree of difficulty of this classification task, and
suggests some directions for future research as dis-
cussed at the end of this paper.
5.4.3 Comparison with Previous Work
As discussed in Section 2, both (Koehn and Senel-
lart, 2010) and (Zhechev and van Genabith, 2010)
used fuzzy match score to determine whether the in-
put sentences should be marked up. The input sen-
tences are only marked up when the fuzzy match
score is above a certain threshold. We present the
results using this method in Table 5. From this ta-
Fuzzy Match Scores
0.50 0.60 0.70 0.80 0.90
BLEU 45.13 45.55 45.58 45.84 45.82
TER 40.99 40.62 40.56 40.29 40.07
Table 5: Performance using fuzzy match score for classi-
fication
ble, we can see an inferior performance compared to
the BASELINE results (cf. Table 3) when the fuzzy
match score is below 0.70. A modest gain can only
be achieved when the fuzzy match score is above
0.8. This is slightly different from the conclusions
drawn in (Koehn and Senellart, 2010), where gains
are observed when the fuzzy match score is above
0.7, and in (Zhechev and van Genabith, 2010) where
gains are only observed when the score is above 0.9.
Comparing Table 5 with Table 4, we can see that
our classification method is more effective. This
confirms our argument in the last paragraph of Sec-
tion 2, namely that fuzzy match score is not informa-
tive enough to determine the usefulness of the sub-
sentences in a fuzzy match, and that a more compre-
hensive set of features, as we have explored in this
paper, is essential for the discriminative learning-
based method to work.
FM Scores w. markup w/o markup
[0,0.5] 37.75 62.24
(0.5,0.6] 40.64 59.36
(0.6,0.7] 40.94 59.06
(0.7,0.8] 46.67 53.33
(0.8,0.9] 54.28 45.72
(0.9,1.0] 44.14 55.86
Table 6: Percentage of training sentences with markup
vs without markup grouped by fuzzy match (FM) score
ranges
To further validate our assumption, we analyse
the training sentences by grouping them accord-
ing to their fuzzy match score ranges. For each
group of sentences, we calculate the percentage of
sentences where markup (and respectively without
markup) can produce better translations. The statis-
tics are shown in Table 6. We can see that for sen-
tences with fuzzy match scores lower than 0.8, more
sentences can be better translated without markup.
For sentences where fuzzy match scores are within
the range (0.8, 0.9], more sentences can be better
translated with markup. However, within the range
(0.9, 1.0], surprisingly, actually more sentences re-
ceive better translation without markup. This indi-
cates that fuzzy match score is not a good measure to
predict whether fuzzy matches are beneficial when
used to constrain the translation of an input sentence.
5.5 Contribution of Features
We also investigated the contribution of our differ-
ent feature sets. We are especially interested in
the contribution of dependency features, as they re-
1245
Example 1
w/o markup after policy name , type the name of the policy ( it shows new host integrity
policy by default ) .
Translation ????????????????? (????? ???????
??????
w. markup after policy name <tm translation=????????????? ??
?? ?????????>, type the name of the policy ( it shows new host
integrity policy by default ) .< /tm>
Translation ????????????????????? ???? ????????
Reference ????????????????????? ???? ????????
Example 2
w/o markup changes apply only to the specific scan that you select .
Translation ??????????????
w. markup changes apply only to the specific scan that you select <tm translation=???>.< /tm>
Translation ???????????????
Reference ???????????????
flect whether translation consistency can be captured
using syntactic knowledge. The classification and
TER BLEU P R
TM+TRANS 40.57 45.51 52.48 27.04
+DEP 39.61 46.46 60.00 32.14
Table 7: Contribution of Features (%)
translation results using different features are re-
ported in Table 7. We observe a significant improve-
ment in both classification precision and recall by
adding dependency (DEP) features on top of TM
and translation features. As a result, the translation
quality also significantly improves. This indicates
that dependency features which can capture struc-
tural and semantic similarities are effective in gaug-
ing the usefulness of the phrase pairs derived from
the fuzzy matches. Note also that without including
the dependency features, our discriminative learning
method cannot outperform the BASELINE (cf. Ta-
ble 3) in terms of translation quality.
5.6 Improved Translations
In order to pinpoint the sources of improvements by
marking up the input sentence, we performed some
manual analysis of the output. We observe that the
improvements can broadly be attributed to two rea-
sons: 1) the use of long phrase pairs which are miss-
ing in the phrase table, and 2) deterministically using
highly reliable phrase pairs.
Phrase-based SMT systems normally impose a
limit on the length of phrase pairs for storage and
speed considerations. Our method can overcome
this limitation by retrieving and reusing long phrase
pairs on the fly. A similar idea, albeit from a dif-
ferent perspective, was explored by (Lopez, 2008),
where he proposed to construct a phrase table on the
fly for each sentence to be translated. Differently
from his approach, our method directly translates
part of the input sentence using fuzzy matches re-
trieved on the fly, with the rest of the sentence trans-
lated by the pre-trained MT system. We offer some
more insights into the advantages of our method by
means of a few examples.
Example 1 shows translation improvements by
using long phrase pairs. Compared to the refer-
ence translation, we can see that for the underlined
phrase, the translation without markup contains (i)
word ordering errors and (ii) a missing right quota-
tion mark. In Example 2, by specifying the transla-
tion of the final punctuation mark, the system cor-
rectly translates the relative clause ?that you select?.
The translation of this relative clause is missing
when translating the input without markup. This
improvement can be partly attributed to the reduc-
tion in search errors by specifying the highly reliable
translations for phrases in an input sentence.
6 Conclusions and Future Work
In this paper, we introduced a discriminative learn-
ing method to tightly integrate fuzzy matches re-
trieved using translation memory technologies with
phrase-based SMT systems to improve translation
consistency. We used an SVM classifier to predict
whether phrase pairs derived from fuzzy matches
could be used to constrain the translation of an in-
1246
put sentence. A number of feature functions includ-
ing a series of novel dependency features were used
to train the classifier. Experiments demonstrated
that discriminative learning is effective in improving
translation quality and is more informative than the
fuzzy match score used in previous research. We re-
port a statistically significant 0.9 absolute improve-
ment in BLEU score using a procedure to promote
translation consistency.
As mentioned in Section 2, the potential improve-
ment in sentence-level translation consistency us-
ing our method can be attributed to the fact that
the translation of new input sentences is closely in-
formed and guided (or constrained) by previously
translated sentences using global features such as
dependencies. However, it is worth noting that
the level of gains in translation consistency is also
dependent on the nature of the TM itself; a self-
contained coherent TM would facilitate consistent
translations. In the future, we plan to investigate
the impact of TM quality on translation consistency
when using our approach. Furthermore, we will ex-
plore methods to promote translation consistency at
document level.
Moreover, we also plan to experiment with
phrase-by-phrase classification instead of sentence-
by-sentence classification presented in this paper,
in order to obtain more stable classification results.
We also plan to label the training examples using
other sentence-level evaluation metrics such as Me-
teor (Banerjee and Lavie, 2005), and to incorporate
features that can measure syntactic similarities in
training the classifier, in the spirit of (Owczarzak et
al., 2007). Currently, only a standard phrase-based
SMT system is used, so we plan to test our method
on a hierarchical system (Chiang, 2005) to facilitate
direct comparison with (Koehn and Senellart, 2010).
We will also carry out experiments on other data sets
and for more language pairs.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grant No 07/CE/I1142) and part funded under
FP7 of the EC within the EuroMatrix+ project (grant
No 231720). The authors would like to thank the
reviewers for their insightful comments and sugges-
tions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, MI.
Ergun Bic?ici and Marc Dymetman. 2008. Dynamic
translation memory: Using statistical machine trans-
lation to improve translation memory. In Proceedings
of the 9th Internation Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 454?465, Haifa, Israel.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
David Chiang. 2005. A hierarchical Phrase-Based model
for Statistical Machine Translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, MI.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181?184, Detroit, MI.
Philipp Koehn and Jean Senellart. 2010. Convergence of
translation memory and statistical machine translation.
In Proceedings of AMTA Workshop on MT Research
and the Translation Industry, pages 21?31, Denver,
CO.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Conference
and the North American Chapter of the Association
for Computational Linguistics, pages 48?54, Edmon-
ton, AB, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
1247
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vec-
tor machines. Machine Learning, 68(3):267?276.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 104?111,
Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classifiers,
pages 61?74.
Richard Sikes. 2007. Fuzzy matching in theory and prac-
tice. Multilingual, 18(6):39?43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation
environment. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 120 ?
127, Ottawa, Ontario, Canada.
James Smith and Stephen Clark. 2009. EBMT for SMT:
A new EBMT-SMT hybrid. In Proceedings of the 3rd
International Workshop on Example-Based Machine
Translation, pages 3?10, Dublin, Ireland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas (AMTA-2006), pages 223?231, Cam-
bridge, MA, USA.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of machine translation quality estimates.
In Proceedings of the Twelfth Machine Translation
Summit (MT Summit XII), pages 136 ? 143, Ottawa,
Ontario, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Ventsislav Zhechev and Josef van Genabith. 2010.
Seeding statistical machine translation with translation
memory output through tree-based structural align-
ment. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 43?
51, Beijing, China.
1248
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101?109,
COLING 2010, Beijing, August 2010.
HMM Word-to-Phrase Alignment with Dependency Constraints
Yanjun Ma Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yma,away}@computing.dcu.ie
Abstract
In this paper, we extend the HMM word-
to-phrase alignment model with syntac-
tic dependency constraints. The syn-
tactic dependencies between multiple
words in one language are introduced
into the model in a bid to produce co-
herent alignments. Our experimental re-
sults on a variety of Chinese?English
data show that our syntactically con-
strained model can lead to as much as
a 3.24% relative improvement in BLEU
score over current HMM word-to-phrase
alignment models on a Phrase-Based
Statistical Machine Translation system
when the training data is small, and
a comparable performance compared to
IBM model 4 on a Hiero-style system
with larger training data. An intrin-
sic alignment quality evaluation shows
that our alignment model with depen-
dency constraints leads to improvements
in both precision (by 1.74% relative) and
recall (by 1.75% relative) over the model
without dependency information.
1 Introduction
Generative word alignment models including
IBM models (Brown et al, 1993) and HMM
word alignment models (Vogel et al, 1996) have
been widely used in various types of Statisti-
cal Machine Translation (SMT) systems. This
widespread use can be attributed to their robust-
ness and high performance particularly on large-
scale translation tasks. However, the quality
of the alignment yielded from these models is
still far from satisfactory even with significant
amounts of training data; this is particularly true
for radically different languages such as Chinese
and English.
The weakness of most generative models of-
ten lies in the incapability of addressing one to
many (1-to-n), many to one (n-to-1) and many
to many (m-to-n) alignments. Some research di-
rectly addresses m-to-n alignment with phrase
alignment models (Marcu and Wong, 2002).
However, these models are unsuccessful largely
due to intractable estimation (DeNero and Klein,
2008). Recent progress in better parameteri-
sation and approximate inference (Blunsom et
al., 2009) can only augment the performance of
these models to a similar level as the baseline
where bidirectional word alignments are com-
bined with heuristics and subsequently used to
induce translation equivalence (e.g. (Koehn et
al., 2003)). The most widely used word align-
ment models, such as IBM models 3 and 4, can
only model 1-to-n alignment; these models are
often called ?asymmetric? models. IBM models
3 and 4 model 1-to-n alignments using the notion
of ?fertility?, which is associated with a ?defi-
ciency? problem despite its high performance in
practice.
On the other hand, the HMM word-to-phrase
alignment model tackles 1-to-n alignment prob-
lems with simultaneous segmentation and align-
ment while maintaining the efficiency of the
models. Therefore, this model sets a good ex-
ample of addressing the tradeoffs between mod-
elling power and modelling complexity. This
model can also be seen as a more generalised
101
case of the HMM word-to-word model (Vogel et
al., 1996; Och and Ney, 2003), since this model
can be reduced to an HMM word-to-word model
by restricting the generated target phrase length
to one. One can further refine existing word
alignment models with syntactic constraints (e.g.
(Cherry and Lin, 2006)). However, most re-
search focuses on the incorporation of syntactic
constraints into discriminative alignment mod-
els. Introducing syntactic information into gen-
erative alignment models is shown to be more
challenging mainly due to the absence of appro-
priate modelling of syntactic constraints and the
?inflexibility? of these generative models.
In this paper, we extend the HMM word-to-
phrase alignment model with syntactic depen-
dencies by presenting a model that can incor-
porate syntactic information while maintaining
the efficiency of the model. This model is based
on the observation that in 1-to-n alignments,
the n words bear some syntactic dependencies.
Leveraging such information in the model can
potentially further aid the model in producing
more fine-grained word alignments. The syn-
tactic constraints are specifically imposed on the
n words involved in 1-to-n alignments, which
is different from the cohesion constraints (Fox,
2002) as explored by Cherry and Lin (2006),
where knowledge of cross-lingual syntactic pro-
jection is used. As a syntactic extension of the
open-source MTTK implementation (Deng and
Byrne, 2006) of the HMM word-to-phrase align-
ment model, its source code will also be released
as open source in the near future.
The remainder of the paper is organised as fol-
lows. Section 2 describes the HMM word-to-
phrase alignment model. In section 3, we present
the details of the incorporation of syntactic de-
pendencies. Section 4 presents the experimental
setup, and section 5 reports the experimental re-
sults. In section 6, we draw our conclusions and
point out some avenues for future work.
2 HMM Word-to-Phrase Alignment
Model
In HMM word-to-phrase alignment, a sentence
e is segmented into a sequence of consecutive
phrases: e = vK1 , where vk represents the kth
phrase in the target sentence. The assumption
that each phrase vk generated as a translation of
one single source word is consecutive is made to
allow efficient parameter estimation. Similarly
to word-to-word alignment models, a variable
aK1 is introduced to indicate the correspondence
between the target phrase index and a source
word index: k ? i = ak indicating a mapping
from a target phrase vk to a source word fak . A
random process ?k is used to specify the num-
ber of words in each target phrase, subject to the
constraints J =
?K
k=1 ?k, implying that the to-
tal number of words in the phrases agrees with
the target sentence length J .
The insertion of target phrases that do not cor-
respond to any source words is also modelled
by allowing a target phrase to be aligned to a
non-existent source word f0 (NULL). Formally,
to indicate whether each target phrase is aligned
to NULL or not, a set of indicator functions
?K1 = {?1, ? ? ? , ?K} is introduced (Deng and
Byrne, 2008): if ?k = 0, then NULL ? vk; if
?k = 1, then fak ? vk.
To summarise, an alignment a in an HMM
word-to-phrase alignment model consists of the
following elements:
a = (K,?K1 , aK1 , ?K1 )
The modelling objective is to define a condi-
tional distribution P (e,a|f) over these align-
ments. Following (Deng and Byrne, 2008),
P (e,a|f) can be decomposed into a phrase count
distribution (1) modelling the segmentation of a
target sentence into phrases (P (K|J, f) ? ?K
with scalar ? to control the length of the hy-
pothesised phrases), a transition distribution (2)
modelling the dependencies between the current
link and the previous links, and a word-to-phrase
translation distribution (3) to model the degree
to which a word and a phrase are translational to
each other.
P (e,a|f) = P (vK1 ,K, aK1 , ?K1 , ?K1 |f)
= P (K|J, f) (1)
P (aK1 , ?K1 , ?K1 |K,J, f) (2)
P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)(3)
102
The word-to-phrase translation distribution
(3) is formalised as in (4):
P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)
=
K?
k=1
pv(vk|?k ? fak , ?k) (4)
Note here that we assume that the translation
of each target phrase is conditionally indepen-
dent of other target phrases given the individual
source words.
If we assume that each word in a target phrase
is translated with a dependence on the previ-
ously translated word in the same phrase given
the source word, we derive the bigram transla-
tion model as follows:
pv(vk|fak , ?k, ?k) = pt1(vk[1]|?k, fak)
?k?
j=2
pt2(vk[j]|vk[j ? 1], ?k, fak)
where vk[1] is the first word in phrase vk, vk[j]
is the jth word in vk, pt1 is an unigram transla-
tion probability and pt2 is a bigram translation
probability. The intuition is that the first word
in vk is firstly translated by fak and the transla-
tion of the remaining words vk[j] in vk from fak
is dependent on the translation of the previous
word vk[j ? 1] from fak . The use of a bigram
translation model can address the coherence of
the words within the phrase vk so that the qual-
ity of phrase segmentation can be improved.
3 Syntactically Constrained HMM
Word-to-Phrase Alignment Models
3.1 Syntactic Dependencies for
Word-to-Phrase Alignment
As a proof-of-concept, we performed depen-
dency parsing on the GALE gold-standard word
alignment corpus using Maltparser (Nivre et al,
2007).1 We find that 82.54% of the consec-
utive English words have syntactic dependen-
cies and 77.46% non-consecutive English words
have syntactic dependencies in 1-to-2 Chinese?
English (ZH?EN) word alignment (one Chi-
nese word aligned to two English words). For
1http://maltparser.org/
English?Chinese (EN?ZH) word alignment, we
observe that 75.62% of the consecutive Chinese
words and 71.15% of the non-consecutive Chi-
nese words have syntactic dependencies. Our
model represents an attempt to encode these lin-
guistic intuitions.
3.2 Component Variables and Distributions
We constrain the word-to-phrase alignment
model with a syntactic coherence model. Given
a target phrase vk consisting of ?k words, we
use the dependency label rk between words vk[1]
and vk[?k] to indicate the level of coherence.
The dependency labels are a closed set obtained
from dependency parsers, e.g. using Maltparser,
we have 20 dependency labels for English and
12 for Chinese in our data. Therefore, we have
an additional variable rK1 associated with the se-
quence of phrases vK1 to indicate the syntactic
coherence of each phrase, defining P (e,a|f) as
below:
P (rK1 , vK1 ,K, aK1 , ?K1 , ?K1 |f) = P (K|J, f)
P (aK1 , ?K1 , ?K1 |K,J, f)P (vK1 |aK1 , ?K1 , ?K1 ,K, J, f)
P (rK1 |aK1 , ?K1 , ?K1 , vK1 ,K, J, f) (5)
The syntactic coherence distribution (5) is
simplified as in (6):
P (rK1 |aK1 , ?K1 , ?K1 , vK1 ,K, J, f)
=
K?
k=1
pr(rk; ?, fak , ?k) (6)
Note that the coherence of each target phrase
is conditionally independent of the coherence of
other target phrases given the source words fak
and the number of words in the current phrase
?k. We name the model in (5) the SSH model.
SSH is an abbreviation of Syntactically con-
strained Segmental HMM, given the fact that
the HMM word-to-phrase alignment model is a
Segmental HMM model (SH) (Ostendorf et al,
1996; Murphy, 2002).
As our syntactic coherence model utilises syn-
tactic dependencies which require the presence
of at least two words in target phrase vk, we
therefore model the cases of ?k = 1 and ?k ? 2
103
separately. We rewrite (6) as follows:
pr(rk; ?, fak , ?k) ={
p?k=1(rk; ?, fak ) if ?k = 1
p?k?2(rk; ?, fak ) if ?k ? 2
where p?k=1 defines the syntactic coherence
when the target phrase only contains one word
(?k = 1) and p?k?2 defines the syntactic co-
herence of a target phrase composed of multiple
words (?k ? 2). We define p?k=1 as follows:
p?k=1(rk; ?, fak ) ? pn(?k = 1; ?, fak )
where the coherence of the target phrase (word)
vk is defined to be proportional to the probability
of target phrase length ?k = 1 given the source
word fak . The intuition behind this model is that
the syntactic coherence is strong iff the probabil-
ity of the source fak fertility ?k = 1 is high.
For p?k?2, which measures the syntactic co-
herence of a target phrase consisting of more
than two words, we use the dependency label rk
between words vk[1] and vk[?k] to indicate the
level of coherence. A distribution over the values
rk ? R = {SBJ,ADJ, ? ? ? } (R is the set of de-
pendency types for a specific language) is main-
tained as a table for each source word associated
with all the possible lengths ? ? {2, ? ? ? ,N})
of the target phrase it can generate, e.g. we set
N = 4 for ZH?EN alignment and N = 2 for
EN?ZH alignment in our experiments.
Given a target phrase vk containing ?k(?k ?
2) words, it is possible that there are no depen-
dencies between the first word vk[1] and the last
word vk[?k]. To account for this fact, we intro-
duce a indicator function ? as in below:
?(vk[1], ?k) =
?
??
??
1 if vk[1] and vk[?k]have
syntactic dependencies
0 otherwise
We can thereafter introduce a distribution p?(?),
where p?(? = 0) = ? (0 ? ? ? 1) and
p?(? = 0) = 1? ? , with ? indicating how likely
it is that the first and final words in a target phrase
do not have any syntactic dependencies. We can
set ? to a small number to favour target phrases
satisfying the syntactic constraints and to a larger
number otherwise. The introduction of this vari-
able enables us to tune the model towards our
different end goals. We can now define p?k?2
as:
p?k?2(rk; ?, fak) = p(rk|?; ?, fak )p?(?)
where we insist that p(rk|?; ?, fak ) = 1 if
? = 0 (the first and last words in the target
phrase do not have syntactic dependencies) to
reflect the fact that in most arbitrary consecu-
tive word sequences the first and last words do
not have syntactic dependencies, and otherwise
p(rk|?; ?, fak ) if ? = 1 (the first and last words
in the target phrase have syntactic dependen-
cies).
3.3 Parameter Estimation
The Forward-Backward Algorithm (Baum,
1972), a version of the EM algorithm (Dempster
et al, 1977), is specifically designed for unsu-
pervised parameter estimation of HMM models.
The Forward statistic ?j(i, ?, ?) in our model
can be calculated recursively over the trellis as
follows:
?j(i, ?, ?) = {
?
i?,??,??
?j??(i?, ??, ??)pa(i|i?, ?; I)}
pn(?; ?, fi)?pt1(ej??+1|?, fi)
j?
j?=j??+2
pt2(ej? |ej??1, ?, fi)pr(rk; ?, fi, ?)
which sums up the probabilities of every path
that could lead to the cell ?j, i, ??. Note that the
syntactic coherence term pr(rk; ?, fi, ?) can ef-
ficiently be added into the Forward procedure.
Similarly, the Backward statistic ?j(i, ?, ?) is
calculated over the trellis as below:
?j(i, ?, ?) =
?
i?,??,??
?j+??(i?, ??, ??)pa(i?|i, h?; I)
pn(??; ??, fi?)?pt1(ej+1|??, fi?)
j+???
j?=j+2
pt2(ej? |ej??1, ??, fi?)pr(rk; ??, fi? , ??)
Note also that the syntactic coherence term
pr(rk; ??, fi? , ??) can be integrated into the Back-
ward procedure efficiently.
104
Posterior probability can be calculated based
on the Forward and Backward probabilities.
3.4 EM Parameter Updates
The Expectation step accumulates fractional
counts using the posterior probabilities for each
parameter during the Forward-Backward passes,
and the Maximisation step normalises the counts
in order to generate updated parameters.
The E-step for the syntactic coherence model
proceeds as follows:
c(r?; f, ??) =
?
(f ,e)?T
?
i,j,?,fi=f
?j(i, ?, ? = 1)
?(?, ??)?(?j(e, ?), r?)
where ?j(i, ?, ?) is the posterior probability that
a target phrase tjj??+1 is aligned to source word
fi, and ?j(e, ?) is the syntactic dependency label
between ej??+1 and ej . The M-step performs
normalisation, as below:
pr(r?; f, ??) =
c(r?; f, ??)?
r c(r; f, ??)
Other component parameters can be estimated
in a similar manner.
4 Experimental Setup
4.1 Data
We built the baseline word alignment and
Phrase-Based SMT (PB-SMT) systems using ex-
isting open-source toolkits for the purposes of
fair comparison. A collection of GALE data
(LDC2006E26) consisting of 103K (2.9 million
English running words) sentence pairs was firstly
used as a proof of concept (?small?), and FBIS
data containing 238K sentence pairs (8 million
English running words) was added to construct a
?medium? scale experiment. To investigate the
intrinsic quality of the alignment, a collection
of parallel sentences (12K sentence pairs) for
which we have manually annotated word align-
ment was added to both ?small? and ?medium?
scale experiments. Multiple-Translation Chinese
Part 1 (MTC1) from LDC was used for Mini-
mum Error-Rate Training (MERT) (Och, 2003),
and MTC2, 3 and 4 were used as development
test sets. Finally the test set from NIST 2006
evaluation campaign was used as the final test
set.
The Chinese data was segmented using the
LDC word segmenter. The maximum-entropy-
based POS tagger MXPOST (Ratnaparkhi, 1996)
was used to tag both English and Chinese texts.
The syntactic dependencies for both English and
Chinese were obtained using the state-of-the-art
Maltparser dependency parser, which achieved
84% and 88% labelled attachment scores for
Chinese and English respectively.
4.2 Word Alignment
The GIZA++ (Och and Ney, 2003) implementa-
tion of IBM Model 4 (Brown et al, 1993) is used
as the baseline for word alignment. Model 4 is
incrementally trained by performing 5 iterations
of Model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4. We
compared our model against the MTTK (Deng
and Byrne, 2006) implementation of the HMM
word-to-phrase alignment model. The model
training includes 10 iterations of Model 1, 5 it-
erations of Model 2, 5 iterations of HMM word-
to-word alignment, 20 iterations (5 iterations re-
spectively for phrase lengths 2, 3 and 4 with un-
igram translation probability, and phrase length
4 with bigram translation probability) of HMM
word-to-phrase alignment for ZH?EN alignment
and 5 iterations (5 iterations for phrase length
2 with uniform translation probability) of HMM
word-to-phrase alignment for EN?ZH. This con-
figuration is empirically established as the best
for Chinese?English word alignment. To allow
for a fair comparison between IBM Model 4
and HMM word-to-phrase alignment models, we
also restrict the maximum fertility in IBM model
4 to 4 for ZH?EN and 2 for EN?ZH (the default
is 9 in GIZA++ for both ZH?EN and EN?ZH).
?grow-diag-final? heuristic described in (Koehn
et al, 2003) is used to derive the refined align-
ment from bidirectional alignments.
4.3 MT system
The baseline in our experiments is a standard
log-linear PB-SMT system. With the word align-
ment obtained using the method described in
105
section 4.2, we perform phrase-extraction using
heuristics described in (Koehn et al, 2003), Min-
imum Error-Rate Training (MERT) (Och, 2003)
optimising the BLEU metric, a 5-gram language
model with Kneser-Ney smoothing (Kneser and
Ney, 1995) trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
MOSES (Koehn et al, 2007) for decoding. A
Hiero-style decoder Joshua (Li et al, 2009) is
also used in our experiments. All significance
tests are performed using approximate randomi-
sation (Noreen, 1989) at p = 0.05.
5 Experimental Results
5.1 Alignment Model Tuning
In order to find the value of ? in the SSH model
that yields the best MT performance, we used
three development test sets using a PB-SMT sys-
tem trained on the small data condition. Figure 1
shows the results on each development test set
using different configurations of the alignment
models. For each system, we obtain the mean
of the BLEU scores (Papineni et al, 2002) on
the three development test sets, and derive the
optimal value for ? of 0.4, which we use here-
after for final testing. It is worth mentioning
that while IBM model 4 (M4) outperforms other
models including the HMM word-to-word (H)
and word-to-phrase (SH) alignment model in our
current setup, using the default IBM model 4 set-
ting (maximum fertility 9) yields an inferior per-
formance (as much as 8.5% relative) compared
to other models.
 0.11
 0.115
 0.12
 0.125
 0.13
 0.135
 0.14
M4 H SH SSH-0.05
SSH-0.1
SSH-0.2
SSH-0.3
SSH-0.4
SSH-0.5
SSH-0.6
BL
EU
 s
co
re
alignment systems
MTC2
MTC3
MTC4
Figure 1: BLEU score on development test set
using PB-SMT system
PB-SMT Hiero
small medium small medium
H 0.1440 0.2591 0.1373 0.2595
SH 0.1418 0.2517 0.1372 0.2609
SSH 0.1464 0.2518 0.1356 0.2624
M4 0.1566 0.2627 0.1486 0.2660
Table 1: Performance of PB-SMT using different
alignment models on NIST06 test set
5.2 Translation Results
Table 1 shows the performance of PB-SMT and
Hiero systems using a small amount of data for
alignment model training on the NIST06 test set.
For the PB-SMT system trained on the small data
set, using SSH word alignment leads to a 3.24%
relative improvement over SH, which is statis-
tically significant. SSH also leads to a slight
gain over the HMM word-to-word alignment
model (H). However, when the PB-SMT system
is trained on larger data sets, there are no sig-
nificant differences between SH and SSH. Addi-
tionally, both SH and SSH models underperform
H on the medium data condition, indicating that
the performance of the alignment model tuned
on the PB-SMT system with small training data
does not carry over to PB-SMT systems with
larger training data (cf. Figure 1). IBM model
4 demonstrates stronger performance over other
models for both small and medium data condi-
tions.
For the Hiero system trained on a small data
set, no significant differences are observed be-
tween SSH, SH and H. On a larger training set,
we observe that SSH alignment leads to better
performance compared to SH. Both SH and SSH
alignments achieved higher translation quality
than H. Note that while IBM model 4 outper-
forms other models on a small data condition, the
difference between IBM model 4 and SSH is not
statistically significant on a medium data condi-
tion. It is also worth pointing out that the SSH
model yields significant improvement over IBM
model 4 with the default fertility setting, indicat-
ing that varying the fertility limit in IBM model
4 has a significant impact on translation quality.
In summary, the SSH model which incorpo-
rates syntactic dependencies into the SH model
achieves consistently better performance than
106
ZH?EN EN?ZH
P R P R
H 0.5306 0.3752 0.5282 0.3014
SH 0.5378 0.3802 0.5523 0.3151
SSH 0.5384 0.3807 0.5619 0.3206
M4 0.5638 0.3986 0.5988 0.3416
Table 2: Intrinsic evaluation of the alignment us-
ing different alignment models
SH in both PB-SMT and Hiero systems under
both small and large data conditions. For a
PB-SMT system trained on the small data set,
the SSH model leads to significant gains over
the baseline SH model. The results also en-
tail an observation concerning the suitability of
different alignment models for different types
of SMT systems; trained on a large data set,
our SSH alignment model is more suitable to
a Hiero-style system than a PB-SMT system,
as evidenced by a lower performance compared
to IBM model 4 using a PB-SMT system, and
a comparable performance compared to IBM
model 4 using a Hiero system.
5.3 Intrinsic Evaluation
In order to further investigate the intrinsic qual-
ity of the word alignment, we compute the Preci-
sion (P), Recall (R) and F-score (F) of the align-
ments obtained using different alignment mod-
els. As the models investigated here are asym-
metric models, we conducted intrinsic evalua-
tion for both alignment directions, i.e. ZH?EN
word alignment where one Chinese word can be
aligned to multiple English words, and EN?ZH
word alignment where one English word can be
aligned to multiple Chinese words.
Table 2 shows the results of the intrinsic eval-
uation of ZH?EN and EN?ZH word alignment
on a small data set (results on the medium data
set follow the same trend but are left out due
to space limitations). Note that the P and R
are all quite low, demonstrating the difficulty of
Chinese?English word alignment in the news do-
main. For the ZH?EN direction, using the SSH
model does not lead to significant gains over SH
in P or R. For the EN?ZH direction, the SSH
model leads to a 1.74% relative improvement in
P, and a 1.75% relative improvement in R over
the SH model. Both SH and SSH lead to gains
over H for both ZH?EN and EN?ZH directions,
while gains in the EN?ZH direction appear to be
more pronounced. IBM model 4 achieves signif-
icantly higher P over other models while the gap
in R is narrow.
Relating Table 2 to Table 1, we observe that
the HMM word-to-word alignment model (H)
can still achieve good MT performance despite
the lower P and R compared to other mod-
els. This provides additional support to previ-
ous findings (Fraser and Marcu, 2007b) that the
intrinsic quality of word alignment does not nec-
essarily correlate with the performance of the re-
sulted MT system.
5.4 Alignment Characteristics
In order to further understand the characteristics
of the alignment that each model produces, we
investigated several statistics of the alignment re-
sults which can hopefully reveal the capabilities
and limitations of each model.
5.4.1 Pairwise Comparison
Given the asymmetric property of these align-
ment models, we can evaluate the quality of the
links for each word and compare the alignment
links across different models. For example, in
ZH?EN word alignment, we can compute the
links for each Chinese word and compare those
links across different models. Additionally, we
can compute the pairwise agreement in align-
ing each Chinese word for any two alignment
models. Similarly, we can compute the pairwise
agreement in aligning each English word in the
EN?ZH alignment direction.
For ZH?EN word alignment, we observe that
the SH and SSH models reach a 85.94% agree-
ment, which is not surprising given the fact that
SSH is a syntactic extension over SH, while IBM
model 4 and SSH reach the smallest agreement
(only 65.09%). We also observe that there is a
higher agreement between SSH and H (76.64%)
than IBM model 4 and H (69.58%). This can be
attributed to the fact that SSH is still a form of
HMM model while IBM model 4 is not. A simi-
lar trend is observed for EN?ZH word alignment.
107
ZH?EN EN?ZH
1-to-0 1-to-1 1-to-n 1-to-0 1-to-1 1-to-n
con. non-con. con. non-con.
HMM 0.3774 0.4693 0.0709 0.0824 0.4438 0.4243 0.0648 0.0671
SH 0.3533 0.4898 0.0843 0.0726 0.4095 0.4597 0.0491 0.0817
SSH 0.3613 0.5092 0.0624 0.0671 0.3990 0.4835 0.0302 0.0872
M4 0.2666 0.5561 0.0985 0.0788 0.3967 0.4850 0.0592 0.0591
Table 3: Alignment types using different alignment models
5.4.2 Alignment Types
Again, by taking advantage of the asymmet-
ric property of these alignment models, we can
compute different types of alignment. For both
ZH?EN (EN?ZH) alignment, we divide the links
for each Chinese (English) word into 1-to-0
where each Chinese (English) word is aligned
to the empty word ?NULL? in English (Chi-
nese), 1-to-1 where each Chinese (English) word
is aligned to only one word in English (Chinese),
and 1-to-n where each Chinese (English) word
is aligned to n (n ? 2) words in English (Chi-
nese). For 1-to-n links, depending on whether
the n words are consecutive, we have consecu-
tive (con.) and non-consecutive (non-con.) 1-to-
n links.
Table 3 shows the alignment types in the
medium data track. We can observe that for
ZH?EN word alignment, both SH and SSH pro-
duce far more 1-to-0 links than Model 4. It can
also be seen that Model 4 tends to produce more
consecutive 1-to-n links than non-consecutive 1-
to-n links. On the other hand, the SSH model
tends to produce more non-consecutive 1-to-n
links than consecutive ones. Compared to SH,
SSH tends to produce more 1-to-1 links than 1-
to-n links, indicating that adding syntactic de-
pendency constraints biases the model towards
only producing 1-to-n links when the n words
follow coherence constraint, i.e. the first and last
word in the chunk have syntactic dependencies.
For example, among the 6.24% consecutive ZH?
EN 1-to-n links produced by SSH, 43.22% of
them follow the coherence constraint compared
to just 39.89% in SH. These properties can have
significant implications for the performance of
our MT systems given that we use the grow-
diag-final heuristics to derive the symmetrised
word alignment based on bidirectional asymmet-
ric word alignments.
6 Conclusions and Future Work
In this paper, we extended the HMM word-to-
phrase word alignment model to handle syntac-
tic dependencies. We found that our model was
consistently better than that without syntactic de-
pendencies according to both intrinsic and ex-
trinsic evaluation. Our model is shown to be ben-
eficial to PB-SMT under a small data condition
and to a Hiero-style system under a larger data
condition.
As to future work, we firstly plan to investi-
gate the impact of parsing quality on our model,
and the use of different heuristics to combine
word alignments. Secondly, the syntactic co-
herence model itself is very simple, in that it
only covers the syntactic dependency between
the first and last word in a phrase. Accordingly,
we intend to extend this model to cover more so-
phisticated syntactic relations within the phrase.
Furthermore, given that we can construct dif-
ferent MT systems using different word align-
ments, multiple system combination can be con-
ducted to avail of the advantages of different sys-
tems. We also plan to compare our model with
other alignment models, e.g. (Fraser and Marcu,
2007a), and test this approach on more data and
on different language pairs and translation direc-
tions.
Acknowledgements
This research is supported by the Science Foundation Ire-
land (Grant 07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin City Uni-
versity. Part of the work was carried out at Cambridge Uni-
versity Engineering Department with Dr. William Byrne.
The authors would also like to thank the anonymous re-
viewers for their insightful comments.
108
References
Baum, Leonard E. 1972. An inequality and associ-
ated maximization technique in statistical estimation for
probabilistic functions of Markov processes. Inequali-
ties, 3:1?8.
Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of ACL-IJCNLP
2009, pages 782?790, Singapore.
Brown, Peter F., Stephen A. Della-Pietra, Vincent J. Della-
Pietra, and Robert L. Mercer. 1993. The mathematics of
Statistical Machine Translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Cherry, Colin and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING-ACL 2006, pages
105?112, Sydney, Australia.
Dempster, Arthur, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
DeNero, John and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-08:
HLT, Short Papers, pages 25?28, Columbus, OH.
Deng, Yonggang and William Byrne. 2006. MTTK: An
alignment toolkit for Statistical Machine Translation. In
Proceedings of HLT-NAACL 2006, pages 265?268, New
York City, NY.
Deng, Yonggang and William Byrne. 2008. HMM word
and phrase alignment for Statistical Machine Transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Fox, Heidi. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the EMNLP 2002, pages
304?3111, Philadelphia, PA, July.
Fraser, Alexander and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Pro-
ceedings of EMNLP-CoNLL 2007, pages 51?60, Prague,
Czech Republic.
Fraser, Alexander and Daniel Marcu. 2007b. Measuring
word alignment quality for Statistical Machine Transla-
tion. Computational Linguistics, 33(3):293?303.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE ICASSP, volume 1, pages 181?
184, Detroit, MI.
Koehn, Philipp, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL 2003, pages 48?54, Edmonton, AB,
Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
Statistical Machine Translation. In Proceedings of ACL
2007, pages 177?180, Prague, Czech Republic.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation. In
Proceedings of the WMT 2009, pages 135?139, Athens,
Greece.
Marcu, Daniel and William Wong. 2002. A Phrase-Based,
joint probability model for Statistical Machine Transla-
tion. In Proceedings of EMNLP 2002, pages 133?139,
Philadelphia, PA.
Murphy, Kevin. 2002. Hidden semi-markov models (seg-
ment models). Technical report, UC Berkeley.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(2):95?135.
Noreen, Eric W. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Och, Franz and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Compu-
tational Linguistics, 29(1):19?51.
Och, Franz. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
Ostendorf, Mari, Vassilios V. Digalakis, and Owen A. Kim-
ball. 1996. From HMMs to segment models: A uni-
fied view of stochastic modeling for speech recognition.
IEEE Transactions on Speech and Audio Processing,
4(5):360?378.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of ACL
2002, pages 311?318, Philadelphia, PA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142, Somerset, NJ.
Stolcke, Andreas. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Vogel, Stefan, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of ACL 1996, pages 836?841,
Copenhagen, Denmark.
109
