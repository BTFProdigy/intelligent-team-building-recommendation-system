Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 407?411,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards the Orwellian Nightmare 
Separation of Business and Personal Emails 
 
Sanaz Jabbari, Ben Allison, David Guthrie, Louise Guthrie 
Department of Computer Science 
University of Sheffield 
211 Portobello St. 
Sheffield 
S1 4DP 
{s.jabbari, b.allison, d.guthrie, l.guthrie}@dcs.shef.ac.uk 
 
Abstract 
This paper describes the largest scale annotation pro-
ject involving the Enron email corpus to date. Over 
12,500 emails were classified, by humans, into the 
categories ?Business? and ?Personal?, and then sub-
categorised by type within these categories. The paper 
quantifies how well humans perform on this task 
(evaluated by inter-annotator agreement). It presents 
the problems experienced with the separation of these 
language types. As a final section, the paper presents 
preliminary results using a machine to perform this 
classification task. 
 
1 Introduction 
Almost since it became a global phenomenon, com-
puters have been examining and reasoning about our 
email. For the most part, this intervention has been 
well natured and helpful ? computers have been try-
ing to protect us from attacks of unscrupulous blanket 
advertising mail shots. However, the use of computers 
for more nefarious surveillance of email has so far 
been limited. The sheer volume of email sent means 
even government agencies (who can legally intercept 
all mail) must either filter email by some pre-
conceived notion of what is interesting, or they must 
employ teams of people to manually sift through the 
volumes of data. For example, the NSA has had mas-
sive parallel machines filtering e-mail traffic for at 
least ten years. 
 
The task of developing such automatic filters at re-
search institutions has been almost impossible, but for 
the opposite reason. There is no shortage of willing 
researchers, but progress has been hampered by the 
lack of any data ? one?s email is often hugely private, 
and the prospect of surrendering it, in its entirety, for 
research purposes is somewhat unsavoury. 
 
Recently, a data resource has become available where 
exactly this condition (several hundred people?s entire 
email archive) has been satisfied ? the Enron dataset. 
During the legal investigation of the collapse of En-
ron, the FERC (Federal Energy Regulatory Commis-
sion) seized the emails of every employee in that 
company. As part of the process, the collection of 
emails was made public and subsequently prepared 
for research use by researchers at Carnegie Melon 
University (Klimt and Yang, 2004).Such a corpus of 
authentic data, on such a large scale, is unique, and an 
invaluable research tool. It then falls to the prospec-
tive researcher to decide which divisions in the lan-
guage of email are interesting, which are possible, and 
how the new resource might best be used.  
 
Businesses which offer employees an email system at 
work (and there are few who do not) have always 
known that they possess an invaluable resource for 
monitoring their employees? work habits. During the 
1990s, UK courts decided that that an employee?s 
email is not private ? in fact, companies can read 
them at will. However, for exactly the reasons de-
scribed above, automatic monitoring has been impos-
sible, and few businesses have ever considered it suf-
ficiently important to employ staff to monitor the 
email use of other staff. However, in monitoring staff 
productivity, few companies would decline the use of 
a system which could analyse the email habits of its 
employees, and report the percentage of time which 
each employee was spending engaged in non-work 
related email activities. 
 
The first step in understanding how this problem 
might be tackled by a computer, and if it is even fea-
sible for this to happen, is to have humans perform the 
task. This paper describes the process of having hu-
mans annotate a corpus of emails, classifying each as 
to whether they are business or personal, and then 
attempting to classify the type of business or personal 
mail being considered. 
 
A resource has been created to develop a system able 
to make these distinctions automatically. Furthermore, 
the process of subcategorising types of business and 
personal has allowed invaluable insights into the areas 
407
where confusion can occur, and how these confusions 
might be overcome. 
 
The paper presents an evolution of appropriate sub-
categories, combined with analysis of performance 
(measured by inter-annotator agreement) and reasons 
for any alterations. It addresses previous work done 
with the Enron dataset, focusing particularly on the 
work of Marti Hearst at Berkeley who attempted a 
smaller-scale annotation project of the Enron corpus, 
albeit with a different focus. It concludes by suggest-
ing that in the main part (with a few exceptions) the 
task is possible for human annotators. The project has 
produced a set of labeled messages (around 14,000, 
plus double annotations for approximately 2,500) with 
arguably sufficiently high business-personal agree-
ment that machine learning algorithms will have suf-
ficient material to attempt the task automatically. 
2 Introduction to the Corpus 
Enron?s email was made public on the Web by FERC 
(Federal Energy Regulatory Commission), during a 
legal investigation on Enron Corporation. The emails 
cover 92 percent of the staff?s emails, because some 
messages have been deleted "as part of a redaction 
effort due to requests from affected employees". The 
dataset was comprised of 619,446 messages from 158 
users in 3,500 folders. However, it turned out that the 
raw data set was suffering from various data integrity 
problems. Various attempts were made to clean and 
prepare the dataset for research purposes. The dataset 
used in this project was the March 2, 2004 version 
prepared at Carnegie Mellon University, acquired 
from http://www.cs.cmu.edu/~enron/. This version of 
the dataset was reduced to 200,399 emails by remov-
ing some folders from each user. Folders like ?discus-
sion threads? and ?all documents?, which were ma-
chine generated and contained duplicate emails, were 
removed in this version.  
 
There were on average 757 emails per each of the 158 
users. However, there are between one and 100,000 
emails per user. There are 30,091 threads present in 
123,091 emails. The dataset does not include attach-
ments. Invalid email addresses were replaced with 
?user@enron.com?.  When no recipient was specified 
the address was replaced with 
?no_address@enron.com? (Klimt and Yang, 2005). 
 
3 Previous Work with the Dataset 
The most relevant piece of work to this paper was 
performed at Berkeley. Marti Hearst ran a small-scale 
annotation project to classify emails in the corpus by 
their type and purpose (Email annotation at Berkely). 
In total, approximately 1,700 messages were anno-
tated by two distinct annotators. Annotation catego-
ries captured four dimensions, but broadly speaking 
they reflected the following qualities of the email: 
coarse genre, the topic of the email if business was 
selected, information about any forwarded or included 
text and the emotional tone of the email. However, the 
categories used at the Berkeley project were incom-
patible with our requirements for several reasons: that 
project allowed multiple labels to be assigned to each 
email; the categories were not designed to facilitate 
discrimination between business and personal emails; 
distinctions between topic, genre, source and purpose 
were present in each of the dimensions; and no effort 
was made to analyse the inter-annotator agreement 
(Email annotation at Berkely). 
 
User-defined folders are preserved in the Enron data, 
and some research efforts have used these folders to 
develop and evaluate machine-learning algorithms for 
automatically sorting emails (Klimt and Yang, 2004). 
However, as users are often inconsistent in organising 
their emails, so the training and testing data in these 
cases are questionable.  For example, many users 
have folders marked ?Personal?, and one might think 
these could be used as a basis for the characterisation 
of personal emails. However, upon closer inspection it 
becomes clear that only a tiny percentage of an indi-
vidual?s personal emails are in these folders. Simi-
larly, many users have folders containing exclusively 
personal content, but without any obvious folder 
name to reveal this. All of these problems dictate that 
for an effective system to be produced, large-scale 
manual annotation will be necessary. 
 
Researchers at Queen?s University, Canada (Keila, 
2005) recently attempted to categorise and identify 
deceptive messages in the Enron corpus. Their 
method used a hypothesis from deception theory (e.g., 
deceptive writing contains cues such as reduced fre-
quency of first-person pronouns and increased fre-
quency of ?negative emotion? words) and as to what 
constitutes deceptive language. Single value decom-
position (SVD) was applied to separate the emails, 
and a manual survey of the results allowed them to 
conclude that this classification method for detecting 
deception in email was promising. 
 
Other researchers have attempted to analyse the Enron 
emails from a network analytic perspective (Deisner, 
2005).  Their goal was to analyse the flow of commu-
nication between employees at times of crisis, and 
develop a characterisation for the state of a communi-
cation network in such difficult times, in order to 
identify looming crises in other companies from the 
state of their communication networks. They com-
pared the network flow of email in October 2000 and 
October 2001.    
 
4 Annotation Categories for this Project 
Because in many cases there is no definite line be-
tween business emails and personal emails, it was 
decided to mark emails with finer categories than 
408
Business and Personal. This subcategorising not only 
helped us to analyse the different types of email 
within business and personal emails, but it helped us 
to find the nature of the disagreements that  occurred 
later on, in inter-annotation.  In other words, this 
process allowed us to observe patterns in disagree-
ment.  
 
Obviously, the process of deciding categories in any 
annotation project is a fraught and contentious one. 
The process necessarily involves repeated cycles of 
category design, annotation, inter-annotation, analysis 
of disagreement, category refinement. While the proc-
ess described above could continue ad infinitum, the 
sensible project manager must identify were this 
process is beginning to converge on a set of well-
defined but nonetheless intuitive categories, and final-
ise them. 
 
Likewise, the annotation project described here went 
through several evolutions of categories, mediated by 
input from annotators and other researchers. The final 
categories chosen were: 
 
Business: Core Business, Routine Admin, Inter-
Employee Relations, Solicited/soliciting mailing, Im-
age. 
 
Personal: Close Personal, Forwarded, Auto generated 
emails. 
 
5 Annotation and Inter-Annotation 
Based on the categories above, approximately 12,500 
emails were single-annotated by a total of four anno-
tators. 
 
The results showed that around 83% of the emails 
were business related, while 17% were personal. The 
company received one personal email for every five 
business emails. 
 
Fig 1: Distribution of Emails in the Corpus
BUSINESS
83%
PERSONAL
17%
BUSINESS
PERSONAL
 
 
A third of the received emails were ?Core Business? 
and a third were ?Routine Admin?. All other catego-
ries comprised the remaining third of the emails. One 
could conclude that approximately one third of emails 
received at Enron were discussions of policy, strategy, 
legislation, regulations, trading, and other high-level 
business matters. The next third of received emails 
were about the peripheral, routine matters of the com-
pany. These are emails related to HR, IT administra-
tion, meeting scheduling, etc. which can be regarded 
as part of the common infrastructure of any large 
scale corporation. 
 
The rest of the emails were distributed among per-
sonal emails, emails to colleagues, company news 
letters, and emails received due to subscription. The 
biggest portion of the last third, are emails received 
due to subscription, whether the subscription be busi-
ness or personal in nature. 
 
In any annotation project consistency should be 
measured. To this end 2,200 emails were double an-
notated between four annotators. As Figure 2 below 
shows, for 82% of the emails both annotators agreed 
that the email was business email and in 12% of the 
emails, both agreed on them being personal. Six per-
cent of the emails were disagreed upon. 
 
Fig 2: Agreements and Disagreements in Inter-Annotation
Disagreement
6%
Personal 
Agreement
12%
Business 
Agreement
82%
Business Agreement
Personal Agreement
Disagreement
 
 
By analysing the disagreed categories, some patterns 
of confusion were found.  
 
Around one fourth of the confusions were solicited 
emails where it was not clear whether the employee 
was subscribed to a particular newsletter group for his 
personal interest, private business, or Enron?s busi-
ness. While some subscriptions were clearly personal 
(e.g. subscription to latest celebrity news) and some 
were clearly business related (e.g. Daily Energy re-
ports), for some it was hard to identify the intention of 
the subscription (e.g. New York Times). 
 
Eighteen percent of the confusions were due to emails 
about travel arrangements, flight and hotel booking 
confirmations, where it was not clear whether the per-
sonal was acting in a business or personal role. 
 
409
Thirteen percent of the disagreements were upon 
whether an email is written between two Enron em-
ployees as business colleagues or friends. The emails 
such as ?shall we meet for a coffee at 2:00?? If insuf-
ficient information exists in the email, it can be hard 
to draw the line between a personal relationship and a 
relationship between colleagues. The annotators were 
advised to pick the category based on the formality of 
the language used in such emails, and reading be-
tween the lines wherever possible. 
 
About eight percent of the disagreements were on 
emails which were about services that Enron provides 
for its employees. For example, the Enron?s running 
club is seeking for runners, and sending an ad to En-
ron?s employers. Or Enron?s employee?s assistance 
Program (EAP), sending an email to all employees, 
letting them know that in case of finding themselves 
in stressful situations they can use some of the ser-
vices that Enron provides for them or their families.  
 
One theme was encountered in many types of confu-
sions: namely, whether to decide an e-mail?s category 
based upon its topic or its form. For example, should 
an email be categorised because it is scheduling a 
meeting or because of the subject of the meeting be-
ing scheduled? One might consider this a distinction 
by topic or by genre. 
 
As the result, final categories were created to reflect 
topic as the only dimension to be considered in the 
annotation. ?Solicited/Soliciting mailing?, ?Solic-
ited/Auto generated mailing? and ?Forwarded? were 
removed and ?Keeping Current?, ?Soliciting? were 
added as business categories and ?Personal Mainte-
nance? and ?Personal Circulation? were added as per-
sonal categories. The inter-annotation agreement was 
measured for one hundred and fifty emails, annotated 
by five annotators. The results confirmed that these 
changes had a positive effect on the accuracy of anno-
tation. 
 
6 Preliminary Results of Automatic 
Classification 
Some preliminary experiments were performed with 
an automatic classifier to determine the feasibility of 
separating business and personal emails by machine. 
The classifier used was a probabilistic classifier based 
upon the distribution of distinguishing words. More 
information can be found in (Guthrie and Walker, 
1994). 
 
Two categories from the annotation were chosen 
which were considered to typify the broad categories 
? these were Core Business (representing business) 
and Close Personal (representing personal). The Core 
Business class contains 4,000 messages (approx 
900,000 words), while Close Personal contains ap-
proximately 1,000 messages (220,000 words). 
 
The following table summarises the performance of 
this classifier in terms of Recall, Precision and F-
Measure and accuracy: 
 
Class Recall Precision F-
Measure 
Accuracy 
Business 0.99 0.92 0.95 0.99 
Personal 0.69 0.95 0.80 0.69 
AVERAGE 0.84 0.94 0.88 0.93 
 
Based upon the results of this experiment, one can 
conclude that automatic methods are also suitable for 
classifying emails as to whether they are business or 
personal. The results indicate that the business cate-
gory is well represented by the classifier, and given 
the disproportionate distribution of emails, the classi-
fier?s tendency towards the business category is un-
derstandable. 
 
Given that our inter-annotator agreement statistic tells 
us that humans only agree on this task 94% of the 
time, preliminary results with 93% accuracy (the sta-
tistic which correlates exactly to agreement) of the 
automatic method are encouraging. While more work 
is necessary to fully evaluate the suitability of this 
task for application to a machine, the seeds of a fully 
automated system are sown. 
 
7 Conclusion 
This paper describes the process of creating an email 
corpus annotated with business or personal labels. By 
measuring inter-annotator agreement it shows that this 
process was successful. Furthermore, by analysing the 
disagreements in the fine categories, it has allowed us 
to characterise the areas where the business/personal 
decisions are difficult.  
 
In general, the separation of business and personal 
mails is a task that humans can perform. Part of the 
project has allowed the identification of the areas 
where humans cannot make this distinction (as dem-
onstrated by inter-annotator agreement scores) and 
one would not expect machines to perform the task 
under these conditions either. In all other cases, where 
the language is not ambiguous as judged by human 
annotators, the challenge has been made to automatic 
classifiers to match this performance. 
 
Some initial results were reported where machines 
attempted exactly this task. They showed that accu-
racy almost as high as human agreement was 
achieved by the system. Further work, using much 
larger sets and incorporating all types of business and 
personal emails, is the next logical step. 
 
410
Any annotation project will encounter its problems in 
deciding appropriate categories. This paper described 
the various stages of evolving these categories to a 
stage where they are both intuitive and logical and 
also, produce respectable inter-annotator agreement 
scores. The work is still in progress in ensuring 
maximal consistency within the data set and refining 
the precise definitions of the categories to avoid pos-
sible overlaps. 
References 
Brian Klimt and Yiming Yang. 2004. Introducing the 
Enron Email Corpus, Carnegie Mellon University.  
 
Brian Klimt and Yiming Yang. 2004. The Enron Cor-
pus: A New Data Set for Email Classification Re-
search. Carnegie Mellon University. 
 
Email Annotation at Berkely   
http://bailando.sims.berkeley.edu/enron_email.html
 
Jana Diesner and Kathleen M. Karley. 2005. Explora-
tion of Communication Networks from the Enron 
Email Corpus, Carnegie Mellon University  
 
Louise Guthrie,  Elbert Walker and Joe Guthrie. 1994 
Document classification by machine: Theory and 
practice. Proc. of COLING'94 
 
Parambir S. Keila and David B. Skillcorn. 2005.  De-
tecting Unusual and Deceptive Communication in 
Email. Queen?s University, CA 
 
 
 
 
 
411
Abstract
AKT is a major research project
applying a variety of technologies to
knowledge management. Knowledge
is a dynamic, ubiquitous resource,
which is to be found equally in an
expert's head, under terabytes of data,
or explicitly stated in manuals. AKT
will extend knowledge management
technologies to exploit the potential
of the semantic web, covering the use
of knowledge over its entire lifecycle,
from acquisition to maintenance and
deletion. In this paper we discuss how
HLT will be used in AKT and how
the use of HLT will affect different
areas of KM, such as knowledge
acquisition, retrieval and publishing.
1 Introduction
As globalisation reduces the competitive
advantage existing between companies, the role
of proprietary information and its appropriate
management becomes all-important. A
company?s value depends more and more on
?intangible assets?1 which exist in the minds of
employees, in databases, in files and in a
multitude of documents. It is the goal of
knowledge management (KM) technologies to
make computer systems which provide access to
this intangible knowledge present in a company
or organisation.  The system must make it
possible to share, store and retrieve the
collective expertise of all the people in an
organization. At present, many companies spend
                                                     
1 A term coined by Karl-Erik Sveiby
considerable resources on knowledge
management; estimates range between 7 and
10% of revenues (Davenport 1998).
In developing a knowledge management
system, the knowledge must first be captured or
acquired in some form which is usable by a
computer. The knowledge acquisition
bottleneck, so well-known in AI, is just as
important in knowledge management. The
acquisition of knowledge does not become less
difficult in a business environment and often
requires a sea-change in company culture in
order to persuade users to accommodate to the
technology adopted, precisely because
knowledge acquisition is so difficult.
Once knowledge has been acquired, it must be
managed, i.e. modelled, updated and published.
Modelling means representing information in a
way that is both manageable and easy to
integrate with the rest of the company?s
knowledge. Updating is necessary because
knowledge is dynamic. Part of its importance
for a company or individual lies in the fact that
knowledge is ever changing and keeping up
with the change is a crucial dimension in
knowledge management. Publishing is the
process that allows sharing the knowledge
across the company. These needs have
crystallised in efforts to develop the so-called
Semantic Web. It is envisaged that in the future,
the content currently available on the Web (both
Internets and Intranets) as raw data will be
automatically annotated with machine-readable
semantic information.  In such a case, we will
no longer speak of information retrieval but
rather of Knowledge Retrieval because instead
of obtaining thousands of potentially relevant or
irrelevant documents, only the dozen or so
documents that are truly needed by the user will
be presented to them.
Using HLT for Acquiring, Retrieving and Publishing Knowledge in AKT:
Position Paper
K. Bontcheva, C. Brewster, F. Ciravegna, H. Cunningham,
L. Guthrie, R. Gaizauskas, Y. Wilks
Department of Computer Science, the University of Sheffield,
Regent Court, 211 Portobello Street, S1 4DP Sheffield, UK
Email: N.Surname@dcs.shef.ac.uk
 In this paper we present the way Human
Language Technology (HLT) is used to address
several facets of the KM problem:  acquiring,
retrieving, and publishing knowledge. The work
presented in this paper is supported by the AKT
project (Advanced Knowledge Technologies), a
multimillion pound six year research project
funded by the EPSRC in the UK. AKT, started
in 2000, involves the University of
Southampton, the Open University, the
University of Edinburgh, the University of
Aberdeen, and the University of Sheffield
together with a large number of major UK
companies. Its objectives are to develop
technologies to cope with the six main
challenges of knowledge management:
? acquisition ? reuse
? modelling ? publication
? retrieval/extraction ? maintenance
These challenges will be addressed by the
University of Sheffield in the context of AKT
by the application of a variety of human
language technologies. Here, we consider only
the contribution of HLT to the acquisition of
knowledge, its retrieval and extraction, its
publication, and finally the role of appropriate
HLT infrastructure to the completion of these
goals.
2 Knowledge Acquisition
Knowledge acquisition (KA) is concerned with
the process of turning data into coherent
knowledge for a computer program.  The need
for effective KA methods increases as the
quantity of data available electronically
increases year by year, and the importance it
plays in our society is more and more
recognised. The challenge, we believe, lies in
designing effective techniques for acquiring the
vast amounts of (largely) tacit knowledge. KA is
a complex process, which traditionally is
extremely time consuming.
Existing KA methodologies are varied but
almost always require a great deal of manual
input. One methodology, often used in Expert
Systems, involves the time-consuming process
of structured interviews (?protocols?), which are
then analysed by knowledge engineers in order
to codify and model the knowledge of an expert
in a particular domain. Even if a complex expert
system is not required, all forms of KA are very
labour intensive. Yahoo currently employs over
100 people to keep its category hierarchy up to
date (Dom 1999). Some methodologies have
started to appear to automate this process,
although still limited to some steps in the KA
process. They depend on replacing the
introspection of knowledge engineers or the
extended elicitations of the protocol methods
(Ericsson and Simon 1984) by using Human
Language Technologies, more specifically
Information Extraction, Natural Language
Processing and Information Retrieval.
 Although knowledge acquisition produces
data (knowledge) for use by a computer
program, the form and content of that
knowledge is often debated in the research
community.  Ontologies have emerged as one of
the most popular means of modelling the
knowledge of a domain.  The meaning of this
word varies somewhat in the literature, but
minimally it is a hierarchical taxonomy of
categories, concepts or words. Ontologies can
act as an index to the memory of an organisation
and facilitate semantic searches and the retrieval
of knowledge from the corporate memory as it
is embodied in documents and other archives.
Repeated research has shown their usefulness,
especially for specific domains (J?rvelin and
Kek?l?inen 2000). The process of ontology
construction is illustrated in the rest of this
section.
2.1 Taxonomy construction
We propose to introduce automation in the stage
of taxonomy construction mainly in order to
eliminate or reduce the need for extensive
elicitation of data.  In the literature approaches
to construction of taxonomies of concepts have
been proposed (Brown et al 1992, McMahon
and Smith 1996, Sanderson and Croft 1999).
Such approaches either use a large collection of
documents as their sole data source, or they can
attempt to use existing concepts to extend the
taxonomy (Agirre et al2000, Scott 1998).  We
intend to develop a semi-automatic method that,
starting from a seed ontology sketched by the
user, produces the final ontology via a cycle of
refinements by eliciting knowledge from a
collection of texts. In this approach the role of
the user should only be that of proposing an
initial ontology and validating/changing the
different versions proposed by the system.
We intend to integrate a methodology for
automatic hierarchy definition (such as
(Sanderson and Croft 1999)) with a method for
the identification of terms related to a concept
in a hierarchy (such as (Scott 1998)). The
advantage of this integration is that, as
knowledge is continually changing, we can
reconstruct an appropriate domain specific
ontology very rapidly. This does not preclude
incorporating an existing ontology and using the
tools to extend and update it on the basis of
appropriate texts. Finally an ontology defined in
this way has the particular advantage that it
overcomes the well-known ?Tennis problem?
associated with many predefined ontologies
such as WordNet, i.e where terms closely
related in a given domain are structurally very
distant such as ball and court, for example.
In addition we intend to employ classic
Information Extraction techniques (described
below) such as named entity recognition
(Humphreys et al 1998) in order to pre-process
the text, as the identification of complex terms
such as proper names, dates, numbers, etc,
allows to reduce data sparseness in learning
(Ciravegna 2000).
We plan to introduce many cycles of ontology
learning and validation. At each stage the
defined ontology can be: i) validated/corrected
by a user/expert; ii) used to retrieve a larger set
of appropriate documents to be used for further
refinement (J?rvelin and Kek?l?inen 2000); iii)
passed on to the next development stage.
2.2 Learning Other Relations
This stage proceeds to build on the skeletal
ontology in order to specify, as much as
possible without human intervention, relations
among concepts in the ontology, other than
ISAs. In order to flesh the concept relations, we
need to identify relations such as synonymy,
meronymy, antonymy and other relations. We
plan to integrate a variety of methods existing in
the literature, e.g. by using recurrences in verb
subcategorisation as a symptom of general
relations (Basili et al 1998), by using Morin?s
user-guided approach to identify the correct
lexico/syntactic environment (Morin 1999), and
by using methods such as (Hays 1997) to locate
specific cases of synonymy.
3 Knowledge Extraction
Assuming that the shape of knowledge has been
acquired and adequately modelled, it will have
to be stored in a repository from which it is
retrieved as and when needed. On the one hand
there is the problem of retrieving instances in
order to populate the resulting knowledge base.
On the other hand, considering that repositories
could become very substantial in size, there is
the necessity to navigate the repository in order
to extract the knowledge when needed. In this
section we focus on the problem of knowledge
base population, as it is in our opinion the most
challenging from the HLT point of view.
3.1 Knowledge Base  Population
Instance identification for Knowledge Base
population can be performed by HLT-based
document analysis. With the term documents,
we mean a wide variety of types of texts such as
plain texts, web pages, knowledge elicitation
interview transcriptions (protocols), etc.  For the
sake of this paper we limit our analysis to
language related tasks only, ignoring the
problem of multi-media information. As a first
step instance identification requires the
identification of relevant documents containing
citation of the interesting information
(document classification). Then it requires the
ability to identify and extract information from
documents (Information Extraction from text).
3.2 Document Classification
Text classification for IE purposes has been
explored both in the MUC conferences as well
as in some commercially oriented projects
(Ciravegna et al 2000). In concrete terms
classification is used in order to identify the
scenario to apply to a specific set of texts, while
IE will identify (i.e. index) the instances in the
texts.  In most cases of application document
classification is quite straightforward, being
limited to the Boolean classification of a
document between relevant/irrelevant (single
scenario application as in the MUC
conferences). In cases in which knowledge may
be distributed along a number of different
detailed scenarios, full document classification
is then needed. In such cases, two main
characteristics are relevant for the classification
approach: flexibility and refinability (Ciravegna
et al 1999). Flexibility is needed with respect
to both the number of the categories and the
granularity of the classification to be coped
with. Three main types of classification can be
identified: coarse-grained, fine-grained, and
content-based. Coarse-grained classification is
performed among a relatively small number of
classes (e.g., some dozens) that are sharply
different (e.g., sport vs finance). This can be
obtained reliably and efficiently by the
application of statistical classifiers. Fine-
grained classification is performed over a
usually larger number of classes that can be
very similar (e.g., discriminating between news
about private bond issues and news about public
bond issues). This type of classification
generally requires some more knowledge-
oriented approaches such as pattern-based
classification. Sometimes categories are so
similar that classification needs to be content-
based, i.e. it can be performed only by
extracting the news content (e.g., finding news
articles issued by English financial institutions
referring to amounts in excess of 100,000 Euro).
In this case some forms of shallow adaptive
Information Extraction can be used (see next
section). Refinability concerns the possibility
of performing classification in a sequence of
steps, each one providing a more precise
classification (from coarse-grained to content-
based). In the current technological situation
coarse-grained classification can be performed
quickly, while the systems available for more
fine-grained classification are much slower and
less general purpose. When the amount of
textual material is large an incremental
approach, based on some level of coarse-grained
classification further refined by successive
analysis, proves to be very effective. A refinable
classification is generally performed over a
hierarchy of classes. A refinement may revise
the categories assigned to specific texts with
more specialised classes from the hierarchy.
More complex techniques are invoked only
when needed and, in any case, within an already
detected context (Ciravegna et al 1999).
We plan to produce a number of solutions for
text classification, adaptable to different
scenarios and situations, following the criteria
mentioned above.
3.3 Information Extraction
Information extraction from text (IE) is the
process of mapping of texts into fixed format
output (templates) representing the key
information (Gaizauskas 1997). In using IE for
KM, templates represent an intermediate format
for mapping the information in the texts into
ontology instances. Templates can be semi-
automatically derived from the ontology. We
plan to use IE for a number of passes: on the
one hand, we plan to populate a knowledge base
with instances as mentioned above. On the other
hand, IE can be used to monitor relevant
changes in the information, providing a
fundamental contribution to the problem of
knowledge updating. We have a long experience
in IE from texts, Sheffield having actively
participated in the MUC conferences and in the
TIPSTER project, activities that historically
have made a fundamental contribution to
making IE as we now know it.  The new
challenge we are currently addressing is
adaptivity. Adaptivity is a major goal for
Information Extraction, especially in the case of
its application to knowledge management, as
KM is a process that has to be distributed
throughout companies. The real value of IE will
become apparent when it can be adapted to new
applications and scenarios directly by the final
user without the intervention of IE experts. The
goal for research in adaptive IE is to create
systems adaptable to new applications/domains
by using only an analyst?s knowledge, i.e.
knowledge about the domain/scenario.
There are two directions of research in
adaptive IE, both involving the use of Machine
Learning. On the one hand machine learning is
used to automate as much as possible the tasks
an IE expert would perform in application
development (Cardie 1997) (Yangarber et al
2000). The goal here is to reduce the porting
time to a new application (and hence the cost).
This area of research comes mainly from the
MUC community. Currently, the technology
makes use mainly of NLP-intensive
technologies and the type of texts addressed are
mainly journal articles.
On the other hand, there is an attempt to make
IE systems adaptable to new
domains/applications by using only an analyst?s
knowledge, i.e. knowledge about the
domain/scenario only (Kushmerick et al 1997),
(Califf 1998), (Muslea et al 1998), (Freitag and
McCallum 1999), (Soderland 1999), (Freitag
and Kushmerick 2000), (Ciravegna 2001a).
Most research has so far focused on Web-
related texts (e.g. web pages, email, etc.)
Successful commercial products have been
created and there is an increasing interest on IE
in the Web-related market.  Current adaptive
technologies make no use of natural language
processing in the web context, as extra linguistic
structures (e.g. HTML tags, document
formatting, and ungrammatical stereotypical
language) are the elements used to identify
information. Linguistically intensive approaches
are difficult or unnecessary in such cases. When
these non-linguistic approaches are used on
texts with a reduced (or no) structure, they tend
to be ineffective.
There is a technological gap between adaptive
IE on free texts and adaptive IE on web-related
texts. For the purposes of KM, such a gap has to
be bridged so to create a set of technologies able
to cover the whole range of potential
applications for different kinds of texts, as the
type of texts to be analysed for KM may vary
dramatically from case to case. We plan to
bridge this gap via the use of lazy natural
language processing. We intend to use an
approach where the system starts with a range
of potential methodologies (from shallow to
linguistically intensive) and learns from a
training corpus which is the most effective
approach for the particular case under
consideration. A number of factors can
influence the choice: from the type of texts to be
analysed to the type of information the user is
able to provide in adapting the system. In the
first case the system will have to identify what
type of task is under consideration and select the
correct level of analysis  (e.g. language based
for free texts). Formally in this case the level of
language analysis is one of the parameters the
learner will have to learn. Concerning the type
of tagging the user is able to provide: different
users are able to provide different levels of
information in training the system: IE-trained
users are able to provide sophisticated tagging,
maybe inclusive of syntactic, semantic or
pragmatic information. Na?ve users on the other
hand are only able to provide some basic
information (e.g. to spot the relevant
information in the texts and highlight it in
different colours). We plan to develop a system
able to cope with a wide of variety of situations
by starting from the (LP)2 algorithm and
enhancing its learning capabilities on free texts
(Ciravegna 2001) and developing a powerful
human-computer interface for system adaptation
(Ciravegna and Petrelli 2001).
4 Knowledge Publishing
Knowledge is only effective if it is delivered in
the right form, at the right place, to the right
person at the right time. Knowledge publishing
is the process that allows getting knowledge to
the people who need it in a form that they can
use. As a matter of fact, different users need to
see knowledge presented and visualised in quite
different ways. The dynamic construction of
appropriate perspectives is a challenge which, in
AKT, we will address from the perspective of
generating automatically such presentations
from the ontologies acquired by the KA and KE
methods, discussed in the previous sections.
Natural Language Generation (NLG) systems
automatically produce language output (ranging
from a single sentence to an entire document)
from computer-accessible data, usually encoded
in a knowledge or data base (Reiter 2000). NLG
techniques have already been used successfully
in a number of application domains, the most
relevant of which is automatic production of
technical documentation (Reiter et al 1995),
(Paris et al 1996). In the context of KM and
knowledge publishing in particular, NLG is
needed for knowledge diffusion and
documenting ontologies. The first task is
concerned with personalised presentation of
knowledge, in the form needed by each specific
user and tailored to the correct language type
and the correct level of details. The latter is a
very important issue, because as discussed
earlier, knowledge is dynamic and needs to be
updated frequently. Consequently, the
accompanying documentation which is vital for
the understanding and successful use of the
acquired knowledge, needs to be updated in
sync. The use of NLG simplifies the ontology
maintenance and update tasks, so that the
knowledge engineer can concentrate on   the
knowledge itself, because the documentation is
automatically updated as the ontology changes.
The NLG-based knowledge publishing tools
will also utilise the ontology instances extracted
from documents using the IE approaches
discussed in Section 3.3. The dynamically
generated documentation will not only include
these instances, as soon as they get extracted,
but it will also provide examples of their
occurrence in the documents, thus facilitating
users? understanding and use of the ontology.
Our approach to knowledge publishing is based
on an existing framework for generation of user-
adapted hypertext explanations (Bontcheva
2001), (Bontcheva and Wilks 2001). The
framework incorporates a powerful agent
modelling module, which is used to tailor the
explanations to the user?s knowledge, task, and
preferences. We are now also extending the
personalisation techniques to account for user
interests. The main challenge for NLG will be
to develop robust and efficient techniques for
knowledge publishing which can operate on
large-scale knowledge resources and support the
personalised presentation of diverse
information, such as speech, video, text,
graphics (see (Maybury 2001)).
The other challenge in using NLG for
knowledge publishing is to develop tools and
techniques that will enable knowledge
engineers, instead of linguists, to create and
customise the linguistic resources (e.g., domain
lexicon) at the same time as they create and edit
the ontology.  In order to allow such inter-
operability with the KA tools, we will integrate
the NLG tools in the GATE infrastructure,
discussed next.
5 HLT Infrastructure
The range and complexity of the task of
knowledge management make imperative the
need for standardisation. While there has been
much talk about the re-use of knowledge
components such ontologies, much less has
been undertaken to standardise the
infrastructure for tools and their development.
The types of data structures typically involved
are large and complex, and without good tools
to manage and allow succinct viewing of the
data we will continue to work below our
potential. The University of Sheffield has
pioneered in the Gate and Gate 2 projects the
development of an architecture for text
engineering (Cunningham et al 1997),
(Cunningham et al 2000). Given the modular
architecture and component structure of Gate, it
is natural to build on this basis to extend the
capabilities of Gate so as to provide the most
suitable possible environment for tool
development, implementation and evaluation in
AKT. The system will provide a single
interaction and deployment point for the roll-out
of HLT in Knowledge Management. We expect
Gate2 to act as the skeleton for a large range of
knowledge management activities within AKT
and plan to extend its capabilities within the life
of the AKT project by integrating with suitable
ontological and lexical databases in order to
permit the use of  the Gate system with large
bodies of heterogeneous data
6 Conclusion and Future Work
We have presented how we plan to use HLT for
helping KM in AKT. We believe that HLT can
make a substantial contribution to the following
issues in  KM:
? Cost reduction: KM is an expensive task,
especially in the acquisition phase. HLT can
aid in automating both the acquisition of the
structure of the ontology to be learnt and in
populating such ontology with instances. It
will also provide support for automatic
knowledge documentation.
? Time reduction: KM is a slow task: HLT
can help in making it more efficient by
reducing the need for the human effort;
? Subjectivity reduction: this is a main
problem in knowledge identification and
selection. Subjective knowledge is difficult
to integrate with the rest of the company?s
knowledge and its use is somehow difficult.
KM constitutes a challenge for HLT as it
provides a number of fields of application and
in particular it challenges the integration of a set
of techniques for a common goal.
Acknowledgement
This work is supported under the Advanced
Knowledge Technologies (AKT)
Interdisciplinary Research Collaboration (IRC),
which is sponsored by the UK Engineering and
Physical Sciences Research Council under grant
number GR/N15764/01. The AKT IRC
comprises the Universities of Aberdeen,
Edinburgh, Sheffield, Southampton and the
Open University.
References
Agirre, E. O. Ansa, E. Hovy, and  D.
Mart?nez 2000. Enriching very large ontologies
using the WWW, Proceedings of the ECAI 2000
workshop ?Ontology Learning?.
Basili, R., R. Catizone, M. Stevenson, P.
Velardi, M. Vindigni, and Y. Wilks. 1998. ?An
Empirical Approach to Lexical Tuning?.
Proceedings of the Adapting Lexical and
Corpus Resources to Sublanguages and
Applications Workshop, held jointly with 1st
LREC Granada, Spain.
Bontcheva, K. 2001. Generating adaptive
hypertext explainations with a nested agent
model.  Ph. D. Thesis, University of Sheffield.
Bontcheva, K. and Wilks, Y. 2001. Dealing
with Dependencies between Content Planning
and Surface Realisation in a Pipeline
Generation Architecture. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Brown, P.F., Peter F., V. J. Della Pietra, P.
V. DeSouza, J. C. Lai, and R. L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18, 467-479.
 Califf, M. E. 1998. Relational Learning
Techniques for Natural Language Information
Extraction. Ph.D. thesis, Univ. Texas, Austin,
www/cs/utexas.edu/users/mecaliff
C. Cardie, `Empirical methods in
information extraction', AI Journal,18(4), 65-
79, (1997).
F. Ciravegna, A. Lavelli, N. Mana, J.
Matiasek, L. Gilardoni, S. Mazza, M. Ferraro,
W. J. Black F. Rinaldi, and D. Mowatt.
FACILE: Classifying Texts Integrating Pattern
Matching and Information Extraction. In
Proceedings of the 16th International Joint
Conference On Artificial Intelligence
(IJCAI99), Stockholm, Sweden, 1999.
F. Ciravegna, A. Lavelli,, L. Gilardoni, S.
Mazza, W. J. Black, M. Ferraro, N. Mana, J.
Matiasek, F. Rinaldi. Flexible Text
Classification for Financial
Applications: The FACILE System. In
Proceedings of Prestigious Applications sub-
conference (PAIS2000) sub-conference of the
14th European Conference On Artificial
Intelligence (ECAI2000), Berlin, Germany,
August, 2000.
 Ciravegna, F. 2001. Adaptive Information
Extraction from Text by Rule Induction and
Generalisation. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Ciravegna, F. and D. Petrelli. 2001. User
Involvement in customizing Adaptive
Information Extraction from Texts: Position
Paper. Proceedings of the IJCAI01 Workshop on
Adaptive Text Extraction and Mining, Seattle.
Cunningham, H., K. Humphreys, R.
Gaizauskas and Y. Wilks. 1997. Software
Infrastructure for Natural Language Processing.
Proceedings of the Fifth Conference on Applied
Natural Language Processing (ANLP-97).
Cunningham H., K. Bontcheva, V. Tablan
and Y. Wilks. 2000. Software Infrastructure for
Language Resources: a Taxonomy of Previous
Work and a Requirements Analysis.
Proceedings of the Second Conference on
Language Resources Evaluation, Athens.
Dom, B. 1999. Automatically finding the
best pages on the World Wide Web (CLEVER).
Search Engines and Beyond: Developing
efficient knowledge management systems.
Boston, MA.
Ericsson, K. A. and H. A. Simon. 1984.
Protocol Analysis: verbal reports as data. MIT
Press, Cambridge, Mass.
Freitag, D. and A. McCallum. 1999
Information Extraction with HMMs and
Shrinkage. AAAI-99 Workshop on Machine
Learning for Information Extraction, Orlando,
FL. (www.isi.edu/~muslea/RISE/ML4IE/)
Freitag, D. and N. Kushmerick. 2000.
Boosted wrapper induction. F. Ciravegna, R.
Basili, R. Gaizauskas, ECAI2000 Workshop on
Machine Learning for Information Extraction,
Berlin, 2000, (www.dcs.shef.ac.uk/~fabio/ecai-
workshop.html)
Hays, P. R. 1997. Collocational Similarity:
Emergent Patterns in Lexical Environments,
PhD. Thesis. School of English, University of
Birmingham
Humphreys, K., R. Gaizauskas, S. Azzam, C.
Huyck, B. Mitchell, H. Cunningham and  Y.
Wilks. 1998. Description of the University of
Sheffield LaSIE-II System as used for MUC-7.
Proceedings of the 7th Message Understanding
Conference.
J?rvelin, K.  and J. Kek?l?inen. 2000. IR
evaluation methods for retrieving highly
relevant documents. Proceedings of the 23rd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, , Athens.
Kushmerick, N., D. Weld, and R.
Doorenbos. 1997. Wrapper induction for
information extraction. Proceedings of 15th
International Conference on Artificial
Intelligence, IJCAI-97.
Manchester, P. 1999. Survey ? Knowledge
Management. Financial Times, 28.04.99.
Maybury, M.. 2001. Human Language
Technologies for Knowledge Management:
Challenges and Opportunities. Workshop on
Human Language Technology and Knowledge
Management. Toulouse, France.
McMahon, J. G. and  F. J. Smith. 1996
Improving Statistical Language Models
Performance with Automatically  Generated
Word Hierarchies. Computational Linguistics,
22(2), 217-247, ACL/MIT.
Morin, E. 1999. Using Lexico-Syntactic
patterns to Extract Semantic Relations between
Terms from Technical Corpus, TKE 99,
Innsbruck, Austria.
Muslea, I., S. Minton, and C. Knoblock.
1998. Wrapper induction for semi-structured,
web-based information sources. Proceedings of
the Conference on Autonomous Learning and
Discovery CONALD-98.
Paris, C. , K. Vander Linden. 1996.
DRAFTER: An interactive support tool for
writing multilingual instructions, IEEE
Computer, Special Issue on Interactive NLP.
Reiter, E. 1995. NLG vs. Templates.
Proceedings of the 5th European workshop on
natural language generation, (ENLG-95),
Leiden.
Reiter, E. , C. Mellish and J. Levine. 1995
Automatic generation of technical
documentation. Journal of Applied Artificial
Intelligence,  9(3) 259-287, 1995
Sanderson, M.  and B. Croft. 1999. Deriving
concept hierarchies from text. Proceedings of
the 22nd ACM SIGIR Conference, 206-213.
Scott, M. 1998. Focusing on the Text and Its
Key Words. TALC 98 Proceedings, Oxford,
Humanities Computing Unit, Oxford University.
Soderland, S. 1999. Learning information
extraction rules for semi-structured and free
text. Machine Learning, (1), 1-44.
Yangarber, R., R. Grishman, P. Tapanainen
and S. Huttunen. 2000. Automatic Acquisition
of Domain Knowledge for Information
Extraction. Proceedings of COLING 2000: The
18th International Conference on
Computational Linguistics, Saarbr?cken.
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 289?292,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Evaluation Metrics for the Lexical Substitution Task
Sanaz Jabbari Mark Hepple Louise Guthrie
Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield, S1 4DP, UK
{S.Jabbari,M.Hepple,L.Guthrie}@dcs.shef.ac.uk
Abstract
We identify some problems of the evaluation
metrics used for the English Lexical Substitu-
tion Task of SemEval-2007, and propose al-
ternative metrics that avoid these problems,
which we hope will better guide the future de-
velopment of lexical substitution systems.
1 Introduction
The English Lexical Substitution task at SemEval-
2007 (here called ELS07) requires systems to find
substitutes for target words in a given sentence (Mc-
Carthy & Navigli, 2007: M&N). For example, we
might replace the target word match with game in
the sentence they lost the match. System outputs are
evaluated against a set of candidate substitutes pro-
posed by human subjects for test items. Targets are
typically sense ambiguous (e.g. match in the above
example), and so task performance requires a com-
bination of word sense disambiguation (by exploit-
ing the given sentential context) and (near) synonym
generation. In this paper, we discuss some problems
of the evaluation metrics used in ELS07, and then
propose some alternative measures that avoid these
problems, and which we believe will better serve to
guide the development of lexical substitution sys-
tems in future work.1 The subtasks within ELS07
divide into two groups, in terms of whether they fo-
cus on a system?s ?best? answer for a test item, or ad-
dress the broader set of answer candidates a system
1We consider here only the case of substituting for single
word targets. Subtasks of ELS07 involving multi-word substi-
tutions are not addressed.
can produce. In what follows, we address these two
cases in separate sections, and then present some re-
sults for applying our new metrics for the second
case. We begin by briefly introducing the test ma-
terials that were created for the ELS07 evaluation.
2 Evaluation Materials
Briefly stated, the ELS07 dataset comprises around
2000 sentences, providing 10 test sentences each
for some 201 preselected target words, which were
required to be sense ambiguous and have at least
one synonym, and which include nouns, verbs, ad-
jectives and adverbs. Five human annotators were
asked to suggest up to three substitutes for the tar-
get word of each test sentence, and their collected
suggestions serve as the gold standard against which
system outputs are compared. Around 300 sentences
were distributed as development data, and the re-
mainder retained for the final evaluation.
To assist defining our metrics, we formally de-
scribe this data as follows.2 For each sentence t
i
in the test data (1 ? i ? N , N the number of test
items), let H
i
denote the set of human proposed sub-
stitutes. A key aspect of the data is the count of hu-
man annotators that proposed each candidate (since
a term appears a stronger candidate if proposed by
annotators). For each t
i
, there is a function freq
i
which returns this count for each term within H
i
(and 0 for any other term), and a value maxfreq
i
corresponding to the maximal count for any term in
H
i
. The pairing of H
i
and freq
i
in effect provides a
multiset representation of the human answer set. We
2For consistency, we also restate the original ELS07 metrics
in these terms, whilst preserving their essential content.
289
use |S|i in what follows to denote the multiset car-
dinality of S according to freq
i
, i.e. ?
a?S
freq
i
(a).
Some of the ELS07 metrics use a notion of mode
answer m
i
, which exists only for test items that
have a single most-frequent human response, i.e.
a unique a ? H
i
such that freq
i
(a) = maxfreq
i
.
To adapt an example from M&N, an item with tar-
get word happy (adj) might have human answers
{glad ,merry , sunny , jovial , cheerful } with counts
(3,3,2,1,1) respectively. We will abbreviate this an-
swer set as H
i
= {G:3,M:3,S:2,J:1,Ch:1} where it
is used later in the paper.
3 Best Answer Measures
Two of the ELS07 tasks address how well systems
are able to find a ?best? substitute for a test item, for
which individual test items are scored as follows:
best(i) =
?
a?A
i
freq
i
(a)
|H
i
|
i
? |A
i
|
mode(i) =
{
1 if bg
i
= m
i
0 otherwise
For the first task, a system can return a set of an-
swers A
i
(the answer set for item i), but since the
score achieved is divided by |A
i
|, returning multiple
answers only serves to allow a system to ?hedge its
bets? if it is uncertain which candidate is really the
best. The optimal score on a test item is achieved by
returning a single answer whose count is maxfreq
i
,
with proportionately lesser credit being received for
any answer in H
i
with a lesser count. For the sec-
ond task, which uses the mode metric, only a single
system answer ? its ?best guess? bg
i
? is allowed,
and the score is simply 0 or 1 depending on whether
the best guess is the mode. Overall performance is
computed by averaging across a broader set of test
items (which for the second task includes only items
having a mode value). M&N distinguish two over-
all performance measures: Recall, which averages
over all relevant items, and Precision, which aver-
ages only over those items for which the system gave
a non-empty response.
We next discuss these measures and make an al-
ternative proposal. The task for the first measure
seems a reasonable one, i.e. assessing the ability of
systems to provide a ?best? answer for a test item,
but allowing them to offer multiple candidates (to
?hedge their bets?). However, the metric is unsatis-
factory in that a system that performs optimally in
terms of this task (i.e. which, for every test item, re-
turns a single correct ?most frequent? response) will
get a score that is well below 1, because the score is
also divided by |H
i
|
i, the multiset cardinality of H
i
,
whose size varies between test items (being a con-
sequence of the number of alternatives suggested by
the human annotators), but which is typically larger
than the numerator value maxfreq
i
of an optimal an-
swer (unless H
i
is singleton). This problem is fixed
in the following modified metric definition, by di-
viding instead by maxfreq
i
, as then a response con-
taining a single optimal answer will score 1.
best(i) =
?
a?A
i
freq
i
(a)
maxfreq
i
? |A
i
|
best
1
(i) =
freq
i
(bg
i
)
maxfreq
i
With H
i
= {G:3,M:3,S:2,J:1,Ch:1}, for example,
an optimal response A
i
= {M} receives score 1,
where the original metric gives score 0.3. Singleton
responses containing a correct but non-optimal an-
swer receive proportionately lower credit, e.g. for
A
i
= {S} we score 0.66 (vs. 0.2 for the origi-
nal metric). For a non-singleton answer set includ-
ing, say, a correct answer and an incorrect one, the
credit for the correct answer will be halved, e.g. for
A
i
= {S,X} we score 0.33.
Regarding the second task, we think it reasonable
to have a task where systems may offer only a single
?best guess? response, but argue that the mode met-
ric used has two key failings: it is too brittle in being
applicable only to items that have a mode answer,
and it loses information valuable to system rank-
ing, in assigning no credit to a response that might
be good but not optimal. We propose instead the
best
1
metric above, which assigns score 1 to a best
guess answer with count maxfreq
i
, but applies to all
test items irrespective of whether or not they have
a unique mode. For answers having lesser counts,
proportionately less credit is assigned. This metric
is equivalent to the new best metric shown beside it
for the case where |A
i
| = 1.
For assessing overall performance, we suggest
just taking the average of scores across all test items,
c.f. M&N?s Recall measure. Their Precision met-
ric is presumably intended to favour a system that
can tell whether it does or does not have any good
answers to return. However, the ability to draw a
290
boundary between good vs. poor candidates will be
reflected widely in a system?s performance and cap-
tured elsewhere (not least by the coverage metrics
discussed later) and so, we argue, does not need to
be separately assessed in this way. Furthermore, the
fact that a system does not return any answers may
have other causes, e.g. that its lexical resources have
failed to yield any substitution candidates for a term.
4 Measures of Coverage
A third task of ELS07 assesses the ability of systems
to field a wider set of good substitution candidates
for a target, rather than just a ?best? candidate. This
?out of ten? (oot) task allows systems to offer a set
A
i
of upto 10 guesses per item i, and is scored as:
oot(i) =
?
a?A
i
freq
i
(a)
|H
i
|
i
Since the score is not divided by the answer set
size |A
i
|, no benefit derives from offering less than
10 candidates.3 When systems are asked to field a
broader set of candidates, we suggest that evalua-
tion should assess if the response set is good in con-
taining as many correct answers as possible, whilst
containing as few incorrect answers as possible. In
general, systems will tackle this problem by com-
bining a means of ranking candidates (drawn from
lexical resources) with a means of drawing a bound-
ary between good and bad candidates, e.g. thresh-
old setting.4 Since the oot metric does not penalise
incorrect answers, it does not encourage systems to
develop such boundary methods, even though this is
important to their ultimate practical utility.
The view of a ?good? answer set described above
suggests a comparison of A
i
to H
i
using versions
of ?recall? and ?precision? metrics, that incorporate
the ?weighting? of human answers via freq
i
. Let us
begin by noting the obvious definitions for recall and
3We do not consider here a related task which assesses
whether the mode answer m
i
is found within an answer set of
up to 10 guesses. We do not favour the use of this metric for
reasons parallel to those discussed for the mode metric of the
previous section, i.e. brittleness and information loss.
4In Jabbari et al (2010), we define a metric that directly
addresses the ability of systems to achieve good ranking of sub-
stitution candidates. This is not itself a measure of lexical sub-
stitution task performance, but addresses a component ability
that is key to the achievement of lexical substitution tasks.
precision metrics without count-weighting:
R(i) =
|H
i
?A
i
|
|H
i
|
P (i) =
|H
i
?A
i
|
|A
i
|
Our definitions of these metrics, given below, do
include count-weighting, and require some explana-
tion. The numerator of our recall definition is |A
i
|
i
not |H
i
? A
i
|
i as |A
i
|
i
= |H
i
? A
i
|
i (as freq
i
as-
signs 0 to any term not in H
i
), an observation which
also affects the numerator of our P definition. Re-
garding the latter?s denominator, merely dividing by
|A
i
|
i would not penalise incorrect terms (as, again,
freq
i
(a) = 0 for any a /? H
i
), so this is done di-
rectly by adding k|A
i
?H
i
|, where |A
i
?H
i
| is the
number of incorrect answers, and k some penalty
factor, which might be k = 1 in the simplest case.
(Note that our weighted R metric is in fact equiv-
alent to the oot definition above.) As usual, an F-
score can be computed as the harmonic mean of
these values (i.e. F = 2PR/(P + R)). For as-
sessing overall performance, we might average P ,
R and F scores across all test items.
R(i) =
|A
i
|
i
|H
i
|
i
P (i) =
|A
i
|
i
|A
i
|
i
+ k|A
i
?H
i
|
With H
i
= {G:3,M:3,S:2,J:1,Ch:1}, for example,
the perfect response set A
i
= {G,M,S, J,Ch}
gives P and R scores of 1. The response
A
i
= {G,M,S, J,Ch,X, Y, Z, V,W}, containing
all correct answers plus 5 incorrect ones, gets R =
1, but only P = 0.66 (assuming k = 1, giving
10/(10 + 5)). The response A
i
= {G,S, J,X, Y },
with 3 out of 5 correct answers, plus 2 incorrect
ones, gets R = 0.6 (6/10) and P = 0.75 (6/6 + 2))
5 Applying the Coverage measure
Although the ?best guess? task is a valuable indicator
of the likely utility of a lexical substitution system
within various broader applications, we would argue
that the core task for lexical substitution is coverage,
i.e. the ability to field a broad set of correct substi-
tution candidates. This task requires systems both to
field and rank promising candidates, and to have a
means of drawing a boundary between the good and
bad candidates, i.e. a boundary strategy.
In this section, we apply the coverage metrics to
the outputs of some lexical substitution systems, and
291
Model 1 2 3 4 5 6 7 8 9 10
bow .067 .114 .151 .173 .191 .201 .212 .219 .222 .225
lm .119 .192 .228 .246 .256 .267 .271 .272 .271 .271
cmlc .139 .205 .251 .271 .284 .288 .291 .290 .289 .286
KU .173 .244 .287 .307 .318 .321 .320 .318 .314 .311
Table 3: Coverage F-scores (macro-avgd), for simple boundary strategies (with penalty factor k = 1).
All By part-of-speech
Model words nouns adj verb adv
bow .326 .343 .334 .205 .461
lm .393 .372 .442 .252 .562
cmlc .414 .404 .447 .311 .534
KU .462 .408 .511 .398 .567
Table 1: Out-of-ten recall scores for all the systems (with
a subdivision by pos of target item).
All By part-of-speech
Model words nouns adj verb adv
bow .298 .315 .302 .189 .422
lm .371 .35 .408 .24 .539
cmlc .395 .383 .419 .31 .506
KU .435 .379 .477 .385 .536
Table 2: Optimal F-scores (macro-avgd) for coverage,
computed over the (oot) ranked outputs of the systems
(with penalty factor k = 1).
compare the indication it provides of relative sys-
tem performance to that of the oot metric. We con-
sider three systems described in Jabbari (2010), de-
veloped as part of an investigation into the means
and benefits of combining models of lexical context:
(i) bow: a system using a bag-of-words model to
rank candidates, (ii) lm: using a (simple) n-gram lan-
guage model, and (iii) cmlc: using a model that com-
bines bow and lm models into one. We also consider
the system KU, which uses a very large language
model and an advanced treatment of smoothing, and
which performed well at ELS07 (Yuret, 2007).5 Ta-
ble 1 shows the oot scores for these systems, includ-
ing a breakdown by part-of-speech, which indicate a
performance ranking: bow < lm < cmlc < KU
Our first problem is that these systems are devel-
oped for the oot task, not coverage, so after rank-
5We thank Deniz Yuret for allowing us to use his system?s
outputs in this analysis.
ing their candidates, they do not attempt to draw
a boundary between the candidates worth returning
and those not. Instead, we here use the oot out-
puts to compute an optimal performance for each
system, i.e. we find, for the ranked candidates of
each question, the cut-off position giving the high-
est F-score, and then average these scores across
questions, which tells us the F-score the system
could achieve if it had an optimal boundary strategy.
These scores, shown in Table 2, indicate a ranking of
systems in line with that in Table 1, which is not sur-
prising as both will ultimately reflect the quality of
candidate ranking achieved by the systems.
Table 3 shows the coverage results achieved by
applying a naive boundary strategy to the system
outputs. The strategy is just to always return the
top n candidates for each question, for a fixed value
n. Again, performance correlates straightforwardly
with the underlying quality of ranking. Comparing
tables, we see, for example, that by always returning
6 candidates, the system KU could achieve a cover-
age of .32 as compared to the .435 optimal score.
References
D. McCarthy and R. Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task.
Proc. of the 4th Int. Workshop on Semantic Eval-
uations (SemEval-2007), Prague.
S. Jabbari. 2010. A Statistical Model of Lexical Con-
text, PhD Thesis, University of Sheffield.
S. Jabbari, M. Hepple and L.Guthrie. 2010. Evaluat-
ing Lexical Substitution: Analysis and NewMea-
sures. Proc. of the 7th Int. Conf. on Language
Resources and Evaluation (LREC-2010). Malta.
D. Yuret. 2007. KU: Word Sense Disambiguation by
Substitution. In Proc. of the 4th Int. Workshop on
Semantic Evaluations (SemEval-2007), Prague.
292
