Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1660?1668,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating a City Exploration Dialogue System Combining
Question-Answering and Pedestrian Navigation
Srinivasan Janarthanam1, Oliver Lemon1, Phil Bartie2, Tiphaine Dalmas2,
Anna Dickinson2, Xingkun Liu1, William Mackaness2, and Bonnie Webber2
1 The Interaction Lab, Heriot-Watt University
2 Edinburgh University
sc445@hw.ac.uk
Abstract
We present a city navigation and tourist
information mobile dialogue app with in-
tegrated question-answering (QA) and ge-
ographic information system (GIS) mod-
ules that helps pedestrian users to nav-
igate in and learn about urban environ-
ments. In contrast to existing mobile apps
which treat these problems independently,
our Android app addresses the prob-
lem of navigation and touristic question-
answering in an integrated fashion using
a shared dialogue context. We evaluated
our system in comparison with Samsung
S-Voice (which interfaces to Google nav-
igation and Google search) with 17 users
and found that users judged our system to
be significantly more interesting to inter-
act with and learn from. They also rated
our system above Google search (with the
Samsung S-Voice interface) for tourist in-
formation tasks.
1 Introduction
We present a mobile dialogue system (an Android
app) called Spacebook that addresses the problem
of pedestrian navigation and tourist information in
urban environments. There has been little prior
work that addresses these two problems - naviga-
tion and tourist information provision - in an inte-
grated way. By navigation, we refer to the prob-
lem of finding appropriate destinations to go to
and the task of wayfinding to reach them and by
tourist information provision we refer to the prob-
lem of meeting the informational needs of a user
about entities such as museums, statues and fa-
mous personalities. A dialogue system such as this
could serve as a personal tour guide to pedestrian
tourists as they walk around unknown cities. With
the proliferation of smartphones, there has been a
number of mobile apps developed to address these
problems. However these apps have the following
problems: first, they demand the user?s visual at-
tention because they predominantly present infor-
mation on a mobile screen. This can be dangerous
in urban environments, as well as being distract-
ing. Second, these apps address the problems of
navigation and tourist information independently
and therefore do not have a shared interaction con-
text. This means that users cannot switch between
information and navigation tasks in a natural and
fluid manner.
User1: Take me to the National Museum.
System2: The National Museum is about 300m away..
System3: At the KFC, turn left on to South Bridge
System4 : Near you is the statue of David Hume.
User2: Who is David Hume.
System5: David Hume was a Scottish philosopher....
User3: Tell me more about David Hume.
System6: He was one of the most important figures in..
System7: You should be able to see the museum ...
User4: Tell me more about the museum.
System8: The National Museum of Scotland is a....
Table 1: An example interaction with the evalu-
ated system
In contrast to many existing mobile apps,
Spacebook has a speech-only interface and ad-
dresses both problems in an integrated way. We
conjecture that with a speech-only interface, users
can immerse themselves in exploring the city,
and that because of the shared context they can
switch between navigation and tourist information
tasks more easily. Using the navigational context,
Spacebook pushes point-of-interest information
which can then initiate tourist information tasks
using the QA module. Table 1 presents an example
interaction with our system showing the integrated
use of navigation and question-answering capabil-
1660
ities. Utterances System4-8 show the system?s ca-
pability to push information about nearby points-
of-interest (PoI) during a navigation task and an-
swer followup questions using the QA system (in
utterances User2 and User3). The final 3 utter-
ances show a natural switch between navigation to
an entity and QA about that entity.
We investigate whether our system using a com-
bination of geographical information system (GIS)
and natural language processing (NLP) technolo-
gies would be a better companion to pedestrian
city explorers than the current state-of-the-art mo-
bile apps. We hypothesize that, (1) users will find
our speech-only interface to navigation efficient as
it allows them to navigate without having to re-
peatedly look at a map and (2), that users will
find a dialogue interface which integrates touris-
tic question-answering and navigation within a
shared context to be useful for finding information
about entities in the urban environment. We first
present some related work in section 2. We de-
scribe the architecture of the system in section 3.
We then present our experimental design, results
and analysis in sections 5, 6 and 7.
2 Related work
Mobile apps such as Siri, Google Maps Naviga-
tion, Sygic, etc. address the problem of naviga-
tion while apps like Triposo, Guidepal, Wikihood,
etc. address the problem of tourist information by
presenting the user with descriptive information
about various points of interest (PoI) in the city.
While some exploratory apps present snippets of
information about a precompiled list of PoIs, other
apps dynamically generate a list of PoIs arranged
based on their proximity to the users. Users can
also obtain specific information about PoIs using
Search apps. Also, since these navigation and ex-
ploratory/search apps do not address both prob-
lems in an integrated way, users need to switch
between them and therefore lose interaction con-
text.
While most apps address these two problems
independently, some like Google Now, Google
Field Trip, etc, mix navigation with exploration.
But such apps present information primarily vi-
sually on the screen for the user to read. Some
of these are available for download at the Google
Play Android app store1. Several dialogue and
natural language systems have addressed the issue
1https://play.google.com/store
of pedestrian navigation (Malaka and Zipf, 2000;
Raubal and Winter, 2002; Dale et al, 2003; Bar-
tie and Mackaness, 2006; Shroder et al, 2011;
Dethlefs and Cuaya?huitl, 2011). There has also
been recent interest in shared tasks for generat-
ing navigation instructions in indoor and urban en-
vironments (Byron et al, 2007; Janarthanam and
Lemon, 2011). Some dialogue systems deal with
presenting information concerning points of inter-
est (Ko et al, 2005; Kashioka et al, 2011) and in-
teractive question answering (Webb and Webber,
2009).
In contrast, Spacebook has the objective of
keeping the user?s cognitive load low and prevent-
ing users from being distracted (perhaps danger-
ously so) from walking in the city (Kray et al,
2003). Also, it allows users to interleave the two
sub-tasks seamlessly and can keep entities dis-
cussed in both tasks in shared context (as shown
in Table 1).
3 Architecture
The architecture of the Spacebook system is
shown in figure 1. Our architecture brings to-
gether Spoken Dialogue Systems (SDS), Geo-
graphic Information Systems (GIS) and Question-
Answering (QA) technologies (Janarthanam et al,
2012). Its essentially a spoken dialogue system
(SDS) consisting of an automatic speech recog-
niser (ASR), a semantic parser, an Interaction
Manager, an utterance generator and a text-to-
speech synthesizer (TTS). The GIS modules in
this architecture are the City Model, the Visibility
Engine, and the Pedestrian tracker. Users commu-
nicate with the system using a smartphone-based
client app (an Android app) that sends users? po-
sition, pace rate, and spoken utterances to the sys-
tem, and delivers synthesised system utterances to
the user.
Figure 1: System Architecture
1661
3.1 Dialogue interface
The dialogue interface consists of a speech recog-
nition module, an utterance parser, an interaction
manager, an utterance generator and a speech syn-
thesizer. The Nuance 9 speech recogniser with
a domain specific language model was used for
speech recognition. The recognised speech is cur-
rently parsed using a rule-based parser into dia-
logue acts and semantic content.
The Interaction Manager (IM) is the central
component of this architecture, which provides
the user with navigational instructions, pushes PoI
information and manages QA questions. It re-
ceives the user?s input in the form of a dialogue
act (DA), the user?s location (latitude and longi-
tude) and pace rate. Based on these inputs and the
dialogue context, it responds with system output
dialogue act, based on a dialogue policy. The IM
initiates the conversation with a calibration phase
where the user?s initial location and orientation are
obtained. The user can then initiate tasks that in-
terest him/her. These tasks include searching for
an entity (e.g. a museum or a restaurant), request-
ing navigation instructions to a destination, ask-
ing questions about the entities in the City Model,
and so on. When the user is mobile, the IM iden-
tifies points of interest2 on the route proximal to
the user. We call this ?PoI push?. The user is en-
couraged to ask for more information if he/she is
interested. The system also answers adhoc ques-
tions from the user (e.g. ?Who is David Hume??,
?What is the Old College??, etc) (see section 3.4).
Navigation instructions are given in-situ by ob-
serving user?s position continuously, in relation
to the next node (street junction) on the current
planned route, and they are given priority if in con-
flict with a PoI push at the same time. Navigation
instructions use landmarks near route nodes when-
ever possible (e.g. ?When you reach Clydesdale
Bank , keep walking forward?). The IM also in-
forms when users pass by recognisable landmarks,
just to reassure them that they are on track (e.g.
?You will pass by Tesco on the right?). In addition
to navigation instructions, the IM also answers
users? questions concerning the route, his/her lo-
cation, and location of and distance to the various
entities. Finally, the IM uses the city model?s Vis-
ibility Engine (VE) to determine whether the des-
tination is visible to the user (see section 3.3).
2Using high scoring ones when there are many, based on
tourist popularity ratings in the City Model.
The shared spatial and dialogue context em-
ploys a feature-based representation which is up-
dated every 1 second (for location), and after every
dialogue turn. Spatial context such as the user?s
coordinates, street names, PoIs and landmarks
proximal to the user, etc are used by PoI push-
ing and navigation. The dialogue context main-
tains the history of landmarks and PoIs pushed,
latest entities mentioned, etc to resolve anaphoric
references in navigation and QA requests, and to
deliver coherent dialogue responses. The IM re-
solves anaphoric references by keeping a record
of entities mentioned in the dialogue context. It
also engages in clarification sub-dialogues when
the speech recognition confidence scores are low.
The IM stores the name and type information for
each entity (such as landmark, building, etc) men-
tioned in navigation instructions and PoI pushes.
Subsequent references to these entities using ex-
pressions such as ?the museum?, ?the cafe? etc
are resolved by searching for the latest entity of
the given type. Pronouns are resolved to the last
mentioned entity.
The IM also switches between navigation, PoI
push, and QA tasks in an intelligent manner by
using the shared context to prioritise its utterances
from these different tasks. The utterance genera-
tor is a Natural Language Generation module that
translates the system DA into surface text which is
converted into speech using the Cereproc Text-to-
Speech Synthesizer using a Scottish female voice.
The only changes made were minor adjustments
to the pronunciation of certain place names.
3.2 Pedestrian tracker
Urban environments can be challenging with lim-
ited sky views, and hence limited line of sight
to satellites, in deep urban corridors. There is
therefore significant uncertainty about the user?s
true location reported by GNSS sensors on smart-
phones (Zandbergen and Barbeau, 2011). This
module improves on the reported user position
by combining smartphone sensor data (e.g. ac-
celerometer) with map matching techniques, to
determine the most likely location of the pedes-
trian (Bartie and Mackaness, 2012).
3.3 City Model
The City Model is a spatial database containing
information about thousands of entities in the city
of Edinburgh (Bartie and Mackaness, 2013). This
data has been collected from a variety of exist-
1662
ing resources such as Ordnance Survey, Open-
StreetMap, Google Places, and the Gazetteer for
Scotland. It includes the location, use class, name,
street address, and where relevant other properties
such as build date and tourist ratings. The model
also includes a pedestrian network (streets, pave-
ments, tracks, steps, open spaces) which is used
by an embedded route planner to calculate min-
imal cost routes, such as the shortest path. The
city model also consists of a Visibility Engine
that identifies the entities that are in the user?s
vista space (Montello, 1993). To do this it ac-
cesses a digital surface model, sourced from Li-
DAR, which is a 2.5D representation of the city
including buildings, vegetation, and land surface
elevation. The Visibility Engine uses this dataset
to offer a number of services, such as determining
the line of sight from the observer to nominated
points (e.g. which junctions are visible), and de-
termining which entities within the city model are
visible. Using these services, the IM determines if
the destination is visible or not.
3.4 Question-Answering server
The QA server currently answers a range of def-
inition and biographical questions such as, ?Tell
me more about the Scottish Parliament?, ?Who
was David Hume??, ?What is haggis??, and re-
quests to resume (eg. ?Tell me more?). QA
is also capable of recognizing out of scope re-
quests, that is, either navigation-related questions
that can be answered by computations from the
City Model and dealt with elsewhere in the sys-
tem (?How far away is the Scottish Parliament??,
?How do I get there??), or exploration queries
that cannot be handled yet (?When is the can-
non gun fired from the castle??). Question clas-
sification is entirely machine learning-based using
the SMO algorithm (Keerthi et al, 1999) trained
over 2013 annotated utterances. Once the question
has been typed, QA proceeds to focus detection
also using machine learning techniques (Mikhail-
sian et al, 2009). Detected foci include possi-
bly anaphoric expressions (?Who was he??, ?Tell
me more about the castle?). These expressions
are resolved against the dialogue history and ge-
ographical context. QA then proceeds to a tex-
tual search on texts from the Gazetteer of Scotland
(Gittings, 2012) and Wikipedia, and definitions
from WordNet glosses. The task is similar to TAC
KBP 2013 Entity Linking Track and named en-
tity disambiguation (Cucerzan, 2007). Candidate
answers are reranked using a trained confidence
score with the top candidate used as the final an-
swer. These are usually long, descriptive answers
and are provided as a flow of sentence chunks that
the user can interrupt (see table 2). The Interaction
Manager queries the QA model and pushes infor-
mation when a salient PoI is in the vicinity of the
user.
?Edinburgh?s most famous and historic thoroughfare,
which has formed the heart of the Old Town since
mediaeval times. The Royal Mile includes Castlehill,
the Lawnmarket, the Canongate and the Abbey Strand,
but, is officially known simply as the High Street.?
Table 2: QA output: query on ?Royal Mile?
3.5 Mobile client
The mobile client app, installed on an Android
smartphone (Samsung Galaxy S3), connects the
user to the dialogue system using a 3G data con-
nection. The client senses the user?s location us-
ing positioning technology using GNSS satellites
(GPS and GLONASS) which is sent to the dia-
logue system at the rate of one update every two
seconds. It also sends pace rate of the user from
the accelerometer sensor. In parallel, the client
also places a phone call using which the user com-
municates with the dialogue system.
4 Baseline system
The baseline system chosen for evaluation was
Samsung S-Voice, a state-of-the-art commercial
smartphone speech interface. S-Voice is a Sam-
sung Android mobile phone app that allows a user
to use the functionalities of device using a speech
interface. For example, the user can say ?Call
John? and it will dial John from the user?s con-
tacts. It launches the Google Navigation app when
users request directions and it activates Google
Search for open ended touristic information ques-
tions. The Navigation app is capable of providing
instructions in-situ using speech. We used the S-
Voice system for comparison because it provided
an integrated state-of-the-art interface to use both
a navigation app and also an information-seeking
app using the same speech interface. Users were
encouraged to use these apps using speech but
were allowed to use the GUI interface when us-
ing speech wasn?t working (e.g. misrecognition of
local names). Users obtained the same kind of in-
1663
formation (i.e. navigation directions, descriptions
about entities such as people, places, etc) from the
baseline system as they would from our system.
However, our system interacted with the user us-
ing the speech modality only.
5 Experimental design
Spacebook and the baseline were evaluated in the
summer of 2012. We evaluated both systems with
17 subjects in the streets of Edinburgh. There
were 11 young subjects (between 20 and 26 years,
mean=22 ? 2) and 6 older subjects (between 50
and 71 years, mean=61 ? 11). They were mostly
native English speakers (88%). 59% of the users
were regular smartphone users and their mean
overall time spent in the city was 76 months. The
test subjects had no previous experience with the
proposed system. They were recruited via email
adverts and mail shots. Subjects were given a task
sheet with 8 tasks in two legs (4 tasks per leg).
These tasks included both navigation and tourist
information tasks (see table 3). Subjects used our
system for one of the legs and the baseline system
for the other and the order was balanced. Each leg
took up to 30 mins to finish and the total duration
including questionnaires was about 1.5 hours. Fig-
ure 2 shows the route taken by the subjects. The
route is about 1.3 miles long. Subjects were fol-
lowed by the evaluator who made notes on their
behaviour (e.g. subject looks confused, subject
looks at or manipulates the phone, subject looks
around, etc).
Subjects filled in a demographic questionnaire
prior to the experiment. After each leg, they filled
in a system questionnaire (see appendix) rating
their experience. After the end of the experi-
ment, they filled out a comparative questionnaire
and were debriefed. They were optionally asked
to elaborate on their questionnaire ratings. Users
were paid ?20 after the experiment was over.
6 Results
Subjects were asked to identify tasks that they
thought were successfully completed. The per-
ceived task success rates of the two systems were
compared for each task using the Chi square test.
The results show that there is no statistically sig-
nificant difference between the two systems in
terms of perceived task success although the base-
line system had a better task completion rate in
tasks 1-3, 5 and 6. Our system performed better in
Figure 2: Task route
tourist information tasks (4, 7) (see table 4).
Task Our system Baseline p
T1 (N) 77.7 100 0.5058
T2 (TI) 88.8 100 0.9516
T3 (N) 100 100 NA
T4 (TI) 100 87.5 0.9516
T5 (N+TI) 62.5 100 0.1654
T6 (N+TI) 87.5 100 0.9516
T7 (TI) 100 55.5 0.2926
T8 (N) 75.0 88.8 0.9105
Table 4: % Perceived Task success - task wise
comparison (N - navigation task, TI - Tourist In-
formation task)
The system questionnaires that were filled out
by users after each leg were analysed. These
consisted of questions concerning each system to
be rated on a six point Likert scale (1-Strongly
Disagree, 2-Disagree, 3-Somewhat Disagree, 4-
Somewhat Agree, 5-Agree, 6-Strongly Agree).
The responses were paired and tested using a
Wilcoxon Sign Rank test. Median and Mode for
each system and significance in differences are
shown in table 5. Results show that although
our system is not performing significantly better
than the baseline system (SQ1-SQ10 except SQ7),
users seem to find it more understanding (SQ7)
and more interesting to interact with (SQ11) than
the baseline. We grouped the subjects by age
group and tested their responses. We found that
the young subjects (age group 20-26), also felt that
1664
Leg 1
(Task 1) Ask the system to guide you to the Red Fort restaurant.
(Task 2) You?ve heard that Mary Queen of Scots lived in Edinburgh. Find out about her.
(Task 3) Walk to the university gym.
(Task 4) Near the gym there is an ancient wall with a sign saying ?Flodden Wall?. Find out what that is.
Leg 2
(Task 5) Try to find John Knox House and learn about the man.
(Task 6) Ask the system to guide you to the Old College. What can you learn about this building?
(Task 7) Try to find out more about famous Edinburgh people and places, for example, David Hume,
John Napier, and Ian Rankin. Try to find information about people and places that you are personally
interested in or that are related to what you see around you.
(Task 8) Ask the system to guide you back to the Informatics Forum.
Table 3: Tasks for the user
they learned something new about the city using it
(SQ12) (p < 0.05) while the elderly (age group
50-71) didn?t. We also found statistically signifi-
cant differences in smartphone users rating for our
system on their learning compared to the baseline
(SQ12).
Subjects were also asked to choose between the
two systems given a number of requirements such
as ease of use, use for navigation, tourist infor-
mation, etc. There was an option to rank the sys-
tems equally (i.e. a tie). They were presented with
the same requirements as the system questionnaire
with one additional question - ?Overall which sys-
tem do you prefer?? (CQ0). Users? choice of sys-
tem based on a variety of requirements is shown
in table 6. Users? choice counts were tested us-
ing Chi-square test. Significant differences were
found in users? choice of system for navigation
and tourist information requirements. Users pre-
ferred the baseline system for navigation (CQ2)
and our system for touristic information (CQ3) on
the city. Although there was a clear choice of sys-
tems based on the two tasks, there was no signifi-
cant preference of one system over the other over-
all (CQ0). They chose our system as the most in-
teresting system to interact with (CQ11) and that
it was more informative than the baseline (CQ12).
Figure 3 shows the relative frequency between
user choices on comparative questions.
7 Analysis
Users found it somewhat difficult to navigate using
Spacebook (see comments in table 7). Although
the perceived task success shows that our system
was able to get the users to their destination and
there was no significant difference between the
two systems based on their questionnaire response
on navigation, they pointed out a number of issues
and suggested a number of modifications. Many
Figure 3: Responses to comparative questions
users noted that a visual map and the directional
arrow in the baseline system was helpful for nav-
igation. In addition, they noted that our system?s
navigation instructions were sometimes not satis-
factory. They observed that there weren?t enough
instructions coming from the system at street junc-
tions. They needed more confirmatory utterances
(that they are walking in the right direction) (5
users) and quicker recovery and notification when
walking the wrong way (5 users). They observed
that the use of street names was confusing some-
times. Some users also wanted a route summary
before the navigation instructions are given.
The problem with Spacebook?s navigation pol-
icy was that it did not, for example, direct the
user via easily visible landmarks (e.g. ?Head to-
wards the Castle?), and relies too much on street
names. Also, due to the latency in receiving GPS
information, the IM sometimes did not present in-
structions soon enough during evaluation. Some-
times it received erroneous GPS information and
therefore got the user?s orientation wrong. These
problems will be addressed in the future version.
Some users did find navigation instructions use-
ful because of the use of proximal landmarks such
1665
Question B Mode B Median S Mode S Median p
SQ1 - Ease of use 4 4 5 4 0.8207
SQ2 - Navigation 4 4 5 4 0.9039
SQ3 - Tourist Information 2 3 4 4 0.07323
SQ4 - Easy to understand 5 5 5 5 0.7201
SQ5 - Useful messages 5 4 5 4 1
SQ6 - Response time 5 5 2 2 0.2283
SQ7 - Understanding 3 3 5 4 0.02546
SQ8 - Repetitive 2 3 2 3 0.3205
SQ9 - Aware of user environment 5 5 4 4 0.9745
SQ10 - Cues for guidance 5 5 5 5 0.1371
SQ11 - Interesting to interact with 5 4 5 5 0.01799
SQ12 - Learned something new 5 4 5 5 0.08942
Table 5: System questionnaire responses (B=Baseline, S=our system)
Task Baseline Our system Tie p-
Preferred Preferred value
CQ0 23.52 35.29 41.17 0.66
CQ1 35.29 29.41 35.29 0.9429
CQ2 64.70 0 35.29 0.004
CQ3 17.64 64.70 17.64 0.0232
CQ4 35.29 29.41 23.52 0.8187
CQ5 23.52 52.94 23.52 0.2298
CQ6 23.52 29.41 35.29 0.8187
CQ7 17.64 47.05 35.29 0.327
CQ8 29.41 23.52 47.05 0.4655
CQ9 29.41 52.94 17.64 0.1926
CQ10 47.05 29.41 23.52 0.4655
CQ11 5.88 76.47 17.64 0.0006
CQ12 0 70.58 29.41 0.005
Table 6: User?s choice on comparative questions
(CQ are the same questions as SQ but requesting
a ranking of the 2 systems)
as KFC, Tesco, etc. (popular chain stores). Some
users also suggested that our system should have
a map and that routes taken should be plotted on
them for reference. Based on the ratings and ob-
servations made by the users, we conclude that our
first hypothesis that Spacebook would be more ef-
ficient for navigation than the baseline because of
its speech-only interface was inconclusive. We be-
lieve so because users? poor ratings for Spacebook
may be due to the current choice of dialogue pol-
icy for navigation. It may be possible to reassure
the user with a better dialogue policy with just the
speech interface. However, this needs further in-
vestigation.
Users found the information-search task inter-
esting and informative when they used Spacebook
(see sample user comments in table 8). They
also found push information on nearby PoIs un-
expected and interesting as they would not have
found them otherwise. Many users believed that
this could be an interesting feature that could help
tourists. They also found that asking questions and
finding answers was much easier with Spacebook
compared to the baseline system, where some-
times users needed to type search keywords in.
Another user observation was that they did not
have to stop to listen to information presented
by our system (as it was in speech) and could
carry on walking. However, with the baseline sys-
tem, they had to stop to read information off the
screen. Although users in general liked the QA
feature, many complained that Spacebook spoke
too quickly when it was presenting answers. Some
users felt that the system might lose context of the
navigation task if presented with a PoI question.
In contrast, some others noted Spacebook?s ability
to interleave the two tasks and found it to be an
advantage.
Users? enthusiasm for our system was observed
when (apart from the points of interest that were
in the experimental task list) they also asked spon-
taneous questions about James Watt, the Talbot
Rice gallery, the Scottish Parliament and Edin-
burgh Castle. Some of the PoIs that the system
pushed information about were the Royal College
of Surgeons, the Flodden Wall, the Museum of
Childhood, and the Scottish Storytelling Centre.
Our system answered a mean of 2.5 out of 6.55
questions asked by users in leg 1 and 4.88 out of
8.5 questions in leg 2. Please note that an utter-
ance is sent to QA if it is not parsed by the parser
and therefore some utterances may not be legit-
mate questions themselves. Users were pushed a
mean of 2.88 and 6.37 PoIs during legs 1 and 2.
There were a total of 17 ?tell me more? requests
requesting the system to present more information
(mean=1.35 ? 1.57).
Evaluators who followed the subjects noted that
the subjects felt difficulty using the baseline sys-
tem as they sometimes struggled to see the screen
1666
1. ?It?s useful when it says ?Keep walking? but it should say it more often.?
2. ?[Your system] not having a map, it was sometimes difficult to check how aware it was of my environment.?
3. ?[Google] seemed to be easier to follow as you have a map as well to help.?
4. ?It told me I had the bank and Kentucky Fried Chicken so I crossed the road because I knew it?d be somewhere over
beside them. I thought ?OK, great. I?m going the right way.? but then it didn?t say anything else. I like those kind of
directions because when it said to go down Nicolson Street I was looking around trying to find a street sign.?
5. ?The system keeps saying ?when we come to a junction, I will tell you where to go?, but I passed junctions and it
didn?t say anything. It should say ?when you need to change direction, I will tell you.??
6. ?I had to stop most of the times for the system to be aware of my position. If walking very slowly, its awareness of
both landmarks and streets is excellent.?
Table 7: Sample user comments on the navigation task
1. ?Google doesn?t *offer* any information. I would have to know what to ask for...?
2. ?Since many information is given without being asked for (by your system), one can discover new places and
landmarks even if he lives in the city. Great feature!!?
3. ?I didn?t feel confident to ask [your system] a question and still feel it would remember my directions?
4. ?Google could only do one thing at a time, you couldn?t find directions for a place whilst learning more.?
5. ?If she talked a little bit slower [I would use the system for touristic purposes]. She just throws masses of information
really, really quickly.?
Table 8: Sample user comments on the tourist information task
in bright sunlight. They sometimes had difficulty
identifying which way to go based on the route
plotted on the map. In comparison, subjects did
not have to look at the screen when they used
our system. Based on the ratings and observa-
tions made by the users about our system?s tourist
information features such as answering questions
and pushing PoI information, we have support for
our second hypothesis: that users find a dialogue
interface which integrates question-answering and
navigation within a shared context to be useful for
finding information about entities in the urban en-
vironment.
8 Future plans
We plan to extend Spacebook?s capabilities to ad-
dress other challenges in pedestrian navigation and
tourist information. Many studies have shown
that visible landmarks provide better cues for nav-
igation than street names (Ashweeni and Steed,
2006; Hiley et al, 2008). We will use visible
landmarks identified using the visibility engine to
make navigation instructions more effective, and
we plan to include entities in dialogue and visual
context as candidates for PoI push, and to imple-
ment an adaptive strategy that will estimate user
interests and push information that is of interest
to them. We are also taking advantage of user?s
local knowledge of the city to present navigation
instructions only for the part of the route that the
user does not have any knowledge of. These fea-
tures, we believe, will make users? experience of
the interface more pleasant, useful and informa-
tive.
9 Conclusion
We presented a mobile dialogue app called Space-
book to support pedestrian users in navigation
and tourist information gathering in urban envi-
ronments. The system is a speech-only interface
and addresses navigation and tourist information
in an integrated way, using a shared dialogue con-
text. For example, using the navigational context,
Spacebook can push point-of-interest information
which can then initiate touristic exploration tasks
using the QA module.
We evaluated the system against a state-of-the-
art baseline (Samsung S-Voice with Google Navi-
gation and Search) with a group of 17 users in the
streets of Edinburgh. We found that users found
Spacebook interesting to interact with, and that
it was their system of choice for touristic infor-
mation exploration tasks. These results were sta-
tistically significant. Based on observations and
user ratings, we conclude that our speech-only
system was less preferred for navigation and more
preferred for tourist information tasks due to fea-
tures such as PoI pushing and the integrated QA
module, when compared to the baseline system.
Younger users, who used Spacebook, even felt that
they learned new facts about the city.
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
1667
agreement no. 270019 (SPACEBOOK project).
References
K. B. Ashweeni and A. Steed. 2006. A natural
wayfinding exploiting photos in pedestrian naviga-
tion systems. In Proceedings of the 8th conference
on Human-computer interaction with mobile devices
and services.
P. Bartie and W. Mackaness. 2006. Development
of a speech-based augmented reality system to sup-
port exploration of cityscape. Transactions in GIS,
10:63?86.
P. Bartie and W. Mackaness. 2012. D3.4 Pedestrian
Position Tracker. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
P. Bartie and W. Mackaness. 2013. D3.1.2 The Space-
Book City Model. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating Instructions in Vir-
tual Environments (GIVE): A challenge and evalua-
tion testbed for NLG. In Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation
in Natural Language Generation.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings
of EMNLP-CoNLL.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using
Natural Language Generation for Navigational As-
sistance. In Proceedings of ACSC2003, Australia.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hierar-
chical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Gener-
ation. In Proc. of ACL.
B. Gittings. 2012. The Gazetteer for Scotland -
http://www.scottish-places.info.
H. Hiley, R. Vedantham, G. Cuellar, A. Liuy,
N. Gelfand, R. Grzeszczuk, and G. Borriello. 2008.
Landmark-based pedestrian navigation from collec-
tions of geotagged photos. In Proceedings of the
7th Int. Conf. on Mobile and Ubiquitous Multimedia
(MUM).
S. Janarthanam and O. Lemon. 2011. The GRUVE
Challenge: Generating Routes under Uncertainty in
Virtual Environments. In Proceedings of ENLG.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, T. Dalmas, and J. Goetze. 2012. Integrat-
ing location, visibility, and Question-Answering in
a spoken dialogue system for Pedestrian City Explo-
ration. In Proc. of SIGDIAL 2012, S. Korea.
H. Kashioka, T. Misu, E. Mizukami, Y. Shiga,
K. Kayama, C. Hori, and H. Kawai. 2011. Multi-
modal Dialog System for Kyoto Sightseeing Guide.
In Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference.
S.S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 1999. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 3:637?649.
J. Ko, F. Murase, T. Mitamura, E. Nyberg, M. Tateishi,
I. Akahori, and N. Hataoka. 2005. CAMMIA: A
Context-Aware Spoken Dialog System for Mobile
Environments. In IEEE ASRU Workshop.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003.
Presenting Route Instructions on Mobile Devices.
In Proceedings of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging
IT research in the framework of a tourist information
system. In Information and Communication Tech-
nologies in Tourism 2000, pages 15?27. Springer.
A. Mikhailsian, T. Dalmas, and R. Pinchuk. 2009.
Learning foci for question answering over topic
maps. In Proceedings of ACL 2009.
D. Montello. 1993. Scale and multiple psychologies
of space. In A. U. Frank and I. Campari, editors,
Spatial information theory: A theoretical basis for
GIS.
M. Raubal and S. Winter. 2002. Enriching wayfinding
instructions with local landmarks. In Second Inter-
national Conference GIScience. Springer, USA.
C.J. Shroder, W. Mackaness, and B. Gittings. 2011.
Giving the Right Route Directions: The Require-
ments for Pedestrian Navigation Systems. Transac-
tions in GIS, pages 419?438.
N. Webb and B. Webber. 2009. Special Issue on Inter-
active Question Answering: Introduction. Natural
Language Engineering, 15(1):1?8.
P. A. Zandbergen and S. J. Barbeau. 2011. Posi-
tional Accuracy of Assisted GPS Data from High-
Sensitivity GPS-enabled Mobile Phones. Journal of
Navigation, 64(3):381?399.
1668
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 19?27,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Click or Type: An Analysis of Wizard?s Interaction for Future Wizard
Interface Design
Srinivasan Janarthanam
1
, Robin Hill
2
, Anna Dickinson
2
, Morgan Fredriksson
3
1
School of Mathematical and Computer Sciences, Heriot-Watt University
2
School of Informatics, University of Edinburgh
3
Liquid Media AB, Stockholm
sc445@hw.ac.uk
Abstract
We present an analysis of a Pedestrian
Navigation and Information dialogue cor-
pus collected using a Wizard-of-Oz inter-
face. We analysed how wizards preferred
to communicate to users given three differ-
ent options: preset buttons that can gen-
erate an utterance, sequences of buttons
and dropdown lists to construct complex
utterances and free text utterances. We
present our findings and suggestions for
future WoZ design based on our findings.
1 Introduction
Wizard-of-Oz environments (WoZ) have been be-
come an essential tool for collecting and studying
dialogue between humans pretending to be ma-
chines and human users in various domains. It is
an effective way to collect dialogues between real
users and dialogue systems before actually imple-
menting the dialogue system. In this framework,
participants interact with an expert human oper-
ator (known as ?Wizard?) who is disguised as a
dialogue system. These Wizards replace one or
more parts of the dialogue system such as speech
recognition, natural language understanding, di-
alogue management, natural language generation
modules and so on. Real users interact differently
with humans and computers. While their expecta-
tions with human interlocutors are high and varied,
they are ready to adapt and ?go easy? on comput-
ers during interaction (Pearson et al., 2006). So, in
a WoZ framework, the conversation between real
users and the Wizards (pretending to be dialogue
systems) are of an appropriate type to be used for
dialogue system design and not as complex as in
human-human conversation.
In order to provide a speedy response, most
WoZ systems are designed in such a way that re-
sponses are hard wired to buttons so that they can
be sent to the synthesizer at the touch of a button.
However, in order to handle unexpected situations,
most WoZ interfaces also have a free text interface
that allows the Wizard to type any text to be syn-
thesised by the synthesizer. Are free text interfaces
used only under unexpected situations? In this pa-
per, we analyse how free text interfaces are used
by Wizards in a pedestrian tourist navigation and
information dialogue and discuss how the results
of our analysis be used to inform future WoZ de-
signs. These dialogues were collected as a part of
SpaceBook EU FP7 project.
In Section 2, we present previous work in WoZ
interfaces and the domain of pedestrian navigation
and information. We then present our WoZ setup
and data collection in Section 3 and 4. In Section
5, we present our analysis of the corpus, issues and
suggestions in Sections 6 and 7.
2 Related work
Wizard-of-Oz (WoZ) frameworks have been used
since early 90s in order to collect human-computer
dialogue data to help design dialogue systems
(Fraser and Gilbert, 1991). WoZ systems have
been used extensively to collect data to learn di-
alogue management policies (Rieser and Lemon,
2011) and information presentation strategies
(Demberg et al., 2011).
Pedestrian navigation and information systems
is a domain of interest to many mobile phone
applications. Applications such as Siri, Google
Maps Navigation, or Sygic deal with the task of
navigation while TripAdvisor, Triposo, etc . focus
on the tourist information problem. Additionally,
several research prototypes have been built to gen-
erate navigation instructions (Bartie and Mack-
aness, 2006; Shroder et al., 2011) and to have con-
versations with tourists (Janarthanam et al., 2013).
WoZ experiments enable the collection of realis-
tic data to assist in the development and testing of
these systems.
19
Figure 1: Wizard of Oz interface - Google Satellite Map and StreetView
3 WoZ setup
The wizard interface consisted a browser window
showing a Google Map and Street View with the
Tourists position. Google StreetView showed
the Tourist?s point of view (see Figure 1). The
Wizard was able to communicate information to
the Tourist in three different ways in the Wizard
Response Panel (see Figure 2):
Hot buttons: By clicking on one of several
buttons with commonly used phrases (e.g. ?OK.
I?ll suggest a route for you?, ?You want to cross
the road whenever you can?, ?Would you like
further information about that??). Buttons were
organised thematically in sections such as: con-
firmations, ways of asking the Tourist to repeat
what they had said, ways to indicate to the Tourist
that the Wizard was doing something and they
should wait (?Just a moment, please?, ?I?m just
finding that out for you now? and ?Apologies for
the delay?) and directions. The range of choices
available via the buttons (there were nine different
confirmations) was intended to allow the Wizard
to mimic the variability of human speech; they
were grouped to facilitate rapid identification and
selection.
Sequences: By generating text from a sequence
of drop-down menus, e.g. (where items in square
brackets are drop-down lists): ?You want to take
the [count] [pathway] on your [direction]).
Free text: By typing free text into a text editor.
Pre-entered phrases for Hot Buttons were se-
lected following two previous Wizard of Oz exper-
iments where the Tourist and the Wizard commu-
nicated by voice; common expressions used dur-
ing these sessions were summarised and presented
on an initial evaluation interface which was evalu-
ated with 15 dyads. Results from that experiment
fed into the WoZ interface above.
At the bottom right of the screen, there was a
scrollable record of the Wizard?s output in case
the participant needed to confirm what had been
sent to the Tourist. Finally, there was a selection
of system comments the Wizard could make, for
example to note system problems such as prob-
lems hearing the Tourist. This information was
recorded by the system but not sent to the Tourist.
Additionally, screen capture software was used to
record all the on-screen interaction. As a back-up,
the lab was videoed on DV cassette using a tripod-
mounted camcorder.
Instructions to participants were developed to
encourage participants (i.e. playing the role of
Tourists) to solve problems without directing them
too much. e.g. ?You?ve heard a story about a
statue of a dog that you think is nearby and would
like to take a photo of the dog and perhaps learn
a little more about the story.?, ?You have arranged
to have lunch with a friend in a nearby pub. You
20
Figure 2: Wizard of Oz interface - Wizard response panel
can?t remember the exact name but you are sure it
had the word ?Bell? in the title.?
The Tourist was equipped with an Android mo-
bile phone (Samsung Galaxy Note) and headset.
The phone ran a custom-built app that sent live
GPS, satellite and accelerometer data back to the
WoZ system while receiving the Wizards text mes-
sages and converting them to speech. As a back-
up, and to ensure the reliability of the position-
ing data, a GPS logging application (My Tracks)
also recorded the data every two seconds on the
phone. Time-stamping within the files permits off-
line synchronisation with the speech data.
4 Data collection
Participants were enrolled using an events organ-
ising website called EventBrite
1
. Two participants
attended each experimental session and were as-
signed to one of two roles: the Tourist or the Wiz-
ard. At the end of the experiment each received
?10. Ten dyads (twenty people) completed the
experiment. They were aged between 19 and 26
(mean 22), and had lived in Edinburgh between
0.7 and 10 years (mean 2.9). 8 were male, and 12
female.
After participants had arrived at the lab, they
signed a consent form and provided demographic
1
www.eventbrite.com
information (age, sex, and length of time in Ed-
inburgh). The task descriptions were handed out
and roles were assigned. The Wizard was given
supplementary information about some of the lo-
cations and Google Map print-outs, but was in-
structed to make up any answers to questions
asked by the Tourist if necessary.
After an initial equipment test and training,
the Tourist dialled a standard Edinburgh landline
number on the mobile phone which connected to a
Skype account and the experiment began. If the
call dropped, the Tourist would redial and con-
tinue. There was a basic set of tasks assigned to
the Tourist, but they were encouraged to expand
and adapt this and were free to ask any tourist or
navigation-based questions that they thought of on
the way.
The Tourist traversed Edinburgh on their own;
the Wizard and experimenter remained in a labo-
ratory. The Wizard used GPS information and dia-
logue with the Tourist to establish location. For the
Wizard, the Tourist?s view had to be reconstructed
using the display software available. These di-
alogue sessions ranged between 41:56 to 66:43
minutes. The average dialogue duration (accord-
ing to the transcriber) for the 10 dyads was 51min
46s.
Please note that for each run, a new pair of Wiz-
21
ard and Tourist were used. Wizards were not re-
tained to do more than one run because we wanted
to collect data from a variety of human wizards in
order to study variations in how each wizard dealt
with the navigation task.
5 Corpus analysis
We analysed the corpus collected based on the
three types of response generation mechanisms:
hot buttons, sequences and free text, to under-
stand their relative utility. We wanted to explore
whether pre-configured text was used when avail-
able, or whether the user?s interaction with the
pre-configured and free text sections of the inter-
face were influenced by other considerations than
availability.
Analysis showed that buttons corresponding
to preset utterances were used only 33% (+/- 14)
of the time. Although wizards had the option of
constructing complex utterances using a sequence
of drop down lists, they only used such sequences
9% (+/- 9) of the time. 58% (+/-19) of Wizard
utterances were generated using the free text inter-
face. This may imply that the buttons did not offer
what the Wizards wanted to say; in which case, we
would anticipate that their self-created utterances
would be very different from those pre-configured.
Individual differences: Use of the button inter-
face varied between Wizards, with some using it
very rarely and others depending on it when it pro-
vided a feature they required. The highest was
82.7% while the lowest use of free text was 31.7%.
Table 1 shows that 6 out of 10 Wizards used the
free text interface more than 60% of the time. It
is likely that these differences were due to individ-
ual variations such as speed of typing and comfort
with using an array of buttons.
Usage of free text interface Wizard count
Below 30% 0
30-40% 3
40-50% 1
50-60% 0
60-70% 3
70-80% 1
80-90% 2
Table 1: Usage of free text interface
As an example of these individual differences,
one Wizard used the button-press interface only
once during the first navigation task (to ask ?What
can you see at the moment??), choosing to direct
the Tourist almost exclusively through use of the
free text interface. By contrast, of the twelve Wiz-
ard utterances in another session?s initial naviga-
tion task, only two were free text. It is interesting
to note, however, that the Tourist commented ?I?ve
a feeling (the Wizard) is laughing at me right now.?
5.1 Hot button interface
We analysed how frequently each hot button in the
interface was used by Wizards. We also counted
how frequently the same text as the buttons was
generated using the free text interface. This will
show us if Wizards tend to type the same text that
can effectively be generated at the push of a hot
button. The following table shows the frequency
of each hot button used over the 10 dialogues that
we analysed.
There were forty buttons in total. Two initial
buttons intended to be used at the start of the ex-
periment or when the call was restarted after a
problem: ?Okay, we are ready to go. Please pre-
tend to have just dialed Space Book and say hello.?
and ?Hello, SpaceBook speaking.? (These were
used 29 times) and two intended for the end of
the call: ?Thank you for using SpaceBook? and
?Goodbye? (10 times). Table 2 shows the fre-
quency of usage for other hot buttons.
Utterance type Frequency
Confirmation (e.g. Yes, Okay, Certainly) 168
Navigation (e.g. ?Keep going straight ahead?) 114
Filler (e.g. ?Just a moment please?) 60
Repeat request (e.g. ?Sorry, could you repeat
that please??) 34
Visual checks (?Can you see it yet??/
?What can you see at the moment??) 32
Offer of further information/ help 30
References (e.g. ?According to Wikipedia?) 20
Negation (?No?, ?No, that?s wrong?) 18
Failure (?I?m afraid I don?t know the
answer to that?) 8
Table 2: Usage of Hot Buttons
The above table presents a Zipfian curve with
some utterances such as ?Okay?, ?Keep going
straight ahead? having high frequency and some
utterances such as ?I?m afraid I don?t know the an-
swer to that,? ?I couldn?t understand that, sorry?
with extremely low frequency. Even the highest
frequency utterance, ?Okay? was only used about
5 times per session on average. This does not
mean that the Wizard acknowledged the subject at
such low frequency but, as the analysis below in-
dicates, decided to acknowledge the user with free
22
text-generated utterances.
5.2 Free text utterances
We analysed the free text utterances generated by
the Wizards. This analysis, we believe, could
show us how to build better Wizard interfaces for
collecting dialogue data for pedestrian navigation.
First, we counted the number of free text utter-
ances that duplicated Hot Button text. Then, we
analysed the other utterances generated using the
free text interface.
Table 3 presents the frequency of utterances that
were generated using the free text interface but
were the same as hot button text. The table shows
that even though there are hot buttons for utter-
ances such as ?Yes?, ?Sorry?, Wizards tended to
type them into the free text interface. In some
cases these words were followed by a more com-
plex utterance which the Wizard had chosen to de-
liver as a single statement (e.g. ?Yes, that?s the
way to go.?, ?no, you should turn around?), and
second, these utterances are short and could easily
be typed rather than searching for the correspond-
ing hot button. Also, Wizards sometimes used
alternative spellings for words such as ?Okay?
which could be produced using a hot button. The
word ?Ok? was used 15 times in 10 sessions.
Text Frequency
Yes 45
Sorry 21
No 21
Take the next left 4
No problem 3
Certainly 2
Thank you 1
Table 3: Usage of Free Text for utterances same as
Hot Buttons
In addition, Wizards use free text to generate
utterances that are paraphrases of hot button utter-
ances, such as:
? ?Keep going?, ?Just keep walking?, etc
? ?Great?, ?Excellent?, etc
? ?One moment?, ?Wait a second please?, etc
? ?Of course?
? ?Okay cool?
These analyses imply that free text is not ac-
cessed only in the last resort because the user can-
not find the hot button that says what they?d like
to say. Clearly, the interaction is more complex
and concerns both speed (the contrast of typing a
short utterance such as ?Yes? compared with the
time needed to discover the correct button on a dis-
play and navigate to it with a mouse) and the user?s
imposition of their own identity on the conversa-
tion; where the hot button interface offered sev-
eral confirmatory utterances, users often used their
own (e.g. ?Great, ?Excellent?, ?Cool?), utterances
which were, presumably, part of the way these
Wizards more normally interacted with peers.
In this section, we present the other types of
utterances Wizards generated using the free text
interface.
1) Check user?s location:
Wizards asked several free text questions to check
where the user was, given that the positioning
system on smartphones was not entirely accurate.
They framed most questions as yes/no check
questions and enriched them with situational cues
(e.g.?Is the Pear Tree on your right??, ?Have
you reached the Royal Mile yet??, ?Can you see
Nicolson Square??, ?Have you passed the primary
school on your left??).
2) Informing user?s location:
Wizards sometimes informed users of their loca-
tion. e.g. ?This is West Nicolson Street?.
3) Complex navigation instructions:
Using the free text interface, Wizards generated
a variety of complex navigation instructions that
were not covered by the hot buttons. These include
instructions where the subject was asked to carry
out two instructions in sequence (e.g. ?Turn left,
and keep walking until you get to Chapel Street?),
orienting the user (e.g. ?You want the road on your
right?, ?Please go back in the direction you came
from?), signaling to the user that he/she was walk-
ing in the wrong direction (e.g. ?You?re going the
wrong way?), a priori instructions to destination
(e.g. ?To get there you will need to keep going
up the Royal Mile. Then turn left at the junction
between North and South Bridge. Walk up South
Bridge, and it will change to Nicolson Street. Sur-
geon?s Hall will be on the left hand side.?).
Some navigation instructions were complex be-
cause they were not general instructions but direct
responses to the Tourist?s question. One exam-
ple of this was by Dynamic Earth (dyad 07) when
23
the Wizard told the Tourist to follow a footpath.
Tourist: ?One of the footpaths banks to the right,
and the other goes straight forward. Which one??,
the Wizard answered: ?You want the one that is
straight forward.?
The navigation directions on hot buttons were
necessarily very general (e.g. Keep going straight
ahead/ Take the next left) and Wizards frequently
used the free text to enrich the directions and make
them more specific, e.g. (dyad 09) ?Walk down
Crichton Street towards the Mosque.? In the ini-
tial navigation task, this Wizard used the free text
interface 7 times, and the navigation hot buttons
only 4 times. Each segment of free text enriched
the interaction by providing specific navigational
information, so where the Wizard could have se-
lected the hot button for ?Keep going straight?,
instead she chose to add value to the interaction
through the use of place names and typed, ?Con-
tinue straight onto West Richmond Street?.
A similar pattern can be seen in the interaction
in dyad 10 where the Wizard used the free text op-
tion to navigate the Tourist according to objects
in his environment. e.g. ?Turn right at the traffic
lights? and ?Walk straight down past the Bingo on
your left.?. Of the 22 Wizard utterances in the first
navigation task in the dyad, only 5 were hot but-
tons. 14 were navigation instructions, of which 3
were button-presses and one (?Walk straight on?)
paraphrased an existing button. The Tourist got
lost in this task, so there was also some checking
on his location.
These are not isolated examples. In total, over
the ten dyads, 308 utterances from the total 927
free text utterances were Wizards ?enriching? their
navigation directions by adding contextual cues,
most commonly the name of the street or a land-
mark to help situate the Tourist. For example,
?You can reach it by turning right down Holyrood
Road at the junction.?, ?Please head towards the
Mosque?.
Although 33% of overall free text utterances
were enriched navigation instructions, this over-
all pattern varied depending on the dyad, ranging
from dyad 03 where 62.5% were enriched instruc-
tions, to dyad 08, where only 8% were enriched.
These value-added uses of the free text suggest
that the addition of contextual cues is regarded as
important by the individuals acting as Wizards.
An improved WoZ interface might seek to support
such utterances.
4) Reassuring user:
Wizards presented information such as landmarks
users can see as they walk along to reassure
them that they are on the right track (e.g. ?You
will pass Richmond Place on your left?, ?You
will walk past the Canongate Kirk on your right
beforehand?).
5) Informing time/distance to destination:
Wizards presented how long it will take to reach
the destination to set the right expectation in the
user?s mind (e.g. ?It will be about a two minute
walk?, ?the gym is 200 metres along this road on
your right?).
6) Providing destination information:
Wizards provided information about the location
of destination in reference to the user (e.g. ?And
Bonsai Bar Bistro will be on the left, just before
you reach The Pleasance?, ?The Museum of
Edinburgh will be on the left?) or other landmarks
(e.g. ?The Scottish Parliament is next to Our
Dynamic Earth?, ?The entrance is on the other
side?). Note that this interaction, too, is normally
enriched by situational cues.
7) Informing destinations that match search
criteria:
Some tasks presented to subjects did not specify
the actual name of the destination. Hence when
they asked the Wizard for a matching destination,
Wizards used free text to suggest destinations
that match the search criteria (e.g. ?There is
a restaurant called Bonsai Bistro?, ?There are
three museums to visit. They are Museum of
Edinburgh, People?s Story Museum, and Museum
of Childhood?).
8) Check if destination reached and identified:
Wizards checked whether users had reached their
destination by asking them to confirm if they had
(e.g. ?Have you reached it??, ?Have you found
the sports centre??). The hot button ?Can you see
it yet?? covered this functionality, but once more,
free text allowed the user to increase situational
specificity by identifying the target.
9) Additional information about landmarks:
Wizards presented additional information about
landmarks such as its name (?the hill besides par-
24
liament is in fact 2 hills, the rocky cliffs you can
see are called crags?, ?behind that is arthurs seat?),
the year it was built/opened (e.g. ?it was opened
in 1999?), what it functions as (e.g. ?offices for a
newspaper publisher?).
In some cases such free text utterances were
produced in response to questions asked by
Tourists. For example, when the Tourist of dyad
05 passed the Fringe office, they asked, ?Do you
know what dates the Fringe is on this year??.
The Wizard used free text to answer the question.
Later in the same experiment, the Tourist identi-
fied Vodka Rev as a landmark (?Down past Vodka
Rev??) and the Wizard responded with free text
about the landmark: ?Vodka Rev does half price
food on Mondays.?.
10) Signalling connection problems:
Wizards informed users when they lost the user?s
GPS signal (e.g. ?hold on 1 second, gps connec-
tion gone?) and to establish contact and check
user?s attention (e.g. ?hello??, ?I can?t hear you at
the moment?).
Further, some Wizards used the free text to hu-
manise the person-to-person element of the inter-
action. They would chat to Tourists, make jokes
(?I cannot answer rhetorical questions, as I am
both a computer and aware they are not meant to
be answered.?) and in one case, invite the Tourist
out for a drink.
6 Issues with free text
As one can imagine, there are issues with free text
utterances generated by Wizards.
Spelling:
Several words used in free text utterances were
misspelled. e.g. ?roundabaout?, ?entrace?, ?ple-
sae?, ?toewards?, ?You want ot cross the roD?)
etc. These ranged from 0 to 13 errors per session
with a mean of 3.6 (+/- 3.9) errors per session.
Adjacent words were sometimes joined together
(e.g. ?atyour?, ?upahead?, etc) and sometimes
incorrectly segmented with space (e.g. ?collection
sof?, ?hea ryou?, etc). Some entity names were
misspelled as well (e.g. ?Critchon?, ?Dyanmic
Earth?, ?artthurs seat?, etc). Spelling errors can
reflect poorly when the utterances are synthesized
and the misspelled words mispronounced.
Syntax:
We also found a few syntactic errors in utterance
construction (e.g. ?Continue going Chambers
street?). Similar to spelling errors, utterances with
improper syntax can sound weird to the Tourist
and could lead to confusion and misunderstanding
instructions.
Incorrect entity names:
Wizards did not always get street names correct,
e.g. in dyad 02, the Wizard directed the Tourist to
?Nicholas Square? and the Tourist needed to seek
clarification that he meant ?Nicolson Square?.
Time and effort:
It takes time and can slow the interaction with the
user, leading to issues like interruptions and the
flow of the conversation being upset.
7 Suggestions
Based on the above analysis, we propose a list
of suggestions to build a better Wizard of Oz in-
terface for collecting dialogues concerning pedes-
trian navigation and exploration. The objective of
the WoZ system is to provide an effective inter-
face to Wizards to interact with Tourists while pre-
tending to be dialogue systems. One of the impor-
tant requirements is that Wizards should be able
to generate context appropriate utterances quickly
to make the dialogue appear more natural with-
out unnecessary lag between a user?s requests and
the system?s responses. Hot buttons are designed
so that the utterance can generated at the push of
a button. However as our data shows, Wizards
tended to use the free text interface about 60% of
the time.
While there are situations in which free text is
necessary, in general it risks slowing the interac-
tion and potentially confusing the Tourist when
words are mis-spelled or omitted. In addition,
supporting the Wizard more effectively with an
improved WoZ interface is likely to permit them
to spend more time supporting and informing
the Tourist. Free text utterances can lead to slow
system response and there is therefore a need to
find a compromise between the two. We have the
following suggestions:
1. More hot buttons:
Some utterances generated using the free text in-
terface could not be generated using the hot but-
25
tons or the sequences. These include reassuring
users, informing them of the time/distance to des-
tination, informing them of search results, etc.
While free text is a useful interface to Wizards to
generate unforeseen utterances, more hot buttons
covering new functionality can be faster to use.
However, introducing additional hot buttons
would add complexity to the interface, which is
likely to have the undesireable effect of encourag-
ing users to avoid the cluttered display in favour if
the straightforward free text interface. One partial
solution is to ensure that buttons are organised and
grouped in ways that are intuitive for the Wizard.
This, and the optimum number of buttons for the
display, should be investigated experimentally.
2. Multi functional hot buttons:
Some free text utterances were complex versions
of simple utterances that were already covered by
hot buttons. For instance, utterances like ?Keep
going up Nicolson Street? or ?Keep walking until
you get to Chapel Street? can be seen as a version
of ?Keep going straight ahead? but with some ap-
pended information (i.e. street name, landmark).
The interface could be designed so that hot
button utterances could be modified or appended
with more information. For example, a single
click the hot button might send the utterance to
the free text editor, allowing the Wizard to add
more information, whereas a double click would
send the utterance directly to the TTS.
3. Spell check, grammar check and auto cor-
rection:
To ensure that the speech synthesizer works as ef-
fectively as possible, the utterances typed in the
free text editor must be correctly spelled. One so-
lution to the frequent mis-spelling made by Wiz-
ards typing at speed is to automatically spell check
and correct text typed in the free text interface.
Ensuring that text is correct would reduce the
risk of the speech synthesizer mispronouncing
misspelt names and words. Similarly, a grammar
check would mean that the synthesised utterances
felt more natural.
Since there is the danger of an automatic spell
checker making mistakes, the spell check and cor-
rection should happen when the Wizard finishes
typing a word or utterance and the auto corrected
word or utterance be shown to the Wizard before
it is sent to the TTS.
4. Autocomplete:
Autocomplete is a feature that predicts the next
words the user intends to type based on those
already typed. It is currently used by search
engines such as Google to complete users? search
queries based on their search history and profile.
A similar feature that can complete utterances
taking into account the user?s request, dialogue
history, and the spatial context could speed up the
response time of the Wizard.
5. Location aware WoZ interface:
The WoZ system could be ?aware? of the user?s
surroundings. Such a solution might enable the
interface to have dynamically changing buttons,
so when the user is headed up Nicolson Street,
the ?Keeping going? button could have Nicolson
Street on it. Information about entities around
the user can also be assigned to hot buttons dy-
namically. However, hot buttons with dynamically
changing labels and functionality could be cogni-
tively overloading to Wizards.
Of course, the addition of such functionality
to the WoZ interface must be carefully evaluated.
A dynamic interface may be harder to learn, and
increasing the number of buttons may, counter-
intuitively, mean that users are less likely to select
hot buttons because the effort to scan the array of
buttons is greater than the effort needed to type ut-
terances, particularly short ones, into a free text
box.
8 Conclusion
In this paper, we presented a Wizard of Oz system
that was used to collect dialogues in the domain of
pedestrian navigation and information. We anal-
ysed the corpus collected to identify how Wizards
preferred to interact with the pedestrian users and
why. We identified issues with free text interfaces
that was used by majority of Wizards and sug-
gested improvements towards future Wizard inter-
face design.
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
agreement no. 270019 (SPACEBOOK project).
26
References
P. Bartie and W. Mackaness. 2006. Development of a
speech-based augmented reality system to support explo-
ration of cityscape. Transactions in GIS, 10:63?86.
Vera Demberg, Andi Winterboer, and Johanna D. Moore.
2011. A strategy for information presentation in spo-
ken dialog systems. Comput. Linguist., 37(3):489?539,
September.
N. Fraser and G. N. Gilbert. 1991. Simulating speech sys-
tems. Computer Speech and Language, 5:81?99.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
J. Pearson, J. Hu, H. P. Branigan, M. J. Pickering, and
C. Nass. 2006. Adaptive language behavior in HCI: how
expectations and beliefs about a system affect users? word
choice. In Proceedings of the SIGCHI conference on Hu-
man Factors in computing systems, Montral.
V. Rieser and O. Lemon. 2011. Learning and Evaluation
of Dialogue Strategies for new Applications: Empirical
Methods for Optimization from Small Data Sets. Compu-
tational Linguistics, 37:1.
C.J. Shroder, W. Mackaness, and B. Gittings. 2011. Giving
the Right Route Directions: The Requirements for Pedes-
trian Navigation Systems. Transactions in GIS, pages
419?438.
27
