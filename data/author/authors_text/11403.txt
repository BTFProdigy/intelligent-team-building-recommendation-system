Proceedings of the 8th International Conference on Computational Semantics, pages 359?370,
Tilburg, January 2009.
c?2009 International Conference on Computational Semantics
Semantic Normalisation : a Framework and an Experiment
Paul Bedaride Claire Gardent
INRIA/LORIA CNRS/LORIA
Universit?e Henri Poincar?e, Nancy Nancy
paul.bedaride@loria.fr claire.gardent@loria.fr
Abstract
We present a normalisation framework for linguistic representations and illustrate its use
by normalising the Stanford Dependency graphs (SDs) produced by the Stanford parser into
Labelled Stanford Dependency graphs (LSDs). The normalised representations are evaluated
both on a testsuite of constructed examples and on free text. The resulting representations
improve on standard Predicate/Argument structures produced by SRL by combining role la-
belling with the semantically oriented features of SDs. Furthermore, the proposed normalisa-
tion framework opens the way to stronger normalisation processes which should be useful in
reducing the burden on inference.
1 Introduction
In automated text understanding, there is a tradeoff between the degree of abstraction provided
by the semantic representations used and the complexity of the logical or probabilistic reasoning
involved. Thus, a system that normalises syntactic passives as actives avoids having to reason
about equivalences between grammatical dependencies. Similarly, normalising phrasal synonyms
into their one word equivalent (e.g., take a turn for the worse/worsen) or converting the semantic
representation of deverbal nominals into their equivalent verbal representations (Caesar?s destruc-
tion of the city/Caesar destroyed the city) avoids having to reason with the corresponding lexical
axioms. In short, the better, semantic representations abstract away from semantically irrelevant
distinctions, the less reasoning needs to be performed.
In this paper, we investigate a normalisation approach and present a framework for normalising
linguistic representations which we apply to converting the dependency structures output by the
Stanford parser (henceforth, Stanford Dependencies or SDs) into labelled SD graphs (LSD) that
is, dependency graphs where grammatical relations have been converted to roles.
The LSD graphs we produce and the normalisation framework we present, provide an inter-
esting alternative both for the shallow Predicate/Argument structures produced by semantic role
labelling (SRL) systems and for the complex logical formulae produced by deep parsers.
Thus as we shall see in Section 2, labelled SDs are richer than the standard Predicate/Argument
structures produced by SRL in that (i) they indicate dependencies between all parts of a sentence,
1
359
not just the verb and its arguments
1
and (ii) they inherit the semantically oriented features of
SDs namely, a detailed set of dependencies, a precise account of noun phrases and a semantically
oriented treatment of role marking prepositions, of heads and of conjunctions.
Furthermore, the normalisation framework (formal system and methodology) we present, can
be extended to model and implement more advanced normalisation steps (e.g., deverbal/verbal and
phrasal/lexical synonym normalisation) thereby potentially supporting a stronger normalisation
process than the semantic role labelling already supported by SRL systems and by deep parsers.
In sum, although the normalised SDs presented in this paper, do not exhibit a stronger normal-
isation than that available in the Predicate/Argument structures already produced by deep parsers
and by SRL systems, we believe that they are interesting in their own right in that they combine
semantic role labelling with the semantic features of SDs. Moreover, the proposed normalisation
framework opens the way for a stronger normalisation process.
The paper is structured as follows. Section 2 presents the representations handled by the system
namely, the SD graphs and their labelled versions, the LSDs. Section 3 presents the rewriting
system used and explains how SDs are converted to LSDs. Section 4 reports on evaluation. Section
5 discusses related work and concludes with pointers for further research.
2 (Normalised) Stanford Dependency graphs
Stanford Dependency graphs. SD graphs are syntactic dependency graphs where nodes are
words and edges are labelled with syntactic relations. As detailed in [dMM06, dM08], SD graphs
differ from other dependency graphs in several ways. First, they involve an extensive set of 56
dependency relations. These relations are organised in a hierarchy thereby permitting underspec-
ifying the relation between a head and its dependent (by using a very generic relation such as
dependent). Second, in contrast to other relational schemes such as the GR [CMB99] and PARC
[KCR
+
03], NP-internal dependency relations are relatively fine-grained
2
thereby permitting a de-
tailed description of NPs internal structure and providing better support for an accurate definition
of their semantics. Third, heads are constrained to be content words i.e., noun, verbs, adjectives,
adverbs but also conjunctions. In particular, contrary to the GR scheme, SD graphs take copula
be to be a dependent rather than a head. Fourth, SD graphs are further simplified in that some
nodes may be collapsed. for instance, role marking prepositions are omitted and a trace kept of
that preposition in the dependency name (e.g., prep-on).
The practical adequacy of SD graphs and their ability to support shallow semantic reasoning
is attested by a relatively high number of experiments. Thus, in 2007, 5 out of the 21 systems
submitted to the RTE (Recognising Textual Entailment) challenge used the SD representations.
SDs have been used in bioinformatics for extracting relations between genes and proteins [EOR07,
CS07]. It has furthermore been used for opinion extraction [ZJZ06], sentence-level sentiment
analysis [MP07] and information extraction [BCS
+
07].
1
In the CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies [SJM
+
08], the aim is to
produce dependency structures labelled with predicate/argument relations. Although such structures are similar to the
LSD graphs we produce, there are differences both in the precise type of structures built and in how these are built. We
discuss this point in more detail in section 5.
2
e.g., appos for apposition, nn for noun-noun compounds, num for a numeric modifier and number for an element in
a compound number.
360
love
John
Mary
nsubj dobj
love
Mary
be John
nsubjpass
agent
auxpass
love
John
Mary
arg0 arg1
Figure 1: SDs and LSDs for ?John loves Mary? and ?Mary is loved by John?
Normalised Stanford Dependency graphs. From the SDs produced by the Stanford parser, we
produce labelled SDs where the syntactic relations between a verb and its arguments are replaced
by the roles. For instance, the SDs and LSDs for the sentences ?john loves mary? and ?mary is
loved by john? are as given in Figure 1. The roles used in LSDs are those used in the PropBank for
core- and adjunct-like arguments namely, A0, A1, A2, A3, A4, AM where AM covers all PropBank
adjunct tags such as AM-TMP, AL-LOC, etc..
As mentioned in the introduction, LSD graphs combine the advantages of SD graphs with se-
mantic role labelling. From semantic role labelling, they take the more semantic predicate/argument
relations. From SD graphs, they inherit the semantic oriented features such as the deletion of con-
tent poor function words, the rich hierarchy of NP internal relations and the detailed description of
the relations holding between words other than the verb and its arguments.
In short, LSD graphs are both more semantic than SD graphs and richer than SRL Predi-
cate/Argument structures.
3 Normalising dependency trees
To normalise the SD graphs, we extend the Stanford parser with a normalisation module de-
signed to translate the grammatical relations between a verb and its arguments into roles. This
normalisation module consists of an ordered set of rewrite rules and is defined semi-automatically
in a two-step procedure as follows.
First, the rewrite rules for transitive verbs are defined. This first step is done manually and is
based on the XTAG [Gro01] inventory of possible syntactic contexts for verbs of this type.
Second, further rewrite rules for verbs of other classes (ditransitive, verbs with sentential ar-
gument, verbs with one prepositional argument, etc.) are automatically derived from the set of
rewrite rules for transitive verbs and from a small set of ?base-form rewrite rules? manually de-
fined for each class. The rules are then lexicalised using the information contained in the PropBank
Frames
3
.
3
The PropBank Frames specify for each verb sense in PropBank, the arguments it accepts and the corresponding
semantic roles.
361
xy
z
x
y
z
nsubj dobj
arg0 arg1
x
y
z
t
x
z
y
nsubjpass
agent
auxpass
arg0 arg1
Figure 2: Rewriting rules for active and passive
3.1 Defining basic rewrite rules
In the first phase, we manually define a set of rewrite rules for each possible syntactic variation of
a transitive verb.
Using the XTAG Tree Adjoining Grammar [Gro01], we start by listing these variations. In-
deed a Tree Adjoining Grammar (TAG) lists the set of all possible syntactic configurations for
basic clauses and groups them into so-called (tree) families. Thus the Tnx0Vnx1 family is a set
of trees describing the possible syntactic contexts in which a transitive verb can occur. Further,
W1nx0Vnx1 names a tree in that family which describes a syntactic context in which a transitive
verb (nx0Vnx1) occurs together with a canonical nominal subject (nx0) and a questioned object
(W1). We use the XTAG families to produce a list of basic clauses illustrating the possible syntac-
tic variations of each verb type. For instance, using the Tnx0Vnx1 XTAG family, we create a ?list
of Tnx0Vnx1 sentences? i.e.,
(1) ?John loves Mary?,?Mary is loved by John?, ?Mary, John loves?, ?It is Mary who is loved
by John?, ?It is John who loves Mary?, ?Mary who is loved by John?, ?John who loves
Mary?,etc.
We then parse these sentences using the Stanford parser and retrieve the correct dependency struc-
ture from the output thus gathering the set of dependency structures associated by the Stanford
parser with the various syntactic realisations of a given verb type.
Finally, for each distinct dependency structure found, we define a rewrite rule which maps this
dependency structure onto a unique (canonical) semantic representation. For instance, the rewrite
rules for the active and passive form of a sentence featuring a transitive verb are as sketched in
Figure 2 (see below for the exact content of these rules).
To define our rewrite rules, we resort to a standard rewriting system namely GrGen [KG07].
Used in multiple domains (e.g., formal calculus, combinatoric algebra, operational semantics),
rewriting is a technique for modelling reduction and simplification. For instance, the rewriting rule
r
1
: x ? y+ x ? z ? x ? (y+ z) permits factorising 5 ? 6+ 5 ? 7+ 5 ? 8 to 5 ? ((6+ 7)+ 8). More
generally, a rewriting system consists of a set of rewriting rules of the form l ? r where l and r
are filtering and rewriting patterns respectively. Given an object o, such a rule will apply to o if o
362
rule nx0Vnx1 {
pattern{
verb:element;
if{verb.verb != "None";}
np0:element;
np1:element;
verb -:nsubj-> np0;
verb -:dobj-> np1;
}
replace {
verb -:arg0-> np0;
verb -:arg1-> np1;
}}
rule nx1Vbynx0 {
pattern{
verb:element;
if{verb.verb != "None";}
np1:element;
be:element;
np0:element;
verb -:nsubjpass-> np1;
verb -:auxpass-> be;
verb -:agent-> np0;
}
replace {
verb -:arg0-> np0;
verb -:arg1-> np1;
}}
Figure 3: Two rewrite rules in the GrGen format
satisfies the filtering pattern l. The result of applying a rule to an object o is o where the sub-part of
o matched by l is rewritten according to the rewriting pattern r. Matching consists in looking for
a homograph homomorphism between the pattern graph l and the host graph h while the allowed
rewriting operations include information duplication, deletion and addition
4
.
In GrGen, the objects handled by rewriting are attributed typed directed multigraphs. These
are directed graphs with typed nodes and edges, where between two nodes more than one edge of
the same type and direction is permitted. According to its type, each node or edge has a defined set
of attributes associated with it. Moreover, the type system suppports multiple inheritance on node
and edge types.
Expressive and efficient, GrGen
5
is well suited to specify our normalisation rules. For instance,
the rewrite rule sketched in figure 2 can be specified as given in Figure 3. The left handside (lhs) of
the rule specifies a pattern in terms of nodes, node attributes, edge labels and conditions on nodes.
The right handside specifies how to rewrite the subgraphs matched by the lhs.
More generally, the SD graphs can be seen as attributed typed directed multigraphs where
node attributes are words and edge labels are grammatical relations. Rewrite rules can then be
used to modify, add or duplicate information present in the dependency graphs to create predicate-
argument structures.
Typically, rewriting is not confluent (different rule application orders yield different results)and
GrGen supports various sophisticated control strategies. So far however, we simply used rule
sequencing : rules are tested and fired in the order in which they are listed. They are ordered by
specificity with the most specific rules listed first. For instance, the rule rewriting a long passive
will precede that for a short passive thereby preventing the short passive rule from applying to a
4
For a more precise definition of satisfaction, matching and replacement, we refer the reader to [EHK
+
99].
5
There are other rewriting systems available such as in particular, the Tsurgen system used in the Stanford Parser to
map parse trees into dependency graphs. We opted for GrGen instead because it fitted our requirements best. GrGen
is efficient, notationally expressive (for specifying graphs but also rules and rule application strategies) and comes with
a sophisticated debugging environment. Importantly, GrGen developers are also quick to react to questions and to
integrate proposed modifications.
363
long passive sentence.
We also use GrGen ?global rewriting mode?. This ensures that whenever the rule filtering pat-
tern matches several subgraphs in the input structures, the rewriting operates on each of the filtered
subgraph. As we shall see in section 3, our rewrite rules are applied on not one but 5 dependency
graphs at a time. Moreover the same rewrite rules may be applicable to several subgraphs in a
sentence analysis (typically when the sentence contains 2 or more verbs occurring in the same
syntactic configuration). Global rewriting thereby avoids having to iterate over the rule set.
3.2 Deriving new rewrite rules
Manually specifying the normalisation rules is time consuming and error prone. To extend the
approach to all types of verbs and syntactic configurations, we semi-automatically derive new
rewrite rules from existing ones.
Let us call source class, the syntactic class from which we derive new rules, target class, the
syntactic class for which rewrite rules are being derived and base-form rewrite rule, a rewrite
rule operating on a ?base-form? that is, either on an active, a passive or a short passive form
subcategorising for canonical (i.e., non extracted) arguments.
Now, let us define the set of primitive rewrite rules used to bootstrap the process as the set of
all rewrite rules defined for the source class together with the set of base-form rewrite rules defined
for the target class.
To derive new rules from the set of primitive rewrite rules, we start by computing the differ-
ences (in terms of edges, node and labels) between a source base-form rewrite rule (RR) and either
a target, base-form RR (DIFF
+arg
) or a source non base-form RR (DIFF
+movt
). We then use the
resulting DIFFs to compute new rewrite rules which differ either from a source RR by a DIFF
+arg
patch or from a target base-form RR by a DIFF
+movt
. Figure 4 illustrates the idea on a specific
example. The RR for a ditransitive verb with questioned object (?What does John put on the ta-
ble??, W1nx0Vnx1pnx2) is derived both by applying a DIFF
+W1
patch to the nx0Vnx1pnx2
active base-form RR (?John put a book on the table.?) and by applying a DIFF
+pnx2
patch to
the source RR operating on W1nx0Vnx1 verbs with questioned object (?Who does Mary love??).
Note that in this way, the same rewrite rule (W1nx0Vnx2nx1) is derived in two different ways
namely, from the W1nx0Vnx1 RR by applying a DIFF
+pnx2
patch and from the nx0Vnx1pnx2
RR by applying a DIFF
+W1
one. We use this double derivation process to check the approach
consistency and found that in all cases, the same rule is derived by both possible paths.
Using the method just sketched, we derived 377 rules from a set of 352 primitive rewrite rules.
Although the ratio might seem weak, automating the derivation of rewrite rules facilitates system
maintenance and extension. This is because whenever a correction in the set of primitive rewrite
rules is carried out, the change automatically propagates to the related derived rules. In practice,
we found that a real feature when adapting the system to the Propbank data. We believe that it will
also be useful when extending the system to deal with nominalisations.
4 Evaluation and discussion
We evaluated our normalisation method both on a testsuite of constructed examples and on real
world data namely, the Propbank corpus.
364
nx0Vnx1
nx0Vnx1pnx2
W1nx0Vnx1
W1nx0Vnx1pnx2
+pnx2
+pnx2
DIFF
arg
+W1 +W1
DIFF
mvt
Source RR
Target RR
Base Form RR
Figure 4: Deriving new rules from existing ones
4.1 Evaluation on a testsuite of constructed examples
This first evaluation aims to provide a systematic, fine grained assessment of how well the system
normalises each of the several syntactic configurations assigned by XTAG to distinct verb types.
The emphasis is here in covering the most exhaustive set of possible syntactic configurations possi-
ble. Because constructing the examples was intricate and time consuming, we did not cover all the
possibilities described by XTAG however. Instead we concentrated on listing all the configurations
specified by XTAG for 4 very distinct families namely, Tnx0Vnx1, Tnx0Vnx2nx1,Tnx0Vplnx1
and Tnx0Vnx1pnx2. The first class is the class for transitive verbs. Because of passive, this class
permits many distinct variations. The second class is the class of verbs with 3 nominal arguments.
This class is difficult for role labelling as the distinction between the complements often relies on
semantic rather than syntactic grounds. The third class is the class of verbs with a particle and 2
nominal arguments (ring up) and the fourth, the class of ditransitive.
For these constructed sentences, we had no gold standard i.e., no role annotation. Hence we
used logical inference to check normalisation. We proceeded by grouping the test items in (non)
entailment pairs and then checked whether the associated LSDs supported the detection of the
correct entailment relation (i.e., true or false).
The testsuite. Using a restricted lexicon, a set of clauses covering the possible syntactic patterns
of the four verb classes and regular expressions describing sentence-semantics pairs, we develop
a script generating (sentence,semantics) pairs where sentences contain one or more clauses. After
having manually verified the correctness of the generated pairs, we used them to construct textual
entailment testsuite items that is, pairs of sentences annotated with TRUE or FALSE dependending
on whether the two sentences are related by entailment (TRUE) or not (FALSE). The resulting
testsuite
6
contains 4 976 items of which 2 335 are entailments between a sentence and a clause
(1V+TE, example 2), 1 019 between two complex sentences (2V+TE, example 3) and 1 622 are
non-entailments (V-TE, example 4).
(2) T
1
: John likes the book that Mary put on the table.
T
2
: John likes a book
Annotations: 1V+TE, TRUE
6
Available at http://www.loria.fr/
?
bedaride/publications/taln08-bedgar/index.html.
365
(3) T
1
: John likes the book that Mary put on the table.
T
2
: The book which is put on the table by Mary, is liked by John
Annotations: 2V+TE, TRUE
(4) T
1
: John likes the book that Mary put on the table.
T
2
: John likes a table
Annotations: V-TE, FALSE
Checking for entailment. For each testsuite item, we then checked for entailment by translating
LSDs into FOL formulae and checking entailment between the first five LSDs derived from the
parser output for the sentences contained in the testsuite item.
The translation of a LSD into a FOL formula is done as follows. Each node is associated with
an existentially quantified variable and a predication over that variable where the predicate used is
the word labelling the node. Each edge translates to a binary relation between the source and the
target node variables. The overall formula associated with an LSD is then the conjunction of the
predications introduced by each node. For instance, for the LSD given in Figure 1, the resulting
formula is ?x, y, z : love(x) ? john(y) ?mary(z) ? arg0(x, y) ? arg1(x, z).
This translation procedure is of course very basic. Nonetheless, because the testsuite builds
on a restricted syntax and vocabulary
7
, it suffices to check how well the normalisation process
succeeds in assigning syntactic variants the same semantic representation.
Results. The test procedure just described is applied to the LSD graphs produced by the normal-
isation module on the testsuite items. Table 5 gives the results. For each class of testsuite items
(1V+TE, 2V+TE, V-TE), we list the percentage of cases recognised by the system as entailment
(+TE) and non entailment (-TE). Because FOL is only semi-decidable, the reasoners do not always
return an answer. The Failure line gives the number of cases for which the reasoners fail.
The results on positive entailments (1V+TE,2V+TE) show that the proposed normalisation
method is generally successful in recognising syntax based entailments with an overall average
precision of 86.3% (and a breakdown of 94.9% for 1V+TE and 66.6% for 2V+TE cases). Impor-
tantly, the results on negative entailments (99.2% overall precision) show that the method is not
overly permissive and does not conflate semantically distinct structures. Finally, it can be seen that
the results degrade for the Tnx0Vnx2nx1 class (John gave Mary a book). This is due mainly to
genuine syntactic ambiguities which cannot be resolved without further semantic (usually ontolog-
ical) knowledge. For instance, both The book which John gave the woman and The woman whom
John gave the book are assigned the same dependency structures by the Stanford parser. Hence
the same rewrite rule applies to both structures and necessarily assigns one of them the wrong
labelling. Other sources of errors are cases where the DIFF patch used to derive a new rule fail
to adequately generalise to the target structure. In such cases, the erroneous rewrite rule can be
modified manually.
7
In particular, the testsuite contains no quantifiers.
366
family ans 1V+TE 2V+TE V-TE
+TE 585 (98.2%) 212 (72.4%) 0 (0.0%)
Tnx0Vnx1 -TE 11 (1.8%) 79 (27.0%) 57 (100.0%)
Failure 0 (0.0%) 2 (0.6%) 0 (0.0%)
+TE 513 (89.2%) 131 (55.7%) 3 (0.4%)
Tnx0Vnx2nx1 -TE 61 (10.6%) 103 (43.8%) 703 (99.6%)
Failure 1 (0.2%) 1 (0.5%) 0 (0.0%)
+TE 567 (95.5%) 169 (67.9%) 0 (0.0%)
Tnx0Vplnx1 -TE 27 (4.5%) 79 (31.7%) 198 (100.0%)
Failure 0 (0.0%) 1 (0.4%) 0 (0.0%)
+TE 550 (96.5%) 167 (69.0%) 10 (1.5%)
Tnx0Vnx1pnx2 -TE 16 (2.8%) 69 (28.5%) 651 (98.5%)
Failure 4 (0.7%) 6 (2.5%) 0 (0.0%)
+TE 2215 (94.9%) 679 (66.6%) 13 (0.8%)
all -TE 115 (4.9%) 330 (32.4%) 1609 (99.2%)
Failure 5 (0.2%) 10 (1.0%) 0 (0.0%)
Figure 5: Precision on constructed examples. Each cell gives the proportion of cases recognised as
entailment by the system. Bold face figures give the precision i.e., the proportion of answers given
by the system that are correct.
4.2 Evaluation on the PropBank
The PropBank (Proposition Bank) was created by semantic annotation of the Wall Street Journal
section of Treebank-2. Each verb occurring in the Treebank has been treated as a semantic pred-
icate and the surrounding text has been annotated for arguments and adjuncts of the predicate as
illustrated in (5).
(5) [A0 He ] [AM-MOD would ] [AM-NEG n?t ] [V accept ] [A1 anything of value ] from [A2
those he was writing about ] .
The labels used for the core and adjunct-like arguments are the following
8
. The labels A0 .. A5
designate arguments associated with a verb predicate as defined in the PropBank Frames scheme.
A0 is the agent, A1 the patient or the theme. For A2 to A5 no consistent generalisation can be
made and the annotation reflects the decisions made when defining the PropBank Frames scheme.
Further, the AM-T label describes adjunct like arguments of various sorts, where T is the type of
the adjunct. Types include locative, temporal, manner, etc.
We used the PropBank to evaluate our normalisation procedure on free text. As in the CoNLL
(Conference on Natural Language Learning) shared task for SRL, the evaluation metrics used
are precision, recall and F measure. An argument is said to be correctly recognised if the words
spanning the argument as well as its semantic role match the PropBank annotation. Precision is
the proportion of arguments predicted by a system which are correct. Recall is the proportion of
correct arguments which are predicted by a system. F-measure is the harmonic mean of precision
and recall. The results are given below.
8
This is in fact simplified. The PropBank corpus additionally provide information about R-* arguments (a reference
such as a trace to some other argument of A* type) and C-* arguments (a continuation phrase in a split argument).
367
args 0 1 2 3 4 5 a m total
recall 68.4% 68.2% 62.4% 47.2% 57.6% 5.3% 0.0% 64.4% 66.1%
precision 88.0% 80.2% 76.4% 83.1% 83.3% 50.0% ? 75.0% 80.6%
f-mesure 77.0% 73.7% 68.7% 60.2% 68.1% 9.5% ? 69.3% 72.6%
Precision (80.6%) is comparable to the results obtained in the ConLL 2005 SRL shared task
where the top 8 systems have an average precision ranging from 76.55% to 82.28%. Recall is
generally a little low (the ConLL05 recall ranged from 64.99% to 76.78%) for mainly two reasons:
either the Stanford parser, did not deliver the correct analysis or the required rewrite rule was not
present.
5 Conclusion
Our approach is akin to so-called semantic role labelling (SRL) approaches [CM05] and to sev-
eral rewriting approaches developed to modify parsing output in RTE systems [Ass07]. It differs
from the SRL approaches in that unlike most SRLs systems, it is based on a hybrid, statistic and
symbolic, framework. As a result, improving or extending the system can be done independently
of the availability of an appropriately annotated corpus. However, the quality, performance and
coverage of the system remains dependent on those of the Stanford parser
9
,
Our approach also differs from approaches that use the lambda calculus to normalise syntactic
variation. In such approaches, a compositional semantics module associates words and grammar
rules or derivation structures with lambda terms which in effect normalise variations such as for
instance, the active/passive variation. One important advantage of lambda based approaches is that
the rewriting system is confluent. The drawback however is that the specification of the appropriate
lambda terms requires expert linguistic skills. In contrast, the rewrite rule approach is compara-
tively easier to handle (the rules presented here were developed by a computer scientist) and its
use is supported by sophisticated developing environments such as GrGen which provides strong
notational expressivity (the rewrite rules can include conditions, can operate on graphs of arbi-
trary depth, etc. ), a good debugging environment and good processing times. In short although,
the lambda calculus approach is undoubtly more principled, the rewrite rule approach is arguably
easier to handle and easier to understand.
Normalisation of linguistic representations is not new. It is used in particular, in [BCC
+
07,
DBBT07, RTF07] for dealing with entailment detection in the RTE (Recognising Textual Entail-
ment) challenge. The approach we present here differs from these approaches both by its sys-
tematic treatment of syntactic variation and by its use of GrGen as a framework for specifying
transformations. More generally, our approach emphasises the following three points namely (i)
the systematic testing of all possible syntactic variations (based on the information contained in
XTAG); (ii) the use of an expressive, efficient and well-understood graph rewriting system for
defining transformations; and (iii) the development of a methodology for automatically deriving
new rewrite rules from existing ones.
By providing a well-defined framework for specifying, deriving and evaluating rewrite rules,
we strive to develop a system that normalises NLP representations in a way that best supports
9
[KM03] report a label F-mesure of 86.3% on section 23 of the Penn Treebank.
368
semantic processing. The emphasis is on aligning Predicate/Argument structures that diverge in
the surface text but that are semantically similar (e.g., John buy a car from Peter/Peter sells a car
to John). In particular, we plan to extend the system to normalise nominal dependencies (using
NomBank) and converse constructions.
References
[Ass07] Association for Computational Linguistics. Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, Prague, Czech Republic, June 2007.
[BCC
+
07] D. G. Bobrow, C. Condoravdi, R. S. Crouch, V. de Paiva, L. Karttunen, T. H. King,
R. Nairn, L. Price, and A. Zaenen. Precision-focused textual inference. In ACL-
PASCAL Workshop on Textual Entailment and Paraphrasing, pages 16?21, Prague,
Czech Republic, June 2007.
[BCS
+
07] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open in-
formation extraction from the web. In IJCAI ?07: Proceedings of International Joint
Conference on Artificial Intelligence, pages 2670?2676, Hyderabad, India, January
2007.
[CM05] X. Carreras and L. Marquez. Introduction to the conll-2005 shared task: Semantic role
labeling. In Proceedings of the CoNLL-2005 Shared Task: Semantic Role Labeling,
pages 152?164, Ann Arbor, Michigan, June 2005.
[CMB99] J. Carroll, G. Minnen, and T. Briscoe. Corpus annotation for parser evaluation. In
EACL Workshop on Linguistically Interpreted Corpora, Bergen, Norway, June 1999.
[CS07] A. B. Clegg and A. J. Shepherd. Benchmarking natural-language parsers for biological
applications using dependency graphs. BMC Bioinformatics, 8:24, January 2007.
[DBBT07] R. Delmonte, A. Bristot, M. A. P. Boniforti, and S. Tonelli. Entailment and anaphora
resolution in rte3. In ACL-PASCAL Workshop on Textual Entailment and Paraphras-
ing, pages 48?53, Prague, Czech Republic, June 2007.
[dM08] M.-C. de Marneffe and C. D. Manning. The stanford typed dependencies representa-
tions. In COLING?08 Workshop on Cross-framework and Cross-domain Parser Eval-
uation, Manchester, England, August 2008.
[dMM06] Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. Gener-
ating typed dependency parses from phrase structure parses. In LREC ?06: Proceed-
ings of 5th International Conference on Language Resources and Evaluation, pages
449?454, Genoa, Italy, May 2006.
[EHK
+
99] H. Ehrig, R. Heckel, M. Korff, Loewe M., L. Ribeiro, A. Wagner, and A. Corradini.
Handbook of Graph Grammars and Computing by Graph Transformation., volume 1,
chapter Algebraic Approaches to Graph Transformation - Part II: Single Pushout A.
and Comparison with Double Pushout A, pages 247?312. World Scientific, 1999.
369
[EOR07] G. Erkan, A. Ozgur, and D. R. Radev. Semi-supervised classification for extracting
protein interaction sentences using dependency parsing. In EMNLP-CoNLL ?07: Pro-
ceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 228?237, Prague,
Czech Republic, June 2007. Association for Computational Linguistics.
[Gro01] XTAG Research Group. A lexicalized tree adjoining grammar for english. Technical
Report IRCS-01-03, IRCS, University of Pennsylvania, 2001.
[KCR
+
03] T. King, R. Crouch, S. Riezler, M. Dalrymple, and R. Kaplan. The parc 700 depen-
dency bank. In EACL workshop on Linguistically Interpreted Corpora, Budapest,
Hungary, April 2003.
[KG07] M. Kroll and R. Gei?. Developing graph transformations with grgen.net. Technical
report, October 2007. preliminary version, submitted to AGTIVE 2007.
[KM03] D. Klein and C. D. Manning. Accurate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics, pages
423?430, Sapporo, Japan, July 2003. Association for Computational Linguistics.
[MP07] A. Meena and T. V. Prabhakar. Sentence level sentiment analysis in the presence
of conjuncts using linguistic analysis. In Ecir ?07: Proceedings of 29th European
Conference on Information Retrieval, pages 573?580, Rome, Italy, April 2007.
[RTF07] A. B. N. Reiter, S. Thater, and A. Frank. A semantic approach to textual entailment:
System evaluation and task analysis. In ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 10?15, Prague, Czech Republic, June 2007.
[SJM
+
08] M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. The CoNLL-2008
shared task on joint parsing of syntactic and semantic dependencies. In CoNLL ?08:
Proceedings of the 12th Conference on Computational Natural Language Learning,
pages 159?177, Manchester, UK, August 2008.
[ZJZ06] L. Zhuang, F. Jing, and X.-Y. Zhu. Movie review mining and summarization. In
CIKM ?06: Proceedings of the 15th ACM international conference on Information
and knowledge management, pages 43?50, Arlington, Virginia, USA, November 2006.
ACM.
370
Coling 2010: Poster Volume, pages 45?53,
Beijing, August 2010
Benchmarking for syntax-based sentential inference
Paul Bedaride
INRIA/LORIA
Universite? Henri Poincare?
paul.bedaride@loria.fr
Claire Gardent
CNRS/LORIA
claire.gardent@loria.fr
Abstract
We propose a methodology for investigat-
ing how well NLP systems handle mean-
ing preserving syntactic variations. We
start by presenting a method for the semi
automated creation of a benchmark where
entailment is mediated solely by meaning
preserving syntactic variations. We then
use this benchmark to compare a seman-
tic role labeller and two grammar based
RTE systems. We argue that the proposed
methodology (i) supports a modular eval-
uation of the ability of NLP systems to
handle the syntax/semantic interface and
(ii) permits focused error mining and er-
ror analysis.
1 Introduction
First launched in 2005, the Recognising Textual
Inference Challenge (RTE)1 aims to assess in how
far computer systems can emulate a human being
in determining whether a short text fragment H
referred to as the hypothesis, follows from or is
contradicted by a text fragment T . In the RTE
benchmarks, the hypothesis is a short constructed
sentence whilst the text fragments are short pas-
sages of naturally occurring texts. As a result, the
RTE challenge permits evaluating the capacity of
NLP systems to handle local textual inference on
real data, an enabling technology for any applica-
tions involving document interpretation.
In this paper, we focus on entailments based on
meaning entailing, syntactic transformations such
as:
(1) The man gives the woman the flowers that
smell nice ? The flowers which are given
to the woman smell nice
1http://www.pascal-network.org/
Challenges/RTE
We start (Section 2) by motivating the ap-
proach. We argue that the proposed evaluation
methodology (i) interestingly complements the
RTE challenge in that it permits a modular, ana-
lytic evaluation of the ability of NLP systems to
handle syntax-based, sentential inference and (ii)
permits focused error mining and analysis .
In Section 3, we go on to describe the bench-
mark construction process. Each item of the con-
structed benchmark associates two sentences with
a truth value (true or false) indicating whether
or not the second sentence can be understood to
follow from the first. The construction of these
benchmark items relies on the use of a gram-
mar based surface realiser and we show how this
permits automatically associating with each infer-
ence item, an entailment value (true or false) and
a detailed syntactic annotation reflecting the syn-
tactic constructs present in the two sentences con-
stituting each benchmark item.
In section 4, we use the benchmark to evaluate
and compare three systems designed to recognise
meaning preserving syntactic variations namely,
a semantic role labeller, Johan Bos? Nutcracker
RTE system (where the syntax/semantic interface
is handled by a semantic construction module
working on the output of combinatory categorial
grammar parser) and the Afazio system, a hybrid
system combining statistical parsing, symbolic se-
mantic role labelling and sentential entailment de-
tection using first order logic. We give the eval-
uation figures for each system. Additionally, we
show how the detailed syntactic annotations au-
tomatically associated with each benchmark item
by the surface realiser can be used to identify the
most likely source of errors that is, the syntactic
constructs that most frequently co-occur with en
entailment recognition error.
45
2 Motivations
Arguably focusing on meaning entailing syntac-
tic transformations is very weak. Indeed, one of
the key conclusions at the second RTE Challenge
Workshop was that entailment modeling requires
vast knowledge resources that correspond to dif-
ferent types of entailment reasoning e.g., ontolog-
ical and lexical relationships, paraphrases and en-
tailment rules, meaning entailing syntactic trans-
formations and last but not least, world knowl-
edge. Further, Manning (2006) has strongly ar-
gued against circumscribing the RTE data to cer-
tain forms of inference such as for instance, infer-
ences based solely on linguistic knowledge. Fi-
nally, it is also often insisted that naturally occur-
ring data should be favored over constructed data.
While we agree that challenges such as the RTE
challenge are useful in testing systems abilities to
cope with real data, we believe there is also room
for more focused evaluation setups.
Focusing on syntax based entailments. As
mentioned above, syntax based entailment is only
one of the many inference types involved in deter-
mining textual entailment. Nevertheless, a manual
analysis of the RTE1 data by (Vanderwende et al,
2005) indicates that 37% of the examples could
be handled by considering syntax alone. Sim-
ilarly, (Garoufi, 2007) shows that 37.5% of the
RTE2 data does not involve deep reasoning and
more specifically, that 33.8% of the RTE2 data in-
volves syntactic or lexical knowledge only. Hence
although the holistic, blackbox type of evaluation
practiced in the RTE challenge is undeniably use-
ful in assessing the ability of existing systems to
handle local textual inference, a more analytic,
modular kind of evaluation targeting syntax-based
entailment reasoning is arguably also of interest.
Another interesting feature of the SSI (syntax-
based sentential entailment) task we propose is
that it provides an alternative way of evaluating
semantic role labelling (SRL) systems. Typically,
the evaluation of SRL systems relies on a hand an-
notated corpus such as PropBank or the FrameNet
corpus. The systems precision and recall are then
computed w.r.t. this reference corpus. As has been
repeatedly argued (Moll and Hutchinson, 2003;
Galliers and Jones, 1993), intrinsic evaluations
may be of very limited value. For semantically
oriented tools such as SRL systems, it is important
to also assess their results w.r.t. the task which
they are meant support namely reasoning : Do
the semantic representations built by SRL help in
making the correct inferences ? Can they be used,
for instance, to determine whether a given sen-
tence answers a given question ? or whether the
content of one sentence follow from that another ?
As explained in (Giampiccolo et al, 2007), entail-
ment recognition is a first, major step towards an-
swering these questions. Accordingly, instead of
comparing the representations produced by SRL
systems against a gold standard, the evaluation
scheme presented here, permits evaluating them
w.r.t. their ability to capture syntax based senten-
tial inference.
It is worth adding that, although the present pa-
per focuses on entailments strictly based on syn-
tax, the proposed methodology should straight-
forwardly extend to further types of entailment
such as in particular, entailments involving lexi-
cal relations (synonymy, antonymy, etc.) or entail-
ments involving more complex semantic phenom-
ena such as the interplay between different classes
of complement taking verbs, polarity and author
commitment discussed in (Nairn et al, 2006).
This is because as we shall see in section 3, our
approach is based on an extensive, hand written
grammar of English integrating syntax and se-
mantics. By modifying the grammar, the lexicon
and/or the semantics, data of varying linguistic
type and complexity can be produced and used for
evaluation.
Hand constructed vs. naturally occurring data.
Although in the 90s, hand tailored testsuites such
as (Lehmann et al, 1996; Cooper et al, 1995)
were deemed useful for evaluating NLP systems,
it is today generally assumed that, for evaluation
purposes, naturally occurring data is best. We ar-
gue that constructed data can interestingly com-
plement naturally occurring data.
To start with, we agree with (Crouch et al,
2006; Cohen et al, 2008) that science generally
benefits from combining laboratory and field stud-
ies and more specifically, that computational lin-
guistics can benefit from evaluating systems on
46
a combination of naturally occurring and con-
structed data.
Moreover, constructed data need not be hand
constructed. Interestingly, automating the produc-
tion of this data can help provide better data anno-
tation as well as better and better balanced data
coverage than both hand constructed data and nat-
urally occurring data. Indeed, as we shall show
in section 4, the benchmark creation process pre-
sented here supports a detailed and fully auto-
mated annotation of the syntactic properties as-
sociated with each benchmark item. As shown
in section 5, this in turn allows for detailed er-
ror mining making it possible to identify the most
likely causes of system errors. Additionally, the
proposed methodology permits controlling over
such benchmark parameters as the size of the data
set, the balance between true and false entail-
ments, the correlation between word overlap and
entailment value and/or the specific syntactic phe-
nomena involved. This is in contrast with the RTE
data collection process where ?the distribution of
examples is arbitrary to a large extent, being de-
termined by manual selection2? (Giampiccolo et
al., 2007). As has been repeatedly pointed out
(Burchardt et al, 2007; Garoufi, 2007), the RTE
datasets are poorly balanced w.r.t., both the fre-
quency and the coverage of the various phenom-
ena interacting with textual inference.
3 Benchmark
We now present the content of an SSI benchmark
and the method for constructing it.
An SSI benchmark item (cf. e.g., Figure 1) con-
sists of two sentences and a truth value (true or
false) indicating whether or not the second sen-
tence can be understood to follow from the first.
In addition, each sentence is associated with a de-
tailed syntactic annotation describing the syntac-
tic constructs present in the sentence.
The benchmark construction process consists
of two main steps. First, a generation bank is
built. Second, this generation bank is drawn upon
2The short texts of the RTE benchmarks are automatically
extracted from real texts using different applications (e.g.,
Q/A, summarisation, information extraction, information re-
trieval systems) but the query used to retrieve these texts is
either constructed manually or post-edited.
T: The man gives the woman the flowers that smell
nice
smell:{n0Va1,active,relSubj,canAdj}
give:{n0Vn2n1,active,canSubj,canObj,canIObj}
H: The flowers are given to the woman
give:{n0Vn1Pn2,shortPassive,canSubj,canIObj}
Entailment: TRUE
Figure 1: An SSI Benchmark item
to construct a balanced data set for SSI evaluation.
We now describe each of these processes in turn.
Constructing a generation bank We use the
term ?generation bank? to refer to a dataset whose
items are produced by a surface realiser i.e., a
sentence generator. A surface realiser in turn
is a program which associates with a given se-
mantic representation, the set of sentences ver-
balising the meaning encoded by that representa-
tion. To construct our generation bank, we use the
GenI surface realiser (Gardent and Kow, 2007).
This realiser uses a Feature based Tree Adjoining
Grammar (FTAG) augmented with a unification
sematics as proposed in (Gardent and Kallmeyer,
2003) to produce all the sentences associated by
the grammar with a given semantic representa-
tion. Interestingly, the FTAG used has been com-
piled out of a factorised representation and as a
result, each elementary grammar unit (i.e., ele-
mentary FTAG tree) and further each parse tree, is
associated with a list of items indicating the syn-
tactic construct(s) captured by that unit/tree3. In
short, GenI permits associating with a given se-
mantics, a set of sentences and further for each of
these sentences, a set of items indicating the syn-
tactic construct(s) present in the syntactic tree of
that sentence. For instance, the sentences and the
syntactic constructs associated by GenI with the
semantics given in (2) are those given in (3).
(2) A:give(B C D E) G:the(C) F:man(C)
H:the(D) I:woman(D) J:the(E) K:flower(E)
L:passive(B) L:smell(M E N) O:nice(N)
(3) a. The flower which smells nice is given to
the woman by the man
3Space is lacking to give a detailed explanation of this
process here. We refer the reader to (Gardent and Kow, 2007)
for more details on how GenI associates with a given seman-
tics, a set of sentences and for each sentence a set of items
indicating the syntactic construct(s) present in the syntactic
tree of that sentence.
47
give:n0Vn1Pn2-Passive-CanSubj-ToObj-ByAgt,
smell:n0V-active-OvertSubjectRelative
b. The flower which smells nice is given
the woman by the man
give:n0Vn2n1-Passive,
smell:n0V-active-OvertSubjectRelative
c. The flower which is given the woman by
the man smells nice
give:n0Vn2n1-Passive-CovertSubjectRelative,
smell:n0V-active
d. The flower which is given to the woman
by the man smells nice
give:n0Vn1Pn2-Passive-OvertSubjectRelative,
smell:n0V-active
e. The flower that smells nice is given to
the woman by the man
give:n0Vn1Pn2-Passive,
smell:n0V-CovertSubjectRelative
f. The flower that smells nice is given the
woman by the man
give:n0Vn2n1-Passive,
smell:n0V-CovertSubjectRelative
g. The flower that is given the woman by
the man smells nice
give:n0Vn2n1-Passive-CovertSubjectRelative,
smell:n0V-active
h. The flower that is given to the woman by
the man smells nice
give:n0Vn1Pn2-Passive-CovertSubjectRelative,
smell:n0V-active
The tagset of syntactic annotation covers the sub-
categorisation type of the verb, a specification of
the verb mood and a description of how arguments
are realised.
The semantic representation language used is
a simplified version of the flat semantics used
in e.g., (Copestake et al, 2005) which is suf-
ficient for the cases handled in the present pa-
per. The grammar and therefore the generator,
can however easily be modified to integrate the
more sophisticated version proposed in (Gardent
and Kallmeyer, 2003) and thereby provide an ad-
equate treatment of scope.
Constructing an SSI benchmark. Given a
generation bank, false and true sentential entail-
ment pairs can be automatically produced by tak-
ing pairs of sentences ?S1, S2? and comparing
their semantics: if the semantics of S2 is entailed
by the semantics of S1, the pair is marked as TRUE
else as FALSE. The syntactic annotations asso-
ciated in the generation bank with each sentence
are carried over to the SSI benchmark thereby en-
suring that the overall information contained in
each SSI benchmark is as illustrated in Figure 1
namely, two pairs of syntactically annotated sen-
tences and a truth value indicating (non) entail-
ment.
To determine whether a sentence textually en-
tails another we translate their flat semantic rep-
resentation into first order logic and check for
logical entailment. Differences in semantic rep-
resentations which are linked to functional sur-
face differences such as active/passive or the
presence/absence of a complementizer (John sees
Mary leaving/John sees that Mary leaves) are
dealt with by (automatically) removing the corre-
sponding semantic literals from the semantic rep-
resentation before translating it to first order logic.
In other words, active/passive variants of the same
sentence are deemed semantically equivalent.
Note that contrary to what is assumed in the
RTE challenge, entailment is here logical rather
than textual (i.e., determined by a human) entail-
ment. By using logical, rather than textual (i.e.,
human based) entailment, it is possible that some
cases of syntax mediated textual entailments are
not taken into account. However, intuitively, it
seems reasonable to assume that for most of the
entailments mediated by syntax alone, logical and
textual entailments coincide.
3.1 The SSI benchmark
Using the methodology just described, we first
produced a generation bank of 226 items using 81
input formula distributed over 4 verb types. From
this generation bank, a total of 6 396 SSI-pairs
were built with a ratio of 42.6% true and 57.4%
false entailments.
For our experiment, we extracted from this SSI-
suite, 1000 pairs with an equal proportion of true
and false entailments and a 7/23/30/40 distribu-
tion of four subcategorisation types namely, ad-
jectival predicative (n0Va1 e.g., The cake tastes
good), intransitive (n0V), transitive (n0Vn1) and
ditransitive (n0Vn2n1)4. We furthermore con-
4The subcategorisation type of an SSI item is determined
manually and refers either to the main verb if the sentence is
48
strained the suite to respect a neutral correlation
between word overlap and entailment. Following
(Garoufi, 2007), we define this correlation as fol-
lows. The word overlap wo(T,H) between two
sentences T and H is the ratio of common lem-
mas between T and H on the number of lemmas
in H (non content words are ignored). If entail-
ment holds, the word overlap/entailment correla-
tion value of the sentence pair is wo(T,H). Oth-
erwise it is 1 ? wo(T,H). The 1000 items of the
SSI suite used in our experiment were chosen in
such a way that the word overlap/entailment cor-
relation value of the SSI suite is 0.49.
In sum, the SSI suite used for testing exhibits
the following features. First, it is balanced w.r.t.
entailment. Second, it displays good syntactic
variability based both on the constrained distribu-
tion of the four subcategorisation types and on the
use of the XTAG grammar to construct sentences
from abstract representations (cf. the paraphrases
in (3) generated by GenI from the representation
given in (2)). Third, it contains 1000 items and
could easily be extended to cover more and more
varied data. Fourth, it is specifically tailored to
check systems on their ability to deal with syntax
based sentential entailment: word overlap is high,
syntactic variability is provided and the correla-
tion between word overlap and entailment is not
biased.
4 System evaluation and comparison
SRL and grammar based systems equipped with
a compositional semantics are primary targets for
an SSI evaluation. Indeed these systems aim to
abstract away from syntactic differences by pro-
ducing semantic representations of a text which
capture predicate/argument relations independent
of their syntactic realisation.
We evaluated three such systems on the SSI
benchmark namely, NutCracker, (Johansson and
Nugues, 2008)?s Semantic Role Labeller and the
Afazio RTE system.
4.1 Systems
Nutcracker Nutcracker is a system for recog-
nising textual entailment which uses deep seman-
a clause or to the embedded verb if the sentence is a complex
sentence.
tic processing and automated reasoning. Deep se-
mantic processing associates each sentence with a
Discourse Representation Structure (DRS (Kamp
and Reyle, 1993)) by first, using a statistical
parser to build the syntactic parse of the sentence
and second, using a symbolic semantic construc-
tion module to associate a DRS with the syn-
tactic parse. Entailment between two DRSs is
then checked by translating this DRS into a first-
order logical (FOL) formula and first trying to
find a proof. If a proof is found then the en-
tailment is set to true. Otherwise, Nutcracker
backs off with a word overlap module computed
over an abstract representation of the input sen-
tences and taking into account WordNet related
information. Nutcracker was entered in the first
RTE challenge and scored an accuraccy (percent-
age of correct judgments) of 0.562 when used as
is and 0.612 when combined with machine learn-
ing techniques. For our experiment, we use the
online version of Nutcracker and the given default
parameters.
Afazio Like Nutcracker, the Afazio system
combines a statistical parser (the Stanford parser)
with a symbolic semantic component. This com-
ponent pipelines several rewrite modules which
translate the parser output into a first order logic
formula intended to abstract away from sur-
face differences and assign syntactic paraphrases
the same representation (Bedaride and Gardent,
2009). Special emphasis is placed on captur-
ing syntax based equivalences such as syntac-
tic (e.g., active/passive) variations, redistributions
and noun/verb variants. Once the parser out-
put has been normalised into predicate/argument
representations capturing these equivalences, the
resulting structures are rewritten into first order
logic formulae. Like Nutcracker, Afazio checks
entailment using first order automated reasoners
namely, Equinox and Paradox 5.
SRL (Johansson and Nugues, 2008)?s seman-
tic role labeller achieved the top score in the
closed CoNLL 2008 challenge reaching a labeled
semantic F1 of 81.65. To allow for compari-
son with Nutcracker and Afazio, we adapted the
5http://www.cs.chalmers.se/?koen/
folkung/
49
rewrite module used in Afazio to rewrite Pred-
icate/Argument structures into FOL formula in
such a way as to fit (Johansson and Nugues,
2008)?s SRL output. We then use FOL automated
reasoner to check entailment.
4.2 Evaluation scheme and results
The results obtained by the three systems are
summarised in Table 1. TP (true positives) is
the number of entailments recognised as such by
the system and TN (true negatives) of non entail-
ments. Conversely, FN and FP indicate how often
the systems get it wrong: FP is the number of non
entailments labelled as entailments by the system
and FN, the number of entailments labelled as non
entailments. ?ERROR? refers to cases where the
CCG parser used by Nutcracker fails to find a
parse. The last three columns indicate the over-
all ability of the systems to recognise false entail-
ments (TN/N with N the number of false entail-
ment in the benchmark), true entailments (TP/P)
and all true and false entailment (Precision).
Overall, Afazio outperforms both Nutcracker
and the SRL system. This is unsurprising since
contrary to these other two systems, Afazio was
specifically designed to handle syntax based sen-
tential entailment. Its strength is that it combines
a full SRL system with a semantic construction
module designed for entailment detection. More
surprisingly, the CCG parser used by Nutcracker
often fails to find a parse.
The SRL system has a high rate of false nega-
tives. Using the error mining technique presented
in the next section, we found that the most sus-
picious syntactic constructs all included a rela-
tivised argument. A closer look at the analyses
showed that this was due to the fact that SRL sys-
tems fail to identify the antecedent of a relative
pronoun, an identification that is necessary for en-
tailment checking. Another important difference
with Afazio is that the SRL system produces a
single output. In contrast, Afazio checks entail-
ment for any of the pairs of semantic representa-
tions derived from the first 9 parses of the Stan-
ford parser. The number 9 was determined em-
pirically and proved to yield the best results over-
all although as we shall see in the error mining
section, taking such a high number of parses into
account often leads to incorrect results when the
hypothesis (H) is short.
Nutcracker, on the other hand, produces many
false positives. This is in part due to cases where
the time bound is reached and the word overlap
backoff triggered. Since the overall word overlap
of the SSI suite is high, the backoff often predicts
an entailment where in fact there is none (for in-
stance, the pair ?John gave flowers to Mary/Mary
gave flowers to John has a perfect word overlap
but entailment does not hold). When removing
the backoff results i.e., when assigning all backoff
cases a negative entailment value, overall preci-
sion approximates 60%. In other words, on cases
such as those present in the SSI benchmark where
word overlap is generally high but the correla-
tion between word overlap and entailment value is
neutral, Nutcracker should be used without back-
off.
5 Finding the source of errors
The annotations contained in the automatically
constructed testsuite can help identify the most
likely sources of failures. We use (Sagot and de
La Clergerie, 2006)?s suspicion rate to compute
the probability that a given pair of sets of syntac-
tic tags is responsible for an RTE detection failure.
The tag set pairs with highest suspicion rate in-
dicate which syntactic phenomena often cooccurs
with failure.
More specifically, we store for each testsuite
item (T,H), all tag pairs (tj , hk) such that the syn-
tactic tags tj and hk are associated with the same
predicate Pi but tj occurs in T and hk in H. That is,
we collect the tag pairs formed by taking the tags
that label the occurrence of the same predicate on
both sides of the implication. If a predicate occurs
only in H then for each syntactic tag hk labelling
this predicate, the pair (nil, hk) is created. Con-
versely, if a predicate occurs only in T, the pair
(tj , nil) is added. Furthermore, the tags describ-
ing the subcategorisation type and the form of the
verb are grouped into a single tag so as to reduce
the tagset and limit data sparseness. For instance,
given the pair of sentences in Figure (1), the fol-
lowing tag pairs are produced:
(n0Va1:active:relSubj, nil)
(n0Va1:active:canAdj, nil)
50
system ERROR TN FN TP FP TN/N TP/P Prec
afazio 0 360 147 353 140 0.7200 0.7060 71.3%
nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w/o B.O.)
srl 0 487 437 63 13 0.9740 0.1260 55.0%
Table 1: Results of the three systems on the SSI-testsuite ( TN = true negatives, FN = false negatives,
TP = true positives, FP = false positives, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: no
parse tree found)
(n0Vn2n1:active:canSubj,n0Vn1Pn2:shortPassive:canSubj)
(n0Vn2n1:active:canSubj,n0Vn1Pn2:shortPassive:canIObj)
(n0Vn2n1:active:canObj,n0Vn1Pn2:shortPassive:canSubj)
(n0Vn2n1:active:canObj,n0Vn1Pn2:shortPassive:canIObj)
(n0Vn2n1:active:canIObj,n0Vn1Pn2:shortPassive:canSubj)
(n0Vn2n1:active:canIObj,n0Vn1Pn2:shortPassive:canIObj)
For each tag pair, we then compute the suspi-
cion rate of that pair using (Sagot and de La Clerg-
erie, 2006)?s fix point algorithm. To also take into
account pairs of sets of tags (rather than just pairs
of single tags), we furthermore preprocess the data
according to (de Kok et al, 2009)?s proposal for
handling n-grams.
Computing the suspicion rate of a tag pair.
The error mining?s suspicion rate algorithm of
(Sagot and de La Clergerie, 2006) is a fix point al-
gorithm used to detect the possible cause of pars-
ing failures. We apply this algorithm to the pair
of annotated sentences resulting from running the
three systems on the automatically created test-
suite as follows. Each such pair consists of a pair
of sentences, a set of tag pairs, an entailment value
(true or false) and a result value namely FP (false
positive), FN (false negative), TP (true positive) or
TN (true negative). To search for the most likely
causes of failure, we consider separately entail-
ments from non entailments. If entailment holds,
the suspicion rate of a sentence pair is 0 for true
positive and 1 for false positives. Conversely, if
entailment does not hold, the suspicion rate of the
sentence pair is 0 for true negatives and 1 for false
negatives.
The aim is to detect the tag pair most likely to
make entailment detection fail6. The algorithm it-
erates between tag pair occurrences and tag pair
forms, redistributing probabilities with each itera-
tion as follows. Initially, all tag pair occurrences
6We make the simplifying hypothesis that for each entail-
ment not recognised, a single tag pair or tag pair n-gram is
the cause of the failure.
in a given sentence have the same suspicion rate
namely, the suspicion rate of the sentence (1 if the
entailment could not be recognised, 0 otherwise)
divided by the number of tag pair occurrences in
that sentence. Next, the suspicion rate of a tag
pair form is defined as the average suspicion rate
of all occurrences of that tag pair. The suspicion
rate of a tag pair occurrence within each particular
sentence is then recalculated as the suspicion rate
of that tag pair form normalised by the suspicion
rates of the other tag pair forms occurring within
the same sentence. The iteration stops when the
process reaches a fixed point where the suspicion
rates have stabilised.
Extending the approach to pairs of tag sets.
To account for entailment recognition due to more
than one tag pair, we follow (de Kok et al, 2009)
and introduce a preprocessing step which first, ex-
pands tag pair unigrams to tag pair n-grams when
there is evidence that it is useful that is, when
an n-gram has a higher suspicion rate than each
of its sub n-grams. For this preprocessing, the
suspicion of a tag pair t is defined as the ratio
of t occurrences in unrecognised entailments and
the total number of t occurrences in the corpus.
To compensate for data sparseness, an additional
expansion factor is used which depends on the
frequency of an n-gram and approaches one for
higher frequency. In this way, long n-grams that
have low frequency are not favoured. The longer
the n-gram is, the more frequent or the more sus-
picious it needs to be in order to be selected by the
preprocessing step.
We apply this extension to the SSI setting. We
first extend the set of available tag pairs with tag
set pairs such that the suspicion rate of these pairs
is higher that the suspicion rate of each of the
smaller tagset pairs that can be constructed from
these sets. We then apply (Sagot and de La Clerg-
51
n0Vs1:act:CanSubj nil 0.85
n0Vn1:act:CanObj nil 0.46
n0V:betaVn nil 0.28
Table 2: The first 3 suspects for false positives
n0V:act n0V:act:RelCSubj 0.73
n0Vs1:act:CanSubj n0Vs1:act:CanSubj 0.69
n0V:act:RelOSubj n0V:betaVn
n0Vs1:act:CanSub n0Vs1:act:CanSubj 0.69
n0V:act:CanSubj n0V:betaVn
Table 3: The first 3 suspects for false negatives
erie, 2006)?s fix point algorithm to compute the
suspicion rate of the resulting tag pairs and tag sets
pairs.
Results and discussion. We now show how er-
ror mining can help shed some light on the most
probable sources of error when using Afazio.
For false positives (non entailment labelled
as entailment by Afazio), the 3 most suspect
tag pairs are given in Table 2. The first pair
(n0Vs1:act:CanSubj,nil) points out to cases such
as Bill sees the woman give the flower to the man
/ The man gives the flower to the woman. where
T contains a verb with a sentential argument not
present in H. In such cases, we found that the sen-
tential argument in T is usually incorrectly anal-
ysed, the analyses produced are fragmented and
entailment goes through. Similarly, the second
suspect (n0Vn1:act:CanObj,nil) points to cases
such as a man sees Lisa dancing / a man dances,
where the transitive verb in T has no counterpart in
H. Here the high number of analyses relied on by
Afazio together with the small size of H leads to
entailment detection: because we consider many
possible analyses for T and H and because H is
very short, one pair of analyses is found to match.
Finally, the third suspect (n0V:betaVn,nil) points
to cases such as Bill insists for the singing man to
dance / Bill dances where the gerund is wrongly
analysed and a relation is incorrectly established
by the parser between Bill and dance (in H).
For false negatives, the first suspect indicates
incorrect analyses for cases where an intransitive
with canonical subject in H is matched by an in-
transitive with covert relative subject (e.g., Bill
sees the woman give the flower to the man / the
man gives the flower to the woman.). The sec-
ond suspect points to cases such as Bill insists for
the man who sings to dance / Bill insists that the
singing man dances. where an embedded verb
with relative overt subject in T (sings) is matched
in H by an embedded gerund. Similarly, the third
suspect points to embedded verbs with canonical
subject matched by gerund verbs as in the man
who Bill insists that dances sings / Bill insists that
the singing man dances.
6 Conclusion
The development of a linguistically principled
treatment of the RTE task requires a clear under-
standing of the strength and weaknesses of RTE
systems w.r.t. to the various types of reasoning in-
volved. The main contribution of this paper is the
specification of an evaluation methodology which
permits a focused evaluation of syntax based rea-
soning on arbitrarily many inputs. As the results
show, there is room for improvment even on that
most basic level. In future work, we plan to extend
the approach to other types of inferences required
for textual entailment recognition. A more so-
phisticated compositional semantics in the gram-
mar used by the sentence generator would allow
for entailments involving more complex semantic
phenomena such as the interplay between implica-
tive verbs, polarity and downward/upward mono-
tonicity discussed in (Nairn et al, 2006). For in-
stance, it would allow for sentence pairs such as
Ed did not forget to force Dave to leave / Dave
left to be assigned the correct entailment value.
References
Bedaride, P. and C. Gardent. 2009. Noun/verb entail-
ment. In 4th Language and Technology Conference,
Poznan, Poland.
Burchardt, A., N. Reiter, S. Thater, and A. Frank.
2007. A semantic approach to textual entailment:
System evaluation and task analysis. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 10?16.
Cohen, K., W. Baumgartner, and L. Hunter. 2008.
Software testing and the naturally occurring data as-
sumption in natural language processing. In Proc.
of ?Software engineering, testing, and quality as-
surance for natural language processing ACL Work-
shop?.
52
Cooper, R., R. Crouch, J. van Eijck, C. Fox, J. van Gen-
abith, J. Jaspars, H. Kamp, M. Pinkal, D. Milward,
M. Poesio, and S. Pulman. 1995. A framework for
computational semantics, FraCaS. Technical report.
MS. Stanford University.
Copestake, A., D. Flickinger, C. Pollard, and I. A.
Sag. 2005. Minimal recursion semantics: an intro-
duction. Research on Language and Computation,
3.4:281?332.
Crouch, R., L. Karttunen, and A. Zaenen. 2006. Cir-
cumscribing is not excluding: A reply to manning.
MS. Palo Alto Research Center.
de Kok, D., J. Ma, and G. van Noord. 2009. A gen-
eralized method for iterative error mining in parsing
results. In Proceedings of the 2009 Workshop on
Grammar Engineering Across Frameworks (GEAF
2009), pages 71?79, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Galliers, J. R. and K. Sparck Jones. 1993. Evaluat-
ing natural language processing systems. Technical
report, Computer Laboratory, University of Cam-
bridge. Technical Report 291.
Gardent, C. and L. Kallmeyer. 2003. Semantic con-
struction in ftag. In Proceedings of the 10th meet-
ing of the European Chapter of the Association for
Computational Linguistics, Budapest, Hungary.
Gardent, C. and E. Kow. 2007. A symbolic approach
to near-deterministic surface realisation using tree
adjoining grammar. In ACL07.
Garoufi, K. 2007. Towards a better understanding of
applied textual entailment: Annotation and evalua-
tion of the rte-2 dataset. Master?s thesis, Saarland
University, Saarbrcken.
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan.
2007. The third pascal recognizing textual en-
tailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9.
Johansson, R. and P. Nugues. 2008. Dependency-
based syntactic-semantic analysis with propbank
and nombank. In CoNLL ?08: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 183?187, Morristown, NJ,
USA. Association for Computational Linguistics.
Kamp, H. and U. Reyle. 1993. From Discourse
to Logic. Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer.
Lehmann, S., S. Oepen, H. Baur, O. Lbdkan, and
D. Arnold. 1996. tsnlp ? test suites for natural
language processing. In In J. Nerbonne (Ed.), Lin-
guistic Databases. CSLI Publications.
Manning, C. D. 2006. Local textual inference: It?s
hard to circumscribe, but you know it when you see
it - and nlp needs it. MS. Stanford University.
Moll, D. and B. Hutchinson. 2003. Intrinsic versus
extrinsic evaluations of parsing systems. In Pro-
ceedings European Association for Computational
Linguistics (EACL), workshop on Evaluation Initia-
tives in Natural Language Processing, Budapest.
Nairn, R., C. Condoravdi, and L. Kartunen. 2006.
Computing relative polarity for textual inference. In
Proceedings of ICoS-5 (Inference in Computational
Semantics), Buxton, UK.
Sagot, B. and E. de La Clergerie. 2006. Error mining
in parsing results. In Proceedings of ACL-CoLing
06, pages 329?336, Sydney, Australie.
Vanderwende, L., D. Coughlin, and B. Dolan. 2005.
What syntax can contribute in entailment task. In
Proceedings of the First PASCAL RTE Workshop,
pages 13?17.
53
