Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2335?2344, Dublin, Ireland, August 23-29 2014.
Relation Classification via Convolutional Deep Neural Network
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{djzeng,kliu,swlai,gyzhou,jzhao}@nlpr.ia.ac.cn
Abstract
The state-of-the-art methods used for relation classification are primarily based on statistical ma-
chine learning, and their performance strongly depends on the quality of the extracted features.
The extracted features are often derived from the output of pre-existing natural language process-
ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders
the performance of these systems. In this paper, we exploit a convolutional deep neural network
(DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as
input without complicated pre-processing. First, the word tokens are transformed to vectors by
looking up word embeddings
1
. Then, lexical level features are extracted according to the given
nouns. Meanwhile, sentence level features are learned using a convolutional approach. These
two level features are concatenated to form the final extracted feature vector. Finally, the fea-
tures are fed into a softmax classifier to predict the relationship between two marked nouns. The
experimental results demonstrate that our approach significantly outperforms the state-of-the-art
methods.
1 Introduction
The task of relation classification is to predict semantic relations between pairs of nominals and can
be defined as follows: given a sentence S with the annotated pairs of nominals e
1
and e
2
, we aim
to identify the relations between e
1
and e
2
(Hendrickx et al., 2010). There is considerable interest in
automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP
applications.
The most representative methods for relation classification use supervised paradigm; such methods
have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu
and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided
into feature-based methods and kernel-based methods. Feature-based methods use a set of features that
are selected after performing textual analysis. They convert these features into symbolic IDs, which are
then transformed into a vector using a paradigm that is similar to the bag-of-words model
2
. Conversely,
kernel-based methods require pre-processed input data in the form of parse trees (such as dependency
parse trees). These approaches are effective because they leverage a large body of linguistic knowledge.
However, the extracted features or elaborately designed kernels are often derived from the output of pre-
existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the
performance of such systems (Bach and Badaskar, 2007). It is attractive to consider extracting features
that are as independent from existing NLP tools as possible.
To identify the relations between pairs of nominals, it is necessary to a skillfully combine lexical and
sentence level clues from diverse syntactic and semantic structures in a sentence. For example, in the
sentence ?The [fire]
e
1
inside WTC was caused by exploding [fuel]
e
2
?, to identify that fire and fuel are in a
This work is licenced under a Creative Commons Attribution 4.0 International License.Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
A word embedding is a distributed representation for a word. For example, Collobert et al. (2011) use a 50-dimensional
vector to represent a word.
2
http://en.wikipedia.org/wiki/Bag-of-words model
2335
Cause-Effect relationship, we usually leverage the marked nouns and the meanings of the entire sentence.
In this paper, we exploit a convolutional DNN to extract lexical and sentence level features for relation
classification. Our method takes all of the word tokens as input without complicated pre-processing,
such as Part-of-Speech (POS) tagging and syntactic parsing. First, all the word tokens are transformed
into vectors by looking up word embeddings. Then, lexical level features are extracted according to the
given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two
level features are concatenated to form the final extracted feature vector. Finally, the features are feed
into a softmax classifier to predict the relationship between two marked nouns.
The idea of extracting features for NLP using convolutional DNN was previously explored by Col-
lobert et al. (2011), in the context of POS tagging, chunking (CHUNK), Named Entity Recogni-
tion (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of Collobert
et al. (2011). In (Collobert et al., 2011), all of the tasks are considered as the sequential labeling prob-
lems in which each word in the input sentence is given a tag. However, our task, ?relation classification?,
can be considered a multi-class classification problem, which results in a different objective function.
Moreover, relation classification is defined as assigning relation labels to pairs of words. It is thus nec-
essary to specify which pairs of words to which we expect to assign relation labels. For that purpose, the
position features (PF) are exploited to encode the relative distances to the target noun pairs. To the best
of our knowledge, this work is the first example of using a convolutional DNN for relation classification.
The contributions of this paper can be summarized as follows.
? We explore the feasibility of performing relation classification without complicated NLP pre-
processing. A convolutional DNN is employed to extract lexical and sentence level features.
? To specify pairs of words to which relation labels should be assigned, position features are proposed
to encode the relative distances to the target noun pairs in the convolutional DNN.
? We conduct experiments using the SemEval-2010 Task 8 dataset. The experimental results demon-
strate that the proposed position features are critical for relation classification. The extracted lexical
and sentence level features are effective for relation classification. Our approach outperforms the
state-of-the-art methods.
2 Related Work
Relation classification is one of the most important topics in NLP. Many approaches have been explored
for relation classification, including unsupervised relation discovery and supervised classification. Re-
searchers have proposed various features to identify the relations between nominals using different meth-
ods.
In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory (Harris,
1954) indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is
assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa
et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply
selected the most frequent words in the contexts to represent the relation between the nominals. Chen
et al. (2005) proposed a novel unsupervised method based on model order selection and discriminative
label identification to address this problem.
In the supervised paradigm, relation classification is considered a multi-classification problem, and re-
searchers concentrate on extracting more complex features. Generally, these methods can be categorized
into two types: feature-based and kernel-based. In feature-based methods, a diverse set of strategies
have been exploited to convert the classification clues (such as sequences and parse trees) into feature
vectors (Kambhatla, 2004; Suchanek et al., 2006). Feature-based methods suffer from the problem
of selecting a suitable feature set when converting the structured representation into feature vectors.
Kernel-based methods provide a natural alternative to exploit rich representations of the input classifica-
tion clues, such as syntactic parse trees. Kernel-based methods allow the use of a large set of features
without explicitly extracting the features. Various kernels, such as the convolution tree kernel (Qian et
2336
WordRepresentation
FeatureExtraction
Output W3x
Figure 1: Architecture of the neural network used
for relation classification.
WindowProcessing
max over timesConvolution
tanh W2x
W1
WF
PF
Sentence levelFeatures
Figure 2: The framework used for extracting sen-
tence level features.
al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and
Mooney, 2005), have been proposed to solve the relation classification problem. However, the methods
mentioned above suffer from a lack of sufficient labeled data for training. Mintz et al. (2009) proposed
distant supervision (DS) to address this problem. The DS method selects sentences that match the facts
in a knowledge base as positive examples. The DS algorithm sometimes faces the problem of wrong
labels, which results in noisy labeled data. To address the shortcoming of DS, Riedel et al. (2010) and
Hoffmann et al. (2011) cast the relaxed DS assumption as multi-instance learning. Furthermore, Taka-
matsu et al. (2012) noted that the relaxed DS assumption would fail and proposed a novel generative
model to model the heuristic labeling process in order to reduce the wrong labels.
The supervised method has been demonstrated to be effective for relation detection and yields rela-
tively high performance. However, the performance of this method strongly depends on the quality of the
designed features. With the recent revival of interest in DNN, many researchers have concentrated on us-
ing Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed
representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al.
(2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in
the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto
et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting
of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality
features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical
and sentence level features for relation classification; our method effectively alleviates the shortcomings
of traditional features.
3 Methodology
3.1 The Neural Network Architecture
Figure 1 describes the architecture of the neural network that we use for relation classification. The
network takes an input sentence and discovers multiple levels of feature extraction, where higher levels
represent more abstract aspects of the inputs. It primarily includes the following three components: Word
Representation, Feature Extraction and Output. The system does not need any complicated syntactic or
semantic preprocessing, and the input of the system is a sentence with two marked nouns. Then, the
word tokens are transformed into vectors by looking up word embeddings. In succession, the lexical and
sentence level features are respectively extracted and then directly concatenated to form the final feature
vector. Finally, to compute the confidence of each relation, the feature vector is fed into a softmax
classifier. The output of the classifier is a vector, the dimension of which is equal to the number of
predefined relation types. The value of each dimension is the confidence score of the corresponding
relation.
2337
Features Remark
L1 Noun 1
L2 Noun 2
L3 Left and right tokens of noun 1
L4 Left and right tokens of noun 2
L5 WordNet hypernyms of nouns
Table 1: Lexical level features.
3.2 Word Representation
In the word representation component, each input word token is transformed into a vector by looking
up word embeddings. Collobert et al. (2011) reported that word embeddings learned from significant
amounts of unlabeled data are far more satisfactory than the randomly initialized embeddings. In relation
classification, we should first concentrate on learning discriminative word embeddings, which carry more
syntactic and semantic information, using significant amounts of unlabeled data. Unfortunately, it usually
takes a long time to train the word embeddings
3
. However, there are many trained word embeddings that
are freely available (Turian et al., 2010). A comparison of the available word embeddings is beyond
the scope of this paper. Our experiments directly utilize the trained embeddings provided by Turian et
al.(2010).
3.3 Lexical Level Features
Lexical level features serve as important cues for deciding relations. The traditional lexical level features
primarily include the nouns themselves, the types of the pairs of nominals and word sequences between
the entities, the quality of which strongly depends on the results of existing NLP tools. Alternatively,
this paper uses generic word embeddings as the source of base features. We select the word embeddings
of marked nouns and the context tokens. Moreover, the WordNet hypernyms
4
are adopted as MVRNN
(Socher et al., 2012). All of these features are concatenated into our lexical level features vector l. Table
1 presents the selected word embeddings that are related to the marked nouns in the sentence.
3.4 Sentence Level Features
As mentioned in section 3.2, all of the tokens are represented as word vectors, which have been demon-
strated to correlate well with human judgments of word similarity. Despite their success, single word
vector models are severely limited because they do not capture long distance features and semantic com-
positionality, the important quality of natural language that allows humans to understand the meanings
of a longer expression. In this section, we propose a max-pooled convolutional neural network to offer
sentence level representation and automatically extract sentence level features. Figure 2 shows the frame-
work for sentence level feature extraction. In the Window Processing component, each token is further
represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the
vector goes through a convolutional component. Finally, we obtain the sentence level features through a
non-linear transformation.
3.4.1 Word Features
Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend
to have similar meanings. To capture this characteristic, the WF combines a word?s vector representation
and the vector representations of the words in its context. Assume that we have the following sequence
of words.
S : [People]
0
have
1
been
2
moving
3
back
4
into
5
[downtown]
6
The marked nouns are associated with a label y that defines the relation type that the marked pair contains.
Each word is also associated with an index into the word embeddings. All of the word tokens of the
sentence S are then represented as a list of vectors (x
0
,x
1
, ? ? ? ,x
6
), where x
i
corresponds to the word
3
Collobert et al. (2011) proposed a pairwise ranking approach to train the word embeddings, and the total training time for
an English corpus (Wikipedia) was approximately four weeks.
4
http://sourceforge.net/projects/supersensetag/
2338
embedding of the i-th word in the sentence. To use a context size of w, we combine the size w windows
of vectors into a richer feature. For example, when we take w = 3, the WF of the third word ?moving?
in the sentence S is expressed as [x
2
,x
3
,x
4
]. Similarly, considering the whole sentence, the WF can be
represented as follows:
{[x
s
,x
0
,x
1
], [x
0
,x
1
,x
2
], ? ? ? , [x
5
,x
6
,x
e
]}
5
3.4.2 Position Features
Relation classification is a very complex task. Traditionally, structure features (e.g., the shortest depen-
dency path between nominals) are used to solve this problem (Bunescu and Mooney, 2005). Apparently,
it is not possible to capture such structure information only through WF. It is necessary to specify which
input tokens are the target nouns in the sentence. For this purpose, PF are proposed for relation classi-
fication. In this paper, the PF is the combination of the relative distances of the current word to w
1
and
w
2
. For example, the relative distances of ?moving? in sentence S to ?people? and ?downtown? are 3
and -3, respectively. In our method, the relative distances also are mapped to a vector of dimension d
e
(a
hyperparameter); this vector is randomly initialized. Then, we obtain the distance vectors d
1
and d
2
with
respect to the relative distances of the current word to w
1
and w
2
, and PF = [d
1
,d
2
]. Combining the WF
and PF, the word is represented as [WF,PF]
T
, which is subsequently fed into the convolution component
of the algorithm.
3.4.3 Convolution
We will see that the word representation approach can capture contextual information through combina-
tions of vectors in a window. However, it only produces local features around each word of the sentence.
In relation classification, an input sentence that is marked with target nouns only corresponds to a re-
lation type rather than predicting label for each word. Thus, it might be necessary to utilize all of the
local features and predict a relation globally. When using neural network, the convolution approach is a
natural method to merge all of the features. Similar to Collobert et al. (2011), we first process the output
of Window Processing using a linear transformation.
Z = W
1
X (1)
X ? R
n
0
?t
is the output of the Window Processing task, where n
0
= w? n, n (a hyperparameter) is the
dimension of feature vector, and t is the token number of the input sentence. W
1
? R
n
1
?n
0
, where n
1
(a
hyperparameter) is the size of hidden layer 1, is the linear transformation matrix. We can see that the
features share the same weights across all times, which greatly reduces the number of free parameters to
learn. After the linear transformation is applied, the output Z ? R
n
1
?t
is dependent on t. To determine
the most useful feature in the each dimension of the feature vectors, we perform a max operation over
time on Z.
m
i
= maxZ(i, ?) 0 ? i ? n
1
(2)
where Z(i, ?) denote the i-th row of matrix Z. Finally, we obtain the feature vector m =
{m
1
,m
2
, ? ? ? ,m
n
1
}, the dimension of which is no longer related to the sentence length.
3.4.4 Sentence Level Feature Vector
To learn more complex features, we designed a non-linear layer and selected hyperbolic tanh as the
activation function. One useful property of tanh is that its derivative can be expressed in terms of the
function value itself:
d
dx
tanhx = 1? tanh
2
x (3)
It has the advantage of making it easy to compute the gradient in the backpropagation training procedure.
Formally, the non-linear transformation can be written as
g = tanh(W
2
m) (4)
5
x
s
and x
e
are special word embeddings that correspond to the beginning and end of the sentence, respectively.
2339
W2
? R
n
2
?n
1
is the linear transformation matrix, where n
2
(a hyperparameter) is the size of hidden
layer 2. Compared with m ? R
n
1
?1
, g ? R
n
2
?1
can be considered higher level features (sentence level
features).
3.5 Output
The automatically learned lexical and sentence level features mentioned above are concatenated into a
single vector f = [l, g]. To compute the confidence of each relation, the feature vector f ? R
n
3
?1
(n
3
equals n
2
plus the dimension of the lexical level features) is fed into a softmax classifier.
o = W
3
f (5)
W
3
? R
n
4
?n
3
is the transformation matrix and o ? R
n
4
?1
is the final output of the network, where n
4
is equal to the number of possible relation types for the relation classification system. Each output can
be then interpreted as the confidence score of the corresponding relation. This score can be interpreted
as a conditional probability by applying a softmax operation (see Section 3.6).
3.6 Backpropagation Training
The DNN based relation classification method proposed here could be stated as a quintuple ? =
(X,N,W
1
,W
2
,W
3
)
6
. In this paper, each input sentence is considered independently. Given an in-
put example s, the network with parameter ? outputs the vector o, where the i-th component o
i
contains
the score for relation i. To obtain the conditional probability p(i|x, ?), we apply a softmax operation over
all relation types:
p(i|x, ?) =
e
o
i
n
4
?
k=1
e
o
k
(6)
Given all our (suppose T ) training examples (x
(i)
; y
(i)
), we can then write down the log likelihood of the
parameters as follows:
J (?) =
T
?
i=1
log p(y
(i)
|x
(i)
, ?) (7)
To compute the network parameter ?, we maximize the log likelihood J(?) using a simple optimization
technique called stochastic gradient descent (SGD). N,W
1
,W
2
and W
3
are randomly initialized and
X is initialized using the word embeddings. Because the parameters are in different layers of the neural
network, we implement the backpropagation algorithm: the differentiation chain rule is applied through
the network until the word embedding layer is reached by iteratively selecting an example (x, y) and
applying the following update rule.
? ? ? + ?
? log p(y|x, ?)
??
(8)
4 Dataset and Evaluation Metrics
To evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset (Hen-
drickx et al., 2010). The dataset is freely available
7
and contains 10,717 annotated examples, including
8,000 training instances and 2,717 test instances. There are 9 relationships (with two directions) and
an undirected Other class. The following are examples of the included relationships: Cause-Effect,
Component-Whole and Entity-Origin. In the official evaluation framework, directionality is taken into
account. A pair is counted as correct if the order of the words in the relationship is correct. For example,
both of the following instances S
1
and S
2
have the relationship Component-Whole.
S
1
: The [haft]
e
1
of the [axe]
e
2
is make ? ? ? ? Component-Whole(e
1
,e
2
)
S
2
: This [machine]
e
1
has two [units]
e
2
? ? ? ? Component-Whole(e
2
,e
1
)
6
N represents the word embeddings of WordNet hypernyms.
7
http://docs.google.com/View?id=dfvxd49s 36c28v9pmw
2340
?# Window size
1 2 3 4 5 6 7
F1
72
74
76
78
80
82
# Hidden layer 1
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
# Hidden layer 2
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
Figure 3: Effect of hyperparameters.
However, these two instances cannot be classified into the same category because Component-
Whole(e
1
,e
2
) and Component-Whole(e
2
,e
1
) are different relationships. Furthermore, the official rank-
ing of the participating systems is based on the macro-averaged F1-scores for the nine proper relations
(excluding Other). To compare our results with those obtained in previous studies, we adopt the macro-
averaged F1-score and also account for directionality into account in our following experiments
8
.
5 Experiments
In this section, we conduct three sets of experiments. The first is to test several variants via cross-
validation to gain some understanding of how the choice of hyperparameters impacts upon the perfor-
mance. In the second set of experiments, we make comparison of the performance among the convolu-
tional DNN learned features and various traditional features. The goal of the third set of experiments is
to evaluate the effectiveness of each extracted feature.
5.1 Parameter Settings
In this section, we experimentally study the effects of the three parameters in our proposed method:
the window size in the convolutional component w, the number of hidden layer 1, and the number of
hidden layer 2. Because there is no official development dataset, we tuned the hyperparameters by trying
different architectures via 5-fold cross-validation.
In Figure 3, we respectively vary the number of hyper parameters w, n
1
and n
2
and compute the F1.
We can see that it does not improve the performance when the window size is greater than 3. Moreover,
because the size of our training dataset is limited, the network is prone to overfitting, especially when
using large hidden layers. From Figure 3, we can see that the parameters have a limited impact on the
results when increasing the numbers of both hidden layers 1 and 2. Because the distance dimension has
little effect on the result (this is not illustrated in Figure 3), we heuristically choose d
e
= 5. Finally,
the word dimension and learning rate are the same as in Collobert et al. (2011). Table 2 reports all the
hyperparameters used in the following experiments.
Hyperparameter Window size Word dim. Distance dim. Hidden layer 1 Hidden layer 2 Learning rate
Value w = 3 n = 50 d
e
= 5 n
1
= 200 n
2
= 100 ? = 0.01
Table 2: Hyperparameters used in our experiments.
5.2 Results of Comparison Experiments
To obtain the final performance of our automatically learned features, we select seven approaches as com-
petitors to be compared with our method in Table 3. The first five competitors are described in Hendrickx
et al. (2010), all of which use traditional features and employ SVM or MaxEnt as the classifier. These
systems design a series of features and take advantage of a variety of resources (WordNet, ProBank,
and FrameNet, for example). RNN represents recursive neural networks for relation classification, as
8
The corpus contains a Perl-based automatic evaluation tool.
2341
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, stemming, syntactic patterns, WordNet 74.8
MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6
SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank,
FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner
82.2
RNN - 74.8
POS, NER, WordNet 77.6
MVRNN - 79.1
POS, NER, WordNet 82.4
Proposed word pair, words around word pair, WordNet 82.7
Table 3: Classifier, their feature sets and the F1-score for relation classification.
proposed by Socher et al. (2012). This method learns vectors in the syntactic tree path that connect two
nominals to determine their semantic relationship. The MVRNN model builds a single compositional
semantics for the minimal constituent, including both nominals as RNN (Socher et al., 2012). It is almost
certainly too much to expect a single fixed transformation to be able to capture the meaning combination
effects of all natural language operators. Thus, MVRNN assigns a matrix to every word and modifies the
meanings of other words instead of only considering word embeddings in the recursive procedure.
Table 3 illustrates the macro-averaged F1 measure results for these competing methods along with the
resources, features and classifier used by each method. Based on these results, we make the following
observations:
(1) Richer feature sets lead to better performance when using traditional features. This improvement
can be explained by the need for semantic generalization from training to test data. The quality of
traditional features relies on human ingenuity and prior NLP knowledge. It is almost impossible to
manually choose the best feature sets.
(2) RNN and MVRNN contain feature learning procedures; thus, they depend on the syntactic tree used
in the recursive procedures. Errors in syntactic parsing inhibit the ability of these methods to learn
high quality features. RNN cannot achieve a higher performance than the best method that uses
traditional features, even when POS, NER and WordNet are added to the training dataset. Compared
with RNN, the MVRNN model can capture the meaning combination effectively and achieve a higher
performance.
(3) Our method achieves the best performance among all of the compared methods. We also perform
a t-test (p 6 0.05), which indicates that our method significantly outperforms all of the compared
methods.
5.3 The Effect of Learned Features
Feature Sets F1
Lexical L1 34.7
+L2 53.1
+L3 59.4
+L4 65.9
+L5 73.3
Sentence WF 69.7
+PF 78.9
Combination all 82.7
Table 4: Score obtained for various sets of features on for the test set. The bottom portion of the table
shows the best combination of lexical and sentence level features.
In our method, the network extract lexical and sentence level features. The lexical level features pri-
marily contain five sets of features (L1 to L5). We performed ablation tests on the five sets of features
from the lexical part of Table 4 to determine which type of features contributed the most. The results are
2342
presented in Table 4, from which we can observe that our learned lexical level features are effective for
relation classification. The F1-score is improved remarkably when new features are added. Similarly, we
perform experiment on the sentence level features. The system achieves approximately 9.2% improve-
ments when adding PF. When all of the lexical and sentence level features are combined, we achieve the
best result.
6 Conclusion
In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence
level features for relation classification. In the network, position features (PF) are successfully proposed
to specify the pairs of nominals to which we expect to assign relation labels. The system obtains a
significant improvement when PF are added. The automatically learned features yield excellent results
and can replace the elaborately designed features that are based on the outputs of existing NLP tools.
Acknowledgments
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61272332, 61333018, 61202329, 61303180).
This work was supported in part by Noah?s Ark Lab of Huawei Tech. Co. Ltd. We thank the anonymous
reviewers for their insightful comments.
References
Nguyen Bach and Sameer Badaskar. 2007. A review of relation extraction. Literature review for Language and
Statistics II.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In
Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language
Processing, pages 724?731.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2005. Unsupervised feature selection for relation
extraction. In Proceedings of the International Joint Conference on Natural Language Processing, pages 262?
267.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zellig Harris. 1954. Distributional structure. Word, 10(23):146?162.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from
large corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages
415?422.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization
of recursive neural networks for semantic relation classification. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, pages 1372?1376.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O. S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic
Evaluation, SemEval ?10, pages 33?38.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 541?
550.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for
extracting relations. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics
on Interactive poster and demonstration sessions.
2343
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003?1011.
Raymond J Mooney and Razvan C Bunescu. 2005. Subsequence kernels for relation extraction. In Advances in
neural information processing systems, pages 171?178.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference
on Computational Linguistics, pages 697?704.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery
in databases: Part III, pages 148?163.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis
to extract relations from web documents. In Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 712?717.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers - Volume 1, pages 721?729.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The
Journal of Machine Learning Research, 3:1083?1106.
GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
2344
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1764?1773,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Opinion Words and Opinion Targets in a Two-Stage Framework
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, ybchen, jzhao}@nlpr.ia.ac.cn
Abstract
This paper proposes a novel two-stage
method for mining opinion words and
opinion targets. In the first stage, we
propose a Sentiment Graph Walking algo-
rithm, which naturally incorporates syn-
tactic patterns in a Sentiment Graph to ex-
tract opinion word/target candidates. Then
random walking is employed to estimate
confidence of candidates, which improves
extraction accuracy by considering confi-
dence of patterns. In the second stage, we
adopt a self-learning strategy to refine the
results from the first stage, especially for
filtering out high-frequency noise terms
and capturing the long-tail terms, which
are not investigated by previous meth-
ods. The experimental results on three real
world datasets demonstrate the effective-
ness of our approach compared with state-
of-the-art unsupervised methods.
1 Introduction
Opinion mining not only assists users to make in-
formed purchase decisions, but also helps busi-
ness organizations understand and act upon cus-
tomer feedbacks on their products or services in
real-time. Extracting opinion words and opinion
targets are two key tasks in opinion mining. Opin-
ion words refer to those terms indicating positive
or negative sentiment. Opinion targets represent
aspects or attributes of objects toward which opin-
ions are expressed. Mining these terms from re-
views of a specific domain allows a more thorough
understanding of customers? opinions.
Opinion words and opinion targets often co-
occur in reviews and there exist modified relations
(called opinion relation in this paper) between
them. For example, in the sentence ?It has a clear
screen?, ?clear? is an opinion word and ?screen? is
an opinion target, and there is an opinion relation
between the two words. It is natural to identify
such opinion relations through common syntactic
patterns (also called opinion patterns in this pa-
per) between opinion words and targets. For ex-
ample, we can extract ?clear? and ?screen? by us-
ing a syntactic pattern ?Adj-{mod}-Noun?, which
captures the opinion relation between them. Al-
though previous works have shown the effective-
ness of syntactic patterns for this task (Qiu et al,
2009; Zhang et al, 2010), they still have some lim-
itations as follows.
False Opinion Relations: As an example, the
phrase ?everyday at school? can be matched by
a pattern ?Adj-{mod}-(Prep)-{pcomp-n}-Noun?,
but it doesn?t bear any sentiment orientation. We
call such relations that match opinion patterns but
express no opinion false opinion relations. Pre-
vious pattern learning algorithms (Zhuang et al,
2006; Kessler and Nicolov, 2009; Jijkoun et al,
2010) often extract opinion patterns by frequency.
However, some high-frequency syntactic patterns
can have very poor precision (Kessler and Nicolov,
2009).
False Opinion Targets: In another case, the
phrase ?wonderful time? can be matched by
an opinion pattern ?Adj-{mod}-Noun?, which is
widely used in previous works (Popescu and Et-
zioni, 2005; Qiu et al, 2009). As can be seen, this
phrase does express a positive opinion but unfortu-
nately ?time? is not a valid opinion target for most
domains such as MP3. Thus, false opinion targets
are extracted. Due to the lack of ground-truth
knowledge for opinion targets, non-target terms
introduced in this way can be hardly filtered out.
Long-tail Opinion Targets: We further no-
tice that previous works prone to extract opinion
targets with high frequency (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Qiu et al, 2009; Zhu
et al, 2009), and they often have difficulty in iden-
tifying the infrequent or long-tail opinion targets.
1764
To address the problems stated above, this pa-
per proposes a two-stage framework for mining
opinion words and opinion targets. The under-
lying motivation is analogous to the novel idea
?Mine the Easy, Classify the Hard? (Dasgupta and
Ng, 2009). In our first stage, we propose a Senti-
ment Graph Walking algorithm to cope with the
false opinion relation problem, which mines easy
cases of opinion words/targets. We speculate that
it may be helpful to introduce a confidence score
for each pattern. Concretely, we create a Sen-
timent Graph to model opinion relations among
opinion word/target/pattern candidates and apply
random walking to estimate confidence of them.
Thus, confidence of pattern is considered in a uni-
fied process. Patterns that often extract false opin-
ion relations will have low confidence, and terms
introduced by low-confidence patterns will also
have low confidence accordingly. This could po-
tentially improve the extraction accuracy.
In the second stage, we identify the hard cases,
which aims to filter out false opinion targets and
extract long-tail opinion targets. Previous super-
vised methods have been shown to achieve state-
of-the-art results for this task (Wu et al, 2009; Jin
and Ho, 2009; Li et al, 2010). However, the big
challenge for fully supervised method is the lack
of annotated training data. Therefore, we adopt a
self-learning strategy. Specifically, we employ a
semi-supervised classifier to refine the target re-
sults from the first stage, which uses some highly
confident target candidates as the initial labeled
examples. Then opinion words are also refined.
Our main contributions are as follows:
? We propose a Sentiment Graph Walking al-
gorithm to mine opinion words and opinion
targets from reviews, which naturally incor-
porates confidence of syntactic pattern in a
graph to improve extraction performance. To
our best knowledge, the incorporation of pat-
tern confidence in such a Sentiment Graph
has never been studied before for opinion
words/targets mining task (Section 3).
? We adopt a self-learning method for refining
opinion words/targets generated by Sentiment
Graph Walking. Specifically, it can remove
high-frequency noise terms and capture long-
tail opinion targets in corpora (Section 4).
? We perform experiments on three real world
datasets, which demonstrate the effectiveness
of our method compared with state-of-the-art
unsupervised methods (Section 5).
2 Related Work
In opinion words/targets mining task, most unsu-
pervised methods rely on identifying opinion rela-
tions between opinion words and opinion targets.
Hu and Liu (2004) proposed an association mining
technique to extract opinion words/targets. The
simple heuristic rules they used may potentially
introduce many false opinion words/targets. To
identify opinion relations more precisely, subse-
quent research work exploited syntax information.
Popescu and Etzioni (2005) used manually com-
plied syntactic patterns and Pointwise Mutual In-
formation (PMI) to extract opinion words/targets.
Qiu et al (2009) proposed a bootstrapping frame-
work called Double Propagation which intro-
duced eight heuristic syntactic rules. While man-
ually defining syntactic patterns could be time-
consuming and error-prone, we learn syntactic
patterns automatically from data.
There have been extensive works on mining
opinion words and opinion targets by syntac-
tic pattern learning. Riloff and Wiebe (2003)
performed pattern learning through bootstrapping
while extracting subjective expressions. Zhuang
et al (2006) obtained various dependency re-
lationship templates from an annotated movie
corpus and applied them to supervised opinion
words/targets extraction. Kobayashi et al (2007)
adopted a supervised learning technique to search
for useful syntactic patterns as contextual clues.
Our approach is similar to (Wiebe and Riloff,
2005) and (Xu et al, 2013), all of which apply
syntactic pattern learning and adopt self-learning
strategy. However, the task of (Wiebe and Riloff,
2005) was to classify sentiment orientations in
sentence level, while ours needs to extract more
detailed information in term level. In addition,
our method extends (Xu et al, 2013), and we
give a more complete and in-depth analysis on
the aforementioned problems in the first section.
There were also many works employed graph-
based method (Li et al, 2012; Zhang et al, 2010;
Hassan and Radev, 2010; Liu et al, 2012), but
none of previous works considered confidence of
patterns in the graph.
In supervised approaches, various kinds of
models were applied, such as HMM (Jin and Ho,
2009), SVM (Wu et al, 2009) and CRFs (Li et al,
2010). The downside of supervised methods was
the difficulty of obtaining annotated training data
in practical applications. Also, classifiers trained
1765
on one domain often fail to give satisfactory re-
sults when shifted to another domain. Our method
does not rely on annotated training data.
3 The First Stage: Sentiment Graph
Walking Algorithm
In the first stage, we propose a graph-based al-
gorithm called Sentiment Graph Walking to mine
opinion words and opinion targets from reviews.
3.1 Opinion Pattern Learning for Candidates
Generation
For a given sentence, we first obtain its depen-
dency tree. Following (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Qiu et al, 2009), we regard all
adjectives as opinion word candidates (OC) and
all nouns or noun phrases as opinion target can-
didates (TC). A statistic-based method in (Zhu et
al., 2009) is used to detect noun phrases. Then
candidates are replaced by wildcards ?<OC>? or
?<TC>?. Figure 1 gives a dependency tree exam-
ple generated by Minipar (Lin, 1998).
p red s det
m od
gor geous<OC>
is(VBE)
style<TC>
the(Det)
of(P r ep) scr een<TC>pcom p-n
the(Det)
det
Figure 1: The dependency tree of the sentence
?The style of the screen is gorgeous?.
We extract two kinds of opinion patterns: ?OC-
TC? pattern and ?TC-TC? pattern. The ?OC-
TC? pattern is the shortest path between an OC
wildcard and a TC wildcard in dependency tree,
which captures opinion relation between an opin-
ion word candidate and an opinion target can-
didate. Similarly, the ?TC-TC? pattern cap-
tures opinion relation between two opinion tar-
get candidates.1 Words in opinion patterns are
replaced by their POS tags, and we constrain
that there are at most two words other than
wildcards in each pattern. In Figure 1, there
are two opinion patterns marked out by dash
lines: ?<OC>{pred}(VBE){s}<TC>? for the
?OC-TC? type and ?<TC>{mod}(Prep){pcomp-
n}<TC>? for the ?TC-TC? type. After all pat-
1We do not identify the opinion relation ?OC-OC? be-
cause this relation is often unreliable.
terns are generated, we drop those patterns with
frequency lower than a threshold F .
3.2 Sentiment Graph Construction
To model the opinion relations among opinion
words/targets and opinion patterns, a graph named
as Sentiment Graph is constructed, which is a
weighted, directed graph G = (V,E,W ), where
? V = {Voc ? Vtc ? Vp} is the set of vertices in
G, where Voc, Vtc and Vp represent the set of
opinion word candidates, opinion target can-
didates and opinion patterns, respectively.
? E = {Epo?Ept} ? {Vp?Voc}?{Vp?Vtc}
is the weighted, bi-directional edge set in G,
where Epo and Ept are mutually exclusive
sets of edges connecting opinion word/target
vertices to opinion pattern vertices. Note that
there are no edges between Voc and Vtc.
? W : E ? R+ is the weight function which
assigns non-negative weight to each edge.
For each (e : va ? vb) ? E, where
va, vb ? V , the weight function w(va, vb) =
freq(va, vb)/freq(va), where freq(?) is the
frequency of a candidate extracted by opinion
patterns or co-occurrence frequency between
two candidates.
Figure 2 shows an example of Sentiment Graph.
n icelarge
screen display
<OC>{mod}<TC> <OC>{mod}<TC>{con j}<TC>
1
0.8
0.7
0.2
0.3
0.4
0.2
0.33
0.33
0.33
0.6
0.4
0.2 0.2
Figure 2: An example of Sentiment Graph.
3.3 Confidence Estimation by Random
Walking with Restart
We believe that considering confidence of patterns
can potentially improve the extraction accuracy.
Our intuitive idea is: (i) If an opinion word/target
is with higher confidence, the syntactic patterns
containing this term are more likely to be used to
express customers? opinion. (ii) If an opinion pat-
tern has higher confidence, terms extracted by this
pattern are more likely to be correct. It?s a rein-
forcement process.
1766
We use Random Walking with Restart (RWR)
algorithm to implement our idea described above.
Let Moc p denotes the transition matrix from Voc
to Vp, for vo ? Voc, vp ? Vp, Moc p(vo, vp) =
w(vo, vp). Similarly, we have Mtc p, Mp oc,
Mp tc. Let c denotes confidence vector of candi-
dates so ctoc, cttc and ctp are confidence vectors for
opinion word/target/pattern candidates after walk-
ing t steps. Initially c0oc is uniformly distributed
on a few domain-independent opinion word seeds,
then the following formula are updated iteratively
until cttc and ctoc converge:
ct+1p = MToc p ? ctoc +MTtc p ? cttc (1)
ct+1oc = (1? ?)MTp oc ? ctp + ?c0oc (2)
ct+1tc = MTp tc ? ctp (3)
where MT is the transpose of matrix M and ? is
a small probability of teleporting back to the seed
vertices which prevents us from walking too far
away from the seeds. In the experiments below, ?
is set 0.1 empirically.
4 The Second Stage: Refining Extracted
Results Using Self-Learning
At the end of the first stage, we obtain a ranked
list of opinion words and opinion targets, in which
higher ranked terms are more likely to be correct.
Nevertheless, there are still some issues needed to
be addressed:
1) In the target candidate list, some high-
frequency frivolous general nouns such as
?thing? and ?people? are also highly ranked.
This is because there exist many opinion ex-
pressions containing non-target terms such as
?good thing?, ?nice people?, etc. in reviews.
Due to the lack of ground-truth knowledge
for opinion targets, the false opinion target
problem still remains unsolved.
2) In another aspect, long-tail opinion targets
may have low degree in Sentiment Graph.
Hence their confidence will be low although
they may be extracted by some high qual-
ity patterns. Therefore, the first stage is in-
capable of dealing with the long-tail opinion
target problem.
3) Furthermore, the first stage also extracts
some high-frequency false opinion words
such as ?every?, ?many?, etc. Many terms
of this kind are introduced by high-frequency
false opinion targets, for there are large
amounts of phrases like ?every time? and
?many people?. So this issue is a side effect
of the false opinion target problem.
To address these issues, we exploit a self-
learning strategy. For opinion targets, we use a
semi-supervised binary classifier called target re-
fining classifier to refine target candidates. For
opinion words, we use the classified list of opin-
ion targets to further refine the extracted opinion
word candidates.
4.1 Opinion Targets Refinement
There are two keys for opinion target refinement:
(i) How to generate the initial labeled data for tar-
get refining classifier. (ii) How to properly repre-
sent a long-tail opinion target candidate other than
comparing frequency between different targets.
For the first key, it is clearly improper to select
high-confidence targets as positive examples and
choose low-confidence targets as negative exam-
ples2, for there are noise with high confidence and
long-tail targets with low confidence. Fortunately,
a large proportion of general noun noises are the
most frequent words in common texts. Therefore,
we can generate a small domain-independent gen-
eral noun (GN) corpus from large web corpora to
cover some most frequently used general noun ex-
amples. Then labeled examples can be drawn from
the target candidate list and the GN corpus.
For the second key, we utilize opinion words
and opinion patterns with their confidence scores
to represent an opinion target. By this means, a
long-tail opinion target can be determined by its
own contexts, whose weights are learnt from con-
texts of frequent opinion targets. Thus, if a long-
tail opinion target candidate has high contextual
support, it will have higher probability to be found
out in despite of its low frequency.
Creation of General Noun Corpora. 1000
most frequent nouns in Google-1-gram3 were se-
lected as general noun candidates. On the other
hand, we added all nouns in the top three levels of
hyponyms in four WordNet (Miller, 1995) synsets
?object?, ?person?, ?group? and ?measure? into
the GN corpus. Our idea was based on the fact that
a term is more general when it sits in higher level
in the WordNet hierarchy. Then inapplicable can-
didates were discarded and a 3071-word English
2Note that the ?positive? and ?negative? here denote opin-
ion targets and non-target terms respectively and they do not
indicate sentiment polarities.
3http://books.google.com/ngrams.
1767
GN corpus was created. Another Chinese GN cor-
pus with 3493 words was generated in the similar
way from HowNet (Gan and Wong, 2000).
Generation of Labeled Examples. Let T =
{Y+1,Y?1} denotes the initial labeled set, where
N most highly confident target candidates but not
in our GN corpora are regarded as the positive ex-
ample set Y+1, other N terms from GN corpora
which are also top ranked in the target list are se-
lected as the negative example set Y?1. The re-
minder unlabeled candidates are denoted by T ?.
Feature Representation for Classifier. Given
T and T ? in the form of {(xi, yi)}. For a target
candidate ti, xi = (o1, . . . , on, p1, . . . , pm)T rep-
resents its feature vector, where oj is the opinion
word feature and pk is the opinion pattern feature.
The value of feature is defined as follows,
x(oj) = conf(oj)?
?
pk freq(ti, oj , pk)
freq(oj)
(4)
x(pk) = conf(pk)?
?
oj freq(ti, oj , pk)
freq(pk)
(5)
where conf(?) denotes confidence score estimated
by RWR, freq(?) has the same meaning as in Sec-
tion 3.2. Particularly, freq(ti, oj , pk) represents
the frequency of pattern pk extracting opinion tar-
get ti and opinion word oj .
Target Refinement Classifier: We use support
vector machine as the binary classifier. Hence, the
classification problem can be formulated as to find
a hyperplane < w, b > that separates both labeled
set T and unlabeled set T ? with maximum mar-
gin. The optimization goal is to minimize over
(T ,T ?,w, b, ?1, ..., ?n, ??1 , ..., ??k):
1
2 ||w||
2 + C
n?
i=0
?i + C?
k?
j=0
??j
subject to : ?ni=1 : yi[w ? xi + b] ? 1? ?i
?kj=1 : y?j [w ? x?j + b] ? 1? ??j
?ni=1 : ?i > 0
?kj=1 : ??j > 0
where yi, y?j ? {+1,?1}, xi and x?j represent
feature vectors, C and C? are parameters set by
user. This optimization problem can be imple-
mented by a typical Transductive Support Vector
Machine (TSVM) (Joachims, 1999).
4.2 Opinion Words Refinement
We use the classified opinion target results to re-
fine opinion words by the following equation,
s(oj) =
?
ti?T
?
pk
s(ti)conf(pk)freq(ti, oj , pk)
freq(ti)
where T is the opinion target set in which each el-
ement is classified as positive during opinion tar-
get refinement, s(ti) denotes confidence score ex-
ported by the target refining classifier. Particularly,
freq(ti) =
?
oj
?
pk freq(ti, oj , pk). A higher
score of s(oj) means that candidate oj is more
likely to be an opinion word.
5 Experiments
5.1 Datasets and Evaluation Metrics
Datasets: We select three real world datasets to
evaluate our approach. The first one is called
Customer Review Dataset (CRD) (Hu and Liu,
2004) which contains reviews on five different
products (represented by D1 to D5) in English.
The second dataset is pre-annotated and published
in COAE084, where two domains of Chinese re-
views are selected. At last, we employ a bench-
mark dataset in (Wang et al, 2011) and named it
as Large. We manually annotated opinion words
and opinion targets as the gold standard. Three
annotators were involved. Firstly, two annotators
were required to annotate out opinion words and
opinion targets in sentences. When conflicts hap-
pened, the third annotator would make the final
judgment. The average Kappa-values of the two
domains were 0.71 for opinion words and 0.66
for opinion targets. Detailed information of our
datasets is shown in Table 1.
Dataset Domain #Sentences #OW #OT
Large
(English)
Hotel 10,000 434 1,015
MP3 10,000 559 1,158
COAE08(Chinese)
Camera 2,075 351 892
Car 4,783 622 1,179
Table 1: The detailed information of datasets. OW
stands for opinion words and OT stands for targets.
Pre-processing: Firstly, HTML tags are re-
moved from texts. Then Minipar (Lin, 1998)
is used to parse English corpora, and Standford
Parser (Chang et al, 2009) is used for Chinese
4http://ir-china.org.cn/coae2008.html
1768
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
Ours-Stage1 0.79 0.85 0.82 0.82 0.87 0.84 0.83 0.87 0.85 0.78 0.88 0.83 0.82 0.88 0.85 0.84
Ours-Full 0.86 0.82 0.84 0.88 0.83 0.85 0.89 0.86 0.87 0.83 0.86 0.84 0.89 0.85 0.87 0.86
Table 2: Results of opinion target extraction on the Customer Review Dataset.
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
Ours-Stage1 0.61 0.75 0.67 0.55 0.80 0.65 0.63 0.75 0.68 0.60 0.69 0.64 0.68 0.70 0.69 0.67
Ours-Full 0.64 0.74 0.69 0.59 0.79 0.68 0.66 0.71 0.68 0.65 0.67 0.66 0.72 0.67 0.69 0.68
Table 3: Results of opinion word extraction on the Customer Review Dataset.
corpora. Stemming and fuzzy matching are also
performed following previous work (Hu and Liu,
2004).
Evaluation Metrics: We evaluate our method
by precision(P), recall(R) and F-measure(F).
5.2 Our Method vs. the State-of-the-art
Three state-of-the-art unsupervised methods are
used as competitors to compare with our method.
Hu extracts opinion words/targets by using ad-
jacency rules (Hu and Liu, 2004).
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009).
Zhang is an enhanced version of DP and em-
ploys HITS algorithm (Kleinberg, 1999) to rank
opinion targets (Zhang et al, 2010).
Ours-Full is the full implementation of our
method. We employ SVMlight (Joachims, 1999)
as the target refining classifier. Default parameters
are used except the bias item is set 0.
Ours-Stage1 only uses Sentiment Graph Walk-
ing algorithm which does?t have opinion word and
opinion target refinement.
All of the above approaches use same five
common opinion word seeds. The choice of opin-
ion seeds seems reasonable, as most people can
easily come up with 5 opinion words such as
?good?, ?bad?, etc. The performance on five prod-
ucts of CRD dataset is shown in Table 2 and Ta-
ble 3. Zhang does not extract opinion words so
their results for opinion words are not taken into
account. We can see that Ours-Stage1 achieves
superior recall but has some loss in precision com-
pared with DP and Zhang. This may be because
the CRD dataset is too small and our statistic-
based method may suffer from data sparseness.
In spite of this, Ours-Full achieves comparable F-
measure with DP, which is a well-designed rule-
based method.
The results on two larger datasets are shown
in Table 4 and Table 5, from which we can have
the following observation: (i) All syntax-based-
methods outperform Hu, showing the importance
of syntactic information in opinion relation identi-
fication. (ii) Ours-Full outperforms the three com-
petitors on all domains provided. (iii) Ours-Stage1
outperforms Zhang, especially in terms of recall.
We believe it benefits from our automatical pattern
learning algorithm. Moreover, Ours-Stage1 do
not loss much in precision compared with Zhang,
which indicates the applicability to estimate pat-
tern confidence in Sentiment Graph. (iv) Ours-
Full achieves 4-9% improvement in precision over
the most accurate method, which shows the effec-
tiveness of our second stage.
5.3 Detailed Discussions
This section gives several variants of our method
to have a more detailed analysis.
Ours-Bigraph constructs a bi-graph between
opinion words and targets, so opinion patterns
are not included in the graph. Then RWR algo-
rithm is used to only assign confidence to opinion
word/target candidates.
Ours-Stage2 only contains the second stage,
which doesn?t apply Sentiment Graph Walking al-
gorithm. Hence the confidence score conf(?) in
Equations (4) and (5) have no values and they are
set to 1. The initial labeled examples are exactly
the same as Ours-Full. Due to the limitation of
space, we only give analysis on opinion target ex-
traction results in Figure 3.
1769
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.53 0.55 0.54 0.55 0.57 0.56 0.63 0.65 0.64 0.62 0.58 0.60 0.58
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
Zhang 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
Ours-Stage1 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
Ours-Full 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
Table 4: Results of opinion targets extraction on Large and COAE08.
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.48 0.65 0.55 0.51 0.68 0.58 0.72 0.74 0.73 0.70 0.71 0.70 0.64
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.80 0.73 0.76 0.79 0.71 0.75 0.68
Ours-Stage1 0.59 0.69 0.64 0.61 0.71 0.66 0.79 0.78 0.78 0.77 0.77 0.77 0.71
Ours-Full 0.64 0.67 0.65 0.67 0.69 0.68 0.82 0.78 0.80 0.80 0.76 0.78 0.73
Table 5: Results of opinion words extraction on Large and COAE08.
Figure 3: Opinion target extraction results.
5.3.1 The Effect of Sentiment Graph Walking
We can see that our graph-based methods (Ours-
Bigraph and Ours-Stage1) achieve higher recall
than Zhang. By learning patterns automatically,
our method captures opinion relations more ef-
ficiently. Also, Ours-Stage1 outperforms Ours-
Bigraph, especially in precision. We believe it is
because Ours-Stage1 estimated confidence of pat-
terns so false opinion relations are reduced. There-
fore, the consideration of pattern confidence is
beneficial as expected, which alleviates the false
opinion relation problem. On another hand, we
find that Ours-Stage2 has much worse perfor-
mance than Ours-Full. This shows the effective-
ness of Sentiment Graph Walking algorithm since
the confidence scores estimated in the first stage
are indispensable and indeed key to the learning
of the second stage.
5.3.2 The Effect of Self-Learning
Figure 4 shows the average Precision@N curve of
four domains on opinion target extraction. Ours-
GN-Only is implemented by only removing 50
initial negative examples found by our GN cor-
pora. We can see that the GN corpora work quite
well, which find out most top-ranked false opin-
ion targets. At the same time, Ours-Full has much
better performance than Ours-GN-Only which in-
dicates that Ours-Full can filter out more noises
other than the initial negative examples. There-
fore, our self-learning strategy alleviates the short-
coming of false opinion target problem. More-
over, Table 5 shows that the performance of opin-
ion word extraction is also improved based on the
classified results of opinion targets.
Figure 4: The average precision@N curve of the
four domains on opinion target extraction.
1770
ID Pattern Example #Ext. Conf. PrO PrT
#1 <OC>{mod}<TC> it has a clear screen 7344 0.3938 0.59 0.66
#2 <TC>{subj}<OC> the sound quality is excellent 2791 0.0689 0.62 0.70
#3 <TC>{conj}<TC> the size and weight make it convenient 3620 0.0208 N/A 0.67
#4 <TC>{subj}<TC> the button layout is a simplistic plus 1615 0.0096 N/A 0.67
#5 <OC>{pnmod}<TC> the buttons easier to use 128 0.0014 0.61 0.34
#6 <TC>{subj}(V){s}(VBE){subj}<OC> software provided is simple 189 0.0015 0.54 0.33
#7 <OC>{mod}(Prep){pcomp-c}(V){obj}<TC> great for playing audible books 211 0.0013 0.43 0.48
Table 6: Examples of English patterns. #Ext. represent number of terms extracted, Conf. denotes confi-
dence score estimated by RWR and PrO/PrT stand for precisions of extraction on opinion words/targets
of a pattern respectively. Opinion words in examples are in bold and opinion targets are in italic.
Figure 5 gives the recall of long-tail opinion
targets5 extracted, where Ours-Full is shown to
have much better performance than Ours-Stage1
and the three competitors. This observation proves
that our method can improve the limitation of
long-tail opinion target problem.
Figure 5: The recall of long-tail opinion targets.
5.3.3 Analysis on Opinion Patterns
Table 6 shows some examples of opinion pattern
and their extraction accuracy on MP3 reviews in
the first stage. Pattern #1 and #2 are the two
most high-confidence opinion patterns of ?OC-
TC? type, and Pattern #3 and #4 demonstrate two
typical ?TC-TC? patterns. As these patterns ex-
tract too many terms, the overall precision is very
low. We give Precision@400 of them, which is
more meaningful because only top listed terms
in the extracted results are regarded as opinion
targets. Pattern #5 and #6 have high precision
on opinion words but low precision on opinion
targets. This observation demonstrates the false
opinion target problem. Pattern #7 is a pattern ex-
ample that extracts many false opinion relations
and it has low precision for both opinion words
and opinion targets. We can see that Pattern #7 has
5Since there is no explicit definition for the notion ?long-
tail?, we conservatively regard 60% opinion targets with the
lowest frequency as the ?long-tail? terms.
a lower confidence compared with Pattern #5 and
#6 although it extracts more words. It?s because
it has a low probability of walking from opinion
seeds to this pattern. This further proves that our
method can reduce the confidence of low-quality
patterns.
5.3.4 Sensitivity of Parameters
Finally, we study the sensitivity of parameters
when recall is fixed at 0.70. Figure 6 shows the
precision curves at different N initial training ex-
amples and F filtering frequency. We can see that
the performance saturates when N is set to 50 and
it does not vary much under different F , showing
the robustness of our method. We thus set N to
50, and F to 3 for CRD, 5 for COAE08 and 10 for
Large accordingly.
Figure 6: Influence of parameters.
1771
6 Conclusion and Future Work
This paper proposes a novel two-stage framework
for mining opinion words and opinion targets. In
the first stage, we propose a Sentiment Graph
Walking algorithm, which incorporates syntactic
patterns in a Sentiment Graph to improve the ex-
traction performance. In the second stage, we pro-
pose a self-learning method to refine the result of
first stage. The experimental results show that our
method achieves superior performance over state-
of-the-art unsupervised methods.
We further notice that opinion words are not
limited to adjectives but can also be other type of
word such as verbs or nouns. Identifying all kinds
of opinion words is a more challenging task. We
plan to study this problem in our future work.
Acknowledgement
Thanks to Prof. Yulan He for her insightful
advices. This work was supported by the Na-
tional Natural Science Foundation of China (No.
61070106, No. 61272332 and No. 61202329),
the National High Technology Development 863
Program of China (No. 2012AA011102), the
National Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, SSST
?09, pages 51?59.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to au-
tomatic sentiment classification. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 701?709.
Kok Wee Gan and Ping Wai Wong. 2000. Anno-
tating information structures in chinese texts using
hownet. In Proceedings of the second workshop on
Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for
Computational Linguistics - Volume 12, CLPW ?00,
pages 85?92, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 395?
403, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 585?594,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, ICML
?09, pages 465?472.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pages 200?209.
Jason Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking
of linguistic configurations. In Proceedings of the
Third International AAAI Conference on Weblogs
and Social Media.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-
of relations in opinion mining. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1065?1074, June.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
653?661, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
410?419, July.
1772
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on Evaluation of Parsing Sys-
tems at ICLRE.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ?12, pages 1346?1356,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, EMNLP ?03,
pages 105?112, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th international
conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?05, pages 486?497.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Walk and learn: A two-stage approach
for opinion words and opinion targets co-extraction.
In Proceedings of the 22nd International World Wide
Web Conference, WWW ?13.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM conference on Information and knowledge
management, CIKM ?09, pages 1799?1802.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management, CIKM
?06, pages 43?50.
1773
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 336?346,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Product Feature Mining: Semantic Clues versus Syntactic Constituents
Liheng Xu, Kang Liu, Siwei Lai and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, jzhao}@nlpr.ia.ac.cn
Abstract
Product feature mining is a key subtask
in fine-grained opinion mining. Previ-
ous works often use syntax constituents in
this task. However, syntax-based methods
can only use discrete contextual informa-
tion, which may suffer from data sparsity.
This paper proposes a novel product fea-
ture mining method which leverages lexi-
cal and contextual semantic clues. Lexical
semantic clue verifies whether a candidate
term is related to the target product, and
contextual semantic clue serves as a soft
pattern miner to find candidates, which ex-
ploits semantics of each word in context
so as to alleviate the data sparsity prob-
lem. We build a semantic similarity graph
to encode lexical semantic clue, and em-
ploy a convolutional neural model to cap-
ture contextual semantic clue. Then Label
Propagation is applied to combine both se-
mantic clues. Experimental results show
that our semantics-based method signif-
icantly outperforms conventional syntax-
based approaches, which not only mines
product features more accurately, but also
extracts more infrequent product features.
1 Introduction
In recent years, opinion mining has helped cus-
tomers a lot to make informed purchase decisions.
However, with the rapid growth of e-commerce,
customers are no longer satisfied with the over-
all opinion ratings provided by traditional senti-
ment analysis systems. The detailed functions or
attributes of products, which are called product
features, receive more attention. Nevertheless, a
product may have thousands of features, which
makes it impractical for a customer to investigate
them all. Therefore, mining product features au-
tomatically from online reviews is shown to be a
key step for opinion summarization (Hu and Liu,
2004; Qiu et al, 2009) and fine-grained sentiment
analysis (Jiang et al, 2011; Li et al, 2012).
Previous works often mine product features via
syntactic constituent matching (Popescu and Et-
zioni, 2005; Qiu et al, 2009; Zhang et al, 2010).
The basic idea is that reviewers tend to comment
on product features in similar syntactic structures.
Therefore, it is natural to mine product features by
using syntactic patterns. For example, in Figure 1,
the upper box shows a dependency tree produced
by Stanford Parser (de Marneffe et al, 2006), and
the lower box shows a common syntactic pattern
from (Zhang et al, 2010), where <feature/NN>
is a wildcard to be fit in reviews and NN denotes
the required POS tag of the wildcard. Usually, the
product name mp3 is specified, and when screen
matches the wildcard, it is likely to be a product
feature of mp3.
 
Figure 1: An example of syntax-based prod-
uct feature mining procedure. The word screen
matches the wildcard <feature/NN>. Therefore,
screen is likely to be a product feature of mp3.
Generally, such syntactic patterns extract prod-
uct features well but they still have some limita-
tions. For example, the product-have-feature pat-
tern may fail to find the fm tuner in a very similar
case in Example 1(a), where the product is men-
tioned by using player instead of mp3. Similarly,
it may also fail on Example 1(b), just with have re-
placed by support. In essence, syntactic pattern is
336
a kind of one-hot representation for encoding the
contexts, which can only use partial and discrete
features, such as some key words (e.g., have) or
shallow information (e.g., POS tags). Therefore,
such a representation often suffers from the data
sparsity problem (Turian et al, 2010).
One possible solution for this problem is us-
ing a more general pattern such as NP-VB-feature,
where NP represents a noun or noun phrase and
VB stands for any verb. However, this pattern be-
comes too general that it may find many irrelevant
cases such as the one in Example 1(c), which is not
talking about the product. Consequently, it is very
difficult for a pattern designer to balance between
precision and generalization.
Example 1:
(a) This player has an
::
fm
:::::
tuner.
(b) This mp3 supports
::::
wma
:::
file.
(c) This review has helped
:::::
people a lot.
(d) This mp3 has some
:::::
flaws.
To solve the problems stated above, it is ar-
gued that deeper semantics of contexts shall be ex-
ploited. For example, we can try to automatically
discover that the verb have indicates a part-whole
relation (Zhang et al, 2010) and support indicates
a product-function relation, so that both sth. have
and sth. support suggest that terms following them
are product features, where sth. can be replaced
by any terms that refer to the target product (e.g.,
mp3, player, etc.). This is called contextual se-
mantic clue. Nevertheless, only using contexts is
not sufficient enough. As in Example 1(d), we can
see that the word flaws follows mp3 have, but it
is not a product feature. Thus, a noise term may
be extracted even with high contextual support.
Therefore, we shall also verify whether a candi-
date is really related to the target product. We call
it lexical semantic clue.
This paper proposes a novel bootstrapping ap-
proach for product feature mining, which lever-
ages both semantic clues discussed above. Firstly,
some reliable product feature seeds are automat-
ically extracted. Then, based on the assumption
that terms that are more semantically similar to
the seeds are more likely to be product features,
a graph which measures semantic similarities be-
tween terms is built to capture lexical semantic
clue. At the same time, a semi-supervised con-
volutional neural model (Collobert et al, 2011) is
employed to encode contextual semantic clue. Fi-
nally, the two kinds of semantic clues are com-
bined by a Label Propagation algorithm.
In the proposed method, words are represented
by continuous vectors, which capture latent se-
mantic factors of the words (Turian et al, 2010).
The vectors can be unsupervisedly trained on large
scale corpora, and words with similar semantics
will have similar vectors. This enables our method
to be less sensitive to lexicon change, so that the
data sparsity problem can be alleviated . The con-
tributions of this paper include:
? It uses semantics of words to encode contextual
clues, which exploits deeper level information
than syntactic constituents. As a result, it mines
product features more accurately than syntax-
based methods.
? It exploits semantic similarity between words
to capture lexical clues, which is shown to be
more effective than co-occurrence relation be-
tween words and syntactic patterns. In addition,
experiments show that the semantic similarity
has the advantage of mining infrequent product
features, which is crucial for this task. For ex-
ample, one may say ?This hotel has low water
pressure?, where low water pressure is seldom
mentioned, but fatal to someone?s taste.
? We compare the proposed semantics-based ap-
proach with three state-of-the-art syntax-based
methods. Experiments show that our method
achieves significantly better results.
The rest of this paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed method in details. Section 4 gives the
experimental results. Lastly, we conclude this pa-
per in Section 5.
2 Related Work
In product feature mining task, Hu and Liu (2004)
proposed a pioneer research. However, the asso-
ciation rules they used may potentially introduce
many noise terms. Based on the observation that
product features are often commented on by simi-
lar syntactic structures, it is natural to use patterns
to capture common syntactic constituents around
product features.
Popescu and Etzioni (2005) designed some syn-
tactic patterns to search for product feature candi-
dates and then used Pointwise Mutual Information
(PMI) to remove noise terms. Qiu et al (2009)
proposed eight heuristic syntactic rules to jointly
extract product features and sentiment lexicons,
where a bootstrapping algorithm named Double
337
Propagation was applied to expand a given seed
set. Zhang et al (2010) improved Qiu?s work
by adding more feasible syntactic patterns, and the
HITS algorithm (Kleinberg, 1999) was employed
to rank candidates. Moghaddam and Ester (2010)
extracted product features by automatical opinion
pattern mining. Zhuang et al (2006) used various
syntactic templates from an annotated movie cor-
pus and applied them to supervised movie feature
extraction. Wu et al (2009) proposed a phrase
level dependency parsing for mining aspects and
features of products.
As discussed in the first section, syntactic pat-
terns often suffer from data sparsity. Further-
more, most pattern-based methods rely on term
frequency, which have the limitation of finding
infrequent but important product features. A re-
cent research (Xu et al, 2013) extracted infrequent
product features by a semi-supervised classifier,
which used word-syntactic pattern co-occurrence
statistics as features for the classifier. However,
this kind of feature is still sparse for infrequent
candidates. Our method adopts a semantic word
representation model, which can train dense fea-
tures unsupervisedly on a very large corpus. Thus,
the data sparsity problem can be alleviated.
3 The Proposed Method
We propose a semantics-based bootstrapping
method for product feature mining. Firstly, some
product feature seeds are automatically extracted.
Then, a semantic similarity graph is created to
capture lexical semantic clue, and a Convolutional
Neural Network (CNN) (Collobert et al, 2011) is
trained in each bootstrapping iteration to encode
contextual semantic clue. Finally we use Label
Propagation to find some reliable new seeds for
the training of the next bootstrapping iteration.
3.1 Automatic Seed Generation
The seed set consists of positive labeled examples
(i.e. product features) and negative labeled exam-
ples (i.e. noise terms). Intuitively, popular product
features are frequently mentioned in reviews, so
they can be extracted by simply mining frequently
occurring nouns (Hu and Liu, 2004). However,
this strategy will also find many noise terms (e.g.,
commonly used nouns like thing, one, etc.). To
produce high quality seeds, we employ a Domain
Relevance Measure (DRM) (Jiang and Tan, 2010),
which combines term frequency with a domain-
specific measuring metric called Likelihood Ratio
Test (LRT) (Dunning, 1993). Let ?(t) denotes the
LRT score of a product feature candidate t,
?(t) =
p
k
1
(1? p)
n
1
?k
1
p
k
2
(1? p)
n
2
?k
2
p
k
1
1
(1? p
1
)
n
1
?k
1
p
k
2
2
(1? p
2
)
n
2
?k
2
(1)
where k
1
and k
2
are the frequencies of t in the
review corpus R and a background corpus
1
B, n
1
and n
2
are the total number of terms in R and B,
p = (k
1
+ k
2
)/(n
1
+ n
2
), p
1
= k
1
/n
1
and p
2
=
k
2
/n
2
. Then a modified DRM
2
is proposed,
DRM(t) =
tf(t)
max[tf(?)]
?
1
log df(t)
?
| log ?(t)| ?min| log ?(?)|
max| log ?(?)| ?min| log ?(?)|
(2)
where tf(t) is the frequency of t inR and df(t) is
the frequency of t in B.
All nouns in R are ranked by DRM(t) in de-
scent order, where top N nouns are taken as the
positive example set V
+
s
. On the other hand, Xu
et al (2013) show that a set of general nouns sel-
dom appear to be product features. Therefore, we
employ their General Noun Corpus to create the
negative example set V
?
s
, where N most frequent
terms are selected. Besides, it is guaranteed that
V
+
s
? V
?
s
= ?, i.e., conflicting terms are taken as
negative examples.
3.2 Capturing Lexical Semantic Clue in a
Semantic Similarity Graph
To capture lexical semantic clue, each word is first
converted into word embedding, which is a con-
tinuous vector with each dimension?s value corre-
sponds to a semantic or grammatical interpretation
(Turian et al, 2010). Learning large-scale word
embeddings is very time-consuming (Collobert et
al., 2011), we thus employ a faster method named
Skip-gram model (Mikolov et al, 2013).
3.2.1 Learning Word Embedding for
Semantic Representation
Given a sequence of training words W =
{w
1
, w
2
, ..., w
m
}, the goal of the Skip-gram
model is to learn a continuous vector space EB =
{e
1
, e
2
, ..., e
m
}, where e
i
is the word embedding
of w
i
. The training objective is to maximize the
1
Google-n-Gram (http://books.google.com/ngrams) is
used as the background corpus.
2
The df(t) part of the original DRM is slightly modified
because we want a tf ? idf -like scheme (Liu et al, 2012).
338
average log probability of using word w
t
to pre-
dict a surrounding word w
t+j
,
?
EB = argmax
e
t
?EB
1
m
m
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
; e
t
)
(3)
where c is the size of the training window. Basi-
cally, p(w
t+j
|w
t
; e
t
) is defined as,
p(w
t+j
|w
t
; e
t
) =
exp(e
?
T
t+j
e
t
)
?
m
w=1
exp(e
?
T
w
e
t
)
(4)
where e
?
i
is an additional training vector associ-
ated with e
i
. This basic formulation is impracti-
cal because it is proportional to m. A hierarchical
softmax approximation can be applied to reduce
the computational cost to log
2
(m), see (Morin and
Bengio, 2005) for details.
To alleviate the data sparsity problem, EB is
first trained on a very large corpus
3
(denoted by
C), and then fine-tuned on the target review cor-
pusR. Particularly, for phrasal product features, a
statistic-based method in (Zhu et al, 2009) is used
to detect noun phrases in R. Then, an Unfold-
ing Recursive Autoencoder (Socher et al, 2011) is
trained on C to obtain embedding vectors for noun
phrases. In this way, semantics of infrequent terms
in R can be well captured. Finally, the phrase-
based Skip-gram model in (Mikolov et al, 2013)
is applied onR.
3.2.2 Building the Semantic Similarity Graph
Lexical semantic clue is captured by measuring se-
mantic similarity between terms. The underlying
motivation is that if we have known some product
feature seeds, then terms that are more semanti-
cally similar to these seeds are more likely to be
product features. For example, if screen is known
to be a product feature of mp3, and lcd is of high
semantic similarity with screen, we can infer that
lcd is also a product feature. Analogously, terms
that are semantically similar to negative labeled
seeds are not product features.
Word embedding naturally meets the demand
above: words that are more semantically similar
to each other are located closer in the embedding
space (Collobert et al, 2011). Therefore, we can
use cosine distance between two embedding vec-
tors as the semantic distance measuring metric.
Thus, our method does not rely on term frequency
3
Wikipedia(http://www.wikipedia.org) is used in practice.
to rank candidates. This could potentially improve
the ability of mining infrequent product features.
Formally, we create a semantic similarity graph
G = (V,E,W ), where V = {V
s
? V
c
} is the
vertex set, which contains the labeled seed set V
s
and the unlabeled candidate set V
c
; E is the edge
set which connects every vertex pair (u, v), where
u, v ? V ; W = {w
uv
: cos(EB
u
, EB
v
)} is a
function which associates a weight to each edge.
3.3 Encoding Contextual Semantic Clue
Using Convolutional Neural Network
The CNN is trained on each occurrence of seeds
that is found in review texts. Then for a candidate
term t, the CNN classifies all of its occurrences.
Since seed terms tend to have high frequency in
review texts, only a few seeds will be enough to
provide plenty of occurrences for the training.
3.3.1 The architecture of the Convolutional
Neural Network
The architecture of the Convolutional Neural Net-
work is shown in Figure 2. For a product feature
candidate t in sentence s, every consecutive sub-
sequence q
i
of s that containing t with a window
of length l is fed to the CNN. For example, as
in Figure 2, if t = {screen}, and l = 3, there
are three inputs: q
1
= [the, ipod, screen], q
2
=
[ipod, screen, is], q
3
= [screen, is, impressive].
Partially, t is replaced by a token ?*PF*? to re-
move its lexicon influence
4
.
 
Figure 2: The architecture of the Convolutional
Neural Network.
To get the output score, q
i
is first converted into
a concatenated vector x
i
= [e
1
; e
2
; ...; e
l
], where
e
j
is the word embedding of the j-th word. In
this way, the CNN serves as a soft pattern miner:
4
Otherwise, the CNN will quickly get overfitting on t, be-
cause very few seed lexicons are used for the training.
339
since words that have similar semantics have sim-
ilar low-dimension embedding vectors, the CNN
is less sensitive to lexicon change. The network is
computed by,
y
(1)
i
= tanh(W
(1)
x
i
+ b
(1)
) (5)
y
(2)
= max(y
(1)
i
) (6)
y
(3)
= W
(3)
y
(2)
+ b
(3)
(7)
where y
(i)
is the output score of the i-th layer, and
b
(i)
is the bias of the i-th layer; W
(1)
? R
h?(nl)
and W
(3)
? R
2?h
are parameter matrixes, where
n is the dimension of word embedding, and h is
the size of nodes in the hidden layer.
In conventional neural models, the candidate
term t is placed in the center of the window. How-
ever, from Example 2, when l = 5, we can see that
the best windows should be the bracketed texts
(Because, intuitively, the windows should contain
mp3, which is a strong evidence for finding the
product feature), where t = {screen} is at the
boundary. Therefore, we use Equ. 6 to formulate
a max-convolutional layer, which is aimed to en-
able the CNN to find more evidences in contexts
than conventional neural models.
Example 2:
(a) The [screen of this mp3 is] great.
(b) This [mp3 has a great screen].
3.3.2 Training
Let ? = {EB,W
(?)
, b
(?)
} denotes all the trainable
parameters. The softmax function is used to con-
vert the output score of the CNN to a probability,
p(t|X; ?) =
exp(y
(3)
)
?
|C|
j=1
exp(y
(3)
j
)
(8)
whereX is the input set for term t, andC = {0, 1}
is the label set representing product feature and
non-product feature, respectively.
To train the CNN, we first use V
s
to collect each
occurrence of the seeds in R to form a training
set T
s
. Then, the training criterion is to minimize
cross-entropy over T
s
,
?
? = argmin
?
|T
s
|
?
i=1
? log ?
i
p(t
i
|X
i
; ?) (9)
where ?
i
is the binomial target label distribution
for one entry. Backpropagation algorithm with
mini-batch stochastic gradient descent is used to
solve this optimization problem. In addition, some
useful tricks can be applied during the training.
The weight matrixes W
(?)
are initialized by nor-
malized initialization (Glorot and Bengio, 2010).
W
(1)
is pre-trained by an autoencoder (Hinton,
1989) to capture semantic compositionality. To
speed up the learning, a momentum method is ap-
plied (Sutskever et al, 2013).
3.4 Combining Lexical and Contextual
Semantic Clues by Label Propagation
We propose a Label Propagation algorithm to
combine both semantic clues in a unified process.
Each term t ? V is assumed to have a label dis-
tribution L
t
= (p
+
t
, p
?
t
), where p
+
t
denotes the
probability of the candidate being a product fea-
ture, and on the contrary, p
?
t
= 1? p
+
t
. The clas-
sified results of the CNN which encode contextual
semantic clue serve as the prior knowledge,
I
t
=
?
?
?
(1, 0), if t ? V
+
s
(0, 1), if t ? V
?
s
(r
+
t
, r
?
t
), if t ? V
c
(10)
where (r
+
t
, r
?
t
) is estimated by,
r
+
t
=
count
+
(t)
count
+
(t) + count
?
(t)
(11)
where count
+
(t) is the number of occurrences of
term t that are classified as positive by the CNN,
and count
?
(t) represents the negative count.
Label Propagation is applied to propagate the
prior knowledge distribution I to the product fea-
ture distribution L via semantic similarity graph
G, so that a product feature candidate is deter-
mined by exploring its semantic relations to all of
the seeds and other candidates globally. We pro-
pose an adapted version on the random walking
view of the Adsorption algorithm (Baluja et al,
2008) by updating the following formula until L
converges,
L
i+1
= (1? ?)M
T
L
i
+ ?DI (12)
where M is the semantic transition matrix built
from G; D = Diag[log tf(t)] is a diagonal ma-
trix of log frequencies, which is designed to as-
sign higher ?confidence? scores to more frequent
seeds; and ? is a balancing parameter. Particu-
larly, when ? = 0, we can set the prior knowledge
I without V
c
to L
0
so that only lexical semantic
clue is used; otherwise if ? = 1, only contextual
semantic clue is used.
340
3.5 The Bootstrapping Framework
We summarize the bootstrapping framework of the
proposed method in Algorithm 1. During boot-
strapping, the CNN is enhanced by Label Propaga-
tion which finds more labeled examples for train-
ing, and then the performance of Label Propaga-
tion is also improved because the CNN outputs a
more accurate prior distribution. After running for
several iterations, the algorithm gets enough seeds,
and a final Label Propagation is conducted to pro-
duce the results.
Algorithm 1: Bootstrapping using semantic clues
Input: The review corpusR, a large corpus C
Output: The mined product feature list P
Initialization: Train word embedding set EB first on
C, and then onR
Step 1: Generate product feature seeds V
s
(Section 3.1)
Step 2: Build semantic similarity graph G (Section 3.2)
while iter < MAX ITER do
Step 3: Use V
s
to collect occurrence set T
s
fromR
for training
Step 4: Train a CNNN on T
s
(Section 3.3)
Apply mini-batch SGD on Equ. 9;
Step 5: Run Label Propagation (Section 3.4)
Classify candidates usingN to setup I;
L
0
? I;
repeat
L
i+1
? (1? ?)M
T
L
i
+ ?DI;
until ||L
i+1
? L
i
||
2
< ?;
Step 6: Expand product feature seeds
Move top T terms from V
c
to V
s
;
iter++
end
Step 7: Run Label Propagation for a final result L
f
Rank terms by L
+
f
to get P , where L
+
f
> L
?
f
;
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: We select two real world datasets to
evaluate the proposed method. The first one
is a benchmark dataset in Wang et al (2011),
which contains English review sets on two do-
mains (MP3 and Hotel)
5
. The second dataset is
proposed by Chinese Opinion Analysis Evalua-
tion 2008 (COAE 2008)
6
, where two review sets
(Camera and Car) are selected. Xu et al (2013)
had manually annotated product features on these
four domains, so we directly employ their annota-
tion as the gold standard. The detailed information
can be found in their original paper.
5
http://timan.cs.uiuc.edu/downloads.html
6
http://ir-china.org.cn/coae2008.html
Evaluation Metrics: We evaluate the proposed
method in terms of precision(P), recall(R) and F-
measure(F). The English results are evaluated by
exact string match. And for Chinese results, we
use an overlap matching metric, because deter-
mining the exact boundaries is hard even for hu-
man (Wiebe et al, 2005).
4.2 Experimental Settings
For English corpora, the pre-processing are the
same as that in (Qiu et al, 2009), and for Chinese
corpora, the Stanford Word Segmenter (Chang
et al, 2008) is used to perform word segmenta-
tion. We select three state-of-the-art syntax-based
methods to be compared with our method:
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009), which is
a conventional syntax-based method.
DP-HITS is an enhanced version of DP pro-
posed by Zhang et al (2010), which ranks product
feature candidates by
s(t) = log tf(t) ? importance(t) (13)
where importance(t) is estimated by the HITS al-
gorithm (Kleinberg, 1999).
SGW is the Sentiment Graph Walking algo-
rithm proposed in (Xu et al, 2013), which first
extracts syntactic patterns and then uses random
walking to rank candidates. Afterwards, word-
syntactic pattern co-occurrence statistic is used
as feature for a semi-supervised classifier TSVM
(Joachims, 1999) to further refine the results. This
two-stage method is denoted as SGW-TSVM.
LEX only uses lexical semantic clue. Label
Propagation is applied alone in a self-training
manner. The dimension of word embedding n =
100, the convergence threshold ? = 10
?7
, and the
number of expanded seeds T = 40. The size of
the seed set N is 40. To output product features,
it ranks candidates in descent order by using the
positive score L
+
f
(t).
CONT only uses contextual semantic clue,
which only contains the CNN. The window size
l is 5. The CNN is trained with a mini-batch size
of 50. The hidden layer size h = 250. Finally,
importance(t) in Equ. 13 is replaced with r
+
t
in
Equ. 11 to rank candidates.
LEX&CONT leverages both semantic clues.
341
Method
MP3 Hotel Camera Car Avg.
P R F P R F P R F P R F F
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
SGW 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
LEX 0.64 0.74 0.69 0.65 0.75 0.70 0.69 0.84 0.76 0.68 0.78 0.73 0.72
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72 0.71
SGW-TSVM 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
LEX&CONT 0.74 0.75 0.74 0.75 0.77 0.76 0.80 0.84 0.82 0.79 0.79 0.79 0.78
Table 1: Experimental results of product feature mining. The precision or recall of CONT is the average
performance over five runs with different random initialization of parameters of the CNN. Avg. stands
for the average score.
4.3 The Semantics-based Methods vs.
State-of-the-art Syntax-based Methods
The experimental results are shown in Table 1,
from which we have the following observations:
(i) Our method achieves the best performance
among all of the compared methods. We
also equally split the dataset into five sub-
sets, and perform one-tailed t-test (p ? 0.05),
which shows that the proposed semantics-
based method (LEX&CONT) significantly out-
performs the three syntax-based strong com-
petitors (DP, DP-HITS and SGW-TSVM).
(ii) LEX&CONT which leverages both lexical and
contextual semantic clues outperforms ap-
proaches that only use one kind of semantic
clue (LEX and CONT), showing that the com-
bination of the semantic clues is helpful.
(iii) Our methods which use only one kind of
semantic clue (LEX and CONT) outperform
syntax-based methods (DP, DP-HITS and
SGW). Comparing DP-HITS with LEX and
CONT, the difference between them is that
DP-HITS uses a syntax-pattern-based algo-
rithm to estimate importance(t) in Equ. 13,
while our methods use lexical or contextual se-
mantic clue instead. We believe the reason that
LEX or CONT is better is that syntactic pat-
terns only use discrete and local information.
In contrast, CONT exploits latent semantics of
each word in context, and LEX takes advantage
of word embedding, which is induced from
global word co-occurrence statistic. Further-
more, comparing SGW and LEX, both methods
are base on random surfer model, but LEX gets
better results than SGW. Therefore, the word-
word semantic similarity relation used in LEX
is more reliable than the word-syntactic pattern
relation used in SGW.
(iv) LEX&CONT achieves the highest recall
among all of the evaluated methods. Since
DP and DP-HITS rely on frequency for rank-
ing product features, infrequent candidates are
ranked low in their extracted list. As for SGW-
TSVM, the features they used for the TSVM
suffer from the data sparsity problem for in-
frequent terms. In contrast, LEX&CONT is
frequency-independent to the review corpus.
Further discussions on this observation are
given in the next section.
4.4 The Results on Extracting Infrequent
Product Features
We conservatively regard 30% product features
with the highest frequencies in R as frequent fea-
tures, so the remaining terms in the gold standard
are infrequent features. In product feature mining
task, frequent features are relatively easy to find.
Table 2 shows the recall of all the four approaches
for mining frequent product features. We can see
that the performance are very close among differ-
ent methods. Therefore, the recall mainly depends
on mining the infrequent features.
Method MP3 Hotel Camera Car
DP 0.89 0.92 0.86 0.84
DP-HITS 0.89 0.91 0.86 0.85
SGW-TSVM 0.87 0.92 0.88 0.87
LEX&CONT 0.89 0.91 0.89 0.87
Table 2: The recall of frequent product features.
Figure 3 gives the recall of infrequent prod-
uct features, where LEX&CONT achieves the best
performance. So our method is less influenced
by term frequency. Furthermore, LEX gets better
recall than CONT and all syntax-based methods,
which indicates that lexical semantic clue does aid
to mine more infrequent features as expected.
342
 1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(a) MP3
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(b) Hotel
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(c) Camera
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(d) Car
Figure 4: Accuracy (y-axis) of product feature seed expansion at each bootstrapping iteration (x-axis).
The error bar shows the standard deviation over five runs.
Method
MP3 Hotel Camera Car
P R F P R F P R F P R F
FW-5 0.62 0.63 0.62 0.64 0.64 0.64 0.68 0.73 0.70 0.67 0.66 0.66
FW-9 0.64 0.65 0.64 0.66 0.68 0.67 0.70 0.76 0.73 0.71 0.70 0.70
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72
Table 3: The results of convolutional method vs. the results of non-convolutional methods.
MP3 Hotel Camera Car
Reca
ll
.4
.5
.6
.7
.8
.9 DPDP-HITSSGW-TSVMCONTLEXLEX&CONT
Figure 3: The recall of infrequent features. The
error bar shows the standard deviation over five
different runs.
4.5 Lexical Semantic Clue vs. Contextual
Semantic Clue
This section studies the effects of lexical seman-
tic clue and contextual semantic clue during seed
expansion (Step 6 in Algorithm 1), which is con-
trolled by ?. When ? = 1, we get the CONT; and
if ? is set 0, we get the LEX. To take into account
the correctly expanded terms for both positive and
negative seeds, we use Accuracy as the evaluation
metric,
Accuracy =
#TP + #TN
# Extracted Seeds
where TP denotes the true positive seeds, and TN
denotes the true negative seeds.
Figure 4 shows the performance of seed ex-
pansion during bootstrapping, in which the accu-
racy is computed on 40 seeds (20 being positive
and 20 being negative) expanded in each itera-
tion. We can see that the accuracies of CONT and
LEX&CONT retain at a high level, which shows
that they can find reliable new product feature
seeds. However, the performance of LEX oscil-
lates sharply and it is very low for some points,
which indicates that using lexical semantic clue
alone is infeasible. On another hand, comparing
CONT with LEX in Table 1, we can see that LEX
performs generally better than CONT. Although
LEX is not so accurate as CONT during seed ex-
pansion, its final performance surpasses CONT.
Consequently, we can draw conclusion that CONT
is more suitable for the seed expansion, and LEX
is more robust for the final result production.
To combine advantages of the two kinds of se-
mantic clues, we set ? = 0.7 in Step 5 of Algo-
rithm 1, so that contextual semantic clue plays a
key role to find new seeds accurately. For Step 7,
we set ? = 0.3. Thus, lexical semantic clue is
emphasized for producing the final results.
4.6 The Effect of Convolutional Layer
Two non-convolutional variations of the proposed
method are used to be compared with the convo-
lutional method in CONT. FW-5 uses a traditional
neural network with a fixed window size of 5 to
replace the CNN in CONT, and the candidate term
to be classified is placed in the center of the win-
dow. Similarly, FW-9 uses a fixed window size
of 9. Note that CONT uses a 5-term dynamic
window containing the candidate term, so the ex-
ploited number of words in the context is equiva-
lent to FW-9.
343
Table 3 shows the experimental results. We can
see that the performance of FW-5 is much worse
than CONT. The reason is that FW-5 only exploits
half of the context as that of CONT, which is not
sufficient enough. Meanwhile, although FW-9 ex-
ploits equivalent range of context as that of CONT,
it gets lower precisions. It is because FW-9 has
approximately two times parameters in the param-
eter matrix W
(1)
than that in Equ. 5 of CONT,
which makes it more difficult to be trained with
the same amount of data. Also, lengths of many
sentences in the review corpora are shorter than 9.
Therefore, the convolutional approach in CONT is
the most effective way among these settings.
4.7 Parameter Study
We investigate two key parameters of the proposed
method: the initial number of seeds N , and the
size of the window l used by the CNN.
Figure 5 shows the performance under differ-
ent N , where the F-Measure saturates when N
equates to 40 and beyond. Hence, very few seeds
are needed for starting our algorithm.
 
N
10 20 30 40 50 60
F-Measure
.65
.70
.75
.80
.85
MP3
Hote l
Came ra
Car
Figure 5: F-Measure vs. N for the final results.
Figure 6 shows F-Measure under different win-
dow size l. We can see that the performance is
improved little when l is larger than 5. Therefore,
l = 5 is a proper window size for these datasets.
 
l
2 3 4 5 6 7
F-Measure
.5
.6
.7
.8
.9
MP3
Hote l
Came ra
Car
Figure 6: F-Measure vs. l for the final results.
5 Conclusion and Future Work
This paper proposes a product feature mining
method by leveraging contextual and lexical se-
mantic clues. A semantic similarity graph is built
to capture lexical semantic clue, and a convo-
lutional neural network is used to encode con-
textual semantic clue. Then, a Label Propaga-
tion algorithm is applied to combine both seman-
tic clues. Experimental results prove the effec-
tiveness of the proposed method, which not only
mines product features more accurately than con-
ventional syntax-based method, but also extracts
more infrequent product features.
In future work, we plan to extend the proposed
method to jointly mine product features along with
customers? opinions on them. The learnt seman-
tic representations of words may also be utilized
to predict fine-grained sentiment distributions over
product features.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2012CB316300), the National Natural Sci-
ence Foundation of China (No. 61272332 and
No. 61202329), the National High Technol-
ogy Development 863 Program of China (No.
2012AA011102), and CCF-Tencent Open Re-
search Fund. This work was also supported in
part by Noahs Ark Lab of Huawei Tech. Ltm.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak
Ravichandran, and Mohamed Aly. 2008. Video
suggestion and discovery for youtube: Taking ran-
dom walks through the view graph. In Proceedings
of the 17th International Conference on World Wide
Web, WWW ?08, pages 895?904, New York, NY,
USA. ACM.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, StatMT ?08, pages 224?232.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
344
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL?06 Workshop on
Spoken Language Technology.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the International Con-
ference on Artificial Intelligence and Statistics.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artificial Intelligence, 40(1C3):185 ? 234.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Xing Jiang and Ah-Hwee Tan. 2010. Crctol: A
semantic-based domain ontology learning system.
Journal of the American Society for Information Sci-
ence and Technology, 61(1):150?168.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the 16th International Conference on
Machine Learning, pages 200?209.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 410?419, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: An unsupervised opinion miner from
unstructured product reviews. In Proceedings of
the 19th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ?10, pages
1825?1828, New York, NY, USA. ACM.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on arti-
ficial intelligence and statistics, AISTATS05, pages
246?252.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS?2011, vol-
ume 24, pages 801?809.
Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. Distributed representations of
words and phrases and their compositionality. In
Proceedings of the 30 th International Conference
on Machine Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, EMNLP ?09, pages 1533?
1541, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
345
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining opinion words and opinion tar-
gets in a two-stage framework. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1764?1773, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1462?1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM Conference on Information and Knowledge
Management, CIKM ?09, pages 1799?1802, New
York, NY, USA. ACM.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM International Conference
on Information and Knowledge Management, CIKM
?06, pages 43?50, New York, NY, USA. ACM.
346
