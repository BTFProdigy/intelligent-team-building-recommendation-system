Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732?742,
Dublin, Ireland, August 23-24, 2014.
UNAL-NLP: Combining Soft Cardinality Features for Semantic
Textual Similarity, Relatedness and Entailment
Sergio Jimenez, George Due
?
nas,
and Julia Baquero
Universidad Nacional de Colombia
Ciudad Universitaria, edificio 453,
oficina 114, Bogot?a, Colombia
[sgjimenezv,geduenasl,
jmbaquerov]@unal.edu.co
Alexander Gelbukh
Center for Computing Research (CIC),
Instituto Polit?ecnico Nacional (IPN),
Av. Juan Dios B?atiz, Av. Mendiz?abal,
Col. Nueva Industrial Vallejo,
Mexico City, Mexico
www.gelbukh.com
Abstract
This paper describes our participation in
the SemEval-2014 tasks 1, 3 and 10. We
used an uniform approach for addressing
all the tasks using the soft cardinality for
extracting features from text pairs, and
machine learning for predicting the gold
standards. Our submitted systems ranked
among the top systems in all the task and
sub-tasks in which we participated. These
results confirm the results obtained in pre-
vious SemEval campaigns suggesting that
the soft cardinality is a simple and useful
tool for addressing a wide range of natural
language processing problems.
1 Introduction
The semantic textual similarity is a core prob-
lem in the computational linguistic field. Con-
sequently, the previous evaluation campaigns of
this task in SemEval have attracted the attention
of many research groups worldwide (Agirre et al.,
2012; Agirre et al., 2013).This year, 3 tasks related
to this problem have been proposed exploring dif-
ferent facets such as semantic relatedness, entail-
ment , multilingualism, lack of training data and
imbalance in the amount of information.
The soft cardinality (Jimenez et al., 2010) is a
simple concept that generalizes the classical set
cardinality by considering the similarities among
the elements in a collection for a more intuitive
quantification of the number of elements in that
collection. This approach can be applied to text
applications representing texts as collections of
words and providing a similarity function that
compares two words. Varying this word-to-word
similarity function the soft cardinality can reflect
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
notions of syntactic similarity, semantic related-
ness, among others. We (and others) have used
this approach to address with success the semantic
textual similarity and other tasks in previous Se-
mEval editions (Jimenez et al., 2012b; Jimenez et
al., 2012a; Jimenez et al., 2013a; Jimenez et al.,
2013b; Jimenez et al., 2013c; Croce et al., 2013).
In this paper we describe our participating sys-
tems in the SemEval-2014 tasks 1, 3, and 10,
which used the soft cardinality as core approach.
2 Features from Soft Cardinalities
The cardinality of a collection of elements is the
counting of non-repeated elements in it. This def-
inition is intrinsically associated with the notion
of set, which is a collection of non-repeated ele-
ments.Thus, the cardinality of a collection or set
A is denoted as |A|. Clearly, the cardinality of a
collection with repeated elements treats groups of
identical elements as a single instance contribut-
ing only with a unit (1) to the element counting.
Jimenez et al. (2010) proposed the soft cardinal-
ity that uses a notion of similarity among elements
for grouping not only identical elements but simi-
lar too. That notion of similarity among elements
is provided by a similarity function that compares
two elements a
i
and a
j
and returns a score in [0,1]
interval, having sim(a
i
, a
i
) = 1. Although, it
is not necessary that sim fulfills another metric
properties aside of identity, symmetry is also de-
sirable. Thus, the soft cardinality of a collection
A, whose elements a
1
, a
2
, . . . , a
|A|
are compara-
ble with a similarity function sim(a
i
, a
j
), is de-
noted as |A|
sim
. This soft cardinality is given by
the following expression:
|A|
sim
=
|A|
?
i=1
w
a
i
?
|A|
j=1
sim(a
i
, a
j
)
p
(1)
It is trivial to see that |A| = |A|
sim
either if
p ? ? or when the function sim is a crisp com-
732
Basic Derived
|A| |A ?B| = |A|+ |B| ? |A ?B|
|B| |A4B| = |A ?B| ? |A ?B||
|A ?B| |A \B| = |A| ? |A ?B|
|B \A| = |B| ? |A ?B|
Table 1: The 7 basic and derived cardinalities for
two sets comparison.
parator, i.e. one that returns 1 for identical ele-
ments and 0 otherwise. This property shows that
the soft cardinality generalizes the classical cardi-
nality and that the parameter p controls its degree
of ?softness?, whose default value is 1. The values
w
a
i
are optional ?importance? weights associated
with each element a
i
, by default those weights can
be assigned to 1.
For the tasks at hand, we represent each short
text (lets say A) as a collection of words a
i
and
the sim function can be any operator that com-
pares pairs of words. The motivation for using the
soft cardinality is that the sim function can reflect
any dimension of word similarity (e.g. syntactic,
semantic) and the soft cardinality projects that no-
tion at sentence level. For instance, if sim pro-
vides the degree of semantic relatedness between
two words using WordNet, two texts A and B
could be compared by computing |A|
sim
, |B|
sim
and |A?B|
sim
. Given that A?B could be empty,
the soft cardinality of the intersection must be ap-
proximated by |A ? B|
sim
? |A|
sim
+ |B|
sim
?
|A ? B|
sim
instead of being computed directly
from A ? B using equation 1. Using that approx-
imation, the commonality (intersection) between
A and B is induced by the pair-wise similarities
provided by sim among the words in A and B.
Since more than a century when Jaccard (1901)
proposed his well-known index, the classical set
cardinality has been used to build similarity func-
tions for set comparison. Any binary-cardinality-
based similarity function is an algebraic combina-
tion of |A|, |B| and either |A ? B| or |A ? B|
(e.g. Jaccard, Dice, Tversky, overlap and cosine
indexes). These three cardinalities describes un-
ambiguously all the regions in the Venn?s diagram
when comparing two sets. Thus, in this scenario 4
possible cardinalities can be derived from these 3
basic cardinalities, see Table 1. Clearly, the same
set of cardinalities can be obtained for the soft car-
dinality.
When training data is available, which is the
# Feature expression
1
|A|
/|A?B|
2
|A|?|A?B|
/|A|
3
|A|?|A?B|
/|A?B|
4
|B|
/|A?B|
5
|B|?|A?B|
/|B|
6
|B|?|A?B|
/|A?B|
7
|A?B|
/|A|
8
|A?B|
/|B|
9
|A?B|
/|A?B|
10
|A?B|?|A?B|
/|A?B|
Table 2: Extended set of 10 rational features.
case for tasks 1, 3 and 10 in SemEval 2014, it
is possible to think that instead of using an ad-
hoc expression (e.g. Jaccard, Dice) the similar-
ity function can be obtained using the cardinalities
in Table 1 as features for a machine-learning re-
gression algorithm. Our hypothesis is that such
learnt function should predict in a more accurate
way the gold standard variable than any other ad-
hoc function. However, these cardinality features
are intrinsically correlated with the length of the
texts where they were obtained. This correlation
makes that the performance of the learnt similar-
ity function could be dependent of the length of
the texts. For instance, if the function was trained
using long texts it is plausible to think that this
function would be more effective when tested with
long texts than with shorter ones. Having this in
mind, an extended set of rational features is pro-
posed, whose values are standardized in [0,1] in-
terval aiming to reduce the effect of the length of
the texts. These features are presented in Table 2.
The soft cardinality has proven to overcome
the classic cardinality in the semantic textual
similarity (STS) task in previous SemEval cam-
paigns (Jimenez et al., 2012b; Jimenez et al.,
2013a). Even using a simplistic function sim
based on q-grams of characters, the soft cardinal-
ity method ranked third among 89 participating
systems (Agirre et al., 2012). Thus, our participat-
ing systems in the SemEval 2014 campaign were
based on the previously described set of 17 fea-
tures, obtained from the soft cardinality with dif-
ferent sim functions for comparing pairs of words.
Each sim function produced a different set of fea-
tures, which were combined with a regression al-
gorithm for similarity and relatedness tasks. Sim-
ilarly, a classification algorithm was used for the
733
entailment task.
3 Systems Description
In this section the different feature sets used for
each submitted system to the different task and
subtask are described. Besides, the data used for
training, parameters and other preprocessing de-
tails are described for each system.
3.1 Task 1: Textual Relatedness and
Entailment
The task 1 is based on the SICK (Sentences
Involving Compositional Knowledge) data set
(Marelli et al., 2014), which contains nearly
10,000 pairs of sentences manually labeled by re-
latedness and entailment. The relatedness gold la-
bels range from 1 to 5, having 1 the minimum level
of relatedness between the texts and 5 for the max-
imum. The entailment labels have three categori-
cal values: neutral, contradiction and entailment.
The two sub tasks consist of predicting the related-
ness and entailment gold standards using approxi-
mately the 50% of the text pairs as training and the
other part as test bed.
Our overall approach consists in extracting 4
different sets of features using the method pre-
sented in section 2 and training a machine learn-
ing algorithm for predicting the gold standard la-
bels in the test data. Each feature set is described
in the following 4 subsections and the subsection
3.1.6 provides details of the used combination of
features, machine learning algorithm and prepro-
cessing details.
3.1.1 String-Matching Features
First, all texts in the SICK data set where prepro-
cessed by lower casing, tokenizing and stop-word
removal (using the NLTK
1
). Then each word was
reduced to its stem using the Porter?s algorithm
(Porter, 1980) and a idf weight (Jones, 2004) was
associated to each stem (w
a
i
weights in eq. 1) us-
ing the very SICK data set as document collec-
tion. Next, for each instance in the data, which
is composed of two texts A and B, the 17 fea-
tures listed in Tables 1 and 2 where extracted using
eq.1. The used word-to-word similarity function
sim decomposes each word in bags of 3-grams
of characters, which are compared using the sym-
metrical Tversky?s index (Tversky, 1977; Jimenez
et al., 2013a). Thus, the similarity between two
1
http://www.nltk.org/
pairs of words w
1
and w
2
, represented each one as
a collection of 3-grams of characters, is given by
the following expression:
sim(w
1
, w
2
) =
|c|
?(?|w
min
|+ (1? ?)|w
max
|) + |c|
(2)
|c| = |w
1
? w
2
|+ bias
sim
,
|w
min
| = min[|w
1
\ w
2
|, |w
2
\ w
1
|],
|w
max
| = max[|w
1
\ w
2
|, |w
2
\ w
1
|].
The values used for the parameters were ? =
1.9, ? = 2.36, bias = ?0.97, and p = 0.39
(where p corresponds to eq.1). The motivation and
justification for these parameters can be found in
(Jimenez et al., 2013a). These values were ob-
tained by building a text similarity function us-
ing the Dice?s coefficient and the soft cardinali-
ties plugging eq.2 in eq.1. Next, this text similar-
ity function is evaluated in the 5,000 training text
pairs and the obtained scores are compared against
the relatedness gold-standard using the Pearson?s
correlation.
w
a
i
are not training parameters, but they are
weights associated with the words. These weights
could have been obtained from a larger corpus,
but we use the training texts to obtain them. This
process is repeated iteratively exploring the search
space defined by these 4 parameters using a hill-
climbing approach until a maximum correlation is
reached. We observe that the optimal values of the
parameters p, ?, ?, and bias vary considerably be-
tween the data sets and for the different sim func-
tions of word-to-word similarity. We do not yet
understand from which factors of the data and the
sim functions depend on these parameters. This
issue will be the objective of further research.
Henceforth, the set of 17 string-based features
described in this subsection will be referred as
SM.
3.1.2 ESA Features
For this set of features we used the idea proposed
by Gabrilovich and Markovitch (2007) of enrich-
ing the representation of a text by representing
each word by its textual definition in a knowl-
edge base, i.e. explicit semantic analysis (ESA).
For that, we used as knowledge base the synset?s
textual definitions provided by WordNet. First,
in order to determine the textual definition asso-
ciated to each word, the texts were tagged using
734
the maximum entropy POS tagger included in the
NLTK. Next, the adapted Lesk algorithm (Baner-
jee and Pedersen, 2002) for word sense disam-
biguation was applied in the texts disambiguating
one word at the time. The software package used
for this disambiguation process was pywsd
2
. The
arguments needed for the disambiguation of each
word are the POS tag of the target word and the
entire sentence as context. Once all the words are
disambiguated with their corresponding WordNet
synsets, each word is replaced by all the words in
their textual definition jointly with the same word
and its lemma. The final result of this stage is that
each text in the data set is replaced by a longer
text including the original text and some related
words. The motivation of this procedure is that the
extended versions of each pair of texts have more
chance of sharing common words that the original
texts.
The extended versions of these texts were used
to obtain another 17 features with the same proce-
dure described in the previous subsection (3.1.1).
This feature subset will henceforth be referred as
ESA.
3.1.3 Features for each part-of-speech
category
This set of features is motivated by the idea pro-
posed by Corley and Mihalcea (2005) of group-
ing words by their POS category before being
compared for semantic textual similarity. Our ap-
proach consist in provide a version of each text
pair in the data set for each POS category in-
cluding only the words belonging to that cate-
gory. For instance, the pair of texts {?A beauti-
ful girl is playing tennis?, ?A nice and handsome
boy is playing football?} produce new pairs such
as: {?beautiful?, ?nice handsome?} for the ADJ
tag, {?girl tennis?, ?boy football?} for NOUN and
{?is playing?, ?is playing?} for VERB.
Again, the POS tags were provided by the
NLTK?s max entropy tagger. The 28 POS cate-
gories were simplified to 9 categories in order to
avoid an excessive number of features and hence
sparseness; the used mapping is shown in Table 3.
Next, for each one of the 9 new POS categories a
set of 17 features (SM) is extracted reusing again
the method proposed in subsection 3.1.1. The only
difference with the method described in that sub-
section is that the stop-words were not removed
2
https://github.com/alvations/pywsd
Reduced tag set NLTK?s POS tag set
ADJ JJ,JJR,JJS
NOUN NN,NNP,NNPS,NNS
ADV RB,RBR,RBS,WRB
VERB VB,VBD,VBG,VBN,VBP,VBZ
PRO WP,WP$,PRP,PRP$
PREP RP,IN
DET PDT,DT,WDT
EX EX
CC CC
Table 3: Mapping reduction of the POS tag set.
and the stemming process was not performed. The
motivation for generating this feature sets by POS
category is that the machine learning algorithms
could weight differently each category. The intu-
ition behind this is that it is reasonable that cat-
egories such as VERB and NOUN could play a
more important role for the task at hand than oth-
ers such as ADJ or PREP. Using these categorized
features, such discrimination among POS cate-
gories can be discovered from the training data.
Finally, the total number of features in this set is
153 (17 features? 9 POS categories). This feature
set will be referred as POS.
3.1.4 Features From Dependencies
The syntactic soft cardinality (Croce et al., 2012;
Croce et al., 2013) extend the soft cardinality
approach by representing texts as bags of de-
pendencies instead of bags of words. Each de-
pendency is a 3-tuple composed of two syntac-
tically related words and the type of their rela-
tionship. For instance, the sentence ?The boy
plays football? can be represented with 3 depen-
dencies: [det,?boy?,?The?], [subj,?plays?,?boy?]
and [obj,?plays?,?football?]. Clearly, this repre-
sentation distinguish pairs of texts such as {?The
dog bites a boy?,?The boy bites a dog?}, which
are indistinguishable when they are represented as
bags of words. This representation can be obtained
automatically using the Stanford Parser (De Marn-
effe et al., 2006), which in addition provides a de-
pendency identifying the root word in a sentence.
We used the version 3.3.1
3
of that parser to obtain
such representation.
Once the texts are represented as bags of de-
pendencies, it is necessary to provide a similar-
ity function between two dependency tuples in or-
3
http://nlp.stanford.edu/software/lex-parser.shtml
735
der to use the soft cardinality (eq. 1) and hence
to obtain the 17 cardinality features in Tables 1
and 2. Such function can be obtained using the
sim function (eq. 2) for comparing the first and
second words between the dependencies and even
the labels of the dependency types. Let?s consider
two dependencies tuples d = [d
dep
, d
w
1
, d
w
2
] and
p = [p
dep
, p
w
1
, p
w
2
] where d
dep
and p
dep
are the
labels of the dependency type; d
w
1
and p
w
1
are
the first words on each dependency tuple; and d
w
2
and p
w
2
are the second words. The similarity func-
tion for comparing two dependency tuples can be a
linear combination of the sim scores between the
corresponding elements of the dependency tuples
by the following expression:
sim
dep
(d, p) =
?sim(d
dep
, p
dep
) + ?sim(d
w
1
, p
w
2
) + ?sim(d
w
2
, p
w
2
)
Although, it is unusual to compare the depen-
dencies? type labels d
dep
and p
dep
with a similar-
ity function designed for words, we observed ex-
perimentally that this approach yield better overall
performance in the relatedness task in comparison
with a simple crisp comparison. The optimal val-
ues for the parameters ? = ?3, ? = 10 and ? = 3
were determined with the same methodology used
in subsection 3.1.1 for determining ?, ? and bias.
Clearly, the fact that ? > ? means that the first
words in the dependency tuples plays a more im-
portant role than the second ones for the task at
hand. However, the fact that ? < 0 is counter intu-
itive because it means that the lower the similarity
between the dependency type labels is, the larger
the similarity between the two dependencies. Up
to date we have been unable to find a plausible ex-
planation for this phenomenon. This set of 17 fea-
tures will be referred hereinafter as DEP.
3.1.5 Additional Features
In addition to the feature sets based in soft car-
dinality, we designed some features aimed to ad-
dress linguistic phenomena such as antonymy, hy-
pernymy and negation.
Antonymy: Consider the following text pair
from the test data {?A man is emptying a container
made of plastic?,?A man is filling a container
made of plastic? }, which is labeled as a contra-
diction with a relatedness score of 3.91. Clearly,
these labels are explained by the antonymy rela-
tion between ?emptying? and ?filling?. Given that
none of the features presented above address this
issue, a list of 11,028 pairs of antonym words was
gathered from several web sites (see Table 4) and
from the antonymy relationships in WordNet, in
order to detect these cases. That list was used to
count the number of occurrences of pairs antonym
words between pairs of texts and in each one of
the texts. Thus, for any pair of texts A and B (rep-
resented as sets of words), three features (referred
henceforth as ANT) were extracted:
antonym AB Counts the number of occurrences
of pairs of antonyms in A ? B (Cartesian
product) or in B ?A .
antonym AA Counts the number of occurrences
of pairs of antonyms in A?A.
antonym BB Counts the number of occurrences
of pairs of antonyms in B ?B.
Hypernymy: Consider the following text pair
from the test data {?A man is sitting comfortably
at a table?,?A person is sitting comfortably at the
table? }, which is labeled as an entailment with
a relatedness score of 3.96. In this case, the en-
tailment is based on the hypernymy between ?per-
son? and ?man?. In order to capture this linguis-
tic factor 3 features similar to the previously de-
scribed antonym features were proposed. First,
word sense disambiguation was performed (as de-
scribed in subsection 3.1.2) for obtaining a synset
label for each word. Secondly, we build a bi-
nary function hyp(ss
1
, ss
2
) that takes two Word-
Net synsets as arguments and returns 1 if ss
1
is
a hypernym of ss
2
with a maximum depth in the
WordNet?s is-a hierarchy of 6 steps, and 0 oth-
erwise. This hypernymy function was build us-
ing the WordNet interface provided by the NLTK.
Next, based on that synset-to-synset function, a
text-to-text function that captures the degree or hy-
pernymy in a text or in a pair of texts was build us-
ing the Monge-Elkan measure (Monge and Elkan,
1996). Thus, for two texts A and B represented
as sets of synset labels, the following expression
measures their degree of hypernymy:
HY P (A,B) =
1
|A|
|A|
?
i=1
|B|
max
j=1
hyp(a
i
, b
j
)
Using the function HY P (?, ?), 3 features are
extracted from each pair of text (referred hence-
forth as HYP):
hypernym AB from HY P (A,B)
736
http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.php
http://www.allaboutspace.com/wordlist/opposites.shtml
http://www.michigan-proficiency-exams.com/antonym-list.html
http://examples.yourdictionary.com/examples-of-antonyms.html
http://www.synonyms-antonyms.com/antonyms.html
http://englishwilleasy.com/word-must-know/vocabulary/vocabulary-list-by-opposites-or-antonyms/
http://www.meridianschools.org/staff/districtcurriculum/moreresources/languagearts/all grades/antonyms.doc
http://mrsbrower.weebly.com/uploads/1/3/2/4/13243672/antonymlist.pdf
https://foxhugh.wordpress.com/word-lists/list-of-antonyms/
http://www.paulnoll.com/Books/Clear-English/English-antonyms-1.html
http://wordnet.princeton.edu/wordnet/download/
Table 4: URLs used for the list of 11,028 antonym pairs (accessed on March 20, 2014).
hypernym AA from HY P (A,A)
hypernym BB from HY P (B,B)
Negation: Negations play an important role in
the task at hand. For instance, consider this pair
of texts {?A person is rinsing a steak with wa-
ter?,?A man is not rinsing a large steak?} labeled
as a contradiction. In that example the negation of
the verb ?rising? is the main factor of contradic-
tion. In order to capture this linguistic feature we
build a simple function that detects the occurrence
of a verb negation if the text contains one of the
following words: ?not?, ?n?t?, ?nor?, ?null?, ?nei-
ther?, ?either?, ?barely?, ?scarcely? and ?hardly?.
Similarly, noun negation is detected looking for
the words: ?no?, ?none?, ?nobody?, ?nowhere?,
?nothing? and ?never?. Thus, for two texts A and
B, 4 features are extracted (referred henceforth as
NEG):
verb neg A if verb negation is detected in A
verb neg B if verb negation is detected in B
noun neg A if noun negation is detected in A
noun neg B if noun negation is detected in B
3.1.6 Submitted Runs and Results
RUN1 (PRIMARY) This system produced pre-
dictions by extracting all the features described
previously (SM, ESA, POS, DEP, ANT,
HYP and NEG) from all the texts in the SICK
data set. Next, two machine learning models were
obtained (WEKA (Hall et al., 2009) was used
for that) using the training part of SICK, one for
regression (relatedness) and another for classifi-
cation (entailment). The regression model was
a reduced-error pruning tree (REPtree) (Quin-
lan, 1987) boosted with 20 iterations of bagging
(Breiman, 1996). The classification model was a
J48Graft tree also boosted with 20 bagging itera-
tions. These two models produced the predictions
for the test part of SICK.
RUN2 This system is similar to the one used
in RUN1, but it used only the feature sets SM and
NEG. Another difference is that a linear regres-
sion was used instead of the REPtree and no bag-
ging was performed.
RUN3 The same as RUN1, but again, linear
regression was used instead of the REPtree and no
bagging was performed.
RUN4 The same as RUN2, but the models
were boosted with 20 iterations of bagging.
RUN5 The same as RUN3, but 30 iterations of
bagging were used instead of 20.
The official results obtained by these systems
(prefixed UNAL-NLP) are shown in Table 5
jointly with those obtained by other 3 top sys-
tems among the 18 participating systems. Our
primary run (RUN1) obtained pretty competitive
results ranking 3th and 4th in the entailment and
relatedness tasks. The RUN4 obtained a remark-
able performance (it would be ranked 6th for en-
tailment and 8th for relatedness) in spite of the
fact that is a system purely based on string match-
ing. The comparison of our runs 1, 3 and 5, which
mainly differs by the use of bagging, shows that
this boosting method provides considerable im-
provements. In fact, comparing RUN3 (all fea-
tures, no bagging) and RUN4 (SM and NEG fea-
ture sets boosted with bagging), they performed
similarly in spite of the considerable larger num-
ber of features used in RUN3. Besides, the RUN5
slightly outperformed our primary run (RUN1) us-
737
Entailment Relatedness
system accuracy official rank Pearson Spearman MSE official rank
UNAL-NLP run1 (primary) 83.05% 3rd/18 0.8043 0.7458 0.3593 4th/17
UNAL-NLP run2 79.81% - 0.7482 0.7033 0.4487 -
UNAL-NLP run3 80.15% - 0.7747 0.7286 0.4081 -
UNAL-NLP run4 80.21% - 0.7662 0.7142 0.4210 -
UNAL-NLP run5 83.24% - 0.8070 0.7489 0.3550 -
ECNU run1 83.64% 2nd/18 0.8280 0.7689 0.3250 1st/17
Stanford run5 74.49% 12th/18 0.8272 0.7559 0.3230 2nd/17
Illinois-LH run1 84.58% 1st/18 0.7993 0.7538 0.3692 5th/17
Table 5: Results for task 1.
ing 10 additional iterations of bagging.
3.1.7 Error Analysis
Our primary run for the task 1 failed in 835 pairs of
sentences out of 4,927 in the entailment subtask.
We wanted to understand in why our system failed
in these 835 instances, so we classified manually
these instances in 4 error categories (each instance
could be assigned to several categories).
Paraphrase not detected (NP): exam-
ple={?Two groups of people are playing football?,
?Two teams are competing in a football match?},
gold standard=entailment, prediction=neutral,
number of occurrences= 420 (50.3%). The system
failed to detect the paraphrase between ?groups of
people? and ?teams?.
Negation not detected (NN) : exam-
ple={?There is no one playing the guitar?,
?Someone is playing the guitar?}, gold stan-
dard=contradiction, prediction=neutral, number
of occurrences=94 (11.3%). The system failed to
detect that the contradiction is due to the negation
in the first text.
False similarity between words (NSS) : ex-
ample={?Two dogs are playing by a tree?,
?Two dogs are sleeping by a tree?}, gold stan-
dard=neutral, prediction=entailment, number of
occurrences=413 (49.5%). The only difference
between these 2 sentences is the gerund ?playing?
vs. ?sleeping?, which the system erroneously con-
sidered as similar.
Antonym not detected (NA): exam-
ple={?Three children are running down hill?,
?Three children are running up hill?}, gold
standard=contradiction, prediction=entailment,
number of occurrences=40 (4.8%). The only
difference between these 2 sentences is the
words ?down? vs. ?up?. In spite that this pair
of antonyms was included in the antonym list,
Error category NP NN NSS NA
NP 420 5 125 0
NN - 94 1 0
NSS - - 413 22
NA - - - 40
Table 6: Co-ocurrences of types of errors in RUN1
(task1).
the system failed to distinguish the contradiction
between the texts.
The matrix in Table 6 reports the number of
co-occurrences of error categories in the 835 in-
stances erroneously classified.
3.2 Task 3: Cross-level Semantic Similarity
The SemEval 2014 task 3 (cross-level semantic
similarity) (Jurgens et al., 2014) proposed the se-
mantic textual similarity task but across differ-
ent textual levels, namely paragraph-to-sentence,
sentence-to-phrase, phrase-to-word and word-to-
sense. As usual, the goal is to predict the gold sim-
ilarity scores for each pair of texts. For each one
of these cross-level comparison types there were
proposed a separated training and test data sets.
Basically, we addressed this task using the set of
features SM presented in subsection 3.1.1 in com-
bination with a text expansion approach similar to
the method presented in subsection 3.1.2.
3.2.1 Paragraph-to-sentence and
Sentence-to-phrase
For these two cross-level comparison types we
extracted the SM feature set using the pro-
vided texts. The model parameters obtained for
paragraph-to-sentence were ? = 0.1, ? = 1.75,
bias = ?1.35, p = 1.55; and for sentence-to-
phrase were ? = 0.68, ? = 0.92, bias = ?0.92,
p = 2.49.
738
The system for the RUN2 used the SM fea-
ture set and a machine learning model build with
the provided training data for generating the simi-
larity score predictions for the test data. For the
paragraph-to-sentence data set the model was a
REPtree for regression boosted with 40 bagging it-
erations. Similarly, the model for the sentence-to-
phrase data set was a linear regressor also boosted
with 40 bagging iterations.
Unlike RUN2, RUN1 does not make use of any
machine learning algorithm. Instead, we used the
only the basic cardinalities (see Table 1) from the
SM feature set in combination with an ad-hoc re-
semblance coefficient, i.e. the Dice?s coefficient
2|A?B|
/|A|+|B| for the paragraph-to-sentence data
set. In turn, for sentence-to-phrase the overlap co-
efficient, i.e.
|A?B|
/min[|A|,|B|], was used.
3.2.2 Phrase-to-word and Word-to-sense
Before applying the same procedure used in the
previous subsection, the texts in the phrase-to-
word and word-to-sense data sets were expanded
with a similar approach to that was used in subsec-
tion 3.1.2.
Phrase-to-word expansion: First, the ?word?
was expanded finding its corresponding WordNet
synset using the adapted Lesk?s algorithm provid-
ing as context the ?phrase?. Then, once the word?s
synset is obtained, the ?word? text is extended
with the textual definition of the synset. Simi-
larity, this procedure is repeated for each word in
the ?phase? obtaining and extended version of the
phrase. Finally, these two texts are used for ex-
tracting the SM feature set. The model param-
eters were ? = 0.8, ? = 1.9, bias = ?0.8,
p = 1.5.
Word-to-sense expansion: First, the ?sense?
(i.e. synset) is replaced by its textual definition
and its lemma. At this point the pair word-sense
becomes a pair word-sentence. Then, the synset
of the ?word? is obtained performing the adapted
Lesk?s algorithm. Next, the ?word? is extended
with textual definition of the synset. Finally, these
two texts are used for extracting the SM feature
set obtaining the following model parameters were
? = 0.59, ? = 0.9, bias = ?0.89, p = 3.91.
3.2.3 Results
The official results obtained by the two submitted
runs jointly with other 3 top systems are shown in
Table 7. Our submissions (prefixed with UNAL-
NLP) ranked 3rd and 5th among 38 participating
test data train data
OnWN (en) OnWN 2012/2013 test
headlines (en) headlines 2013 test
images (en) MSRvid 2012 train and test
deft-news (en) MSRpar 2013 train and test
deft-forum (en)
MSRvid 2012 train and test
OnWN 2012/2013 test
tweet-news (en)
SMTeuroparl 2012 test
SMTnews 2012 test
Wikipedia (es) SMTeuroparl 2012 train
news (es) SMTeuroparl 2012 train
Table 8: Training data used for the STS-2014 data
sets (task 10).
systems, showing that the SM (string-matching)
feature set is effective for the prediction of sim-
ilarity scores. Particularly, in the paragraph-to-
sentence data set, which has the longest text,
RUN2 obtained the best official score. In contrast,
the scores obtained for the phrase-to-word and
word-to-sense data sets were considerably lower
in comparison with the top system, but still com-
petitive against most of the other participating sys-
tems.
3.3 Task 10: Multilingual Semantic
Similarity
The SemEval-2014 task 10 (multilingual seman-
tic similarity) (Agirre et al., 2014) is the sequel of
the semantic textual similarity (STS) evaluations
at SemEval in the past two years (Agirre et al.,
2012; Agirre et al., 2013). This year 6 test data
sets were proposed in English and 2 data sets in
Spanish. Similarly to the 2013 campaign, there is
not explicit training data for each data set. Conse-
quently, different data sets from the previous STS
evaluations were selected to be used as training
data for the new data sets. The selection criterion
was the average character length and type of the
texts. The Table 8 shows the training data used for
each test data set.
3.3.1 English Subtask
The RUN1 for the English data sets was produced
with a parameterized similarity function based on
the SM feature set and the symmetrized Tversky?s
index (Tversky, 1977; Jimenez et al., 2013a). For
a detailed description of this function and its pa-
rameters, please refer to the STS
sim
feature in
the system description paper of the NTNU team
(Lynum et al., 2014). The parameters used in that
739
System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank
SimCompass run1 0.811 0.742 0.415 0.356 1st/38
ECNU run1 0.834 0.771 0.315 0.269 2nd/38
UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38
SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38
UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38
Table 7: Official results for task 3 (Pearson?s correlation).
Data ? ? bias p ?
?
?
?
bias
?
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
Table 9: Optimal parameters used for task 10 in
English.
function are reported in Table 9. Unlike subsec-
tion 3.1.1 where the Dice?s coefficient was used as
the text similarity function, here the symmetrical
Tversky?s index (eq. 2) was reused generating the
three additional parameters marked with apostro-
phe (?
?
, ?
?
and bias
?
).
For the RUN2 the SM feature set was extracted
from all the data sets in English (en) listed in Table
8. Then, a REPtree (Quinlan, 1987) boosted with
50 bagging iterations (Breiman, 1996) was trained
using the training data sets selected for each test
data set. Finally, these machine learning models
produced the similarity score predictions for each
test data set.
The RUN3 was identical to the RUN2 but in-
cluded additional feature sets apart from SM,
namely: ESA, POS and WN. The WN feature
set is the same as SM, but replacing the word-to-
word similarity function in eq. 2 by the path mea-
sure from the WordNet::Similarity package (Ped-
ersen et al., 2004).
3.3.2 Spanish Subtask
The Spanish system was based entirely in the SM
feature set with some small changes for adapt-
ing the system to Spanish. Basically, the list of
English stop-words was replaced by the Spanish
stop-words provided by the NLTK. In addition,
the Porter stemmer was replaced by its Spanish
equivalent, i.e. the Snowball stemmer for Span-
ish. The RUN1 is equivalent to the RUN1 for the
data set run1 run2 run3
deft-forum 0.5043 0.3826 0.4607
deft-news 0.7205 0.7305 0.7216
headlines 0.7616 0.7645 0.7605
images 0.8071 0.7706 0.7782
OnWN 0.7823 0.8268 0.8426
tweet-news 0.6145 0.4028 0.6583
mean (en) 0.7113 0.6573 0.7209
official rank (en) 12th/38 22th/38 9th/38
Wikipedia 0.7804 0.7566 0.6894
news 0.8154 0.7829 0.7965
mean (es) 0.8013 0.7723 0.7533
official rank (es) 3rd/22 9th/22 12th/22
Table 10: Official results for the task 10 (Pearson?s
correlation).
English subtask described in the previous subsec-
tion. The parameters used for the text similarity
function were ? = 1.16, ? = 1.08, bias = 0.02,
p = 1.02, ?
?
= 1.54, ?
?
= 0.08 and bias
?
= 1.37.
The description and meaning of these parameters
can be found in (Lynum et al., 2014) associated to
the STS
sim
feature.
The RUN2 was obtained using the SM feature
set and a linear regressor for generating the simi-
larity score predictions. Similarity, RUN3 used the
same feature set SM in combination with a REP-
tree boosted with 30 bagging iterations.
3.3.3 Results
The results for the 3 submitted runs correspond-
ing to the 2 sub tasks (English and Spanish) are
shown in Table 10. It is important to note that
the RUN1 for the Wikipedia data set in Spanish
was the top system among 22 participating sys-
tems. This result is remarkable given that this sys-
tem was trained with a data set in English showing
the domain adaptation ability of the soft cardinal-
ity approach.
740
4 Conclusions
We participated in the SemEval-2014 task 1, 3 and
10 with an uniform approach based on soft cardi-
nality features, obtaining pretty satisfactory results
in all data sets, tasks and sub tasks. This approach
has been used since SemEval-2012 in all versions
of the following tasks: semantic textual similar-
ity (Jimenez et al., 2012b; Jimenez et al., 2013a),
typed similarity (Croce et al., 2013), cross-lingual
textual entailment (Jimenez et al., 2012a; Jimenez
et al., 2013c), student response analysis (Jimenez
et al., 2013b), and multilingual semantic textual
similarity (Lynum et al., 2014). In the majority
of the cases, the systems based on soft cardinality,
built by us and other teams, have been among the
top systems. Given the uniformity of the approach,
the consistency of the results, the few computa-
tional resources required and the overall concep-
tual simplicity, the soft cardinality is established
as a useful tool for a wide spectrum of applications
in natural language processing.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval@*SEM 2012), Montreal,Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. Atlanta, Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Weibe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using WordNet. In Computational linguis-
tics and intelligent text processing, page 136?145.
Springer.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, EMSEE ?05,
page 13?18, Stroudsburg, PA, USA.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242?249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: SemanticTextual Similarity, page 59, Atlanta,
Georgia, USA.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, page 449?454,
Genoa, Italy, May.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI?07, page 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bern-
hard Pfahringer. 2009. The WEKA data min-
ing software: An update. SIGKDD Explorations,
11(1):10?18.
Paul Jaccard. 1901. Etude comparative de la distribu-
tion florare dans une portion des alpes et des jura.
Bulletin de la Soci?et?e Vaudoise des Sciences Na-
turelles, pages 547?579.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297?302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In SemEval
2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In SemEval 2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In *SEM 2013, Atlanta,
Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Se-
mEval 2013, Atlanta, Georgia, USA, June.
741
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013c. SOFTCARDINALITY: Learning
to identify directional cross-lingual entailment from
cardinalities and SMT. In SemEval 2013, Atlanta,
Georgia, USA, June.
Karen Sp?arck Jones. 2004. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 60(5):493?502, October.
David Jurgens, Mohammad T. Pilehvar, and Roberto
Navigli. 2014. SemEval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinalty. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Lucia
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik, Iceland, May.
Alvaro E. Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceeding of the 2nd International Conference on
Knowledge Discovery and Data Mining (KDD-96),
pages 267?270, Portland, OR.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::similarity: measuring the
relatedness of concepts. In Proceedings HLT-
NAACL?Demonstration Papers, Stroudsburg, PA,
USA.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
J. Ross Quinlan. 1987. Simplifying decision
trees. International journal of man-machine studies,
27(3):221?234.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327?352, July.
742
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 743?747,
Dublin, Ireland, August 23-24, 2014.
UNAL-NLP: Cross-Lingual Phrase Sense Disambiguation with
Syntactic Dependency Trees
Emilio Silva-Schlenker
Departamento de Ling??stica
Universidad Nacional de Colombia
Departamento de Ingenier?a de Sistemas
Universidad de los Andes,
Bogot? D.C., Colombia
esilvas@unal.edu.co
Sergio Jimenez and Julia Baquero
Universidad Nacional de Colombia,
Bogot? D.C., Colombia
sgjimenezv@unal.edu.co
jmbauqerov@unal.edu.co
Abstract
In this paper we describe our participa-
tion in the SemEval 2014, Task 5, con-
sisting of the construction of a translation
assistance system that translates L1 frag-
ments, written in L2 context, to their cor-
rect L2 translation. Our approach con-
sists of a bilingual parallel corpus, a sys-
tem of syntactic features extraction and a
statistical memory-based classification al-
gorithm. Our system ranked 4th and 6th
among the 10 participating systems that
used the English-Spanish data set.
1 Introduction
An L2 writing assistant is a tool intended for lan-
guage learners who need to improve their writing
skills. This tool lets them write a text in L2, but fall
back to their native L1 whenever they are not sure
about a certain word or expression. In these cases,
the assistant automatically translates this text for
them (van Gompel et al., 2014).
Although at first glance this may be seen as
a classification problem, it might be better ful-
filled by a cross-lingual word sense disambigua-
tion (WSD) approach, which takes context into
account by means of contextual features used in
a machine learning setting. The main differences
between this and previous approaches to cross-
lingual WSD are the bilingual nature of the input
sentences (see section 2.3) and the annotation of
target phrases, rather than single words.
The remainder of this article is organized as fol-
lows. Section 2 describes the proposed method.
A description of the system we submitted, the ob-
tained results and an error analysis are discussed
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
in section 3. In section 4 we present a brief dis-
cussion about the results. Finally, in section 5 we
make some concluding remarks.
2 Method Description
The core of the proposed system uses techniques
from memory-based classification to find the most
appropriate translation of a target phrase in a
given context. It receives an input as in (1) and
yields an output as in (2).
(1) No creo que ella is coming.
(2) No creo que ella venga.
It does so on the basis of a syntactic selec-
tion of context features, a large bilingual parallel
corpus and a classifier built using the Tilburg
Memory-Based Learner, TiMBL (Daelemans et
al., 2010).
The proposed system consists of several stages.
First, a large bilingual corpus is aligned at word
and phrase level. Next, an index is built by each
phrase in the L1 side of the corpus to retrieve ef-
ficiently the occurrences of a particular L1 phrase
in the aligned corpus along with their translations
and contexts in L2 (subsection 2.1). Second, the
relevant contexts for each L1 phrase in the test set
(example sentences) are retrieved from the corpus
and a set of syntactic features are extracted from
each sentence (subsection 2.2). Third, a special
two-stage process is used to extract the same fea-
tures from the sentences in the test set to deal with
the fact that these sentences were written in two
languages (subsection 2.3). Finally, each target
phrase is translated using the IBL algorithm and
the translations were incorporated in the original
test sentences (subsection 2.4).
743
Input sentence Parallel example sentences
No creo que las necesidades
afectivas de las personas est?n
necesariamente linked al
matrimonio.
He said Boyd already linked
him to Brendan.
Dijo que Boyd ya le hab?a rela-
cionado con Brendan.
The three things are inextrica-
bly linked, and I have the for-
mula right here.
Las tres cosas est?n es-
trechamente vinculadas, y
tengo la f?rmula aqu?.
Table 1: An input sentence and 2 example sentences from Linguee.com.
2.1 Parallel Corpus Selection and
Preparation
As no training corpus was given prior to develop-
ing this system, finding and processing the most
suitable corpus for this task was paramount. As
the purpose of this system is to help language stu-
dents, the corpus needs to account for simple yet
correct everyday speech.
In an initial stage of development we opted
to use the 70-million sentences OpenSubtitles.org
corpus compiled by the Opus Project (Tiedemann,
2012), which includes many informal everyday ut-
terances, at the expense of a less accurate transla-
tion quality
1
. Although the use of this training cor-
pus yielded over 95% of recall on the trial corpus
given by the task organizers, only 80% of the trial
sentences had enough (>100) training examples in
order to produce a quality translation. To solve this
issue, an ad-hoc corpus compilation mechanism
was created by using the Linguee.com. Thus, a
set of parallel example sentences is retrieved from
Linguee.com by querying all the L1 target phrases
from the evaluation data (see an example in Table
1).
The corpus preparation procedure consisted of
several steps. The first step was to clean the cor-
pus with the Moses cleaning script (Koehn et al.,
2007). Next, the corpus was tokenized and PoS-
tagged using FreeLing (Padr? and Stanilovsky,
2012) (HMM tagger was used). After that, the
corpus was word-aligned using Giza++ (Och and
Ney, 2003) over Moses (Koehn et al., 2007). The
resulting alignment was then combined with the
tagged version of the corpus. Finally, a phrase in-
dex was built using a SMT phrase extraction algo-
rithm (Ling et al., 2010) including for each phrase
pointers to all its occurrences in the corpus for fur-
ther retrieval.
1
The EPPS corpus (Lambert et al., 2005) was very useful
as a training corpus in the developing stages of this system.
It was however not used in the final system training.
2.2 Syntactic Feature Extraction
The syntactic tags feature is a novel feature we are
introducing for the CLWSD problem (Lefever and
Hoste, 2013). They are linearizations of syntactic
dependency trees. These trees were built by Freel-
ing?s Txala Parser (Lloberes et al., 2010) and were
introduced as individual tags in a sentence analy-
sis by parsing the tree and mapping its leaves with
their corresponding order in the source sentence.
Then, each leaf?s label and parent number was ex-
tracted. For the root, the special parent tag ?S? was
used.
The WSD literature commonly distinguishes
between local and global context features (Mar-
tinez and Agirre, 2001). The former are extracted
from the neighboring words and the latter are ex-
tracted from words of the whole context provided
using some heuristic to select relevant. Unlike
global features, the relevance of the surrounding
words is not put into question or are weighted
by the degree of relevance according to their po-
sition in the sentence and lexicographic distance
from the target phrase (van Gompel, 2010). There
is a linguistic explanation as to why surrounding
words play a significant role in determining the
target?s translation. Often, these words have a di-
rect dependency relation with the target. Indeed,
physical closeness is an approximation of syntac-
tic relatedness. What we propose in this paper is
that the relevance of the context words for deter-
mining a correct translation is proportional to their
syntactic relatedness to the target, rather than their
physical closeness in the sentence. Unlike Mar-
tinez et al. (2002), what we propose here is to use
syntax as a feature selector, rather than as a feature
itself.
Instead of defining a local and a global set of
relevant words, we selected a single set of relevant
words according to their syntactic relation to the
target phrase. This set consisted of all the children
of the target words, and the parents of the main
target words. The main target words are the subset
744
0 1 2 3 4 5 6
Forms Las tres cosas est?n estrechamente vinculadas .
Lemmas el 3 cosa estar estrechamente vincular .
PoS Tags DA0FP0 Z NCFP000 VAIP3P0 RG VMP00PF Fp
Syn Tags espec:1 espec:2 subj:3 co-v:7 espec:5 att:3 ?:7
Table 2: Tagging of the sentence ?Las tres cosas est?n estrechamente vinculadas.?
of words with the highest number of (nested) chil-
dren within the target phrase. Table 3 features the
rules used for selecting the relevant words.
This Feature Extraction method uses the depen-
dency labels as a means of selecting only rele-
vant examples. Take for instance the example sen-
tences in Table 1. Given that the target word is an
attribute, the subject is included as a relevant fea-
ture, as per the last rule in Table 3. Any example
sentence in which there is no subject as the sib-
ling of the target word (as is the case for the first
example sentence in Table 1) will have an empty
feature, which increases its likelihood of not being
included in the training set of this sentence.
2.3 Test Data Pre-processing
The test data for this task is composed of bilin-
gual input sentences, making it impossible to ob-
tain a correct tagging or parsing. To overcome this
issue, a two-stage process wherein the first stage
obtains translations for the L1 portions was per-
formed. These plausible translations are obtained
by TiMBL using as features the neighboring words
of the target phrases. Once the sentences are in a
single language (L2) they are tagged and parsed
syntactically. Finally, the second stage consists in
applying the same feature selection algorithm pro-
posed in subsection 2.2.
2.4 Translation Selection
The processing of each sentence consists of sev-
eral steps. In the first step, the L1 target phrase
is searched for in the phrase index Given an L1
phrase, a binary search algorithm iterates through
the phrase index and returns an array of point-
ers
2
to the corpus. Then, a multi-threaded subrou-
tine reads the word-aligned bilingual corpus and
extracts all the referenced sentences. Thus, for
each input sentence, a set of example bilingual
word-aligned sentences is extracted from the cor-
pus. Relevant features are extracted according to
2
Given that line breaks are just regular characters, what is
actually referenced in the phrase index are byte offsets.
a syntactic analysis as explained in subsection 2.2,
and written to text files in the C4.5 format. The
features extracted from the example sentences, as
well as the L2 translations of the target phrases
in each sentence, are used as the training set for
TiMBL, while the features extracted from the in-
put sentence are used as its (singleton) test set.
The L2 translations of each target phrase in the
example sentences are used as the classes for the
training set, in order to turn a bilingual disam-
biguation problem into a machine learning clas-
sification problem. TiMBL learns how to classify
the training feature vectors into their correspond-
ing classes and then predicts the class for the test
set feature vector, i.e. its most likely translation
using an IBL algorithm (Aha et al., 1991), which
is a variation of the k-nearest neighbor classifier.
3 System Submissions
We submitted three result sets for the English-
Spanish language pair. Two of them were submit-
ted for the ?Best? evaluation type, and the other
one was submitted for the ?out-of-five? evaluation
type. The difference between these two evaluation
types is that out-of-five evaluation expects up to
five different translations for every target phrase,
while ?best? only accepts one. The evaluation met-
rics include accuracy and recall, and also a word-
based special type of accuracy, which takes into
account partially correct translations.
Of the two runs submitted in the ?Best? evalu-
ation type, Run1-best (see table 4) used our pro-
posed syntactic feature extraction method, while
Run2-best used a regular 2-word window around
the target phrase. For the Run1-oof we combined
the two methods mentioned above with different
values of k.
3.1 Results
The test data consisted in 500 sentences written in
Spanish, with target English phrases. The official
results obtained by our runs are shown in Table 4.
Our control run, Run2-best, yielded slightly
745
Case Rule Example
One of the target words is a
subject.
Include any sibling which is an
auxiliary or modal verb.
Our cat quiere comerse la en-
salada.
The parent of one of the main
target words is a coordinative
conjunction.
Include its closest sibling. No quer?a ni eat, ni dormir.
The parent of one of the main
target words is a relative pro-
noun.
Include its grandparent. No creo que ella is coming.
One of the target words is an
attribute.
Include any sibling which is
subject.
Mis t?as est?n very tired.
Table 3: Relevant word selection rules.
better results than our experimental run, Run1-
best. This means that our method of syntactic fea-
tures extraction did not improve translation qual-
ity.
3.2 Error Analysis
By analyzing our results, we detected three groups
of recurrent errors. The first group of errors is re-
lated to verb morphology, in which a single En-
glish verbal form corresponds to many Spanish
verbal forms. In these cases, our system often out-
puts an infinitive form or a past participle instead
of a finite verb.
The second group of errors we detected com-
prises incomplete translations. In these cases, a
single word in English has a multiword Spanish
translation, but our system often outputs a single-
word translation.
The third group of errors are related to English
words with multiple possible parts of speech, as
?flood?, which can be a noun but also a verb. Our
system tends to output nouns instead of verbs and
vice versa.
4 Discussion
There are two main reasons as to why the syntac-
tic feature extraction method did not work. The
first reason is related to the nature of the task; the
second is related to the scope of the method.
The fact that this task involved analyzing sen-
tences partly written in two languages made syn-
tactic analysis extremely difficult as dependencies
span all over the bilingual sentence. The best solu-
tion we found for this was to divide the operation
of the system in two stages, where the first one did
not involve syntactic dependencies and provided a
working translation, and the second one used this
first translation to perform a syntactic analysis and
then rerun the classification step. This, however,
favored error propagation. Although translation
quality did improve between the two stages, there
were many cases in which a bad initial translation
involved a bad syntactic analysis, which in turn re-
sulted in a bad final translation.
A more sophisticated version of his method was
initially developed for the English-Spanish lan-
guage pair and involved several language-specific
rules. However, we decided to make this method
language-independent, so we simplified it to its ac-
tual version. This simplified version uses syntactic
dependencies as feature selectors, but the features
themselves are regular lemma/PoS combinations,
which is not always the best feature choice.
5 Conclusion
Syntactic dependency relations are an important
means of analyzing the internal structure of a sen-
tence and can successfully be used to improve the
feature selection process in WSD. However, syn-
tactic parsing is far away from optimal in Spanish,
a fortiori if it involves sentences written in two lan-
guages. For this kind of task, perhaps a statistical
language model of L2 would have yielded better
results.
Acknowledgments
We would like to specially thank Professor Sil-
via Takahashi of Universidad de los Andes for her
continued advice and support. We would also like
to thank Pedro Rodr?guez for his development of
the Linguee crawler, Mar?a De-Arteaga, Alejandro
Riveros and David Hoyos for their useful sugges-
tions in the conception and development of this
project, and the rest of the UNAL-NLP team for
746
Run Recall Accuracy Word Accuracy Rank (runs) Rank (systems)
Run1-best 0.993 0.721 0.794 5 2
Run2-best 0.993 0.733 0.809 4 2
Run1-oof 0.993 0.823 0.880 6 3
Table 4: Official results.
their interest and encouragement. Many thanks to
Jay C. Soper for proof-reading this article.
References
David W. Aha, Dennis Kibler, and Marc K. Albert.
1991. Instance-based learning algorithms. Machine
Learning, 6:37?66.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,
and Antal Van den Bosch. 2010. Timbl: Tilburg
memory-based learner. reference guide. ILK Re-
search Group, Tilburg University.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, and
Richard Zens. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, page 177?180.
Patrik Lambert, Adri? Gispert, Rafael Banchs, and
Jos? B. Mari?o. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267?285, Decem-
ber.
Els Lefever and V?ronique Hoste. 2013. Semeval-
2013 task 10: Cross-lingual word sense disambigua-
tion. In Second joint conference on lexical and com-
putational semantics, volume 2, page 158?166.
Wang Ling, Tiago Lu?s, Jo?o Gra?a, Lu?sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT?10:
International Workshop on Spoken Language Trans-
lation, page 313?320.
Marina Lloberes, Irene Castell?n, and Llu?s Padr?.
2010. Spanish FreeLing dependency grammar. In
LREC, volume 10, page 693?699.
David Martinez and Eneko Agirre. 2001. Deci-
sion lists for english and basque. In The Proceed-
ings of the Second International Workshop on Eval-
uating Word Sense Disambiguation Systems, page
115?118.
David Mart?nez, Eneko Agirre, and Llu?s M?rquez.
2002. Syntactic features for high precision word
sense disambiguation. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, page 1?7.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Llu?s Padr? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence, pages 2473?2479, Istambul, Turkey, May.
J?rg Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the Lan-
guage Resources and Evaluation Conference, page
2214?2218, Istambul, Turkey, May.
Maarten van Gompel, Iris Hendrickx, Antal van den
Bosh, Els Lefever, and V?ronique Hoste. 2014.
Semeval-2014 task 5: L2 writing assistant. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland,
August.
Maarten van Gompel. 2010. UvT-WSD1: a cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th international workshop on se-
mantic evaluation, page 238?241, Uppsala, Sweden.
747
