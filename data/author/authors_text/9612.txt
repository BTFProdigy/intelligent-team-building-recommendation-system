Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 877?886,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Two Languages are Better than One (for Syntactic Parsing)
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
We show that jointly parsing a bitext can sub-
stantially improve parse quality on both sides.
In a maximum entropy bitext parsing model,
we define a distribution over source trees, tar-
get trees, and node-to-node alignments be-
tween them. Features include monolingual
parse scores and various measures of syntac-
tic divergence. Using the translated portion
of the Chinese treebank, our model is trained
iteratively to maximize the marginal likeli-
hood of training tree pairs, with alignments
treated as latent variables. The resulting bi-
text parser outperforms state-of-the-art mono-
lingual parser baselines by 2.5 F1 at predicting
English side trees and 1.8 F1 at predicting Chi-
nese side trees (the highest published numbers
on these corpora). Moreover, these improved
trees yield a 2.4 BLEU increase when used in
a downstream MT evaluation.
1 Introduction
Methods for machine translation (MT) have increas-
ingly leveraged not only the formal machinery of
syntax (Wu, 1997; Chiang, 2007; Zhang et al,
2008), but also linguistic tree structures of either the
source side (Huang et al, 2006; Marton and Resnik,
2008; Quirk et al, 2005), the target side (Yamada
and Knight, 2001; Galley et al, 2004; Zollmann et
al., 2006; Shen et al, 2008), or both (Och et al,
2003; Aue et al, 2004; Ding and Palmer, 2005).
These methods all rely on automatic parsing of one
or both sides of input bitexts and are therefore im-
pacted by parser quality. Unfortunately, parsing gen-
eral bitexts well can be a challenge for newswire-
trained treebank parsers for many reasons, including
out-of-domain input and tokenization issues.
On the other hand, the presence of translation
pairs offers a new source of information: bilin-
gual constraints. For example, Figure 1 shows a
case where a state-of-the-art English parser (Petrov
and Klein, 2007) has chosen an incorrect structure
which is incompatible with the (correctly chosen)
output of a comparable Chinese parser. Smith and
Smith (2004) previously showed that such bilin-
gual constraints can be leveraged to transfer parse
quality from a resource-rich language to a resource-
impoverished one. In this paper, we show that bilin-
gual constraints and reinforcement can be leveraged
to substantially improve parses on both sides of a
bitext, even for two resource-rich languages.
Formally, we present a log-linear model over
triples of source trees, target trees, and node-to-
node tree alignments between them. We consider
a set of core features which capture the scores of
monolingual parsers as well as measures of syntactic
alignment. Our model conditions on the input sen-
tence pair and so features can and do reference input
characteristics such as posterior distributions from a
word-level aligner (Liang et al, 2006; DeNero and
Klein, 2007).
Our training data is the translated section of the
Chinese treebank (Xue et al, 2002; Bies et al,
2007), so at training time correct trees are observed
on both the source and target side. Gold tree align-
ments are not present and so are induced as latent
variables using an iterative training procedure. To
make the process efficient and modular to existing
monolingual parsers, we introduce several approxi-
mations: use of k-best lists in candidate generation,
an adaptive bound to avoid considering all k2 com-
binations, and Viterbi approximations to alignment
posteriors.
877
Figure 1: Two possible parse pairs for a Chinese-English sentence pair. The parses in a) are chosen by independent
monolingual statistical parsers, but only the Chinese side is correct. The gold English parse shown in b) is further down
in the 100-best list, despite being more consistent with the gold Chinese parse. The circles show where the two parses
differ. Note that in b), the ADVP and PP nodes correspond nicely to Chinese tree nodes, whereas the correspondence
for nodes in a), particularly the SBAR node, is less clear.
We evaluate our system primarily as a parser and
secondarily as a component in a machine translation
pipeline. For both English and Chinese, we begin
with the state-of-the-art parsers presented in Petrov
and Klein (2007) as a baseline. Joint parse selection
improves the English trees by 2.5 F1 and the Chi-
nese trees by 1.8 F1. While other Chinese treebank
parsers do not have access to English side transla-
tions, this Chinese figure does outperform all pub-
lished monolingual Chinese treebank results on an
equivalent split of the data.
As MT motivates this work, another valuable
evaluation is the effect of joint selection on down-
stream MT quality. In an experiment using a
syntactic MT system, we find that rules extracted
from joint parses results in an increase of 2.4
BLEU points over rules extracted from independent
parses.1 In sum, jointly parsing bitexts improves
parses substantially, and does so in a way that that
carries all the way through the MT pipeline.
2 Model
In our model, we consider pairs of sentences (s, s?),
where we use the convention that unprimed vari-
ables are source domain and primed variables are
target domain. These sentences have parse trees t
(respectively t?) taken from candidate sets T (T ?).
1It is anticipated that in some applications, such as tree trans-
ducer extraction, the alignments themselves may be of value,
but in the present work they are not evaluated.
Non-terminal nodes in trees will be denoted by n
(n?) and we abuse notation by equating trees with
their node sets. Alignments a are simply at-most-
one-to-one matchings between a pair of trees t and
t? (see Figure 2a for an example). Note that we will
also mention word alignments in feature definitions;
a and the unqualified term alignment will always re-
fer to node alignments. Words in a sentence are de-
noted by v (v?).
Our model is a general log-linear (maximum en-
tropy) distribution over triples (t, a, t?) for sentence
pairs (s, s?):
P(t, a, t|s, s?) ? exp(w>?(t, a, t?))
Features are thus defined over (t, a, t?) triples; we
discuss specific features below.
3 Features
To use our model, we need features of a triple
(t, a, t?) which encode both the monolingual quality
of the trees as well as the quality of the alignment
between them. We introduce a variety of features in
the next sections.
3.1 Monolingual Features
To capture basic monolingual parse quality, we be-
gin with a single source and a single target feature
whose values are the log likelihood of the source
tree t and the target tree t?, respectively, as given
878
by our baseline monolingual parsers. These two fea-
tures are called SOURCELL and TARGETLL respec-
tively. It is certainly possible to augment these sim-
ple features with what would amount to monolin-
gual reranking features, but we do not explore that
option here. Note that with only these two features,
little can be learned: all positive weightsw cause the
jointly optimal parse pair (t, t?) to comprise the two
top-1 monolingual outputs (the baseline).
3.2 Word Alignment Features
All other features in our model reference the entire
triple (t, a, t?). In this work, such features are de-
fined over aligned node pairs for efficiency, but gen-
eralizations are certainly possible.
Bias: The first feature is simply a bias feature
which has value 1 on each aligned node pair (n, n?).
This bias allows the model to learn a general prefer-
ence for denser alignments.
Alignment features: Of course, some alignments
are better than others. One indicator of a good node-
to-node alignment between n and n? is that a good
word alignment model thinks that there are many
word-to-word alignments in their bispan. Similarly,
there should be few alignments that violate that bis-
pan. To compute such features, we define a(v, v?)
to be the posterior probability assigned to the word
alignment between v and v? by an independent word
aligner.2
Before defining alignment features, we need to
define some additional variables. For any node n ? t
(n? ? t?), the inside span i(n) (i(n?)) comprises
the input tokens of s (s?) dominated by that node.
Similarly, the complement, the outside span, will be
denoted o(n) (o(n?)), and comprises the tokens not
dominated by that node. See Figure 2b,c for exam-
ples of the resulting regions.
INSIDEBOTH =
?
v?i(n)
?
v??i(n?)
a(v, v?)
INSRCOUTTRG =
?
v?i(n)
?
v??o(n?)
a(v, v?)
INTRGOUTSRC =
?
v?o(n)
?
v??i(n?)
a(v, v?)
2It is of course possible to learn good alignments using lexi-
cal indicator functions or other direct techniques, but given our
very limited training data, it is advantageous to leverage counts
from an unsupervised alignment system.
Hard alignment features: We also define the
hard versions of these features, which take counts
from the word aligner?s hard top-1 alignment output
?:
HARDINSIDEBOTH =
?
v?i(n)
?
v??i(n?)
?(v, v?)
HARDINSRCOUTTRG =
?
v?i(n)
?
v??o(n?)
?(v, v?)
HARDINTRGOUTSRC =
?
v?o(n)
?
v??i(n?)
?(v, v?)
Scaled alignment features: Finally, undesirable
larger bispans can be relatively sparse at the word
alignment level, yet still contain many good word
alignments simply by virtue of being large. We
therefore define a scaled count which measures den-
sity rather than totals. The geometric mean of span
lengths was a superior measure of bispan ?area? than
the true area because word-level alignments tend to
be broadly one-to-one in our word alignment model.
SCALEDINSIDEBOTH =
INSIDEBOTH
?
|i(n)| ? |i(n?)|
SCALEDINSRCOUTTRG =
INSRCOUTTRG
?
|i(n)| ? |o(n?)|
SCALEDINTRGOUTSRC =
INTRGOUTSRC
?
|o(n)| ? |i(n?)|
Head word alignment features: When consider-
ing a node pair (n, n?), especially one which dom-
inates a large area, the above measures treat all
spanned words as equally important. However, lex-
ical heads are generally more representative than
other spanned words. Let h select the headword of
a node according to standard head percolation rules
(Collins, 2003; Bikel and Chiang, 2000).
ALIGNHEADWORD = a(h(n), h(n?))
HARDALIGNHEADWORD = ?(h(n), h(n?))
3.3 Tree Structure Features
We also consider features that measure correspon-
dences between the tree structures themselves.
Span difference: We expect that, in general,
aligned nodes should dominate spans of roughly the
same length, and so we allow the model to learn to
879
Figure 2: a) An example of a legal alignment on a Chinese-English sentence fragment with one good and one bad node
pair, along with sample word alignment posteriors. Hard word alignments are bolded. b) The word alignment regions
for the good NP-NP alignment. InsideBoth regions are shaded in black, InSrcOutTrg in light grey, and InTrgOutSrc in
grey. c) The word alignment regions for the bad PP-NP alignment.
penalize node pairs whose inside span lengths differ
greatly.
SPANDIFF = ||i(n)| ? |i(n?)||
Number of children: We also expect that there
will be correspondences between the rules of the
CFGs that generate the trees in each language. To
encode some of this information, we compute in-
dicators of the number of children c that the nodes
have in t and t?.
NUMCHILDREN?|c(n)|, |c(n?)|? = 1
Child labels: In addition, we also encode whether
certain label pairs occur as children of matched
nodes. Let c(n, `) select the children of n with la-
bel `.
CHILDLABEL?`, `?? = |c(n, `)| ? |c(n?, `?)|
Note that the corresponding ?self labels? feature
is not listed because it arises in the next section as a
typed variant of the bias feature.
3.4 Typed vs untyped features
For each feature above (except monolingual fea-
tures), we create label-specific versions by conjoin-
ing the label pair (`(n), `(n?)). We use both the
typed and untyped variants of all features.
4 Training
Recall that our data condition supplies sentence
pairs (s, s?) along with gold parse pairs (g, g?). We
do not observe the alignments a which link these
parses. In principle, we want to find weights which
maximize the marginal log likelihood of what we do
observe given our sentence pairs:3
w? = arg max
w
?
a
P(g, a, g?|s, s?, w) (1)
= arg max
w
?
a exp(w
>?(g, a, g?))
?
(t,t?)
?
a exp(w
>?(t, a, t?))
(2)
There are several challenges. First, the space of
symmetric at-most-one-to-one matchings is #P-hard
3In this presentation, we only consider a single sentence pair
for the sake of clarity, but our true objective was multiplied over
all sentence pairs in the training data.
880
to sum over exactly (Valiant, 1979). Second, even
without matchings to worry about, standard meth-
ods for maximizing the above formulation would re-
quire summation over pairs of trees, and we want
to assume a fairly generic interface to independent
monolingual parsers (though deeper joint modeling
and/or training is of course a potential extension).
As we have chosen to operate in a reranking mode
over monolingual k-best lists, we have another is-
sue: our k-best outputs on the data which trains
our model may not include the gold tree pair. We
therefore make several approximations and modifi-
cations, which we discuss in turn.
4.1 Viterbi Alignments
Because summing over alignments a is intractable,
we cannot evaluate (2) or its derivatives. However,
if we restrict the space of possible alignments, then
we can make this optimization more feasible. One
way to do this is to stipulate in advance that for each
tree pair, there is a canonical alignment a0(t, t?). Of
course, we want a0 to reflect actual correspondences
between t and t?, so we want a reasonable definition
that ensures the alignments are of reasonable qual-
ity. Fortunately, it turns out that we can efficiently
optimize a given a fixed tree pair and weight vector:
a? = arg max
a
P(a|t, t?, s, s?, w)
= arg max
a
P(t, a, t?|s, s?, w)
= arg max
a
exp(w>?(t, a, t?))
This optimization requires only that we search for
an optimal alignment. Because all our features can
be factored to individual node pairs, this can be done
with the Hungarian algorithm in cubic time.4 Note
that we do not enforce any kind of domination con-
sistency in the matching: for example, the optimal
alignment might in principle have the source root
aligning to a target non-root and vice versa.
We then define a0(t, t?) as the alignment that
maximizes w>0 ?(t, a, t
?), where w0 is a fixed initial
weight vector with a weight of 1 for INSIDEBOTH,
-1 for INSRCOUTTRG and INTRGOUTSRC, and 0
4There is a minor modification to allow nodes not to match.
Any alignment link which has negative score is replaced by a
zero-score link, and any zero-score link in the solution is con-
sidered a pair of unmatched nodes.
for all other features. Then, we simplify (2) by fix-
ing the alignments a0:
w? = arg max
w
exp(w>?(g, a0(g, g?), g?))
?
(t,t?) exp(w
>?(t, a0(t, t?), t?))
(3)
This optimization has no latent variables and is
therefore convex and straightforward. However,
while we did use this as a rapid training procedure
during development, fixing the alignments a priori is
both unsatisfying and also less effective than a pro-
cedure which allows the alignments a to adapt dur-
ing training.
Again, for fixed alignments a, optimizing w is
easy. Similarly, with a fixed w, finding the optimal
a for any particular tree pair is also easy. Another
option is therefore to use an iterative procedure that
alternates between choosing optimal alignments for
a fixed w, and then reoptimizing w for those fixed
alignments according to (3). By iterating, we per-
form the following optimization:
w? = arg max
w
maxa exp(w>?(g, a, g?))
?
(t,t?) maxa exp(w
>?(t, a, t?))
(4)
Note that (4) is just (2) with summation replaced
by maximization. Though we do not know of any
guarantees for this EM-like algorithm, in practice
it converges after a few iterations given sufficient
training data. We initialize the procedure by setting
w0 as defined above.
4.2 Pseudo-gold Trees
When training our model, we approximate the sets
of all trees with k-best lists, T and T ?, produced
by monolingual parsers. Since these sets are not
guaranteed to contain the gold trees g and g?, our
next approximation is to define a set of pseudo-gold
trees, following previous work in monolingual parse
reranking (Charniak and Johnson, 2005). We define
T? (T? ?) as the F1-optimal subset of T (T ?). We then
modify (4) to reflect the fact that we are seeking to
maximize the likelihood of trees in this subset:
w? = arg max
w
?
(t,t?)?(T? ,T? ?)
P(t, t?|s, s?, w) (5)
where P(t, t?|s, s?, w) =
maxa exp(w>?(t, a, t?))
?
(t?,t??)?(T,T ?) maxa exp(w
>?(t?, a, t??))
(6)
881
4.3 Training Set Pruning
To reduce the time and space requirements for train-
ing, we do not always use the full k-best lists. To
prune the set T , we rank all the trees in T from 1 to
k, according to their log likelihood under the base-
line parsing model, and find the rank of the least
likely pseudo-gold tree:
r? = min
t?T?
rank(t)
Finally, we restrict T based on rank:
Tpruned = {t ? T |rank(t) ? r
? + }
where  is a free parameter of the pruning procedure.
The restricted set T ?pruned is constructed in the same
way. When training, we replace the sum over all tree
pairs in (T, T ?) in the denominator of (6) with a sum
over all tree pairs in (Tpruned, T ?pruned).
The parameter  can be set to any value from 0
to k, with lower values resulting in more efficient
training, and higher values resulting in better perfor-
mance. We set  by empirically determining a good
speed/performance tradeoff (see ?6.2).
5 Joint Selection
At test time, we have a weight vector w and so
selecting optimal trees for the sentence pair (s, s?)
from a pair of k best lists, (T, T ?) is straightforward.
We just find:
(t?, t??) = arg max
(t,t?)?(T,T ?)
max
a
P(t, a, t?|s, s?, w)
= arg max
(t,t?)?(T,T ?)
max
a
w>?(t, a, t?)
Note that with no additional cost, we can also find
the optimal alignment between t? and t??:
a? = arg max
a
w>?(t?, a, t??)
5.1 Test Set Pruning
Because the size of (T, T ?) grows asO(k2), the time
spent iterating through all these tree pairs can grow
unreasonably long, particularly when reranking a set
of sentence pairs the size of a typical MT corpus. To
combat this, we use a simple pruning technique to
limit the number of tree pairs under consideration.
Training Dev Test
Articles 1-270 301-325 271-300
Ch Sentences 3480 352 348
Eng Sentences 3472 358 353
Bilingual Pairs 2298 270 288
Table 1: Sentence counts from bilingual Chinese tree-
bank corpus.
To prune the list of tree pairs, first we rank them
according to the metric:
wSOURCELL ? SOURCELL +wTARGETLL ? TARGETLL
Then, we simply remove all tree pairs whose rank-
ing falls below some empirically determined cutoff.
As we show in ?6.3, by using this technique we are
able to speed up reranking by a factor of almost 20
without an appreciable loss of performance.
6 Statistical Parsing Experiments
All the data used to train the joint parsing model and
to evaluate parsing performance were taken from ar-
ticles 1-325 of the Chinese treebank, which all have
English translations with gold-standard parse trees.
The articles were split into training, development,
and test sets according to the standard breakdown for
Chinese parsing evaluations. Not all sentence pairs
could be included for various reasons, including
one-to-many Chinese-English sentence alignments,
sentences omitted from the English translations, and
low-fidelity translations. Additional sentence pairs
were dropped from the training data because they
had unambiguous parses in at least one of the two
languages. Table 1 shows how many sentences were
included in each dataset.
We had two training setups: rapid and full. In the
rapid training setup, only 1000 sentence pairs from
the training set were used, and we used fixed align-
ments for each tree pair rather than iterating (see
?4.1). The full training setup used the iterative train-
ing procedure on all 2298 training sentence pairs.
We used the English and Chinese parsers in
Petrov and Klein (2007)5 to generate all k-best lists
and as our evaluation baseline. Because our bilin-
gual data is from the Chinese treebank, and the data
5Available at http://nlp.cs.berkeley.edu.
882
typically used to train a Chinese parser contains the
Chinese side of our bilingual training data, we had
to train a new Chinese grammar using only articles
400-1151 (omitting articles 1-270). This modified
grammar was used to generate the k-best lists that
we trained our model on. However, as we tested on
the same set of articles used for monolingual Chi-
nese parser evaluation, there was no need to use
a modified grammar to generate k-best lists at test
time, and so we used a regularly trained Chinese
parser for this purpose.
We also note that since all parsing evaluations
were performed on Chinese treebank data, the Chi-
nese test sentences were in-domain, whereas the
English sentences were very far out-of-domain for
the Penn Treebank-trained baseline English parser.
Hence, in these evaluations, Chinese scores tend to
be higher than English ones.
Posterior word alignment probabilities were ob-
tained from the word aligner of Liang et al (2006)
and DeNero and Klein (2007)6, trained on approxi-
mately 1.7 million sentence pairs. For our alignment
model we used an HMM in each direction, trained to
agree (Liang et al, 2006), and we combined the pos-
teriors using DeNero and Klein?s (2007) soft union
method.
Unless otherwise specified, the maximum value
of k was set to 100 for both training and testing, and
all experiments used a value of 25 as the  parameter
for training set pruning and a cutoff rank of 500 for
test set pruning.
6.1 Feature Ablation
To verify that all our features were contributing to
the model?s performance, we did an ablation study,
removing one group of features at a time. Table 2
shows the F1 scores on the bilingual development
data resulting from training with each group of fea-
tures removed.7 Note that though head word fea-
tures seemed to be detrimental in our rapid train-
ing setup, earlier testing had shown a positive effect,
so we reran the comparison using our full training
setup, where we again saw an improvement when
including these features.
6Available at http://nlp.cs.berkeley.edu.
7We do not have a test with the basic alignment features
removed because they are necessary to compute a0(t, t?).
Baseline Parsers
Features Ch F1 Eng F1 Tot F1
Monolingual 84.95 76.75 81.15
Rapid Training
Features Ch F1 Eng F1 Tot F1
All 86.37 78.92 82.91
?Hard align 85.83 77.92 82.16
?Scaled align 86.21 78.62 82.69
?Head word 86.47 79.00 83.00
?Span diff 86.00 77.49 82.07
?Num children 86.26 78.56 82.69
?Child labels 86.35 78.45 82.68
Full Training
Features Ch F1 Eng F1 Tot F1
All 86.76 79.41 83.34
?Head word 86.42 79.53 83.22
Table 2: Feature ablation study. F1 on dev set after train-
ing with individual feature groups removed. Performance
with individual baseline parsers included for reference.
 Ch F1 Eng F1 Tot F1 Tree Pairs
15 85.78 77.75 82.05 1,463,283
20 85.88 77.27 81.90 1,819,261
25 86.37 78.92 82.91 2,204,988
30 85.97 79.18 82.83 2,618,686
40 86.10 78.12 82.40 3,521,423
50 85.95 78.50 82.50 4,503,554
100 86.28 79.02 82.91 8,997,708
Table 3: Training set pruning study. F1 on dev set after
training with different values of the  parameter for train-
ing set pruning.
6.2 Training Set Pruning
To find a good value of the  parameter for train-
ing set pruning we tried several different values, us-
ing our rapid training setup and testing on the dev
set. The results are shown in Table 3. We selected
25 as it showed the best performance/speed trade-
off, on average performing as well as if we had done
no pruning at all, while requiring only a quarter the
memory and CPU time.
6.3 Test Set Pruning
We also tried several different values of the rank cut-
off for test set pruning, using the full training setup
883
Cutoff Ch F1 Eng F1 Tot F1 Time (s)
50 86.34 79.26 83.04 174
100 86.61 79.31 83.22 307
200 86.67 79.39 83.28 509
500 86.76 79.41 83.34 1182
1000 86.80 79.39 83.35 2247
2000 86.78 79.35 83.33 4476
10,000 86.71 79.37 83.30 20,549
Table 4: Test set pruning study. F1 on dev set obtained
using different cutoffs for test set pruning.
and testing on the dev set. The results are in Table 4.
For F1 evaluation, which is on a very small set of
sentences, we selected 500 as the value with the best
speed/performance tradeoff. However, when rerank-
ing our entire MT corpus, we used a value of 200,
sacrificing a tiny bit of performance for an extra fac-
tor of 2 in speed.8
6.4 Sensitivity to k
Since our bitext parser currently operates as a
reranker, the quality of the trees is limited by the
quality of the k-best lists produced by the baseline
parsers. To test this limitation, we evaluated perfor-
mance on the dev set using baseline k-best lists of
varying length. Training parameters were fixed (full
training setup with k = 100) and test set pruning was
disabled for these experiments. The results are in Ta-
ble 5. The relatively modest gains with increasing k,
even as the oracle scores continue to improve, indi-
cate that performance is limited more by the model?s
reliance on the baseline parsers than by search errors
that result from the reranking approach.
6.5 Final Results
Our final evaluation was done using the full training
setup. Here, we report F1 scores on two sets of data.
First, as before, we only include the sentence pairs
from our bilingual corpus to fully demonstrate the
gains made by joint parsing. We also report scores
on the full test set to allow easier comparison with
8Using a rank cutoff of 200, the reranking step takes slightly
longer than serially running both baseline parsers, and generat-
ing k-best lists takes slightly longer than getting 1-best parses,
so in total, joint parsing takes about 2.3 times as long as mono-
lingual parsing. With a rank cutoff of 500, total parsing time is
scaled by a factor of around 3.8.
Joint Parsing Oracle
k Ch F1 Eng F1 Ch F1 Eng F1
1 84.95 76.75 84.95 76.75
10 86.23 78.43 90.05 81.99
25 86.64 79.27 90.99 83.37
50 86.61 79.10 91.82 84.14
100 86.71 79.37 92.23 84.73
150 86.67 79.47 92.49 85.17
Table 5: Sensitivity to k study. Joint parsing and oracle
F1 obtained on dev set using different maximum values
of k when generating baseline k-best lists.
F1 on bilingual data only
Parser Ch F1 Eng F1 Tot F1
Baseline 83.50 79.25 81.44
Joint 85.25 81.72 83.52
F1 on full test set
Parser Ch F1 Eng F1 Tot F1
Baseline 82.91 78.93 81.00
Joint 84.24 80.87 82.62
Table 6: Final evaluation. Comparison of F1 on test set
between baseline parsers and joint parser.
past work on Chinese parsing. For the latter evalu-
ation, sentences that were not in the bilingual cor-
pus were simply parsed with the baseline parsers.
The results are in Table 6. Joint parsing improves
F1 by 2.5 points on out-of-domain English sentences
and by 1.8 points on in-domain Chinese sentences;
this represents the best published Chinese treebank
parsing performance, even after sentences that lack
a translation are taken into account.
7 Machine Translation
To test the impact of joint parsing on syntactic MT
systems, we compared the results of training an MT
system with two different sets of trees: those pro-
duced by the baseline parsers, and those produced by
our joint parser. For this evaluation, we used a syn-
tactic system based on Galley et al (2004) and Gal-
ley et al (2006), which extracts tree-to-string trans-
ducer rules based on target-side trees. We trained the
system on 150,000 Chinese-English sentence pairs
from the training corpus of Wang et al (2007), and
used a large (close to 5 billion tokens) 4-gram lan-
884
Baseline Joint Moses
BLEU 18.7 21.1 18.8
Table 7: MT comparison on a syntactic system trained
with trees output from either baseline monolingual
parsers or our joint parser. To facilitate relative compari-
son, the Moses (Koehn et al, 2007) number listed reflects
the default Moses configuration, including its full distor-
tion model, and standard training pipeline.
guage model for decoding. We tuned and evaluated
BLEU (Papineni et al, 2001) on separate held-out
sets of sentences of up to length 40 from the same
corpus. The results are in Table 7, showing that joint
parsing yields a BLEU increase of 2.4.9
8 Conclusions
By jointly parsing (and aligning) sentences in a
translation pair, it is possible to exploit mutual con-
straints that improve the quality of syntactic analy-
ses over independent monolingual parsing. We pre-
sented a joint log-linear model over source trees,
target trees, and node-to-node alignments between
them, which is used to select an optimal tree pair
from a k-best list. On Chinese treebank data, this
procedure improves F1 by 1.8 on Chinese sentences
and by 2.5 on out-of-domain English sentences. Fur-
thermore, by using this joint parsing technique to
preprocess the input to a syntactic MT system, we
obtain a 2.4 BLEU improvement.
Acknowledgements
We would like to thank the anonymous reviewers for
helpful comments on an earlier draft of this paper
and Adam Pauls and Jing Zheng for help in running
our MT experiments.
References
Anthony Aue, Arul Menezes, Bob Moore, Chris Quirk,
and Eric Ringger. 2004. Statistical machine trans-
lation using labeled semantic dependency graphs. In
TMI.
9Note that all numbers are single-reference BLEU scores
and are not comparable to multiple reference scores or scores
on other corpora.
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English chinese translation treebank v 1.0. Web
download. LDC2007T02.
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the chinese treebank. In
Second Chinese Language Processing Workshop.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In ACL.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, CLSP, Johns Hopkins University.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. Research report, IBM.
RC22176.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
885
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In ACL.
Libin Shen, Jinxi Xu, and Ralph Weishedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
ACL.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using english
to parse korean. In EMNLP.
Leslie G. Valiant. 1979. The complexity of computing
the permanent. In Theoretical Computer Science 8.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with struc-
tured and web-based language models. In IEEE ASRU
Workshop.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The cmu-aka syntax aug-
mented machine translation system for iwslt-06. In
IWSLT.
886
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 863?872, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Transforming Trees to Improve Syntactic Convergence
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
We describe a transformation-based learning
method for learning a sequence of mono-
lingual tree transformations that improve the
agreement between constituent trees and word
alignments in bilingual corpora. Using the
manually annotated English Chinese Transla-
tion Treebank, we show how our method au-
tomatically discovers transformations that ac-
commodate differences in English and Chi-
nese syntax. Furthermore, when transforma-
tions are learned on automatically generated
trees and alignments from the same domain as
the training data for a syntactic MT system,
the transformed trees achieve a 0.9 BLEU im-
provement over baseline trees.
1 Introduction
Monolingually, many Treebank conventions are
more or less equally good. For example, the En-
glish WSJ treebank (Marcus et al1993) attaches
verbs to objects rather than to subjects, and it at-
taches prepositional modifiers outside of all quan-
tifiers and determiners. The former matches most
linguistic theories while the latter does not, but to
a monolingual parser, these conventions are equally
learnable. However, once bilingual data is involved,
such treebank conventions entail constraints on rule
extraction that may not be borne out by semantic
alignments. To the extent that there are simply di-
vergences in the syntactic structure of the two lan-
guages, it will often be impossible to construct syn-
tax trees that are simultaneously in full agreement
with monolingual linguistic theories and with the
alignments between sentences in both languages.
To see this, consider the English tree in Figure 1a,
taken from the English side of the English Chi-
nese Translation Treebank (Bies et al2007). The
lowest VP in this tree is headed by ?select,? which
aligns to the Chinese verb ???.? However, ??
?? also aligns to the other half of the English in-
finitive, ?to,? which, following common English lin-
guistic theory, is outside the VP. Because of this
violating alignment, many syntactic machine trans-
lation systems (Galley et al2004; Huang et al
2006) won?t extract any translation rules for this
constituent. However, by applying a simple trans-
formation to the English tree to set up the infinitive
as its own constituent, we get the tree in Figure 1b,
which may be less well-motivated linguistically, but
which corresponds better to the Chinese-mediated
semantics and permits the extraction of many more
syntactic MT rules.
In this work, we develop a method based on
transformation-based learning (Brill, 1995) for au-
tomatically acquiring a sequence of tree transforma-
tions of the sort in Figure 1. Once the transformation
sequence has been learned, it can be deterministi-
cally applied to any parsed sentences, yielding new
parse trees with constituency structures that agree
better with the bilingual alignments yet remain con-
sistent across the corpus. In particular, we use this
method to learn a transformation sequence for the
English trees in a set of English to Chinese MT train-
ing data. In experiments with a string-to-tree trans-
lation system, we show resulting improvements of
up to 0.9 BLEU.
A great deal of research in syntactic machine
translation has been devoted to handling the inher-
ent syntactic divergence between source and target
languages. Some systems attempt to model the dif-
ferences directly (Yamada and Knight, 2001; Eis-
ner, 2003), but most recent work focuses on reduc-
ing the sensitivity of the rule-extraction procedure
to the constituency decisions made by 1-best syn-
tactic parsers, either by using forest-based methods
863
The
first step is to select team members
S
NP
S
VP
VP
VPTO
VBZ
ADVPVB

?? ? ? ?
(a) Before
The
first step is to select team members
S
NP
S
VP
VP
VP
VBZ

?? ? ? ?
TO+VB
VBTO ADVP
(b) After
Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the trans-
formation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly
created TO+VB node are extractable.
for learning translation rules (Mi and Huang, 2008;
Zhang et al2009), or by learning rules that en-
code syntactic information but do not strictly ad-
here to constituency boundaries (Zollmann et al
2006; Marton and Resnik, 2008; Chiang, 2010). The
most closely related MT system is that of Zhao et al
(2011), who train a rule extraction system to trans-
form the subtrees that make up individual translation
rules using a manually constructed set of transfor-
mations similar to those learned by our system.
Instead of modifying the MT system to work
around the input annotations, our system modifies
the input itself in order to improve downstream
translation. Most systems of this sort learn how to
modify word alignments to agree better with the syn-
tactic parse trees (DeNero and Klein, 2007; Fossum
et al2008), but there has also been other work di-
rectly related to improving agreement by modifying
the trees. Burkett et al2010) train a bilingual pars-
ing model that uses bilingual agreement features to
improve parsing accuracy. More closely related to
the present work, Katz-Brown et al2011) retrain a
parser to directly optimize a word reordering metric
in order to improve a downstream machine transla-
tion system that uses dependency parses in a prepro-
cessing reordering step. Our system is in the same
basic spirit, using a proxy evaluation metric (agree-
ment with alignments; see Section 2 for details) to
improve performance on a downstream translation
task. However, we are concerned more generally
with the goal of creating trees that are more com-
patible with a wide range of syntactically-informed
translation systems, particularly those that extract
translation rules based on syntactic constituents.
2 Agreement
Our primary goal in adapting parse trees is to im-
prove their agreement with a set of external word
alignments. Thus, our first step is to define an agree-
ment score metric to operationalize this concept.
Central to the definition of our agreement score
is the notion of an extractable node. Intuitively, an
extractable English1 tree node (also often called a
?frontier node? in the literature), is one whose span
aligns to a contiguous span in the foreign sentence.
Formally, we assume a fixed word alignment a =
{(i, j)}, where (i, j) ? a means that English word
i is aligned to foreign word j. For an English span
[k, `] (inclusive), the set of aligned foreign words is:
fset([k, `]) = {j | ? i : k ? i ? `; (i, j) ? a}
We then define the aligned foreign span as:
fspan([k, `]) = [min(fset([k, `])),max(fset([k, `]))]
1For expositional clarity, we will refer to ?English? and ?for-
eign? sentences/trees, but our definitions are in no way language
dependent and apply equally well to any language pair.
864
The aligned English span for a given foreign span
[s, t] is defined analogously:
eset([s, t]) = {i | ? j : s ? j ? t; (i, j) ? a}
espan([s, t]) = [min(eset([s, t])),max(eset([s, t]))]
Finally, we define [k, `] to be extractable if and
only if it has at least one word alignment and its
aligned foreign span aligns back to a subspan of
[k, `]:
fset([k, `]) 6= ? ? espan(fspan([k, `])) ? [k, `]
With this definition of an extractable span, we can
now define the agreement score ga(t) for an English
tree t, conditioned on an alignment a:2
ga(t) =
?
[k,`]?t:
|[k,`]|>1
sign([k, `]) (1)
Where
sign([k, `]) =
{
1 [k, `] is extractable
?1 otherwise
Importantly, the sum in Equation 1 ranges over all
unique spans in t. This is simply to make the met-
ric less gameable, preventing degenerate solutions
such as an arbitrarily long chain of unary produc-
tions over an extractable span. Also, since all indi-
vidual words are generated by preterminal part-of-
speech nodes, the sum skips over all length 1 spans.
As a concrete example of agreement score, we can
return to Figure 1. The tree in Figure 1a has 6 unique
spans, but only 5 are extractable, so the total agree-
ment score is 5 - 1 = 4. After the transformation,
though, the tree in Figure 1b has 6 extractable spans,
so the agreement score is 6.
3 Transformation-Based Learning
Transformation-based learning (TBL) was origi-
nally introduced via the Brill part-of-speech tag-
ger (Brill, 1992) and has since been applied to a wide
variety of NLP tasks, including binary phrase struc-
ture bracketing (Brill, 1993), PP-attachment disam-
biguation (Brill and Resnik, 1994), base NP chunk-
ing (Ramshaw and Marcus, 1995), dialogue act tag-
ging (Samuel et al1998), and named entity recog-
nition (Black and Vasilakopoulos, 2002).
2Unextractable spans are penalized in order to ensure that
space is saved for the formation of extractable ones.
The generic procedure is simple, and requires
only four basic inputs: a set of training sentences, an
initial state annotator, an inventory of atomic trans-
formations, and an evaluation metric. First, you ap-
ply the initial state annotator (here, the source of
original trees) to your training sentences to ensure
that they all begin with a legal annotation. Then,
you test each transformation in your inventory to see
which one will yield the greatest improvement in the
evaluation metric if applied to the training data. You
greedily apply this transformation to the full training
set and then repeat the procedure, applying transfor-
mations until some stopping criterion is met (usu-
ally either a maximum number of transformations,
or a threshold on the marginal improvement in the
evaluation metric).
The output of the training procedure is an ordered
set of transformations. To annotate new data, you
simply label it with the same initial state annotator
and then apply each of the learned transformations
in order. This process has the advantage of being
quite fast (usually linear in the number of transfor-
mations and the length of the sentence; for parsing,
the cost will typically be dominated by the cost of
the initial state annotator), and, unlike the learned
parameters of a statistical model, the set of learned
transformations itself can often be of intrinsic lin-
guistic interest.
For our task, we have already defined the evalua-
tion metric (Section 2) and the initial state annotator
will either be the gold Treebank trees or a Treebank-
trained PCFG parser. Thus, to fully describe our sys-
tem, it only remains to define the set of possible tree
transformations.
4 Tree Transformations
The definition of an atomic transformation consists
of two parts: a rewrite rule and the triggering envi-
ronment (Brill, 1995). Tree transformations are best
illustrated visually, and so for each of our transfor-
mation types, both parts of the definition are repre-
sented schematically in Figures 2-7. We have also
included a real-world example of each type of trans-
formation, taken from the English Chinese Transla-
tion Treebank.
Altogether, we define six types of tree transfor-
mations. Each class of transformation takes be-
865
A... ...
B C
A
... ...
B C
B+C
Type: ARTICULATE
Args:
A: PARENT, B: LEFT, C: RIGHT
(a) Schematic
??
?
S
Other members will arrive in two groups .
? ?
? ? ? ?
VPNP
.
??
?
S
Other members will arrive in two groups .
? ?
? ? ? ?
VPNP
.NP+VP
(b) Example: ARTICULATE?S, NP, VP?
Figure 2: ARTICULATE transformations.
A
...
B
...
C D
A
... ...
C D
Type: FLATTEN
Args:
A: PARENT, B: TARGET
A
...
B
...
D E
A
... ...
E C
Type: FLATTENINCONTEXT
Args:
A: PARENT, B: TARGET,
C: SIBLING, left: DIRECTION
C D
(a) Schematic
?? ??
NP
the China Trade Promotion Council
NML NNP
NNPNNP
NNPDT
?? ??
NP
the China Trade Promotion Council
NNPNNPNNPNNPDT
(b) Example: FLATTENINCONTEXT?NP, NML, NNP, left?
Figure 3: FLATTEN transformations.
A
...
B
C
...
A
...
B
...
C
Type: PROMOTE
Args:
A: GRANDPARENT, B: PARENT,
C: CHILD, left: DIRECTION
(a) Schematic
NP
?? ? ???
IN
by the French player N. Taugia
NP NP
PP
NP
?? ? ???
IN
by the French player N. Taugia
NP
NP
PP
(b) Example: PROMOTE?PP, NP, NP, left?
Figure 4: PROMOTE transformations.
A
...
B C
...
A
...
B
...
C
Type: DEMOTE
Args:
A: PARENT, B: DEMOTER,
C: DEMOTED, left: DIRECTION
......
(a) Schematic
? ?? ?
fly
to Beijing on the 2nd
VP
??
IN NP NPIN
PPVB PP
? ?? ?
fly
to Beijing on the 2nd
VP
??
IN NP NPINVB
PP PP
(b) Example: DEMOTE?VP, PP, VB, right?
Figure 5: DEMOTE transformations.
866
AB
C
D
...
A
B
C
......
Type: TRANSFER
Args:
A: GRANDPARENT, B: AUNT,
C: PARENT, D: TARGET,
left: DIRECTION
...
D
(a) Schematic
serious consequences that cause losses
NP
NP
SBAR
SWHNP
???? ? ?
JJ NNS
serious consequences that cause losses
NP
NP SBAR
S
???? ? ?
WHNPJJ NNS
(b) Example: TRANSFER?NP, NP, SBAR, WHNP, left?
Figure 6: TRANSFER transformations.
A
B C
D
...
A
B+D
C
...
B D
Type: ADOPT
Args:
A: GRANDPARENT, B: AUNT,
C: PARENT, D: TARGET,
left: DIRECTION
(a) Schematic
Sabor also tied with Setangon
S
VP
PP
?? ? ??
RB VBD
NP ADVP
????
VP
Sabor also tied with Setangon
S
PP
?? ? ??
RB
VBD
NP
????
RB+VP
(b) Example: ADOPT?S, VP, ADVP, RB, right?
Figure 7: ADOPT transformations.
tween two and four syntactic category arguments,
and most also take a DIRECTION argument that
can have the value left or right.3 We refer to the
nodes in the schematics whose categories are argu-
ments of the transformation definition as participat-
ing nodes. Basically, a particular transformation is
triggered anywhere in a parse tree where all partici-
pating nodes appear in the configuration shown. The
exact rules for the triggering environment are:
1. Each participating node must appear in the
schematically illustrated relationship to the
others. The non-participating nodes in the
schematic do not have to appear. Similarly, any
number of additional nodes can appear as sib-
lings, parents, or children of the explicitly illus-
trated nodes.
2. Any node that will gain a new child as a re-
sult of the transformation must already have at
least one nonterminal child. We have drawn the
schematics to reflect this, so this condition is
3To save space, the schematic for each of these transforma-
tions is only shown for the left direction, but the right version is
simply the mirror image.
equivalent to saying that any participating node
that is drawn with children must have a phrasal
syntactic category (i.e. it cannot be a POS).
3. Repeated mergings are not allowed. That is, the
newly created nodes that result from an ARTIC-
ULATE or ADOPT transformation cannot then
participate as the LEFT or RIGHT argument of a
subsequent ARTICULATE transformation or as
the AUNT or TARGET argument of a subsequent
ADOPT transformation. This is simply to pre-
vent the unrestrained proliferation of new syn-
tactic categories.
The rewrite rule for a transformation is essentially
captured in the corresponding schematic. Additional
nodes that do not appear in the schematic are gener-
ally handled in the obvious way: unillustrated chil-
dren or parents of illustrated nodes remain in place,
while unillustrated siblings of illustrated nodes are
handled identically to their illustrated siblings. The
only additional part of the rewrite that is not shown
explicitly in the schematics is that if the node in the
PARENT position of a TRANSFER or ADOPT trans-
formation is left childless by the transformation (be-
867
cause the TARGET node was its only child), then it is
deleted from the parse tree. In the case of a transfor-
mation whose triggering environment appears multi-
ple times in a single tree, transformations are always
applied leftmost/bottom-up and exhaustively.4
In principle, our transformation inventory consists
of all possible assignments of syntactic categories to
the arguments of each of the transformation types
(subject to the triggering environment constraints).
In practice, though, we only ever consider trans-
formations whose triggering environments appear in
the training corpus (including new triggering envi-
ronments that appear as the result of earlier trans-
formations). While the theoretical space of possi-
ble transformations is exponentially large, the set
of transformations we actually have to consider is
quite manageable, and empirically grows substan-
tially sublinearly in the size of the training set.
5 Results and Analysis
There are two ways to use this procedure. One is to
apply it to the entire data set, with no separate train-
ing phase. Given that the optimization has no notion
of gold transformations, this procedure is roughly
like an unsupervised learner that clusters its entire
data. Another way is to learn annotations on a sub-
set of data and apply it to new data. We choose the
latter primarily for reasons of efficiency and simplic-
ity: many common use cases are easiest to manage
when annotation systems can be trained once offline
and then applied to new data as it comes in.
Since we intend for our system to be used as
a pre-trained annotator, it is important to ensure
that the learned transformation sequence achieves
agreement score gains that generalize well to un-
seen data. To minimize errors that might be intro-
duced by the noise in automatically generated parses
and word alignments, and to maximize reproducibil-
ity, we conducted our initial experiments on the En-
glish Chinese Translation Treebank. For this dataset,
the initial state annotations (parse trees) were man-
ually created by trained human annotators, as were
the word alignments used to compute the agreement
4The transformation is repeatedly applied at the lowest, left-
most location of the parse tree where the triggering environment
appears, until the triggering environment no longer appears any-
where in the tree.
0 1 
2 3 
4 5 
6 7 
8 9 
0 500 1000 1500 2000 2500 
Aver
age A
greem
ent S
core 
Impr
ovem
ent 
Number of Transformations 
Training Dev 
Figure 8: Transformation results on the English Chinese
Translation Treebank. The value plotted is the average
(per-sentence) improvement in agreement score over the
baseline trees.
Transfor- Total Extractable Agreement
mations Spans Spans Score
0 13.15 9.78 6.40
10 12.57 10.36 8.15
50 13.41 11.38 9.35
200 14.03 11.96 9.89
1584 14.58 12.36 10.15
2471 14.65 12.35 10.06
Table 1: Average span counts and agreement scores on
the English Chinese Translation Treebank development
set. The highest agreement score was attained at 1584
transformations, but most of the improvement happened
much earlier.
score.5 The data was divided into training/dev/test
using the standard Chinese parsing split; we trained
the system on the training set (2261 sentences af-
ter filtering out sentences with missing annotations),
and evaluated on the development set (223 sentences
after filtering).
The improvements in agreement score are shown
in Figure 8, with a slightly more detailed breakdown
at a few fixed points in Table 1. While the system
was able to find up to 2471 transformations that im-
proved the training set agreement score, the major-
ity of the improvement, and especially the majority
of the improvement that generalized to the test set,
5The annotation guidelines for the English side of this Tree-
bank are similar, though not identical, to those for the WSJ
Treebank.
868
1 ARTICULATE?S,NP,VP?
2 FLATTENINCONTEXT?PP,NP,IN,right?
3 PROMOTE?VP,VP,VBN,left?
4 ADOPT?VP,TO,VP,VB,left?
5 ADOPT?PP,VBG,PP,IN,left?
6 FLATTEN?VP,VP?
7 ARTICULATE?VP,VBD,NP?
8 FLATTENINCONTEXT?PP,NML,NNP,left?
9 ARTICULATE?NP,NNP,NNS?
10 ARTICULATE?S,NP,ADVP?
11 TRANSFER?NP,NP,SBAR,WHNP,left?
12 FLATTENINCONTEXT?NP,NML,NNP,left?
13 ARTICULATE?NP,NN,NNS?
14 TRANSFER?NP,NP+,,SBAR,WHNP,left?
15 ADOPT?PP,IN,PP,IN,left?
16 PROMOTE?S,VP,CC+VP,right?
17 ARTICULATE?VP,VBZ,VBN?
18 ARTICULATE?VP,VBD,PP?
19 ARTICULATE?VP,MD,ADVP?
20 ADOPT?PP,SYM,QP,CD,right?
Table 2: The first 20 learned transformations, excluding
those that only merged punctuation or conjunctions with
adjacent phrases. The first 5 are illustrated in Figure 9.
was achieved within the first 200 or so transforma-
tions. We also see from Table 1 that, though the first
few transformations deleted many non-extractable
spans, the overall trend was to produce more finely
articulated trees, with the full transformation se-
quence increasing the number of spans by more than
10%.
As discussed in Section 3, one advantage of TBL
is that the learned transformations can themselves
often be interesting. For this task, some of the high-
est scoring transformations did uninteresting things
like conjoining conjunctions or punctuation, which
are often either unaligned or aligned monotonically
with adjacent phrases. However, by filtering out
all ARTICULATE transformations where either the
LEFT or RIGHT argument is ?CC?, ?-RRB-?, ?,?, or
?.? and taking the top 20 remaining transformations,
we get the list in Table 2, the first 5 of which are
also illustrated in Figure 9. Some of these (e.g. #1,
#7, #10) are additional ways of creating new spans
when English and Chinese phrase structures roughly
agree, but many others do recover known differences
in English and Chinese syntax. For example, many
of these transformations directly address compound
verb forms in English, which tend to align to single
words in Chinese: #3 (past participle constructions),
#4 (infinitive), #6 (all), and #17 (present perfect).
We also see differences between English and Chi-
nese internal NP structure (e.g. #9, #12, #13).
6 Machine Translation
The ultimate goal of our system is to improve
the agreement between the automatically generated
parse trees and word alignments that are used as
training data for syntactic machine translation sys-
tems. Given the amount of variability between the
outputs of different parsers and word aligners (or
even the same systems with different settings), the
best way to improve agreement is to learn a trans-
formation sequence that is specifically tuned for the
same annotators (parsers and word aligners) we are
evaluating with. In particular, we found that though
training on the English Chinese Translation Tree-
bank produces clean, interpretable rules, prelimi-
nary experiments showed little to no improvement
from using these rules for MT, primarily because
actual alignments are not only noisier but also sys-
tematically different from gold ones. Thus, all rules
used for MT experiments were learned from auto-
matically annotated text.
For our Chinese to English translation experi-
ments, we generated word alignments using the
Berkeley Aligner (Liang et al2006) with default
settings. We used an MT pipeline that conditions
on target-side syntax, so our initial state annotator
was the Berkeley Parser (Petrov and Klein, 2007),
trained on a modified English treebank that has been
adapted to match standard MT tokenization and cap-
italization schemes.
As mentioned in Section 5, we could, in principle
train on all 500k sentences of our MT training data.
However, this would be quite slow: each iteration of
the training procedure requires iterating through all
n training sentences6 once for each of the m can-
didate transformations, for a total cost of O(nm)
where m grows (albeit sublinearly) with n. Since the
6By using a simple hashing scheme to keep track of trigger-
ing environments, this cost can be reduced greatly but is still
linear in the number of training sentences.
869
S... ...
NP VP
S
... ...
NP VP
NP+VP
(a) ARTICULATE?S,NP,VP?
PP
...
IN
...
A B
PP
... ...
A BNP IN
(b) FLATTENINCONTEXT?PP,NP,IN,right?
VP
...
VP
VBN
...
VP
...
VP
...
VBN
(c) PROMOTE?VP,VP,VBN,left?
VP
TO VP
VB
...
VP
TO+VB VP
...
TO VB
(d) ADOPT?VP,TO,VP,VB,left?
PP
VBG PP
IN
...
VP
VBG+IN PP
...
VBG IN
(e) ADOPT?PP,VBG,PP,IN,left?
Figure 9: Illustrations of the top 5 transformations from
Table 2.
most useful transformations almost by definition are
ones that are triggered the most frequently, any rea-
sonably sized training set is likely to contain them,
and so it is not actually likely that dramatically in-
creasing the size of the training set will yield partic-
ularly large gains.
Thus, to train our TBL system, we extracted a ran-
dom subset of 3000 sentences to serve as a train-
ing set.7 We also extracted an additional 1000 sen-
tence test set to use for rapidly evaluating agreement
score generalization. Figure 10 illustrates the im-
provements in agreement score for the automatically
annotated data, analogous to Figure 8. The same
general patterns hold, although we do see that the
automatically annotated data is more idiosyncratic
and so more than twice as many transformations are
learned before training set agreement stops improv-
ing, even though the training set sizes are roughly
the same.8 Furthermore, test set generalization in
the automatic annotation setting is a little bit worse,
with later transformations tending to actually hurt
test set agreement.
For our machine translation experiments, we used
the string-to-tree syntactic pipeline included in the
current version of Moses (Koehn et al2007).
Our training bitext was approximately 21.8 mil-
lion words, and the sentences and word alignments
were the same for all experiments; the only differ-
ence between each experiment was the English trees,
for which we tested a range of transformation se-
quence prefixes (including a 0-length prefix, which
just yields the original trees, as a baseline). Since
the transformed trees tended to be more finely artic-
ulated, and increasing the number of unique spans
often helps with rule extraction (Wang et al2007),
we equalized the span count by also testing bina-
rized versions of each set of trees, using the left-
branching and right-branching binarization scripts
included with Moses.9
We tuned on 1000 sentence pairs and tested on
7The sentences were shorter on average than those in the En-
glish Chinese Translation Treebank, so this training set contains
roughly the same number of words as that used in the experi-
ments from Section 5.
8Note that the training set improvement curves don?t actu-
ally flatten out because training halts once no improving trans-
formation exists.
9Binarized trees are guaranteed to have k ? 1 unique spans
for sentences of length k.
870
0 
1 
2 
3 
4 
5 
6 
7 
0 1000 2000 3000 4000 5000 
Aver
age A
greem
ent S
core 
Impr
ovem
ent 
Number of Transformations 
Training Test 
Figure 10: Transformation results on a subset of the MT
training data. The training and test sets are disjoint in
order to measure how well the learned transformation se-
quence generalizes. Once again, we plot the average im-
provement over the baseline trees. Though 5151 transfor-
mations were learned from the training set, the maximum
test set agreement was achieved at 630 transformations,
with an average improvement of 2.60.
642 sentence pairs from the NIST MT04 and MT05
data sets, using the BLEU metric (Papineni et al
2001). As discussed by Clark et al2011), the op-
timizer included with Moses (MERT, Och, 2003) is
not always particularly stable, and results (even on
the tuning set) can vary dramatically across tuning
runs. To mitigate this effect, we first used the Moses
training scripts to extract a table of translation rules
for each set of English trees. Then, for each rule
table, we ran MERT 11 times and selected the pa-
rameters that achieved the maximum tuning BLEU
to use for decoding the test set.
Table 3 shows the results of our translation exper-
iments. The best translation results are achieved by
using the first 139 transformations, giving a BLEU
improvement of more than 0.9 over the strongest
baseline.
7 Conclusion
We have demonstrated a simple but effective pro-
cedure for learning a tree transformation sequence
that improves agreement between parse trees and
word alignments. This method yields clear improve-
ments in the quality of Chinese to English trans-
lation, showing that by manipulating English syn-
tax to converge with Chinese phrasal structure, we
improve our ability to explicitly model the types of
Transfor- Agrmnt BLEU
mations Score None Left Right
0 5.36 31.66 31.81 31.84
32 7.17 32.41 32.17 32.06
58 7.42 32.18 32.68* 32.37
139 7.81 32.20 32.60* 32.77*
630 7.96 32.48 32.06 32.22
5151 7.89 32.13 31.84 32.12
Table 3: Machine translation results. Agreement scores
are taken from the test data used to generate Figure 10.
Note that using 0 transformations just yields the original
baseline trees. The transformation sequence cutoffs at 32,
58, and 139 were chosen to correspond to marginal train-
ing (total) agreement gain thresholds of 50, 25, and 10,
respectively. The cutoff at 630 was chosen to maximize
test agreement score and the cutoff at 5151 maximized
training agreement score. Column headings for BLEU
scores (?None,? ?Left,? ?Right?) refer to the type of bina-
rization used after transformations. Entries marked with
a ?*? show a statistically significant difference (p < 0.05)
from the strongest (right-binarized) baseline, according
to the paired bootstrap (Efron and Tibshirani, 1994).
structural relationships between languages that syn-
tactic MT systems are designed to exploit, even if we
lose some fidelity to the original monolingual anno-
tation standards in the process.
Acknowledgements
This project is funded by an NSF graduate research
fellowship to the first author and by BBN under
DARPA contract HR0011-12-C-0014.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
William J. Black and Argyrios Vasilakopoulos. 2002.
Language independent named entity classification by
modified transformation-based learning and by deci-
sion tree induction. In COLING.
Eric Brill and Philip Resnik. 1994. A transformation-
based approach to prepositional phrase attachment dis-
ambiguation. In COLING.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the workshop on Speech and
Natural Language.
871
Eric Brill. 1993. Automatic grammar induction and pars-
ing free text: A transformation-based approach. In
ACL.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In NAACL:HLT.
David Chiang. 2010. Learning to translate with source
and target syntax. In ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL:HLT.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Bradley Efron and R. J. Tibshirani. 1994. An Introduc-
tion to the Bootstrap (Chapman & Hall/CRC Mono-
graphs on Statistics & Applied Probability). Chapman
and Hall/CRC.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In HLT-NAACL.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for ma-
chine translation reordering. In EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In ACL:HLT.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In EMNLP.
Franz Josef Och. 2003. Miminal error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. Research report, IBM.
RC22176.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning. In
ACL Workshop on Very Large Corpora.
Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-based
learning. In COLING.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In EMNLP.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In ACL-IJCNLP.
Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL:HLT.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax aug-
mented machine translation system for IWSLT-06. In
IWSLT.
872
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 995?1005, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An Empirical Investigation of Statistical Significance in NLP
Taylor Berg-Kirkpatrick David Burkett Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, dburkett, klein}@cs.berkeley.edu
Abstract
We investigate two aspects of the empirical
behavior of paired significance tests for NLP
systems. First, when one system appears
to outperform another, how does significance
level relate in practice to the magnitude of the
gain, to the size of the test set, to the similar-
ity of the systems, and so on? Is it true that for
each task there is a gain which roughly implies
significance? We explore these issues across
a range of NLP tasks using both large collec-
tions of past systems? outputs and variants of
single systems. Next, once significance lev-
els are computed, how well does the standard
i.i.d. notion of significance hold up in practical
settings where future distributions are neither
independent nor identically distributed, such
as across domains? We explore this question
using a range of test set variations for con-
stituency parsing.
1 Introduction
It is, or at least should be, nearly universal that NLP
evaluations include statistical significance tests to
validate metric gains. As important as significance
testing is, relatively few papers have empirically in-
vestigated its practical properties. Those that do
focus on single tasks (Koehn, 2004; Zhang et al
2004) or on the comparison of alternative hypothe-
sis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani
and Ney, 2004; Riezler and Maxwell, 2005).
In this paper, we investigate two aspects of the
empirical behavior of paired significance tests for
NLP systems. For example, all else equal, larger
metric gains will tend to be more significant. How-
ever, what does this relationship look like and how
reliable is it? What should be made of the conven-
tional wisdom that often springs up that a certain
metric gain is roughly the point of significance for
a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU
in machine translation)? We show that, with heavy
caveats, there are such thresholds, though we also
discuss the hazards in their use. In particular, many
other factors contribute to the significance level, and
we investigate several of them. For example, what
is the effect of the similarity between the two sys-
tems? Here, we show that more similar systems tend
to achieve significance with smaller metric gains, re-
flecting the fact that their outputs are more corre-
lated. What about the size of the test set? For ex-
ample, in designing a shared task it is important to
know how large the test set must be in order for sig-
nificance tests to be sensitive to small gains in the
performance metric. Here, we show that test size
plays the largest role in determining discrimination
ability, but that we get diminishing returns. For ex-
ample, doubling the test size will not obviate the
need for significance testing.
In order for our results to be meaningful, we must
have access to the outputs of many of NLP sys-
tems. Public competitions, such as the well-known
CoNLL shared tasks, provide one natural way to ob-
tain a variety of system outputs on the same test
set. However, for most NLP tasks, obtaining out-
puts from a large variety of systems is not feasible.
Thus, in the course of our investigations, we propose
a very simple method for automatically generating
arbitrary numbers of comparable system outputs and
we then validate the trends revealed by our synthetic
method against data from public competitions. This
methodology itself could be of value in, for exam-
ple, the design of new shared tasks.
Finally, we consider a related and perhaps even
more important question that can only be answered
empirically: to what extent is statistical significance
on a test corpus predictive of performance on other
test corpora, in-domain or otherwise? Focusing on
constituency parsing, we investigate the relationship
between significance levels and actual performance
995
on data from outside the test set. We show that when
the test set is (artificially) drawn i.i.d. from the same
distribution that generates new data, then signifi-
cance levels are remarkably well-calibrated. How-
ever, as the domain of the new data diverges from
that of the test set, the predictive ability of signifi-
cance level drops off dramatically.
2 Statistical Significance Testing in NLP
First, we review notation and standard practice in
significance testing to set up our empirical investi-
gation.
2.1 Hypothesis Tests
When comparing a new system A to a baseline sys-
tem B, we want to know if A is better than B on
some large population of data. Imagine that we sam-
ple a small test set x = x1, . . . , xn on which A
beats B by ?(x). Hypothesis testing guards against
the case where A?s victory over B was an unlikely
event, due merely to chance. We would therefore
like to know how likely it would be that a new, in-
dependent test set x? would show a similar victory
for A assuming that A is no better than B on the
population as a whole; this assumption is the null
hypothesis, denoted H0.
Hypothesis testing consists of attempting to esti-
mate this likelihood, written p(?(X) > ?(x)|H0),
where X is a random variable over possible test sets
of size n that we could have drawn, and ?(x) is a
constant, the metric gain we actually observed. Tra-
ditionally, if p(?(X) > ?(x)|H0) < 0.05, we say
that the observed value of ?(x) is sufficiently un-
likely that we should reject H0 (i.e. accept that A?s
victory was real and not just a random fluke). We
refer to p(?(X) > ?(x)|H0) as p-value(x).
In most cases p-value(x) is not easily computable
and must be approximated. The type of approxi-
mation depends on the particular hypothesis testing
method. Various methods have been used in the NLP
community (Gillick and Cox, 1989; Yeh, 2000; Rie-
zler and Maxwell, 2005). We use the paired boot-
strap1 (Efron and Tibshirani, 1993) because it is one
1Riezler and Maxwell (2005) argue the benefits of approx-
imate randomization testing, introduced by Noreen (1989).
However, this method is ill-suited to the type of hypothesis we
are testing. Our null hypothesis does not condition on the test
data, and therefore the bootstrap is a better choice.
1. Draw b bootstrap samples x(i) of size n by
sampling with replacement from x.
2. Initialize s = 0.
3. For each x(i) increment s if ?(x(i)) > 2?(x).
4. Estimate p-value(x) ? sb
Figure 1: The bootstrap procedure. In all of our experiments
we use b = 106, which is more than sufficient for the bootstrap
estimate of p-value(x) to stabilize.
of the most widely used (Och, 2003; Bisani and Ney,
2004; Zhang et al 2004; Koehn, 2004), and be-
cause it can be easily applied to any performance
metric, even complex metrics like F1-measure or
BLEU (Papineni et al 2002). Note that we could
perform the experiments described in this paper us-
ing another method, such as the paired Student?s t-
test. To the extent that the assumptions of the t-test
are met, it is likely that the results would be very
similar to those we present here.
2.2 The Bootstrap
The bootstrap estimates p-value(x) though a com-
bination of simulation and approximation, drawing
many simulated test sets x(i) and counting how often
A sees an accidental advantage of ?(x) or greater.
How can we get sample test sets x(i)? We lack the
ability to actually draw new test sets from the un-
derlying population because all we have is our data
x. The bootstrap therefore draws each x(i) from x
itself, sampling n items from x with replacement;
these new test sets are called bootstrap samples.
Naively, it might seem like we would then check
how often A beats B by more than ?(x) on x(i).
However, there?s something seriously wrong with
these x(i) as far as the null hypothesis is concerned:
the x(i) were sampled from x, and so their average
?(x(i)) won?t be zero like the null hypothesis de-
mands; the average will instead be around ?(x). If
we ask how many of these x(i) have A winning by
?(x), about half of them will. The solution is a re-
centering of the mean ? we want to know how often
A does more than ?(x) better than expected. We ex-
pect it to beat B by ?(x). Therefore, we count up
how many of the x(i) have A beating B by at least
2?(x).2 The pseudocode is shown in Figure 1.
2Note that many authors have used a variant where the event
tallied on the x(i) is whether ?(x(i)) < 0, rather than ?(x(i)) >
2?(x). If the mean of ?(x(i)) is ?(x), and if the distribution of
?(x(i)) is symmetric, then these two versions will be equivalent.
996
As mentioned, a major benefit of the bootstrap is
that any evaluation metric can be used to compute
?(x).3 We run the bootstrap using several metrics:
F1-measure for constituency parsing, unlabeled de-
pendency accuracy for dependency parsing, align-
ment error rate (AER) for word alignment, ROUGE
score (Lin, 2004) for summarization, and BLEU
score for machine translation.4 We report all met-
rics as percentages.
3 Experiments
Our first goal is to explore the relationship be-
tween metric gain, ?(x), and statistical significance,
p-value(x), for a range of NLP tasks. In order to say
anything meaningful, we will need to see both ?(x)
and p-value(x) for many pairs of systems.
3.1 Natural Comparisons
Ideally, for a given task and test set we could obtain
outputs from all systems that have been evaluated
in published work. For each pair of these systems
we could run a comparison and compute both ?(x)
and p-value(x). While obtaining such data is not
generally feasible, for several tasks there are pub-
lic competitions to which systems are submitted by
many researchers. Some of these competitions make
system outputs publicly available. We obtained sys-
tem outputs from the TAC 2008 workshop on auto-
matic summarization (Dang and Owczarzak, 2008),
the CoNLL 2007 shared task on dependency parsing
(Nivre et al 2007), and the WMT 2010 workshop
on machine translation (Callison-Burch et al 2010).
For cases where the metric linearly decomposes over sentences,
the mean of ?(x(i)) is ?(x). By the central limit theorem, the
distribution will be symmetric for large test sets; for small test
sets it may not.
3Note that the bootstrap procedure given only approximates
the true significance level, with multiple sources of approxima-
tion error. One is the error introduced from using a finite num-
ber of bootstrap samples. Another comes from the assumption
that the bootstrap samples reflect the underlying population dis-
tribution. A third is the assumption that the mean bootstrap gain
is the test gain (which could be further corrected for if the metric
is sufficiently ill-behaved).
4To save time, we can compute ?(x) for each bootstrap sam-
ple without having to rerun the evaluation metric. For our met-
rics, sufficient statistics can be recorded for each sentence and
then sampled along with the sentences when constructing each
x(i) (e.g. size of gold, size of guess, and number correct are suf-
ficient for F1). This makes the bootstrap very fast in practice.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
ROUGE
Different research groups
Same research group
Figure 2: TAC 2008 Summarization: Confidence vs.
ROUGE improvement on TAC 2008 test set for comparisons
between all pairs of the 58 participating systems at TAC 2008.
Comparisons between systems entered by the same research
group and comparisons between systems entered by different
research groups are shown separately.
3.1.1 TAC 2008 Summarization
In our first experiment, we use the outputs of the
58 systems that participated in the TAC 2008 work-
shop on automatic summarization. For each possi-
ble pairing, we compute ?(x) and p-value(x) on the
non-update portion of the TAC 2008 test set (we or-
der each pair so that the gain, ?(x), is always pos-
itive).5 For this task, test instances correspond to
document collections. The test set consists of 48
document collections, each with a human produced
summary. Figure 2 plots the ROUGE gain against
1 ? p-value, which we refer to as confidence. Each
point on the graph corresponds to an individual pair
of systems.
As expected, larger gains in ROUGE correspond
to higher confidences. The curved shape of the plot
is interesting. It suggests that relatively quickly we
reach ROUGE gains for which, in practice, signif-
icance tests will most likely be positive. We might
expect that systems whose outputs are highly corre-
lated will achieve higher confidence at lower met-
ric gains. To test this hypothesis, in Figure 2 we
5In order to run bootstraps between all pairs of systems
quickly, we reuse a random sample counts matrix between boot-
strap runs. As a result, we no longer need to perform quadrat-
ically many corpus resamplings. The speed-up from this ap-
proach is enormous, but one undesirable effect is that the boot-
strap estimation noise between different runs is correlated. As a
remedy, we set b so large that the correlated noise is not visible
in plots.
997
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2  2.5  3  3.5  4
1  
-  
p-
va
lue
Unlabeled Acc.
Different research groups
Same research group
Figure 3: CoNLL 2007 Dependency parsing: Confidence vs.
unlabeled dependency accuracy improvement on the Chinese
CoNLL 2007 test set for comparisons between all pairs of the
21 participating systems in CoNLL 2007 shared task. Com-
parisons between systems entered by the same research group
and comparisons between systems entered by different research
groups are shown separately.
separately show the comparisons between systems
entered by the same research group and compar-
isons between systems entered by different research
groups, with the expectation that systems entered by
the same group are likely to have more correlated
outputs. Many of the comparisons between systems
submitted by the same group are offset from the
main curve. It appears that they do achieve higher
confidences at lower metric gains.
Given the huge number of system comparisons in
Figure 2, one obvious question to ask is whether
we can take the results of all these statistical sig-
nificance tests and estimate a ROUGE improvement
threshold that predicts when future statistical sig-
nificance tests will probably be significant at the
p-value(x) < 0.05 level. For example, let?s say we
take all the comparisons with p-value between 0.04
and 0.06 (47 comparisons in all in this case). Each
of these comparisons has an associated metric gain,
and by taking, say, the 95th percentile of these met-
ric gains, we get a potentially useful threshold. In
this case, the computed threshold is 1.10 ROUGE.
What does this threshold mean? Well, based on
the way we computed it, it suggests that if somebody
reports a ROUGE increase of around 1.10 on the ex-
act same test set, there is a pretty good chance that a
statistical significance test would show significance
at the p-value(x) < 0.05 level. After all, 95% of
the borderline significant differences that we?ve al-
ready seen showed an increase of even less than 1.10
ROUGE. If we?re evaluating past work, or are in
some other setting where system outputs just aren?t
available, the threshold could guide our interpreta-
tion of reports containing only summary scores.
That being said, it is important that we don?t over-
interpret the meaning of the 1.10 ROUGE threshold.
We have already seen that pairs of systems submit-
ted by the same research group and by different re-
search groups follow different trends, and we will
soon see more evidence demonstrating the impor-
tance of system correlation in determining the rela-
tionship between metric gain and confidence. Addi-
tionally, in Section 4, we will see that properties of
the test corpus have a large effect on the trend. There
are many factors are at work, and so, of course, met-
ric gain alone will not fully determine the outcome
of a paired significance test.
3.1.2 CoNLL 2007 Dependency Parsing
Next, we run an experiment for dependency pars-
ing. We use the outputs of the 21 systems that par-
ticipated in the CoNLL 2007 shared task on depen-
dency parsing. In Figure 3, we plot, for all pairs,
the gain in unlabeled dependency accuracy against
confidence on the CoNLL 2007 Chinese test set,
which consists of 690 sentences and parses. We
again separate comparisons between systems sub-
mitted by the same research group and those submit-
ted by different groups, although for this task there
were fewer cases of multiple submission. The re-
sults resemble the plot for summarization; we again
see a curve-shaped trend, and comparisons between
systems from the same group (few that they are)
achieve higher confidences at lower metric gains.
3.1.3 WMT 2010 Machine Translation
Our final task for which system outputs are pub-
licly available is machine translation. We run an ex-
periment using the outputs of the 31 systems par-
ticipating in WMT 2010 on the system combination
portion of the German-English WMT 2010 news test
set, which consists of 2,034 German sentences and
English translations. We again run comparisons for
pairs of participating systems. We plot gain in test
BLEU score against confidence in Figure 4. In this
experiment there is an additional class of compar-
998
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
Different research groups
Same research group
System combination
Figure 4: WMT 2010 Machine translation: Confidence vs.
BLEU improvement on the system combination portion of the
German-English WMT 2010 news test set for comparisons be-
tween pairs of the 31 participating systems at WMT 2010.
Comparisons between systems entered by the same research
group, comparisons between systems entered by different re-
search groups, and comparisons between system combination
entries are shown separately.
isons that are likely to have specially correlated sys-
tems: 13 of the submitted systems are system com-
binations, and each take into account the same set
of proposed translations. We separate comparisons
into three sets: comparisons between non-combined
systems entered by different research groups, com-
parisons between non-combined systems entered by
the same research group, and comparisons between
system-combinations.
We see the same curve-shaped trend we saw for
summarization and dependency parsing. Differ-
ent group comparisons, same group comparisons,
and system combination comparisons form distinct
curves. This indicates, again, that comparisons be-
tween systems that are expected to be specially cor-
related achieve high confidence at lower metric gain
levels.
3.2 Synthetic Comparisons
So far, we have seen a clear empirical effect, but, be-
cause of the limited availability of system outputs,
we have only considered a few tasks. We now pro-
pose a simple method that captures the shape of the
effect, and use it to extend our analysis.
3.2.1 Training Set Resampling
Another way of obtaining many different sys-
tems? outputs is to obtain implementations of a
handful of systems, and then vary some aspect of
the training procedure in order to produce many dif-
ferent systems from each implementation. Koehn
(2004) uses this sort of amplification; he uses a sin-
gle machine translation implementation, and then
trains it from different source languages. We take
a slightly different approach. For each task we pick
some fixed training set. Then we generate resampled
training sets by sampling sentences with replace-
ment from the original. In this way, we can gen-
erate as many new training sets as we like, each of
which is similar to the original, but with some vari-
ation. For each base implementation, we train a new
system on each resampled training set. This results
in slightly tweaked trained systems, and is intended
to very roughly approximate the variance introduced
by incremental system changes during research. We
validate this method by comparing plots obtained by
the synthetic approach with plots obtained from nat-
ural comparisons.
We expect that each new system will be differ-
ent, but that systems originating from the same base
model will be highly correlated. This provides a use-
ful division of comparisons: those between systems
built with the same model, and those between sys-
tems built with different models. The first class can
be used to approximate comparisons of systems that
are expected to be specially correlated, and the latter
for comparisons of systems that are not.
3.2.2 Dependency Parsing
We use three base models for dependency parsing:
MST parser (McDonald et al 2005), Maltparser
(Nivre et al 2006), and the ensemble parser of Sur-
deanu and Manning (2010). We use the CoNLL
2007 Chinese training set, which consists of 57K
sentences. We resample 5 training sets of 57K sen-
tences, 10 training sets of 28K sentences, and 10
training sets of 14K sentences. Together, this yields
a total of 75 system outputs on the CoNLL 2007
Chinese test set, 25 systems for each base model
type. The score ranges of all the base models over-
lap. This ensures that for each pair of model types
we will be able to see comparisons where the metric
gains are small. The results of the pairwise compar-
isons of all 75 system outputs are shown in Figure
5, along with the results of the CoNLL 2007 shared
task system comparisons from Figure 3.
999
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2  2.5  3  3.5  4
1  
-  
p-
va
lue
Unlabeled Acc.
Different model types
Same model type
CoNLL 2007 comparisons
Figure 5: Dependency parsing: Confidence vs. unlabeled de-
pendency accuracy improvement on the Chinese CoNLL 2007
test set for comparisons between all pairs of systems gener-
ated by using resampled training sets to train either MST parser,
Maltparser, or the ensemble parser. Comparisons between sys-
tems generated using the same base model type and compar-
isons between systems generated using different base model
types are shown separately. The CoNLL 2007 shared task com-
parisons from Figure 3 are also shown.
The overlay of the natural comparisons suggests
that the synthetic approach reasonably models the
relationship between metric gain and confidence.
Additionally, the different model type and same
model type comparisons exhibit the behavior we
would expect, matching the curves corresponding to
comparisons between specially correlated systems
and standard comparisons respectively.
Since our synthetic approach yields a large num-
ber of system outputs, we can use the procedure
described in Section 3.1.1 to compute the thresh-
old above which the metric gain is probably signifi-
cant. For comparisons between systems of the same
model type, the threshold is 1.20 unlabeled depen-
dency accuracy. For comparisons between systems
of different model types, the threshold is 1.51 un-
labeled dependency accuracy. These results indi-
cate that the similarity of the systems being com-
pared is an important factor. As mentioned, rules-
of-thumb derived from such thresholds cannot be
applied blindly, but, in special cases where two sys-
tems are known to be correlated, the former thresh-
old should be preferred over the latter. For example,
during development most comparisons are made be-
tween incremental variants of the same system. If
adding a feature to a supervised parser increases un-
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
Different model types
Same model type
WMT 2010 comparisons
Figure 6: Machine translation: Confidence vs. BLEU im-
provement on the system combination portion of the German-
English WMT 2010 news test set for comparisons between all
pairs of systems generated by using resampled training sets to
train either Moses or Joshua. Comparisons between systems
generated using the same base model type and comparisons be-
tween systems generated using different base model types are
shown separately. The WMT 2010 workshop comparisons from
Figure 4 are also shown.
labeled accuracy by 1.3, it is useful to be able to
quickly estimate that the improvement is probably
significant. This still isn?t the full story; we will
soon see that properties of the test set al play a
major role. But first, we carry our analysis to sev-
eral more tasks.
3.2.3 Machine Translation
Our two base models for machine translation
are Moses (Koehn et al 2007) and Joshua (Li et
al., 2009). We use 1.4M sentence pairs from the
German-English portion of the WMT-provided Eu-
roparl (Koehn, 2005) and news commentary corpora
as the original training set. We resample 75 training
sets, 20 of 1.4M sentence pairs, 29 of 350K sentence
pairs, and 26 of 88K sentence pairs. This yields a
total of 150 system outputs on the system combi-
nation portion of the German-English WMT 2010
news test set. The results of the pairwise compar-
isons of all 150 system outputs are shown in Figure
6, along with the results of the WMT 2010 workshop
system comparisons from Figure 4.
The natural comparisons from the WMT 2010
workshop align well with the comparisons between
synthetically varied models. Again, the different
model type and same model type comparisons form
distinct curves. For comparisons between systems
1000
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
AER
Different model types
Same model type
Figure 7: Word alignment: Confidence vs. AER improve-
ment on the Hansard test set for comparisons between all pairs
of systems generated by using resampled training sets to train
either the ITG aligner, the joint HMM aligner, or GIZA++.
Comparisons between systems generated using the same base
model type and comparisons between systems generated using
different base model types are shown separately.
of the same model type the computed p-value <
0.05 threshold is 0.28 BLEU. For comparisons be-
tween systems of different model types the threshold
is 0.37 BLEU.
3.2.4 Word Alignment
Now that we have validated our simple model of
system variation on two tasks, we go on to gen-
erate plots for tasks that do not have competitions
with publicly available system outputs. The first
task is English-French word alignment, where we
use three base models: the ITG aligner of Haghighi
et al(2009), the joint HMM aligner of Liang et al
(2006), and GIZA++ (Och and Ney, 2003). The last
two aligners are unsupervised, while the first is su-
pervised. We train the unsupervised word aligners
using the 1.1M sentence pair Hansard training cor-
pus, resampling 20 training sets of the same size.6
Following Haghighi et al(2009), we train the super-
vised ITG aligner using the first 337 sentence pairs
of the hand-aligned Hansard test set; again, we re-
sample 20 training sets of the same size as the origi-
nal data. We test on the remaining 100 hand-aligned
sentence pairs from the Hansard test set.
Unlike previous plots, the points corresponding
to comparisons between systems with different base
6GIZA++ failed to produce reasonable output when trained
with some of these training sets, so there are fewer than 20
GIZA++ systems in our comparisons.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
Different model types
Same model type
Figure 8: Constituency parsing: Confidence vs. F1 improve-
ment on section 23 of the WSJ corpus for comparisons between
all pairs of systems generated by using resampled training sets
to train either the Berkeley parser, the Stanford parser, or the
Collins parser. Comparisons between systems generated us-
ing the same base model type and comparisons between sys-
tems generated using different base model types are shown sep-
arately.
model types form two distinct curves. It turns out
that the upper curve consists only of comparisons
between ITG and HMM aligners. This is likely due
to the fact that the ITG aligner uses posteriors from
the HMM aligner for some of its features, so the
two models are particularly correlated. Overall, the
spread of this plot is larger than previous ones. This
may be due to the small size of the test set, or possi-
bly some additional variance introduced by unsuper-
vised training. For comparisons between systems of
the same model type the p-value < 0.05 threshold
is 0.50 AER. For comparisons between systems of
different model types the threshold is 1.12 AER.
3.2.5 Constituency Parsing
Finally, before we move on to further types of
analysis, we run an experiment for the task of con-
stituency parsing. We use three base models: the
Berkeley parser (Petrov et al 2006), the Stanford
parser (Klein and Manning, 2003), and Dan Bikel?s
implementation (Bikel, 2004) of the Collins parser
(Collins, 1999). We use sections 2-21 of the WSJ
corpus (Marcus et al 1993), which consists of 38K
sentences and parses, as a training set. We resample
10 training sets of size 38K, 10 of size 19K, and 10
of size 9K, and use these to train systems. We test
on section 23. The results are shown in Figure 8.
For comparisons between systems of the same
1001
model type, the p-value < 0.05 threshold is 0.47
F1. For comparisons between systems of different
model types the threshold is 0.57 F1.
4 Properties of the Test Corpus
For five tasks, we have seen a trend relating met-
ric gain and confidence, and we have seen that the
level of correlation between the systems being com-
pared affects the location of the curve. Next, we
look at how the size and domain of the test set play
a role, and, finally, how significance level predicts
performance on held out data. In this section, we
carry out experiments for both machine translation
and constituency parsing, but mainly focus on the
latter because of the availability of large test corpora
that span more than one domain: the Brown corpus
and the held out portions of the WSJ corpus.
4.1 Varying the Size
Figure 9 plots comparisons for machine translation
on variously sized initial segments of the WMT
2010 news test set. Similarly, Figure 10 plots com-
parisons for constituency parsing on initial segments
of the Brown corpus. As might be expected, the
size of the test corpus has a large effect. For both
machine translation and constituency parsing, the
larger the corpus size, the lower the threshold for
p-value < 0.05 and the smaller the spread of the
plot. At one extreme, the entire Brown corpus,
which consists of approximately 24K sentences, has
a threshold of 0.22 F1, while at the other extreme,
the first 100 sentences of the Brown corpus have a
threshold of 3.00 F1. Notice that we see diminishing
returns as we increase the size of the test set. This
phenomenon follows the general shape of the cen-
tral limit theorem, which predicts that variances of
observed metric gains will shrink according to the
square root of the test size. Even using the entire
Brown corpus as a test set there is a small range
where the result of a paired significance test was not
completely determined by metric gain.
It is interesting to note that for a fixed test size,
the domain has only a small effect on the shape of
the curve. Figure 11 plots comparisons for a fixed
test size, but with various test corpora.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
100 sent.
200 sent.
400 sent.
800 sent.
1600 sent.
Figure 9: Machine translation; varying test size: Confidence
vs. BLEU improvement on portions of the German-English
WMT 2010 news test set.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
100 sent.
200 sent.
400 sent.
800 sent.
1600 sent.
3200 sent.
Entire Brown corpus
Figure 10: Constituency parsing; varying test size: Con-
fidence vs. F1 improvement on portions of the Brown corpus.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
WSJ sec 22
WSJ sec 23
WSJ sec 24
Brown corpus
Figure 11: Constituency parsing; varying domain: Confi-
dence vs. F1 improvement on the first 1,600 sentences of sec-
tions 22, 23, and 24 of the WSJ corpus, and the Brown corpus.
1002
4.2 Empirical Calibration across Domains
Now that we have a way of generating outputs for
thousands of pairs of systems, we can check empir-
ically the practical reliability of significance testing.
Recall that the bootstrap p-value(x) is an approxi-
mation to p(?(X) > ?(x)|H0). However, we often
really want to determine the probability that the new
system is better than the baseline on the underlying
test distribution or even the distribution from another
domain. There is no reason a priori to expect these
numbers to coincide.
In our next experiment, we treat the entire Brown
corpus, which consists of 24K sentences, as the true
population of English sentences. For each system
generated in the way described in Section 3.2.5 we
compute F1 on all of Brown. Since we are treat-
ing the Brown corpus as the actual population of En-
glish sentences, for each pair of parsers we can say
that the sign of the F1 difference indicates which is
the truly better system. Now, we repeatedly resam-
ple small test sets from Brown, each consisting of
1,600 sentences, drawn by sampling sentences with
replacement. For each pair of systems, and for each
resampled test set, we compute p-value(x) using the
bootstrap. Out of the 4K bootstraps computed in this
way, 942 had p-value between 0.04 and 0.06, 869
of which agreed with the sign of the F1 difference
we saw on the entire Brown corpus. Thus, 92% of
the significance tests with p-value in a tight range
around 0.05 correctly identified the better system.
This result is encouraging. It suggests that sta-
tistical significance computed using the bootstrap is
reasonably well calibrated. However, test sets are
almost never drawn i.i.d. from the distribution of in-
stances the system will encounter in practical use.
Thus, we also wish to compute how calibration de-
grades as the domain of the test set changes. In an-
other experiment, we look at how significance near
p-value = 0.05 on section 23 of the WSJ corpus
predicts performance on sections 22 and 24 and the
Brown corpus. This time, for each pair of generated
systems we run a bootstrap on section 23. Out of
all these bootstraps, 58 system pairs had p-value be-
tween 0.04 and 0.06. Of these, only 83% had the
same sign of F1 difference on section 23 as they did
on section 22, 71% the had the same sign on sec-
tion 23 as on section 24, and 48% the same sign on
Sec. 23 p-value
% Sys. A > Sys. B
Sec. 22 Sec. 24 Brown
0.00125 - 0.0025 97% 95% 73%
0.0025 - 0.005 92% 92% 60%
0.005 - 0.01 92% 85% 56%
0.01 - 0.02 88% 92% 54%
0.02 - 0.04 87% 78% 51%
0.04 - 0.08 83% 74% 48%
Table 1: Empirical calibration: p-value on section 23 of the
WSJ corpus vs. fraction of comparisons where system A beats
system B on section 22, section 24, and the Brown corpus. Note
that system pairs are ordered so that A always outperforms B on
section 23.
section 23 as on the Brown corpus. This indicates
that reliability degrades as we switch the domain. In
the extreme, achieving a p-value near 0.05 on sec-
tion 23 provides no information about performance
on the Brown corpus.
If we intend to use our system on out-of-domain
data, these results are somewhat discouraging. How
low does p-value(x) have to get before we start get-
ting good information about out-of-domain perfor-
mance? We try to answer this question for this par-
ticular parsing task by running the same domain cal-
ibration experiment for several different ranges of
p-value. The results are shown in Table 1. From
these results, it appears that for constituency pars-
ing, when testing on section 23, a p-value level be-
low 0.00125 is required to reasonably predict perfor-
mance on the Brown corpus.
It should be considered a good practice to include
statistical significance testing results with empiri-
cal evaluations. The bootstrap in particular is easy
to run and makes relatively few assumptions about
the task or evaluation metric. However, we have
demonstrated some limitations of statistical signifi-
cance testing for NLP. In particular, while statistical
significance is usually a minimum necessary condi-
tion to demonstrate that a performance difference is
real, it?s also important to consider the relationship
between test set performance and the actual goals
of the systems being tested, especially if the system
will eventually be used on data from a different do-
main than the test set used for evaluation.
5 Conclusion
We have demonstrated trends relating several im-
portant factors to significance level, which include
1003
both properties of the systems being compared and
properties of the test corpus, and have presented a
simple approach to approximating the response of
these factors for tasks where large numbers of sys-
tem outputs are not available. Our results reveal
that the relationship between metric gain and sta-
tistical significance is complex, and therefore sim-
ple thresholds are not a replacement for significance
tests. Indeed, we strongly advocate the use of statis-
tical significance testing to validate metric gains in
NLP, but also note that informal rules-of-thumb do
arise in popular discussion and that, for some set-
tings when previous systems are unavailable, these
empirical results can supplement less sensitive un-
paired tests (e.g. bar-overlaps-point test) in evalua-
tion of progress. Finally, even formal testing has its
limits. We provide cautionary evidence to this ef-
fect, showing that the information provided by a test
quickly degrades as the target corpus shifts domain.
Acknowledgements
This work was partially supported by NSF fellow-
ships to the first and second authors and by the NSF
under grant 0643742.
References
D.M. Bikel. 2004. Intricacies of collins? parsing model.
Computational Linguistics.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in asr performance evaluation. In
Proc. of ICASSP.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proc. of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
H.T. Dang and K. Owczarzak. 2008. Overview of the
tac 2008 update summarization task. In Proc. of Text
Analysis Conference.
B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman & Hall/CRC.
L. Gillick and S.J. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proc. of ICASSP.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,
S. Khudanpur, L. Schwartz, W.N.G. Thornton,
J. Weese, and O.F. Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proc. of the Fourth Workshop on Statistical Machine
Translation.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proc. of the Workshop on Text
Summarization.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
The penn treebank. Computational linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of EMNLP.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley, New
York.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
1004
S. Riezler and J.T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing for mt.
In Proc. of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
M. Surdeanu and C.D. Manning. 2010. Ensemble mod-
els for dependency parsing: cheap and good? In Proc.
of NAACL.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proc. of ACL.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system. In Proc. of LREC.
1005
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127?135,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Joint Parsing and Alignment with Weakly Synchronized Grammars
David Burkett John Blitzer Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,blitzer,klein}@cs.berkeley.edu
Abstract
Syntactic machine translation systems extract
rules from bilingual, word-aligned, syntacti-
cally parsed text, but current systems for pars-
ing and word alignment are at best cascaded
and at worst totally independent of one an-
other. This work presents a unified joint model
for simultaneous parsing and word alignment.
To flexibly model syntactic divergence, we de-
velop a discriminative log-linear model over
two parse trees and an ITG derivation which
is encouraged but not forced to synchronize
with the parses. Our model gives absolute
improvements of 3.3 F1 for English pars-
ing, 2.1 F1 for Chinese parsing, and 5.5 F1
for word alignment over each task?s indepen-
dent baseline, giving the best reported results
for both Chinese-English word alignment and
joint parsing on the parallel portion of the Chi-
nese treebank. We also show an improvement
of 1.2 BLEU in downstream MT evaluation
over basic HMM alignments.
1 Introduction
Current syntactic machine translation (MT) sys-
tems build synchronous context free grammars from
aligned syntactic fragments (Galley et al, 2004;
Zollmann et al, 2006). Extracting such grammars
requires that bilingual word alignments and mono-
lingual syntactic parses be compatible. Because of
this, much recent work in both word alignment and
parsing has focused on changing aligners to make
use of syntactic information (DeNero and Klein,
2007; May and Knight, 2007; Fossum et al, 2008)
or changing parsers to make use of word align-
ments (Smith and Smith, 2004; Burkett and Klein,
2008; Snyder et al, 2009). In the first case, how-
ever, parsers do not exploit bilingual information.
In the second, word alignment is performed with a
model that does not exploit syntactic information.
This work presents a single, joint model for parsing
and word alignment that allows both pieces to influ-
ence one another simultaneously.
While building a joint model seems intuitive,
there is no easy way to characterize how word align-
ments and syntactic parses should relate to each
other in general. In the ideal situation, each pair
of sentences in a bilingual corpus could be syntacti-
cally parsed using a synchronous context-free gram-
mar. Of course, real translations are almost always
at least partially syntactically divergent. Therefore,
it is unreasonable to expect perfect matches of any
kind between the two sides? syntactic trees, much
less expect that those matches be well explained at
a word level. Indeed, it is sometimes the case that
large pieces of a sentence pair are completely asyn-
chronous and can only be explained monolingually.
Our model exploits synchronization where pos-
sible to perform more accurately on both word
alignment and parsing, but also allows indepen-
dent models to dictate pieces of parse trees and
word alignments when synchronization is impossi-
ble. This notion of ?weak synchronization? is pa-
rameterized and estimated from data to maximize
the likelihood of the correct parses and word align-
ments. Weak synchronization is closely related to
the quasi-synchronous models of Smith and Eis-
ner (2006; 2009) and the bilingual parse reranking
model of Burkett and Klein (2008), but those models
assume that the word alignment of a sentence pair is
known and fixed.
To simultaneously model both parses and align-
127
ments, our model loosely couples three separate
combinatorial structures: monolingual trees in the
source and target languages, and a synchronous ITG
alignment that links the two languages (but is not
constrained to match linguistic syntax). The model
has no hard constraints on how these three struc-
tures must align, but instead contains a set of ?syn-
chronization? features that are used to propagate
influence between the three component grammars.
The presence of synchronization features couples
the parses and alignments, but makes exact inference
in the model intractable; we show how to use a vari-
ational mean field approximation, both for comput-
ing approximate feature expectations during train-
ing, and for performing approximate joint inference
at test time.
We train our joint model on the parallel, gold
word-aligned portion of the Chinese treebank.
When evaluated on parsing and word alignment, this
model significantly improves over independently-
trained baselines: the monolingual parser of Petrov
and Klein (2007) and the discriminative word
aligner of Haghighi et al (2009). It also improves
over the discriminative, bilingual parsing model
of Burkett and Klein (2008), yielding the highest
joint parsing F1 numbers on this data set. Finally,
our model improves word alignment in the context
of translation, leading to a 1.2 BLEU increase over
using HMM word alignments.
2 Joint Parsing and Alignment
Given a source-language sentence, s, and a target-
language sentence, s?, we wish to predict a source
tree t, a target tree t?, and some kind of alignment
a between them. These structures are illustrated in
Figure 1.
To facilitate these predictions, we define a condi-
tional distribution P(t, a, t?|s, s?). We begin with a
generic conditional exponential form:
P(t, a, t?|s, s?) ? exp ?>?(t, a, t?, s, s?) (1)
Unfortunately, a generic model of this form is in-
tractable, because we cannot efficiently sum over
all triples (t, a, t?) without some assumptions about
how the features ?(t, a, t?, s, s?) decompose.
One natural solution is to restrict our candidate
triples to those given by a synchronous context free
grammar (SCFG) (Shieber and Schabes, 1990). Fig-
ure 1(a) gives a simple example of generation from
a log-linearly parameterized synchronous grammar,
together with its features. With the SCFG restric-
tion, we can sum over the necessary structures using
the O(n6) bitext inside-outside algorithm, making
P(t, a, t?|s, s?) relatively efficient to compute expec-
tations under.
Unfortunately, an SCFG requires that all the con-
stituents of each tree, from the root down to the
words, are generated perfectly in tandem. The re-
sulting inability to model any level of syntactic di-
vergence prevents accurate modeling of the individ-
ual monolingual trees. We will consider the run-
ning example from Figure 2 throughout the paper.
Here, for instance, the verb phrase established in
such places as Quanzhou, Zhangzhou, etc. in En-
glish does not correspond to any single node in the
Chinese tree. A synchronous grammar has no choice
but to analyze this sentence incorrectly, either by ig-
noring this verb phrase in English or postulating an
incorrect Chinese constituent that corresponds to it.
Therefore, instead of requiring strict synchroniza-
tion, our model treats the two monolingual trees and
the alignment as separate objects that can vary arbi-
trarily. However, the model rewards synchronization
appropriately when the alignment brings the trees
into correspondence.
3 Weakly Synchronized Grammars
We propose a joint model which still gives probabil-
ities on triples (t, a, t?). However, instead of using
SCFG rules to synchronously enforce the tree con-
straints on t and t?, we only require that each of t
and t? be well-formed under separate monolingual
CFGs.
In order to permit efficient enumeration of all pos-
sible alignments a, we also restrict a to the set of
unlabeled ITG bitrees (Wu, 1997), though again we
do not require that a relate to t or t? in any particular
way. Although this assumption does limit the space
of possible word-level alignments, for the domain
we consider (Chinese-English word alignment), the
reduced space still contains almost all empirically
observed alignments (Haghighi et al, 2009).1 For
1See Section 8.1 for some new terminal productions re-
quired to make this true for the parallel Chinese treebank.
128
NP VP
S
NP
VP
IP
b
0
b
1
b
2
Features
?( (IP, b
0
, S), s, s? )
?( (NP, b
1
, NP), s, s? )
?( (VP, b
2
, VP), s, s? )
NP VP
S
NP
IP
b
0
b
1
b
2
VP
AP
Features
   
   
(IP, s)     
   
(b
0
, s, s?)
   
   
(NP, s)     
   
(b
1
, s, s?)
   
   
(VP, s)     
   
(b
2
, s, s?)
   
   
(S, s?)
      (IP, b
0
)
   
   
(NP, s?)
      (b
0
, S)
   
   
(AP, s?)
      (b
1
, NP)
   
   
(VP, s?)
      (IP, b
0
, S)
Parsing
Alignment
Synchronization
?E
?E
?E
?E
?F
?F
?F
?A
?A
?A
?!
?!
?!"
?!
(a) Synchronous Rule (b) Asynchronous Rule
Figure 1: Source trees, t (right), alignments, a (grid), and target trees, t? (top), and feature decompositions for syn-
chronous (a) and weakly synchronous (b) grammars. Features always condition on bispans and/or anchored syntactic
productions, but weakly synchronous grammars permit more general decompositions.
example, in Figure 2, the word alignment is ITG-
derivable, and each of the colored rectangles is a bi-
span in that derivation.
There are no additional constraints beyond the
independent, internal structural constraints on t, a,
and t?. This decoupling permits derivations like that
in Figure 1(b), where the top-level syntactic nodes
align, but their children are allowed to diverge. With
the three structures separated, our first model is a
completely factored decomposition of (1).
Formally, we represent a source tree t as a set of
nodes {n}, each node representing a labeled span.
Likewise, a target tree t? is a set of nodes {n?}.2 We
represent alignments a as sets of bispans {b}, indi-
cated by rectangles in Figure 1.3 Using this notation,
the initial model has the following form:
P(t, a, t?|s, s?) ? exp
?
?
?
n?t
?>?F (n, s)+
?
b?a
?>?A(b, s, s?)+
?
n??t?
?>?E(n?, s?)
?
?
(2)
Here ?F (n, s) indicates a vector of source node fea-
tures, ?E(n?, s?) is a vector of target node features,
and ?A(b, s, s?) is a vector of alignment bispan fea-
tures. Of course, this model is completely asyn-
2For expositional clarity, we describe n and n? as labeled
spans only. However, in general, features that depend on n or
n? are permitted to depend on the entire rule, and do in our final
system.
3Alignments a link arbitrary spans of s and s? (including
non-constituents and individual words). We discuss the relation
to word-level alignments in Section 4.
chronous so far, and fails to couple the trees and
alignments at all. To permit soft constraints between
the three structures we are modeling, we add a set of
synchronization features.
For n ? t and b ? a, we say that n b if n and b
both map onto the same span of s. We define b n?
analogously for n? ? t?. We now consider three
different types of synchronization features. Source-
alignment synchronization features ?(n, b) are ex-
tracted whenever n  b. Similarly, target-alignment
features ?(b, n?) are extracted if b  n?. These
features capture phenomena like that of bispan b7
in Figure 2. Here the Chinese noun? synchronizes
with the ITG derivation, but the English projection
of b7 is a distituent. Finally, we extract source-target
features ?./(n, b, n?) whenever nbn?. These fea-
tures capture complete bispan synchrony (as in bi-
span b8) and can be expressed over triples (n, b, n?)
which happen to align, allowing us to reward syn-
chrony, but not requiring it. All of these licensing
conditions are illustrated in Figure 1(b).
With these features added, the final form of the
model is:
P(t, a, t?|s, s?) ? exp
?
?
?
n?t
?>?F (n, s)+
?
b?a
?>?A(b, s, s?)+
?
n??t?
?>?E(n?, s?)+
?
nb
?>?(n, b)+
?
bn?
?>?(b, n?)+
?
nbn?
?>?./(n, b, n?)
?
?
(3)
129
We emphasize that because of the synchronization
features, this final form does not admit any known
efficient dynamic programming for the exact com-
putation of expectations. We will therefore turn to a
variational inference method in Section 6.
4 Features
With the model?s locality structure defined, we
just need to specify the actual feature function,
?. We divide the features into three types: pars-
ing features (?F (n, s) and ?E(n?, s?)), alignment
features (?A(b, s, s?)) and synchronization features
(?(n, b), ?(b, n?), and ?./(n, b, n?)). We detail
each of these in turn here.
4.1 Parsing
The monolingual parsing features we use are sim-
ply parsing model scores under the parser of Petrov
and Klein (2007). While that parser uses heavily re-
fined PCFGs with rule probabilities defined at the
refined symbol level, we interact with its posterior
distribution via posterior marginal probabilities over
unrefined symbols. In particular, to each unrefined
anchored production iAj ? iBkCj , we associate a
single feature whose value is the marginal quantity
log P(iBkCj |iAj , s) under the monolingual parser.
These scores are the same as the variational rule
scores of Matsuzaki et al (2005).4
4.2 Alignment
We begin with the same set of alignment features
as Haghighi et al (2009), which are defined only for
terminal bispans. In addition, we include features on
nonterminal bispans, including a bias feature, fea-
tures that measure the difference in size between
the source and target spans, features that measure
the difference in relative sentence position between
the source and target spans, and features that mea-
sure the density of word-to-word alignment poste-
riors under a separate unsupervised word alignment
model.
4Of course the structure of our model permits any of the
additional rule-factored monolingual parsing features that have
been described in the parsing literature, but in the present work
we focus on the contributions of joint modeling.
4.3 Synchronization
Our synchronization features are indicators for the
syntactic types of the participating nodes. We de-
termine types at both a coarse (more collapsed
than Treebank symbols) and fine (Treebank sym-
bol) level. At the coarse level, we distinguish be-
tween phrasal nodes (e.g. S, NP), synthetic nodes
introduced in the process of binarizing the grammar
(e.g. S?, NP?), and part-of-speech nodes (e.g. NN,
VBZ). At the fine level, we distinguish all nodes
by their exact label. We use coarse and fine types
for both partially synchronized (source-alignment or
target-alignment) features and completely synchro-
nized (source-alignment-target) features. The inset
of Figure 2 shows some sample features. Of course,
we could devise even more sophisticated features by
using the input text itself. As we shall see, however,
our model gives significant improvements with these
simple features alone.
5 Learning
We learn the parameters of our model on the paral-
lel portion of the Chinese treebank. Although our
model assigns probabilities to entire synchronous
derivations of sentences, the parallel Chinese tree-
bank gives alignments only at the word level (1 by
1 bispans in Figure 2). This means that our align-
ment variable a is not fully observed. Because of
this, given a particular word alignment w, we max-
imize the marginal probability of the set of deriva-
tions A(w) that are consistent with w (Haghighi et
al., 2009).5
L(?)=log
?
a?A(wi)
P(ti, a, t?i|si, s?i)
We maximize this objective using standard gradient
methods (Nocedal and Wright, 1999). As with fully
visible log-linear models, the gradient for the ith sen-
tence pair with respect to ? is a difference of feature
expectations:
?L(?) =EP(a|ti,wi,t?i,si,s?i)
[
?(ti, a, t?i, si, s?i)
]
? EP(t,a,t?|si,s?i)
[
?(t, a, t?, si, s?i)
] (4)
5We also learn from non-ITG alignments by maximizing the
marginal probability of the set of minimum-recall error align-
ments in the same way as Haghighi et al (2009)
130
NP
NP
IN
PP
NPIN
PPVBN
VPVBD
VPNP
S
JJ NNS
...
were established in such places as Quanzhou Zhangzhou etc.
?
??
??
?
?
??
?
...
NP
P
NN
NP
PP
VP
VV
AS
NP
VP
b
8
b
7
b
4
Sample Synchronization Features
NP, b8,NP
NN, b7
?!"( ) = CoarseSourceTarget?phrasal, phrasal? : 1
FineSourceTarget?NP,NP? : 1
?!( ) = CoarseSourceAlign?pos? : 1
FineSourceAlign?NN? : 1
Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimal
ITG derivation, including one totally unsynchronized bispan (b4), one partially synchronized bispan (b7), and and fully
synchronized bispan (b8). The inset provides some examples of active synchronization features (see Section 4.3) on
these bispans. On this example, the monolingual English parser erroneously attached the lower PP to the VP headed by
established, and the non-syntactic ITG word aligner misaligned? to such instead of to etc. Our joint model corrected
both of these mistakes because it was rewarded for the synchronization of the two NPs joined by b8.
We cannot efficiently compute the model expecta-
tions in this equation exactly. Therefore we turn next
to an approximate inference method.
6 Mean Field Inference
Instead of computing the model expectations from
(4), we compute the expectations for each sentence
pair with respect to a simpler, fully factored distri-
bution Q(t, a, t?) = q(t)q(a)q(t?). Rewriting Q in
log-linear form, we have:
Q(t, a, t?) ? exp
?
?
?
n?t
?n +
?
b?a
?b +
?
n??t?
?n?
?
?
Here, the ?n, ?b and ?n? are variational parameters
which we set to best approximate our weakly syn-
chronized model from (3):
?? = argmin
?
KL
(
Q?||P?(t, a, t?|s, s?)
)
Once we have found Q, we compute an approximate
gradient by replacing the model expectations with
expectations under Q:
EQ(a|wi)
[
?(ti, a, t?i, si, s?i)
]
? EQ(t,a,t?|si,s?i)
[
?(t, a, t?, si, s?i)
]
Now, we will briefly describe how we compute Q.
First, note that the parameters ? of Q factor along
individual source nodes, target nodes, and bispans.
The combination of the KL objective and our par-
ticular factored form of Q make our inference pro-
cedure a structured mean field algorithm (Saul and
Jordan, 1996). Structured mean field techniques are
well-studied in graphical models, and our adaptation
in this section to multiple grammars follows stan-
dard techniques (see e.g. Wainwright and Jordan,
2008).
Rather than derive the mean field updates for ?,
we describe the algorithm (shown in Figure 3) pro-
cedurally. Similar to block Gibbs sampling, we it-
eratively optimize each component (source parse,
target parse, and alignment) of the model in turn,
conditioned on the others. Where block Gibbs sam-
pling conditions on fixed trees or ITG derivations,
our mean field algorithm maintains uncertainty in
131
Input: sentence pair (s, s?)
parameter vector ?
Output: variational parameters ?
1. Initialize
?0n ? ?
>?F (n, s)
?0b??
>?A(b, s, s?)
?0n???
>?E(n?, s?)
?0n ?
?
t q?0(t)I(n ? t), etc for ?
0
b , ?
0
n?
2. While not converged, for each n, n?, b in
the monolingual and ITG charts
?in ? ?
>
(
?F (n, s) +
?
b,nb ?
i?1
b ?(n, b)+
?
b,nb
?
n?,bn? ?
i?1
b ?
i?1
n? ?./(n, b, n
?)
)
?in ?
?
t q?(t)I(n ? t) (inside-outside)
?ib ? ?
>
(
?A(b, s, s?) +
?
n,nb ?
i?1
n ?(n, b)+
?
n?,bn? ?
i?1
n? ?(b, n
?)+
?
n,nb
?
n?,bn? ?
i?1
n ?
i?1
n? ?./(n, b, n
?)
)
?b ?
?
a q?(a)I(b ? a) (bitext inside-outside)
updates for ?in? , ?
i
n? analogous to ?
i
n, ?
i
n
3. Return variational parameters ?
Figure 3: Structured mean field inference for the weakly
synchronized model. I(n ? t) is an indicator value for
the presence of node n in source tree t.
the form of monolingual parse forests or ITG forests.
The key components to this uncertainty are the
expected counts of particular source nodes, target
nodes, and bispans under the mean field distribution:
?n =
?
t
q?(t)I(n ? t)
?n? =
?
t?
q?(t?)I(n? ? t?)
?b =
?
a
q?(a)I(b ? a)
Since dynamic programs exist for summing over
each of the individual factors, these expectations can
be computed in polynomial time.
6.1 Pruning
Although we can approximate the expectations from
(4) in polynomial time using our mean field distribu-
tion, in practice we must still prune the ITG forests
and monolingual parse forests to allow tractable in-
ference. We prune our ITG forests using the same
basic idea as Haghighi et al (2009), but we em-
ploy a technique that allows us to be more aggres-
sive. Where Haghighi et al (2009) pruned bispans
based on how many unsupervised HMM alignments
were violated, we first train a maximum-matching
word aligner (Taskar et al, 2005) using our super-
vised data set, which has only half the precision er-
rors of the unsupervised HMM. We then prune ev-
ery bispan which violates at least three alignments
from the maximum-matching aligner. When com-
pared to pruning the bitext forest of our model with
Haghighi et al (2009)?s HMM technique, this new
technique allows us to maintain the same level of ac-
curacy while cutting the number of bispans in half.
In addition to pruning the bitext forests, we also
prune the syntactic parse forests using the mono-
lingual parsing model scores. For each unrefined
anchored production iAj ? iBkCj , we com-
pute the marginal probability P(iAj ,i Bk,k Cj |s) un-
der the monolingual parser (these are equivalent to
the maxrule scores from Petrov and Klein 2007). We
only include productions where this probability is
greater than 10?20. Note that at training time, we are
not guaranteed that the gold trees will be included
in the pruned forest. Because of this, we replace the
gold trees ti, t?i with oracle trees from the pruned for-
est, which can be found efficiently using a variant of
the inside algorithm (Huang, 2008).
7 Testing
Once the model has been trained, we still need to
determine how to use it to predict parses and word
alignments for our test sentence pairs. Ideally, given
the sentence pair (s, s?), we would find:
(t?, w?, t??) = argmax
t,w,t?
P(t, w, t?|s, s?)
= argmax
t,w,t?
?
a?A(w)
P(t, a, t?|s, s?)
Of course, this is also intractable, so we once again
resort to our mean field approximation. This yields
the approximate solution:
(t?, w?, t??) = argmax
t,w,t?
?
a?A(w)
Q(t, a, t?)
However, recall that Q incorporates the model?s mu-
tual constraint into the variational parameters, which
132
factor into q(t), q(a), and q(t?). This allows us to
simplify further, and find the maximum a posteriori
assignments under the variational distribution. The
trees can be found quickly using the Viterbi inside
algorithm on their respective qs. However, the sum
for computing w? under q is still intractable.
As we cannot find the maximum probability word
alignment, we provide two alternative approaches
for finding w?. The first is to just find the Viterbi
ITG derivation a? = argmaxa q(a) and then set w?
to contain exactly the 1x1 bispans in a?. The second
method, posterior thresholding, is to compute poste-
rior marginal probabilities under q for each 1x1 cell
beginning at position i, j in the word alignment grid:
m(i, j) =
?
a
q(a)I((i, i+ 1, j, j + 1) ? a)
We then include w(i, j) in w? if m(w(i, j)) > ? ,
where ? is a threshold chosen to trade off precision
and recall. For our experiments, we found that the
Viterbi alignment was uniformly worse than poste-
rior thresholding. All the results from the next sec-
tion use the threshold ? = 0.25.
8 Experiments
We trained and tested our model on the translated
portion of the Chinese treebank (Bies et al, 2007),
which includes hand annotated Chinese and English
parses and word alignments. We separated the data
into three sets: train, dev, and test, according to the
standard Chinese treebank split. To speed up train-
ing, we only used training sentences of length ? 50
words, which left us with 1974 of 2261 sentences.
We measured the results in two ways. First, we
directly measured F1 for English parsing, Chinese
parsing, and word alignment on a held out section of
the hand annotated corpus used to train the model.
Next, we further evaluated the quality of the word
alignments produced by our model by using them as
input for a machine translation system.
8.1 Dataset-specific ITG Terminals
The Chinese treebank gold word alignments include
significantly more many-to-many word alignments
than those used by Haghighi et al (2009). We are
able to produce some of these many-to-many align-
ments by including new many-to-many terminals in
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(a) 2x2
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(b) 2x3
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(c) Gapped 2x3
Figure 4: Examples of phrasal alignments that can be rep-
resented by our new ITG terminal bispans.
our ITG word aligner, as shown in Figure 4. Our
terminal productions sometimes capture non-literal
translation like both sides or in recent years. They
also can allow us to capture particular, systematic
changes in the annotation standard. For example,
the gapped pattern from Figure 4 captures the stan-
dard that English word the is always aligned to the
Chinese head noun in a noun phrase. We featurize
these non-terminals with features similar to those
of Haghighi et al (2009), and all of the alignment
results we report in Section 8.2 (both joint and ITG)
employ these features.
8.2 Parsing and Word Alignment
To compute features that depend on external models,
we needed to train an unsupervised word aligner and
monolingual English and Chinese parsers. The un-
supervised word aligner was a pair of jointly trained
HMMs (Liang et al, 2006), trained on the FBIS cor-
pus. We used the Berkeley Parser (Petrov and Klein,
2007) for both monolingual parsers, with the Chi-
nese parser trained on the full Chinese treebank, and
the English parser trained on a concatenation of the
Penn WSJ corpus (Marcus et al, 1993) and the En-
glish side of train.6
We compare our parsing results to the mono-
lingual parsing models and to the English-Chinese
bilingual reranker of Burkett and Klein (2008),
trained on the same dataset. The results are in
Table 1. For word alignment, we compare to
6To avoid overlap in the data used to train the monolingual
parsers and the joint model, at training time, we used a separate
version of the Chinese parser, trained only on articles 400-1151
(omitting articles in train). For English parsing, we deemed it
insufficient to entirely omit the Chinese treebank data from the
monolingual parser?s training set, as otherwise the monolingual
parser would be trained entirely on out-of-domain data. There-
fore, at training time we used two separate English parsers: to
compute model scores for the first half of train, we used a parser
trained on a concatenation of the WSJ corpus and the second
half of train, and vice versa for the remaining sentences.
133
Test Results
Ch F1 Eng F1 Tot F1
Monolingual 83.6 81.2 82.5
Reranker 86.0 83.8 84.9
Joint 85.7 84.5 85.1
Table 1: Parsing results. Our joint model has the highest
reported F1 for English-Chinese bilingual parsing.
Test Results
Precision Recall AER F1
HMM 86.0 58.4 30.0 69.5
ITG 86.8 73.4 20.2 79.5
Joint 85.5 84.6 14.9 85.0
Table 2: Word alignment results. Our joint model has the
highest reported F1 for English-Chinese word alignment.
the baseline unsupervised HMM word aligner and
to the English-Chinese ITG-based word aligner
of Haghighi et al (2009). The results are in Table 2.
As can be seen, our model makes substantial im-
provements over the independent models. For pars-
ing, we improve absolute F1 over the monolingual
parsers by 2.1 in Chinese, and by 3.3 in English.
For word alignment, we improve absolute F1 by 5.5
over the non-syntactic ITG word aligner. In addi-
tion, our English parsing results are better than those
of the Burkett and Klein (2008) bilingual reranker,
the current top-performing English-Chinese bilin-
gual parser, despite ours using a much simpler set
of synchronization features.
8.3 Machine Translation
We further tested our alignments by using them to
train the Joshua machine translation system (Li and
Khudanpur, 2008). Table 3 describes the results of
our experiments. For all of the systems, we tuned
Rules Tune Test
HMM 1.1M 29.0 29.4
ITG 1.5M 29.9 30.4?
Joint 1.5M 29.6 30.6
Table 3: Tune and test BLEU results for machine transla-
tion systems built with different alignment tools. ? indi-
cates a statistically significant difference between a sys-
tem?s test performance and the one above it.
on 1000 sentences of the NIST 2004 and 2005 ma-
chine translation evaluations, and tested on 400 sen-
tences of the NIST 2006 MT evaluation. Our train-
ing set consisted of 250k sentences of newswire dis-
tributed with the GALE project, all of which were
sub-sampled to have high Ngram overlap with the
tune and test sets. All of our sentences were of
length at most 40 words. When building the trans-
lation grammars, we used Joshua?s default ?tight?
phrase extraction option. We ran MERT for 4 itera-
tions, optimizing 20 weight vectors per iteration on
a 200-best list.
Table 3 gives the results. On the test set, we also
ran the approximate randomization test suggested by
Riezler and Maxwell (2005). We found that our joint
parsing and alignment system significantly outper-
formed the HMM aligner, but the improvement over
the ITG aligner was not statistically significant.
9 Conclusion
The quality of statistical machine translation mod-
els depends crucially on the quality of word align-
ments and syntactic parses for the bilingual training
corpus. Our work presented the first joint model
for parsing and alignment, demonstrating that we
can improve results on both of these tasks, as well
as on downstream machine translation, by allowing
parsers and word aligners to simultaneously inform
one another. Crucial to this improved performance
is a notion of weak synchronization, which allows
our model to learn when pieces of a grammar are
synchronized and when they are not. Although ex-
act inference in the weakly synchronized model is
intractable, we developed a mean field approximate
inference scheme based on monolingual and bitext
parsing, allowing for efficient inference.
Acknowledgements
We thank Adam Pauls and John DeNero for their
help in running machine translation experiments.
We also thank the three anonymous reviewers for
their helpful comments on an earlier draft of this
paper. This project is funded in part by NSF
grants 0915265 and 0643742, an NSF graduate re-
search fellowship, the CIA under grant HM1582-09-
1-0021, and BBN under DARPA contract HR0011-
06-C-0022.
134
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuki Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Jon May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In EMNLP.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Workshop on Intrinsic and Extrinsic Eval-
uation Methods for MT and Summarization, ACL.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In ACL.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax aug-
mented machine translation system for IWSLT-06. In
IWSLT.
135
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29?38,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Fast Inference in Phrase Extraction Models with Belief Propagation
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
Modeling overlapping phrases in an align-
ment model can improve alignment quality
but comes with a high inference cost. For
example, the model of DeNero and Klein
(2010) uses an ITG constraint and beam-based
Viterbi decoding for tractability, but is still
slow. We first show that their model can be
approximated using structured belief propaga-
tion, with a gain in alignment quality stem-
ming from the use of marginals in decoding.
We then consider a more flexible, non-ITG
matching constraint which is less efficient for
exact inference but more efficient for BP. With
this new constraint, we achieve a relative error
reduction of 40% in F5 and a 5.5x speed-up.
1 Introduction
Modern statistical machine translation (MT) sys-
tems most commonly infer their transfer rules from
word-level alignments (Koehn et al, 2007; Li and
Khudanpur, 2008; Galley et al, 2004), typically
using a deterministic heuristic to convert these to
phrase alignments (Koehn et al, 2003). There have
been many attempts over the last decade to develop
model-based approaches to the phrase alignment
problem (Marcu and Wong, 2002; Birch et al, 2006;
DeNero et al, 2008; Blunsom et al, 2009). How-
ever, most of these have met with limited success
compared to the simpler heuristic method. One key
problem with typical models of phrase alignment
is that they choose a single (latent) segmentation,
giving rise to undesirable modeling biases (DeNero
et al, 2006) and reducing coverage, which in turn
reduces translation quality (DeNeefe et al, 2007;
DeNero et al, 2008). On the other hand, the extrac-
tion heuristic identifies many overlapping options,
and achieves high coverage.
In response to these effects, the recent phrase
alignment work of DeNero and Klein (2010) mod-
els extraction sets: collections of overlapping phrase
pairs that are consistent with an underlying word
alignment. Their extraction set model is empirically
very accurate. However, the ability to model over-
lapping ? and therefore non-local ? features comes
at a high computational cost. DeNero and Klein
(2010) handle this in part by imposing a structural
ITG constraint (Wu, 1997) on the underlying word
alignments. This permits a polynomial-time algo-
rithm, but it is still O(n6), with a large constant
factor once the state space is appropriately enriched
to capture overlap. Therefore, they use a heavily
beamed Viterbi search procedure to find a reason-
able alignment within an acceptable time frame. In
this paper, we show how to use belief propagation
(BP) to improve on the model?s ITG-based struc-
tural formulation, resulting in a new model that is
simultaneously faster and more accurate.
First, given the model of DeNero and Klein
(2010), we decompose it into factors that admit
an efficient BP approximation. BP is an inference
technique that can be used to efficiently approxi-
mate posterior marginals on variables in a graphical
model; here the marginals of interest are the phrase
pair posteriors. BP has only recently come into use
in the NLP community, but it has been shown to be
effective in other complex structured classification
tasks, such as dependency parsing (Smith and Eis-
ner, 2008). There has also been some prior success
in using BP for both discriminative (Niehues and
Vogel, 2008) and generative (Cromie`res and Kuro-
hashi, 2009) word alignment models.
By aligning all phrase pairs whose posterior under
BP exceeds some fixed threshold, our BP approxi-
mation of the model of DeNero and Klein (2010) can
29
achieve a comparable phrase pair F1. Furthermore,
because we have posterior marginals rather than a
single Viterbi derivation, we can explicitly force the
aligner to choose denser extraction sets simply by
lowering the marginal threshold. Therefore, we also
show substantial improvements over DeNero and
Klein (2010) in recall-heavy objectives, such as F5.
More importantly, we also show how the BP fac-
torization allows us to relax the ITG constraint, re-
placing it with a new set of constraints that per-
mit a wider family of alignments. Compared to
ITG, the resulting model is less efficient for exact
inference (where it is exponential), but more effi-
cient for our BP approximation (where it is only
quadratic). Our new model performs even better
than the ITG-constrained model on phrase align-
ment metrics while being faster by a factor of 5.5x.
2 Extraction Set Models
Figure 1 shows part of an aligned sentence pair, in-
cluding the word-to-word alignments, and the ex-
tracted phrase pairs licensed by those alignments.
Formally, given a sentence pair (e, f), a word-level
alignment a is a collection of links between target
words ei and source words fj . Following past work,
we further divide word links into two categories:
sure and possible, shown in Figure 1 as solid and
hatched grey squares, respectively. We represent a
as a grid of ternary word link variables aij , each of
which can take the value sure to represent a sure link
between ei and fj , poss to represent a possible link,
or off to represent no link.
An extraction set pi is a set of aligned phrase pairs
to be extracted from (e, f), shown in Figure 1 as
green rounded rectangles. We represent pi as a set of
boolean variables pighk`, which each have the value
true when the target span [g, h] is phrase-aligned to
the source span [k, `]. Following previous work on
phrase extraction, we limit the size of pi by imposing
a phrase length limit d: pi only contains a variable
pighk` if h? g < d and `? k < d.
There is a deterministic mapping pi(a) from a
word alignment to the extraction set licensed by that
word alignment. We will briefly describe it here, and
then present our factorized model.
e3 e4 e5 e6 e7
f5
f6
f7
f8
f9
?f5 = [7, 7]
?f6 = [5, 6]
?f7 = [5, 6]
?f8 = [4, 4]
?f9 = [?1,?]
Figure 1: A schematic representation of part of a sen-
tence pair. Solid grey squares indicate sure links (e.g.
a48 = sure), and hatched squares possible links (e.g.
a67 = poss). Rounded green rectangles are extracted
phrase pairs (e.g. pi5667 = true). Target spans are shown
as blue vertical lines and source spans as red horizontal
lines. Because there is a sure link at a48, ?f8 = [4, 4] does
not include the possible link at a38. However, f7 only
has possible links, so ?f7 = [5, 6] is the span containing
those. f9 is null-aligned, so ?f9 = [?1,?], which blocks
all phrase pairs containing f9 from being extracted.
2.1 Extraction Sets from Word Alignments
The mapping from a word alignment to the set of
licensed phrase pairs pi(a) is based on the standard
rule extraction procedures used in most modern sta-
tistical systems (Koehn et al, 2003; Galley et al,
2006; Chiang, 2007), but extended to handle pos-
sible links (DeNero and Klein, 2010). We start by
using a to find a projection from each target word ei
onto a source span, represented as blue vertical lines
in Figure 1. Similarly, source words project onto
target spans (red horizontal lines in Figure 1). pi(a)
contains a phrase pair iff every word in the target
span projects within the source span and vice versa.
Figure 1 contains an example for d = 2.
Formally, the mapping introduces a set of spans
?. We represent the spans as variables whose values
are intervals, where ?ei = [k, `] means that the tar-
get word ei projects to the source span [k, `]. The
set of legal values for ?ei includes any interval with
0 ? k ? ` < |f| and ` ? k < d, plus the special in-
terval [?1,?] that indicates ei is null-aligned. The
span variables for source words ?fj have target spans
[g, h] as values and are defined analogously.
For a set I of positions, we define the range func-
30
tion:
range(I) =
{
[?1,?] I = ?
[mini?I i,maxi?I i] else
(1)
For a fixed word alignment a we set the target
span variable ?ei :
?ei,s = range({j : aij = sure}) (2)
?ei,p = range({j : aij 6= off}) (3)
?ei = ?ei,s ? ?ei,p (4)
As illustrated in Figure 1, this sets ?ei to the min-
imal span containing all the source words with a
sure link to ei if there are any. Otherwise, because
of the special case for range(I) when I is empty,
?ei,s = [?1,?], so ?ei is the minimal span containing
all poss-aligned words. If all word links to ei are off,
indicating that ei is null-aligned, then ?ei is [?1,?],
preventing the alignment of any phrase pairs con-
taining ei.
Finally, we specify which phrase pairs should be
included in the extraction set pi. Given the spans ?
based on a, pi(a) sets pighk` = true iff every word in
each phrasal span projects within the other:
?ei ? [k, `] ?i ? [g, h] (5)
?fj ? [g, h] ?j ? [k, `]
2.2 Formulation as a Graphical Model
We score triples (a, pi, ?) as the dot product of a
weight vector w that parameterizes our model and a
feature vector ?(a, pi, ?). The feature vector decom-
poses into word alignment features ?a, phrase pair
features ?pi and target and source null word features
?e? and ?
f
? :
1
?(a, pi, ?) =
?
i,j
?a(aij) +
?
g,h,k,`
?pi(pighk`)+
?
i
?e?(?ei ) +
?
j
?f?(?
f
j ) (6)
This feature function is exactly the same as that
used by DeNero and Klein (2010).2 However, while
1In addition to the arguments we write out explicitly, all fea-
ture functions have access to the observed sentence pair (e, f).
2Although the null word features are not described in DeN-
ero and Klein (2010), all of their reported results include these
features (DeNero, 2010).
they formulated their inference problem as a search
for the highest scoring triple (a, pi, ?) for an ob-
served sentence pair (e, f), we wish to derive a con-
ditional probability distribution p(a, pi, ?|e, f). We
do this with the standard transformation for linear
models: p(a, pi, ?|e, f) ? exp(w??(a, pi, ?)). Due to
the factorization in Eq. (6), this exponentiated form
becomes a product of local multiplicative factors,
and hence our model forms an undirected graphical
model, or Markov random field.
In addition to the scoring function, our model
also includes constraints on which triples (a, pi, ?)
have nonzero probability. DeNero and Klein (2010)
implicitly included these constraints in their repre-
sentation: instead of sets of variables, they used a
structured representation that only encodes triples
(a, pi, ?) satisfying both the mapping pi = pi(a) and
the structural constraint that a can be generated by
a block ITG grammar. However, our inference pro-
cedure, BP, requires that we represent (a, pi, ?) as an
assignment of values to a set of variables. Therefore,
we must explicitly encode all constraints into the
multiplicative factors that define the model. To ac-
complish this, in addition to the soft scoring factors
we have already mentioned, our model also includes
a set of hard constraint factors. Hard constraint fac-
tors enforce the relationships between the variables
of the model by taking a value of 0 when the con-
straints they encode are violated and a value of 1
when they are satisfied. The full factor graph rep-
resentation of our model, including both soft scor-
ing factors and hard constraint factors, is drawn
schematically in Figure 2.
2.2.1 Soft Scoring Factors
The scoring factors all take the form exp(w ? ?),
and so can be described in terms of their respective
local feature vectors, ?. Depending on the values of
the variables each factor depends on, the factor can
be active or inactive. Features are only extracted for
active factors; otherwise ? is empty and the factor
produces a value of 1.
SURELINK. Each word alignment variable aij
has a corresponding SURELINK factor Lij to incor-
porate scores from the features ?a(aij). Lij is ac-
tive whenever aij = sure. ?a(aij) includes poste-
riors from unsupervised jointly trained HMM word
alignment models (Liang et al, 2006), dictionary
31
a11 a21
Lij
L11 L21
a12
ai1
Li1
L12 L22
a22
a1j aij
L1j
A
(a) ITG factor
agk ahk
ag? ah?
ag|f| ah|f|
a|e|k
a|e|?
Pghk?
Rghk?
?ghk?
Seg Seh
NehNeg
?eg ?eh
Sfk
Sf?
Nf?
Nfk
?fk
?f?
(b) SPAN and EXTRACT factors
Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph
separated into two components: one containing the factors that only neighbor word link variables, and one containing
the remaining factors.
and identical word features, a position distortion fea-
ture, and features for numbers and punctuation.
PHRASEPAIR. For each phrase pair variable
pighk`, scores from ?pi(pighk`) come from the factor
Rghk`, which is active if pighk` = true. Most of the
model?s features are on these factors, and include
relative frequency statistics, lexical template indica-
tor features, and indicators for numbers of words and
Chinese characters. See DeNero and Klein (2010)
for a more comprehensive list.
NULLWORD. We can determine if a word is
null-aligned by looking at its corresponding span
variable. Thus, we include features from ?e?(?ei ) in
a factor N ei that is active if ?ei = [?1,?]. The
features are mostly indicators for common words.
There are also factors Nfj for source words, which
are defined analogously.
2.2.2 Hard Constraint Factors
We encode the hard constraints on relationships
between variables in our model using three fami-
lies of factors, shown graphically in Figure 2. The
SPAN and EXTRACT factors together ensure that
pi = pi(a). The ITG factor encodes the structural
constraint on a.
SPAN. First, for each target word ei we include
a factor Sei to ensure that the span variable ?ei has
a value that agrees with the projection of the word
alignment a. As shown in Figure 2b, Sei depends
on ?ei and all the word alignment variables aij in
column i of the word alignment grid. Sei has value
1 iff the equality in Eq. (4) holds. Our model also
includes a factor Sfj to enforce the analogous rela-
tionship between each ?fj and corresponding row j
of a.
EXTRACT. For each phrase pair variable pighk`
we have a factor Pghk` to ensure that pighk` = true
iff it is licensed by the span projections ?. As shown
in Figure 2b, in addition to pighk`, Pghk` depends on
the range of span variables ?ei for i ? [g, h] and ?fj
for j ? [k, `]. Pghk` is satisfied when pighk` = true
and the relations in Eq. (5) all hold, or when pighk` =
false and at least one of those relations does not hold.
ITG. Finally, to enforce the structural constraint
on a, we include a single global factor A that de-
pends on all the word link variables in a (see Fig-
ure 2a). A is satisfied iff a is in the family of
block inverse transduction grammar (ITG) align-
ments. The block ITG family permits multiple links
to be on (aij 6= off) for a particular word ei via termi-
nal block productions, but ensures that every word is
32
in at most one such terminal production, and that the
full set of terminal block productions is consistent
with ITG reordering patterns (Zhang et al, 2008).
3 Relaxing the ITG Constraint
The ITG factor can be viewed as imposing two dif-
ferent types of constraints on allowable word align-
ments a. First, it requires that each word is aligned
to at most one relatively short subspan of the other
sentence. This is a linguistically plausible con-
straint, as it is rarely the case that a single word will
translate to an extremely long phrase, or to multiple
widely separated phrases.3
The other constraint imposed by the ITG factor
is the ITG reordering constraint. This constraint
is imposed primarily for reasons of computational
tractability: the standard dynamic program for bi-
text parsing depends on ITG reordering (Wu, 1997).
While this constraint is not dramatically restric-
tive (Haghighi et al, 2009), it is plausible that re-
moving it would permit the model to produce better
alignments. We tested this hypothesis by develop-
ing a new model that enforces only the constraint
that each word align to one limited-length subspan,
which can be viewed as a generalization of the at-
most-one-to-one constraint frequently considered in
the word-alignment literature (Taskar et al, 2005;
Cromie`res and Kurohashi, 2009).
Our new model has almost exactly the same form
as the previous one. The only difference is that A is
replaced with a new family of simpler factors:
ONESPAN. For each target word ei (and each
source word fj) we include a hard constraint factor
U ei (respectively U
f
j ). U ei is satisfied iff |?ei,p| < d
(length limit) and either ?ei,p = [?1,?] or ?j ?
?ei,p, aij 6= off (no gaps), with ?ei,p as in Eq. (3). Fig-
ure 3 shows the portion of the factor graph from Fig-
ure 2a redrawn with the ONESPAN factors replacing
the ITG factor. As Figure 3 shows, there is no longer
a global factor; each U ei depends only on the word
link variables from column i.
3Short gaps can be accomodated within block ITG (and in
our model are represented as possible links) as long as the total
aligned span does not exceed the block size.
a11 a21
Lij
L11 L21
a12
ai1
Li1
L12 L22
a22
a1j aij
L1j
Uf1
Uf2
Ufj
Ue1 Ue2 Uei
Figure 3: ONESPAN factors
4 Belief Propagation
Belief propagation is a generalization of the well
known sum-product algorithm for undirected graph-
ical models. We will provide only a procedural
sketch here, but a good introduction to BP for in-
ference in structured NLP models can be found
in Smith and Eisner (2008), and Chapters 16 and 23
of MacKay (2003) contain a general introduction to
BP in the more general context of message-passing
algorithms.
At a high level, each variable maintains a local
distribution over its possible values. These local dis-
tribution are updated via messages passed between
variables and factors. For a variable V , N (V ) de-
notes the set of factors neighboring V in the fac-
tor graph. Similarly, N (F ) is the set of variables
neighboring the factor F . During each round of BP,
messages are sent from each variable to each of its
neighboring factors:
q(k+1)V?F (v) ?
?
G?N (V ),G 6=F
r(k)G?V (v) (7)
and from each factor to each of its neighboring vari-
ables:
r(k+1)F?V (v) ?
?
XF ,XF [V ]=v
F (XF )
?
U?N (F ),U 6=V
q(k)U?F (v) (8)
where XF is a partial assignment of values to just
the variables in N (F ).
33
Marginal beliefs at time k can be computed by
simply multiplying together all received messages
and normalizing:
b(k)V (v) ?
?
G?N (V )
r(k)G?V (v) (9)
Although messages can be updated according to
any schedule, generally one iteration of BP updates
each message once. The process iterates until some
stopping criterion has been met: either a fixed num-
ber of iterations or some convergence metric.
For our models, we say that BP has converged
whenever
?
V,v
(
b(k)V (v)? b
(k?1)
V (v)
)2
< ? for
some small ? > 0.4 While we have no theoretical
convergence guarantees, it usually converges within
10 iterations in practice.
5 Efficient BP for Extraction Set Models
In general, the efficiency of BP depends directly on
the arity of the factors in the model. Performed
na??vely, the sum in Eq. (8) will take time that grows
exponentially with the size of N (F ). For the soft-
scoring factors, which each depend only on a single
variable, this isn?t a problem. However, our model
also includes factors whose arity grows with the in-
put size: for example, explicitly enumerating all as-
signments to the word link variables that the ITG
factor depends on would take O(3n2) time.5
To run BP in a reasonable time frame, we need
efficient factor-specific propagators that can exploit
the structure of the factor functions to compute out-
going messages in polynomial time (Duchi et al,
2007; Smith and Eisner, 2008). Fortunately, all of
our hard constraints permit dynamic programs that
accomplish this propagation. Space does not permit
a full description of these dynamic programs, but we
will briefly sketch the intuitions behind them.
SPAN and ONESPAN. Marginal beliefs for Sei or
U ei can be computed inO(nd2) time. The key obser-
vation is that for any legal value ?ei = [k, `], Sei and
U ei require that aij = off for all j /? [k, `].6 Thus, we
start by computing the product of all the off beliefs:
4We set ? = 0.001.
5For all asymptotic analysis, we define n = max(|e|, |f|).
6For ease of exposition, we assume that all alignments are
either sure or off ; the modifications to account for the general
case are straightforward.
Factor Runtime Count Total
SURELINK O(1) O(n2) O(n2)
PHRASEPAIR O(1) O(n2d2) O(n2d2)
NULLWORD O(nd) O(n) O(n2d)
SPAN O(nd2) O(n) O(n2d2)
EXTRACT O(d3) O(n2d2) O(n2d5)
ITG O(n6) 1 O(n6)
ONESPAN O(nd2) O(n) O(n2d2)
Table 1: Asymptotic complexity for all factors.
b? =
?
j qaij (off). Then, for each of the O(nd) legal
source spans [k, `] we can efficiently find a joint be-
lief by summing over consistent assignments to the
O(d) link variables in that span.
EXTRACT. Marginal beliefs for Pghk` can be
computed inO(d3) time. For each of theO(d) target
words, we can find the total incoming belief that ?ei
is within [k, `] by summing over the O(d2) values
[k?, `?] where [k?, `?] ? [k, `]. Likewise for source
words. Multiplying together these per-word beliefs
and the belief that pighk` = true yields the joint be-
lief of a consistent assignment with pighk` = true,
which can be used to efficiently compute outgoing
messages.
ITG. To build outgoing messages, the ITG fac-
torA needs to compute marginal beliefs for all of the
word link variables aij . These can all be computed
in O(n6) time by using a standard bitext parser to
run the inside-outside algorithm. By using a normal
form grammar for block ITG with nulls (Haghighi
et al, 2009), we ensure that there is a 1-1 correspon-
dence between the ITG derivations the parser sums
over and word alignments a that satisfy A.
The asymptotic complexity for all the factors is
shown in Table 1. The total complexity for inference
in each model is simply the sum of the complexities
of its factors, so the complexity of the ITG model is
O(n2d5 + n6), while the complexity of the relaxed
model is just O(n2d5). The complexity of exact in-
ference, on the other hand, is exponential in d for the
ITG model and exponential in both d and n for the
relaxed model.
34
6 Training and Decoding
We use BP to compute marginal posteriors, which
we use at training time to get expected feature counts
and at test time for posterior decoding. For each sen-
tence pair, we continue to pass messages until either
the posteriors converge, or some maximum number
of iterations has been reached.7 After running BP,
the marginals we are interested in can all be com-
puted with Eq. (9).
6.1 Training
We train the model to maximize the log likelihood of
manually word-aligned gold training sentence pairs
(with L2 regularization). Because pi and ? are deter-
mined when a is observed, the model has no latent
variables. Therefore, the gradient takes the standard
form for loglinear models:
OLL = ?(a, pi, ?) ? (10)
?
a?,pi?,??
p(a?, pi?, ??|e, f)?(a?, pi?, ??)? ?w
The feature vector ? contains features on sure
word links, extracted phrase pairs, and null-aligned
words. Approximate expectations of these features
can be efficiently computed using the marginal be-
liefs baij (sure), bpighk`(true), and b?ei ([?1,?]) and
b?fj ([?1,?]), respectively. We learned our final
weight vectorw using AdaGrad (Duchi et al, 2010),
an adaptive subgradient version of standard stochas-
tic gradient ascent.
6.2 Testing
We evaluate our model by measuring precision and
recall on extracted phrase pairs. Thus, the decod-
ing problem takes a sentence pair (e, f) as input, and
must produce an extraction set pi as output. Our ap-
proach, posterior thresholding, is extremely simple:
we set pighk` = true iff bpighk`(true) ? ? for some
fixed threshold ? . Note that this decoding method
does not require that there be any underlying word
alignment a licensing the resulting extraction set pi,8
7See Section 7.2 for an empirical investigation of this maxi-
mum.
8This would be true even if we computed posteriors ex-
actly, but is especially true with approximate marginals from
BP, which are not necessarily consistent.
but the structure of the model is such that two con-
flicting phrase pairs are unlikely to simultaneously
have high posterior probability.
Most publicly available translation systems ex-
pect word-level alignments as input. These can
also be generated by applying posterior threshold-
ing, aligning target word i to source word j when-
ever baij (sure) ? t.9
7 Experiments
Our experiments are performed on Chinese-to-
English alignment. We trained and evaluated all
models on the NIST MT02 test set, which consists
of 150 training and 191 test sentences and has been
used previously in alignment experiments (Ayan and
Dorr, 2006; Haghighi et al, 2009; DeNero and
Klein, 2010). The unsupervised HMM word aligner
used to generate features for the model was trained
on 11.3 million words of FBIS newswire data. We
test three models: the Viterbi ITG model of DeNero
and Klein (2010), our BP ITG model that uses the
ITG factor, and our BP Relaxed model that replaces
the ITG factor with the ONESPAN factors. In all of
our experiments, the phrase length d was set to 3.10
7.1 Phrase Alignment
We tested the models by computing precision and
recall on extracted phrase pairs, relative to the gold
phrase pairs of up to length 3 induced by the gold
word alignments. For the BP models, we trade
off precision and recall by adjusting the decoding
threshold ? . The Viterbi ITG model was trained to
optimize F5, a recall-biased measure, so in addition
to F1, we also report the recall-biased F2 and F5
measures. The maximum number of BP iterations
was set to 5 for the BP ITG model and to 10 for the
BP Relaxed model.
The phrase alignment results are shown in Fig-
ure 4. The BP ITG model performs comparably to
the Viterbi ITG model. However, because posterior
decoding permits explicit tradeoffs between preci-
sion and recall, it can do much better in the recall-
biased measures, even though the Viterbi ITG model
was explicitly trained to maximize F5 (DeNero and
9For our experiments, we set t = 0.2.
10Because the runtime of the Viterbi ITG model grows expo-
nentially with d, it was not feasible to perform comparisons for
higher phrase lengths.
35
beta p r f2 0.69 0.742 0.7309823
60 
65 
70 
75 
80 
60 65 70 75 80 85 
Re
ca
ll 
Precision 
Viterbi ITG BP ITG BP Relaxed 
Model
Best Scores Sentences
F1 F2 F5 per Second
Viterbi ITG 71.6 73.1 74.0 0.21
BP ITG 71.8 74.8 83.5 0.11
BP Relaxed 72.6 75.2 84.5 1.15
Figure 4: Phrase alignment results. A portion of the Pre-
cision/Recall curve is plotted for the BP models, with the
result from the Viterbi ITG model provided for reference.
Klein, 2010). The BP Relaxed model performs the
best of all, consistently achieving higher recall for
fixed precision than either of the other models. Be-
cause of its lower asymptotic runtime, it is also much
faster: over 5 times as fast as the Viterbi ITG model
and over 10 times as fast as the BP ITG model.11
7.2 Timing
BP approximates marginal posteriors by iteratively
updating beliefs for each variable based on cur-
rent beliefs about other variables. The iterative na-
ture of the algorithm permits us to make an explicit
speed/accuracy tradeoff by limiting the number of
iterations. We tested this tradeoff by limiting both
of the BP models to run for 2, 3, 5, 10, and 20 iter-
ations. The results are shown in Figure 5. Neither
model benefits from running more iterations than
used to obtain the results in Figure 4, but each can
be sped up by a factor of almost 1.5x in exchange
for a modest (< 1 F1) drop in accuracy.
11The speed advantage of Viterbi ITG over BP ITG comes
from Viterbi ITG?s aggressive beaming.
Speed F12.08333333 61.32 67.61.58730159 71.91.14942529 72.60.96153846 72.6
67 
68 
69 
70 
71 
72 
73 
0.5 1 2 4 8 16 
Be
st 
F1
 
Time (seconds per sentence) 
Viterbi ITG BP ITG BP Relaxed 
67 
68 
69 
70 
71 
72 
73 
0.0625 0.125 0.25 0.5 1 2 
Be
st 
F1
 
Speed (sentences per second) 
Viterbi ITG BP ITG BP Relaxed 
Figure 5: Speed/accuracy tradeoff. The speed axis is on
a logarithmic scale. From fastest to slowest, data points
correspond to maximums of 2, 5, 10, and 20 BP itera-
tions. F1 for the BP Relaxed model was very low when
limited to 2 iterations, so that data point is outside the
visible area of the graph.
Model BLEU
Relative Hours to
Improve. Train/Align
Baseline 32.8 +0.0 5
Viterbi ITG 33.5 +0.7 831
BP Relaxed 33.6 +0.8 39
Table 2: Machine translation results.
7.3 Translation
We ran translation experiments using Moses (Koehn
et al, 2007), which we trained on a 22.1 mil-
lion word parallel corpus from the GALE program.
We compared alignments generated by the baseline
HMM model, the Viterbi ITG model and the Re-
laxed BP model.12 The systems were tuned and
evaluated on sentences up to length 40 from the
NIST MT04 and MT05 test sets. The results, shown
in Table 2, show that the BP Relaxed model achives
a 0.8 BLEU improvement over the HMM baseline,
comparable to that of the Viterbi ITG model, but tak-
ing a fraction of the time,13 making the BP Relaxed
model a practical alternative for real translation ap-
plications.
12Following a simplified version of the procedure described
by DeNero and Klein (2010), we added rule counts from the
HMM alignments to the extraction set algners? counts.
13Some of the speed difference between the BP Relaxed and
Viterbi ITG models comes from better parallelizability due to
drastically reduced memory overhead of the BP Relaxed model.
36
8 Conclusion
For performing inference in a state-of-the-art, but in-
efficient, alignment model, belief propagation is a
viable alternative to greedy search methods, such as
beaming. BP also results in models that are much
more scalable, by reducing the asymptotic complex-
ity of inference. Perhaps most importantly, BP per-
mits the relaxation of artificial constraints that are
generally taken for granted as being necessary for
efficient inference. In particular, a relatively mod-
est relaxation of the ITG constraint can directly be
applied to any model that uses ITG-based inference
(e.g. Zhang and Gildea, 2005; Cherry and Lin, 2007;
Haghighi et al, 2009).
Acknowledgements
This project is funded by an NSF graduate research
fellowship to the first author and by BBN under
DARPA contract HR0011-06-C-0022.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond AER: An extensive analysis of word alignments
and their impact on MT. In ACL.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In AMTA.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL-IJCNLP.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
NAACL Workshop on Syntax and Structure in Statisti-
cal Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In EMNLP-CoNLL.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
ACL.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In EMNLP.
John DeNero. 2010. Personal Communication.
John Duchi, Danny Tarlow, Gal Elidan, and Daphne
Koller. 2007. Using combinatorial optimization
within max-product belief propagation. In NIPS 2006.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning and
stochastic optimization. In COLT.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL-IJCNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
David J.C. MacKay. 2003. Information theory, infer-
ence, and learning algorithms. Cambridge Univ Press.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
ACL Workshop on Statistical Machine Translation.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL.
37
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL:HLT.
38
T5: Variational Inference for Structured NLP 
Models 
David Burkett, Dan Klein 
ABSTRACT
Historically, key breakthroughs in structured NLP models, such as chain CRFs or 
PCFGs, have relied on imposing careful constraints on the locality of features in order to 
permit efficient dynamic programming for computing expectations or finding the highest-
scoring structures. However, as modern structured models become more complex and 
seek to incorporate longer-range features, it is more and more often the case that 
performing exact inference is impossible (or at least impractical) and it is necessary to 
resort to some sort of approximation technique, such as beam search, pruning, or 
sampling. In the NLP community, one increasingly popular approach is the use of 
variational methods for computing approximate distributions. 
The goal of the tutorial is to provide an introduction to variational methods for 
approximate inference, particularly mean field approximation and belief propagation. 
The intuition behind the mathematical derivation of variational methods is fairly simple: 
instead of trying to directly compute the distribution of interest, first consider some 
efficiently computable approximation of the original inference problem, then find the 
solution of the approximate inference problem that minimizes the distance to the true 
distribution. Though the full derivations can be somewhat tedious, the resulting 
procedures are quite straightforward, and typically consist of an iterative process of 
individually updating specific components of the model, conditioned on the rest. 
Although we will provide some theoretical background, the main goal of the tutorial is to 
provide a concrete procedural guide to using these approximate inference techniques, 
illustrated with detailed walkthroughs of examples from recent NLP literature. 
Once both variational inference procedures have been described in detail, we'll provide 
a summary comparison of the two, along with some intuition about which approach is 
appropriate when. We'll also provide a guide to further exploration of the topic, briefly 
discussing other variational techniques, such as expectation propagation and convex 
relaxations, but concentrating mainly on providing pointers to additional resources for 
those who wish to learn more. 
OUTLINE
1. Introduction 
1. Approximate inference background 
2. Definition of variational inference 
3. Structured NLP problem setting, loglinear models 
4. Graphical model notation, feature locality 
2. Mean Field Approximation 
1. General description and theoretical background 
2. Derivation of updates for simple two-variable model 
3. Structured mean field: extension to joint CRFs 
4. Joint parsing and word alignment 
5. High level description of other models (Coref, Nonparametric Bayes) 
3. Belief Propagation 
1. General description and theoretical background 
2. Factor graph notation 
3. Formulas for messages and beliefs, with joint CRF example 
4. Dependency parsing 
5. Word alignment 
4. Wrap-up 
1. Mean Field vs Belief Propagation (i.e. what to use when) 
2. Other variational methods 
3. Additional resources 
BIOS
David Burkett
University of California, Berkeley 
dburkett--AT--cs.berkeley.edu 
David Burkett is a Ph.D. candidate in the Computer Science Division at the University of 
California, Berkeley. The main focus of his research is on modeling syntactic agreement 
in bilingual corpora. His interests are diverse, though, and he has worked on parsing, 
phrase alignment, language evolution, coreference resolution, and even video game AI. 
He has worked as an instructional assistant for multiple AI courses at Berkeley and won 
the Outstanding Graduate Student Instructor award in 2009. 
Dan Klein
University of California, Berkeley 
klein--AT--cs.berkeley.edu 
Dan Klein is an Associate Professor of Computer Science at the University of California, 
Berkeley. His research includes many areas of statistical natural language processing, 
including grammar induction, parsing, machine translation, information extraction, 
document summarization, historical linguistics, and speech recognition. His academic 
awards include a Sloan Fellowship, a Microsoft Faculty Fellowship, an NSF CAREER 
Award, the ACM Grace Murray Hopper Award, Best Paper Awards at ACL, EMNLP and 
NAACL, and the UC Berkeley Distinguished Teaching Award. 
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 9?10,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Variational Inference for Structured NLP Models
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Description
Historically, key breakthroughs in structured NLP
models, such as chain CRFs or PCFGs, have re-
lied on imposing careful constraints on the local-
ity of features in order to permit efficient dynamic
programming for computing expectations or find-
ing the highest-scoring structures. However, as
modern structured models become more complex
and seek to incorporate longer-range features, it is
more and more often the case that performing ex-
act inference is impossible (or at least impractical)
and it is necessary to resort to some sort of approx-
imation technique, such as beam search, pruning,
or sampling. In the NLP community, one increas-
ingly popular approach is the use of variational
methods for computing approximate distributions.
The goal of the tutorial is to provide an intro-
duction to variational methods for approximate in-
ference, particularly mean field approximation and
belief propagation. The intuition behind the math-
ematical derivation of variational methods is fairly
simple: instead of trying to directly compute the
distribution of interest, first consider some effi-
ciently computable approximation of the original
inference problem, then find the solution of the ap-
proximate inference problem that minimizes the
distance to the true distribution. Though the full
derivations can be somewhat tedious, the resulting
procedures are quite straightforward, and typically
consist of an iterative process of individually up-
dating specific components of the model, condi-
tioned on the rest. Although we will provide some
theoretical background, the main goal of the tu-
torial is to provide a concrete procedural guide to
using these approximate inference techniques, il-
lustrated with detailed walkthroughs of examples
from recent NLP literature.
Once both variational inference procedures
have been described in detail, we?ll provide a sum-
mary comparison of the two, along with some in-
tuition about which approach is appropriate when.
We?ll also provide a guide to further exploration of
the topic, briefly discussing other variational tech-
niques, such as expectation propagation and con-
vex relaxations, but concentrating mainly on pro-
viding pointers to additional resources for those
who wish to learn more.
Outline
1. Structured Models and Factor Graphs
? Factor graph notation
? Example structured NLP models
? Inference
2. Mean Field
? Warmup (iterated conditional modes)
? Mean field procedure
? Derivation of mean field update
? Example
3. Structured Mean Field
? Structured approximation
? Computing structured updates
? Example: Joint parsing and alignment
4. Belief Propagation
? Intro
? Messages and beliefs
? Loopy BP
5. Structured Belief Propagation
? Warmup (efficient products for mes-
sages)
? Example: Word alignment
? Example: Dependency parsing
6. Wrap-Up
? Mean field vs BP
? Other approximation techniques
9
Presenter Bios
David Burkett is a postdoctoral researcher in the
Computer Science Division at the University of
California, Berkeley. The main focus of his re-
search is on modeling syntactic agreement in bilin-
gual corpora. His interests are diverse, though, and
he has worked on parsing, phrase alignment, lan-
guage evolution, coreference resolution, and even
video game AI. He has worked as an instructional
assistant for multiple AI courses at Berkeley and
won multiple Outstanding Graduate Student In-
structor awards.
Dan Klein is an Associate Professor of Com-
puter Science at the University of California,
Berkeley. His research includes many areas of
statistical natural language processing, includ-
ing grammar induction, parsing, machine trans-
lation, information extraction, document summa-
rization, historical linguistics, and speech recog-
nition. His academic awards include a Sloan Fel-
lowship, a Microsoft Faculty Fellowship, an NSF
CAREER Award, the ACM Grace Murray Hop-
per Award, Best Paper Awards at ACL, EMNLP
and NAACL, and the UC Berkeley Distinguished
Teaching Award.
10
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1041?1051,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Structured Learning for Taxonomy Induction with Belief Propagation
Mohit Bansal
TTI Chicago
mbansal@ttic.edu
David Burkett
Twitter Inc.
dburkett@twitter.com
Gerard de Melo
Tsinghua University
gdm@demelo.org
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
We present a structured learning approach
to inducing hypernym taxonomies using a
probabilistic graphical model formulation.
Our model incorporates heterogeneous re-
lational evidence about both hypernymy
and siblinghood, captured by semantic
features based on patterns and statistics
from Web n-grams and Wikipedia ab-
stracts. For efficient inference over tax-
onomy structures, we use loopy belief
propagation along with a directed span-
ning tree algorithm for the core hyper-
nymy factor. To train the system, we ex-
tract sub-structures of WordNet and dis-
criminatively learn to reproduce them, us-
ing adaptive subgradient stochastic opti-
mization. On the task of reproducing
sub-hierarchies of WordNet, our approach
achieves a 51% error reduction over a
chance baseline, including a 15% error re-
duction due to the non-hypernym-factored
sibling features. On a comparison setup,
we find up to 29% relative error reduction
over previous work on ancestor F1.
1 Introduction
Many tasks in natural language understanding,
such as question answering, information extrac-
tion, and textual entailment, benefit from lexical
semantic information in the form of types and hy-
pernyms. A recent example is IBM?s Jeopardy!
system Watson (Ferrucci et al, 2010), which used
type information to restrict the set of answer can-
didates. Information of this sort is present in term
taxonomies (e.g., Figure 1), ontologies, and the-
sauri. However, currently available taxonomies
such as WordNet are incomplete in coverage (Pen-
nacchiotti and Pantel, 2006; Hovy et al, 2009),
unavailable in many domains and languages, and
vertebrate
mammal
placental
cow rodent
squirrel rat
metatherian
marsupial
kangaroo
reptile
diapsid
snake crocodilian
anapsid
chelonian
turtle
1
Figure 1: An excerpt of WordNet?s vertebrates taxonomy.
time-intensive to create or extend manually. There
has thus been considerable interest in building lex-
ical taxonomies automatically.
In this work, we focus on the task of taking col-
lections of terms as input and predicting a com-
plete taxonomy structure over them as output. Our
model takes a loglinear form and is represented
using a factor graph that includes both 1st-order
scoring factors on directed hypernymy edges (a
parent and child in the taxonomy) and 2nd-order
scoring factors on sibling edge pairs (pairs of hy-
pernym edges with a shared parent), as well as in-
corporating a global (directed spanning tree) struc-
tural constraint. Inference for both learning and
decoding uses structured loopy belief propagation
(BP), incorporating standard spanning tree algo-
rithms (Chu and Liu, 1965; Edmonds, 1967; Tutte,
1984). The belief propagation approach allows us
to efficiently and effectively incorporate hetero-
geneous relational evidence via hypernymy and
siblinghood (e.g., coordination) cues, which we
capture by semantic features based on simple sur-
face patterns and statistics from Web n-grams and
Wikipedia abstracts. We train our model to max-
imize the likelihood of existing example ontolo-
gies using stochastic optimization, automatically
learning the most useful relational patterns for full
taxonomy induction.
As an example of the relational patterns that our
1041
system learns, suppose we are interested in build-
ing a taxonomy for types of mammals (see Fig-
ure 1). Frequent attestation of hypernymy patterns
like rat is a rodent in large corpora is a strong sig-
nal of the link rodent ? rat. Moreover, sibling
or coordination cues like either rats or squirrels
suggest that rat is a sibling of squirrel and adds
evidence for the links rodent ? rat and rodent
? squirrel. Our supervised model captures ex-
actly these types of intuitions by automatically dis-
covering such heterogeneous relational patterns as
features (and learning their weights) on edges and
on sibling edge pairs, respectively.
There have been several previous studies on
taxonomy induction. e.g., the incremental tax-
onomy induction system of Snow et al (2006),
the longest path approach of Kozareva and Hovy
(2010), and the maximum spanning tree (MST)
approach of Navigli et al (2011) (see Section 4 for
a more detailed overview). The main contribution
of this work is that we present the first discrimina-
tively trained, structured probabilistic model over
the full space of taxonomy trees, using a struc-
tured inference procedure through both the learn-
ing and decoding phases. Our model is also the
first to directly learn relational patterns as part of
the process of training an end-to-end taxonomic
induction system, rather than using patterns that
were hand-selected or learned via pairwise clas-
sifiers on manually annotated co-occurrence pat-
terns. Finally, it is the first end-to-end (i.e., non-
incremental) system to include sibling (e.g., coor-
dination) patterns at all.
We test our approach in two ways. First, on
the task of recreating fragments of WordNet, we
achieve a 51% error reduction on ancestor-based
F1 over a chance baseline, including a 15% error
reduction due to the non-hypernym-factored sib-
ling features. Second, we also compare to the re-
sults of Kozareva and Hovy (2010) by predicting
the large animal subtree of WordNet. Here, we
get up to 29% relative error reduction on ancestor-
based F1. We note that our approach falls at a
different point in the space of performance trade-
offs from past work ? by producing complete,
highly articulated trees, we naturally see a more
even balance between precision and recall, while
past work generally focused on precision.
1
To
1
While different applications will value precision and
recall differently, and past work was often intentionally
precision-focused, it is certainly the case that an ideal solu-
tion would maximize both.
avoid presumption of a single optimal tradeoff, we
also present results for precision-based decoding,
where we trade off recall for precision.
2 Structured Taxonomy Induction
Given an input term set x = {x
1
, x
2
, . . . , x
n
},
we wish to compute the conditional distribution
over taxonomy trees y. This distribution P (y|x)
is represented using the graphical model formu-
lation shown in Figure 2. A taxonomy tree y is
composed of a set of indicator random variables
y
ij
(circles in Figure 2), where y
ij
= ON means
that x
i
is the parent of x
j
in the taxonomy tree
(i.e. there exists a directed edge from x
i
to x
j
).
One such variable exists for each pair (i, j) with
0 ? i ? n, 1 ? j ? n, and i 6= j.
2
In a factor graph formulation, a set of factors
(squares and rectangles in Figure 2) determines the
probability of each possible variable assignment.
Each factor F has an associated scoring function
?
F
, with the probability of a total assignment de-
termined by the product of all these scores:
P (y|x) ?
?
F
?
F
(y) (1)
2.1 Factor Types
In the models we present here, there are three
types of factors: EDGE factors that score individ-
ual edges in the taxonomy tree, SIBLING factors
that score pairs of edges with a shared parent, and
a global TREE factor that imposes the structural
constraint that y form a legal taxonomy tree.
EDGE Factors. For each edge variable y
ij
in
the model, there is a corresponding factor E
ij
(small blue squares in Figure 2) that depends only
on y
ij
. We score each edge by extracting a set
of features f(x
i
, x
j
) and weighting them by the
(learned) weight vector w. So, the factor scoring
function is:
?
E
ij
(y
ij
) =
{
exp(w ? f(x
i
, x
j
)) y
ij
= ON
exp(0) = 1 y
ij
= OFF
SIBLING Factors. Our second model also in-
cludes factors that permit 2nd-order features look-
ing at terms that are siblings in the taxonomy tree.
For each triple (i, j, k) with i 6= j, i 6= k, and
j < k,
3
we have a factor S
ijk
(green rectangles in
2
We assume a special dummy root symbol x
0
.
3
The ordering of the siblings x
j
and x
k
doesn?t mat-
ter here, so having separate factors for (i, j, k) and (i, k, j)
would be redundant.
1042
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
T
(a) Edge Features Only
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
S12n
S21n
Sn12
T
(b) Full Model
Figure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.
Figure 2b) that depends on y
ij
and y
ik
, and thus
can be used to encode features that should be ac-
tive whenever x
j
and x
k
share the same parent, x
i
.
The scoring function is similar to the one above:
?
S
ijk
(y
ij
, y
ik
) =
{
exp(w ? f(x
i
, x
j
, x
k
)) y
ij
= y
ik
= ON
1 otherwise
TREE Factor. Of course, not all variable as-
signments y form legal taxonomy trees (i.e., di-
rected spanning trees). For example, the assign-
ment ?i, j, y
ij
= ON might get a high score, but
would not be a valid output of the model. Thus,
we need to impose a structural constraint to ensure
that such illegal variable assignments are assigned
0 probability by the model. We encode this in our
factor graph setting using a single global factor T
(shown as a large red square in Figure 2) with the
following scoring function:
?
T
(y) =
{
1 y forms a legal taxonomy tree
0 otherwise
Model. For a given global assignment y, let
f(y) =
?
i,j
y
ij
=ON
f(x
i
, x
j
) +
?
i,j,k
y
ij
=y
ik
=ON
f(x
i
, x
j
, x
k
)
Note that by substituting our model?s factor scor-
ing functions into Equation 1, we get:
P (y|x) ?
{
exp(w ? f(y)) y is a tree
0 otherwise
Thus, our model has the form of a standard loglin-
ear model with feature function f .
2.2 Inference via Belief Propagation
With the model defined, there are two main in-
ference tasks we wish to accomplish: computing
expected feature counts and selecting a particular
taxonomy tree for a given set of input terms (de-
coding). As an initial step to each of these pro-
cedures, we wish to compute the marginal prob-
abilities of particular edges (and pairs of edges)
being on. In a factor graph, the natural infer-
ence procedure for computing marginals is belief
propagation. Note that finding taxonomy trees is
a structurally identical problem to directed span-
ning trees (and thereby non-projective dependency
parsing), for which belief propagation has previ-
ously been worked out in depth (Smith and Eisner,
2008). Therefore, we will only briefly sketch the
procedure here.
Belief propagation is a general-purpose infer-
ence method that computes marginals via directed
messages passed from variables to adjacent fac-
tors (and vice versa) in the factor graph. These
messages take the form of (possibly unnormal-
ized) distributions over values of the variable. The
two types of messages (variable to factor or fac-
tor to variable) have mutually recursive defini-
tions. The message from a factor F to an adjacent
variable V involves a sum over all possible val-
ues of every other variable that F touches. While
the EDGE and SIBLING factors are simple enough
to compute this sum by brute force, performing
the sum na??vely for computing messages from the
TREE factor would take exponential time. How-
1043
ever, due to the structure of that particular factor,
all of its outgoing messages can be computed si-
multaneously in O(n
3
) time via an efficient adap-
tation of Kirchhoff?s Matrix Tree Theorem (MTT)
(Tutte, 1984) which computes partition functions
and marginals for directed spanning trees.
Once message passing is completed, marginal
beliefs are computed by merely multiplying to-
gether all the messages received by a particular
variable or factor.
2.2.1 Loopy Belief Propagation
Looking closely at Figure 2a, one can observe
that the factor graph for the first version of our
model, containing only EDGE and TREE factors,
is acyclic. In this special case, belief propagation
is exact: after one round of message passing, the
beliefs computed (as discussed in Section 2.2) will
be the true marginal probabilities under the cur-
rent model. However, in the full model, shown
in Figure 2b, the SIBLING factors introduce cy-
cles into the factor graph, and now the messages
being passed around often depend on each other
and so they will change as they are recomputed.
The process of iteratively recomputing messages
based on earlier messages is known as loopy belief
propagation. This procedure only finds approx-
imate marginal beliefs, and is not actually guar-
anteed to converge, but in practice can be quite
effective for finding workable marginals in mod-
els for which exact inference is intractable, as is
the case here. All else equal, the more rounds
of message passing that are performed, the closer
the computed marginal beliefs will be to the true
marginals, though in practice, there are usually di-
minishing returns after the first few iterations. In
our experiments, we used a fairly conservative up-
per bound of 20 iterations, but in most cases, the
messages converged much earlier than that.
2.3 Training
We used gradient-based maximum likelihood
training to learn the model parameters w. Since
our model has a loglinear form, the derivative
of w with respect to the likelihood objective is
computed by just taking the gold feature vec-
tor and subtracting the vector of expected feature
counts. For computing expected counts, we run
belief propagation until completion and then, for
each factor in the model, we simply read off the
marginal probability of that factor being active (as
computed in Section 2.2), and accumulate a par-
tial count for each feature that is fired by that fac-
tor. This method of computing the gradient can be
incorporated into any gradient-based optimizer in
order to learn the weights w. In our experiments
we used AdaGrad (Duchi et al, 2011), an adaptive
subgradient variant of standard stochastic gradient
ascent for online learning.
2.4 Decoding
Finally, once the model parameters have been
learned, we want to use the model to find taxon-
omy trees for particular sets of input terms. Note
that if we limit our scores to be edge-factored,
then finding the highest scoring taxonomy tree
becomes an instance of the MST problem (also
known as the maximum arborescence problem
for the directed case), which can be solved effi-
ciently in O(n
2
) quadratic time (Tarjan, 1977) us-
ing the greedy, recursive Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967).
4
Since the MST problem can be solved effi-
ciently, the main challenge becomes finding a way
to ensure that our scores are edge-factored. In the
first version of our model, we could simply set the
score of each edge to be w?f(x
i
, x
j
), and the MST
recovered in this way would indeed be the high-
est scoring tree: arg maxyP (y|x). However, this
straightforward approach doesn?t apply to the full
model which also uses sibling features. Hence, at
decoding time, we instead start out by once more
using belief propagation to find marginal beliefs,
and then set the score of each edge to be its belief
odds ratio:
b
Y
ij
(ON)
b
Y
ij
(OFF)
.
5
3 Features
While spanning trees are familiar from non-
projective dependency parsing, features based on
the linear order of the words or on lexical identi-
4
See Georgiadis (2003) for a detailed algorithmic proof,
and McDonald et al (2005) for an illustrative example. Also,
we constrain the Chu-Liu-Edmonds MST algorithm to out-
put only single-root MSTs, where the (dummy) root has ex-
actly one child (Koo et al, 2007), because multi-root span-
ning ?forests? are not applicable to our task.
Also, note that we currently assume one node per term. We
are following the task description from previous work where
the goal is to create a taxonomy for a specific domain (e.g.,
animals). Within a specific domain, terms typically just have
a single sense. However, our algorithms could certainly be
adapted to the case of multiple term senses (by treating the
different senses as unique nodes in the tree) in future work.
5
The MST that is found using these edge scores is actually
the minimum Bayes risk tree (Goodman, 1996) for an edge
accuracy loss function (Smith and Eisner, 2008).
1044
ties or syntactic word classes, which are primary
drivers for dependency parsing, are mostly unin-
formative for taxonomy induction. Instead, induc-
ing taxonomies requires world knowledge to cap-
ture the semantic relations between various unseen
terms. For this, we use semantic cues to hyper-
nymy and siblinghood via features on simple sur-
face patterns and statistics in large text corpora.
We fire features on both the edge and the sibling
factors. We first describe all the edge features
in detail (Section 3.1 and Section 3.2), and then
briefly describe the sibling features (Section 3.3),
which are quite similar to the edge ones.
For each edge factor E
ij
, which represents the
potential parent-child term pair (x
i
, x
j
), we add
the surface and semantic features discussed below.
Note that since edges are directed, we have sepa-
rate features for the factors E
ij
versus E
ji
.
3.1 Surface Features
Capitalization: Checks which of x
i
and x
j
are
capitalized, with one feature for each value of the
tuple (isCap(x
i
), isCap(x
j
)). The intuition is that
leaves of a taxonomy are often proper names and
hence capitalized, e.g., (bison, American bison).
Therefore, the feature for (true, false) (i.e., parent
capitalized but not the child) gets a substantially
negative weight.
Ends with: Checks if x
j
ends with x
i
, or not. This
captures pairs such as (fish, bony fish) in our data.
Contains: Checks if x
j
contains x
i
, or not. This
captures pairs such as (bird, bird of prey).
Suffix match: Checks whether the k-length suf-
fixes of x
i
and x
j
match, or not, for k =
1, 2, . . . , 7.
LCS: We compute the longest common substring
of x
i
and x
j
, and create indicator features for
rounded-off and binned values of |LCS|/((|x
i
|+
|x
j
|)/2).
Length difference: We compute the signed length
difference between x
j
and x
i
, and create indica-
tor features for rounded-off and binned values of
(|x
j
| ? |x
i
|)/((|x
i
| + |x
j
|)/2). Yang and Callan
(2009) use a similar feature.
3.2 Semantic Features
3.2.1 Web n-gram Features
Patterns and counts: Hypernymy for a term pair
(P=x
i
, C=x
j
) is often signaled by the presence
of surface patterns like C is a P, P such as C
in large text corpora, an observation going back
to Hearst (1992). For each potential parent-child
edge (P=x
i
, C=x
j
), we mine the top k strings
(based on count) in which both x
i
and x
j
occur
(we use k=200). We collect patterns in both direc-
tions, which allows us to judge the correct direc-
tion of an edge (e.g., C is a P is a positive signal
for hypernymy whereas P is a C is a negative sig-
nal).
6
Next, for each pattern in this top-k list, we
compute its normalized pattern count c, and fire
an indicator feature on the tuple (pattern, t), for
all thresholds t (in a fixed set) s.t. c ? t. Our
supervised model then automatically learns which
patterns are good indicators of hypernymy.
Pattern order: We add features on the order (di-
rection) in which the pair (x
i
, x
j
) found a pattern
(in its top-k list) ? indicator features for boolean
values of the four cases: P . . . C, C . . . P , neither
direction, and both directions. Ritter et al (2009)
used the ?both? case of this feature.
Individual counts: We also compute the indi-
vidual Web-scale term counts c
x
i
and c
x
j
, and
add a comparison feature (c
x
i
>c
x
j
), plus features
on values of the signed count difference (|c
x
i
| ?
|c
x
j
|)/((|c
x
i
| + |c
x
j
|)/2), after rounding off, and
binning at multiple granularities. The intuition is
that this feature could learn whether the relative
popularity of the terms signals their hypernymy di-
rection.
3.2.2 Wikipedia Abstract Features
The Web n-grams corpus has broad coverage but
is limited to up to 5-grams, so it may not contain
pattern-based evidence for various longer multi-
word terms and pairs. Therefore, we supplement
it with a full-sentence resource, namely Wikipedia
abstracts, which are concise descriptions (hence
useful to signal hypernymy) of a large variety of
world entities.
Presence and distance: For each potential edge
(x
i
, x
j
), we mine patterns from all abstracts in
which the two terms co-occur in either order, al-
lowing a maximum term distance of 20 (because
beyond that, co-occurrence may not imply a rela-
tion). We add a presence feature based on whether
the process above found at least one pattern for
that term pair, or not. We also fire features on
the value of the minimum distance d
min
at which
6
We also allow patterns with surrounding words, e.g., the
C is a P and C , P of.
1045
the two terms were found in some abstract (plus
thresholded versions).
Patterns: For each term pair, we take the top-k
?
patterns (based on count) of length up to l from
its full list of patterns, and add an indicator feature
on each pattern string (without the counts). We use
k
?
=5, l=10. Similar to the Web n-grams case, we
also fire Wikipedia-based pattern order features.
3.3 Sibling Features
We also incorporate similar features on sibling
factors. For each sibling factor S
ijk
which rep-
resents the potential parent-children term triple
(x
i
, x
j
, x
k
), we consider the potential sibling term
pair (x
j
, x
k
). Siblinghood for this pair would be
indicated by the presence of surface patterns such
as either C
1
or C
2
, C
1
is similar to C
2
in large cor-
pora. Hence, we fire Web n-gram pattern features
and Wikipedia presence, distance, and pattern fea-
tures, similar to those described above, on each
potential sibling term pair.
7
The main difference
here from the edge factors is that the sibling fac-
tors are symmetric (in the sense that S
ijk
is redun-
dant to S
ikj
) and hence the patterns are undirected.
Therefore, for each term pair, we first symmetrize
the collected Web n-grams and Wikipedia patterns
by accumulating the counts of symmetric patterns
like rats or squirrels and squirrels or rats.
8
4 Related Work
In our work, we assume a known term set and
do not address the problem of extracting related
terms from text. However, a great deal of past
work has considered automating this process, typ-
ically taking one of two major approaches. The
clustering-based approach (Lin, 1998; Lin and
Pantel, 2002; Davidov and Rappoport, 2006; Ya-
mada et al, 2009) discovers relations based on the
assumption that similar concepts appear in sim-
7
One can also add features on the full triple (x
i
, x
j
, x
k
)
but most such features will be sparse.
8
All the patterns and counts for our Web and Wikipedia
edge and sibling features described above are extracted after
stemming the words in the terms, the n-grams, and the ab-
stracts (using the Porter stemmer). Also, we threshold the
features (to prune away the sparse ones) by considering only
those that fire for at least t trees in the training data (t = 4 in
our experiments).
Note that one could also add various complementary types of
useful features presented by previous work, e.g., bootstrap-
ping using syntactic heuristics (Phillips and Riloff, 2002),
dependency patterns (Snow et al, 2006), doubly anchored
patterns (Kozareva et al, 2008; Hovy et al, 2009), and Web
definition classifiers (Navigli et al, 2011).
ilar contexts (Harris, 1954). The pattern-based
approach uses special lexico-syntactic patterns to
extract pairwise relation lists (Phillips and Riloff,
2002; Girju et al, 2003; Pantel and Pennacchiotti,
2006; Suchanek et al, 2007; Ritter et al, 2009;
Hovy et al, 2009; Baroni et al, 2010; Ponzetto
and Strube, 2011) and semantic classes or class-
instance pairs (Riloff and Shepherd, 1997; Katz
and Lin, 2003; Pas?ca, 2004; Etzioni et al, 2005;
Talukdar et al, 2008).
We focus on the second step of taxonomy induc-
tion, namely the structured organization of terms
into a complete and coherent tree-like hierarchy.
9
Early work on this task assumes a starting par-
tial taxonomy and inserts missing terms into it.
Widdows (2003) place unknown words into a re-
gion with the most semantically-similar neigh-
bors. Snow et al (2006) add novel terms by greed-
ily maximizing the conditional probability of a set
of relational evidence given a taxonomy. Yang and
Callan (2009) incrementally cluster terms based
on a pairwise semantic distance. Lao et al (2012)
extend a knowledge base using a random walk
model to learn binary relational inference rules.
However, the task of inducing full taxonomies
without assuming a substantial initial partial tax-
onomy is relatively less well studied. There is
some prior work on the related task of hierarchical
clustering, or grouping together of semantically
related words (Cimiano et al, 2005; Cimiano and
Staab, 2005; Poon and Domingos, 2010; Fountain
and Lapata, 2012). The task we focus on, though,
is the discovery of direct taxonomic relationships
(e.g., hypernymy) between words.
We know of two closely-related previous sys-
tems, Kozareva and Hovy (2010) and Navigli et
al. (2011), that build full taxonomies from scratch.
Both of these systems use a process that starts
by finding basic level terms (leaves of the fi-
nal taxonomy tree, typically) and then using re-
lational patterns (hand-selected ones in the case of
Kozareva and Hovy (2010), and ones learned sep-
arately by a pairwise classifier on manually anno-
tated co-occurrence patterns for Navigli and Ve-
lardi (2010), Navigli et al (2011)) to find interme-
diate terms and all the attested hypernymy links
between them.
10
To prune down the resulting tax-
9
Determining the set of input terms is orthogonal to our
work, and our method can be used in conjunction with vari-
ous term extraction approaches described above.
10
Unlike our system, which assumes a complete set of
terms and only attempts to induce the taxonomic structure,
1046
onomy graph, Kozareva and Hovy (2010) use a
procedure that iteratively retains the longest paths
between root and leaf terms, removing conflicting
graph edges as they go. The end result is acyclic,
though not necessarily a tree; Navigli et al (2011)
instead use the longest path intuition to weight
edges in the graph and then find the highest weight
taxonomic tree using a standard MST algorithm.
Our work differs from the two systems above
in that ours is the first discriminatively trained,
structured probabilistic model over the full space
of taxonomy trees that uses structured inference
via spanning tree algorithms (MST and MTT)
through both the learning and decoding phases.
Our model also automatically learns relational pat-
terns as a part of the taxonomic training phase, in-
stead of relying on hand-picked rules or pairwise
classifiers on manually annotated co-occurrence
patterns, and it is the first end-to-end (i.e., non-
incremental) system to include heterogeneous re-
lational information via sibling (e.g., coordina-
tion) patterns.
5 Experiments
5.1 Data and Experimental Regime
We considered two distinct experimental setups,
one that illustrates the general performance of
our model by reproducing various medium-sized
WordNet domains, and another that facilitates
comparison to previous work by reproducing the
much larger animal subtree provided by Kozareva
and Hovy (2010).
General setup: In order to test the accuracy
of structured prediction on medium-sized full-
domain taxonomies, we extracted from WordNet
3.0 all bottomed-out full subtrees which had a
tree-height of 3 (i.e., 4 nodes from root to leaf),
and contained (10, 50] terms.
11
This gives us
761 non-overlapping trees, which we partition into
both these systems include term discovery in the taxonomy
building process.
11
Subtrees that had a smaller or larger tree height were dis-
carded in order to avoid overlap between the training and test
divisions. This makes it a much stricter setting than other
tasks such as parsing, which usually has repeated sentences,
clauses and phrases between training and test sets.
To project WordNet synsets to terms, we used the first (most
frequent) term in each synset. A few WordNet synsets have
multiple parents so we only keep the first of each such pair of
overlapping trees. We also discard a few trees with duplicate
terms because this is mostly due to the projection of different
synsets to the same term, and theoretically makes the tree a
graph.
70/15/15% (533/114/114 trees) train/dev/test sets.
Comparison setup: We also compare our method
(as closely as possible) with related previous work
by testing on the much larger animal subtree made
available by Kozareva and Hovy (2010), who cre-
ated this dataset by selecting a set of ?harvested?
terms and retrieving all the WordNet hypernyms
between each input term and the root (i.e., an-
imal), resulting in ?700 terms and ?4,300 is-a
ancestor-child links.
12
Our training set for this an-
imal test case was generated from WordNet us-
ing the following process: First, we strictly re-
move the full animal subtree from WordNet in or-
der to avoid any possible overlap with the test data.
Next, we create random 25-sized trees by picking
random nodes as singleton trees, and repeatedly
adding child edges from WordNet to the tree. This
process gives us a total of ?1600 training trees.
13
Feature sources: The n-gram semantic features
are extracted from the Google n-grams corpus
(Brants and Franz, 2006), a large collection of
English n-grams (for n = 1 to 5) and their fre-
quencies computed from almost 1 trillion tokens
(95 billion sentences) of Web text. The Wikipedia
abstracts are obtained via the publicly available
dump, which contains almost ?4.1 million ar-
ticles.
14
Preprocessing includes standard XML
parsing and tokenization. Efficient collection of
feature statistics is important because these must
be extracted for millions of query pairs (for each
potential edge and sibling pair in each term set).
For this, we use a hash-trie on term pairs (sim-
ilar to that of Bansal and Klein (2011)), and scan
once through the n-gram (or abstract) set, skipping
many n-grams (or abstracts) based on fast checks
of missing unigrams, exceeding length, suffix mis-
matches, etc.
5.2 Evaluation Metric
Ancestor F1: Measures the precision, recall, and
F
1
= 2PR/(P +R) of correctly predicted ances-
12
This is somewhat different from our general setup where
we work with any given set of terms; they start with a large
set of leaves which have substantial Web-based relational
information based on their selected, hand-picked patterns.
Their data is available at http://www.isi.edu/
?
kozareva/
downloads.html.
13
We tried this training regimen as different from that of
the general setup (which contains only bottomed-out sub-
trees), so as to match the animal test tree, which is of depth
12 and has intermediate nodes from higher up in WordNet.
14
We used the 20130102 dump.
1047
System P R F1
Edges-Only Model
Baseline 5.9 8.3 6.9
Surface Features 17.5 41.3 24.6
Semantic Features 37.0 49.1 42.2
Surface+Semantic 41.1 54.4 46.8
Edges + Siblings Model
Surface+Semantic 53.1 56.6 54.8
Surface+Semantic (Test) 48.0 55.2 51.4
Table 1: Main results on our general setup. On the devel-
opment set, we present incremental results on the edges-only
model where we start with the chance baseline, then use sur-
face features only, semantic features only, and both. Finally,
we add sibling factors and features to get results for the full,
edges+siblings model with all features, and also report the
final test result for this setting.
tors, i.e., pairwise is-a relations:
P =
|isa
gold
? isa
predicted
|
|isa
predicted
|
, R =
|isa
gold
? isa
predicted
|
|isa
gold
|
5.3 Results
Table 1 shows our main results for ancestor-based
evaluation on the general setup. We present a de-
velopment set ablation study where we start with
the edges-only model (Figure 2a) and its random
tree baseline (which chooses any arbitrary span-
ning tree for the term set). Next, we show results
on the edges-only model with surface features
(Section 3.1), semantic features (Section 3.2), and
both. We see that both surface and semantic fea-
tures make substantial contributions, and they also
stack. Finally, we add the sibling factors and fea-
tures (Figure 2b, Section 3.3), which further im-
proves the results significantly (8% absolute and
15% relative error reduction over the edges-only
results on the ancestor F1 metric). The last row
shows the final test set results for the full model
with all features.
Table 2 shows our results for comparison to
the larger animal dataset of Kozareva and Hovy
(2010).
15
In the table, ?Kozareva2010? refers
to Kozareva and Hovy (2010) and ?Navigli2011?
refers to Navigli et al (2011).
16
For appropri-
15
These results are for the 1st order model due to the scale
of the animal taxonomy (?700 terms). For scaling the 2nd
order sibling model, one can use approximations, e.g., prun-
ing the set of sibling factors based on 1st order link marginals,
or a hierarchical coarse-to-fine approach based on taxonomy
induction on subtrees, or a greedy approach of adding a few
sibling factors at a time. This is future work.
16
The Kozareva and Hovy (2010) ancestor results are ob-
tained by using the output files provided on their webpage.
System P R F1
Previous Work
Kozareva2010 98.6 36.2 52.9
Navigli2011
??
97.0
??
43.7
??
60.3
??
This Paper
Fixed Prediction 84.2 55.1 66.6
Free Prediction 79.3 49.0 60.6
Table 2: Comparison results on the animal dataset of
Kozareva and Hovy (2010). Here, ?Kozareva2010? refers to
Kozareva and Hovy (2010) and ?Navigli2011? refers to Nav-
igli et al (2011). For appropriate comparison to each previ-
ous work, we show our results both for the ?Fixed Prediction?
setup, which assumes the true root and leaves, and for the
?Free Prediction? setup, which doesn?t assume any prior in-
formation. The ?? results of Navigli et al (2011) represent a
different ground-truth data condition, making them incompa-
rable to our results; see Section 5.3 for details.
ate comparison to each previous work, we show
results for two different setups. The first setup
?Fixed Prediction? assumes that the model knows
the true root and leaves of the taxonomy to provide
for a somewhat fairer comparison to Kozareva and
Hovy (2010). We get substantial improvements
on ancestor-based recall and F1 (a 29% relative
error reduction). The second setup ?Free Predic-
tion? assumes no prior knowledge and predicts the
full tree (similar to the general setup case). On
this setup, we do compare as closely as possible
to Navigli et al (2011) and see a small gain in F1,
but regardless, we should note that their results are
incomparable (denoted by ?? in Table 2) because
they have a different ground-truth data condition:
their definition and hypernym extraction phase in-
volves using the Google define keyword, which
often returns WordNet glosses itself.
We note that previous work achieves higher an-
cestor precision, while our approach achieves a
more even balance between precision and recall.
Of course, precision and recall should both ide-
ally be high, even if some applications weigh one
over the other. This is why our tuning optimized
for F1, which represents a neutral combination
for comparison, but other F
?
metrics could also
be optimized. In this direction, we also tried an
experiment on precision-based decoding (for the
?Free Prediction? scenario), where we discard any
edges with score (i.e., the belief odds ratio de-
scribed in Section 2.4) less than a certain thresh-
old. This allowed us to achieve high values of pre-
cision (e.g., 90.8%) at still high enough F1 values
(e.g., 61.7%).
1048
Hypernymy features
C and other P > P > C
C , P of C is a P
C , a P P , including C
C or other P P ( C
C : a P C , american P
C - like P C , the P
Siblinghood features
C
1
and C
2
C
1
, C
2
(
C
1
or C
2
of C
1
and / or C
2
, C
1
, C
2
and either C
1
or C
2
the C
1
/ C
2
<s> C
1
and C
2
</s>
Table 3: Examples of high-weighted hypernymy and sibling-
hood features learned during development.
butterfly
copper
American copper
hairstreak
Strymon melinus
admiral
white admiral
1
Figure 3: Excerpt from the predicted butterfly tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
6 Analysis
Table 3 shows some of the hypernymy and sibling-
hood features given highest weight by our model
(in general-setup development experiments). The
training process not only rediscovers most of the
standard Hearst-style hypernymy patterns (e.g., C
and other P, C is a P), but also finds various
novel, intuitive patterns. For example, the pattern
C, american P is prominent because it captures
pairs like Lemmon, american actor and Bryon,
american politician, etc. Another pattern > P >
C captures webpage navigation breadcrumb trails
(representing category hierarchies). Similarly, the
algorithm also discovers useful siblinghood fea-
tures, e.g., either C
1
or C
2
, C
1
and / or C
2
, etc.
Finally, we look at some specific output errors
to give as concrete a sense as possible of some sys-
tem confusions, though of course any hand-chosen
examples must be taken as illustrative. In Figure
3, we attach white admiral to admiral, whereas
the gold standard makes these two terms siblings.
In reality, however, white admirals are indeed a
species of admirals, so WordNet?s ground truth
turns out to be incomplete. Another such example
is that we place logistic assessment in the evalu-
bottle
flask
vacuum flask thermos Erlenmeyer flask
wine bottle jeroboam
1
Figure 4: Excerpt from the predicted bottle tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
ation subtree of judgment, but WordNet makes it
a direct child of judgment. However, other dictio-
naries do consider logistic assessments to be eval-
uations. Hence, this illustrates that there may be
more than one right answer, and that the low re-
sults on this task should only be interpreted as
such. In Figure 4, our algorithm did not recog-
nize that thermos is a hyponym of vacuum flask,
and that jeroboam is a kind of wine bottle. Here,
our Web n-grams dataset (which only contains fre-
quent n-grams) and Wikipedia abstracts do not
suffice and we would need to add richer Web data
for such world knowledge to be reflected in the
features.
7 Conclusion
Our approach to taxonomy induction allows het-
erogeneous information sources to be combined
and balanced in an error-driven way. Direct indi-
cators of hypernymy, such as Hearst-style context
patterns, are the core feature for the model and are
discovered automatically via discriminative train-
ing. However, other indicators, such as coordina-
tion cues, can indicate that two words might be
siblings, independently of what their shared par-
ent might be. Adding second-order factors to our
model allows these two kinds of evidence to be
weighed and balanced in a discriminative, struc-
tured probabilistic framework. Empirically, we
see substantial gains (in ancestor F1) from sibling
features, and also over comparable previous work.
We also present results on the precision and recall
trade-offs inherent in this task.
Acknowledgments
We would like to thank the anonymous review-
ers for their insightful comments. This work
was supported by BBN under DARPA contract
HR0011-12-C-0014, 973 Program China Grants
2011CBA00300, 2011CBA00301, and NSFC
Grants 61033001, 61361136003.
1049
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram corpus version 1.1. LDC2006T13.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14(1396-1400):270.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided agglom-
erative clustering algorithm. In Proceedings of the
ICML 2005 Workshop on Learning and Extending
Lexical Ontologies with Machine Learning Meth-
ods.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. Journal of Arti-
ficial Intelligence Research, 24(1):305?339.
Dmitry Davidov and Ari Rappoport. 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
In Proceedings of COLING-ACL.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71:233?240.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the Web:
An experimental study. Artificial Intelligence,
165(1):91?134.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the DeepQA
project. AI magazine, 31(3):59?79.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy
induction using hierarchical random graphs. In Pro-
ceedings of NAACL.
Leonidas Georgiadis. 2003. Arborescence optimiza-
tion problems solvable by edmonds algorithm. The-
oretical Computer Science, 301(1):427?437.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of NAACL.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of EMNLP.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In Proceedings of the Workshop on NLP for Ques-
tion Answering (EACL 2003).
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of
EMNLP-CoNLL.
Zornitsa Kozareva and Eduard Hovy. 2010. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of EMNLP.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP.
Dekang Lin and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceedings of COLING.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexical
taxonomies from scratch. In Proceedings of IJCAI.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL.
1050
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proceedings of
COLING-ACL.
William Phillips and Ellen Riloff. 2002. Exploiting
strong syntactic heuristics and co-training to learn
semantic lexicons. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively
built knowledge repository. Artificial Intelligence,
175(9):1737?1756.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of ACL.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In
Proceedings of EMNLP.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of AAAI Spring Sympo-
sium on Learning by Reading and Learning to Read.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP.
Robert E. Tarjan. 1977. Finding optimum branchings.
Networks, 7:25?35.
William T. Tutte. 1984. Graph theory. Addison-
Wesley.
Dominic Widdows. 2003. Unsupervised methods
for developing taxonomies by combining syntactic
and statistical information. In Proceedings of HLT-
NAACL.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym dis-
covery based on distributional similarity and hierar-
chical structures. In Proceedings of EMNLP.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of ACL-IJCNLP.
1051
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46?54,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Monolingual Models with Unannotated Bilingual Text
David Burkett? Slav Petrov? John Blitzer? Dan Klein?
?University of California, Berkeley ?Google Research
{dburkett,blitzer,klein}@cs.berkeley.edu slav@google.com
Abstract
This work shows how to improve state-of-the-art
monolingual natural language processing models
using unannotated bilingual text. We build a mul-
tiview learning objective that enforces agreement
between monolingual and bilingual models. In
our method the first, monolingual view consists of
supervised predictors learned separately for each
language. The second, bilingual view consists of
log-linear predictors learned over both languages
on bilingual text. Our training procedure estimates
the parameters of the bilingual model using the
output of the monolingual model, and we show how
to combine the two models to account for depen-
dence between views. For the task of named entity
recognition, using bilingual predictors increases F1
by 16.1% absolute over a supervised monolingual
model, and retraining on bilingual predictions
increases monolingual model F1 by 14.6%. For
syntactic parsing, our bilingual predictor increases
F1 by 2.1% absolute, and retraining a monolingual
model on its output gives an improvement of 2.0%.
1 Introduction
Natural language analysis in one language can be
improved by exploiting translations in another lan-
guage. This observation has formed the basis for
important work on syntax projection across lan-
guages (Yarowsky et al, 2001; Hwa et al, 2005;
Ganchev et al, 2009) and unsupervised syntax
induction in multiple languages (Snyder et al,
2009), as well as other tasks, such as cross-lingual
named entity recognition (Huang and Vogel, 2002;
Moore, 2003) and information retrieval (Si and
Callan, 2005). In all of these cases, multilingual
models yield increased accuracy because differ-
ent languages present different ambiguities and
therefore offer complementary constraints on the
shared underlying labels.
In the present work, we consider a setting where
we already possess supervised monolingual mod-
els, and wish to improve these models using unan-
notated bilingual parallel text (bitext). We cast this
problem in the multiple-view (multiview) learning
framework (Blum and Mitchell, 1998; Collins and
Singer, 1999; Balcan and Blum, 2005; Ganchev et
al., 2008). Our two views are a monolingual view,
which uses the supervised monolingual models but
not bilingual information, and a bilingual view,
which exploits features that measure agreement
across languages. The parameters of the bilin-
gual view are trained to reproduce the output of
the monolingual view. We show that by introduc-
ing weakened monolingual models into the bilin-
gual view, we can optimize the parameters of the
bilingual model to improve monolingual models.
At prediction time, we automatically account for
the between-view dependence introduced by the
weakened monolingual models with a simple but
effective view-combination heuristic.
We demonstrate the performance of this method
on two problems. The first is named en-
tity recognition (NER). For this problem, our
method automatically learns (a variation on) ear-
lier hand-designed rule-based bilingual NER pre-
dictors (Huang and Vogel, 2002; Moore, 2003),
resulting in absolute performance gains of up to
16.1% F1. The second task we consider is statis-
tical parsing. For this task, we follow the setup
of Burkett and Klein (2008), who improved Chi-
nese and English monolingual parsers using par-
allel, hand-parsed text. We achieve nearly iden-
tical improvements using a purely unlabeled bi-
text. These results carry over to machine transla-
tion, where we can achieve slightly better BLEU
improvements than the supervised model of Bur-
kett and Klein (2008) since we are able to train
our model directly on the parallel data where we
perform rule extraction.
Finally, for both of our tasks, we use our bilin-
gual model to generate additional automatically
labeled monolingual training data. We compare
46
this approach to monolingual self-training and
show an improvement of up to 14.4% F1 for entity
recognition. Even for parsing, where the bilingual
portion of the treebank is much smaller than the
monolingual, our technique still can improve over
purely monolingual self-training by 0.7% F1.
2 Prior Work on Learning from
Bilingual Text
Prior work in learning monolingual models from
bitexts falls roughly into three categories: Unsu-
pervised induction, cross-lingual projection, and
bilingual constraints for supervised monolingual
models. Two recent, successful unsupervised
induction methods are those of Blunsom et al
(2009) and Snyder et al (2009). Both of them es-
timate hierarchical Bayesian models and employ
bilingual data to constrain the types of models that
can be derived. Projection methods, on the other
hand, were among the first applications of parallel
text (after machine translation) (Yarowsky et al,
2001; Yarowsky and Ngai, 2001; Hwa et al, 2005;
Ganchev et al, 2009). They assume the existence
of a good, monolingual model for one language
but little or no information about the second lan-
guage. Given a parallel sentence pair, they use the
annotations for one language to heavily constrain
the set of possible annotations for the other.
Our work falls into the final category: We wish
to use bilingual data to improve monolingual mod-
els which are already trained on large amounts of
data and effective on their own (Huang and Vo-
gel, 2002; Smith and Smith, 2004; Snyder and
Barzilay, 2008; Burkett and Klein, 2008). Proce-
durally, our work is most closely related to that
of Burkett and Klein (2008). They used an an-
notated bitext to learn parse reranking models for
English and Chinese, exploiting features that ex-
amine pieces of parse trees in both languages. Our
method can be thought of as the semi-supervised
counterpart to their supervised model. Indeed, we
achieve nearly the same results, but without anno-
tated bitexts. Smith and Smith (2004) consider
a similar setting for parsing both English and Ko-
rean, but instead of learning a joint model, they
consider a fixed combination of two parsers and
a word aligner. Our model learns parameters for
combining two monolingual models and poten-
tially thousands of bilingual features. The result
is that our model significantly improves state-of-
the-art results, for both parsing and NER.
3 A Multiview Bilingual Model
Given two input sentences x = (x1, x2) that
are word-aligned translations of each other, we
consider the problem of predicting (structured)
labels y = (y1, y2) by estimating conditional
models on pairs of labels from both languages,
p(y1, y2|x1, x2). Our model consists of two views,
which we will refer to as monolingual and bilin-
gual. The monolingual view estimates the joint
probability as the product of independent marginal
distributions over each language, pM (y|x) =
p1(y1|x1)p2(y2|x2). In our applications, these
marginal distributions will be computed by state-
of-the-art statistical taggers and parsers trained on
large monolingual corpora.
This work focuses on learning parameters for
the bilingual view of the data. We parameterize
the bilingual view using at most one-to-one match-
ings between nodes of structured labels in each
language (Burkett and Klein, 2008). In this work,
we use the term node to indicate a particular com-
ponent of a label, such as a single (multi-word)
named entity or a node in a parse tree. In Fig-
ure 2(a), for example, the nodes labeled NP1 in
both the Chinese and English trees are matched.
Since we don?t know a priori how the components
relate to one another, we treat these matchings as
hidden. For each matching a and pair of labels
y, we define a feature vector ?(y1, a, y2) which
factors on edges in the matching. Our model is
a conditional exponential family distribution over
matchings and labels:
p?(y, a|x) = exp
[
?>?(y1, a, y2)?A(?;x)
]
,
where ? is a parameter vector, and A(?;x) is the
log partition function for a sentence pair x. We
must approximate A(?;x) because summing over
all at most one-to-one matchings a is #P-hard. We
approximate this sum using the maximum-scoring
matching (Burkett and Klein, 2008):
A?(?;x) = log
?
y
max
a
(
exp
[
?>?(y1, a, y2)
])
.
In order to compute the distribution on labels y, we
must marginalize over hidden alignments between
nodes, which we also approximate by using the
maximum-scoring matching:
q?(y|x)
def
= max
a
exp
[
?>?(y1, a, y2)?A?(?;x)
]
.
47
the reports of European Court
ORG1
of Auditors
die Berichte des Europ?ischen Rechnungshofes
ORG1
the
Figure 1: An example where English NER can be
used to disambiguate German NER.
We further simplify inference in our model by
working in a reranking setting (Collins, 2000;
Charniak and Johnson, 2005), where we only con-
sider the top k outputs from monolingual models
in both languages, for a total of k2 labels y. In
practice, k2 ? 10, 000 for our largest problem.
3.1 Including Weakened Models
Now that we have defined our bilingual model, we
could train it to agree with the output of the mono-
lingual model (Collins and Singer, 1999; Ganchev
et al, 2008). As we will see in Section 4, however,
the feature functions ?(y1, a, y2) make no refer-
ence to the input sentences x, other than through a
fixed word alignment. With such limited monolin-
gual information, it is impossible for the bilingual
model to adequately capture all of the information
necessary for NER or parsing. As a simple ex-
ample, a bilingual NER model will be perfectly
happy to label two aligned person names as ORG
instead of PER: both labelings agree equally well.
We briefly illustrate how poorly such a basic bilin-
gual model performs in Section 10.
One way to solve this problem is to include the
output of the full monolingual models as features
in the bilingual view. However, we are training the
bilingual view to match the output of these same
models, which can be trivially achieved by putting
weight on only the monolingual model scores and
never recruiting any bilingual features. There-
fore, we use an intermediate approach: we intro-
duce the output of deliberately weakened mono-
lingual models as features in the bilingual view.
A weakened model is from the same class as the
full monolingual models, but is intentionally crip-
pled in some way (by removing feature templates,
for example). Crucially, the weakened models will
make predictions that are roughly similar to the
full models, but systematically worse. Therefore,
model scores from the weakened models provide
enough power for the bilingual view to make accu-
Feat. types Examples
Algn Densty INSIDEBOTH=3 INENONLY=0
Indicators LBLMATCH=true BIAS=true
Table 1: Sample features used for named entity
recognition for the ORG entity in Figure 1.
rate predictions, but ensure that bilingual features
will be required to optimize the training objective.
Let `W1 = log p
W
1 (y1|x1), `
W
2 = log p
W
2 (y2|x2)
be the log-probability scores from the weakened
models. Our final approximation to the marginal
distribution over labels y is:
q?1,?2,?(y|x)
def
= max
a
exp
h
?1`
W
1 + ?2`
W
2 +
?>?(y1, a, y2)? A?(?1, ?2,?;x)
i
.
(1)
Where
A?(?1, ?2,?;x) =
log
X
y
max
a
exp
h
?1`
W
1 + ?2`
W
2 + ?
>?(y1, a, y2)
i
is the updated approximate log partition function.
4 NER and Parsing Examples
Before formally describing our algorithm for find-
ing the parameters [?1, ?2,?], we first give exam-
ples of our problems of named entity recognition
and syntactic parsing, together with node align-
ments and features for each. Figure 1 depicts a
correctly-labeled sentence fragment in both En-
glish and German. In English, the capitalization of
the phrase European Court of Auditors helps iden-
tify the span as a named entity. However, in Ger-
man, all nouns are capitalized, and capitalization
is therefore a less useful cue. While a monolin-
gual German tagger is likely to miss the entity in
the German text, by exploiting the parallel English
text and word alignment information, we can hope
to improve the German performance, and correctly
tag Europa?ischen Rechnungshofes.
The monolingual features are standard features
for discriminative, state-of-the-art entity recogniz-
ers, and we can produce weakened monolingual
models by simply limiting the feature set. The
bilingual features, ?(y1, a, y2), are over pairs of
aligned nodes, where nodes of the labels y1 and
y2 are simply the individual named entities. We
use a small bilingual feature set consisting of two
types of features. First, we use the word alignment
density features from Burkett and Klein (2008),
which measure how well the aligned entity pair
matches up with alignments from an independent
48
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
unannotated bilingual data: U
Output: bilingual parameters: ??, ??1, ??2
1. Label U with full monolingual models:
?x ? U, y?M = argmaxy p1(y1|x1)p2(y2|x2).
2. Return argmax?1,?2,?
Q
x?U q?,?1,?2 (y?M |x),
where q?,?1,?2 has the form in Equation 1.
Figure 3: Bilingual training with multiple views.
word aligner. We also include two indicator fea-
tures: a bias feature that allows the model to learn
a general preference for matched entities, and a
feature that is active whenever the pair of nodes
has the same label. Figure 1 contains sample val-
ues for each of these features.
Another natural setting where bilingual con-
straints can be exploited is syntactic parsing. Fig-
ure 2 shows an example English prepositional
phrase attachment ambiguity that can be resolved
bilingually by exploiting Chinese. The English
monolingual parse mistakenly attaches to to the
verb increased. In Chinese, however, this ambi-
guity does not exist. Instead, the word ?, which
aligns to to, has strong selectional preference for
attaching to a noun on the left.
In our parsing experiments, we use the Berke-
ley parser (Petrov et al, 2006; Petrov and Klein,
2007), a split-merge latent variable parser, for our
monolingual models. Our full model is the re-
sult of training the parser with five split-merge
phases. Our weakened model uses only two. For
the bilingual model, we use the same bilingual fea-
ture set as Burkett and Klein (2008). Table 2 gives
some examples, but does not exhaustively enumer-
ate those features.
5 Training Bilingual Models
Previous work in multiview learning has focused
on the case of agreement regularization (Collins
and Singer, 1999; Ganchev et al, 2008). If we had
bilingual labeled data, together with our unlabeled
data and monolingual labeled data, we could ex-
ploit these techniques. Because we do not possess
bilingual labeled data, we must train the bilingual
model in another way. Here we advocate train-
ing the bilingual model (consisting of the bilin-
gual features and weakened monolingual models)
to imitate the full monolingual models. In terms
of agreement regularization, our procedure may be
thought of as ?regularizing? the bilingual model to
be similar to the full monolingual models.
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
bilingual parameters: ??, ??1, ??2
bilingual input: x = (x1, x2)
Output: bilingual label: y?
Bilingual w/ Weak Bilingual w/ Full
1a. l1 = log
`
pw1 (y1|x1)
?
1b. l1 = log
`
p1(y1|x1)
?
2a. l2 = log
`
pw2 (y2|x2)
?
2b. l2 = log
`
p2(y2|x2)
?
3. Return argmaxy maxa ??1l1 + ??2l2+??
>
?(y1, a, y2)
Figure 4: Prediction by combining monolingual
and bilingual models.
Our training algorithm is summarized in Fig-
ure 3. For each unlabeled point x = (x1, x2), let
y?M be the joint label which has the highest score
from the independent monolingual models (line
1). We then find bilingual parameters ??, ??1, ??2
that maximize q??,??1,??2(y?x|x) (line 2). This max-
likelihood optimization can be solved by an EM-
like procedure (Burkett and Klein, 2008). This
procedure iteratively updates the parameter esti-
mates by (a) finding the optimum alignments for
each candidate label pair under the current pa-
rameters and then (b) updating the parameters to
maximize a modified version of Equation 1, re-
stricted to the optimal alignments. Because we re-
strict alignments to the set of at most one-to-one
matchings, the (a) step is tractable using the Hun-
garian algorithm. With the alignments fixed, the
(b) step just involves maximizing likelihood under
a log-linear model with no latent variables ? this
problem is convex and can be solved efficiently
using gradient-based methods. The procedure has
no guarantees, but is observed in practice to con-
verge to a local optimum.
6 Predicting with Monolingual and
Bilingual Models
Once we have learned the parameters of the bilin-
gual model, the standard method of bilingual pre-
diction would be to just choose the y that is most
likely under q??,??1,??2 :
y? = argmax
y
q??,??1,??2(y|x) . (2)
We refer to prediction under this model as ?Bilin-
gual w/ Weak,? to evoke the fact that the model is
making use of weakened monolingual models in
its feature set.
Given that we have two views of the data,
though, we should be able to leverage additional
information in order to make better predictions. In
49
VB 
NP1 
NP 
VP 
S 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
(a) 
NP PP PP 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
VB 
NP 
NP 
VP1 
S 
NP PP PP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
(b) 
Figure 2: An example of PP attachment that is ambiguous in English, but simple in Chinese. In (a) the
correct parses agree (low PP attachment), whereas in (b) the incorrect parses disagree.
Feature Types Feature Templates
Examples
Correct Incorrect
Alignment Density INSIDEBOTH, INSIDEENONLY INSIDEENONLY=0 INSIDEENONLY=1
Span Difference ABSDIFFERENCE ABSDIFFERENCE=3 ABSDIFFERENCE=4
Syntactic Indicators LABEL?E,C?, NUMCHILDREN?E,C? LABEL?NP,NP?=true LABEL?VP,NP?=true
Table 2: Sample bilingual features used for parsing. The examples are features that would be extracted
by aligning the parents of the PP nodes in Figure 2(a) (Correct) and Figure 2(b) (Incorrect).
particular, the monolingual view uses monolingual
models that are known to be superior to the mono-
lingual information available in the bilingual view.
Thus, we would like to find some way to incorpo-
rate the full monolingual models into our predic-
tion method. One obvious choice is to choose the
labeling that maximizes the ?agreement distribu-
tion? (Collins and Singer, 1999; Ganchev et al,
2008). In our setting, this amounts to choosing:
y? = argmax
y
pM (y|x) q??,??1??2(y|x) . (3)
This is the correct decision rule if the views are
independent and the labels y are uniformly dis-
tributed a priori,1 but we have deliberately in-
troduced between-view dependence in the form
of the weakened monolingual models. Equa-
tion 3 implicitly double-counts monolingual infor-
mation.
One way to avoid this double-counting is to
simply discard the weakened monolingual models
when making a joint prediction:
y? = argmax
y
max
a
pM (y|x)
exp
[
??
>
?(y1, a, y2)
]
.
(4)
1See, e.g. Ando & Zhang(Ando and Zhang, 2007) for a
derivation of the decision rule from Equation 3 under these
assumptions.
This decision rule uniformly combines the two
monolingual models and the bilingual model.
Note, however, that we have already learned non-
uniform weights for the weakened monolingual
models. Our final decision rule uses these weights
as weights for the full monolingual models:
y? = argmax
y
max
a
exp
[
??1 log
(
p1(y1|x1)
)
+
??2 log
(
p2(y2|x2)
)
+??
>
?(y1, a, y2)
]
. (5)
As we will show in Section 10, this rule for com-
bining the monolingual and bilingual views per-
forms significantly better than the alternatives, and
comes close to the optimal weighting for the bilin-
gual and monolingual models.
We will refer to predictions made with Equa-
tion 5 as ?Bilingual w/ Full?, to evoke the use of
the full monolingual models alongside our bilin-
gual features. Prediction using ?Bilingual w/
Weak? and ?Bilingual w/ Full? is summarized in
Figure 4.
7 Retraining Monolingual Models
Although bilingual models have many direct ap-
plications (e.g. in machine translation), we also
wish to be able to apply our models on purely
monolingual data. In this case, we can still take
50
Input: annotated monolingual data: L1, L2
unannotated bilingual data: U
monolingual models: p1(y1|x1), p2(y2|x2)
bilingual parameters: ??, ??1, ??2
Output: retrained monolingual models:
pr1(y1|x1), p
r
2(y2|x2)
?x = (x1, x2) ? U:
Self-Retrained Bilingual-Retrained
1a. y?x1 = argmaxy1 p1(y1|x1) 1b. Pick y?x, Fig. 4
y?x2 = argmaxy2 p2(y2|x2) (Bilingual w/ Full)
2. Add (x1, y?x1 ) to L1 and add (x2, y?x2 ) to L2.
3. Return full monolingual models pr1(y1|x1),
pr2(y2|x2) trained on newly enlarged L1, L2.
Figure 5: Retraining monolingual models.
advantage of parallel corpora by using our bilin-
gual models to generate new training data for the
monolingual models. This can be especially use-
ful when we wish to use our monolingual models
in a domain for which we lack annotated data, but
for which bitexts are plentiful.2
Our retraining procedure is summarized in Fig-
ure 5. Once we have trained our bilingual param-
eters and have a ?Bilingual w/ Full? predictor (us-
ing Equation 5), we can use that predictor to an-
notate a large corpus of parallel data (line 1b). We
then retrain the full monolingual models on a con-
catenation of their original training data and the
newly annotated data (line 3). We refer to the new
monolingual models retrained on the output of the
bilingual models as ?Bilingual-Retrained,? and we
tested such models for both NER and parsing. For
comparison, we also retrained monolingual mod-
els directly on the output of the original full mono-
lingual models, using the same unannotated bilin-
gual corpora for self-training (line 1a). We refer to
these models as ?Self-Retrained?.
We evaluated our retrained monolingual mod-
els on the same test sets as our bilingual mod-
els, but using only monolingual data at test time.
The texts used for retraining overlapped with the
bitexts used for training the bilingual model, but
both sets were disjoint from the test sets.
8 NER Experiments
We demonstrate the utility of multiview learn-
ing for named entity recognition (NER) on En-
glish/German sentence pairs. We built both our
full and weakened monolingual English and Ger-
man models from the CoNLL 2003 shared task
2Of course, unannotated monolingual data is even more
plentiful, but as we will show, with the same amount of data,
our method is more effective than simple monolingual self-
training.
training data. The bilingual model parameters
were trained on 5,000 parallel sentences extracted
from the Europarl corpus. For the retraining
experiments, we added an additional 5,000 sen-
tences, for 10,000 in all. For testing, we used
the Europarl 2006 development set and the 2007
newswire test set. Neither of these data sets were
annotated with named entities, so we manually an-
notated 200 sentences from each of them.
We used the Stanford NER tagger (Finkel et
al., 2005) with its default configuration as our full
monolingual model for each language. We weak-
ened both the English and German models by re-
moving several non-lexical and word-shape fea-
tures. We made one more crucial change to our
monolingual German model. The German entity
recognizer has extremely low recall (44 %) when
out of domain, so we chose y?x from Figure 3 to
be the label in the top five which had the largest
number of named entities.
Table 3 gives results for named entity recogni-
tion. The first two rows are the full and weak-
ened monolingual models alone. The second two
are the multiview trained bilingual models. We
first note that for English, using the full bilin-
gual model yields only slight improvements over
the baseline full monolingual model, and in prac-
tice the predictions were almost identical. For this
problem, the monolingual German model is much
worse than the monolingual English model, and so
the bilingual model doesn?t offer significant im-
provements in English. The bilingual model does
show significant German improvements, however,
including a 16.1% absolute gain in F1 over the
baseline for parliamentary proceedings.
The last two rows of Table 3 give results for
monolingual models which are trained on data that
was automatically labeled using the our models.
English results were again mixed, due to the rel-
atively weak English performance of the bilin-
gual model. For German, though, the ?Bilingual-
Retrained? model improves 14.4% F1 over the
?Self-Retrained? baseline.
9 Parsing Experiments
Our next set of experiments are on syntactic pars-
ing of English and Chinese. We trained both our
full and weakened monolingual English models
on the Penn Wall Street Journal corpus (Marcus
et al, 1993), as described in Section 4. Our full
and weakened Chinese models were trained on
51
Eng Parliament Eng Newswire Ger Parliament Ger Newswire
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Monolingual Models (Baseline)
Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7
Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7
Multiview Trained Bilingual Models
Bilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3
Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9
Retrained Monolingual Models
Self-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6
Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4
Table 3: NER Results. Rows are grouped by data condition. We bold all entries that are best in their
group and beat the strongest monolingual baseline.
Chinese English
Monolingual Models (Baseline)
Weak Monolingual 78.3 67.6
Full Monolingual 84.2 75.4
Multiview Trained Bilingual Models
Bilingual w/ Weak 80.4 70.8
Bilingual w/ Full 85.9 77.5
Supervised Trained Bilingual Models
Burkett and Klein (2008) 86.1 78.2
Retrained Monolingual Models
Self-Retrained 83.6 76.7
Bilingual-Retrained 83.9 77.4
Table 4: Parsing results. Rows are grouped by data
condition. We bold entries that are best in their
group and beat the the Full Monolingual baseline.
the Penn Chinese treebank (Xue et al, 2002) (ar-
ticles 400-1151), excluding the bilingual portion.
The bilingual data consists of the parallel part of
the Chinese treebank (articles 1-270), which also
includes manually parsed English translations of
each Chinese sentence (Bies et al, 2007). Only
the Chinese sentences and their English transla-
tions were used to train the bilingual models ? the
gold trees were ignored. For retraining, we used
the same data, but weighted it to match the sizes
of the original monolingual treebanks. We tested
on the standard Chinese treebank development set,
which also includes English translations.
Table 4 gives results for syntactic parsing. For
comparison, we also show results for the super-
vised bilingual model of Burkett and Klein (2008).
This model uses the same features at prediction
time as the multiview trained ?Bilingual w/ Full?
model, but it is trained on hand-annotated parses.
We first examine the first four rows of Table 4. The
?Bilingual w/ Full? model significantly improves
performance in both English and Chinese relative
to the monolingual baseline. Indeed, it performs
Phrase-Based System
Moses (No Parser) 18.8
Syntactic Systems
Monolingual Parser 18.7
Supervised Bilingual (Treebank Bi-trees) 21.1
Multiview Bilingual (Treebank Bitext) 20.9
Multiview Bilingual (Domain Bitext) 21.2
Table 5: Machine translation results.
only slightly worse than the supervised model.
The last two rows of Table 4 are the results of
monolingual parsers trained on automatically la-
beled data. In general, gains in English, which
is out of domain relative to the Penn Treebank,
are larger than those in Chinese, which is in do-
main. We also emphasize that, unlike our NER
data, this bitext was fairly small relative to the an-
notated monolingual data. Therefore, while we
still learn good bilingual model parameters which
give a sizable agreement-based boost when doing
bilingual prediction, we don?t expect retraining to
result in a coverage-based boost in monolingual
performance.
9.1 Machine Translation Experiments
Although we don?t have hand-labeled data for our
largest Chinese-English parallel corpora, we can
still evaluate our parsing results via our perfor-
mance on a downstream machine translation (MT)
task. Our experimental setup is as follows: first,
we used the first 100,000 sentences of the English-
Chinese bitext from Wang et al (2007) to train
Moses (Koehn et al, 2007), a phrase-based MT
system that we use as a baseline. We then used the
same sentences to extract tree-to-string transducer
rules from target-side (English) trees (Galley et al,
2004). We compare the single-reference BLEU
scores of syntactic MT systems that result from
using different parsers to generate these trees.
52
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 
68-71 65-68 62-65 59-62 56-59 
English Weight 
German
 Weigh
t 
German F1 
70.3 70.1 59.1 
* + * + 
(a) 
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 
81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 
English Weight 
Chines
e Weig
ht 
Combined F1 
82.1 82.0 81.4 
* + ? 
* + 
? 
(b) 
Figure 6: (a) NER and (b) parsing results for different values of ?1 and ?2 (see Equation 6). ?*? shows
optimal weights, ?+? shows our learned weights, and ?-? shows uniform combination weights.
For our syntactic baseline, we used the mono-
lingual English parser. For our remaining experi-
ments, we parsed both English and Chinese simul-
taneously. The supervised model and the first mul-
tiview trained model are the same Chinese tree-
bank trained models for which we reported pars-
ing results. We also used our multiview method to
train an additional bilingual model on part of the
bitext we used to extract translation rules.
The results are shown in Table 5. Once again,
our multiview trained model yields comparable re-
sults to the supervised model. Furthermore, while
the differences are small, our best performance
comes from the model trained on in-domain data,
for which no gold trees exist.
10 Analyzing Combined Prediction
In this section, we explore combinations of the full
monolingual models, p1(y1|x1) and p2(y2|x2),
and the bilingual model, max
a
??
>
?(y1, a, y2). For
parsing, the results in this section are for combined
F1. This simply computes F1 over all of the sen-
tences in both the English and Chinese test sets.
For NER, we just use German F1, since English is
relatively constant across runs.
We begin by examining how poorly our model
performs if we do not consider monolingual in-
formation in the bilingual view. For parsing, the
combined Chinese and English F1 for this model
is 78.7%. When we combine this model uniformly
with the full monolingual model, as in Equation 4,
combined F1 improves to 81.2%, but is still well
below our best combined score of 82.1%. NER
results for a model trained without monolingual
information show an even larger decline.
Now let us consider decision rules of the form:
y? = argmax
y
max
a
exp[?1 log
`
p1(y1|x1)
?
+
?2 log
`
p2(y2|x2)
?
+??
>
?(y1, a, y2)] .
Note that when ?1 = ?2 = 1, this is exactly
the uniform decision rule (Equation 4). When
?1 = ??1 and ?2 = ??2, this is the ?Bilingual w/
Full? decision rule (Equation 5). Figure 6 is a
contour plot of F1 with respect to the parameters
?1 and ?2. Our decision rule ?Bilingual w/ Full?
(Equation 5, marked with a ?+?) is near the opti-
mum (?*?), while the uniform decision rule (?-?)
performs quite poorly. This is true for both NER
(Figure 6a) and parsing (Figure 6b).
There is one more decision rule which we have
yet to consider: the ?conditional independence?
decision rule from Equation 3. While this rule can-
not be shown on the plots in Figure 6 (because
it uses both the full and weakened monolingual
models), we note that it also performs poorly in
both cases (80.7% F1 for parsing, for example).
11 Conclusions
We show for the first time that state-of-the-art,
discriminative monolingual models can be signifi-
cantly improved using unannotated bilingual text.
We do this by first building bilingual models that
are trained to agree with pairs of independently-
trained monolingual models. Then we combine
the bilingual and monolingual models to account
for dependence across views. By automatically
annotating unlabeled bitexts with these bilingual
models, we can train new monolingual models that
do not rely on bilingual data at test time, but still
perform substantially better than models trained
using only monolingual resources.
Acknowledgements
This project is funded in part by NSF grants
0915265 and 0643742, an NSF graduate research
fellowship, the DNI under grant HM1582-09-1-
0021, and BBN under DARPA contract HR0011-
06-C-0022.
53
References
Rie Kubota Ando and Tong Zhang. 2007. Two-view
feature generation model for semi-supervised learn-
ing. In ICML.
Maria-Florina Balcan and Avrim Blum. 2005. A pac-
style model for learning from labeled and unlabeled
data. In COLT.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v 1.0. Web download. LDC2007T02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In NIPS.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-view learning over structured
and non-identical outputs. In UAI.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Special Issue of the Journal of Natural Language
Engineering on Parallel Texts, 11(3):311?325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Robert Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In COLING-ACL.
Luo Si and Jamie Callan. 2005. Clef 2005: Multi-
lingual retrieval by combining multiple multilingual
ranked lists. In CLEF.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: using english to
parse korean. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
AAAI.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In IEEE
ASRU Workshop.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In NAACL.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Human Language Technologies.
54
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 102?106,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Mention Detection: Heuristics for the OntoNotes annotations
Jonathan K. Kummerfeld, Mohit Bansal, David Burkett and Dan Klein
Computer Science Division
University of California at Berkeley
{jkk,mbansal,dburkett,klein}@cs.berkeley.edu
Abstract
Our submission was a reduced version of
the system described in Haghighi and Klein
(2010), with extensions to improve mention
detection to suit the OntoNotes annotation
scheme. Including exact matching mention
detection in this shared task added a new and
challenging dimension to the problem, partic-
ularly for our system, which previously used
a very permissive detection method. We im-
proved this aspect of the system by adding
filters based on the annotation scheme for
OntoNotes and analysis of system behavior on
the development set. These changes led to im-
provements in coreference F-score of 10.06,
5.71, 6.78, 6.63 and 3.09 on the MUC, B3,
Ceaf-e, Ceaf-m and Blanc, metrics, respec-
tively, and a final task score of 47.10.
1 Introduction
Coreference resolution is concerned with identifying
mentions of entities in text and determining which
mentions are referring to the same entity. Previously
the focus in the field has been on the latter task.
Typically, mentions were considered correct if their
span was within the true span of a gold mention, and
contained the head word. This task (Pradhan et al,
2011) has set a harder challenge by only considering
exact matches to be correct.
Our system uses an unsupervised approach based
on a generative model. Unlike previous work, we
did not use the Bllip or Wikipedia data described in
Haghighi and Klein (2010). This was necessary for
the system to be eligible for the closed task.
The system detects mentions by finding the max-
imal projection of every noun and pronoun. For the
OntoNotes corpus this approach posed several prob-
lems. First, the annotation scheme explicitly rejects
noun phrases in certain constructions. And second,
it includes coreference for events as well as things.
In preliminary experiments on the development set,
we found that spurious mentions were our primary
source of error. Using an oracle to exclude all spu-
rious mentions at evaluation time yielded improve-
ments ranging from five to thirty percent across the
various metrics used in this task. Thus, we decided
to focus our efforts on methods for detecting and fil-
tering spurious mentions.
To improve mention detection, we filtered men-
tions both before and after coreference resolution.
Filters prior to coreference resolution were con-
structed based on the annotation scheme and partic-
ular cases that should never be mentions (e.g. single
word spans with the EX tag). Filters after corefer-
ence resolution were constructed based on analysis
of common errors on the development set.
These changes led to considerable improvement
in mention detection precision. The heuristics used
in post-resolution filtering had a significant negative
impact on recall, but this cost was out-weighed by
the improvements in precision. Overall, the use of
these filters led to a significant improvement in F1
across all the coreference resolution evaluation met-
rics considered in the task.
2 Core System
We use a generative approach that is mainly un-
supervised, as described in detail in Haghighi and
102
Klein (2010), and briefly below.
2.1 Model
The system uses all three of the standard abstrac-
tions in coreference resolution; mentions, entities
and types. A mention is a span in the text, the en-
tity is the actual object or event the mention refers
to, and each type is a group of entities. For example,
?the Mountain View based search giant? is a men-
tion that refers to the entity Google, which is of type
organization.
At each level we define a set of properties (e.g.
proper-head). For mentions, these properties are
linked directly to words from the span. For enti-
ties, each property corresponds to a list of words,
instances of which are seen in specific mentions of
that entity. At the type level, we assign a pair of
multinomials to each property. The first of these
multinomials is a distribution over words, reflecting
their occurrence for this property for entities of this
type. The second is a distribution over non-negative
integers, representing the length of word lists for this
property in entities of this type.
The only form of supervision used in the system
is at the type level. The set of types is defined and
lists of prototype words for each property of each
type are provided. We also include a small number
of extra types with no prototype words, for entities
that do not fit well in any of the specified types.
These abstractions are used to form a generative
model with three components; a semantic module, a
discourse module and a mention module. In addi-
tion to the properties and corresponding parameters
described above, the model is specified by a multi-
nomial prior over types (?), log-linear parameters
over discourse choices (pi), and a small number of
hyperparameters (?).
Entities are generated by the semantic module by
drawing a type t according to ?, and then using that
type?s multinomials to populate word lists for each
property.
The assignment of entities to mentions is handled
by the discourse module. Affinities between men-
tions are defined by a log-linear model with param-
eters pi for a range of standard features.
Finally, the mention module generates the ac-
tual words in the span. Words are drawn for each
property from the lists for the relevant entity, with
a hyper-parameter for interpolation between a uni-
form distribution over the words for the entity and
the underlying distribution for the type. This allows
the model to capture the fact that some properties
use words that are very specific to the entity (e.g.
proper names) while others are not at all specific
(e.g. pronouns).
2.2 Learning and Inference
The learning procedure finds parameters that are
likely under the model?s posterior distribution. This
is achieved with a variational approximation that
factors over the parameters of the model. Each set
of parameters is optimized in turn, while the rest are
held fixed. The specific update methods vary for
each set of parameters; for details see Section 4 of
Haghighi and Klein (2010).
3 Mention detection extensions
The system described in Haghighi and Klein (2010)
includes every NP span as a mention. When run on
the OntoNotes data this leads to a large number of
spurious mentions, even when ignoring singletons.
One challenge when working with the OntoNotes
data is that singleton mentions are not annotated.
This makes it difficult to untangle errors in coref-
erence resolution and errors in mention detection. A
mention produced by the system might not be in the
gold set for one of two reasons; either because it is
a spurious mention, or because it is not co-referent.
Without manually annotating the singletons in the
data, these two cases cannot be easily separated.
3.1 Baseline mention detection
The standard approach used in the system to detect
mentions is to consider each word and its maximal
projection, accepting it only if the span is an NP or
the word is a pronoun. This approach will intro-
duce spurious mentions if the parser makes a mis-
take, or if the NP is not considered a mention in the
OntoNotes corpus. In this work, we considered the
provided parses and parses produced by the Berke-
ley parser (Petrov et al, 2006) trained on the pro-
vided training data. We added a set of filters based
on the annotation scheme described by Pradhan et al
(2007). Some filters are applied before coreference
resolution and others afterward, as described below.
103
Data Set Filters P R F
Dev
None 37.59 76.93 50.50
Pre 39.49 76.83 52.17
Post 59.05 68.08 63.24
All 58.69 67.98 63.00
Test All 56.97 69.77 62.72
Table 1: Mention detection performance with various
subsets of the filters.
3.2 Before Coreference Resolution
The pre-resolution filters were based on three reli-
able features of spurious mentions:
? Appositive constructions
? Attributes signaled by copular verbs
? Single word mentions with a POS tag in the set:
EX, IN, WRB, WP
To detect appositive constructions we searched
for the following pattern:
NP
NP , NP . . .
And to detect attributes signaled by copular struc-
tures we searched for this pattern:
VP
cop verb NP
where we used the fairly conservative set of cop-
ular verbs: {is, are, was, ?m}. In both
cases, any mention whose maximal NP projection
appeared as the bold node in a subtree matching the
pattern was excluded.
In all three cases, errors from the parser (or POS
tagger) may lead to the deletion of valid mentions.
However, we found the impact of this was small and
was outweighed by the number of spurious mentions
removed.
3.3 After Coreference Resolution
To construct the post-coreference filters we analyzed
system output on the development set, and tuned
Filters MUC B3 Ceaf-e Blanc
None 25.24 45.89 50.32 59.12
Pre 27.06 47.71 50.15 60.17
Post 42.08 62.53 43.88 66.54
All 42.03 62.42 43.56 66.60
Table 2: Precision for coreference resolution on the dev
set.
Filters MUC B3 Ceaf-e Blanc
None 50.54 78.54 26.17 62.77
Pre 51.20 77.73 27.23 62.97
Post 45.93 64.72 39.84 61.20
All 46.21 64.96 39.24 61.28
Table 3: Recall for coreference resolution on the dev set.
based on MUC and B3 performance. The final set
of filters used were:
? Filter if the head word is in a gazetteer, which
we constructed based on behavior on the devel-
opment set (head words found using the Collins
(1999) rules)
? Filter if the POS tag is one of WDT, NNS, RB,
JJ, ADJP
? Filter if the mention is a specific case of you
or it that is more often generic (you know,
you can, it is)
? Filter if the mention is any cardinal other than
a year
A few other more specific filters were also in-
cluded (e.g. ?s when tagged as PRP) and one type
of exception (if all words are capitalized, the men-
tion is kept).
4 Other modifications
The parses in the OntoNotes data include the addi-
tion of structure within noun phrases. Our system
was not designed to handle the NML tag, so we
removed such nodes, reverting to the standard flat-
tened NP structures found in the Penn Treebank.
We also trained the Berkeley parser on the pro-
vided training data, and used it to label the develop-
ment and test sets.1 We found that performance was
1In a small number of cases, the Berkeley parser failed, and
we used the provided parse tree instead.
104
Filters MUC B3 Ceaf-e Ceaf-m Blanc
None 33.67 57.93 34.43 42.72 60.60
Pre 35.40 59.13 35.29 43.72 61.38
Post 43.92 63.61 41.76 49.74 63.26
All 44.02 63.66 41.29 49.46 63.34
Table 4: F1 scores for coreference resolution on the dev
set.
slightly improved by the use of these parses instead
of the provided parses.
5 Results
Since our focus when extending our system for this
task was on mention detection, we present results
with variations in the sets of mention filters used. In
particular, we have included results for our baseline
system (None), when only the filters before coref-
erence resolution are used (Pre), when only the fil-
ters after coreference resolution are used (Post), and
when all filters are used (All).
The main approach behind the pre-coreference fil-
ters was to consider the parse to catch cases that are
almost never mentions. In particular, these filters
target cases that are explicitly excluded by the an-
notation scheme. As Table 1 shows, this led to a
1.90% increase in mention detection precision and
0.13% decrease in recall, which is probably a result
of parse errors.
For the post-coreference filters, the approach was
quite different. Each filter was introduced based on
analysis of the errors in the mention sets produced
by our system on the development set. Most of the
filters constructed in this way catch some true men-
tions as well as spurious mentions, leading to signif-
icant improvements in precision at the cost of recall.
Specifically an increase of 21.46% in precision and
decrease of 8.85% in recall, but an overall increase
of 12.74% in F1-score.
As Tables 2 and 3 show, these changes in mention
detection performance generally lead to improve-
ments in precision at the expense of recall, with the
exception of Ceaf-e where the trends are reversed.
However, as shown in Table 4, there is an overall
improvement in F1 in all cases.
In general the change from only post-coreference
filters to all filters is slightly negative. The final sys-
Metric R P F1
MUC 46.39 39.56 42.70
B3 63.60 57.30 60.29
Ceaf-m 45.35 45.35 45.35
Ceaf-e 35.05 42.26 38.32
Blanc 58.74 61.58 59.91
Table 5: Complete results on the test set
tem used all of the filters because the process used to
create the post-coreference filters was more suscep-
tible to over-fitting, and the pre-coreference filters
provided such an unambiguously positive contribu-
tion to mention detection.
6 Conclusion
We modified the coreference system of Haghighi
and Klein (2010) to improve mention detection per-
formance. We focused on tuning using the MUC and
B3 metrics, but found considerable improvements
across all metrics.
One important difference between the system de-
scribed here and previous work was the data avail-
able. Unlike Haghighi and Klein (2010), no extra
data from Wikipedia or Bllip was used, a restriction
that was necessary to be eligible for the closed part
of the task.
By implementing heuristics based on the annota-
tion scheme for the OntoNotes data set and our own
analysis of system behavior on the development set
we were able to achieve the results shown in Table 5,
giving a final task score of 47.10.
7 Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
ported by the Office of Naval Research under MURI
Grant No. N000140911081, and a General Sir John
Monash Fellowship.
References
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
105
ings of NAACL, pages 385?393, Los Angeles, Califor-
nia, June. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
ontonotes. In Proceedings of the International Confer-
ence on Semantic Computing, pages 446?453, Wash-
ington, DC, USA. IEEE Computer Society.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
106
