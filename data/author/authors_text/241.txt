Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 506?513, Prague, June 2007. c?2007 Association for Computational Linguistics
Methods to integrate a language model with semantic  
information for a word prediction component 
Tonio Wandmacher 
Laboratoire d?Informatique (LI) 
Universit? Fran?ois Rabelais de Tours 
3 place Jean-Jaur?s, 41000 Blois, France 
tonio.wandmacher@ 
univ-tours.fr 
Jean-Yves Antoine 
Laboratoire d?Informatique (LI) 
Universit? Fran?ois Rabelais de Tours 
3 place Jean-Jaur?s, 41000 Blois, France 
jean-yves.antoine@ 
univ-tours.fr 
 
Abstract 
Most current word prediction systems make 
use of n-gram language models (LM) to es-
timate the probability of the following word 
in a phrase. In the past years there have 
been many attempts to enrich such lan-
guage models with further syntactic or se-
mantic information. We want to explore the 
predictive powers of Latent Semantic 
Analysis (LSA), a method that has been 
shown to provide reliable information on 
long-distance semantic dependencies be-
tween words in a context. We present and 
evaluate here several methods that integrate 
LSA-based information with a standard 
language model: a semantic cache, partial 
reranking, and different forms of interpola-
tion. We found that all methods show sig-
nificant improvements, compared to the 4-
gram baseline, and most of them to a sim-
ple cache model as well. 
1 Introduction: NLP for AAC systems 
Augmented and Alternative Communication 
(AAC) is a field of research which concerns natural 
language processing as well as human-machine 
interaction, and which aims at restoring the com-
municative abilities of disabled people with severe 
speech and motion impairments. These people can 
be for instance cerebrally and physically handi-
capped persons or they suffer from a locked-in 
syndrome due to a cerebral apoplexy. Whatever the 
disease or impairment considered, oral communica-
tion is impossible for these persons who have in 
addition serious difficulties to control physically 
their environment. In particular, they are not able to 
use standard input devices of a computer. Most of 
the time, they can only handle a single switch de-
vice. As a result, communicating with an AAC sys-
tem consists of typing messages by means of a vir-
tual table of symbols (words, letters or icons) 
where the user successively selects the desired 
items. 
Basically, an AAC system, such as FASTY 
(Trost et al 2005) or SIBYLLE (Schadle et al 2004), 
consists of four components. At first, one finds a 
physical input interface connected to the computer. 
This device is adapted to the motion capacities of 
the user. When the latter must be restricted to a 
single switch (eye glimpse or breath detector, for 
instance), the control of the environment is reduced 
to a mere Yes/No command.  
Secondly, a virtual keyboard is displayed on 
screen. It allows the user to select successively the 
symbols that compose the intended message. In 
SIBYLLE, key selection is achieved by pointing let-
ters through a linear scan procedure: a cursor suc-
cessively highlights each key of the keyboard.  
The last two components are a text editor (to 
write e-mails or other documents) and a speech 
synthesis module, which is used in case of spoken 
communication. The latest version of SIBYLLE 
works for French and German, and it is usable with 
any Windows? application (text editor, web 
browser, mailer...), which means that the use of a 
specific editor is no longer necessary.  
The main weakness of AAC systems results from 
the slowness of message composition. On average, 
disabled people cannot type more than 1 to 5 words 
per minute; moreover, this task is very tiring. The 
use of NLP techniques to improve AAC systems is 
therefore of first importance. 
 
506
  
Figure 1: User interface of the SIBYLLE AAC system  
 
 
Two complementary approaches are possible to 
speed up communication. The first one aims at 
minimizing the duration of each item selection. 
Considering a linear scan procedure, one could for 
instance dynamically reorganize the keyboard in 
order to present the most probable symbols at first. 
The second strategy tries to minimize the number 
of keystrokes to be made. Here, the system tries to 
predict the words which are likely to occur just af-
ter those already typed. The predicted word is then 
either directly displayed after the end of the in-
serted text (a method referred to as ?word comple-
tion?, cf. Boissi?re and Dours, 1996), or a list of N-
best (typically 3 to 7) predictions is provided on the 
virtual keyboard. When one of these predictions 
corresponds to the intended word, it can be selected 
by the user. As can be seen in figure 1, the interface 
of the SIBYLLE system presents such a list of most 
probable words to the user. 
Several approaches can be used to carry out 
word prediction. Most of the commercial AAC sys-
tems make only use of a simple lexicon: in this ap-
proach, the context is not considered. 
On the other hand, stochastic language models 
can provide a list of word suggestions, depending 
on the n-1 (typically n = 3 or 4) last inserted words. 
It is obvious that such a model cannot take into ac-
count long-distance dependencies. There have been 
attempts to integrate part-of-speech information 
(Fazly and Hirst, 2003) or more complex syntactic 
models (Schadle et al 2004) to achieve a better 
prediction. In this paper, we will nevertheless limit 
our study to a standard 4-gram model as a baseline 
to make our results comparable. Our main aim is 
here to investigate the use of long-distance seman-
tic dependencies to dynamically adapt the predic-
tion to the current semantic context of communica-
tion. Similar work has been done by Li and Hirst 
(2005) and Matiasek and Baroni (2003), who ex-
ploit Pointwise Mutual Information (PMI; Church 
and Hanks, 1989). Trnka et al (2005) dynamically 
interpolate a high number of topic-oriented models 
in order to adapt their predictions to the current 
topic of the text or conversation. 
Classically, word predictors are evaluated by an 
objective metric called Keystroke Saving Rate 
(ksr): 
1001 ????
?
???
?
?=
a
p
n k
k
ksr  (1) 
 
with kp, ka being the number of keystrokes 
needed on the input device when typing a message 
with (kp) and without prediction (ka = number of 
characters in the text that has been entered, n = 
length of the prediction list, usually n = 5). As 
507
Trost et al (2005) and Trnka et al (2005), we as-
sume that one additional keystroke is required for 
the selection of a word from the list and that a 
space is automatically inserted afterwards. Note 
also that words, which have already occurred in the 
list, will not reappear after the next character has 
been inserted.  
The perplexity measure, which is frequently 
used to assess statistical language models, proved 
to be less accurate in this context. We still present 
perplexities as well in order to provide comparative 
results. 
2 Language modeling and semantics 
2.1 Statistical Language Models 
For about 10 to 15 years statistical language model-
ing has had a remarkable success in various NLP 
domains, for instance in speech recognition, ma-
chine translation, Part-of-Speech tagging, but also 
in word prediction systems. N-gram based lan-
guage models (LM) estimate the probability of oc-
currence for a word, given a string of n-1 preceding 
words. However, computers have only recently 
become powerful enough to estimate probabilities 
on a reasonable  amount of training data. More-
over, the larger n gets, the more important the prob-
lem of combinatorial explosion for the probability 
estimation becomes. A reasonable trade-off be-
tween performance and number of estimated events 
seems therefore to be an n of 3 to 5, including so-
phisticated techniques in order to estimate the 
probability of unseen events (smoothing methods). 
Whereas n-gram-like language models are al-
ready performing rather well in many applications, 
their capacities are also very limited in that they 
cannot exploit any deeper linguistic structure. 
Long-distance syntactic relationships are neglected 
as well as semantic or thematic constraints. 
In the past 15 years many attempts have been 
made to enrich language models with more com-
plex syntactic and semantic models, with varying 
success (cf. (Rosenfeld, 1996), (Goodman, 2002) 
or in a word prediction task: (Fazly and Hirst, 
2003), (Schadle, 2004), (Li and Hirst, 2005)). We 
want to explore here an approach based on Latent 
Semantic Analysis (Deerwester et al 1990). 
2.2 Latent Semantic Analysis 
Several works have suggested the use of Latent 
Semantic Analysis (LSA) in order to integrate se-
mantic similarity to a language model (cf. Belle-
garda, 1997; Coccaro and Jurafsky, 1998). LSA 
models semantic similarity based on co-occurrence 
distributions of words, and it has shown to be help-
ful in a variety of NLP tasks, but also in the domain 
of cognitive modeling (Landauer et al 1997). 
LSA is able to relate coherent contexts to spe-
cific content words, and it is good at predicting the 
occurrence of a content word in the presence of 
other thematically related terms. However, since it 
does not take word order into account (?bag-of-
words? model) it is very poor at predicting their 
actual position within the sentence, and it is com-
pletely useless for the prediction of function words. 
Therefore, some attempts have been made to inte-
grate the information coming from an LSA-based 
model with standard language models of the n-
gram type.  
In the LSA model (Deerwester et al 1990) a 
word wi is represented as a high-dimensional vec-
tor, derived by Singular Value Decomposition 
(SVD) from a term ? document (or a term ? term) 
co-occurrence matrix of a training corpus. In this 
framework, a context or history h (= w1, ... , wm) 
can be represented by the sum of the (already nor-
malized) vectors corresponding to the words it con-
tains (Landauer et al 1997):  
 
?
=
=
m
i
iwh
1
rr
 (2) 
 
This vector reflects the meaning of the preceding 
(already typed) section, and it has the same dimen-
sionality as the term vectors. It can thus be com-
pared to the term vectors by well-known similarity 
measures (scalar product, cosine).  
2.3 Transforming LSA similarities into prob-
abilities 
We make the assumption that an utterance or a 
text to be entered is usually semantically cohesive. 
We then expect all word vectors to be close to the 
current context vector, whose corresponding words 
belong to the semantic field of the context. This 
forms the basis for a simple probabilistic model of 
LSA: After calculating the cosine similarity for 
each word vector iw
r
 with the vector h
r
 of the cur-
rent context, we could use the normalized similari-
ties as probability values. This probability distribu-
tion however is usually rather flat (i.e. the dynamic 
508
range is low). For this reason a contrasting (or tem-
perature) factor ? is normally applied (cf. Coccaro 
and Jurafsky, 1998), which raises the cosine to 
some power (? is normally between 3 and 8). After 
normalization we obtain a probability distribution 
which can be used for prediction purposes. It is 
calculated as follows: 
 
( )
( )? ?
?
=
k
?
k
?
i
iLSA
hhw
hhwhwP
)(cos),cos(
)(cos),cos()(
min
min rrr
rrr
  (3) 
 
wi is a word in the vocabulary, h is the current con-
text (history) 
iw
r
and h
r
are their corresponding vec-
tors in the LSA space; cosmin( h
r
) returns the lowest 
cosine value measured for h
r
). The denominator 
then normalizes each similarity value to ensure that 
? =nk kLSA hwP 1),( . 
Let us illustrate the capacities of this model by 
giving a short example from the French version of 
our own LSA predictor: 
 
Context: ?Mon p?re ?tait professeur en math?matiques 
et je pense que ? 
 (?My dad has been a professor in mathemat-
ics and I think that ?) 
 
Rank Word P 
1. professeur (?professor?) 0.0117 
2. math?matiques (?mathematics?) 0.0109 
3. enseign? (participle of ?taught?) 0.0083 
4. enseignait (?taught?) 0.0053 
5. mathematicien (?mathematician?) 0.0049 
6. p?re (?father?) 0.0046 
7. math?matique (?mathematics?) 0.0045 
8. grand-p?re (?grand-father?) 0.0043 
9. sciences (?sciences?) 0.0036 
10. enseignant (?teacher?) 0.0032 
Example 1: Most probable words returned by the 
LSA model for the given context. 
 
As can be seen in example 1, all ten predicted 
words are semantically related to the context, they 
should therefore be given a high probability of oc-
currence. However, this example also shows the 
drawbacks of the LSA model: it totally neglects the 
presence of function words as well as the syntactic 
structure of the current phrase. We therefore need 
to find an appropriate way to integrate the informa-
tion coming from a standard n-gram model and the 
LSA approach. 
2.4 Density as a confidence measure 
Measuring relation quality in an LSA space, 
Wandmacher (2005) pointed out that the reliability 
of LSA relations varies strongly between terms. He 
also showed that the entropy of a term does not 
correlate with relation quality (i.e. number of se-
mantically related terms in an LSA-generated term 
cluster), but he found a medium correlation (Pear-
son coeff. = 0.56) between the number of semanti-
cally related terms and the average cosine similar-
ity of the m nearest neighbors (density). The closer 
the nearest neighbors of a term vector are, the more 
probable it is to find semantically related terms for 
the given word. In turn, terms having a high density 
are more likely to be semantically related to a given 
context (i.e. their specificity is higher). 
We define the density of a term wi as follows: 
 
  ?
=
?=
m
j
ijiim wNNw
m
wD
1
))(,cos(1)( rr  (4) 
 
In the following we will use this measure (with 
m=100) as a confidence metric to estimate the reli-
ability of a word being predicted by the LSA com-
ponent, since it showed to give slightly better re-
sults in our experiments than the entropy measure.  
3 Integrating semantic information 
In the following we present several different meth-
ods to integrate semantic information as it is pro-
vided by an LSA model into a standard LM. 
3.1 Semantic cache model 
Cache (or recency promotion) models have shown 
to bring slight but constant gains in language mod-
eling (Kuhn and De Mori, 1990). The underlying 
idea is that words that have already occurred in a 
text are more likely to occur another time. There-
fore their probability is raised by a constant or ex-
ponentially decaying factor, depending on the posi-
tion of the element in the cache. The idea of a de-
caying cache function is that the probability of re-
occurrence depends on the cosine similarity of the 
word in the cache and the word to be predicted. 
The highest probability of reoccurrence is usually 
after 15 to 20 words. 
Similar to Clarkson and Robinson (1997), we im-
plemented an exponentially decaying cache of 
length l (usually between 100 and 1000), using the 
509
following decay function for a word wi and its posi-
tion p in the cache. 
 
2)(5,0
),( ??
???
? ??
=
?
?p
id epwf  (5) 
 
? = ?/3 if p < ? and  ? = l/3 if p ? ?. The func-
tion returns 0 if wi is not in the cache, and it is 1 if 
p = ?. A typical graph for (5) can be seen in figure 
(2). 
 
 
Figure 2: Decay function with ?=20 and l=300. 
 
We extend this model by calculating for each ele-
ment having occurred in the context its m nearest 
LSA neighbors ( ),( ?wNN
occm
r
, using cosine simi-
larity), if their cosine lies above a threshold ?, and 
add them to the cache as well, right after the word 
that has occurred in the text (?Bring your friends?-
strategy). The size of the cache is adapted accord-
ingly (for ?, ? and l), depending on the number of 
neighbors added. This results in the following 
cache function: 
 
),(),()(
1 cos
pwfwwf?wP id
l
i
i
occicache ? ??=    (6) 
 
with l = size of the cache. ? is a constant  con-
trolling the influence of the component (usually ? ? 
0.1/l); wiocc is a word that has already recently oc-
curred in the context and is therefore added as a 
standard cache element, whereas wi is a nearest 
neighbor to wiocc. fcos(wiocc, wi) returns the cosine 
similarity between i
occ
w
r
 and iw
r
, with cos( i
occ
w
r
, iw
r ) 
> ? (Rem: wi with cos( ioccw
r
, iw
r ) ? ? have not been 
added to the cache).  Since cos( iw
r
, iw
r )=1, terms 
having actually occurred before will be given full 
weight, whereas all wi being only nearest LSA 
neighbors to wiocc will receive a weight correspond-
ing to their cosine similarity with wiocc , which is 
less than 1 (but larger than ?). 
fd(wi,p) is the decay factor for the current posi-
tion p of wi in the cache, calculated as shown in 
equation (5).  
3.2 Partial reranking 
The underlying idea of partial reranking is to re-
gard only the best n candidates from the basic lan-
guage model for the semantic model in order to 
prevent the LSA model from making totally im-
plausible (i.e. improbable) predictions. Words be-
ing improbable for a given context will be disre-
garded as well as words that do not occur in the 
semantic model (e.g. function words), because LSA 
is not able to give correct estimates for this group 
of words (here the base probability remains un-
changed). 
For the best n candidates their semantic probability 
is calculated and each of these words is assigned an 
additional value, after a fraction of its base prob-
ability has been subtracted (jackpot strategy). 
For a given context h we calculate the ordered set 
BESTn(h) = <w1, ? , wn>, so that P(w1|h) ? 
P(w2|h) ???P(wn|h) 
For each wi in BESTn(h) we then calculate its 
reranking probability as follows: 
 
)),(()(),cos()( iniiiRR whBestIwDhw?wP ???=
rr
 (7) 
 
? is a weighting constant controlling the overall 
influence of the reranking process, cos( iw
r
, iw
r ) re-
turns the cosine of the word?s vector and the cur-
rent context vector, D(wi) gives the confidence 
measure of wi and I is an indicator function being 
1, iff wi ?BEST(h), and 0 otherwise.  
3.3 Standard interpolation 
Interpolation is the standard way to integrate in-
formation from heterogeneous resources. While for 
a linear combination we simply add the weighted 
probabilities of two (or more) models, geometric 
interpolation multiplies the probabilities, which are 
weighted by an exponential coefficient (0??1?1): 
 
Linear Interpolation (LI): 
 
)()1()()(' 11 isibi wP?wP?wP ??+?=    (8) 
 
510
Geometric Interpolation (GI): 
 
?
=
?
?
?
?
= n
j
?
js
?
jb
?
is
?
ib
i
wPwP
wPwP
wP
1
)11(1
)11(1
)()(
)()()('     (9) 
 
The main difference between the two methods is 
that the latter takes the agreement of two models 
into account. Only if each of the single models as-
signs a high probability to a given event will the 
combined probability be assigned a high value. If 
one of the models assigns a high probability and 
the other does not the resulting probability will be 
lower. 
3.4 Confidence-weighted interpolation 
Whereas in standard settings the coefficients are 
stable for all probabilities, some approaches use 
confidence-weighted coefficients that are adapted 
for each probability. In order to integrate n-gram 
and LSA probabilities, Coccaro and Jurafsky 
(1998) proposed an entropy-related confidence 
measure for the LSA component, based on the ob-
servation that words that occur in many different 
contexts (i.e. have a high entropy), cannot well be 
predicted by LSA. We use here a density-based 
measure (cf. section 2.2), because we found it more 
reliable than entropy in preliminary tests. For inter-
polation purposes we calculate the coefficient of 
the LSA component as follows: 
 
)( ii wD?? ?= , iff D(wi) > 0; 0 otherwise (10) 
 
with ? being a weighting constant to control the 
influence of the LSA predictor. For all experi-
ments, we set ? to 0.4 (i.e. 0 ? ?i ? 0.4), which 
proved to be optimal in pre-tests. 
4 Results 
We calculated our baseline n-gram model on a 44 
million word corpus from the French daily Le 
Monde (1998-1999). Using the SRI toolkit (Stol-
cke, 2002)1 we computed a 4-gram LM over a con-
trolled 141,000 word vocabulary, using modified 
Kneser-Ney discounting (Goodman, 2001), and we 
applied Stolcke pruning (Stolcke, 1998) to reduce 
the model to a manageable size (? = 10-7). 
                                                 
1
 SRI Toolkit: www.speech.sri.com. 
The LSA space was calculated on a 100 million 
word corpus from Le Monde (1996 ? 2002). Using 
the Infomap toolkit2, we generated a term ? term 
co-occurrence matrix for an 80,000 word vocabu-
lary (matrix size = 80,000 ? 3,000), stopwords 
were excluded. After several pre-tests, we set the 
size of the co-occurrence window to ?100. The ma-
trix was then reduced by singular value decomposi-
tion to 150 columns, so that each word in the vo-
cabulary was represented by a vector of 150 di-
mensions, which was normalized to speed up simi-
larity calculations (the scalar product of two nor-
malized vectors equals the cosine of their angle).  
Our test corpus consisted of 8 sections from the 
French newspaper Humanit?, (January 1999, from 
5,378 to 8,750 words each), summing up to 58,457 
words. We then calculated for each test set the key-
stroke saving rate based on a 5-word list (ksr5) and 
perplexity for the following settings3: 
1. 4-gram LM only (baseline) 
2. 4-gram + decaying cache (l = 400) 
3. 4-gram + LSA using linear interpolation 
with ?LSA = 0.11 (LI). 
4. 4-gram + LSA using geometric interpola-
tion, with ?LSA = 0.07 (GI). 
5. 4-gram + LSA using linear interpolation 
and (density-based) confidence weighting 
(CWLI). 
6. 4-gram + LSA using geometric interpola-
tion and (density-based) confidence 
weighting (CWGI). 
7. 4-gram + partial reranking (n = 1000, ? = 
0.001) 
8. 4-gram + decaying semantic cache  
(l = 4000; m = 10; ? = 0.4, ? = 0.0001)  
Figures 3 and 4 display the overall results in terms 
of ksr and perplexity.  
                                                 
2
 Infomap Project: http://infomap-nlp.sourceforge.net/ 
3
 All parameter settings presented here are based on results of 
extended empirical pre-tests. We used held-out development 
data sets that have randomly been chosen from the Humanit? 
corpus.(8k to 10k words each). The parameters being pre-
sented here were optimal for our test sets. For reasons of sim-
plicity we did not use automatic optimization techniques such 
as the EM algorithm (cf. Jelinek, 1990). 
 
511
 Figure 3: Results (ksr5) for all methods tested. 
 
 
 
Figure 4: Results (perplexity) for all methods 
tested. 
 
Using the results of our 8 samples, we performed 
paired t tests for every method with the baseline as 
well as with the cache model. All gains for ksr 
turned out to be highly significant (sig. level < 
0.001), and apart from the results for CWLI, all 
perplexity reductions were significant as well (sig. 
level < 0.007), with respect to the cache results. We 
can therefore conclude that, with exception of 
CWLI, all methods tested have a beneficial effect, 
even when compared to a simple cache model. The 
highest gain in ksr (with respect to the baseline) 
was obtained for the confidence-weighted geo-
metric interpolation method (CWGI; +1.05%), the 
highest perplexity reduction was measured for GI 
as well as for CWGI (-9.3% for both). All other 
methods (apart from IWLI) gave rather similar re-
sults (+0.6 to +0.8% in ksr, and -6.8% to -7.7% in 
perplexity). 
We also calculated for all samples the correla-
tion between ksr and perplexity. We measured a 
Pearson coefficient of -0.683 (Sig. level < 0.0001).  
At first glance, these results may not seem over-
whelming, but we have to take into account that 
our ksr baseline of 57.9% is already rather high, 
and at such a level, additional gains become hard to 
achieve (cf. Lesher et al 2002). 
The fact that CWLI performed worse than even 
simple LI was not expected, but it can be explained 
by an inherent property of linear interpolation: If 
one of the models to be interpolated overestimates 
the probability for a word, the other cannot com-
pensate for it (even if it gives correct estimates), 
and the resulting probability will be too high. In 
our case, this happens when a word receives a high 
confidence value; its probability will then be over-
estimated by the LSA component. 
5 Conclusion and further work 
Adapting a statistical language model with seman-
tic information, stemming from a distributional 
analysis like LSA, has shown to be a non-trivial 
problem. Considering the task of word prediction 
in an AAC system, we tested different methods to 
integrate an n-gram LM with LSA: A semantic 
cache model, a partial reranking approach, and 
some variants of interpolation. 
We evaluated the methods using two different 
measures, the keystroke saving rate (ksr) and per-
plexity, and we found significant gains for all 
methods incorporating LSA information, compared 
to the baseline. In terms of ksr the most successful 
method was confidence-weighted geometric inter-
polation (CWGI; +1.05% in ksr); for perplexity, 
the greatest reduction was obtained for standard as 
well as for confidence-weighted geometric interpo-
lation (-9.3% for both). Partial reranking and the 
semantic cache gave very similar results, despite 
their rather different underlying approach.  
We could not provide here a comparison with 
other models that make use of distributional infor-
mation, like the trigger approach by Rosenfeld 
(1996), Matiasek and Baroni (2003) or the model 
presented by Li and Hirst (2005), based on Point-
wise Mutual Information (PMI). A comparison of 
these similarities with LSA remains to be done.  
Finally, an AAC system has not only the func-
tion of simple text entering but also of providing 
cognitive support to its user, whose communicative 
abilities might be totally depending on it. There-
fore, she or he might feel a strong improvement of 
the system, if it can provide semantically plausible 
predictions, even though the actual gain in ksr 
might be modest or even slightly decreasing. For 
this reason we will perform an extended qualitative 
512
analysis of the presented methods with persons 
who use our AAC system SIBYLLE.  This is one of 
the main aims of the recently started ESAC_IMC 
project. It is conducted at the Functional Reeduca-
tion and Rehabilitation Centre of Kerpape, Brit-
tany, where SIBYLLE is already used by 20 children 
suffering from traumatisms of the motor cortex. 
They appreciate the system not only for communi-
cation but also for language learning purposes. 
Moreover, we intend to make the word predictor 
of SIBYLLE publicly available (AFM Voltaire pro-
ject) in the not-too-distant future.  
Acknowledgements 
This research is partially founded by the UFA 
(Universit? Franco-Allemande) and the French 
foundations APRETREIMC (ESAC_IMC project) 
and AFM (VOLTAIRE project). We also want to 
thank the developers of the SRI and the Infomap 
toolkits for making their programs available. 
References 
Bellegarda, J. (1997): ?A Latent Semantic Analysis 
Framework for Large-Span Language Modeling?, 
Proceedings of the Eurospeech 97, Rhodes, Greece. 
Boissi?re Ph. and Dours D. (1996).  ?VITIPI : Versatile 
interpretation of text input by persons with impair-
ments?. Proceedings ICCHP'1996. Linz, Austria. 
Church, K. and Hanks, P. (1989). ?Word association 
norms, mutual information and lexicography?. Pro-
ceedings of ACL, pp. 76-83. 
Clarkson, P. R. and Robinson, A.J. (1997). ?Language 
Model Adaptation using Mixtures and an Exponen-
tially Decaying Cache?, in Proc. of the IEEE 
ICASSP-97, Munich. 
Coccaro, N. and Jurafsky, D. (1998). ?Towards better 
integration of semantic predictors in statistical lan-
guage modeling?, Proc. of the ICSLP-98, Sydney. 
Deerwester, S. C., Dumais, S., Landauer, T., Furnas, G. 
and Harshman, R. (1990). ?Indexing by Latent Se-
mantic Analysis?, JASIS  41(6), pp. 391-407. 
Fazly, A. and Hirst, G. (2003). ?Testing the efficacy of 
part-of-speech information in word completion?, 
Proceedings of the Workshop on Language Modeling 
for Text Entry Methods on EACL, Budapest. 
Goodman, J. (2001): ?A Bit of Progress in Language 
Modeling?, Extended Version Microsoft Research 
Technical Report MSR-TR-2001-72. 
Jelinek, F. (1990): ?Self-organized Language Models for 
Speech Recognition?, In: A. Waibel and K.-F. Lee 
(eds.), Readings in Speech Recognition, Morgan 
Kaufman Publishers, pp. 450-506. 
Kuhn, R. and De Mori, R. (1990). ?A Cache-Based 
Natural Language Model for Speech Reproduction?, 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence, 12 (6), pp. 570-583. 
Landauer, T. K., Laham, D., Rehder, B. and Schreiner, 
M. E. (1997). ?How well can passage meaning be de-
rived without using word order? A comparison of 
LSA and humans?, Proceedings of the 19th annual 
meeting of the Cognitive Science Society, pp. 412-
417, Erlbaum Mawhwah, NJ. 
Lesher, G. W., Moulton, B. J, Higginbotham, D.J. and 
Alsofrom, B. (2002). ?Limits of human word predic-
tion performance?, Proceedings of the CSUN 2002. 
Li, J., Hirst, G. (2005). ?Semantic knowledge in a word 
completion task?, Proc. of the 7th Int. ACM Confer-
ence on Computers and Accessibility, Baltimore. 
Matiasek, H. and Baroni, M. (2003). ?Exploiting long 
distance collocational relations in predictive typing?, 
Proceedings of the EACL-03 Workshop on Language 
Modeling for Text Entry Methods, Budapest. 
Rosenfeld, R. (1996). ?A maximum entropy approach to 
adaptive statistical language modelling?, Computer 
Speech and Language, 10 (1), pp. 187-228. 
Schadle I., Antoine J.-Y., Le P?v?dic B., Poirier F. 
(2004).  ?Sibyl - AAC system using NLP tech-
niques?. Proc. ICCHP?2004, Paris, France. LNCS 
3118, Springer Verlag. 
Stolcke, A. (1998): ?Entropy-based pruning of backoff 
language models?. Proc.s of the DARPA Broadcast 
News Transcription and Understanding Workshop. 
Stolcke, A. (2002): ?SRILM - An Extensible Language 
Modeling Toolkit?, in Proc. of the Intl. Conference 
on Spoken Language Processing, Denver, Colorado. 
Trnka, K., Yarrington, D., McCoy, K. F. and Penning-
ton, C. (2006): ?Topic Modeling in Fringe Word Pre-
diction for AAC?, In Proceedings of the 2006 Inter-
national Conference on Intelligent User Interfaces, 
pp. 276 ? 278, Sydney, Australia. 
Trost, H., Matiasek, J. and Baroni, M. (2005): ?The 
Language Component of the FASTY Text Prediction 
System?, Applied Artificial Intelligence, 19 (8), pp. 
743-781. 
Wandmacher, T. (2005): ?How semantic is Latent Se-
mantic Analysis??, in Proceedings of 
TALN/RECITAL 2005, Dourdan, France, 6-10 june. 
513
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 50?57,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Deeper spoken language understanding for man-machine dialogue on
broader application domains: a logical alternative to concept spotting
Jeanne Villaneau
UEB (Universite? Europe?enne de Bretagne)
VALORIA
France
villanea@univ-ubs.fr
Jean-Yves Antoine
Universite? Franc?ois Rabelais - Tours
LI
France
Jean-Yves.Antoine@univ-tours.fr
Abstract
LOGUS is a French-speaking spoken lan-
guage understanding (SLU) system which
carries out a deeper analysis than those
achieved by standard concept spotters. It
is designed for multi-domain conversa-
tional systems or for systems that are
working on complex application domains.
Based on a logical approach, the sys-
tem adapts the ideas of incremental ro-
bust parsing to the issue of SLU. The pa-
per provides a detailed description of the
system as well as results from two evalu-
ation campaigns that concerned all of cur-
rent French-speaking SLU systems. The
observed error rates suggest that our log-
ical approach can stand comparison with
concept spotters on restricted application
domains, but also that its behaviour is
promising for larger domains. The ques-
tion of the generality of the approach is
precisely addressed by our current inves-
tigations on a new task: SLU for an emo-
tional robot companion for young hospital
patents.
1 Introduction
Despite the indisputable advances of automatic
speech recognition (ASR), highly spontaneous
speech remains an important barrier to the wide
spreading of speech based applications. The goal
of spontaneous speech understanding remains fea-
sible, provided the interaction between the user
and the system is restricted to a task-oriented di-
alogue (restricted vocabulary). Present research is
investigating mixed or user initiated dialogue for
less restricted tasks. It is the purpose of this paper,
which focuses on spontaneous speech understand-
ing in such complex applications.
Generally speaking, information speech dia-
logue systems are based on the same architecture.
At first, a speech recognizer processes the speech
signal and provides a string (or a lattice) of words
that should correspond to the spoken sentence.
Then, this string is parsed by a spoken language
understanding module (SLU) in order to build a
semantic representation that represents its propo-
sitional meaning. Finally, this semantic structure
is sent to a dialogue manager which controls the
interaction with the user (database interrogation,
dialogue management, answer generation). The
answers to the user can be displayed on screen
and/or through a message generated by a text-to-
speech synthesis. This paper focuses on the SLU
module of such a dialogue system. On the whole,
SLU has to cope with two main difficulties:
? speech recognition errors: highly sponta-
neous speech remains hard to recognize for
current ASR systems (Zue et al, 2000).
Therefore, the SLU module has to work on
a strongly corrupted string of words.
? spoken disfluencies: filled pauses, repetitions
and repairs make the parsing of conversa-
tional spoken language significantly harder to
achieve (Heeman, Allen, 2001).
In order to overcome those difficulties, most SLU
systems follow a selective strategy which comes
down to a simple concept spotting: they restrict
the semantic analysis to a mapping of the sentence
with the main expectations of the user in relation
with the task (Minker W. et al, 1999; Bangalore S.
et al, 2006). Consider, for instance, an air trans-
port information system and the following spoken
utterance:
(1) Cou- could you list me the flights uh the
scheduled flights for Tenerife Tenerife Tenerife
North please
Satisfying the speaker?s goals only requires de-
tecting the nature of their requests (list flights) and
the required destination (Tenerife North). Those
50
two concepts (list, Tenerife North) will fill a shal-
low semantic frame which is supposed to repre-
sent the useful meaning of the sentence. Such
task-driven approaches meet, to a great extent, the
needs of SLU in terms of robustness, since they
only involve a partial analysis of the sentence.
Whether the processing is based on a statistical or
a knowledge-based approach, several evaluation
campaigns proved that concept spotting is suitable
for spoken language understanding, provided the
application task is sufficiently restricted. How-
ever, concept spotters suffer from noticeable limi-
tations:
? Although they resist gracefully speech recog-
nition errors, they are not able to detect their
eventual presence, since they do not consider
the global structure of the sentence. This lim-
itation can be particularly penalizing when
the error is related to a key element, for ex-
ample when the error prevents the system to
determine the type (dialogue act) of the ut-
terance. Indeed, concept spotters often base
SLU on the initial characterization of the
question type. When analyzing the errors
of his statistical concept spotter, Minker has
shown that the correct identification of the
question type is a key issue in terms of final
robustness (Minker W. et al, 1999).
? Since they are based on the identification of
rather flat semantic frames, these approaches
hardly succeed in representing complex syn-
tactic relations such as overlapping coordi-
nate phrases or negations.
? Although it is well known that generality is
an important issue for SLU, this question
is generally approached in term of technical
portability from one (narrow) task to another.
Now, one should wonder whether concept
spotting is still suitable on larger application
domains. It seems that the robustness of the
spotting process depends strongly on the de-
gree of lexical ambiguity of the considered
task. For instance, Bousquet has shown that
the concept error rate of her stochastic spot-
ter is two times higher on ambiguous words
than on non ambiguous ones (Bousquet et al,
2003).
Such considerations tend to show that to apply
concept spotting to more complex tasks could be
difficult. Such observations are well known (Zech-
ner K., 1998; Van Noord et al, 1999), and no-
ticeable attempts have already been done to reach
a deeper semantic analysis. However, statistical
or knowledge-based concept spotting remains the
prevailing paradigm in SLU, mainly because of
engineering motivations (quick and easy build-
ing). On the contrary, we have decided to de-
velop a SLU system (LOGUS1) which carries out
a complete analysis of the utterance while keep-
ing the robustness of standard concept spotting ap-
proaches. The system, which is based on a logi-
cal approach, adapts the ideas of incremental ro-
bust parsing (A??t-Mokhtar S., 2002; Basili, 2003)
to the issue of speech conversational systems. In
section 2, we will describe the system into de-
tail. Then, section 3 will present results from dif-
ferent evaluation campaigns in which we partici-
pated. These experiments concerned standard re-
stricted tasks (hotel reservation for instance) for
which concept spotting is well adapted. As a re-
sult, this section does not aim to prove a supe-
riority of our approach, but simply to show that
this deeper processing is able to keep a satisfac-
tory robustness, by comparison with prevailing ap-
proaches. Finally, we give in section 4 a brief de-
scription on our present work concerning the inte-
gration of LOGUS in a conversational robot which
is dedicated to general interaction with children
who are in hospital for a long-stay. This exam-
ple will illustrate the portability abilities of our ap-
proach on complex application tasks, in addition
with our previous works on general tourism infor-
mation.
2 Description of the LOGUS system
The task of a SLU is to turn a sequence or a graph
of words into a semantic representation; so a SLU
system has to perform a translation from natural
language to a formal target language. This section
begins with the description of the formal language
chosen for the LOGUS system. We then explain
the basic principles of parsing and its main steps.
2.1 Semantic representation
When it comes to the choice of a target language
for the system, the following points must be taken
into account.
? We want to implement automatic understand-
ing in application domains where predefined
1LOGical Understanding System.
51
semantic frames are not sufficient to repre-
sent all the possible queries (Van Noord et al,
1999). Furthermore, any SLU aims at pro-
viding results usable by a dialogue manager:
the target language must reconcile simplicity
with precision.
? This semantic representation must obviously
extend to a pragmatic one. That means that it
should involve the characterization of the di-
alogue acts related to the speech turn (Austin
J.-L., 1962).
We have chosen a formalism compatible with
these constraints and inspired by the illocutionary
logic of D. Vanderveken (Vanderveken D., 1981).
In this formalism, the form of an elementary illo-
cutionary act is F(P) where F is the illocutionary
force, and P its propositional content.
The LOGUS system thus provides a logical for-
mula as the semantic representation of an utter-
ance. A language act contains clues about the in-
tentions of the speaker: it is labelled illocutionary
force, while the propositional content is a structure
built with the domain objects and their properties
which is called an object string.
The following example shows a single speech
turn uttered for a tourism information system:
(2) j?ai re?serve? une chambre dans un deux
e?toiles l?ho?tel euh l?ho?tel Rex pour y aller d?ici
comment est-ce que je peux faire (I booked a room
in a two-star hotel in the hotel hum in the Rex hotel
from here how can I go at there)
This turn expresses two different language acts,
which is quite usual in conversational speech:
a piece of information (I booked a room...) is
followed by the user question (... how can I go....
Such complex speech turns are difficult to analyze
for concept spotters, since they usually base
the parsing on one language act detection. The
logical formula LOGUS provides is split into two
language acts: (information act) and (question
how). The second act is interpreted by the system
in the context of the first one:
((information act)
(of (reservation [])
(hotel [(ident. (name ?Rex?)),(star (int 2))])))
((question how)
(to go [(to (contextual location [])),
(from (hotel [(ident. (name ?Rex?))]))]))
In the formula, reservation, hotel and to go are
object labels; (ident. (name ?Rex?)), (star (int 2))
are properties. The two objects of labels reserva-
tion and hotel are linked with the generic relation
of, which indicates a subordination relation. It is
the main relation, (in addition with logical coordi-
nations and, or and not) which is used for building
complex object strings.
2.2 General system architecture
Incremental parsing methodology is used for text
parsing in order to combine efficiency with robust-
ness (A??t-Mokhtar S., 2002). With LOGUS, we
tried to show that such methods can be extended
to spoken language parsing.
The system has to parse out-of-grammar con-
structions but spoken language studies have shown
that minimal syntactic structures are generally pre-
served in repairs and false-starts (Mc Kelvie D.,
1998). We have thus chosen to carry out an
incremental bottom-up parsing, where words are
gradually combined. At the beginning, the parser
groups words according to mainly syntactic rules
in order to form minimal chunks that correspond
to basic concepts of the application domain. Then,
as word group size increases, their meaning be-
comes more precise, enough to relax syntactic cri-
teria and thereby overcome the problem of out-of-
grammar sentences.
The general architecture of the system is shown
in Figure 1. The parsing is essentially split into
three stages. The first stage is chunking (Ab-
ney S., 1991) where grammatical words are linked
to the lexical words to which they are referred.
The following stage gradually builds links be-
tween the chunks in order to detect semantic re-
lations between the corresponding concepts, and
the last one achieves a contextual interpretation
(anaphoric resolution for instance). The process
of building links between chunks and contextual
understanding uses a domain ontology.
Only one formalism is used during these pars-
ing stages. It is designed to distinguish syntax and
semantics and to preserve genericity of the pars-
ing rules. Each component is specified by a list
of what we can call definitions; each of them is a
triplet < C, R, T > where
C: is a syntactic label, called syntactic category:
for example adjective, (verb 1 present).
R: points out the semantic function of the compo-
nent. It is called semantic role: for example
object, (prop price) where prop is for prop-
erty.
52
? level 1
? level 2
? level 3
? semantic kernel
dependenciescontextual understanding
chunk dependencies
chunking
word sequence
domain
ontology
lexicon
logical formula
Figure 1: General architecture of the LOGUS sys-
tem
T: is the semantic translation. It is an element
of the logical formula built by the system. It
belongs thus to the target language.
The first two triplet elements, C and R, are
widely domain independent. A basic principle is
to define parsing rules from these elements in or-
der to preserve the genericity of the system. Each
parsing rule combines two or three triplets in order
to build a new result triplet.
2.3 Chunking
Our experiments with LOGUS have clearly shown
that chunking is effective for spoken language,
provided the chunks are very short: more pre-
cisely, errors made at the speech recognition level
make it dangerous to link objects or properties ac-
cording to pure syntactic criteria, without check-
ing these links with semantic criteria. Therefore
the chunks built by LOGUS include only one con-
tent word: we call them minimal chunks. Chunk-
ing is based on the principle of linking function
words to the near content word.
The formalism used in this step is inspired by
Categorial Grammars of the AB type2, whose
rules are generalized from the first two elements of
the constituent triplets. Function words have def-
initions in which syntactic category and semantic
role are fractional. In such definitions, the seman-
tic translation is a ?-abstraction (in the ?-calculus
meaning)3. The semantic translation of the re-
sult triplet is achieved by applying this abstrac-
tion to the semantic translation of the un-fractional
triplet. Formally, the following two rules are ap-
plied, where F is an abstraction:
< CA/CB, RA/RB, F >, < CB, RB, SB >
? < CA, RA, (F SB) >
< CB, RB, SB >,< CB\CA, RB\RA, F >
? < CA, RA, (F SB) >
2The formalism can be expressed in terms of pregroup
formalism too (Lambek J., 1999).
3LOGUS is implemented in ?Prolog, a logic programming
language whose terms are ?-terms with simple types.
In the following example only one definition is
shown for each component (gn is for nominal
group).
trois (three) e?toiles (stars)
C adj num adj num\gn
R (prop nb) (prop nb)\(prop nb star)
S (int 3) ?x.(star x)
By applying the second rule, we obtain the fol-
lowing chunk:
?trois e?toiles? (three stars)
<gn, (prop nb star), (star (int 3))>.
The semantic translation of the result triplet
is obtained by ?-reduction of the ?-term
(?x. (star x) (int 3)). For example, the utterance
(3) ? `A l?ho?tel Caumartin quels sont le les tar-
ifs pour pour une chambre double? (In Caumartin
hotel what are the the prices for for a double room)
is segmented into six chunks during the chunking
stage. Their semantic translations are:
[1] (hotel []),
[2] (identity (name ?Caumartin?))]),
[3] (what (interrogation)), [4] (price []),
[5] (room []), [6] (size double).
At the end of the chunking process, the deter-
miner le and the first occurrence of the preposition
pour are deleted because they are fragments with-
out semantic content. Deletions such as these are
a first way of dealing with repairs.
2.4 Domain ontology
The limited scope of the application domain
makes it possible to describe exhaustively the
pragmatic and semantic domain knowledge. A do-
main ontology specifies how objects and proper-
ties can be compounded. The handled processings
are expected to be generic while using a domain
dependent ontology: to achieve that, the ontology
is defined by generic predicates whose domain ob-
jects and domain properties are the arguments.
For example, the possibility of building the con-
ceptual relation of between two objects (cf. 2.1)
is defined by the predicate is sub object whose
arguments are two object labels: so the relation
is sub object(room, hotel) expresses a part-whole
relation possibility between such two objects.
2.5 Chunk dependencies
Chunk dependencies are built by an incremental
process which is compound of several successive
stages. Each stage is based on rewriting rules
53
which are specified from the first two components
of the constituent triplets and from the generic on-
tology predicates. They are thus not specific to the
domain of application, what assures, to a certain
extent, the genericity of the process.
Consider for instance the following rule, which
leads to the binding of two consecutive chunks
which share a meronomic (part of) relation:
< C1, object, O1 >, < C2, object, O2 >
- O1 simple object of label Et1
- O2 object string of label Et2
- is sub object(Et1, Et2)
< C, object, (of O1 O2) >
where C is obtained by composing C1 and C2.
As an illustration, this rule will form a com-
plex object (of (price []) (room [(size double)]))
from the initial two chunks (price []) and (room
(size double)). This rule is completely generic and
should apply on any task. The knowledge spe-
cific to the task intervenes only on the definition of
the predicate is sub object. As a result, one could
speak of procedural genericity to qualify our sys-
tem.
As long as possible, the first processing stages
try to respect syntactic criteria. However, in pres-
ence of spoken disfluencies or speech recogni-
tion errors, it is likely that the utterance is out-
of-grammar. Therefore, since the detected links
between chunks make the meaning of the linked
chunks more specific, the next stage tries to detect
chunk dependencies more on more on semantic or
pragmatic features only. Subsequently, studying
dependencies between the components makes it
possible to eliminate some components, especially
in the case of word recognition errors.
As an illustration, Figure 2 shows how links are
gradually built during the parsing stage of utter-
ance (3) (cf. section 2.3). The chunks are in rect-
angular boxes in dotted lines.
The first step of chunk binding links the first two
chunks into the object:
(hotel [(ident. (name ?Caumartin?))]).
The second step links the object (room []) with
the property (size (double)) to obtain the object
(room [(size double)]). Then, the two objects price
and room are linked with the conceptual relation of
to obtain (of (price []) (room [(size double)])) and
this object string is connected to the language act:
(question what). The position of the prepositional
phrase a` l?hotel Caumartin is not usual in French
syntagmatic ordering. It is indeed an example of
extraposition which is not accepted by the syn-
tactic constraints considered by the system. As a
result, the conceptual relation of, which links the
object of label room with the object (hotel [ident.
(name ?Caumartin?)]) is built later, when these
constraints are relaxed.
2.6 Contextual understanding
Many sentences are elliptical and incomplete in a
dialogue. Therefore, it is necessary to use the cur-
rent context of the task and the dialogue history
in order to complete their understanding. The ob-
jectives of the contextual understanding in LOGUS
are thus close to the objectives of the authors of the
OntoSem system (McShane M., 2005): the com-
pletion of semantic fragments. Reference resolu-
tion is thereby extended to a more general comple-
tion of the semantic representation.
While syntactic anaphora criteria are generally
respected in texts, anaphora gender and number
are frequently broken in spoken language. More-
over, gender and number morphological marks are
hardly perceptible in spoken French. They are
therefore very often corrupted by speech recogni-
tion errors. So, in the LOGUS system, anaphora
resolution is based on the same principles as the
rest of the parsing: combining syntactic and se-
mantic criteria. Both nominal and pronominal
anaphora (with definite expressions) are consid-
ered during this contextual interpretation stage.
Completion is based on the concept of object
string. A property or an object may be completed
by an ?over-object? of the context, if the ontology
makes it possible to do so. For example, the ob-
ject price of the sentence ?quel est le tarif? (what
is the price) is automatically completed in
(of (price []) (of (room []) (hotel [(name ?Rex?)]))
if the object string (of (room []) (hotel [(name
?Rex?)])) is an object string which is part of the
previous utterance.
3 Evaluations and results
LOGUS is a French-speaking system. It took part
in the two evaluation campaigns that were carried
out in the last year designed for French spoken
language understanding: the GDR-I3 challenge-
based campaign and the MEDIA project.
3.1 The GDR-I3 campaign
LOGUS took part in the challenge-based cam-
paign, held by the GDR-I3 consortium of the
54
question
what
[a l hotel] [Caumartin] [quels sont]
identity
[les tarifs]
price
[pour une
 chambre]
room
[double]
size
double
of
of
:level 3
: level2
name
"Caumartin"
pourand le and elimination of 
: level 1
hotel
processAfter chunking   
conceptchunk conceptual relation
Figure 2: Characterization of chunk dependencies : example on the utterance ? a` l?hotel Caumartin quels
sont le les tarifs pour pour une chambre double? (in Caumartin hotel what are the the prices for for a
double room.
French CNRS research agency (Antoine et al,
2002). We won?t describe here in detail the re-
sults of this campaign, since it concerned a for-
mer version of LOGUS. It seems however in-
teresting to analyse the distribution of the errors
made by LOGUS to have an idea of the benefits
of our approach. The evaluation corpus was di-
vided among several tests which were respectively
related to a specific difficulty: speech recognition
errors, speech repairs and other disfluences, and fi-
nally messages of a structural complexity (embed-
ded coordination or subordination, for instance)
significantly higher than those usually met in stan-
dard ATIS-like application domains.
The distribution of the concept error rates of the
LOGUS SLU system is the following:
Speech recognition: 9.5%
Complex structures: 9.8%
Repairs: 15%
It should be noted here that the robustness of
LOGUS decreases rather gracefully on complex
messages, while SLU systems based on concept
spotting meet real difficulties on such utterances.
For instance, Cacao (Bousquet-Vernhettes et al,
1999; Bousquet-Vernhettes et al, 2003) is a con-
cept spotter which participated to the GDR-I3
campaign. It has been shown that most of its er-
rors resulted from its difficulties to resolve lexical
ambiguities in complex sentences. This observa-
tion suggests that our logical deep parsing should
fulfill better than concept spotting the needs of
complex application domains such as general pur-
pose tourist information or collaborative plan-
ning (Allen J. et al, 2002), or even multi-domain
applications (Dzikovska M. et al, 2005). Unfortu-
natedly, French evaluation campaigns have never
investigated such difficult tasks.
3.2 The MEDIA project
MEDIA-EVALDA was an evaluation campaign
hold by the French Ministry of Research. It con-
cerned all the French laboratory working on SLU.
Once again, this evaluation investigated a rather
restricted application domain: hotel reservation.
It is well known that concept spotters fit succes-
fully such simple tasks. Nevertheless, we decided
to take part in this evaluation in order to see to
which extent LOGUS should be compared to stan-
dard concept spotters in such disavantageous con-
ditions.
Participants defined reservation scenarios which
were used to build a corpus made up of 1250
recorded dialogues. Recording used a WOZ sys-
tem simulating vocal tourist phone server (Dev-
illers et al, 2004). The MEDIA corpus, which
is made up of real-life French spontaneous dia-
logues, is surely to become a benchmark reference
for French contextual SLU.
The evaluation paradigm forced every partici-
pant to convert his own semantic representation
into a common reference, which relies-on an at-
tribute/value frame: each utterance is divided into
semantic segments, aligned on the sentence, and
each segment is represented by a triplet: (mode,
attribute, value). Relations between attributes are
represented by their order in the representation and
the composed attribute names.
Nine systems participated to this first campaign.
An error was count for any difference with one
of the elements of the reference (mode, attribute
55
System 1 2 3 4 (LOGUS) 5
Approach concept
spotting
concept
spotting
syntactic
deep parsing
logical
deep parsing
concept
spotting
Error rate 29.0% 30.3% 36.3% 37.8% 41.3%
Table 1: MEDIA results.
or value). Table 1 summarises the results of the
best five systems. At first glance, one should find
the reported error rates rather deceptive. How-
ever, one must realize that the test corpus involved
highly spontaneous conversational speech, with
very frequent speech disfluences. As a result,
these results should be compared, for instance,
to ASR errors rates observed on the SWITCH-
BOARD corpus (Greenberg S. et al, 2000).
LOGUS was ranked fourth and its robustness
was rather close to the best participants. Now,
if you consider that the systems ranked 1st, 2nd
and 5th were using a concept spotter, these re-
sults shows that our approach can bear compar-
ison with standard approaches even on this task.
These encouraging performances suggest that it is
possible to achieve a deep understanding of con-
versational speech while respecting at the same
time some robustness requirements: our approach
seems indeed competitive even in a domain where
concept spotters are known to be very efficient. To
our mind, the interest of our approach is that this
robustness should remain on larger application do-
mains. We are precisely trying to test this gener-
icity by adapting LOGUS to a wider application
domain in the framework of the Emotirob project.
4 Genericity and portability experiment
We are currently testing the portability of our
approach by adapting LOGUS to a really differ-
ent task, which corresponds to an unrestricted
application domain, general purpose understand-
ing of child language, with additional emotional
state detection. The whole project, supported by
ANR (National French Research Agency), aims
at achieving a robot companion which can inter-
act with sick or disabled young children with the
help of facial expressions. Although the robot
does not have to react to every speech act of the
child, we have to deal with spoken understanding
in an unrestricted domain. Fortunately, the age of
the children involved (3-5) implies a restricted vo-
cabulary. This work is still in progress. Our first
investigations suggest however that LOGUS is a
suitable understanding system for the pursued pur-
pose: since there will never be significant corpora
related to this kind of task, we can?t use statisti-
cal methods. Moreover, because of the generic-
ity of LOGUS, the main part of the analysis can
be reused without important changes. Thus, three-
month work was enough to build a first prototype
of the system and the problem is restricted to the
main problem of this project: building an ontology
which models the cognitive and emotional world
of young children.
The generality of the used formalism makes it
possible to include an emotional component by
turning the triplet structure into a quadruplet struc-
ture. Of course, composition rules have to in-
clude this new component. We are currently work-
ing on the computation of the emotional states
from both prosodic and lexical cues. Whereas
many works have investigated a prosodic-based
detection (Devillers et al, 2005), word-based ap-
proaches remain quite original. Our hypothesis is
that emotion is compositional, e.g. that is pos-
sible to compute the global emotion carried by a
sentence from the emotion of every content word.
This calculation depends obviously of the seman-
tic structure of the utterance: our system will
precisely benefit from the characterization of the
chunk dependencies carried on by LOGUS. For the
moment being, we are working on the definition of
a complete lexical norm of emotional values from
children of 3, 5 and 7 years. This norm will be
established in collaboration with psycholinguists
from Montpellier University, France.
5 Conclusion
When we started implementing the LOGUS sys-
tem, one of our objectives was to achieve robust
parsing of spontaneous spoken language while
making the application domain much wider than
is currently done. Logical formalisms are not usu-
ally viewed as efficient tools for pragmatic appli-
cations. The promising results of LOGUS show
that they can be brought into interesting new ap-
proaches.
56
Another objective was to have a rather generic
system, despite the use of a domain-based seman-
tic knowledge. We have fulfilled this constraint
through the definition of generic predicates as
well as generic rules working on semantic triplets
or quadruplets which makes it possible to have
generic chunk linking rules. The performances of
LOGUS show that a deeper understanding can bear
comparison with concept spotting approaches.
References
Abney S. 1991. Parsing by Chunks. Principle Based
Parsing. R. Berwick, S. Abney and C. Tenny Eds.
Kluwer Academix Publishers.
A??t-Mokhtar S., Chanod J.-P. and Roux C. 2002.
Robustness beyond Shallowness: Incremental Deep
Parsing. Natural Language Engineering, 8 (2-3):
p. 121?144.
Allen J. and Ferguson G. 2002. Human-Machine Col-
laborative Planning. Proc. of the 3rd International
NASA Workshop on Planning and Scheduling for
Space, Houston, TX.
Antoine J.-Y. et al 2002. Predictive and Ob-
jective Evaluation of Speech Understanding: the
?challenge? evaluation campaign of the I3 speech
workgroup of the french CNRS. Proceedings of
the LREC 2002, 3rd International Conference on
Language Resources and Evaluation, Las Palmas,
Spain.
Austin J.-L. 1962. How to do things with words. Ox-
ford.
Bangalore S., Hakkani-Tu?r D. and Tu?r G. 2006. Spe-
cial issue on Spoken Language Understanding in
Conversational Systems. Speech Communication.
48.
Basili R. and Zanzotto F.M. 2003. Parsing engineering
and empirical robustness. Natural Language Engi-
neering. 8 (2-3).
Bousquet-Vernhettes C., Bouraoui J.-L. and
Vigouroux N. 2003. Language Model Study for
Speech Understanding. Proc. Internationnal Work-
shop on Speech and Computer (SPECOM?2003) ,
Moscow, Russia, p. 205?208.
Bousquet-Vernhettes C., Privat R. and Vigouroux N.
2003. Error handling in spoken dialogue systems:
toward corrective dialogue. ISCA workshop on Er-
ror Handling in Spoken Dialogue Systems, Chteau-
d?Oex-Vaud, Suisse, p. 41?45.
Bousquet-Vernhettes C., Vigouroux N. and Pe?rennou
G. 1999. Stochastic Conceptual Model for Spo-
ken Language Understanding. Proc. Internationnal
Workshop on Speech and Computer (SPECOM?99) ,
Moscow, Russia, p. 71?74.
Devillers L. et al 2004. The French Evalda-Media
project: the evaluation of the understanding ca-
pabilities of Spoken Language Dialogue Systems.
Proceedings of the LREC 2004, 4rd International
Conference on Language Resources and Evaluation,
Lisboa, Portugal.
Devillers L., Vidrascu, L. and Lamel, L. 2005. Chal-
lenges in real-life emotion annotation and machine
learning based detection. Neural Networks, 18, p.
407-422.
Dzikovska M., Swift M. and Allen J. and de Beau-
mont W. 2005. Generic parsing for multi-domain
semantic interpretation. Proc. 9th International
Workshop on Parsing Technologies (IWPT05)), Van-
couver BC.
Greenberg S. and Chang, S. 2000. Linguistic dissec-
tion of switchboard-corpus automatic speech recog-
nition systems. Proc. ISCA Workshop on Automatic
Speech Recognition: Challenges for the New Mil-
lennium, Paris, France.
Heeman P. and Allen J. 2001. Improving robustness
by modeling spontaneous events. Robustness in lan-
guage and speech technology, Kluwer Academics.
Dordrecht, NL. p. 123?152.
Lambek J. 1999. Type grammars revisited. Logical
Aspects of Computational Linguistics, A. Lecomte,
F. Lamarche and G. Perrier (eds), LNAI 1582,
Springer, Berlin, p. 1?27.
Mc Kelvie D. 1998. The syntax of disfluency in spon-
taneous spoken language. HCRC Research Paper,
HCRC/RP-95.
McShane M. 2005. Semantics-based resolution of
fragments and underspecified structures. Traitement
Automatique des Langues, 46(1): p. 163?184.
Minker W., Waibel A. and Mariani J.. 1999. Stochas-
tically based semantic analysis. Kluwer Ac., Ams-
terdam, The Netherlands.
Vanderveken D. 2001. Universal Grammar and
Speech act Theory. Essays in Speech Act The-
ory. Eds J. Benjamin, D. Vanderveken and S. Kubo,
p. 25?62.
van Noord G., Bouma G. and Koeling R. and Nederhof
M. 1999. Robust grammatical analysis for spoken
dialogue systems. Natural Language Engineering.
5(1): p. 45?93.
Zechner K. 1998. Automatic construction of frame
representations for spontaneous speech in unre-
stricted domains. COLING-ACL?1998. Montreal,
Canada. p. 1448?1452.
Zue V., Seneff S., Glass J., Polifrini J., Pao C., Hazen
T.J. and Hetherington L. 2000. Jupiter: a telephone-
based conversational interface for weather informa-
tion. IEEE Transactions on speech and audio pro-
cessing. 8(1).
57
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 550?559,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Weighted Krippendorff's alpha is a more reliable metrics for multi-
coders ordinal annotations: experimental studies on emotion, opinion 
and coreference annotation  
Jean-Yves Antoine 
Universit? Fran?ois Rabelais de 
Tours, LI (EA 6300) 
Blois, France 
Jean-Yves.Antoine@univ-tours.fr 
Jeanne Villaneau 
Universit? Europ?enne de  
Bretagne, IRISA 
Lorient, France 
Jeanne.Villaneau@univ-ubs.fr 
Ana?s Lefeuvre 
Universit? Fran?ois Rabelais 
de Tours, LI (EA 6300) 
Blois, France 
anais.lefeuvre@univ-tours.fr 
 
 
Abstract 
The question of data reliability is of first im-
portance to assess the quality of manually an-
notated corpora. Although Cohen?s ? is the 
prevailing reliability measure used in NLP, al-
ternative statistics have been proposed. This 
paper presents an experimental study with four 
measures (Cohen?s ?, Scott?s pi, binary and 
weighted Krippendorff ? s ?) on three tasks: 
emotion, opinion and coreference annotation. 
The reported studies investigate the factors of 
influence (annotator bias, category prevalence, 
number of coders, number of categories) that 
should affect reliability estimation. Results 
show that the use of a weighted measure re-
stricts this influence on ordinal annotations. 
They suggest that weighted ? is the most reli-
able metrics for such an annotation scheme. 
1 Introduction 
The newly intensive use of machine learning 
techniques as well as the need of evaluation data 
has led Natural Language Processing (NLP) to 
develop large annotated corpora. The interest for 
such enriched language resources has reached 
domains (semantics, pragmatics, affective com-
puting) where the annotation process is highly 
affected by the coders subjectivity. The reliabil-
ity of the resulting annotations must be trusted by 
measures that assess the inter-coders agreement. 
While medecine, psychology, and more gener-
ally content analysis, have considered for years 
the issue of data reliability, NLP has only inves-
tigated this question from the mid 1990s. The 
influential work of Carletta (1996) has led the ? 
statistic (Cohen, 1960) to become the prevailing 
standard for measuring the reliability of corpus 
annotation. Many studies have however ques-
tioned the limitations of the ? statistic and have 
proposed alternative measures of reliability. 
Krippendorff claims that ?popularity of ? not-
withstanding, Cohen?s ? is simply unsuitable as 
a measure of the reliability of data? in a paper 
presenting his ? coefficient (Krippendorff, 
2008).  
Except for some rare but noticeable studies 
(Arstein and Poesio, 2005), most of these critical 
works restrict to theoretical issues about chance 
agreement estimation or limitations due to vari-
ous statistical biases (Arstein and Poesio, 2008). 
On the opposite, this paper investigates experi-
mentally these questions on three different tasks: 
emotion, opinion and coreference annotation. 
Four measures of reliability will be considered: 
Cohen?s ? (Cohen, 1960), Scott?s pi (Scott, 1955) 
and two measures of Krippendorff?s ? (Krippen-
dorff, 2004) with different distance.  
 Section 2 gives a comprehensive presentation 
of these metrics. Section 3 details the potential 
methodological biases that should affect the reli-
ability estimation. In section 4, we explain the 
methodology we followed for this study. Lastly, 
experimental results are presented in section 5. 
2 Reliability measures 
Any reliability measure considers the most perti-
nent criterion to estimate data reliability to be 
reproducibility. Reproducibility can be estimated 
by observing the agreement among independent 
annotators (Krippendorff, 2004): the more the 
coders agree on the data they have produced, the 
more their annotations are likely to be repro-
duced by any other set of coders.  
Pure observed agreement is not considered as 
a good estimator since it does not give any ac-
count to the amount of chance that yields to this 
agreement. For instance, a restricted number of 
coding categories should favor chance agree-
ment. What must be estimated is the proportion 
of observed agreement beyond the one that is 
expected by chance: 
(1)  Measure = 
e
eo
A
AA
?
?
1
 
550
where Ao is the observed agreement between 
coders and Ae is an estimation of the possible 
chance agreement. Reliability metrics differ by 
the way they estimate this chance agreement. 
Cohen?s ? (Cohen, 1960) defines chance as 
the statistical independence of the use of coding 
categories by the annotators. It postulates that 
chance annotation is governed by prior distribu-
tions that are specific to each coder (annotator 
bias). ? was originally developed for two coders 
and nominal data. (Davies and Fleiss, 1982) has 
proposed a generalization to any number of cod-
ers, while (Cohen, 1968) has defined a weighted 
version of the ? measure that fulfils better the 
need of reliability estimation for ordinal annota-
tions: the disagreement between two ordinal an-
notations is no more binary, but depends on a 
Euclidian distance. This weighted generalization 
restricts however to a two coders scheme (Art-
stein and Poesio, 2008): a weighted version of 
the multi-coders ? statistics is still missing. 
Unlike Cohen?s ?, Scott?s pi (Scott, 1955) 
does not aim at modelling annotator bias. It de-
fines chance as the statistical independence of 
the data and the set of coding categories, inde-
pendently from the coders. It considers therefore 
the annotation process and not the behaviour of 
the annotators. Scott?s original proposal con-
cerned only two coders. (Fleiss 1971) gave a 
generalisation of the statistics to any number of 
coders through a measure of pairwise agreement.  
Krippendorff?s ? (Krippendorff, 2004) con-
siders chance independently from coders like 
Scott?s pi, but data reliability is estimated de-
pending on disagreement instead of agreement: 
(2)  Alpha  = 
e
oe
D
DD ?
 
where Do is the observed disagreement be-
tween coders and De is an estimation of the pos-
sible chance disagreement. Another original as-
pect of this metrics is to allow disagreement es-
timation between two categories through any 
distance measure. This implies that ? handles 
directly any number of coders and any kind of 
annotation (nominal or ordinal coding scheme). 
In this paper, we will consider the ? statistics 
with a binary as well as a Euclidian distance, in 
order to assess separately the influence of the 
distance measure and the metrics by itself. 
3 Quality criteria for reliability metrics 
There is an abundant literature about the criteria 
of quality a reliability measure should satisfy 
(Hayes, 2007). These works emphasize on two 
important points: 
? A trustworthy measure should provide sta-
ble results: measures must be reasonably 
independent of any factor of influence. 
? The magnitude of the measure must be in-
terpreted in terms of absolute level of reli-
ability: the statistics must come up with 
trustworthy reliability thresholds. 
These questions have mainly been investigated 
from a theoretical point of view. This section 
summarizes the main conclusions that should be 
drawn from these critical studies.  
3.1 Annotator bias and number of coders 
Annotator bias refers to the influence of the idio-
syncratic behavior of the coders. It can be esti-
mated by a bias index which measures the extent 
to which the distribution of categories differs 
from one coder?s annotation to another (Sim and 
Wright, 2005). Annotator bias has an influence 
on the magnitude of the reliability measures 
(Feinstein and Cicchetti,1990). Besides, it con-
cerns the invariance of the measures to the per-
mutation or selection of annotators but also to the 
number of coders. A review of the literature 
shows that theoretical studies on annotator bias 
are not convergent. In particular, opposite argu-
ments have been proposed concerning Cohen?s ? 
(Di Eugenio and Glass 2004, Arstein and Poesio 
2008, Hayes, 2007). This is why we have carried 
on experiments that investigate: 
?  to what extent measures depend on the se-
lection of a specific set of coders (? 5.3), 
?  to what extent the stability of the measures 
depends on the number of coders (? 5.4). 
Arstein and Poesio (2005) have shown 
that the greater the number of coders is, 
the lower the annotator bias decreases. 
Our aim is to go further this conclusion: 
we will study whether one measure needs 
fewer coders than another one to converge 
towards an acceptable annotator bias. 
3.2 Category prevalence 
Prevalence refers to the influence on reliability 
estimation of a coding category under which a 
disproportionate amount of annotated data falls. 
It can be estimated by a prevalence index which 
measures the frequency differences of categories 
on cases where the coders agree (Sim and 
Wright, 2005). When the prevalence index is 
551
high, chance-corrected measures are spuriously 
reduced since chance agreement is higher in this 
situation (Brennan and Sliman, 1992; Di Eugenio 
and Glass, 2004). This yields some authors to 
propose corrected coefficients like the PABAK 
measure (Byrt and al., 1993), which is a preva-
lence adjusted and annotator bias adjusted ver-
sion of Cohen?s ?. The influence of prevalence 
will not be investigated here, since no category is 
significantly prevalent in our data. 
3.3 Number of coding categories 
The number of coding categories has an influ-
ence on the reliability measures magnitude: the 
larger the number of categories is, the less the 
coders have a chance to agree. Even if this de-
crease should concern chance agreement too, 
lower reliability estimations are observed with 
high numbers of categories (Brenner and 
Kliebsch, 1996). This paper investigates this in-
fluence by comparing reliability values obtained 
with a 3-categories and a 5-categories coding 
scheme applied on the same data (see ? 5.1). 
3.4  Interpreting the magnitude of meas-
ures in terms of effective reliability 
One last question concerns the interpretation of 
the reliability measures magnitude. It has been 
particularly investigated with Cohen?s ?. Carletta 
(1996) advocates 0.8 to be a threshold of good 
reliability, while a value between 0.67 and 0.8 is 
considered sufficient to allow tentative conclu-
sion to be drawn. On the opposite, Krippendorff 
(2004b) claims that this 0.67 cutoff is a pretty 
low standard while Neuendorf (2002) supports 
an even more restrictive interpretation.  
Thus, the definition of relevant levels of reli-
ability remains an open problem. We will see 
how our experiments should draw a methodo-
logical framework to answer this crucial issue. 
4 Experiments: methodology  
4.1 Introduction 
We have conducted experiments on three dif-
ferent annotation tasks in order to guarantee an 
appreciable generality of our findings. The first 
two experiments correspond to an ordinal anno-
tation. They concern the affective dimension of 
language (emotion and opinion annotation). They 
have been conducted with na?ve coders to pre-
serve the spontaneity of judgment which is 
searched for in affective computing. 
The third experiment concerns coreference 
annotation. It is a nominal annotation that has 
been designed to be used as a comparison with 
the previous ordinal annotations tasks. 
The corresponding annotated corpora are 
available (TestAccord database) on the french 
Parole_Publique1 corpus repository under a CC-
BY-SA Creative Commons licence. 
4.2 Emotion corpus 
Emotion annotation consists in adding emo-
tional information to written messages or speech 
transcripts. There is no real consensus about how 
an emotion has to be described in an annotation 
scheme. Two main approaches can be found in 
the literature. On the one hand, emotions are 
coded by affective modalities (Scherer, 2005), 
among which sadness, disgust, enjoyment, fear, 
surprise and anger are the most usual (Ekman, 
1999; Cowie and Cornelius, 2003). On the other 
hand, an ordinal classification in a multidimen-
sional space is considered. Several dimensions 
have been proposed among which three are pre-
vailing (Russell, 1980): valence, intensity and 
activation. Activation distinguishes passive from 
active emotional states. Valence describes 
whether the emotional state conveyed by the text 
is positive, negative or neutral. Lastly, intensity 
describes the level of emotion conveyed.  
Whatever the approach, low to moderate inter-
annotator agreements are observed, what ex-
plains that reference annotation must be achieved 
through a majority vote with a significant num-
ber of coders (Schuller and al. 2009). Inter-coder 
agreement is particularly low when emotions are 
coded into modalities (Devillers and al., 2005; 
Callejas and Lopez-Cozar, 2008). This is why 
this study focuses on an ordinal annotation. 
Our works on emotion detection (Le Tallec 
and al., 2011) deal with a specific context: affec-
tive robotics. We consider an affective multimo-
dal interaction between hospitalized children and 
a companion robot. Consequently, this experi-
ment will concern a child-dedicated corpus. Al-
though many works already focused on child 
language (MacWhinney, 2000), no emotional 
child corpus is currently available in French, our 
studied language. We have decided to create a 
little corpus (230 sentences) of fairy tales, which 
are regularly used in works related to child affect 
analysis (Alm and al., 2005; Volkova and al., 
2010). The selected texts come from modern 
fairy tales (Vassallo, 2004; Vanderheyden, 1995) 
which present the interest of being quite confi-
dential. This guarantees that the coders discover 
                                                 
1
 www.info.univ-tours.fr/~antoine/parole_publique 
552
the text during the annotation. We asked 25 sub-
jects to characterize the emotional value con-
veyed by every sentence through a 5-items scale 
of values, ranging from very negative to very 
positive. 
As shown on Table 1, this affective scale en-
compasses valence and intensity dimensions. It 
enables to compare without methodological bias 
an annotation with 3 coding categories (valence: 
negative, positive, neutral) and the original 5-
categories (valence+intensity) annotation. 
A preliminary experiment showed us that 
children meet difficulties to handle a 5-values 
emotional scale. This is why the annotation was 
conducted on the fairy tales corpus with adults 
(11 men/14 women; average age: 31.6 years). All 
the coders have a superior level of education (at 
least, high-school diploma), they did not know 
each other and worked separately during the an-
notation task. Only four of them had a prior ex-
perience in corpus annotation. 
 
Value Meaning Valence / 
Polarity 
Intensity / 
Strength 
-2 very negative negative strong 
-1 moderately 
negative 
negative moderate 
0 no emotion neutral none 
1 moderately 
positive 
positive moderate 
2 very positive positive strong 
 
Table 1. emotion or opinion annotation schemes 
 
The coders were not trained but were given 
precise annotation guidelines providing some 
explanations and examples on the emotional val-
ues they had to use. They achieved the annota-
tion once, without any restriction on time. They 
had to rely on their own judgment, without con-
sidering any additional information. Sentences 
were given in a random order to investigate an 
out-of-context perception of emotion. We con-
ducted a second experiment where the order of 
the sentences followed the original fairy tale, in 
order to study the influence of the discourse con-
text. The criterion of data significance ? at least 
five chance agreements per category ? proposed 
by (Krippendorff, 2004) is greatly satisfied for 
the valence annotation (3 categories). It is ap-
proached on the complete annotation where we 
can assure 4 chance agreements per category. 
4.3 Opinion corpus 
The second experiment concerns opinion an-
notation. Emotion detection can be related to a 
certain extent, with opinion mining (or sentiment 
analysis), whose aim is to detect the attitude of 
people in the texts they produce. A basic task in 
opinion mining consists in classifying the polar-
ity of a given text, which should be either a sen-
tence (Wilson and al., 2005), a speech turn or a 
complete document (Turney, 2002). Polarity 
plays the same role as valence does for affect 
analysis: it describes whether the expressed 
judgment is positive, negative, or neutral. One 
should also characterize the sentiment strength 
(Thelwall and al., 2010). This feature can be re-
lated to the notion of intensity used in emotional 
annotation. Both polarity and sentiment strength 
are considered in our annotation task. 
This experiment has been carried out on a cor-
pus of film reviews. The reviews were relatively 
short texts written by ordinary people on dedi-
cated French websites (www.senscritique.com 
and www.allocine.fr). They concerned the same 
French movie. The corpus contains 183 sen-
tences. Its annotation was conducted by the 25 
previous subjects. The methodology is identical 
to the emotion annotation task. The subjects were 
asked to qualify the opinion that was conveyed 
by every sentence of the reviews by means of  
the same scale of values (Table 1). This scale 
encompasses this time the polarity and sentiment 
strength dimensions. Once again, the sentences 
were given in a random order and contextual or-
der respectively. The criterion of data signifi-
cance is satisfied here too. 
On both annotations, experiments with the 
random or the contextual order give similar re-
sults. Results from the contextual annotation will 
be given only when necessary. 
4.4 Coreference corpus 
The last experiment concerns coreference an-
notation. We have developed an annotated cor-
pus (ANCOR) which clusters various types of 
spontaneous and conversational speech. With a 
total of 488,000 lexical units, it is one of the 
largest coreference corpora dedicated to spoken 
language (Muzerelle and al. 2014). Its annotation 
was split into three successive phases: 
? Entity mentions marking, 
? Referential relations marking, 
? Referential relations characterization 
The experiment described in this paper con-
cerns the characterization of the referential rela-
tions. This nominal annotation consists in classi-
fying relations among five different types: 
553
? Direct coreference (DIR) ? Coreferent 
mentions are NPs with same lexical heads. 
? Indirect coreference (IND) ? These men-
tions are NPs with distinct lexical heads. 
? Pronominal anaphora (PRO) ? The subse-
quent coreferent mention is a pronoun. 
? Bridging anaphora (BRI) ? The subse-
quent mention does not refer to its antece-
dent but depends on it for its referential in-
terpretation (example: meronymy). 
? Bridging pronominal anaphora (BPA) ? 
Bridging anaphora where the subsequent 
mention is a pronoun. This type empha-
sizes metonymies (example: Avoid Cen-
tral Hostel? they are unpleasant) 
The subjects (3 men / 6 women) were adult 
people (average age: 41.2 years) with a high pro-
ficiency in linguistics (researchers in NLP or cor-
pus linguistics). They know each other but 
worked separately during the annotation, without 
any restriction on time. They are considered as 
experts since they participated to the definition 
of the annotation guide. The study was con-
ducted on an extract of 10 dialogues, represent-
ing 384 relations. Krippendorff?s (2004) criterion 
of significance is therefore satisfied here too. 
4.5 Reliability measures 
The experiments have been conducted with four 
chance-balanced reliability measures2 : 
? Multi-? : multiple coders/binary distance 
Cohen?s ?  (Davies and Fleiss, 1982),  
? Multi-pi : multiple coders/binary distance 
Scott?s pi  (Fleiss, 1971),  
? ?b : Krippendorff?s ? with binary distance, 
? ? : standard Krippendorff?s ? with a 1-
dimension Euclidian distance. 
The use of Euclidian distance is unfounded on 
coreference which handles a nominal annotation. 
Thus, ? will not be computed on this last corpus. 
                                                 
2
 Experiments were also conducted with Cronbach??c 
?(Cronbach, 1951). This metrics is based on a correlation 
measure. Krippendorff (2009) considers soundly that corre-
lation coefficients are inappropriate to estimate reliability. 
Our results show that ?c is systematically outperformed by 
the other metrics. In particular, it is highly dependent to 
coder bias. For instance we observed a relative standard 
deviation of ?c measures higher than 22% when measuring 
the influence of coders set permuation (? 5.3, table 5). This 
observation discards Cronbach??c ?as a trustworthy measure. 
5 Results   
5.1 Influence of the number of categories 
Our affective coding scheme enables a direct 
comparison between a 3-classes (valence or po-
larity) and a 5-classes annotation. The 3-classes 
scheme clusters the coding categories with the 
same valence or polarity. For instance {-2,-1} 
negative values are clustered in the same cate-
gory which receive the index 1. For the computa-
tion of the weighted ?, the distance between 
negative (-1) and positive (1) classes will be 
equal to 2. Table 2 presents the reliability meas-
ures observed on all of the corpora. 
 
Corpus Emotion (fairy tales) 
Metric M-? M-pi ?b ? 
3-classes 0.41 0.41 0.41 0.57   
5-classes 0.29 0.29 0.29 0.57 
Abs. diff. 0.12 0.12 0.12 0.0 
Corpus Opinion (film reviews) 
Metric M-? M-pi ?b ? 
3-classes 0.58 0.58 0.58 0.75 
5-classes 0.45 0.45 0.45 0.80 
Abs. diff. 0.13 0.13 0.13 0.05 
Corpus Coreference (spoken dialogues) 
Metric M-? M-pi ?b ? 
5-classes 0.69 0.69 0.69 n.s. 
 
Table 2. Reliability measures: emotion and opinion 
random annotation as well as coreference annotation 
 
Several general conclusions can be drawn 
from these figures. At first, low inter-coder 
agreements are observed on affective annotation, 
which is coherent with many other studies (Dev-
illers and al., 2005; Callejas and Lopez-Cozar, 
2008). Non-weighted metrics (multi-?, multi-pi, 
?b) range from 0.29 to 0.58, depending on the 
annotation scheme. This confirms that these an-
notation tasks are prone to high subjectivity. 
Higher levels of agreement may have been ob-
tained if the annotators were trained with super-
vision. As said before, this would have reduced 
the spontaneity of judgment. Furthermore, a 
comprehensive meta-analysis (Bayerl and Paul, 
2011) has shown that no difference may be found 
on data reliability between experts and novices. 
The reliability measures given by the weighted 
version of Krippendorff?s ? on the two affective 
tasks are significantly higher: ? values range 
from 0.57 to 0.80, which suggests a rather suffi-
cient reliability. These results are not an artifact. 
They come from better disagreement estimation. 
For instance, the difference between a positive 
554
and a negative annotation is more serious than 
between the positive and the neutral emotion, 
what a weighted metrics accounts for.  
Satisfactory measures are found on the con-
trary on the coreference task (0.69 with every 
metric). This result was expected, since a large 
part of the annotation decisions are based on ob-
jective (syntactic or semantic) considerations.  
Whatever the experiment you consider, multi-
?, multi-pi and ?b coefficients present very close 
values (identical until the 3rd decimal). A similar 
observation was made by (Arstein and Poesio, 
2005) with 18 coders. This validates the theoreti-
cal hypothesis on the convergence of individual-
distribution and single-distribution measures 
when the number of coders increases. Our ex-
periments show that annotator bias is moderate 
with 25 coders when inter-coders agreement is 
rather low (affective tasks), while 9 coders are 
enough to guarantee a low annotator bias when 
data reliability is higher (coreference task). 
Lastly, the comparison between the two anno-
tation schemes (3 or 5 classes) in affective tasks 
provides some indications on the influence of the 
number of coding categories on reliability esti-
mation3. As expected (see ? 3.3), multi-?, multi-pi 
and ?b values increase significantly when the 
number of classes decreases.  
On the contrary, weighted ? is significantly 
less affected by the increase of the number of 
categories. The ? value remains unchanged on 
the emotional corpus and its variation restricts to 
0.05 on the opinion task. It seems that the use of 
a Euclidian distance counterbalances the higher 
risk of disagreement when the number of catego-
ries grows. Such an independence of the number 
of coding categories is an interesting property for 
a reliability measure, which has never been re-
ported as far as we know. 
 
Metric M-? M-pi ?b ? 
3-classes 0.61 0.61 0.61 0.78 
5-classes 0.49 0.49 0.49 0.83 
Abs. diff. 0.12 0.12 0.12 0.05 
 
Table 3. Reliability measures with 3 and 5 annotation 
classes: opinion contextual annotation (film reviews). 
 
Finally, Table 3 presents as an illustration the 
reliabilities measures we obtained with the con-
textual annotation of the opinion corpus. These 
                                                 
3
 The 3-classes coding scheme is a semantic reduction of the 
5-classes one. One should wonder whether the same results 
can be observed with unrelated categories. (Chu-Ren and 
al., 2002) shows indeed that expanding PoS tags with sub-
categories does not increase categorical ambiguity. 
results are fully coherent with the previous ones. 
One should note in addition that reliability meas-
ures are significantly higher on these contextual 
annotations: the context of discourse helps the 
coders to qualify opinions more objectively. 
5.2 Influence of prevalence 
Table 4 presents the distribution of the annota-
tions on the three corpora. (Devillers and al., 
2005; Callejas and Lopez-Cozar, 2008) reported 
that more than 80% of the speech turns are clas-
sified as neutral in their emotional corpora. This 
prevalence was not found on our affective cor-
pora. Positive annotations are nearly as frequent 
as the neutral ones on the emotion task. This ob-
servation is due to the deliberate emotional na-
ture of fairy tales. Likewise, the neutral opinion 
is minority among the film reviews, which aim 
frequently at expressing pronounced judgments. 
Positive opinions are slightly majority on the 
opinion corpus but this prevalence is limited: it 
represents an increase of only 50% of frequency, 
by comparison with a uniform distribution.  
 
Corpus Emotion (fairy tales) 
5-classes 
?2 ?1 0 1 2 
Distribution 8% 17% 38% 23%   14% 
3-classes Negative neutral Positive 
Distribution 25% 38% 37% 
Corpus Opinion (film reviews) 
5-classes -2 -1 0 1 2 
Distribution 15% 21% 14% 26% 25% 
3-classes negative neutral positive 
Distribution 36% 14% 51% 
Corpus Coreference (spoken dialogues) 
5-classes DIR IND PRO BRI BPA 
Distribution 40% 7% 42% 10% 1% 
 
Table 4. Distribution of the coding categories  
 
In the coreference corpus, two classes are 
highly dominant, but they are not prevalent 
alone. There is no indication in the literature that 
the prevalence of two balanced categories has a 
bias on data reliability measure. For all these rea-
sons, we didn't investigate the influence of preva-
lence. Besides, relevant works are questioning 
the importance of the influence of prevalence on 
inter-coders agreement measures (Vach, 2005). 
5.3 Influence of coders set permutation 
?a coefficient for assessing the reliability of data 
must treat coders as interchangeable (Krippen-
dorff, 2004b). We have studied the stability of 
reliability measures computed on any combina-
tion of 10 coders (among 25) on the affective 
corpora, and 4 coders (among 9) on the corefer-
555
ence corpus. The influence of permutation is 
quantified by a measure of relative standard de-
viation (e.g. related to the average value) among 
the sets of coders (Table 5).   
 
Corpus Emotion (fairy tales) 
Metric M-? M-pi ?b ? 
3-classes 7.4% 7.7% 7.6% 6.2%   
5-classes 9.0% 9.1% 9.1% 6.1%   
Corpus Opinion (film reviews) 
3-classes 3.4% 3.3% 3.3% 2.6% 
5-classes 4.0% 4.0% 4.1% 1.7% 
Corpus Coreference (spoken dialogues) 
5-classes 4.6% 4.6% 4.6% n.c. 
 
Table 5. Relative standard deviation of measures on 
any independent sets of coders 
Binary metrics do not differ on this criterion: 
multi-?, multi-pi and ?b present very similar re-
sults. On the opposite, the benefit of a Euclidian 
distance of agreement is clear: ? is significantly 
less influenced by coders set permutation. 
5.4 Influence of the number of coders 
A good way to limit annotator bias is to enroll an 
important number of annotators. This need is 
unfortunately contradictory with a restriction of 
annotation costs. The estimation of data reliabil-
ity must thereby remain trustworthy with a 
minimal number of coders. As far as we know, 
there is no clear indication in the literature about 
the definition of such a minimal size. 
We have conducted an experiment which in-
vestigates the influence of the number of coders 
on the relevancy of reliability estimation. Con-
sidering N annotations (N=25 for affective anno-
tation and N=9 for coreference annotation), we 
compute all the possible reliability values with 
any subsets of S coders, S varying from 2 to N. 
As an estimation of the trustworthiness of the 
coefficients, the relative standard deviation of the 
reliability values is computed for every size S 
(Figures 1 to 3). The influence of the number of 
coders is obvious: detrimental standard devia-
tions are found with small coders set sizes. This 
finding concerns above all multi-?, multi-pi and 
?b, which present very close behaviors on all 
annotations. One the opposite, the weighted 
? coefficient converges significantly faster to a 
trustworthy reliability measure The comparison 
between ?b and ?  is enlightening. It shows again 
that the main benefit of Krippendorff?s proposal 
results from its accounting for a weighted dis-
tance in a multi-coders ordinal annotation. 
 
 
0%
10%
20%
30%
40%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
R
e
la
tiv
e
 
s
td
de
v
 
(%
)
multi-pi
alpha
multi-k
alpha binary
 
0%
5%
10%
15%
20%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
R
el
at
iv
e 
st
dd
ev
 
(%
)
multi-pi
alpha
multi-k
binary alpha
 
 
Figure 1. Relative standard deviation on any set of 
coders of a given size. 5-classes coding scheme. Emo-
tion (top) and opinion (bottom) random annotation. 
 
0%
10%
20%
30%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
R
e
la
tiv
e
 
s
td
de
v
 
(%
)
multi-pi
alpha
multi-k
binary alpha
 
0%
5%
10%
15%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
Re
la
tiv
e 
st
dd
ev
 
(%
)
multi-pi
alpha
multi-k
alpha binary
 
 
Figure 2. Relative standard deviation on any set of 
coders of a given size. 3-classes coding scheme. Emo-
tion (top) and opinion (bottom) random annotation. 
556
0%
5%
10%
2 3 4 5 6 7 8
Number of coders
Re
la
tiv
e 
s
td
de
v
 
(%
) multi-pi
multi-k
binary alpha
 
Figure 3. Relative std deviation of measures on any 
sets of coders for a given coders set size: coreference 
6 Conclusion and perspectives 
Our experiments were conducted on various an-
notation tasks which assure a certain representa-
tiveness of our conclusions: 
 
? Cohen?s ?, Krippendorff?s ? ?and Scott?s pi? 
provide close values when they use the 
same measure of disagreement. 
? A convergence of these measures has been 
noticed in the literature when the number 
of coders is high. We observed it even on 
very restricted sets of annotators. 
? The use of a weighted measure (Euclidian 
distance) has several benefits on ordinal 
data. It restricts the influence on reliability 
measure of both the number of categories 
and the number of coders. Unfortunately, 
Cohen?s ? ??statistics cannot consider a 
weighted distance in a multi-coders 
framework contrary to Krippendorff?s ?.  
? There is no benefit of using Krippendorff?s 
? on nominal data, since a binary distance 
is mandatory on this situation. 
To conclude, the main interest of Krippen-
dorff?s ? is thus its ability to integrate any kind 
of distance. In light of our results, the weighted 
version of this coefficient must be preferred 
every time an ordinal annotation with multiple 
coders is considered. 
Our experiments leave open an essential ques-
tion: the objective definition of trustworthy 
thresholds of reliability. We propose to investi-
gate this question in terms of expected modifica-
tions of the reference annotation. A majority vote 
is generally used as a gold standard to create this 
reference with multiple coders. As a preliminary 
experiment, we have compared our reference 
affective annotations (25 coders) with those ob-
tained on any other included set of coders.  
0%
10%
20%
30%
40%
50%
1 3 5 7 9 11 13 15 17 19 21 23
number of coders
%
 o
f m
od
ifi
ca
tio
ns 3 classes
5 classes
0,0%
10,0%
20,0%
30,0%
40,0%
50,0%
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
number of coders
%
 
o
f m
o
di
fic
a
tio
n
s HC 3 classes
HC 5 classes
 
Figure 4. Average modifications of the reference ac-
cording to the number of coders. Emotion annotation 
(top) and opinion annotation (bottom) 
 
Figure 4 presents the average percentage of 
modifications of the reference according to the 
number of coders. We wonder to what extent 
these curves can be related to reliability meas-
ures. It seems indeed that the higher the meas-
ures are, the lower the modifications are too. For 
instance, almost all of the coefficients present 
higher or equal reliability values with 3 coding 
categories (Tables 2 & 3), which corresponds to 
lower levels of modifications on Figure 3. Like-
wise, reliability measures are higher on the opin-
ion annotation, where we observe lower modifi-
cations of the reference.  
As a result, we expect results like those pre-
sented on figure 4 to enable a direct interpreta-
tion of reliability measures. For instance, with a 
multi-? values of 0.41, or a ?b value of 0.57 (Ta-
ble 2, 3-classes emotion annotation), one should 
expect around 8% of errors on our reference an-
notation if 10 coders are considered. We plan to 
extend these experiments with simultated syn-
thetic data to characterize precisely the relations 
between absolute reliability measures and ex-
pected confidence in the reference annotation. 
We expect to obtain with simulated annotation a 
sufficient variety of agreement to establish sound 
recommendations on data reliability thresholds. 
We intend to modify randomly human annota-
tions to conduct this simulation.  
557
References  
Cecilia Alm, Dan Roth, Richard Sproat. 2005. Emo-
tions from Text: Machine Learning for Text-based 
Emotion Prediction, In Proc. HLT&EMNLP?2005. 
Vancouver, Canada. 579-586 
Ron Arstein and Masimo Poesio. 2008. Inter-Coder 
Agreement for Computational Linguistics. Compu-
tational Linguistics. 34(4):555-596. 
Ron Artstein and Massimo Poesio. 2005. Bias de-
creases in proportion to the number of annotators. 
In Proceedings FG-MoL?2005, 141:150, Edin-
burgh, UK. 
Petra Saskia Bayerl and Karsten Ingmar Paul, 2011. 
What Determines Inter-Coder Agreement in Man-
ual Annotations? A Meta-Analytic Investigation  . 
Computational Linguistics. 37(4), 699:725. 
Paul Brennan and Alan Silman. 1992. Statistical 
methods for assessing observer variability in clini-
cal measures. BMJ, 304:1491-1494. 
Ted Byrt, Janet Bishop, John Carlin. 1993. Bias, 
prevalence and kappa. Journal of Clinical Epide-
miology, 46:423-429. 
Hermann Brenner and Ulrike Kliebsch. 1996. Depen-
dance of weighted kappa coefficients on the num-
ber of categories. Epidemiology. 7:199-202. 
Zoraida Callejas and Ramon Lopez-Cozar. 2008. In-
fluence of contextual information in emotion anno-
tation for spoken dialogue systems, Speech Com-
munication, 50:416-433 
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the Kappa statistic. Computational 
Linguistics, 22(2):249-254 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20:37-46. 
Jacob Cohen. 1968. Weighted kappa: nominal scale 
agreement with provision for scaled disagreement 
or partial credit. Psychol. Bulletin, 70(4):213?220 
Roddy Cowie and Randolph Cornelius. 2003. De-
scribing the emotional states that are expressed in 
speech. Speech Communication. 40 :5-32. 
Lee J. Cronbach. 1951. Coefficient alpha and the in-
ternal structure of tests. Psychometrica. 16:297-334 
Laurence Devillers, Laurence Vidrascu, Lori Lamel. 
2005. Emotion detection in real-life spoken dialogs 
recorded in call center. Journal of Neural Net-
works, 18(4):407-422. 
Paul Ekman. 1999. Patterns of emotions: New Analy-
sis of Anxiety and Emotion. Plenum Press, New-
York, NY. 
Barbara Di Eugenio and Michael Glass. 2004. The 
kappa statistic: A second look. Computational Lin-
guistics, 30(1):95?101 
Mark Davies and Joseph Fleiss. 1982. Measuring 
agreement for multinomial data. Biometrics, 
38(4):1047-1051. 
Alvan Feinstein and Domenic Cicchetti. 1990. High 
agreement but low Kappa : the problem of two 
paradoxes. J. of Clinical Epidemiology, 43:543-549 
Joseph L. Fleiss. 1971 Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76(5): 378?382 
Andrew Hayes. 2007. Answering the call for a stan-
dard reliability measure for coding data. Communi-
cation Methods and Measures 1, 1:77-89. 
Klaus Krippendorff. 2004. Content Analysis: an In-
troduction to its Methodology. Chapter 11. Sage: 
Thousand Oaks, CA. 
Klaus Krippendorff. 2004b. Reliability in Content 
Analysis: Some Common Misconceptions and 
Recommendations. Human Communication Re-
search, 30(3): 411-433, 2004 
Klaus Krippendorff. 2008. Testing the reliability of 
content analysis data: what is involved and why. In 
Klaus Krippendorff, Mark Angela Bloch (Eds) The 
content analysis reader. Sage Publications. Thou-
sand Oaks, CA. 
Klaus Krippendorff. 2009. Testing the reliability of 
content analysis data: what is involved and why. In 
Klaus Krippendorff , Mary Angela Bock. The Con-
tent Analysis Reader. Sage: Thousand Oaks, CA 
Marc Le Tallec, Jeanne Villaneau, Jean-Yves An-
toine, Dominique Duhaut. 2011 Affective Interac-
tion with a Companion Robot for vulnerable Chil-
dren: a Linguistically based Model for Emotion 
Detection. In Proc. Language Technology Confer-
ence 2011, Poznan, Poland, 445-450. 
Brian MacWhinney. 2000. The CHILDES project : 
Tools for analyzing talk. 3rd edition. Lawrence Erl-
baum associates Mahwah, NJ. 
Judith Muzerelle, Ana?s Lefeuvre, Emmanuel Schang, 
Jean-Yves Antoine, Aurore Pelletier, Denis Mau-
rel, Iris Eshkol, Jeanne Villaneau. 2014. AN-
COR_Centre, a large free spoken French corefer-
ence corpus: description of the resource and reli-
ability measures. In Proc. LREC?2014 (submitted). 
Kimberly Neuendorf. 2002. The Content Analysis 
Guidebook. Sage Publications, Thousand Oaks, CA 
James Russell. 1980. A Circumplex Model of Affect, 
J. Personality and Social Psy., 39(6): 1161-1178. 
Klaus Scherer. 2005. What are emotions? and how 
can they be measured? Social Science Information, 
44 (4):694?729. 
558
Bj?rn Schuller, Stefan Steidl, Anto Batliner. 2009. 
The Interspeech'2009 emotion challenge. In Pro-
ceedings Interspeech'2009, Brighton, UK. 312:315. 
William Scott. 1955. Reliability of content analysis: 
the case of nominal scale coding. Public Opinions 
Quaterly, 19:321-325. 
Julius Sim and Chris Wright. 2005. The Kappa Statis-
tic in Reliability Studies: Use, Interpretation, and 
Sample Size Requirements. Physical Therapy, 
85(3):257:268. 
Mike Thelwall, Kevan Buckley, Georgios Paltoglou, 
Di Cai, Arvid Kappas. 2010. Sentiment strength 
detection in short informal text. Journal of the 
American Society for Information Science and 
Technology, 61 (12): 2544?2558. 
Peter Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews, In Proceedings ACL?02, 
Philadelphia, Pennsylvania, 417-424. 
Werner Vach, 2005. The dependence of Cohen?s 
kappa on the prevalence does not matter, Journal 
of Clinical Epidemiology, 58, 655-661).  
Rose-Marie Vassallo. 2004. Comment le Grand Nord 
d?couvrit l??t?. Flammarion, Paris, France. 
Kees Vanderheyden. 1995. Le Noel des animaux de la 
montagne. Fairy tale available at the URL : 
http://www.momes.net/histoiresillustrees/contesde
montagne/noelanimaux.html 
Ekaterina Volkova, Betty Mohler, Detmar Meurers, 
Dale Gerdemann and Heinrich B?lthoff. 2010. 
Emotional perception of fairy tales: achieving 
agreement in emotion annotation of   text, In Pro-
ceedings NAACL HLT 2010. Los Angeles, CA. 
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. 
Recognizing contextual polarity in phrase-level 
sentiment analysis. In Proc. of HLT-EMNLP?2005. 
347-354. 
 
 
 
 
559
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 69?77,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Coupling Knowledge-Based and Data-Driven Systems
for Named Entity Recognition
Damien Nouvel Jean-Yves Antoine Nathalie Friburger Arnaud Soulet
Universite? Franc?ois Rabelais Tours, Laboratoire d?Informatique
3, place Jean Jaures, 41000 Blois, France
{damien.nouvel, jean-yves.antoine, nathalie.friburger, arnaud.soulet}@univ-tours.fr
Abstract
Within Information Extraction tasks,
Named Entity Recognition has received
much attention over latest decades. From
symbolic / knowledge-based to data-driven
/ machine-learning systems, many ap-
proaches have been experimented. Our
work may be viewed as an attempt to
bridge the gap from the data-driven per-
spective back to the knowledge-based one.
We use a knowledge-based system, based
on manually implemented transducers,
that reaches satisfactory performances. It
has the undisputable advantage of being
modular. However, such a hand-crafted
system requires substantial efforts to
cope with dedicated tasks. In this con-
text, we implemented a pattern extractor
that extracts symbolic knowledge, using
hierarchical sequential pattern mining
over annotated corpora. To assess the
accuracy of mined patterns, we designed a
module that recognizes Named Entities in
texts by determining their most probable
boundaries. Instead of considering Named
Entity Recognition as a labeling task, it
relies on complex context-aware features
provided by lower-level systems and
considers the tagging task as a markovian
process. Using thos systems, coupling
knowledge-based system with extracted
patterns is straightforward and leads to a
competitive hybrid NE-tagger. We report
experiments using this system and compare
it to other hybridization strategies along
with a baseline CRF model.
1 Introduction
Named Entity Recognition (NER) is an informa-
tion extraction (IE) task that aims at extracting
and categorizing specific entities (proper names
or dedicated linguistic units as time expressions,
amounts, etc.) in texts. These texts can be pro-
duced in diverse conditions. In particular, they
may correspond to either electronic written doc-
uments (Marsh & Perzanowski, 1998) or more
recently speech transcripts provided by a human
expert or an automatic speech recognition (ASR)
system (Galliano et al, 2009). The recognized en-
tities may later be used by higher-level tasks for
different purposes such as Information Retrieval
or Open-Domain Question-Answering (Voorhees
& Harman, 2000).
While NER is often considered as quite a sim-
ple task, there is still room for improvement when
it is confronted to difficult contexts. For instance,
NER systems may have to cope with noisy data
such as word sequences containing speech recog-
nition errors in ASR. In addition, NER is no more
circumscribed to proper names, but may also in-
volve common nouns (e.g., ?the judge?) or com-
plex multi-word expressions (e.g. ?the Com-
puter Science department of the New York Uni-
versity?). These complementary needs for robust
and detailed processing explain that knowledge-
based and data-driven approaches remain equally
competitive on NER tasks as shown by numerous
evaluation campaigns. For instance, the French-
speaking Ester2 evaluation campaign on radio
broadcasts (Galliano et al, 2009) has shown that
knowledge-based approaches outperformed data-
driven ones on manual transcriptions while a sys-
tem based on Conditional Random Fields (CRFs,
participant LIA) is ranked first on noisy ASR tran-
scripts. This is why the development of hybrid
systems has been investigated by the NER com-
munity.
69
In this paper, we present a strategy of hy-
bridization benefiting from features produced by
a knowledge-based system (CasEN) and a data-
driven pattern extractor (mineXtract). CasEN
has been manually implemented based on finite-
state transducers. Such a hand-crafted system
requires substantial efforts to be adapted to ded-
icated tasks. We developed mineXtract, a text-
mining system that automatically extracts infor-
mative rules, based on hierarchical sequential pat-
tern mining. Both implement processings that are
context-aware and use lexicons. Finally, to rec-
ognize NEs, we propose mStruct, a light multi-
purpose automatic annotator, parameterized using
logistic regression over available features. It takes
into account features provided by lower-level sys-
tems and annotation scheme constraints to output
a valid annotation maximizing likelihood. Our ex-
periments show that the resulting hybrid system
outperforms standalone systems and reaches per-
formances comparable to a baseline hybrid CRF
system. We consider this as a step forward to-
wards a tighter integration of knowledge-based
and data-driven approaches for NER.
The paper is organized as follows. Section 2
describes the context of this work and reviews
related work. Section 3 describes CasEN, the
knowledge-based NE-tagger. Section 4 details the
process of extracting patterns from annotated data
as informative rules. We then introduce the au-
tomatic annotator mStruct in Section 5. Section 6
describes how to gather features from systems and
present diverse hybridization strategies. Corpora,
metrics used and evaluation results are reported in
Section 7. We conclude in Section 8.
2 Context and Related Work
2.1 Ester2 Evaluation Campaign
This paper focuses on NER in the context of
the Ester2 evaluation campaign (Galliano et al,
2009). This campaign assesses system?s perfor-
mance for IE tasks over ASR outputs and manual
transcriptions of radio broadcast news (see details
in Section 7). The annotation guidelines speci-
fied 7 kinds of entities to be detected and cate-
gorized: persons (?pers?), organizations (?org?),
locations (?loc?), amounts (?amount?), time ex-
pressions (?time?), functions (?func?), products
(?prod?). Technically, the annotation scheme is
quite simple: only one annotation per entity, al-
D
Sent. Tokens and NEs
s1 <pers> Isaac Newton </pers> was admitted in
<time> June 1661 </time> to <org> Cambridge
</org>.
s2 <time> In 1696 </time>, he moved to <loc> Lon-
don </loc> as <func> warden of the Royal Mint
</func>.
s3 He was buried in <loc> Westminster Abbey </loc>.
Table 1: Sentences from an annotated corpus
most no nesting (except for persons collocated
with their function: both should be embedded in
an encompassing ?pers? NE).
We illustrate the annotation scheme using a
running example. Table 1 presents the expected
annotation in the context of Ester2 from ?Isaac
Newton was admitted in June 1661 to Cam-
bridge. In 1696, he moved to London as warden
of the Royal Mint. He was buried in Westmin-
ster Abbey.?. This example illustrates frequent
problems for NER task. Determining the extent
of a NE may be difficult. For instance, NER
should consider here either ?Westminster? (city)
or ?Westminster Abbey? (church, building). Cat-
egorizing NEs is confronted to words ambiguities,
for instance ?Cambridge? may be considered as a
city (?loc?) or a university (?org?). In addition, oral
transcripts may contain disfluencies, repetitions,
hesitations, speech recognition errors: overall dif-
ficulty is significantly increased. For these rea-
sons, NER over such noisy data is a challenging
task.
2.2 State of the Art
Knowledge-based approaches Most of the
symbolic systems rely on shallow parsing tech-
niques, applying regular expressions or linguistic
patterns over Part-Of-Speech (POS), in addition
to proper name lists checking. Some of them han-
dle a deep syntactic analysis which has proven
its ability to reach outstanding levels of perfor-
mances (Brun & Hage`ge, 2004; Brun & Hage`ge,
2009; van Shooten et al, 2009).
Data-driven approaches A large diversity of
data-driven approaches have been proposed dur-
ing the last decade for NER. Generative models
such as Hidden Markov Models or stochastic fi-
nite state transducers (Miller et al, 1998; Favre et
al., 2005) benefit from their ability to take into
account the sequential nature of language. On
the other hand, discriminative classifiers such as
70
Support Vector Machines (SVMs) are very effec-
tive when a large variety of features (Isozaki &
Kazawa, 2002) is used, but lack the ability to
take a global decision over an entire sentence.
Context Random Fields (CRFs) (Lafferty et al,
2001) have enabled NER to benefit from the ad-
vantages of both generative and discriminative ap-
proaches (McCallum & Li, 2003; Zidouni et al,
2010; Be?chet & Charton, 2010). Besides, the
robustness of data-driven / machine-learning ap-
proaches explains that the latter are more appro-
priate on noisy data such as ASR transcripts.
Hybrid systems Considering the complemen-
tary behaviors of knowledge-based and data-
driven systems for NER, projects have been con-
ducted to investigate how to conciliate both ap-
proaches. Work has been done to automatically
induce symbolic knowledge (Hingston, 2002;
Kushmerick et al, 1997) that may be used as
NE taggers. But in most cases, hybridization for
NER relies a much simpler principle: outputs of
knowledge-based systems are considered as fea-
tures by a machine learning algorithm. For in-
stance, maximum entropy may be used when a
high diversity of knowledge sources are to be
taken into account (Borthwick et al, 1998). CRFs
also have demonstrated their ability to merge
symbolic and statistic processes in a machine
learning framework (Zidouni et al, 2010).
We propose an approach to combine
knowledge-based and data-driven approaches in
a modular way. Our first concern is to implement
a module that automatically extracts knowledge
that should be interoperable with the existing
system?s transducers. This is done by focusing, in
annotated corpora, more on ?markers? (tags) that
are to be inserted between tokens (e.g. <pers>,
</pers>, <org>, </org>, etc.), than on
?labels? assigned to each token, as transducer
do. By doing so, we expect to establish a better
grounding for hybriding manually implemented
and automatically extracted patterns. Afterwards,
another module is responsible of annotating
NEs by using those context-aware patterns and
standard machine-learning techniques.
3 CasEN: a knowledge-based system
The knowledge-based system is based on CasSys
(Friburger & Maurel, 2004), a finite-state cascade
system that implements processings on texts at di-
verse levels (morphology, lexicon, chunking). It
may be used for various IE tasks, or simply to
transform or prepare a text for further processings.
The principle of this finite-state processor is to
first consider islands of certainty (Abney, 2011),
so as to give priority to most confident rules. Each
transducer describes local patterns corresponding
to NEs or interesting linguistic units available to
subsequent transducers within the cascade.
Casen is the set of NE recognition transduc-
ers. It was initially designed to process written
texts, taking into account diverse linguistic clues,
proper noun lists (covering a broad range of first
names, countries, cities, etc.) and lexical evi-
dences (expressions that may trigger recognition
of a named entity).
Figure 1: A transducer recognizing person names
Figure 2: Transducer ?patternFirstName?
As an illustration, Figure 1 presents a very sim-
ple transducer tagging person names made of an
optional title, a first name and a surname. The
boxes contain the transitions of the transducer as
items to be matched for recognizing a person?s
name. Grayed boxes contain inclusions of other
transducers (e.g. box ?patternFirstName? in Fig-
ure 1 is to be replaced by the transducer depicted
in Figure 2). Other boxes can contain lists of
words or diverse tags (e.g. <N+firstname>
for a word tagged as first name by lexicon). The
outputs of transducers are displayed below boxes
(e.g. ?{? and ?,.entity+pers+hum}? in Figure 1).
For instance, that transducer matches the
word sequence ?Isaac Newton? and outputs:
?{{Isaac ,.firstname} {Newton ,.surname} ,.en-
tity+pers+hum}?. By applying multiple transduc-
71
ers on a text sequence, CasEN can provide sev-
eral (possibly nested) annotations on a NE and
its components. This has the advantage of pro-
viding detailed information about CasEN internal
processings for NER.
Finally, the processing of examples in Table 1
leads to annotations such as:
? { { June ,.month} { 1661 ,.year} ,en-
tity+time+date+rel}
? { Westminster ,.entity+loc+city}
{ Abbey ,buildingName} ,.en-
tity+loc+buildingCityName }
In standalone mode, post-processing steps con-
vert outputs into Ester2 annotation scheme (e.g.
<pers> Isaac Newton </pers>).
Experiments conducted on newspaper docu-
ments for recognizing persons, organizations and
locations on an extract of the Le Monde corpus
have shown that CasEN reaches 93.2% of recall
and 91.1% of f-score (Friburger, 2002). Dur-
ing the Ester2 evaluation campaign, CasEN (?LI
Tours? participant in (Galliano et al, 2009)) ob-
tained 33.7% SER (Slot Error Rate, see section
about metrics description) and a f-score of 75%.
This may be considered as satisfying when one
knows the lack of adaptation of Casen to speci-
ficities of oral transcribed texts.
4 mineXtract: Pattern Mining Method
4.1 Enriching an Annotated Corpus
We investigate the use of data mining techniques
in order to supplement our knowledge-based sys-
tem. To this end, we use an annotated corpus to
mine patterns related to NEs. Sentences are con-
sidered as sequences of items (this precludes ex-
traction of patterns accross sentences). An item is
either a word from natural language (e.g. ?admit-
ted?, ?Newton?) or a tag delimiting NE categories
(e.g., <pers>, </pers> or <loc>). The an-
notated corpus D is a multiset of sequences.
Preprocessing steps enrich the corpus by (1) us-
ing lexical resources (lists of toponyms, anthro-
ponyms and so on) and (2) lemmatizing and ap-
plying a POS tagger. This results in a multi-
dimensional corpus where a token may gradually
be generalized to its lemma, POS or lexical cate-
gory. Figure 3 illustrates this process on the words
sequence ?moved to <loc> London </loc>?.
move
VER
moved
PRP
to
<loc> PN
CITY
</loc>
Figure 3: Multi-dimensional representation of the
phrase ?moved to <loc> London </loc>?
The first preprocessing step consists in consid-
ering lexical resources to assign tokens to lexi-
cal categories (e.g., CITY for ?London?) when-
ever possible. Note that those resources contain
multi-word expressions. Figure 4 provides a short
extract limited to tokens of Table 1) of lexical
ressources (totalizing 201,057 entries). This as-
signment should be ambiguous. For instance, pro-
cessing ?Westminster Abbey? would lead to cat-
egorizing ?Westminster? as CITY and the whole
as INST.
Afterwards, a POS tagger based on TreeTag-
ger (Schmid, 1994) distinguishes common nouns
(NN) from proper names (PN). Besides, token is
deleted (only PN category is kept) to avoid extrac-
tion of patterns that would be specific to a given
proper name (on Figure 3, ?London? is removed).
Figure 5 shows how POS, tokens and lemmas are
organized as a hierarchy.
Category Tokens
ANTHRO Newton, Royal . . .
CITY Cambridge, London, Westminster . . .
INST Cambridge, Royal Mint, Westminster Abbey . . .
METRIC Newton . . .
. . . . . .
Figure 4: Lexical Ressources
in of to
PRP
admit
admitted
be
was
bury
buried
VER
Figure 5: Items Hierarchy
4.2 Discovering Informative Rules
We mine this large enriched annotated corpus to
find generalized patterns correlated to NE mark-
ers. It consists in exhaustively enumerating all the
contiguous patterns mixing words, POS and cat-
72
egories. This provides a very broad spectrum of
patterns, diversely accurate to recognize NEs. As
an illustration, if you consider the words sequence
?moved to <loc> London </loc>? in Figure 3
leads to examining patterns as:
? ? VER PRP <loc> PN </loc>?
? ? VER to <loc> PN </loc>?
? ? moved PRP <loc> CITY </loc>?
The most relevant patterns will be filtered by
considering two thresholds which are usual in
data mining: support and confidence (Agrawal
& Srikant, 1994). The support of a pattern P
is its number of occurrences in D, denoted by
supp(P,D). The greater the support of P , the
more general the pattern P . As we are only inter-
ested in patterns sufficiently correlated to mark-
ers, a transduction rule R is defined as a pattern
containing at least one marker. To estimate em-
pirically how much R is accurate to detect mark-
ers, we calculate its confidence. A dedicated func-
tion suppNoMark(R,D) returns the support of
R when markers are omitted both in the rule and
in the data. The confidence of R is:
conf(R,D) =
supp(R,D)
suppNoMark(R,D)
For instance, consider the rule R = ? VER PRP
<loc>? in Table 1. Its support is 2 (sentences
s2 and s3). But its support without considering
markers is 3, since sentence s1 matches the rule
when markers are not taken in consideration. The
confidence of R is 2/3.
In practice, the whole collection of transduc-
tion rules exceeding minimal support and con-
fidence thresholds remains too large, especially
when searching for less frequent patterns. Conse-
quently, we filter-out ?redundant rules?: those for
which a more specific rule exists with same sup-
port (both cover same examples in corpus). For
instance, the rules R1 = ? VER VER in <loc>?
and R2 = ? VER in <loc>? are more general
and have same support than R3 = ? was VER
in <loc>?: we only retain the latter.
The system mineXtract implements those pro-
cessing using a level-wise algorithm (Mannila &
Toivonen, 1997).
5 mStruct: Stochastic Model for NER
We have established a common ground for the
systems to interact with a higher level model.
Our assumption is that lower level systems ex-
amine the input (sentences) and provide valu-
able clues playing a key role in the recognition
of NEs. In that context, the annotator is im-
plemented as an abstracted view of sentences.
Decisions will only have to be taken whenever
one of the lower-level systems provides infor-
mation. Formally, beginning or ending a NE
at a given position i may be viewed as the af-
fectation of a random variable P (Mi = mji)
where the value of mji is one of the markers
({?,<pers>,</pers>,<loc>,<org>, . . . }).
For a given sentence, we use binary features
triggered by lower-level systems at a given posi-
tion (see section 6.1) for predicting what marker
would be the most probable at that very position.
This may be viewed as an instance of a classifi-
cation problem (more precisely multilabel clas-
sification since several markers may appear at a
single position, but we won?t enter into that level
of detail due to lack of space). Empirical exper-
iments with diverse machine learning algorithms
using Scikit-learn (Pedregosa et al, 2011) lead us
to consider logistic regression as the most effec-
tive on the considered task.
Considering those probabilities, it is now pos-
sible to estimate the likelihood of a given annota-
tion over a sentence. Here, markers are assumed
to be independent. With this approximation, the
likehood of an annotation is computed by a sim-
ple product:
P (M1 = mj1 ,M2 = mj2 , . . . ,Mn = mjn)
?
?
i=1...n
P (Mi = mji)
As an illustration, Figure 6 details the compu-
tation of an annotation given the probability of ev-
ery markers, using the Ester2 annotation scheme.
For clarity purposes, only sufficiently probable
markers (including ?) are displayed at each po-
sition. A possible <func> is discarded (crossed
out), being less probable than a previous one. An
annotation solution <org> . . .</org> is evalu-
ated, but is less likely (0.3 ? 0.4 ? 0.9 ? 0.4 ? 0.4 ?
0.1 = 0.0017) than warden of the Royal Mint as a
function (0.6?0.4?0.9?0.3?0.5?0.4 = 0.0129)
73
which will be retained (and is the expected anno-
tation).
as
PRP
? 0.3
<func> 0.6
warden
NN
JOB
? 0.4
</func> 0.5
of
PRP
? 0.9
the
DET
? 0.3
<org> 0.2
<pers> 0.4
Royal
NP
INST
? 0.5
</pers> 0.4
Mint
NP
INST
? 0.1
</func> 0.4
<org> 0.4
Figure 6: Stochastic Annotation of a Sequence
Estimating markers probabilities allows the
model to combine evidences from separate
knowledge sources when recognizing starting or
ending boundaries. For instance, CasEN may re-
congize intermediary structures but not the whole
entity (e.g. when unexpected words appear inside
it) while extracted rules may propose markers that
are not necessarily paired. The separate detection
of markers enables the system to recognize named
entities without modeling all their tokens. This
may be useful when NER has to face noisy data
or speech disfluences.
Finally, it is not necessary to compute likeli-
hoods over all possible combination of markers,
since the annotation scheme is much constrained.
As the sentence is processed, some annotation so-
lutions are to be discarded. It is straightforward
to see that this problem may be resolved using
dynamic programming, as did Borthwick et al
(1998). Depending on the annotation scheme,
constraints are provided to the annotator which
outputs an annotation for a given sentence that
is valid and that maximizes likelihood. Our sys-
tem mStruct (micro-Structure) implements this
(potentially multi-purpose) automatic annotation
process as a separate module.
6 Hybriding systems
6.1 Gathering Clues from Systems
Figure 7 describes the diverse resources and algo-
rithms that are plugged together. The knowledge-
based system uses lists that recognize lexical pat-
terns useful for NER (e.g. proper names, but also
automata to detect time expressions, functions,
etc.). Those resources are exported and available
to the data mining software as lexical resources
(see section 4) and (as binary features) to the base-
line CRF model.
Lists
Mining
Corpus mineXtract
Transducers CasEN
Learning
Corpus
Hybridation
Gather
Features
mStruct
Figure 7: Systems Modules (Hybrid data flow)
Each system processes input text and provides
features used by the Stochastic Model mStruct. It
is quite simple to take in consideration mined in-
formative rules: each time a rule i proposes its
jth marker, a Boolean feature Mij is activated.
What is provided by CasEN is more sophisticated,
since each transducer is able to indicate more de-
tailed information (see section 3), as multiple fea-
tures separated by ?+? (e.g. ?entity+pers+hum?).
We want to benefit as much as possible from this
richness: whenever a CasEN tag begins or ends,
we activate a boolean feature for each mentioned
feature plus one for each prefixes of features (e.g.
?entity?, ?pers?, ?hum? but also ?entity.pers? and
?entity.pers.hum?).
6.2 Coupling Strategies
We report results for the following hybridizations
and CRF-based system using Wapiti (Lavergne et
al., 2010).
? CasEN: knowledge-based system standalone
? mXS: mineXtract extracts, mStruct annotates
? Hybrid: gather features from CasEN and mineX-
tract, mStruct annotates
? Hybrid-sel: as Hybrid, but features are selected
? CasEN-mXS-mine: as mXS, but text is pre-
processed by CasEN (adding a higher general-
ization level above lexical lists)
? mXS-CasEN-vote: as mXS, plus a post-
processing step as a majority vote based on mXS
and CasEN outputs
? CRF: baseline CRF, using BIO and common fea-
tures (unigrams: lemma and lexical lists, bi-
grams: previous, current and next POS)
74
Corpus Tokens Sentences NEs
Ester2-Train 1 269 138 44 211 80 227
Ester2-Dev 73 375 2 491 5 326
Ester2-Test-corr 39 704 1 300 2 798
Ester2-Test-held 47 446 1 683 3 067
Table 2: Characteristics of Corpora
? CasEN-CRF: same as CRF, but the output of
CasEN is added as a single feature (concatena-
tion of CasEN features)
7 Experimentations
7.1 Corpora and Metrics
For experimentations, we use the corpus that has
been made available after the Ester2 evaluation
campaign. Table 2 gives statistics on diverse sub-
parts of this corpus. Unfortunately, many incon-
sistencies where noted for manual annotation, es-
pecially for ?Ester2-Train? part that won?t be used
for training.
There were fewer irregularities in other parts of
the corpus. Although, manual corrections were
done on half of the Test corpus (Nouvel et al,
2010) (Ester2-Test-corr in Table 2), to obtain a
gold standard that we will use to evaluate our ap-
proach. The remaining part of the Test corpus
(Ester2-Test-held in Table 2) merged with the Dev
part constitute our training set (Ester2-Dev in Ta-
ble 2), used as well to extract rules with mineX-
tract, to estimate stochastic model probabilities of
mStruct and to learn CRF models.
We evaluate systems using following metrics:
? detect: rate of detection of the presence of
any marker (binary decision) at any position
? desamb: f-score of markers when comparing
N actual markers to N most probable mark-
ers, computed over positions where k mark-
ers are expected (N=k) or the most probable
marker is not ? (N=1)
? precision, recall, f-score: evaluation of NER
by categories by examining labels assigned
to tokens (similarly to Ester2 results)
? SER (Slot Error Rate): weighted error rate of
NER (official Ester2 performance metric, to
be lowered), where errors are discounted per
entity as Galliano et al (2009) (deletion and
insertion errors are weighted 1 whereas type
and boundary errors, 0.5)
System support confidence detect disamb f-score SER
CasEN ? ? ? ? 78 30.8
mXS 5 0.1 97 73 76 28.4
5 0.5 96 71 74 31.2
15 0.1 96 72 73 30.1
Hybrid 5 0.1 97 78 79 26.3
5 0.5 97 77 77 28.3
15 0.1 97 78 76 28.2
inf inf 96 71 70 42.0
Table 3: Performance of Systems
7.2 Comparing Hybridation with Systems
First, we separately evaluate systems. While
CasEN is not to be parameterized, mineXtract
has to be given minimum frequency and support
thresholds. Table 3 shows results for each sys-
tem separately and for the combination of sys-
tems. Results obtained by mXS show that even
less confident rules are improving performances.
Generally speaking, the detect score is very high,
but this mainly due to the fact that the ? case is
very frequent. The disamb score is much corre-
lated to the SER. This reflects the fact that the
challenge is for mStruct to determine the correct
markers to insert.
Comparing systems shows that the hybridiza-
tion strategy is competitive. The knowledge-
based system yields to satisfying results. mXS
obtains slightly better SER and the hybrid sys-
tem outperforms both in most cases. Considering
SER, the only exception to this is the ?inf? line
(mStruct uses only CasEN features) where perfor-
mances are degraded. We note that mStruct ob-
tains better results as more rules are extracted.
7.3 Assessing Hybridation Strategies
amount func loc org pers time all
10
20
30
40
50
CasEN
mXS
Hybrid
Hybrid-sel
Figure 8: SER of Systems by NE types
75
System precision recall f-score SER
Hybrid-sel 83.1 74.8 79 25.2
CasEN-mXS-mine 76.8 75.5 76 29.4
mXS-CasEN-vote 78.7 79.0 79 26.9
CRF 83.8 77.3 80 26.1
CasEN-CRF 84.1 77.5 81 26.0
Table 4: Comparing performances of systems
In a second step, we look in detail what NE
types are the most accurately recognized. Those
results are reported in Figure 8, where is depicted
the error rates (to be lowered) for main types
(?prod?, being rare, is not reported). This revealed
that features provided by CasEN for ?loc? type ap-
peared to be unreliable for mStruct. Therefore, we
filtered-out related features, so as to couple sys-
tems in a more efficient fashion. This leads to a
1.1 SER gain (from 26.3 to 25.2) when running
the so-called ?Hybrid-sel? system, and demon-
strates that the hybridation is very sensitive to
what is provided by CasEN.
With this constrained hybridization, we com-
pare previous results to other hybridization strate-
gies and a baseline CRF system as described in
section 6. Those experiments are reported in Ta-
ble 4. We see that, when considering SER, the hy-
bridization strategy using CasEN features within
mStruct stochastic model slightly outperforms
?simpler? hybridizations schemes (pre-processing
or post-processing with CasEN) and the CRF
model (even when it uses CasEN preprocessing
as a single unigram feature).
However the f-score metric gives advantage
to CasEN-CRF, especially when considering re-
call. By looking indepth into errors and when re-
minded that SER is a weighted metric based on
slots (entities) while f-score is based on tokens
(see section 7.1), we noted that on longest NEs
(mainly ?func?), Hybrid-sel does type errors (dis-
counted as 0.5 in SER) while CasEN-CRF does
deletion errors (1 in SER). This is pointed out by
Table 5. The influence of error?s type is clear
when considering the SER for ?func? type for
which Hybrid-sel is better while f-score doesn?t
measure such a difference.
7.4 Discussion and Perspectives
Assessment of performances using a baseline
CRF pre-processed by CasEN and the hybrided
strategy system shows that our approach is com-
petitive, but do not allow to draw definitive con-
System NE type insert delet type SER f-score
Hybrid-sel func 8 21 7 40.3 65
all 103 205 210 25.2 79
CasEN-CRF func 9 37 0 53.5 64
all 77 251 196 26.0 81
Table 5: Impact of ?func? over SER and f-score
clusions. We keep in mind that the evaluated CRF
could be further improved. Other methods have
been successfully experimented to couple more
efficiently that kind of data-driven approach with
a knowledge-based one (for instance Zidouni et
al. (2010) reports 20.3% SER on Ester2 test cor-
pus, but they leverage training corpus).
Nevertheless, the CRFs models do not allow
to directly extract symbolic knowledge from data.
We aim at organizing our NER system in a mod-
ular way, so as to be able to adapt it to dedicated
tasks, even if no training data is available. Results
show that this proposed hybridization reaches a
satisfactory level of performances.
This kind of hybridization, focusing on ?mark-
ers?, is especially relevant for annotation tasks.
As a next step, experiments are to be conducted
on other tasks, especially those involving nested
annotations that our current system is able to pro-
cess. We will also consider how to better organize
and integrate automatically extracted informative
rules into our existing knowledge-based system.
8 Conclusion
In this paper, we consider Named Entity Recog-
nition task as the ability to detect boundaries of
Named Entities. We use CasEN, a knowledge-
based system based on transducers, and mineX-
tract, a text-mining approach, to extract informa-
tive rules from annotated texts. To test these rules,
we propose mStruct, a light multi-purpose annota-
tor that has the originality to focus on boundaries
of Named Entities (?markers?), without consider-
ing the labels associated to tokens. The extraction
module and the stochastic model are plugged to-
gether, resulting in mXS, a NE-tagger that gives
satisfactory results. Those systems altogether
may be hybridized in an efficient fashion. We as-
sess performances of our approach by reporting
results of our system compared to other baseline
hybridization strategies and CRF systems.
76
References
Steven P. Abney. 1991. Parsing by Chunks. Principle-
Based Parsing, 257?278.
Rakesh Agrawal and Ramakrishnan Srikant. 1994.
Fast algorithms for mining association rules in large
databases. Very Large Data Bases, 487?499.
Fre?de?ric Bechet and Eric Charton. 2010. Unsuper-
vised knowledge acquisition for Extracting Named
Entities from speech. Acoustics, Speech, and Signal
Processing (ICASSP?10), Dallas, USA.
Andrew Borthwick, John Sterling, Eugene Agichtein
and Ralph Grishman. 1998. Exploiting Di-
verse Knowledge Sources via Maximum Entropy
in Named Entity Recognition. Very Large Corpora
(VLC?98), Montreal, Canada.
Caroline Brun and Caroline Hage`ge. 2004. Intertwin-
ing Deep Syntactic Processing and Named Entity
Detection. Advances in Natural Language Process-
ing, 3230:195-206.
Caroline Brun and Maud Ehrmann. 2009. Adapta-
tion of a named entity recognition system for the es-
ter 2 evaluation campaign. Natural Language Pro-
cessing and Knowledge Engineering (NLPK?09),
Dalian, China.
Beno??t Favre, Fre?de?ric Be?chet, and Pascal Nocera.
2005. Robust Named Entity Extraction from Large
Spoken Archives. Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP?05), Vancouver, Canada.
Nathalie Friburger. 2002. Reconnaissance automa-
tique des noms propres: Application a` la classifica-
tion automatique de textes journalistiques. PhD.
Nathalie Friburger and Denis Maurel. 2004. Finite-
state transducer cascades to extract named entities.
Theoretical Computer Sciences (TCS), 313:93?104.
Sylvain Galliano, Guillaume Gravier and Laura
Chaubard. 2009. The ESTER 2 evaluation cam-
paign for the rich transcription of French radio
broadcasts. International Speech Communication
Association (INTERSPEECH?09), Brighton, UK.
Philip Hingston. 2002. Using Finite State Automata
for Sequence Mining. Australasian Computer Sci-
ence Conference (ACSC?02), Melbourne, Australia.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. Conference on Computational linguistics
(COLING?02), Taipei, Taiwan.
Nicholas Kushmerick and Daniel S. Weld and Robert
Doorenbos. 1997. Wrapper Induction for Informa-
tion Extraction. International Joint Conference on
Artificial Intelligence (IJCAI?97), Nagoya, Japan.
John D. Lafferty, Andrew McCallum and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. International Conference on Ma-
chine Learning (ICML?01), Massachusetts, USA.
Thomas Lavergne and Olivier Cappe? and Franc?ois
Yvon 2010. Practical Very Large Scale CRFs. As-
sociation for Computational Linguistics (ACL?10),
Uppsala, Sweden.
Heikki Mannila and Hannu Toivonen. 1997. Level-
wise search and borders of theories in knowledge
discovery. Data Mining and Knowledge Discovery,
1(3):241?258.
Elaine Marsh and Dennis Perzanowski. 1998. MUC-7
Evaluation of IE Technology: Overview of Results.
Message Understanding Conference (MUC-7).
Andrew McCallum and Wei Li. 2003. Early re-
sults for named entity recognition with conditional
random fields, feature induction and web-enhanced
lexicons. Computational Natural Language Learn-
ing (CONLL?03), Edmonton, Canada.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone and
Ralph Weischedel. 1998. Algorithms That Learn
To Extract Information BBN: Description Of The
Sift System As Used For MUC-7. Message Under-
standing Conference (MUC-7).
Damien Nouvel, Jean-Yves Antoine, Nathalie
Friburger and Denis Maurel. 2010. An Analysis
of the Performances of the CasEN Named Entities
Recognition System in the Ester2 Evaluation
Campaign. Language Resources and Evaluation
(LREC?10), Valetta, Malta.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot and E?douard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Helmut Schmid. 1994. Probabilistic POS Tagging
Using Decision Trees. New Methods in Language
Processing (NEMLP?94, Manchester, UK.
Boris W. van Schooten, Sophie Rosset, Olivier Galib-
ert, Aure?lien Max, Rieks op den Akker, and Gabriel
Illouz. 2009. Handling speech in the ritel QA
dialogue system. International Speech Communi-
cation Association (INTERSPEECH?09), Brighton,
UK.
Ellen M. Voorhees and Donna Harman. 2000.
Overview of the Ninth Text REtrieval Conference
(TREC-9). International Speech Communication
Association (INTERSPEECH?09), Brighton, UK.
Azeddine Zidouni and Sophie Rosset and Herve? Glotin
2010. Efficient combined approach for named
entity recognition in spoken language. Interna-
tional Speech Communication Association (INTER-
SPEECH?10), Makuhari, Japan.
77
