Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 791?800, Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Structured Output Learning
based on a Hybrid Generative and Discriminative Approach
Jun Suzuki, Akinori Fujino and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{jun, a.fujino, isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a framework for
semi-supervised structured output learning
(SOL), specifically for sequence labeling,
based on a hybrid generative and discrim-
inative approach. We define the objective
function of our hybrid model, which is writ-
ten in log-linear form, by discriminatively
combining discriminative structured predic-
tor(s) with generative model(s) that incor-
porate unlabeled data. Then, unlabeled
data is used in a generative manner to in-
crease the sum of the discriminant functions
for all outputs during the parameter estima-
tion. Experiments on named entity recogni-
tion (CoNLL-2003) and syntactic chunking
(CoNLL-2000) data show that our hybrid
model significantly outperforms the state-
of-the-art performance obtained with super-
vised SOL methods, such as conditional ran-
dom fields (CRFs).
1 Introduction
Structured output learning (SOL) methods, which
attempt to optimize an interdependent output space
globally, are important methodologies for certain
natural language processing (NLP) tasks such as
part-of-speech tagging, syntactic chunking (Chunk-
ing) and named entity recognition (NER), which are
also referred to as sequence labeling tasks. When we
consider the nature of these sequence labeling tasks,
a semi-supervised approach appears to be more nat-
ural and appropriate. This is because the number of
features and parameters typically become extremely
large, and labeled examples can only sparsely cover
the parameter space, even if thousands of labeled ex-
amples are available. In fact, many attempts have re-
cently been made to develop semi-supervised SOL
methods (Zhu et al, 2003; Li and McCallum, 2005;
Altun et al, 2005; Jiao et al, 2006; Brefeld and
Scheffer, 2006).
With the generative approach, we can easily in-
corporate unlabeled data into probabilistic models
with the help of expectation-maximization (EM) al-
gorithms (Dempster et al, 1977). For example, the
Baum-Welch algorithm is a well-known algorithm
for training a hidden Markov model (HMM) of se-
quence learning. Generally, with sequence learning
tasks such as NER and Chunking, we cannot expect
to obtain better performance than that obtained us-
ing discriminative approaches in supervised learning
settings.
In contrast to the generative approach, with the
discriminative approach, it is not obvious how un-
labeled training data can be naturally incorporated
into a discriminative training criterion. For ex-
ample, the effect of unlabeled data will be elimi-
nated from the objective function if the unlabeled
data is directly used in traditional i.i.d. conditional-
probability models. Nevertheless, several attempts
have recently been made to incorporate unlabeled
data in the discriminative approach. An approach
based on pairwise similarities, which encourage
nearby data points to have the same class label, has
been proposed as a way of incorporating unlabeled
data discriminatively (Zhu et al, 2003; Altun et al,
2005; Brefeld and Scheffer, 2006). However, this
approach generally requires joint inference over the
whole data set for prediction, which is not practi-
cal as regards the large data sets used for standard
sequence labeling tasks in NLP. Another discrim-
inative approach to semi-supervised SOL involves
the incorporation of an entropy regularizer (Grand-
791
valet and Bengio, 2004). Semi-supervised condi-
tional random fields (CRFs) based on a minimum
entropy regularizer (SS-CRF-MER) have been pro-
posed in (Jiao et al, 2006). With this approach, the
parameter is estimated to maximize the likelihood of
labeled data and the negative conditional entropy of
unlabeled data. Therefore, the structured predictor
is trained to separate unlabeled data well under the
entropy criterion by parameter estimation.
In contrast to these previous studies, this paper
proposes a semi-supervised SOL framework based
on a hybrid generative and discriminative approach.
A hybrid approach was first proposed in a super-
vised learning setting (Raina et al, 2003) for text
classification. (Fujino et al, 2005) have developed a
semi-supervised approach by discriminatively com-
bining a supervised classifier with generative mod-
els that incorporate unlabeled data. We extend this
framework to the structured output domain, specifi-
cally for sequence labeling tasks. Moreover, we re-
formalize the objective function to allow the incor-
poration of discriminative models (structured pre-
dictors) trained from labeled data, since the original
framework only considers the combination of gen-
erative classifiers. As a result, our hybrid model can
significantly improve on the state-of-the-art perfor-
mance obtained with supervised SOL methods, such
as CRFs, even if a large amount of labeled data is
available, as shown in our experiments on CoNLL-
2003 NER and CoNLL-2000 Chunking data. In
addition, compared with SS-CRF-MER, our hybrid
model has several good characteristics including a
low calculation cost and a robust optimization in
terms of a sensitiveness of hyper-parameters. This
is described in detail in Section 5.3.
2 Supervised SOL: CRFs
This paper focuses solely on sequence labeling
tasks, such as named entity recognition (NER) and
syntactic chunking (Chunking), as SOL problems.
Thus, let x=(x1, . . . , xS)?X be an input sequence,
and y=(y0, . . . , yS+1)?Y be a particular output se-
quence, where y0 and yS+1 are special fixed labels
that represent the beginning and end of a sequence.
As regards supervised sequence learning, CRFs
are recently introduced methods that constitute flex-
ible and powerful models for structured predictors
based on undirected graphical models that have been
globally conditioned on a set of inputs (Lafferty
et al, 2001). Let ? be a parameter vector and
f(ys?1, ys,x) be a (local) feature vector obtained
from the corresponding position s given x. CRFs
define the conditional probability, p(y|x), as being
proportional to a product of potential functions on
the cliques. That is, p(y|x) on a (linear-chain) CRF
can be defined as follows:
p(y|x;?) = 1Z(x)
S+1?
s=1
exp(? ? f (ys?1, ys,x)).
Z(x) =
?
y
?S+1
s=1 exp(? ? f(ys?1, ys,x)) is a nor-
malization factor over all output values, Y , and is
also known as the partition function.
For parameter estimation (training), given labeled
data Dl = {(xk,yk)}Kk=1, the Maximum a Posteri-
ori (MAP) parameter estimation, namely maximiz-
ing log p(?|Dl), is now the most widely used CRF
training criterion. Thus, we maximize the following
objective function to obtain optimal ?:
LCRF(?) =
?
k
[
? ?
?
s
f s ? logZ(x
k)
]
+ log p(?), (1)
where f s is an abbreviation of f(ys?1, ys,x) and
p(?) is a prior probability distribution of ?. A
gradient-based optimization algorithm such as L-
BFGS (Liu and Nocedal, 1989) is widely used for
maximizing Equation (1). The gradient of Equation
(1) can be written as follows:
?LCRF(?) =
?
k
Ep?(yk,xk;?)
[?
s
f s
]
?
?
k
Ep(Y|xk;?)
[?
s
f s
]
+? log p(?).
Calculating Ep(Y|x,?) as well as the partition func-
tion Z(x) is not always tractable. However, for
linear-chain CRFs, a dynamic programming algo-
rithm similar in nature to the forward-backward al-
gorithm in HMMs has already been developed for
an efficient calculation (Lafferty et al, 2001).
For prediction, the most probable output, that is,
y? = argmaxy?Y p(y|x;?), can be efficiently ob-
tained by using the Viterbi algorithm.
3 Hybrid Generative and Discriminative
Approach to Semi-Supervised SOL
In this section, we describe our formulation of a
hybrid approach to SOL and a parameter estima-
tion method for sequence predictors. We assume
792
that we have a set of labeled and unlabeled data,
D = {Dl,Du}, where Dl = {(xn,yn)}Nn=1 and
Du = {xm}Mm=1.
Let us assume that we have I-units of discrimina-
tive models, pDi , and J-units of generative models,
pGj . Our hybrid model for a structured predictor is
designed by the discriminative combination of sev-
eral joint probability densities of x and y, p(x,y).
That is, the posterior probability of our hybrid model
is defined by providing the log-values of p(x,y) as
the features of a log-linear model, such that:
R(y|x;?,?,?)
=
?
i p
D
i (x,y;?i)?i
?
j p
G
j (x,y; ?j)?j?
y
?
i p
D
i (x,y;?i)?i
?
j p
G
j (x,y; ?j)?j
=
?
i p
D
i (y|x;?i)?i
?
j p
G
j (x,y; ?j)?j?
y
?
i p
D
i (y|x;?i)?i
?
j p
G
j (x,y; ?j)?j
.
(2)
Here, ? = {{?i}Ii=1, {?j}
I+J
j=I+1} represents the
discriminative combination weight of each model
where ?i,?j? [0, 1]. Moreover, ?={?i}Ii=1 and ?=
{?j}Jj=1 represent model parameters of individual
models estimated from labeled and unlabeled data,
respectively. Using pD(x,y) = pD(y|x)pD(x), we
can derive the third line from the second line, where
pDi (x;?i)?i for all i are canceled out. Thus, our hy-
brid model is constructed by combining discrimina-
tive models, pDi (y|x;?i), with generative models,
pGj (x,y;?j).
Hereafter, let us assume that our hybrid model
consists of CRFs for discriminative models, pDi , and
HMMs for generative models, pGj , shown in Equa-
tion (2), since this paper focuses solely on sequence
modeling. For HMMs, we consider a first order
HMM defined in the following equation:
p(x,y|?) =
S+1?
s=1
?ys?1,ys?ys,xs ,
where ?ys?1,ys and ?ys,xs represent the transition
probability between states ys?1 and ys and the sym-
bol emission probability of the s-th position of the
corresponding input sequence, respectively, where
?yS+1,xS+1 = 1.
It can be seen that the formalization in the log-
linear combination of our hybrid model is very sim-
ilar to that of LOP-CRFs (Smith et al, 2005). In
fact, if we only use a combination of discriminative
models (CRFs), which is equivalent to ?j = 0 for
all j, we obtain essentially the same objective func-
tion as that of the LOP-CRFs. Thus, our framework
can also be seen as an extension of LOP-CRFs that
enables us to incorporate unlabeled data.
3.1 Discriminative Combination
For estimating the parameter ?, let us assume that
we already have discriminatively trained models on
labeled data, pDi (y|x;?i). We maximize the fol-
lowing objective function for estimating parameter
? under a fixed ?:
LHySOL(?|?) =
?
n
logR(yn|xn;?,?,?)+log p(?). (3)
where p(?) is a prior probability distribution of ?.
The value of ? providing a global maximum of
LHySOL(?|?) is guaranteed under an arbitrary fixed
value in the ? domain, since LHySOL(?|?) is a con-
cave function of ?. Thus, we can easily maximize
Equation (3) by using a gradient-based optimization
algorithm such as (bound constrained) L-BFGS (Liu
and Nocedal, 1989).
3.2 Incorporating Unlabeled Data
We cannot directly incorporate unlabeled data for
discriminative training such as Equation (3) since
the correct outputs y for unlabeled data are un-
known. On the other hand, generative approaches
can easily deal with unlabeled data as incomplete
data (data with missing variable y) by using a mix-
ture model. A well-known way to achieve this in-
corporation is to maximize the log likelihood of un-
labeled data with respect to the marginal distribution
of generative models as
L(?) =
?
m
log
?
y
p(xm,y; ?).
In fact, (Nigam et al, 2000) have reported that using
unlabeled data with a mixture model can improve
the text classification performance.
According to Bayes? rule, p(y|x;?) ?
p(x,y;?), the discriminant functions of gener-
ative classifiers are provided by generative models
p(x,y;?). Therefore, we can regard L(?) as the
logarithm of the sum of discriminant functions for
all missing variables y of unlabeled data. Following
this view, we can directly incorporate unlabeled
data into our hybrid model by maximizing the
793
discriminant functions g of our hybrid model in
the same way as for a mixture model as explained
above. Thus, we maximize the following objective
function for estimating the model parameters ? for
generative models of unlabeled data:
G(?|?) =
?
m
log
?
y
g(xm,y;?) + log p(?). (4)
where p(?) is a prior probability distribution of ?.
Here, the discriminant function g of output y given
input x in our hybrid model can be obtained by the
numerator on the third line of Equation (2), since the
denominator does not affect the determination of y,
that is,
g(x,y;?) =
?
i
pDi (y|x;?i)?i
?
j
pGj (x,y; ?j)?j .
Under a fixed ?, we can estimate the local max-
imum of G(?|?) around the initialized value of ?
by an iterative computation such as the EM algo-
rithm (Dempster et al, 1977). Let ??? and ?? be
estimates of ? in the next and current steps, respec-
tively. Using Jensen?s inequality, log a ? a ? 1,
we obtain a Q-function that satisfies the inequality
G(???|?)?G(??|?)?Q(???,??;?)?Q(??,??;?),
such that
Q(???,??;?)
=
?
j
?j
?
m
?
y
R(y|xm;?,??,?) log pGj (xm,y;???)
+ log p(???).
(5)
Since Q(??,??;?) is independent of ???, we can
improve the value of G(?|?) by computing ??? to
maximize Q(???,??;?). We can obtain a ? es-
timate by iteratively performing this update while
G(?|?) is hill climbing.
As shown in Equation (5), R is used for estimat-
ing the parameter ?. The intuitive effect of maxi-
mizing Equation (4) is similar to performing ?soft-
clustering?. That is, unlabeled data is clustered with
respect to the R distribution, which also includes in-
formation about labeled data, under the constraint of
generative model structures.
3.3 Parameter Estimation Procedure
According to our definition, the ? and ? estima-
tions are mutually dependent. That is, the param-
eters of the hybrid model, ?, should be estimated
1.Given training set: Du = {xm}Mm=1 and
Dl = {D?l = {(xk,yk)}Kk=1, D??l = {(xn,yn)}Nn=1}
2.Compute ?, using D?l.
3.Initialize ?(0), ?(0) and t ? 0.
4.Perform the following until |?
(t+1)??(t)|
|?(t)| < ?.
4.1. Compute ?(t+1) to maximize Equation (4)
under fixed ?(t) and ? using Du.
4.2. Compute ?(t+1) to maximize Equation (3)
under fixed ?(t+1) and ? using D??l .
4.3. t ? t + 1.
5.Output a structured predictor R(y|x,?,?(t),?(t)).
Figure 1: Algorithm of learning model parameters
used in our hybrid model.
using Equation (3) with a fixed ?, while the param-
eters of the generative models, ?, should be esti-
mated using Equation (4) with a fixed ?. As a solu-
tion to our parameter estimation, we search for the
? and ? that maximize LHySOL(?|?) and G(?|?)
simultaneously. For this search, we compute ? and
? by maximizing the objective functions shown in
Equations (4) and (3) iteratively and alternately. We
summarize the algorithm for estimating these model
parameters in Figure 1.
Note that during the ? estimation (procedure 4.2
in Figure 1), ? can be over-fitted to the labeled train-
ing data if we use the same labeled training data as
used for the? estimation. There are several possible
ways to reduce this over-fit. In this paper, we select
one of the simplest; we divide the labeled training
data Dl into two distinct sets D?l and D??l . Then, D?l
and D??l are individually used for estimating ? and
?, respectively. In our experiments, we divide the
labeled training data Dl so that 4/5 is used for D?l
and the remaining 1/5 for D??l .
3.4 Efficient Parameter Estimation Algorithm
Let NR(x) represent the denominator of Equation
(2), that is the normalization factor of R. We can
rearrange Equation (2) as follows:
R(y|x;?,?,?) =
?
s
?
i
[
V Di,s
]?i ?
j
[
V Gj,s
]?j
NR(x)
?
i[Zi(x)]?i
, (6)
where V Di,s represents the potential function of the
s-th position of the sequence in the i-th CRF and
V Gj,s represents the probability of the s-th position
in the j-th HMM, that is, V Di,s = exp(?i ? f s) and
V Gj,s = ?ys?1,ys?ys,xs , respectively. See the Ap-
pendix for the derivation of Equation (6) from Equa-
tion (2).
794
To estimate ?(t+1), namely procedure 4.2 in Fig-
ure 1, we employ the derivatives with respect to ?i
and ?j shown in Equation (6), which are the parame-
ters of the discriminative and generative models, re-
spectively. Thus, we obtain the following derivatives
with respect to ?i:
?LHySOL(?|?)
??i
=
?
n
log pDi (yn|xn) +
?
n
logZDi (xn)
?
?
n
ER(Y|xn;?,?,?)
[?
s
log V Di,s
]
.
The first and second terms are constant during it-
erative procedure 4 in our optimization algorithm
shown in Figure 1. Thus, we only need to calcu-
late these values once at the beginning of proce-
dure 4. Let ?s(y) and ?s(y) represent the forward
and backward state costs at position s with output
y for corresponding input x. Let Vs(y, y?) repre-
sent the products of the total value of the transition
cost between s?1 and s with labels y and y? in the
corresponding input sequence, that is, Vs(y, y?) =?
i[V Di,s(y, y?)]?i
?
j [V Gj,s(y, y?)]?j . The third term,
which indicates the expectation of potential func-
tions, can be rewritten in the form of a forward-
backward algorithm, that is,
ER(Y|x;?,?,?)
[?
s
log V Di,s
]
= 1ZR(x)
?
s
?
y,y?
?s?1(y)Vs(y, y?)?s(y
?) log V Di,s(y, y?),
(7)
where ZR(x) represents the partition function of our
hybrid model, that is, ZR(x)=NR(x)
?
i[Zi(x)]?i .
Hence, the calculation of derivatives with respect to
?i is tractable since we can incorporate the same
forward-backward algorithm as that used in a stan-
dard CRF.
Then, the derivatives with respect to ?j , which are
the parameters of generative models, can be written
as follows:
?LHySOL(?|?)
??j
=
?
n
log pGj (xn,yn)?
?
n
ER(Y|xn;?,?,?)
[?
s
log V Gj,s
]
.
Again, the second term, which indicates the expec-
tation of transition probabilities and symbol emis-
sion probabilities, can be rewritten in the form of a
forward-backward algorithm in the same manner as
?i, where the only difference is that V Di,s is substi-
tuted by V Gj,s in Equation (7).
To estimate?(t+1), which is procedure 4.1 in Fig-
ure 1, the same forward-backward algorithm as used
in standard HMMs is available since the form of our
Q-function shown in Equation (5) is the same as that
of standard HMMs. The only difference is that our
method uses marginal probabilities given by R in-
stead of the p(x,y;?) of standard HMMs.
Therefore, only a forward-backward algorithm is
required for the efficient calculation of our param-
eter estimation process. Note that even though our
hybrid model supports the use of a combination of
several generative and discriminative models, we
only need to calculate the forward-backward algo-
rithm once for each sample during optimization pro-
cedures 4.1 and 4.2. This means that the required
number of executions of the forward-backward al-
gorithm for our parameter estimation is independent
of the number of models used in the hybrid model.
In addition, after training, we can easily merge all
the parameter values in a single parameter vector.
This means that we can simply employ the Viterbi-
algorithm for evaluating unseen samples, as well as
that of standard CRFs, without any additional cost.
4 Experiments
We examined our hybrid model (HySOL) by ap-
plying it to two sequence labeling tasks, named
entity recognition (NER) and syntactic chunking
(Chunking). We used the same Chunking and
?English? NER data as those used for the shared
tasks of CoNLL-2000 (Tjong Kim Sang and Buch-
holz, 2000) and CoNLL-2003 (Tjong Kim Sang and
Meulder, 2003), respectively.
For the baseline method, we performed a condi-
tional random field (CRF), which is exactly the same
training procedure described in (Sha and Pereira,
2003) with L-BFGS. Moreover, LOP-CRF (Smith et
al., 2005) is also compared with our hybrid model,
since the formalism of our hybrid model can be seen
as an extension of LOP-CRFs as described in Sec-
tion 3. For CRF, we used the Gaussian prior as
the second term on the RHS in Equation (1), where
?2 represents the hyper-parameter in the Gaussian
prior. In contrast, for LOP-CRF and HySOL, we
used the Dirichlet priors as the second term on the
795
?1 f(words), f(lwords), f(poss), f(wtypes),
f(poss?1, poss), f(wtypes?1, wtypes),
f(poss, poss+1), f(wtypes, wtypes+1),
f(pref1s), f(pref2s), f(pref3s), f(pref4s),
f(suf1s), f(suf2s), f(suf3s), f(suf4s)
?2 f(words), f(lwords), f(poss), f(wtypes),
f(words?1), f(lwords?1), f(poss?1), f(wtypes?1),
f(words?2), f(lwords?2), f(poss?2), f(wtypes?2),
f(poss?2, poss?1), f(wtypes?2, wtypes?1)
?3 f(words), f(lwords), f(poss), f(wtypes),
f(words+1), f(lwords+1), f(poss+1), f(wtypes+1),
f(words+2), f(lwords+2), f(poss+2), f(wtypes+2),
f(poss+1, poss+2), f(wtypes+1, wtypes+2)
?4 all of the above
lword : lowercase of word, wtype : ?word type?
pref1-4: 1-4 character prefix of word
suf1-4 : 1-4 character suffix of word
Table 1: Features used in NER experiments
RHS in Equations (3), and (4), where ? and ? are the
hyper-parameters in each Dirichlet prior.
4.1 Named Entity Recognition Experiments
The English NER data consists of 203,621, 51,362
and 46,435 words from 14,987, 3,466 and 3,684 sen-
tences in training, development and test data, re-
spectively, with four named entity tags, PERSON,
LOCATION, ORGANIZATION and MISC, plus the
?O? tag. The unlabeled data consists of 17,003,926
words from 1,029,122 sentences. These data sets are
exactly the same as those provided for the shared
task of CoNLL-2003.
We slightly extended the feature set of the sup-
plied data by adding feature types such as ?word
type?, and word prefix and suffix. Examples of
?word type? include whether the word is capitalized,
contains digit or contains punctuation, which basi-
cally follows the baseline features of (Sutton et al,
2006) without regular expressions. Note that, unlike
several previous studies, we did not employ addi-
tional information from external resources such as
gazetteers. All our features can be automatically ex-
tracted from the supplied data.
For LOP-CRF and HySOL, we used four base dis-
criminative models trained by CRFs with different
feature sets. Table 1 shows the feature sets we used
for training these models. The design of these fea-
ture sets was derived from a suggestion in (Smith et
al., 2005), which exhibited the best performance in
the several feature division. Note that the CRF for
the comparison method was trained by using all fea-
?1 f(words), (poss),
f(words?1, words), f(poss?1, poss),
f(words, words+1), f(poss, poss+1)
?2 f(words), (poss),
f(words?1), f(poss?1), f(words?2), f(poss?2),
f(words?2, words?1), f(poss?2, poss?1)
?3 f(words), (poss),
f(words+1), f(poss+1), f(words+2), f(poss+2),
f(words+1, words+2), f(poss+1, poss+2)
?4 all of the above
Table 2: Features used in Chunking experiments
ture types, namely the same as ?4.
As we explained in Section 3.3, for training
HySOL, the parameters of four discriminative mod-
els, ?, were trained from 4/5 of the labeled training
data, and ? were trained from remaining 1/5. For
the features of the generative models, we used all of
the feature types shown in Figure 1. Note that one
feature type corresponds to one HMM. Thus, each
HMM maintains to consist of a non-overlapping fea-
ture set since each feature type only generates one
symbol per state.
4.2 Syntactic Chunking Experiments
CoNLL-2000 Chunking data was obtained from the
Wall Street Journal (WSJ) corpus: sections 15-18 as
training data (8,936 sentences and 211,727 words),
and section 20 as test data (2,012 sentences and
47,377 words), with 11 different chunk-tags, such
as NP and VP plus the ?O? tag, which represents the
region outside any target chunk.
For LOP-CRF and HySOL, we also used four
base discriminative models trained by CRFs with
different feature sets. Table 2 shows the feature set
we used in the Chunking experiments. We used the
feature set of the supplied data without any exten-
sion of additional feature types.
To train HySOL, we used the same unlabeled data
as used for our NER experiments (17,003,926 words
from the Reuters corpus). Moreover, the division of
the labeled training data and the feature set of the
generative models were derived in the same man-
ner as our NER experiments (see Section 4.1). That
is, we divided the labeled training data into 4/5 for
estimating ? and 1/5 for estimating ?; one feature
type shown in Table 2 is assigned in one generative
model.
796
methods (hyper-params) F?=1 (gain) Sent (gain)
CRF (?2=100.0) 84.70 - 78.30 -
(4/5 labeled data, ?2=100.0) 83.74 (-0.96) 77.06 (-1.24)
LOP-CRF (??=0.1) 84.90 (+0.20) 79.02 (+0.72)
HySOL (??=0.1,??=0.0001) 87.20 (+2.50) 81.19 (+2.89)
(w/o prior) 86.86 (+2.16) 80.75 (+2.45)
w/o pGj ?j ( ??=1.0) 84.56 (-0.14) 78.23 (-0.07)
Table 3: NER performance (CoNLL-2003)
methods (hyper-params) F?=1 (gain) Sent (gain)
CRF (?2=10.0) 93.87 - 59.84 -
(4/5 labeled data, ?2=10.0) 93.70 (-0.17) 58.85 (-0.99)
LOP-CRF (??=0.1) 93.91 (+0.04) 60.34 (+0.50)
HySOL (??=1.0,??=0.0001) 94.30 (+0.43) 61.73 (+1.89)
(w/o prior) 94.17 (+0.30) 61.23 (+1.39)
w/o pGj ?j (??=1.0) 93.84 (-0.03) 59.74 (-0.10)
Table 4: Chunking performance (CoNLL-2000)
5 Results and Discussion
We evaluated the performance in terms of the F?=1
score, which is the evaluation measure used in
CoNLL-2000 and 2003, and sentence accuracy,
since all the methods in our experiments optimize
sequence loss. Tables 3 and 4 show the results of
the NER and Chunking experiments, respectively.
The F?=1 and ?Sent? columns show the performance
evaluated using the F?=1 score and sentence accu-
racy, respectively. ?2, ? and ?, which are the hyper-
parameters in Gaussian or Dirichlet priors, are se-
lected from a certain value set by using a develop-
ment set1, that is, ?2 ? {0.01, 0.1, 1, 10, 100, 1000},
? ? 1 = ?? ? {0.01, 0.1, 1, 10} and ? ? 1 = ?? ?
{0.00001, 0.0001, 0.001, 0.01}. The second rows of
CRF in Tables 3 and 4 represent the performance of
base discriminative models used in HySOL with all
the features, which are trained with 4/5 of the la-
beled training data. The third rows of HySOL show
the performance obtained without using generative
models (unlabeled data). The model itself is essen-
tially the same as LOP-CRFs. However the perfor-
mance in the third HySOL rows was consistently
lower than that of LOP-CRF since the discrimina-
tive models in HySOL are trained with 4/5 labeled
data.
As shown in Tables 3 and 4, HySOL signifi-
1Chunking (CoNLL-2000) data has no common develop-
ment set. Thus, our preliminary examination employed by using
4/5 labeled training data with the remaining 1/5 as development
data to determine the hyper-parameter values.


 
 
    
	 
      
 	 
   
 	 
   
 	 
   
       
      
      
      
 
  	 
   


 
 
   
  	 
   
	 
      
 	 
   
 	 
   
 	 
   
       
      
      
      
 
(a) NER (b) Chunking
Figure 2: Changes in the performance and the con-
vergence condition value (procedure 4 in Figure 1)
of HySOL.
cantly improved the performance of supervised set-
ting, CRF and LOP-CRF, as regards both NER and
Chunking experiments.
5.1 Impact of Incorporating Unlabeled Data
The contributions provided by incorporating unla-
beled data in our hybrid model can be seen by com-
parison with the performance of the first and third
rows in HySOL, namely a 2.64 point F-score and a
2.96 point sentence accuracy gain in the NER exper-
iments and a 0.46 point F-score and a 1.99 point sen-
tence accuracy gain in the Chunking experiments.
We believe there are two key ideas that enable
the unlabeled data in our approach to exhibit this
improvement compared with the the state-of-the-art
performance provided by discriminative models in
supervised settings. First, unlabeled data is only
used for optimizing Equation (4) to obtain a similar
effect to ?soft-clustering?, which can be calculated
without information about the correct output. Sec-
ond, by using a combination of generative models,
we can enhance the flexibility of the feature design
for unlabeled data. For example, we can handle ar-
bitrary overlapping features, similar to those used in
discriminative models, for unlabeled data by assign-
ing one feature type for one generative model as in
our experiments.
5.2 Impact of Iterative Parameter Estimation
Figure 2 shows the changes in the performance and
the convergence condition value of HySOL dur-
ing parameter estimation iteration in our NER and
Chunking experiments, respectively. As shown in
the figure, HySOL was able to reach the conver-
797
gence condition in a small number of iterations in
our experiments. Moreover, the change in the per-
formance remains quite stable during the iteration.
However, theoretically, our optimization procedure
is not guaranteed to converge in the ? and ? space,
since the optimization of ? has local maxima. Even
if we were unable to meet the convergence condi-
tion, we were easily able to obtain model parame-
ters by performing a sufficient fixed number of itera-
tions, and then select the parameters when Equation
(4) obtained the maximum objective value.
5.3 Comparison with SS-CRF-MER
When we consider semi-supervised SOL methods,
SS-CRF-MER (Jiao et al, 2006) is the most compet-
itive with HySOL, since both methods are defined
based on CRFs. We planned to compare the perfor-
mance with that of SS-CRF-MER in our NER and
Chunking experiments. Unfortunately, we failed to
implement SS-CRF-MER since it requires the use of
a slightly complicated algorithm, called the ?nested?
forward-backward algorithm.
Although, we cannot compare the performance,
our hybrid approach has several good characteris-
tics compared with SS-CRF-MER. First, it requires
a higher order algorithm, namely a ?nested? forward-
backward algorithm, for the parameter estimation of
unlabeled data whose time complexity is O(L3S2)
for each unlabeled data, where L and S represent the
output label size and unlabeled sample length, re-
spectively. Thus, our hybrid approach is more scal-
able for the size of unlabeled data, since HySOL
only needs a standard forward-backward algorithm
whose time complexity is O(L2S). In fact, we
still have a question as to whether SS-CRF-MER
is really scalable in practical time for such a large
amount of unlabeled data as used in our experi-
ments, which is about 680 times larger than that of
(Jiao et al, 2006). Scalability for unlabeled data
will become really important in the future, as it will
be natural to use millions or billions of unlabeled
data for further improvement. Second, SS-CRF-
MER has a sensitive hyper-parameter in the objec-
tive function, which controls the influence of the un-
labeled data. In contrast, our objective function only
has a hyper-parameter of prior distribution, which is
widely used for standard MAP estimation. More-
over, the experimental results shown in Tables 3 and
F?=1 additional resources
ASO-semi 89.31 unlabeled data (27M words)
(Ando and Zhang, 2005)
(Florian et al, 2003) 88.76 their own large gazetteers,
2M-word labeled data
(Chieu and Ng, 2003) 88.31 their own large gazetteers,
very elaborated features
HySOL 88.14 unlabeled data (17M words)
supplied gazetters
HySOL 87.20 unlabeled data (17M words)
Table 5: Previous top systems in NER (CoNLL-
2003) experiments
F?=1 additional resources
ASO-semi 94.39 unlabeled data
(Ando and Zhang, 2005) (15M words: WSJ)
HySOL 94.30 unlabeled data
(17M words: Reuters)
(Zhang et al, 2002) 94.17 full parser output
(Kudo and Matsumoto, 2001) 93.91 ?
Table 6: Previous top systems in Chunking
(CoNLL-2000) experiments
4 indicate that HySOL is rather robust with respect
to the hyper-parameter since we can obtain fairly
good performance without a prior distribution.
5.4 Comparison with Previous Top Systems
With respect to the performance of NER and Chunk-
ing tasks, the current best performance is reported
in (Ando and Zhang, 2005), which we refer to as
?ASO-semi?, as shown in Figures 5 and 6. ASO-
semi also incorporates unlabeled data solely for
the additional information in the same way as our
method. Unfortunately, our results could not reach
their level of performance, although the size and
source of the unlabeled data are not the same for cer-
tain reasons. First, (Ando and Zhang, 2005) does not
describe the unlabeled data used in their NER ex-
periments in detail, and second, we are not licensed
to use the TREC corpus including WSJ unlabeled
data that they used for their Chunking experiments
(training and test data for Chunking is derived from
WSJ). Therefore, we simply used the supplied unla-
beled data of the CoNLL-2003 shared task for both
NER and Chunking. If we consider the advantage of
our approach, our hybrid model incorporating gener-
ative models seems rather intuitive, since it is some-
times difficult to find out a design of effective auxil-
iary problems for the target problem.
Interestingly, the additional information obtained
798
F?=1 (gain)
HySOL (??=0.1,??=0.0001) 87.20 -
+ w/ F-score opt. (Suzuki et al, 2006) 88.02 (+0.82)
+ unlabeled data (17M ? 27M words) 88.41 (+0.39)
+ supplied gazetters 88.90 (+0.49)
+ add dev. set for estimating ? 89.27 (+0.37)
Table 7: The HySOL performance with the F-
score optimization technique and some additional
resources in NER (CoNLL-2003) experiments
F?=1 (gain)
HySOL (??=0.1,??=0.0001) 94.30 -
+ w/ F-score opt. (Suzuki et al, 2006) 94.36 (+0.06)
Table 8: The HySOL performance with the F-score
optimization technique on Chunking (CoNLL-2000)
experiments
from unlabeled data appear different from each
other. ASO-semi uses unlabeled data for construct-
ing auxiliary problems to find the ?shared structures?
of auxiliary problems that are expected to improve
the performance of the main problem. Moreover,
it is possible to combine both methods, for exam-
ple, by incorporating the features obtained with their
method in our base discriminative models, and then
construct a hybrid model using our method. There-
fore, there may be a possibility of further improving
the performance by this simple combination.
In NER, most of the top systems other than
ASO-semi boost performance by employing exter-
nal hand-crafted resources such as large gazetteers.
This is why their results are superior to those ob-
tained with HySOL. In fact, if we simply add the
gazetteers included in CoNLL-2003 supplied data as
features, HySOL achieves 88.14.
5.5 Applying F-score Optimization Technique
In addition, we can simply apply the F-score opti-
mization technique for the sequence labeling tasks
proposed in (Suzuki et al, 2006) to boost the
HySOL performance since the base discriminative
models pD(y|x) and discriminative combination,
namely Equation (3), in our hybrid model basically
uses the same optimization procedure as CRFs. Ta-
bles 7 and 8 show the F-score gain when we apply
the F-score optimization technique. As shown in the
Tables, the F-score optimization technique can eas-
ily improve the (F-score) performance without any
additional resources or feature engineering.
In NER, we also examined HySOL with addi-
tional resources to observe the performance gain.
The third row represents the performance when we
add approximately 10M words of unlabeled data (to-
tal 27M words)2 that are derived from 1996/11/15-
30 articles in Reuters corpus. Then, the fourth and
fifth rows represent the performance when we add
the supplied gazetters in the CoNLL-2003 data as
features, and adding development data as training
data of ?. In this case, HySOL achieved a com-
parable performance to that of the current best sys-
tem, ASO-semi, in both NER and Chunking exper-
iments even though the NER experiment is not a
fair comparison since we added additional resources
(gazetters and dev. set) that ASO-semi does not use
in training.
6 Conclusion and Future Work
We proposed a framework for semi-supervised SOL
based on a hybrid generative and discriminative ap-
proach. Experimental results showed that incorpo-
rating unlabeled data in a generative manner has
the power to further improve on the state-of-the-art
performance provided by supervised SOL methods
such as CRFs, with the help of our hybrid approach,
which discriminatively combines with discrimina-
tive models. In future we intend to investigate more
appropriate model and feature design for unlabeled
data, which may further improve the performance
achieved in our experiments.
Appendix
Let V Di,s = exp(? ? f s) and V Gj,s = ?ys?1,ys?ys,xs .
Equation (6) can be obtained by the following rear-
rangement of Equation (2) :
R(y|x;?,?,?)
=
?
i p
D
i (y|x,?i)?i
?
j p
G
j (x,y, ?j)?j?
y
?
i p
D
i (y|x,?i)?i
?
j p
G
j (x,y, ?j)?j
= 1NR(x)
?
i
[?
s V
D
i,s
Zi(x)
]?i?
j
[?
s
V Gj,s
]?j
= 1
NR(x)
?
i[Zi(x)]?i
?
i
[?
s
V Di,s
]?i?
j
[?
s
V Gj,s
]?j
= 1
NR(x)
?
i[Zi(x)]?i
?
s
?
i
[
V Di,s
]?i ?
j
[
V Gj,s
]?j .
2In order to keep the consistency of POS tags, we re-
attached POS tags of the supplied data set and new 10M words
of unlabeled data using a POS tagger trained from WSJ corpus.
799
References
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum Margin Semi-Supervised Learning for Struc-
tured Variables. In Proc. of NIPS*2005.
R. Ando and T. Zhang. 2005. A High-Performance
Semi-Supervised Learning Method for Text Chunking.
In Proc. of ACL-2005, pages 1?9.
U. Brefeld and T. Scheffer. 2006. Semi-Supervised
Learning for Structured Output Variables. In Proc. of
ICML-2006.
H. L. Chieu and Hwee T. Ng. 2003. Named Entity
Recognition with a Maximum Entropy Approach. In
Proc. of CoNLL-2003, pages 160?163.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39:1?38.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Recognition through Classifier Combi-
nation. In Proc. of CoNLL-2003, pages 168?171.
A. Fujino, N. Ueda, and K. Saito. 2005. A Hybrid Gen-
erative/Discriminative Approach to Semi-Supervised
Classifier Design. In Proc. of AAAI-05, pages 764?
769.
Y. Grandvalet and Y. Bengio. 2004. Semi-Supervised
Learning by Entropy Minimization. In Proc. of
NIPS*2004, pages 529?536.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-Supervised Conditional Random
Fields for Improved Sequence Segmentation and La-
beling. In Proc. of COLING/ACL-2006, pages 209?
216.
T. Kudo and Y. Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of NAACL 2001, pages
192?199.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282?289.
W. Li and A. McCallum. 2005. Semi-Supervised Se-
quence Modeling with Syntactic Topic Models. In
Proc. of AAAI-2005, pages 813?818.
D. C. Liu and J. Nocedal. 1989. On the Limited Memory
BFGS Method for Large Scale Optimization. Math.
Programming, Ser. B, 45(3):503?528.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Unlabeled
Documents using EM. Machine Learning, 39:103?
134.
R. Raina, Y. Shen, A. Y. Ng, and A. McCallum. 2003.
Classification with Hybrid Generative/Discriminative
Models. In Proc. of NIPS*2003.
F. Sha and F. Pereira. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. of HLT/NAACL-2003,
pages 213?220.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarith-
mic Opinion Pools for Conditional Random Fields. In
Proc. of ACL-2005, pages 10?17.
C. Sutton, M. Sindelar, and A. McCallum. 2006. Reduc-
ing Weight Undertraining in Structured Discriminative
Learning. In Proc. of HTL-NAACL 2006, pages 89?95.
J. Suzuki, E. McDermott, and H. Isozoki. 2006. Training
Conditional Random Fields with Multivariate Evalua-
tion Measure. In Proc. of COLING/ACL-2006, pages
217?224.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In
Proc. of CoNLL-2000 and LLL-2000, pages 127?132.
E. T. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003, pages 142?147.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
Chunking based on a Generalization of Winnow. Ma-
chine Learning Research, 2:615?637.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
Supervised Learning using Gaussian Fields and Har-
monic Functions. In Proc.of ICML-2003, pages 912?
919.
800
Multi-label Text Categorization with Model Combination
based on F1-score Maximization
Akinori Fujino, Hideki Isozaki, and Jun Suzuki
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0237
{a.fujino,isozaki,jun}@cslab.kecl.ntt.co.jp
Abstract
Text categorization is a fundamental task in
natural language processing, and is gener-
ally defined as a multi-label categorization
problem, where each text document is as-
signed to one or more categories. We fo-
cus on providing good statistical classifiers
with a generalization ability for multi-label
categorization and present a classifier de-
sign method based on model combination
and F
1
-score maximization. In our formu-
lation, we first design multiple models for
binary classification per category. Then,
we combine these models to maximize the
F
1
-score of a training dataset. Our experi-
mental results confirmed that our proposed
method was useful especially for datasets
where there were many combinations of cat-
egory labels.
1 Introduction
Text categorization is a fundamental task in such
aspects of natural language processing as informa-
tion retrieval, information extraction, and text min-
ing. Since a text document often belongs to multiple
categories in real tasks such as web pages and in-
ternational patent categorization, text categorization
is generally defined as assigning one or more pre-
defined category labels to each data sample. There-
fore, developing better classifiers with a generaliza-
tion ability for such multi-label categorization tasks
is an important issue in the field of machine learning.
A major and conventional machine learning ap-
proach to multi-label categorization is based on bi-
nary classification. With this approach, we assume
the independence of categories and design a binary
classifier for each category that determines whether
or not to assign a category label to data samples.
Statistical classifiers such as the logistic regression
model (LRM), the support vector machine (SVM),
and naive Bayes are employed as binary classi-
fiers (Joachims, 1998).
In text categorization, the F
1
-score is often used
to evaluate classifier performance. Recently, meth-
ods for training binary classifiers to maximize the
F
1
-score have been proposed for SVM (Joachims,
2005) and LRM (Jansche, 2005). It was con-
firmed experimentally that these training methods
were more effective for obtaining binary classifiers
with better F
1
-score performance than the minimum
error rate and maximum likelihood used for train-
ing conventional classifiers, especially when there
was a large imbalance between positive and nega-
tive samples. In multi-label categorization, macro-
and micro-averaged F
1
-scores are often used to eval-
uate classification performance. Therefore, we can
expect to improve multi-label classification perfor-
mance by using binary classifiers trained to maxi-
mize the F
1
-score.
On the other hand, classification frameworks
based on classifier combination have also been stud-
ied in many previous works such as (Wolpert, 1992;
Larkey and Croft, 1996; Ting and Witten, 1999;
Ghahramani and Kim, 2003; Bell et al, 2005;
Fumera and Roli, 2005), to provide better classi-
fier systems. In the classifier combination research
field, it is known that weighted linear combinations
of multiple classifiers often provide better classifica-
tion performance than individual classifiers.
823
We present a classifier design method based on
the combination of multiple binary classifiers to im-
prove multi-label classification performance. In our
framework, we first train multiple binary classifiers
for each category. Then, we combine these bi-
nary classifiers with weights estimated to maximize
micro- or macro-averaged F
1
-scores, which are of-
ten used for evaluating multi-label classifiers. To es-
timate combination weights, we extend the F
1
-score
maximization training algorithm for LRM described
in (Jansche, 2005). Using three real text datasets,
we show experimentally that our classifier design
method is more effective than the conventional bi-
nary classification approaches to multi-label catego-
rization.
Our method is based on a binary classification ap-
proach. However, Kazawa et al (2005) proposed
a method for modeling a map directly from data
samples to the combination of assigned category la-
bels, and confirmed experimentally that the method
outperformed conventional binary classification ap-
proaches. Therefore, we also compare our method
with the direct mapping method experimentally.
2 F
1
-score Maximization Training of LRM
We first review the F
1
-score maximization training
method for linear models using a logistic function
described in (Jansche, 2005). The method was pro-
posed in binary classification settings, where classi-
fiers determine a class label assignment y ? {1, 0}
for a data sample represented by a feature vector x.
Here, y(n) = 1 (= 0) indicates that the class label is
assigned (unassigned) to the nth feature vector x(n).
The discriminative function of a binary classifier
based on a linear model is often defined as
f(x;?) = ?t
1
x + ?
0
, (1)
where ? = (?
0
,?t
1
)t is a model parameter vector,
and ?t
1
x implies the inner product of ?
1
and x. A
binary classifier using f(x;?) outputs a predicted
class label assignment y? for x as y?(n) = 1 (= 0)
when f(x(n);?) ? 0 (< 0).
An LRM is a binary classifier that uses the dis-
criminative function f(x;?). In this model, the
class posterior probability distribution is defined by
using a logistic function:
g(z) = {1 + exp(?z)}?1. (2)
That is, P (y = 1|x;?) = g(f(x;?)) and P (y =
0|x;?) = 1 ? P (y = 1|x;?) = g(?f(x;?)).
The LRM determines that y(n) = 1 (= 0) when
P (y = 1|x(n);?) ? 0.5 (< 0.5), since g(0) = 0.5.
The model parameter vector ? is usually estimated
to maximize the likelihood of P (y|x;?) for training
dataset D = {x(m), y(m)}Mm=1 and the prior proba-
bility density of ?:
JR(?) =
M
?
m=1
log P (y(m)|x(m);?) + log p(?). (3)
In this paper, the classifier design approach that em-
ploys this training method is called LRM-L.
By contrast, in the training method proposed
by (Jansche, 2005), the discriminative function
f(x;w) is estimated to maximize the F
1
-score of
training dataset D. This training method employs an
approximate form of the F
1
-score obtained by using
a logistic function.
The F
1
-score is defined as F
1
= 2(1/PR +
1/RE)?1, where PR and RE represent precision
and recall defined as PR = C/A and RE = C/B,
respectively. Here, C represents the number of data
samples whose true and predicted class label assign-
ments, y(n) and y?(n), respectively, correspond to 1.
A represents the number of data samples for which
y?
(n) = 1. B represents the number of data samples
for which y(n) = 1. C , A, and B are computed
for training dataset D as C =
?M
m=1 y
(m)
y?
(m)
,
A =
?M
m=1 y?
(m)
, and B =
?M
m=1 y
(m)
.
In (Jansche, 2005), y?(m) was approximated by us-
ing the discriminative and logistic functions shown
in Eqs. (1) and (2) as
y?
(m)
? g(?f(x(m);?)), ? > 0, (4)
because lim??? g(?f(x(m);?)) = y?(m). Then, an
approximate distribution of the F
1
-score for training
dataset D was provided as
F?
1
(?) =
2
?M
m=1 g(?f(x;?))y
(m)
?M
m=1 y
(m) +
?M
m=1 g(?f(x;?))
. (5)
The ? estimate for the discriminative function
f(x;?) can be computed to maximize JF (?) =
log F?
1
(?) + log p(?) around the initial ? value by
using a gradient method. In this paper, the classi-
fier design approach that uses this training method
is called LRM-F.
824
3 Proposed Method
We propose a framework for designing a multi-label
classifier based on the combination of multiple mod-
els. In our formulation, multiple models are com-
bined with weights estimated to maximize the F
1
-
scores of the training dataset. In this section, we
show our formulation for model combination and
training methods for combination weights.
3.1 Combination of Multiple Models for
Multi-label Categorization
Multi-label categorization is the task of selecting
multiple category labels from K pre-defined cat-
egory labels for each data sample. Multi-label
classifiers provide a map from a feature vector
x to a category label assignment vector y =
(y
1
, . . . , yk, . . . , yK)
t
, where y(n)k = 1 (= 0) indi-
cates that the kth category label is assigned (unas-
signed) to x(n).
In our formulation, we first design multiple mod-
els for binary classification per category and ob-
tain J ? K discriminative functions, where J is the
number of models. The discriminative function of
the jth model for the kth category is denoted by
fjk(x;?jk), where ?jk represents the model param-
eter vector. Let ? = {?jk}j,k be a model parameter
set. We train model parameter vectors individually
with each model training algorithm and obtain the
estimate ?? = {??jk}jk. Then, we define the dis-
criminative function of our multi-label classifier by
combining multiple models such as
fk(x; ??,w) =
J
?
j=1
wjfjk(x; ??jk) + w0, ?k, (6)
where w = (w
0
, w
1
, . . . , wj , . . . , wJ)
t is a weight
parameter vector and is independent of k. wj pro-
vides the combination weight of the jth model, and
w
0
is the bias factor for adjusting the threshold of
the category label assignment.
We estimate the w value to maximize the
micro-averaged F
1
-score (F?), which is often used
for evaluating multi-label categorization perfor-
mance. The F?-score of training dataset D =
{x(m),y(m)}Mm=1 is calculated as
F? =
2
?M
m=1
?K
k=1 y
(m)
k y?
(m)
k
?M
m=1
?K
k=1 y
(m)
k +
?M
m=1
?K
k=1 y?
(m)
k
, (7)
We provide an approximate form of the F?-score of
the training dataset, F??(??,w), by using the approx-
imation:
y?
(m)
k ? g(?fk(x
(m); ??,w)), ? > 0, (8)
as shown in Eq. (4). In our proposed method, w is
estimated to maximize F??(??,w).
However, training dataset D is also used to es-
timate ?. Using the same training data samples
for both ? and w may lead to a bias estimation of
w. Thus, we used an n-fold cross-validation of the
training data samples to estimate w as in (Wolpert,
1992). Let ??(?m) be the model parameter set esti-
mated by using n ? 1 training data subsets not con-
taining {x(m),y(m)}. Then, using
F?? =
2
?
m,k y
(m)
k g(?fk(x; ??
(?m)
,w))
?
m,k y
(m)
k +
?
m,k g(?fk(x; ??
(?m)
,w))
, (9)
we provide the objective function of w such that
J?(w) = log F?? + log p(w), (10)
where p(w) is a prior probability density of w.
We use a Gaussian prior (Chen and Rosenfeld,
1999) with the form as p(w) ? ?Jj=0 exp{?(wj ?
?j)
2
/2?2j }, where ?j , and ?j are hyperparameters in
the Gaussian prior. We compute an estimate of w to
maximize J?(w) around the initial w value by using
a quasi-Newton method. In this paper, this formula-
tion is called model combination by micro-averaged
F
1
-score maximization (MC-F?).
3.2 Other Training Methods
In multi-label categorization problems, the macro-
averaged F
1
-score (FM ) is also used to evaluate
classifiers. Moreover, the average labeling F
1
-score
(FL) has been used to evaluate the average labeling
performance of classifiers for data samples (Kazawa
et al, 2005). These F
1
-scores are computed for
training dataset D as
FM =
1
K
K
?
k=1
2
?M
m=1 y
(m)
k y?
(m)
k
?M
m=1 y
(m)
k +
?M
m=1 y?
(m)
k
, (11)
FL =
1
M
M
?
m=1
2
?K
k=1 y
(m)
k y?
(m)
k
?K
k=1 y
(m)
k +
?K
k=1 y?
(m)
k
. (12)
Using Eq. (8), we can also obtain the approxi-
mate forms, F?M (??,w) and F?L(??,w), of the FM -
825
and FL-scores, and then present similar objective
functions to that for the F?-score. Therefore, in
the next section, we examine experimentally the per-
formance of classifiers obtained by estimating w to
maximize F?M (??,w) and F?L(??,w). In this paper,
these model combination methods based on FM -
and FL-scores are called MC-FM and MC-FL, re-
spectively.
4 Experiments
4.1 Test Collections
To evaluate our proposed method empirically, we
used three test collections: Reuters-21578 (Reuters),
WIPO-alpha (WIPO), and Japanese Patent (JPAT)
datasets. Reuters and WIPO are English document
datasets and have often been employed for bench-
mark tests of multi-label classifiers.
The Reuters dataset contains news articles from
the Reuters newswire and consists of 135 topic cate-
gories. Following the setup in (Yang and Liu, 1999),
we extracted 7770 and 3019 articles as training and
test samples, respectively. A subset consisting of the
training and test samples contained 90 topic cate-
gories. We removed vocabulary words included ei-
ther in the stoplist or in only one article. There were
16365 vocabulary words in the dataset.
The WIPO dataset consists of patent documents
categorized using the International Patent Classifica-
tion (IPC) taxonomy (Fall et al, 2003). The IPC tax-
onomy has four hierarchical layers: Section, Class,
Subclass, and Group. Using patent documents be-
longing to Section D (textiles; paper), we evalu-
ated classifiers in a task that consisted of selecting
assigned category labels from 160 groups for each
patent document. Following the setting provided in
the dataset, we extracted 1352 and 358 patent docu-
ments as training and test samples, respectively. We
removed vocabulary words in the same way as for
Reuters. There were 45895 vocabulary words in the
dataset.
The JPAT dataset (Iwayama et al, 2007) con-
sists of Japanese patent documents published be-
tween 1993 and 1999 by the Japanese Patent Office.
These documents are categorized using a taxonomy
consisting of Themes and F-terms. The themes are
top-label categories, and the patent documents be-
longing to each theme are categorized by using F-
Reuters WIPO JPAT
Nav 1.17 1.28 10.5
Nmax 15 6 40
K 90 160 268
Nds 10789 1710 2464
NLC 468 378 2430
Nds/NLC 23.1 4.52 1.01
Table 1: Statistical information of three datasets:
Nav and Nmax are the average and maximum num-
ber of assigned category labels per data sample, re-
spectively. K and Nds are the number of category
labels and data samples, respectively. NLC is the
number of category label combinations appearing in
each dataset.
terms. Using patent documents belonging to Theme
5J104, we evaluated classifiers in a task that con-
sisted of selecting assigned category labels from 268
F-terms for each patent document. 1920 patent doc-
uments published between 1993 and 1997 were used
as training samples, and 544 patent documents pub-
lished between 1998 and 1999 were used test sam-
ples. We extracted Japanese nouns, verbs, and adjec-
tives from patent documents by using a morpholog-
ical analyzer named MeCab 1, and removed vocab-
ulary words included in only one patent document.
There were 21135 vocabulary words in the dataset.
Table 1 shows statistical information about the
category label assignment of the data samples for the
three datasets. The average numbers of assigned cat-
egory labels per data sample, Nav , for Reuters and
WIPO were close to 1 and much smaller than that
for JPAT. The number of category label combina-
tions, NLC , included in JPAT was larger than those
for Reuters and WIPO. These statistical information
results show that JPAT is a more complex multi-label
dataset than Reuters or WIPO.
4.2 Experimental Settings
For text categorization tasks, we employed word-
frequency vectors of documents as feature vectors
input into classifiers, using the independent word-
based representation, known as the Bag-of-Words
(BOW) representation. We normalized the L1-
norms of the word-frequency vectors to 1, to miti-
gate the effect of vector size on computation. We
did not employ any word weighting methods such
as inverse document frequency (IDF).
1http://mecab.sourceforge.net/
826
We constructed three multi-label text classifiers
based on our proposed model combination methods,
MC-F?, MC-FM , and MC-FL, where LRM and
SVM (J = 2) were employed as binary classifica-
tion models combined with each method. We trained
the LRM by using LRM-L described in Section 2,
where a Gaussian prior was used as the prior proba-
bility density of the parameter vectors. We provided
the SVM by using SVMlight 2 (SVM-L), where we
employed a linear kernel function and tuned the C
(penalty cost) parameter as a hyperparameter.
To evaluate our proposed method, we examined
the micro- and macro-averaged, and average label-
ing F
1
-scores (F?, FM , and FL), of test samples ob-
tained with the three classifiers based on MC-F?,
MC-FM , and MC-FL. We compared the perfor-
mance of the three classifiers with that of two binary
classification approaches, where LRM-L or SVM-L
was used for binary classification.
We also examined two binary classification ap-
proaches using LRM-F and SVM-F. For LRM-F, we
used a Gaussian prior and provided the initial pa-
rameter vector with a parameter estimate obtained
with LRM-L. SVM-F is a binary classifier design
approach that employs SVMperf 3. For SVM-F, we
used a linear kernel function, set the L (loss parame-
ter) parameter to maximize the F
1
-score, and tuned
the C (penalty cost) parameter as a hyperparameter.
Moreover, we examined the performance of the
Maximal Margin Labeling (MML) method (Kazawa
et al, 2005), which models the map from feature
vectors to category label assignment vectors, be-
cause it was reported that MML provides better per-
formance than binary classification approaches.
We tuned the hyperparameter of SVM-F for JPAT
to provide good performance for test samples, be-
cause the computational cost for training was high.
We tuned the other hyperparameters by using a 10-
fold cross-validation of training samples.
4.3 Results and Discussion
In Table 2, we show the classification performance
obtained for three datasets with our proposed and
other methods described in Section 4.2. We ex-
amined nine evaluation scores: the micro-averaged
F
1
-score (F?), precision (P?), and recall (R?), the
2http://svmlight.joachims.org/
3http://svmlight.joachims.org/svm perf.html
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 87.0 (87.4/86.7) 51.3 (60.0/48.4) 90.0 (90.1/92.3)
MC-FM 85.0 (80.8/89.5) 53.9 (54.9/58.4) 89.7 (88.5/94.1)
MC-FL 86.3 (84.3/88.3) 53.4 (59.6/52.6) 90.0 (89.3/93.6)
LRM-L 85.2 (87.3/83.2) 46.1 (55.0/43.1) 86.9 (87.6/88.6)
LRM-F 85.2 (87.2/83.2) 47.4 (58.5/42.7) 87.0 (87.6/88.7)
SVM-L 87.1 (92.9/82.0) 48.9 (58.9/45.8) 88.1 (89.3/88.8)
SVM-F 82.4 (78.9/86.2) 51.4 (49.4/60.1) 87.4 (86.9/91.4)
MML 87.8 (92.6/83.4) 59.3 (62.6/60.0) 91.2 (91.7/93.2)
(a) Reuters
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 51.4 (57.3/46.6) 30.4 (35.8/30.3) 46.9 (48.3/51.5)
MC-FM 48.1 (46.1/50.4) 32.2 (33.8/36.0) 46.8 (46.3/56.0)
MC-FL 48.6 (45.8/51.9) 32.5 (33.4/36.5) 47.1 (46.4/56.8)
LRM-L 40.5 (68.0/28.9) 22.1 (33.7/17.9) 32.7 (36.5/32.0)
LRM-F 41.0 (68.6/29.2) 22.3 (34.0/18.1) 33.2 (37.0/32.4)
SVM-L 41.8 (61.9/31.5) 24.4 (34.2/21.0) 35.1 (38.8/35.3)
SVM-F 48.3 (53.8/43.8) 32.3 (37.4/31.8) 45.6 (47.9/49.6)
MML 48.6 (54.9/43.6) 30.8 (36.5/29.7) 49.4 (56.2/48.4)
(b) WIPO
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 41.8 (42.6/41.1) 17.5 (21.4/17.4) 40.2 (43.5/44.4)
MC-FM 40.6 (35.8/46.7) 20.2 (20.4/23.1) 39.4 (37.7/50.6)
MC-FL 42.1 (42.3/41.9) 17.6 (21.1/17.8) 40.5 (43.2/45.2)
LRM-L 33.9 (44.4/27.4) 15.8 (20.9/14.0) 32.2 (46.5/29.9)
LRM-F 36.9 (44.6/31.5) 16.9 (22.9/14.7) 35.1 (47.3/34.1)
SVM-L 33.3 (39.6/28.7) 16.3 (20.9/14.6) 31.9 (42.4/31.6)
SVM-F 32.2 (28.6/36.8) 19.7 (15.0/38.4) 31.0 (30.7/40.0)
MML 32.7 (42.1/26.8) 14.7 (19.4/12.9) 32.2 (51.8/30.5)
(c) JPAT
Table 2: Micro- and macro-averaged, and average
labeling F
1
-scores (%) with our proposed and con-
ventional methods.
macro-averaged F
1
-score (FM ), precision (PM ),
and recall (RM ), and the average labeling F1-score
(FL), precision (PL), and recall (RL) of the test sam-
ples. FM and PM were calculated by regarding both
the F
1
-score and precision as zero for the categories
where there were no data samples predicted as posi-
tive samples.
LRM-F and SVM-F outperformed LRM-L and
SVM-L in terms of FM -score for the three datasets,
respectively. The training methods of LRM-F and
SVM-F were useful to improve the FM -scores of
LRM and SVM, as reported in (Jansche, 2005;
Joachims, 2005). The F?- and FL-scores of LRM-F
were similar or better than those of LRM-L. LRM-
F was effective in improving not only the FM -score
but also the F?- and FL-scores obtained with LRM.
Let us evaluate our model combination methods.
827
MC-F? provided better F?-scores than LRM-F and
SVM-F. The FM -scores of MC-FM were similar or
better than those of LRM-F and SVM-F. Moreover,
MC-FL outperformed LRM-F and SVM-F in terms
of FL-scores. The binary classifiers designed by us-
ing LRM-F and SVM-F were trained to maximize
the F
1
-score for each category. On the other hand,
MC-F?, MC-FM , and MC-FL classifiers were con-
structed by combining LRM and SVM with weights
estimated to maximize the F?-, FM -, and FL-scores,
respectively. The experimental results show that our
training methods for combination weights were use-
ful for obtaining better multi-label classifiers.
MC-F?, MC-FM , and MC-FL outperformed
MML as regards the three F
1
-scores for JPAT. How-
ever, MML performed better for Reuters than MC-
F?, MC-FM , and MC-FL, and provided a better FL-
score for WIPO. As shown in Table 1, there were
more category label combinations for JPAT than for
Reuters or WIPO. As a result, there were fewer data
samples for the same category label assignment for
JPAT. Therefore, MML, which learns the map di-
rectly from the feature vectors to the category label
assignment vectors, would have been overfitted to
the training dataset for JPAT. By contrast, our model
combination methods employ binary classifiers for
each category, which mitigates such an overfitting
problem. Our model combination methods will be
useful for complex datasets where there are many
category label combinations.
5 Conclusion
We proposed a multi-label classifier design method
based on model combination. The main idea be-
hind our proposed method is to combine multiple
models with weights estimated to maximize evalua-
tion scores such as the micro- and macro-averaged,
and average labeling F
1
-scores. Using three real
text datasets, we confirmed experimentally that our
proposed method provided similar or better perfor-
mance than conventional binary classification ap-
proaches to multi-label categorization. We also con-
firmed that our proposed method was useful for
datasets where there were many combinations of
category labels. Future work will involve training
our multi-label classifier by using labeled and un-
labeled samples, which are data samples with and
without category label assignment.
References
David A. Bell, J. W. Guan, and Yaxin Bi. 2005. On
combining classifier mass functions for text categorization.
IEEE Transactions on Knowledge and Data Engineering,
17(10):1307?1319.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical report,
Carnegie Mellon University.
C. J. Fall, A. To?rcsva?ri, K. Benzineb, and G. Karetka. 2003.
Automated categorization in the international patent classifi-
cation. ACM SIGIR Forum, 37(1):10?25.
Giorgio Fumera and Fabio Roli. 2005. A theoretical and exper-
imental analysis of linear combiners for multiple classifier
systems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(6):942?956.
Zoubin Ghahramani and Hyun-Chul Kim. 2003. Bayesian
classifier combination. Technical report, Gatsby Computa-
tional Neuroscience Unit, University College London.
Makoto Iwayama, Atsushi Fujii, and Noriko Kando. 2007.
Overview of classification subtask at NTCIR-6 patent re-
trieval task. In Proceedings of the 6th NTCIR Workshop
Meeting on Evaluation of Information Access Technologies
(NTCIR-6), pages 366?372.
Martin Jansche. 2005. Maximum expected F-measure train-
ing of logistic regression models. In Proceedings of
Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing
(HLT/EMNLP2005), pages 692?699.
Thorsten Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant features. In
Proceedings of the 10th European Conference on Machine
Learning (ECML ?98), pages 137?142.
Thorsten Joachims. 2005. A support vector method for multi-
variate performance measures. In Proceedings of the 22nd
International Conference on Machine Learning (ICML?05),
pages 377?384.
Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, and
Eisaku Maeda. 2005. Maximal margin labeling for multi-
topic text categorization. In Advances in Neural Information
Processing Systems 17, pages 649?656. MIT Press, Cam-
bridge, MA.
Leah S. Larkey and W. Bruce Croft. 1996. Combining classi-
fiers in text categorization. In Proceedings of the 19th ACM
International Conference on Research and Development in
Information Retrieval (SIGIR-96), pages 289?297.
Kai Ming Ting and Ian H. Witten. 1999. Issues in stacked
generalization. Journal of Artificial Intelligence Research,
10:271?289.
David H. Wolpert. 1992. Stacked generalization. Newral Net-
works, 5(2):241?259.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In Proceedings of the 22nd ACM
International Conference on Research and Development in
Information Retrieval (SIGIR-99), pages 42?49.
828
Proceedings of the ACL 2010 Conference Short Papers, pages 137?141,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Word Alignment with Synonym Regularization
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo,a.fujino}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We present a novel framework for word
alignment that incorporates synonym
knowledge collected from monolingual
linguistic resources in a bilingual proba-
bilistic model. Synonym information is
helpful for word alignment because we
can expect a synonym to correspond to
the same word in a different language.
We design a generative model for word
alignment that uses synonym information
as a regularization term. The experimental
results show that our proposed method
significantly improves word alignment
quality.
1 Introduction
Word alignment is an essential step in most phrase
and syntax based statistical machine translation
(SMT). It is an inference problem of word cor-
respondences between different languages given
parallel sentence pairs. Accurate word alignment
can induce high quality phrase detection and trans-
lation probability, which leads to a significant im-
provement in SMT performance. Many word
alignment approaches based on generative mod-
els have been proposed and they learn from bilin-
gual sentences in an unsupervised manner (Vo-
gel et al, 1996; Och and Ney, 2003; Fraser and
Marcu, 2007).
One way to improve word alignment quality
is to add linguistic knowledge derived from a
monolingual corpus. This monolingual knowl-
edge makes it easier to determine corresponding
words correctly. For instance, functional words
in one language tend to correspond to functional
words in another language (Deng and Gao, 2007),
and the syntactic dependency of words in each lan-
guage can help the alignment process (Ma et al,
2008). It has been shown that such grammatical
information works as a constraint in word align-
ment models and improves word alignment qual-
ity.
A large number of monolingual lexical seman-
tic resources such as WordNet (Miller, 1995) have
been constructed in more than fifty languages
(Sagot and Fiser, 2008). They include word-
level relations such as synonyms, hypernyms and
hyponyms. Synonym information is particularly
helpful for word alignment because we can ex-
pect a synonym to correspond to the same word
in a different language. In this paper, we explore a
method for using synonym information effectively
to improve word alignment quality.
In general, synonym relations are defined in
terms of word sense, not in terms of word form. In
other words, synonym relations are usually con-
text or domain dependent. For instance, ?head?
and ?chief? are synonyms in contexts referring to
working environment, while ?head? and ?forefront?
are synonyms in contexts referring to physical po-
sitions. It is difficult, however, to imagine a con-
text where ?chief? and ?forefront? are synonyms.
Therefore, it is easy to imagine that simply replac-
ing all occurrences of ?chief? and ?forefront? with
?head? do sometimes harm with word alignment
accuracy, and we have to model either the context
or senses of words.
We propose a novel method that incorporates
synonyms from monolingual resources in a bilin-
gual word alignment model. We formulate a syn-
onym pair generative model with a topic variable
and use this model as a regularization term with a
bilingual word alignment model. The topic vari-
able in our synonym model is helpful for disam-
biguating the meanings of synonyms. We extend
HM-BiTAM, which is a HMM-based word align-
ment model with a latent topic, with a novel syn-
onym pair generative model. We applied the pro-
posed method to an English-French word align-
ment task and successfully improved the word
137
Figure 1: Graphical model of HM-BiTAM
alignment quality.
2 Bilingual Word Alignment Model
In this section, we review a conventional gener-
ative word alignment model, HM-BiTAM (Zhao
and Xing, 2008).
HM-BiTAM is a bilingual generative model
with topic z, alignment a and topic weight vec-
tor ? as latent variables. Topic variables such
as ?science? or ?economy? assigned to individual
sentences help to disambiguate the meanings of
words. HM-BiTAM assumes that the nth bilin-
gual sentence pair, (En, Fn), is generated under a
given latent topic zn ? {1, . . . , k, . . . ,K}, where
K is the number of latent topics. Let N be the
number of sentence pairs, and In and Jn be the
lengths of En and Fn, respectively. In this frame-
work, all of the bilingual sentence pairs {E,F} =
{(En, Fn)}Nn=1 are generated as follows.
1. ? ? Dirichlet (?): sample topic-weight vector
2. For each sentence pair (En, Fn)
(a) zn ? Multinomial (?): sample the topic
(b) en,i:In |zn ? p (En |zn;? ): sample English
words from a monolingual unigram model given
topic zn
(c) For each position jn = 1, . . . , Jn
i. ajn ? p (ajn |ajn?1;T ): sample an align-
ment link ajn from a first order Markov pro-
cess
ii. fjn ? p (fjn |En, ajn , zn;B ): sample a
target word fjn given an aligned source
word and topic
where alignment ajn = i denotes source word ei
and target word fjn are aligned. ? is a parame-
ter over the topic weight vector ?, ? = {?k,e} is
the source word probability given the kth topic:
p (e |z = k ). B = {Bf,e,k} represents the word
translation probability from e to f under the kth
topic: p (f |e, z = k ). T =
{
Ti,i?
}
is a state tran-
sition probability of a first order Markov process.
Fig. 1 shows a graphical model of HM-BiTAM.
The total likelihood of bilingual sentence pairs
{E,F} can be obtained by marginalizing out la-
tent variables z, a and ?,
p (F,E; ?) =
?
z
?
a

p (F,E, z, a, ?; ?) d?, (1)
where ? = {?, ?, T,B} is a parameter set. In
this model, we can infer word alignment a by max-
imizing the likelihood above.
3 Proposed Method
3.1 Synonym Pair Generative Model
We design a generative model for synonym pairs
{f, f ?} in language F , which assumes that the
synonyms are collected from monolingual linguis-
tic resources. We assume that each synonym pair
(f, f ?) is generated independently given the same
?sense? s. Under this assumption, the probability
of synonym pair (f, f ?) can be formulated as,
p
(
f, f ?
)
?
?
s
p (f |s ) p
(
f ? |s
)
p (s) . (2)
We define a pair (e, k) as a representation of
the sense s, where e and k are a word in a dif-
ferent language E and a latent topic, respectively.
It has been shown that a word e in a different
language is an appropriate representation of s in
synonym modeling (Bannard and Callison-Burch,
2005). We assume that adding a latent topic k for
the sense is very useful for disambiguating word
meaning, and thus that (e, k) gives us a good ap-
proximation of s. Under this assumption, the syn-
onym pair generative model can be defined as fol-
lows.
p
(
{
f, f ?
}
; ??
)
?
?
(f,f ?)
?
e,k
p(f |e, k; ??)p(f ?|e, k; ??)p(e, k; ??),(3)
where ?? is the parameter set of our model.
3.2 Word Alignment with Synonym
Regularization
In this section, we extend the bilingual genera-
tive model (HM-BiTAM) with our synonym pair
model. Our expectation is that synonym pairs
138
Figure 2: Graphical model of synonym pair gen-
erative process
correspond to the same word in a different lan-
guage, thus they make it easy to infer accurate
word alignment. HM-BiTAM and the synonym
model share parameters in order to incorporate
monolingual synonym information into the bilin-
gual word alignment model. This can be achieved
via reparameterizing ?? in eq. 3 as,
p
(
f
?
?
?
e, k; ??
)
? p (f |e, k;B ) , (4)
p
(
e, k; ??
)
? p (e |k;? ) p (k;?) . (5)
Overall, we re-define the synonym pair model
with the HM-BiTAM parameter set ?,
p(
{
f, f ?
}
; ?)
? 1?
k? ?k?
?
(f,f ?)
?
k,e
?k?k,eBf,e,kBf ?,e,k. (6)
Fig. 2 shows a graphical model of the synonym
pair generative process. We estimate the param-
eter values to maximize the likelihood of HM-
BiTAM with respect to bilingual sentences and
that of the synonym model with respect to syn-
onym pairs collected from monolingual resources.
Namely, the parameter estimate, ??, is computed
as
?? = argmax
?
{
log p(F,E; ?) + ? log p(
{
f, f ?
}
; ?)
}
,
(7)
where ? is a regularization weight that should
be set for training. We can expect that the second
term of eq. 7 to constrain parameter set ? and
avoid overfitting for the bilingual word alignment
model. We resort to the variational EM approach
(Bernardo et al, 2003) to infer ?? following HM-
BiTAM. We omit the parameter update equation
due to lack of space.
4 Experiments
4.1 Experimental Setting
For an empirical evaluation of the proposed
method, we used a bilingual parallel corpus of
English-French Hansards (Mihalcea and Pedersen,
2003). The corpus consists of over 1 million sen-
tence pairs, which include 447 manually word-
aligned sentences. We selected 100 sentence pairs
randomly from the manually word-aligned sen-
tences as development data for tuning the regu-
larization weight ?, and used the 347 remaining
sentence pairs as evaluation data. We also ran-
domly selected 10k, 50k, and 100k sized sentence
pairs from the corpus as additional training data.
We ran the unsupervised training of our proposed
word alignment model on the additional training
data and the 347 sentence pairs of the evaluation
data. Note that manual word alignment of the
347 sentence pairs was not used for the unsuper-
vised training. After the unsupervised training, we
evaluated the word alignment performance of our
proposed method by comparing the manual word
alignment of the 347 sentence pairs with the pre-
diction provided by the trained model.
We collected English and French synonym pairs
from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4
(Sagot and Fiser, 2008), respectively. WOLF is a
semantic resource constructed from the Princeton
WordNet and various multilingual resources. We
selected synonym pairs where both words were in-
cluded in the bilingual training set.
We compared the word alignment performance
of our model with that of GIZA++ 1.03 1 (Vo-
gel et al, 1996; Och and Ney, 2003), and HM-
BiTAM (Zhao and Xing, 2008) implemented by
us. GIZA++ is an implementation of IBM-model
4 and HMM, and HM-BiTAM corresponds to ? =
0 in eq. 7. We adopted K = 3 topics, following
the setting in (Zhao and Xing, 2006).
We trained the word alignment in two direc-
tions: English to French, and French to English.
The alignment results for both directions were re-
fined with ?GROW? heuristics to yield high preci-
sion and high recall in accordance with previous
work (Och and Ney, 2003; Zhao and Xing, 2006).
We evaluated these results for precision, recall, F-
measure and alignment error rate (AER), which
are standard metrics for word alignment accuracy
(Och and Ney, 2000).
1http://fjoch.com/GIZA++.html
139
10k Precision Recall F-measure AER
GIZA++ standard 0.856 0.718 0.781 0.207
with SRH 0.874 0.720 0.789 0.198
HM-BiTAM standard 0.869 0.788 0.826 0.169
with SRH 0.884 0.790 0.834 0.160
Proposed 0.941 0.808 0.870 0.123
(a)
50k Precision Recall F-measure AER
GIZA++ standard 0.905 0.770 0.832 0.156
with SRH 0.903 0.759 0.825 0.164
HM-BiTAM standard 0.901 0.814 0.855 0.140
with SRH 0.899 0.808 0.853 0.145
Proposed 0.947 0.824 0.881 0.112
(b)
100k Precision Recall F-measure AER
GIZA++ standard 0.925 0.791 0.853 0.136
with SRH 0.934 0.803 0.864 0.126
HM-BiTAM standard 0.898 0.851 0.874 0.124
with SRH 0.909 0.860 0.879 0.114
Proposed 0.927 0.862 0.893 0.103
(c)
Table 1: Comparison of word alignment accuracy.
The best results are indicated in bold type. The
additional data set sizes are (a) 10k, (b) 50k, (c)
100k.
4.2 Results and Discussion
Table 1 shows the word alignment accuracy of the
three methods trained with 10k, 50k, and 100k ad-
ditional sentence pairs. For all settings, our pro-
posed method outperformed other conventional
methods. This result shows that synonym infor-
mation is effective for improving word alignment
quality as we expected.
As mentioned in Sections 1 and 3.1, the main
idea of our proposed method is to introduce la-
tent topics for modeling synonym pairs, and then
to utilize the synonym pair model for the regu-
larization of word alignment models. We expect
the latent topics to be useful for modeling poly-
semous words included in synonym pairs and to
enable us to incorporate synonym information ef-
fectively into word alignment models. To con-
firm the effect of the synonym pair model with
latent topics, we also tested GIZA++ and HM-
BiTAM with what we call Synonym Replacement
Heuristics (SRH), where all of the synonym pairs
in the bilingual training sentences were simply re-
placed with a representative word. For instance,
the words ?sick? and ?ill? in the bilingual sentences
# vocabularies 10k 50k 100k
English standard 8578 16924 22817
with SRH 5435 7235 13978
French standard 10791 21872 30294
with SRH 9737 20077 27970
Table 2: The number of vocabularies in the 10k,
50k and 100k data sets.
were replaced with the word ?sick?. As shown in
Table 2, the number of vocabularies in the English
and French data sets decreased as a result of em-
ploying the SRH.
We show the performance of GIZA++ and HM-
BiTAM with the SRH in the lines entitled ?with
SRH? in Table 1. The GIZA++ and HM-BiTAM
with the SRH slightly outperformed the standard
GIZA++ and HM-BiTAM for the 10k and 100k
data sets, but underperformed with the 50k data
set. We assume that the SRH mitigated the over-
fitting of these models into low-frequency word
pairs in bilingual sentences, and then improved the
word alignment performance. The SRH regards
all of the different words coupled with the same
word in the synonym pairs as synonyms. For in-
stance, the words ?head?, ?chief? and ?forefront? in
the bilingual sentences are replaced with ?chief?,
since (?head?, ?chief?) and (?head?, ?forefront?) are
synonyms. Obviously, (?chief?, ?forefront?) are
not synonyms, which is detrimented to word align-
ment.
The proposed method consistently outper-
formed GIZA++ and HM-BiTAM with the SRH
in 10k, 50k and 100k data sets in F-measure.
The synonym pair model in our proposed method
can automatically learn that (?head?, ?chief?) and
(?head?, ?forefront?) are individual synonyms with
different meanings by assigning these pairs to dif-
ferent topics. By sharing latent topics between
the synonym pair model and the word alignment
model, the synonym information incorporated in
the synonym pair model is used directly for train-
ing word alignment model. The experimental re-
sults show that our proposed method was effec-
tive in improving the performance of the word
alignment model by using synonym pairs includ-
ing such ambiguous synonym words.
Finally, we discuss the data set size used for un-
supervised training. As shown in Table 1, using
a large number of additional sentence pairs im-
proved the performance of all the models. In all
our experimental settings, all the additional sen-
140
tence pairs and the evaluation data were selected
from the Hansards data set. These experimental
results show that a larger number of sentence pairs
was more effective in improving word alignment
performance when the sentence pairs were col-
lected from a homogeneous data source. However,
in practice, it might be difficult to collect a large
number of such homogeneous sentence pairs for
a specific target domain and language pair. One
direction for future work is to confirm the effect
of the proposed method when training the word
alignment model by using a large number of sen-
tence pairs collected from various data sources in-
cluding many topics for a specific language pair.
5 Conclusions and Future Work
We proposed a novel framework that incorpo-
rates synonyms from monolingual linguistic re-
sources in a word alignment generative model.
This approach utilizes both bilingual and mono-
lingual synonym resources effectively for word
alignment. Our proposed method uses a latent
topic for bilingual sentences and monolingual syn-
onym pairs, which is helpful in terms of word
sense disambiguation. Our proposed method im-
proved word alignment quality with both small
and large data sets. Future work will involve ex-
amining the proposed method for different lan-
guage pairs such as English-Chinese and English-
Japanese and evaluating the impact of our pro-
posed method on SMT performance. We will also
apply our proposed method to a larger data sets
of multiple domains since we can expect a fur-
ther improvement in word alignment accuracy if
we use more bilingual sentences and more mono-
lingual knowledge.
References
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 597?604. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
J. M. Bernardo, M. J. Bayarri, J. O. Berger, A. P.
Dawid, D. Heckerman, A. F. M. Smith, and M. West.
2003. The variational bayesian EM algorithm for in-
complete data: with application to scoring graphical
model structures. In Bayesian Statistics 7: Proceed-
ings of the 7th Valencia International Meeting, June
2-6, 2002, page 453. Oxford University Press, USA.
Y. Deng and Y. Gao. 2007. Guiding statistical word
alignment models with prior knowledge. In Pro-
ceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 1?8,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51?60, Prague, Czech Republic,
June. Association for Computational Linguistics.
Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008.
Improving word alignment using syntactic depen-
dencies. In Proceedings of the ACL-08: HLT Sec-
ond Workshop on Syntax and Structure in Statisti-
cal Translation (SSST-2), pages 69?77, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
HLT-NAACL 2003 Workshop on building and using
parallel texts: data driven machine translation and
beyond-Volume 3, page 10. Association for Compu-
tational Linguistics.
G. A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):41.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 440?447. Association for Computational
Linguistics Morristown, NJ, USA.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
B. Sagot and D. Fiser. 2008. Building a free French
wordnet from multilingual resources. In Proceed-
ings of Ontolex.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th Conference on Computa-
tional Linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main Conference
Poster Sessions, page 976. Association for Compu-
tational Linguistics.
B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
In Advances in Neural Information Processing Sys-
tems 20, pages 1689?1696, Cambridge, MA. MIT
Press.
141
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 206?211,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion Operator for Bayesian Tree Substitution Grammars
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
Abstract
We propose a model that incorporates an in-
sertion operator in Bayesian tree substitution
grammars (BTSG). Tree insertion is helpful
for modeling syntax patterns accurately with
fewer grammar rules than BTSG. The exper-
imental parsing results show that our model
outperforms a standard PCFG and BTSG for
a small dataset. For a large dataset, our model
obtains comparable results to BTSG, making
the number of grammar rules much smaller
than with BTSG.
1 Introduction
Tree substitution grammar (TSG) is a promising for-
malism for modeling language data. TSG general-
izes context free grammars (CFG) by allowing non-
terminal nodes to be replaced with subtrees of arbi-
trary size.
A natural extension of TSG involves adding an
insertion operator for combining subtrees as in
tree adjoining grammars (TAG) (Joshi, 1985) or
tree insertion grammars (TIG) (Schabes and Wa-
ters, 1995). An insertion operator is helpful for ex-
pressing various syntax patterns with fewer gram-
mar rules, thus we expect that adding an insertion
operator will improve parsing accuracy and realize a
compact grammar size.
One of the challenges of adding an insertion op-
erator is that the computational cost of grammar in-
duction is high since tree insertion significantly in-
creases the number of possible subtrees. Previous
work on TAG and TIG induction (Xia, 1999; Chi-
ang, 2003; Chen et al, 2006) has addressed the prob-
lem using language-specific heuristics and a maxi-
mum likelihood estimator, which leads to overfitting
the training data (Post and Gildea, 2009).
Instead, we incorporate an insertion operator in a
Bayesian TSG (BTSG) model (Cohn et al, 2011)
that learns grammar rules automatically without
heuristics. Our model uses a restricted variant of
subtrees for insertion to model the probability dis-
tribution simply and train the model efficiently. We
also present an inference technique for handling a
tree insertion that makes use of dynamic program-
ming.
2 Overview of BTSG Model
We briefly review the BTSG model described in
(Cohn et al, 2011). TSG uses a substitution operator
(shown in Fig. 1a) to combine subtrees. Subtrees for
substitution are referred to as initial trees, and leaf
nonterminals in initial trees are referred to as fron-
tier nodes. Their task is the unsupervised induction
of TSG derivations from parse trees. A derivation
is information about how subtrees are combined to
form parse trees.
The probability distribution over initial trees is de-
fined by using a Pitman-Yor process prior (Pitman
and Yor, 1997), that is,
e |X ? GX
GX |dX , ?X ? PYP (dX , ?X , P0 (? |X )) ,
where X is a nonterminal symbol, e is an initial tree
rooted with X , and P0 (? |X ) is a base distribution
over the infinite space of initial trees rooted with X .
dX and ?X are hyperparameters that are used to con-
trol the model?s behavior. Integrating out all possi-
ble values of GX , the resulting distribution is
206
p (ei |e?i, X, dX , ?X ) = ?ei,X + ?XP0 (ei, |X ) , (1)
where ?ei,X =
n?iei,X
?dX ?tei,X
?X+n
?i
?,X
and ?X =
?X+dX ?t?,X
?X+n
?i
?,X
. e?i = e1, . . . , ei?1 are previously gen-
erated initial trees, and n?iei,X is the number of times
ei has been used in e?i. tei,X is the number of ta-
bles labeled with ei. n
?i
?,X =
?
e n
?i
e,X and t?,X =?
e te,X are the total counts of initial trees and ta-
bles, respectively. The PYP prior produces ?rich get
richer? statistics: a few initial trees are often used
for derivation while many are rarely used, and this is
shown empirically to be well-suited for natural lan-
guage (Teh, 2006b; Johnson and Goldwater, 2009).
The base probability of an initial tree, P0 (e |X ),
is given as follows.
P0 (e |X ) =
?
r?CFG(e)
PMLE (r)?
?
A?LEAF(e)
sA
?
?
B?INTER(e)
(1? sB) , (2)
where CFG (e) is a set of decomposed CFG produc-
tions of e, PMLE (r) is a maximum likelihood esti-
mate (MLE) of r. LEAF (e) and INTER (e) are sets
of leaf and internal symbols of e, respectively. sX is
a stopping probability defined for each X .
3 Insertion Operator for BTSG
3.1 Tree Insertion Model
We propose a model that incorporates an insertion
operator in BTSG. Figure 1b shows an example of
an insertion operator. To distinguish them from ini-
tial trees, subtrees for insertion are referred to as
auxiliary trees. An auxiliary tree includes a special
nonterminal leaf node labeled with the same sym-
bol as the root node. This leaf node is referred to
as a foot node (marked with the subscript ?*?). The
definitions of substitution and insertion operators are
identical with those of TIG and TAG.
Since it is computationally expensive to allow any
auxiliary trees, we tackle the problem by introduc-
ing simple auxiliary trees, i.e., auxiliary trees whose
root node must generate a foot node as an immediate
child. For example, ?(N (JJ pretty) N*)? is a simple
auxiliary tree, but ?(S (NP ) (VP (V think) S*))? is
(a)
(b)
Figure 1: Example of (a) substitution and (b) inser-
tion (dotted line).
not. Note that we place no restriction on the initial
trees.
Our restricted formalism is a strict subset of TIG.
We briefly refer to some differences between TAG,
TIG and our insertion model. TAG generates tree
adjoining languages, a strict superset of context-
free languages, and the computational complexity
of parsing is O
(
n6
)
. TIG is a similar formalism
to TAG, but it does not allow wrapping adjunction
in TAG. Therefore, TIG generates context-free lan-
guages and the parsing complexity is O
(
n3
)
, which
is a strict subset of TAG. On the other hand, our
model prohibits neither wrapping adjunction in TAG
nor simultaneous adjunction in TIG, and allows only
simple auxiliary trees. The expressive power and
computational complexity of our formalism is iden-
tical to TIG, however, our model allows us to de-
fine the probability distribution over auxiliary trees
as having the same form as BTSG model. This en-
sures that we can make use of a dynamic program-
ming technique for training our model, which we de-
scribe the detail in the next subsection.
We define a probability distribution over simple
auxiliary trees as having the same form as eq. 1, that
is,
207
p (ei |e?i, X, d?X , ?
?
X ) = ?
?
ei,X + ?
?
XP
?
0 (ei, |X ) , (3)
where d?X and ?
?
X are hyperparameters of the in-
sertion model, and the definition of
(
??ei,X , ?
?
X
)
is
the same as that of (?ei,X , ?X) in eq. 1.
However, we need modify the base distribution
over simple auxiliary trees, P ?0 (e |X ), as follows,
so that all probabilities of the simple auxiliary trees
sum to one.
P ?0 (e |X ) = P
?
MLE (TOP (e))?
?
r?INTER_CFG(e)
PMLE (r)
?
?
A?LEAF(e)
sA ?
?
B?INTER(e)
(1? sB) , (4)
where TOP (e) is the CFG production that
starts with the root node of e. For example,
TOP (N (JJ pretty) (N*)) returns ?N ? JJ N*?.
INTER_CFG (e) is a set of CFG productions of e
excluding TOP (e). P ?MLE (r
?) is a modified MLE
for simple auxiliary trees, which is given by
{
C(r?)
C(X?X?Y )+C(X?Y X?) if r
?includes a foot node
0 else
where C (r?) is the frequency of r? in parse trees.
It is ensured that P ?0 (e |X ) generates a foot node as
an immediate child.
We define the probability distribution over both
initial trees and simple auxiliary trees with a PYP
prior. The base distribution over initial trees is de-
fined as P0 (e |X ), and the base distribution over
simple auxiliary trees is defined as P ?0 (e |X ). An
initial tree ei replaces a frontier node with prob-
ability p (ei |e?i, X, dX , ?X ). On the other hand,
a simple auxiliary tree e?i inserts an internal node
with probability aX?p?
(
e?i
?
?e??i, X, d
?
X , ?
?
X
)
, where
aX is an insertion probability defined for each X .
The stopping probabilities are common to both ini-
tial and auxiliary trees.
3.2 Grammar Decomposition
We develop a grammar decomposition technique,
which is an extension of work (Cohn and Blunsom,
2010) on BTSG model, to deal with an insertion
operator. The motivation behind grammar decom-
position is that it is hard to consider all possible
Figure 2: Derivation of Fig. 1b transformed by
grammar decomposition.
CFG rule probability
NP(NP (DT the) (N girl))?DT(DT the)Nins (N girl) (1? aDT)? aN
DT(DT the) ? the 1
Nins (N girl) ?Nins (N girl)(N (JJ pretty) N*) ?
?
(N (JJ pretty) N*),N
Nins (N girl)(N (JJ pretty) N*) ? JJ(JJ pretty)N(N girl) (1? aJJ)? 1
JJ(JJ pretty) ?pretty 1
N(N girl) ?girl 1
Table 1: The rules and probabilities of grammar de-
composition for Fig. 2.
derivations explicitly since the base distribution as-
signs non-zero probability to an infinite number of
initial and auxiliary trees. Alternatively, we trans-
form a derivation into CFG productions and assign
the probability for each CFG production so that its
assignment is consistent with the probability distri-
butions. We can efficiently calculate an inside prob-
ability (described in the next subsection) by employ-
ing grammar decomposition.
Here we provide an example of the derivation
shown in Fig. 1b. First, we can transform the deriva-
tion in Fig. 1b to another form as shown in Fig. 2.
In Fig. 2, all the derivation information is embed-
ded in each symbol. That is, NP(NP (DT the) (N girl)) is
a root symbol of the initial tree ?(NP (DT the) (N
girl))?, which generates two child nodes: DT(DT the)
and N(N girl). DT(DT the) generates the terminal node
?the?. On the other hand, Nins (N girl) denotes that
N(N girl) is inserted by some auxiliary tree, and
Nins (N girl)(N (JJ pretty) N*) denotes that the inserted simple aux-
iliary tree is ?(N (JJ pretty) (N*))?. The inserted
auxiliary tree, ?(N (JJ pretty) (N*))?, must generate
a foot node: ?(N girl)? as an immediate child.
208
Second, we decompose the transformed tree into
CFG productions and then assign the probability for
each CFG production as shown in Table 1, where
aDT, aN and aJJ are insertion probabilities for non-
terminal DT, N and JJ, respectively. Note that the
probability of a derivation according to Table 1 is
the same as the probability of a derivation obtained
from the distribution over the initial and auxiliary
trees (i.e. eq. 1 and eq. 3).
In Table 1, we assume that the auxiliary tree
?(N (JJ pretty) (N*))? is sampled from the first
term of eq. 3. When it is sampled from the sec-
ond term, we alternatively assign the probability
??(N (JJ pretty) N*), N.
3.3 Training
We use a blocked Metropolis-Hastings (MH) algo-
rithm (Cohn and Blunsom, 2010) to train our model.
The MH algorithm learns BTSG model parameters
efficiently, and it can be applied to our insertion
model. The MH algorithm consists of the following
three steps. For each sentence,
1. Calculate the inside probability (Lari and
Young, 1991) in a bottom-up manner using the
grammar decomposition.
2. Sample a derivation tree in a top-down manner.
3. Accept or reject the derivation sample by using
the MH test.
The MH algorithm is described in detail in (Cohn
and Blunsom, 2010). The hyperparameters of our
model are updated with the auxiliary variable tech-
nique (Teh, 2006a).
4 Experiments
We ran experiments on the British National Cor-
pus (BNC) Treebank 3 and the WSJ English Penn
Treebank. We did not use a development set since
our model automatically updates the hyperparame-
ters for every iteration. The treebank data was bina-
rized using the CENTER-HEAD method (Matsuzaki
et al, 2005). We replaced lexical words with counts
? 1 in the training set with one of three unknown
1Results from (Cohn and Blunsom, 2010).
2Results for length ? 40.
3http://nclt.computing.dcu.ie/~jfoster/resources/
corpus method F1
CFG 54.08
BNC BTSG 67.73
BTSG + insertion 69.06
CFG 64.99
BTSG 77.19
WSJ BTSG + insertion 78.54
(Petrov et al, 2006) 77.931
(Cohn and Blunsom, 2010) 78.40
Table 2: Small dataset experiments
# rules (# aux. trees) F1
CFG 35374 (-) 71.0
BTSG 80026 (0) 85.0
BTSG + insertion 65099 (25) 85.3
(Post and Gildea, 2009) - 82.62
(Cohn and Blunsom, 2010) - 85.3
Table 3: Full Penn Treebank dataset experiments
words using lexical features. We trained our model
using a training set, and then sampled 10k deriva-
tions for each sentence in a test set. Parsing results
were obtained with the MER algorithm (Cohn et al,
2011) using the 10k derivation samples. We show
the bracketing F1 score of predicted parse trees eval-
uated by EVALB4, averaged over three independent
runs.
In small dataset experiments, we used BNC (1k
sentences, 90% for training and 10% for testing) and
WSJ (section 2 for training and section 22 for test-
ing). This was a small-scale experiment, but large
enough to be relevant for low-resource languages.
We trained the model with an MH sampler for 1k
iterations. Table 2 shows the parsing results for
the test set. We compared our model with standard
PCFG and BTSG models implemented by us.
Our insertion model successfully outperformed
CFG and BTSG. This suggests that adding an inser-
tion operator is helpful for modeling syntax trees ac-
curately. The BTSG model described in (Cohn and
Blunsom, 2010) is similar to ours. They reported
an F1 score of 78.40 (the score of our BTSG model
was 77.19). We speculate that the performance gap
is due to data preprocessing such as the treatment of
rare words.
4http://nlp.cs.nyu.edu/evalb/
209
(N?P (N?P ) (: ?))
(N?P (N?P ) (ADVP (RB respectively)))
(P?P (P?P ) (, ,))
(V?P (V?P ) (RB then))
(Q?P (Q?P ) (IN of))
( ?SBAR ( ?SBAR ) (RB not))
(S? (S? ) (: ;))
Table 4: Examples of lexicalized auxiliary trees ob-
tained from our model in the full treebank dataset.
Nonterminal symbols created by binarization are
shown with an over-bar.
We also applied our model to the full WSJ Penn
Treebank setting (section 2-21 for training and sec-
tion 23 for testing). The parsing results are shown in
Table 3. We trained the model with an MH sampler
for 3.5k iterations.
For the full treebank dataset, our model obtained
nearly identical results to those obtained with BTSG
model, making the grammar size approximately
19% smaller than that of BTSG. We can see that only
a small number of auxiliary trees have a great impact
on reducing the grammar size. Surprisingly, there
are many fewer auxiliary trees than initial trees. We
believe this to be due to the tree binarization and our
restricted assumption of simple auxiliary trees.
Table 4 shows examples of lexicalized auxiliary
trees obtained with our model for the full treebank
data. We can see that punctuation (???, ?,?, and ?;?)
and adverb (RB) tend to be inserted in other trees.
Punctuation and adverb appear in various positions
in English sentences. Our results suggest that rather
than treat those words as substitutions, it is more rea-
sonable to consider them to be ?insertions?, which is
intuitively understandable.
5 Summary
We proposed a model that incorporates an inser-
tion operator in BTSG and developed an efficient
inference technique. Since it is computationally ex-
pensive to allow any auxiliary trees, we tackled the
problem by introducing a restricted variant of aux-
iliary trees. Our model outperformed the BTSG
model for a small dataset, and achieved compara-
ble parsing results for a large dataset, making the
number of grammars much smaller than the BTSG
model. We will extend our model to original TAG
and evaluate its impact on statistical parsing perfor-
mance.
References
J. Chen, S. Bangalore, and K. Vijay-Shanker. 2006.
Automated extraction of Tree-Adjoining Grammars
from treebanks. Natural Language Engineering,
12(03):251?299.
D. Chiang, 2003. Statistical Parsing with an Automati-
cally Extracted Tree Adjoining Grammar, chapter 16,
pages 299?316. CSLI Publications.
T. Cohn and P. Blunsom. 2010. Blocked inference in
Bayesian tree substitution grammars. In Proceedings
of the ACL 2010 Conference Short Papers, pages 225?
230, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Induc-
ing tree-substitution grammars. Journal of Machine
Learning Research. To Appear.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on unsu-
pervised word segmentation with adaptor grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 317?325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
A.K. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions? Natural Language Parsing:
Psychological, Computational, and Theoretical Per-
spectives, pages 206?250.
K. Lari and S.J. Young. 1991. Applications of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech & Language, 5(3):237?257.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 75?82. Association
for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), pages 433?440, Syd-
ney, Australia, July. Association for Computational
Linguistics.
210
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. The Annals of Probability, 25(2):855?900.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 45?48,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Y. Schabes and R.C. Waters. 1995. Tree insertion gram-
mar: a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees pro-
duced. Fuzzy Sets and Systems, 76(3):309?317.
Y. W. Teh. 2006a. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, School
of Computing, National University of Singapore.
Y. W. Teh. 2006b. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (ICCL-
ACL), pages 985?992.
F. Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS), pages 398?403.
211
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 429?433,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Is Machine Translation Ripe for Cross-lingual Sentiment Classification?
Kevin Duh and Akinori Fujino and Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
{kevin.duh,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
Abstract
Recent advances in Machine Translation (MT)
have brought forth a new paradigm for build-
ing NLP applications in low-resource scenar-
ios. To build a sentiment classifier for a
language with no labeled resources, one can
translate labeled data from another language,
then train a classifier on the translated text.
This can be viewed as a domain adaptation
problem, where labeled translations and test
data have some mismatch. Various prior work
have achieved positive results using this ap-
proach.
In this opinion piece, we take a step back and
make some general statements about cross-
lingual adaptation problems. First, we claim
that domain mismatch is not caused by MT
errors, and accuracy degradation will occur
even in the case of perfect MT. Second, we ar-
gue that the cross-lingual adaptation problem
is qualitatively different from other (monolin-
gual) adaptation problems in NLP; thus new
adaptation algorithms ought to be considered.
This paper will describe a series of carefully-
designed experiments that led us to these con-
clusions.
1 Summary
Question 1: If MT gave perfect translations (seman-
tically), do we still have a domain adaptation chal-
lenge in cross-lingual sentiment classification?
Answer: Yes. The reason is that while many trans-
lations of a word may be valid, the MT system might
have a systematic bias. For example, the word ?awe-
some? might be prevalent in English reviews, but in
translated reviews, the word ?excellent? is generated
instead. From the perspective of MT, this translation
is correct and preserves sentiment polarity. But from
the perspective of a classifier, there is a domain mis-
match due to differences in word distributions.
Question 2: Can we apply standard adaptation algo-
rithms developed for other (monolingual) adaptation
problems to cross-lingual adaptation?
Answer: No. It appears that the interaction between
target unlabeled data and source data can be rather
unexpected in the case of cross-lingual adaptation.
We do not know the reason, but our experiments
show that the accuracy of adaptation algorithms in
cross-lingual scenarios have much higher variance
than monolingual scenarios.
The goal of this opinion piece is to argue the need
to better understand the characteristics of domain
adaptation in cross-lingual problems. We invite the
reader to disagree with our conclusion (that the true
barrier to good performance is not insufficient MT
quality, but inappropriate domain adaptation meth-
ods). Here we present a series of experiments that
led us to this conclusion. First we describe the ex-
periment design (?2) and baselines (?3), before an-
swering Question 1 (?4) and Question 2 (?5).
2 Experiment Design
The cross-lingual setup is this: we have labeled data
from source domain S and wish to build a sentiment
classifier for target domain T . Domain mismatch
can arise from language differences (e.g. English vs.
translated text) or market differences (e.g. DVD vs.
Book reviews). Our experiments will involve fixing
429
T to a common testset and varying S. This allows us
to experiment with different settings for adaptation.
We use the Amazon review dataset of Pretten-
hofer (2010)1, due to its wide range of languages
(English [EN], Japanese [JP], French [FR], Ger-
man [DE]) and markets (music, DVD, books). Un-
like Prettenhofer (2010), we reverse the direction of
cross-lingual adaptation and consider English as tar-
get. English is not a low-resource language, but this
setting allows for more comparisons. Each source
dataset has 2000 reviews, equally balanced between
positive and negative. The target has 2000 test sam-
ples, large unlabeled data (25k, 30k, 50k samples
respectively for Music, DVD, and Books), and an
additional 2000 labeled data reserved for oracle ex-
periments. Texts in JP, FR, and DE are translated
word-by-word into English with Google Translate.2
We perform three sets of experiments, shown in
Table 1. Table 2 lists all the results; we will interpret
them in the following sections.
Target (T ) Source (S)
1 Music-EN Music-JP, Music-FR, Music-DE,
DVD-EN, Book-EN
2 DVD-EN DVD-JP, DVD-FR, DVD-DE,
Music-EN, Book-EN
3 Book-EN Book-JP, Book-FR, Book-DE,
Music-EN, DVD-EN
Table 1: Experiment setups: Fix T , vary S.
3 How much performance degradation
occurs in cross-lingual adaptation?
First, we need to quantify the accuracy degrada-
tion under different source data, without consider-
ation of domain adaptation methods. So we train
a SVM classifier on labeled source data3, and di-
rectly apply it on test data. The oracle setting, which
has no domain-mismatch (e.g. train on Music-EN,
test on Music-EN), achieves an average test accu-
racy of (81.6 + 80.9 + 80.0)/3 = 80.8%4. Aver-
1http://www.webis.de/research/corpora/webis-cls-10
2This is done by querying foreign words to build a bilingual
dictionary. The words are converted to tfidf unigram features.
3For all methods we try here, 5% of the 2000 labeled source
samples are held-out for parameter tuning.
4See column EN of Table 2, Supervised SVM results.
age cross-lingual accuracies are: 69.4% (JP), 75.6%
(FR), 77.0% (DE), so degradations compared to or-
acle are: -11% (JP), -5% (FR), -4% (DE).5 Cross-
market degradations are around -6%6.
Observation 1: Degradations due to market and
language mismatch are comparable in several cases
(e.g. MUSIC-DE and DVD-EN perform similarly
for target MUSIC-EN). Observation 2: The ranking
of source language by decreasing accuracy is DE >
FR > JP. Does this mean JP-EN is a more difficult
language pair for MT? The next section will show
that this is not necessarily the case. Certainly, the
domain mismatch for JP is larger than DE, but this
could be due to phenomenon other than MT errors.
4 Where exactly is the domain mismatch?
4.1 Theory of Domain Adaptation
We analyze domain adaptation by the concepts of
labeling and instance mismatch (Jiang and Zhai,
2007). Let pt(x, y) = pt(y|x)pt(x) be the target
distribution of samples x (e.g. unigram feature vec-
tor) and labels y (positive / negative). Let ps(x, y) =
ps(y|x)ps(x) be the corresponding source distribu-
tion. We assume that one (or both) of the following
distributions differ between source and target:
? Instance mismatch: ps(x) 6= pt(x).
? Labeling mismatch: ps(y|x) 6= pt(y|x).
Instance mismatch implies that the input feature
vectors have different distribution (e.g. one dataset
uses the word ?excellent? often, while the other uses
the word ?awesome?). This degrades performance
because classifiers trained on ?excellent? might not
know how to classify texts with the word ?awe-
some.? The solution is to tie together these features
(Blitzer et al, 2006) or re-weight the input distribu-
tion (Sugiyama et al, 2008). Under some assump-
tions (i.e. covariate shift), oracle accuracy can be
achieved theoretically (Shimodaira, 2000).
Labeling mismatch implies the same input has
different labels in different domains. For exam-
ple, the JP word meaning ?excellent? may be mis-
translated as ?bad? in English. Then, positive JP
5See ?Adapt by Language? columns of Table 2. Note
JP+FR+DE condition has 6000 labeled samples, so is not di-
rectly comparable to other adaptation scenarios (2000 samples).
Nevertheless, mixing languages seem to give good results.
6See ?Adapt by Market? columns of Table 2.
430
Target Classifier Oracle Adapt by Language Adapt by Market
EN JP FR DE JP+FR+DE MUSIC DVD BOOK
MUSIC-EN Supervised SVM 81.6 68.5 75.2 76.3 80.3 - 76.8 74.1
Adapted TSVM 79.6 73.0 74.6 77.9 78.6 - 78.4 75.6
DVD-EN Supervised SVM 80.9 70.1 76.4 77.4 79.7 75.2 - 74.5
Adapted TSVM 81.0 71.4 75.5 76.3 78.4 74.8 - 76.7
BOOK-EN Supervised SVM 80.0 69.6 75.4 77.4 79.9 73.4 76.2 -
Adapted TSVM 81.2 73.8 77.6 76.7 79.5 75.1 77.4 -
Table 2: Test accuracies (%) for English Music/DVD/Book reviews. Each column is an adaptation scenario using
different source data. The source data may vary by language or by market. For example, the first row shows that for
the target of Music-EN, the accuracy of a SVM trained on translated JP reviews (in the same market) is 68.5, while the
accuracy of a SVM trained on DVD reviews (in the same language) is 76.8. ?Oracle? indicates training on the same
market and same language domain as the target. ?JP+FR+DE? indicates the concatenation of JP, FR, DE as source
data. Boldface shows the winner of Supervised vs. Adapted.
reviews will be associated with the word ?bad?:
ps(y = +1|x = bad) will be high, whereas the true
conditional distribution should have high pt(y =
?1|x = bad) instead. There are several cases for
labeling mismatch, depending on how the polarity
changes (Table 3). The solution is to filter out these
noisy samples (Jiang and Zhai, 2007) or optimize
loosely-linked objectives through shared parameters
or Bayesian priors (Finkel and Manning, 2009).
Which mismatch is responsible for accuracy
degradations in cross-lingual adaptation?
? Instance mismatch: Systematic MT bias gener-
ates word distributions different from naturally-
occurring English. (Translation may be valid.)
? Label mismatch: MT error mis-translates a word
into something with different polarity.
Conclusion from ?4.2 and ?4.3: Instance mis-
match occurs often; MT error appears minimal.
Mis-translated polarity Effect
? ? 0 Loose a discriminative
e.g. (?good? ? ?the?) feature
0 ? ? Increased overlap in
e.g. (?the? ? ?good?) positive/negative data
+ ? ? and ? ? + Association with
e.g. (?good? ? ?bad?) opposite label
Table 3: Label mismatch: mis-translating positive (+),
negative (?), or neutral (0) words have different effects.
We think the first two cases have graceful degradation,
but the third case may be catastrophic.
4.2 Analysis of Instance Mismatch
To measure instance mismatch, we compute statis-
tics between ps(x) and pt(x), or approximations
thereof: First, we calculate a (normalized) average
feature from all samples of source S, which repre-
sents the unigram distribution of MT output. Simi-
larly, the average feature vector for target T approx-
imates the unigram distribution of English reviews
pt(x). Then we measure:
? KL Divergence between Avg(S) and Avg(T ),
where Avg() is the average vector.
? Set Coverage of Avg(T ) on Avg(S): how many
word (type) in T appears at least once in S.
Both measures correlate strongly with final accu-
racy, as seen in Figure 1. The correlation coefficients
are r = ?0.78 for KL Divergence and r = 0.71 for
Coverage, both statistically significant (p < 0.05).
This implies that instance mismatch is an important
reason for the degradations seen in Section 3.7
4.3 Analysis of Labeling Mismatch
We measure labeling mismatch by looking at dif-
ferences in the weight vectors of oracle SVM and
adapted SVM. Intuitively, if a feature has positive
weight in the oracle SVM, but negative weight in the
adapted SVM, then it is likely a MT mis-translation
7The observant reader may notice that cross-market points
exhibit higher coverage but equal accuracy (74-78%) to some
cross-lingual points. This suggests that MT output may be more
constrained in vocabulary than naturally-occurring English.
431
68 70 72 74 76 78 80 82
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Accuracy
KL
 D
iv
er
ge
nc
e
68 70 72 74 76 78 80 82
0.4
0.5
0.6
0.7
0.8
0.9
Accuracy
Te
st
 C
ov
er
ag
e
Figure 1: KL Divergence and Coverage vs. accuracy. (o)
are cross-lingual and (x) are cross-market data points.
is causing the polarity flip. Algorithm 1 (with
K=2000) shows how we compute polarity flip rate.8
We found that the polarity flip rate does not cor-
relate well with accuracy at all (r = 0.04). Conclu-
sion: Labeling mismatch is not a factor in perfor-
mance degradation. Nevertheless, we note there is a
surprising large number of flips (24% on average). A
manual check of the flipped words in BOOK-JP re-
vealed few MT mistakes. Only 3.7% of 450 random
EN-JP word pairs checked can be judged as blatantly
incorrect (without sentence context). The majority
of flipped words do not have a clear sentiment ori-
entation (e.g. ?amazon?, ?human?, ?moreover?).
5 Are standard adaptation algorithms
applicable to cross-lingual problems?
One of the breakthroughs in cross-lingual text clas-
sification is the realization that it can be cast as do-
main adaptation. This makes available a host of pre-
existing adaptation algorithms for improving over
supervised results. However, we argue that it may be
8The feature normalization in Step 1 is important to ensure
that the weight magnitudes are comparable.
Algorithm 1 Measuring labeling mismatch
Input: Weight vectors for source ws and target wt
Input: Target data average sample vector avg(T )
Output: Polarity flip rate f
1: Normalize: ws = avg(T ) * ws; wt = avg(T ) * wt
2: Set S+ = { K most positive features in ws}
3: Set S? = { K most negative features in ws}
4: Set T+ = { K most positive features in wt}
5: Set T? = { K most negative features in wt}
6: for each feature i ? T+ do
7: if i ? S? then f = f + 1
8: end for
9: for each feature j ? T? do
10: if j ? S+ then f = f + 1
11: end for
12: f = f2K
better to ?adapt? the standard adaptation algorithm
to the cross-lingual setting. We arrived at this con-
clusion by trying the adapted counterpart of SVMs
off-the-shelf. Recently, (Bergamo and Torresani,
2010) showed that Transductive SVMs (TSVM),
originally developed for semi-supervised learning,
are also strong adaptation methods. The idea is to
train on source data like a SVM, but encourage the
classification boundary to divide through low den-
sity regions in the unlabeled target data.
Table 2 shows that TSVM outperforms SVM in
all but one case for cross-market adaptation, but
gives mixed results for cross-lingual adaptation.
This is a puzzling result considering that both use
the same unlabeled data. Why does TSVM exhibit
such a large variance on cross-lingual problems, but
not on cross-market problems? Is unlabeled target
data interacting with source data in some unexpected
way?
Certainly there are several successful studies
(Wan, 2009; Wei and Pal, 2010; Banea et al, 2008),
but we think it is important to consider the possi-
bility that cross-lingual adaptation has some fun-
damental differences. We conjecture that adapting
from artificially-generated text (e.g. MT output)
is a different story than adapting from naturally-
occurring text (e.g. cross-market). In short, MT is
ripe for cross-lingual adaptation; what is not ripe is
probably our understanding of the special character-
istics of the adaptation problem.
432
References
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity analy-
sis using machine translation. In Proc. of Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve ob-
ject classification: a domain adaptation approach. In
Advances in Neural Information Processing Systems
(NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Jenny Rose Finkel and Chris Manning. 2009. Hierarchi-
cal Bayesian domain adaptation. In Proc. of NAACL
Human Language Technologies (HLT).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proc. of the As-
sociation for Computational Linguistics (ACL).
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural correspon-
dence learning. In Proc. of the Association for Com-
putational Linguistics (ACL).
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inferenc, 90.
Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,
Hisashi Kashima, Paul von Bu?nau, and Motoaki
Kawanabe. 2008. Direct importance estimation for
covariate shift adaptation. Annals of the Institute of
Statistical Mathematics, 60(4).
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proc. of the Association for
Computational Linguistics (ACL).
Bin Wei and Chris Pal. 2010. Cross lingual adaptation:
an experiment on sentiment classification. In Proceed-
ings of the ACL 2010 Conference Short Papers.
433
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440?448,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Bayesian Symbol-Refined Tree Substitution Grammars
for Syntactic Parsing
Hiroyuki Shindo? Yusuke Miyao? Akinori Fujino? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
?National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
yusuke@nii.ac.jp
Abstract
We propose Symbol-Refined Tree Substitu-
tion Grammars (SR-TSGs) for syntactic pars-
ing. An SR-TSG is an extension of the con-
ventional TSG model where each nonterminal
symbol can be refined (subcategorized) to fit
the training data. We aim to provide a unified
model where TSG rules and symbol refine-
ment are learned from training data in a fully
automatic and consistent fashion. We present
a novel probabilistic SR-TSG model based
on the hierarchical Pitman-Yor Process to en-
code backoff smoothing from a fine-grained
SR-TSG to simpler CFG rules, and develop
an efficient training method based on Markov
Chain Monte Carlo (MCMC) sampling. Our
SR-TSG parser achieves an F1 score of 92.4%
in the Wall Street Journal (WSJ) English Penn
Treebank parsing task, which is a 7.7 point im-
provement over a conventional Bayesian TSG
parser, and better than state-of-the-art discrim-
inative reranking parsers.
1 Introduction
Syntactic parsing has played a central role in natural
language processing. The resulting syntactic analy-
sis can be used for various applications such as ma-
chine translation (Galley et al, 2004; DeNeefe and
Knight, 2009), sentence compression (Cohn and La-
pata, 2009; Yamangil and Shieber, 2010), and ques-
tion answering (Wang et al, 2007). Probabilistic
context-free grammar (PCFG) underlies many sta-
tistical parsers, however, it is well known that the
PCFG rules extracted from treebank data via maxi-
mum likelihood estimation do not perform well due
to unrealistic context freedom assumptions (Klein
and Manning, 2003).
In recent years, there has been an increasing inter-
est in tree substitution grammar (TSG) as an alter-
native to CFG for modeling syntax trees (Post and
Gildea, 2009; Tenenbaum et al, 2009; Cohn et al,
2010). TSG is a natural extension of CFG in which
nonterminal symbols can be rewritten (substituted)
with arbitrarily large tree fragments. These tree frag-
ments have great advantages over tiny CFG rules
since they can capture non-local contexts explic-
itly such as predicate-argument structures, idioms
and grammatical agreements (Cohn et al, 2010).
Previous work on TSG parsing (Cohn et al, 2010;
Post and Gildea, 2009; Bansal and Klein, 2010) has
consistently shown that a probabilistic TSG (PTSG)
parser is significantly more accurate than a PCFG
parser, but is still inferior to state-of-the-art parsers
(e.g., the Berkeley parser (Petrov et al, 2006) and
the Charniak parser (Charniak and Johnson, 2005)).
One major drawback of TSG is that the context free-
dom assumptions still remain at substitution sites,
that is, TSG tree fragments are generated that are
conditionally independent of all others given root
nonterminal symbols. Furthermore, when a sentence
is unparsable with large tree fragments, the PTSG
parser usually uses naive CFG rules derived from
its backoff model, which diminishes the benefits ob-
tained from large tree fragments.
On the other hand, current state-of-the-art parsers
use symbol refinement techniques (Johnson, 1998;
Collins, 2003; Matsuzaki et al, 2005). Symbol
refinement is a successful approach for weaken-
ing context freedom assumptions by dividing coarse
treebank symbols (e.g. NP and VP) into sub-
categories, rather than extracting large tree frag-
ments. As shown in several studies on TSG pars-
ing (Zuidema, 2007; Bansal and Klein, 2010), large
440
tree fragments and symbol refinement work comple-
mentarily for syntactic parsing. For example, Bansal
and Klein (2010) have reported that deterministic
symbol refinement with heuristics helps improve the
accuracy of a TSG parser.
In this paper, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. SR-TSG is an extension of the conventional
TSG model where each nonterminal symbol can be
refined (subcategorized) to fit the training data. Our
work differs from previous studies in that we focus
on a unified model where TSG rules and symbol re-
finement are learned from training data in a fully au-
tomatic and consistent fashion. We also propose a
novel probabilistic SR-TSG model with the hierar-
chical Pitman-Yor Process (Pitman and Yor, 1997),
namely a sort of nonparametric Bayesian model, to
encode backoff smoothing from a fine-grained SR-
TSG to simpler CFG rules, and develop an efficient
training method based on blocked MCMC sampling.
Our SR-TSG parser achieves an F1 score of
92.4% in the WSJ English Penn Treebank pars-
ing task, which is a 7.7 point improvement over a
conventional Bayesian TSG parser, and superior to
state-of-the-art discriminative reranking parsers.
2 Background and Related Work
Our SR-TSG work is built upon recent work on
Bayesian TSG induction from parse trees (Post and
Gildea, 2009; Cohn et al, 2010). We firstly review
the Bayesian TSG model used in that work, and then
present related work on TSGs and symbol refine-
ment.
A TSG consists of a 4-tuple, G = (T,N, S,R),
where T is a set of terminal symbols, N is a set of
nonterminal symbols, S ? N is the distinguished
start nonterminal symbol and R is a set of produc-
tions (a.k.a. rules). The productions take the form
of elementary trees i.e., tree fragments of height
? 1. The root and internal nodes of the elemen-
tary trees are labeled with nonterminal symbols, and
leaf nodes are labeled with either terminal or nonter-
minal symbols. Nonterminal leaves are referred to
as frontier nonterminals, and form the substitution
sites to be combined with other elementary trees.
A derivation is a process of forming a parse tree.
It starts with a root symbol and rewrites (substi-
tutes) nonterminal symbols with elementary trees
until there are no remaining frontier nonterminals.
Figure 1a shows an example parse tree and Figure
1b shows its example TSG derivation. Since differ-
ent derivations may produce the same parse tree, re-
cent work on TSG induction (Post and Gildea, 2009;
Cohn et al, 2010) employs a probabilistic model of
a TSG and predicts derivations from observed parse
trees in an unsupervised way.
A Probabilistic Tree Substitution Grammar
(PTSG) assigns a probability to each rule in the
grammar. The probability of a derivation is defined
as the product of the probabilities of its component
elementary trees as follows.
p (e) =
?
x?e?e
p (e |x) ,
where e = (e1, e2, . . .) is a sequence of elemen-
tary trees used for the derivation, x = root (e) is the
root symbol of e, and p (e |x) is the probability of
generating e given its root symbol x. As in a PCFG,
e is generated conditionally independent of all oth-
ers given x.
The posterior distribution over elementary trees
given a parse tree t can be computed by using the
Bayes? rule:
p (e |t) ? p (t |e) p (e) .
where p (t |e) is either equal to 1 (when t and e
are consistent) or 0 (otherwise). Therefore, the task
of TSG induction from parse trees turns out to con-
sist of modeling the prior distribution p (e). Recent
work on TSG induction defines p (e) as a nonpara-
metric Bayesian model such as the Dirichlet Pro-
cess (Ferguson, 1973) or the Pitman-Yor Process to
encourage sparse and compact grammars.
Several studies have combined TSG induction and
symbol refinement. An adaptor grammar (Johnson
et al, 2007a) is a sort of nonparametric Bayesian
TSG model with symbol refinement, and is thus
closely related to our SR-TSG model. However,
an adaptor grammar differs from ours in that all its
rules are complete: all leaf nodes must be termi-
nal symbols, while our model permits nonterminal
symbols as leaf nodes. Furthermore, adaptor gram-
mars have largely been applied to the task of unsu-
pervised structural induction from raw texts such as
441
(a) (b) (c)
Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of
(a). The refinement annotation is hyphenated with a nonterminal symbol.
morphology analysis, word segmentation (Johnson
and Goldwater, 2009), and dependency grammar in-
duction (Cohen et al, 2010), rather than constituent
syntax parsing.
An all-fragments grammar (Bansal and Klein,
2010) is another variant of TSG that aims to uti-
lize all possible subtrees as rules. It maps a TSG
to an implicit representation to make the grammar
tractable and practical for large-scale parsing. The
manual symbol refinement described in (Klein and
Manning, 2003) was applied to an all-fragments
grammar and this improved accuracy in the English
WSJ parsing task. As mentioned in the introduc-
tion, our model focuses on the automatic learning of
a TSG and symbol refinement without heuristics.
3 Symbol-Refined Tree Substitution
Grammars
In this section, we propose Symbol-Refined Tree
Substitution Grammars (SR-TSGs) for syntactic
parsing. Our SR-TSG model is an extension of
the conventional TSG model where every symbol of
the elementary trees can be refined to fit the train-
ing data. Figure 1c shows an example of SR-TSG
derivation. As with previous work on TSG induc-
tion, our task is the induction of SR-TSG deriva-
tions from a corpus of parse trees in an unsupervised
fashion. That is, we wish to infer the symbol sub-
categories of every node and substitution site (i.e.,
nodes where substitution occurs) from parse trees.
Extracted rules and their probabilities can be used to
parse new raw sentences.
3.1 Probabilistic Model
We define a probabilistic model of an SR-TSG based
on the Pitman-Yor Process (PYP) (Pitman and Yor,
1997), namely a sort of nonparametric Bayesian
model. The PYP produces power-law distributions,
which have been shown to be well-suited for such
uses as language modeling (Teh, 2006b), and TSG
induction (Cohn et al, 2010). One major issue as
regards modeling an SR-TSG is that the space of the
grammar rules will be very sparse since SR-TSG al-
lows for arbitrarily large tree fragments and also an
arbitrarily large set of symbol subcategories. To ad-
dress the sparseness problem, we employ a hierar-
chical PYP to encode a backoff scheme from the SR-
TSG rules to simpler CFG rules, inspired by recent
work on dependency parsing (Blunsom and Cohn,
2010).
Our model consists of a three-level hierarchy. Ta-
ble 1 shows an example of the SR-TSG rule and its
backoff tree fragments as an illustration of this three-
level hierarchy. The topmost level of our model is a
distribution over the SR-TSG rules as follows.
e |xk ? Gxk
Gxk ? PYP
(
dxk , ?xk , P
sr-tsg (? |xk )
)
,
where xk is a refined root symbol of an elemen-
tary tree e, while x is a raw nonterminal symbol
in the corpus and k = 0, 1, . . . is an index of the
symbol subcategory. Suppose x is NP and its sym-
bol subcategory is 0, then xk is NP0. The PYP has
three parameters: (dxk , ?xk , P
sr-tsg). P sr-tsg (? |xk )
442
SR-TSG SR-CFG RU-CFG
Table 1: Example three-level backoff.
is a base distribution over infinite space of symbol-
refined elementary trees rooted with xk, which pro-
vides the backoff probability of e. The remaining
parameters dxk and ?xk control the strength of the
base distribution.
The backoff probability P sr-tsg (e |xk ) is given by
the product of symbol-refined CFG (SR-CFG) rules
that e contains as follows.
P sr-tsg (e |xk ) =
?
f?F (e)
scf ?
?
i?I(e)
(1? sci)
? H (cfg-rules (e |xk ))
? |xk ? Hxk
Hxk ? PYP
(
dx, ?x, P
sr-cfg (? |xk )
)
,
where F (e) is a set of frontier nonterminal nodes
and I (e) is a set of internal nodes in e. cf and ci
are nonterminal symbols of nodes f and i, respec-
tively. sc is the probability of stopping the expan-
sion of a node labeled with c. SR-CFG rules are
CFG rules where every symbol is refined, as shown
in Table 1. The function cfg-rules (e |xk ) returns
the SR-CFG rules that e contains, which take the
form of xk ? ?. Each SR-CFG rule ? rooted
with xk is drawn from the backoff distribution Hxk ,
and Hxk is produced by the PYP with parameters:(
dx, ?x, P sr-cfg
)
. This distribution over the SR-CFG
rules forms the second level hierarchy of our model.
The backoff probability of the SR-CFG rule,
P sr-cfg (? |xk ), is given by the root-unrefined CFG
(RU-CFG) rule as follows,
P sr-cfg (? |xk ) = I (root-unrefine (? |xk ))
? |x ? Ix
Ix ? PYP
(
d?x, ?
?
x, P
ru-cfg (? |x )
)
,
where the function root-unrefine (? |xk ) returns
the RU-CFG rule of ?, which takes the form of x?
?. The RU-CFG rule is a CFG rule where the root
symbol is unrefined and all leaf nonterminal sym-
bols are refined, as shown in Table 1. Each RU-CFG
rule ? rooted with x is drawn from the backoff distri-
bution Ix, and Ix is produced by a PYP. This distri-
bution over the RU-CFG rules forms the third level
hierarchy of our model. Finally, we set the back-
off probability of the RU-CFG rule, P ru-cfg (? |x),
so that it is uniform as follows.
P ru-cfg (? |x ) =
1
|x? ?|
.
where |x? ?| is the number of RU-CFG rules
rooted with x. Overall, our hierarchical model en-
codes backoff smoothing consistently from the SR-
TSG rules to the SR-CFG rules, and from the SR-
CFG rules to the RU-CFG rules. As shown in (Blun-
som and Cohn, 2010; Cohen et al, 2010), the pars-
ing accuracy of the TSG model is strongly affected
by its backoff model. The effects of our hierarchical
backoff model on parsing performance are evaluated
in Section 5.
4 Inference
We use Markov Chain Monte Carlo (MCMC) sam-
pling to infer the SR-TSG derivations from parse
trees. MCMC sampling is a widely used approach
for obtaining random samples from a probability
distribution. In our case, we wish to obtain deriva-
tion samples of an SR-TSG from the posterior dis-
tribution, p (e |t,d,?, s).
The inference of the SR-TSG derivations corre-
sponds to inferring two kinds of latent variables:
latent symbol subcategories and latent substitution
443
sites. We first infer latent symbol subcategories for
every symbol in the parse trees, and then infer latent
substitution sites stepwise. During the inference of
symbol subcategories, every internal node is fixed as
a substitution site. After that, we unfix that assump-
tion and infer latent substitution sites given symbol-
refined parse trees. This stepwise learning is simple
and efficient in practice, but we believe that the joint
learning of both latent variables is possible, and we
will deal with this in future work. Here we describe
each inference algorithm in detail.
4.1 Inference of Symbol Subcategories
For the inference of latent symbol subcategories, we
adopt split and merge training (Petrov et al, 2006)
as follows. In each split-merge step, each symbol
is split into at most two subcategories. For exam-
ple, every NP symbol in the training data is split into
either NP0 or NP1 to maximize the posterior prob-
ability. After convergence, we measure the loss of
each split symbol in terms of the likelihood incurred
when removing it, then the smallest 50% of the
newly split symbols as regards that loss are merged
to avoid overfitting. The split-merge algorithm ter-
minates when the total number of steps reaches the
user-specified value.
In each splitting step, we use two types of blocked
MCMC algorithm: the sentence-level blocked
Metroporil-Hastings (MH) sampler and the tree-
level blocked Gibbs sampler, while (Petrov et al,
2006) use a different MLE-based model and the EM
algorithm. Our sampler iterates sentence-level sam-
pling and tree-level sampling alternately.
The sentence-level MH sampler is a recently pro-
posed algorithm for grammar induction (Johnson et
al., 2007b; Cohn et al, 2010). In this work, we apply
it to the training of symbol splitting. The MH sam-
pler consists of the following three steps: for each
sentence, 1) calculate the inside probability (Lari
and Young, 1991) in a bottom-up manner, 2) sample
a derivation tree in a top-down manner, and 3) ac-
cept or reject the derivation sample by using the MH
test. See (Cohn et al, 2010) for details. This sampler
simultaneously updates blocks of latent variables as-
sociated with a sentence, thus it can find MAP solu-
tions efficiently.
The tree-level blocked Gibbs sampler focuses on
the type of SR-TSG rules and simultaneously up-
dates all root and child nodes that are annotated
with the same SR-TSG rule. For example, the
sampler collects all nodes that are annotated with
S0 ? NP1VP2, then updates those nodes to an-
other subcategory such as S0 ? NP2VP0 according
to the posterior distribution. This sampler is simi-
lar to table label resampling (Johnson and Goldwa-
ter, 2009), but differs in that our sampler can update
multiple table labels simultaneously when multiple
tables are labeled with the same elementary tree.
The tree-level sampler also simultaneously updates
blocks of latent variables associated with the type of
SR-TSG rules, thus it can find MAP solutions effi-
ciently.
4.2 Inference of Substitution Sites
After the inference of symbol subcategories, we
use Gibbs sampling to infer the substitution sites of
parse trees as described in (Cohn and Lapata, 2009;
Post and Gildea, 2009). We assign a binary variable
to each internal node in the training data, which in-
dicates whether that node is a substitution site or not.
For each iteration, the Gibbs sampler works by sam-
pling the value of each binary variable in random
order. See (Cohn et al, 2010) for details.
During the inference, our sampler ignores
the symbol subcategories of internal nodes of
elementary trees since they do not affect the
derivation of the SR-TSG. For example, the
elementary trees ?(S0 (NP0 NNP0) VP0)? and
?(S0 (NP1 NNP0) VP0)? are regarded as being the
same when we calculate the generation probabilities
according to our model. This heuristics is help-
ful for finding large tree fragments and learning
compact grammars.
4.3 Hyperparameter Estimation
We treat hyperparameters {d,?} as random vari-
ables and update their values for every MCMC it-
eration. We place a prior on the hyperparameters as
follows: d ? Beta (1, 1), ? ? Gamma (1, 1). The
values of d and ? are optimized with the auxiliary
variable technique (Teh, 2006a).
444
5 Experiment
5.1 Settings
5.1.1 Data Preparation
We ran experiments on the Wall Street Journal
(WSJ) portion of the English Penn Treebank data
set (Marcus et al, 1993), using a standard data
split (sections 2?21 for training, 22 for development
and 23 for testing). We also used section 2 as a
small training set for evaluating the performance of
our model under low-resource conditions. Hence-
forth, we distinguish the small training set (section
2) from the full training set (sections 2-21). The tree-
bank data is right-binarized (Matsuzaki et al, 2005)
to construct grammars with only unary and binary
productions. We replace lexical words with count
? 5 in the training data with one of 50 unknown
words using lexical features, following (Petrov et al,
2006). We also split off all the function tags and
eliminated empty nodes from the data set, follow-
ing (Johnson, 1998).
5.1.2 Training and Parsing
For the inference of symbol subcategories, we
trained our model with the MCMC sampler by us-
ing 6 split-merge steps for the full training set and 3
split-merge steps for the small training set. There-
fore, each symbol can be subdivided into a maxi-
mum of 26 = 64 and 23 = 8 subcategories, respec-
tively. In each split-merge step, we initialized the
sampler by randomly splitting every symbol in two
subcategories and ran the MCMC sampler for 1000
iterations. After that, to infer the substitution sites,
we initialized the model with the final sample from
a run on the small training set, and used the Gibbs
sampler for 2000 iterations. We estimated the opti-
mal values of the stopping probabilities s by using
the development set.
We obtained the parsing results with the MAX-
RULE-PRODUCT algorithm (Petrov et al, 2006) by
using the SR-TSG rules extracted from our model.
We evaluated the accuracy of our parser by brack-
eting F1 score of predicted parse trees. We used
EVALB1 to compute the F1 score. In all our exper-
iments, we conducted ten independent runs to train
our model, and selected the one that performed best
on the development set in terms of parsing accuracy.
1http://nlp.cs.nyu.edu/evalb/
Model F1 (small) F1 (full)
CFG 61.9 63.6
*TSG 77.1 85.0
SR-TSG (P sr-tsg) 73.0 86.4
SR-TSG (P sr-tsg, P sr-cfg) 79.4 89.7
SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg) 81.7 91.1
Table 2: Comparison of parsing accuracy with the
small and full training sets. *Our reimplementation
of (Cohn et al, 2010).
Figure 2: Histogram of SR-TSG and TSG rule sizes
on the small training set. The size is defined as the
number of CFG rules that the elementary tree con-
tains.
5.2 Results and Discussion
5.2.1 Comparison of SR-TSG with TSG
We compared the SR-TSG model with the CFG
and TSG models as regards parsing accuracy. We
also tested our model with three backoff hierarchy
settings to evaluate the effects of backoff smoothing
on parsing accuracy. Table 2 shows the F1 scores
of the CFG, TSG and SR-TSG parsers for small and
full training sets. In Table 2, SR-TSG (P sr-tsg) de-
notes that we used only the topmost level of the hi-
erarchy. Similary, SR-TSG (P sr-tsg, P sr-cfg) denotes
that we used only the P sr-tsg and P sr-cfg backoff mod-
els.
Our best model, SR-TSG (P sr-tsg, P sr-cfg, P ru-cfg),
outperformed both the CFG and TSG models on
both the small and large training sets. This result
suggests that the conventional TSG model trained
from the vanilla treebank is insufficient to resolve
445
Model F1 (? 40) F1 (all)
TSG (no symbol refinement)
Post and Gildea (2009) 82.6 -
Cohn et al (2010) 85.4 84.7
TSG with Symbol Refinement
Zuidema (2007) - *83.8
Bansal et al (2010) 88.7 88.1
SR-TSG (single) 91.6 91.1
SR-TSG (multiple) 92.9 92.4
CFG with Symbol Refinement
Collins (1999) 88.6 88.2
Petrov and Klein (2007) 90.6 90.1
Petrov (2010) - 91.8
Discriminative
Carreras et al (2008) - 91.1
Charniak and Johnson (2005) 92.0 91.4
Huang (2008) 92.3 91.7
Table 3: Our parsing performance for the testing set compared with those of other parsers. *Results for the
development set (? 100).
structural ambiguities caused by coarse symbol an-
notations in a training corpus. As we expected, sym-
bol refinement can be helpful with the TSG model
for further fitting the training set and improving the
parsing accuracy.
The performance of the SR-TSG parser was
strongly affected by its backoff models. For exam-
ple, the simplest model, P sr-tsg, performed poorly
compared with our best model. This result suggests
that the SR-TSG rules extracted from the training
set are very sparse and cannot cover the space of
unknown syntax patterns in the testing set. There-
fore, sophisticated backoff modeling is essential for
the SR-TSG parser. Our hierarchical PYP model-
ing technique is a successful way to achieve back-
off smoothing from sparse SR-TSG rules to simpler
CFG rules, and offers the advantage of automatically
estimating the optimal backoff probabilities from the
training set.
We compared the rule sizes and frequencies of
SR-TSG with those of TSG. The rule sizes of SR-
TSG and TSG are defined as the number of CFG
rules that the elementary tree contains. Figure 2
shows a histogram of the SR-TSG and TSG rule
sizes (by unrefined token) on the small training set.
For example, SR-TSG rules: S1 ? NP0VP1 and
S0 ? NP1VP2 were considered to be the same to-
ken. In Figure 2, we can see that there are almost
the same number of SR-TSG rules and TSG rules
with size = 1. However, there are more SR-TSG
rules than TSG rules with size ? 2. This shows
that an SR-TSG can use various large tree fragments
depending on the context, which is specified by the
symbol subcategories.
5.2.2 Comparison of SR-TSG with Other
Models
We compared the accuracy of the SR-TSG parser
with that of conventional high-performance parsers.
Table 3 shows the F1 scores of an SR-TSG and con-
ventional parsers with the full training set. In Ta-
ble 3, SR-TSG (single) is a standard SR-TSG parser,
446
and SR-TSG (multiple) is a combination of sixteen
independently trained SR-TSG models, following
the work of (Petrov, 2010).
Our SR-TSG (single) parser achieved an F1 score
of 91.1%, which is a 6.4 point improvement over
the conventional Bayesian TSG parser reported by
(Cohn et al, 2010). Our model can be viewed as
an extension of Cohn?s work by the incorporation
of symbol refinement. Therefore, this result con-
firms that a TSG and symbol refinement work com-
plementarily in improving parsing accuracy. Com-
pared with a symbol-refined CFG model such as the
Berkeley parser (Petrov et al, 2006), the SR-TSG
model can use large tree fragments, which strength-
ens the probability of frequent syntax patterns in
the training set. Indeed, the few very large rules of
our model memorized full parse trees of sentences,
which were repeated in the training set.
The SR-TSG (single) is a pure generative model
of syntax trees but it achieved results comparable to
those of discriminative parsers. It should be noted
that discriminative reranking parsers such as (Char-
niak and Johnson, 2005) and (Huang, 2008) are con-
structed on a generative parser. The reranking parser
takes the k-best lists of candidate trees or a packed
forest produced by a baseline parser (usually a gen-
erative model), and then reranks the candidates us-
ing arbitrary features. Hence, we can expect that
combining our SR-TSG model with a discriminative
reranking parser would provide better performance
than SR-TSG alone.
Recently, (Petrov, 2010) has reported that com-
bining multiple grammars trained independently
gives significantly improved performance over a sin-
gle grammar alone. We applied his method (referred
to as a TREE-LEVEL inference) to the SR-TSG
model as follows. We first trained sixteen SR-TSG
models independently and produced a 100-best list
of the derivations for each model. Then, we erased
the subcategory information of parse trees and se-
lected the best tree that achieved the highest likeli-
hood under the product of sixteen models. The com-
bination model, SR-TSG (multiple), achieved an F1
score of 92.4%, which is a state-of-the-art result for
the WSJ parsing task. Compared with discriminative
reranking parsers, combining multiple grammars by
using the product model provides the advantage that
it does not require any additional training. Several
studies (Fossum and Knight, 2009; Zhang et al,
2009) have proposed different approaches that in-
volve combining k-best lists of candidate trees. We
will deal with those methods in future work.
Let us note the relation between SR-CFG, TSG
and SR-TSG. TSG is weakly equivalent to CFG and
generates the same set of strings. For example, the
TSG rule ?S ? (NP NNP) VP? with probability p
can be converted to the equivalent CFG rules as fol-
lows: ?S ? NPNNP VP ? with probability p and
?NPNNP ? NNP? with probability 1. From this
viewpoint, TSG utilizes surrounding symbols (NNP
of NPNNP in the above example) as latent variables
with which to capture context information. The
search space of learning a TSG given a parse tree
is O (2n) where n is the number of internal nodes
of the parse tree. On the other hand, an SR-CFG
utilizes an arbitrary index such as 0, 1, . . . as latent
variables and the search space is larger than that of a
TSG when the symbol refinement model allows for
more than two subcategories for each symbol. Our
experimental results comfirm that jointly modeling
both latent variables using our SR-TSG assists accu-
rate parsing.
6 Conclusion
We have presented an SR-TSG, which is an exten-
sion of the conventional TSG model where each
symbol of tree fragments can be automatically sub-
categorized to address the problem of the condi-
tional independence assumptions of a TSG. We pro-
posed a novel backoff modeling of an SR-TSG
based on the hierarchical Pitman-Yor Process and
sentence-level and tree-level blocked MCMC sam-
pling for training our model. Our best model sig-
nificantly outperformed the conventional TSG and
achieved state-of-the-art result in a WSJ parsing
task. Future work will involve examining the SR-
TSG model for different languages and for unsuper-
vised grammar induction.
Acknowledgements
We would like to thank Liang Huang for helpful
comments and the three anonymous reviewers for
thoughtful suggestions. We would also like to thank
Slav Petrov and Hui Zhang for answering our ques-
tions about their parsers.
447
References
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In In Proc.
of ACL, pages 1098?1107.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP, pages 1204?1213.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL, 1:173?180.
Shay B Cohen, David M Blei, and Noah A Smith. 2010.
Variational Inference for Adaptor Grammars. In In
Proc. of HLT-NAACL, pages 564?572.
Trevor Cohn and Mirella Lapata. 2009. Sentence Com-
pression as Tree Transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing Tree-Substitution Grammars. Journal
of Machine Learning Research, 11:3053?3096.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational Lin-
guistics, 29:589?637.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proc. of
EMNLP, page 727.
Thomas S Ferguson. 1973. A Bayesian Analysis of
Some Nonparametric Problems. Annals of Statistics,
1:209?230.
Victoria Fossum and Kevin Knight. 2009. Combining
Constituent Parsers. In Proc. of HLT-NAACL, pages
253?256.
Michel Galley, Mark Hopkins, Kevin Knight, Daniel
Marcu, Los Angeles, and Marina Del Rey. 2004.
What?s in a Translation Rule? Information Sciences,
pages 273?280.
Liang Huang. 2008. Forest Reranking : Discriminative
Parsing with Non-Local Features. In Proc. of ACL,
19104:0.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In In Proc. of HLT-NAACL, pages 317?325.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007a. Adaptor Grammars : A Frame-
work for Specifying Compositional Nonparametric
Bayesian Models. Advances in Neural Information
Processing Systems 19, 19:641?648.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwa-
ter. 2007b. Bayesian Inference for PCFGs via Markov
chain Monte Carlo. In In Proc. of HLT-NAACL, pages
139?146.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D Manning. 2003. Accurate
Unlexicalized Parsing. In Proc. of ACL, 1:423?430.
K Lari and S J Young. 1991. Applications of Stochas-
tic Context-Free Grammars Using the Inside?Outside
Algorithm. Computer Speech and Language, 5:237?
257.
Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, pages 75?82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of ACL, pages
433?440.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Proc. of HLT-NAACL, pages 19?27.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25:855?900.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In In Proc. of ACL-
IJCNLP, pages 45?48.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
YW Teh. 2006b. A Hierarchical Bayesian Language
Model based on Pitman-Yor Processes. In Proc. of
ACL, 44:985?992.
J Tenenbaum, TJ O?Donnell, and ND Goodman. 2009.
Fragment Grammars: Exploring Computation and
Reuse in Language. MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series.
Mengqiu Wang, Noah A Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model ? A Quasi-
Synchronous Grammar for QA. In Proc. of EMNLP-
CoNLL, pages 22?32.
Elif Yamangil and Stuart M Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In In
Proc. of ACL, pages 937?947.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li.
2009. K-Best Combination of Syntactic Parsers. In
Proc. of EMNLP, pages 1552?1560.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. of EMNLP-CoNLL, pages 551?560.
448
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 383?386,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MSS: Investigating the Effectiveness of Domain Combinations and
Topic Features for Word Sense Disambiguation
Sanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki Shindo
NTT Communication Science Laboratories
{sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jp
Abstract
We participated in the SemEval-2010
Japanese Word Sense Disambiguation
(WSD) task (Task 16) and focused on
the following: (1) investigating domain
differences, (2) incorporating topic fea-
tures, and (3) predicting new unknown
senses. We experimented with Support
Vector Machines (SVM) and Maximum
Entropy (MEM) classifiers. We achieved
80.1% accuracy in our experiments.
1 Introduction
We participated in the SemEval-2010 Japanese
Word Sense Disambiguation (WSD) task (Task 16
(Okumura et al, 2010)), which has two new char-
acteristics: (1) Both training and test data across
3 or 4 domains. The training data include books
or magazines (called PB), newspaper articles (PN),
and white papers (OW). The test data also include
documents from a Q&A site on the WWW (OC);
(2) Test data include new senses (called X) that are
not defined in dictionary.
There is much previous research on WSD. In
the case of Japanese, unsupervised approaches
such as extended Lesk have performed well (Bald-
win et al, 2010), although they are outperformed
by supervised approaches (Tanaka et al, 2007;
Murata et al, 2003). Therefore, we selected a su-
pervised approach and constructed Support Vector
Machines (SVM) and Maximum Entropy (MEM)
classifiers using common features and topic fea-
tures. We performed extensive experiments to in-
vestigate the best combinations of domains for
training.
We describe the data in Section 2, and our sys-
tem in Section 3. Then in Section 4, we show the
results and provide some discussion.
2 Data Description
2.1 Given Data
We show an example of Iwanami Kokugo Jiten
(Nishio et al, 1994), which is a dictionary used as
a sense inventory. As shown in Figure 1, each en-
try has POS information and definition sentences
including example sentences.
We show an example of the given training data
in (1). The given data are morphologically ana-
lyzed and partly tagged with Iwanami?s sense IDs,
such as '37713-0-0-1-1( in (1).
(1) <mor pos='??-?}( rd='??( bfm='?
?( sense= '37713-0-0-1-1( >1<</mor>
This task includes 50 target words that were
split into 219 senses in Iwanami; among them, 143
senses including two Xs that were not defined in
Iwanami, appear in the training data. In the test
data, 150 senses including eight Xs appear. The
training and test data share 135 senses including
two Xs; that is, 15 senses including six Xs in the
test data are unseen in the training data.
2.2 Data Pre-processing
We performed two preliminary pre-processing
steps. First, we restored the base forms because
the given training and test data have no informa-
tion about the base forms. (1) shows an example
of the original morphological data, and then we
added the base form (lemma), as shown in (2).
(2) <mor pos=' ? ?-? } ( rd=' ? ? (
bfm=' ? ? ( sense='37713-0-0-1-1(
lemma='1d(>1<</mor>
Secondly, we extracted example sentences from
Iwanami, which is used as a sense inventory. To
compensate for the lack of training data, we an-
alyzed examples with a morphological analyzer,
Mecab1 UniDic version, because the training and
test data were tagged with POS based on UniDic.
1http://mecab.sourceforge.net/
383
??
?
?
?
HEADWORD Ad91d[ddNd:take (?; Transitive Verb)
37713-0-0-1-0
[
<1> ??<8[GCBk3D?= to get something left into one?s hand
]
37713-0-0-1-1
[
<y> 3??=53k-<??(6
take and hold by hand. 'to lead someone by the hand(
]
?
?
?
?
?
Figure 1: Simplified Entry for Iwanami Kokugo Jiten: Ad take
For example, from the entry for Ad take, as
shown in Figure 1, we extracted an example sen-
tence and morphologically analyzed it, as shown
in (3)2, for the second sense, 37713-0-0-1-1. In
(3), the underlined part is the headword and is
tagged with 37713-0-0-1-1.
(3) 3
hand
k
ACC
1<
take
?
and
?(
lead
?(I) take someone?s hand and lead him/her?
3 System Description
3.1 Features
In this section, we describe the features we gener-
ated.
3.1.1 Baseline Features
For each target word w, we used the surface form,
the base form, the POS tag, and the top POS cat-
egories, such as nouns, verbs, and adjectives of
w. Here the target is the ith word, so we also
used the same information of i? 2, i? 1, i+ 1, and
i+2th words. We used bigrams, trigrams, and skip-
bigrams back and forth within three words. We re-
fer to the model that uses these baseline features
as bl.
3.1.2 Bag-of-Words
For each target word w, we got all base forms of
the content words within the same document or
within the same article for newspapers (PN). We
refer to the model that uses these baseline features
as bow.
3.1.3 Topic Features
In the SemEval-2007 English WSD tasks, a sys-
tem incorporating topic features achieved the
highest accuracy (Cai et al, 2007). Inspired by
(Cai et al, 2007), we also used topic features.
Their approach uses Bayesian topic models (La-
tent Dirichlet Allocation: LDA) to infer topics in
an unsupervised fashion. Then the inferred topics
2We use ACC as an abbreviation of accusative
postposition.
are added as features to reduce the sparsity prob-
lem with word-only features.
In our proposed approach, we use the inferred
topics to find 'related?( words and directly add
these word counts to the bag-of-words representa-
tion.
We applied gibbslda++3 to the training and test
data to obtain multiple topic classification per doc-
ument or article for newspapers (PN). We used the
document or article topics for newspapers (PN) in-
cluding the target word. We refer to the model
that uses these topic features as tpX, where X is
the number of topics and tpdistX with the topics
weighted by distributions. In particular, the topic
distribution of each document/article is inferred by
the LDA topic model using standard Gibbs sam-
pling.
We also add the most typical words in the topic
as a bag-of-words. For example, one topic might
include ? city, ?? Tokyo, ? train line, ? ward
and so on. A second topic might include ?? dis-
section, ? after, ?? medicine, U grave and so
on. If a document is inferred to contain the first
topic, then the words (? city, ?? Tokyo, ? train
line, ...) are added to the bag-of-words feature. We
refer to these features as twdY, including the most
typical Y words as bag-of-words.
3.2 Investigation between Domains
In preliminary experiments, we used both SVM4
and MEM (Nigam et al, 1999), with optimization
method L-BFGS (Liu and Nocedal, 1989) to train
the WSD model.
First, we investigated the effect between do-
mains (PN, PB, and OW). For training data, we se-
lected words that occur in more than 50 sentences,
separated the training data by domain, and tested
different domain combinations.
Table 1 shows the SVM results of the domain
combinations. For Table 1, we did a 5-fold cross
validation for the self domain and for comparison
3http://gibbslda.sourceforge.net/
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
384
Table 1: Investigation of Domain Combinations
on Training data (features: bl + bow, SVM)
Target Words 77, No. of Instances > 50
Domain Acc.(%) Diff. Comment
PN 78.7 - 63 words,
PN +OW 79.25 0.55 1094 instances
PN +PB 79.43 0.73
PN +ALL 79.34 0.64
PB 79.29 - 75 words,
PB +PN 78.85 -0.45 2463 instances
PB +OW 78.56 -0.73
PB +ALL 78.4 -0.89
OW 87.91 - 42 words,
OW +PN 89.05 1.14 703 instances
OW +PB 88.34 0.43
OW +ALL 89.05 1.14
with the results after adding the other domain data.
In Table 1, Diff. shows the differences to the self
domain.
As shown in Table 1, for PN and OW, using other
domains improved the results, but for PB, other do-
mains degraded the results. So we decided to se-
lect the domains for each target word.
In the formal run, for each pair of domain and
target words, we selected the combination of do-
main and dictionary examples that got the best
cross-validation result in the training data. Note
that in the case of no training data for the test data
domain, for example, since no OCs have training
data, we used all training data and dictionary ex-
amples.
We show the number of selected domain combi-
nations for each target domain in Table 2. Because
the distribution of target words is very unbalanced
in domains, not all types of target words appear in
every domain, as shown in Table 2.
3.3 Method for Predicting New Senses
We also tried to predict new senses (X) that didn?t
appear in the training data by calculating the en-
tropy for each target given in the MEM. We as-
sumed that high entropy (when the probabilities
of classes are uniformly dispersed) was indicative
of X; i.e., if [entropy > threshold] => predict X;
else => predict with MEM?s output sense tag.
Note that we used the words that were tagged
with Xs in the training data, except for the target
words. We compared the entropies of X and not
X of the words and heuristically tuned the thresh-
old based on the differences among entropies. Our
three official submissions correspond to different
thresholds.
Table 2: Used Domain Combinations
Used MEM SVM
Domain No. (%) No. (%)
Target: PB (48 types of target words)
ALL +EX 26 54.2 23 47.9
ALL 4 8.3 6 12.5
PB 11 22.9 8 16.7
PB +EX 1 2.1 1 2.1
PB +OW 1 2.1 3 6.3
PB +PN 5 10.4 7 14.6
Target: PN (46 types of target words)
ALL +EX 30 65.2 30 65.2
ALL 4 8.7 4 8.7
PN 4 8.7 1 2.2
PN +EX 0 0 1 2.2
PN +OW 2 4.3 2 4.3
PN +PB 6 13 8 17.4
Target: OW (16 types of target words)
ALL +EX 5 31.3 5 31.3
ALL 2 12.5 1 6.3
OW 6 37.5 3 18.8
OW +PB 3 18.8 3 18.8
OW +PN 0 0 4 25.0
Target: OC (46 types of target words)
ALL +EX 46 100 46 100
4 Results and Discussions
Our cross-validation experiments on the training
set showed that selecting data by domain combi-
nations works well, but unfortunately this failed
to achieve optimal results on the formal run. In
this section, we show the results using all of the
training data with no domain selections (also after
fixing some bugs).
Table 3 shows the results for the combination
of features on the test data. MEM greatly outper-
formed SVM. Its effective features are also quite
different. In the case of MEM, baseline features
(bl) almost gave the best result, and the topic fea-
tures improved the accuracy, especially when di-
vided into 200 topics. But for SVM, the topic
features are not so effective, and the bag-of-words
features improved accuracy.
For MEM with bl +tp200, which produced the
best result, the following are the best words: ?
outside (accuracy is 100%), C^ economy (98%),
?!d think (98%), d& big (98%), and %Z
culture (98%). On the other hand, the following
are the worst words: 1d take (36%), ? good
(48%), ?+d raise (48%), w2 put out (50%),
and ?= stand up (54%).
In Table 4, we show the results for each POS (bl
+tp200, MEM). The results for the verbs are com-
parably lower than the others. In future work, we
will consider adding syntactic features that may
improve the results.
385
Table 3: Comparisons among Features and Test data
TYPE Precision (%)
MEM SVM Explain
Base Line 68.96 68.96 Most Frequent Sense
bl 79.3 69.6 Base Line Features
bl +bow 77.0 70.8 + Bag-of-Words (BOW)
bl +bow +tp100 76.4 70.7 +BOW + Topics (100)
bl +bow +tp200 77.0 70.7 +BOW + Topics (200)
bl +bow +tp300 77.4 70.7 +BOW + Topics (300)
bl +bow +tp400 76.8 70.7 +BOW + Topics (400)
bl +bow +tpdist300 77.0 70.8 +BOW + Topics (300)*distribution
bl +bow +tp300 +twd100 76.2 70.8 + Topics (300) with 100 topic words
bl +bow +tp300 +twd200 76.0 70.8 + Topics (300) with 200 topic words
bl +bow +tp300 +twd300 75.9 70.8 + Topics (300) with 300 topic words
without bow
bl +tp100 79.3 69.6 + Topics (100)
bl +tp200 80.1 69.6 + Topics (200)
bl +tp300 79.6 69.6 + Topics (300)
bl +tp400 79.6 69.6 + Topics (400)
bl +tpdist100 79.3 69.6 + Topics (100)*distribution
bl +tpdist200 79.3 69.6 + Topics (200)*distribution
bl +tpdist300 79.3 69.6 + Topics (300)*distribution
bl +tp200 +twd100 74.6 69.6 + Topics (200) with 100 topic words
bl +tp300 +twd10 74.4 69.4 + Topics (300) with 10 topic words
bl +tp300 +twd20 75.2 69.3 + Topics (300) with 20 topic words
bl +tp300 +twd50 74.8 69.2 + Topics (300) with 50 topic words
bl +tp300 +twd200 74.6 69.6 + Topics (300) with 200 topic words
bl +tp300 +twd300 75.0 69.6 + Topics (300) with 300 topic words
bl +tp400 +twd100 74.1 69.6 + Topics (400) with 100 topic words
bl+tpdist100 +twd20 79.3 69.6 + Topics (100)*distribution with 20 topic words
bl+tpdist200 +twd20 79.3 69.6 + Topics (200)*distribution with 20 topic words
bl+tpdist400 +twd20 79.3 69.6 + Topics (400)*distribution with 20 topic words
Table 4: Results for each POS (bl +tp200, MEM)
POS No. of Types Acc. (%)
Nouns 22 85.5
Adjectives 5 79.2
Transitive Verbs 15 76.9
Intransitive Verbs 8 71.8
Total 50 80.1
In the formal run, we selected training data
for each pair of domain and target words and
used entropy to predict new unknown senses. Al-
though these two methods worked well in our
cross-validation experiments, they did not perform
well for the test data, probably due to domain mis-
match.
Finally, we also experimented with SVM and
MEM, and MEM gave better results.
References
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2010. A Re-
examination of MRD-based Word Sense Disambiguation.
Transactions on Asian Language Information Process, As-
sociation for Computing Machinery (ACM), 9(4):1?21.
Jun Fu Cai, Wee Sun Lee, and YW Teh. 2007. Improv-
ing Word Sense Disambiguation using Topic Features. In
Proceedings of EMNLP-CoNLL-2007, pp. 1015?1023.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited Mem-
ory BFGS Method for Large Scale Optimization. Math.
Programming, 45(3, (Ser. B)):503?528.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and Hitoshi Isahara. 2003. CRL at Japanese
dictionary-based task of SENSEVAL-2. Journal of Nat-
ural Language Processing, 10(3):115?143. (in Japanese).
Kamal Nigam, John Lafferty, and Andrew McCallum. 1999.
Using Maximum Entropy for Text Classification. In
IJCAI-99 Workshop on Machine Learning for Information
Filtering, pp. 61?67.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 Task: Japanese
WSD. In SemEval-2: Evaluation Exercises on Semantic
Evaluation.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fu-
jita, and Chikara Hashimoto. 2007. Word Sense Disam-
biguation Incorporating Lexical and Structural Semantic
Information. In Proceedings of EMNLP-CoNLL-2007, pp.
477?485.
386
