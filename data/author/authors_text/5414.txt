Web Corpus Mining by instance of Wikipedia
Ru?diger Gleim, Alexander Mehler & Matthias Dehmer
Bielefeld University, D-33615 Bielefeld, Germany
Ruediger.Gleim@uni-bielefeld.de
Alexander.Mehler@uni-bielefeld.de
Technische Universita?t Darmstadt, Fachbereich Informatik
dehmer@tk.informatik.tu-darmstadt.de
Abstract
In this paper we present an approach to
structure learning in the area of web doc-
uments. This is done in order to approach
the goal of webgenre tagging in the area of
web corpus linguistics. A central outcome
of the paper is that purely structure ori-
ented approaches to web document classi-
fication provide an information gain which
may be utilized in combined approaches of
web content and structure analysis.
1 Introduction
In order to reliably judge the collocative affinity of
linguistic items, it has to be considered that judge-
ments of this kind depend on the scope of certain
genres or registers. According to Stubbs (2001),
words may have different collocates in different
text types or genres and therefore may signal one
of those genres when being observed. Conse-
quently, corpus analysis requires, amongst oth-
ers, a comparison of occurrences in a given text
with typical occurrences in other texts of the same
genre (Stubbs, 2001, p. 120).
This raises the question how to judge the mem-
bership of texts, in which occurrences of linguistic
items are observed, to the genres involved. Evi-
dently, because of the size of the corpora involved,
this question is only adequately answered by ref-
erence to the area of automatic classification. This
holds all the more for web corpus linguistics (Kil-
garriff and Grefenstette, 2003; Baroni and Bernar-
dini, 2006) where large corpora of web pages,
whose membership in webgenres is presently un-
known, have to be analyzed. Consequently, web
corpus linguistics faces two related task:
1. Exploration: The task of initially exploring
which webgenres actually exist.
2. Categorization: The task of categorizing hy-
pertextual units according to their member-
ship in the genres being explored in the latter
step.
In summary, web corpus linguistics is in need
of webgenre-sensitive corpora, that is, of corpora
in which for the textual units being incorporated
the membership to webgenres is annotated. This
in turn presupposes that these webgenres are first
of all explored.
Currently to major classes of approaches can
be distinguished: On the one hand, we find ap-
proaches to the categorization of macro structures
(Amitay et al, 2003) such as web hierarchies, di-
rectories and corporate sites. On the other hand,
this concerns the categorization of micro struc-
tures as, for example, single web pages (Klein-
berg, 1999) or even page segments (Mizuuchi and
Tajima, 1999). The basic idea of all these ap-
proaches is to perform categorization as a kind of
function learning for mapping web units above, on
or below the level of single pages onto at most
one predefined category (e.g. genre label) per unit
(Chakrabarti et al, 1998). Thus, these approaches
focus on the categorization task while disregarding
the exploration task. More specifically, the ma-
jority of these approaches utilizes text categoriza-
tion methods in conjunction with HTML markup,
metatags and link structure beyond bag-of-word
representations of the pages? wording as input of
feature selection (Yang et al, 2002) ? in some
cases also of linked pages (Fu?rnkranz, 1999).
What these approaches are missing is a more
general account of web document structure as a
source of genre-oriented categorization. That is,
67
they solely map web units onto feature vectors by
disregarding their structure. This includes linkage
beyond pairwise linking as well as document inter-
nal structures according to the Document Object
Model (DOM). A central pitfall of this approach is
that it disregards the impact of genre membership
to document structure and, thus, the signalling of
the former by the latter (Ventola, 1987). Therefore
a structure-sensitive approach is needed in the area
of corpus linguistics which allows for automatic
webgenre tagging. That is, an approach which
takes both levels of structuring of web documents
into account: On the level of their hyperlink-based
linkage and on the level of their internal structure.
In this paper we present an algorithm as a
preliminary step for tackling the exploration and
categorization task together. More specifically,
we present an approach to unsupervised structure
learning which uses tree alignment algorithms as
similarity kernels and cluster analysis for class de-
tection. The paper includes a comparative study of
several approaches to tree alignment as a source of
similarity measuring of web documents. Its cen-
tral topics are:
? To what extent is it possible to predict the
membership of a web document in a certain
genre (or register) solely on grounds of its
structure when its lexical content and other
content bearing units are completely deleted?
In other words, we ask to what extent struc-
ture signals membership in genre.
? A more methodical question regards the
choice of appropriate measures of structural
similarity to be included into structure learn-
ing. In this context, we comparatively study
several variants of measuring similarities of
trees, that is, tree edit distance as well as a
class of algorithms which are based on tree
linearizations as input to sequence alignment.
Our overall findings hint at two critical points:
First, there is a significant contribution of
structure-oriented methods to webgenre catego-
rization which is unexplored in predominant ap-
proaches. Second, and most surprisingly, all
methods analyzed toughly compete with a method
based on random linearization of input documents.
Why is this research important for web corpus
linguistics? An answer to this question can be out-
lined as follows:
? We explore a further resource of reliably tag-
ging web genres and registers, respectively,
in the form of document structure.
? We further develop the notion of webgenre
and thus help to make document structure ac-
cessible to collocation and other corpus lin-
guistic analyses.
In order to support this argumentation, we first
present a structure insensitive approach to web cat-
egorization in section (2). It shows that this in-
sensitivity systematically leads to multiple cate-
gorizations which cannot be traced back to ambi-
guity of category assignment. In order to solve
this problem, an alternative approach to structure
learning is presented in sections (3.1), (3.2) and
(3.3). This approach is evaluated in section (3.4)
on grounds of a corpus of Wikipedia articles. The
reason for utilizing this test corpus is that the
content-based categories which the explored web
documents belong to are known so that we can ap-
ply the classical apparatus of evaluation of web
mining. The final section concludes and prospects
future work.
2 Hypertext Categorization
The basic assumption behind present day ap-
proaches to hypertext categorization is as follows:
Web units of similar function/content tend to have
similar structures. The central problem is that
these structures are not directly accessible by seg-
menting and categorizing single web pages. This
is due to polymorphism and its reversal relation
of discontinuous manifestation: Generally speak-
ing, polymorphism occurs if the same (hyper-
)textual unit manifests several categories. This
one-to-many relation of expression and content
units is accompanied by a reversal relation accord-
ing to which the same content or function unit
is distributed over several expression units. This
combines to a many-to-many relation between
explicit, manifesting web structure and implicit,
manifested functional or content-based structure.
Our hypothesis is that if polymorphism is a
prevalent characteristic of web units, web pages
cannot serve as input of categorization since poly-
morphic pages simultaneously instantiate several
categories. Moreover, these multiple categoriza-
tions are not simply resolved by segmenting the
focal pages, since they possibly manifest cate-
gories only discontinuously so that their features
68
do not provide a sufficient discriminatory power.
In other words: We expect polymorphism and dis-
continuous manifestation to be accompanied by
many multiple categorizations without being re-
ducible to the problem of disambiguating category
assignments. In order to show this, we perform
a categorization experiment according to the clas-
sical setting of function learning, using a corpus
of the genre of conference websites. Since these
websites serve recurrent functions (e.g. paper sub-
mission, registration etc.) they are expected to be
structured homogeneously on the basis of stable,
recurrent patterns. Thus, they can be seen as good
candidates of categorization.
The experiment is performed as follows: We
apply support vector machine (SVM) classifica-
tion which proves to be successful in case of
sparse, high dimensional and noisy feature vec-
tors (Joachims, 2002). SVM classification is per-
formed with the help of the LibSVM (Hsu et al,
2003). We use a corpus of 1,078 English con-
ference websites and 28,801 web pages. Hyper-
text representation is done by means of a bag-
of-features approach using about 85,000 lexical
and 200 HTML features. This representation was
done with the help of the HyGraph system which
explores websites and maps them onto hypertext
graphs (Mehler and Gleim, 2005). Following (Hsu
et al, 2003), we use a Radial Basis Function ker-
nel and make optimal parameter selection based
on a minimization of a 5-fold cross validation er-
ror. Further, we perform a binary categorization
for each of the 16 categories based on 16 training
sets of pos./neg. examples (see table 1). The size
of the training set is 1,858 pages (284 sites); the
size of the test set is 200 (82 sites). We perform 3
experiments:
1. Experiment A ? one against all: First we ap-
ply a one against all strategy, that is, we use
X \ Yi as the set of negative examples for
learning category Ci where X is the set of all
training examples and Yi is the set of positive
examples of Ci. The results are listed in table
(1). It shows the expected low level of effec-
tivity: recall and precession perform very low
on average. In three cases the classifiers fail
completely. This result is confirmed when
looking at column A of table (2): It shows
the number of pages with up to 7 category
assignments. In the majority of cases no cat-
egory could be applied at all ? only one-third
Category rec. prec.
Abstract(s) 0.2 1.0
Accepted Papers 0.3 1.0
Call for Papers 0.1 1.0
Committees 0.5 0.8
Contact Information 0 0
Exhibition 0.4 1.0
Important Dates 0.8 1.0
Invited Talks 0 0
Menu 0.7 0.7
Photo Gallery 0 0
Program, Schedule 0.8 1.0
Registration 0.9 1.0
Sections, Sessions, Plenary etc. 0.1 0.3
Sponsors and Partners 0 0
Submission Guidelines etc. 0.5 0.8
Venue, Travel, Accommodation 0.9 1.0
Table 1: The categories of the conference website
genre applied in the experiment.
of the pages was categorized.
2. Experiment B ? lowering the discriminatory
power: In order to augment the number of
categorizations, we lowered the categories?
selectivity by restricting the number of neg-
ative examples per category to the number
of the corresponding positive examples by
sampling the negative examples according to
the sizes of the training sets of the remain-
ing categories. The results are shown in ta-
ble (2): The number of zero categorizations
is dramatically reduced, but at the same time
the number of pages mapped onto more than
one category increases dramatically. There
are even more than 1,000 pages which are
mapped onto more than 5 categories.
3. Experiment C ? segment level categorization:
Thirdly, we apply the classifiers trained on
the monomorphic training pages on segments
derived as follows: Pages are segmented into
spans of at least 30 tokens reflecting segment
borders according to the third level of the
pages? document object model trees. Col-
umn C of table (2) shows that this scenario
does not solve the problem of multiple cate-
gorizations since it falls back to the problem
of zero categorizations. Thus, polymorphism
is not resolved by simply segmenting pages,
as other segmentations along the same line of
constraints confirmed.
There are competing interpretations of these re-
sults: The category set may be judged to be wrong.
But it reflects the most differentiated set applied
so far in this area. Next, the representation model
69
number of ca- A B C
tegorizations page level page level segment level
0 12,403 346 27,148
1 6,368 2387 9,354
2 160 5076 137
3 6 5258 1
4 0 3417 0
5 0 923 0
6 0 1346 0
7 0 184 0
Table 2: The number of pages mapped onto
0, 1, ..., 7 categories in experiment A, B and C.
may be judged to be wrong, but actually it is usu-
ally applied in text categorization. Third, the cate-
gorization method may be seen to be ineffective,
but SVMs are known to be one of the most ef-
fective methods in this area. Further, the clas-
sifiers may be judged to be wrong ? of course
the training set could be enlarged, but already in-
cludes about 2,000 manually selected monomor-
phic training units. Finally, the focal units (i.e.
web pages) may be judged to be unsystematically
polymorph in the sense of manifesting several log-
ical units. It is this interpretation which we believe
to be supported by the experiment.
If this interpretation is true, the structure of
web documents comes into focus. This raises
the question, what can be gained at all when ex-
ploring the visible structuring of documents as
found on the web. That is, what is the infor-
mation gain when categorizing documents solely
based on their structures. In order to approach this
question we perform an experiment in structure-
oriented classification in the next section. As
we need to control the negative impact of poly-
morphism, we concentrate on monomorphic pages
which uniquely belong to single categories. This
can be guaranteed with the help of Wikipedia arti-
cles which, with the exception of special disam-
biguation pages, only address one topic respec-
tively.
3 Structure-Based Categorization
3.1 Motivation
In this section we investigate how far a corpus of
documents can be categorized by solely consid-
ering the explicit document structure without any
textual content. It is obvious that we cannot ex-
pect the results to reach the performance of con-
tent based approaches. But if this approach allows
to significantly distinguish between categories in
contrast to a reference random decider we can con-
clude that the involvement of structure informa-
tion may positively affect categorization perfor-
mance. A positive evaluation can be seen to mo-
tivate an implementation of the Logical Document
Structure (LDS) algorithm proposed by Mehler et
al. (2005) who include graph similarity measuring
as its kernel. We expect the same experiment to
perform significantly better on the LDS instead of
the explicit structures. However this experiment
can only be seen as a first attempt. Further studies
with larger corpora are required.
3.2 Experiment setup
In our experiment, we chose a corpus of articles
from the German Wikipedia addressing the fol-
lowing categories:
? American Presidents (41 pages)
? European Countries (50 pages)
? German Cities (78 pages)
? German Universities (93 pages)
With the exception of the first category most ar-
ticles, being represented as a HTML web page,
share a typical, though not deterministic visible
structure. For example a Wikipedia article about a
city contains an info box to the upper right which
lists some general information like district, pop-
ulation and geographic location. Furthermore an
article about a city contains three or more sections
which address the history, politics, economics
and possibly famous buildings or persons. Like-
wise there exist certain design guidelines by the
Wikipedia project to write articles about countries
and universities. However these guidelines are not
always followed or they are adapted from one case
to another. Therefore, a categorization cannot rely
on definite markers in the content. Nevertheless,
the expectation is that a human reader, once he has
seen a few samples of each category, can with high
probability guess the category of an article by sim-
ple looking at the layout or visible structure and ig-
noring the written content. Since the layout (esp.
the structure) of a web page is encoded in HTML
we consider the structure of their DOM1-trees for
our categorization experiment. If two articles of
the same category share a common visible struc-
ture, this should lead to a significant similarity of
1Document Object Model.
70
the DOM-trees. The articles of category ?Ameri-
can Presidents? form an exception to this principle
up to now because they do not have a typical struc-
ture. The articles about the first presidents are rel-
atively short whereas the articles about the recent
presidents are much more structured and complex.
We include this category to test how well a struc-
ture based categorizer performs on such diverse
structurations. We examine two corpus variants:
I. All HTML-Tags of a DOM-tree are used for
similarity measurement.
II. Only those HTML-tags of a DOM-tree are
used which have an impact on the visible
structure (i.e. inline tags like font or i are ig-
nored).
Both cases, I and II, do not include any text
nodes. That is, all lexical content is ignored. By
distinguishing these two variants we can examine
what impact these different degrees of expressive-
ness have on the categorization performance.
3.3 Distance measurement and clustering
The next step of the experiment is marked by a
pairwise similarity measurement of the wikipedia
articles which are represented by their DOM-trees
according to the two variants described in section
3.2. This allows to create a distance matrix which
represents the (symmetric) distances of a given ar-
ticle to any other. In a subsequent and final step
the distance matrix will be clustered and the re-
sults analyzed.
How to measure the similarity of two DOM-
trees? This raises the question what exactly the
subject of the measurement is and how it can be
adequately modeled. Since the DOM is a tree and
the order of the HTML-tags matters, we choose
ordered trees. Furthermore we want to represent
what tag a node represents. This leads to ordered
labeled trees for representation. Since trees are
a common structure in various areas such as im-
age analysis, compiler optimization and bio infor-
matics (i.e. RNA analysis) there is a high inter-
est in methods to measure the similarity between
trees (Tai, 1979; Zhang and Shasha, 1989; Klein,
1998; Chen, 2001; Ho?chsmann et al, 2003). One
of the first approaches with a reasonable compu-
tational complexity was introduced by Tai (1979)
who extended the problem of sequence edit dis-
tance to trees.
T2T1
Post-order linearization and alignment:
T2
S
T1
S
HTML
HEAD
TITLE H1 P
HTML
HEAD
TITLE H1
TITLE HEAD H1 P HTML
TITLE HEAD H1 HTML<gap>
BODY
BODY
BODY BODY
Figure 1: An example for Post-order linearization
and sequence alignment.
The following description of tree edit distances
is due to Bille (2003): The principle to compute
the edit distance between two trees T
1
, T
2
is to
successively perform elementary edit operations
on the former tree to turn it into the formation of
the latter. The edit operations on a given tree T
are as follows: Relabel changes the label of a node
v ? T . Delete deletes a non-root node v ? T with
a parent node w ? T . Since v is being deleted,
its child nodes (if any) are inserted as children of
node w. Finally the Insert operation marks the
complement of delete. Next, an edit script S is
a list of consecutive edit operations which turn T
1
into T
2
. Given a cost function for each edit opera-
tion the cost of S is the sum of its elementary op-
eration costs. The optimal edit script (there is pos-
sibly more than one) between T
1
and T
2
is given
by the edit script of minimum cost which equals
the tree edit distance.
There are various algorithms known to com-
pute the edit distance (Tai, 1979; Zhang and
Shasha, 1989; Klein, 1998; Chen, 2001). They
vary in computational complexity and whether
they can be used for general purpose or un-
der special restrictions only (which allows for
better optimization). In this experiment we
use the general-purpose algorithm of Zhang
and Shasha (1989) which shows a complexity
of O(|T
1
||T
2
|min(L
1
,D
1
)min(L
2
,D
2
)) where
|Ti|, Li, Di denote the number of nodes, the num-
ber of leafs and the depth of the trees respectively.
The approach of tree edit distance forms a good
balance between accurate distance measurement
of trees and computational complexity. However,
especially for large corpora it might be useful
to examine how well other (i.e. faster) methods
71
perform. We therefore consider another class of
algorithms for distance measurement which are
based on sequence alignments via dynamic pro-
gramming. Since this approach is restricted to the
comparison of sequences, a suitable linearization
of the DOM trees has to be found. For this task we
use several strategies of tree node traversal: Pre-
Order, Post-Order and Breath-First-Search (BFS)
traversal. Figure (1) shows a linearization of two
sample trees using Post-Order and how the result-
ing sequences STi may have been aligned for the
best alignment distance. We have enhanced the
labels of the linearized nodes by adding the in-
and out degrees corresponding to the former po-
sition of the nodes in the tree. This information
can be used during the computation of the align-
ment cost: An example of this is that the align-
ment of two nodes with identical HTML-tags but
different in/out degrees will result in a higher cost
than in cases where these degrees match. Follow-
ing this strategy, at least part of the structure in-
formation is preserved. This approach is followed
by Dehmer (2005) who develops a special form of
tree linearization which is based on tree levels.
Obviously, a linearization poses a loss of struc-
ture information which has impact on the results
of distance measurement. But the computational
complexity of sequence alignments (O(n2)) is sig-
nificantly better than of tree edit distances. This
leads to a trade-off between the expressiveness of
the DOM-Tree representation (in our case tree vs.
linearization to a sequence) and the complexity of
the algorithms to compute the distance thereon.
In order to have a baseline for tree linearization
techniques we have also tested random lineariza-
tions. According to this method, trees are trans-
formed into sequences of nodes in random order.
For our experiment we have generated 16 random
linearizations and computed the median of their
categorization performances.
Next, we perform pairwise distance measure-
ments of the DOM-trees using the set of algo-
rithms described above. We then apply two clus-
tering methods on the resulting distance matrices:
hierarchical agglomerative and k-means cluster-
ing. Hierarchical agglomerative clustering does
not need any information on the expected number
of clusters so we examine all possible clusterings
and chose the one maximizing the F -measure.
However we also examine how well hierarchical
clustering performs if the number of partitions is
restricted to the number of categories. In contrast
to the previous approach, k-means needs to be in-
formed about the number of clusters in advance,
which in the present experiment equals the num-
ber of categories, which in our case is four. Be-
cause we know the category of each article we can
perform an exhaustive parameter study to maxi-
mize the well known efficiency measures purity,
inverse purity and the combined F -measure.
3.4 Results and discussion
The tables (3) and (5) show the results for cor-
pus variant I (using all HTML-tags) and variant
II (using structure relevant HTML-tags only) (see
section 3.2). The general picture is that hierarchi-
cal clustering performs significantly better than k-
means. However this is only the case for an un-
restricted number of clusters. If we restrict the
number of clusters for hierarchical clustering to
the number of categories, the differences become
much less apparent (see tables 4 and 6). The only
exception to this is marked by the tree edit dis-
tance: The best F -measure of 0.863 is achieved
by using 58 clusters. If we restrict the number of
clusters to 4, tree edit still reaches an F -measure
of 0.710 which is significantly higher than the best
k-means result of 0.599.
As one would intuitively expect the results
achieved by the tree edit distance are much bet-
ter than the variants of tree linearization. The edit
distance operates on trees whereas the other algo-
rithms are bound to less informative sequences.
Interestingly, the differences become much less
apparent for the corpus variant II which consists
of the simplified DOM-trees (see section 3.2). We
can assume that the advantage of the tree edit dis-
tance over the linearization-based approaches di-
minishes, the smaller the trees to be compared are.
The performance of the different variants of tree
linearization vary only significantly in the case of
unrestricted hierarchical clustering (see tables 3
and 5). In the case of k-means as well as in the
case of restricting hierarchical clustering to ex-
actly 4 clusters, the performances are about equal.
In order to provide a baseline for better rating
the cluster results, we perform random clustering.
This leads to an F -measure of 0.311 (averaged
over 1,000 runs). Content-based categorization
experiments using the bag of words model have
reported F -measures of about 0.86 (Yang, 1999).
The baseline for the different variants of lin-
72
Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specifical
tree edit distance hierarchical 58 0.863 0.996 0.786 none weighted linkage
post-order linearization hierarchical 13 0.775 0.809 0.775 spearman single linkage
pre-order linearization hierarchical 19 0.741 0.817 0.706 spearman single linkage
tree level linearization hierarchical 36 0.702 0.882 0.603 spearman single linkage
bfs linearization hierarchical 13 0.696 0.698 0.786 spearman single linkage
tree edit distance k-means 4 0.599 0.618 0.641 - cosine distance
pre-order linearization k-means 4 0.595 0.615 0.649 - cosine distance
post-order linearization k-means 4 0.593 0.615 0.656 - cosine distance
tree level linearization k-means 4 0.593 0.603 0.649 - cosine distance
random lin. (medians only) - - 0.591 0.563 0.795 - -
bfs linearization k-means 4 0.580 0.595 0.656 - cosine distance
- random 4 0.311 0.362 0.312 - -
Table 3: Evaluation results using all tags.
Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specifical
tree edit distance hierarchical 4 0.710 0.698 0.851 spearman single linkage
bfs linearization hierarchical 4 0.599 0.565 0.851 none weighted linkage
tree level linearization hierarchical 4 0.597 0.615 0.676 spearman complete linkage
post-order linearization hierarchical 4 0.595 0.615 0.683 spearman average linkage
pre-order linearization hierarchical 4 0.578 0.599 0.660 cosine average linkage
Table 4: Evaluation results using all tags and hierarchical clustering with a fixed number of clusters.
earization is given by random linearizations: We
perform 16 random linearizations, run the differ-
ent variants of distance measurement as well as
clustering and compute the median of the best F -
measure values achieved. These are 0.591 for cor-
pus variant I and 0.581 for the simplified vari-
ant II. These results are in fact surprising because
they are only little worse than the other lineariza-
tion techniques. This result may indicate that ?
in the present scenario ? the linearization based
approaches to tree distance measurement are not
suitable because of the loss of structure informa-
tion. More specifically, this raises the following
antithesis: Either, the sequence-oriented models
of measuring structural similarities taken into ac-
count are insensitive to the structuring of web doc-
uments. Or: this structuring only counts what
regards the degrees of nodes and their labels ir-
respective of their order. As tree-oriented meth-
ods perform better, we view this to be an argu-
ment against linearization oriented methods, at
least what regards the present evaluation scenario
to which only DOM trees are input but not more
general graph structures.
The experiment has shown that analyzing the
document structure provides a remarkable amount
of information to categorization. It also shows that
the sensitivity of the approaches used in different
contexts needs to be further explored which we
will address in our future research.
4 Conclusion
We presented a cluster-based approach to struc-
ture learning in the area of web documents. This
was done in order to approach the goal of a com-
bined algorithm of webgenre exploration and cat-
egorization. As argued in section (1), such an al-
gorithm is needed in web corpus linguistics for
webgenre tagging as a prerequisite of measuring
genre-sensitive collocations. In order to evaluate
the present approach, we utilized a corpus of wiki-
based articles. The evaluation showed that there is
an information gain when measuring the similar-
ities of web documents irrespective of their lexi-
cal content. This is in the line of the genre model
of systemic functional linguistics (Ventola, 1987)
which prospects an impact of genre membership
on text structure. As the corpus used for evalua-
tion is limited to tree-like structures, this approach
is in need for further development. Future work
will address this task. This regards especially the
classification of graph-like representations of web
documents which take their link structure into ac-
count.
References
Einat Amitay, David Carmel, Adam Darlow, Ronny
Lempel, and Aya Soffer. 2003. The connectivity
sonar: detecting site functionality by structural pat-
terns. In Proc. of the 14th ACM conference on Hy-
pertext and Hypermedia, pages 38?47.
Marco Baroni and Silvia Bernardini, editors. 2006.
WaCky! Working papers on the Web as corpus.
Gedit, Bologna, Italy.
Philip Bille. 2003. Tree edit distance, alignment dis-
tance and inclusion. Technical report TR-2003-23.
Soumen Chakrabarti, Byron Dom, and Piotr Indyk.
73
Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specifical
tree edit distance hierarchical 51 0.756 0.905 0.691 none weighted linkage
pre-order linearization hierarchical 20 0.742 0.809 0.771 spearman single linkage
post-order linearization hierarchical 23 0.732 0.813 0.756 spearman single linkage
tree level linearization hierarchical 2 0.607 0.553 0.878 spearman weighted linkage
bfs linearization hierarchical 4 0.589 0.603 0.641 cosine weighted linkage
tree edit distance k-means 4 0.713 0.718 0.718 - cosine distance
pre-order linearization k-means 4 0.587 0.603 0.634 - cosine distance
tree level linearization k-means 4 0.584 0.603 0.641 - cosine distance
bfs linearization k-means 4 0.583 0.599 0.637 - cosine distance
post-order linearization k-means 4 0.582 0.592 0.630 - cosine distance
random lin. (medians only) - - 0.581 0.584 0.674 - -
- random 4 0.311 0.362 0.312 - -
Table 5: Evaluation results using structure relevant tags only.
Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specifical
tree edit distance hierarchical 4 0.643 0.645 0.793 spearman average linkage
post-order linearization hierarchical 4 0.629 0.634 0.664 spearman average linkage
tree level linearization hierarchical 4 0.607 0.595 0.679 spearman weighted linkage
bfs linearization hierarchical 4 0.589 0.603 0.641 cosine weighted linkage
pre-order linearization hierarchical 4 0.586 0.603 0.660 cosine complete linkage
Table 6: Evaluation results using all tags and hierarchical clustering with a fixed number of clusters.
1998. Enhanced hypertext categorization using hy-
perlinks. In Proceedings of ACM SIGMOD Inter-
national Conference on Management of Data, pages
307?318. ACM.
Weimin Chen. 2001. New algorithm for ordered tree-
to-tree correction problem. Journal of Algorithms,
40(2):135?158.
Matthias Dehmer. 2005. Strukturelle Analyse Web-
basierter Dokumente. Ph.D. thesis, Technische Uni-
versita?t Darmstadt, Fachbereich Informatik.
Johannes Fu?rnkranz. 1999. Exploiting structural infor-
mation for text classification on the WWW. In Pro-
ceedings of the Third International Symposium on
Advances in Intelligent Data Analysis, pages 487?
498, Berlin/New York. Springer.
M. Ho?chsmann, T. To?ller, R. Giegerich, and S. Kurtz.
2003. Local similarity in rna secondary struc-
tures. In Proc. Computational Systems Bioinformat-
ics, pages 159?168.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2003. A practical guide to SVM classification.
Technical report, Department of Computer Science
and Information Technology, National Taiwan Uni-
versity.
Thorsten Joachims. 2002. Learning to classify text
using support vector machines. Kluwer, Boston.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333?347.
P. Klein. 1998. Computing the edit-distance between
unrooted ordered trees. In G. Bilardi, G. F. Italiano,
A. Pietracaprina, and G. Pucci, editors, Proceedings
of the 6th Annual European Symposium, pages 91?
102, Berlin. Springer.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
Alexander Mehler and Ru?diger Gleim. 2005. The net
for the graphs ? towards webgenre representation
for corpus linguistic studies. In Marco Baroni and
Silvia Bernardini, editors, WaCky! Working papers
on the Web as corpus. Gedit, Bologna, Italy.
Alexander Mehler, Ru?diger Gleim, and Matthias
Dehmer. 2005. Towards structure-sensitive hyper-
text categorization. In Proceedings of the 29th An-
nual Conference of the German Classification Soci-
ety, Berlin. Springer.
Yoshiaki Mizuuchi and Keishi Tajima. 1999. Finding
context paths for web pages. In Proceedings of the
10th ACM Conference on Hypertext and Hyperme-
dia, pages 13?22.
Michael Stubbs. 2001. On inference theories and code
theories: Corpus evidence for semantic schemas.
Text, 21(3):437?465.
K. C. Tai. 1979. The tree-to-tree correction problem.
Journal of the ACM, 26(3):422?433.
Eija Ventola. 1987. The Structure of Social Interac-
tion: a Systemic Approach to the Semiotics of Ser-
vice Encounters. Pinter, London.
Yiming Yang, Sean Slattery, and Rayid Ghani. 2002.
A study of approaches to hypertext categorization.
Journal of Intelligent Information Systems, 18(2-
3):219?241.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. Journal of Informa-
tion Retrieval, 1(1/2):67?88.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM Journal of Computing, 18:1245?
1262.
74
Text Linkage in the Wiki Medium ? A Comparative Study
Alexander Mehler
Department of Computational Linguistics & Text Technology
Bielefeld University
Bielefeld, Germany
Alexander.Mehler@uni-bielefeld.de
Abstract
We analyze four different types of docu-
ment networks with respect to their small
world characteristics. These characteris-
tics allow distinguishing wiki-based sys-
tems from citation and more traditional
text-based networks augmented by hyper-
links. The study provides evidence that a
more appropriate network model is needed
which better reflects the specifics of wiki
systems. It puts emphasize on their topo-
logical differences as a result of wiki-
related linking compared to other text-
based networks.
1 Introduction
With the advent of web-based communication,
more and more corpora are accessible which man-
ifest complex networks based on intertextual rela-
tions. This includes the area of scientific commu-
nication (e.g. digital libraries as CiteSeer), press
communication (e.g. the New York Times which
links topically related articles), technical commu-
nication (e.g. the Apache Software Foundation?s
documentations of open source projects) and elec-
tronic encyclopedia (e.g. Wikipedia and its re-
leases in a multitude of languages). These are
sources of large corpora of web documents which
are connected by citation links (digital libraries),
content-based add-ons (online press communica-
tion) or hyperlinks to related lexicon articles (elec-
tronic encyclopedias).
Obviously, a corpus of such documents is more
than a set of textual units. There is structure for-
mation above the level of single documents which
can be described by means of graph theory and
network analysis (Newman, 2003). But what is
new about this kind of structure formation? Or do
we just have to face the kind of structuring which
is already known from other linguistic networks?
This paper focuses on the specifics of network-
ing in wiki-based systems. It tackles the following
questions: What structure do wiki-based text net-
works have? Can we expect a wiki-specific topol-
ogy compared to more traditional (e.g. citation)
networks? Or can we expect comparable results
when applying network analysis to these emerging
networks? In the following sections, these ques-
tions are approached by example of a language
specific release of the Wikipedia as well as by
wikis for technical documentation. That is, we
contribute to answering the question why wiki can
be seen as something new compared to other text
types from the point of view of networking.
In order to support this argumentation, section
(2) introduces those network coefficients which
are analyzed within the present comparative study.
As a preprocessing step, section (3) outlines a
webgenre model which in sections (4.1) and (4.2)
is used to represent and extract instances of four
types of document networks. This allows apply-
ing the coefficients of section (2) to these instances
(section 4.3) and narrowing down wiki-based net-
works (section 5). The final section concludes and
prospects future work.
2 Network Analysis
For the time being, the overall structure of com-
plex networks is investigated in terms of Small
Worlds (SW) (Newman, 2003). Since its inven-
tion by Milgram (1967), this notion awaited for-
malization as a measurable property of large com-
plex networks which allows distinguishing small
worlds from random graphs. Such a formalization
was introduced by Watts & Strogatz (1998) who
1
characterize small worlds by two properties: First,
other than in regular graphs, any randomly chosen
pair of nodes in a small world has, on average, a
considerably shorter geodesic distance.1 Second,
compared to random graphs, small worlds show a
considerably higher level of cluster formation.
In this framework, cluster formation is mea-
sured by means of the average fraction of the num-
ber O(vi) of triangles connected to vertex vi and
the number Y(vi) of triples centered on vi (Watts
and Strogatz, 1998):2
C2 = 1n
?
i
O(vi)
Y(vi) (1)
Alternatively, the cluster coefficient C1 com-
putes the fraction of the number of triangles in the
whole network and the number of its connected
vertex triples. Further, the mean geodesic distance
l of a network is the arithmetic mean of all shortest
paths of all pairs of vertices in the network. Watts
and Strogatz observe high cluster values and short
average geodesic distances in small worlds which
apparently combine cluster formation with short-
cuts as prerequisites of efficient information flow.
In the area of information networks, this property
has been demonstrated for the WWW (Adamic,
1999), but also for co-occurrence networks (Ferrer
i Cancho and Sole?, 2001) and semantic networks
(Steyvers and Tenenbaum, 2005).
In addition to the SW model of Watts & Stro-
gatz, link distributions were also examined in or-
der to characterize complex networks: Baraba?si &
Albert (1999) argue that the vertex connectivity of
social networks is distributed according to a scale-
free power-law. They recur to the observation ?
confirmed by many social-semiotic networks, but
not by instances of the random graph model of
Erdo?s & Re?nyi (Bolloba?s, 1985) ? that the num-
ber of links per vertex can be reliably predicted
by a power-law. Thus, the probability P (k) that a
randomly chosen vertex interacts with k other ver-
tices of the same network is approximately
P (k) ? k?? (2)
Successfully fitting a power law to the distrib-
ution of out degrees of vertices in complex net-
works indicates ?that most nodes will be relatively
1The geodesic distance of two vertices in a graph is the
length of the shortest path in-between.
2A triangle is a subgraph of three nodes linked to each
other. Note that all coefficients presented in the following
sections relate by default to undirected graphs.
poorly connected, while a select minority of hubs
will be very highly connected.? (Watts, 2003,
p.107). Thus, for a fixed number of links, the
smaller the ? value, the shallower the slope of the
curve in a log-log plot, the higher the number of
edges to which the most connected hub is incident.
A limit of this model is that it views the prob-
ability of linking a source node to a target node
to depend solely on the connectivity of the lat-
ter. In contrast to this, Newman (2003) proposes
a model in which this probability also depends on
the connectivity of the former. This is done in or-
der to account for social networks in which ver-
tices tend to be linked if they share certain proper-
ties (Newman and Park, 2003), a tendency which
is called assortative mixing. According to New-
man & Park (2003) it allows distinguishing social
networks from non-social (e.g. artificial and bio-
logical) ones even if they are uniformly attributed
as small worlds according to the model of Watts
& Strogatz (1998). Newman & Park (2003) ana-
lyze assortative mixing of vertex degrees, that is,
the correlation of the degrees of linked vertices.
They confirm that this correlation is positive in the
case of social, but negative in the case of techni-
cal networks (e.g. the Internet) which thus prove
disassortative mixing (of degrees).
Although these SW models were applied to cita-
tion networks, WWW graphs, semantic networks
and co-occurrence graphs, and thus to a variety
of linguistic networks, a comparative study which
focuses on wiki-based structure formation in com-
parison to other networks of textual units is miss-
ing so far. In this paper, we present such a study.
That is, we examine SW coefficients which allow
distinguishing wiki-based systems from more ?tra-
ditional? networks. In order to do that, a general-
ized web document model is needed to uniformly
represent the document networks to be compared.
In the following section, a webgenre model is out-
lined for this purpose.
3 A Webgenre Structure Model
Linguistic structures vary with the functions of the
discourses in which they are manifested (Biber,
1995; Karlgren and Cutting, 1994). In anal-
ogy to the weak contextual hypothesis (Miller and
Charles, 1991) one might state that structural dif-
ferences reflect functional ones as far as they are
confirmed by a significantly high number of tex-
tual units and thus are identifiable as recurrent pat-
2
terns. In this sense, we expect web documents
to be distinguishable by the functional structures
they manifest. More specifically, we agree with
the notion of webgenre (Yoshioka and Herman,
2000) according to which the functional structure
of web documents is determined by their member-
ship in genres (e.g. of conference websites, per-
sonal home pages or electronic encyclopedias).
Our hypothesis is that what is common to in-
stances of different webgenres is the existence of
an implicit logical document structure (LDS) ? in
analogy to textual units whose LDS is described
in terms of section, paragraph and sentence cate-
gories (Power et al, 2003). In the case of web doc-
uments we hypothesize that their LDS comprises
four levels:
? Document networks consist of documents
which serve possibly heterogenous functions
if necessary independently of each other. A
web document network is given, for example,
by the system of websites of a university.
? Web documents manifest ? typically in the
form of websites ? pragmatically closed acts
of web-based communication (e.g. confer-
ence organization or online presentation).
Each web document is seen to organize a sys-
tem of dependent subfunctions which in turn
are manifested by modules.
? Document modules are, ideally, functionally
homogeneous subunits of web documents
which manifest single, but dependent sub-
functions in the sense that their realization is
bound to the realization of other subfunctions
manifested by the same encompassing docu-
ment. Examples of such subfunctions are call
for papers, program presentation or confer-
ence venue organization as subfunctions of
the function of web-based conference orga-
nization.
? Finally, elementary building blocks (e.g. lists,
tables, sections) only occur as dependent
parts of document modules.
This enumeration does not imply a one-to-one
mapping between functionally demarcated mani-
fested units (e.g. modules) and manifesting (lay-
out) units (e.g. web pages). Obviously, the same
functional variety (e.g. of a personal academic
home page) which is mapped by a website of
dozens of interlinked pages may also be mani-
fested by a single page. The many-to-many re-
lation induced by this and related examples is de-
scribed in more detail in Mehler & Gleim (2005).
The central hypothesis of this paper is that genre
specific structure formation also concerns docu-
ment networks. That is, we expect them to vary
with respect to structural characteristics according
to the varying functions they meet. Thus, we do
not expect that different types of document net-
works (e.g. systems of genre specific websites vs.
wiki-based networks vs. online citation networks)
manifest homogeneous characteristics, but signif-
icant variations thereof. As we concentrate on co-
efficients which were originally introduced in the
context of small world analyses, we expect, more
concretely, that different network types vary ac-
cording to their fitting to or deviation from the
small world model. As we analyze only a couple
of networks, this observation is bound to the cor-
pus of networks considered in this study. It never-
theless hints at how to rethink network analysis in
the context of newly emerging network types as,
for example, Wikipedia.
In order to support this argumentation, the fol-
lowing section presents a model for representing
and extracting document networks. After that,
the SW characteristics of these networks are com-
puted and discussed.
4 Network Modeling and Analysis
4.1 Graph Modeling
In order to analyse the characteristics of docu-
ment networks, a format for uniformly represent-
ing their structure is needed. In this section, we
present generalized trees for this task. Generalized
trees are graphs with a kernel tree-like structure ?
henceforth called kernel hierarchy ? superimposed
by graph-forming edges as models of hyperlinks.
Figure (1) illustrates this graph model. It distin-
guishes three levels of structure formation:
1. According to the webgenre model of section
(3), L1-graphs map document networks and
thus corpora of interlinked (web) documents.
In section (4.3), four sources of such networks
are explored: wiki document networks, citation
networks, webgenre corpora and, for comparison
with a more traditional medium, networks of news-
paper articles.
3
Figure 1: The stratified model of network representation with kernel hierarchies of L2-graphs.
2. L2-graphs model the structure of web doc-
uments as constituents of a given network.
This structure is seen to be based on ker-
nel hierarchies superimposed, amongst oth-
ers, by up, down and across links (see fig. 1).
In the case of webgenre corpora, L2-graphs mo-
del websites. In the case of citation networks, they
map documents which consist of a scientific arti-
cle and add-ons in the form of citation links. Like-
wise, in the case of online newspapers, L2-graphs
model articles together with content-based hyper-
links. Finally, in the case of wikis, L2-graphs rep-
resent wiki documents each of which consists of a
wiki article together with a corresponding discus-
sion and editing page. According to the webgenre
model of section (3), L2-graphs model web docu-
ments which consist of nodes whose structuring is
finally described by L3-graphs:
3. L3-graphs model the structure of document
modules.
In the case of webgenre corpora, L3-graphs
map the DOM3-based structure of the web pages
of the websites involved. In the case of all other
networks distinguished above they represent the
logical structure of single text units (e.g. the sec-
tion and paragraph structuring of a lexicon, news-
paper or scientific article). Note that the tree-like
structure of a document module may be superim-
posed by hyperlinks, too, as illustrated in figure
(1) by the vertices m and n.
3I.e. Document Object Model.
The kernel hierarchy of an L2-graph is consti-
tuted by kernel links which are distinguished from
across, up, down and outside links (Amitay et
al., 2003; Eiron and McCurley, 2003; Mehler and
Gleim, 2005). These types can be distinguished as
follows:
? Kernel links associate dominating nodes with
their immediately dominated successor nodes
in terms of the kernel hierarchy.
? Down links associate nodes with one of their
(mediately) dominated successor nodes in
terms of the kernel hierarchy.
? Up links analogously associate nodes of the
kernel hierarchy with one of their (mediately
dominating) predecessor nodes.
? Across links associate nodes of the kernel hi-
erarchy none of which is an (im-)mediate pre-
decessor of the other in terms of the kernel
hierarchy.
? Extra (or outside) links associate nodes of the
kernel hierarchy with nodes of other docu-
ments.
Kernel hierarchies are exemplified by a confer-
ence website headed by a title and menu page re-
ferring to, for example, the corresponding call for
papers which in turn leads to pages on the different
conference sessions etc. so that finally a hierarchi-
cal structure evolves. In this example the kernel hi-
erarchy evidently reflects navigational constraints.
That is, the position of a page in the tree reflects
4
the probability to be navigated by a reader starting
from the root page and following kernel links only.
The kernel hierarchy of a wiki document is
spanned by an article page in conjunction with
the corresponding discussion (or talk), history and
edit this or view source pages which altogether
form a flatly structured tree. Likewise in the
case of citation networks as the CiteSeer system
(Lawrence et al, 1999), a document consists of
the various (e.g. PDF or PS) versions of the focal
article as well as of one or more web pages mani-
festing its citations by means of hyperlinks.
From the point of view of document network
analysis, L2-graphs and inter links (see fig. 1) are
most relevant as they span the corresponding net-
work mediated by documents (e.g. websites) and
modules (e.g. web pages). This allows specifying
which links of which type in which network are
examined in the present study:
? In the case of citation networks, citation links
are modeled as interlinks as they relate (sci-
entific) articles encapsulated by documents of
this network type. Citation networks are ex-
plored by example of the CiteSeer system:
We analyze a sample of more than 550,000
articles (see table 1) ? the basic population
covers up to 800,000 documents.
? In the case of newspaper article networks,
content-based links are explored as resources
of networking. This is done by example of
the 1997 volume of the German newspaper
Su?ddeutsche Zeitung (see table 1). That is,
firstly, nodes are given by articles where two
nodes are interlinked if the corresponding ar-
ticles contain see also links to each other.
In the online and ePaper issue of this news-
paper these links are manifested as hyper-
links. Secondly, articles are linked if they
appear on the same page of the same is-
sue so that they belong to the same thematic
field. By means of these criteria, a bipar-
tite network (Watts, 2003) is built in which
the top-mode is spanned by topic and page
units, whereas the bottom-mode consists of
text units. In such a network, two texts are in-
terlinked whenever they relate to at least one
common topic or appear on the same page of
the same issue.
? In the case of webgenres we explore a cor-
pus of 1,096 conference websites (see table
variable value
number of web sites 1,096
number of web pages 50,943
number of hyperlinks 303,278
maximum depth 23
maximum width 1,035
average size 46
average width 38
average height 2
Table 2: A corpus of conference and workshop
websites (counting unit: web pages).
1 and 2) henceforth called indogram cor-
pus.4 We analyze the out degrees of all web
pages of these websites and thus explore ker-
nel, up, down, across, inter and outside links
on the level of L2-graphs. This is done in
order to get a base line for our comparative
study, since WWW-based networks are well
known for their small world behavior. More
specifically, this relates to estimations of the
exponent ? of power laws fitted to their de-
gree distributions (Newman, 2003).
? These three networks are explored in or-
der to comparatively study networking in
Wikipedia which is analyzed by example of
its German release de.wikipedia.org
(see table 1). Because of the rich system of its
node and link types (see section 4.2) we ex-
plore three variants thereof. Further, in order
to get a more reliable picture of wiki-based
structure formation, we also analyze wikis in
the area of technical documentation. This
is done by example of three wikis on open
source projects of the Apache Software Foun-
dation (cf. wiki.apache.org).
In the following section, the extraction of Wiki-
pedia-based networks is explained in more detail.
4.2 Graph Extraction ? the Case of Wiki-
based Document Networks
In the following section we analyze the network
spanned by document modules of the German
Wikipedia and their inter links.5 This cannot sim-
ply be done by extracting all its article pages.
The reason is that Wikipedia documents consist
4See http://ariadne.coli.uni-bielefeld.
de/indogram/resources.html for the list of URLs
of the documents involved.
5We downloaded and extracted the XML release of
this wiki ? cf. http://download.wikimedia.org/
wikipedia/de/pages current.xml.bz2.
5
network network genre node |V | |E|
de.wikipedia.org electronic encyclopedia wiki unit
variant I (e.g. article or talk) 303,999 5,895,615
variant II 406,074 6,449,906
variant III 796,454 9,161,706
wiki.apache.org/jakarta online technical documentation wiki unit 916 21,835
wiki.apache.org/struts online technical documentation wiki unit 1,358 40,650
wiki.apache.org/ws online technical documentation wiki unit 1,042 23,871
citeseer.ist.psu.edu digital library open archive record 575,326 5,366,832
indogram conference websites genre web page 50,943 303,278
Su?ddeutsche Zeitung 1997 press communication newspaper article 87,944 2,179,544
Table 1: The document networks analyzed and the sizes |V | and |E| of their vertex and edge sets.
of modules (manifested by pages) of various types
which are likewise connected by links of differ-
ent types. Consequently, the choice of instances
of these types has to be carefully considered.
Table (3) lists the node types (and their fre-
quencies) as found in the wiki or additionally in-
troduced into the study in order to organize the
type system into a hierarchy. One heuristic for
extracting instances of node types relates to the
URL of the corresponding page. Category, por-
tal and media wiki pages, for example, contain the
prefix Kategorie, Portal and MediaWiki,
respectively, separated by a colon from its page
name suffix (as in http://de.wikipedia.
org/wiki/Kategorie:Musik).
Analogously, table (4) lists the edge types ei-
ther found within the wiki or additionally intro-
duced into the study. Of special interest are redi-
rect nodes and links which manifest transitive and,
thus, mediate links of content-based units. An arti-
cle node v may be linked, for example, with a redi-
rect node r which in turn redirects to an article w.
In this case, the document network contains two
edges (v, r), (r, w) which have to be resolved to a
single edge (v, w) if redirects are to be excluded in
accordance with what the MediaWiki system does
when processing them.
Based on these considerations, we compute net-
work characteristics of three extractions of the
German Wikipedia (see table 1): Variant I con-
sists of a graph whose vertex set contains all Ar-
ticle nodes and whose edge set is based on In-
terlinks and appropriately resolved Redirect links.
Variant II enlarges variant I by including other
content-related wiki units, i.e. ArticleTalk, Portal,
PortalTalk, and Disambiguation pages (multiply
typed nodes were excluded). Variant III consists
of a graph whose vertex set covers all vertices and
edges found in the extraction.
Type Frequency
Documents total 796,454
Article 303,999
RedirectNode 190,193
Talk 115,314
ArticleTalk 78,224
UserTalk 30,924
ImageTalk 2,379
WikipediaTalk 1,380
CategoryTalk 1,272
TemplateTalk 705
PortalTalk 339
MediaWikiTalk 64
HelpTalk 27
Image 97,402
User 32,150
Disambiguation 22,768
Category 21,999
Template 6,794
Wikipedia 3,435
MediaWiki 1,575
Portal 791
Help 34
Table 3: The system of node types and their fre-
quencies within the German Wikipedia.
4.3 Network Analysis
Based on the input networks described in the pre-
vious section we compute the SW coefficients de-
scribed in section (2). Average geodesic distan-
ces are computed by means of the Dijkstra algo-
rithm based on samples of 1,000 vertices of the
input networks (or the whole vertex set if it is of
minor cardinality). Power law fittings were com-
puted based on the model P (x) = ax?? + b. Note
that table (1) does not list the cardinalities of multi
sets of edges and, thus, does not count multiple
edges connecting the same pair of vertices within
the corresponding input network ? therefore, the
numbers in table (1) do not necessarily conform to
the counts of link types in table (4). Note further
that we compute, as usually done in SW analyses,
characteristics of undirected graphs. In the case of
wiki-based networks, this is justified by the possi-
bility to process back links in Media Wiki sys-
tems. In the case of the CiteSeer system this is
justified by the fact that it always displays citation
6
Type Frequency
Links total 17,814,539
Interlink 12,818,378
CategoryLink 1,415,295
Categorizes 704,092
CategorizedBy 704,092
CategoryAssociatesWith 7,111
TopicOfTalk 103,253
TalkOfTopic 88,095
HyponymOf 26,704
HyperonymOf 26,704
InterPortalAssociation 1,796
Broken 2,361,902
Outside 1,276,818
InterWiki 789,065
External 487,753
Intra 1,175,290
Kernel 1,153,928
Across 6,331
Up 6,121
Reflexive 5,433
Down 3,477
Redirect 182,151
Table 4: The system of link types and their fre-
quencies within the German Wikipedia.
and cited by links. Finally, in the case of the news-
paper article network, this is due to the fact that
it is based on a bipartite graph (see above). Note
that the indogram corpus consists of predomi-
nantly unrelated websites and thus does not allow
computing cluster and distance coefficients.
5 Discussion
The numerical results in table (5) are remarkable
as they allow identifying three types of networks:
? On the one hand, we observe the extreme
case of the Su?ddeutsche Zeitung, that
is, of the newspaper article network. It is the
only network which, at the same time, has
very high cluster values, short geodesic dis-
tances and a high degree of assortative mix-
ing. Thus, its values support the assertion that
it behaves as a small world in the sense of the
model of Watts & Strogatz. The only excep-
tion is the remarkably low ? value, where,
according to the model of Baraba?si & Al-
bert (1999), a higher value was expected.
? On the other hand, the CiteSeer sample is the
reverse case: It has very low values of C1 and
C2, tends to show neither assortative, nor dis-
assortative mixing, and at the same time has a
low ? value. The small cluster values can be
explained by the low probability with which
two authors cited by a focal article are related
by a citation relation on their own.6
6Although articles can be expected which cite, for exam-
? The third group is given by the wiki-based
networks: They tend to have higher C1 and
C2 values than the citation network does, but
also tend to show stochastic mixing and short
geodesic distances. The cluster values are
confirmed by the wikis of technical docu-
mentation (also w.r.t their numerical order).
Thus, these wikis tend to be small worlds ac-
cording to the model of Watts & Strogatz,
but also prove disassortative mixing ? compa-
rable to technical networks but in departure
from social networks. Consequently, they are
ranked in-between the citation and the news-
paper article network.
All these networks show rather short geodesic
distances. Thus, l seems to be inappropriate with
respect to distinguishing them in terms of SW
characteristics. Further, all these examples show
remarkably low values of the ? coefficient. In con-
trast to this, power laws as fitted in the analyses
reported by Newman (2003) tend to have much
higher exponents ? Newman reports on values
which range between 1.4 and 3.0. This result is
only realized by the indogram corpus of confer-
ence websites, thus, by a sample of WWW docu-
ments whose out degree distribution is fitted by a
power law with exponent ? = 2.562.
These findings support the view that compared
to WWW-based networks wiki systems behave
more like ?traditional? networks of textual units,
but are new in the sense that their topology nei-
ther approximates the one of citation networks nor
of content-based networks of newspaper articles.
In other words: As intertextual relations are genre
sensitive (e.g. citations in scientific communica-
tion vs. content-based relations in press commu-
nication vs. hyperlinks in online encyclopedias),
networks based on such relations seem to inherit
this genre sensitivity. That is, for varying genres
(e.g. of scientific, technical or press communica-
tion) differences in topological characteristics of
their instance networks are expected. The study
presents results in support of this view of the genre
sensitivity of text-based networks.
6 Conclusion
We presented a comparative study of document
networks based on small world characteristics.
ple, de Saussure and Chomsky, there certainly exist much less
citations of de Saussure in articles of Chomsky.
7
instance type ?d? l ? C1 C2 r
Wikipedia variant I undirected 19.39 3.247 0.4222 0.009840 0.223171 ?0.10
Wikipedia variant II undirected 15.88 3.554 0.5273 0.009555 0.186392 ?0.09
Wikipedia variant III undirected 11.50 4.004 0.7405 0.007169 0.138602 ?0.05
wiki.apache.org/jakarta undirected 23.84 4.488 0.2949 0.193325 0.539429 ?0.50
wiki.apache.org/struts undirected 29.93 4.530 0.2023 0.162044 0.402418 ?0.45
wiki.apache.org/ws undirected 22.91 4.541 0.1989 0.174974 0.485342 ?0.48
citeseer.ist.psu.edu undirected 9.33 4.607 0.9801 0.027743 0.067786 ?0.04
indogram directed 5.95 ??? 2.562 ??? ??? ???
Su?ddeutsche Zeitung undirected 24.78 4.245 0.1146 0.663973 0.683839 0.699
Table 5: Numerical values of SW-related coefficients of structure formation in complex networks: the
average number ?d? of edges per node, the mean geodesic distance l, the exponent ? of successfully fitted
power laws, the cluster values C1, C2 and the coefficient r of assortative mixing.
According to our findings, three classes of net-
works were distinguished. This classification sep-
arates wiki-based systems from more traditional
text networks but also from WWW-based web-
genres. Thus, the study provides evidence that
there exist genre specific characteristics of text-
based networks. This raises the question for mod-
els of network growth which better account for
these findings. Future work aims at elaborating
such a model.
References
Lada A. Adamic. 1999. The small world of web. In
Serge Abiteboul and Anne-Marie Vercoustre, edi-
tors, Research and Advanced Technology for Digital
Libraries, pages 443?452. Springer, Berlin.
Einat Amitay, David Carmel, Adam Darlow, Ronny
Lempel, and Aya Soffer. 2003. The connectivity
sonar: detecting site functionality by structural pat-
terns. In Proc. of the 14th ACM conference on Hy-
pertext and Hypermedia, pages 38?47.
Albert-La?szlo? Baraba?si and Re?ka Albert. 1999. Emer-
gence of scaling in random networks. Science,
286:509?512.
Douglas Biber. 1995. Dimensions of Register Varia-
tion: A Cross-Linguistic Comparison. Cambridge
University Press, Cambridge.
Be?la Bolloba?s. 1985. Random Graphs. Academic
Press, London.
Nadav Eiron and Kevin S. McCurley. 2003. Untan-
gling compound documents on the web. In Proceed-
ings of the 14th ACM conference on Hypertext and
Hypermedia, Nottingham, UK, pages 85?94.
Ramon Ferrer i Cancho and Ricard V. Sole?. 2001. The
small-world of human language. Proceedings of the
Royal Society of London. Series B, Biological Sci-
ences, 268(1482):2261?2265, November.
Jussi Karlgren and Douglass Cutting. 1994. Recogniz-
ing text genres with simple metrics using discrimi-
nant analysis. In Proc. of COLING ?94, volume II,
pages 1071?1075, Kyoto, Japan.
Steve Lawrence, C. Lee Giles, and Kurt Bollacker.
1999. Digital libraries and Autonomous Citation In-
dexing. IEEE Computer, 32(6):67?71.
Alexander Mehler and Ru?diger Gleim. 2005. The net
for the graphs ? towards webgenre representation
for corpus linguistic studies. In Marco Baroni and
Silvia Bernardini, editors, WaCky! Working papers
on the Web as corpus. Gedit, Bologna, Italy.
Stanley Milgram. 1967. The small-world problem.
Psychology Today, 2:60?67.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Mark E. J. Newman and Juyong Park. 2003. Why so-
cial networks are different from other types of net-
works. Physical Review E, 68:036122.
Mark E. J. Newman. 2003. The structure and function
of complex networks. SIAM Review, 45:167?256.
Richard Power, Donia Scott, and Nadjet Bouayad-
Agha. 2003. Document structure. Computational
Linguistics, 29(2):211?260.
Mark Steyvers and Josh Tenenbaum. 2005. The
large-scale structure of semantic networks: Statisti-
cal analyses and a model of semantic growth. Cog-
nitive Science, 29(1):41?78.
Duncan J. Watts and Steven H. Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393:440?442.
Duncan J. Watts. 2003. Six Degrees. The Science of a
Connected Age. Norton & Company, New York.
Takeshi Yoshioka and George Herman. 2000. Coordi-
nating information using genres. Technical report,
Massachusetts Institute of Technology, August.
8
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 65?72,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Correlations in the organization of large-scale syntactic
dependency networks
Ramon Ferrer i Cancho
Departament de F??sica Fonamental
Universitat de Barcelona
Mart?? i Franque`s 1, 08028 Barcelona, Spain.
ramon.ferrericancho@ub.edu
Alexander Mehler
Department of Computational Linguistics and Text Technology
Bielefeld University
D-33615 Bielefeld, Germany
Alexander.Mehler@uni-bielefeld.de
Olga Pustylnikov
Department of Computational Linguistics and Text Technology
Bielefeld University
D-33615 Bielefeld, Germany
Olga.Pustylnikov.@uni-bielefeld.de
Albert D??az-Guilera
Departament de F??sica Fonamental
Universitat de Barcelona
Mart?? i Franque`s 1, 08028 Barcelona, Spain.
albert.diaz@ub.edu
Abstract
We study the correlations in the connec-
tivity patterns of large scale syntactic de-
pendency networks. These networks are
induced from treebanks: their vertices de-
note word forms which occur as nuclei
of dependency trees. Their edges con-
nect pairs of vertices if at least two in-
stance nuclei of these vertices are linked
in the dependency structure of a sentence.
We examine the syntactic dependency net-
works of seven languages. In all these
cases, we consistently obtain three find-
ings. Firstly, clustering, i.e., the probabil-
ity that two vertices which are linked to
a common vertex are linked on their part,
is much higher than expected by chance.
Secondly, the mean clustering of vertices
decreases with their degree ? this find-
ing suggests the presence of a hierarchical
network organization. Thirdly, the mean
degree of the nearest neighbors of a ver-
tex x tends to decrease as the degree of
x grows ? this finding indicates disassor-
tative mixing in the sense that links tend
to connect vertices of dissimilar degrees.
Our results indicate the existence of com-
mon patterns in the large scale organiza-
tion of syntactic dependency networks.
1 Introduction
During the last decade, the study of the statisti-
cal properties of networks as different as technical,
biological and social networks has grown tremen-
dously. See (Baraba?si and Albert, 2002; Dorogovt-
sev and Mendes, 2002; Newman, 2003) for a review.
Among themmany kinds of linguistic networks have
been studied: e.g., free word association networks
(Steyvers and Tenenbaum, 2005), syllable networks
(Soares et al, 2005), thesaurus networks (Sigman
65
and Cecchi, 2002), and document networks (Mehler,
2006). See (Mehler, 2007a) for a review of linguis-
tic network studies. Here we focus on the so called
global syntactic dependency networks (GSDN) (Fer-
rer i Cancho et al, 2004; Ferrer i Cancho, 2005).
A GSDN is induced from a dependency treebank in
two steps:
1. The vertices of the network are obtained from
the word forms appearing as nuclei in the in-
put treebank and from punctuation marks as far
as they have been annotated and mapped onto
dependency trees. The notion of a nucleus is
adapted from Lucien Tesnie`re: a nucleus is a
node of a dependency tree. Note that multipart
nuclei may also occur. We use the term type
in order to denote word forms and punctuation
marks. The reason that we induce vertices from
types, but not from lexemes, is that not all cor-
pora are lemmatized. Thus, the type level is the
least common denominator which allows com-
paring the different networks. Note also that a
systematization of the corpora with respect to
the inclusion of punctuation marks is needed.
2. Two vertices (i.e. types) of a GSDN are con-
nected if there is at least one dependency tree in
which their corresponding instance nuclei are
linked. When it comes to applying the appa-
ratus of complex network theory, the arc direc-
tion is generally disregarded (Newman, 2003).
Thus, GSDNs are simple undirected graphs
without loops or multiple edges.
The attribute ?global? distinguishes macroscopic
syntactic dependency networks from their micro-
scopic counterparts in the form of syntactic depen-
dency structures of single sentences. The latter are
the usual object of dependency grammars and re-
lated formalisms. The goal of this article is to shed
light on the large-scale organization of syntactic de-
pendency structures. In terms of theoretical linguis-
tics, we aim to determine the statistical properties
that are common to all languages (if they exist), the
ones that are not and to explain our findings. To
achieve this goal, we must overcome the limits of
many studies of linguistic networks. Firstly, by us-
ing GSDNs we intend to solve the problems of co-
occurrence networks in which words are linked if
they (a) are adjacent, (b) co-occur within a short
window (Ferrer i Cancho and Sole?, 2001; Milo et al,
2004; Antiqueira et al, 2006; Masucci and Rodgers,
2006) or (c) appear in the same sentence (Caldeira
et al, 2006). This approach is problematic: with
a couple of exceptions (Bordag et al, 2003; Fer-
rer i Cancho and Sole?, 2001), no attempt is made
to filter out statistically insignificant co-occurrences.
Unfortunately the filter used in (Ferrer i Cancho
and Sole?, 2001) is not well-defined because it does
not consider fluctuations of the frequencies of word
co-occurrences. (Bordag et al, 2003) implement
a collocation measure based on the Poisson distri-
bution and, thus, induce collocation instead of co-
occurrence networks. However, the notion of a sen-
tence window and related notions are problematic as
the probability that two words depend syntactically
decays exponentially with the number of intermedi-
ate words (Ferrer i Cancho, 2004). Further, (Ferrer
i Cancho et al, 2004) shows that the proportion of
syntactically wrong links captured from a sentence
by linking adjacent words is about 0.3 while this
proportion is about 0.5 when linking a word to its
1st and 2nd neighbors. Thus, dependency treebanks
offer connections between words that are linguisti-
cally precise according to a dependency grammar
formalism. Secondly, the majority of linguistic net-
work studies is performed on English only ? with
some exceptions (Soares et al, 2005; Ferrer i Can-
cho et al, 2004; Mehler, 2006). Concerning GS-
DNs, (Ferrer i Cancho et al, 2004) considers three
languages but the syntactic dependency information
of sentences is systematically incomplete in two of
them. Here we aim to use complete treebanks and
analyze more (i.e. seven) languages so that we can
obtain stronger conclusions about the common sta-
tistical patterns of GSDNs than in (Ferrer i Cancho
et al, 2004).
Therefore, this article is about statistical regulari-
ties of the organization of GSDNs. These networks
are analyzed with the help of complex network the-
ory and, thus by means of quantitative graph the-
ory. We hypothesize that GSDNs are homoge-
neous in terms of their network characteristics while
they differ from non-syntactic networks. The long-
term objective to analyze such distinctive features
is to explore quality criteria of dependency tree-
banks which allow separating high quality annota-
66
tions from erroneous ones.
The remainder of this article is organized as fol-
lows: Section 2 introduces the statistical measures
that will be used for studying GSDNs of seven lan-
guages. Section 3 presents the treebanks and their
unified representations from which we induce these
networks. Section 4 shows the results and Section 5
discusses them.
2 The statistical measures
Two essential properties of a network are N , the
number of vertices (i.e. the number of types), and k?
the mean vertex degree (Baraba?si and Albert, 2002).
The literature about distinctive indices and distribu-
tions of complex networks is huge. Here we focus
on correlations in the network structure (Serrano et
al., 2006). The reason is that correlation analysis
provides a deeper understanding of network orga-
nization compared to classical aggregative ?small-
world? indices. For instance, two networks may
have the same degree distribution (whose similarity
is measured by the exponent of power laws fitted to
them) while they differ in the degree correlation of
the vertices forming a link. Correlation analysis is
performed as follows: We define p(k) as the propor-
tion of vertices with degree k. Here we study three
measures of correlation (Serrano et al, 2006):
? k?nn(k) is the average degree of the nearest
neighbors of the vertices with degree k (Pastor-
Satorras et al, 2001). If k?nn(k) tends to grow
as k grows the network is said to exhibit assor-
tative mixing. In this case, edges tend to con-
nect vertices of similar degree. If in contrast to
this k?nn(k) tends to shrink as k grows, the net-
work is said to exhibit disassortative mixing. In
this case, edges tend to connect vertices of dis-
similar degree. If there are no correlations, then
k?nn(k) = ? with ? =
?
k2
?
/ ?k?; ?k? = k? is
the 1st and
?
k2
?
the 2nd moment of the degree
distribution, namely
?k? =
N?1?
k=1
kp(k) (1)
?
k2
?
=
N?1?
k=1
k2p(k). (2)
In order to enable comparisons of different net-
works, k?nn(k) is normalized using ? and re-
placed by k?nn(k)/?.
? c?(k) is the mean clustering coefficient of ver-
tices of degree k. The clustering coefficient of
a vertex is defined as the proportion of pairs of
adjacent vertices (u, v) such that u and v are
linked.
? c? is the mean clustering coefficient defined as
c? =
N?1?
k=1
p(k)c?(k). (3)
In order to test the significance of c?, we calcu-
late c?binom = k?/(N ? 1), the expected cluster-
ing coefficient in a control binomial graph. In
a binomial graph, two vertices are linked with
probability p. p = k?/(N ? 1) is chosen so that
the expected number of links of the binomial
graphs is nk?/2 as in the original network.
Assortative mixing is known to be characteristic
for social-semiotic, but not for technical networks
(Newman, 2003). Recently, (Mehler, 2006) has
shown that this characteristic varies a lot for differ-
ent document networks and thus allows distinguish-
ing linguistic networks which are homogeneously
called ?small-worlds?. We have excluded on purpose
the Pearson correlation coefficient of the degrees at
the endpoints of edges that has been used in previous
studies (Ferrer i Cancho et al, 2004) due to the sta-
tistical problems that this measure has in large net-
works with power degree distributions (Serrano et
al., 2006).
3 The treebanks
We analyze seven treebanks each from a different
language. Their features are summarized in Table
1. A comprehensive description of these and re-
lated banks is given by (Kakkonen, 2005). As ex-
plained by Kakkonen, one generally faces the prob-
lem of the heterogeneity not only of the annotation
schemes, but also of the serialization formats used
by them. Thus, we unified the various formats in or-
der to get a single interface to the analysis of syntac-
tic dependency networks derived thereof. Although
67
there exists a representation format for syntactic an-
notations (i.e. TIGER-XML? cf. (Mengel and Lez-
ius, 2000)) we decided to use the Graph eXchange
Language (GXL) in order to solve the heterogeneity
problem. The GXL has been proposed as a uniform
format for data interchange (Holt et al, 2006). It
allows representing attributed, directed, undirected,
mixed, ordered, hierarchical graphs as well as hyper-
graphs. Its application-dependent attribution model
concerns vertices, edges and graphs. Because of
its expressiveness it was utilized in modeling con-
stituency structures (Pustylnikov, 2006) as well as
nonlinear document structures (Mehler, 2007b). We
utilize it to map syntactic dependency structures.
Our GXL binding is schematically explained as
follows: corpora are mapped onto graphs which se-
rialize graph models of sentence-related dependency
structures. Each of these structures is mapped as a
forest whose directed edges are mapped by means
of the GXL?s edge model. This model preserves the
orientation of the input dependency relations. Figure
1 visualizes a sample dependency tree of the Slovene
dependency treebank (Dz?eroski et al, 2006).
Figure 1: Visualization of a sample sentence of the
Slovene dependency treebank (Dz?eroski et al, 2006)
based on its reconstruction in terms of the GXL.
4 Results
A summary of the network measures obtained on the
seven corpora is shown in Table 2. We find that
c?  c?binom indicating a clear tendency of vertices
connected to be connected if they are linked to the
same vertex.
Since the Italian and the Romanian corpus are
Romanic languages and the size of their networks
is similar, they are paired in the figures. Figure 2
shows that the clustering c?(k) decreases as k in-
creases. Figure 3 shows that k?nn(k) decreases as
k increases, indicating the presence of disassortative
mixing when forming links, i.e. links tend to com-
bine vertices of dissimilar degrees. For sufficiently
large k the curves suggest a power-law behavior, i.e.
k?nn(k) ? k??.
5 Discussion
We have found that the behavior of k?nn(k) suggests
k?nn(k) ? k?? for sufficiently large k. A power-law
behavior has been found in technical systems (Ser-
rano et al, 2006). In a linguistic context, a power-
law like behavior with two regimes has been found
in the word adjacency network examined in (Ma-
succi and Rodgers, 2006). A decreasing k?nn(k) for
growing k (an indicator of dissortative mixing) has
been found in biological and social systems (Serrano
et al, 2006). A decreasing c?(k) for growing k has
been found in many non-linguistic systems (e.g. the
Internet map at the autonomous system level), and
also in a preliminary study of Czech and German
syntactic dependency networks (Ferrer i Cancho et
al., 2004). (Ravasz and Baraba?si, 2003) suggest
that this behavior indicates the existence of a hierar-
chical network organization (Ravasz and Baraba?si,
2003). In our case this may indicate the existence
of a core vocabulary surrounded by more and more
special vocabularies. This observation is in accor-
dance with a multipart organization of the rank fre-
quency distribution of the lexical units involved. But
this stratification is not simply due to the words?
collocation patterns, but to their behavior in syntac-
tic dependency structures. We have also found that
c?  c?binom, which is a common feature of non-
linguistic (Newman, 2003) and linguistic networks
(Mehler, 2007a) and, thus, is not very informative.
In sum, we have seen that GSDNs follow a com-
mon pattern of statistical correlations regardless of
the heterogeneity of the languages and annotation
criteria used. This suggests that the structure of
GSDNs may originate from language independent
principles. Since the correlational properties of GS-
DNs are not unique to these networks, our findings
suggest that these principles may also be common
to certain non-linguistic systems. Thus, in order to
make GSDNs distinguishable in terms of their char-
acteristics, finding more expressive network coeffi-
68
Figure 2: c?(k), the mean clustering coefficient of vertices of degree k. (a) Danish, (b) Dutch, (c) Russian,
(d) Slovene, (e) Swedish and (f) Italian (black) and Romanian (gray).
69
Figure 3: k?nn(k)/?, the normalized mean degree of the nearest neighbors of vertices of degree k. (a) Danish,
(b) Dutch, (c) Russian, (d) Slovene, (e) Swedish and (f) Italian (black) and Romanian (gray).
70
Treebank Language Size (#nuclei) Marks included Reference
Alpino Treebank v. 1.2 Dutch 195.069 yes (van der Beek et al, 2002)
Danish Dependency Treebank v. 1.0 Danish 100.008 yes (Kromann, 2003)
Sample of sentences of the http://www.phobos.ro/
Dependency Grammar Annotator Romanian 36.150 no roric/DGA/dga.html
Russian National Corpus Russian 253.734 no (Boguslavsky et al, 2002)
A sample of the Slovene
Dependency Treebank v. 0.4 Slovene 36.554 yes (Dz?eroski et al, 2006)
Talkbanken05 v. 1.1 Swedish 342.170 yes (Nivre et al, 2006)
Turin University Treebank v. 0.1 Italian 44.721 no (Bosco et al, 2000)
Table 1: Summary of the features of the treebanks used in this study. Besides the name, language and
version of the corpus we indicate its size in terms of the number of nuclei tokens in the treebank. We also
indicate if punctuation marks are treated as vertices of the syntactic structure of sentencess or not.
Language N k? c? c?binom
Alpino Treebank v. 1.2 28491 8.1 0.24 0.00028
Danish Dependency Treebank v. 1.0 19136 5.7 0.20 0.00030
Dependency Grammar Annotator 8867 5.3 0.093 0.00060
Russian National Corpus 58285 6.1 0.088 0.00010
Slovene Dependency Treebank v. 0.4 8354 5.3 0.12 0.00064
Talkbanken05 v. 1.1 25037 10.5 0.27 0.00042
Turin University Treebank v. 0.1 8001 6.9 0.18 0.00086
Table 2: Summary of the properties of the GSDNs analyzed. N is the number of vertices, k? is the mean
degree, c? is the mean clustering coefficient, c?binom is the clustering coefficient of the control binomial graph.
cients is needed. A possible track could be consid-
ering the weight of a link, which is known to pro-
vide a more accurate description of the architecture
of complex networks (Barrat et al, 2004).
Acknowledgement
We are grateful to Tomaz? Erjavec for the opportu-
nity to analyze a sample of the Slovene Dependency
Treebank (http://nl.ijs.si/sdt/). ADG
and RFC?s work was funded by the projects
FIS2006-13321-C02 and BFM2003-08258-C02-02
of the Spanish Ministry of Education and Science.
AM and OP?s work is supported by the SFB
673 Alignment in Communication (http://
ariadne.coli.uni-bielefeld.de/sfb/)
funded by the German Research Foundation (DFG).
References
Lucas Antiqueira, Maria das Gracas V. Nunes, Os-
valdo N. Oliveira, and Luciano da F. Costa. 2006.
Strong correlations between text quality and complex
networks features. Physica A, 373:811?820.
Albert-La?szlo? Baraba?si and Re?ka Albert. 2002. Statisti-
cal mechanics of complex networks. Rev. Mod. Phys.,
74:47?97.
A. Barrat, M. Barthe?lemy, R. Pastor-Satorras, and
A. Vespignani. 2004. The architecture of complex
weighted networks. In Proc. Nat. Acad. Sci. USA, vol-
ume 101, pages 3747?3752.
Igor Boguslavsky, Ivan Chardin, Svetlana Grigorieva,
Nikolai Grigoriev, Leonid Iomdin, Leonid Kreidlin,
and Nadezhda Frid. 2002. Development of a depen-
dency treebank for russian and its possible applications
in NLP. In Proc. of LREC 2002.
Stefan Bordag, Gerhard Heyer, and Uwe Quasthoff.
2003. Small worlds of concepts and other principles of
semantic search. In Proc. of the Second International
Workshop on Innovative Internet Computing Systems
(IICS ?03).
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Lesmo Lesmo. 2000. Building a treebank for
Italian: a data-driven annotation schema. In Proc. of
LREC 2000.
71
Silvia Maria Gomes Caldeira, Thierry Petit Loba?o,
Roberto Fernandes Silva Andrade, Alexis Neme, and
J. G. Vivas Miranda. 2006. The network of con-
cepts in written texts. European Physical Journal B,
49:523?529.
Serguei N. Dorogovtsev and Jose Fernando Ferreira
Mendes. 2002. Evolution of random networks. Adv.
Phys., 51:1079?1187.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pajas,
Zdenek Z?abokrtsky?, and Andreja Z?ele. 2006. Towards
a Slovene dependency treebank. In Proc. of LREC
2006.
Ramon Ferrer i Cancho and Ricard V. Sole?. 2001. The
small-world of human language. Proc. R. Soc. Lond.
B, 268:2261?2266.
Ramon Ferrer i Cancho, Ricard V. Sole?, and Reinhard
Ko?hler. 2004. Patterns in syntactic dependency net-
works. Physical Review E, 69:051915.
Ramon Ferrer i Cancho. 2004. Euclidean distance be-
tween syntactically linked words. Physical Review E,
70:056135.
Ramon Ferrer i Cancho. 2005. The structure of syn-
tactic dependency networks from recent advances in
the study of linguistic networks. In V. Levickij and
G. Altmann, editors, The problems in quantitative lin-
guistics, pages 60?75. Ruta, Chernivtsi.
Richard C. Holt, Andy Schu?rr, Susan Elliott Sim, and An-
dreas Winter. 2006. GXL: A graph-based standard ex-
change format for reengineering. Science of Computer
Programming, 60(2):149?170.
Tuomo Kakkonen. 2005. Dependency treebanks: meth-
ods, annotation schemes and tools. In Proc. of
NODALIDA 2005, pages 94?104, Joensuu, Finland.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Joakim Nivre and Erhard Hinrichs, editors, Proc. of
TLT 2003. Va?xjo? University Press.
Adolfo Paolo Masucci and Geoff J. Rodgers. 2006. Net-
work properties of written human language. Physical
Review E, 74:026102.
Alexander Mehler. 2006. Text linkage in the wiki
medium ? a comparative study. In Proc. of the EACL
Workshop on New Text ? Wikis and blogs and other
dynamic text sources, pages 1?8.
Alexander Mehler. 2007a. Large text networks as an
object of corpus linguistic studies. In A. Lu?deling and
M. Kyto?, editors, Corpus linguistics. An international
handbook of the science of language and society. de
Gruyter, Berlin/New York.
Alexander Mehler. 2007b. Structure formation in the
web. A graph-theoretical model of hypertext types. In
A. Witt and D. Metzing, editors, Linguistic Modeling
of Information and Markup Languages. Springer, Dor-
drecht.
Andreas Mengel and Wolfgang Lezius. 2000. An XML-
based representation format for syntactically annotated
corpora. In Proc. of LREC 2000.
Ron Milo, Shalev Itzkovitz, Nadav Kashtan, Reuven
Levitt, Shai Shen-Orr, Inbal Ayzenshtat, Michal Shef-
fer, and Uri Alon. 2004. Superfamilies of evolved and
designed networks. Science, 303:1538?1542.
Mark E. J. Newman. 2003. The structure and function of
complex networks. SIAM Review, 45:167?256.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A swedish treebank with phrase structure
and dependency annotation. In Proc. of LREC 2006.
Romualdo Pastor-Satorras, Alexei Va?zquez, and
Alessandro Vesipignani. 2001. Dynamical and
correlation properties of the internet. Physical Review
Letters, 87(25):268701.
Olga Pustylnikov. 2006. How much information is pro-
vided by text structure? Automatic text classification
using structural features (in German). Master thesis,
University of Bielefeld, Germany.
Erzse?bet Ravasz and Albert-La?szlo? Baraba?si. 2003. Hi-
erarchical organization in complex networks. Phys.
Rev. E, 67:026112.
M. A?ngeles Serrano, Marian Bogun?a?, Romualdo Pastor-
Satorras, and Alessandro Vespignani. 2006. Corre-
lations in complex networks. In G. Caldarelli and
A. Vespignani, editors, Structure and Dynamics of
Complex Networks, From Information Technology to
Finance and Natural Science, chapter 1. World Scien-
tific.
Mariano Sigman and Guillermo A. Cecchi. 2002. Global
organization of the WordNet lexicon. In Proc. Natl.
Acad. Sci. USA, volume 99, pages 1742?1747.
Ma?rcio Medeiros Soares, Gilberto Corso, and Liacir dos
Santos Lucena. 2005. The network of syllables in
Portuguese. Physica A, 355(2-4):678?684.
Mark Steyvers and Josh Tenenbaum. 2005. The large-
scale structure of semantic networks: statistical anal-
yses and a model of semantic growth. Cognitive Sci-
ence, 29(1):41?78.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Proc. of the Conf. on Computa-
tional Linguistics in the Netherlands (CLIN ?02).
72
Proceedings of the Linguistic Annotation Workshop, pages 140?147,
Prague, June 2007. c?2007 Association for Computational Linguistics
Web-based Annotation of Anaphoric Relations and Lexical Chains
Maik St?hrenberg and Daniela Goecke and Nils Diewald and Alexander Mehler
Bielefeld University
Germany
{maik.stuehrenberg|daniela.goecke|nils.diewald|alexander.mehler}@uni-bielefeld.de
Irene Cramer
Dortmund University
Germany
irene.cramer@uni-dortmund.de
Abstract
Annotating large text corpora is a time-
consuming effort. Although single-user an-
notation tools are available, web-based an-
notation applications allow for distributed
annotation and file access from different lo-
cations. In this paper we present the web-
based annotation application Serengeti for
annotating anaphoric relations which will be
extended for the annotation of lexical chains.
1 Introduction
The relevance of corpus work for different tasks in
the fields of linguistics is widely accepted. This
holds especially for the area of (semi-)automatic
text and discourse analysis which demands reference
corpora in which instances of various levels of dis-
course structure have been annotated. Such anno-
tation tasks are typically carried out by a combina-
tion of automatic and manual techniques. Manual
annotation of large text corpora is a time consum-
ing effort. Therefore, annotation tools are an indis-
pensable means to overcome the limits of manual
annotations. In spite of their limited level of au-
tomatization, such tools nevertheless help to semi-
automatically support the annotation process and to
secure consistency of manual annotations. This pa-
per describes such an annotation tool which focuses
on a certain type of discourse structures. More
specifically, we deal with anaphoric relations and
lexical cohesion. Our starting point is the obser-
vation that these two resources of textual cohesion
(Halliday and Hasan, 1976) homogeneously induce
chain-like discourse structures: one the one hand we
have reference chains started by some antecedence
and continued by some anaphora linked to the same
antecedence. On the other hand, lexical cohesion
generates so called lexical chains of semantically
related tokens. Based on this observation we de-
scribe the annotation tool Serengeti which reflects
this structural homogeneity on the level of its struc-
tural representation model as well as by its proce-
dural annotation model. Serengeti includes an an-
notation scheme which is extended in order to sup-
port the annotation of reference chains and lexical
chains. The paper is organized as follows: Section
2.1 describes the application scenario of anaphoric
relations and the scheme we use to annotate them.
Section 2.2 deals with the second application sce-
nario: lexical chains. As our starting point was the
former scenario, its extension to the latter one will be
motivated by a separate case study of lexical chain-
ing. Section 3 refers to related work, while Section
4 describes our annotation tool in detail. Finally, the
application of Serengeti to annotating lexical chains
is described in Section 5.
2 Annotating Large Text Corpora
The main focus of the joint work presented in this
paper1 is text technological information modelling
and analysis of various types of discourse. Within
our research group we deal with the integration of
1The work presented in this paper is a joint ef-
fort of the projects A2, A4 and B1 of the Research
Group Text-technological modelling of information funded
by the German Research Foundation. See http://www.
text-technology.de for further details.
140
heterogeneous linguistic resources. This applies es-
pecially to the Sekimo project (A2) which focusses
on the application domain of anaphora resolution.
We use the term ?heterogeneity? to refer to resources
that differ either in terms of form (text, audio, video)
or in terms of function (e. g. lexicons, annotated
texts). Connection between these resources can
be established with the means of XML, cf. Si-
mons (2004). Integrating resources via an abstract
interface is necessary due to different reasons: The
resources used have often been developed indepen-
dently from each other and a cascaded application
of one resource to the output of another resource is
not always possible. Furthermore, the output of dif-
ferent resources often cannot be encoded in a single
structure without driving into incompatibilites (i. e.
XML overlap). Therefore an architecture was devel-
oped which allows for the combination of the out-
put structures of several linguistic resources into a
single XML annotated document and which is de-
scribed in detail in Witt et al (2005) and St?hren-
berg et al (2006) .
2.1 Anaphoric Relations
Motivation and Background Resolving anapho-
ric relations needs a variety of different informa-
tion (e. g. POS, distance information, grammati-
cal function, semantic knowledge, see, for exam-
ple, Mitkov (2002) for an overview). Several re-
sources are applied to a corpus of 47 texts and the
output structures are combined into a single XML
document using the architecture mentioned above.
In order not only to integrate but also evaluate re-
sources for a given linguistic task formally in terms
of precision and recall, it should be possible to ei-
ther switch on or switch off a given resource. In
the application domain of anaphora resolution eval-
uation is done as follows. Each discourse entity
or referent (cf. Karttunen (1976)) is annotated as
an XML element which holds a variety of attribute
information. Each XML element is reinterpreted
as a feature vector; pairs of discourse entities be-
tween which an anaphoric relation holds form a sin-
gle feature vector with additional information rele-
vant for anaphora resolution (e. g. distance informa-
tion, identity of grammatical form, semantic relat-
edness of underlying lemmata and the like). In or-
der to evaluate different resource settings, decision
trees with varying sets of feature vectors are used
for the process of anaphora resolution. Xiaofeng et
al. (2004) or Strube and M?ller (2003) have shown
the feasibility of decision trees for the domain of
anaphora resolution; we have chosen this approach
as it makes it possible to easily switch the informa-
tion set for training and evaluation as opposed to e. g.
rewriting rule sets. Both, training and evaluation as
well as empirically based analysis of anaphora need
an annotated reference corpus (Poesio et al, 2002).
Scheme and annotation process are described in the
following section.
The Annotation Scheme for Anaphoric Rela-
tions Several annotation schemes for annotat-
ing anaphoric relations have been developed in
the last years, e. g. the UCREL anaphora an-
notation scheme (Fligelstone, 1992; Garside et
al., 1997), the SGML-based MUC annotation
scheme (Hirschmann, 1997), and the MATE/G-
NOME Scheme (Poesio, 2004), amongst others.
In order to annotate discourse relations ? either
anaphoric relations or lexical chains (cf. Sec-
tion 2.2) ? two types of information have to be spec-
ified. First, the markables, i. e. the elements that can
be part of a relation, have to be specified (cf. M?ller
and Strube (2003)). Second, the relation(s) between
markables and their respective types and subtypes
have to be defined. The markables form a basis for
the annotation process and therefore have to be an-
notated in advance. Normally, for a domain under
investigation, elements are denoted as being mark-
ables either via a specific element or via the use of
a universal attribute. In our system, discourse enti-
ties are detected automatically on the basis of POS
and parsing information. The annotation scheme
for annotating anaphoric relations is an extension
of the scheme presented by Holler et al (2004) that
has been developed for annotations in the context of
text-to-hypertext conversion in the project B1 Hy-
Tex. We adopt the distinction between coreference
and cospecification but we extend the annotation
scheme for an explicit distinction between cospec-
ification (direct anaphora) and bridging (associative
or indirect anaphora). Thus, we add the primary re-
lation type bridgingLink (denoting bridging) to the
already existing one (cospecLink). Each primary
relation type includes different secondary relation
141
Listing 1: The annotation format for anaphoric relations. Shortened and manually revised output
1 <chs:chs>
2 <chs:text>
3 <cnx:de deID="de8" deType="namedEntity" headRef="w36">
4 <cnx:token ref="w36">Maik</cnx:token></cnx:de>
5 <cnx:token ref="w37">hat</cnx:token> <cnx:token ref="w38">kein</cnx:token>
6 <cnx:token ref="w39">eigenes</cnx:token> <cnx:token ref="w40">Fahrrad</cnx:token>,
7 <cnx:token ref="w42">und</cnx:token>
8 <cnx:de deID="de10" deType="namedEntity" headRef="w43">
9 <cnx:token ref="w43">Marie</cnx:token></cnx:de>
10 <cnx:token ref="w45">f?hrt</cnx:token> <cnx:token ref="w46">nicht</cnx:token>
11 <cnx:token ref="w47">in</cnx:token>
12 <cnx:de deID="de11" deType="nom" headRef="w49">
13 <cnx:token ref="w48">den</cnx:token>
14 <cnx:token ref="w49">Urlaub</cnx:token></cnx:de>.
15 <cnx:de deID="de12" deType="nom" headRef="w53">
16 <cnx:token ref="w52">Zwei</cnx:token>
17 <cnx:token ref="w53">Kinder</cnx:token></cnx:de>,
18 <cnx:de deID="de13" deType="nom" headRef="w56">
19 <cnx:token ref="w55">eine</cnx:token>
20 <cnx:token ref="w56">Gemeinsamkeit</cnx:token></cnx:de>:
21 </chs:text>
22 <cnx:token_ref id="w36" head="w37" pos="N" syn="@NH" depV="subj" morph="MSC SG NOM" />
23 <chs:semRel>
24 <chs:bridgingLink relType="hasMember" antecedentIDRefs="de8 de10" phorIDRef="de12"/>
25 </chs:semRel>
26 </chs:chs>
types that specify the subtype of the relation, e. g.
ident or hypernym as secondary types of cospecLink
or meronym or setMember as secondary types of
bridgingLink. An example annotation of an indirect
anaphoric relation (element bridgingLink, line
30) between the discourse entities de12 (lines 18 to
21) and de8 (lines 3 to 5) and de10 (lines 9 to 11)
can be seen in Listing 1.
2.2 Lexical Chaining
Motivation and Background Based on the con-
cept of lexical cohesion (Halliday and Hasan,
1976), computational linguists (inter alia Morris and
Hirst (1991)) developed a method to compute a par-
tial text representation: lexical chains. These span
over passages or even the complete text linking lex-
ical items. The exemplary annotation in Figure 1
illustrates that lexical chaining is achieved by the
selection of vocabulary and significantly accounts
for the cohesive structure of a text passage. Items
in a lexical chain are connected via semantic re-
lations. Accordingly, lexical chains are computed
on the basis of a lexical semantic resource such as
WordNet (Fellbaum, 1998). Figure 1 also depicts
Figure 1: Chaining Example (adapted from Halliday
et al (1976))
several unsystematic relations, which should in prin-
ciple be considered. Unfortunately, common lexical
resources do not incorporate them sufficiently. Most
systems consist of the fundamental modules shown
in Table 1.
However, in order to formally evaluate the perfor-
mance of a given chainer in terms of precision and
recall, a (preferably standardized and freely avail-
able) test set would be required. To our knowledge
such a resource does not exist ? neither for English
142
Module Subtasks
chaining candidate selection preprocessing of corpora:
determine chaining window,
sentence boundaries,
tokens, POS-tagging
chunks etc.
calculation of chains / look-up: lexical semantic
meta-chains resource (e.g. WordNet),
scoring of relations,
sense disambiguation
output creation rate chain strength
(e.g. select strong chains),
build application specific
representation
Table 1: Overview of Chainer Modules
nor for German. We therefore plan to develop an
evaluation corpus (gold standard), which on the one
hand includes the annotation of lexical chains and
on the other hand reveals the rich interaction be-
tween various principles to achieve a cohesive text
structure. In order to systematically construct sound
guidelines for the annotation of this gold standard,
we conducted a case study.
Case Study Six subjects were asked to annotate
lexical chains in three short texts and in doing so
record all challenges and uncertainties they experi-
enced. The subjects were asked to read three texts
? a wikipedia entry (137 words), a newspaper
article (233 words), and an interview (306 words).
They were then given a list of all nouns occurring
in the articles (almost all chainers exclusively con-
sider nouns as chaining candidates), which they had
to rate with respect to their ?importance? in under-
standing the text. On this basis they were asked
to determine the semantic relations of every pos-
sible chaining candidate pair, thus chain the nouns
and annotate the three texts. Just like previously re-
ported case studies (Beigman Klebanov, 2005; Mor-
ris and Hirst, 2004; Morris and Hirst, 2005) aim-
ing at the annotation of lexical chains, we found
that the inter-annotator agreement was in general
relatively low. Only the annotation of very promi-
nent items in the three texts, which accounted for
approximately one fifth of the chaining candidates,
resulted in a satisfying agreement (that is: the ma-
jority of the subjects produced an identical or very
similar annotation). However, all subjects com-
plained about the task. They found it rather diffi-
cult to construct linearized or quasi-linearized struc-
tures, in short, chains. Instead, most of the subjects
built clusters and drew very complex graphs to illus-
trate the cohesive relations they found. They also
pointed out that only a small fraction of the can-
didate list contributed to their text understanding.
This clearly supports our observation that most of
the subjects first skimmed through the text to find
the most prominent items, established chains for this
selection and then worked the text over to distribute
the remaining items to these chains. We therefore as-
sume that lexical chains do not directly reflect read-
ing and understanding processes. Nevertheless, they
do in some way contribute to them. Many subjects
additionally noted that a reasonable candidate list
should also include multi-word units (e.g. techni-
cal terms) or even phrases. Furthermore, as already
reported in previous work (Morris and Hirst, 2004),
the semantic relations usually considered seem not
to suffice. Accordingly, some subjects proposed new
relations to characterize the links connecting can-
didate pairs. Given our own findings and the re-
sults reported in previous work, it is obviously de-
manding to find a clear-cut border between the con-
cepts of lexical chaining, semantic fields, and co-
reference/anaphora resolution. Definitely, the anno-
tation of co-reference/anaphora and lexical chains is
inherently analogous. In both cases an annotation
layer consisting of labelled edges between pairs of
annotation candidates is constructed. However, we
assume that the lexical chaining layer might contain
more edges between annotation candidates. As a
consequence, its structure presumably is more com-
plex and its connectivity higher. We thus plan to
conduct an extended follow-up study in order to ex-
plore these differences between the annotation of
lexical chains and co-reference/anaphora. We also
intend to take advantage of ? amongst other aspects
? the inter-annotator comparison functionality pro-
vided by Serengeti (see Section 4 for a detailed de-
scription) in order to implement a formally correct
inter-annotator agreement test.
3 Available Tools for Annotating
Linguistic Corpora
Both the anaphora resolution and the lexical chain-
ing scenario have shown the importance of an easy-
143
to-use annotation tool. Although a wide range of
annotation tools is available, one has to separate
tools for annotating multimodal corpora from tools
for annotating unimodal (i. e. text) corpora. Dip-
per et al (2004) evaluated some of the most com-
monly used tools of both categories (TASX Anno-
tator, EXMARaLDA, MMAX, PALinkA and Sys-
tematic Coder). Besides, other tools such as ELAN2
or Anvil3 are available as well, as are tool kits such
as the Annotation Graph Toolkit (AGTK)4 or the
NITE XML Toolkit.5 While multimodal annotation
demands a framework supporting the time-aligned
handling of video and audio streams and, therefore,
much effort has been spent on the design and devel-
opment of tools, unimodal annotation has often been
fulfilled by using ordinary XML editors which can
be error-prone. Nevertheless, specialized annota-
tion frameworks are available as well, e. g. MMAX
can be used for multi-level annotation projects (cf.
M?ller and Strube (2001; 2003)). However, as an-
notation projects grow in size and complexity (often
multiple annotation layers are generated), collabo-
rative annotation and the use of annotation tools is
vital.
? Ma et al (2002), for example, describe collab-
orative annotation in the context of the AGTK.
But since most of the aforementioned applica-
tions have to be installed locally on a PC, work-
ing on a corpus and managing annotations ex-
ternally can be difficult.
? Another problem worth to be mentioned is data
management. Having several annotators work-
ing on one text, unification and comparison of
the markup produced is quite difficult.
? Furthermore, annotation tools help to increase
both the quality and quantity of the annotation
process.
Recent web technologies allow the design of web-
based applications that resemble locally installed
desktop programs on the one hand and provide cen-
tral data management on the other hand. Therefore
2http://www.lat-mpi.eu/tools/elan/
3http://www.dfki.de/~kipp/anvil/
4http://agtk.sourceforge.net/
5http://www.ltg.ed.ac.uk/NITE/
distributed annotation is possible regardless of loca-
tion, provided that an internet connection is avail-
able. In this paper we propose the web-based anno-
tation application Serengeti.
4 A new Approach: Serengeti
As the Sekimo project is part of a research group
with interrelated application domains, annotation
layers from different projects have been evaluated
for their interrelationship (e. g. Bayerl et al (2003;
2006)). This led directly to the open design of
Serengeti ? an annotation tool with the fundamen-
tal idea in mind: making possible the annotation
of a single layer (or resource) and the use of the
best annotation possible and the best available re-
sources. Serengeti allows for several experts to an-
notate a single text at the same time as well as to
compare the different annotations (inter-annotator-
agreement) and merge them afterwards. Access to
the documents is available from everywhere (an in-
ternet connection and a browser is required).
4.1 Technical Overview
Serengeti is a web application developed for Mozilla
Firefox,6 thus its architecture is separated into a
client and a server side, following the principles and
tools of AJAX (Asynchronous JavaScript and XML,
cf. Garrett (2005)). While groups, documents and
annotations are managed centrally on the server side,
all user interactions are rendered locally on the client
side.7
4.2 Graphical User Interface
The Graphical User Interface (GUI) of Serengeti is
subdivided into several areas (cf. Figure 2). The
main area renders the text to be annotated, roughly
laid out in terms of paragraphs, lists, tables and non-
text sections according to the input XML data. Ad-
ditionally, predefined markables are underlined and
followed by boxes containing the markables? unique
identifiers. These boxes serve as clickable buttons
to choose markables during the annotation. At this
6Serengeti is targeted at platform independence, so we?ve
chosen Firefox, which is freely available for several operating
systems. Future versions will support other browsers as well.
7Each Serengeti installation supports more than one work-
group. Server sided data management allows the use of ver-
sioning systems like CVS or, in our case, Subversion.
144
time, adding markables, i. e. changing the input
data, is not allowed.8 This ensures that all annota-
tors use the same base layer. A section at the bottom
of the interface represents the annotation panel with
a list of all annotated relations on the left and all
editing tools on the right side. An application bar at
the top of the GUI provides functions for choosing
and managing groups, documents and annotations.
4.3 Annotation Process
After logging in and choosing a document to anno-
tate, new relations between markables can be cre-
ated. The markables that take part in the relation
are chosen by left-clicking the boxes attached to the
underlined markables in the text and, if necessary,
unchecked by clicking them once again. To encode
the type of a relation between chosen markables, an
input form at the bottom right of the page provides
various options for specifying the relation accord-
ing to the annotation scheme. The OKAY command
adds created relations to the list, which can subse-
quently be edited or deleted. In regard to their state,
relation bars in the list can be highlighted differ-
ently to simplify the post-editing (i. e. new relations,
old/saved relations, commented relations or incom-
plete relations).9 The user can save his work to the
server at any time. After the annotation process is
completed, the COMMIT command (located in the
document menu) declares the annotation as finished.
4.4 Comparing Annotations and Reaching a
Consensus
In order to achieve the best annotation results it is
necessary to provide an opportunity for the evalua-
tion of single annotations or comparing of multiple
annotations on one single document (either by dif-
ferent annotators or identical annotators at different
points in time). This allows for verification of the
quality of the annotation scheme and for valid train-
ing data for automated natural language processing
tools. For this purpose, a special user access, the
Consensus User (CU), has been developed as part of
Serengeti?s concept. Loading a document as a CU, it
8The definition of XML elements as markables and the lay-
out and relation type specification is driven via an external con-
figuration script, adjustable for each group.
9It is possible to hide relations according to their state as
well.
is possible to choose a single annotation done by any
other annotator (either work in progress or commit-
ted) as the basis for the final annotation. This is done
with the same tools as those for the annotation pro-
cess. If satisfied, the CU can declare the annotation
as ultimately closed via the COMMIT command.
Figure 3: Serengeti?s comparison window in the
lower left part of the GUI.
Furthermore, the CU can compare two annota-
tions with each other. The relations annotated by
both users are then displayed in the relation list and
juxtaposed in case they differ in at least one aspect
(e. g. different relation types as in Figure 3).10 On
this basis the CU can decide which relation to accept
and which one to reject. Again, all editing options
are at the user?s disposal.
While editing single or multiple user annotations,
the CU can save the current state of his work at any
time. Afterwards these annotations will appear in
the ANNOTATIONS MENU as well and can be se-
lected for further evaluation and comparison.11
5 Extending Serengeti
Although one might doubt that Serengeti is directly
applicable to annotating lexical chains, this can nev-
ertheless be done straightforwardly using the anno-
tation described in Section 2.1. Our starting point is
as follows: As markables we refer to entities of the
parser output (i. e. tokens) where a user can mark
a token as the initial vertex of a chain. In order
to reflect the findings of our case study on lexical
chaining we distinguish two cases: Either the an-
notator decides that a newly entered token enlarges
10At this point the assignment of relations is important.
Anaphoric relations, for example, are assigned to each other
if their anaphoric element is the same. If there is more than
one relation with identical anaphoric elements, the relations are
sorted by their relation types and their antecedent(s).
11Comparisons require conflictless annotations, i. e. saved
comparisons have to be free from juxtaposed relations.
145
Figure 2: Serengeti?s User Interface. Screenshots of Serengeti Version 0.7.1
an already marked-up chain by explicitly relating it
to one of its links or he implicitly assigns the to-
ken to that chain as a whole which is visually rep-
resented as part of Serengeti?s interface. In the first
case we just face another use case of our annota-
tion scheme, that is, a link between two tokens or
spans of a text where this link may be typed accord-
ing to some linguistic relation that holds between the
spans, e. g. hyponymy. In the second case of an im-
plicit chain assignment we proceed as follows: We
link the newly processed token to the last vertex of
the lexical chain to which the token is attached and
type this relation non-specifically as association. As
a result, we reduce this use case to the one already
mapped by our general annotation scheme. In or-
der to make this a workable solution, we will in-
tegrate a representation of lexical chains by means
of tag clouds where each chain is represented by a
subset of those lexical units which because of their
frequency are most important in representing that
chain. Following this line of extending Serengeti, we
manage to use it as an annotation tool which handles
anaphoric relations as well as lexical chains.
6 Discussion and Outlook
Serengeti can be used to create corpus data for
training and evaluation purposes. An installation
of Serengeti is available online.12 Currently, the
tool is being generalized to allow the annotation
of lexical chains and several other annotation tasks.
More specifically, we plan to incorporate any kind of
chain-like structuring of text segments and to make
the chains an object of annotation so that they can
be interrelated. This will allow to incorporate con-
stituency relations into the annotation process. Be-
yond that we will incorporate metadata handling to
document all steps of the annotation process.
References
P. S. Bayerl, H. L?ngen, D. Goecke, A. Witt, and
D. Naber. 2003. Methods for the Semantic Analy-
sis of Document Markup. In C. Roisin, E. Muson,
and C. Vanoirbeek, editors, Proceedings of the 2003
ACM symposium on Document engineering (DocEng),
pages 161?170, Grenoble. ACM Press.
12http://coli.lili.uni-bielefeld.de/
serengeti/
146
B. Beigman Klebanov. 2005. Using readers to identify
lexical cohesive structures in texts. In Proceedings of
ACL Student Research Workshop.
S. Dipper, M. G?tze, and M. Stede. 2004. Simple Anno-
tation Tools for Complex Annotation Tasks: an Evalu-
ation. In Proceedings of the LREC Workshop on XML-
based Richly Annotated Corpora, pages 54?62, Lis-
bon, Portugal.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
S. Fligelstone. 1992. Developing a Scheme for Annotat-
ing Text to Show Anaphoric Relations. In G. Leitner,
editor, New Directions in English Language Corpora:
Methodology, Results, Software Developments, pages
153?170. Mouton de Gruyter, Berlin.
J. J. Garrett, 2005. AJAX: A New Approach to Web
Applications. Adaptive Path LLC, February, 18.
Online: http://www.adaptivepath.com/
publications/essays/archives/000385.
php.
R. Garside, S. Fligelstone, and S. Botley. 1997. Dis-
course Annotation: Anaphoric Relations in Corpora.
In R. Garside, G. Leech, and A. McEnery, editors,
Corpus Annotation: Linguistic Information from Com-
puter Text Corpora, pages 66?84. Addison-Wesley
Longman, London.
D. Goecke and A. Witt. 2006. Exploiting Logical Docu-
ment Structure for Anaphora Resolution. In Proceed-
ings of the 5th International Conference., Genoa, Italy.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
L. Hirschmann. 1997. MUC-7 Coreference Task Defini-
tion (version 3.0). In L. Hirschman and N. Chinchor,
editors, Proceedings of Message Understanding Con-
ference (MUC-7).
A. Holler, J.-F. Maas, and A. Storrer. 2004. Exploiting
Coreference Annotations for Text-to-Hypertext Con-
version. In Proceeding of LREC, volume II, pages
651?654, Lisbon, Portugal.
L. Karttunen. 1976. Discourse Referents. Syntax and
Semantics: Notes from the Linguistic Underground,
7:363?385.
X. Ma, L. Haejoong, S. Bird, and K. Maeda. 2002.
Models and Tools for Collaborative Annotation. In
Proceedings of the Third International Conference on
Language Resources and Evaluation, Paris. European
Language Resources Association.
R. Mitkov. 2002. Anaphora Resolution. Longman, Lon-
don.
J. Morris and G. Hirst. 1991. Lexical cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational linguistics, 17(1):21?48, March.
J. Morris and G. Hirst. 2004. Non-classical lexical
semantic relations. In Proceedings of HLT-NAACL
Workshop on Computational Lexical Semantics.
J. Morris and G. Hirst. 2005. The subjectivity of lexi-
cal cohesion in text. In J. C. Chanahan, C. Qu, and
J. Wiebe, editors, Computing attitude and affect in text.
Springer.
C. M?ller and M.l Strube. 2001. Annotating Anaphoric
and Bridging Relations with MMAX. In Proceedings
of the 2nd SIGdial Workshop on Discourse and Dia-
logue, pages 90?95, Aalborg, Denmark.
C. M?ller and M. Strube. 2003. Multi-Level Annotation
in MMAX. In Proceedings of the 4th SIGdial Work-
shop on Discourse and Dialogue, pages 198?207, Sap-
poro, Japan.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Viera. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proc. of the 3rd Conference
on Language Resources and Evaluation (LREC).
M. Poesio. 2004. The MATE/GNOME Scheme for
Anaphoric Annotation, Revisited. In Proceedings of
SIGDIAL, Boston, April.
G. Simons, W. Lewis, S. Farrar, T. Langendoen, B. Fitzsi-
mons, and H. Gonzalez. 2004. The semantics of
markup. In Proceedings of the ACL 2004 Workshop
on RDF/RDFS and OWL in Language Technology
(NLPXML-2004), Barcelona.
M. Strube and C. M?ller. 2003. A Machine Learning
Approach to Pronoun Resolution in Spoken Dialogue.
In Proceedings of the 41st Annual Meeting on Associ-
ation for Computational Linguistics, volume 1, pages
168?175. ACL 03.
M. St?hrenberg, A. Witt, D. Goecke, D. Metzing, and
O. Schonefeld. 2006. Multidimensional Markup
and Heterogeneous Linguistic Resources. In D. Ahn,
E. T. K. Sang, and G. Wilcock, editors, Proceedings of
the 5th Workshop on NLP and XML (NLPXML-2006):
Multi-Dimensional Markup in Natural Language Pro-
cessing, pages 85?88.
A. Witt, D. Goecke, F. Sasaki, and H. L?ngen.
2005. Unification of XML Documents with Con-
current Markup. Literary and Lingustic Computing,
20(1):103?116.
Y. Xiaofeng, J. Su, G. Zhou, and C. L. Tan. 2004. Im-
proving Pronoun Resolution by Incorporating Coref-
erential Information of Candidates. In Proceedings of
ACL.
147
Hierarchical Orderings of Textual Units
Alexander Mehler
University of Trier
Universita?tsring 15
D-54286 Trier, Germany
mehler@uni-trier.de
Abstract
Text representation is a central task for any ap-
proach to automatic learning from texts. It re-
quires a format which allows to interrelate texts
even if they do not share content words, but
deal with similar topics. Furthermore, measur-
ing text similarities raises the question of how
to organize the resulting clusters. This paper
presents cohesion trees (CT) as a data structure
for the perspective, hierarchical organization of
text corpora. CTs operate on alternative text
representation models taking lexical organiza-
tion, quantitative text characteristics, and text
structure into account. It is shown that CTs
realize text linkages which are lexically more
homogeneous than those produced by minimal
spanning trees.
1 Introduction
Text representation is a central task for ap-
proaches to text classification or categorization.
They require a format which allows to seman-
tically relate words, texts, and thematic cate-
gories. The majority of approaches to automa-
tic learning from texts use the vector space or
bag of words model. Although there is much re-
search for alternative formats, whether phrase-
or hyperonym-based, their effects seem to be
small (Scott and Matwin, 1999). More serious-
ly (Riloff, 1995) argues that the bag of words
model ignores morphological and syntactical in-
formation which she found to be essential for
solving some categorization tasks. An alterna-
tive to the vector space model are semantic
spaces, which have been proposed as a high-
dimensional format for representing relations of
semantic proximity. Relying on sparse know-
ledge resources, they prove to be efficient in cog-
nitive science (Kintsch, 1998; Landauer and Du-
mais, 1997), computational linguistics (Rieger,
1984; Schu?tze, 1998), and information retrieval.
Although semantic spaces prove to be an al-
ternative to the vector space model, they leave
the question unanswered of how to explore and
visualize similarities of signs mapped onto them.
In case that texts are represented as points in
semantic space, this question refers to the ex-
ploration of their implicit, content based rela-
tions. Several methods for solving this task have
been proposed which range from simple lists via
minimal spanning trees to cluster analysis as
part of scatter/gahter algorithms (Hearst and
Pedersen, 1996). Representing a sign?s environ-
ment in space by means of lists runs the risk of
successively ordering semantically or themati-
cally diverse units. Obviously, lists neglect the
poly-hierarchical structure of semantic spaces
which may induce divergent thematic progres-
sions starting from the same polysemous unit.
Although clustering proves to be an alternative
to lists, it seeks a global, possibly nested par-
tition in which clusters represent sets of indis-
tinguishable objects regarding the cluster crite-
rion. In contrast to this, we present cohesion
trees (CT) as a data structure, in which single
objects are hierarchically ordered on the basis of
lexical cohesion. CTs, whose field of application
is the management of search results in IR, shift
the perspective from sets of clustered objects to
cohesive paths of interlinked signs.
The paper is organized as follows: the next
section presents alternative text representation
models as extensions of the semantic space ap-
proach. They are used in section (3) as a back-
ground of the discussion of cohesion trees. Both
types of models, i.e. the text representation
models and cohesion trees as a tool for hierarchi-
cally traversing semantic spaces, are evaluated
in section (4). Finally, section (5) gives some
conclusions and prospects future work.
2 Numerical Text Representation
This paper uses semantic spaces as a format
for text representation. Although it neglects
sentence as well as rhetorical structure, it de-
parts from the bag of words model by refer-
ring to paradigmatic similarity as the funda-
mental feature type: instead of measuring in-
tersections of lexical distributions, texts are in-
terrelated on the basis of the paradigmatic regu-
larities of their constituents. A coordinate value
of a feature vector of a sign mapped onto se-
mantic space measures the extent to which this
sign (or its constituents in case of texts) shares
paradigmatic usage regularities with the word
defining the corresponding dimension. Because
of this sensitivity to paradigmatics, semantic
spaces can capture indirect meaning relations:
words can be linked even if they never co-occur,
but tend to occur in similar contexts. Further-
more, texts can be linked even if they do not
share content words, but deal with similar top-
ics (Landauer and Dumais, 1997). Using this
model as a starting point, we go a step further
in departing from the bag of words model by
taking quantitative characteristics of text struc-
ture into account (see below).
Semantic spaces focus on meaning as use
as described by the weak contextual hypothe-
sis (Miller and Charles, 1991), which says that
the similarity of contextual representations of
words contributes to their semantic similarity.
Regarding the level of texts, reformulating this
hypothesis is straightforward:
Contextual hypothesis for texts: the contextual
similarity of the lexical constituents of two texts
contributes to their semantic similarity.
In other words: the more two texts share se-
mantically similar words, the higher the proba-
bility that they deal with similar topics. Clearly,
this hypothesis does not imply that texts having
contextually similar components to a high de-
gree also share propositional content. It is the
structural (connotative), not the propositional
(denotative) meaning aspect to which this hy-
pothesis applies. Moreover, this version of the
contextual hypothesis neglects the structural di-
mension of similarity relations: not only that
a text is structured into thematic components,
each of which may semantically relate to differ-
ent units, but units similar to the text as a whole
do not form isolated, unstructured clumps. Ne-
glecting the former we focus on the latter phe-
nomenon, which demands a supplementary hy-
pothesis:
Structure sensitive contextual hypothesis: units,
which are similar to a text according to the con-
textual hypothesis, contribute to the structur-
ing of its meaning.
Since we seek a model for automatic text rep-
resentation for which nonlinguistic context is
inaccessible, we limit contextual similarity to
paradigmatic similarity. On this basis the latter
two hypotheses can be summarized as follows:
Definition 1. Let C be a corpus in which we
observe paradigmatic regularities of words. The
textual connotation of a text x with respect to
C includes those texts of C, whose constituents
realize similar paradigmatic regularities as the
lexical constituents of x. The connotation of x is
structured on the basis of the same relation of
(indirect) paradigmatic similarity interrelating
the connoted texts.
In order to model this concept of structured
connotation, we use the space model M0 of
(Rieger, 1984) as a point of departure and de-
rive three text representation models M1, M2,
M3. Since M0 only maps words onto semantic
space we extend it in order to derive meaning
points of texts. This is done as follows:
M0 analyses word meanings as the result of
a two-stage process of unsupervised learning. It
builds a lexical semantic space by modeling syn-
tagmatic regularities with a correlation coeffi-
cient ? : W ? C ? Rn and their differences with
an Euclidean metric ? : C ? S ? Rn, where W
is the set of words, C is called corpus space repre-
senting syntagmatic regularities, and S is called
semantic space representing paradigmatic regu-
larities. |W | = n is the number of dimensions of
both spaces. Neighborhoods of meaning points
assigned to words model their semantic similar-
ity: the shorter the points? distances in seman-
tic space, the more paradigmatically similar the
words.
The set of words W , spanning the semantic
space, is selected on the basis of the criterion of
document frequency, which proves to be of com-
parable effectiveness as information gain and
?2-statistics (Yang and Pedersen, 1997). Fur-
thermore, instead of using explicit stop word
lists, we restricted W to the set of lemmatized
nouns, verbs, adjectives, and adverbs.
M1: In a second step, we use S as a format
for representing meaning points of texts, which
are mapped onto S with the help of a weighted
mean of the meaning points assigned to their
lexical constituents:
~xk =
?
ai?W (xk)
wik~ai ? S (1)
~xk is the meaning point of text xk ? C, ~ai the
meaning point of word ai ? W , and W (xk) is
the set of all types of all tokens in xk. Finally,
wik is a weight having the same role as the tfidf-
scores in IR (Salton and Buckley, 1988). As
a result of mapping texts onto S, they can be
compared with respect to the paradigmatic sim-
ilarity of their lexical organization. This is done
with the help of a similarity measure ? based
on an Euclidean metric ? operating on meaning
points and standardized to the unit interval:
? : {~x |x ? C}2 ? [0, 1] (2)
? is interpreted as follows: the higher ?(~x, ~y)
for two texts x and y, the shorter the distance
of their meaning points ~x and ~y in semantic
space, the more similar the paradigmatic usage
regularities of their lexical constituents, and fi-
nally the more semantically similar these texts
according to the extended contextual hypothe-
sis. This is the point, where semantic spaces
depart from the vector space model, since they
do not demand that the texts in question share
any lexical constituents in order to be similar;
the intersection of the sets of their lexical con-
stituents may even be empty.
M2: So far, only lexical features are consid-
ered. We depart a step further from the bag
of words model by additionally comparing texts
with respect to their organization. This is done
with the help of a set of quantitative text char-
acteristics used by (Tuldava, 1998) for auto-
matic genre analysis: type-token ratio, hapax
legomena, (variation of) mean word frequency,
average sentence length, and action coefficient
(i.e. the standardized ratio of verbs and adjec-
tives in a text). In order to make these fea-
tures comparable, they were standardized us-
ing z-scores so that random variables were de-
rived with means of 0 and variances of 1. Be-
yond these characteristics, a further feature was
considered: each text was mapped onto a so
called text structure string representing its divi-
sion into sections, paragraphs, and sentences as
a course approximation of its rhetorical struc-
ture. For example, a text structure string
(T (D(S))(D(S ? S ? S))) (3)
denotes a text T of two sections D, where the
first includes 1 and the second 3 sentences S.
Using the Levenshtein metric for string compa-
rison, this allows to measure the rhetorical simi-
larity of texts in a first approximation. The idea
is to distinguish units connoted by a text, which
in spite of having similar lexical organizations
differ texturally. If for example a short com-
mentary connotes two equally similar texts, an-
other commentary and a long report, the com-
mentary should be preferred. Thus, in M2 the
textual connotation of a text is not only seen to
be structured on the basis of the criterion of sim-
ilarity of lexical organization, but also by means
of genre specific features modeled as quantita-
tive text characteristics. This approach follows
(Herdan, 1966), who programmatically asked,
whether difference in style correlates with dif-
ference in frequency of use of linguistic forms.
See (Wolters and Kirsten, 1999) who, following
this approach, already used POS frequency as a
source for genre classification, a task which goes
beyond the scope of the given paper.
On this background a compound text similar-
ity measure can be derived as a linear model:
?(x, y) =
3?
i=1
?i?i(x, y) ? [0, 1] (4)
a. where ?1(x, y) = ?(~x, ~y) models lexical se-
mantics of texts x, y according to M1;
b. ?2 uses the Levenshtein metric for measur-
ing the similarity of the text structure stings
assigned to x and y;
c. and ?3 measures, based on an Euclidean me-
tric, the similarity of texts with respect to
the quantitative features enumerated above.
?i biases the contribution of these different di-
mensions of text representation. We yield good
results for ?1 = 0.9, ?2 = ?3 = 0.05.
M3: Finally, we experimented with a text
representation model resulting from the aggre-
gation (i.e. weighted mean) of the vector repre-
sentations of a text in both spaces, i.e. vector
and semantic space. This approach, which de-
mands both spaces to have exactly the same
dimensions and standardized coordinate values,
follows the idea to reduce the noise inherent to
both models: whether syntagmatic as in case
of vector spaces, or paradigmatic as in case of
semantic spaces. We experimented with equal
weights of both input vectors.
In the next section we use the text represen-
tation models M1, M2, M3 as different starting
points for modeling the concept of structured
connotation as defined in definition (1):
3 Text Linkage
Departing from ordinary list as well as cluster
structures, we model the connotation of a text
as a hierarchy, where each node represents a sin-
gle connoted text (and not a set of texts as in
case of agglomerative cluster analysis). In or-
der to narrow down a solution for this task we
need a linguistic criterion, which bridges be-
tween the linguistic knowledge represented in
semantic spaces and the task of connotative text
linkage. For this purpose we refer to the con-
cept of lexical cohesion introduced by (Halliday
and Hasan, 1976); see (Morris and Hirst, 1991;
Hearst, 1997; Marcu, 2000) who already use this
concept for text segmentation. According to
this approach, lexical cohesion results from re-
iterating words, which are semantically related
on the basis of (un-)systematic relations (e.g.
synonymy or hyponymy). Unsystematic lexi-
cal cohesion results from patterns of contextual,
paradigmatic similarity: ?[. . . ] lexical items
having similar patterns of collocation?that is,
tending to appear in similar contexts?will gen-
erate a cohesive force if they occur in adjacent
sentences.? (Halliday and Hasan, 1976, p. 286).
Several factors influencing this cohesive force
are decisive for reconstructing the concept of
textual connotation:(i) the contextual similarity
of the words in question, (ii) their syntagmatic
order, and (iii) the distances of their occurren-
ces. These factors cooperate as follows: the
shorter the distance of similar words in a text
the higher their cohesive force. Furthermore,
preceding lexical choices restrict (the interpre-
tation of) subsequent ones, an effect, which re-
tards as their distance grows. But longer dis-
tances may be compensated by higher contex-
tual similarities so that highly related words can
contribute to the cohesion of a text span even
if they distantly co-occur. By means of restrict-
ing contextual to paradigmatic similarity and
therefore measuring unsystematic lexical cohe-
sion as a function of paradigmatic regularities,
the transfer of this concept to the task of hierar-
chically modeling textual connotations becomes
straightforward. Given a text x, whose connota-
tion is to be represented as a tree T , we demand
for any path P starting with root x:
(i) Similarity: If text y is more similar to x
than z, then the path between x and y is
shorter than between x and z, supposed
that y and z belong to the same path P .
(ii) Order: The shorter the distance between y
and z in P , the higher their cohesive force,
and vice versa: the longer the path, the
higher the probability that the subsequent
z is paradigmatically dissimilar to y.
(iii) Distance: A cohesive impact is preserved
even in case of longer paths, supposed that
the textual nodes lying in between are
paradigmatically similar to a high degree.
The reason underlying these criteria is the
need to control negative effects of intransitive
similarity relations: in case that text x is highly
similar to y, and y to z, it is not guaranteed that
(x, y, z) is a cohesive path, since similarity is not
transitive. In order to reduce this risk of incohe-
sive paths, the latter criteria demand that there
is a cohesive force even between nodes which are
not immediately linked. This demand decreases
as the path distance of nodes increases so that
topic changes latently controlled by preceding
nodes can be realized. In other words: adding
text z to the hierarchically structured connota-
tion of x, we do not simply look for an already
inserted text y, to which z is most similar, but
to a path P , which minimizes the loss of cohe-
sion in the overall tree, when z is attached to P .
These comments induce an optimality criterion
which tries to optimize cohesion not only of di-
rectly linked nodes, but of whole paths, thereby
reflecting their syntagmatic order. Looking for
a mathematical model of this optimality crite-
rion, minimal spanning trees (MST) drop out,
since they only optimize direct node-to-node
similarities disregarding any path context. Fur-
thermore, whereas we expect to yield differ-
ent trees modeling the connotations of differ-
ent texts, MSTs ignore this aspect dependency
since they focus on a unique spanning tree of
the underlying feature space. Another candi-
date is given by dependency trees (Rieger, 1984)
which are equal to similarity trees (Lin, 1998):
for a given root x, the nodes are inserted into
its similarity tree (ST) in descending order of
their similarity to x, where the predecessor of
any node z is chosen to be the node y already
inserted, to which z is most similar. Although
STs already capture the aspect dependency in-
duced by their varying roots, the path criterion
is still not met. Thus, we generalize the concept
of a ST to that of a cohesion tree as follows:
First, we observe that the construction of STs
uses two types of order relations: the first, let
it call ?1x, determines the order of the nodes
inserted dependent on root x; the second, let
it call ?2y, varies with node y to be inserted
and determines its predecessor. Next, in order
to build cohesion trees out of this skeleton, we
instantiate all relations ?2y in a way, which finds
the path of minimal loss of cohesion when y is
attached to it. This is done with the help of
a distance measure which induces a descending
order of cohesion of paths:
Definition 2. Let G = ?V,E? be a graph and
P = (v1, . . . , vk) a simple path in G. The path
sensitive distance ??(P, y) of y ? V with respect
to P is defined as
??(P, y) =
1
max(?)
?
vi?V (P )
?i?(~y,~vi) ? [0, 1],
where
?
vi?V (P )
?i ? 1, max(?) is the maximal
value assumed by distance measure ?, and V (P )
is the set of all nodes of path P .
It is clear that for any of the text representa-
tion models M1, M2, M3 and their correspond-
ing similarity measures we get different distance
measures ?? which can be used to instantiate the
order relations ?2y in order to determine the end
vertex of the path of minimal loss of cohesion
when y is attached to it. In case of increasing
biases ?i for increasing index i in definition (2)
the syntagmatic order of path P is reflected in
the sense that the shorter the distance of x to
any vertex in P , the higher the impact of their
(dis-)similarity measured by ?, the higher their
cohesive force. Using the relations ?2y we can
now formalize the concept of a cohesion tree:
Definition 3. Let G = ?V,E, ?? be a complete
weighted graph induced by a semantic space,
and x ? V a node. The graph D(G, x) =
?V, E , ?? with E = {{v, w} | v <1x w ? ??y ? V :
y <1x w?y <
2
w v} and ? : E ? R, the restriction
of ? to E , is called cohesion tree induced by x.
Using this definition of a cohesion tree (CT)
we can compute hierarchical models of the con-
notations of texts, in which not only aspect de-
pendency induced by the corresponding root,
but also path cohesion is taken into account.
A note on the relation between CTs and clus-
ter analysis: CTs do not only depart from clus-
ter hierarchies, since their nodes represent sin-
gle objects, and not sets, but also because they
refer to a local, contextsensitive building crite-
rion (with respect to their roots and paths). In
contrast to this, cluster analysis tries to find a
global partition of the data set. Nevertheless
there is a connection between both methods of
unsupervised learning: Given a MST, there is
a simple procedure to yield a divisive partition
(Duda et al, 2001). Moreover, single linkage
graphs are based on a comparable criterion as
MSTs. Analogously, a given CT can be divided
into non-overlapping clusters by deleting those
edges whose length is above a certain threshold.
This induces, so to say, perspective clusters or-
ganized dependent on the perspective of the root
and paths of the underlying CT.
4 Evaluation
Figure (1) exemplifies a CT based on M3 using a
textual root dealing with the ?BSE Food Scan-
dal? from 1996. The text sample belongs to a
corpus of 502 texts of the German newspaper
Su?ddeutsche Zeitung of about 320,000 run-
ning words. Each text belongs to an element of
a set T of 18 different subject categories (e.g.
politics, sports). Based on the lemmatized cor-
pus a semantic space of 2715 lexical dimensions
was built and all texts were mapped onto this
space according to the specifications of M3. In
figure (1) each textual node of the CT is rep-
resented by its headline and subject category
as found in the newspaper. All computations
Figure 1: A sample CT.
were performed using a set of C++ programs es-
pecially implemented for this study.
In order to rate models M1, M2, M3 in com-
parison to the vector space model (VS) using
MSTs, STs and CTs as alternative hierarchi-
cal models we proceed as follows: as a simple
measure of representational goodness we com-
pute the average categorial cohesion of links of
all MSTs, STs and CTs for the different models
and all texts in the corpus. Let G = ?V,E? be
a tree of textual nodes x ? V , each of which is
assigned to a subject category ?(x) ? T , and
P (G) the set of all paths in G starting with
root x and ending with a leaf, then the cate-
gorial cohesion of G is the average number of
links (vi, vj) ? E per path P ? P (G), where
?(vi) = ?(vj). The more nodes of identical cat-
egories are linked in paths in G, the more cat-
egorially homogeneous these paths, the higher
the average categorial cohesion of G. According
to the conceptual basis of CTs we expect these
trees to be of highest categorial link cohesion,
but this is not true: MSTs produce the highest
cohesion values in case of VS and M3. Further-
more, we observe that model M3 induces trees
of highest cohesion and lowest variance, whereas
VS shows the highest variance and lowest cohe-
sion scores in case of STs and CTs. In other
words: based on semantic spaces, models M1,
M2, and M3 produce more stable results than
the vector space model.
Using M3 as a starting point it can be
asked more precisely, which tree class produces
the most cohesive model of text connotation.
Clearly, the measure of categorial link cohesion
is not sufficient to evaluate the classes, since two
immediately linked texts belonging to the same
Model MSTs STs CTs
VS 1325.88 462.04 598.87
M1 1093.06 680.06 1185.92
M2 1097.39 661.72 1168.63
M3 1488.38 628.51 1032.55
Table 1: Alternative representation models and
scores of trees derived from them.
subject category may nevertheless deal with dif-
ferent topics. Thus we need a finer-grained
measure which operates directly on the texts?
meaning representations. In case of unsuper-
vised clustering, where fine-grained class labels
are missed, (Steinbach et al, 2000) propose a
measure which estimates the overall cohesion of
a cluster. This measure can be directly applied
to trees: let Pv1,vn = (v1, . . . , vn) be a path in
tree G = ?V,E? starting with root v1 = x, we
compute the cohesion of P irrespective of the
order of its nodes as follows:
?(Pv1,vn) = 1?
1
n2
n?
i,j=1
1
max(?)
?(vi, vj) (5)
The more similar the nodes of path P accor-
ding to metric ?, the more cohesive P . ? is
derived from the distance measure operating
on the semantic space to which texts vi are
mapped. As before, all scores ?(P ) are summed
up for all paths in P (G) and standardized by
means of |P (G)|. This guarantees that neither
trees of maximum height (MHT) nor of max-
imum degree (MDT), i.e. trees which trivially
correspond to lists, are assigned highest cohe-
sion values. The results of summing up these
scores for all trees of a given class for all texts
in the test corpus are shown in table (2). Now,
Type
?
?(G) Type
?
?(G)
MDT 388.1 MST 416.3
MHT 388.1 DT 430.9
RST 386.6 CT 438.6
Table 2: The sum of the cohesion scores for all
tree classes and all texts in the test corpus.
CTs and STs realize the most cohesive struc-
tures. This is more obvious if the scores ?(G)
are compared for each text in separation: in
494 cases, CTs are of highest cohesion accord-
ing to measure (5). In only 7 cases, MST are
of highest cohesion, and in only one case, the
corresponding ST is of highest cohesion. More-
over, even the stochastically organized so called
random successor trees (RST), in which succes-
sor node?s and their predecessors are randomly
chosen, produce more cohesive structures than
lists (i.e. MDTs and MHTs), which form the
predominant format used to organize search re-
sults in Internet.
To sum up: Table (2) rates CTs in combi-
nation with model M3 on highest level. Thus,
from the point of view of lexical semantics CTs
realize more cohesive branches than MSTs. But
whether these differences are significant, is hard
to evaluate, since their theoretical distribution
is unknown. Thus, future work will be on find-
ing these distributions.
5 Conclusion
This paper proposed 3 numerical representation
formats as means for modeling the hierarchical
connotation of texts in combination with cohe-
sion trees. This was done by extending the weak
contextual hypothesis onto the level of texts in
combination with a reinterpretation of the con-
cept of lexical cohesion as a source for text link-
age. Although the formats used depart from
the bag of words model there is still the need of
investigating numerical formats which rely on
linguistically more profound discourse models.
References
R. O. Duda, P. E. Hart, and D. G. Stork. 2001.
Pattern Classification. Wiley, New York.
Michael A. K. Halliday and R. Hasan. 1976.
Cohesion in English. Longman, London.
M. A. Hearst and J. O. Pedersen. 1996. Reex-
amining the cluster hypothesis: Scatter/gath-
er on retrieval results. In Proc. ACM SIGIR.
M. A. Hearst. 1997. Texttiling: Segmenting
text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1):33?64.
G. Herdan. 1966. The Advanced Theory of
Language as Choice and Chance. Springer,
Berlin.
W. Kintsch. 1998. Comprehension. A Paradigm
for Cognition. Cambridge University Press.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to plato?s problem. Psychological Review,
104(2):211?240.
D. Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proc. COLING-ACL.
D. Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT
Press, Cambridge, Massachusetts.
G. A. Miller and W. G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1?28.
J. Morris and G. Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indi-
cator of the structure of text. Computational
Linguistics, 17(1):21?48.
B. Rieger. 1984. Semantic relevance and as-
pect dependency in a given subject domain.
In Proc. 10th COLING.
E. Riloff. 1995. Little words can make a big
difference for text classification. In Proc.
SIGIR-95.
G. Salton and C. Buckley. 1988. Term
weighting approaches in automatic text re-
trieval. Information Processing Management,
24(5):513?523.
H. Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?123.
S. Scott and S. Matwin. 1999. Feature engi-
neering for text classification. In Proc. 16th
ICML, pages 379?388.
M. Steinbach, G. Karypis, and V. Kumar. 2000.
A comparison of document clustering tech-
niques. In KDD Workshop on Text Mining.
J. Tuldava. 1998. Probleme und Methoden der
quantitativ-systemischen Lexikologie. Wis-
senschaftlicher Verlag, Trier.
M. Wolters and M. Kirsten. 1999. Exploring
the use of linguistic features in domain and
genre classication. In Proc. EACL.
Y. Yang and J. O. Pedersen. 1997. A compar-
ative study on feature selection in text cate-
gorization. In Proc. 14th ICML.
Proceedings of the EACL 2009 Demonstrations Session, pages 21?24,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
eHumanities Desktop - An Online System for Corpus Management and
Analysis in Support of Computing in the Humanities
Ru?diger Gleim1, Ulli Waltinger2, Alexandra Ernst2, Alexander Mehler1,
Tobias Feith2 & Dietmar Esch2
1Goethe-Universita?t Frankfurt am Main, 2Universita?t Bielefeld
Abstract
This paper introduces eHumanities Desk-
top- an online system for corpus manage-
ment and analysis in support of Comput-
ing in the Humanities. Design issues and
the overall architecture are described as
well as an initial set of applications which
are offered by the system.
1 Introduction
Since there is an ongoing shift towards computer
based studies in the humanities new challenges
in maintaining and analysing electronic resources
arise. This is all the more because research groups
are often distributed over several institutes and
universities. Thus, the ability to collaboratively
work on shared resources becomes an important
issue. This aspect also marks a turn point in
the development of Corpus Management Systems
(CMS). Apart from the aspect of pure resource
management, processing and analysis of docu-
ments have traditionally been the domain of desk-
top applications. Sometimes even to the point of
command line tools. Therefore the technical skills
needed to use for example linguistic tools have ef-
fectively constrained their usage by a larger com-
munity. We emphasise the approach to offer low-
threshold access to both corpus management as
well as processing and analysis in order to address
a broader public in the humanities.
The eHumanities Desktop1 is designed as a gen-
eral purpose platform for scientists in humanities.
Based on a sophisticated data model to manage au-
thorities, resources and their interrelations the sys-
tem offers an extensible set of application modules
to process and analyse data. Users do not need to
undertake any installation efforts but simply can
login from any computer with internet connection
1http://hudesktop.hucompute.org
Figure 1: The eHumanities Desktop environment
showing the document manager and administra-
tion dialog.
using a standard browser. Figure 1 shows the desk-
top with the Document Manager and the Adminis-
tration Dialog opened.
In the following we describe the general archi-
tecture of the system. The second part addresses
an initial set of application modules which are
currently available through eHumanities Desktop.
The last section summarises the system descrip-
tion and gives a prospect of future work.
2 System Architecture
Figure 2 gives an overview of the general archi-
tecture. The eHumanities Desktop is implemented
as a client/server system which can be used via
any JavaScript/Java capable Web Browser. The
GUI is based on the ExtJS Framework2 and pro-
vides a look and feel similar to Windows Vista.
The server side is based on Java Servlet technol-
ogy using the Tomcat3 Servlet Container. The core
of the system is the Command Dispatcher which
2http://extjs.com
3http://tomcat.apache.org
21
manages the communication with the client and
the execution of tasks like downloading a docu-
ment for example. The Master Data include infor-
mation about all objects managed by the system,
for example users, groups, documents, resources
and their interrelations. All this information is
stored in a transactional Relational Database (us-
ing MySQL4). The underlying data model is de-
scribed later in more detail. Another important
component is the Storage Handler: Based on an
automatic mime type5 detection it decides how
to store and retrieve documents. For example
videos and audio material are best stored as files
whereas XML documents are better accessible via
a XML Database Management System or spe-
cialized DBMS (e.g. HyGraphDB (Gleim et al,
2007)). Which kind of Storage Backend is used
to archive a given document is transparent to the
user- and also to developers using the Storage
Handler. The Document Indexer allows for struc-
ture sensitive indexing of text documents. That
way a full text search can be realised. However
this feature is not fully integrated at the moment
and thus subject of future work. Finally the Com-
mand Dispatcher connects to an extensible set of
application modules which allow to process and
analyse stored documents. These are briefly intro-
duced in the next section.
To get a better idea of how the described com-
ponents work together we give an example of how
the task to perform PoS tagging on a text docu-
ment is accomplished: The task to process a spe-
cific document is sent from the client to the server.
As a first step the Command Dispatcher checks
based on the Master Data if the requesting user
is logged in correctly, authorized to perform PoS
tagging and has permission to read the document
to be tagged. The next step is to fetch the docu-
ment from the Storage Handler as input to the PoS
Tagger application module. The tagger creates a
new document which is handed over to the Storage
Handler which decides how to store the resource.
Since the output of the tagger is a XML document
it is stored as a XML Database. Finally the in-
formation about the new document is stored in the
Master Data including a reference to the original
one in order to state from which document it has
been derived. That way it is possible to track on
which basis a given document has been created.
4http://dev.mysql.com
5http://www.iana.org/assignments/
media-types/
Finally the Command Dispatcher signals the suc-
cessful completion of the task back to the Client.
Figure 3 shows the class diagram of the master
data model. The design is woven around the gen-
eral concept that authorities have access permis-
sions on resources. Authorities are distinguished
into users and groups. Users can be members of
one or more groups. Furthermore authorities can
have permissions to use features of the system.
That way it is possible to individually configure
the spectrum of functions someone can effectively
use. Resources are distinguished by documents
and repositories. Repositories are containers, sim-
ilar to directories known from file systems. An im-
portant addition is that resources can be member
of an arbitrary number of repositories. That way a
document or a repository can be used in different
contexts allowing for easy corpus compilation.
A typical scenario which benefits from such a
data model is a distributed research group consist-
ing of several research teams: One team collects
data from field research, a second processes and
annotates the raw data and a third team performs
statistical analysis. In this example every group
has the need to share resources with others while
keeping control over the data: The statistics team
should be able to read the annotated data but must
not be allowed to edit resources and so on.
Figure 2: Overview of the System Architecture.
Figure 3: UML Class Diagram of the Master Data.
22
Figure 4: The eHumanities Desktop environment showing a chained document and the PoS Tagger
dialog.
3 Applications
In the following we outline the initial set of appli-
cations which is currently available via eHuman-
ities Desktop. Figure 4 gives an idea of the look
and feel of the system. It shows the visualisation
of a chained document and the PoS Tagger win-
dow with an opened document selection dialog.
3.1 Document Manager
The Document Manager is the core of the desktop.
It allows to upload and download documents as
well as sharing them with other users and groups.
It follows the look and feel of the Windows Ex-
plorer. Documents and repositories can be created
and edited via context menus. They can be moved
via drag and drop between different repositories.
Both can be copied via drag and drop while press-
ing the Ctrl-key. Note that repositories only con-
tain references- so a copy is not a physical redupli-
cation. Documents which are not assigned to any
repository the current user can see are gathered in
a special repository called Floating Documents. A
double click on a file will open a document viewer
which offers a rendered view of textual contents.
The button ?Access Permissions? opens a dialog
which allows to edit the rights of other users and
groups on the currently selected resources. Finally
a search dialog at the top makes documents search-
able.
3.2 PoS Tagging
The PoS-Tagging module enables users to pre-
process their uploaded documents. Besides to-
kenisation and sentence boundary detection, a tri-
gram HMM-Tagger is implemented in the pre-
processing system (Waltinger and Mehler, 2009).
The tagging module was trained and evaluated
based on the German Negra Corpus (Uszkoreit
et al, 2006) (F-measure of 0.96) and the En-
glish Penn Treebank (Marcus et al, 1994) (F-
measure of 0.956). Additionally a lemmatisation
and stemming module is included for both lan-
guages. As an unifying exchange format the com-
ponent utilises TEI P5 (Burnard, 2007).
3.3 Lexical Chaining
As a further linguistic application module a lex-
ical chainer (Mehler, 2005; Mehler et al, 2007;
Waltinger et al, 2008a; Waltinger et al, 2008b)
has been included in the online desktop environ-
ment. That is, semantically related tokens of a
given text can be tracked and connected by means
of a lexical reference system. The system cur-
rently uses two different terminological ontolo-
gies - WordNet (Fellbaum, 1998) and GermaNet
(Hamp and Feldweg, 1997) - as chaining resources
which have been mapped onto the database for-
mat. However the list of resources for chaining
can easily be extended.
23
3.4 Lexicon Exploration
With regards to lexicon exploration, the system ag-
gregates different lexical resources including En-
glish, German and Latin. In this module, not only
co-occurrence data, social and terminological on-
tologies but also social tagging enhanced data are
available for a given input token.
3.5 Text Classification
An easy to use text classifier (Waltinger et al,
2008a) has been implemented into the system. In
this, an automatic mapping of an unknown text
onto a social ontology is enabled. The system
uses the category tree of the German and English
Wikipedia-Project in order to assign category in-
formation to textual data.
3.6 Historical Semantics Corpus
Management
The HSCM is developed by the research project
Historical Semantics Corpus Management (Jussen
et al, 2007). The system aims at a texttechno-
logical representation and quantitative analysis of
chronologically layered corpora. It is possible to
query for single terms or entire phrases. The con-
tents can be accessed as rendered HTML as well
as TEI P56 encoded. In its current state is supports
to browse and analyse the Patrologia Latina7.
4 Conclusion
This paper introduced eHumanities Desktop- a
web based corpus management system which
offers an extensible set of application modules
which allow online exploration, processing and
analysis of resources in humanities. The use
of the system was exemplified by describing the
Document Manager, PoS Tagging, Lexical Chain-
ing, Lexicon Exploration, Text Classification and
Historical Semantics Corpus Management. Fu-
ture work will include flexible XML indexing and
queries as well as full text search on documents.
Furthermore the set of applications will be gradu-
ally extended.
References
Lou Burnard. 2007. New tricks from an old dog:
An overview of tei p5. In Lou Burnard, Milena
6http://www.tei-c.org/Guidelines/P5
7http://pld.chadwyck.co.uk/
Dobreva, Norbert Fuhr, and Anke Lu?deling, edi-
tors, Digital Historical Corpora- Architecture, An-
notation, and Retrieval, number 06491 in Dagstuhl
Seminar Proceedings, Dagstuhl, Germany. Interna-
tionales Begegnungs- und Forschungszentrum fu?r
Informatik (IBFI), Schloss Dagstuhl, Germany.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge.
Ru?diger Gleim, Alexander Mehler, and Hans-Ju?rgen
Eikmeyer. 2007. Representing and maintaining
large corpora. In Proceedings of the Corpus Lin-
guistics 2007 Conference, Birmingham (UK).
Birgit Hamp and Helmut Feldweg. 1997. Germanet - a
lexical-semantic net for german. In In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Bernhard Jussen, Alexander Mehler, and Alexandra
Ernst. 2007. A corpus management system for his-
torical semantics. Appears in: Sprache und Daten-
verarbeitung.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Alexander Mehler, Ulli Waltinger, and Armin Weg-
ner. 2007. A formal text representation model
based on lexical chaining. In Proceedings of the
KI 2007 Workshop on Learning from Non-Vectorial
Data (LNVD 2007) September 10, Osnabru?ck, pages
17?26, Osnabru?ck. Universita?t Osnabru?ck.
Alexander Mehler. 2005. Lexical chaining as a
source of text chaining. In Jon Patrick and Christian
Matthiessen, editors, Proceedings of the 1st Compu-
tational Systemic Functional Grammar Conference,
University of Sydney, Australia, pages 12?21.
Hans Uszkoreit, Thorsten Brants, Sabine Brants, and
Christine Foeldesi. 2006. Negra corpus.
Ulli Waltinger and Alexander Mehler. 2009. Web as
preprocessed corpus: Building large annotated cor-
pora from heterogeneous web document data. In
preparation.
Ulli Waltinger, Alexander Mehler, and Gerhard Heyer.
2008a. Towards automatic content tagging: En-
hanced web services in digital libraries using lexi-
cal chaining. In 4th Int. Conf. on Web Information
Systems and Technologies (WEBIST ?08), 4-7 May,
Funchal, Portugal. Barcelona.
Ulli Waltinger, Alexander Mehler, and Maik
Stu?hrenberg. 2008b. An integrated model of
lexical chaining: Application, resources and its
format. In Angelika Storrer, Alexander Geyken,
Alexander Siebert, and Kay-Michael Wu?rzner,
editors, Proceedings of KONVENS 2008 ?
Erga?nzungsband Textressourcen und lexikalisches
Wissen, pages 59?70.
24
