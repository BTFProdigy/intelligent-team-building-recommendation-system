Selectional Restr ict ions in HPSG 
I on  Androutsopou los  Rober t  Da le  
Sottware and I (nowledge Engineer ing Laboratory  Language Tcclmology Group 
Inst i tute  of Intbrmat ics  and Te lecommmficat ions  Depar tment  of Comlmt ing  
Nat ional  Centre for Scientific Macquar ie University 
Research "Demokr i tos"  Sydney NSW 2109, Austral ia  
153 10 Ag. Paraskevi,  Athens,  Greece. e-maih Robert .Dale@mq.  edu.  au 
e-maih ionandr@?i t ,  demokr i tos ,  gr 
Abst rac t  
Selectional restrictions arc semantic sortal con- 
straints ilnposed on the particil)ants of lin- 
guistic constructions to capture contextually- 
dependent constraints on interpretation. De- 
spite their linfitations, selectional restrictions 
have t)roven very useflfl in natural bmguage ap- 
pli('ations, where they have been used frequently 
in word sense disambiguation, syntactic disam- 
biguation, and anaphora resolution. Given their 
practical wtlue, we explore two methods to in- 
corporate selectional restrictions in the HPSG 
theory, assuming that the reader is familiar with 
HPSG. The first method eml)loys ItPSG~S BACK- 
GROUND feature and a constraint-satisfaction 
comt)onent t)il)e-lined after the parser. The 
second method uses subsorts of retbrential in- 
dices, and 1)locks readings that violat(', sole(:- 
tional restrictions during parsing. While the- 
oretically less satisfactory, we have Ibund the 
second method particularly useflfl in the devel- 
opment of practical systems. 
1_ I n t roduct ion  
~lPhe term selectional restrictions refers to se- 
mantic sortal constraints imposed on the 1)ar -
ticipants of linguistic constructions. Selectional 
restrictions arc invoked, for example, to account 
tbr the oddity of (1) and (3) (cf. (2) and (4)). 
(1) ?Tom ate a keyboard. 
(2) Tom ate a banana. 
(3) ?Tom repaired the technician. 
(4) Tom repaired the keyboard. 
~IPo account br (1) and (2), one would typically 
introduce a constraint requiring the object of 
"to eat" to denote an edible entity. The odd- 
ity of (1) can then be attr ibuted to a violation 
of this constraint, since keyboards are typically 
not edible. Silnilarly, in (3) and (4) one could 
postulate that "to repair" can only be used with 
objects denoting artifacts. This constraint is vi- 
olated by (3), because technicians are typically 
persons, and persons are not artifacts. 
We note that selectional restrictions attempt 
to capture contextually-dei)endent constraints 
on interpretation. There is nothing inherently 
wrong with (1), and one can think of special 
contexts (e.g. where Tom is a circus pertbrmer 
whose act includes gnawing on comtmter pe- 
ripherals) where (1) is felicitous. The oddity 
of (1) is due to the fact that in most contexts 
l)eople do not eat keyboards. Similarly, (3) is 
ti;licitous in a science-fiction context where the 
technician is a robot, |rot not in most usual con- 
texts. Selectional restrictions are typically used 
to capture flints about tlm world which are gen- 
c.r~lly, but not necessarily, true. 
In w~rious forms, selectional restrictions have 
been used tbr many years, and their l imitations 
are well-known (Allen, 1995). For example, 
they cmmot account br lnetaphoric uses of lan- 
guage (e.g. (5)), and they run into 1,roblen,s in 
negated sentences (e.g. unlike (1), there is noth- 
ing odd about (6)). 
(5) My car drinks gasoline. 
(6) Tom cannot cat a keyboard. 
Despite their limitations, selectional restric- 
tions have proven very useflfl in practical appli- 
cations, and they have been employed in sev- 
eral large-scale natural language understanding 
systems (Martin et al, 1086) (Alshawi, 1992). 
Apart fl'om blocking pragmatically i lMbrmed 
sentences like (1) and (3), selectional restric- 
tions can also be used in word sense disanfl)igua- 
tion, syntactic dismnbiguation, and anaphora 
15 
resolution. In (7), for example, tile '))~qnter" 
refers to at computer peripheral, while in (8) it 
refers to a person. The correct sense of "printer" 
can be chosen in each case by requiring the ob- 
ject of "to repair" to denote an artifact, and the 
subject of "to call" (when referring to a phone 
call) to denote a person. 
(7) Tom repaired the printer. 
(8) The printer called this mornfilg. 
Silnilarly, (9) is from a syntactic point of view 
potentially ambiguous: the relative clause may 
refer either to the departments or the employ- 
ees. The correct reading can be chosen by speci- 
fying that the subject of "retire" (the relativised 
nominal in this case) must denote a person. 
(9) List tile employees of the overseas depart- 
nlents that will retire next year. 
Given tile value of selectional restrictions in 
t)ractical applications, we explore how they can 
be utilised in the HPS~ theory (Pollard and 
Sag, 1994), assuming that the reader is familiar 
with HPSG. Onr  proposals are based on expe- 
rience obtained from using IIPSG in a natural 
language database interface (Androutsot)oulos 
et al, 1998) and a dialogue system for a mobile 
robot. To the best of our knowledge, selectional 
restrictions have not been explored so far in the 
context of HPSG. 
We note that, although they often exploit 
similar techniques (e.g. semantic sort hierar- 
chies), selectional restrictions costitnte a differ- 
ent topic from linking theories (Davis, 1996). 
Roughly speaking, linking theories explore the 
relation between thematic roles (e.g. agent, pa- 
tient) and grammatical thnctions (e.g. subject, 
complement), while selectional restrictions at- 
tempt to account br the types of world entities 
that can fill the thematic roles. 
We discuss in sections 2 and 3 two ways that 
we have considered to incorporate selectional 
restrictions into HPSO. Section 4 concludes by 
comparing briefly the two approaches. 
2 Background res t r i c t ions  
The first way to accommodate selec- 
tional restrictions in HPSG USeS the 
CONTEXTIBACKGROUND (abbreviated here 
as CXlBG) feature, which Pollard and Sag 
(Pollard and Sag, 1994) reserve tbr "Micity 
conditions on the utterance context", "presup- 
positions or conventional iml)licatures", and 
"at)prot)riateness conditions" (op cit pp. 27, 
332). To express electional restrictions, we add 
qfpsoas (quantifier-flee parameterised states 
of atfairs) with a single semantic role (slot) 
iI1 CXIBG. 1 For exanlple, apart fi'om the eat 
qf'psoa in its NUCLEUS (NUt), the lexical sign 
tbr "ate" (shown in (10)) would introduce an 
edible qfpsoa in Bo, requiring \[\] (the entity 
denoted by tile object of "ate") to be edible. 
(10) "PIION (ate) 
CAT 
SS I LOC CONT \[ NUC 
cx I ~(~ 
\[ I,EAD lcTb l \ ] \ ]  
COMPS ( NP\[~J >i l l  
c.t \[,,~A'r,~N FlJ // 
In the case of lexical signs for proper names 
(e.g. (11)), the treatment of Pollard and Sag 
inserts a naming (namg) qfpsoa in BG, which 
requires the BEARER (BRER) to by identifiable 
in the context by means of the proper name. 
(11) also requires the bearer to be a man. 
(11) "PIION <ToTrt> 
oa,l, \[,,,:AD 
CONT \[INDEX 
\[RESTR ~}\] 
Tile ItPSO t)rinciples that control the propa- 
gation of the BG feature are not fully developed. 
For our purposes, however, tile simplistic prin- 
co)Ic of contextual consistency of Pollard and 
Sag will suffice. This principle causes the BG 
value of each phrase to be the union of tile BG 
values of its daughters. Assuming that the lex- 
ical sign of "keyboard" is (12), (10)-(12) cause 
(1) to receive (13), that requires \[\] to denote an 
edible keyboard. 
1To save space, we use qfpsoas wherever Pollard and 
Sag use quantified psoas. We also ignore tense and as- 
pect here. Consult (Androutsopoulos et al, 1998) for 
the treatment of tense and aspect in our ltpsG-based 
database interface. 
16 
(12) 
(13) 
/ \[INI)EX \[\] 
"PlION 
SS I LOC 
(~/bm, ate, a, keyboard) 
-II\],;AI) verb 1 
CAT SUIL1 <> \[ 
COMI'S ( )  J 
-QUANTS ( keybd \[INST \[~\]\])\] \[":a"E" Hm \] J CONT i NUC k I~A'rI':N eat 
|NAME :/bin|' /
namg ~ " l> 
\[ 
\[lNS'l' \[~l / edible L J 
A(:('ording t,, (13), to accept (1), one has to 
place it; ill ;~ st)ecial conte.xt where edible key- 
bonrds exist (e.g. (1) is thlMtous if it reM's to ;~ 
miniature cho(:ol~te keyt)oard). Su(:h (:ontexts, 
however, are rare, ;rod hen(:e (1) sounds gen- 
erally odd. Alternatively, one has to relax the 
B(~ constraint hat the keyboard 1111181; BC edi|)le. 
We assmne that special contexts ~dlow t)a.rticu - 
l~r BG constraints to be relaxed (this is how we 
wouht a(:(:omlt fin" the use of (1) ill ~L circus con- 
text), \])ut we (Io not ll;we any t'ornl~d lne(:hanisnl 
to sl)e(:itly exactly when B(~ (:(mstr;dnts ('~m l)e 
relnxcd. 
Similnr connnents apply to (3). Assuming 
th;Lt the sign of "req)aired" is (ld), nnd that 
the sign of "teclmi(:iml" is similar to (12) ex- 
cept that  it; introdu('es a technician index, (3) 
receives a sign theft requires the repairer to 1)e a 
technician who is all artifact. U k~(:hnicians, how- 
ever, are generally not artifacts, which accounts 
for the oddity of (3). 
(14) -PIION <repaired) 
l / / /  
L(.MP  < NI'  >J // 
"3SIL()C CONTINuc repair \[IIEPAI,\[I,3,) ~\]jl I
Let us now (:onsider \]lOW it (:omtmter sysl;em 
~o,,ld ~e(:o,ll~t for (~)-(~). For ex~ulU,>,, how 
entity 
abstract physical 
animate edible inanimate "" 
" ' "  lllall tech.ician ' /  X~ V "" keybd 
son animal non arfct ~ lc t  
edible animal edible_non afcl 
? " male tech "" 
. . . . . . . . .  ballalla 
Figure 1: A simt)listic semantic hierarchy 
WOilkl the system fig.re out fl'om (13) that (:1) 
is pragm~tieal ly odd? Among other things, it 
wouhl need to know that keyl)o~r(ts ~rc not edi- 
ble. Similarly, in (2) it would need to know that 
|)~m~m~s are edible, ~md in (3) (d) it; would need 
I;() 1)e nwarc that technicians are. llot artifacts, 
while keyboards m:e. Systenls that employ se- 
lc(:tionnl restri(:tions usunlly encode knowledge 
of this kind in the. fol:nl of sort hierarchies of 
worhl entii;ies. A siml)listic exmnt)le of such n 
hierm:chy is det)i('ted ill tigure 1. The hierarchy 
of tigure \] shows thnt nil lllell &lid technicians 
are 1)ersons, all 1)ersons are ~tniln;~|;e entities, all 
aninlate entities are t)\]lysi(:al ol)je(:ts, mitt so on. 
Some (1)ut not all) persons are 1)oth teehni(:ians 
:rod lnen at the same time; these t)ersons are 
nmml)ers of I;he male_tech sort. Similarly, all 
l)mlmms are edil)h; ;rod liot artifacts. No person 
is e(lil)le, because the sorts person and edible 
h~we no (:onnnon su|)sorts. 
It is, of course, extremely difficult to con- 
stru('t hierm'chies th~Lt include all the sorts of 
world entities. Ill natural  bmguage systenls that 
target sl)ecifi(: and restricted olmfins, however, 
constructing such hier;~rchies is feasible, because 
the relevant entity sorts and the possible hi- 
erarchical reb~tions between them are limited. 
In naturM lmlguage database interfimes, tbr ex- 
ample, the relevant entity sorts and the rela- 
tions between theln nre often identilied dur- 
ing the, (tesing of the database, in the tbrm 
of entity-relatiolMli 1) diagrams. We also note 
l;h;~t large-scah; smmmtic sort hierarchies are al- 
ready ill use ill artiticinl intelligence ~md natural  
17 
language gener~tion projects (tbr example, Cyc 
(Lenat, 1995) and KPML'S Upper Model (Bate- 
man, 1997)), and that the techniques that we 
discuss in this paper are in principle compatible 
with these hierarchies. 
To decide whether or not a sentence violates 
any selectional restrictions, we collect from the 
CONT and BO features of its sign ((13) in the 
case of (1)) all the single-role qfpsoas for which 
there is a sort in the hierarchy with the same 
name. (This rules out single-slot qt~)soas in- 
troduced by the CONTs of intransitive verbs.) 
The decision can then be seen as a constraint- 
satisthction problem, with the collected qfpsoas 
acting as constraints. (15) shows the constraints 
tbr (1), rewritten in a tbrm closer to predicate 
logic. HPSG indices (the boxed nmnbers) are 
used as variables. 
(15) kcybd(~\]) A man(~) A edible(~\]) 
Given two contstraints cl, c2 on the same vari- 
al)le, c~ subsumes c2 if the corresponding hier- 
archy sort of cl is an ancestor of that of c2 or 
if cl = c2. c~ and c2 can be replaced by a new 
single constraint c, if cl and c2 subsume c, and 
there is no other constraint d which is subsumed 
by cl,c2 and subsumes c. c and c' must be con- 
straints on the same variable as ct, c2, and must 
each correspond to a sort of the hierarchy. If the 
constraints of a sentence can be turned in this 
way into a tbrm where there is only one con- 
straint fbr each variable, then (and only then) 
the sentence violates no selectional restrictions. 
(15), and cdil, ,'am ot be re- 
p\]aced by a single constraint, because keybd and 
edible have no common subsorts. Hence, a se- 
lectional restriction is violated, which accounts 
for the oddity of (1). In contrast, in (2) the 
constraints would be as in (16). 
(16) banana(~) A man(m) A edible(~\]) 
banana(~\]) and cdible(F~) can now be re- 
placed by banana(F~), because both subsume 
banana(~\]), and no other constraint subsmned 
by both banana(~\]) and cdible(~) subsulnes 
banana(~). This leads to (17) and the conch> 
sion that (2) does not violate aw selectional 
restrictions. 
(17) banana(E\]) A man(D\]) 
This constraint-satisfaction reasoning, how- 
ever, requires a set)arate inferencing component 
that would be pipe-lined after the parser to rule 
out signs corresponding to sentences (or read- 
ings) that violate selectional restrictions. In the 
next section, we discuss an alternative approach 
that allows hierarchies of world entities to be 
represented using the existing HPSG framework, 
and to be exploited during parsing without an 
additional inferencing component. 
3 I ndex  subsor ts  
HPSG has already a hierarchy of feature struc- 
ture sorts (Pollard and Sag, 1994). This hierar- 
chy can be augmented to include a new part 
that encodes intbrmation about the types of 
entities that exist in the world. This can be 
achieved by partitioning the ref HPSO sort (cur- 
rently, a leaf node of the hierarchy of feature 
structures that contains all indices that refer 
to world entities) into subsorts that correst)ond 
to entity types. To encode the information of 
figure 1, rEf would have the snbsorts abstract 
and physical, physical would have the subsorts 
animate, edible, inanimate, and so on. That 
is, referential indices are partitioned into sorts, 
mid the indices of each sort can only be an- 
chored to world entities of the corresponding 
type (e.g. keybd indices can only be anchored 
to keyboards). 
With tiffs arrangement, the lexical sign for 
"ate" becomes (18). The Bo edible restriction 
of (10) has been replaced by the restriction that 
the index of the object must be of sort edible. 
(18) -PIION (at8) 
LCOVpS ( N~'CN 
L c?NTI \[I~aTEN I edible 
eat 
Similarly, the sign for "Tom" becomes (19) (cE 
(11)), and the sign for "keyboard" introduces an 
i ,dex of sort k vbd as shown in (9O) (cf. (12)). 
(19) "PITON (Tom) 
o,T no,4 \]\] 
CONT \[,mSTI~ (} J l /  
18 
\[ rm/N 1,:~fl, oa',.d) \] 
/ ' ' \[I{.I,,'STIt {} i l l  \[ ss  I J , oc  
\[cxJ BG {} JJ 
Unification of indices pro(:eeds in the, s;lille 
maturer as unificatioll of all other typed feature 
structm:es ((Jarlienter , 1!)!/2). 'Fhe parsing of 
(\]) iIOW fails, 1)ecause it al, te, nq)ts to unilly an 
il dox or (i,lt,:o,hl(::ed t/y with 
an index of so,*; t,:eybd (introduced t,y (20)), and 
no Ill'SO sorl; is sul)sumed l)y both. in (:ontrast, 
the parsing o17 (2) would su(:('eed, because the 
sign of "bmuma" would introduce an index of 
sort banana, which is a sut)sort of edible (Iigur(~ 
1); hence the two indi(:es can 1)(', ratified. (3) and 
(4) would l)e l)ro('essed sinfilarly. 
in (7) and (8), there would 1)e two lcxi(:nl 
signs for "ln'illtcr": one inl;ro(lu('ing ml index of 
sort pri'nter_pe'r.s'o'n, and one im:o(lu(:ing an in- 
dex of sort pri'nte'r_periph,(~'ral. (printe'r4)er.~'on 
and l)rinter_periph, cral would t)e daughters of 
person and art'@tel respectively in tigure 1.) 
The sign for "repairc, d", would require the index 
of its ol)je(:t to be of sort arl,'l\[fact, and l;he sign 
of "(:ail(~d" wou\](l re(tuire its sul)je('l; index to t)e 
of sort per,so'n. This (:orre(:tly admits only the 
reading where the rel)aire(l entity is a (:Omlml;er 
peripheral, ml(t l;tm (:aller is ;t t)(',rson. Simil~tr 
l l leC\] ial l isnls (;;/,ll })e llse(t to (l(~,\[;(!lTillille t;tlP, ( ;o f  
reel; reading of (9). 
With the al)proa(:h of this see, lion, it; is also 
possible to speciily seh;ctional restrictions in the 
declarations of qflIsoas in the Ill'SO hierarchy of 
feature structures, as shown in tigure 2, rather 
than in the lexi(:on. 2 When the same qft)soa is 
used in several exical signs, this saves having to 
repeat tile same, selectional restrictions in each 
one of the lexical signs. For example, the verbs 
"rq)air" and "Iix" iiiay both introduce a repair 
qfpsoa. The restriction that the repaired entity 
must be an artifact can lie sl)eeified once in the 
declaration of repair in the hierarchy of feature 
structures, rather than twice in the lexieal signs 
of ~'cl)air" and "fix". 
2Additional ayers can be included betwc,(m qfpsoa 
and the leaf sort;s, as sketched in section 8.5 of (Pollard 
and Sag, 1994), to group together qfpsoas with common 
selnalltifi roles. 
qfDso( 
\[EATI.H\[ (t,~i~r~ttt(;\] . . .  \[I~.E1)AIIIEIt l,('.7".~O,t \]
cat\[EATI,ZN edible \] ,v.pctirkRl.ZPAnU.:D artiJhct\] 
Figure 2: Declarations of qfpsoas 
4 Conc lus ions  
We have presented two met;hods to incorpo- 
rate selectional restrictions ill I lPSG: (i) express- 
ing selectional restrictions as BACKGROUND con- 
straints, and (it) enq)loying subsorts of referen- 
trial indices. The first method has the advantage 
that it requires no modification of the cmTent 
IIPSO feature structures. It also lnzdntains Pol- 
lard and Sag's distinction bel;ween "literal" mid 
"non-literal" meaning (expressed t)y CeNT and 
I~ACKGI/OUN\]) respe, ctively), a distinction whi('h 
is lflm'red in the second approach (e.g. nothing 
in (18) shows th~lt requiring the obje('t to denote 
an edil)le entity is part of the non-literal mean- 
ing; of. (10)). Unlike the tirst method, however, 
the second apt)roach re(tuires no additional ta- 
ft;renting comtionent br determining when se- 
lecl;ional restrictions h~tve been violated. With 
sentences that contain several potentially aiil- 
lfiguous words or phrases, t;11(,, second apl)roat:h 
is also more etlicienl;, ~ls it blocks signs that 
violalx', selectionnl testa'tel;ions during parsing. 
In the tirsl; aplm)ach, these signs remain un- 
detected uring parsing, and they may have a 
multiplicative ffect, h;ading to a large nmnber 
of parses, which then have to l)e checked individ- 
ually by the taft;renting component. We have 
timnd the se(:ond at)l)roach t)articularly useful 
in the develolnnent of practical systems. 
There is a deet)er question here al)out the 
proper place to maintain the kind of intbrnla- 
lion encoded in selectional restrictions. The 
applicabil ity of selectional restrictions is always 
context-dependent;  and for any selectional re- 
striction, we can ahnost always find a context 
where it does not hold. Our second method 
above effectively admits that we cromer develop 
a general tmrlIosc solution to the problem of 
meaning interprel;ation, and that we have to at- 
cept that our systems alwws operate in specific 
contexts. By committing to a particular con- 
text of interpretation, we 'compile into' what 
was tradit ional ly thought of as literal meaning a 
19 
set of contextually-determined constraints, and 
thus enable these constraints to assist in the 
HPSG language analysis without requiring an 
additional reasoning component. We take the 
view here that this latter approach is very ap- 
propriate in the construction ofreal applications 
which are, and are likely to be ibr the tbresee- 
able future, restricted to operating in limited 
domains. 
References  
J.F. Allen. 1995. Natural Language Under- 
standing. Benjamin/Cmnmings. 
H. Alshawi, editor. 1992. The Core Language 
Engine. MIT Press. 
I. Androutsopoulos, G.D. Ritchie, and 
P. Thanisch. 1998. Time, Tense and 
Aspect in Natural Lmlguage Database 
Interfaces. Natural Language Engineering, 
4(3):229-276. 
J.A. Batenlan. 1.997. Enat)ling Technology for 
Multilingual Natural Language Generation: 
the KPML Development Environment. Nat- 
ural Language Engineering, 3(1):15-55. 
B. Carpenter. 1992. The Logic of Typed Feature 
Structures. Number 32 in Canlbridge ~h'acts 
in Theoretical Computer Science. Cambridge 
University Press. 
T. Davis. 1996. Lczical Semantics and Link- 
ing in the Iticra~'chical Lezicon. Ph.D. thesis, 
Stanford University. 
D.B. Lenat. 1995. CYC: A Large-Scale Invest;- 
merit in Knowledge Infl'astructure. Cornm'a- 
nications of ACM, 38(11):33-38. 
P. Martin, D. Appelt, and F. Pereirm 1986. 
Transt)ortability and Generality in a Natural- 
Language Interface Systein. In B. Grosz, 
K. Sparek Jones, and B. Webber, editors, 
Readings in Natural Language PTvcessing, 
pages 585 593. Morgan KaufmamL 
C. Pollard and I.A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. University of 
Chicago Press and Center tbr the Study of 
Language and Information, Stanford. 
20 
In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), L. 
Lee and D. Harman (Eds.), pp. 44?50, Carnegie Mellon University, Pittsburgh, PA, USA, 2001. 
Stacking classifiers for anti-spam filtering of e-mail 
 
Georgios Sakkis?, Ion Androutsopoulos?, Georgios Paliouras?, Vangelis Karkaletsis?,  
Constantine D. Spyropoulos?, and Panagiotis Stamatopoulos? 
 
?Department of Informatics  
University of Athens 
TYPA Buildings, Panepistimiopolis  
GR-157 71 Athens, Greece 
e-mail: {stud0926, 
T.Stamatopoulos}@di.uoa.gr 
 
?Software and Knowledge Engineering 
Laboratory 
Institute of Informatics and Telecommunications 
National Centre for Scientific Research 
?Demokritos? 
GR-153 10 Ag. Paraskevi, Athens, Greece 
e-mail: {ionandr, paliourg, vangelis, 
costass}@iit.demokritos.gr 
 
 
Abstract 
We evaluate empirically a scheme for 
combining classifiers, known as stacked 
generalization, in the context of anti-spam 
filtering, a novel cost-sensitive application of 
text categorization. Unsolicited commercial e-
mail, or ?spam?, floods mailboxes, causing 
frustration, wasting bandwidth, and exposing 
minors to unsuitable content. Using a public 
corpus, we show that stacking can improve the 
efficiency of automatically induced anti-spam 
filters, and that such filters can be used in real-
life applications. 
Introduction 
This paper presents an empirical evaluation of 
stacked generalization, a scheme for combining 
automatically induced classifiers, in the context 
of anti-spam filtering, a novel cost-sensitive 
application of text categorization.  
The increasing popularity and low cost of e-
mail have intrigued direct marketers to flood the 
mailboxes of thousands of users with unsolicited 
messages, advertising anything, from vacations 
to get-rich schemes. These messages, known as 
spam or more formally Unsolicited Commercial 
E-mail, are extremely annoying, as they clutter 
mailboxes, prolong dial-up connections, and 
often expose minors to unsuitable content 
(Cranor & Lamacchia, 1998).  
Legal and simplistic technical counter-
measures, like blacklists and keyword-based 
filters, have had a very limited effect so far.1 The 
success of machine learning techniques in text 
categorization (Sebastiani, 2001) has recently 
led to alternative, learning-based approaches 
(Sahami, et al 1998; Pantel & Lin, 1998; 
Drucker, et al 1999). A classifier capable of 
distinguishing between spam and non-spam, 
hereafter legitimate, messages is induced from a 
manually categorized learning collection of 
messages, and is then used to identify incoming 
spam e-mail. Initial results have been promising, 
and experiments are becoming more systematic, 
by exploiting recently introduced benchmark 
corpora, and cost-sensitive evaluation measures 
(Gomez Hidalgo, et al 2000; Androutsopoulos, 
et al 2000a, b, c). 
Stacked generalization (Wolpert, 1992), or 
stacking, is an approach for constructing 
classifier ensembles. A classifier ensemble, or  
committee, is a set of classifiers whose 
individual decisions are combined in some way 
to classify new instances (Dietterich, 1997). 
Stacking combines multiple classifiers to induce 
a higher-level classifier with improved 
performance. The latter can be thought of as the 
president of a committee with the ground-level 
classifiers as members. Each unseen incoming 
message is first given to the members; the 
president then decides on the category of the 
                                                     
1 Consult www.cauce.org, spam.abuse.net, and 
www.junkemail.org. 
 message by considering the opinions of the 
members and the message itself.  Ground-level 
classifiers often make different classification 
errors. Hence, a president that has successfully 
learned when to trust each of the members can 
improve overall performance. 
We have experimented with two ground-
level classifiers for which results on a public 
benchmark corpus are available: a Na?ve Bayes 
classifier (Androutsopoulos, et al 2000a, c) and 
a memory-based classifier (Androutsopoulos, et 
al. 2000b; Sakkis, et al 2001). Using a third, 
memory-based classifier as president, we 
investigated two versions of stacking and two 
different cost-sensitive scenarios. Overall, our 
results indicate that stacking improves the 
performance of the ground-level classifiers, and 
that the performance of the resulting anti-spam 
filter is acceptable for real-life applications.  
Section 1 below presents the benchmark 
corpus and the preprocessing of the messages; 
section 2 introduces cost-sensitive evaluation 
measures; section 3 provides details on the 
stacking approaches that were explored; section 
4 discusses the learning algorithms that were 
employed and the motivation for selecting them; 
section 5 presents our experimental results 
followed by conclusions. 
1 Benchmark corpus and 
preprocessing  
Text categorization has benefited from public 
benchmark corpora. Producing such corpora for 
anti-spam filtering is not straightforward, since 
user mailboxes cannot be made public without 
considering privacy issues. A useful public 
approximation of a user?s mailbox, however, can 
be constructed by mixing spam messages with 
messages extracted from spam-free public 
archives of mailing lists. The corpus that we 
used, Ling-Spam, follows this approach 
(Androutsopoulos, et al 2000a, b; Sakkis, et al 
2001). It is a mixture of spam messages and 
messages sent via the Linguist, a moderated list 
about the science and profession of linguistics. 
The corpus consists of 2412 Linguist messages 
and 481 spam messages. 
Spam messages constitute 16.6% of Ling-
Spam, close to the rates reported by Cranor and 
LaMacchia (1998), and Sahami et al (1998). 
Although the Linguist messages are more topic-
specific than most users? e-mail, they are less 
standardized than one might expect. For 
example, they contain job postings, software 
availability announcements and even flame-like 
responses. Moreover, recent experiments with an 
encoded user mailbox and a Na?ve Bayes (NB) 
classifier (Androutsopoulos, et al 2000c) 
yielded results similar to those obtained with 
Ling-Spam (Androutsopoulos, et al 2000a). 
Therefore, experimentation with Ling-Spam can 
provide useful indicative results, at least in a 
preliminary stage. Furthermore, experiments 
with Ling-Spam can be seen as studies of anti-
spam filtering of open unmoderated lists.  
Each message of Ling-Spam was converted 
into a vector nxxxxx ,,,, 321 h

= , where 
nxx ,,1   are the values of attributes 
nXX ,,1 h . Each attribute shows if a particular 
word (e.g. ?adult?) occurs in the message. All 
attributes are binary: 1=iX  if the word is 
present; otherwise 0=iX . To avoid treating 
forms of the same word as different attributes, a 
lemmatizer was applied, converting each word 
to its base form. 
To reduce the dimensionality, attribute 
selection was performed. First, words occurring 
in less than 4 messages were discarded. Then, 
the Information Gain (IG) of each candidate 
attribute X  was computed: 
)()(
),(log),(),(
},{},1,0{ cPxP
cxPcxPCXIG
legitspamcx ?
?=
?
??
 
The attributes with the m highest IG-scores were 
selected, with m corresponding to the best 
configurations of the ground classifiers that have 
been reported for Ling-Spam (Androutsopoulos, 
et al 2000a; Sakkis, et al 2001); see Section 4.  
2 Evaluation measures 
Blocking a legitimate message is generally more 
severe an error than accepting a spam message. 
Let SL ?  and LS ?  denote the two error 
types, respectively, and let us assume that 
SL ?  is ? times as costly as LS ? .  
Previous research has considered three cost 
scenarios, where ? = 1, 9, or 999 
 (Androutsopoulos, et al 2000a, b, c; Sakkis, et 
al. 2001).  In the scenario where ? = 999, 
blocked messages are deleted immediately. 
SL ?  is taken to be 999 times as costly as 
LS ? , since most users would consider losing 
a legitimate message unacceptable. In the 
scenario where ? = 9, blocked messages are 
returned to their senders with a request to resend 
them to an unfiltered address. In this case, 
SL ?  is penalized more than LS ? , to 
account for the fact that recovering from a 
blocked legitimate message is more costly 
(counting the sender?s extra work) than 
recovering from a spam message that passed the 
filter (deleting it manually). In the third scenario, 
where ? = 1, blocked messages are simply 
flagged as possibly spam. Hence, SL ?  is no 
more costly than LS ? . Previous experiments 
indicate that the Na?ve Bayes ground-classifier 
is unstable when ? = 999 (Androutsopoulos, et 
al. 2000a). Hence, we have considered only the 
cases where ? = 1 or 9.  
Let )(xWL   and )(xWS   be the confidence of 
a classifier (member or president) that message 
x  is legitimate and spam, respectively. The 
classifier classifies x  as spam iff:  
?>)(
)(
xW
xW
L
S


 
If )(xWL   and )(xWS   are accurate estimates of 
)|( xlegitP   and )|( xspamP  , respectively, the 
criterion above achieves optimal results (Duda 
& Hart, 1973).  
To measure the performance of a filter, 
weighted accuracy (WAcc) and its 
complementary weighted error rate (WErr = 1 ?  
WAcc) are used (Androutsopoulos, et al 2000a, 
b, c; Sakkis, et al 2001): 
SL
SSLL
NN
NNWAcc
+??
+??
=
??  
where ZYN ?  is the number of messages in 
category Y  that the filter classified as Z ,  
SLLLL NNN ?? += ,  LSSSS NNN ?? += . 
That is, when a legitimate message is blocked, 
this counts as ? errors; and when it passes the 
filter, this counts as ? successes.  
We consider the case where no filter is 
present as our baseline: legitimate messages are 
never blocked, and spam messages always pass. 
The weighted accuracy of the baseline is: 
SL
Lb
NN
NWAcc
+??
??
=    
The total cost ratio (TCR) compares the 
performance of a filter to the baseline: 
LSSL
S
b
NN
N
WErr
WErrTCR
??
+?
==
?
 
Greater TCR values indicate better performance. 
For TCR < 1, not using the filter is better.  
Our evaluation measures also include spam 
recall (SR) and spam precision (SP): 
LSSS
SS
NN
NSR
??
?
+
=   
SLSS
SS
NN
NSP
??
?
+
=  
SR measures the percentage of spam messages 
that the filter blocks (intuitively, its 
effectiveness), while SP measures how many 
blocked messages are indeed spam (its safety). 
Despite their intuitiveness, comparing different 
filter configurations using SR and SP is difficult: 
each configuration yields a pair of SR and SP 
results; and without a single combining measure, 
like TCR, that incorporates the notion of cost, it 
is difficult to decide which pair is better. 
In all the experiments, stratified 10-fold 
cross-validation was used. That is, Ling-Spam 
was partitioned into 10 equally populated parts, 
maintaining the original spam-legitimate ratio.  
Each experiment was repeated 10 times, each 
time reserving a different part jS  (j = 1, ?, 10) 
for testing, and using the remaining 9 parts as 
the training set jL .  
3 Stacking  
In the first version of stacking that we explored 
(Wolpert, 1992), which we call cross-validation 
stacking, the training set of the president was 
prepared using a second-level 3-fold cross-
validation. Each training set jL  was further 
partitioned into three equally populated parts, 
and the training set of the president was 
prepared in three steps. At each step, a different 
part iLS  (i = 1, 2, 3) of jL  was reserved, and 
 the members were trained on the union iLL  of 
the other two parts. Each mxxx ,,1  =  of 
iLS  was enhanced with the members? 
confidence )(1 xWS
  and )(2 xWS
  that x  is spam, 
yielding an enhanced 'iLS  with vectors 
)(),(,,,' 211 xWxWxxx SSm  = . At the end of 
the 3-fold cross-validation, the president was 
trained on '''' 321 LSLSLSLj = . It was then 
tested on jS , after retraining the members on 
the entire jL  and enhancing the vectors of jS  
with the predictions of the members.   
The second stacking version that we 
explored, dubbed holdout stacking, is similar to 
Kohavi?s (1995) holdout accuracy estimation. It 
differs from the first version, in two ways: the 
members are not retrained on the entire jL ; and 
each partitioning of jL  into iLL  and iLS  leads 
to a different president, trained on 'iLS , which 
is then tested on the enhanced jS . Hence, there 
are 103?  presidents in a 10-fold experiment, 
while in the first version there are only 10. In 
each case, WAcc is averaged over the presidents, 
and TCR is reported as WErrb over the average 
WErr.    
Holdout stacking is likely to be less effective 
than cross-validation stacking, since its 
classifiers are trained on smaller sets. 
Nonetheless, it requires fewer computations, 
because the members are not retrained. 
Furthermore, during classification the president 
consults the same members that were used to 
prepare its training set. In contrast, in cross-
validation stacking the president is tested using 
members that have received more training than 
those that prepared its training set. Hence, the 
model that the president has acquired, which 
shows when to trust each member, may not 
apply to the members that the president consults 
when classifying incoming messages.   
4 Inducers employed  
As already mentioned, we used a Na?ve Bayes 
(NB) and a memory-based learner as members 
of the committee (Mitchell 1997; Aha, et al 
1991). For the latter, we used TiMBL, an 
implementation of the k-Nearest Neighbor 
algorithm (Daelemans, et al 2000).  
With NB, the degree of confidence )(xWS
  
that x  is spam is:  
== )|()( xspamPxW NBS
  
?
?
?
?
=
=
?
?
=
},{ 1
1
)|()(
)|()(
legitspamk
m
i
i
m
i
i
kxPkP
spamxPspamP
 
NB assumes that mXX ,,1   are conditionally 
independent given the category (Duda & Hart, 
1973). 
With k-NN, a distance-weighted method is 
used, with a voting function analogous to the 
inverted cube of distance (Dudani 1976). The k 
nearest neighbors ix  of x  are considered: 
?
?
=
=
?
?
= k
i
i
k
i
ii
NNk
S
xxd
xxdxCspam
xW
1
3
1
3
),(1
),())(,(
)(


 , 
where )( ixC   is the category of neighbor ix , 
),( ji xxd   is the distance between ix  and jx ,  
and 1),( 21 =cc? , if 21 cc = , and 0 otherwise.  
This formula weighs the contribution of each 
neighbor by its distance from the message to be 
classified, and the result is scaled to [0,1]. The 
distance is computed by an attribute-weighted 
function (Wettschereck, et al 1995), employing 
Information Gain (IG):  
),(),(
1
j
r
i
r
n
t
tji xxIGxxd ?
=
?? ?
 ,  
where imii xxx ,,1 l =  , jmjj xxx ,,1 l = , and 
tIG  is the IG score of tX  (Section 1). 
In Tables 1 and 2, we reproduce the best 
performing configurations of the two learners on 
Ling-Spam (Androutsopoulos, et al 2000b; 
Sakkis, et al 2001). These configurations were 
used as members of the committee. 
The same memory-based learner was used as 
the president. However, we experimented with 
several configurations, varying the 
neighborhood size (k) from 1 to 10, and 
 providing the president with the m  best word-
attributes, as in Section 1, with m  ranging from 
50 to 700 by 50. The same attribute- and 
distance-weighting schemes were used for the 
president, as with the ground-level memory-
based learner. 
 
? m SR  SP ?CR 
1 100 82.4% 99.0% 5.41 
9 100 77.6% 99.5% 3.82 
 
? k m SR  SP ?CR 
1 8 600 88.6% 97.4% 7.18 
9 2 700 81.9% 98.8% 3.64 
  
? true class 
only one 
fails both fail
Legitimate 0.66% 0.08%
Spam 12.27% 8.52%1 
All 2.59% 1.49%
Legitimate 0.33% 0.08%
Spam 19.12% 10.19%9 
All 3.46% 1.76%
 
Our motivation for combining NB with k-NN 
emerged from preliminary results indicating that 
the two ground-level learners make rather 
uncorrelated errors. Table 3 shows the average 
percentages of messages where only one, or both 
ground-level classifiers fail, per cost scenario (?) 
and message category. The figures are for the 
configurations of Tables 1 and 2. It can be seen 
that the common errors are always fewer than 
the cases where both classifiers fail. Hence, 
there is much space for improved accuracy, if a 
president can learn to select the correct member.   
5 Experimental results 
Tables 4 and 5 summarize the performance of 
the best configurations of the president in our 
experiments, for each cost scenario. Comparing 
the TCR scores in these tables with the 
corresponding scores of Tables 1 and 2 shows 
that stacking improves the performance of the 
overall filter. From the two stacking versions, 
cross-validation stacking is slightly better than 
holdout stacking. It should also be noted that 
stacking was beneficial for most of the 
configurations of the president that we tested, 
i.e. most sub-optimal presidents outperformed 
the best configurations of the members. This is 
encouraging, since the optimum configuration is 
often hard to determine a priori, and may vary 
from one user to the other.   
  
? k m SR SP ?CR
1 5 100 91.7% 96.5% 8.44 
9 3 200 84.2% 98.9% 3.98 
 
? k m SR SP ?CR
1 7 300 89.6% 98.7% 8.60 
9 3 100 84.8% 98.8% 4.08 
 
There was one interesting exception in the 
positive impact of stacking. The 1-NN and 2-NN 
(k = 1, 2) presidents were substantially worse 
than the other k-NN presidents, often performing 
worse than the ground-level classifiers. We 
witnessed this behavior in both cost scenarios, 
and with most values of m (number of 
attributes). In a ?postmortem? analysis, we 
ascertained that most messages misclassified by 
1-NN and 2-NN, but not the other presidents, are 
legitimate, with their nearest neighbor being 
spam. Therefore, the additional errors of 1-NN 
and 2-NN, compared to the other presidents, are 
of the SL ?  type. Interestingly, in most of 
Table 2: Best configurations of k-NN per usage
scenario and the corresponding performance. 
Table 1: Best configurations of NB per usage
scenario and the corresponding performance. 
Table 5: Best configurations of cross-validation 
stacking per usage scenario and the 
corresponding performance. 
Table 4: Best configurations of holdout 
stacking per usage scenario and the 
corresponding performance. 
Table 3: Analysis of the common errors of the
best configurations of NB and k-NN per
scenario (?) and message class. 
 those cases, both members of the committee 
classify the instance correctly, as legitimate. 
This is an indication, that for small values of the 
parameter k the additional two features, i.e., the 
members? confidence )(1 xWS
  and )(2 xWS
 , do 
not enhance but distort the representation of 
instances. As a result, the close neighborhood of 
the unclassified instance is not a legitimate, but a 
spam e-mail. This behavior of the memory-
based classifier is also noted in (Sakkis, et al 
2001). The suggested solution there was to use a 
larger value for k, combined with a strong 
distance weighting function, such as the one 
presented in section 4. 
Conclusion 
In this paper we adopted a stacked 
generalization approach to anti-spam filtering, 
and evaluated its performance. The 
configuration that we examined combined a 
memory-based and a Na?ve Bayes classifier in a 
two-member committee, in which another 
memory-based classifier presided. The 
classifiers that we chose as members of the 
committee have been evaluated individually on 
the same data as in our evaluation, i.e. the Ling-
Spam corpus. The results of these earlier studies 
were used as a basis for comparing the 
performance of our method.  
Our experiments, using two different 
approaches to stacking and two different 
misclassification cost scenarios, show that 
stacking consistently improves the performance 
of anti-spam filtering. This is explained by the 
fact that the two members of the committee 
disagree more often than agreeing in their 
misclassification errors. Thus, the president is 
able to improve the overall performance of the 
filter, by choosing the right member?s decision 
when they disagree. 
The results presented here motivate further 
work in the same direction. In particular, we are 
interested in combining more classifiers, such as 
decision trees (Quinlan, 1993) and support 
vector machines (Drucker, et al 1999), within 
the stacking framework. A larger variety of 
classifiers is expected to lead the president to 
more informed decisions, resulting in further 
improvement of the filter?s performance. 
Furthermore, we would like to evaluate other 
classifiers in the role of the president. Finally, it 
would be interesting to compare the 
performance of the stacked generalization 
approach to other multi-classifier methods, such 
as boosting (Schapire & Singer, 2000). 
References  
Aha, W. D., Kibler D., and Albert, M.K., (1991) 
Instance-Based Learning Algorithms. ?Machine 
Learning?, Vol. 6, pp. 37?66. 
Androutsopoulos, I., Koutsias, J., Chandrinos, K.V., 
Paliouras, G., and Spyropoulos, C.D. (2000a) ?An 
evaluation of na?ve Bayesian anti-spam filtering?. 
In Proceedings of the Workshop on Machine 
Learning in the New Information Age, 11th 
European Conference on Machine Learning 
(ECML 2000), Barcelona, Spain, pp. 9?17. 
Androutsopoulos, I., Paliouras, G., Karkaletsis, V., 
Sakkis, G., Spyropoulos, C.D., and Stamatopoulos, 
P. (2000b). ?Learning to filter spam e-mail: a 
comparison of a na?ve Bayesian and a memory-
based approach?. In Proceedings of the Workshop 
on Machine Learning and Textual Information 
Access, PKDD 2000, Lyon, France, pp. 1? 3. 
Androutsopoulos, I, Koutsias, J, Chandrinos, K.V., 
and Spyropoulos, C.D. (2000c) ?An experimental 
comparison of na?ve Bayesian and keyword-based 
anti-spam filtering with encrypted personal e-mail 
messages?. In Proceedings of SIGIR 2000, Athens, 
Greece, pp. 160?167. 
Cranor, L.F., and LaMacchia, B.A. (1998). ?Spam!?, 
Communications of ACM, 41(8):74?83.  
Daelemans, W., Zavrel, J., van der Sloot, K., and van 
den Bosch, A. (2000) TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. ILK, 
Computational Linguistics, Tilburg University. 
http:/ilk.kub.nl/~ilk/papers. 
Dietterich, G. T. (1997). ?Machine Learning 
Research: Four Current Directions?. AI Magazine 
18(4):97-136.     
Drucker, H. D. ,Wu, D., and Vapnik V. (1999). 
?Support Vector Machines for Spam 
Categorization?. IEEE Transactions On Neural 
Networks, 10(5). 
Duda, R.O, and Hart, P.E. (1973). ?Bayes decision 
theory?. Chapter 2 in Pattern Classification and 
Scene Analysis, pp. 10?43, John Wiley. 
Dudani, A. S. (1976). ?The distance-weighted k-
nearest neighbor rule?. IEEE Transactions on 
Systems, Man and Cybernetics, 6(4):325?327. 
G?mez Hidalgo, J.M., Ma?a L?p?z, M., and Puertas 
Sanz, E. (2000). ?Combining text and heuristics for 
 cost-sensitive spam filtering?. In Proceedings of 
the 4th Computational Natural Language Learning 
Workshop, CoNLL-2000, Lisbon, Portugal, pp. 99?
102.   
Kohavi, R. (1995). ?A study of cross-validation and 
bootstrap for accuracy estimation and model 
selection?. In Proceedings of the 12th International 
Joint Conference on Artificial Intelligence (IJCAI-
1995), Morgan Kaufmann, pp. 1137?1143.  
Mitchell, T.M. (1997). Machine Learning. McGraw-
Hill. 
Pantel, P., and Lin, D. (1998). ?SpamCop: a spam 
classification and organization program?. In 
Learning for Text Categorization ? Papers from 
the AAAI Workshop, pp. 95?98, Madison 
Wisconsin. AAAI Technical Report WS-98-05. 
Quinlan, J.R. (1993). C4.5: Programs for Machine 
Learning, Morgan Kaufmann, San Mateo, 
California.  
Sahami, M., Dumais, S., Heckerman D., and Horvitz, 
E. (1998). ?A Bayesian approach to filtering junk 
e-mail?. In Learning for Text Categorization ? 
Papers from the AAAI Workshop, pp. 55?62, 
Madison Wisconsin. AAAI Technical Report WS-
98-05. 
Sakkis, G., Androutsopoulos, I., Paliouras, G., 
Karkaletsis, V., Spyropoulos, C.D., and 
Stamatopoulos, P. (2001) ?A memory-based 
approach to anti-spam filtering?. NCSR 
?Demokritos? Technical Report, Athens, Greece. 
Schapire, R.E., and Singer, Y. (2000). ?BoosTexter: a 
boosting-based system for text categorization?. 
Machine Learning, 39(2/3):135?168. 
Sebastiani, F. (2001). Machine Learning in 
Automated Text Categorization. Revised version of 
Technical Report IEI-B4-31-1999, Istituto di 
Elaborazione dell?Informazione, Consiglio 
Nazionale delle Ricerche, Pisa, Italy. 
Wettschereck, D., Aha, W. D., and Mohri, T. (1995). 
A Review and Comparative Evaluation of Feature 
Weighting Methods for Lazy Learning Algorithms. 
Technical Report AIC-95-012, Naval Research 
Laboratory, Navy Center for Applied Research in 
Artificial Intelligence, Washington, D.C. 
Wolpert, D. (1992). ?Stacked Generalization?. 
Neural Networks, 5(2):241?260. 
Learning to Identify Single-Snippet Answers to Definition Questions 
Spyridoula MILIARAKI and Ion ANDROUTSOPOULOS 
Department of Informatics, Athens University of Economics and Business  
Patission 76, GR-104 34, Athens, Greece 
 
Abstract 
We present a learning-based method to identify 
single-snippet answers to definition questions in 
question answering systems for document 
collections. Our method combines and extends two 
previous techniques that were based mostly on 
manually crafted lexical patterns and WordNet 
hypernyms. We train a Support Vector Machine 
(SVM) on vectors comprising the verdicts or 
attributes of the previous techniques, and additional 
phrasal attributes that we acquire automatically. 
The SVM is then used to identify and rank single 
250-character snippets that contain answers to 
definition questions. Experimental results indicate 
that our method clearly outperforms the techniques 
it builds upon. 
1 Introduction 
Since the introduction of the TREC QA track 
(Voorhees, 2001), question answering systems for 
document collections have attracted a lot of 
attention. The goal is to return from the collection 
text snippets (eg., 50 or 250 characters long) or 
exact answers (e.g., names, dates) that answer 
natural language questions submitted by users. 
A typical system first classifies the question into 
one of several categories (questions asking for 
locations, persons, etc.), producing expectations of 
types of named entities that must be present in the 
answer (location names, person names, etc.). Using 
the question terms as a query, an information 
retrieval (IR) system identifies possibly relevant 
passages in the collection, often after query 
expansion (e.g., adding synonyms). Snippets of 
these passages are then selected and ranked, based 
on criteria such as whether or not they contain the 
expected types of named entities, the percentage of 
question words in each snippet, the percentage of 
words that also occur in other candidate snippets, 
etc. The system reports the most highly-ranked 
snippets, or, in the case of exact answers, named 
entities of the required type therein. 
Unfortunately, the approach highlighted above 
falls short with questions that do not generate 
expectations of particular types of named entities 
and contain very few non-stop-words. Definition 
questions (e.g. ?What is a nanometer??, ?Who was 
Duke Ellington??) have both properties, and are 
particularly common. In TREC-2001, where the 
distribution of question types reflected that of real 
user logs, 27% of the questions were requests for 
definitions. Hence, techniques to handle this 
category of questions are very important. 
We propose a new method to answer definition 
questions, that combines and extends the technique 
of Prager et al (2001, 2002), which relied on 
WordNet hypernyms, and that of Joho et al (2001, 
2002), which relied on manually crafted lexical 
patterns, sentence position, and word co-
occurrence across candidate answers. We train an 
SVM (Sch?lkopf and Smola, 2002) on vectors 
whose attributes include the verdict of Prager et 
al.?s method, the attributes of Joho et al, and 
additional phrasal attributes that we acquire 
automatically. The SVM is then used to identify 
and rank 250-character snippets, each intended to 
contain a stand-alone definition of a given term, 
much as in TREC QA tasks prior to 2003. 
In TREC-2003, the answers to definition 
questions had to be lists of complementary 
snippets (Voorhees, 2003), as opposed to single-
snippet definitions. Here, we focus on the pre-2003 
task, for which TREC data were publicly available 
during our work. We believe that this task is still 
interesting and of practical use. For example, a list 
of single-snippet definitions accompanied by their 
source URLs can be a good starting point for users 
of search engines wishing to find definitions. 
Single-snippet definitions can also be useful in 
information extraction, where the templates to be 
filled in often require short entity descriptions; see 
Radev and McKeown (1997). Experiments indicate 
that our method clearly outperforms the techniques 
it builds upon in the task we considered. We sketch 
in section 6 how we plan to adapt our method to 
the post-2003 TREC task.  
2 Previous techniques 
Prager et al (2001, 2002) observe that definition 
questions can often be answered by hypernyms; for 
example, ?schizophrenia? is a ?mental illness?, 
where the latter is a hypernym of the former in 
WordNet. Deciding which hypernym to report, 
however, is not trivial. To use an example of 
Prager et al, in ?What is a meerkat?? WordNet 
provides hypernym synsets such as {?viverrine?, 
?viverrine mammal?} (level 1), {?carnivore?} 
(level 2), {?placental?, ?} (level 3), {?mammal?} 
(level 4), up to {?entity?, ?something?} (level 9). 
In a neutral context, the most natural response is 
arguably ?mammal? or ?animal?. A hypernym on 
its own may also not be a satisfactory answer. 
Responding that an amphibian is an animal is less 
satisfactory than saying it is an animal that lives 
both on land and in water.  
Prager et al identify the best hypernyms by 
counting how often they co-occur with the 
definition term in two-sentence passages of the 
document collection. They then short-list the 
passages that contain both the term and any of the 
best hypernyms, and rank them using measures 
similar to those of Radev et al (2000). More 
precisely, given a term to define, they compute the 
level-adapted count (LAC) of each of its 
hypernyms, defined as the number of two-sentence 
passages where the hypernym co-occurs with the 
term divided by the distance between the term and 
the hypernym in WordNet?s hierarchy. They then 
retain the hypernym with the largest LAC, and all 
the hypernyms whose LAC is within a 20% margin 
from the largest one. To avoid very general 
hypernyms (e.g., {?entity?, ?something?}), Prager 
et al discard the hypernyms of the highest 1 or 2 
levels in WordNet?s trees, if the distance from the 
top of the tree to the definition term is up to 3 or 5, 
respectively; if the distance is longer, they discard 
the top three levels. This ceiling is raised gradually 
if no co-occurring hypernym is found.  
Prager et al?s method performed well with the 
definition questions and documents of TREC-9 
(Prager et al, 2001). In 20 out of the 24 definition 
questions they considered (83.3%), it managed to 
return at least one correct response in the five most 
highly ranked two-sentence passages; in all 20 
questions, the correct response was actually the 
highest ranked. In TREC-2001, however, where 
there were more definition questions mirroring 
more directly real user queries, this percentage 
dropped to 46% (Prager et al, 2002). In 44 of the 
130 definition questions (33.8%) they considered, 
WordNet did not help at all, because it either did 
not contain the term whose definition was sought, 
or none of its hypernyms were useful even as 
partial definitions. Even when WordNet contained 
a hypernym that constituted a self-contained 
definition, it was not always selected.  
In work that led to an alternative method, Hearst 
(1998) sketched a method to identify patterns that 
signal particular lexical semantic relations, in 
effect a bootstrapping approach. Applying this 
process by hand, Hearst was able to identify four 
high precision hyponym?hypernym patterns that 
are common across text genres. The patterns are 
shown below in the slightly modified form of Joho 
and Sanderson (2000). Here, qn (query noun) and 
dp (descriptive phrase) are phrases containing 
hyponyms and hypernyms, respectively.  
 
(1) (dp such | such dp) as qn 
e.g., ?injuries such as broken bones? 
(2) qn (and | or) other dp 
e.g., ?broken bones and other injuries? 
(3) dp especially qn 
e.g., ?injuries especially broken bones? 
(4) dp including qn 
e.g., ?European countries including England, 
France, and Spain? 
 
Compared to Prager et al?s method, Hearst?s 
patterns have the advantage that they may identify 
hyponym?hypernym relations that are not present 
in WordNet, which is often the case with domain-
specific terminology and proper names. 
Joho and Sanderson (2000) observe that co-
occurrences of hyponyms and hypernyms are often 
indicative of contexts that define the hyponyms. 
Instead of using WordNet, they identify hyponym?
hypernym contexts using Hearst?s patterns, to 
which they add the following ones.1 Here, qn is the 
term to define, and dp is a phrase that contains a 
definition of qn; dp no longer necessarily contains 
a hypernym of qn. Each pattern is assigned a 
weight, which is its precision on a training set (the 
number of sentences it correctly identifies as 
containing a definition, over the number of 
sentences it matches). 
 
(5) qn ?(? dp ?)? | ?(? dp ?)? qn 
e.g., ?MP (Member of Parliament)? 
(6) qn (is | was | are | were) (a | an | the) dp 
e.g., ?Tony Blair is a politician? 
(7) qn , (a | an | the) dp 
e.g., ?Tony Blair, the politician? 
(8) qn , which (is | was | are | were) dp 
e.g., ?bronchitis, which is a disease of?? 
(9) qn  , dp , (is | was | are | were) 
e.g., ?Blair, Prime Minister of Britain, is?? 
 
Joho and Sanderson first locate all the sentences 
of the document collection that contain the term to 
define, and then rank them using three attributes. 
The first one (KPW) is the weight of the pattern the 
sentence matched, if any. The second attribute 
(SN) is the ordinal number of the sentence in its 
document, ignoring sentences that do not contain 
the term; sentences that mention first the term in a 
document are more likely to define it. The third 
attribute (WC) shows how many of the words that 
                                                     
1 We ignore a variant of (7) that ends in ?.?, ??? or ?!?. 
are common across candidate answers are present 
in the sentence. More precisely, to compute WC, 
Joho and Sanderson retrieve the first sentence of 
each document that contains the definition term, 
and retain the 20 most frequent words of these 
sentences after applying a stop-list and a 
lemmatizer. WC is the percentage of these words 
that are present in the sentence being ranked. The 
sentences are ranked using the weighted sum of the 
three attributes, after hand-tuning the weights of 
the sum on a training set. In Joho et al (2001), this 
method was evaluated with 50 definition questions 
and the top 600 documents that Google returned 
for each definition term. It returned a correct 
definition in the five top-ranked sentences in 66% 
of the questions.2 As with Prager et al?s method, 
then, there is scope for improvement. 
3 Our method 
Prager et al?s approach is capable of answering 
a large number of definition questions, though, as 
discussed above, it does not always manage to 
locate the most appropriate hypernym, and the 
appropriate hyponym-hypernym relation may not 
be present in WordNet. Joho et al?s technique 
does not rely on a predetermined ontology, which 
makes it less vulnerable to domain-specific 
terminology and proper names. Its limited pattern 
set, however, cannot capture definitions that are 
phrased in less typical ways, and the fact that the 
weights of the three attributes are hand-tuned 
raises doubts as to whether they are optimal. Our 
method overcomes these limitations by combining 
the two approaches in a common machine learning 
framework, and by using a larger attribute set. 
We assume that a question processing module 
that separates definition from other types of 
questions is available, and that in each definition 
question it also identifies the term to be defined. 
When such a module is not available, the user can 
be asked to specify explicitly the question type and 
the term to be defined, via a form-based interface 
as in Buchholtz and Daelemans (2001). Hence, the 
input to our method is a (possibly multi-word) 
term, along with the most highly ranked documents 
that an IR engine returned for that term (in the 
experiments of section 4, we used the top 50 
documents). The goal is to identify five 250-
character snippets in these documents, such that at 
least one of the snippets contains an acceptable 
definition of the input term. As an example, we 
show below the two snippets that configuration 4 
of our method (to be discussed) considered most 
                                                     
2 Joho et al report better results when using a less stringent 
form of evaluation that admits partial answers, but their 
acceptance criteria in that case appear to be over-permissive. 
appropriate in the case of ?What are pathogens??. 
The first snippet defines pathogens as hazardous 
microorganisms. The second one provides 
instances of pathogens, but does not actually state 
what a pathogen is.  
 
?considerations as nutrient availability. In particular, 
the panel concluded that the fear of creating hazardous 
microorganisms, or pathogens, is overstated. "It is 
highly unlikely that moving one or a few genes from a 
pathogen to? 
?definite intraspecial physiological and morphological 
diversity. Ph. helianthi thrives at higher temperatures 
than other sunflower pathogens (Sclerotinia 
sclerotiorum and Botrytis cinerea) do. In various 
nutrient media, Ph. helianthi ? 
 
We select the five snippets to report using 
alternatively a baseline and four different 
configurations of our learning-based method.  
3.1 Baseline 
As a baseline, we use a reimplementation of 
Prager et al?s method that operates with 250-
character snippets.3 Unlike Prager et al, our 
reimplementation does not use a ranking function 
(Radev et al, 2000). When more than five snippets 
contain both the input term and one of its best 
hypernyms, we rank the snippets according to the 
ranking (RK) of the documents they derive from, 
i.e., the ranking of the IR engine. Our evaluation 
results (section 4) indicate that the performance of 
our implementation is still very similar to that of 
Prager et al 
3.2 Learning-based method 
In all the configurations of our learning-based 
method, we use a Support Vector Machine (SVM) 
with a simple inner product (polynomial of first 
degree) kernel (Sch?lkopf and Smola, 2002), 
which in effect learns an optimal linear classifier 
without moving to a higher-dimension space. (We 
have experimented with higher degree polynomial 
kernels, but there was no sign of improvement.) 
The SVM is trained as follows. Given a training set 
of terms to be defined and the corresponding 
documents that the IR engine returned, we collect 
from the documents all the 250-character snippets 
                                                     
3 Following Prager et al (2002), we consider 
synonyms of the input term as level-0 hypernyms, and 
include them in the search for best hypernyms; their 
LAC is the number of times they co-occur with the 
input term. We did not implement the tests for 
orthographic variations and count ratios of Prager et al 
When an input term occurs in multiple synsets, which 
produces multiple paths towards hypernyms, we select 
the hypernyms with the best overall LAC scores, instead 
of the best scores per path, unlike Prager et al (2001). 
that have the term at their center. Each snippet is 
then converted to a training vector, the attributes of 
which differ across the configurations presented 
below. The training vectors are manually classified 
in two categories, depending on whether or not the 
corresponding snippets contain acceptable 
definitions. The SVM is trained on these vectors to 
predict the category of new, unseen vectors from 
their attributes. 
The SVM implementation we used actually 
returns confidence scores, showing how probable it 
is that a particular vector belongs to each 
category.4 Using a classifier that returns confidence 
scores instead of binary decisions is crucial, 
because the training vectors that correspond to 
definitions are much fewer than the vectors for 
non-definitions (3004 vs. 15469 in our dataset of 
section 4). As a result, the induced classifier is 
biased towards non-definitions, and, hence, most 
unseen vectors receive higher confidence scores 
for the category of non-definitions than for the 
category of definitions. We do not compare the 
two scores. We pick the five vectors whose 
confidence score for the category of definitions is 
highest, and report the corresponding snippets; in 
effect, we use the SVM as a ranker, rather than a 
classifier; see also Ravichandran et al (2003). The 
imbalance between the two categories can be 
reduced by considering (during both training and 
classification) only the first three snippets of each 
document, which discards mostly non-definitions. 
3.2.1 Configuration 1: attributes of Joho at al. 
In the first configuration of our learning-based 
approach, the attributes of the vectors are roughly 
those of Joho et al: two numeric attributes for SN 
and WC (section 2), and a binary attribute for each 
one of patterns (1)?(9) showing if the pattern is 
satisfied. We have also added binary attributes for 
the following manually crafted patterns, and a 
numeric attribute for RK (section 3.1). 
(10) dp like qn 
e.g., ?antibiotics like amoxicillin? 
(11) qn or dp 
e.g., ?autism or some other type of disorder? 
(12) qn (can | refer | have) dp 
e.g., ?amphibians can live on land and?? 
(13) dp (called | known as | defined) qn 
e.g., ?the giant wave known as tsunami? 
 
This configuration can be seen as an 
approximation of Joho et al?s method, although it 
is actually an improvement, because of the extra 
attributes and the fact that it uses an SVM learner 
                                                     
4 We used Weka?s SVM implementation. Weka is 
available from http://www.cs.waikato.ac.nz/ml/weka/ . 
instead of a weighted sum with hand-tuned weights 
to combine the attributes.  
3.2.2 Configuration 2: adding WordNet 
The second configuration is identical to the first 
one, except that it adds an extra binary attribute 
showing if the snippet contains one of its best 
hypernyms, as returned by the baseline. This is 
essentially a combination of the approaches of 
Prager et al and Joho et al Unlike simple voting 
(Chu-Carroll et al, 2003), the two methods 
contribute attributes to the instance representations 
(vectors) of an overall learner (the SVM). This 
allows the overall learner to assess their reliability 
and adjust their weights accordingly. 
3.2.3 Configuration 3: n-gram attributes 
The third configuration adds m extra binary 
attributes, each corresponding to an automatically 
acquired pattern. (In the experiments of section 4, 
m ranges from 100 to 300.) Observing that most of 
the previously proposed patterns are anchored at 
the term to define, we collect from the documents 
that the IR engine returns for the training questions 
all the n-grams (n ? {1, 2, 3}) of tokens that occur 
immediately before or after the definition term. 
The n-grams that are encountered at least 10 times 
are considered candidate patterns. From these, we 
select the m patterns with the highest precision 
scores, where precision is defined as in section 2, 
but for snippets instead of sentences. 
In our experiments, this pattern acquisition 
process re-discovered several of the patterns (1)?
(13) or sub-expressions of them, namely [dp such 
as qn], [qn and other dp], [qn ?(? dp], [dp ?)? qn], 
[qn is (a | an | the) dp], [qn (are | were) dp], [qn , (a 
| an | the) dp), [qn , which is dp], [qn , dp], [qn or 
dp], [qn can dp], [dp called qn], [dp known as qn]. 
It also discovered some reasonable variations of 
patterns (1)?(13); e.g, [qn is one dp], [dp , (a | an) 
qn] (as in ?A sudden health problem, a heart attack 
or ??), [dp , qn], [qn , or dp]. We include dp, the 
phrase that defines qn, in the acquired patterns to 
make them easier to compare to patterns (1)?(13). 
The acquired patterns, however, do not predict the 
position of dp in a snippet. 
Many of the acquired patterns at first look odd, 
but under a closer examination turn out to be 
reasonable. For example, definition sentences often 
start with the term they define, as in ?Amphibians 
are??, ?An alligator is??, sometimes with the 
term quoted, which is how patterns like [. qn], [. 
An qn], [. ' ' qn] arise.  Many of the questions in 
our data set were about diseases, and, hence, 
several of the acquired patterns were expressions 
(e.g,, ?people with?, ?symptoms of?) that co-occur 
frequently with definitions of diseases in snippets. 
This suggests that automatic pattern acquisition 
would allow domain-specific question answering 
systems (e.g., systems for medical documents) to 
exploit domain-specific indicators of definitions.  
The pattern acquisition process also produced 
many patterns (e.g., ?the hallucinogenic drug?,        
?President Rafael?) that do not seem to provide 
any useful information in general, although they 
occurred frequently in definition snippets of our 
training data. Since we did not filter manually the 
automatically acquired patterns, patterns of the 
latter kind gave rise to irrelevant attributes that 
carried noise. This is a common problem in 
machine learning, which most learning algorithms 
are designed to cope with. Our experimental results 
indicate that the SVM learner benefits from the n-
gram attributes, despite the noise they introduce. 
We also experimented with keeping both high 
and low precision patterns, hoping to acquire both 
n-grams that are indicative of definitions and n-
grams that are indicative of contexts that do not 
constitute definitions. The experimental results, 
however, were inferior, and manual inspection of 
the low precision n-grams indicated that they 
carried mostly noise, suggesting that it is difficult 
to identify frequent n-grams whose presence rules 
out definitions reliably. 
The reader may wonder why we rely only on 
precision, rather than selecting also attributes with 
high recall. (Here, recall is the number of snippets 
the pattern correctly identifies as containing a 
definition, over the total number of snippets that 
contain a definition.) In multi-source document 
collections, like the Web or the TREC documents, 
one can expect to find several definitions of the 
same term, phrased in different ways. Unlike 
traditional document retrieval tasks, we are not 
interested in obtaining all the definitions; a single 
correct one suffices. Furthermore, as the number of 
snippets that can be reported is limited, we need to 
be confident that the snippets we return are indeed 
definitions. Hence, we need to rely on high-
precision indicators. Preliminary experiments we 
performed using the F-measure (with ?=1), a 
combination of recall and precision, instead of 
precision led to inferior results, confirming that 
attribute recall is not helpful in this task. 
We have also experimented with information 
gain. In that case, one selects the n-grams with the 
m highest IG(C,X) scores, defined below, where C 
and X are random variables denoting the category 
of a snippet and the value of the n-gram?s binary 
attribute, respectively, and H(C) and H(C|X) are 
the entropy and conditional entropy of C. By 
selecting the attributes with the highest information 
gain scores, one selects the attributes that carry 
most information about the value of C. 
)|()()(),(
}1,0{
xXCHxXPCHXCIG
x
=?=?= ?
?
   Although information gain is one of the best 
attribute selection measures in text categorization 
(Yang and Pedersen, 1997), in our case it led to 
very few attributes with non-zero IG(C,X) scores 
(around 90 attributes from the entire dataset of 
section 4). This is because most of the n-grams are 
very rare (i.e., P(X=0) is very large), and their 
absence (X=0) provides very little information 
about C (i.e., H(C) ? H(C | X = 0)). For example, 
not encountering [dp such as qn] provides very 
little information on whether or not the snippet is a 
definition. The experiments we performed with the 
resulting attributes led to inferior results, compared 
to those that we got via precision. 
3.2.4 Configuration 4: discarding WordNet 
The fourth configuration is identical to the third 
one, except that it does not use the attribute that 
shows if the snippet contains one of the best 
hypernyms of Prager et al (The attribute is present 
in configurations 2 and 3). The intention is to 
explore if the performance of configuration 3 can 
be sustained without the use of WordNet.  
4 Experimental results 
We evaluated the baseline and the machine 
learning configurations of section 3 on the 
definition questions of TREC-9 (2000) and TREC-
2001, the same data used by Prager et al For each 
question, the TREC organizers provide the 50 most 
highly ranked documents that an IR engine 
returned from the TREC documents. The task is to 
return for each question five 250-character snippets 
from the corresponding 50 documents, such that at 
least one of the snippets contains an acceptable 
definition. Following Prager et al, we count a 
snippet as containing an acceptable definition, if it 
satisfies the Perl answer patterns that the TREC 
organizers provide for the corresponding question 
(Voorhees, 2001).  The answer patterns incorporate 
the correct responses of all the participants of the 
corresponding TREC competition. In TREC-9, the 
correlation between system rankings produced by 
answer patterns and rankings produced by humans 
was at the same level as the average correlation 
between rankings of different human assessors 
(Voorhees and Tice 2000). In TREC-2001, the 
correlation between patterns and judges was lower, 
but still similar for 250-character responses 
(Voorhees, 2001). 
All the experiments with machine learning 
configurations were performed with 10-fold cross-
validation. That is, the question set was divided 
into 10 parts, and each experiment was repeated 10 
times. At each iteration, the questions of a different 
part and the corresponding documents were 
reserved for testing, while the questions and 
documents of the remaining nine parts were used 
for training. (In configurations 3 and 4, pattern 
acquisition was repeated at each iteration.) Table 1 
reports the total number of questions that each 
method managed to handle successfully over the 
ten iterations; i.e., questions with at least one 
acceptable definition in the five returned snippets. 
When questions for which there was no answer in 
the corresponding 50 documents and/or there was 
no answer pattern are excluded, the results are 
those shown in italics. The second row contains the 
results reported by Prager et al (2001, 2002), 
while the third one shows the results of our 
reimplementation. We include six questions that 
Prager et al appear to have excluded, which is 
why the total number of questions is different; 
there are also some implementation differences, as 
discussed in section 3. 
 
Method % questions handled correctly 
Prager et al 51.95 (80/154), 60.15 (80/133) 
baseline 50.00 (80/160), 58.39 (80/137) 
config. 1 61.88 (99/160), 72.26 (99/137) 
config. 2 63.13 (101/160), 73.72 (101/137) 
config. 3 72.50 (116/160), 84.67 (116/137) 
config. 4 71.88 (115/160), 83.94 (115/137) 
Table 1: Results on TREC-9 & TREC-2001 data 
The SVM learner with roughly Joho et al?s 
attributes (config. 1) clearly outperforms Prager et 
al.?s WordNet-based method. Adding Prager et 
al.?s method as an extra attribute to the SVM 
(config. 2) leads to only a marginal improvement. 
Automatic pattern acquisition (config. 3) is much 
more beneficial. Removing the attribute of the 
WordNet-based method (config. 4) caused the 
system to fail in only one of the questions that 
configuration 3 handled successfully, which again 
suggests that the WordNet-based method does not 
contribute much to the performance of 
configuration 3. This is particularly interesting for 
languages with no WordNet-like resources.  
The results of configurations 3 and 4 in table 1 
were obtained using the 200 n-gram attributes with 
the highest precision scores. When using the 100 
n-gram attributes with the highest and the 100 n-
gram attributes with the lowest precision scores, 
the results of configuration 3 were 70.63% and 
82.48%. When using all the n-gram attributes with 
non-zero information gain scores, the results of 
configuration 3 were 66.25% and 77.42%.  
Configurations 2 and 3 achieved inferior results 
with 300 highest-precision n-gram attributes (table 
2), which may be a sign that low reliability n-
grams are beginning to dominate the attribute set. 
 
n-grams config. 3 (%) config. 4 (%) 
100 68.13, 79.56 70.00, 81.75 
200 72.50, 84.67 71.88, 83.94 
300 68.75, 80.29 71.25, 83.21 
Table 2: Results for variable number of n-grams 
5 Related work 
Ng et al (2000) use machine learning (C5 with 
boosting) to classify and rank candidate answers, 
but do not treat definition questions in any special 
way, and use only four generic attributes across all 
question categories. Some of their worst results are 
for ?What ??? questions, that presumably include 
a large number of definition questions.  
Ittycheriah and Roukos (2002) employ a 
maximum entropy model to rank candidate 
answers, which uses a very rich set of attributes 
that includes 8,500 patterns. The latter are n-grams 
of words that occur frequently in answers of the 
training data, each associated with a two-word 
question prefix (e.g., ?What is?) that also has to be 
matched for the pattern to be satisfied. Unlike our 
work, the n-grams have to be five or more words 
long, and, in the case of definition questions, they 
do not need to be anchored at the term to define. 
Ittycheriah and Roukos (2002) do not provide 
separate figures on the performance of their system 
on definition questions. 
Blair-Goldensohn et al (2003) focus on 
definition questions, but aim at producing coherent 
multi-sentence definitions, rather than identifying 
single defining snippets. At the heart of their 
approach is a component that uses machine 
learning (Riper) to identify sentences that are 
candidates for inclusion in the multi-sentence 
definition. This component plays a role similar to 
that of our SVM learner, but it is intended to admit 
a larger range of sentences, and appears to employ 
only attributes conveying the position of the 
sentence in its document and the frequency of the 
definition term in the context of the sentence. 
Automatically acquired n-gram patterns can also 
be used for query expansion in information 
retrieval, as in Agichtein et al (2001). 
6 Conclusions and future work 
We have presented a new method to identify 
single-snippet definitions in question answering 
systems. Our method combines previously 
proposed techniques as attributes of an SVM 
learner, to which an automatic pattern acquisition 
process contributes additional attributes. We have 
evaluated several configurations of our method on 
TREC data, with results indicating it outperforms 
previous techniques.  
The performance of our method may improve if 
n-grams that start or end within a margin of a few 
tokens from the term to define are added. This may 
allow definitions like ?X, that Y defined as ?? to 
be found. Further improvements may be possible 
by using a sentence splitter instead of windows of 
fixed length, anaphora resolution, clustering of 
similar snippets to avoid ranking them separately, 
and identifying additional n-gram attributes by 
bootstrapping (Ravichandran et al 2003). 
We believe that it is possible to address the post-
2003 TREC task for definition questions with the 
same approach, but training the SVM learner to 
identify snippets that should be included in multi-
snippet definitions. With sufficient training, we 
expect that n-grams indicative of information 
commonly included in multi-snippet definitions 
(e.g., dates of birth, important works for persons) 
will be discovered. Larger amounts of training 
data, however, will be required. We are currently 
working on a method to generate training examples 
in an unsupervised manner from parallel texts. 
References  
E. Agichtein, S. Lawrence, and L. Gravano. 2001. 
Learning Search Engine Specific Query 
Transformations for Question Answering. Pro-
ceedings of WWW-10, Hong Kong, pp. 169?178. 
S. Blair-Goldensohn, K.R. McKeown, and A.H. 
Schlaikjer. 2003. A Hybrid Approach for 
Answering Definitional Questions. Technical 
Report CUCS-006-03, Columbia University. 
S. Buchholtz and W. Daelemans. 2001. Complex 
Answers: A Case Study Using a WWW 
Question Answering System. Natural Language 
Engineering, 7(4):301?323. 
J. Chu-Carroll, K. Czuba, J. Prager, and A. 
Ittycheriah. 2003. In Question Answering, Two 
Heads are Better than One. Proceedings of HLT-
NAACL 2003, Edmonton, Canada, pp. 24?31.  
M.A. Hearst. 1998. Automated Discovery of 
Wordnet Relations. In C. Fellbaum (Ed.), Word-
Net: An Electronic Lexical Database. MIT Press. 
A. Ittycheriah and S. Roukos. 2002. IBM?s 
Statistical Question Answering System ? TREC-
11. Proceedings of TREC-2002.  
H. Joho and M. Sanderson. 2000. Retrieving 
Descriptive Phrases from Large Amounts of Free 
Text. Proceedings of the 9th ACM Conference on 
Information and Knowledge Management, 
McLean, VA, pp. 180?186.  
H. Joho and M. Sanderson. 2001. Large Scale 
Testing of a Descriptive Phrase Finder. 
Proceedings of the 1st Human Language 
Technology Conference, San Diego, CA, pp. 
219?221. 
H.T. Ng, J.L.P. Kwan, and Y. Xia. 2001. Question 
Answering Using a Large Text Database: A 
Machine Learning Approach. Proceedings of 
EMNLP 2001, Pittsburgh, PA, pp. 67?73.  
J. Prager, D. Radev, and K. Czuba. 2001. 
Answering What-Is Questions by Virtual 
Annotation. Proceedings of the 1st Human 
Language Technology Conference, San Diego, 
CA, pp. 26?30. 
J. Prager, J. Chu-Carroll, and K. Czuba. 2002. Use 
of WordNet Hypernyms for Answering What-Is 
Questions. Proceedings of TREC-2001. 
D.R. Radev and K.R. McKeown. 1997. Buliding a 
Generation Knowledge Source using Internet-
Accessible Newswire. Proceedings of the 5th 
ANLP, Washington, D.C., pp. 221?228.  
D.R. Radev, J. Prager, and V. Samn. 2000. 
Ranking Suspected Answers to Natural 
Language Questions Using Predictive 
Annotation. Proceedings of NAACL/ANLP-2000, 
Seattle, WA, pp. 150?157. 
D. Ravichandran, E. Hovy, and F.J. Och. 2003. 
Statistical QA ? Classifier vs. Ranker: What?s 
the Difference? Proceedings of the ACL 
workshop on Multilingual Summarization and 
Question Answering, Sapporo, Japan. 
D. Ravichandran, A. Ittycheriah, and S. Roukos. 
2003. Automatic Derivation of Surface Text 
Patterns for a Maximum Entropy Based Question 
Answering System. Proceedings of HLT-NAACL 
2003, Edmonton, Canada, pp. 85?87.  
B. Sch?lkopf and A. Smola. 2002. Learning with 
Kernels.  MIT Press. 
E.M. Voorhees and D.M. Tice. 2000. Building a 
Question Answering Test Collection. Proc. of 
SIGIR-2000, Athens, Greece, pp. 200?207. 
E.M. Voorhees. 2001. The TREC QA Track. 
Natural Language Engineering, 7(4):361?378. 
E.M. Voorhees. 2003. Evaluating Answers to 
Definition Questions. Proceedings of HLT-
NAACL 2003, Edmonton, Canada, pp. 109?111.  
Y. Yang and J.O. Pedersen. 1997. A Comparative 
Study on Feature Selection in Text 
Categorization. Proceedings of the 14th 
International Conference on Machine Learning, 
Nashville, TN, pp. 412?420. 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 323?330, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Practically Unsupervised Learning Method to Identify Single-Snippet
Answers to Definition Questions on the Web
Ion Androutsopoulos and Dimitrios Galanis
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34, Athens, Greece
Abstract
We present a practically unsupervised
learning method to produce single-snippet
answers to definition questions in ques-
tion answering systems that supplement
Web search engines. The method exploits
on-line encyclopedias and dictionaries to
generate automatically an arbitrarily large
number of positive and negative definition
examples, which are then used to train an
SVM to separate the two classes. We show
experimentally that the proposed method
is viable, that it outperforms the alterna-
tive of training the system on questions
and news articles from TREC, and that it
helps the search engine handle definition
questions significantly better.
1 Introduction
Question answering (QA) systems for document col-
lections typically aim to identify in the collections
text snippets (e.g., 50 or 250 characters long) or ex-
act answers (e.g., names, dates) that answer natu-
ral language questions submitted by their users. Al-
though they are commonly evaluated on newspaper
archives, as in the TREC QA track, QA systems can
also supplement Web search engines, to help them
return snippets, as opposed to Web pages, that pro-
vide more directly the information users require.
Most current QA systems first classify the input
question into one of several categories (e.g., ques-
tions asking for locations, persons, etc.), producing
expectations for types of named entities that must
be present in the answer (locations, person names,
etc.). Using the question?s terms as a query, an infor-
mation retrieval (IR) system identifies relevant doc-
uments. Snippets of these documents are then se-
lected and ranked, using criteria such as whether or
not they contain the expected types of named enti-
ties, the percentage of the question?s terms they con-
tain, etc. The system then outputs the most highly-
ranked snippets, or named entities therein.
The approach highlighted above performs poorly
with definition questions (e.g., ?What is gasohol??,
?Who was Duke Ellington??), because definition
questions do not generate expectations for particular
types of named entities, and they typically contain
only a single term. Definition questions are particu-
larly common; in the QA track of TREC-2001, where
the distribution of question types reflected that of
real user logs, 27% of the questions were requests
for definitions. Of course, answers to many defini-
tion questions can be found in on-line encyclopedias
and dictionaries.1 There are always, however, new
or less widely used terms that are not included in
such resources, and this is also true for many names
of persons and products. Hence, techniques to dis-
cover definitions in ordinary Web pages and other
document collections are valuable. Definitions of
this kind are often ?hidden? in oblique contexts (e.g.,
?He said that gasohol, a mixture of gasoline and
ethanol, has been great for his business.?).
In recent work, Miliaraki and Androutsopoulos
(2004), hereafter M&A, proposed a method we call
1See, for example, Wikipedia (http://www.wikipedia.org/).
WordNet?s glosses are another on-line source of definitions.
323
DEFQA, which handles definition questions. The
method assumes that a question preprocessor sep-
arates definition from other types of questions, and
that in definition questions this module also identi-
fies the term to be defined, called the target term.2
The input to DEFQA is a (possibly multi-word) tar-
get term, along with the r most highly ranked docu-
ments that an IR system returned for that term. The
output is a list of k 250-character snippets from the
r documents, at least one of which must contain an
acceptable short definition of the target term, much
as in the QA track of TREC-2000 and TREC-2001.3
We note that since 2003, TREC requires defini-
tion questions to be answered by lists of comple-
mentary snippets, jointly providing a range of in-
formation nuggets about the target term (Voorhees,
2003). In contrast, here we focus on locating single-
snippet definitions. We believe this task is still in-
teresting and of practical use. For example, a list
of single-snippet definitions accompanied by their
source URLs is a good starting point for users of
search engines wishing to obtain definitions. Single-
snippet definitions can also be useful in information
extraction, where the templates to be filled in often
require short entity descriptions. We also note that
the post-2003 TREC task has encountered evaluation
problems, because it is difficult to agree on which
nuggets should be included in the multi-snippet def-
initions (Hildebrandt et al, 2004). In contrast, our
experimental results of Section 4 indicate strong
inter-assessor agreement for single-snippet answers,
suggesting that it is easier to agree upon what con-
stitutes an acceptable single-snippet definition.
DEFQA relies on an SVM, which is trained to clas-
sify 250-character snippets that have the target term
at their centre, hereafter called windows, as accept-
able definitions or non-definitions.4 To train the
SVM, a collection of q training target terms is used;
M&A used the target terms of definition questions
from TREC-2000 and TREC-2001. The terms are
submitted to an IR system, which returns the r most
2Alternatively, the user can be asked to specify explicitly the
question type and target term via a form-based interface.
3Definition questions were not considered in TREC-2002.
4See, for example, Scholkopf and Smola (2002) for in-
formation on SVMs. Following M&A, we use a linear SVM,
as implemented by Weka?s SMO class (http://www.cs.waikato.
ac.nz/ml/weka/). The windows may be shorter than 250 charac-
ters, when the surrounding text is limited.
highly ranked documents per target term. The win-
dows of the q ? r resulting documents are tagged
as acceptable definitions or non-definitions, and be-
come the training instances of the SVM. At run time,
when a definition question is submitted, the r top-
ranked documents are obtained, their windows are
collected, and for each window the SVM returns a
score indicating how confident it is that the window
is a definition. The k windows with the highest con-
fidence scores are then reported to the user.
The SVM actually operates on vector representa-
tions of the windows, that comprise the verdicts or
attributes of previous methods by Joho and Sander-
son (2000) and Prager et al (2002), as well as
attributes corresponding to automatically acquired
lexical patterns. On TREC-2000 and TREC-2001
data, M&A found that DEFQA clearly outperformed
the original methods of Joho and Sanderson and
Prager et al Their best configuration answered cor-
rectly 73% of 160 definition questions in a cross-
validation experiment with k = 5, r = 50, q = 160.
A limitation of DEFQA is that it cannot be trained
easily on new document collections, because it re-
quires the training windows to be tagged as defini-
tions or non-definitions. In the experiments of M&A,
there were 18,473 training windows. Tagging them
was easy, because the windows were obtained from
TREC questions and documents, and the TREC or-
ganizers provide Perl patterns that can be used to
judge whether a snippet from TREC?s documents is
among the acceptable answers of a TREC question.5
For non-TREC questions and document collections,
however, where such patterns are unavailable, sep-
arating thousands of training windows into the two
categories by hand is a laborious task.
In this paper, we consider the case where DE-
FQA is used as an add-on to a Web search engine.
There are three training alternatives in this setting:
(i) train DEFQA on TREC questions and documents;
(ii) train DEFQA on a large collection of manually
tagged training windows obtained from Web pages
that the search engine returned for training target
terms; or (iii) devise techniques to tag automatically
the training windows of (ii). We have developed a
technique along alternative (iii), which exploits on-
5The patterns? judgements are not always perfect, which in-
troduces some noise in the training examples.
324
line encyclopedias and dictionaries. This allows us
to generate and tag automatically an arbitrarily large
number of training windows, in effect converting
DEFQA to an unsupervised method. We show ex-
perimentally that the new unsupervised method is
viable, that it outperforms alternative (i), and that it
helps the search engine handle definition questions
significantly better than on its own.
2 Attributes of DEFQA
DEFQA represents each window as a vector compris-
ing the values of the following attributes:6
SN: The ordinal number of the window in the
document, in our case Web page, it originates from.
The intuition is that windows that mention the target
term first in a document are more likely to define it.
WC: What percentage of the 20 words that are
most frequent across all the windows of the tar-
get term are present in the particular window rep-
resented by the vector. A stop-list and a stemmer are
applied first when computing WC .7 In effect, the 20
most frequent words form a simplistic centroid of all
the candidate answers, and WC measures how close
the vector?s window is to this centroid.
RK: The ranking of the Web page the window
originates from, as returned by the search engine.
Manual patterns: 13 binary attributes, each sig-
nalling whether or not the window matches a differ-
ent manually constructed lexical pattern (e.g., ?tar-
get, a/an/the?, as in ?Tony Blair, the British prime
minister?). The patterns are those used by Joho and
Sanderson, and four more added by M&A. They are
intended to perform well across text genres.
Automatic patterns: A collection of m binary at-
tributes, each showing if the window matches a dif-
ferent automatically acquired lexical pattern. The
patterns are sequences of n tokens (n ? {1, 2, 3})
that must occur either directly before or directly af-
ter the target term (e.g., ?target, which is?). These
patterns are acquired as follows. First, all the n-
grams that occur directly before or after the target
terms in the training windows are collected. The n-
grams that have been encountered at least 10 times
are considered candidate patterns. From those, the
6SN and WC originate from Joho and Sanderson (2000).
7We use the 100 most frequent words of the British National
Corpus as the stop-list, and Porter?s stemmer.
m patterns with the highest precision scores are re-
tained, where precision is the number of training
definition windows the pattern matches divided by
the total number of training windows it matches. We
set m to 200, the value that led to the best results
in the experiments of M&A. The automatically ac-
quired patterns allow the system to detect definition
contexts that are not captured by the manual pat-
terns, including genre-specific contexts.
M&A also explored an additional attribute, which
carried the verdict of Prager et al?s WordNet-based
method (2002). However, they found the additional
attribute to lead to no significant improvements, and,
hence, we do not use it. We have made no attempt to
extend the attribute set of M&A; for example, with
attributes showing if the window contains the target
term in italics, if the window is part of a list that
looks like a glossary, or if the window derives from
an authority Web page. We leave such extensions
for future work. Our contribution is the automatic
generation of training examples.
3 Generating training examples
When training DEFQA on windows from Web pages,
a mechanism to tag the training windows as defi-
nitions or non-definitions is required. Rather than
tagging them manually, we use a measure of how
similar the wording of each training window is to
the wording of definitions of the same target term
obtained from on-line encyclopedias and dictionar-
ies. This is possible because we pick training target
terms for which there are several definitions in dif-
ferent on-line encyclopedias and dictionaries; here-
after we call these encyclopedia definitions.8 Train-
ing windows whose wording is very similar to that
of the corresponding encyclopedia definitions are
tagged as definition windows (positive examples),
while windows whose wording differs significantly
from the encyclopedia definitions are tagged as non-
definitions (negative examples). Training windows
for which the similarity score does not indicate great
similarity or dissimilarity to the wording of the en-
cyclopedia definitions are excluded from DEFQA?s
8We use randomly selected entries from the index of
http://www.encyclopedia.com/ as training terms, and Google?s
?define:? feature, that returns definitions from on-line encyclo-
pedias and dictionaries, to obtain the encyclopedia definitions.
325
training, as they cannot be tagged as positive or neg-
ative examples with sufficiently high confidence.
Note that encyclopedia definitions are used only
to tag training windows. Once the system has been
trained, it can be used to discover on ordinary Web
pages definitions of terms for which there are no en-
cyclopedia definitions, and indeed this is the main
purpose of the system. Note also that we train DE-
FQA on windows obtained from Web pages returned
by the search engine for training terms. This allows
it to learn characteristics of the particular search en-
gine being used; for example, what weight to as-
sign to RK , depending on how much the search
engine succeeds in ranking pages containing defi-
nitions higher. More importantly, it allows DEFQA
to select lexical patterns that are indicative of def-
initions in Web pages, as opposed to patterns that
are indicative of definitions in electronic encyclope-
dias and dictionaries. The latter explains why we
do not train DEFQA directly on encyclopedia defi-
nitions; another reason is that DEFQA requires both
positive and negative examples, while encyclopedia
definitions provide only positive ones.
We now explain how we compute the similar-
ity between a training window and the collection
C of encyclopedia definitions for the window?s tar-
get term. We first remove stop-words, punctua-
tion, other non-alphanumeric characters and the tar-
get term from the training window, and apply a stem-
mer, leading to a new form W of the training win-
dow. We then compute the similarity of W to C as:
sim(W,C) = 1/|W | ? ?|W |i=1sim(wi, C)
where |W | is the number of distinct words in W , and
sim(wi, C) is the similarity of the i-th distinct word
of W to C, defined as follows:
sim(wi, C) = fdef (wi, C) ? idf (wi)
fdef (wi, C) is the percentage of definitions in C that
contain wi, and idf (wi) is the inverse document fre-
quency of wi in the British National Corpus (BNC):
idf (wi) = 1 + log
N
df(wi)
N is the number of documents in BNC, and df (wi)
the number of BNC documents where wi occurs; if
wi does not occur in BNC, we use the lowest df score
of BNC. sim(wi, C) is highest for words that oc-
cur in all the encyclopedia definitions and are used
rarely in English. A training window with a large
proportion of such words most probably defines the
target term. More formally, given two thresholds t+
and t? with t? ? t+, we tag W as a positive ex-
ample if sim(W,C) ? t+, as a negative example if
sim(W,C) ? t?, and we exclude it from the train-
ing of DEFQA if t? < sim(W,C) < t+. Hereafter,
we refer to this method of generating training exam-
ples as the similarity method.
To select reasonable values for t+ and t?, we con-
ducted a preliminary experiment for t? = t+ = t;
i.e., both thresholds were set to the same value t
and no training windows were excluded. We used
q = 130 training target terms from TREC definition
questions, for which we had multiple encyclopedia
definitions. For each term, we collected the r = 10
most highly ranked Web pages.9 To alleviate the
class imbalance problem, whereby the positive ex-
amples (definitions) are much fewer than the nega-
tive ones (non-definitions), we kept only the first 5
windows from each Web page (SN ? 5), based on
the observation that windows with great SN scores
are almost certainly non-definitions; we do the same
in the training stage of all the experiments of this pa-
per, and at run-time, when looking for windows to
report, we ignore windows with SN > 5. From the
resulting collection of training windows, we selected
randomly 400 windows, and tagged them both man-
ually and via the similarity method, with t ranging
from 0 to 1. Figures 1 and 2 show the precision and
recall of the similarity method on positive and neg-
ative training windows, respectively, for varying t.
Here, positive precision is the percentage of training
windows the similarity method tagged as positive
examples (definitions) that were indeed positive; the
true classes of the training windows were taken to be
those assigned by the human annotators. Positive re-
call is the percentage of truly positive examples that
the similarity method tagged as positive. Negative
precision and negative recall are defined similarly.
Figures 1 and 2 indicate that there is no single
threshold t that achieves both high positive preci-
sion and high negative precision. To be confident
9In all our experiments, we used the Altavista search engine.
326
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1similarity threshold
p
r
e
c
i
s
i
o
n
-
r
e
c
a
l
l
precision of positive examplesrecall of positive examples
Figure 1: Positive precision and recall
that the training windows the similarity method will
tag as positive examples are indeed positive (high
positive precision), one has to set t close to 1; and to
be confident that the training windows the similar-
ity method will tag as negative examples are indeed
negative (high negative precision), t has to be set
close to 0. This is why we use two separate thresh-
olds and discard the training windows whose simi-
larity score is between t? and t+. Figures 1 and 2
also indicate that in both positive and negative ex-
amples the similarity method achieves perfect pre-
cision only at the cost of very low recall; i.e., if we
insist that all the resulting training examples must
have been tagged correctly (perfect positive and neg-
ative precision), the resulting examples will be very
few (low positive and negative recall). There is also
another consideration when selecting t? and t+: the
ratio of positive to negative examples that the sim-
ilarity method generates must be approximately the
same as the true ratio before discarding any training
windows, in order to avoid introducing an artificial
bias in the training of DEFQA?s SVM; the true ratio
among the 400 training windows before discarding
any windows was approximately 0.37 : 1.
Based on the considerations above, in the remain-
ing experiments of this paper we set t+ to 0.5. In
Figure 1, this leads to a positive precision of 0.72
(and positive recall 0.49), which does not improve
much by adopting a larger t+, unless one is willing
to set t+ at almost 1 at the price of very low posi-
tive recall. In the case of t?, setting it to any value
less than 0.34 leads to a negative precision above
0.9, though negative recall drops sharply as t? ap-
proaches 0 (Figure 2). For example, setting t? to
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1similarity threshold
p
r
e
c
i
s
i
o
n
-
r
e
c
a
l
l
precision of negative examples
recall of negative examples
Figure 2: Negative precision and recall
0.32, leads to 0.92 negative precision, 0.75 negative
recall, and approximately the same positive to nega-
tive ratio (0.31 : 1) as the true observed ratio. In the
experiments of Section 4, we keep t+ fixed to 0.5,
and set t? to the value in the range (0, 0.34) that
leads to the positive to negative ratio that is closest
to the true ratio we observed in the 400 windows.
The high negative precision we achieve (> 0.9)
suggests that the resulting negative examples are al-
most always truly negative. In contrast, the lower
positive precision (0.72) indicates that almost one in
every four resulting positive examples is in reality a
non-definition. This is a point where our similarity
method needs to be improved; we return to this point
in Section 6. Our experiments, however, show that
despite this noise, the similarity method already out-
performs the alternative of training DEFQA on TREC
data. Note also that once the thresholds have been
selected, we can generate automatically an arbitrar-
ily large set of training examples, by starting with a
sufficiently large number q of training terms to com-
pensate for discarded training examples.
4 Evaluation
We tested two different forms of DEFQA. The first
one, dubbed DEFQAt , was trained on the q = 160
definition questions of TREC-2000 and TREC-2001
and the corresponding TREC documents, resulting in
3,800 training windows.10 The second form of DE-
10For each question, the TREC organizers provide the 50 most
highly ranked documents that an IR engine returned from the
TREC document collection. We keep the top r = 10 of these
documents, while M&A kept all 50. Furthermore, as discussed
in Section 3, we retain up to the first 5 windows from each doc-
327
FQA, dubbed DEFQAs , was trained via the similarity
method, with q = 480 training target terms, leading
to 7,200 training windows; as discussed in Section 3,
one of the advantages of the similarity method is that
one can generate an arbitrarily large set of training
windows. As in the preliminary experiment of Sec-
tion 3, r (Web pages per target term) was set to 10
in both systems. To simplify the evaluation and test
DEFQA in a more demanding scenario, we set k to
1, i.e., the systems were allowed to return only one
snippet per question, as opposed to the more lenient
k = 5 in the experiments of M&A.
We also wanted a measure of how well DEFQAt
and DEFQAs perform compared to a search engine
on its own. For this purpose, we compared the per-
formance of the two systems to that of a baseline,
dubbed BASE1 , which always returns the first win-
dow of the Web page the search engine ranked first.
In a search engine that highlights question terms
in the returned documents, the snippet returned by
BASE1 is presumably the first snippet a user would
read hoping to find an acceptable definition. To
study how much DEFQAt and DEFQAs improve upon
random behaviour, we also compared them to a sec-
ond baseline, BASEr , which returns a randomly se-
lected window among the first five windows of all r
Web pages returned by the search engine.
All four systems were evaluated on 81 unseen tar-
get terms. Their responses were judged indepen-
dently by two human assessors, who had to mark
each response as containing an acceptable short def-
inition or not. As already pointed out, DEFQAt and
DEFQAs consult encyclopedia definitions only dur-
ing training, and at run time the systems are in-
tended to be used with terms for which no ency-
clopedia definitions are available. During this eval-
uation, however, we deliberately chose the 81 test
terms from the index of an on-line encyclopedia.
This allowed us to give the encyclopedia?s defini-
tions to the assessors, to help them judge the accept-
ability of the single-snippet definitions the systems
located on Web pages; many terms where related to,
for example, medicine or biology, and without the
encyclopedia?s definitions the assessors would not
be aware of their meanings. The following is a snip-
pet returned correctly by DEFQAs for ?genome?:
ument. This is why we have fewer training windows than M&A.
discipline comparative genomics functional genomics bioinfor-
matics the emergence of genomics as a discipline in 1920 , the
term genome was proposed to denote the totality of all genes on
all chromosomes in the nucleus of a cell . biology has. . .
while what follows is a non-definition snippet re-
turned wrongly by BASE1 :
what is a genome national center for biotechnology information
about ncbi ncbi at a glance a science primer databases. . .
The examples illustrate the nature of the snippets
that the systems and assessors had to consider. The
snippets often contain phrases that acted as links in
the original pages, or even pieces of programming
scripts that our rudimental preprocessing failed to
remove. (We remove only HTML tags, and apply
a simplistic tokenizer.) Nevertheless, in most cases
the assessors had no trouble agreeing whether or
not the resulting snippets contained acceptable short
definitions. KCo was 0.80, 0.81, 0.90, 0.89, and
0.86 in the assessment of the responses of DEFQAs ,
DEFQAt , BASEr , BASE1 , and all responses, respec-
tively, indicating strong inter-assessor agreement.11
The agreement was slightly lower in DEFQAs and
DEFQAt , because there were a few marginally ac-
ceptable or truncated definitions the assessors were
uncertain about. There were also 4 DEFQAs answers
and 3 BASE1 answers that defined secondary mean-
ings of the target terms; e.g., apart from a kind of
lizard, ?gecko? is also the name of a graphics engine,
and ?Exodus? is also a programme for ex-offenders.
Such answers were counted as wrong, though this
may be too strict. With a larger k, there would be
space to return both the main and secondary mean-
ings, and the evaluation could require this.
Table 1 shows that DEFQAs answered correctly
approximately 6 out of 10 definition questions. This
is lower than the score reported by M&A (73%),
but remarkably high given that in our evaluation
the systems were allowed to return only one snip-
pet per question; i.e., the task was much harder than
in M&A?s experiments. DEFQAs answered correctly
more than twice as many questions as DEFQAt , de-
spite the fact that its training data contained a lot of
noise. (Single-tailed difference-of-proportions tests
show that all the differences of Table 1 are statisti-
11We follow the notation of Di Eugenio and Glass (2004).
The KS&C figures were identical. The 2 ? P (A) ? 1 figures
were 0.80, 0.85, 0.95, 0.95, and 0.89 respectively.
328
assessor 1 assessor 2 average
BASEr 14.81 (12) 14.81 (12) 14.81 (12)
BASE1 14.81 (12) 12.35 (10) 13.58 (11)
DEFQAt 25.93 (21) 25.93 (21) 25.93 (21)
DEFQAs 55.56 (45) 60.49 (49) 58.02 (47)
Table 1: Percentage of questions answered correctly
cally significant at ? = 0.001.) The superiority of
DEFQAs appears to be mostly due to its automati-
cally acquired patterns. DEFQAt too was able to ac-
quire several good patterns (e.g., ?by target?, ?known
as target?, ?target, which is?, ?target is used in?), but
its pattern set alo comprises a large number of irrel-
evant n-grams; this had also been observed by M&A.
In contrast, the acquired pattern set of DEFQAs is
much cleaner, with much fewer irrelevant n-grams,
which is probably due to the largest, almost double,
number of training windows. Furthermore, the pat-
tern set of DEFQAs contains many n-grams that are
indicative of definitions on the Web. For example,
many Web pages that define terms contain text of the
form ?What is a target? A target is. . . ?, and DEFQAs
has discovered patterns of the form ?what is a/an/the
target?, ?? A/an/the target?, etc. It has also discov-
ered patterns like ?FAQ target?, ?home page target?,
?target page? etc., that seem to be good indications
of Web windows containing definitions.
Overall, DEFQA?s process of acquiring lexical pat-
terns worked better in DEFQAs than in DEFQAt , and
we believe that the performance of DEFQAs could be
improved further by acquiring more than 200 pat-
terns; we hope to investigate this in future work,
along with an investigation of how the performance
of DEFQAs relates to q, the number of training target
terms. Finally, note that the scores of both baselines
are very poor, indicating that DEFQAs performs sig-
nificantly better than picking the first, or a random
snippet among those returned by the search engine.
5 Related work
Definition questions have recently attracted several
QA researchers. Many of the proposed approaches,
however, rely on manually crafted patterns or heuris-
tics to identify definitions, and do not employ learn-
ing algorithms (Liu et al, 2003; Fujii and Ishikawa,
2004; Hildebrandt et al, 2004; Xu et al, 2004).
Ng et al (2001) use machine learning (C5 with
boosting) to classify and rank candidate answers in
a general QA system, but they do not treat defi-
nition questions in any special way; consequently,
their worst results are for ?What. . . ?? questions,
that presumably include definition questions. Itty-
cheriah and Roukos (2002) employ a maximum en-
tropy model to rank candidate answers in a general-
purpose QA system. Their maximum entropy model
uses a very rich set of attributes, that includes 8,500
n-gram patterns. Unlike our work, their n-grams are
five or more words long, they are coupled to two-
word question prefixes, and, in the case of definition
questions, they do not need to be anchored at the tar-
get term. The authors, however, do not provide sep-
arate performance figures for definition questions.
Blair-Goldensohn et al (2003) focus on defini-
tion questions, but aim at producing coherent multi-
snippet definitions, rather than single-snippet defi-
nitions. The heart of their approach is a compo-
nent that uses machine learning (Ripper) to identify
sentences that can be included in the multi-sentence
definition. This component plays a role similar to
that of our SVM, but it is intended to admit a larger
range of sentences, and appears to employ only at-
tributes conveying the ordinal number of the sen-
tence in its document and the frequency of the target
term in the sentence?s context.
Since TREC-2003, several researchers have pro-
posed ways to generate multi-snippet definitions
(Cui et al, 2004; Fujii and Ishikawa, 2004; Hilde-
brandt et al, 2004; Xu et al, 2004). The typical
approach is to locate definition snippets, much as
in our work, and then report definition snippets that
are sufficiently different; most of the proposals use
some form of clustering to avoid reporting redun-
dant snippets. Such methods could also be applied
to DEFQA, to extend it to the post-2003 TREC task.
On-line encyclopedias and dictionaries have been
used to handle definition questions in the past, but
not as in our work. Hildebrandt et al (2004) look up
target terms in encyclopedias and dictionaries, and
then, knowing the answers, try to find supporting
evidence for them in the TREC document collection.
Xu et al (2004) collect from on-line encyclopedias
and dictionaries words that co-occur with the tar-
get term; these words and their frequencies are then
used as a centroid of the target term, and candidate
329
answers are ranked by computing their similarity to
the centroid. This is similar to our WC attribute.
Cui et al (2004) also employ a form of centroid,
comprising words that co-occur with the target term.
The similarity to the centroid is taken into consider-
ation when ranking candidate answers, but it is also
used to generate training examples for a learning
component that produces soft patterns, in the same
way that we use the similarity method to produce
training examples for the SVM. As in our work, the
training examples that the centroid generates may
be noisy, but the component that produces soft pat-
terns manages to generalize over the noise. To the
best of our knowledge, this is the only other unsu-
pervised learning approach for definition questions
that has been proposed. We hope to compare the
two approaches experimentally in future work. For
the moment, we can only point out that Cui et al?s
centroid approach generates only positive examples,
while our similarity method generates both positive
and negative ones; this allows us to use a principled
SVM learner, as opposed to Cui et al?s more ad hoc
soft patterns that incorporate only positive examples.
6 Conclusions and future work
We presented an unsupervised method to learn to lo-
cate single-snippet answers to definition questions
in QA systems that supplement Web search en-
gines. The method exploits on-line encyclopedias
and dictionaries to generate automatically an arbi-
trarily large number of positive and negative defini-
tion examples, which are then used to train an SVM
to separate the two classes. We have shown experi-
mentally that the proposed method is viable, that it
outperforms training the QA system on TREC data,
and that it helps the search engine handle definition
questions significantly better than on its own.
We have already pointed out the need to improve
the positive precision of the training examples. One
way may be to combine our similarity method with
Cui et al?s centroids. We also plan to study the ef-
fect of including more automatically acquired pat-
terns and using more training target terms. Finally,
our method can be improved by including attributes
for the layout and authority of Web pages.
References
S. Blair-Goldensohn, K.R. McKeown, and A.H. Schlaik-
jer. 2003. A hybrid approach for answering defi-
nitional questions. Technical Report CUCS-006-03,
Columbia University.
H. Cui, M.-Y. Kan, and T.-S. Chua. 2004. Unsupervised
learning of soft patterns for generating definitions from
online news. In Proceedings of WWW-2004, pages
90?99, New York, NY.
B. Di Eugenio and M. Glass. 2004. The kappa statistic:
A second look. Comput. Linguistics, 30(1):95?101.
A. Fujii and T. Ishikawa. 2004. Summarizing encyclo-
pedic term descriptions on the Web. In Proceedings of
COLING-2004, pages 645?651, Geneva, Switzerland.
W. Hildebrandt, B. Katz, and J. Lin. 2004. An-
swering definition questions using multiple knowledge
sources. In Proceedings of HLT-NAACL 2004, pages
49?56, Boston, MA.
A. Ittycheriah and S. Roukos. 2002. IBM?s statistical
question answering system ? TREC-11. In Proceed-
ings of TREC-2002.
H. Joho and M. Sanderson. 2000. Retrieving descriptive
phrases from large amounts of free text. In Proc. of the
9th ACM Conference on Information and Knowledge
Management, pages 180?186, McLean, VA.
B. Liu, C.W. Chin, and H.T. Ng. 2003. Mining topic-
specific concepts and definitions on the Web. In Pro-
ceedings of WWW-2003, Budapest, Hungary.
S. Miliaraki and I. Androutsopoulos. 2004. Learning to
identify single-snippet answers to definition questions.
In Proceedings of COLING-2004, pages 1360?1366,
Geneva, Switzerland.
H.T. Ng, J.L.P. Kwan, and Y. Xia. 2001. Question an-
swering using a large text database: A machine learn-
ing approach. In Proceedings of EMNLP-2001, pages
67?73, Pittsburgh, PA.
J. Prager, J. Chu-Carroll, and K. Czuba. 2002. Use of
WordNet hypernyms for answering what-is questions.
In Proceedings of TREC-2001.
B. Scholkopf and A. Smola. 2002. Learning with ker-
nels. MIT Press.
E.M. Voorhees. 2003. Evaluating answers to definition
questions. In Proceedings of HLT-NAACL 2003, pages
109?111, Edmonton, Canada.
J. Xu, R. Weischedel, and A. Licuanan. 2004. Eval-
uation of an extraction-based approach to answering
definitional questions. In Proceedings of SIGIR-2004,
pages 418?424, Sheffield, U.K.
330
Exploiting OWL Ontologies in the Multilingual Generation of Object Descriptions
Ion Androutsopoulos?, Spyros Kallonis?+ and Vangelis Karkaletsis+
?Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
+Institute of Informatics and Telecommunications
National Centre for Scientific Research ?Demokritos?
P.O. Box 60228, GR-153 10 Aghia Paraskevi, Greece
Abstract
We present three ways in which a natural language
generator that produces textual descriptions of ob-
jects from symbolic information can exploit OWL
ontologies, using M-PIRO?s multilingual generation
system as a concrete example.
1 Introduction
A strand of work in Natural Language Generation (NLG) has
been devoted to the generation of textual descriptions of ob-
jects from symbolic information in ontologies and databases.
An example of such work is ILEX [O?Donnell et al, 2001],
which was demonstrated mostly in the museums domain,
where it could produce personalised English descriptions of
exhibits; the Power system [Dale et al, 1998] is another ex-
ample from the same domain. More recently, the M-PIRO
project [Isard et al, 2003] developed a multilingual exten-
sion of ILEX, which has been tested in a variety of domains,
including museum exhibits and items for sale.1 A major prob-
lem in this and many other NLG subareas is the difficulty of
obtaining source symbolic information in forms compatible
with the requirements of the language generators. This issue
has mainly been addressed so far by extracting source infor-
mation from structured and semi-structured data [Dale et al,
1998], and by developing authoring tools that help in the cre-
ation of source information and domain-dependent linguistic
resources. Such tools were developed, for example, in GIST
[Power and Cavallotto, 1996], DRAFTER [Hartley and Paris,
1997], ITRI?s WYSIWYM systems [Van Deemter and Power,
2003], and M-PIRO [Androutsopoulos et al, 2002].
In recent years, considerable effort has been invested in the
Semantic Web, which can be seen as an attempt to develop
mechanisms that will allow computer applications to reason
more easily about the semantics of the resources (documents,
services, etc.) of the Web. A major target is the development
of standard representation formalisms, that will allow ontolo-
gies to be published on the Web and be shared by different
1M-PIRO was an IST project of the European Union. It ran from
2000 to 2003. Its partners were: the University of Edinburgh, ITC-
irst, NCSR ?Demokritos?, the National and Kapodistrian University
of Athens, the Foundation of the Hellenic World, and System Simu-
lation. This paper includes additional work, carried out at the Athens
University of Economics and Business and NCSR ?Demokritos?.
computer applications. The emerging standard for specifying
ontologies is OWL, an extension of RDF.2 In NLG systems
that describe objects, pre-existing OWL ontologies can pro-
vide much of the required source information, reducing the
authoring effort and providing a common standard represen-
tation to generate from.3 We discuss the role that OWL on-
tologies can play in M-PIRO?s authoring process, and report
on progress we made towards extending M-PIRO?s authoring
tool to support OWL. We argue that the benefit from using
OWL would be greater, if the ontologies included the domain-
dependent linguistic resources and user modelling informa-
tion that NLG systems need. This would allow content to be
published on the Sematic Web in the form of OWL ontologies,
with different NLG engines acting as browsers responsible for
rendering the content in different natural languages and tai-
loring it to the interests and interaction history of the users.
A challenge for the NLG community, then, is to agree upon
standards on how linguistic resources and user modelling in-
formation should be embedded in OWL ontologies.
Section 2 below introduces briefly M-PIRO and its author-
ing tool. Section 3 then shows how M-PIRO?s ontologies can
be expressed in OWL, and presents facilities we have added
to the authoring tool to export ontologies in OWL. Among
other benefits, this allows machine-generated texts to be pub-
lished on the Web along with the ontology they were gen-
erated from, and to be annotated with OWL entries that ex-
press their semantics in terms of the ontology, making the
semantics accessible to computer applications. Section 4 sub-
sequently discusses how existing OWL ontologies can be im-
ported into the authoring tool, and the benefits that this brings.
Our import facilities currently support only a subset of OWL;
part of section 4 is devoted to problems that remain to be
solved. Section 5 focuses on the need to establish standards
to embed linguistic resources and user modelling information
in OWL ontologies, and how this would allow NLG engines
to become the browsers of the Semantic Web. Section 6 con-
cludes and summarises directions for future research.
2 M-PIRO?s authoring tool
M-PIRO?s authoring tool allows authors, i.e., persons respon-
sible for porting M-PIRO?s technology to new application do-
2Consult http://www.w3.org/TR/owl-guide/.
3See also [Wilcock, 2003], [Bontcheva and Wilks, 2004].
Figure 1: An M-PIRO ontology and a clause plan.
mains, to modify the domain-dependent resources: the ontol-
ogy, some language resources, and the end-user stereotypes.
M-PIRO generates texts from an ontology that provides in-
formation on the entities of a domain (e.g., the statues and
artists in a museum), the relationships between the entities
(e.g., the association of statues with their artists), and the enti-
ties? attributes (e.g., their names or dimensions). Entities are
not necessarily physical objects; they may be abstract con-
cepts (e.g, historical periods). They are organized in a tax-
onomy of entity types, as illustrated in Figure 1, where ?ex-
hibit? and ?historical-period? are basic entity types, i.e., they
have no super-types. The ?exhibit? type is further subdivided
into ?coin?, ?statue?, and ?vessel?. The latter has the sub-types
?amphora?, ?kylix?, and ?lekythos?. Each entity belongs to a
particular type; e.g., ?exhibit22? belongs to ?kylix?, and is,
therefore, also a ?vessel? and an ?exhibit?. For simplicity, M-
PIRO adopts single inheritance, i.e., a type may not have more
than one parents, and an entity may not belong to more than
one types.4 This introduces some problems when importing
OWL ontologies; related discussion follows.
Relationships are expressed using fields. It is possible
to introduce new fields at any entity type, which then be-
come available at all the entities of that type and its subtypes.
In Figure 1, the fields ?painting-technique-used?, ?painted-
by?, and ?potter-is? are introduced at the type ?vessel?. (The
top right panel shows the fields of the type selected in the
left panel.) Hence, all entities of type ?vessel? and its sub-
types, i.e., ?amphora?, ?kylix?, and ?lekythos?, carry these
fields. Furthermore, entities of type ?vessel? inherit the fields
?creation-period?, ?current-location?, etc., up to ?references?,
which are introduced at the ?exhibit? type. (The ?images? field
is used to associate images with entities.) The fillers of each
field, i.e., the possible values, must be entities of a particular
type. In Figure 1, the fillers of ?potter-is? are of type ?potter?;
hence, the entities ?sotades? and ?aristos? are the only possible
values. To represent that a particular ?vessel? entity was cre-
4M-PIRO?s core language generator actually supports some forms
of multiple inheritance, but the authoring tool does not.
Figure 2: Source information and the resulting English text.
ated during the classical period by ?aristos?, one would fill in
that entity?s ?creation-period? with ?classical-period?, and its
?potter-is? with ?aristos?. Figure 2 shows the fields of entity
?exhibit22?, and the resulting English description. M-PIRO
supports English, Greek, and Italian; descriptions can be gen-
erated in all three languages from the same ontology.
The ?Many? column in Figure 1 is used to mark fields
whose values are sets of fillers of the specified type. In the
?made-of? field, this allows the value to be a set of materials
(e.g., gold and silver). It is, thus, possible to represent many-
to-one (e.g., only one material per exhibit) and many-to-many
relationships (many materials per exhibit), but not one-to-one
relationships (e.g., a unique social security code per person).
OWL, in contrast, supports one-to-one relationships.
Fields are also used to represent attributes of entities (e.g.,
their names or dimensions). Several built-in data-types are
available (?string?, ?number?, ?date?, etc.), and they are used
to specify the possible values of attribute-denoting fields. The
?Many? column also applies to attributes. In Figure 1, the
values of ?references? and ?exhibit-purpose? are strings. The
two fields are intended to hold canned texts containing bibli-
ographic references and descriptions of what a particular ex-
hibit was used for; e.g., ?This statue honours the memory of
Kroissos, a young man who died in battle?. Information can
be stored as canned text in string-valued fields when it is dif-
ficult to represent in symbolic form. The drawback is that
canned texts have to be entered in all three languages.
The authoring tool also allows the authors to specify user
types, i.e., types of end-users the texts are intended for (e.g.,
?average-adult?, ?child?), and stereotypes. The latter assign,
for each user type, values to parameters that control, for ex-
ample, the length of the texts, or the extent to which aggregat-
ing clauses to form longer sentences is allowed. The stereo-
types also specify how interesting each field is for each user
type; this allows the system to tailor the content of the de-
scriptions to the users? interests. M-PIRO employs additional
personal user models, where it stores the interaction history
of each particular end-user, allowing, for example, the system
to generate comparisons to previously seen objects.
M-PIRO uses systemic grammars, one for each language, to
convert sentence specifications to surface text. The grammars
can be used in a variety of object description applications
without modifications, and, hence, can be treated as domain-
independent for M-PIRO?s purposes. However, a part of the
lexicon that the grammars employ, known as the domain-
dependent lexicon, has to be filled in by the authors when the
system is ported to a new application. The domain-dependent
lexicon contains entries for nouns and verbs; when moving to
a new application, it is initially empty. The authors enter the
base forms of the nouns and verbs they wish the system to
use, and there are facilities to generate the other forms au-
tomatically. Noun entries are linked to entity types, to allow,
for example, the system to generate referring noun phrases; in
Figure 1, the entity type ?vessel? is associated with the lexicon
entry ?vessel-noun? (see the area next to ?Edit nouns?). The
entries are trilingual; e.g., ?vessel-noun? contains the nouns
?vessel?, ??????o?, and ?vaso? of the three languages.
For each field and each language, the authors have to pro-
vide at least one micro-plan, that specifies how the field
can be expressed as a clause in that language. Follow-
ing ILEX, M-PIRO?s primary form of micro-plans are clause
plans, where the author specifies the clause to be generated
in abstract terms, by selecting the verb to be used (from the
domain-dependent lexicon), the voice and tense of the result-
ing clause, etc. As with nouns, verb-entries are trilingual;
e.g., the ?paint-verb? entry of the clause plan of Figure 1 con-
tains the base verb forms ?paint?, ????????????, and ?dipin-
gere?. By default, the entity that carries the field becomes the
subject of the resulting clause, and the filler of the field the
object. The clause plan of Figure 1 leads to clauses like ?This
vessel was painted by Eucharides?. Appropriate referring ex-
pressions, e.g., ?Eucharides?, ?a painter?, ?him?, are gener-
ated automatically. Alternatively, micro-plans can be speci-
fied as simplistic templates, i.e., sequences of canned strings
and automatically generated referring expressions; see [An-
droutsopoulos et al, 2002] for details.
Unlike ILEX, M-PIRO allows multiple micro-plans to be
specified per field, and this allows greater variety in the gen-
erated texts. Furthermore, the user stereotypes can be used
to indicate that particular micro-plans are more appropriate
to particular user types, and this allows the system to tailor
the expressions it produces. When planning the text, M-PIRO
attempts to place clauses that convey more interesting fields
towards the beginning of the text. It is also possible for the
authors to specify particular orderings; otherwise, M-PIRO?s
text planner is domain-independent.
3 Exporting M-PIRO ontologies to OWL
M-PIRO?s ontological assumptions are very similar to those
of OWL. As with M-PIRO, OWL assumes there are entity
types, called classes, and entities, called individuals. M-
PIRO?s fields correspond to OWL?s properties. Relationships
between entities are expressed by defining object properties,
that map entities to other entities, while attributes of entities
are expressed via datatype properties, that map entities to lit-
erals of specific datatypes. It is, thus, relatively straightfor-
ward to export an M-PIRO ontology to OWL, as sketched be-
low. There are actually three different versions of OWL, called
OWL LITE, OWL DL, and OWL FULL, with increasing sophis-
tication. The mapping from M-PIRO?s ontologies to OWL pro-
duces ontologies in OWL LITE, which can be thought of as a
subset of OWL DL and OWL FULL.
When exporting M-PIRO ontologies to OWL, entity types
give rise to class definitions; e.g., the ?vessel? entity type of
Figure 1 leads to the following OWL class:
<owl:Class rdf:ID="Vessel">
<rdfs:subClassOf>
<owl:Class rdf:about="#Exhibit" />
</rdfs:subClassOf>
</owl:Class>
Fields are exported as OWL properties; e.g., the ?painted-
by? field of Figure 1 leads to the following object property
that associates vessels with painters:
<owl:ObjectProperty rdf:ID="painted-by">
<rdfs:domain rdf:resource="#Vessel" />
<rdfs:range rdf:resource="#Painter" />
</owl:ObjectProperty>
The ?exhibit-purpose? field of Figure 1 leads to the following
datatype property, that associates exhibits with strings:
<owl:DatatypeProperty rdf:ID="exhibit-purpose">
<rdfs:domain rdf:resource="#Exhibit" />
<rdfs:range rdf:resource=
"http://www.w3.org/2001/XMLSchema#string" />
</owl:DatatypeProperty>
Entities map to OWL individuals, as with statue ?exhibit42?
below. String-valued fields, like ?exhibit-purpose?, lead to
properties with separate values per language.
<Statue rdf:ID="exhibit42">
<current-location rdf:resource="#acropolis-museum" />
<creation-period rdf:resource="#archaic-period" />
<exhibit-purpose xml:lang="EN">This statue honours the
memory of Kroissos, a... </exhibit-purpose>
<exhibit-purpose xml:lang="IT">Questa...</exhibit-purpose>
<exhibit-purpose xml:lang="GRC">...</exhibit-purpose>
...
</Statue>
One problem we have encountered is that OWL provides
no mechanism to specify default values of properties. In M-
PIRO, it is possible to introduce a generic entity per entity
type, and the values of its fields are used as default values of
all the entities in that type. For example, one could specify
that kouroi, a kind of statue, were made in the archaic pe-
riod, by introducing a ?generic-kouros? entity, similar to the
?generic-kylix? of Figure 1, and filling its ?creation-period?
with ?archaic-period?. This would save us from having to
specify the creation period of each individual kouros; their
?creation-period? fields would be left empty. It is also possi-
ble to override default information: to specify that a particular
kouros was created during the classical period, perhaps the art
of an eccentric classical sculptor, one would fill its ?creation-
period? with ?classical-period?, and this would licence texts
like ?Kouroi were created during the archaic period. How-
ever, this kouros was created during the classical period?. We
export generic entities as ordinary OWL individuals, but use
a special prefix in their identifiers, which allows M-PIRO?s
system to assign them special status when reloading the on-
tology. Another system, however, that relies only on OWL?s
official semantics would have no way to realize that such in-
dividuals should be assigned special status.
A second problem is that some of M-PIRO?s datatypes
(e.g., dates) do not correspond exactly to OWL?s recom-
mended datatypes. We have defined new datatypes in OWL,
using XML SCHEMA, that correspond exactly to M-PIRO?s
datatypes, and we currently use those in the exported ontolo-
gies instead of the recommended OWL datatypes. We hope
to modify M-PIRO?s datatypes to correspond exactly to the
recommended ones in future versions of M-PIRO?s system.
The mapping from M-PIRO ontologies to OWL that we
sketched above has been fully implemented, and it now al-
lows the authoring tool to export its ontologies in OWL. Apart
from allowing other systems to reuse M-PIRO?s ontologies,
the mapping also opens up the possibility of generating object
descriptions in both human-readable and machine readable
forms. Every natural language description that M-PIRO pro-
duces can in principle also be rendered in a machine-readable
form consisting of OWL individuals, this time using the map-
ping to translate into OWL the parts of the ontology that the
system has decided to convey. For example, the English de-
scription of Figure 2 can be rendered in OWL as:
<Kylix rdf:ID="exhibit22">
<creation-period rdf:resource="#archaic-period />
<painting-technique-used
rdf:resource="#red-figure-technique />
<painted-by rdf:resource="#eucharides />
...
</Kylix>
M-PIRO?s generator might have also included in the resulting
text information deriving from the fields of the painter, e.g.,
the city the painter was born in, or other entities mentioned in
the text. In that case, the OWL rendering of the description?s
content would include additional individuals, such as:
<Painter rdf:ID="eucharides">
<painter-city rdf:resource="#athens" />
...
</Painter>
In the machine-readable forms of the descriptions, the OWL
individuals would include only properties corresponding to
fields the generator has decided to convey, unlike when ex-
porting the full ontology. That is, the OWL individuals may
not include properties corresponding to fields deemed unin-
teresting for the particular end-user, or fields that have already
been conveyed; e.g., the painter?s city may have already been
conveyed when describing another work of the same artist.
It is thus possible to annotate the generated texts with OWL
individuals representing their semantics. This would allow
computer applications (e.g., Web agents visiting the site of
a retailer that generates product descriptions using M-PIRO?s
technology) to reason about the semantics of the texts (e.g,
locate items of interest). Alternatively, it is possible to de-
fine user types for both human users (e.g., ?expert?, ?average-
adult?) and artificial agents acting for users of different in-
terests and expertise (e.g., ?agent-expert?, ?agent-average-
adult?), and produce human-readable or machine-readable
descriptions depending on the user type (in M-PIRO?s demon-
strators, there is a login stage where visitors select their
types). The OWL ontology without its individuals (classes
and properties only) can also be published on the Web to help
the agents? developers figure out the structure and semantics
of the OWL individuals their agents may encounter.
4 Importing OWL ontologies
When porting M-PIRO?s system to a new domain, much of
the authoring effort is devoted to defining entity types, and
the fields that express attributes and relationships. This is a
time-consuming process, partly because the ontology often
has to be reshaped as more experience about the domain is
gained. If a well-thought OWL ontology about the domain al-
ready exists, as will be the case with the gradual expansion
of the Semantic Web, the authoring can be accelerated by im-
porting the existing ontology into the authoring tool. There-
after, the authors can focus on adding the necessary domain-
dependent linguistic resources (micro-plans, lexicon entries,
etc.), setting up the user stereotypes, and populating the on-
tology with entities that were not already present in the im-
ported one. For the latter, we have developed software that
allows the authoring tool to construct entities automatically
from data in relational databases via ODBC; the authors only
need to establish a mapping between the fields of the entity
types and the attributes of the database?s relations.
As already mentioned, there are three versions of OWL
(OWL LITE, OWL DL, OWL FULL) with increasing sophisti-
cation. The mapping from M-PIRO?s ontologies to OWL of
the previous section uses only a subset of OWL LITE. Hence,
importing an arbitrary OWL ontology, as opposed to an OWL
ontology exported by the authoring tool, is not simply a mat-
ter of following the inverse mapping of the previous section.
Below we highlight the problems that arise when importing
arbitrary OWL LITE ontologies, to offer a taste of the work
that remains to be carried out to make M-PIRO?s system fully
compatible with OWL LITE. We also point to some additional
problems that arise when one moves on to OWL DL and OWL
FULL. The discussion is based on experiments we conducted
with more than a dozen of existing OWL ontologies.5
One of the main difficulties is that OWL (all versions) al-
lows multiple inheritance, while M-PIRO does not (section
2). Importing an ontology with multiple inheritance currently
causes the process to fail. The need for multiple inheritance
has also been noted by authors, who often encounter cases
where, for example, a person has to be categorized as both
painter and potter. We hope to support multiple inheritance
in future versions; this requires, among others, modifications
in how the ontology is presented in the authoring tool.
Another problem is that OWL (all versions) supports prop-
erty inheritance. For example, there may be a property ?is-
player-of?, used to represent the relationship between soccer
players and their teams, and another property ?is-goalkeeper-
of?, that associates goalkeepers with their teams. The latter
is a subproperty of the former, in the sense that if X is the
goalkeeper of Y , then X is also a player of Y . The import
facilities of the authoring tool currently ignore subproperty
inheritance, because there is no corresponding notion in M-
PIRO?s ontologies; i.e., the two properties would be treated
as unrelated. Subproperty inheritance, however, could help
the generator avoid expressing information that follows from
other information it has already conveyed; e.g., if a user has
been told that X is the goalkeeper of Y , avoid saying that X
is also a player of Y . We hope to extend M-PIRO?s model
5See http://protege.stanford.edu/plugins/owl/ontologies.html.
with subproperty inheritance in future work.
A further complication is that OWL LITE allows the range
of possible values of a property to be the intersection of sev-
eral classes, while in M-PIRO?s model the values of each field
must come from a single, named entity type. A possible solu-
tion is to create automatically a new entity type in M-PIRO?s
ontology for each intersection in the OWL ontology, but this
leads back to the single inheritance problem, because the in-
tersection has to inherit from all the intersected types. This
problem is more acute in OWL DL and OWL FULL, where sev-
eral set operations (e.g., union, complement) between classes
are allowed when specifying the ranges of properties.
In OWL it is also possible to refine a property?s range.
For example, an ontology may specify that individuals of
the class ?product? have a property ?made-by?, which asso-
ciates them with individuals of the class ?manufacturer?; there
would be an rdfs:range in the definition of ?product? set-
ting the range of ?made-by? to ?manufacturer?. We may then
wish to specify that individuals of ?automobile?, a subclass
of ?product?, accept as values of ?made-by? only individu-
als of ?automobile-manufacturer?, a subclass of ?manufac-
turer?. There are mechanisms in OWL (all versions) to state
this (allValuesFrom tag), but there is no equivalent mech-
anism in M-PIRO?s ontological model. We currently ignore
range refinements when importing OWL ontologies, but this
has the risk that authors may violate refinements (e.g., when
adding individuals), creating ontologies that are no longer
compatible with the imported ones.6 Additional work is
needed to support OWL?s (all versions) someValuesFrom,
which allows stating that in set-valued properties (cf. M-
PIRO?s ?Many? column) at least one of the elements of each
set-value should belong to a particular class. A further mech-
anism in OWL DL and OWL FULL (hasValue tag) allows
specifying that all the individuals of a class have a particu-
lar value at some of their properties; e.g., that all wines of
class ?burgundy? have ?dry? taste. Such information can be
imported into M-PIRO?s generic entities (Section 3), though
the correspondence is not exact, as generic entities carry de-
fault information that may be overridden.
As already pointed out (Section 1), M-PIRO does not al-
low relationships or attributes to be declared as one-to-one.
In contrast, OWL (all versions) provides appropriate facili-
ties, as well as facilities to declare properties (relationships
or attributes) as transitive, symmetric, or the inverse of an-
other one. All such declarations are currently ignored when
importing OWL ontologies; again, this has the risk that the
authors may modify the ontologies in ways that are incom-
patible with the ignored declarations. An additional problem
in OWL FULL is that classes can be used as individuals, allow-
ing the use of relationships to associate classes, as opposed to
individuals; this violates M-PIRO?s current ontological model.
It should be clear, then, that there are still issues to be re-
solved in M-PIRO?s ontological assumptions to make M-PIRO
fully compatible with OWL LITE, and there are additional dif-
ficulties with OWL DL and OWL FULL. As discussed above,
however, most of the necessary improvements appear to be
6 ILEX and M-PIRO?s core generation engine provide some sup-
port for such refinements, but M-PIRO?s authoring tool does not.
within reach, at least for OWL LITE. Overall, it appears rea-
sonable to conclude that future versions of NLG systems like
M-PIRO?s will be able to exploit fully OWL ontologies.
5 Towards semantic browsers
We have so far proposed two ways in which OWL ontologies
can be exploited in systems like M-PIRO?s: first, the gener-
ated texts can be accompanied by OWL specifications of their
semantics, with an OWL ontology establishing the semantic
vocabulary; and, second, existing OWL ontologies can be im-
ported, to accelerate the authoring. In both cases, the on-
tologies are linked to domain-dependent language resources
(micro-plans, lexicon entries, etc.) and user stereotypes (the
interest of each field per user type, etc.), but these additional
resources are not parts of the OWL ontologies: when export-
ing M-PIRO ontologies to OWL, the authoring tool produces
additional proprietary XML files that contain the domain-
dependent language resources and stereotypes; and when im-
porting OWL ontologies developed by others, the additional
resources have to be filled in by the authors. We argue below
that agreeing upon standards on how the additional resources
could be embedded in OWL ontologies would allow NLG sys-
tems like M-PIRO to play a central role in the Semantic Web.
Note, first, that it is possible to represent in OWL M-PIRO?s
domain-dependent linguistic resources and user stereotypes.
For example, micro-plans could be treated as individuals of
a class ?Microplan? with subclasses ?ClausePlan? and ?Tem-
plate?. In a similar manner, there would be a class ?Voice?
with individuals ?active? and ?passive?, and similarly for
tenses, genders, supported languages, etc. There would also
be a class ?LexiconEntry? with subclasses ?VerbEntry? and
?NounEntry?, and individuals corresponding to the entries
of the domain-dependent lexicon. (Classes corresponding to
language resources could be grouped under a ?LinguisticRe-
source? super-class.) Then, for example, the English micro-
plan of Figure 1 would roughly be represented in OWL as:
<ClausePlan rdf:ID="painted-by-mp1-en">
<for-property rdf:resource="#painted-by" />
<for-language rdf:resource="#english" />
<use-verb rdf:resource="#paint-verb" />
<use-voice rdf:resource="#passive" />
<use-tense rdf:resource="#past" />
<use-preposition>by</use-preposition>
</ClausePlan>
Similarly, the English part of the trilingual lexicon entry
?vessel-noun? could roughly be represented in OWL as:
<NounEntry rdf:ID="vessel-noun-en">
<lexicon-entry-id>vessel-noun</lexicon-entry>
<for-language rdf:resource="#english" />
<refers-to-class rdf:resource="#vessel" />
<base-form>vase</base-form />
<has-gender rdf:resource="#neuter" />
...
</NounEntry>
One complication is that we need to establish mappings
from micro-plans to the properties (fields) they can express,
and this requires using property names as values of other
properties. This can be seen in the micro-plan above, where
we used the property (field) name ?painted-by? as the value of
property ?for-property? to signal that the micro-plan can ex-
press ?painted-by?. Using property names as values of proper-
ties, however, requires OWL FULL. There is a similar problem
with noun entries, which have to be associated with classes
(entity types) they can refer to: in the noun entry above, we
used the class name ?vessel? as the value of property ?refers-
to-class?. Using class names as values of properties again
requires OWL FULL. Similar problems arise with stereotypes.
We are currently exploring how M-PIRO?s domain-
dependent language resources and stereotypes can be best
embedded in OWL ontologies. This embedding will lead to
?language-enabled? ontologies, that will include all the re-
sources a system like M-PIRO needs to render the ontologies
in several natural languages. This opens up another possibil-
ity for publishing content on the Semantic Web: a site could
publish only its language-enabled ontology (including the in-
dividuals that correspond, for example, to the items it sells),
and the NLG technology to render the ontology in natural
language could take the form of a browser plug-in. When
visiting a site of this kind, a human user would be initially
presented with an inventory of objects that can be described
(e.g., product thumbnails). Selecting an object would trans-
mit to the browser the ontology or its relevant parts, and it
would be the responsibility of the NLG plug-in to produce
an appropriate description in the user?s language and tailor it
to the user?s type and interaction history. If the NLG com-
munity could establish standards for language-enabled on-
tologies, there could be different NLG plug-ins by different
makers, perhaps each specialising in particular languages and
user types, in the same way that there are different browsers
for HTML. There could also be a market for developers of
language-enabled ontologies for particular sectors (e.g., mu-
seums, retailers of computer equipment), who would sell
their ontologies to organisations wishing to publish content
in those sectors. The client organisations would only need to
populate the ontologies with their own individuals (e.g., ex-
hibits, products), possibly by reusing databases, and publish
them at their sites. Artificial agents would interact directly
with the ontologies of the various sites, invoking their own
NLG plug-ins to report their findings in natural language.
Establishing standards is, of course, far from trivial. For
example, different NLG systems may require very different
domain-dependent language resources, or make different as-
sumptions on which resources are domain-dependent or inde-
pendent. Nevertheless, we believe it is worth trying to move
towards this direction, as there are large potential gains for
both the NLG community and the users of the emerging Se-
mantic Web. Furthermore, the effort to establish standards
should proceed in cooperation with other fields that could ex-
ploit language-enabled ontologies. For example, the associ-
ation between entity types and noun entries can be used for
query expansion in information retrieval; and the association
between micro-plans and ontology fields can be useful in in-
formation extraction systems that populate ontologies.
6 Conclusions and future work
We have presented three ways in which an NLG system that
generates object descriptions from symbolic information can
exploit OWL ontologies, using M-PIRO?s system as an exam-
ple. First, the NLG system?s source symbolic information can
be exported in the form of an OWL ontology. Apart from
enabling other OWL-aware systems to reuse the source infor-
mation, this allows the generated texts to be accompanied by
OWL descriptions of their semantics, with the OWL ontology
establishing the semantic vocabulary. Thus, the semantics of
the generated texts become fully accessible to computer ap-
plications, such as Web agents. Second, when porting the
NLG system to a new domain, it is possible to import a pre-
existing OWL ontology, saving a significant amount of ef-
fort. Third, it is possible to embed in OWL ontologies all the
domain-dependent language resources and user modelling in-
formation that NLG systems like M-PIRO?s need. This would
allow Web sites that carry information about objects to pub-
lish their content solely in the form of OWL ontologies, pass-
ing the responsibility of generating natural language descrip-
tions to NLG browser plug-ins. The latter requires the NLG
community to develop appropriate standards.
We hope that future work will address the remaining in-
compatibilities between M-PIRO?s technology and OWL. We
also plan to explore more fully how NLG engines could be-
come central components of the Semantic Web?s browsers,
and release prototypes that will demonstrate these ideas.
References
[Androutsopoulos et al, 2002] I. Androutsopoulos, D. Spi-
liotopoulos, K. Stamatakis, A. Dimitromanolaki, V. Karkaletsis,
and C.D. Spyropoulos. Symbolic authoring for multilingual
natural language generation. In Methods and Applications of
Artificial Intelligence, number 2308 in Lecture Notes in Artificial
Intelligence, pages 131?142. Springer, 2002.
[Bontcheva and Wilks, 2004] K. Bontcheva and Y. Wilks. Auto-
matic report generation from ontologies: the MIAKT approach.
In Proc. of the 9th International Conference on Applications
of Natural Language to Information Systems, pages 324?335,
Manchester, U.K., 2004.
[Dale et al, 1998] R. Dale, S.J. Green, M. Milosavljevic, C. Paris,
C. Verspoor, and S. Williams. Dynamic document delivery: gen-
erating natural language texts on demand. In Proc. of the 9th
International Conference and Workshop on Database and Expert
Systems Applications, pages 131?136, Vienna, Austria, 1998.
[Hartley and Paris, 1997] A. Hartley and C. Paris. Multilingual
document production ? from support for translating to support
for authoring. Machine Translation, 12(1?2):109?129, 1997.
[Isard et al, 2003] A. Isard, J. Oberlander, I. Androutsopoulos, and
C. Matheson. Speaking the users? languages. IEEE Intelligent
Systems, 18(1):40?45, 2003.
[O?Donnell et al, 2001] M. O?Donnell, C. Mellish, J. Oberlander,
and A. Knott. ILEX: an architecture for a dynamic hypertext
generation system. Natural Language Engineering, 7(3):225?
250, 2001.
[Power and Cavallotto, 1996] R. Power and N. Cavallotto. Multi-
lingual generation of administrative forms. In Proc. of the 8th
International Workshop on Natural Language Generation, pages
17?19, Herstmonceux Castle, U.K., 1996.
[Van Deemter and Power, 2003] K. Van Deemter and R. Power.
High-level authoring of illustrated documents. Natural Language
Engineering, 9(2):101?126, 2003.
[Wilcock, 2003] G. Wilcock. Talking OWLs: towards an ontology
verbalizer. In Proc. of the workshop ?Human Language Technol-
ogy for the Semantic Web?, 2nd International Semantic Web Con-
ference, pages 109?112, Sanibel Island, Florida, U.S.A., 2003.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 42?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Textual Entailment using SVMs and String Similarity Measures
Prodromos Malakasiotis and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patision 76, GR-104 34 Athens, Greece
Abstract
We present the system that we submitted to
the 3rd Pascal Recognizing Textual Entail-
ment Challenge. It uses four Support Vector
Machines, one for each subtask of the chal-
lenge, with features that correspond to string
similarity measures operating at the lexical
and shallow syntactic level.
1 Introduction
Textual Entailment is desirable in many natural lan-
guage processing areas, such as question answer-
ing, information extraction, information retrieval,
and multi-document summarization. In the Pascal
Recognizing Textual Entailment Challenge (RTE), it
is defined as the task of deciding whether or not the
meaning of a hypothesis text (H) can be inferred
from the meaning of another text (T ).1 For instance:
T : The drugs that slow down or halt Alzheimer?s disease
work best the earlier you administer them.
H: Alzheimer?s disease is treated using drugs.
is a correct entailment pair, but the following is not:
T : Drew Walker, NHS Tayside?s public health director, said:
?It is important to stress that this is not a confirmed case
of rabies.?
H: A case of rabies was confirmed.
In previous RTE challenges (Dagan et al, 2006;
Bar-Haim et al, 2006), several machine-learning ap-
proaches appeared, but their results showed that sig-
nificant improvements were still necessary. In this
paper, we present the system we used in the third
1See http://www.pascal-network.org/.
RTE challenge. The latter had four different devel-
opment and test sets (QA, IR, IE, SUM), intended to
evaluate textual entailment recognition in the four
natural language processing areas mentioned above.
2 System overview
Our system uses SVMs (Vapnik, 1998) to determine
whether each T?H pair constitutes a correct tex-
tual entailment or not. In particular, it employs four
SVMs, each trained on the development dataset of
the corresponding RTE subtask (QA, IR, IE, SUM)
and used on the corresponding test dataset. Pre-
liminary experiments indicated that training a single
SVM on all four subsets leads to worse results, de-
spite the increased size of the training set, presum-
ably because of differences in how the pairs were
constructed in each subtask, which do not allow a
single SVM to generalize well over all four.
The system is based on the assumption that string
similarity at the lexical and shallow syntactic level
can be used to identify textual entailment reason-
ably well, at least in question answering, the main
area we are interested in. We, therefore, try to cap-
ture different kinds of similarity by employing 10
different string similarity measures, to be discussed
below. In each T?H case, every measure is applied
to the following 8 pairs of strings, producing a total
of 80 measurements:
pair 1: two strings with the original words of T and
H , respectively; although we refer to ?words?,
this and the following string pairs also contain
non-word tokens, such as punctuation.2
2We use OPENNLP?s tokenizer, POS-tagger, and chunker (see
http://opennlp.sourceforge.net/), and our own
implementation of Porter?s stemmer.
42
pair 2: two strings containing the corresponding
stems of the words of T and H , respectively;
pair 3: two strings containing the part-of-speech
(POS) tags of the words of T and H;
pair 4: two strings containing the chunk tags (see
below) of the words of T and H;
pair 5: two strings containing only the nouns of T
and H , as identified by a POS-tagger;
pair 6: two strings containing only the stems of the
nouns of T and H;
pair 7: two strings containing only the verbs of T
and H , as identified by a POS-tagger;
pair 8: two strings containing only the stems of the
verbs of T and H .
Chunk tags are of the form B-x, I-x or O, were B and
I indicate the initial and other words of the chunks,
respectively, whereas O indicates words outside all
chunks; x can be NP, VP, or PP, for noun phrase,
verb phrase, and prepositional phrase chunks.
Partial matches: When applying the string simi-
larity measures, one problem is that T may be much
longer than H , or vice versa. Consider, for exam-
ple, the following T?H pair. The difference in the
lengths of T and H may mislead many similarity
measures to indicate that the two texts are very dis-
similar, even though H is included verbatim in T .
T : Charles de Gaulle died in 1970 at the age of eighty. He
was thus fifty years old when, as an unknown officer re-
cently promoted to the (temporary) rank of brigadier gen-
eral, he made his famous broadcast from London reject-
ing the capitulation of France to the Nazis after the deba-
cle of May-June 1940.
H: Charles de Gaulle died in 1970.
To address this problem, when we consider a pair
of strings (s1, s2), if s1 is longer than s2, we also
compute the ten values fi(s?1, s2), where fi (1 ? i ?
10) are the string similarity measures, for every s?1
that is a substring of s1 of the same length as s2. We
then locate the s?1 with the best average similarity to
s2, shown below as s??1 :
s??1 = argmax
s?1
10?
i=1
fi(s
?
1, s2)
and we keep the ten fi(s??1 , s2) values and their aver-
age as 11 additional measurements. Similarly, if s2
is longer than s1, we keep the ten fi(s1, s??2 ) values
and their average. This process could be applied to
all pairs 1?8 above, but the system we submitted ap-
plied it only to pairs 1?4; hence, there is a total of 44
additional measurements in each T?H case.
The 124 measurements discussed above provide
124 candidate numeric features that can be used by
the SVMs.3 To those, we add the following four:
Negation: Two Boolean features, showing if T or
H , respectively, contain negation, identified by
looking for words like ?not?, ?won?t?, etc.
Length ratio: This is min(LT ,LH)max(LT ,LH) , were LT and
LH are the lengths, in words, of T and H .
Text length: Binary feature showing if the markup
of the dataset flags T as ?long? or ?short?.
Hence, there are 128 candidate features in total.
From those, we select a different subset for the
SVM of each subtask, as will be discussed in fol-
lowing sections. Note that similarity measures have
also been used in previous RTE systems as fea-
tures in machine learning algorithms; see, for ex-
ample, Kozareva and Montoyo (2006), Newman et
al. (2006). However, the results of those systems in-
dicate that improvements are still necessary, and we
believe that one possible improvement is the use of
more and different similarity measures.
We did not use similarity measures that operate
on parse trees or semantic representations, as we are
interested in RTE methods that can also be applied to
less spoken languages, where reliable parsers, fact
extractors, etc. are often difficult to obtain.
2.1 String similarity measures
We now describe the ten string similarity measures
that we use.4 The reader is reminded that the mea-
sures are applied to string pairs (s1, s2), where s1
and s2 derive from T and H , respectively.
Levenshtein distance: This is the minimum num-
ber of operations (edit distance) needed to transform
one string (in our case, s1) into the other one (s2),
3All feature values are normalized in [?1, 1].
4We use the SIMMETRICS library; see http://www.
dcs.shef.ac.uk/?sam/simmetrics.html.
43
where an operation is an insertion, deletion, or sub-
stitution of a single character. In pairs of strings that
contain POS or chunk tags, it would be better to con-
sider operations that insert, delete, or substitute en-
tire tags, instead of characters, but the system we
submitted did not do this; we addressed this issue in
subsequent work, as will be discussed below.
Jaro-Winkler distance: The Jaro-Winkler dis-
tance (Winkler, 1999) is a variation of the Jaro dis-
tance (Jaro, 1995), which we describe first. The Jaro
distance dj of s1 and s2 is defined as:
dj(s1, s2) =
m
3 ? l1
+
m
3 ? l2
+
m ? t
3 ? m
,
where l1 and l2 are the lengths (in characters) of s1
and s2, respectively. The value m is the number of
characters of s1 that match characters of s2. Two
characters from s1 and s2, respectively, are taken to
match if they are identical and the difference in their
positions does not exceed max(l1,l2)2 ? 1. Finally, to
compute t (?transpositions?), we remove from s1 and
s2 all characters that do not have matching charac-
ters in the other string, and we count the number of
positions in the resulting two strings that do not con-
tain the same character; t is half that number.
The Jaro-Winkler distance dw emphasizes prefix
similarity between the two strings. It is defined as:
dw(s1, s2) = dj(s1, s2) + l ? p ? [1 ? dj(s1, s2)],
where l is the length of the longest common prefix
of s1 and s2, and p is a constant scaling factor that
also controls the emphasis placed on prefix similar-
ity. The implementation we used considers prefixes
up to 6 characters long, and sets p = 0.1.
Again, in pairs of strings (s1, s2) that contain POS
tags or chunk tags, it would be better to apply this
measure to the corresponding lists of tags in s1 and
s2, instead of treating s1 and s2 as strings of char-
acters, but the system we submitted did not do this;
this issue was also addressed in subsequent work.
Soundex: Soundex is an algorithm intended to
map each English name to an alphanumeric code,
so that names whose pronunciations are the same
are mapped to the same code, despite spelling dif-
ferences.5 Although Soundex is intended to be used
5See http://en.wikipedia.org/wiki/Soundex.
on names, and in effect considers only the first let-
ter and the first few consonants of each name, we
applied it to s1 and s2, in an attempt to capture simi-
larity at the beginnings of the two strings; the strings
were first stripped of all white spaces and non-letter
characters. We then computed similarity between
the two resulting codes using the Jaro-Winkler dis-
tance. A better approach would be to apply Soundex
to all words in T and H , forming a 9th pair (s1, s2),
on which other distance measures would then be ap-
plied; we did this in subsequent work.
Manhattan distance: Also known as City Block
distance or L1, this is defined for any two vectors
~x = ?x1, . . . , xn? and ~y = ?y1, . . . , yn? in an n-
dimensional vector space as:
L1(~x, ~y) =
n?
i=1
|xi ? yi|.
In our case, n is the number of distinct words (or
tags) that occur in s1 and s2 (in any of the two);
and xi, yi show how many times each one of these
distinct words occurs in s1 and s2, respectively.
Euclidean distance: This is defined as follows:
L2(~x, ~y) =
?
?
?
?
n?
i=1
(xi ? yi)2.
In our case, ~x and ~y correspond to s1 and s2, respec-
tively, as in the previous measure.
Cosine similarity: The definition follows:
cos(~x, ~y) =
~x ? ~y
?~x? ? ?~y?
.
In our system ~x and ~y are as above, except that they
are binary, i.e., xi and yi are 1 or 0, depending on
whether or not the corresponding word (or tag) oc-
curs in s1 or s2, respectively.
N-gram distance: This is the same as L1, but in-
stead of words we use all the (distinct) character n-
grams in s1 and s2; we used n = 3.
Matching coefficient: This is |X ? Y |, where X
and Y are the sets of (unique) words (or tags) of s1
and s2, respectively; i.e., it counts how many com-
mon words s1 and s2 have.
44
Dice coefficient: This is the following quantity; in
our case, X and Y are as in the previous measure.
2 ? |X ? Y |
|X| + |Y |
Jaccard coefficient: This is defined as |X?Y ||X?Y | ;
again X and Y are as in the matching coefficient.
2.2 SVM tuning and feature selection
As already noted, we employed four SVMs, one for
each subtask of the challenge (IR, IE, QA, SUM).6
In each subtask, feature selection was performed as
follows. We started with a set of 20 features, which
correspond to the ten similarity measures applied to
both words and stems (string pairs 1 and 2 of section
1); see table 1. We then added the 10 features that
correspond to the ten similarity measures applied to
POS tags (string pair 3). In IE and IR, this addi-
tion led to improved leave-one-out cross-validation
results on the corresponding development sets, and
we kept the additional features (denoted by ?X? in
table 1). In contrast, in QA and SUM the additional
10 features were discarded, because they led to no
improvement in the cross-validation. We then added
the 10 features that corresponded to the ten similar-
ity measures applied to chunk tags (string pair 4),
which were retained only in the IE SVM, and so on.
The order in which we considered the various ex-
tensions of the feature sets is the same as the order of
the rows of table 1, and it reflects the order in which
it occurred to us to consider the corresponding ad-
ditional features while preparing for the challenge.
We hope to investigate additional feature selection
schemes in further work; for instance, start with all
128 features and explore if pruning any groups of
features improves the cross-validation results.
With each feature set that we considered, we
actually performed multiple leave-one-out cross-
validations on the development dataset, for different
values of the parameters of the SVM and kernel, us-
ing a grid-search utility. Each feature set was eval-
uated by considering its best cross-validation result.
The best cross-validation results for the final feature
sets of the four SVMs are shown in table 2.
6We use LIBSVM (Chang and Lin, 2001), with a Radial Basis
Function kernel, including LIBSVM?s grid search tuning utility.
Subtask Accuracy (%)
QA 86.50 (90.00)
IR 80.00 (75.50)
SUM 73.00 (72.50)
IE 62.00 (61.50)
all 75.38 (74.88)
Table 2: Best cross-validation results of our system
on the development datasets. Results with subse-
quent improvements are shown in brackets.
Subtask Accuracy (%) Average Precision (%)
QA 73.50 (76.00) 81.03 (81.08)
IR 64.50 (63.50) 63.61 (67.28)
SUM 57.00 (60.50) 60.88 (61.58)
IE 52.00 (49.50) 58.16 (51.57)
all 61.75 (62.38) 68.08 (68.28)
Table 3: Official results of our system. Results with
subsequent improvements are shown in brackets.
3 Official results and discussion
We submitted only one run to the third RTE chal-
lenge. The official results of our system are shown
in table 3.7 They are worse than the best results we
had obtained in the cross-validations on the devel-
opment datasets (cf. table 2), but this was expected
to a large extent, since the SVMs were tuned on the
development datasets; to some extent, the lower of-
ficial results may also be due to different types of
entailment being present in the test datasets, which
had not been encountered in the training sets.
As in the cross-validation results, our system per-
formed best in the QA subtask; the second and third
best results of our system were obtained in IR and
SUM, while the worst results were obtained in IE.
Although a more thorough investigation is neces-
sary to account fully for these results, it appears that
they support our initial assumption that string simi-
larity at the lexical and shallow syntactic level can be
used to identify textual entailment reasonably well
in question answering systems. Some further reflec-
tions on the results of our system follow.
In the QA subtask of the challenge, it appears that
each T was a snippet returned by a question answer-
ing system for a particular question.8 We are not
aware of exactly how the T s were selected by the
7See the RTE Web site for a definition of ?average precision?.
8Consult http://www.pascal-network.org/Chal
lenges/RTE3/Introduction/.
45
Feature sets features IE IR QA SUM
similarity measures on words 10 X X X X
similarity measures on stems 10 X X X X
+ similarity measures on POS tags +10 X X
+ similarity measures on chunk tags +10 X X
+ average of sim. measures on words of best partial match +1 X
+ average of sim. measures on stems of best partial match +1 X X
+ average of sim. measures on POS tags of best partial match +1 X X
+ average of sim. measures on chunk tags of best partial match +1 X X
+ similarity measures on words of best partial match +10
+ similarity measures on stems of best partial match +10 X
+ similarity measures on POS tags of best partial match +10 X
+ similarity measures on chunk tags of best partial match +10
+ negation +2 X
+ length ratio +1 X
+ similarity measures on nouns +10 X
+ similarity measures on noun stems +10
+ similarity measures on verbs +10 X
+ similarity measures on verb stems +10
+ short/long T +1 X X
Total 128 64 31 23 54
Table 1: Feature sets considered and chosen in each subtask.
systems used, but QA systems typically return T s
that contain the expected answer type of the input
question; for instance, if the question is ?When did
Charles de Gaulle die??, T will typically contain a
temporal expression. Furthermore, QA systems typi-
cally prefer T s that contain many words of the ques-
tion, preferably in the same order, etc. (Radev et
al., 2000; Ng et al, 2001; Harabagiu et al, 2003).
Hence, if the answers are sought in a document col-
lection with high redundancy (e.g., the Web), i.e.,
a collection where each answer can be found with
many different phrasings, the T s (or parts of them)
that most QA systems return are often very similar,
in terms of phrasings, to the questions, provided that
the required answers exist in the collection.
In the QA datasets of the challenge, for each T ,
which was a snippet returned by a QA system for a
question (e.g., ?When did Charle de Gaulle die??),
an H was formed by ?plugging into? the question
an expression of the expected answer type from T .
In effect, this converted all questions to propositions
(e.g., ?Charle de Gaulle died in 1970.?) that require
a ?yes? or ?no? answer. Note that this plugging in
does not always produce a true proposition; T may
contain multiple expressions of the expected answer
type (e.g., ?Charle de Gaulle died in 1970. In 1990,
a monument was erected. . . ?) and the wrong one
may be plugged into the question (H = ?Charle de
Gaulle died in 1990.?).
Let us first consider the case where the proposi-
tion (H) is true. Assuming that the document collec-
tion is redundant and that the answer to the question
exists in the collection, T (or part of it) will often be
very similar to H , since it will be very similar to the
question that H was derived from. In fact, the simi-
larity between T andH may be greater than between
T and the question, since an expression from T has
been plugged into the question to form H . Being
very similar, T will very often entail H , and, hence,
the (affirmative) responses of our system, which are
based on similarity, will be correct.
Let us now consider the case whereH is false. Al-
though the same arguments apply, and, hence, one
might again expect T to be very similar to H , this
is actually less likely now, because H is false and,
hence, it is more difficult to find a very similarly
phrased T in the presumed trustful document collec-
tion. The reduced similarity between T and H will
lead the similarity measures to suggest that the T?H
entailment does not hold; and in most cases, this is a
correct decision, because H is false and, thus, it can-
not be entailed by a (true) T that has been extracted
from a trustful document collection.
Similar arguments apply to the IR subtask, where
our system achieved its second best results. Our re-
sults in this subtask were lower than in the QA sub-
46
task, presumably because the T s were no longer fil-
tered by the additional requirement that they must
contain an expression of the expected answer type.
We attribute the further deterioration of our re-
sults in the SUM subtask to the fact that, accord-
ing to the challenge?s documentation, all the T?H
pairs of that subtask, both true and false entailments,
were chosen to have high lexical similarity, which
does not allow the similarity measures of our system
to distinguish well between the two cases. Finally,
the lower results obtained in the IE subtask may be
due to the fact that the T?H pairs of that subtask
were intended to reflect entailments identified by in-
formation extraction systems, which specialize on
identifying particular semantic relations by employ-
ing more complicated machinery (e.g., named entity
recognizers and matchers, fact extractors, etc.) than
simple string similarity measures; the results may
also be partly due to the four different ways that
were used to construct the T?H pairs of that sub-
task. It is interesting to note (see table 1) that the
feature sets were larger in the subtasks where our
system scored worse, which may be an indication of
the difficulties the corresponding SVMs encountered.
4 Conclusions and further work
We presented a textual entailment recognition sys-
tem that relies on SVMs whose features correspond
to string similarity measures applied to the lexical
and shallow syntactic level. Experimental results in-
dicate that the system performs reasonably well in
question answering (QA), which was our main tar-
get, with results deteriorating as we move to infor-
mation retrieval (IR), multi-document summariza-
tion (SUM), and information extraction (IE).
In work carried out after the official submission
of our system, we incorporated two of the possible
improvements that were mentioned in previous sec-
tions: we treated strings containing POS or chunk
tags as lists of tags; and we applied Soundex to each
word of T and H , forming a 9th pair of strings, on
which all other similarity measures were applied;
feature selection was then repeated anew. The cor-
responding results are shown in brackets in tables
2 and 3. There was an overall improvement in all
tasks (QA, IR, SUM), except for IE, where textual en-
tailment is more difficult to capture via textual simi-
larity, as commented above. We have suggested two
additional possible improvements: applying partial
matching to all of the string pairs that we consider,
and investigating other feature selection schemes. In
future work, we also plan to exploit WordNet to cap-
ture synonyms, hypernyms, etc.
Acknowledgements
This work was funded by the Greek PENED 2003 programme,
which is co-funded by the European Union (75%), and the
Greek General Secretariat for Research and Technology (25%).
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo,
B. Magnini, and I. Szpektor. 2006. The 2nd PASCAL recog-
nising textual entailment challenge. In Proceedings of the
2nd PASCAL Challenges Workshop on Recognising Textual
Entailment, Venice, Italy.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL
recognising textual entailment challenge. In Quin?onero-
Candela et al, editor, MLCW 2005, LNAI, volume 3904,
pages 177?190. Springer-Verlag.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. Open-
domain textual question answering techniques. Natural Lan-
guage Engineering, 9(3):231?267.
M.A. Jaro. 1995. Probabilistic linkage of large public health
data file. Statistics in Medicine, 14:491?498.
Z. Kozareva and A. Montoyo. 2006. MLENT: The machine
learning entailment system of the University of Alicante. In
Proc. of 2nd PASCAL Challenges Workshop on Recognising
Textual Entailment, Venice, Italy.
E. Newman, J. Dunnion, and J. Carthy. 2006. Constructing a
decision tree classifier using lexical and syntactic features.
In Proc. of 2nd PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
H.T. Ng, J.L.P. Kwan, and Y. Xia. 2001. Question answering
using a large text database: A machine learning approach. In
Proc. of Empirical Methods in Natural Language Process-
ing, Carnegie Mellon Univ., PA.
D.R. Radev, J. Prager, and V. Samn. 2000. Ranking suspected
answers to natural language questions using predictive an-
notation. In Proc. of NAACL-ANLP, pages 150?157, Seattle,
WA.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
W.E. Winkler. 1999. The state of record linkage and current
research problems. Statistical Research Report RR99/04, US
Bureau of the Census, Washington, DC.
47
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270?1279,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Finding Short Definitions of Terms on Web Pages
Gerasimos Lampouras
?
and Ion Androutsopoulos
?+
?
Department of Informatics, Athens University of Economics and Business, Greece
+
Digital Curation Unit, Research Centre ?Athena?, Athens, Greece
Abstract
We present a system that finds short def-
initions of terms on Web pages. It em-
ploys a Maximum Entropy classifier, but it
is trained on automatically generated ex-
amples; hence, it is in effect unsupervised.
We use ROUGE-W to generate training ex-
amples from encyclopedias and Web snip-
pets, a method that outperforms an alter-
native centroid-based one. After training,
our system can be used to find definitions
of terms that are not covered by encyclo-
pedias. The system outperforms a compa-
rable publicly available system, as well as
a previously published form of our system.
1 Introduction
Definitions of terms are among the most com-
mon types of information users search for on the
Web. In the TREC 2001 QA track (Voorhees,
2001), where the distribution of question types re-
flected that of real user logs, 27% of the ques-
tions were requests for definitions (e.g., ?What is
gasohol??, ?Who was Duke Ellington??). Conse-
quently, some Web search engines provide special
facilities (e.g., Google?s ?define:? query prefix)
that seek definitions of user-specified terms in on-
line encyclopedias or glossaries; to save space, we
call both ?encyclopedias?. There are, however, of-
ten terms that are too recent, too old, or less widely
used to be included in encyclopedias. Their defi-
nitions may be present on other Web pages (e.g.,
newspaper articles), but they may be provided in-
directly (e.g., ?He said that gasohol, a mixture of
gasoline and ethanol, has been great for his busi-
ness.?) and they may be difficult to locate with
generic search engines that may return dozens of
pages containing, but not defining the terms.
We present a system to find short definitions
of user-specified terms on Web pages. It can be
used as an add-on to generic search engines, when
no definitions can be found in on-line encyclope-
dias. The system first invokes a search engine us-
ing the (possibly multi-word) term whose defini-
tion is sought, the target term, as the query. It
then scans the top pages returned by the search
engine to locate 250-character snippets with the
target term at their centers; we call these snippets
windows. The windows are candidate definitions
of the target term, and they are then classified as
acceptable (positive class) or unacceptable (nega-
tive class) using supervised machine learning. The
system reports the windows for which it is most
confident that they belong in the positive class. Ta-
ble 1 shows examples of short definitions found by
our system. In our experiments, we allow the sys-
tem to return up to five windows per target term,
and the system?s response is counted as correct if
any of the returned windows contains an accept-
able short definition of the target. This is similar
to the treatment of definition questions in TREC
2000 and 2001 (Voorhees, 2000; Voorhees, 2001),
but the answer is sought on the Web, not in a given
document collection of a particular genre.
More recent TREC QA tracks required definition
questions to be answered by lists of complemen-
tary text snippets, jointly providing required or op-
tional information nuggets (Voorhees, 2003). In
contrast, we focus on locating single snippets that
include self-contained short definitions. Despite
its simpler nature, we believe the task we address
is of practical use: a list of single-snippet defini-
tions from Web pages accompanied by the source
URLs is a good starting point for users seeking
definitions of terms not covered by encyclopedias.
We also note that evaluating multi-snippet defini-
tions can be problematic, because it is often dif-
ficult to agree which information nuggets should
be treated as required, or even optional (Hilde-
brandt et al, 2004). In contrast, earlier experimen-
tal results we have reported (Androutsopoulos and
Galanis, 2005) show strong inter-assessor agree-
ment (K > 0.8) for single-snippet definitions (Eu-
genio and Glass, 2004). The task we address also
differs from DUC?s query focused summarization
(Dang, 2005; Dang, 2006). Our queries are sin-
gle terms, whereas DUC queries are longer topic
1270
Target term: Babesiosis
(...) Babesiosis is a rare, severe and sometimes fatal tick-
borne disease caused by various types of Babesia, a micro-
scopic parasite that infects red blood cells. In New York
state, the causative parasite is babesia microti. Who gets
Babesiosis? Babesiosis (...)
Target term: anorexia nervosa
(...) anorexia nervosa is an illness that usually occurs in
teenage girls, but it can also occur in teenage boys, and adult
women and men. People with anorexia are obsessed with
being thin. They lose a lot of weight and are terrified of
gaining weight. The (...)
Target term: Kinabalu
(...) one hundred and thirty eight kilometers from Kota Kin-
abalu, the capital of the Malaysian state of Sabah, rises the
majestic mount Kinabalu. With its peak at 4,101 meters
(and growing), mount Kinabalu is the highest mountain in
south-east Asia. This (...)
Target term: Pythagoras
(...) Pythagoras of Samos about 569 BC - about 475
BC click the picture above to see eleven larger pictures
Pythagoras was a Greek philosopher who made important
developments in mathematics, astronomy, and the theory of
music. The theorem now known as (...)
Target term: Sacajawea
(...) Sacajawea was a Shoshone Indian princess. The
Shoshone lived from the rocky mountains to the plains.
They lived primarily on buffalo meat. The shoshone trav-
eled for many days searching for buffalo. They hunted on
horseback using the buffalo for food (...)
Target term: tale of Genji
(...) the tale of Genji This site aims to promote a wider
understanding and appreciation of the tale of Genji - the
11th century Japanese classic written by a Heian court lady
known asMurasaki Shikibu. It also serves as a kind of travel
guide to the world (...)
Target term: Jacques Lacan
(...) who is Jacques Lacan? John Haber in New York
city a primer for pre-post-structuralists Jacques Lacan is a
Parisian psychoanalyst who has influenced literary criticism
and feminism. He began work in the 1950s, in the Freudian
society there. It was a (...)
Table 1: Definitions found by our system.
descriptions, often entire paragraphs; furthermore,
we do not attempt to compose coherent and cohe-
sive summaries from several snippets.
The system we present is based on our ear-
lier work (Miliaraki and Androutsopoulos, 2004),
where an SVM classifier (Cristianini and Shawe-
Taylor, 2000) was used to separate acceptable win-
dows from unacceptable ones; the SVM also re-
turned confidence scores, which were used to rank
the acceptable windows. On datasets from the
TREC 2000 and 2001 QA tracks, our earlier sys-
tem clearly outperformed the methods of Joho and
Sanderson (2000; 2001) and Prager et al (2001;
2002), as reported in previous work (Miliaraki
and Androutsopoulos, 2004). To train the SVM,
however, thousands of training windows were re-
quired, each tagged as a positive or negative exam-
ple. Obtaining large numbers of training windows
is easy, but manually tagging them is very time-
consuming. In the TREC 2000 and 2001 datasets,
it was possible to tag the training windows auto-
matically by using training target terms and ac-
companying regular expression patterns provided
by the TREC organizers. The regular expressions
covered all the known acceptable definitions of the
corresponding terms that can be extracted from the
datasets. When the training windows, however,
are obtained from the Web, it is impossible to con-
struct manually regular expressions for all the pos-
sible phrasings of the acceptable definitions in the
training windows.
In subsequent work (Androutsopoulos and
Galanis, 2005), we developed ATTW (automatic
tagging of training windows), a technique that pro-
duces arbitrarily large collections of training win-
dows from the Web with practically no manual
effort, in effect making our overall system unsu-
pervised. ATTW uses training terms for which
several encyclopedia definitions are available, and
compares each Web training window (each win-
dow extracted from the pages the search engine
returned for a training term) to the corresponding
encyclopedia definitions. Web training windows
that are very similar (or dissimilar) to the corre-
sponding encyclopedia definitions are tagged as
positive (or negative) examples; if the similarity is
neither too high nor too low, the window is not in-
cluded in the classifier?s training data. Previously
reported experiments (Androutsopoulos and Gala-
nis, 2005) showed that ATTW leads to significantly
better results, compared to training the classifier
on all the available TREC windows, for which reg-
ular expressions are available, and then using it to
classify Web windows.
Note that in ATTW the encyclopedia definitions
are used only during training. Once the classifier
has been trained, it can be used to discover defini-
tions on arbitrary Web pages. In fact, during test-
ing we discard windows originating from on-line
encyclopedias, simulating the case where we seek
definitions of terms not covered by encyclopedias;
we also ignore windows from on-line encyclope-
dias during training. Also, note that the classifier
is trained on Web windows, not directly on ency-
clopedia definitions, which allows it to avoid rely-
ing excessively on phrasings that are common in
encyclopedia definitions, but uncommon in more
indirect definitions of arbitrary Web pages. Fur-
1271
thermore, training the classifier directly on ency-
clopedia definitions would not provide negative
examples.
In our previous work with ATTW (Androut-
sopoulos and Galanis, 2005) we used a mea-
sure constructed by ourselves to assess the sim-
ilarity between Web windows and encyclopedia
definitions. Here, we use the more established
ROUGE-W measure (Lin, 2004) instead. ROUGE-
W and other versions of ROUGE have been used in
summarization to measure how close a machine-
authored summary is to multiple human sum-
maries of the same input. We use ROUGE-W in
a similar setting, to measure how close a training
window is to multiple encyclopedia definitions of
the same term. A further difference from our pre-
vious work is that we also use ROUGE-W when
computing the features of the windows to be clas-
sified. Previously, the SVM relied, among others,
on Boolean features indicating if the target term
was preceded or followed in the window to be
classified by a particular phrase indicating a def-
inition (e.g., ?target, a kind of?, ?such as target?).
The indicative phrases are selected automatically
during training, but now the corresponding fea-
tures are not Boolean; their values are the ROUGE-
W similarity scores between an indicative phrase
and the context of the target term in the window.
This allows the system to soft-match the phrases
to the windows (e.g., encountering ?target, another
kind of?, instead of ?target, a kind of?).
1
In our new system we also use a Maximum En-
tropy (MAXENT) classifier (Ratnaparkhi, 1997) in-
stead of an SVM, because much faster implemen-
tations of the former are available.
2
We present
experimental results showing that our new sys-
tem significantly outperforms our previously pub-
lished one. The use of the MAXENT classifier by it-
self improved slightly our results, but the improve-
ments come mostly from using ROUGE-W.
Apart from presenting an improved version of
our system, the main contribution of this paper is a
detailed experimental comparison of our new sys-
tem against Cui et al?s (2004; 2005; 2006; 2007).
The latter is particularly interesting, because it
is well published, it includes both an alterna-
tive, centroid-based technique to automatically tag
training examples and a soft-matching classifier,
1
We also experimented with other similarity measures
(e.g., edit distance) and ROUGE variants, but we obtained the
best results with ROUGE-W.
2
We use Stanford?s classifier; see http://nlp.stanford.edu/.
and it is publicly available.
3
We show that ATTW
outperforms Cui et al?s centroid-based technique,
and that our overall system is also clearly better
than Cui et al?s in the task we address.
Section 2 discusses ATTW with ROUGE-W, Cui
et al?s centroid-based method to tag training ex-
amples, and experiments showing that ATTW is
better. Section 3 describes our new overall system,
the system of Cui et al, and the baselines. Sec-
tion 4 reports experimental results showing that
our system is better than Cui et al?s, and better
than our previously published system. Section 5
discusses related work; and section 6 concludes.
2 Tagging training windows
During both training and testing, for each tar-
get term we keep the r most highly ranked Web
pages the search engine returns. We then extract
the first f windows of the target term from each
page, since early occurrences of the target terms
on pages are more likely to be definitions. We,
thus, obtain r ? f windows per term.
4
When test-
ing, we return the k windows of the target term
that the classifier is most certain they belong in the
positive class. In our experiments, r = 10, f = 5,
k = 5. During training, we train the classifier on
the q ? r ? f windows we obtain for q training tar-
get terms; in our experiments, q ranged from 50 to
1500. Training requires tagging first the training
windows as positive or negative, possibly discard-
ing windows that cannot be tagged automatically.
2.1 ATTW with ROUGE-W similarity
To tag a training window w of a training term t
with ATTW and ROUGE-W, we obtain a set C
t
of
definitions of t from encyclopedias.
5
Stop-words,
punctuation, and non-alphanumeric characters are
removed from C
t
and w, and a stemmer is ap-
plied; the testing windows undergo the same pre-
processing.
6
For each definition d ? C
t
, we find
the longest common word subsequence of w and
d. If w is the word sequence ?A,B, F,C,D,E?
3
See http://www.cuihang.com/software.html. The soft-
ware and a demo of our system, and the datasets we used
are also freely available; see http://nlp.cs.aueb.gr/.
4
We used Altavista in our experiments. We remove HTML
tags and retain only the plain text of the pages.
5
The training terms were randomly selected from the in-
dex of http://www.encyclopedia.com/. We used Google?s
?define:? to obtain definitions from other encyclopedias.
6
We use the 100 most frequent words of the BNC corpus
(http://www.natcorp.ox.ac.uk/) as the stop-list, and Porter?s
stemmer (http://tartarus.org/?martin/PorterStemmer/).
1272
and d = ?A,B,E,C,G,D?, the longest com-
mon subsequence is ?A,B,C,D?. The longest
common subsequence is divided into consecutive
matches, producing in our example ?A,B|C|D?.
We then compute the following score (weighted
longest common subsequence), where m is the
number of consecutive matches, k
i
is the length
of the i-th consecutive match, and f is a weight-
ing function. We use f(k) = k
a
, where a > 1 is a
parameter we tune experimentally.
WLCS (w, d) =
?
m
i=0
f(k
i
)
We then compute the following quantities, where
|?| is word length, and f
?1
is the inverse of f .
P (w, d) = f
?1
(
WLCS(w,d)
f(|w|)
)
R(w, d) = f
?1
(
WLCS(w,d)
f(|d|)
)
F (w, d) =
(1+?
2
)?R(w,d)?P (w,d)
R(w,d)+?
2
?P (w,d)
In effect, P (w, d) examines how close the
longest common substring is to w and R(w, d)
how close it is to d. Following Lin (2004), we use
? = 8, assigning greater importance toR(w, d). If
R(w, d) is high, the longest common substring is
very similar to d; then w (which also includes the
longest common substring) intuitively contains al-
most all the information of d, i.e., all the informa-
tion of a known acceptable definition (high recall).
If P (w, d) is high, the longest common substring
is very similar to w; then d (which also includes
the longest common substring) contains almost all
the information of w, i.e., w does not contain any
(redundant) information not included in a known
acceptable definition, something we care less for.
The ROUGE-W similarity sim(w,C
t
) between
w and C
t
is the maximum F (w, d), for all d ?
C
t
. Training windows with sim(w,C
t
) > T
+
are
tagged as positive; if sim(w,C
t
) < T
?
, they are
tagged as negative; and if T
?
? sim(w,C
t
) ?
T
+
, they are discarded. We tune the thresholds T
+
and T
?
experimentally, as discussed below.
2.2 The centroid-based tagging approach
This method is used in the system of Cui et al
(2004; 2005; 2006; 2007). For each training target
term, we construct a ?centroid? pseudo-text con-
taining the words that co-occur most frequently
with the target term. We then compute the similar-
ity between each training window and the centroid
of its target term. If it exceeds a threshold, the win-
dow is tagged as positive; Cui et al produce only
positive examples.
The centroid of a training target term t is con-
structed as follows. For each word u in t?s training
windows, we compute the centrality score defined
below, where SF
t
is the number of t?s training
windows, SF
u
is the number of u?s windows that
can be extracted from the retained Web pages the
search engine returned for t, SF
t?u
is the number
of windows on the same pages that contain both
t and u, and idf(u) is the inverse document fre-
quency of w.
7
Centrality scores are pointwise mu-
tual information with an extra idf (u) factor.
centrality(u) = ?log(
SF
t?u
SF
t
+SF
u
) ? idf (u)
The words u whose centrality scores exceed the
mean by at least a standard deviation are added
to the centroid of t. Before computing the cen-
trality scores, stop-words, punctuation, and non-
alphanumeric characters are removed, and a stem-
mer is applied, as in ATTW. The similarities be-
tween training windows and centroids are then
computed using cosine similarity, after turning the
centroids and windows into binary vectors that
show which words they contain.
2.3 Comparing the tagging approaches
To evaluate the two methods that tag training win-
dows, we selected randomly q = 200 target terms,
different from those used for training and testing.
We collected the q ? r ? f = 200 ? 10 ? 5 windows
from the corresponding Web pages, we selected
randomly 400 from the collected 10,000 windows,
and tagged them manually as positive or negative.
Figure 1 plots the positive precision of the two
methods against their positive recall, and figure 2
shows negative precision against negative recall.
For different values of T
+
, we obtain a different
point in figure 1; similarly for T
?
and figure 2.
Positive precision is TP/(TP +FP), positive re-
call is TP/(TP + FN ), and likewise for nega-
tive precision and recall; TP (true positives) are
the positive training windows the method has cor-
rectly tagged as positive, FP are the negative win-
dows the method has tagged as positives etc.
For very high (strict) T
+
values, the methods tag
very few (or none) training windows as positive;
hence, both TP and TP + FP approach (or be-
come) zero; we take positive precision to be zero
in that case. Positive recall also approaches (or be-
comes) zero, which is why both positive recall and
7
We obtained idf (u) from BNC. Cui et al use sentences
instead of windows, reducing the risk of truncating defini-
tions. We used windows in all systems, to compare fairly.
1273
Figure 1: Results of generating positive examples.
Figure 2: Results of generating negative examples.
precision reach zero in the left of figure 1. Simi-
lar comments apply to figure 2, though both meth-
ods always tagged correctly at least a few training
windows as negative, for the T
?
values we tried;
hence, negative precision was never zero.
Positive precision shows how certain we can be
that training windows tagged as positive are in-
deed positive; whereas positive recall is the per-
centage of true positive examples that we manage
to tag as such. Figure 1 shows that when using
ATTW, we need to settle for a low positive recall,
i.e., miss out many positive examples, in order to
obtain a reasonably high precision. It also shows
that the centroid method is clearly worse when tag-
ging positive examples; its positive precision is al-
most always less than 0.3. Figure 2 shows that
both methods achieve high negative precision and
recall; they manage to assign trustworthy nega-
tive labels without missing many negative exam-
ples. However, ATTW is significantly better when
tagging positive examples, as shown in figure 1;
hence, it is better than the centroid method.
8
8
We tried different values of ROUGE-W?s a parameter in
When using ATTW in practice, we need to se-
lect T
+
and T
?
. We assign more importance to
selecting a T
+
(a point of ATTW?s curve in figure
1) that yields high positive precision; the choice of
T
?
(point in figure 2) is less important, because
ATTW?s negative precision is always reasonably
high. Based on figure 1, we set T
+
to 0.58, which
corresponds to positive precision 0.66 and posi-
tive recall 0.16. By tuning the two thresholds we
can control the number of positively or negatively
tagged examples we produce (and their ratio), and
the number of examples we discard. Having set
T
+
, we set T
?
to 0.30, a value that maintains the
ratio of truly positive to truly negative windows
of the 400 manually tagged windows (0.2 to 1),
since this is approximately the ratio the classifier
will confront during testing; we also experimented
with a 1 to 1 ratio, but the results were worse. This
T
?
value corresponds negative precision 0.70 and
negative recall 0.02. Thus, both positive and neg-
ative precision is approximately 0.7, which means
that approximately 30% of the tags we assign to
the examples are incorrect. Our experiments, how-
ever, indicate that the classifier is able to general-
ize well over this noise.
3 Finding new definitions
We now present our overall system, the system of
Cui et al, and the baselines.
3.1 Our system
Given a target term, our system extracts r ? f =
10 ? 5 windows from the pages returned by the
search engine, and uses the MAXENT classifier to
separate them into acceptable and unacceptable
definitions.
9
It then returns the k = 5 windows
the classifier is most confident they are acceptable.
The classifier is trained on windows tagged as pos-
itive or negative using ATTW. It views each win-
dow as a vector of the following features:
10
SN: The ordinal number of the window on the
page it originates from (e.g., second window of the
target term from the beginning of the page). Early
mentions of a term are more likely to define it.
RK: The ranking of the Web page the window
originates from, as returned by the search engine.
the interval (1, 2]. We use a = 1.4, which was the value with
the best results on the 400 windows. We did not try a > 2, as
the results were declining as a approached 2.
9
We do not discuss MAXENT classifiers, since they are a
well documented in the literature.
10
SN andWC originate from Joho and Sanderson (2000).
1274
WC: We create a simple centroid of the window?s
target term, much as in section 2.2. The centroid?s
words are chosen based on their frequency in the r?
f windows of the target term; the 20 most frequent
words are chosen. WC is the percentage of the 20
words that appear in the vector?s window.
Manual patterns: 13 Boolean features, each sig-
naling if the window matches a different manually
constructed lexical pattern (e.g., ?target, a/an/the?,
as in ?Tony Blair, the British prime minister?).
The patterns are those used by Joho and Sander-
son (2000), and four more introduced in our pre-
vious work (Androutsopoulos and Galanis, 2005)
and (Miliaraki and Androutsopoulos, 2004). They
are intended to perform well across text genres.
Automatic patterns: m numeric features, each
showing the degree to which the window matches
a different automatically acquired lexical pattern.
The patterns are word n-grams (n ? {1, 2, 3}) that
must occur directly before or after the target term
(e.g., ?target which is?). The patterns are acquired
as follows. First, all the n-grams directly before
or after any target term in the training windows
are collected. The n-grams that have been en-
countered at least 10 times are candidate patterns.
From those, the m patterns with the highest pre-
cision scores are retained, where precision is the
number of positive training windows the pattern
matches over the total number of training windows
it matches; we use m = 300 in our experiments,
based on the results of our previous work. The au-
tomatically acquired patterns allow the system to
detect definition contexts that are not captured by
the manual patterns, including genre-specific con-
texts. The value of each feature is the ROUGE-W
score between a pattern and the left or right con-
text of the target term in the window.
3.2 Cui et al?s system
Given a target term t, Cui et al (2004; 2005; 2006;
2007) initially locate sentences containing t in rel-
evant documents. We use the r?f = 10?5windows
from the pages returned by the search engine, in-
stead of sentences. Cui et al then construct the
centroid of t, and compute the cosine similarity of
each one of the r ? f windows to the centroid, as
in section 2.2. The 10 windows that are closer to
the centroid are considered candidate answers. All
candidate answers are then processed by a part-of-
speech (POS) tagger and a chunker. The words
of the centroid are replaced in all the candidate
answers by their POS tags; the target term, noun
phrases, forms of the verb ?to be?, and articles
are replaced by special tags (e.g., TARGET, NP),
while adjectives and adverbs are removed. The
candidate answers are then cropped to L tokens
to the left and right of the target term, producing
two subsequences (left and right) per candidate an-
swer; we set L = 3, which is Cui et al?s default.
Cui et al experimented with two approaches to
rank the candidate answers, called Bigram Model
and Profile Hidden Markov Model (PHMM). Both
are learning components that produce soft pat-
terns, though PHMM is much more complicated. In
their earlier work, Cui et al (2005) found the Bi-
gramModel to perform better than PHMM; in more
recent experiments with more data (Cui, 2006; Cui
et al, 2007) they found PHMM to perform better,
but the difference was not statistically significant.
Given these results and the complexity of PHMM,
we experimented only with the Bigram Model.
In the Bigram Model, the left and right subse-
quences of each candidate answer are considered
separately. Below S
1
, . . . , S
L
refer to the slots
(word positions) of a (left or right) subsequence,
and t
1
, . . . , t
L
to the particular words in the slots.
For each subsequence ?S
1
= t
1
, . . . , S
L
= t
L
? of
a candidate answer, we first estimate:
P (t
i
|S
i
) =
|S
i
(t
i
)| + ?
?
t
?
|S
i
(t
i
)| + ? ?N
P (t
i
|t
i?1
) =
|S
i
(t
i
) ? S
i?1
(t
i?1
)|
|S
i
(t
i
)|
P (t
i
|S
i
) is the probability that t
i
will appear in
slot S
i
of a left or right subsequence (depending on
the subsequence considered) of an acceptable can-
didate answer. P (t
i
|t
i?1
) is the probability that
t
i
will follow t
i?1
in a (left or right) subsequence
of an acceptable candidate answer. Cui et al use
only positive training examples, generated by the
centroid-based approach of section 2.2. |S
i
(t
i
)| is
the number of times t
i
appeared in S
i
in the (left
or right) subsequences of the training examples.
t
?
ranges over all the words that occurred in S
i
in
the training examples. |S
i
(t
i
) ? S
i?1
(t
i?1
)| is the
number of times t
i
and t
i?1
co-occurred in the cor-
responding slots in the training examples. N is the
number of different words that occurred in the (left
or right) training subsequences, and ? is a constant
set to 2, as in Cui et al?s experiments. Following
Cui et al, if t
i
is a POS or other special tag then
the probabilities above are estimated by counting
1275
only the tags of the training examples. Similarly,
if t
i
is an actual word, only the actual words (not
tags) of the training examples are considered.
The probability of each subsequence could then
be estimated as:
P (t
1
, . . . , t
L
) = P (t
1
|S
1
) ?
L
?
i=2
(? ? P (t
i
|t
i?1
) + (1 ? ?) ? P (t
i
|S
i
))
Instead, Cui et al use the following scoring mea-
sure, which also accounts for the fact that some
subsequences may have length l < L. They tune
? by Expectation Maximization.
P
norm
(t
1
, . . . , t
L
) =
1
l
? [logP (t
1
|S
1
) +
L
?
i=2
log(? ? P (t
i
|t
i?1
) + (1 ? ?) ? P (t
i
|S
i
))]
The overall score of a candidate answer is then:
P = (1 ? ?) ? P
norm
(left) + ? ? P
norm
(right)
Again, Cui et al tune a by Expectation Maximiza-
tion. Instead, we tuned ? and ? by a grid search
in [0, 1] ? [0, 1], with step 0.1 for both parame-
ters. For the tuning, we trained Cui et al?s system
on 2,000 randomly selected target terms, exclud-
ing terms used for other purposes. We used 160
manually tagged windows to evaluate the system?s
performance with the different values of ? and ?;
the 160 windows were selected randomly from the
10,000 windows of section 2.3, after excluding the
400 manually tagged windows of that section. The
resulting values for ? and ? were 0.7 and 0.6, re-
spectively. Apart from the modifications we men-
tioned, we use Cui et al?s original implementation.
3.3 Baseline methods
The first baseline selects the first window of each
one of the five highest ranked Web pages, as re-
turned by the search engine, and returns the five
windows. The second baseline returns five win-
dows chosen randomly from the r ? f = 10 ? 5
available ones. The third baseline (centroid base-
line) creates a centroid of the r ? f windows, as in
section 2.2, and returns the five windows with the
highest cosine similarity to the centroid.
11
11
We also reimplemented the definitions component of
Chu-Carroll et al (2004; 2005), but its performance was
worse than our centroid baseline.
Figure 3: Correct responses, 5 answers/question.
4 Evaluation of systems
We used q training target terms in the experi-
ments of this section, with q ranging from 50 to
1500, and 200 testing terms, with no overlap be-
tween training and testing terms, and excluding
terms that had been used for other purpose.
12
We
had to use testing terms for which encyclopedia
definitions were also available, to judge the ac-
ceptability of the systems? responses, since many
terms are highly technical. We discarded, how-
ever, windows extracted from encyclopedia pages
when testing, simulating the case where the target
terms are not covered by encyclopedias.
As already mentioned, for each target term we
extract r ? f = 10 ? 5 windows (or fewer, if fewer
are available) from the pages the search engine re-
turns. We then provide these windows to each of
the systems, allowing them to return up to k = 5
windows, ordered by decreasing confidence. If
any of the k windows contains an acceptable short
definition of the target term, as judged by a hu-
man evaluator, the system?s response is counted as
correct. We also calculate the Mean Reciprocal
Rank (MRR) of each system?s responses, as in the
TREC QA track: if the first acceptable definition of
a response is in the j-th position (1 ? j ? k), the
response?s score is 1/j; MRR is the mean of the re-
sponses? scores, i.e., it rewards systems that return
acceptable definitions higher in their responses.
Figures 3 and 4 show the results of our experi-
ments as percentage of correct responses and MRR,
respectively; the error bars of figure 3 correspond
to 95% confidence intervals. Our system clearly
outperforms Cui et al?s, despite the fact that the
12
The reader is reminded that all terms were selected ran-
domly from the index of an on-line encyclopedia.
1276
Figure 4: MRR scores, 5 answers per question.
latter uses more linguistic resources (a POS tag-
ger and a chunker). Both systems outperform the
baselines, of which the centroid baseline is the
best, and both systems perform better as the size
of the training set increases. The baselines con-
tain no learning components; hence, their curves
are flat. We also show the results (Base-Attrs)
of our system when the features that correspond
to automatically acquired patterns are excluded.
Clearly, these patterns help our system achieve
significantly better results; however, our system
outperforms Cui et al?s even without them. With-
out the automatic patterns, our system also shows
signs of saturation as the training data increase.
Figures 5 and 6 show the performance of our
new system against our previously published one
(Androutsopoulos and Galanis, 2005); the new
system clearly outperforms the old one. Addi-
tional experiments we conducted with the old sys-
tem replacing the SVM by the MAXENT classifier
(without using ROUGE-W) indicate that the use of
MAXENT by itself also improved slightly the re-
sults, but the differences are too minor to show; the
improvement is mostly due to the use of ROUGE-
W instead of our previous measure.
5 Related work
Xu et al (2004) use an information extraction en-
gine to extract linguistic features from documents
relevant to the target term. The features are mostly
phrases, such as appositives, and phrases express-
ing relations. The features are then ranked by their
type and similarity to a centroid, and the most
highly ranked ones are returned. Xu et al seem
to aim at generating multi-snippet definitions, un-
like the single-snippet definitions we seek.
Blair-Goldensohn et al (2003; 2004) extract
sentences that may provide definitional informa-
Figure 5: Correct responses of our new and previ-
ous system, allowing 5 answers per question.
Figure 6: MRR of our new and previous system.
tion from documents retrieved for the target term;
a decision tree learner and manually tagged train-
ing data are used. The sentences are then matched
against manually constructed patterns, which op-
erate on syntax trees, to detect sentences ex-
pressing the target term?s genus, species, or both
(genus+species). The system composes its an-
swer by placing first the genus+species sentence
that is closer to the centroid of the extracted sen-
tences. The remaining sentences are ranked by
their distance from the centroid, and the most
highly ranked ones are clustered. The system then
selects iteratively the cluster that is closer to the
centroid of the extracted sentences and the most
recently used cluster. The cluster?s most repre-
sentative sentence, i.e., the sentence closest to the
centroid of the cluster?s sentences, is added to the
response. The iterations stop when a maximum re-
sponse length is reached. Multi-snippet definitions
are generated.
Han et al (2004; 2006) parse a definition ques-
tion to locate the head word of the target term.
They also use a named entity recognizer to deter-
mine the target term?s type (person, organization,
1277
etc.). They then extract from documents relevant
to the target term sentences containing its head
word, as well as sentences the extracted ones refer
to (e.g., via pronouns). The resulting sentences are
matched against manually constructed syntactic
patterns to detect phrases conveying definitional
information. The resulting phrases are ranked by
criteria like the degree to which the phrase con-
tains words common in definitions of the target
term?s type, and the highest ranked phrases are in-
cluded in a multi-snippet summary. Other mecha-
nisms discard phrases duplicating information.
Xu et al (2005) aim to extract all the definitions
in a document collection. They parse the docu-
ments to detect base noun phrases (without em-
bedded noun phrases). Base noun phrases are pos-
sible target terms; the paragraphs containing them
are matched against manually constructed patterns
that look for definitions. An SVM then separates
the remaining paragraphs into good, indifferent,
and bad definitions. Redundant paragraphs, iden-
tified by edit distance similarity, are removed.
6 Conclusions and future work
We presented a freely available system that finds
short definitions of user-specified terms on Web
pages. It employs a MAXENT classifier, which
is trained on automatically generated examples;
hence, the system is in effect unsupervised. We
use ROUGE-W to generate training examples from
Web snippets and encyclopedias, a method that
outperforms an alternative centroid-based one.
Once our system has been trained, it can find short
definitions of terms that are not covered by ency-
clopedias. Experiments show our system outper-
forms a comparable well-published system and a
previously published form of our system.
Our system does not require linguistic process-
ing tools, such as named entity recognizers, POS
taggers, chunkers, parsers; hence, it can be easily
used in languages where such tools are unavail-
able. It could be improved by exploiting the HTML
markup of Web pages and the Web?s hyperlinks.
For example, the target term is sometimes written
in italics in definitions, and some definitions are
provided on pages (e.g., pop-up windows) that oc-
currences of the target term link to.
The work reported here was conducted in the
context of project INDIGO, where an autonomous
robotic guide for museum collections is being de-
veloped (Galanis et al, 2009). The guide engages
the museum?s visitors in spoken dialogues, and it
describes the exhibits the visitors select by gen-
erating spoken natural language descriptions from
an ontology. Among other requests, the visitors
can ask follow up questions, and we have found
that the most common kind of follow up questions
are requests to define terms (e.g., names of per-
sons, events, architectural terms, etc.) mentioned
in the generated exhibit descriptions. Some of
these definition requests can be handled by gener-
ating new texts from the ontology, but some times
the ontology contains no information for the target
terms. We are, thus, experimenting with the possi-
bility of obtaining short definitions from the Web,
using the system we presented.
Acknowledgements
This work was carried out in INDIGO, an FP6 IST
project funded by the European Union, with addi-
tional funding provided by the Greek General Sec-
retariat of Research and Technology.
13
References
Androutsopoulos, I., and Galanis, D. 2005. A Prac-
tically Unsupervised Learning Method to Identify
Single-Snippet Answers to Definition Questions on
the Web. In HLT/EMNLP, Vancouver, Canada, 323?
330.
Blair-Goldensohn, S., McKeown, K., Schlaikjer, A.H.
2003. A Hybrid Approach for QA Track Definitional
Questions. In TREC 2003, Gaithersburg, MD, USA.
Blair-Goldensohn, S., McKeown, K.R., and Schlaikjer,
A.H. 2004. Answering Definitional Questions: A
Hybrid Approach. In Maybury, M. (Ed.), New Di-
rections in Question answering, AAAI Press.
Chu-Carroll, J., Czuba, K., Prager, J., Ittycheriah, A.,
Blair-Goldensohn, S. 2004. IBM?s PIQUANT II in
TREC 2004. In TREC 2004, Gaithersburg, MD, USA.
Chu-Carroll, J., Czuba, K., Duboue, P., and Prager, J.
2005. IBM?s PIQUANT II in TREC 2005. In TREC
2005, Gaithersburg, MD, USA.
Cristianini, N. and Shawe-Taylor, J. 2000. An Intro-
duction to SVMs. Cambridge University Press.
Cui, H., Kan, M.-Y., Chua, T.-S., and Xiao, J. 2004. A
Comparative Study on Sentence Retrieval for Defi-
nitional Question Answering. In SIGIR workshop on
Information Retrieval for Question Answering, Sal-
vador, Brazil.
13
Consult http://www.ics.forth.gr/indigo/.
1278
Cui, H., Kan, M.Y., Chua, T.S. 2004. Unsupervised
Learning of Soft Patterns for Generating Definitions
from Online News. In WWW, New York, NY, USA.
Cui, H., Kan, M.Y., Chua, T.S. 2005. Generic Soft Pat-
tern Models for Definitional Question Answering. In
ACM SIGIR, Salvador, Brazil.
Cui, H. 2006. Soft Matching for Question Answering.
Ph.D. thesis, National University of Singapore.
Cui, H., Kan, M., and Chua, T. 2007. Soft Pattern
Matching Models for Definitional Question Answer-
ing. ACM Transactions on Information Systems,
25(2):1?30.
Dang, H. T. 2005. Overview of DUC 2005. In DUC at
HLT-EMNLP, Vancouver, Canada.
Dang, H. T. 2006. Overview of DUC 2006. In DUC at
HLT-NAACL, New York, NY, USA.
Eugenio, B. D., Glass, M. 2004. The Kappa Statis-
tic: a Second Look. Computational Linguistics,
301(1):95-101.
Galanis, D., Karakatsiotis, G., Lampouras, G., and An-
droutsopoulos, I. 2009. An Open-Source Natu-
ral Language Generator for OWL Ontologies and
its Use in Protege and Second Life. EACL system
demonstration, Athens, Greece.
Han, K.S., Chung, H., Kim, S.B., Song, Y.I., Lee, J.Y.,
Rim, H.C. 2004. Korea University QA System at
TREC 2004. In TREC 2004, Gaithersburg, MD, USA.
Han, K.S., Song, Y.I., Kim, S.B., and Rim, H.C. 2006.
A Definitional Question Answering System Based on
Phrase Extraction Using Syntactic Patterns. IEICE
Transactions on Information and Systems, vol. E89-
D, No. 4, 1601?1605.
Hildebrandt, W., Katz, B., and Lin, J. 2004. Answer-
ing Definition Questions Using Multiple Knowledge
Sources. In HLT-NAACL, Boston, MA, USA, 49?56.
Joho, H. and Sanderson, M. 2000. Retrieving Descrip-
tive Phrases from Large Amounts of Free Text. Inter-
national Conference on Information and Knowledge
Management, McLean, VA, USA, 180?186.
Joho, H. and Sanderson, M. 2001. Large Scale Testing
of a Descriptive Phrase Finder. In HLT-NAACL, San
Diego, CA, USA, 219?221.
Lin, C.Y. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In ACL workshop ?Text
Summarization Branches Out?, Barcelona, Spain.
Miliaraki, S. and Androutsopoulos, I. 2004. Learn-
ing to Identify Single-Snippet Answers to Definition
Questions. In COLING, Geneva, Switzerland, 1360?
1366.
Prager, J., Radev, D., and Czuba, K. 2001. Answering
What-Is Questions by Virtual Annotation. In HLT-
NAACL, San Diego, CA, USA, 26?30.
Prager, J., Chu-Carroll, J., and Czuba, K. 2002. Use of
WordNet Hypernyms for Answering What-Is Ques-
tions. In TREC 2001, Gaithersburg, MD, USA.
Ratnaparkhi A. 1997. A Simple Introduction to Max-
imum Entropy Models for Natural Language Pro-
cessing. Technical Report 97-08, Institute for Re-
search in Cognitive Science, University of Pennsyl-
vania, 1997.
Voorhees, E.M. 2000. Overview of the TREC-9 Ques-
tion Answering Track. NIST, USA.
Voorhees, E.M. 2001. Overview of the TREC 2001
Question Answering Track. NIST, USA.
Voorhees, E.M. 2001. The TREC QA Track. Natural
Language Engineering, 7(4):361?378.
Voorhees, E.M. 2003. Evaluating Answers to Defini-
tion Questions. In HLT-NAACL, Edmonton, Canada.
Xu, J., Weischedel, R., Licuanan, A. 2004. Evaluation
of an Extraction-based Approach to Answering Def-
initional Questions. In ACM SIGIR, Sheffield, UK.
Xu, J., Cao, Y., Li, H., Zhao, M. 2005. Ranking Defini-
tions with Supervised Learning Methods. In WWW,
Chiba, Japan, 811?819.
1279
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96?106,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Generate and Rank Approach to Sentence Paraphrasing
Prodromos Malakasiotis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Centre ?Athena?, Greece
Abstract
We present a method that paraphrases a given
sentence by first generating candidate para-
phrases and then ranking (or classifying)
them. The candidates are generated by ap-
plying existing paraphrasing rules extracted
from parallel corpora. The ranking compo-
nent considers not only the overall quality of
the rules that produced each candidate, but
also the extent to which they preserve gram-
maticality and meaning in the particular con-
text of the input sentence, as well as the de-
gree to which the candidate differs from the
input. We experimented with both a Max-
imum Entropy classifier and an SVR ranker.
Experimental results show that incorporating
features from an existing paraphrase recog-
nizer in the ranking component improves per-
formance, and that our overall method com-
pares well against a state of the art paraphrase
generator, when paraphrasing rules apply to
the input sentences. We also propose a new
methodology to evaluate the ranking compo-
nents of generate-and-rank paraphrase gener-
ators, which evaluates them across different
combinations of weights for grammaticality,
meaning preservation, and diversity. The pa-
per is accompanied by a paraphrasing dataset
we constructed for evaluations of this kind.
1 Introduction
In recent years, significant effort has been devoted
to research on paraphrasing (Androutsopoulos and
Malakasiotis, 2010; Madnani and Dorr, 2010). The
methods that have been proposed can be roughly
classified into three categories: (i) recognition meth-
ods, i.e., methods that detect whether or not two in-
put sentences or other texts are paraphrases; (ii) gen-
eration methods, where the aim is to produce para-
phrases of a given input sentence; and (iii) extraction
methods, which aim to extract paraphrasing rules
(e.g., ?X wrote Y ? ?? Y was authored by X?) or
similar patterns from corpora. Most of the methods
that have been proposed belong in the first category,
possibly because of the thrust provided by related
research on textual entailment recognition (Dagan et
al., 2009), where the goal is to decide whether or not
the information of a given text is entailed by that of
another. Significant progress has also been made in
paraphrase extraction, where most recent methods
produce large numbers of paraphrasing rules from
multilingual parallel corpora (Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Zhao et al,
2008; Zhao et al, 2009a; Zhao et al, 2009b; Kok
and Brockett, 2010). In this paper, we are concerned
with paraphrase generation, which has received less
attention than the other two categories.
There are currently two main approaches to para-
phrase generation. The first one treats paraphrase
generation as a machine translation problem, with
the peculiarity that the target language is the same as
the source one. To bypass the lack of large monolin-
gual parallel corpora, which are needed to train sta-
tistical machine translation (SMT) systems for para-
phrasing, monolingual clusters of news articles re-
ferring to the same event (Quirk et al, 2004) or
other similar monolingual comparable corpora can
be used, though sentence alignment methods for par-
allel corpora may perform poorly on comparable
corpora (Nelken and Shieber, 2006); alternatively,
large collections of paraphrasing rules obtained via
paraphrase extraction from multilingual parallel cor-
pora can be used as monolingual phrase tables in a
96
phrase-based SMT systems (Zhao et al, 2008; Zhao
et al, 2009a); in both cases, paraphrases can then
be generated by invoking an SMT system?s decoder
(Koehn, 2009). A second paraphrase generation ap-
proach is to treat existing machine translation en-
gines as black boxes, and translate each input sen-
tence to a pivot language and then back to the orig-
inal language (Duboue and Chu-Carroll, 2006). An
extension of this approach uses multiple translation
engines and pivot languages (Zhao et al, 2010).
In this paper, we investigate a different paraphrase
generation approach, which does not produce para-
phrases by invoking machine translation system(s).
We use an existing collection of monolingual para-
phrasing rules extracted from multilingual parallel
corpora (Zhao et al, 2009b); each rule is accompa-
nied by one or more scores, intended to indicate the
rule?s overall quality without considering particular
contexts where the rule may be applied. Instead of
using the rules as a monolingual phrase table and in-
voking an SMT system?s decoder, we follow a gen-
erate and rank approach, which is increasingly com-
mon in several language processing tasks.1 Given
an input sentence, we use the paraphrasing rules to
generate a large number of candidate paraphrases.
The candidates are then represented as feature vec-
tors, and a ranker (or classifier) selects the best ones;
we experimented with a Maximum Entropy classi-
fier and a Support Vector Regression (SVR) ranker.
The vector of each candidate paraphrase includes
features indicating the overall quality of the rules
that produced the candidate, the extent to which the
rules preserve grammaticality and meaning in the
particular context of the input sentence, and the de-
gree to which the candidate?s surface form differs
from that of the input; we call the latter factor di-
versity. The intuition is that a good paraphrase is
grammatical, preserves the meaning of the original
sentence, while also being as different as possible.
Experimental results show that including in the
ranking (or classification) component features from
an existing paraphrase recognizer leads to improved
results. We also propose a new methodology to eval-
uate the ranking components of generate-and-rank
paraphrase generators, which evaluates them across
different combinations of weights for grammatical-
1See, for example, Collins and Koo (2005).
ity, meaning preservation, and diversity. The paper
is accompanied by a new publicly available para-
phrasing dataset we constructed for evaluations of
this kind. Further experiments indicate that when
paraphrasing rules apply to the input sentences, our
paraphrasing method is competitive to a state of the
art paraphrase generator that uses multiple transla-
tion engines and pivot languages (Zhao et al, 2010).
We note that paraphrase generation is useful in
several language processing tasks. In question an-
swering, for example, paraphrase generators can be
used to paraphrase the user?s queries (Duboue and
Chu-Carroll, 2006; Riezler and Liu, 2010); and
in machine translation, paraphrase generation can
help improve the translations (Callison-Burch et al,
2006; Marton et al, 2009; Mirkin et al, 2009; Mad-
nani et al, 2007), or it can be used when evaluat-
ing machine translation systems (Lepage and De-
noual, 2005; Zhou et al, 2006; Kauchak and Barzi-
lay, 2006; Pado? et al, 2009).
The remainder of this paper is structured as fol-
lows: Section 2 explains how our method gener-
ates candidate paraphrases; Section 3 introduces the
dataset we constructed, which is also used in sub-
sequent sections; Section 4 discusses how candi-
date paraphrases are ranked; Section 5 compares our
overall method to a state of the art paraphrase gen-
erator; and Section 6 concludes.
2 Generating candidate paraphrases
We use the approximately one million English para-
phrasing rules of Zhao et al (2009b). Roughly
speaking, the rules were extracted from a parallel
English-Chinese corpus, based on the assumption
that two English phrases e1 and e2 that are often
aligned to the same Chinese phrase c are likely to
be paraphrases and, hence, they can be treated as a
paraphrasing rule e1 ? e2.2 Zhao et al?s method ac-
tually operates on slotted English phrases, obtained
from parse trees, where slots correspond to part of
speech (POS) tags. Hence, rules like the following
three may be obtained, where NNi indicates a noun
slot and NNPi a proper name slot.
2This pivot-based paraphrase extraction approach was first
proposed by Bannard and Callison-Burch (2005). It under-
lies several other paraphrase extraction methods (Riezler et al,
2007; Callison-Burch, 2008; Kok and Brockett, 2010).
97
(1) a lot of NN1? plenty of NN1
(2) NNP1 area? NNP1 region
(3) NNP1 wrote NNP2? NNP2 was written by NNP1
In the basic form of their method, called Model
1, Zhao et al (2009b) use a log-linear ranker to as-
sign scores to candidate English paraphrase pairs
?e1, e2?; the ranker uses the alignment probabilities
P (c|e1) and P (e2|c) as features, along with features
that assess the quality of the corresponding align-
ments. In an extension of their method, Model 2,
Zhao et al consider two English phrases e1 and e2 as
paraphrases, if they are often aligned to two Chinese
phrases c1 and c2, which are themselves paraphrases
according to Model 1 (with English used as the pivot
language). Again, a log-linear ranker assigns a score
to each ?e1, e2? pair, now with P (c1|e1), P (c2|c1),
and P (e2|c1) as features, along with similar features
for alignment quality. In a further extension, Model
3, all the candidate phrase pairs ?e1, e2? are collec-
tively treated as a monolingual parallel corpus. The
phrases of the corpus are aligned, as when aligning
a bilingual parallel corpus, and additional features,
based on the alignment, are added to the log-linear
ranker, which again assigns a score to each ?e1, e2?.
The resulting paraphrasing rules e1 ? e2 typi-
cally contain short phrases (up to four or five words
excluding slots) on each side; hence, they can be
used to rewrite only parts of longer sentences. Given
an input (source) sentence S, we generate candidate
paraphrases by applying rules whose left or right
hand side matches any part of S. For example, rule
(1) matches the source sentence (4); hence, (4) can
be rewritten as the candidate paraphrase (5).3
(4) S: He had a lot of [NN 1admiration] for his job.
(5) C: He had plenty of [NN 1admiration] for his job.
Several rules may apply to S; for example, they may
rewrite different parts of S, or they may replace the
same parts of S by different phrases. We allow all
possible combinations of applicable rules to apply to
S, excluding combinations that include rules rewrit-
ing overlapping parts of S.4 To avoid generating too
many candidates (C), we use only the 20 rules (that
3We use Stanford?s POS tagger, MaxEnt classifier, and de-
pendency parser; see http://nlp.stanford.edu/.
4A possible extension, which we have not explored, would
be to recursively apply the same process to the resulting Cs.
apply to S) with the highest scores. Zhao et al actu-
ally associate each rule with three scores. The first
one, hereafter called r1, is the Model 1 score, and the
other two, r2 and r3, are the forward and backward
alignment probabilities of Model 3; see Zhao et al
(2009b) for details. We use the average of the three
scores, hereafter r4, when generating candidates.
Unfortunately, Zhao et al?s scores reflect the over-
all quality of each rule, without considering the con-
text of the particular S where the rule is applied.
Szpektor et al (2008) point out that, for example,
a rule like ?X acquire Y ?? ?X buy Y ? may work
well in many contexts, but not in ?Children acquire
language quickly?. Similarly, ?X charged Y with?
? ?X accused Y of? should not be applied to sen-
tences about charging batteries. Szpektor et al pro-
pose, roughly speaking, to associate each rule with
a model of the contexts where the rule is applicable,
as well as models of the expressions that typically
fill its slots, in order to be able to assess the applica-
bility of each rule in specific contexts. The rules that
we use do not have associated models of this kind,
but we follow Szpektor et al?s idea of assessing the
applicability of each rule in each particular context,
when ranking candidates, as discussed below.
3 A dataset of candidate paraphrases
Our generate and rank method relies on existing
large collections of paraphrasing rules to generate
candidate paraphrases. Our main contribution is in
the ranking of the candidates. To be able to evalu-
ate the performance of different rankers in the task
we are concerned with, we first constructed an eval-
uation dataset that contains pairs ?S,C? of source
(input) sentences and candidate paraphrases, and we
asked human judges to assess the degree to which
the C of each pair was a good paraphrase of S.
We selected randomly 75 source (S) sentences
from the AQUAINT corpus, such that at least one
of the paraphrasing rules applied to each S.5 For
each S, we generated candidate Cs using Zhao et
al.?s rules, as discussed in Section 2. This led to
1,935 ?S,C? pairs, approx. 26 pairs for each S. The
pairs were given to 13 judges other than the authors.6
Each judge evaluated approx. 148 (different) ?S,C?
5The corpus is available from the LDC (LDC2002T31).
6The judges were fluent, but not native English speakers.
98
Figure 1: Distribution of overall quality scores in the
evaluation dataset (1 = totally unacceptable, 4 = perfect).
pairs; each of the 1,935 pairs was evaluated by one
judge. The judges were asked to provide grammati-
cality, meaning preservation, and overall paraphrase
quality scores for each ?S,C? pair, each score on a
1?4 scale (1 for totally unacceptable, 4 for perfect);
guidelines and examples were also provided.
Figure 1 shows the distribution of the overall qual-
ity scores in the 1,935 ?S,C? pairs of the evalua-
tion dataset; the distributions of the grammaticality
and meaning preservation scores are similar. No-
tice that although we used only the 20 applicable
paraphrasing rules with the highest scores to gen-
erate the ?S,C? pairs, less than half of the candidate
paraphrases (C) were considered good, and approx-
imately only 20% perfect. In other words, apply-
ing paraphrasing rules (even only those with the 20
best scores) to each input sentence S and randomly
picking one of the resulting candidate paraphrases
C, without any further filtering (or ranking) of the
candidates, would on average produce unacceptable
paraphrases more frequently than acceptable ones.
Hence, the role of the ranking component is crucial.
We also measured inter-annotator agreement by
constructing, in the same way, 100 additional ?S,C?
pairs (other than the 1,935) and asking 3 of the 13
judges to evaluate all of them. We measured the
mean absolute error, i.e., the mean absolute differ-
ence in the judges? scores (averaged over all pairs
of judges) and the mean (over all pairs of judges)
K statistic (Carletta, 1996). In the overall scores,
K was 0.64, which is in the range often taken to
indicate substantial agreement (0.61?0.80).7 Agree-
ment was higher for grammaticality (K = 0.81),
7It is also close to 0.67, which is sometimes taken to be a
cutoff for substantial agreement in computational linguistics.
mean abs. diff. K-statistic
grammaticality 0.20 0.81
meaning preserv. 0.26 0.59
overall quality 0.22 0.64
Table 1: Inter-annotator agreement when manually eval-
uating candidate paraphrases.
and lower (K = 0.59) for meaning preservation. Ta-
ble 1 shows that the mean absolute difference in the
annotators? scores was 15 to 14 of a point.
Several judges commented that they had trouble
deciding to what extent the overall quality score
should reflect grammaticality or meaning preserva-
tion. They also wondered if it was fair to consider as
perfect candidate paraphrases that differed in only
one or two words from the source sentences, i.e.,
candidates with low diversity. These comments led
us to ignore the judges? overall quality scores in
some experiments, and to use a weighted average
of grammaticality, meaning preservation, and (auto-
matically measured) diversity instead, with different
weight combinations corresponding to different ap-
plication requirements, as discussed further below.
In the same way, 1,500 more ?S,C? pairs (other
than the 1,935 and the 100, not involving previously
seen Ss) were constructed, and they were evaluated
by the first author. The 1,500 pairs were used as
a training dataset in experiments discussed below.
Both the 1,500 training and the 1,935 evaluation
(test) pairs are publicly available.8 We occasionally
refer to the training and evaluation datasets as a sin-
gle dataset, but they are clearly separated.
4 Ranking candidate paraphrases
We now discuss the ranking component of our
method, which assesses the candidate paraphrases.
4.1 Features of the ranking component
Each ?S,C? pair is represented as a feature vector.
To allow the ranking component to assess the degree
to which a candidate C is grammatical, or at least
as grammatical as the source S, we include in the
feature vectors the language model scores of S, C,
and the difference between the two scores. We use
a 3-gram language model trained on approximately
8See the paper?s supplementary material.
99
6.5 million sentences of the AQUAINT corpus.9 To
allow the ranker to consider the (context-insensitive)
quality scores of the rules that generated C from S,
we also include as features the highest, lowest, and
average r1, r2, r3, and r4 scores (Section 2) of these
rules, 12 features in total.
The features discussed so far are similar to those
employed by Zhao et al (2009a) in the only compa-
rable paraphrase generation method we are aware of
that uses paraphrasing rules. That method, hereafter
called ZHAO-RUL, uses the language model score
of C and scores similar to r1, r2, r3 in a log-linear
model.10 The log-linear model of ZHAO-RUL is used
by an SMT-like decoder to identify the transforma-
tions (applications of rules) that produce the (hope-
fully) best paraphrase. By contrast, we first gen-
erate a large number of candidates using the para-
phrasing rules, and we then rank them. Unfortu-
nately, we did not have access to an implementa-
tion of ZHAO-RUL to compare against, but below
we compare against another paraphraser proposed
by Zhao et al (2010), hereafter called ZHAO-ENG,
which uses multiple machine translation engines and
pivot languages, instead of paraphrasing rules, and
which Zhao et al found to outperform ZHAO-RUL.
To further help the ranking component assess the
degree to which C preserves the meaning of S, we
also optionally include in the vectors of the ?S,C?
pairs the features of an existing paraphrase recog-
nizer (Malakasiotis, 2009) that obtained the best
published results (Androutsopoulos and Malakasio-
tis, 2010) on the widely used MSR paraphrasing cor-
pus.11 Most of the recognizer?s features are com-
puted by using nine similarity measures: Leven-
shtein, Jaro-Winkler, Manhattan, Euclidean, and n-
gram (n = 3) distance, cosine similarity, Dice, Jac-
card, and matching coefficients, all computed on to-
kens; consult Malakasiotis (2009) for details. For
each ?S,C? pair, the nine similarity measures are ap-
9We use SRILM; see http://www-speech.sri.com/.
10Application-specific features are also included, which can
be used, for example, to favor paraphrases that are shorter than
the input in sentence compression (Knight and Marcu, 2002;
Clarke and Lapata, 2008). Similar features could also be added
to application-specific versions of our method.
11The MSR corpus contains pairs that are paraphrases or not.
It is a benchmark for paraphrase recognizers, not generators. It
provides only one paraphrase (true or false) of each source, and
few of the true paraphrases can be obtained by the rules we use.
plied to ten different forms ?s1, c1? , . . . , ?s10, c10?
of ?S,C?, described below, leading to 90 features.
?s1, c1? : The original forms of S and C.
?s2, c2? : S and C with tokens replaced by stems.
?s3, c3? : S and C, with tokens replaced by POS tags.
?s4, c4? : S and C, tokens replaced by soundex codes.12
?s5, c5? : S and C, but having removed non-nouns.
?s6, c6? : As previously, but nouns replaced by stems.
?s7, c7? : As previously, nouns replaced by soundex.
?s8, c8? : S and C, but having removed non-verbs.
?s9, c9? : As previously, but verbs replaced by stems.
?s10, c10? : As previously, verbs replaced by soundex.
When constructing all ten forms ?si, ci? of ?S,C?,
synonyms (in any WordNet synset) are treated as
identical words. Additional variants of some of the
90 features compare a sliding window of some of
the si forms to the corresponding ci forms (or vice
versa), adding 40 more features; see Malakasiotis
(2009). Two more Boolean features indicate the ex-
istence or absence of negation in S or C, respec-
tively; and another feature computes the ratio of the
lengths of S and C, measured in tokens. Finally,
three additional features compare the dependency
trees of S and C:
RS =
|common dependencies of S,C|
|dependencies of S|
RC =
|common dependencies of S,C|
|dependencies of C|
F?=1 =
2 ?RS ?RC
RS +RC
The recognizer?s features are 136 in total.13
Hence, the full feature set of our paraphraser?s rank-
ing component comprises 151 features.
12The Soundex algorithm maps English words to alphanu-
meric codes, so that words with the same pronunciations
receive the same codes, despite spelling differences; see
http://en.wikipedia.org/wiki/Soundex.
13Malakasiotis (2009) shows that although there is a lot of re-
dundancy in the recognizer?s feature set, the full feature set still
leads to better paraphrase recognition results, compared to sub-
sets constructed via feature selection with hill-climbing or beam
search. The same paper reports that the recognizer performs al-
most as well without the last three features, which may not be
available in languages with no reliable dependency parsers. No-
tice, also, that the recognizer does not use paraphrasing rules.
100
4.2 Learning rate with a MaxEnt classifier
To obtain a first indication of whether or not a rank-
ing component equipped with the features discussed
above could learn to distinguish good from bad can-
didate paraphrases, and to investigate if our train-
ing dataset is sufficiently large, we initially experi-
mented with a Maximum Entropy classifier (with the
151 features) as the ranking component. This initial
version of the ranking component, called ME-REC,
was trained on increasingly larger parts of the train-
ing dataset of Section 3, and it was always evaluated
on the entire test dataset of that section. For simplic-
ity, we used only the judges? overall quality scores
in these experiments, and we treated the problem as
one of binary classification; overall quality scores of
1 and 2 where conflated to a negative category, and
scores of 3 and 4 to a positive category.
Figure 2 plots the error rate of ME-REC, com-
puted both on the test set and the encountered train-
ing subset. The error rate on the training instances
a learner has encountered is typically lower than the
error rate on the test set (unseen instances); hence,
the former error rate can be seen as a lower bound
of the latter. ME-REC shows signs of having reached
its lower bound when the entire training dataset is
used, suggesting that the training dataset is suffi-
ciently large. The baseline (BASE) of Figure 2 uses
only a threshold on the average r4 (Section 2) of the
rules that turned S into C. If the average r4 is higher
than the threshold, the ?S,C? pair is classified in the
positive class, otherwise in the negative one. The
threshold was tuned by experimenting on a sepa-
rate tuning dataset. Clearly, ME-REC outperforms
the baseline, which uses only the average (context-
insensitive) scores of the applied paraphrasing rules.
4.3 Experiments with an SVR ranker
As already noted, when our dataset were constructed
the judges felt it was not always clear to what ex-
tent the overall quality scores should reflect meaning
preservation or grammaticality; and they also won-
dered if the overall quality scores should have also
taken into consideration diversity. To address these
concerns, in the experiments described in this sec-
tion (and the remainder of the paper) we ignored the
judges? overall scores, and we used a weighted av-
erage of the grammaticality, meaning preservation,
 
15%
20%
25%
30%
35%
40%
45%
50%
75 15
0
22
5
30
0
37
5
45
0
52
5
60
0
67
5
75
0
82
5
90
0
97
5
10
50
11
25
12
00
12
75
13
50
14
25
15
00
E
r
r
o
r
r
a
t
e
Training instances used
ME-REC.TRAIN
ME-REC.TEST
BASE
Figure 2: Learning curves of a Maximum Entropy classi-
fier used as the ranking component of our method.
and diversity scores instead; the grammaticality and
meaning preservation scores were those provided by
the judges, while diversity was automatically com-
puted as the edit distance (Levenshtein, computed
on tokens) between S and C. Stated otherwise, the
correct score y(xi) of each training or test instance
xi (i.e., of each feature vector of an ?S,C? pair) was
taken to be a linear combination of the grammati-
cality score g(xi), the meaning preservation score
m(xi), and the diversity d(xi), as in Equation (6),
where ?3 = 1? ?1 ? ?2.
y(xi) = ?1 ? g(xi) + ?2 ?m(xi) + ?3 ? d(xi) (6)
We believe that the ?i weights should in prac-
tice be application-dependent. For example, when
paraphrasing user queries to a search engine that
turns them into bags of words, diversity and meaning
preservation may be more important than grammati-
cality; by contrast, when paraphrasing the sentences
of a generated text to avoid repeating the same ex-
pressions, grammaticality is very important. Hence,
generic paraphrase generators, like ours, intended to
be useful in many different applications, should be
evaluated for many different combinations of the ?i
weights. Consequently, in the experiments of this
section we trained and evaluated the ranking com-
ponent of our method (on the training and evalua-
tion part, respectively, of the dataset of Section 3)
several times, each time with a different combina-
tion of ?1, ?2, ?3 values, with the values of each ?i
ranging from 0 to 1 with a step of 0.2.
We employed a Support Vector Regression (SVR)
model in the experiments of this section, instead of
101
 0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
?1=0.0 ?2=0.0
?1=0.0 ?2=0.2
?1=0.0 ?2=0.4
?1=0.0 ?2=0.6
?1=0.0 ?2=0.8
?1=0.0 ?2=1.0
?1=0.2 ?2=0.0
?1=0.2 ?2=0.2
?1=0.2 ?2=0.4
?1=0.2 ?2=0.6
?1=0.2 ?2=0.8?1=0.4 ?2=0.0
?1=0.4 ?2=0.2
?1=0.4 ?2=0.4
?1=0.4 ?2=0.6
?1=0.6 ?2=0.0
?1=0.6 ?2=0.2
?1=0.6 ?2=0.4
?1=0.8 ?2=0.0
?1=0.8 ?2=0.2
?1=1.0 ?2=0.0
SVR-REC
SVR-BASE
?2 
Figure 3: Performance of our method?s SVR ranking com-
ponent with (SVR-REC) and without (SVR-BASE) the ad-
ditional features of the paraphrase recognizer.
a classifier, given that the y(xi) scores that we want
to predict are real values.14 An SVR is very similar
to a Support Vector Machine (Vapnik, 1998; Cris-
tianini and Shawe-Taylor, 2000; Joachims, 2002),
but it is trained on examples of the form ?xi, y(xi)?,
where xi ? Rn and y(xi) ? R, and it learns a rank-
ing function f : Rn ? R that is intended to return
f(xi) values as close as possible to the correct ones
y(xi), given feature vectors xi. In our case, the cor-
rect y(xi) values were those of Equation (6). We call
SVR-REC the SVR ranker with all the 151 features of
Section 4.2, and SVR-BASE the SVR ranker without
the 136 features of the paraphrase recognizer.
We used the squared correlation coefficient ?2 to
evaluate SVR-REC against SVR-BASE.15 The ?2 co-
efficient shows how well the scores returned by the
SVR are correlated with the desired scores y(xi); the
higher the ?2 the higher the agreement. Figure 3
14Additional experiments confirmed that the SVR per-
forms better than ME-REC as the ranking component. We
use the SVR implementation of LIBSVM, available from
http://www.csie.ntu.edu.tw/?cjlin/libsvm/,
with an RBF kernel and default settings. All the features are
normalized in [?1, 1], when using SVR or ME-REC.
15If n is the number of test pairs, f(xi) the score returned by
the SVR for the i-th pair, and y(xi) the correct score, then ?2 is:
(n?ni=1 f(xi)yi ?
?n
i=1 f(xi)
?n
i=1 y(xi))
2
(n
?n
i=1 f(xi)2 ? (
?n
i=1 f(xi))2)(n
?n
i=1 y2i ? (
?n
i=1 y(xi))2)
shows the experimental results. Each line from the
diagram?s center represents a different experimental
setting, i.e., a different combination of ?1 and ?2;
recall that ?3 = 1 ? ?1 ? ?2. The distance of a
method?s curve from the center is the method?s ?2
for that setting. The farther a point is from the center
the higher ?2 is; hence, methods whose curves are
closer to the diagram?s outmost perimeter are better.
Clearly, SVR-REC (which includes the recognizer?s
features) outperforms SVR-BASE (which relies only
on the language model and the scores of the rules).
The two peaks of SVR-REC?s curve are when ?3
is very high (1 or 0.8), i.e., when y(xi) is dominated
by the diversity score; in these cases, SVR-REC is
at a clear advantage, since it includes features for
surface string similarity (e.g., Levenshtein distance
measured on ?s1, c1?), which in effect measure di-
versity, unlike SVR-BASE. Even when ?1 is very
high (1 or 0.8), i.e., when all or most of the weight
is placed on grammaticality, SVR-REC outperforms
SVR-BASE, indicating that the extra features in SVR-
REC also contribute towards assessing grammatical-
ity; by contrast SVR-BASE relies exclusively on the
language model for grammaticality. Unfortunately,
when ?2 is very high (1 or 0.8), i.e., when all or
most of the weight is placed on meaning preserva-
tion, there is no or very small difference between
SVR-REC and SVR-BASE, suggesting that the extra
features of the paraphrase recognizer are not as use-
ful to the SVR, when assessing meaning preserva-
tion, as we would have hoped. Nevertheless, SVR-
REC is overall better than SVR-BASE.
We believe that the dataset of Section 3 and the
evaluation methodology summarized by Figure 3
will prove useful to other researchers, who may wish
to evaluate other ranking components of generate-
and-rank paraphrasing methods against ours, for ex-
ample with different ranking algorithms or features.
Similar datasets of candidate paraphrases can also
be created using different collections of paraphras-
ing rules.16 The same methodology can then be used
to evaluate ranking components on those datasets.
5 Comparison to the state of the art
Having established that SVR-REC is a better config-
uration of our method?s ranker than SVR-BASE, we
16See Androutsopoulos and Malakasiotis (2010) for pointers.
102
proceed to investigate how well our overall generate-
and-rank method (with SVR-REC) compares against
a state of the art paraphrase generator.
As already mentioned, Zhao et al (2010) recently
presented a method (we call it ZHAO-ENG) that out-
performs their previous method (Zhao et al, 2009a),
which used paraphrasing rules and an SMT-like de-
coder (we call that previous method ZHAO-RUL).
Given an input sentence S, ZHAO-ENG produces
candidate paraphrases by translating S to 6 pivot
languages via 3 different commercial machine trans-
lation engines (treated as black boxes) and then back
to the original language, again via 3 machine transla-
tion engines (54 combinations). Roughly speaking,
ZHAO-ENG then ranks the candidate paraphrases by
their average distance from all the other candidates,
selecting the candidate(s) with the smallest distance;
distance is measured as BLEU score (Papineni et
al., 2002).17 Hence, ZHAO-ENG is also, in effect,
a generate-and-rank paraphraser, but the candidates
are generated by invoking multiple machine transla-
tion engines instead of applying paraphrasing rules,
and they are ranked by the average distance measure
rather than using an SVR.
An obvious practical advantage of ZHAO-ENG is
that it exploits the vast resources of existing com-
mercial machine translation engines when generat-
ing candidate paraphrases, which allows it to always
obtain large numbers of candidate paraphrases. By
contrast, the collection of paraphrasing rules that we
currently use does not manage to produce any can-
didate paraphrases in 40% of the sentences of the
New York Times part of AQUAINT, because no rule
applies. Hence, in terms of ability to always para-
phrase the input, ZHAO-ENG is clearly better, though
it should be possible to improve our methods?s per-
formance in that respect by using larger collections
of paraphrasing rules.18 A further interesting ques-
tion, however, is how good the paraphrases of the
two methods are, when both methods manage to
paraphrase the input, i.e., when at least one para-
17We use the version of ZHAO-ENG that Zhao et al (2010)
call ?selection-based?, since they reported it performs overall
better than an alternative decoding-based version.
18Recall that the paraphrasing rules we use were extracted
from an English-Chinese parallel corpus. Additional rules
could be extracted from other parallel corpora, like Europarl
(http://www.statmt.org/europarl/).
phrasing rule applies to S. This scenario can be seen
as an emulation of the case where the collection of
paraphrasing rules is sufficiently large to guarantee
that at least one rule applies to any source sentence.
To answer the latter question, we re-implemented
ZHAO-ENG, with the same machine translation en-
gines and languages used by Zhao et al (2010).
We also trained our paraphraser (with SVR-REC) on
the training part of the dataset of Section 3. We
then selected 300 random source sentences S from
AQUAINT that matched at least one of the paraphras-
ing rules, excluding sentences that had been used be-
fore. Then, for each one of the 300 S sentences, we
kept the single best candidate paraphraseC1 andC2,
respectively, returned by our paraphraser and ZHAO-
ENG. The resulting ?S,C1? and ?S,C2? pairs were
given to 10 human judges. This time the judges
assigned only grammaticality and meaning preser-
vation scores (on a 1?4 scale); diversity was again
computed as edit distance. Each pair was evaluated
by one judge, who was given an equal number of
pairs from the two methods, without knowing which
method each pair came from. The same judge never
rated two pairs with the same S. Since we had no
way to make ZHAO-ENG sensitive to ?1, ?2, ?3, we
trained SVR-REC with ?1 = ?2 = 1/3, as the most
neutral combination of weights.
Table 2 lists the average grammaticality, meaning
preservation, and diversity scores of the two meth-
ods. All scores were normalized in [0, 1], but the
reader should keep in mind that diversity was com-
puted as edit distance, whereas the other two scores
were provided by human judges on a 1?4 scale. The
grammaticality score of our method was better than
ZHAO-ENG?s, and the difference was statistically
significant.19 In meaning preservation, ZHAO-ENG
was slightly better, but the difference was not statis-
tically significant. The difference in diversity was
larger and statistically significant, with the diversity
scores indicating that it takes approximately twice as
many edit operations (insert, delete, replace) to turn
each source sentence to ZHAO-ENG?s paraphrase,
compared to the paraphrase of our method.
We note that our method can be tuned, by ad-
justing the ?i weights, to produce paraphrases with
19We used Analysis of Variance (ANOVA) (Fisher, 1925), fol-
lowed by post-hoc Tukey tests to check whether the scores of
the two methods differ significantly (p < 0.05).
103
score (%) our method ZHAO-ENG
grammaticality 90.89 85.33
meaning preserv. 76.67 78.56
diversity 6.50 14.58
Table 2: Evaluation of our paraphrasing method (with
SVR-REC) against ZHAO-ENG, using human judges. Re-
sults in bold indicate statistically significant differences.
higher grammaticality, meaning preservation, or di-
versity scores; for example, we could increase ?3
and decrease ?1 to obtain higher diversity at the cost
of lower grammaticality in the results of Table 2.20 It
is unclear how ZHAO-ENG could be tuned that way.
Overall, our method seems to perform well
against ZHAO-ENG, despite the vastly larger re-
sources of ZHAO-ENG, provided of course that we
limit ourselves to source sentences to which para-
phrasing rules apply. It would be interesting to in-
vestigate in future work if our method?s coverage
(sentences it can paraphrase) can increase to ZHAO-
ENG?s level by using larger collections of paraphras-
ing rules. It would also be interesting to combine the
two methods, perhaps by using SVR-REC (without
features for the quality scores of the rules) to rank
candidate paraphrases generated by ZHAO-ENG.
6 Conclusions and future work
We presented a generate-and-rank method to para-
phrase sentences. The method first produces can-
didate paraphrases by applying existing paraphras-
ing rules extracted from parallel corpora, and it then
ranks (or classifies) the candidates to keep the best
ones. The ranking component considers not only the
context-insensitive quality scores of the paraphras-
ing rules that produced each candidate, but also fea-
tures intended to measure the extent to which the
rule applications preserve grammaticality and mean-
ing in the particular context of the input sentence, as
well as the degree to which the resulting candidate
differs from the input sentence (diversity).
Initial experiments with a Maximum Entropy
classifier confirmed that the features we use can help
a ranking component select better candidate para-
phrases than a baseline ranker that considers only
20Additional application-specific experiments confirm that
this tuning is possible (Malakasiotis, 2011).
the average context-insensitive quality scores of the
applied rules. Further experiments with an SVR
ranker indicated that our full feature set, which in-
cludes features from an existing paraphrase recog-
nizer, leads to improved performance, compared to
a smaller feature set that includes only the context-
insensitive scores of the rules and language model-
ing scores. We also propose a new methodology to
evaluate the ranking components of generate-and-
rank paraphrase generators, which evaluates them
across different combinations of weights for gram-
maticality, meaning preservation, and diversity. The
paper is accompanied by a paraphrasing dataset we
constructed for evaluations of this kind.
Finally, we evaluated our overall method against
a state of the art sentence paraphraser, which
generates candidates by using several commercial
machine translation systems and pivot languages.
Overall, our method performed well, despite the vast
resources of the machine translation systems em-
ployed by the system we compared against. Our
method performed better in terms of grammaticality,
equally well in meaning preservation, and worse in
diversity, but it could be tuned to obtain higher diver-
sity at the cost of lower grammaticality, whereas it
is unclear how the system we compare against could
be tuned this way. On the other hand, an advantage
of the paraphraser we compared against is that it al-
ways produces paraphrases; by contast, our system
does not produce paraphrases when no paraphrasing
rule applies to the source sentence. Larger collec-
tions of paraphrasing rules would be needed to im-
prove our method in that respect.
Apart from obtaining and experimenting with
larger collections of paraphrasing rules, it would be
interesting to evaluate our method in vivo, for ex-
ample by embedding it in question answering sys-
tems (to paraphrase the questions), in information
extraction systems (to paraphrase extraction tem-
plates), or in natural language generators (to para-
phrase template-like sentence plans). We also plan
to investigate the possibility of embedding our SVR
ranker in the sentence paraphraser we compared
against, i.e., to rank candidates produced by using
several machine translation systems and pivot lan-
guages, as in ZHAO-ENG.
104
Acknowledgments
This work was partly carried out during INDIGO, an
FP6 IST project funded by the European Union, with
additional funding from the Greek General Secre-
tariat of Research and Technology.21
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of the 43rd
ACL, pages 597?604, Ann Arbor, MI.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of HLT-NAACL, pages 17?24, New
York, NY.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proc. of
EMNLP, pages 196?205, Honolulu, HI, October.
J. Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22:249?254.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
1(31):399?429.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?69.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009. Rec-
ognizing textual entailment: Rational, evaluation and
approaches. Natural Lang. Engineering, 15(4):i?xvii.
Editorial of the special issue on Textual Entailment.
P. A. Duboue and J. Chu-Carroll. 2006. Answering the
question you wish they had asked: The impact of para-
phrasing for question answering. In Proc. of HLT-
NAACL, pages 33?36, New York, NY.
Ronald A. Fisher. 1925. Statistical Methods for Re-
search Workers. Oliver and Boyd.
T. Joachims. 2002. Learning to Classify Text Using Sup-
port Vector Machines: Methods, Theory, Algorithms.
Kluwer.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL, pages
455?462, New York, NY.
21Consult http://www.ics.forth.gr/indigo/.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probalistic approach to
sentence compression. Artif. Intelligence, 139(1):91?
107.
P. Koehn. 2009. Statistical Machine Translation. Cam-
bridge University Press.
S. Kok and C. Brockett. 2010. Hitting the right para-
phrases in good time. In Proc. of HLT-NAACL, pages
145?153, Los Angeles, CA.
Y. Lepage and E. Denoual. 2005. Automatic genera-
tion of paraphrases to be used as translation references
in objective evaluation measures of machine transla-
tion. In Proc. of the 3rd Int. Workshop on Paraphras-
ing, pages 57?64, Jesu Island, Korea.
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
N. Madnani, F. Ayan, P. Resnik, and B. J. Dorr. 2007.
Using paraphrases for parameter tuning in statistical
machine translation. In Proc. of 2nd Workshop on Sta-
tistical Machine Translation, pages 120?127, Prague,
Czech Republic.
P. Malakasiotis. 2009. Paraphrase recognition us-
ing machine learning to combine similarity measures.
In Proc. of the Student Research Workshop of ACL-
AFNLP, Singapore.
P. Malakasiotis. 2011. Paraphrase and Textual Entail-
ment Recognition and Generation. Ph.D. thesis, De-
partment of Informatics, Athens University of Eco-
nomics and Business, Greece.
Y. Marton, C. Callison-Burch, and P. Resnik. 2009.
Improved statistical machine translation using
monolingually-derived paraphrases. In Proc. of
EMNLP, pages 381?390, Singapore.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and I. Szpektor. 2009. Source-language en-
tailment modeling for translating unknown terms. In
Proc. of ACL-AFNLP, pages 791?799, Singapore.
R. Nelken and S. M. Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proc. of the 11th EACL, pages 161?168,
Trento, Italy.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Robust machine translation evaluation with en-
tailment features. In Proc. of ACL-AFNLP, pages 297?
305, Singapore.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of the 40th ACL, pages 311?318,
Philadelphia, PA.
C. Quirk, C. Brockett, and W. B. Dolan. 2004. Mono-
lingual machine translation for paraphrase generation.
In Proc. of the Conf. on EMNLP, pages 142?149,
Barcelona, Spain.
105
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proc. of the 45th
ACL, pages 464?471, Prague, Czech Republic.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proc. of ACL-HLT,
pages 683?691, Columbus, OH.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
S. Zhao, H. Wang, T. Liu, and S. Li. 2008. Pivot ap-
proach for extracting paraphrase patterns from bilin-
gual corpora. In Proc. of ACL-HLT, pages 780?788,
Columbus, OH.
S. Zhao, X. Lan, T. Liu, and S. Li. 2009a. Application-
driven statistical paraphrase generation. In Proc. of
ACL-AFNLP, pages 834?842, Singapore.
S. Zhao, H. Wang, T. Liu, and Li. S. 2009b. Extract-
ing paraphrase patterns from bilingual parallel cor-
pora. Natural Language Engineering, 15(4):503?526.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010. Leverag-
ing multiple MT engines for paraphrase generation. In
Proceedings of the 23rd COLING, pages 1326?1334,
Beijing, China.
L. Zhou, C.-Y. Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proc. of the Conf. on EMNLP, pages 77?84.
106
Proceedings of the EACL 2009 Demonstrations Session, pages 17?20,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
An Open-Source Natural Language Generator for OWL Ontologies and
its Use in Prote?ge? and Second Life
Dimitrios Galanis?, George Karakatsiotis?, Gerasimos Lampouras?, Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Athens, Greece
+Digital Curation Unit, Research Centre ?Athena?, Athens, Greece
Abstract
We demonstrate an open-source natural
language generation engine that produces
descriptions of entities and classes in En-
glish and Greek from OWL ontologies that
have been annotated with linguistic and
user modeling information expressed in
RDF. We also demonstrate an accompany-
ing plug-in for the Prote?ge? ontology editor,
which can be used to create the ontology?s
annotations and generate previews of the
resulting texts by invoking the generation
engine. The engine has been embedded in
robots acting as museum tour guides in the
physical world and in Second Life; here
we demonstrate the latter application.
1 Introduction
NaturalOWL (Galanis and Androutsopoulos, 2007;
Androutsopoulos and Galanis, 2008) is a natu-
ral language generation engine that produces de-
scriptions of entitities (e.g., items for sale, mu-
seum exhibits) and classes (e.g., types of exhibits)
in English and Greek from OWL DL ontologies;
the ontologies must have been annotated with lin-
guistic and user modeling annotations expressed
in RDF.1 An accompanying plug-in for the well
known Prote?ge? ontology editor is available, which
can be used to create the linguistic and user model-
ing annotations while editing an ontology, as well
as to generate previews of the resulting texts by
invoking the generation engine.2
NaturalOWL is based on ideas from ILEX
(O?Donnell et al, 2001) and M-PIRO (Isard et al,
2003; Androutsopoulos et al, 2007), but it uses
1See http://www.w3.org/TR/owl-features/
for information on OWL and its versions. For information
on RDF, consult http://www.w3.org/RDF/.
2M-PIRO?s authoring tool (Androutsopoulos et al, 2007),
now called ELEON (Bilidas et al, 2007), can also be used; see
http://www.iit.demokritos.gr/skel/.
Figure 1: Generating texts in Second Life.
templates instead of systemic grammars, it is pub-
licly available as open-source software, it is writ-
ten entirely in Java, and it provides native support
for OWL ontologies, making it particularly useful
for Semantic Web applications (Antoniou and van
Harmelen, 2004).3 Well known advantages of nat-
ural language generation (Reiter and Dale, 2000)
include the ability to generate texts in multiple lan-
guages from the same ontology; and the ability to
tailor the semantic content and language expres-
sions of the texts to the user type (e.g., child vs.
adult) and the interaction history (e.g., by avoiding
repetitions, or by comparing to previous objects).
In project XENIOS (Vogiatzis et al, 2008), Nat-
uralOWL was embedded in a mobile robot acting
as a museum guide, and in project INDIGO it is
being integrated in a more advanced robotic guide
that includes a multimodal dialogue manager, fa-
cial animation, and mechanisms to recognize and
express emotions (Konstantopoulos et al, 2009).
Here, we demonstrate a similar application, where
NaturalOWL is embedded in a robotic avatar acting
3NaturalOWL comes with a GNU General Public Li-
cense (GPL). The software can be downloaded from
http://nlp.cs.aueb.gr/.
17
as a museum guide in Second Life (Oberlander et
al., 2008), as shown in figure 1. We also demon-
strate how the underlying ontology of the museum
and its linguistic and user modeling annotations
can be edited in Prote?ge?.
2 NaturalOWL?s architecture
NaturalOWL adopts a typical natural language
generation pipeline (Reiter and Dale, 2000). It
produces texts in three stages: document planning,
microplanning, and surface realization.
In document planning, the system first selects
from the ontology the logical facts (OWL triples)
that will be conveyed to the user, taking into ac-
count interest scores manually assigned to the
facts via the annotations of the ontology, as well
as a dynamcally updated user model that shows
what information has already been conveyed to the
user. Logical facts that report similarities or differ-
ences to previously encountered entities may also
be included in the output of content selection, giv-
ing rise to comparisons like the one in figure 1.
The selected facts are then ordered using a man-
ually specified partial order, which is also part of
the ontology?s annotations.
In micro-planning, the system turns each se-
lected fact into a sentence by using micro-plans, in
effect patterns that leave referring expressions un-
derspecified. Figure 2 shows a micro-plan being
edited with NaturalOWL?s Prote?ge? plug-in. The
micro-plan specifies that to express a fact that in-
volves the made-of property, the system should
concatenate an automatically generated referring
expression (e.g., name, pronoun, definite noun
phrase) in nominative case for the owner of the
fact (semantic subject of the triple), the verb form
?is made? (or ?are made?, if the subject is in plu-
ral), the preposition ?of?, and then another au-
tomatically generated referring expression in ac-
cusative case for the filler of the property (seman-
tic object). The referring expressions are gener-
ated by taking into account the context of each
sentence, attempting to avoid repetitions without
introducing ambiguities. Domain-independent ag-
gregation rules are then employed to combine the
resulting sentences into longer ones.
In surface realization, the final form of the text
is produced; it can be marked up automatically
with tags that indicate punctuation symbols, gram-
matical categories, the logical facts expressed by
the sentences, the interest (Int) of each sen-
tence?s information, the degree (Assim) to which
the information is taken to be assimilated by the
user etc., as shown below. In INDIGO, compar-
isons are also marked up with angles that guide
the robot to turn to the object(s) it compares to.
<Period>
<Sentence Property=".../#type"
Int="3" Assim="0">
<Demonstrative ref=".../#exhibit1"
role="owner">
This</Demonstrative>
<Verb>is</Verb>
<NP ref=".../#Amphora" role="filler">
an amphora</NP>
</Sentence>
<Punct>,</Punct>
<Sentence Property=".../#subtype
Int="3" Assim="1">
<EmptyRef ref=".../#Amphora"
role="owner"/>
<NP ref=".../#Vessel" role="filler">
a type of vessel</NP>
</Sentence>
<Punct>;</Punct>
<Sentence Property=".../#paintedBy"
Int="2" Assim="0">
<Pronoun ref=".../#exhibit1"
role="owner">
it</Pronoun>
<Verb>was painted</Verb>
<Preposition>by</Preposition>
<Name ref=".../#pKleo" role="filler">
the painter of Kleophrades</Name>
</Sentence>
<Punct>.</Punct>
</Period>
2.1 Using NaturalOWL?s Prote?ge? plug-in
NaturalOWL?s plug-in for Prote?ge? can be used to
specify all the linguistic and user modeling an-
notations of the ontologies that NaturalOWL re-
quires. The annotations in effect establish a
domain-dependent lexicon, whose entries are as-
sociated with classes or entities of the ontology;
micro-plans, which are associated with proper-
ties of the ontology; a partial order of proper-
ties, which is used in document planning; interest
scores, indicating how interesting the various facts
of the ontology are to each user type; parameters
that control, for example, the desired length of the
generated texts. The plug-in can also be used to
generate previews of the resulting texts, for differ-
ent types of users, with or without comparisons,
etc., as illustrated in figure 3. The resulting anno-
tations are then saved in RDF.
2.2 Using NaturalOWL in Second Life
In Second Life, each user controls an avatar, which
can, among other actions, move in the virtual
world, touch objects, or communicate with other
18
Figure 2: Specifying a micro-plan with NaturalOWL?s Prote?ge? plug-in.
Figure 3: Generating a text preview with NaturalOWL?s Prote?ge? plug-in.
19
avatars; in the latter case, the user types text on the
keyboard. In the Second Life application that we
demonstrate, the robot is an avatar that is not con-
trolled by a human, but by our own Second Life
client software.4 The client software includes a
navigation component, which controls the robot?s
movement, and it allows the robot to ?utter? texts
generated by NaturalOWL, instead of expecting
keyboard input. Whenever a visitor near the robot
touches an exhibit, an appropriate event is sent to
the robot, which then goes near the exhibit and
starts describing it.5
3 Conclusions and further work
The demonstration presents an open-source nat-
ural language generation engine for OWL ontolo-
gies, which generates descriptions of entities and
classes in English and Greek. The engine is ac-
companied by a Prote?ge? plug-in, which can be
used to annotate the ontologies with linguistic and
user modeling information required by the gener-
ation engine. The demonstration includes an ap-
plication in Second Life, where the generation en-
gine is embedded in a robotic avatar acting as a
museum guide. We are currently extending Natu-
ralOWL to handle follow up questions about enti-
ties or classes mentioned in the generated texts.
Acknowledgments
NaturalOWL was developed in project XENIOS,
which was funded by the Greek General Secre-
tariat of Research and Technology and the Euro-
pean Union.6 NaturalOWL is now being extended
in project INDIGO, which is funded by the Euro-
pean Union; our work in INDIGO is also supported
by additional funding from the Greek General Sec-
retariat of Research and Technology.7
References
I. Androutsopoulos and D. Galanis. 2008. Generating
natural language descriptions fromOWL ontologies:
experience from the NaturalOWL system. Technical
report, Department of Informatics, Athens Univer-
sity of Economics and Business, Greece.
4Our client was built using the libsecondlife li-
brary; see http://www.libsecondlife.org/. More
precisly, the robot is an object controlled by an invisible
robotic avatar, which is in turn controlled by our client.
5A video showing the robotic avatar in action is available
at http://www.vimeo.com/801099.
6See http://www.ics.forth.gr/xenios/.
7See http://www.ics.forth.gr/indigo/.
I. Androutsopoulos, J. Oberlander, and V. Karkaletsis.
2007. Source authoring for multilingual generation
of personalised object descriptions. Natural Lan-
guage Engineering, 13(3):191?233.
G. Antoniou and F. van Harmelen. 2004. A Semantic
Web primer. MIT Press.
D. Bilidas, M. Theologou, and V. Karkaletsis. 2007.
Enriching OWL ontologies with linguistic and user-
related annotations: the ELEON system. In Proceed-
ings of the 19th IEEE International Conference on
Tools with Artificial Intelligence, Patras, Greece.
D. Galanis and I. Androutsopoulos. 2007. Generat-
ing multilingual descriptions from linguistically an-
notated OWL ontologies: the NATURALOWL sys-
tem. In Proceedings of the 11th European Workshop
on Natural Language Generation, pages 143?146,
Schloss Dagstuhl, Germany.
A. Isard, J. Oberlander, I. Androutsopoulos, and
C. Matheson. 2003. Speaking the users? languages.
IEEE Intelligent Systems, 18(1):40?45.
S. Konstantopoulos, A. Tegos, D. Bilidas, I. Androut-
sopoulos, G. Lampouras, P. Malakasiotis, C. Math-
eson, and O. Deroo. 2009. Adaptive natural-
language interaction. In Proceedings of 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics (system demonstra-
tions), Athens, Greece.
J. Oberlander, G. Karakatsiotis, A. Isard, and I. An-
droutsopoulos. 2008. Building an adaptive museum
gallery in Second Life. In Proceedings of Museums
and the Web, Montreal, Quebec, Canada.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7(3):225?250.
E. Reiter and R. Dale. 2000. Building natural lan-
guage generation systems. Cambridge University
Press.
D. Vogiatzis, D. Galanis, V. Karkaletsis, I. Androut-
sopoulos, and C.D. Spyropoulos. 2008. A conver-
sant robotic guide to art collections. In Proceedings
of the 2nd Workshop on Language Technology for
Cultural Heritage Data, Language Resources and
Evaluation Conference, Marrakech, Morocco.
20
Proceedings of the EACL 2009 Demonstrations Session, pages 37?40,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Adaptive Natural Language Interaction
Stasinos Konstantopoulos
Athanasios Tegos
Dimitris Bilidas
NCSR ?Demokritos?, Athens, Greece
Colin Matheson
Human Communication Research Centre
Edinburgh University, U.K.
Ion Androutsopoulos
Gerasimos Lampouras
Prodromos Malakasiotis
Athens Univ. of Economics and Business
Greece
Olivier Deroo
Acapela Group, Belgium
Abstract
The subject of this demonstration is natu-
ral language interaction, focusing on adap-
tivity and profiling of the dialogue man-
agement and the generated output (text
and speech). These are demonstrated in
a museum guide use-case, operating in a
simulated environment. The main techni-
cal innovations presented are the profiling
model, the dialogue and action manage-
ment system, and the text generation and
speech synthesis systems.
1 Introduction
In this demonstration we present a number of
state-of-the art language technology tools, imple-
menting and integrating the latest discourse and
knowledge representation theories into a complete
application suite, including:
? dialogue management, natural language gen-
eration, and speech synthesis, all modulated
by a flexible and highly adaptable profiling
mechanism;
? robust speech recognition and language inter-
pretation; and,
? an authoring environment for developing the
representation of the domain of discourse as
well as the associated linguistic and adaptiv-
ity resources.
The system demonstration is based on a use
case of a virtual-tour guide in a museum domain.
Demonstration visitors interact with the guide us-
ing headsets and are able to experiment with load-
ing different interaction profiles and observing the
differences in the guide?s behaviour. The demon-
stration also includes the screening of videos from
an embodied instantiation of the system as a robot
guiding visitors in a museum.
2 Technical Content
The demonstration integrates a number of state-of-
the-art language components into a highly adap-
tive natural language interaction system. Adap-
tivity here refers to using interaction profiles that
modulate dialogue management as well as text
generation and speech synthesis. Interaction pro-
files are semantic models that extend the objective
ontological model of the domain of discourse with
subjective information, such as how ?interesting?
or ?important? an entity or statement of the objec-
tive domain model is.
Advanced multimodal dialogue management
capabilities involving and combining input and
output from various interaction modalities and
technologies, such as speech recognition and syn-
thesis, natural language interpretation and gener-
ation, and recognition of/response to user actions,
gestures, and facial expressions.
State-of-the art natural language generation
technology, capable of producing multi-sentence,
coherent natural language descriptions of objects
based on their abstract semantic representation.
The resulting descriptions vary dynamically in
terms of content as well as surface language ex-
pressions used to realize each description, depend-
ing on the interaction history (e.g., comparing
to previously given information) and the adaptiv-
ity parameters (exhibiting system personality and
adapting to user background and interests).
3 System Description
The system is capable of interacting in a vari-
ety of modalities, including non-verbal ones such
as gesture and face-expression recognition, but in
this demonstration we focus on the system?s lan-
guage interaction components. In this modality,
abstract, language-independent system actions are
first planned by the dialogue and action manager
(DAM), then realized into language-specific text
37
by the natural language generation engine, and fi-
nally synthesized into speech. All three layers are
parametrized by a profiling and adaptivity module.
3.1 Profiling and Adaptation
Profiling and adaptation modulates the output of
dialogue management, generation, and speech
synthesis so that the system exhibits a synthetic
personality, while at the same time adapting to
user background and interests.
User stereotypes (e.g., ?expert? or ?child?) pro-
vide generation parameters (such as maximum de-
scription length) and also initialize the dynamic
user model with interest rates for all the ontologi-
cal entities (individuals and properties) of the do-
main of discourse. This same information is also
provided in system profiles reflecting the system?s
(as opposed to the users?) preferences; one can,
for example, define a profile that favours using
the architectural attributes to describe a building
where another profile would choose to concentrate
on historical facts regarding the same building.
Stereotypes and profiles are combined into a
single set of parameters by means of personal-
ity models. Personality models are many-valued
Description Logic definitions of the overall pref-
erence, grounded in stereotype and profile data.
These definitions model recognizable personality
traits so that, for example, an open personality will
attend more to the user?s requests than its own
interests in deriving overall preference (Konstan-
topoulos et al, 2008).
Furthermore, the system dynamically adapts
overall preference according to both interaction
history and the current dialogue state. So, for one,
the initial (static model) interest factor of an ontol-
ogy entity is reduced each time this entity is used
in a description in order to avoid repetitions. On
the other hand, preference will increase if, for ex-
ample, in the current state the user has explicitly
asked about an entity.
3.2 Dialogue and Action Management
The DAM is built around the information-state
update dialogue paradigm of the TRINDIKIT
dialogue-engine toolkit (Cooper and Larsson,
1998) and takes into account the combined user-
robot interest factor when determining informa-
tion state updates.
The DAM combines various interaction modal-
ities and technologies in both interpretation/fusion
and generation/fission. In interpreting user ac-
tions the system recognizes spoken utterances,
simple gestures, and touch-screen input, all of
which may be combined into a representation of
a multi-modal user action. Similarly, when plan-
ning robotic actions the DAM coordinates a num-
ber of available output modalities, including spo-
ken language, text (on the touchscreen), the move-
ment and configuration of the robotic platform, fa-
cial expressions, and simple head gestures.1
To handle multimodal input, the DAM uses a fu-
sion module which combines messages from the
language interpretation, gesture, and touchscreen
modules into a single XML structure. Schemati-
cally, this can be represented as:
<userAction>
<userUtterance>hello</userUtterance>
<userButton content="13"/>
</userAction>
This structure represents a user pressing some-
thing on the touchscreen and saying hello at the
same time.2
The representation is passed essentially un-
changed to the DAM, to be processed by its up-
date rules, where the ID of button press is inter-
preted in context and matched with the speech.
In most circumstances, the natural language pro-
cessing component (see 3.3) produces a seman-
tic representation of the input which appears in
the userUtterance element; the use of ?hello?
above is for illustration. An example update rule
which will fire in the context of a greeting from
the user is (in schematic form):
if
in(/latest_utterance/moves, hello)
then
output(start)
Update rules contain a list of conditions and a
list of effects. Here there is one condition (that the
latest moves from the user includes ?hello?), and
one effect (the ?start? procedure). The latter initi-
ates the dialogue by, among other things, having
the system utter a standardised greeting.
As noted above, the DAM is also multimodal
on the output side. An XML representation is
created which can contain robot utterances and
robot movements (both head movements and mo-
bile platform moves). Information can also be pre-
sented on the touchscreen.
1Expressions and gestures will not be demonstrated, as
they can not be materialized in the simulated robot.
2The precise meaning of ?at the same time? is determined
by the fusion module.
38
3.3 Natural Language Processing
The NATURALOWL natural language generation
(NLG) engine (Galanis et al 2009) produces
multi-sentence, coherent natural language descrip-
tions of objects in multiple languages from a sin-
gle semantic representation; the resulting descrip-
tions are annotated with prosodic markup for driv-
ing the speech synthesisers.
The generated descriptions vary dynamically, in
both content and language expressions, depending
on the interaction profile as well as the dynamic
interaction history. The dynamic preference factor
of the item itself is used to decide the level of de-
tail of the description being generated. The prefer-
ence factors of the properties are used to order the
contents of the descriptions to ensure that, in cases
where not all possible facts are to be presented in
a single turn, the most relevant ones are chosen.
The interaction history is used to check previously
given information to avoid repeating the same in-
formation in different contexts and to create com-
parisons with earlier objects.
NaturalOWL demonstrates the benefits of
adopting NLG on the Semantic Web. Organiza-
tions that need to publish information about ob-
jects, such as exhibits or products, can publish
OWL ontologies instead of texts. NLG engines,
embedded in browsers or Web servers, can then
render the ontologies in natural language, whereas
computer programs may access the ontologies, in
effect logical statements, directly. The descrip-
tions can be very simple and brief, relying on
question answering to provide more information
if such is requested. This way, machine-readable
information can be more naturally inspected and
consulted by users.
In order to generate a list of possible follow
up questions that the system can handle, we ini-
tially construct a list of the particular individuals
or classes that are mentioned in the generated de-
scription; the follow up questions will most likely
refer to them. Only individuals and classes for
which there is further information in the ontology
are extracted.
After identifying the referred individuals and
classes, we proceed to predict definition (e.g.,
?Who was Ares??) and property questions (e.g.,
?Where is Mount Penteli??) about them that
could be answered by the information in the on-
tology. We avoid generating questions that cannot
be answered. The expected definition questions
are constructed by inserting the names of the re-
ferred individuals and classes into templates such
as ?who is/was person X?? or ?what do you know
about class or entity Y??.
In the case of referred individuals, we also gen-
erate expected property questions using the pat-
terns NaturalOWL generates the descriptions with.
These patterns, called microplans, show how to
express the properties of the ontology as sentences
of the target languages. For example, if the indi-
vidual templeOfAres has the property excavate-
dIn, and that property has a microplan of the form
?resource was excavated in period?, we anticipate
questions such as ?when was the Temple of Ares
excavated?? and ?which period was the Temple of
Ares excavated in??.
Whenever a description (e.g., of a monument)
is generated, the expected follow up questions for
that description (e.g., about the monument?s ar-
chitect) are dynamically included in the rules of
the speech recognizer?s grammar, to increase word
recognition accuracy. The rules include compo-
nents that extract entities, classes, and properties
from the recognized questions, thus allowing the
dialogue and action manager to figure out what the
user wishes to know.
3.4 Speech Synthesis and Recognition
The natural language interface demonstrates ro-
bust speech recognition technology, capable of
recognizing spoken phrases in noisy environ-
ments, and advanced speech synthesis, capable of
producing spoken output of very high quality. The
main challenge that the automatic speech recogni-
tion (ASR) module needs to address is background
noise, especially in the robot-embodied use case.
A common technique used in order to handle this
is training acoustic models with the anticipated
background noise, but that is not always possi-
ble. The demonstrated ASR module can be trained
on noise-contaminated data where available, but
also incorporates multi-band acoustic modelling
(Dupont, 2003) for robust recognition under noisy
conditions. Speech recognition rates are also sub-
stantially improved by using the predictions made
by NATURALOWL and the DAM to dynamically
restrict the lexical and phrasal expectations at each
dialogue turn.
The speech synthesis module of the demon-
strated system is based on unit selection technol-
ogy, generally recognized as producing more nat-
39
ural output that previous technologies such as di-
phone concatenation or formant synthesis. The
main innovation that is demonstrated is support for
emotion, a key aspect of increasing the naturalness
of synthetic speech. This is achieved by combin-
ing emotional unit recordings with run-time trans-
formations. With respect to the former, a complete
?voice? now comprises three sub-voices (neutral,
happy, and sad), based on recordings of the same
speaker. The recording time needed is substan-
tially decreased by prior linguistic analysis that se-
lects appropriate text covering all phonetic units
needed by the unit selection system. In addition to
the statically defined sub-voices, the speech syn-
thesis module implements dynamic transforma-
tions (e.g., emphasis), pauses, and variable speech
speed. The system combines all these capabilities
in order to dynamically modulate the synthesised
speech to convey the impression of emotionally
modulated speech.
3.5 Authoring
The interaction system is complemented by
ELEON (Bilidas et al, 2007), an authoring tool for
annotating domain ontologies with the generation
and adaptivity resources described above. The do-
main ontology can be authored in ELEON, but any
existing OWL ontology can also annotated.
More specifically, ELEON supports author-
ing linguistic resources, including a domain-
dependent lexicon, which associates classes and
individuals of the ontology with nouns and proper
names of the target natural languages; microplans,
which provide the NLG with patterns for realizing
property instances as sentences; and a partial or-
dering of properties, which allows the system to
order the resulting sentences as a coherent text.
The adaptivity and profiling resources include
interest rates, indicating how interesting the enti-
ties of the ontology are in any given profile; and
stereotype parameters that control generation as-
pects such as the number of facts to include in a
description or the maximum sentence length.
Furthermore, ELEON supports the author with
immediate previews, so that the effect of any
change in either the ontology or the associated re-
sources can be directly reviewed. The actual gen-
eration of the preview is relegated to external gen-
eration engines.
4 Conclusions
The demonstrated system combines semantic rep-
resentation and reasoning technologies with lan-
guage technology into a human-computer interac-
tion system that exhibits a large degree of adapt-
ability to audiences and circumstances and is able
to take advantage of existing domain model cre-
ated independently of the need to build a natural
language interface. Furthermore by clearly sepa-
rating the abstract, semantic layer from that of the
linguistic realization, it allows the re-use of lin-
guistic resources across domains and the domain
model and adaptivity resources across languages.
Acknowledgements
The demonstrated system is being developed by
the European (FP6-IST) project INDIGO.3 IN-
DIGO develops and advances human-robot inter-
action technology, enabling robots to perceive nat-
ural human behaviour, as well as making them
act in ways that are more familiar to humans. To
achieve its goals, INDIGO advances various tech-
nologies, which it integrates in a robotic platform.
References
Dimitris Bilidas, Maria Theologou, and Vangelis
Karkaletsis. 2007. Enriching OWL ontologies
with linguistic and user-related annotations: the
ELEON system. In Proc. 19th Intl. Conf. on
Tools with Artificial Intelligence (ICTAI-2007).
Robin Cooper and Staffan Larsson. 1998. Dia-
logue Moves and Information States. In: Pro-
ceedings of the 3rd Intl. Workshop on Computa-
tional Semantics (IWCS-3).
Ste?phane Dupont. 2003. Robust parameters
for noisy speech recognition. U.S. Patent
2003182114.
Dimitrios Galanis, George Karakatsiotis, Gerasi-
mos Lampouras and Ion Androutsopoulos.
2009. An open-source natural language gener-
ator for OWL ontologies and its use in Prote?ge?
and Second Life. In this volume.
Stasinos Konstantopoulos, Vangelis Karkaletsis,
and Colin Matheson. 2008. Robot personality:
Representation and externalization. In Proc.
Computational Aspects of Affective and Emo-
tional Interaction (CAFFEi 08), Patras, Greece.
3http://www.ics.forth.gr/indigo/
40
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78?87,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Multi-Granular Aspect Aggregation in Aspect-Based Sentiment Analysis
John Pavlopoulos and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
http://nlp.cs.aueb.gr/
Abstract
Aspect-based sentiment analysis estimates
the sentiment expressed for each particu-
lar aspect (e.g., battery, screen) of an en-
tity (e.g., smartphone). Different words
or phrases, however, may be used to re-
fer to the same aspect, and similar as-
pects may need to be aggregated at coarser
or finer granularities to fit the available
space or satisfy user preferences. We in-
troduce the problem of aspect aggrega-
tion at multiple granularities. We decom-
pose it in two processing phases, to al-
low previous work on term similarity and
hierarchical clustering to be reused. We
show that the second phase, where aspects
are clustered, is almost a solved prob-
lem, whereas further research is needed
in the first phase, where semantic simi-
larity measures are employed. We also
introduce a novel sense pruning mecha-
nism for WordNet-based similarity mea-
sures, which improves their performance
in the first phase. Finally, we provide pub-
licly available benchmark datasets.
1 Introduction
Given a set of texts discussing a particular en-
tity (e.g., reviews of a laptop), aspect-based senti-
ment analysis (ABSA) attempts to identify the most
prominent (e.g., frequently discussed) aspects of
the entity (e.g., battery, screen) and the average
sentiment (e.g., 1 to 5 stars) for each aspect or
group of aspects, as in Fig. 1. Most ABSA systems
perform all or some of the following (Liu, 2012):
subjectivity detection to retain only sentences (or
other spans) expressing subjective opinions; as-
pect extraction to extract (and possibly rank) terms
corresponding to aspects (e.g., ?battery?); aspect
aggregation to group aspect terms that are near-
synonyms (e.g., ?price?, ?cost?) or to obtain aspects
Figure 1: Aspect groups and scores of an entity.
at a coarser granularity (e.g., ?chicken?,?steak?,
and ?fish? may be replaced by ?food? in restaurant
reviews); and aspect sentiment score estimation to
estimate the average sentiment for each aspect or
group of aspects. In this paper, we focus on aspect
aggregation, the least studied stage of the four.
Aspect aggregation is needed to avoid reporting
separate sentiment scores for aspect terms that are
very similar. In Fig. 1, for example, showing sep-
arate lines for ?money?, ?price?, and ?cost? would
be confusing. The extent to which aspect terms
should be aggregated, however, also depends on
the available space and user preferences. On de-
vices with smaller screens, it may be desirable to
aggregate aspect terms that are similar, though not
necessarily near-synonyms (e.g., ?design?, ?color?,
?feeling?) to show fewer lines (Fig. 1), but finer as-
pects may be preferable on larger screens. Users
may also wish to adjust the granularity of aspects,
e.g., by stretching or narrowing the height of Fig. 1
on a smartphone to view more or fewer lines.
Hence, aspect aggregation should be able to pro-
duce groups of aspect terms for multiple granular-
ities. We assume that the aggregated aspects are
displayed as lists of terms, as in Fig. 1. We make
no effort to order (e.g., by frequency) the terms in
each list, nor do we attempt to produce a single
(more general) term to describe each aggregated
aspect, leaving such tasks for future work.
ABSA systems usually group synonymous (or
near-synonymous) aspect terms (Liu, 2012). Ag-
78
gregating only synonyms (or near-synonyms),
however, does not allow users to select the desir-
able aspect granularity, and ignores the hierarchi-
cal relations between aspect terms. For example,
?pizza? and ?steak? are kinds of ?food? and, hence,
the three terms can be aggregated to show fewer,
coarser aspects, even though they are not syn-
onyms. Carenini et al. (2005) used a predefined
domain-specific taxonomy to hierarchically aggre-
gate aspect terms, but taxonomies of this kind
are often not available. By contrast, we use only
general-purpose taxonomies (e.g., WordNet), term
similarity measures based on general-purpose tax-
onomies or corpora, and hierarchical clustering.
We define multi-granular aspect aggregation to
be the task of partitioning a given set of aspect
terms (generated by a previous aspect extraction
stage) into k non-overlapping clusters, for multi-
ple values of k. A further constraint is that the
clusters have to be consistent for different k val-
ues, meaning that if two aspect terms t
1
, t
2
are
placed in the same cluster for k = k
1
, then t
1
and t
2
must also be grouped together (in the same
cluster) for every k = k
2
with k
2
< k
1
, i.e., for
every coarser grouping. For example, if ?waiter?
and ?service? are grouped together for k = 5, they
must also be grouped together for k = 4, 3, 2
and (trivially) k = 1, to allow the user to feel
that selecting a smaller number of aspect groups
(narrowing the height of Fig. 1) has the effect of
zooming out (without aspect terms jumping un-
expectedly to other aspect groups), and similarly
for zooming in.
1
This requirement is satisfied by
using agglomerative hierarchical clustering algo-
rithms (Manning and Sch?utze, 1999; Hastie et al.,
2001), which in our case produce term hierarchies
like the ones of Fig. 2. By using slices (nodes at a
particular depth) of the hierarchies that are closer
to the root or the leaves, we obtain fewer or more
clusters. The vertical dotted lines of Fig. 2 illus-
trate two slices for k = 4. By contrast, flat clus-
tering algorithms (e.g., k-means) do not satisfy the
consistency constraint for different k values.
Agglomerative clustering algorithms require a
measure of the distance between individuals, in
our case a measure of how similar two aspect
terms are, and a linkage criterion to specify which
clusters should be merged to form larger (coarser)
clusters. To experiment with different term sim-
1
We also require the clusters to be non-overlapping to
make this zooming in and out metaphor clearer to the user.
Figure 2: Example aspect hierarchies produced by
agglomerative hierarchical clustering.
food fish sushi dishes wine
food 5 4 4 4 2
fish 4 5 4 2 1
sushi 4 4 5 3 1
dishes 4 2 3 5 2
wine 2 1 1 2 5
Table 1: An aspect term similarity matrix.
ilarity measures and linkage criteria, we decom-
pose multi-granular aspect aggregation in two pro-
cessing phases. Phase A fills in a symmetric ma-
trix, like the one of Table 1, with scores show-
ing the similarity of each pair of input aspect
terms; the matrix in effect defines the distance
measure to be used by agglomerative clustering.
In Phase B, the aspect terms are grouped into k
non-overlapping clusters, for varying values of k,
given the matrix of Phase A and a linkage crite-
rion; a hierarchy like the ones of Fig. 2 is first
formed via agglomerative clustering, and fewer or
more clusters (for different values of k) are then
obtained by using different slices of the hierarchy,
as already discussed. Our two-phase decomposi-
tion can also accommodate non-hierarchical clus-
tering algorithms, provided that the consistency
constraint is satisfied, but we consider only ag-
glomerative hierarchical clustering in this paper.
The decomposition in two phases has three
main advantages. Firstly, it allows reusing previ-
ous work on term similarity measures (Zhang et
al., 2013), which can be used to fill in the ma-
trix of Phase A. Secondly, the decomposition al-
lows different linkage criteria to be experimen-
tally compared (in Phase B) using the same sim-
ilarity matrix (of Phase A), i.e., the same distance
79
measure. Thirdly, the decomposition leads to high
inter-annotator agreement, as we show experimen-
tally. By contrast, in preliminary experiments we
found that asking humans to directly evaluate as-
pect hierarchies produced by hierarchical cluster-
ing, or to manually create gold aspect hierarchies
led to poor inter-annotator agreement.
We show that existing term similarity measures
perform reasonably well in Phase A, especially
when combined, but there is a large scope for im-
provement. We also propose a novel sense pruning
method for WordNet-based similarity measures,
which leads to significant improvements in Phase
A. In Phase B, we experiment with agglomera-
tive clustering using four different linkage criteria,
concluding that they all perform equally well and
that Phase B is almost a solved problem when the
gold similarity matrix of Phase A is used; how-
ever, further improvements are needed in the sim-
ilarity measures of Phase A to produce a suffi-
ciently good similarity matrix. We also make pub-
licly available the datasets of our experiments.
Our main contributions are: (i) to the best
of our knowledge, we are the first to consider
multi-granular aspect aggregation (not just merg-
ing near-synonyms) in ABSA without manually
crafted domain-specific ontologies; (ii) we pro-
pose a two-phase decomposition that allows previ-
ous work on term similarity and hierarchical clus-
tering to be reused and evaluated with high inter-
annotator agreement; (iii) we introduce a novel
sense pruning mechanism that improves WordNet-
based similarity measures; (iv) we provide the first
public datasets for multi-granular aspect aggrega-
tion; (v) we show that the second phase of our de-
composition is almost a solved problem, and that
research should focus on the first phase. Although
we experiment with customer reviews of products
and services, ABSA and the work of this paper in
particular are, at least in principle, also applicable
to texts expressing opinions about other kinds of
entities (e.g., politicians, organizations).
Section 2 below discusses related work. Sec-
tions 3 and 4 present our work for Phase A and B,
respectively. Section 5 concludes.
2 Related work
Most existing approaches to aspect aggregation
aim to produce a single, flat partitioning of as-
pect terms into aspect groups, rather than aspect
groups at multiple granularities. The most com-
mon approaches (Liu, 2012) are to aggregate only
synonyms or near-synonyms, using WordNet (Liu
et al., 2005), statistics from corpora (Chen et al.,
2006; Bollegala et al., 2007a; Lin and Wu, 2009),
or semi-supervised learning (Zhai et al., 2010;
Zhai et al., 2011), or to cluster the aspect terms
using (latent) topic models (Titov and McDonald,
2008a; Guo et al., 2009; Brody and Elhadad, 2010;
Jo and Oh, 2011). Topic models do not perform
better than other methods (Zhai et al., 2010), and
their clusters may overlap.
2
The topic model of
Titov et al. (2008b) uses two granularity levels;
we consider many more (3?10 levels).
Carenini et al. (2005) used a predefined domain-
specific taxonomy and similarity measures to ag-
gregate related terms. Yu et al. (2011) used a tai-
lored version of an existing taxonomy. By con-
trast, we assume no domain-specific taxonomy.
Kobayashi et al. (2007) proposed methods to ex-
tract aspect terms and relations between them, in-
cluding hierarchical relations. They extract, how-
ever, relations by looking for clues in texts (e.g.,
particular phrases). By contrast, we employ simi-
larity measures and hierarchical clustering, which
allows us to group similar aspect terms even when
they do not cooccur in texts. Also, in contrast
to Kobayashi et al. (2007), we respect the consis-
tency constraint discussed in Section 1.
A similar task is taxonomy induction. Cimi-
ano and Staab (2005) automatically construct tax-
onomies from texts via agglomerative clustering,
much as in our Phase B, but not in the context of
ABSA, and without trying to learn a similarity ma-
trix first. They also label the hierarchy?s concepts,
a task we do not consider. Klapaftis and Manand-
har (2010) show how word sense induction can be
combined with agglomerative clustering to obtain
more accurate taxonomies, again not in the con-
text of ABSA. Our sense pruning method was in-
fluenced by their work, but is much simpler than
their word sense induction. Fountain and Lapata
(2012) study unsupervised methods to induce con-
cept taxonomies, without considering ABSA.
3 Phase A
We now discuss our work for Phase A. Recall that
in this phase the input is a set of aspect terms and
2
Topic models are typically also used to perform aspect
extraction, apart from aspect aggregation, but simple heuris-
tics (e.g., most frequent nouns) often outperform them in as-
pect extraction (Liu, 2012; Moghaddam and Ester, 2012).
80
the goal is to fill in a matrix (Table 1) with scores
showing the similarity of each pair of aspect terms.
3.1 Datasets used in Phase A
We used two benchmark datasets that we had pre-
viously constructed to evaluate ABSA methods for
subjectivity detection, aspect extraction, and as-
pect score estimation, but not aspect aggregation.
We extended them to support aspect aggregation,
and we make them publicly available.
3
The two original datasets contain sentences
from customer reviews of restaurants and laptops,
respectively. The reviews are manually split into
sentences, and each sentence is manually anno-
tated as ?subjective? (expressing opinion) or ?ob-
jective? (not expressing opinion). The restaurants
dataset contains 3,710 English sentences from the
restaurant reviews of Ganu et al. (2009). The lap-
tops dataset contains 3,085 English sentences from
394 customer reviews, collected from sites that
host customer reviews. In the experiments of this
paper, we use only the 3,057 (out of 3,710) sub-
jective restaurant sentences and the 2,631 (out of
3,085) subjective laptop sentences.
For each subjective sentence, our datasets show
the words that human annotators marked as aspect
terms. For example, in ?The dessert was divine!?
the aspect term is ?dessert?, and in ?Really bad
waiter.? it is ?waiter?. Among the 3,057 subjective
restaurant sentences, 1,129 contain exactly one as-
pect term, 829 more than one, and 1,099 no aspect
term; a subjective sentence may express an opin-
ion about the restaurant (or laptop) being reviewed
without mentioning a specific aspect (e.g., ?Really
nice restaurant!?), which is why no aspect terms
are present in some subjective sentences. There
are 558 distinct multi-word aspect terms and 431
distinct single-word aspect terms in the subjective
restaurant sentences. Among the 2,631 subjective
sentences of the laptop reviews, 823 contain ex-
actly one aspect term, 389 more than one, and
1,419 no aspect term. There are 273 distinct multi-
word aspect terms and 330 distinct single-word as-
pect terms in the subjective laptop sentences.
From each dataset, we selected the 20 (distinct)
aspect terms that the human annotators had anno-
tated most frequently, taking annotation frequency
to be an indicator of importance; there are only
two multi-word aspect terms (?hard drive?, ?bat-
3
The datasets are available at http://nlp.cs.
aueb.gr/software.html.
tery life?) among the 20 most frequent ones in the
laptops dataset, and none among the 20 most fre-
quent aspect terms of the restaurants dataset. We
then formed all the 190 possible pairs of the 20
terms and constructed an empty similarity matrix
(Fig. 1), one for each dataset, which was given
to three human judges to fill in (1: strong dis-
similarity, 5: strong similarity).
4
For each aspect
term, all the subjective sentences mentioning the
term were also provided, to help the judges un-
derstand how the terms are used in the particu-
lar domains (e.g., ?window? and ?Windows? have
domain-specific meanings in laptop reviews).
The Pearson correlation coefficient indicated
high inter-annotator agreement (0.81 for restau-
rants, 0.74 for laptops). We also measured the ab-
solute inter-annotator agreement a(l
1
, l
2
), defined
below, where l
1
, l
2
are lists containing the scores
(similarity matrix values) of two judges, N is the
length of each list, and v
max
, v
min
are the largest
and smallest possible scores (5 and 1).
a(l
1
, l
2
) =
1
N
N
?
i=1
[
1?
|l
1
(i)? l
2
(i)|
v
max
? v
min
]
The absolute interannotator agreement was also
high (0.90 for restaurants, 0.91 for laptops).
5
With
both measures, we compute the agreement of each
judge with the averaged (for each matrix cell)
scores of the other two judges, and we report the
mean of the three agreement estimates. Finally, we
created the gold similarity matrix of each dataset
by placing in each cell the average scores that the
three judges had provided for that cell.
In preliminary experiments, we gave aspect
terms to human judges, asking them to group any
terms they considered near-synonyms. We then
asked the judges to group the aspect terms into
fewer, coarser groups by grouping terms that could
be viewed as direct hyponyms of the same broader
term (e.g., ?pizza? and ?steak? are both kinds of
?food?), or that stood in a hyponym-hypernym re-
lation (e.g., ?pizza? and ?food?). We used the
Dice coefficient to measure inter-annotator agree-
ment, and we obtained reasonably good agreement
for near-synonyms (0.77 for restaurants, 0.81 for
laptops), but poor agreement for the coarser as-
4
The matrix is symmetric; hence, the judges had to fill in
only half of it. The guidelines and an annotation tool that
were given to the judges are available upon request.
5
The Pearson correlation ranges from ?1 to 1, whereas
the absolute inter-annotator agreement ranges from 0 to 1.
81
pects (0.25 and 0.11).
6
In other preliminary ex-
periments, we asked human judges to rank alter-
native aspect hierarchies that had been produced
by applying agglomerative clustering with differ-
ent linkage criteria to 20 aspect terms, but we ob-
tained very poor inter-annotator agreement (Pear-
son score ?0.83 for restaurants and 0 for laptops).
3.2 Phase A methods
We employed five term similarity measures. The
first two are WordNet-based (Budanitsky and
Hirst, 2006). The next two combine WordNet with
statistics from corpora. The fifth one is a corpus-
based distributional similarity measure.
The first measure is Wu and Palmer?s (1994). It
is actually a sense similarity measure (a term may
have multiple senses). Given two senses s
ij
, s
i
?
j
?
of terms t
i
, t
i
?
, the measure is defined as follows:
WP(s
ij
, s
i
?
j
?
) = 2 ?
depth(lcs(s
ij
, s
i
?
j
?
))
depth(s
ij
) + depth(s
ij
)
,
where lcs(s
ij
, s
i
?
j
?
) is the least common sub-
sumer, i.e., the most specific common ancestor of
the two senses in WordNet, and depth(s) is the
depth of sense s in WordNet?s hierarchy.
Most terms have multiple senses, however,
and word sense disambiguation methods (Navigli,
2009) are not yet robust enough. Hence, when
given two aspect terms t
i
, t
i
?
, rather than particular
senses of the terms, a simplistic greedy approach
is to compute the similarities of all the possible
pairs of senses s
ij
, s
i
?
j
?
of t
i
, t
i
?
, and take the sim-
ilarity of t
i
, t
i
?
to be the maximum similarity of
the sense pairs (Bollegala et al., 2007b; Zesch and
Gurevych, 2010). We use this greedy approach
with all the WordNet-based measures, but we also
propose a sense pruning mechanism below, which
improves their performance. In all the WordNet-
based measures, if a term is not in WordNet, we
take its similarity to any other term to be zero.
7
The second measure, PATH (s
ij
, s
i
?
j
?
), is sim-
ply the inverse of the length (plus one) of the short-
est path connecting the senses s
ij
, s
i
?
j
?
in WordNet
(Zhang et al., 2013). Again, the greedy approach
can be used with terms having multiple senses.
6
The Dice coefficient ranges from 0 to 1. There was a very
large number of possible responses the judges could provide
and, hence, it would be inappropriate to use Cohen?s K.
7
This never happened in the restaurants dataset. In the
laptops dataset, it only happened for ?hard drive? and ?bat-
tery life?. We use the NLTK implementation of the first four
measures (see http://nltk.org/) and our own imple-
mentation of the distributional similarity measure.
The third measure is Lin?s (1998), defined as:
LIN (s
ij
, s
i
?
j
?
) =
2 ? ic(lcs(s
ij
, s
i
?
j
?
))
ic(s
ij
) + ic(s
i
?
j
?
)
,
where s
ij
, s
i
?
j
?
are senses of terms t
i
, t
i
?
,
lcs(s
ij
, s
i
?
j
?
) is the least common subsumer of
s
ij
, s
i
?
j
?
in WordNet, and ic(s) = ? logP(s) is
the information content of sense s (Pedersen et al.,
2004), estimated from a corpus. When the cor-
pus is not sense-tagged, we follow the common
approach of treating each occurrence of a word as
an occurrence of all of its senses, when estimat-
ing ic(s).
8
We experimented with two variants of
Lin?s measure, one where the ic(s) scores were
estimated from the Brown corpus (Marcus et al.,
1993), and one where they were estimated from
the (restaurant or laptop) reviews of our datasets.
The fourth measure is Jiang and Conrath?s
(1997), defined below. Again, we experimented
with two variants of ic(s), as above.
JCN (s
ij
, s
i
?
j
?
) =
1
ic(s
ij
) + ic(s
i
?
j
?
)? 2 ? lcs(s
ij
, s
i
?
j
?
)
For all the above WordNet-based measures, we
experimented with a sense pruning mechanism,
which discards some of the senses of the aspect
terms, before applying the greedy approach. For
each aspect term t
i
, we consider all of its Word-
Net senses s
ij
. For each s
ij
and each other aspect
term t
i
?
, we compute (using PATH ) the similar-
ity between s
ij
and each sense s
i
?
j
?
of t
i
?
, and we
consider the relevance of s
ij
to t
i
?
to be:
9
rel(s
ij
, t
i
?
) = max
s
i
?
j
?
? senses(t
i
?
)
PATH (s
ij
, s
i
?
j
?
)
The relevance of s
ij
to all of the N other aspect
terms t
i
?
is taken to be:
rel(s
ij
) =
1
N
?
?
i
?
6=i
rel(s
ij
, t
i
?
)
For each aspect term t
i
, we retain only its senses
s
ij
with the top rel(s
ij
) scores, which tends to
8
http://www.d.umn.edu/
?
tpederse/Data/
README-WN-IC-30.txt. We use the default counting.
9
We also experimented with other similarity measures
when computing rel(s
ij
, t
i
?
), instead of PATH , but there
was no significant difference. We use NLTK to tokenize, re-
move punctuation, and stop-words.
82
without SP with SP
Method Rest. Lapt. Rest. Lapt.
WP 0.475 0.216 0.502 0.265
PATH 0.524 0.301 0.529 0.332
LIN@domain 0.390 0.256 0.456 0.343
LIN@Brown 0.434 0.329 0.471 0.391
JCN@domain 0.467 0.348 0.509 0.448
JCN@Brown 0.403 0.469 0.419 0.539
DS 0.283 0.517 (0.283) (0.517)
AVG 0.499 0.352 0.537 0.426
WN 0.490 0.328 0.530 0.395
WNDS 0.523 0.453 0.545 0.546
Table 2: Phase A results (Pearson correlation to
gold similarities) with and without sense pruning.
prune senses that are very irrelevant to the par-
ticular domain (e.g., laptops). This sense prun-
ing mechanism is novel, and we show experimen-
tally that it improves the performance of all the
WordNet-based similarity measures we examined.
We also implemented a distributional simi-
larity measure (Harris, 1968; Pad?o and Lap-
ata, 2007; Cimiano et al., 2009; Zhang et al.,
2013). Following Lin and Wu (2009), for
each aspect term t, we create a vector ~v(t) =
?PMI (t, w
1
), . . . ,PMI (t, w
n
)?. The vector com-
ponents are the Pointwise Mutual Information
scores of t and each word w
i
of a corpus:
PMI (t, w
i
) = ? log
P (t, w
i
)
P (t) ? P (w
i
)
We treat P (t, w
i
) as the probability of t, w
i
cooc-
curring in the same sentence, and we use the (lap-
top or restaurant) reviews of our datasets as the
corpus to estimate the probabilities. The distribu-
tional similarity DS (t, t
?
) of two aspect terms t, t
?
is the cosine similarity of ~v(t), ~v(t
?
).
10
Finally, we tried combinations of the similarity
measures: AVG is the average of all five; WN is
the average of the first four, which employ Word-
Net; and WNDS is the average of WN and DS ;
all the scores range in [0, 1]. We also tried regres-
sion (e.g., SVR), but there was no improvement.
3.3 Phase A experimental results
Each similarity measure was evaluated by comput-
ing its Pearson correlation with the scores of the
gold similarity matrix. Table 2 shows the results.
Our sense pruning consistently improves all
four WordNet-based measures. It does not apply to
10
We also experimented with Euclidean distance, a nor-
malized PMI (Bouma, 2009), and the Brown corpus, but
there was no improvement.
DS , which is why the DS results are identical with
and without pruning. A paired t test indicates that
the other differences (with and without pruning) of
Table 2 are statistically significant (p < 0.05). We
used the senses with the top five rel(s
ij
) scores for
each aspect term t
i
during sense pruning. We also
experimented with keeping fewer senses, but the
results were inferior or there was no improvement.
Lin?s measure performed better when infor-
mation content was estimated on the (much
larger, but domain-independent) Brown corpus
(LIN@Brown), as opposed to using the (domain-
specific) reviews of our datasets (LIN@domain),
but we observed no similar consistent pattern for
JCN . Given its simplicity, PATH performed re-
markably well in the restaurants dataset; it was
the best measure (including combinations) without
sense pruning, and the best uncombined measure
with sense pruning. It performed worse, however,
compared to several other measures in the laptops
dataset. Similar comments apply to WP , which is
among the top-performing uncombined measures
in restaurants, both with and without sense prun-
ing, but the worst overall measure in laptops. DS
is the best overall measure in laptops when com-
pared to measures without sense pruning, and the
third best overall when compared to measures that
use sense pruning, but the worst overall in restau-
rants both with and without pruning. LIN and
JCN , which use both WordNet and corpus statis-
tics, have a more balanced performance across the
two datasets, but they are not top-performers in
any of the two. Combinations of similarity mea-
sures seem more stable across domains, as the re-
sults of AVG , WN , and WNDS indicate, though
experiments with more domains are needed to in-
vestigate this issue. WNDS is the best overall
method with sense pruning, and among the best
three methods without pruning in both datasets.
To get a better view of the performance of
WNDS with sense pruning, i.e., the best overall
measure of Table 2, we compared it to two state of
the art semantic similarity systems. First, we ap-
plied the system of Han et al. (2013), one of the
best systems of the recent *Sem 2013 semantic
text similarity competition, to our Phase A data.
The performance (Pearson correlation with gold
similarities) of the same system on the widely used
WordSim353 word similarity dataset (Agirre et al.,
2009) is 0.73, much higher than the same system?s
performance on our Phase A data (see Table 3),
83
Method Restaurants Laptops
Han et al. (2013) 0.450 0.471
Word2Vec 0.434 0.485
WNDS with SP 0.545 0.546
Judge 1 0.913 0.875
Judge 2 0.914 0.894
Judge 3 0.888 0.924
Table 3: Phase A results (Pearson correlation to
gold similarities) of WNDS with SP against se-
mantic similarity systems and human judges.
which suggests that our data are more difficult.
11
We also employed the recent Word2Vec sys-
tem, which computes continuous vector space rep-
resentations of words from large corpora and has
been reported to improve results in word similarity
tasks (Mikolov et al., 2013). We used the English
Wikipedia to compute word vectors with 200 fea-
tures.
12
The similarity between two aspect terms
was taken to be the cosine similarity of their vec-
tors. This system performed better than Han et
al.?s with laptops, but not with restaurants.
Table 3 shows that WNDS (with sense prun-
ing) performed clearly better than the system of
Han et al. and Word2Vec. Table 3 also shows
the Pearson correlation of each judge?s scores to
the gold similarity scores, as an indication of the
best achievable results. Although WNDS (with
sense pruning) performs reasonably well in both
domains,
13
there is large scope for improvement.
4 Phase B
In Phase B, the aspect terms are to be grouped
into k non-overlapping clusters, for varying val-
ues of k, given a Phase A similarity matrix. We
experimented with both the gold similarity matrix
of Phase A and similarity matrices produced by
WNDS (with SP), the best Phase A method.
4.1 Phase B methods
We experimented with agglomerative clustering
and four linkage criteria: single, complete, av-
erage, and Ward (Manning and Sch?utze, 1999;
Hastie et al., 2001). Let d(t
1
, t
2
) be the distance of
11
The system of Han et al. (2013) is available from
http://semanticwebarchive.cs.umbc.edu/
SimService/; we use the STS similarity.
12
Word2Vec is available from https://code.
google.com/p/word2vec/. We used the continuous
bag of words model with default parameters, the first billion
characters of the English Wikipedia, and the preprocessing of
http://mattmahoney.net/dc/textdata.html.
13
Recall that the Pearson correlation ranges from ?1 to 1.
two individual instances t
1
, t
2
; in our case, the in-
stances are aspect terms and d(t
1
, t
2
) is the inverse
of the similarity of t
1
, t
2
, defined by the Phase A
similarity matrix (gold or produced by WNDS ).
Different linkage criteria define differently the dis-
tance of two clusters D(C
1
, C
2
), which affects
the choice of clusters that are merged to produce
coarser (higher-level) clusters:
D
single
(C
1
, C
2
) = min
t
1
?C
1
,t
2
?C
2
d(t
1
, t
2
)
D
compl
(C
1
, C
2
) = max
t
1
?C
1
,t
2
?C
2
d(t
1
, t
2
)
D
avg
(C
1
, C
2
) =
1
|C
1
||C
2
|
?
t
1
?C
1
?
t
2
?C
2
d(t
1
, t
2
)
Complete linkage tends to produce more compact
clusters, compared to single linkage, with average
linkage being in between. Ward minimizes the to-
tal in-cluster variance; consult Milligan (1980) for
further details.
14
4.2 Phase B experimental results
To evaluate the k clusters produced at each aspect
granularity by the different linkage criteria, we
used the Silhouette Index (SI ) (Rousseeuw, 1987),
a cluster evaluation measure that considers both
inter- and intra-cluster coherence.
15
Given a set of
clusters {C
1
, . . . , C
k
}, each SI (C
i
) is defined as:
SI (C
i
) =
1
|C
i
|
?
|C
i
|
?
j=1
b
j
? a
j
max(b
j
, a
j
)
,
where a
j
is the mean distance from the j-th in-
stance of C
i
to the other instances in C
i
, and b
j
is
the mean distance from the j-th instance of C
i
to
the instances in the cluster nearest to C
i
. Then:
SI ({C
1
, . . . , C
k
}) =
1
k
?
k
?
i=1
SI (C
i
)
We always use the correct (gold) distances of the
instances (terms) when computing the SI scores.
As shown in Fig. 3, no linkage criterion clearly
outperforms the others, when the gold matrix of
Phase A is used; all four criteria perform reason-
ably well. Note that the SI ranges from ?1 to
14
We used the SCIPY implementations of agglomera-
tive clustering with the four criteria (see http://www.
scipy.org), relying on maxclust to obtain the slice of the
resulting hierarchy that leads to k (or approx. k) clusters.
15
We used the SI implementation of Pedregosa et
al. (2011); see http://scikit-learn.org/. We also
experimented with the Dunn Index (Dunn, 1974) and the
Davies-Bouldin Index (1979), but we obtained similar results.
84
(a) restaurants (b) laptops
Figure 3: Silhouette Index (SI) results for Phase
B, using the gold similarity matrix of Phase A.
(a) restaurants (b) laptops
Figure 4: SI results for Phase B, using the WNDS
(with SP) similarity matrix of Phase A.
1, with higher values indicating better clustering.
Figure 4 shows that when the similarity matrix of
WNDS (with SP) is used, the SI scores deterio-
rate significantly; again, there is no clear winner
among the linkage criteria, but average and Ward
seem to be overall better than the others.
(a) Restaurants (b) Laptops
Figure 5: Human evaluation of aspect groups.
In a final experiment, we showed clusterings
of varying granularities (k values) to four human
judges (graduate CS students). The clusterings
were produced by two systems: one that used the
gold similarity matrix of Phase A and agglomer-
ative clustering with average linkage in Phase B,
and one that used the similarity matrix of WNDS
(with SP) and again agglomerative clustering with
average linkage. We showed all the clusterings
to all the judges. Each judge was asked to eval-
uate each clustering on a 1?5 scale. We measured
the absolute inter-annotator agreement, as in Sec-
tion 3.1, and found high agreement in all cases
(0.93 and 0.83 for the two systems, respectively,
in restaurants; 0.85 for both in laptops).
16
Figure 5 shows the average human scores of
the two systems for different granularities. The
judges considered the aspect groups always per-
fect or near-perfect when the gold similarity ma-
trix of Phase A was used, but they found the as-
pect groups to be of rather poor quality when
the similarity matrix of the best Phase A mea-
sure was used. These results, along with those of
Fig. 3?4, show that more effort needs to be devoted
to improving the similarity measures of Phase A,
whereas Phase B is in effect an almost solved
problem, if a good similarity matrix is available.
5 Conclusions
We considered a new, more demanding form of
aspect aggregation in ABSA, which aims to aggre-
gate aspects at multiple granularities, as opposed
to simply merging near-synonyms, and without as-
suming that manually crafted domain-specific on-
tologies are available. We decomposed the prob-
lem in two processing phases, which allow pre-
vious work on term similarity and hierarchical
clustering to be reused and evaluated appropri-
ately with high inter-annotator agreement. We
showed that the second phase, where we used ag-
glomerative clustering, is an almost solved prob-
lem, whereas further research is needed in the first
phrase, where term similarity measures are em-
ployed. We also introduced a sense pruning mech-
anism that significantly improves WordNet-based
similarity measures, leading to a measure that out-
performs state of the art similarity methods in the
first phase of our decomposition. We also made
publicly available the datasets of our experiments.
Acknowledgments
We thank G. Batistatos, A. Zosakis, and G. Lam-
pouras for their annotations in Phase A. We thank
A. Kosmopoulos, G. Lampouras, P. Malakasiotis,
and I. Lourentzou for their annotations in Phase B.
16
The Pearson correlation cannot be computed, as several
judges gave the same rating to the first system, for all k.
85
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas?ca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of the Annual
Conference of NAACL, pages 19?27, Boulder, CO,
USA.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007a.
An integrated approach to measuring semantic sim-
ilarity between words using information available
on the web. In Proceedings of HLT-NAACL, pages
340?347, Rochester, NY, USA.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007b.
Measuring semantic similarity between words using
web search engines. In Proceedings of the 16th In-
ternational Conference of WWW, volume 766, pages
757?766, Banff, Alberta, Canada.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. Proceedings of
the Biennial Conference of GSCL, pages 31?40.
S. Brody and N. Elhadad. 2010. An unsupervised
aspect-sentiment model for online reviews. In Pro-
ceedings of the Annual Conference of NAACL, pages
804?812, Los Angeles, CA, USA.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
G. Carenini, R. T. Ng, and E. Zwart. 2005. Extract-
ing knowledge from evaluative text. In Proceedings
of the 3rd International Conference on Knowledge
Capture, pages 11?18, Banff, Alberta, Canada.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking.
In Proceedings of the 21st International Conference
of COLING and the 44th Annual Meeting of ACL,
pages 1009?1016, Sydney, Australia.
P. Cimiano and S. Staab. 2005. Learning concept hier-
archies from text with a guided hierarchical cluster-
ing algorithm. In Proceedings of ICML ? Workshop
on Learning and Extending Lexical Ontologies with
Machine Learning Methods, Bonn, Germany.
P. Cimiano, A. M?adche, S. Staab, and J. V?olker. 2009.
Ontology learning. In Handbook on Ontologies,
pages 245?267. Springer.
D. L. Davies and D. W. Bouldin. 1979. A cluster sepa-
ration measure. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 1(2):224?227.
J. C. Dunn. 1974. Well-separated clusters and optimal
fuzzy partitions. Journal of Cybernetics, 4(1):95?
104.
T. Fountain and M. Lapata. 2012. Taxonomy induction
using hierarchical random graphs. In Proceedings of
NAACL:HLT, pages 466?476, Montreal, Canada.
G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of the 12th Interna-
tional Workshop on the Web and Databases, Prov-
idence, RI, USA.
H. Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su. 2009.
Product feature categorization with multilevel latent
semantic association. In Proceedings of the 18th
CIKM, pages 1087?1096.
L. Han, A. Kashyap, T. Finin, J. Mayfield, and
J. Weese. 2013. Umbc ebiquity-core: Semantic tex-
tual similarity systems. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, pages 44?52, Atlanta, GA, USA.
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING, pages 19?33, Taiwan,
China.
Y. Jo and A. H. Oh. 2011. Aspect and sentiment unifi-
cation model for online review analysis. In Proceed-
ings of the 4th International Conference of WSDM,
pages 815?824, Hong Kong, China.
I. P. Klapaftis and S. Manandhar. 2010. Taxonomy
learning using word sense induction. In Proceedings
of NAACL, pages 82?90, Los Angeles, CA, USA.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proceedings of the Joint Confer-
ence on EMNLP-CoNLL, pages 1065?1074, Prague,
Czech Republic.
D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of ACL, pages
1030?1038, Suntec, Singapore. ACL.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th ICML, pages
296?304, Madison, WI, USA.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer:
analyzing and comparing opinions on the web. In
Proceedings of the 14th International Conference of
WWW, pages 342?351, Chiba, Japan.
B. Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies. Morgan & Claypool.
C. D. Manning and H. Sch?utze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, MA, USA.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
86
T. Mikolov, C. Kai, G. Corrado, and J. Dean. 2013.
Efficient estimation of word representations in vec-
tor space. CoRR, abs/1301.3781.
G.W. Milligan. 1980. An examination of the effect of
six types of error perturbation on fifteen clustering
algorithms. Psychometrika, 45(3):325?342.
S. Moghaddam and M. Ester. 2012. On the design of
lda models for aspect-based opinion mining. In Pro-
ceedings of the 21st CIKM, pages 803?812, Maui,
HI, USA.
R. Navigli. 2009. Word sense disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):10:1?10:69.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity: measuring the relatedness of
concepts. In Proceedings of NAACL:HTL ? Demon-
strations, pages 38?41, Boston, MA, USA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in python. Journal of Machine Learning Re-
search, 12:2825?2830.
P. Rousseeuw. 1987. Silhouettes: a graphical aid to
the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathemat-
ics, 20(1):53?65.
I. Titov and R. T. McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of ACL-
HLT, pages 308?316, Columbus, OH, USA.
I. Titov and R. T. McDonald. 2008b. Modeling online
reviews with multi-grain topic models. In Proceed-
ings of the 17th International Conference of WWW,
pages 111?120, Beijing, China.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexi-
cal selection. In Proceedings of the 32nd ACL, pages
133?138, Las Cruces, NM, USA.
J. Yu, Z. Zha, M. Wang, K. Wang, and T. Chua. 2011.
Domain-assisted product aspect hierarchy genera-
tion: towards hierarchical organization of unstruc-
tured consumer reviews. In Proceedings of EMNLP,
pages 140?150, Edinburgh, UK.
T. Zesch and I. Gurevych. 2010. Wisdom of crowds
versus wisdom of linguists - measuring the semantic
relatedness of words. Natural Language Engineer-
ing, 16(1):25?59.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Group-
ing product features using semi-supervised learning
with soft-constraints. In Proceedings of the 23rd
International Conference of COLING, pages 1272?
1280, Beijing, China.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering
product features for opinion mining. In Proceedings
of the 4th International Conference of WSDM, pages
347?354, Hong Kong, China.
Z. Zhang, A. Gentile, and F. Ciravegna. 2013. Re-
cent advances in methods of lexical semantic relat-
edness - a survey. Natural Language Engineering,
FirstView(1):1?69.
87
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885?893,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An extractive supervised two-stage method for sentence compression
Dimitrios Galanis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Centre ?Athena?, Greece
Abstract
We present a new method that compresses
sentences by removing words. In a first stage,
it generates candidate compressions by re-
moving branches from the source sentence?s
dependency tree using a Maximum Entropy
classifier. In a second stage, it chooses the
best among the candidate compressions using
a Support Vector Machine Regression model.
Experimental results show that our method
achieves state-of-the-art performance without
requiring any manually written rules.
1 Introduction
Sentence compression is the task of producing a
shorter form of a single given sentence, so that the
new form is grammatical and retains the most im-
portant information of the original one (Jing, 2000).
Sentence compression is valuable in many applica-
tions, for example when displaying texts on small
screens (Corston-Oliver, 2001), in subtitle genera-
tion (Vandeghinste and Pan, 2004), and in text sum-
marization (Madnani et al, 2007).
People use various methods to shorten sentences,
including word or phrase removal, using shorter
paraphrases, and common sense knowledge. How-
ever, reasonable machine-generated sentence com-
pressions can often be obtained by only remov-
ing words. We use the term extractive to refer to
methods that compress sentences by only removing
words, as opposed to abstractive methods, where
more elaborate transformations are also allowed.
Most of the existing compression methods are ex-
tractive (Jing, 2000; Knight and Marcu, 2002; Mc-
Donald, 2006; Clarke and Lapata, 2008; Cohn and
Lapata, 2009). Although abstractive methods have
also been proposed (Cohn and Lapata, 2008), and
they may shed more light on how people compress
sentences, they do not always manage to outperform
extractive methods (Nomoto, 2009). Hence, from an
engineering perspective, it is still important to inves-
tigate how extractive methods can be improved.
In this paper, we present a new extractive sentence
compression method that relies on supervised ma-
chine learning.1 In a first stage, the method gener-
ates candidate compressions by removing branches
from the source sentence?s dependency tree using a
Maximum Entropy classifier (Berger et al, 2006). In
a second stage, it chooses the best among the candi-
date compressions using a Support Vector Machine
Regression (SVR) model (Chang and Lin, 2001). We
show experimentally that our method compares fa-
vorably to a state-of-the-art extractive compression
method (Cohn and Lapata, 2007; Cohn and Lapata,
2009), without requiring any manually written rules,
unlike other recent work (Clarke and Lapata, 2008;
Nomoto, 2009). In essence, our method is a two-
tier over-generate and select (or rerank) approach to
sentence compression; similar approaches have been
adopted in natural language generation and parsing
(Paiva and Evans, 2005; Collins and Koo, 2005).
2 Related work
Knight and Marcu (2002) presented a noisy channel
sentence compression method that uses a language
model P (y) and a channel model P (x|y), where x
1An implementation of our method will be freely available
from http://nlp.cs.aueb.gr/software.html
885
is the source sentence and y the compressed one.
P (x|y) is calculated as the product of the proba-
bilities of the parse tree tranformations required to
expand y to x. The best compression of x is the
one that maximizes P (x|y) ? P (y), and it is found
using a noisy channel decoder. In a second, alter-
native method Knight and Marcu (2002) use a tree-
to-tree transformation algorithm that tries to rewrite
directly x to the best y. This second method uses
C4.5 (Quinlan, 1993) to learn when to perform tree
rewriting actions (e.g., dropping subtrees, combin-
ing subtrees) that transform larger trees to smaller
trees. Both methods were trained and tested on
data from the Ziff-Davis corpus (Knight and Marcu,
2002), and they achieved very similar grammatical-
ity and meaning preservation scores, with no statis-
tically significant difference. However, their com-
pression rates (counted in words) were very dif-
ferent: 70.37% for the noisy-channel method and
57.19% for the C4.5-based one.
McDonald (2006) ranks each candidate compres-
sion using a function based on the dot product of a
vector of weights with a vector of features extracted
from the candidate?s n-grams, POS tags, and depen-
dency tree. The weights were learnt from the Ziff-
Davis corpus. The best compression is found us-
ing a Viterbi-like algorithm that looks for the best
sequence of source words that maximizes the scor-
ing function. The method outperformed Knight and
Marcu?s (2002) tree-to-tree method in grammatical-
ity and meaning preservation on data from the Ziff-
Davis corpus, with a similar compression rate.
Clarke and Lapata (2008) presented an unsuper-
vised method that finds the best compression using
Integer Linear Programming (ILP). The ILP obejc-
tive function takes into account a language model
that indicates which n-grams are more likely to be
deleted, and a significance model that shows which
words of the input sentence are important. Man-
ually defined constraints (in effect, rules) that op-
erate on dependency trees indicate which syntactic
constituents can be deleted. This method outper-
formed McDonald?s (2006) in grammaticality and
meaning preservation on test sentences from Edin-
burgh?s ?written? and ?spoken? corpora.2 However,
the compression rates of the two systems were dif-
2See http://homepages.inf.ed.ac.uk/s0460084/data/.
ferent (72.0% vs. 63.7% for McDonald?s method,
both on the written corpus).
We compare our method against Cohn and Lap-
ata?s T3 system (Cohn and Lapata, 2007; Cohn and
Lapata, 2009), a state-of-the-art extractive sentence
compression system that learns parse tree transduc-
tion operators from a parallel extractive corpus of
source-compressed trees. T3 uses a chart-based de-
coding algorithm and a Structured Support Vector
Machine (Tsochantaridis et al, 2005) to learn to
select the best compression among those licensed
by the operators learnt.3 T3 outperformed McDon-
ald?s (2006) system in grammaticality and meaning
preservation on Edinburgh?s ?written? and ?spoken?
corpora, achieving comparable compression rates
(Cohn and Lapata, 2009). Cohn and Lapata (2008)
have also developed an abstractive version of T3,
which was reported to outperform the original, ex-
tractive T3 in meaning preservation; there was no
statistically significant difference in grammaticality.
Finally, Nomoto (2009) presented a two-stage ex-
tractive method. In the first stage, candidate com-
pressions are generated by chopping the source sen-
tence?s dependency tree. Many ungrammatical com-
pressions are avoided using hand-crafted drop-me-
not rules for dependency subtrees. The candidate
compressions are then ranked using a function that
takes into account the inverse document frequen-
cies of the words, and their depths in the source
dependency tree. Nomoto?s extractive method was
reported to outperform Cohn and Lapata?s abstrac-
tive version of T3 on a corpus collected via RSS
feeds. Our method is similar to Nomoto?s, in that
it uses two stages, one that chops the source depen-
dency tree generating candidate compressions, and
one that ranks the candidates. However, we experi-
mented with more elaborate ranking models, and our
method does not employ any manually crafted rules.
3 Our method
As already mentioned, our method first generates
candidate compressions, which are then ranked. The
candidate compressions generator operates by re-
moving branches from the dependency tree of the
3T3 appears to be the only previous sentence compres-
sion method whose implementation is publicly available; see
http://www.dcs.shef.ac.uk/?tcohn/t3/.
886
input sentence (figure 1); this stage is discussed in
section 3.1 below. We experimented with different
ranking functions, discussed in section 3.2, which
use features extracted from the source sentence s
and the candidate compressions c1, . . . , ck.
3.1 Generating candidate compressions
Our method requires a parallel training corpus con-
sisting of sentence-compression pairs ?s, g?. The
compressed sentences g must have been formed by
only deleting words from the corresponding source
sentences s. The ?s, g? training pairs are used to es-
timate the propability that a dependency edge e of a
dependency tree Ts of an input sentence s is retained
or not in the dependency tree Tg of the compressed
sentence g. More specifically, we want to estimate
the probabilities P (Xi|context(ei)) for every edge
ei of Ts, where Xi is a variable that can take one
of the following three values: not del, for not delet-
ing ei; del u for deleting ei along with its head; and
del l for deleting e along with its modifier. The head
(respectively, modifier) of ei is the node ei originates
from (points to) in the dependency tree. context(ei)
is a set of features that represents ei?s local context
in Ts, as well as the local context of the head and
modifier of ei in s.
The propabilities above can be estimated using
the Maximum Entropy (ME) framework (Berger et
al., 2006), a method for learning the distribution
P (X|V ) from training data, where X is discrete-
valued variable and V = ?V1, . . . , Vn? is a real or
discrete-valued vector. Here, V = context(ei) and
X = Xi. We use the following features in V :
? The label of the dependency edge ei, as well as
the POS tag of the head and modifier of ei.
? The entire head-label-modifier triple of ei. This
feature overlaps with the previous two features,
but it is common in ME models to use feature
combinations as additional features, since they
may indicate a category more strongly than the
individual initial features.4
? The POS tag of the father of ei?s head, and the
label of the dependency that links the father to
ei?s head.
4http://nlp.stanford.edu/pubs/maxent-tutorial-slides.pdf.
? The POS tag of each one of the three previous
and the three following words of ei?s head and
modifier in s (12 features).
? The POS tag bi-grams of the previous two and
the following two words of ei?s head and mod-
ifier in s (4 features).
? Binary features that show which of the possible
labels occur (or not) among the labels of the
edges that have the same head as ei in Ts (one
feature for each possible dependency label).
? Two binary features that show if the subtree
rooted at the modifier of ei or ei?s uptree (the
rest of the tree, when ei?s subtree is removed)
contain an important word. A word is consid-
ered important if it appears in the document s
was drawn from significantly more often than
in a background corpus. In summarization,
such words are called signature terms and are
thought to be descriptive of the input; they can
be identified using the log-likelihood ratio ? of
each word (Lin and Hovy, 2000; Gupta et al,
2007).
For each dependency edge ei of a source training
sentence s, we create a training vector V with the
above features. If ei is retained in the dependency
tree of the corresponding compressed sentence g in
the corpus, V is assigned the category not del. If
ei is not retained, it is assigned the category del l
or del u, depending on whether the head (as in the
ccomp of ?said? in Figure 1) or the modifier (as in
the dobj of ?attend?) of ei has also been removed.
When the modifier of an edge is removed, the entire
subtree rooted at the modifier is removed, and simi-
larly for the uptree, when the head is removed. We
do not create training vectors for the edges of the
removed subtree of a modifier or the edges of the
removed uptree of a head.
Given an input sentence s and its dependency tree
Ts, the candidate compressions generator produces
the candidate compressed sentences c1, . . . , cn by
deleting branches of Ts and putting the remaining
words of the dependency tree in the same order as in
s. The candidates c1, . . . , cn correspond to possible
assignments of values to theXi variables (recall that
Xi = not del|del l|del u) of the edges ei of Ts.
887
source: gold:
said
ccomp

nsubj
K
%%K
K
attend
nsubj

aux
KK
%%K
K
attend
nusbj

aux
JJ
%%J
J dobj
*j
*j
***j
*j
prep
+k+k
+k+k
+k
+++k
+k+k
+k+k
+k
he Mother
num

num
JJ
%%J
J
J amod
TT
TT
**TT
TT
will
Mother
num

num
JJ
$$J
J
J amod
TT
TT
**TT
TT
will hearing
det
##G
G
on
pobj
F
""
Catherine 82 superior
measure

Catherine 82 superior
measure

the Friday mother
det
mother
det
the
the
Figure 1: Dependency trees of a source sentence and its compression by a human (taken from Edinburgh?s ?written?
corpus). The source sentence is: ?Mother Catherine, 82, the mother superior, will attend the hearing on Friday, he
said.? The compressed one is: ?Mother Catherine, 82, the mother superior, will attend.? Deleted edges and words are
shown curled and underlined, respectively.
Hence, there are at most 3m?1 candidate compres-
sions, where m is the number of words in s. This
is a large number of candidates, even for modestly
long input sentences. In practice, the candidates are
fewer, because del l removes an entire subtree and
del u an entire uptree, and we do not need to make
decisions Xi about the edges of the deleted subtrees
and uptrees. To reduce the number of candidates
further, we ignore possible assignments that contain
decisions Xi = x to which the ME model assigns
probabilities below a threshold t; i.e., the ME model
is used to prune the space of possible assignments.
When generating the possible assignments to the
Xi variables, we examine the edges ei of Ts in a
top-down breadth-first manner. In the source tree of
Figure 1, for example, we first consider the edges
of ?said?; the left-to-right order is random, but let
us assume that we consider first the ccomp edge.
There are three possible actions: retain the edge
(not del), remove it along with the head ?said?
(del u), or remove it along with the modifier ?at-
tend? and its subtree (del l). If the ME model assigns
a low probability to one of the three actions, that ac-
tion is ignored. For each one of the (remaining) ac-
tions, we obtain a new form of Ts, and we continue
to consider its (other) edges. We process the edges
in a top-down fashion, because the ME model allows
del l actions much more often than del u actions,
and when del l actions are performed near the root
of Ts, they prune large parts of the space of possible
assignments to the Xi variables. Some of the candi-
date compressions that were generated for an input
sentence by setting t = 0.2 are shown in Table 1,
along with the gold (human-authored) compression.
3.2 Ranking candidate compressions
Given that we now have a method that generates
candidate compressions c1, . . . , ck for a sentence s,
we need a function F (ci|s) that will rank the candi-
date compressions. Many of them are ungrammat-
ical and/or do not convey the most important infor-
mation of s. F (ci|s) should help us select a short
candidate that is grammatical and retains the most
important information of s.
3.2.1 Grammaticality and importance rate
A simple way to rank the candidate compressions
is to assign to each one a score intended to measure
its grammaticality and importance rate. By gram-
maticality, Gramm(ci), we mean how grammati-
cally well-formed candidate ci is. A common way
to obtain such a measure is to use an n-gram lan-
888
s: Then last week a second note, in the same handwriting, informed Mrs Allan that the search was
on the wrong side of the bridge.
g: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.
c1: Last week a second note informed Mrs Allan that the search was on the side.
c2: Last week a second note informed Mrs Allan that the search was.
c3: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.
c4: Last week in the same handwriting informed Mrs Allan the search was on the wrong side of the bridge.
Table 1: A source sentence s, its gold (human authored) compression g, and candidate compressions c1, . . . , c4.
guage model trained on a large background corpus.
However, language models tend to assign smaller
probabilities to longer sentences; therefore they fa-
vor short sentences, but not necessarily the most ap-
propriate compressions. To overcome this problem,
we follow Cordeiro et al (2009) and normalize the
score of a trigram language model as shown below,
where w1, . . . , wm are the words of candidate ci.
Gramm(ci) = logPLM (ci)
1/m =
(1/m) ? log(
m?
j=1
P (wj |wj?1, wj?2)) (1)
The importance rate ImpRate(ci|s), defined be-
low, estimates how much information of the original
sentence s is retained in candidate ci. tf(wi) is the
term frequency of wi in the document that contained
? (? = ci, s), and idf(wi) is the inverse document
frequency of wi in a background corpus. We actu-
ally compute idf(wi) only for nouns and verbs, and
set idf(wi) = 0 for other words.
ImpRate(ci|s) = Imp(ci)/Imp(s) (2)
Imp(?) =
?
wi??
tf(wi) ? idf(wi) (3)
The ranking F (c|s) is then defined as a linear
combination of grammaticality and importance rate:
F (ci|s) = ? ?Gramm(ci) + (1? ?) ?
? ImpRate(ci|s)? ? ? CR(ci|s) (4)
A compression rate penalty factor CR(ci|s) =
|c|/|s| is included, to bias our method towards gen-
erating shorter or longer compressions; | ? | denotes
word length in words (punctuation is ignored). We
explain how the weigths ?, ? are tuned in following
sections. We call LM-IMP the configuration of our
method that uses the ranking function of equation 4.
3.2.2 Support Vector Regression
A more sophisticated way to select the best com-
pression is to train a Support Vector Machines Re-
gression (SVR) model to assign scores to feature vec-
tors, with each vector representing a candidate com-
pression. SVR models (Chang and Lin, 2001) are
trained using l training vectors (x1, y1), . . . , (xl, yl),
where xi ? Rn and yi ? R, and learn a function
f : Rn ? R that generalizes the training data. In
our case, xi is a feature vector representing a candi-
date compression ci, and yi is a score indicating how
good a compression ci is. We use 98 features:
? Gramm(ci) and ImpRate(ci|s), as above.
? 2 features indicating the ratio of important and
unimportant words of s, identified as in section
3.1, that were deleted.
? 2 features that indicate the average depth of
the deleted and not deleted words in the depen-
dency tree of s.
? 92 features that indicate which POS tags appear
in s and how many of them were deleted in ci.
For every POS tag label, we use two features,
one that shows how many POS tags of that la-
bel are contained in s and one that shows how
many of these POS tags were deleted in ci.
To assign a regression score yi to each training
vector xi, we experimented with the following func-
tions that measure how similar ci is to the gold com-
pression g, and how grammatical ci is.
? Grammatical relations overlap: In this case, yi
is theF1-score of the dependencies of ci against
those of the gold compression g. This measure
has been shown to correlate well with human
judgements (Clarke and Lapata, 2006). As in
889
the ranking function of section 3.2.1, we add a
compression rate penalty factor.
yi = F1(d(ci)), d(g))? ? ? CR(ci|s) (5)
d(?) denotes the set of dependencies. We call
SVR-F1 the configuration of our system that
uses equation 5 to rank the candidates.
? Tokens accuracy and grammaticality: Tokens
accuracy, TokAcc(ci|s, g), is the percentage of
tokens of s that were correctly retained or re-
moved in ci; a token was correctly retained
or removed, if it was also retained (or re-
moved) in the gold compression g. To cal-
culate TokAcc(ci|s, g), we need the word-to-
word alignment of s to g, and s to ci. These
alignments were obtained as a by-product of
computing the corresponding (word) edit dis-
tances. We also want the regression model to
favor grammatical compressions. Hence, we
use a linear combination of tokens accuracy
and grammaticality of ci:
yi = ? ? TokAcc(ci|s, g) +
(1? ?) ?Gramm(ci)? ? ? CR(ci|s) (6)
Again, we add a compression rate penalty, to
be able to generate shorter or longer compres-
sions. We call SVR-TOKACC-LM the config-
uration of our system that uses equation 6.
4 Baseline and T3
As a baseline, we use a simple algorithm based on
the ME classifier of section 3.1. The baseline pro-
duces a single compression c for every source sen-
tence s by considering sequentially the edges ei of
s?s dependency tree in a random order, and perform-
ing at each ei the single action (not del, del u, or
del l) that the ME model considers more probable;
the words of the chopped dependency tree are then
put in the same order as in s. We call this system
Greedy-Baseline.
We compare our method against the extractive
version of T3 (Cohn and Lapata, 2007; Cohn and
Lapata, 2009), a state-of-the-art sentence compres-
sion system that applies sequences of transduction
operators to the syntax trees of the source sentences.
The available tranduction operators are learnt from
the syntax trees of a set of source-gold pairs. Ev-
ery operator transforms a subtree ? to a subtree ?,
rooted at symbols X and Y , respectively.
To find the best sequence of transduction opera-
tors that can be applied to a source syntax tree, a
chart-based dynamic programming decoder is used,
which finds the best scoring sequence q?:
q? = arg max
q
score(q;w) (7)
where score(q;w) is the dot product ??(q), w?.
?(q) is a vector-valued feature function, and w is a
vector of weights learnt using a Structured Support
Vector Machine (Tsochantaridis et al, 2005).
?(q) consists of: (i) the log-probability of the re-
sulting candidate, as returned by a tri-gram language
model; and (ii) features that describe how the opera-
tors of q are applied, for example the number of the
terminals in each operator?s ? and ? subtrees, the
POS tags of the X and Y roots of ? and ? etc.
5 Experiments
We used Stanford?s parser (de Marneffe et al, 2006)
and ME classifier (Manning et al, 2003).5 For
the (trigram) language model, we used SRILM with
modified Kneser-Ney smoothing (Stolcke, 2002).6
The language model was trained on approximately
4.5 million sentences of the TIPSTER corpus. To
obtain idf(wi) values, we used approximately 19.5
million verbs and nouns from the TIPSTER corpus.
T3 requires the syntax trees of the source-gold
pairs in Penn Treebank format, as well as a trigram
language model. We obtained T3?s trees using Stan-
ford?s parser, as in our system, unlike Cohn and La-
pata (2009) that use Bikel?s (2002) parser. The lan-
guage models in T3 and our system are trained on
the same data and with the same options used by
Cohn and Lapata (2009). T3 also needs a word-to-
word alignment of the source-gold pairs, which was
obtained by computing the edit distance, as in Cohn
and Lapata (2009) and SVR-TOKACC-LM.
We used Edinburgh?s ?written? sentence com-
pression corpus (section 2), which consists of
source-gold pairs (one gold compression per source
5Both available from http://nlp.stanford.edu/.
6See http://www.speech.sri.com/projects/srilm/.
890
sentence). The gold compressions were created by
deleting words. We split the corpus in 3 parts: 1024
training, 324 development, and 291 testing pairs.
5.1 Best configuration of our method
We first evaluated experimentally the three configu-
rations of our method (LM-IMP, SVR-F1, SVR-
TOKACC-LM), using the F1-measure of the de-
pendencies of the machine-generated compressions
against those of the gold compressions as an auto-
matic evaluation measure. This measure has been
shown to correlate well with human judgements
(Clarke and Lapata, 2006).
In all three configurations, we trained the ME
model of section 3.1 on the dependency trees of the
source-gold pairs of the training part of the corpus.
We then used the trained ME classifier to generate
the candidate compressions of each source sentence
of the training part. We set t = 0.2, which led to
at most 10,000 candidates for almost every source
sentence. We kept up to 1.000 candidates for each
source sentence, and we selected randonly approx-
imately 10% of them, obtaining 18,385 candidates,
which were used to train the two SVR configurations;
LM-IMP requires no training.
To tune the ? parameters of LM-IMP and SVR-
TOKACC-LM in equations 4 and 6, we initially set
? = 0 and we experimented with different val-
ues of ?. For each one of the two configurations
and for every different ? value, we computed the
average compression rate of the machine-generated
compressions on the development set. In the rest
of the experiments, we set ? to the value that gave
an average compression rate approximatelly equal to
that of the gold compressions of the training part.
We then experimented with different values of ?
in all three configurations, in equations 4?6, to pro-
duce smaller or longer compression rates. The ? pa-
rameter provides a uniform mechanism to fine-tune
the compression rate in all three configurations, even
in SVR-F1 that has no ?. The results on the de-
velopment part are shown in Figure 2, along with
the baseline?s results. The baseline has no param-
eters to tune; hence, its results are shown as a sin-
gle point. Both SVR models outperform LM-IMP,
which in turn outperforms the baseline. Also, SVR-
TOKACC-LM performs better or as well as SVR-
F1 for all compression rates. Note, also, that the
perfomance of the two SVR configurations might be
improved further by using more training examples,
whereas LM-IMP contains no learning component.
Figure 2: Evaluation results on the development set.
5.2 Our method against T3
We then evaluated the best configuration of our
method (SVR-TOKACC-LM) against T3, both au-
tomatically (F1-measure) and with human judges.
We trained both systems on the training set of the
corpus. In our system, we used the same ? value that
we had obtained from the experiments of the previ-
ous section. We then varied the values of our sys-
tem?s ? parameter to obtain approximately the same
compression rate as T3.
For the evaluation with the human judges, we se-
lected randomly 80 sentences from the test part. For
each source sentence s, we formed three pairs, con-
taining s, the gold compression, the compression
of SVR-TOKACC-LM, and the compression of T3,
repsectively, 240 pairs in total. Four judges (grad-
uate students) were used. Each judge was given 60
pairs in a random sequence; they did not know how
the compressed sentenes were obtained and no judge
saw more than one compression of the same source
sentence. The judges were told to rate (in a scale
from 1 to 5) the compressed sentences in terms of
grammaticality, meaning preservation, and overall
quality. Their average judgements are shown in Ta-
ble 2, where the F1-scores are also included. Cohn
and Lapata (2009) have reported very similar scores
891
for T3 on a different split of the corpus (F1: 49.48%,
CR: 61.09%).
system G M Ov F1 (%) CR (%)
T3 3.83 3.28 3.23 47.34 59.16
SVR 4.20 3.43 3.57 52.09 59.85
gold 4.73 4.27 4.43 100.00 78.80
Table 2: Results on 80 test sentences. G: grammaticality,
M: meaning preservation, Ov: overall score, CR: com-
pression rate, SVR: SVR-TOKACC-LM.
Our system outperforms T3 in all evaluation mea-
sures. We used Analysis of Variance (ANOVA) fol-
lowed by post-hoc Tukey tests to check whether the
judge ratings differ significantly (p < 0.1); all judge
ratings of gold compressions are significantly differ-
ent from T3?s and those of our system; also, our sys-
tem differs significantly from T3 in grammaticality,
but not in meaning preservation and overall score.
We also performed Wilcoxon tests, which showed
that the difference in the F1 scores of the two sys-
tems is statistically significant (p < 0.1) on the 80
test sentences. Table 3 shows the F1 scores and the
average compression rates for all 291 test sentences.
Both systems have comparable compression rates,
but again our system outperforms T3 in F1, with a
statistically significant difference (p < 0.001).
system F1 CR
SVR-TOKACC-LM 53.75 63.72
T3 47.52 64.16
Table 3: F1 scores on the entire test set.
Finally, we computed the Pearson correlation r of
the overall (Ov) scores that the judges assigned to
the machine-generated compressions with the corre-
sponding F1 scores. The two measures were found
to corellate reliably (r = 0.526). Similar results
have been reported (Clarke and Lapata, 2006) for
Edinburgh?s ?spoken? corpus (r = 0.532) and the
Ziff-Davis corpus (r = 0.575).
6 Conclusions and future work
We presented a new two-stage extractive method
for sentence compression. The first stage gener-
ates candidate compressions by removing or not
edges from the source sentence?s dependency tree;
an ME model is used to prune unlikely edge deletion
or non-deletions. The second stage ranks the can-
didate compressions; we experimented with three
ranking models, achieving the best results with an
SVR model trained with an objective function that
combines token accuracy and a language model.
We showed experimentally, via automatic evalua-
tion and with human judges, that our method com-
pares favorably to a state-of-the-art extractive sys-
tem. Unlike other recent approaches, our system
uses no hand-crafted rules. In future work, we plan
to support more complex tranformations, instead of
only removing words and experiment with different
sizes of training data.
The work reported in this paper was carried out in
the context of project INDIGO, where an autonomous
robotic guide for museum collections is being devel-
oped. The guide engages the museum?s visitors in
spoken dialogues, and it describes the exhibits that
the visitors select by generating textual descriptions,
which are passed on to a speech synthesizer. The
texts are generated from logical facts stored in an on-
tology (Galanis et al, 2009) and from canned texts;
the latter are used when the corresponding informa-
tion is difficult to encode in symbolic form (e.g., to
store short stories about the exhibits). The descrip-
tions of the exhibits are tailored depending on the
type of the visitor (e.g., child vs. adult), and an im-
portant tailoring aspect is the generation of shorter
or longer descriptions. The parts of the descrip-
tions that are generated from logical facts can be
easily made shorter or longer, by conveying fewer
or more facts. The methods of this paper are used
to automatically shorten the parts of the descrip-
tions that are canned texts, instead of requiring mul-
tiple (shorter and longer) hand-written versions of
the canned texts.
Acknowledgements
This work was carried out in INDIGO, an FP6 IST
project funded by the European Union, with addi-
tional funding provided by the Greek General Sec-
retariat of Research and Technology.7
7Consult http://www.ics.forth.gr/indigo/.
892
References
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
2006. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of the 2nd International Conference on Human Lan-
guage Technology Research, pages 24?27.
C.C Chang and C.J Lin. 2001. LIBSVM: a library for
Support Vector Machines. Technical report. Software
available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
J. Clarke and M. Lapata. 2006. Models for sentence
compression: A comparison across domains, training
requirements and evaluation measures. In Proceedings
of COLING, pages 377?384.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Artificial Intelligence Research, 1(31):399?
429.
T. Cohn and M. Lapata. 2007. Large margin syn-
chronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoNLL,
pages 73?82.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING,
pages 137?144.
T. Cohn and M. Lapata. 2009. Sentence compression
as tree to tree tranduction. Artificial Intelligence Re-
search, 34:637?674.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?69.
J. Cordeiro, G. Dias, and P. Brazdil. 2009. Unsupervised
induction of sentence compression rules. In Proceed-
ings of the ACL Workshop on Language Generation
and Summarisation, pages 391?399.
S. Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the NAACL
Workshop on Automatic Summarization, pages 89?98.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC,
pages 449?454.
Dimitrios Galanis, George Karakatsiotis, Gerasimos
Lampouras, and Ion Androutsopoulos. 2009. An
open-source natural language generator for OWL on-
tologies and its use in protege and second life. In Pro-
ceedings of the Demonstrations Session at EACL 2009,
pages 17?20, Athens, Greece, April. Association for
Computational Linguistics.
S. Gupta, A. Nenkova, and D. Jurafsky. 2007. Measur-
ing importance and query relevance in topic-focused
multi-document summarization. In Proceedings of
ACL, pages 193?196.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP, pages 310?
315.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probalistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91?
107.
C.W. Lin and E. Hovy. 2000. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of ACL, pages 495?501.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions
for automatic text summarization. In Proceedings of
DUC.
C. D. Manning, D. Klein, and C. Manning. 2003. Op-
timization, maxent models, and conditional estimation
without magic. In tutorial notes of HLT-NAACL 2003
and ACL 2003.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, pages 297?304.
T. Nomoto. 2009. A comparison of model free versus
model intensive approaches to sentence compression.
In Proceedings of EMNLP, pages 391?399.
D. Paiva and R. Evans. 2005. Empirically-based con-
trol of natural language generation. In Proceedings of
ACL.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, pages 901?
904.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for indepen-
dent and structured output spaces. Machine Learning
Research, 6:1453?1484.
V. Vandeghinste and Y. Pan. 2004. Sentence compres-
sion for automated subtitling: A hybrid approach. In
Proceedings of the ACL Workshop ?Text Summariza-
tion Branches Out?, pages 89?95.
893
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561?566,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Integer Linear Programming in Concept-to-Text Generation to
Produce More Compact Texts
Gerasimos Lampouras and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
http://nlp.cs.aueb.gr/
Abstract
We present an ILP model of concept-to-
text generation. Unlike pipeline archi-
tectures, our model jointly considers the
choices in content selection, lexicaliza-
tion, and aggregation to avoid greedy de-
cisions and produce more compact texts.
1 Introduction
Concept-to-text natural language generation
(NLG) generates texts from formal knowledge
representations (Reiter and Dale, 2000). With the
emergence of the Semantic Web (Antoniou and
van Harmelen, 2008), interest in concept-to-text
NLG has been revived and several methods
have been proposed to express axioms of OWL
ontologies (Grau et al, 2008) in natural language
(Bontcheva, 2005; Mellish and Sun, 2006; Gala-
nis and Androutsopoulos, 2007; Mellish and Pan,
2008; Schwitter et al, 2008; Schwitter, 2010;
Liang et al, 2011; Williams et al, 2011).
NLG systems typically employ a pipeline archi-
tecture. They usually start by selecting the logi-
cal facts to express. The next stage, text planning,
ranges from simply ordering the selected facts to
complex decisions about the rhetorical structure
of the text. Lexicalization then selects the words
and syntactic structures that will realize each fact,
specifying how each fact can be expressed as a
single sentence. Sentence aggregation then com-
bines sentences into longer ones. Another compo-
nent generates appropriate referring expressions,
and surface realization produces the final text.
Each stage of the pipeline is treated as a lo-
cal optimization problem, where the decisions of
the previous stages cannot be modified. This ar-
rangement produces texts that may not be optimal,
since the decisions of the stages have been shown
to be co-dependent (Danlos, 1984; Marciniak and
Strube, 2005; Belz, 2008). For example, content
selection and lexicalization may lead to more or
fewer sentence aggregation opportunities.
We present an Integer Linear Programming
(ILP) model that combines content selection, lex-
icalization, and sentence aggregation. Our model
does not consider text planning, nor referring ex-
pression generation, which we hope to include in
future work, but it is combined with an external
simple text planner and a referring expression gen-
eration component; we also do not discuss sur-
face realization. Unlike pipeline architectures, our
model jointly examines the possible choices in the
three NLG stages it considers, to avoid greedy local
decisions. Given an individual (entity) or class of
an OWL ontology and a set of facts (OWL axioms)
about the individual or class, we aim to produce a
text that expresses as many of the facts in as few
words as possible. This is important when space is
limited or expensive (e.g., product descriptions on
smartphones, advertisements in search engines).
Although the search space of our model is very
large and ILP problems are in general NP-hard, ILP
solvers can be used, they are very fast in practice,
and they guarantee finding a global optimum. Ex-
periments show that our ILP model outperforms,
in terms of compression, an NLG system that uses
the same components, but connected in a pipeline,
with no deterioration in fluency and clarity.
2 Related work
Marciniak and Strube (2005) propose a general
ILP approach for language processing applications
where the decisions of classifiers that consider
particular, but co-dependent, subtasks need to be
combined. They also show how their approach
can be used to generate multi-sentence route di-
rections, in a setting with very different inputs and
processing stages than the ones we consider.
Barzilay and Lapata (2005) treat content selec-
tion as an optimization problem. Given a pool of
facts and scores indicating the importance of each
561
fact or pair of facts, they select the facts to express
by formulating an optimization problem similar
to energy minimization. In other work, Barzilay
and Lapata (2006) consider sentence aggregation.
Given a set of facts that a content selection stage
has produced, aggregation is viewed as the prob-
lem of partitioning the facts into optimal subsets.
Sentences expressing facts that are placed in the
same subset are aggregated to form a longer sen-
tence. An ILP model is used to find the partitioning
that maximizes the pairwise similarity of the facts
in each subset, subject to constraints limiting the
number of subsets and the facts in each subset.
Althaus et al (2004) show that ordering a set
of sentences to maximize sentence-to-sentence co-
herence is equivalent to the traveling salesman
problem and, hence, NP-complete. They also show
how an ILP solver can be used in practice.
Joint optimization ILP models have also been
used in multi-document text summarization and
sentence compression (McDonald, 2007; Clarke
and Lapata, 2008; Berg-Kirkpatrick et al, 2011;
Galanis et al, 2012; Woodsend and Lapata, 2012),
where the input is text, not formal knowledge rep-
resetations. Statistical methods to jointly perform
content selection, lexicalization, and surface real-
ization have also been proposed in NLG (Liang et
al., 2009; Konstas and Lapata, 2012a; Konstas and
Lapata, 2012b), but they are currently limited to
generating single sentences from flat records.
To the best of our knowledge, this article is the
first one to consider content selection, lexicaliza-
tion, and sentence aggregation as an ILP joint opti-
mization problem in the context of multi-sentence
concept-to-text generation. It is also the first arti-
cle to consider ILP in NLG from OWL ontologies.
3 Our ILP model of NLG
Let F = {f1, . . . , fn} be the set of all the facts fi
(OWL axioms) about the individual or class to be
described. OWL axioms can be represented as sets
of RDF triples of the form ?S,R,O?, where S is an
individual or class, O is another individual, class,
or datatype value, and R is a relation (property)
that connects S to O. Hence, we can assume that
each fact fi is a triple ?Si, Ri, Oi?.1
For each fact fi, a set Pi = {pi1, pi2, . . . }
of alternative sentence plans is available. Each
1We actually convert the RDF triples to simpler message
triples, so that each message triple can be easily expressed by
a simple sentence, but we do not discuss this conversion here.
sentence plan pik specifies how to express fi =
?Si, Ri, Oi? as an alternative single sentence. In
our work, a sentence plan is a sequence of slots,
along with instructions specifying how to fill the
slots in; and each sentence plan is associated
with the relations it can express. For example,
?exhibit12,foundIn,athens? could be ex-
pressed using a sentence plan like ?[ref (S)]
[findpast] [in] [ref (O)]?, where square brackets
denote slots, ref (S) and ref (O) are instructions
requiring referring expressions for S and O in
the corresponding slots, and ?findpast? requires the
simple past form of ?find?. In our example, the
sentence plan would lead to a sentence like ?Ex-
hibit 12 was found in Athens?. We call elements
the slots with their instructions, but with ?S?
and ?O? accompanied by the individuals, classes,
or datatype values they refer to; in our exam-
ple, the elements are ?[ref (S: exhibit12)]?,
?[findpast]?, ?[in]?, ?[ref (O: athens)]?. Dif-
ferent sentence plans may lead to more or fewer
aggregation opportunities; for example, sentences
with the same verb are easier to aggregate. We use
aggregation rules (Dalianis, 1999) that operate on
sentence plans and usually lead to shorter texts.
Let s1, . . . , sm be disjoint subsets of F , each
containing 0 to n facts, with m < n. A single
sentence is generated for each subset sj by aggre-
gating the sentences (more precisely, the sentence
plans) expressing the facts of sj .2 An empty sj
generates no sentence, i.e., the resulting text can
be at most m sentences long. Let us also define:
ai =
{
1, if fact fi is selected
0, otherwise (1)
likj =
?
?
?
1, if sentence plan pik is used to express
fact fi, and fi is in subset sj
0, otherwise
(2)
btj =
{
1, if element et is used in subset sj
0, otherwise (3)
and let B be the set of all the distinct elements (no
duplicates) from all the available sentence plans
that can express the facts of F . The length of an
aggregated sentence resulting from a subset sj can
be roughly estimated by counting the distinct el-
ements of the sentence plans that have been cho-
sen to express the facts of sj ; elements that occur
more than once in the chosen sentence plans of sj
2All the sentences of every possible subset sj can be ag-
gregated, because all the sentences share the same subject,
the class or individual being described. If multiple aggrega-
tion rules apply, we use the one that leads to a shorter text.
562
are counted only once, because they will probably
be expressed only once, due to aggregation.
Our objective function (4) maximizes the num-
ber of selected facts fi and minimizes the number
of distinct elements in each subset sj , i.e., the ap-
proximate length of the corresponding aggregated
sentence; an alternative explanation is that by min-
imizing the number of distinct elements in each sj ,
we favor subsets that aggregate well. By a and b
we jointly denote all the ai and btj variables. The
two parts (sums) of the objective function are nor-
malized to [0, 1] by dividing by the total number
of available facts |F | and the number of subsets m
times the total number of distinct elements |B|. In
the first part of the objective, we treat all the facts
as equally important; if importance scores are also
available for the facts, they can be added as mul-
tipliers of ?i. The parameters ?1 and ?2 are used
to tune the priority given to expressing many facts
vs. generating shorter texts; we set ?1 + ?2 = 1.
max
a,b
?1 ?
|F |?
i=1
ai
|F | ? ?2 ?
m?
j=1
|B|?
t=1
btj
m ? |B| (4)
subject to:
ai =
m?
j=1
|Pi|?
k=1
likj , for i = 1, . . . , n (5)
?
et?Bik
btj ? |Bik| ? likj , for
i = 1, . . . , n
j = 1, . . . ,m
k = 1, . . . , |Pi|
(6)
?
pik?P (et)
likj ? btj , for t = 1, . . . , |B|j = 1, . . . ,m (7)
|B|?
t=1
btj ? Bmax, for j = 1, . . . ,m (8)
|Pi|?
k=1
likj +
|Pi? |?
k?=1
li?k?j ? 1, for
j = 1, . . . ,m, i = 2, . . . , n
i? = 1, . . . , n? 1; i 6= i?
section(fi) 6= section(f ?i)
(9)
Constraint 5 ensures that for each selected fact,
only one sentence plan in only one subset is se-
lected; if a fact is not selected, no sentence plan
for the fact is selected either. |?| denotes the car-
dinality of a set ?. In constraint 6, Bik is the set of
distinct elements et of the sentence plan pik. This
constraint ensures that if pik is selected in a subset
sj , then all the elements of pik are also present in
sj . If pik is not selected in sj , then some of its el-
ements may still be present in sj , if they appear in
another selected sentence plan of sj .
In constraint 7, P (et) is the set of sentence plans
that contain element et. If et is used in a subset sj ,
then at least one of the sentence plans of P (et)
must also be selected in sj . If et is not used in sj ,
then no sentence plan of P (et) may be selected in
sj . Lastly, constraint 8 limits the number of ele-
ments that a subset sj can contain to a maximum
allowed number Bmax, in effect limiting the max-
imum length of an aggregated sentence.
We assume that each relation R has been man-
ually mapped to a single topical section; e.g., re-
lations expressing the color, body, and flavor of
a wine may be grouped in one section, and rela-
tions about the wine?s producer in another. The
section of a fact fi = ?Si, Ri, Oi? is the section
of its relation Ri. Constraint 9 ensures that facts
from different sections will not be placed in the
same subset sj , to avoid unnatural aggregations.
4 Experiments
We used NaturalOWL (Galanis and Androutsopou-
los, 2007; Galanis et al, 2009; Androutsopoulos
et al, 2013), an NLG system for OWL ontologies
that relies on a pipeline of content selection, text
planning, lexicalization, aggregation, referring ex-
pression generation, and surface realization.3 We
modified content selection, lexicalization, and ag-
gregation to use our ILP model, maintaining the
aggregation rules of the original system.4 For re-
ferring expression generation and surface realiza-
tion, the new system, called ILPNLG, invokes the
corresponding components of NaturalOWL.
The original system, called PIPELINE, assumes
that each relation has been mapped to a topical
section, as in ILPNLG. It also assumes that a man-
ually specified order of the sections and the rela-
tions of each section is available, which is used
by the text planner to order the selected facts (by
their relations). The subsequent components of the
pipeline are not allowed to change the order of the
facts, and aggregation operates only on sentence
plans of adjacent facts from the same section. In
ILPNLG, the manually specified order of sections
and relations is used to order the sentences of each
subset sj (before aggregating them), the aggre-
gated sentences in each section (each aggregated
sentence inherits the minimum order of its con-
stituents), and the sections (with their sentences).
We used the Wine Ontology, which had been
3All the software and data we used are freely available
from http://nlp.cs.aueb.gr/software.html.
We use version 2 of NaturalOWL.
4We use the Branch and Cut implementation of GLPK; see
sourceforge.net/projects/winglpk/.
563
used in previous experiments with PIPELINE.5 We
kept the 2 topical sections, the ordering of sec-
tions and relations, and the sentence plans that
had been used in the previous experiments, but we
added more sentence plans to ensure that 3 sen-
tence plans were available per fact. We gener-
ated texts for the 52 wine individuals of the on-
tology; we did not experiment with texts describ-
ing classes of wines, because we could not think
of multiple alternative sentence plans for many of
their axioms. For each individual, there were 5
facts on average and a maximum of 6 facts.
PIPELINE has a parameter M specifying the
maximum number of facts it is allowed to report
per text. When M is smaller than the number of
available facts |F | and all the facts are treated as
equally important, as in our experiments, it se-
lects randomly M of the available facts. We re-
peated the generation of PIPELINE?s texts for the
52 individuals for M = 2, 3, 4, 5, 6. For each M ,
the texts of PIPELINE for the 52 individuals were
generated three times, each time using one of the
different alternative sentence plans of each rela-
tion. We also generated the texts using a variant of
PIPELINE, dubbed PIPELINESHORT, which always
selects the shortest (in elements) sentence plan
among the available ones. In all cases, PIPELINE
and PIPELINESHORT were allowed to form ag-
gregated sentences containing up to Bmax = 22
distinct elements, which was the number of dis-
tinct elements of the longest aggregated sentence
in the previous experiments, where PIPELINE was
allowed to aggregate up to 3 original sentences.
With ILPNLG, we repeated the generation of the
texts of the 52 individuals using different values
of ?1 (?2 = 1 ? ?1), which led to texts express-
ing from zero to all of the available facts. We set
the maximum number of fact subsets to m = 3,
which was the maximum number of aggregated
sentences observed in the texts of PIPELINE and
PIPELINESHORT. Again, we set Bmax = 22.
We compared ILPNLG to PIPELINE and PIPELI-
NESHORT by measuring the average number of
facts they reported divided by the average text
length (in words). Figure 1 shows this ratio as a
function of the average number of reported facts,
along with 95% confidence intervals (of sample
means). PIPELINESHORT achieved better results
than PIPELINE, but the differences were small.
For ?1 < 0.2, ILPNLG produces empty texts,
5See www.w3.org/TR/owl-guide/wine.rdf.
Figure 1: Facts/words ratio of the generated texts.
since it focuses on minimizing the number of dis-
tinct elements of each text. For ?1 ? 0.225, it per-
forms better than the other systems. For ?1 ? 0.3,
it obtains the highest fact/words ratio by select-
ing the facts and sentence plans that lead to the
most compressive aggregations. For greater val-
ues of ?1, it selects additional facts whose sen-
tence plans do not aggregate that well, which is
why the ratio declines. For small numbers of facts,
the two pipeline systems select facts and sentence
plans that offer very few aggregation opportuni-
ties; as the number of selected facts increases,
some more aggregation opportunities arise, which
is why the facts/words ratio of the two systems
improves. In all the experiments, the ILP solver
was very fast (average: 0.08 sec, worst: 0.14 sec).
Experiments with human judges also showed that
the texts of ILPNLG cannot be distinguished from
those of PIPELINESHORT in terms of fluency and
text clarity. Hence, the highest compactness of the
texts of ILPNLG does not come at the expense of
lower text quality. Space does not permit a more
detailed description of these experiments.
We show below texts produced by PIPELINE
(M = 4) and ILPNLG (?1 = 0.3).
PIPELINE: This is a strong Sauternes. It is made from Semil-
lon grapes and it is produced by Chateau D?ychem.
ILPNLG: This is a strong Sauternes. It is made from Semillon
grapes by Chateau D?ychem.
PIPELINE: This is a full Riesling and it has moderate flavor.
It is produced by Volrad.
ILPNLG: This is a full sweet moderate Riesling.
In the first pair, PIPELINE uses different verbs for
the grapes and producer, whereas ILPNLG uses the
same verb, which leads to a more compressive ag-
gregation; both texts describe the same wine and
report 4 facts. In the second pair, ILPNLG has cho-
sen to express the sweetness instead of the pro-
ducer, and uses the same verb (?be?) for all the
facts, leading to a shorter sentence; again both
texts describe the same wine and report 4 facts.
564
In both examples, some facts are not aggregated
because they belong in different sections.
5 Conclusions
We presented an ILP model for NLG that jointly
considers the choices in content selection, lexical-
ization, and aggregation to avoid greedy local de-
cisions and produce more compact texts. Exper-
iments verified that our model can express more
facts per word, compared to a pipeline, which is
important when space is scarce. An off-the-shelf
ILP solver took approximately 0.1 sec for each
text. We plan to extend our model to include text
planning and referring expressions generation.
Acknowledgments
This research has been co-financed by the Euro-
pean Union (European Social Fund ? ESF) and
Greek national funds through the Operational Pro-
gram ?Education and Lifelong Learning? of the
National Strategic Reference Framework (NSRF)
? Research Funding Program: Heracleitus II. In-
vesting in knowledge society through the Euro-
pean Social Fund.
References
E. Althaus, N. Karamanis, and A. Koller. 2004. Com-
puting locally coherent discourses. In 42nd Annual
Meeting of ACL, pages 399?406, Barcelona, Spain.
I. Androutsopoulos, G. Lampouras, and D. Gala-
nis. 2013. Generating natural language descrip-
tions from OWL ontologies: the NaturalOWL sys-
tem. Technical report, Natural Language Processing
Group, Department of Informatics, Athens Univer-
sity of Economics and Business.
G. Antoniou and F. van Harmelen. 2008. A Semantic
Web primer. MIT Press, 2nd edition.
R. Barzilay and M. Lapata. 2005. Collective content
selection for concept-to-text generation. In HLT-
EMNLP, pages 331?338, Vancouver, BC, Canada.
R. Barzilay and M. Lapata. 2006. Aggregation via
set partitioning for natural language generation. In
HLT-NAACL, pages 359?366, New York, NY.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011.
Jointly learning to extract and compress. In 49th
Annual Meeting of ACL, pages 481?490, Portland,
OR.
K. Bontcheva. 2005. Generating tailored textual sum-
maries from ontologies. In 2nd European Semantic
Web Conf., pages 531?545, Heraklion, Greece.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 1(31):399?429.
H. Dalianis. 1999. Aggregation in natural language
generation. Comput. Intelligence, 15(4):384?414.
L. Danlos. 1984. Conceptual and linguistic decisions
in generation. In 10th COLING, pages 501?504,
Stanford, CA.
D. Galanis and I. Androutsopoulos. 2007. Generating
multilingual descriptions from linguistically anno-
tated OWL ontologies: the NaturalOWL system. In
11th European Workshop on Natural Lang. Genera-
tion, pages 143?146, Schloss Dagstuhl, Germany.
D. Galanis, G. Karakatsiotis, G. Lampouras, and I. An-
droutsopoulos. 2009. An open-source natural lan-
guage generator for OWL ontologies and its use in
Prote?ge? and Second Life. In 12th Conf. of the Euro-
pean Chapter of ACL (demos), Athens, Greece.
D. Galanis, G. Lampouras, and I. Androutsopoulos.
2012. Extractive multi-document summarization
with Integer Linear Programming and Support Vec-
tor Regression. In COLING, pages 911?926, Mum-
bai, India.
B.C. Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-
Schneider, and U. Sattler. 2008. OWL 2: The next
step for OWL. Web Semantics, 6:309?322.
I. Konstas and M. Lapata. 2012a. Concept-to-text gen-
eration via discriminative reranking. In 50th Annual
Meeting of ACL, pages 369?378, Jeju Island, Korea.
I. Konstas and M. Lapata. 2012b. Unsupervised
concept-to-text generation with hypergraphs. In
HLT-NAACL, pages 752?761, Montre?al, Canada.
P. Liang, M. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
47th Meeting of ACL and 4th AFNLP, pages 91?99,
Suntec, Singapore.
S.F. Liang, R. Stevens, D. Scott, and A. Rector. 2011.
Automatic verbalisation of SNOMED classes using
OntoVerbal. In 13th Conf. AI in Medicine, pages
338?342, Bled, Slovenia.
T. Marciniak and M. Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In 9th Con-
ference on Computational Natural Language Learn-
ing, pages 136?143, Ann Arbor, MI.
R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Eu-
ropean Conference on Information Retrieval, pages
557?564, Rome, Italy.
565
C. Mellish and J.Z. Pan. 2008. Natural language di-
rected inference from ontologies. Artificial Intelli-
gence, 172:1285?1315.
C. Mellish and X. Sun. 2006. The Semantic Web as a
linguistic resource: opportunities for nat. lang. gen-
eration. Knowledge Based Systems, 19:298?303.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge Univ. Press.
R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and
G. Hart. 2008. A comparison of three controlled
nat. languages for OWL 1.1. In 4th OWL Experi-
ences and Directions Workshop, Washington DC.
R. Schwitter. 2010. Controlled natural languages for
knowledge representation. In 23rd COLING, pages
1113?1121, Beijing, China.
S. Williams, A. Third, and R. Power. 2011. Levels
of organization in ontology verbalization. In 13th
European Workshop on Natural Lang. Generation,
pages 158?163, Nancy, France.
K. Woodsend and M. Lapata. 2012. Multiple aspect
summarization using integer linear programming. In
EMNLP-CoNLL, pages 233?243, Jesu Island, Ko-
rea.
566
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27?35,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 4: Aspect Based Sentiment Analysis
Maria Pontiki
Institute for Language
and Speech Processing,
?Athena? Research Center
mpontiki@ilsp.gr
Haris Papageorgiou
Institute for Language
and Speech Processing,
?Athena? Research Center
xaris@ilsp.gr
Dimitrios Galanis
Institute for Language
and Speech Processing,
?Athena? Research Center
galanisd@ilsp.gr
Ion Androutsopoulos
Dept. of Informatics
Athens University of
Economics and Business
ion@aueb.gr
John Pavlopoulos
Dept. of Informatics,
Athens University of
Economics and Business
annis@aueb.gr
Suresh Manandhar
Dept. of Computer Science,
University of York
suresh@cs.york.ac.uk
Abstract
Sentiment analysis is increasingly viewed
as a vital task both from an academic and
a commercial standpoint. The majority of
current approaches, however, attempt to
detect the overall polarity of a sentence,
paragraph, or text span, irrespective of the
entities mentioned (e.g., laptops) and their
aspects (e.g., battery, screen). SemEval-
2014 Task 4 aimed to foster research in the
field of aspect-based sentiment analysis,
where the goal is to identify the aspects
of given target entities and the sentiment
expressed for each aspect. The task pro-
vided datasets containing manually anno-
tated reviews of restaurants and laptops, as
well as a common evaluation procedure. It
attracted 163 submissions from 32 teams.
1 Introduction
With the proliferation of user-generated content on
the web, interest in mining sentiment and opinions
in text has grown rapidly, both in academia and
business. Early work in sentiment analysis mainly
aimed to detect the overall polarity (e.g., positive
or negative) of a given text or text span (Pang et
al., 2002; Turney, 2002). However, the need for a
more fine-grained approach, such as aspect-based
(or ?feature-based?) sentiment analysis (ABSA),
soon became apparent (Liu, 2012). For example,
laptop reviews not only express the overall senti-
ment about a specific model (e.g., ?This is a great
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
laptop?), but also sentiments relating to its spe-
cific aspects, such as the hardware, software, price,
etc. Subsequently, a review may convey opposing
sentiments (e.g., ?Its performance is ideal, I wish
I could say the same about the price?) or objective
information (e.g., ?This one still has the CD slot?)
for different aspects of an entity.
ABSA is critical in mining and summarizing
opinions from on-line reviews (Gamon et al.,
2005; Titov and McDonald, 2008; Hu and Liu,
2004a; Popescu and Etzioni, 2005). In this set-
ting, ABSA aims to identify the aspects of the en-
tities being reviewed and to determine the senti-
ment the reviewers express for each aspect. Within
the last decade, several ABSA systems of this kind
have been developed for movie reviews (Thet et
al., 2010), customer reviews of electronic products
like digital cameras (Hu and Liu, 2004a) or net-
book computers (Brody and Elhadad, 2010), ser-
vices (Long et al., 2010), and restaurants (Ganu et
al., 2009; Brody and Elhadad, 2010).
Previous publicly available ABSA benchmark
datasets adopt different annotation schemes within
different tasks. The restaurant reviews dataset of
Ganu et al. (2009) uses six coarse-grained aspects
(e.g., FOOD, PRICE, SERVICE) and four overall
sentence polarity labels (positive, negative, con-
flict, neutral). Each sentence is assigned one or
more aspects together with a polarity label for
each aspect; for example, ?The restaurant was ex-
pensive, but the menu was great.? would be as-
signed the aspect PRICE with negative polarity and
FOOD with positive polarity. In the product re-
views dataset of Hu and Liu (2004a; 2004b), as-
pect terms, i.e., terms naming aspects (e.g., ?ra-
dio?, ?voice dialing?) together with strength scores
(e.g., ?radio?: +2, ?voice dialing?: ?3) are pro-
27
vided. No predefined inventory of aspects is pro-
vided, unlike the dataset of Ganu et al.
The SemEval-2014 ABSA Task is based on lap-
top and restaurant reviews and consists of four
subtasks (see Section 2). Participants were free to
participate in a subset of subtasks and the domains
(laptops or restaurants) of their choice.
2 Task Description
For the first two subtasks (SB1, SB2), datasets on
both domains (restaurants, laptops) were provided.
For the last two subtasks (SB3, SB4), datasets only
for the restaurant reviews were provided.
Aspect term extraction (SB1): Given a set of
review sentences, the task is to identify all as-
pect terms present in each sentence (e.g., ?wine?,
?waiter?, ?appetizer?, ?price?, ?food?). We require
all the aspect terms to be identified, including as-
pect terms for which no sentiment is expressed
(neutral polarity). These will be useful for con-
structing an ontology of aspect terms and to iden-
tify frequently discussed aspects.
Aspect term polarity (SB2): In this subtask,
we assume that the aspect terms are given (as de-
scribed in SB1) and the task is to determine the po-
larity of each aspect term (positive, negative, con-
flict, or neutral). The conflict label applies when
both positive and negative sentiment is expressed
about an aspect term (e.g., ?Certainly not the best
sushi in New York, however, it is always fresh?).
An alternative would have been to tag the aspect
term in these cases with the dominant polarity, but
this in turn would be difficult to agree on.
Aspect category detection (SB3): Given a
predefined set of aspect categories (e.g., PRICE,
FOOD) and a set of review sentences (but without
any annotations of aspect terms and their polari-
ties), the task is to identify the aspect categories
discussed in each sentence. Aspect categories are
typically coarser than the aspect terms as defined
in SB1, and they do not necessarily occur as terms
in the sentences. For example, in ?Delicious but
expensive?, the aspect categories FOOD and PRICE
are not instantiated through specific aspect terms,
but are only inferred through the adjectives ?deli-
cious? and ?expensive?. SB1 and SB3 were treated
as separate subtasks, thus no information linking
aspect terms to aspect categories was provided.
Aspect category polarity (SB4): For this sub-
task, aspect categories for each review sentence
are provided. The goal is to determine the polar-
ity (positive, negative, conflict, or neutral) of each
aspect category discussed in each sentence.
Subtasks SB1 and SB2 are useful in cases where
no predefined inventory of aspect categories is
available. In these cases, frequently discussed as-
pect terms of the entity can be identified together
with their overall sentiment polarities. We hope to
include an additional aspect term aggregation sub-
task in future (Pavlopoulos and Androutsopoulos,
2014b) to cluster near-synonymous (e.g., ?money?,
?price?, ?cost?) or related aspect terms (e.g., ?de-
sign?, ?color?, ?feeling?) together with their aver-
aged sentiment scores as shown in Fig. 1.
Figure 1: Aggregated aspect terms and average
sentiment polarities for a target entity.
Subtasks SB3 and SB4 are useful when a pre-
defined inventory of (coarse) aspect categories is
available. A table like the one of Fig. 1 can then
also be generated, but this time using the most
frequent aspect categories to label the rows, with
stars showing the proportion of reviews express-
ing positive vs. negative opinions for each aspect
category.
3 Datasets
3.1 Data Collection
The training and test data sizes are provided in Ta-
ble 1. The restaurants training data, consisting of
3041 English sentences, is a subset of the dataset
from Ganu et al. (2009), which included annota-
tions for coarse aspect categories (as in SB3) and
overall sentence polarities. We added annotations
for aspect terms occurring in the sentences (SB1),
aspect term polarities (SB2), and aspect category
polarities (SB4). Additional restaurant reviews
were collected and annotated (from scratch) in
the same manner and used as test data (800 sen-
tences). The laptops dataset contains 3845 English
28
sentences extracted from laptop custumer reviews.
Human annotators tagged the aspect terms (SB1)
and their polarities (SB2); 3045 sentences were
used for training and 800 for testing (evaluation).
Domain Train Test Total
Restaurants 3041 800 3841
Laptops 3045 800 3845
Total 6086 1600 7686
Table 1: Sizes (sentences) of the datasets.
3.2 Annotation Process
For a given target entity (a restaurant or a lap-
top) being reviewed, the annotators were asked to
provide two types of information: aspect terms
(SB1) and aspect term polarities (SB2). For the
restaurants dataset, two additional annotation lay-
ers were added: aspect category (SB3) and aspect
category polarity (SB4).
The annotators used BRAT (Stenetorp et al.,
2012), a web-based annotation tool, which was
configured appropriately for the needs of the
ABSA task.
1
Figure 2 shows an annotated sen-
tence in BRAT, as viewed by the annotators.
Stage 1: Aspect terms and polarities. During
a first annotation stage, the annotators tagged all
the single or multiword terms that named par-
ticular aspects of the target entity (e.g., ?I liked
the service and the staff, but not the food? ?
{?service?, ?staff?, ?food?}, ?The hard disk is very
noisy?? {?hard disk?}). They were asked to tag
only aspect terms explicitly naming particular as-
pects (e.g., ?everything about it? or ?it?s expen-
sive? do not name particular aspects). The as-
pect terms were annotated as they appeared, even
if misspelled (e.g., ?warrenty? instead of ?war-
ranty?). Each identified aspect term also had to be
assigned a polarity label (positive, negative, neu-
tral, conflict). For example, ?I hated their fajitas,
but their salads were great? ? {?fajitas?: nega-
tive, ?salads?: positive}, ?The hard disk is very
noisy?? {?hard disk?: negative}.
Each sentence of the two datasets was anno-
tated by two annotators, a graduate student (an-
notator A) and an expert linguist (annotator B).
Initially, two subsets of sentences (300 from each
dataset) were tagged by annotator A and the anno-
tations were inspected and validated by annotator
1
Consult http://brat.nlplab.org/ for more in-
formation about BRAT.
B. The disagreements between the two annotators
were confined to borderline cases. Taking into ac-
count the types of these disagreements (discussed
below), annotator A was provided with additional
guidelines and tagged the remainder of the sen-
tences in both datasets.
2
When A was not confi-
dent, a decision was made collaboratively with B.
When A and B disagreed, a decision was made
collaboratively by them and a third expert annota-
tor. Most disagreements fall into one of the fol-
lowing three types:
Polarity ambiguity: In several sentences, it was
unclear if the reviewer expressed positive or neg-
ative opinion, or no opinion at all (just reporting
a fact), due to lack of context. For example, in
?12.44 seconds boot time? it is unclear if the re-
viewer expresses a positive, negative, or no opin-
ion about the aspect term ?boot time?. In future
challenges, it would be better to allow the annota-
tors (and the participating systems) to consider the
entire review instead of each sentence in isolation.
Multi-word aspect term boundaries: In sev-
eral cases, the annotators disagreed on the exact
boundaries of multi-word aspect terms when they
appeared in conjunctions or disjunctions (e.g.,
?selection of meats and seafoods?, ?noodle and
rices dishes?, ?school or office use?). In such
cases, we asked the annotators to tag as a sin-
gle aspect term the maximal noun phrase (the en-
tire conjunction or disjunction). Other disagree-
ments concerned the extent of the aspect terms
when adjectives that may or may not have a sub-
jective meaning were also present. For example,
if ?large? in ?large whole shrimp? is part of the
dish name, then the guidelines require the adjec-
tive to be included in the aspect term; otherwise
(e.g., in ?large portions?) ?large? is a subjectivity
indicator not to be included in the aspect term. De-
spite the guidelines, in some cases it was difficult
to isolate and tag the exact aspect term, because of
intervening words, punctuation, or long-term de-
pendencies.
Aspect term vs. reference to target entity: In
some cases, it was unclear if a noun or noun phrase
was used as the aspect term or if it referred to the
entity being reviewed as whole. In ?This place
is awesome?, for example, ?place? most probably
refers to the restaurant as a whole (hence, it should
not be tagged as an aspect term), but in ?Cozy
2
The guidelines are available at: http://alt.qcri.
org/semeval2014/task4/data/uploads/.
29
Figure 2: A sentence in the BRAT tool, annotated with four aspect terms (?appetizers?, ?salads?, ?steak?,
?pasta?) and one aspect category (FOOD). For aspect categories, the whole sentence is tagged.
place and good pizza? it probably refers to the am-
bience of the restaurant. A broader context would
again help in some of these cases.
We note that laptop reviews often evaluate each
laptop as a whole, rather than expressing opinions
about particular aspects. Furthermore, when they
express opinions about particular aspects, they of-
ten do so by using adjectives that refer implicitly
to aspects (e.g., ?expensive?, ?heavy?), rather than
using explicit aspect terms (e.g., ?cost?, ?weight?);
the annotators were instructed to tag only explicit
aspect terms, not adjectives implicitly referring to
aspects. By contrast, restaurant reviews contain
many more aspect terms (Table 2, last column).
3
Dataset Pos. Neg. Con. Neu. Tot.
LPT-TR 987 866 45 460 2358
LPT-TE 341 128 16 169 654
RST-TR 2164 805 91 633 3693
RST-TE 728 196 14 196 1134
Table 2: Aspect terms and their polarities per do-
main. LPT and RST indicate laptop and restau-
rant reviews, respectively. TR and TE indicate the
training and test set.
Another difference between the two datasets
is that the neutral class is much more frequent
in (the aspect terms of) laptops, since laptop re-
views often mention features without expressing
any (clear) sentiment (e.g., ?the latest version does
not have a disc drive?). Nevertheless, the positive
class is the majority in both datasets, but it is much
more frequent in restaurants (Table 2). The ma-
jority of the aspect terms are single-words in both
datasets (2148 in laptops, 4827 in restaurants, out
of 3012 and 4827 total aspect terms, respectively).
Stage 2: Aspect categories and polarities. In
this task, each sentence needs to be tagged with
the aspect categories discussed in the sentence.
The aspect categories are FOOD, SERVICE, PRICE,
AMBIENCE (the atmosphere and environment of
3
We count aspect term occurrences, not distinct terms.
a restaurant), and ANECDOTES/MISCELLANEOUS
(sentences not belonging in any of the previous
aspect categories).
4
For example, ?The restau-
rant was expensive, but the menu was great? is
assigned the aspect categories PRICE and FOOD.
Additionally, a polarity (positive, negative, con-
flict, neutral) for each aspect category should be
provided (e.g., ?The restaurant was expensive, but
the menu was great?? {PRICE: negative, FOOD:
positive}.
One annotator validated the existing aspect cat-
egory annotations of the corpus of Ganu et al.
(2009). The agreement with the existing anno-
tations was 92% measured as average F
1
. Most
disagreements concerned additions of missing as-
pect category annotations. Furthermore, the same
annotator validated and corrected (if needed) the
existing polarity labels per aspect category anno-
tation. The agreement for the polarity labels was
87% in terms of accuracy and it was measured
only on the common aspect category annotations.
The additional 800 sentences (not present in Ganu
et al.?s dataset) were used for testing and were an-
notated from scratch in the same manner. The dis-
tribution of the polarity classes per category is pre-
sented in Table 3. Again, ?positive? is the majority
polarity class while the dominant aspect category
is FOOD in both the training and test restaurant
sentences.
Determining the aspect categories of the sen-
tences and their polarities (Stage 2) was an easier
task compared to detecting aspect terms and their
polarities (Stage 1). The annotators needed less
time in Stage 2 and it was easier to reach agree-
ment. Exceptions were some sentences where it
was difficult to decide if the categories AMBIENCE
or ANECDOTES/MISCELLANEOUS applied (e.g.,
?One of my Fav spots in the city?). We instructed
the annotators to classify those sentences only in
ANECDOTES/MISCELLANEOUS, if they conveyed
4
In the original dataset of Ganu et al. (2009), ANECDOTES
and MISCELLANEOUS were separate categories, but in prac-
tice they were difficult to distinguish and we merged them.
30
Positive Negative Conflict Neutral Total
Category Train Test Train Test Train Test Train Test Train Test
FOOD 867 302 209 69 66 16 90 31 1232 418
PRICE 179 51 115 28 17 3 10 1 321 83
SERVICE 324 101 218 63 35 5 20 3 597 172
AMBIENCE 263 76 98 21 47 13 23 8 431 118
ANECD./MISC. 546 127 199 41 30 15 357 51 1132 234
Total 2179 657 839 159 163 52 500 94 3713 1025
Table 3: Aspect categories distribution per sentiment class.
general views about a restaurant, without explic-
itly referring to its atmosphere or environment.
3.3 Format and Availability of the Datasets
The datasets of the ABSA task were provided in
an XML format (see Fig. 3). They are avail-
able with a non commercial, no redistribution li-
cense through META-SHARE, a repository de-
voted to the sharing and dissemination of language
resources (Piperidis, 2012).
5
4 Evaluation Measures and Baselines
The evaluation of the ABSA task ran in two
phases. In Phase A, the participants were asked
to return the aspect terms (SB1) and aspect cate-
gories (SB3) for the provided test datasets. Subse-
quently, in Phase B, the participants were given
the gold aspect terms and aspect categories (as
in Fig. 3) for the sentences of Phase A and they
were asked to return the polarities of the aspect
terms (SB2) and the polarities of the aspect cate-
gories of each sentence (SB4).
6
Each participat-
ing team was allowed to submit up to two runs
per subtask and domain (restaurants, laptops) in
each phase; one constrained (C), where only the
provided training data and other resources (e.g.,
publicly available lexica) excluding additional an-
notated sentences could be used, and one uncon-
strained (U), where additional data of any kind
could be used for training. In the latter case, the
teams had to report the resources they used.
To evaluate aspect term extraction (SB1) and as-
pect category detection (SB3) in Phase A, we used
5
The datasets can be downloaded from http://
metashare.ilsp.gr:8080/. META-SHARE (http:
//www.meta-share.org/) was implemented in the
framework of the META-NET Network of Excellence
(http://www.meta-net.eu/).
6
Phase A ran from 9:00 GMT, March 24 to 21:00 GMT,
March 25, 2014. Phase B ran from 9:00 GMT, March 27 to
17:00 GMT, March 29, 2014.
the F
1
measure, defined as usually:
F
1
=
2 ? P ?R
P + R
(1)
where precision (P ) and recall (R) are defined as:
P =
|S ?G|
|S|
, R =
|S ?G|
|G|
(2)
Here S is the set of aspect term or aspect category
annotations (in SB1 and SB3, respectively) that a
system returned for all the test sentences (of a do-
main), and G is the set of the gold (correct) aspect
term or aspect category annotations.
To evaluate aspect term polarity (SB2) and as-
pect category polarity (SB4) detection in Phase B,
we calculated the accuracy of each system, defined
as the number of correctly predicted aspect term
or aspect category polarity labels, respectively, di-
vided by the total number of aspect term or aspect
category annotations. Recall that we used the gold
aspect term and category annotations in Phase B.
We provided four baselines, one per subtask:
7
Aspect term extraction (SB1) baseline: A se-
quence of tokens is tagged as an aspect term in
a test sentence (of a domain), if it is listed in a
dictionary that contains all the aspect terms of the
training sentences (of the same domain).
Aspect term polarity (SB2) baseline: For each
aspect term t in a test sentence s (of a particu-
lar domain), this baseline checks if t had been
encountered in the training sentences (of the do-
main). If so, it retrieves the k most similar to s
training sentences (of the domain), and assigns to
the aspect term t the most frequent polarity it had
in the k sentences. Otherwise, if t had not been en-
countered in the training sentences, it is assigned
the most frequent aspect term polarity label of the
7
Implementations of the baselines and further information
about the baselines are available at: http://alt.qcri.
org/semeval2014/task4/data/uploads/.
31
<sentence id="11351725#582163#9">
<text>Our waiter was friendly and it is a shame that he didnt have a supportive
staff to work with.</text>
<aspectTerms>
<aspectTerm term="waiter" polarity="positive" from="4" to="10"/>
<aspectTerm term="staff" polarity="negative" from="74" to="79"/>
</aspectTerms>
<aspectCategories>
<aspectCategory category="service" polarity="conflict"/>
</aspectCategories>
</sentence>
Figure 3: An XML snippet that corresponds to the annotated sentence of Fig. 2.
training set. The similarity between two sentences
is measured as the Dice coefficient of the sets of
(distinct) words of the two sentences. For exam-
ple, the similarity between ?this is a demo? and
?that is yet another demo? is
2?2
4+5
= 0.44.
Aspect category extraction (SB3) baseline: For
every test sentence s, the k most similar to s train-
ing sentences are retrieved (as in the SB2 base-
line). Then, s is assigned the m most frequent as-
pect category labels of the k retrieved sentences;
m is the most frequent number of aspect category
labels per sentence among the k sentences.
Aspect category polarity (SB4): This baseline
assigns to each aspect category c of a test sentence
s the most frequent polarity label that c had in the
k most similar to s training sentences (of the same
domain), considering only training sentences that
have the aspect category label c. Sentence similar-
ity is computed as in the SB2 baseline.
For subtasks SB2 and SB4, we also use a major-
ity baseline that assigns the most frequent polarity
(in the training data) to all the aspect terms and as-
pect categories. The scores of all the baselines and
systems are presented in Tables 4?6.
5 Evaluation Results
The ABSA task attracted 32 teams in total and 165
submissions (systems), 76 for phase A and 89 for
phase B. Based on the human-annotation experi-
ence, the expectations were that systems would
perform better in Phase B (SB3, SB4, involving
aspect categories) than in Phase A (SB1, SB2, in-
volving aspect terms). The evaluation results con-
firmed our expectations (Tables 4?6).
5.1 Results of Phase A
The aspect term extraction subtask (SB1) attracted
24 teams for the laptops dataset and 24 teams for
the restaurants dataset; consult Table 4.
Laptops Restaurants
Team F
1
Team F
1
IHS RD. 74.55? DLIREC 84.01*
DLIREC 73.78* XRCE 83.98
DLIREC 70.4 NRC-Can. 80.18
NRC-Can. 68.56 UNITOR 80.09
UNITOR 67.95* UNITOR 79.96*
XRCE 67.24 IHS RD. 79.62?
SAP RI 66.6 UWB 79.35*
IITP 66.55 SeemGo 78.61
UNITOR 66.08 DLIREC 78.34
SeemGo 65.99 ECNU 78.24
ECNU 65.88 SAP RI 77.88
SNAP 62.4 UWB 76.23
DMIS 60.59 IITP 74.94
UWB 60.39 DMIS 72.73
JU CSE. 59.37 JU CSE. 72.34
lsis lif 56.97 Blinov 71.21*
USF 52.58 lsis lif 71.09
Blinov 52.07* USF 70.69
UFAL 48.98 EBDG 69.28*
UBham 47.49 UBham 68.63*
UBham 47.26* UBham 68.51
SINAI 45.28 SINAI 65.41
EBDG 41.52* V3 60.43*
V3 36.62* UFAL 58.88
COMMIT. 25.19 COMMIT. 54.38
NILCUSP 25.19 NILCUSP 49.04
iTac 23.92 SNAP 46.46
iTac 38.29
Baseline 35.64 Baseline 47.15
Table 4: Results for aspect term extraction (SB1).
Stars indicate unconstrained systems. The ? indi-
cates a constrained system that was not trained on
the in-domain training dataset (unlike the rest of
the constrained systems), but on the union of the
two training datasets (laptops, restaurants).
32
Restaurants Restaurants
Team F
1
Team Acc.
NRC-Can. 88.57 NRC-Can. 82.92
UNITOR 85.26* XRCE 78.14
XRCE 82.28 UNITOR 76.29*
UWB 81.55* SAP RI 75.6
UWB 81.04 SeemGo 74.63
UNITOR 80.76 SA-UZH 73.07
SAP RI 79.04 UNITOR 73.07
SNAP 78.22 UWB 72.78
Blinov 75.27* UWB 72.78*
UBham 74.79* lsis lif 72.09
UBham 74.24 UBham 71.9
EBDG 73.98* EBDG 69.75
SeemGo 73.75 SNAP 69.56
SINAI 73.67 COMMIT. 67.7
JU CSE. 70.46 Blinov 65.65*
lsis lif 68.27 Ualberta. 65.46
ECNU 67.29 JU CSE. 64.09
UFAL 64.51 ECNU 63.41
V3 60.20* UFAL 63.21
COMMIT. 59.3 iTac 62.73*
iTac 56.95 ECNU 60.39*
SINAI 60.29
V3 47.21
Baseline 65.65
Baseline 63.89 Majority 64.09
Table 5: Results for aspect category detection
(SB3) and aspect category polarity (SB4). Stars
indicate unconstrained systems.
Overall, the systems achieved significantly
higher scores (+10%) in the restaurants domain,
compared to laptops. The best F
1
score (74.55%)
for laptops was achieved by the IHS RD. team,
which relied on Conditional Random Fields (CRF)
with features extracted using named entity recog-
nition, POS tagging, parsing, and semantic anal-
ysis. The IHS RD. team used additional reviews
from Amazon and Epinions (without annotated
terms) to learn the sentiment orientation of words
and they trained their CRF on the union of the
restaurant and laptop training data that we pro-
vided; the same trained CRF classifier was then
used in both domains.
The second system, the unconstrained system of
DLIREC, also uses a CRF, along with POS and
dependency tree based features. It also uses fea-
tures derived from the aspect terms of the train-
ing data and clusters created from additional re-
views from YELP and Amazon. In the restaurants
domain, the unconstrained system of DLIREC
ranked first with an F
1
of 84.01%, but the best
unconstrained system, that of XRCE, was very
close (83.98%). The XRCE system relies on a
parser to extract syntactic/semantic dependencies
(e.g., ?dissapointed???food?). For aspect term ex-
traction, the parser?s vocabulary was enriched with
the aspect terms of the training data and a term
list extracted from Wikipedia and Wordnet. A set
of grammar rules was also added to detect multi-
word terms and associate them with the corre-
sponding aspect category (e.g., FOOD, PRICE).
The aspect category extraction subtask (SB3)
attracted 18 teams. As shown in Table 5, the best
score was achieved by the system of NRC-Canada
(88.57%), which relied on five binary (one-vs-all)
SVMs, one for each aspect category. The SVMs
used features based on various types of n-grams
(e.g., stemmed) and information from a lexicon
learnt from YELP data, which associates aspect
terms with aspect categories. The latter lexicon
significantly improved F
1
. The constrained UN-
ITOR system uses five SVMs with bag-of-words
(BoW) features, which in the unconstrained sub-
mission are generalized using distributional vec-
tors learnt from Opinosis and TripAdvisor data.
Similarly, UWB uses a binary MaxEnt classifier
for each aspect category with BoW and TF-IDF
features. The unconstrained submission of UWB
also uses word clusters learnt using various meth-
ods (e.g., LDA); additional features indicate which
clusters the words of the sentence being classi-
fied come from. XRCE uses information identi-
fied by its syntactic parser as well as BoW features
to train a logistic regression model that assigns to
the sentence probabilities of belonging to each as-
pect category. A probability threshold, tuned on
the training data, is then used to determine which
categories will be assigned to the sentence.
5.2 Results of Phase B
The aspect term polarity detection subtask (SB2)
attracted 26 teams for the laptops dataset and 26
teams for the restaurants dataset. DCU and NRC-
Canada had the best systems in both domains (Ta-
ble 6). Their scores on the laptops dataset were
identical (70.48%). On the laptops dataset, the
DCU system performed slightly better (80.95%
vs. 80.15%). For SB2, both NRC-Canada and
DCU relied on an SVM classifier with features
33
mainly based on n-grams, parse trees, and sev-
eral out-of-domain, publicly available sentiment
lexica (e.g., MPQA, SentiWordnet and Bing Liu?s
Opinion Lexicon). NRC-Canada also used two
automatically compiled polarity lexica for restau-
rants and laptops, obtained from YELP and Ama-
zon data, respectively. Furthermore, NRC-Canada
showed by ablation experiments that the most use-
ful features are those derived from the sentiment
lexica. On the other hand, DCU used only publicly
available lexica, which were manually adapted by
filtering words that do not express sentiment in
laptop and restaurant reviews (e.g., ?really?) and
by adding others that were missing and do express
sentiment (e.g., ?mouthwatering?).
The aspect category polarity detection subtask
(SB4) attracted 20 teams. NRC-Canada again had
the best score (82.92%) using an SVM classifier.
The same feature set as in SB2 was used, but it
was further enriched to capture information re-
lated to each specific aspect category. The second
team, XRCE, used information from its syntactic
parser, BoW features, and an out-of-domain senti-
ment lexicon to train an SVM model that predicts
the polarity of each given aspect category.
6 Conclusions and Future Work
We provided an overview of Task 4 of SemEval-
2014. The task aimed to foster research in aspect-
based sentiment analysis (ABSA). We constructed
and released ABSA benchmark datasets contain-
ing manually annotated reviews from two domains
(restaurants, laptops). The task attracted 163 sub-
missions from 32 teams that were evaluated in four
subtasks centered around aspect terms (detecting
aspect terms and their polarities) and coarser as-
pect categories (assigning aspect categories and
aspect category polarities to sentences). The task
will be repeated in SemEval-2015 with additional
datasets and a domain-adaptation subtask.
8
In the
future, we hope to add an aspect term aggrega-
tion subtask (Pavlopoulos and Androutsopoulos,
2014a).
Acknowledgements
We thank Ioanna Lazari, who provided an ini-
tial version of the laptops dataset, Konstantina Pa-
panikolaou, who carried out a critical part of the
8
Consult http://alt.qcri.org/semeval2015/
task12/.
Laptops Restaurants
Team Acc. Team Acc.
DCU 70.48 DCU 80.95
NRC-Can. 70.48 NRC-Can. 80.15?
SZTE-NLP 66.97 UWB 77.68*
UBham 66.66 XRCE 77.68
UWB 66.66* SZTE-NLP 75.22
lsis lif 64.52 UNITOR 74.95*
USF 64.52 UBham 74.6
SNAP 64.06 USF 73.19
UNITOR 62.99 UNITOR 72.48
UWB 62.53 SeemGo 72.31
IHS RD. 61.62 lsis lif 72.13
SeemGo 61.31 UWB 71.95
ECNU 61.16 SA-UZH 70.98
ECNU 61.16* IHS RD. 70.81
SINAI 58.71 SNAP 70.81
SAP RI 58.56 ECNU 70.72
UNITOR 58.56* ECNU 70.72*
SA-UZH 58.25 INSIGHT. 70.72
COMMIT 57.03 SAP RI 69.92
INSIGHT. 57.03 EBDG 68.6
UMCC. 57.03* UMCC. 66.84*
UFAL 56.88 UFAL 66.57
UMCC. 56.11 UMCC. 66.57
EBDG 55.96 COMMIT 65.96
JU CSE. 55.65 JU CSE. 65.52
UO UA 55.19* Blinov 63.58*
V3 53.82 iTac 62.25*
Blinov 52.29* V3 59.78
iTac 51.83* SINAI 58.73
DLIREC 36.54 DLIREC 42.32*
DLIREC 36.54* DLIREC 41.71
IITP 66.97 IITP 67.37
Baseline 51.37 Baseline 64.28
Majority 52.14 Majority 64.19
Table 6: Results for the aspect term polarity sub-
task (SB2). Stars indicate unconstrained systems.
The ? indicates a constrained system that was not
trained on the in-domain training dataset (unlike
the rest of the constrained systems), but on the
union of the two training datasets. IITP?s original
submission files were corrupted; they were resent
and scored after the end of the evaluation period.
annotation process, and Juli Bakagianni, who sup-
ported our use of the META-SHARE platform.
We are also very grateful to the participants for
their feedback. Maria Pontiki and Haris Papageor-
giou were supported by the IS-HELLEANA (09-
34
72-922) and the POLYTROPON (KRIPIS-GSRT,
MIS: 448306) projects.
References
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812, Los An-
geles, California.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric K. Ringger. 2005. Pulse: Mining customer
opinions from free text. In IDA, pages 121?132,
Madrid, Spain.
Gayatree Ganu, Noemie Elhadad, and Am?elie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB, Providence, Rhode Island, USA.
Minqing Hu and Bing Liu. 2004a. Mining and sum-
marizing customer reviews. In Proceedings of KDD,
pages 168?177, Seattle, WA, USA.
Minqing Hu and Bing Liu. 2004b. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760, San Jose, California.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of COLING (Posters),
pages 766?774, Beijing, China.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, Pennsylvania,
USA.
John Pavlopoulos and Ion Androutsopoulos. 2014a.
Aspect term extraction for sentiment analysis: New
datasets, new evaluation measures and an improved
unsupervised method. In Proceedings of LASM-
EACL, pages 44?52, Gothenburg, Sweden.
John Pavlopoulos and Ion Androutsopoulos. 2014b.
Multi-granular aspect aggregation in aspect-based
sentiment analysis. In Proceedings of EACL, pages
78?87, Gothenburg, Sweden.
Stelios Piperidis. 2012. The META-SHARE language
resources sharing infrastructure: Principles, chal-
lenges, solutions. In Proceedings of LREC-2012,
pages 36?42, Istanbul, Turkey.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP, pages 339?346, Van-
couver, British Columbia, Canada.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. BRAT: a web-based tool for NLP-assisted
text annotation. In Proceedings of EACL, pages
102?107, Avignon, France.
Tun Thura Thet, Jin-Cheon Na, and Christopher S. G.
Khoo. 2010. Aspect-based sentiment analysis of
movie reviews on discussion boards. J. Information
Science, 36(6):823?848.
Ivan Titov and Ryan T. McDonald. 2008. A joint
model of text and aspect ratings for sentiment sum-
marization. In Proceedings of ACL, pages 308?316,
Columbus, Ohio, USA.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL, pages
417?424, Philadelphia, Pennsylvania, USA.
35
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1?11,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A New Sentence Compression Dataset and Its Use in an Abstractive
Generate-and-Rank Sentence Compressor
Dimitrios Galanis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Center ?Athena?, Greece
Abstract
Sentence compression has attracted much in-
terest in recent years, but most sentence com-
pressors are extractive, i.e., they only delete
words. There is a lack of appropriate datasets
to train and evaluate abstractive sentence com-
pressors, i.e., methods that apart from delet-
ing words can also rephrase expressions. We
present a new dataset that contains candi-
date extractive and abstractive compressions
of source sentences. The candidate compres-
sions are annotated with human judgements
for grammaticality and meaning preservation.
We discuss how the dataset was created, and
how it can be used in generate-and-rank ab-
stractive sentence compressors. We also re-
port experimental results with a novel abstrac-
tive sentence compressor that uses the dataset.
1 Introduction
Sentence compression is the task of producing a
shorter form of a grammatical source (input) sen-
tence, so that the new form will still be grammati-
cal and it will retain the most important information
of the source (Jing, 2000). Sentence compression is
useful in many applications, such as text summariza-
tion (Madnani et al, 2007) and subtitle generation
(Corston-Oliver, 2001). Methods for sentence com-
pression can be divided in two categories: extrac-
tive methods produce compressions by only remov-
ing words, whereas abstractive methods may addi-
tionally rephrase expressions of the source sentence.
Extractive methods are generally simpler and have
dominated the sentence compression literature (Jing,
2000; Knight and Marcu, 2002; McDonald, 2006;
Cohn and Lapata, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Nomoto, 2009; Galanis
and Androutsopoulos, 2010; Yamangil and Shieber,
2010). Abstractive methods, however, can in prin-
ciple produce shorter compressions that convey the
same information as longer extractive ones. Further-
more, humans produce mostly abstractive compres-
sions (Cohn and Lapata, 2008); hence, abstractive
compressors may generate more natural outputs.
When evaluating extractive methods, it suffices
to have a single human gold extractive compres-
sion per source sentence, because it has been shown
that measuring the similarity (as F1-measure of de-
pendencies) between the dependency tree of the
gold compression and that of a machine-generated
compression correlates well with human judgements
(Riezler et al, 2003; Clarke and Lapata, 2006a).
With abstractive methods, however, there is a much
wider range of acceptable abstractive compressions
of each source sentence, to the extent that a single
gold compression per source is insufficient. Indeed,
to the best of our knowledge no measure to com-
pare a machine-generated abstractive compression
to a single human gold compression has been shown
to correlate well with human judgements.
One might attempt to provide multiple human
gold abstractive compressions per source sentence
and employ measures from machine translation, for
example BLEU (Papineni et al, 2002), to compare
each machine-generated compression to all the cor-
responding gold ones. However, a large number of
gold compressions would be necessary to capture all
(or at least most) of the acceptable shorter rephras-
1
ings of the source sentences, and it is questionable
if human judges could provide (or even think of) all
the acceptable rephrasings. In machine translation,
n-gram-based evaluation measures like BLEU have
been criticized exactly because they cannot cope
sufficiently well with paraphrases (Callison-Burch
et al, 2006), which play a central role in abstractive
sentence compression (Zhao et al, 2009a).1
Although it is difficult to construct datasets for
end-to-end automatic evaluation of abstractive sen-
tence compression methods, it is possible to con-
struct datasets to evaluate the ranking components
of generate-and-rank abstractive sentence compres-
sors, i.e., compressors that first generate a large set
of candidate abstractive (and possibly also extrac-
tive) compressions of the source and then rank them
to select the best one. In previous work (Galanis and
Androutsopoulos, 2010), we presented a generate-
and-rank extractive sentence compressor, hereafter
called GA-EXTR, which achieved state-of-the art re-
sults. We aim to construct a similar abstractive
generate-and-rank sentence compressor. As part of
this endeavour, we needed a dataset to automatically
test (and train) several alternative ranking compo-
nents. In this paper, we introduce a dataset of this
kind, which we also make publicly available.2
The dataset consists of pairs of source sentences
and candidate extractive or abstractive compres-
sions. The candidate compressions were generated
by first using GA-EXTR and then applying exist-
ing paraphrasing rules (Zhao et al, 2009b) to the
best extractive compressions of GA-EXTR. Each pair
(source and candidate compression) was then scored
by a human judge for grammaticality and meaning
preservation. We discuss how the dataset was con-
structed and how we established upper and lower
performance boundaries for ranking components of
compressors that may use it. We also present the
1Ways to extend n-gram measures to account for para-
phrases have been proposed (Zhou et al, 2006; Kauchak and
Barzilay, 2006; Pado? et al, 2009), but they require accu-
rate paraphrase recognizers (Androutsopoulos and Malakasio-
tis, 2010), which are not yet available; or they assume that
the same paraphrase generation resources (Madnani and Dorr,
2010), for example paraphrasing rules, that some abstractive
sentence compressors (including ours) use always produce ac-
ceptable paraphrases, which is not the case as discussed below.
2The new dataset and GA-EXTR are freely available from
http://nlp.cs.aueb.gr/software.html.
current version of our abstractive sentence compres-
sor, and we discuss how its ranking component was
improved by performing experiments on the dataset.
Section 2 below summarizes prior work on ab-
stractive sentence compression. Section 3 discusses
the dataset we constructed. Section 4 describes our
abstractive sentence compressor. Section 5 presents
our experimental results, and Section 6 concludes.
2 Prior work on abstractive compression
The first abstractive compression method was pro-
posed by Cohn and Lapata (2008). It learns a set of
parse tree transduction rules from a training dataset
of pairs, each pair consisting of a source sentence
and a single human-authored gold abstractive com-
pression. The set of transduction rules is then aug-
mented by applying a pivoting approach to a par-
allel bilingual corpus; we discuss similar pivoting
mechanisms below. To compress a new sentence, a
chart-based decoder and a Structured Support Vec-
tor Machine (Tsochantaridis et al, 2005) are used to
select the best abstractive compression among those
licensed by the rules learnt.
The dataset that Cohn and Lapata (2008) used
to learn transduction rules consists of 570 pairs of
source sentences and abstractive compressions. The
compressions were produced by humans who were
allowed to use any transformation they wished. We
used a sample of 50 pairs from that dataset to con-
firm that humans produce mostly abstractive com-
pressions. Indeed, 42 (84%) of the compressions
were abstractive, and only 7 (14%) were simply ex-
tractive.3 We could not use that dataset, however,
for automatic evaluation purposes, since it only pro-
vides a single human gold abstract compression per
source, which is insufficient as already discussed.
More recently, Zhao et al (2009a) presented a
sentence paraphrasing method that can be config-
ured for different tasks, including a form of sentence
compression. For each source sentence, Zhao et al?s
method uses a decoder to produce the best possible
paraphrase, much as in phrase-based statistical ma-
chine translation (Koehn, 2009), but with phrase ta-
bles corresponding to paraphrasing rules (e.g., ?X
3Cohn and Lapata?s dataset is available from http://
staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/#
Corpus. One pair (2%) of our sample had a ?compression?
that was identical to the input.
2
is the author of Y ? ? ?X wrote Y ?) obtained from
parallel and comparable corpora (Zhao et al, 2008).
The decoder uses a log-linear objective function, the
weights of which are estimated with a minimum er-
ror rate training approach (Och, 2003). The objec-
tive function combines a language model, a para-
phrase model (combining the quality scores of the
paraphrasing rules that turn the source into the can-
didate paraphrase), and a task-specific model; in the
case of sentence compression, the latter model re-
wards shorter candidate paraphrases.
We note that Zhao et al?s method (2009a) is in-
tended to produce paraphrases, even when config-
ured to prefer shorter paraphrases, i.e., the compres-
sions are still intended to convey the same informa-
tion as the source sentences. By contrast, most sen-
tence compression methods (both extractive and ab-
stractive, including ours) are expected to retain only
the most important information of the source sen-
tence, in order to achieve better compression rates.
Hence, Zhao et al?s sentence compression task is not
the same as the task we are concerned with, and the
compressions we aim for are significantly shorter.
3 The new dataset
To construct the new dataset, we used source sen-
tences from the 570 pairs of Cohn and Lapata (Sec-
tion 2). This way a human gold abstractive com-
pression is also available for each source sentence,
though we do not currently use the gold compres-
sions in our experiments. We actually used only 346
of the 570 source sentences of Cohn and Lapata, re-
serving the remaining 224 for further experiments.4
To obtain candidate compressions, we first ap-
plied GA-EXTR to the 346 source sentences, and we
then applied the paraphrasing rules of Zhao et al
(2009b) to the resulting extractive compressions; we
provide more information about GA-EXTR and the
paraphrasing rules below. We decided to apply para-
phrasing rules to extractive compressions, because
we noticed that most of the 42 human abstractive
compressions of the 50 sample pairs from Cohn and
Lapata?s dataset that we initially considered (Sec-
tion 2) could be produced from the corresponding
source sentences by first deleting words and then us-
4The 346 sources are from 19 randomly selected articles
among the 30 that Cohn and Lapata drew source sentences from.
ing shorter paraphrases, as in the following example.
source: Constraints on recruiting are constraints on
safety and have to be removed.
extractive: Constraints on recruiting have to be re-
moved.
abstractive: Recruiting constraints must be removed.
3.1 Extractive candidate compressions
GA-EXTR, which we first applied to the dataset?s
source sentences, generates extractive candidate
compressions by pruning branches of each source?s
dependency tree; a Maximum Entropy classifier is
used to guide the pruning. Subsequently, GA-EXTR
ranks the extractive candidates using a Support Vec-
tor Regression (SVR) model, which assigns a score
F (eij |si) to each candidate extractive compression
eij of a source sentence si by examining features
of si and eij ; consult our previous work (Galanis
and Androutsopoulos, 2010) for details.5 For each
source si, we kept the (at most) kmax = 10 extrac-
tive candidates eij with the highest F (eij |si) scores.
3.2 Abstractive candidate compressions
We then applied Zhao et al?s (2009b) paraphrasing
rules to each one of the extractive compressions eij .
The rules are of the form left ? right, with left and
right being sequences of words and slots; the slots
are part-of-speech tagged and they can be filled in
with words of the corresponding categories. Exam-
ples of rules are shown below.
? get rid of NNS1? remove NNS1
? get into NNP1? enter NNP1
? NNP1 was written by NNP2? NNP2 wrote NNP1
Roughly speaking, the rules were extracted from
a parallel English-Chinese corpus, based on the as-
sumption that two English phrases ?1 and ?2 that
are often aligned to the same Chinese phrase ? are
5We trained GA-EXTR on approximately 1,050 pairs of
source sentences and gold human extractive compressions,
obtained from Edinburgh?s ?written? extractive dataset; see
http://jamesclarke.net/research/resources.
The source sentences of that dataset are from 82 documents.
The 1,050 pairs that we used had source sentences from 52 out
of the 82 documents. We did not use source sentences from
the other 30 documents, because they were used by Cohn and
Lapata (2008) to build their abstractive dataset (Section 2),
from which we drew source sentences for our dataset.
3
si
 '' ,,ei1
 '' ,,
ei2
,, -- ..
? ? ? eik
-- ..ai1.1
 && ,,
ai1.2
,,
? ? ? ai1.mi1 ai2.1 ai2.2 ? ? ? ai2.mi2 ? ? ? aik.1 ? ? ? aik.mik
ai1.1.1

ai1.1.2 ? ? ? ai1.1.mi1.1 ai1.2.1 ? ? ?
? ? ?
Figure 1: Generating candidate extractive (eij) and abstractive (aij...) compressions from a source sentence (si).
likely to be paraphrases and, hence, can be treated
as a paraphrasing rule ?1 ? ?2. This pivoting was
used, for example, by Bannard and Callison-Burch
(2005), and it underlies several other paraphrase
extraction methods (Riezler et al, 2007; Callison-
Burch, 2008; Kok and Brockett, 2010). Zhao et
al. (2009b) provide approximately one million rules,
but we use only approximately half of them, because
we use only rules that can shorten a sentence, and
only in the direction that shortens the sentence.
From each extractive candidate eij , we pro-
duced abstractive candidates aij.1, aij.2, . . . , aij.mij
(Figure 1) by applying a single (each time
different) applicable paraphrasing rule to eij .
From each of the resulting abstractive candidates
aij.l, we produced further abstractive candidates
aij.l.1, aij.l.2, . . . , aij.l.mij.l by applying again a sin-
gle (each time different) rule. We repeated this pro-
cess in a breadth-first manner, allowing up to at most
rulemax = 5 rule applications to an extractive candi-
date eij , i.e., up to depth six in Figure 1, and up to
a total of abstrmax = 50 abstractive candidates per
eij . Zhao et al (2009b) associate each paraphrasing
rule with a score, intended to indicate its quality.6
Whenever multiple paraphrasing rules could be ap-
plied, we applied the rule with the highest score first.
3.3 Human judgement annotations
For each one of the 346 sources si, we placed its
extractive (at most kmax = 10) and abstractive (at
most abstrmax = 50) candidate compressions into
a single pool (extractive and abstractive together),
and we selected from the pool the (at most) 10 can-
didate compressions cij with the highest language
6Each rule is actually associated with three scores. We use
the ?Model 1? score; see Zhao et al (2009b) for details.
model scores, computed using a 3-gram language
model.7 For each cij , we formed a pair ?si, cij?,
where si is a source sentence and cij a candidate
(extractive or abstractive) compression. This led to
3,072 ?si, cij? pairs. Each pair was given to a human
judge, who scored it for grammaticality (how gram-
matical cij was) and meaning preservation (to what
extent cij preserved the most important information
of si). Both scores were provided on a 1?5 scale (1
for rubbish, 5 for perfect). The dataset that we use
in the following sections and that we make publicly
available comprises the 3,072 pairs and their gram-
maticality and meaning preservation scores.
We define the GM score of an ?si, cij? pair to be
the sum of its grammaticality and meaning preser-
vation scores. Table 1 shows the distribution of
GM scores in the 3,072 pairs. Low GM scores (2?
5) are less frequent than higher scores (6?10), but
this is not surprising given that we selected pairs
whose cij had high language model scores, that
we used the kmax extractive compressions of each
si that GA-EXTR considered best, and that we as-
signed higher preference to applying paraphrasing
rules with higher scores. We note, however, that ap-
plying a paraphrasing rule does not necessarily pre-
serve neither grammaticality nor meaning, even if
the rule has a high score. Szpektor et al (2008) point
out that, for example, a rule like ?X acquire Y ??
?X buy Y ? may work well in many contexts, but
not in ?Children acquire language quickly?. Sim-
ilarly, ?X charged Y with? ? ?X accused Y of?
should not be applied to sentences about batteries.
Many (but not all) inappropriate rule applications
7We used SRILM with modified Kneser-Ney smoothing
(Stolcke, 2002). We trained the language model on approxi-
mately 4.5 million sentences from the TIPSTER corpus.
4
Training part Test part
GM extractive abstractive total extractive abstractive total
score candidates candidates candidates candidates candidates candidates
2 13 (1.3%) 10 (1.3%) 23 (1.3%) 19 (1.9%) 2 (0.4%) 21 (1.5%)
3 26 (2.7%) 28 (3.6%) 54 (3.1%) 10 (1.0%) 0 (0%) 10 (0.7%)
4 55 (5.8%) 29 (5.1%) 94 (5.5%) 51 (5.3%) 26 (6.2%) 77 (5.5%)
5 52 (5.5%) 65 (8.5%) 117 (6.9%) 77 (8.0%) 42 (10.0%) 119 (8.6%)
6 102 (10.9%) 74 (9.7%) 176 (10.3%) 125 (13.0%) 83 (19.8%) 208 (15.1%)
7 129 (13.8%) 128 (16.8%) 257 (15.1%) 151 (15.7%) 53 (12.6%) 204 (14.8%)
8 157 (16.8%) 175 (23.0%) 332 (19.5%) 138 (14.3%) 85 (20.3%) 223 (16.1%)
9 177 (18.9%) 132 (17.3%) 309 (18.2%) 183 (19.0%) 84 (20.1%) 267 (19.3%)
10 223 (23.8%) 110 (14.4%) 333 (19.6%) 205 (21.3%) 43 (10.2%) 248 (18.0%)
total 934 (55.1%) 761 (44.9%) 1,695 (100%) 959 (69.6%) 418 (30.4%) 1,377 (100%)
Table 1: Distribution of GM scores (grammaticality plus meaning preservation) in our dataset.
lead to low language model scores, which is partly
why there are more extractive than abstractive can-
didate compressions in the dataset; another reason is
that few or no paraphrasing rules apply to some of
the extractive candidates.
We use 1,695 (from 188 source sentences) of the
3,072 pairs to train different versions of our abstrac-
tive compressor?s ranking component, discussed be-
low, and 1,377 pairs (from 158 sources) as a test set.
3.4 Inter-annotator agreement
Although we used a total of 16 judges (computer sci-
ence graduate students), each one of the 3,072 pairs
was scored by a single judge, because a prelimi-
nary study indicated reasonably high inter-annotator
agreement.8 More specifically, before the dataset
was constructed, we created 161 ?si, cij? pairs (from
22 source sentences) in the same way, and we gave
them to 3 of the 16 judges. Each pair was scored by
all three judges. The average (over pairs of judges)
Pearson correlation of the grammaticality, meaning
preservation, and GM scores, was 0.63, 0.60, and
0.69, respectively.9 We conjecture that the higher
correlation of GM scores, compared to grammati-
cality and meaning preservation, is due to the fact
that when a candidate compression looks bad the
judges sometimes do not agree if they should re-
duce the grammaticality or the meaning preservation
8The judges were fluent, but not native, English speakers.
9The Pearson correlation ranges in [?1,+1] and measures
the linear relationship of two variables. A correlation of +1 in-
dicates perfect positive relationship, while ?1 indicates perfect
negative relationship; a correlation of 0 signals no relationship.
candidate average Pearson
compressions correlation
Extractive 112 0.71
Abstractive 49 0.64
All 161 0.69
Table 2: Inter-annotator agreement on GM scores.
score, but the difference does not show up in the GM
score (the sum). Table 2 shows the average corre-
lation of the GM scores of the three judges on the
161 pairs, and separately for pairs that involved ex-
tractive or abstractive candidate compressions. The
judges agreed more on extractive candidates, since
the paraphrasing stage that is involved in the abstrac-
tive candidates makes the task more subjective.10
3.5 Performance boundaries
When presented with two pairs ?si, cij? and?
si, cij?
?
with the same si and equally long cij and
cij? , an ideal ranking component should prefer the
pair with the highest GM score. More generally, to
consider the possibly different lengths of cij and cij? ,
we first define the compression rate CR(cij |si) of a
candidate compression cij as follows, where |?| is
length in characters; lower values of CR are better.
CR(cij |si) =
|cij |
|si|
The GMC? score of a candidate compression, which
also considers the compression rate by assigning it a
10The correlation that we measured on extractive candidates
(0.71) is very close to the corresponding figure (0.746) that has
been reported by Clarke and Lapata (2006b).
5
Figure 2: Results of three SVR-based ranking components on our dataset, along with performance boundaries obtained
using an oracle and a random baseline. The right diagram shows how the performance of our best SVR-based ranking
component is affected when using only 33% and 63% of the training examples.
weight ?, is then defined as follows.
GMC?(cij |si) = GM(cij |si)? ? ? CR(cij |si)
For a given ?, when presented with ?si, cij? and?
si, cij?
?
, an ideal ranking component should prefer
the pair with the highest GMC? score.
The upper curve of the left diagram of Figure 2
shows the performance of an ideal ranking com-
ponent, an oracle, on the test part of the dataset.
For every source si, the oracle selects the ?si, cij?
pair (among the at most 10 pairs of si) for which
GMC?(cij |si) is maximum; if two pairs have iden-
tical GMC? scores, it prefers the one with the low-
est CR(cij |si). The vertical axis shows the average
GM(cij |si) score of the selected pairs, for all the si
sources, and the horizontal axis shows the average
CR(cij |si). Different points of the curve are obtained
by using different ? values. As the selected candi-
dates get shorter (lower compression rate), the aver-
age GM score decreases, as one would expect.11
11The discontinuity in the oracle?s curve for average com-
The other curves of Figure 2 correspond to al-
ternative ranking components that we tested, dis-
cussed below, which do not consult the judges? GM
scores. For each si, these ranking components at-
tempt to guess the GM scores of the ?si, cij? pairs
that are available for si, and they then rank the pairs
by GMC? using the guessed GM scores. The lower
points of the left diagram were obtained with a base-
line ranking component that assigns a random GM
score to each pair. The oracle and the baseline can
be seen as establishing upper and lower performance
boundaries of ranking components on our dataset.
4 Our abstractive compressor
Our abstractive sentence compressor operates in two
stages. Given a source sentence si, extractive and
pression rates above 0.7, i.e., when long compressions are only
mildly penalized, is caused by the fact that many long candi-
date compressions have high and almost equal GM scores, but
still very different compression rates; hence, a slight modifica-
tion of ? leads the oracle to select candidates with the same GM
scores, but very different compression rates.
6
abstractive candidate compressions are first gener-
ated as in Sections 3.1 and 3.2. In a second stage, a
ranking component is used to select the best candi-
date. Below we discuss the three SVR-based ranking
components that we experimented with.
4.1 Ranking candidates with an SVR
An SVR is very similar to a Support Vector Machine
(Vapnik, 1998; Cristianini and Shawe-Taylor, 2000;
Joachims, 2002), but it is trained on examples of the
form ?xl, y(xl)?, where each xl ? Rn is a vector of n
features, and y(xl) ? R. The SVR learns a function
f : Rn ? R intended to return f(x) values as close
as possible to the correct y(x) values.12 In our case,
each vector xij contains features providing informa-
tion about an ?si, cij? pair of a source sentence si
and a candidate compression cij . For pairs that have
been scored by human judges, the f(xij) returned by
the SVR should ideally be y(xij) = GMC?(cij |si);
once trained, however, the SVR may be presented
with xij vectors of unseen ?si, cij? pairs.
For an unseen source si, our abstractive compres-
sor first generates extractive and abstractive candi-
dates cij , it then forms the vectors xij of all the
pairs ?si, cij?, and it returns the cij for which the
SVR?s f(xij) is maximum. On a test set (like the
test part of our dataset), if the f(xij) values the
SVR returns are very close to the corresponding
y(xij) = GMC?(cij |si) scores, the ranking compo-
nent will tend to select the same cij for each si as the
oracle, i.e., it will achieve optimum performance.
4.2 Base form of our SVR ranking component
The simplest form of our SVR-based ranking compo-
nent, called SVR-BASE, uses vectors xij that include
the following features of ?si, cij?. Hereafter, if cij is
an extractive candidate, then e(cij) = cij ; otherwise
e(cij) is the extractive candidate that cij was derived
from by applying paraphrasing rules.13
? The language model score of si and cij (2 fea-
12We use LIBSVM (http://www.csie.ntu.edu.tw/
?cjlin/libsvm) with an RBF kernel, which permits the
SVR to learn non-linear functions. We also experimented with a
ranking SVM, but the results were slightly inferior.
13All the feature values are normalized in [0, 1]; this also ap-
plies to the GMC? scores when they are used by the SVR. The
e(cij) of each cij and the paraphrasing rules that were applied
to e(cij) to produce cij are also included in the dataset.
tures), computed as in Section 3.3.
? The F (e(cij)|si) score that GA-EXTR returned.
? The compression rate CR(e(cij)|si).
? The number (possibly zero) of paraphrasing
rules that were applied to e(cij) to produce cij .
4.3 Additional PMI-based features
For two words w1, w2, their PMI score is:
PMI(w1, w2) = log
P (w1, w2)
P (w1) ? P (w2)
where P (w1, w2) is the probability of w1, w2 co-
occurring; we require them to co-occur in the same
sentence at a maximum distance of 10 tokens.14
If w1, w2 are completely independent, then their
PMI score is zero. If they always co-occur, their
PMI score is maximum, equal to ? logP (w1) =
? logP (w2).15 We use PMI to assess if the words
of a candidate compression co-occur as frequently
as those of the source sentence; if not, this may indi-
cate an inappropriate application of a paraphrasing
rule (e.g., having replaced ?charged Y with? by ?X
accused Y of? in a sentence about batteries).
More specifically, we define the PMI(?) score of
a sentence ? to be the average PMI(wi, wj) of ev-
ery two content words wi, wj that co-occur in ? at
a maximum distance of 10 tokens; below N is the
number of such pairs.
PMI(?) =
1
N
?
?
i,j
PMI(wi, wj)
In our second SVR-based ranking component, SVR-
PMI, we compute PMI(si), PMI(e), and PMI(cij),
and we include them as three additional features;
otherwise SVR-PMI is identical to SVR-BASE.
14We used texts from TIPSTER and AQUAINT, a total of 953
million tokens, to estimate PMI(w1, w2).
15A problem with PMI is that two frequent and completely de-
pendent words receive lower scores than two other, less frequent
completely dependent words (Manning and Schutze, 2000).
Pecina (2005), however, found PMI to be the best collocation
extraction measure; and Newman et al (2010) found it to be the
best measure of ?topical coherence? for sets of words.
7
4.4 Additional LDA-based features
Our third SVR-based ranking component includes
features from a Latent Dirichlet Allocation (LDA)
model (Blei et al, 2003). Roughly speaking, LDA
models assume that each document d of |d| words
w1, . . . , w|d| is generated by iteratively (for r =
1, . . . , |d|) selecting a topic tr from a document-
specific multinomial distribution P (t|d) overK top-
ics, and then (for each r) selecting a word wr from a
topic-specific multinomial distribution P (w|t) over
the vocabulary.16 The probability, then, of encoun-
tering a word w in a document d is the following.
P (w|d) =
?
t
P (w|t) ? P (t|d) (1)
An LDA model can be trained on a corpus to estimate
the parameters of the distributions it involves; and
given a trained model, there are methods to infer the
topic distribution P (t|d?) of a new document d?.17
In our case, we treat each source sentence as
a new document d?, and we use an LDA model
trained on a generic corpus to infer the topic distri-
bution P (t|d?) of the source sentence.18 We assume
that a good candidate compression should contain
words with high P (w|d?), computed as in Equation
1 with P (t|d) = P (t|d?) and using the P (w|t) that
was learnt during training, because words with high
P (w|d?) are more likely to express (high P (w|t))
prominent topics (high P (t|d?)) of the source.
Consequently, we can assess how good a can-
didate compression is by computing the average
P (w|d?) of its words; we actually compute the
average logP (w|d?). More specifically, for a
given source si and another sentence ?, we define
LDA(?|si) as follows (d? = si), where w1, . . . , w|?|
are now the words of ?, ignoring stop-words.
LDA(?|si) =
1
|?|
?
|?|?
r=1
logP (wr|si)
16The document-specific parameters of the first multinomial
distribution are drawn from a Dirichlet distribution.
17We use MALLET (http://mallet.cs.umass.edu),
with Gibbs sampling (Griffiths and Steyvers, 2004). We
set K = 800, having first experimented with K =
200, 400, 600, 800, 1000.
18We trained the LDA model on approximately 106,000 arti-
cles from the TIPSTER and AQUAINT corpora.
In our third SVR-based ranking component, SVR-
PMI-LDA, the feature vector xij of each ?si, cij?
pair includes LDA(cij |si), LDA(e(cij)|si), and
LDA(si|si) as additional features; otherwise, SVR-
PMI-LDA is identical to SVR-PMI. The third feature
allows the SVR to check how far LDA(cij |si) and
LDA(e(cij)|si) are from LDA(si|si).
5 Experiments
To assess the performance of SVR-BASE, SVR-PMI,
and SVR-PMI-LDA, we trained the three SVR-based
ranking components on the training part of our
dataset, and we evaluated them on the test part. We
repeated the experiments for 81 different ? values to
obtain average GM scores at different average com-
pression rates (Section 3.5). The resulting curves
of the three SVR-based ranking components are in-
cluded in Figure 2 (left diagram). Overall, SVR-
PMI-LDA performed better than SVR-PMI and SVR-
BASE, since it achieved the best average GM scores
throughout the range of average compression rates.
In general, SVR-PMI also performed better than
SVR-BASE, though the average GM score of SVR-
BASE was sometimes higher. All three SVR-based
ranking components performed better than the ran-
dom baseline, but worse than the oracle; hence, there
is scope for further improvements in the ranking
components, which is also why we believe other re-
searchers may wish to experiment with our dataset.
The oracle selected abstractive (as opposed to
simply extractive) candidates for 20 (13%) to 30
(19%, depending on ?) of the 158 source sentences
of the test part; the same applies to the SVR-based
ranking components. Hence, good abstractive can-
didates (or at least better than the corresponding ex-
tractive ones) are present in the dataset. Humans,
however, produce mostly abstractive compressions,
as already discussed; the fact that the oracle (which
uses human judgements) does not select abstrac-
tive candidates more frequently may be an indica-
tion that more or better abstractive candidates are
needed. We plan to investigate alternative methods
to produce more abstractive candidates. For exam-
ple, one could translate each source to multiple pivot
languages and back to the original language by using
multiple commercial machine translation engines in-
stead of, or in addition to applying paraphrasing
8
source generated
Gillette was considered a leading financial analyst on the beverage in-
dustry - one who also had an expert palate for wine tasting.
Gillette was seen as a leading financial analyst on the beverage industry
- one who also had an expert palate.
Nearly 200,000 lawsuits were brought by women who said they suf-
fered injuries ranging from minor inflammation to infertility and in
some cases, death.
Lawsuits were made by women who said they suffered injuries ranging
from inflammation to infertility in some cases, death.
Marcello Mastroianni, the witty, affable and darkly handsome Italian
actor who sprang on international consciousness in Federico Fellini?s
1960 classic ?La Dolce Vita,? died Wednesday at his Paris home.
Marcello Mastroianni died Wednesday at his home.
A pioneer in laparoscopy, he held over 30 patents for medical instru-
ments used in abdominal surgery such as tubal ligations.
He held over 30 patents for the medical tools used in abdominal surgery.
LOS ANGELES - James Arnold Doolittle, a Los Angeles dance im-
presario who brought names such as Joffrey and Baryshnikov to local
dance stages and ensured that a high-profile ?Nutcracker Suite? was
presented here every Christmas, has died.
James Arnold Doolittle, a Los Angeles dance impresario is dead.
After working as a cashier for a British filmmaker in Rome, he joined
an amateur theatrical group at the University of Rome, where he was
taking some classes.
After working as a cashier for a British filmmaker in Rome, he joined
an amateur group at the University of Rome, where he was using some
classes.
He was a 1953 graduate of the Johns Hopkins Medical School and after
completing his residency in gynecology and surgery, traveled to Den-
mark where he joined the staff of the National Cancer Center there.
He was a graduate of the Johns Hopkins Medical School and traveled
to Denmark where he joined a member of the National Cancer Center
there.
Mastroianni, a comic but also suave and romantic leading man in some
120 motion pictures, had suffered from pancreatic cancer.
Mastroianni, a leading man in some 120 motion pictures, had subjected
to cancer.
Table 3: Examples of good (upper five) and bad (lower three) compressions generated by our abstractive compressor.
rules. An approach of this kind has been proposed
for sentence paraphrasing (Zhao et al, 2010).
The right diagram of Figure 2 shows how the per-
formance of SVR-PMI-LDA is affected when using
33% or 63% of the training ?si, ci? pairs. As more
examples are used, the performance improves, sug-
gesting that better results could be obtained by using
more training data. Finally, Table 3 shows examples
of good and bad compressions the abstractive com-
pressor produced with SVR-PMI-LDA.
6 Conclusions and future work
We presented a new dataset that can be used to train
and evaluate the ranking components of generate-
and-rank abstractive sentence compressors. The
dataset contains pairs of source sentences and can-
didate extractive or abstractive compressions. The
candidate compressions were obtained by first ap-
plying a state-of-the-art extractive compressor to the
source sentences, and then applying existing para-
phrasing rules, obtained from parallel corpora. The
dataset?s pairs have been scored by human judges
for grammaticality and meaning preservation. We
discussed how performance boundaries for ranking
components that use the dataset can be established
by using an oracle and a random baseline, and by
considering different compression rates. We also
discussed the current version of an abstractive sen-
tence compressor that we are developing, and how
the dataset was used to train and evaluate three dif-
ferent SVR-based ranking components of the com-
pressor with gradually more elaborate features sets.
The feature set of the best ranking component that
we tested includes language model scores, the con-
fidence and compression rate of the underlying ex-
tractive compressor, the number of paraphrasing
rules that have been applied, word co-occurrence
features, as well as features based on an LDA model.
In future work, we plan to improve our abstractive
sentence compressor, possibly by including more
features in the ranking component. We also plan
to investigate alternative ways to produce candidate
compressions, such as sentence paraphrasing meth-
ods that exploit multiple commercial machine trans-
lation engines to translate the source sentences to
multiple pivot languages and back to the original
language (Zhao et al, 2010). Using methods of this
kind, it may be possible to produce a second, alterna-
tive dataset with more and possibly better abstractive
candidates. We also plan to make the final version of
our abstractive compressor publicly available.
Acknowledgments
This work was partly carried out during INDIGO, an
FP6 IST project funded by the European Union, with
additional funding from the Greek General Secre-
9
tariat of Research and Technology.19
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of
ACL, pages 597?604, Ann Arbor, MI.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. In Journal of Machine Learning Research.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation
research. In Proceedings of EACL, pages 249?256,
Trento, Italy.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of EMNLP, pages 196?205, Honolulu, HI.
J. Clarke and M. Lapata. 2006a. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of ACL-COLING.
J. Clarke and M. Lapata. 2006b. Models for sentence
compression: A comparison across domains, training
requirements and evaluation measures. In Proceedings
of ACL-COLING.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
1(31):399?429.
T. Cohn and M. Lapata. 2007. Large margin syn-
chronous generation and its application to sentence
compression. In Proceedings of EMNLP-CONLL.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING.
T. Cohn and M. Lapata. 2009. Sentence compression as
tree to tree tranduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
S. Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the NAACL
Workshop on Automatic Summarization.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
D. Galanis and I. Androutsopoulos. 2010. An extrac-
tive supervised two-stage method for sentence com-
pression. In Proceedings of HLT-NAACL.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In Proceedings of the National Academy of Sci-
ences.
19Consult http://www.ics.forth.gr/indigo/.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP.
T. Joachims. 2002. Learning to Classify Text Using Sup-
port Vector Machines: Methods, Theory, Algorithms.
Kluwer.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the HLT-
NAACL, pages 455?462, New York, NY.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probalistic approach to
sentence compression. Artificial Intelligence, 139(1).
P. Koehn. 2009. Statistical Machine Translation. Cam-
bridge University Press.
S. Kok and C. Brockett. 2010. Hitting the right para-
phrases in good time. In Proceedings of HLT-NAACL,
pages 145?153, Los Angeles, CA.
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions
for automatic text summarization. In Proceedings of
DUC.
C.D. Manning and H. Schutze. 2000. Foundations of
Statistical Natural Language Processing. MIT Press.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010.
Automatic evaluation of topic coherence. In Proceed-
ings of HLT-NAACL.
T. Nomoto. 2009. A comparison of model free versus
model intensive approaches to sentence compression.
In Proceedings of EMNLP.
J. F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Robust machine translation evaluation with en-
tailment features. In Proceedings of ACL-IJCNLP,
pages 297?305, Singapore.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, PA.
P. Pecina. 2005. An extensive empirical study of colloca-
tion extraction methods. In Proceedings of the Student
Research Workshop of ACL.
S. Riezler, T.H. King, R. Crouch, and A. Zaenen.
2003. Statistical sentence condensation using ambigu-
ity packing and stochastic disambiguation methods for
lexical-functional grammar. In Proceedings of HLT-
NAACL.
10
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proceedings of ACL,
pages 464?471, Prague, Czech Republic.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, pages 901?
904.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL-HLT, pages 683?691, Columbus, OH.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for indepen-
dent and structured output spaces. Machine Learning
Research, 6:1453?1484.
V. Vapnik. 1998. Statistical Learning Theory. John Wi-
ley.
E. Yamangil and S. M. Shieber. 2010. Bayesian syn-
chronous tree-substitution grammar induction and its
application to sentence compression. In Proceedings
of ACL.
S. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008. Com-
bining multiple resources to improve SMT-based para-
phrasing model. In Proceedings of ACL-HLT, pages
1021?1029, Columbus, OH.
S. Zhao, X. Lan, T. Liu, and S. Li. 2009a. Application-
driven statistical paraphrase generation. In Proceed-
ings of ACL.
S. Zhao, H. Wang, T. Liu, and S. Li. 2009b. Extract-
ing paraphrase patterns from bilingual parallel cor-
pora. Natural Language Engineering, 15(4):503?526.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010. Leverag-
ing multiple MT engines for paraphrase generation. In
Proceedings of COLING.
L. Zhou, C.-Y. Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP, pages 77?84.
11
Proceedings of the 14th European Workshop on Natural Language Generation, pages 51?60,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Using Integer Linear Programming for Content Selection, Lexicalization,
and Aggregation to Produce Compact Texts from OWL Ontologies
Gerasimos Lampouras and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
http://nlp.cs.aueb.gr/
Abstract
We present an Integer Linear Program-
ming model of content selection, lexical-
ization, and aggregation that we devel-
oped for a system that generates texts from
OWL ontologies. Unlike pipeline archi-
tectures, our model jointly considers the
available choices in these three text gen-
eration stages, to avoid greedy decisions
and produce more compact texts. Experi-
ments with two ontologies confirm that it
leads to more compact texts, compared to
a pipeline with the same components, with
no deterioration in the perceived quality of
the generated texts. We also present an ap-
proximation of our model, which allows
longer texts to be generated efficiently.
1 Introduction
Concept-to-text natural language generation
(NLG) generates texts from formal knowledge
representations (Reiter and Dale, 2000). With the
emergence of the Semantic Web (Berners-Lee et
al., 2001; Shadbolt et al, 2006; Antoniou and
van Harmelen, 2008), interest in concept-to-text
NLG has been revived and several methods have
been proposed to express axioms of OWL ontolo-
gies (Grau et al, 2008), a form of description
logic (Baader et al, 2002), in natural language
(Bontcheva, 2005; Mellish and Sun, 2006; Gala-
nis and Androutsopoulos, 2007; Mellish and Pan,
2008; Schwitter et al, 2008; Schwitter, 2010;
Liang et al, 2011; Williams et al, 2011).
NLG systems typically employ a pipeline archi-
tecture. They usually start by selecting the logical
facts (axioms, in the case of an OWL ontology) to
be expressed. The purpose of the next stage, text
planning, ranges from simply ordering the facts to
be expressed to making more complex decisions
about the rhetorical structure of the text. Lexical-
ization then selects the words and syntactic struc-
tures that will realize each fact, specifying how
each fact can be expressed as a single sentence.
Sentence aggregation may then combine shorter
sentences to form longer ones. Another compo-
nent generates appropriate referring expressions,
and surface realization produces the final text.
Each stage of the pipeline is treated as a lo-
cal optimization problem, where the decisions of
the previous stages cannot be modified. This ar-
rangement produces texts that may not be optimal,
since the decisions of the stages have been shown
to be co-dependent (Danlos, 1984; Marciniak and
Strube, 2005; Belz, 2008). For example, deci-
sions made during content selection may maxi-
mize importance measures, but may produce facts
that are difficult to turn into a coherent text; also,
content selection and lexicalization may lead to
more or fewer sentence aggregation opportunities.
Some of these problems can be addressed by over-
generating at each stage (e.g., producing several
alternative sets of facts at the end of content selec-
tion, several alternative lexicalizations etc.) and
employing a final ranking component to select the
best combination (Walker et al, 2001). This over-
generate and rank approach, however, may also
fail to find an optimal solution, and it generates an
exponentially large number of candidate solutions
when several components are pipelined.
In this paper, we present an Integer Linear Pro-
gramming (ILP) model that combines content se-
lection, lexicalization, and sentence aggregation.
Our model does not consider directly text plan-
ning, nor referring expression generation, which
we hope to include in future work, but it is com-
bined with an external simple text planner and an
external referring expression generation compo-
nent; we also do not discuss surface realization.
Unlike pipeline architectures, our model jointly
examines the possible choices in the three NLG
stages it considers, to avoid greedy local decisions.
51
Given an individual (entity) or class of an OWL
ontology and a set of facts (axioms) about the in-
dividual or class, we aim to produce a compact
text that expresses as many facts in as few words
as possible. This is desirable when space is lim-
ited or expensive, e.g., when displaying product
descriptions on smartphones, or when including
advertisements in Web search results. If an impor-
tance score is available for each fact, our model
can take it into account to prefer expressing im-
portant facts, again using as few words as possi-
ble. The model itself, however, does not produce
importance scores, i.e., we assume that the scores
are produced by a separate process (Barzilay and
Lapata, 2005; Demir et al, 2010), not included in
our content selection. In the experiments of this
article, we treat all the facts as equally important.
Although the search space of our model is very
large and ILP problems are in general NP-hard, off-
the-shelf ILP solvers can be used, which can be
very fast in practice and guarantee finding a global
optimum. Experiments with two ontologies show
that our ILP model outperforms, in terms of ex-
pressed facts per word, an NLG system that uses
the same components connected in a pipeline, with
no deterioration in perceived text quality; the ILP
model may actually lead to texts of higher quality,
compared to those of the pipeline, when there are
many facts to express. We also present an approx-
imation of our ILP model, which is more efficient
when larger numbers of facts need to be expressed.
Section 2 discusses previous related work. Sec-
tion 3 defines our ILP model. Section 4 presents
our experimentals. Section 5 concludes.
2 Related work
Marciniak and Strube (2005) propose a general
ILP approach for language processing applications
where the decisions of classifiers that consider
particular, but co-dependent, subtasks need to be
combined. They also show how their approach
can be used to generate multi-sentence route di-
rections, in a setting with very different inputs and
processing stages than the ones we consider.
Barzilay and Lapata (2005) treat content selec-
tion as an optimization problem. Given a pool of
facts and scores indicating their importance, they
select the facts to express by formulating an op-
timization problem similar to energy minimiza-
tion. The problem is solved by applying a minimal
cut partition algorithm to a graph representing the
pool of facts and the importance scores. The im-
portance scores of the facts are obtained via super-
vised machine learning (AdaBoost) from a dataset
of (sports) facts and news articles expressing them.
In other work, Barzilay and Lapata (2006) con-
sider sentence aggregation. Given a set of facts
that a content selection stage has produced, aggre-
gation is viewed as the problem of partitioning the
facts into optimal subsets. Sentences expressing
facts of the same subset are aggregated to form a
longer sentence. The optimal partitioning maxi-
mizes the pairwise similarity of the facts in each
subset, subject to constraints that limit the number
of subsets and the number of facts in each sub-
set. A Maximum Entropy classifier predicts the
semantic similarity of each pair of facts, and an
ILP model is used to find the optimal partitioning.
Althaus et al (2004) show that ordering a set of
sentences to maximize local coherence is equiva-
lent to the traveling salesman problem and, hence,
NP-complete. They also show an ILP formulation
of the problem, which can be solved efficiently in
practice using branch-and-cut with cutting planes.
Kuznetsova et al (2012) use ILP to generate im-
age captions. They train classifiers to detect the
objects in each image. Having identified the ob-
jects of a given image, they retrieve phrases from
the captions of a corpus of images, focusing on
the captions of objects that are similar (color, tex-
ture, shape) to the ones in the given image. To
select which objects of the image to report and
in what order, Kuznetsova et al maximize (via
ILP) the mean of the confidence scores of the ob-
ject detection classifiers and the sum of the co-
occurrence probabilities of the objects that will be
reported in adjacent positions in the caption. Hav-
ing decided which objects to report and their order,
Kuznetsova et al use a second ILP model to decide
which phrases to use for each object and to order
the phrases. The second ILP model maximizes the
confidence of the phrase retrieval algorithm and
the local cohesion between subsequent phrases.
Joint optimization ILP models have also been
used in multi-document text summarization and
sentence compression (McDonald, 2007; Clarke
and Lapata, 2008; Berg-Kirkpatrick et al, 2011;
Galanis et al, 2012; Woodsend and Lapata, 2012),
where the input is text, not formal knowledge rep-
resetations. Statistical methods to jointly perform
content selection, lexicalization, and surface real-
ization have also been proposed in NLG (Liang et
52
al., 2009; Konstas and Lapata, 2012a; Konstas and
Lapata, 2012b), but they are currently limited to
generating single sentences from flat records, as
opposed to ontologies. Our method is the first one
to consider content selection, lexicalization, and
sentence aggregation as an ILP joint optimization
problem in the context of multi-sentence concept-
to-text generation.
3 Our ILP model of NLG
Let F = {f1, . . . , fn} be the set of all the facts fi
(OWL axioms) about the individual or class to be
described. OWL axioms can be represented as sets
of RDF triples of the form ?S,R,O?, where S is an
individual or class, O is another individual, class,
or datatype value, and R is a relation (property)
that connects S to O.1 Hence, we can assume that
each fact fi is a triple ?Si, Ri, Oi?.2
For each fact fi, a set Pi = {pi1, pi2, . . . }
of alternative sentence plans is available. Each
sentence plan pik specifies how to express fi =
?Si, Ri, Oi? as an alternative single sentence. In
our work, a sentence plan is a sequence of slots,
along with instructions specifying how to fill the
slots in; and each sentence plan is associated
with the relations it can express. For example,
?exhibit12,foundIn,athens? could be ex-
pressed using a sentence plan like ?[ref (S)]
[findpast] [in] [ref (O)]?, where square brackets
denote slots, ref (S) and ref (O) are instructions
requiring referring expressions for S and O in
the corresponding slots, and ?findpast? requires the
simple past form of ?find?. In our example, the
sentence plan would lead to a sentence like ?Ex-
hibit 12 was found in Athens?. We call elements
the slots with their instructions, but with ?S?
and ?O? accompanied by the individuals, classes,
or datatype values they refer to; in our exam-
ple, the elements are ?[ref (S: exhibit12)]?,
?[findpast]?, ?[in]?, ?[ref (O: athens)]?.
Different sentence plans may lead to more or
fewer aggregation opportunities; e.g., sentences
with the same verb are easier to aggregate. We
use aggregation rules similar to those of Dalianis
(1999), which operate on sentence plans and usu-
ally lead to shorter texts, as in the example below.
Bancroft Chardonnay is a kind of Chardonnay. It is
1See www.w3.org/TR/owl2-mapping-to-rdf/.
2We actually convert the RDF triples to simpler message
triples, so that each message triple can be easily expressed by
a simple sentence, but we do not discuss this conversion here.
made in Bancroft. ? Bancroft Chardonnay is a kind
of Chardonnay made in Bancroft.
Let s1, . . . , sm be disjoint subsets of F , each
containing 0 to n facts, with m < n. A single
sentence is generated for each subset sj by aggre-
gating the sentences (more precisely, the sentence
plans) expressing the facts of sj .3 An empty sj
generates no sentence, i.e., the resulting text can
be at most m sentences long. Let us also define:
ai =
{
1, if fact fi is selected
0, otherwise (1)
likj =
?
?
?
1, if sentence plan pik is used to express
fact fi, and fi is in subset sj
0, otherwise
(2)
btj =
{
1, if element et is used in subset sj
0, otherwise (3)
and let B be the set of all the distinct elements (no
duplicates) from all the available sentence plans
that can express the facts of F . The length of an
aggregated sentence resulting from a subset sj can
be roughly estimated by counting the distinct el-
ements of the sentence plans that have been cho-
sen to express the facts of sj ; elements that occur
more than once in the chosen sentence plans of sj
are counted only once, because they will probably
be expressed only once, due to aggregation.
Our objective function (4) maximizes the to-
tal importance of the selected facts (or simply the
number of selected facts, if all facts are equally
important), and minimizes the number of distinct
elements in each subset sj , i.e., the approximate
length of the corresponding aggregated sentence;
an alternative explanation is that by minimizing
the number of distinct elements in each sj , we fa-
vor subsets that aggregate well. By a and b we
jointly denote all the ai and btj variables. The
two parts of the objective function are normalized
to [0, 1] by dividing by the total number of avail-
able facts |F | and the number of subsets m times
the total number of distinct elements |B|. We as-
sume that the importance scores imp(fi) are pro-
vided by a separate component (Barzilay and La-
pata, 2005; Demir et al, 2010) and range in [0, 1].
The parameters ?1, ?2 are used to tune the prior-
ity given to expressing many important facts vs.
3All the sentences of every possible subset sj can be ag-
gregated, because all the sentences share the same subject,
the class or individual being described. If multiple aggrega-
tion rules apply, we use the one that leads to a shorter text.
53
generating shorter texts; we set ?1 + ?2 = 1.
max
a,b
?1 ?
|F |?
i=1
ai ? imp(fi)
|F |
? ?2 ?
m?
j=1
|B|?
t=1
btj
m ? |B|
(4)
subject to:
ai =
m?
j=1
|Pi|?
k=1
likj , for i = 1, . . . , n (5)
?
et?Bik
btj ? |Bik| ? likj , for
i = 1, . . . , n
j = 1, . . . ,m
k = 1, . . . , |Pi|
(6)
?
pik?P (et)
likj ? btj , for
t = 1, . . . , |B|
j = 1, . . . ,m (7)
|B|?
t=1
btj ? Bmax, for j = 1, . . . ,m (8)
|Pi|?
k=1
likj +
|Pi? |?
k?=1
li?k?j ? 1, for
j = 1, . . . ,m, i = 2, . . . , n
i? = 1, . . . , n? 1; i 6= i?
section(fi) 6= section(f ?i)
(9)
Constraint 5 ensures that for each selected fact,
only one sentence plan in only one subset is se-
lected; if a fact is not selected, no sentence plan
for the fact is selected either. |?| denotes the car-
dinality of a set ?. In constraint 6, Bik is the set of
distinct elements et of the sentence plan pik. This
constraint ensures that if pik is selected in a subset
sj , then all the elements of pik are also present in
sj . If pik is not selected in sj , then some of its el-
ements may still be present in sj , if they appear in
another selected sentence plan of sj .
In constraint 7, P (et) is the set of sentence plans
that contain element et. If et is used in a subset sj ,
then at least one of the sentence plans of P (et)
must also be selected in sj . If et is not used in sj ,
then no sentence plan of P (et) may be selected in
sj . Lastly, constraint 8 limits the number of ele-
ments that a subset sj can contain to a maximum
allowed number Bmax, in effect limiting the max-
imum length of an aggregated sentence.
We assume that each relation R has been man-
ually mapped to a single topical section; e.g., re-
lations expressing the color, body, and flavor of
a wine may be grouped in one section, and rela-
tions about the wine?s producer in another. The
section of a fact fi = ?Si, Ri, Oi? is the section
of its relation Ri. Constraint 9 ensures that facts
from different sections will not be placed in the
same subset sj , to avoid unnatural aggregations.
4 Experiments
We used NaturalOWL (Galanis and Androutsopou-
los, 2007; Galanis et al, 2009; Androutsopou-
los et al, 2013), an NLG system for OWL on-
tologies that relies on a pipeline of content selec-
tion, text planning, lexicalization, aggregation, re-
ferring expression generation, and surface realiza-
tion components.4 We modified the content selec-
tion, lexicalization, and aggregation components
to use our ILP model, maintaining the aggrega-
tion rules of the original system. For referring ex-
pressions and surface realization, the new system,
called ILPNLG, invokes the corresponding compo-
nents of the original system. We use branch-and-
cut to solve the ILP problems.5
The original system, hereafter called PIPELINE,
assumes that each relation has been mapped to a
topical section, as in ILPNLG. It also assumes that
a manually specified order of the sections and the
relations of each section is available, which is used
by the text planner to order the selected facts (by
their relations). The subsequent components of the
pipeline are not allowed to change the order of the
facts, and aggregation operates only on sentence
plans of adjacent facts from the same section. In
ILPNLG, the manually specified order of sections
and relations is used to order the sentences of each
subset sj (before aggregating them), the aggre-
gated sentences in each section (each aggregated
sentence inherits the minimum order of its con-
stituents), and the sections (with their sentences).
4.1 Experiments with the Wine Ontology
In a first set of experiments, we used the Wine On-
tology, which had also been used in previous ex-
periments with PIPELINE (Androutsopoulos et al,
2013). The ontology contains 63 wine classes, 52
wine individuals, a total of 238 classes and indi-
viduals (including wineries, regions, etc.), and 14
properties.6 We kept the 2 topical sections, the
ordering of sections and relations, and the sen-
tence plans of the previous experiments, but we
added more sentence plans to ensure that 3 sen-
tence plans were available per relation. We gen-
erated English texts for the 52 wine individuals
4All the software and data that we used will be
freely available from http://nlp.cs.aueb.gr/
software.html. We use version 2 of NaturalOWL.
5We use the branch-and-cut implementation of GLPK with
mixed integer rounding, mixed cover, and clique cuts; see
sourceforge.net/projects/winglpk/.
6See www.w3.org/TR/owl-guide/wine.rdf.
54
of the ontology; we did not experiment with texts
describing classes, because we could not think of
multiple alternative sentence plans for many of
their axioms. For each wine individual, there were
5 facts on average and a maximum of 6 facts. We
set the importance scores imp(fi) of all the facts
fi to 1, to make the decisions of PIPELINE and
ILPNLG easier to understand; both systems use the
same importance scores. PIPELINE does not pro-
vide any mechanism to estimate the importance
scores, assuming that they are provided manually.
PIPELINE has a parameter M specifying the
maximum number of facts it is allowed to report
per text. When M is smaller than the number of
available facts (|F |) and all the facts are treated
as equally important, as in our experiments, it se-
lects randomly M of the available facts. We re-
peated the generation of PIPELINE?s texts for the
52 individuals for M = 2, 3, 4, 5, 6. For each M ,
the texts of PIPELINE for the 52 individuals were
generated three times, each time using one of the
different alternative sentence plans of each rela-
tion. We also generated the texts using a variant of
PIPELINE, dubbed PIPELINESHORT, which always
selects the shortest (in elements) sentence plan
among the available ones. In all cases, PIPELINE
and PIPELINESHORT were allowed to form ag-
gregated sentences containing up to Bmax = 22
distinct elements, which was the number of dis-
tinct elements of the longest aggregated sentence
in the previous experiments (Androutsopoulos et
al., 2013), where PIPELINE was allowed to aggre-
gate up to 3 original sentences.7
With ILPNLG, we repeated the generation of the
texts of the 52 individuals using different values
of ?1 (?2 = 1 ? ?1), which led to texts express-
ing from zero to all of the available facts. We set
the maximum number of fact subsets to m = 3,
which was the maximum number of (aggregated)
sentences in the texts of PIPELINE and PIPELI-
NESHORT. Again, we set Bmax = 22.
We compared ILPNLG to PIPELINE and PIPELI-
NESHORT by measuring the average number of
facts they reported divided by the average text
length (in words). Figure 1 shows this ratio as a
function of the average number of reported facts,
along with 95% confidence intervals (of sample
means). PIPELINESHORT achieved better results
than PIPELINE, but the differences were small.
For ?1 < 0.2, ILPNLG produces empty texts,
7We modified the two pipeline systems to count elements.
Figure 1: Facts/words of Wine Ontology texts.
because it focuses on minimizing the number of
distinct elements of each text. For ?1 ? 0.225,
it performs better than the other systems. For
?1 ? 0.3, it obtains the highest fact/words ratio
by selecting the facts and sentence plans that lead
to the most compressive aggregations. For greater
values of ?1, it selects additional facts whose sen-
tence plans do not aggregate that well, which is
why the ratio declines. For small numbers of facts,
the two pipeline systems select facts and sentence
plans that offer few aggregation opportunities; as
the number of selected facts increases, some more
aggregation opportunities arise, which is why the
facts/words ratio of the two systems improves. In
all the experiments, the ILP solver was very fast
(average: 0.08 sec, worst: 0.14 sec per text).
We show below texts produced by PIPELINE
(M = 4) and ILPNLG (?1 = 0.3).
PIPELINE: This is a strong Sauternes. It is made from Semil-
lon grapes and it is produced by Chateau D?ychem.
ILPNLG: This is a strong Sauternes. It is made from Semillon
grapes by Chateau D?ychem.
PIPELINE: This is a full Riesling and it has moderate flavor.
It is produced by Volrad.
ILPNLG: This is a full sweet moderate Riesling.
In the first pair, PIPELINE uses different verbs for
the grapes and producer, whereas ILPNLG uses the
same verb, which leads to a more compressive ag-
gregation; both texts describe the same wine and
report 4 facts. In the second pair, ILPNLG has cho-
sen to express the sweetness instead of the pro-
ducer, and uses the same verb (?be?) for all the
facts, leading to a shorter sentence; again both
texts describe the same wine and report 4 facts.
In both examples, some facts are not aggregated
because they belong in different sections.
We also wanted to investigate the effect that the
higher facts/words ratio of ILPNLG has on the per-
ceived quality of the generated texts, compared
to the texts of the pipeline. We were concerned
that the more compressive aggregations of ILPNLG
55
Criteria PIPELINESHORT ILPNLG
Sentence fluency 4.75 ? 0.21 4.85 ? 0.10
Text structure 4.94 ? 0.06 4.88 ? 0.14
Clarity 4.77 ? 0.18 4.75 ? 0.15
Overall 4.52 ? 0.20 4.60 ? 0.18
Table 1: Human scores for Wine Ontology texts.
might lead to sentences that sound less fluent or
unnatural, though aggregation often helps produce
more natural texts. We were also concerned that
the more compact texts of ILPNLG might be per-
ceived as being more difficult to understand (less
clear) or less well-structured. To investigate these
issues, we showed the 52 ? 2 = 104 texts of
PIPELINESHORT (M = 4) and ILPNLG (?1 = 0.3)
to 6 computer science students not involved in the
work of this article; they were all fluent, though
not native, English speakers. Each one of the 104
texts was given to exactly one student. Each stu-
dent was given approximately 9 randomly selected
texts of each system. The OWL statements that the
texts were generated from were not shown, and the
students did not know which system had generated
each text. Each student was shown all of his/her
texts in random order, regardless of the system that
generated them. The students were asked to score
each text by stating how strongly they agreed or
disagreed with statements S1?S3 below. A scale
from 1 to 5 was used (1: strong disagreement, 3:
ambivalent, 5: strong agreement).
(S1) Sentence fluency: The sentences of the text are fluent,
i.e., each sentence on its own is grammatical and sounds nat-
ural. When two or more smaller sentences are combined to
form a single, longer sentence, the resulting longer sentence
is also grammatical and sounds natural.
(S2) Text structure: The order of the sentences is appro-
priate. The text presents information by moving reasonably
from one topic to another.
(S3) Clarity: The text is easy to understand, provided that
the reader is familiar with basic wine terms.
The students were also asked to provide an over-
all score (1?5) per text. We did not score referring
expressions, since both systems use the same com-
ponent to generate them.
Table 1 shows the average scores of the two
systems with 95% confidence intervals (of sam-
ple means). For each criterion, the best score is
shown in bold. The sentence fluency and over-
all scores of ILPNLG are slightly higher than those
of PIPELINESHORT, whereas PIPELINESHORT ob-
tained a slightly higher score for text structure and
clarity. The differences, however, are very small,
especially in clarity, and there is no statistically
significant difference between the two systems in
any of the criteria.8 Hence, there was no evidence
in these experiments that the highest facts/words
ratio of ILPNLG comes at the expense of lower per-
ceived text quality. We investigated these issues
further in a second set of experiments, discussed
next, where the generated texts were longer.
4.2 Consumer Electronics experiments
In the second set of experiments, we used the
Consumer Electronics Ontology, which had also
been used in previous work with PIPELINE. The
ontology comprises 54 classes and 441 individ-
uals (e.g., printer types, paper sizes), but no in-
formation about particular products.9 In previ-
ous work, 30 individuals (10 digital cameras, 10
camcorders, 10 printers) were added to the ontol-
ogy; they were randomly selected from a publicly
available dataset of 286 digital cameras, 613 cam-
corders, and 58 printers, whose instances comply
with the Consumer Electronics Ontology.10 We
kept the 6 topical sections, the ordering of sec-
tions and relations, and the sentence plans of the
previous work, but we added more sentence plans
to ensure that 3 sentence plans were available for
almost every relation; for some relations we could
not think of enough sentence plans. Again, we set
the importance scores of all the facts to 1.
We generated texts with PIPELINE and PIPELI-
NESHORT for the 30 individuals, for M =
3, 6, 9, . . . , 21. Again for each M , the texts of
PIPELINE were generated three times, each time
using one of the different alternative sentence
plans of each relation. PIPELINE and PIPELI-
NESHORT were allowed to form aggregated sen-
tences containing up to Bmax = 39 distinct ele-
ments, which was the number of distinct elements
of the longest aggregated sentence in the previous
work with this ontology, where PIPELINE was al-
lowed to aggregate up to 3 original sentences. We
also set Bmax = 39 in ILPNLG.
There are 14 facts (F ) on average and a max-
imum of 21 facts for each one of the 30 individ-
uals, compared to the 5 facts on average and the
maximum of 6 facts of the experiments with the
Wine Ontology. Hence, the texts of the Consumer
8The confidence intervals do not overlap, and we also per-
formed paired two-tailed t-tests (? = 0.05) to check for sta-
tistical significance. In previous work, where judges were
asked to score texts using the same criteria, inter-annotator
agreement was strong (sample Pearson correlation r ? 0.91).
9Ontology available from www.ebusiness-unibw.
org/ontologies/consumerelectronics/v1.
10See rdf4ecommerce.esolda.com/.
56
Figure 2: Average solver times for ILPNLG for dif-
ferent maximum numbers of fact subsets (m).
Electronics Ontology are much longer, when they
report all the available facts. To generate texts for
the 30 individuals with ILPNLG, we would have
to set the maximum number of fact subsets to
m = 10, which was the maximum number of (ag-
gregated) sentences in the texts of PIPELINE and
PIPELINESHORT. The number of variables of our
ILP model, however, grows exponentially tom and
the number of available facts |F |. Figure 2 shows
the average time the ILP solver took for different
values ofm in the experiments with the Consumer
Electronics ontology; the results are also averaged
for ?1 = 0.4, 0.5, 0.6 (?2 = 1 ? ?1). For m = 4,
the solver took 1 minute and 47 seconds on av-
erage per text; recall that |F | is also much larger
now, compared to the experiments of the previous
section. Form = 5, the solver was so slow that we
aborted the experiment. Figure 3 shows the aver-
age solver time for different numbers of available
facts |F |, for m = 3; in this case, we modified
the set of available facts (F ) of every individual
to contain 3, 6, 9, 12, 15, 18, 21 facts; the results
are averaged for ?1 = 0.4, 0.5, 0.6. Although
the times of Fig. 3 also grow exponentially, they
remain under 4 seconds, showing that the main
problem for ILPNLG is m, the number of fact sub-
sets, which is also the maximum allowed number
of (aggregated) sentences of each text.
To be able to efficiently generate texts with
larger m values, we use a variant of ILPNLG,
called ILPNLGAPPROX, which considers each fact
subset separately. ILPNLGAPPROX starts with the
full set of available facts (F ) and uses our ILP
model (Section 3) with m = 1 to produce the first
(aggregated) sentence of the text. It then removes
the facts expressed by the first (aggregated) sen-
tence from F , and uses the ILP model, again with
Figure 3: Average solver times for ILPNLG for dif-
ferent numbers of available facts (|F |) andm = 3.
Figure 4: Avg. solver times for ILPNLGAPPROX
for different max. numbers of fact subsets (m).
m = 1, to produce the second (aggregated) sen-
tence etc. This process is repeated until we pro-
duce the maximum number of allowed aggregated
sentences, or until we run out of facts. ILPNLGAP-
PROX is an approximation of ILPNLG, in the sense
that it does not consider all the fact subsets jointly
and, hence, does not guarantee finding a globally
optimal solution for the entire text. Figures 4?5
show the average solver times of ILPNLGAPPROX
for different values of m and |F |; all the other set-
tings are as in Figures 2?3. The solver times of
ILPNLGAPPROX grow approximately linearly tom
and |F | and are under 0.3 seconds in all cases.
Figure 6 shows the average facts/words ratio of
ILPNLGAPPROX (m = 10), PIPELINE and PIPELI-
NESHORT, along with 95% confidence intervals
(of sample means), for the texts of the 30 individu-
als. Again, PIPELINESHORT achieves slightly bet-
ter results than PIPELINE, but the differences are
now smaller (cf. Fig. 1). ILPNLGAPPROX behaves
very similarly to ILPNLG in the Wine Ontology ex-
periments (cf. Fig. 1); for ?1 ? 0.35, it produces
empty texts, while for ?1 ? 0.4 it performs better
than the other systems. ILPNLGAPPROX obtains
57
Figure 5: Avg. solver times for ILPNLGAPPROX
for different |F | values and m = 3.
Figure 6: Facts/words for Consumer Electronics.
the highest facts/words ratio for ?1 = 0.45, where
it selects the facts and sentence plans that lead to
the most compressive aggregations. For greater
values of ?1, it selects additional facts whose sen-
tence plans do not aggregate that well, which is
why the ratio declines. The two pipeline systems
select facts and sentence plans that offer very few
aggregation opportunities; as the number of se-
lected facts increases, some more aggregation op-
portunities arise, which is why the facts/words ra-
tio of the two systems improves slightly, though
the improvement is now hardly noticeable.
We show below two example texts produced by
PIPELINE (M = 6) and ILPNLGAPPROX (?1 =
0.45). Both texts report 6 facts, but ILPNLGAP-
PROX has selected facts and sentence plans that
allow more compressive aggregations. Recall that
we treat all the facts as equally important.
PIPELINE: Sony DCR-TRV270 requires minimum illumina-
tion of 4.0 lux and its display is 2.5 in. It features a sports
scene mode, it includes a microphone and an IR remote con-
trol. Its weight is 780.0 grm.
ILPNLGAPPROX: Sony DCR-TRV270 has a microphone and
an IR remote control. It is 98.0 mm high, 85.0 mm wide,
151.0 mm deep and it weighs 780.0 grm.
We showed the 30 ? 2 = 60 texts of PIPELI-
NESHORT (M = 6) and ILPNLGAPPROX (?1 =
Criteria PIPELINESHORT ILPNLGAPPROX
Sentence fluency 4.50 ? 0.30 4.87 ? 0.12
Text structure 4.33 ? 0.36 4.73 ? 0.22
Clarity 4.53 ? 0.29 4.97 ? 0.06
Overall 4.10 ? 0.31 4.73 ? 0.16
Table 2: Human scores for Consumer Electronics.
0.45) to the same 6 students, as in Section 4.1.
Again, each text was given to exactly one student.
Each student was given approximately 5 randomly
selected texts of each system. The OWL statements
that the texts were generated from were not shown,
and the students did not know which system had
generated each text. Each student was shown all
of his/her texts in random order, regardless of the
system that generated them. The students were
asked to score each text by stating how strongly
they agreed or disagreed with statements S1?S3,
as in Section 4.1. They were also asked to provide
an overall score (1?5) per text.
Table 2 shows the average scores of the two
systems with 95% confidence intervals (of sam-
ple means). For each criterion, the best score is
shown in bold; the confidence interval of the best
score is also shown in bold, if it does not overlap
with the confidence interval of the other system.
Unlike the Wine Ontology experiments, the scores
of our ILP approach are now higher than those of
the pipeline in all of the criteria, and the differ-
ences are also larger, though the differences are
statistically significant only for clarity and over-
all quality.11 We attribute these differences to the
fact that the texts are now longer and the sentence
plans more varied, which often makes the texts of
the pipeline sound verbose and, hence, more diffi-
cult to follow, compared to the more compact texts
of ILPNLGAPPROX, which sound more concise.
Overall, the human scores of the experiments
with the two ontologies suggest that the higher
facts/words ratio of our ILP approach does not
come at the expense of lower perceived text qual-
ity. On the contrary, the texts of the ILP approach
may be perceived as clearer and overall better than
those of the pipeline, when the texts are longer.
5 Conclusions
We presented an ILP model of content selection,
lexicalization, and aggregation that jointly con-
siders the possible choices in the three stages, to
11When two confidence intervals do not overlap, the dif-
ference is statistically significant. When they overlap, the
difference may still be statistically significant; we performed
additional paired two-tailed t-tests (? = 0.05) in those cases.
58
avoid greedy local decisions and produce more
compact texts. The model has been embedded in
NaturalOWL, a NLG system for OWL ontologies,
which used a pipeline architecture in its original
form. Experiments with two ontologies confirmed
that our approach leads to expressing more facts
per word, with no deterioration in the perceived
text quality; the ILP approach may actually lead to
texts perceived as clearer and overall better, com-
pared to the pipeline, when there are many facts
to express. We also presented an approximation
of our ILP model, which allows longer texts to
be generated efficiently. We plan to extend our
model to include text planning, referring expres-
sion generation, and mechanisms to obtain impor-
tance scores.
Acknowledgments
This research has been co-financed by the Euro-
pean Union (European Social Fund ? ESF) and
Greek national funds through the Operational Pro-
gram ?Education and Lifelong Learning? of the
National Strategic Reference Framework (NSRF)
? Research Funding Program: Heracleitus II. In-
vesting in knowledge society through the Euro-
pean Social Fund.
References
E. Althaus, N. Karamanis, and A. Koller. 2004. Com-
puting locally coherent discourses. In 42nd Annual
Meeting of ACL, pages 399?406, Barcelona, Spain.
I. Androutsopoulos, G. Lampouras, and D. Gala-
nis. 2013. Generating natural language descrip-
tions from OWL ontologies: the NaturalOWL sys-
tem. Technical report, Natural Language Processing
Group, Department of Informatics, Athens Univer-
sity of Economics and Business.
G. Antoniou and F. van Harmelen. 2008. A Semantic
Web primer. MIT Press, 2nd edition.
F. Baader, D. Calvanese, D.L. McGuinness, D. Nardi,
and P.F. Patel-Schneider, editors. 2002. The De-
scription Logic Handbook. Cambridge Univ. Press.
R. Barzilay and M. Lapata. 2005. Collective content
selection for concept-to-text generation. In HLT-
EMNLP, pages 331?338, Vancouver, BC, Canada.
R. Barzilay and M. Lapata. 2006. Aggregation via
set partitioning for natural language generation. In
HLT-NAACL, pages 359?366, New York, NY.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011.
Jointly learning to extract and compress. In 49th
Meeting of ACL, pages 481?490, Portland, OR.
T. Berners-Lee, J. Hendler, and O. Lassila. 2001. The
Semantic Web. Scientific American, May:34?43.
K. Bontcheva. 2005. Generating tailored textual sum-
maries from ontologies. In 2nd European Semantic
Web Conf., pages 531?545, Heraklion, Greece.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 1(31):399?429.
H. Dalianis. 1999. Aggregation in natural language
generation. Comput. Intelligence, 15(4):384?414.
L. Danlos. 1984. Conceptual and linguistic decisions
in generation. In 10th COLING, pages 501?504,
Stanford, CA.
S. Demir, S. Carberry, and K.F. McCoy. 2010.
A discourse-aware graph-based content-selection
framework. In 6th Int. Nat. Lang. Generation Con-
ference, pages 17?25, Trim, Co. Meath, Ireland.
D. Galanis and I. Androutsopoulos. 2007. Generating
multilingual descriptions from linguistically anno-
tated OWL ontologies: the NaturalOWL system. In
11th European Workshop on Natural Lang. Genera-
tion, pages 143?146, Schloss Dagstuhl, Germany.
D. Galanis, G. Karakatsiotis, G. Lampouras, and I. An-
droutsopoulos. 2009. An open-source natural lan-
guage generator for OWL ontologies and its use in
Prote?ge? and Second Life. In 12th Conf. of the Euro-
pean Chapter of ACL (demos), Athens, Greece.
D. Galanis, G. Lampouras, and I. Androutsopoulos.
2012. Extractive multi-document summarization
with ILP and Support Vector Regression. In COL-
ING, pages 911?926, Mumbai, India.
B.C. Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-
Schneider, and U. Sattler. 2008. OWL 2: The next
step for OWL. Web Semantics, 6:309?322.
I. Konstas and M. Lapata. 2012a. Concept-to-text gen-
eration via discriminative reranking. In 50th Annual
Meeting of ACL, pages 369?378, Jeju Island, Korea.
I. Konstas and M. Lapata. 2012b. Unsupervised
concept-to-text generation with hypergraphs. In
HLT-NAACL, pages 752?761, Montre?al, Canada.
P. Kuznetsova, V. Ordonez, A. Berg, T. Berg, and
Y. Choi. 2012. Collective generation of natural im-
age descriptions. In 50th Annual Meeting of ACL,
pages 359?368, Jeju Island, Korea.
59
P. Liang, M. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
47th Meeting of ACL and 4th AFNLP, pages 91?99,
Suntec, Singapore.
S.F. Liang, R. Stevens, D. Scott, and A. Rector. 2011.
Automatic verbalisation of SNOMED classes using
OntoVerbal. In 13th Conf. AI in Medicine, pages
338?342, Bled, Slovenia.
T. Marciniak and M. Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In 9th Con-
ference on Computational Natural Language Learn-
ing, pages 136?143, Ann Arbor, MI.
R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Eu-
ropean Conference on Information Retrieval, pages
557?564, Rome, Italy.
C. Mellish and J.Z. Pan. 2008. Natural language di-
rected inference from ontologies. Artificial Intelli-
gence, 172:1285?1315.
C. Mellish and X. Sun. 2006. The Semantic Web as a
linguistic resource: opportunities for nat. lang. gen-
eration. Knowledge Based Systems, 19:298?303.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge Univ. Press.
R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and
G. Hart. 2008. A comparison of three controlled
nat. languages for OWL 1.1. In 4th OWL Experi-
ences and Directions Workshop, Washington DC.
R. Schwitter. 2010. Controlled natural languages for
knowledge representation. In 23rd COLING, pages
1113?1121, Beijing, China.
N. Shadbolt, T. Berners-Lee, and W. Hall. 2006.
The Semantic Web revisited. IEEE Intell. Systems,
21:96?101.
M.A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In 2nd Annual Meet-
ing of NAACL, pages 17?24, Pittsburgh, PA.
S. Williams, A. Third, and R. Power. 2011. Levels
of organization in ontology verbalization. In 13th
European Workshop on Natural Lang. Generation,
pages 158?163, Nancy, France.
K. Woodsend and M. Lapata. 2012. Multiple as-
pect summarization using ILP. In EMNLP-CoNLL,
pages 233?243, Jesu Island, Korea.
60
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 44?52,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Aspect Term Extraction for Sentiment Analysis: New Datasets, New
Evaluation Measures and an Improved Unsupervised Method
John Pavlopoulos and Ion Androutsopoulos
Dept. of Informatics, Athens University of Economics and Business, Greece
http://nlp.cs.aueb.gr/
Abstract
Given a set of texts discussing a particular
entity (e.g., customer reviews of a smart-
phone), aspect based sentiment analysis
(ABSA) identifies prominent aspects of the
entity (e.g., battery, screen) and an aver-
age sentiment score per aspect. We fo-
cus on aspect term extraction (ATE), one
of the core processing stages of ABSA that
extracts terms naming aspects. We make
publicly available three new ATE datasets,
arguing that they are better than previously
available ones. We also introduce new
evaluation measures for ATE, again argu-
ing that they are better than previously
used ones. Finally, we show how a pop-
ular unsupervised ATE method can be im-
proved by using continuous space vector
representations of words and phrases.
1 Introduction
Before buying a product or service, consumers of-
ten search the Web for expert reviews, but increas-
ingly also for opinions of other consumers, ex-
pressed in blogs, social networks etc. Many useful
opinions are expressed in text-only form (e.g., in
tweets). It is then desirable to extract aspects (e.g.,
screen, battery) from the texts that discuss a par-
ticular entity (e.g., a smartphone), i.e., figure out
what is being discussed, and also estimate aspect
sentiment scores, i.e., how positive or negative
the (usually average) sentiment for each aspect is.
These two goals are jointly known as Aspect Based
Sentiment Analysis (ABSA) (Liu, 2012).
In this paper, we consider free text customer re-
views of products and services; ABSA, however,
is also applicable to texts about other kinds of
entities (e.g., politicians, organizations). We as-
sume that a search engine retrieves customer re-
views about a particular target entity (product or
Figure 1: Automatically extracted prominent as-
pects (shown as clusters of aspect terms) and aver-
age aspect sentiment scores of a target entity.
service), that multiple reviews written by different
customers are retrieved for each target entity, and
that the ultimate goal is to produce a table like the
one of Fig. 1, which presents the most prominent
aspects and average aspect sentiment scores of the
target entity. Most ABSA systems in effect perform
all or some of the following three subtasks:
Aspect term extraction: Starting from texts
about a particular target entity or entities of the
same type as the target entity (e.g., laptop re-
views), this stage extracts and possibly ranks by
importance aspect terms, i.e., terms naming as-
pects (e.g., ?battery?, ?screen?) of the target en-
tity, including multi-word terms (e.g., ?hard disk?)
(Liu, 2012; Long et al., 2010; Snyder and Barzi-
lay, 2007; Yu et al., 2011). At the end of this stage,
each aspect term is taken to be the name of a dif-
ferent aspect, but aspect terms may subsequently
be clustered during aspect aggregation; see below.
Aspect term sentiment estimation: This stage
estimates the polarity and possibly also the inten-
sity (e.g., strongly negative, mildly positive) of the
opinions for each aspect term of the target entity,
usually averaged over several texts. Classifying
texts by sentiment polarity is a popular research
topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau
and Palpanas, 2012). The goal, however, in this
44
ABSA subtask is to estimate the (usually average)
polarity and intensity of the opinions about partic-
ular aspect terms of the target entity.
Aspect aggregation: Some systems group aspect
terms that are synonyms or near-synonyms (e.g.,
?price?, ?cost?) or, more generally, cluster aspect
terms to obtain aspects of a coarser granularity
(e.g., ?chicken?, ?steak?, and ?fish? may all be re-
placed by ?food?) (Liu, 2012; Long et al., 2010;
Zhai et al., 2010; Zhai et al., 2011). A polar-
ity (and intensity) score can then be computed for
each coarser aspect (e.g., ?food?) by combining
(e.g., averaging) the polarity scores of the aspect
terms that belong in the coarser aspect.
In this paper, we focus on aspect term extrac-
tion (ATE). Our contribution is threefold. Firstly,
we argue (Section 2) that previous ATE datasets are
not entirely satisfactory, mostly because they con-
tain reviews from a particular domain only (e.g.,
consumer electronics), or they contain reviews for
very few target entities, or they do not contain an-
notations for aspect terms. We constructed and
make publicly available three new ATE datasets
with customer reviews for a much larger number
of target entities from three domains (restaurants,
laptops, hotels), with gold annotations of all the
aspect term occurrences; we also measured inter-
annotator agreement, unlike previous datasets.
Secondly, we argue (Section 3) that commonly
used evaluation measures are also not entirely sat-
isfactory. For example, when precision, recall,
and F -measure are computed over distinct as-
pect terms (types), equal weight is assigned to
more and less frequent aspect terms, whereas fre-
quently discussed aspect terms are more impor-
tant; and when precision, recall, and F -measure
are computed over aspect term occurrences (to-
kens), methods that identify very few, but very fre-
quent aspect terms may appear to perform much
better than they actually do. We propose weighted
variants of precision and recall, which take into ac-
count the rankings of the distinct aspect terms that
are obtained when the distinct aspect terms are or-
dered by their true and predicted frequencies. We
also compute the average weighted precision over
several weighted recall levels.
Thirdly, we show (Section 4) how the popular
unsupervised ATE method of Hu and Liu (2004),
can be extended with continuous space word vec-
tors (Mikolov et al., 2013a; Mikolov et al., 2013b;
Mikolov et al., 2013c). Using our datasets and
evaluation measures, we demonstrate (Section 5)
that the extended method performs better.
2 Datasets
We first discuss previous datasets that have been
used for ATE, and we then introduce our own.
2.1 Previous datasets
So far, ATE methods have been evaluated mainly
on customer reviews, often from the consumer
electronics domain (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Ding et al., 2008).
The most commonly used dataset is that of Hu
and Liu (2004), which contains reviews of only
five particular electronic products (e.g., Nikon
Coolpix 4300). Each sentence is annotated with
aspect terms, but inter-annotator agreement has
not been reported.
1
All the sentences appear to
have been selected to express clear positive or neg-
ative opinions. There are no sentences express-
ing conflicting opinions about aspect terms (e.g.,
?The screen is clear but small?), nor are there
any sentences that do not express opinions about
their aspect terms (e.g., ?It has a 4.8-inch screen?).
Hence, the dataset is not entirely representative of
product reviews. By contrast, our datasets, dis-
cussed below, contain reviews from three domains,
including sentences that express conflicting or no
opinions about aspect terms, they concern many
more target entities (not just five), and we have
also measured inter-annotator agreement.
The dataset of Ganu et al. (2009), on which
one of our datasets is based, is also popular. In
the original dataset, each sentence is tagged with
coarse aspects (?food?, ?service?, ?price?, ?ambi-
ence?, ?anecdotes?, or ?miscellaneous?). For exam-
ple, ?The restaurant was expensive, but the menu
was great? would be tagged with the coarse as-
pects ?price? and ?food?. The coarse aspects, how-
ever, are not necessarily terms occurring in the
sentence, and it is unclear how they were obtained.
By contrast, we asked human annotators to mark
the explicit aspect terms of each sentence, leaving
the task of clustering the terms to produce coarser
aspects for an aspect aggregation stage.
The ?Concept-Level Sentiment Analysis Chal-
lenge? of ESWC 2014 uses the dataset of Blitzer
et al. (2007), which contains customer reviews of
1
Each aspect term occurrence is also annotated with a sen-
timent score. We do not discuss these scores here, since we
focus on ATE. The same comment applies to the dataset of
Ganu et al. (2009) and our datasets.
45
DVDs, books, kitchen appliances, and electronic
products, with an overall sentiment score for each
review. One of the challenge?s tasks requires sys-
tems to extract the aspects of each sentence and a
sentiment score (positive or negative) per aspect.
2
The aspects are intended to be concepts from on-
tologies, not simply aspect terms. The ontologies
to be used, however, are not fully specified and no
training dataset with sentences and gold aspects is
currently available.
Overall, the previous datasets are not entirely
satisfactory, because they contain reviews from
a particular domain only, or reviews for very
few target entities, or their sentences are not en-
tirely representative of customer reviews, or they
do not contain annotations for aspect terms, or
no inter-annotator agreement has been reported.
To address these issues, we provide three new
ATE datasets, which contain customer reviews of
restaurants, hotels, and laptops, respectively.
3
2.2 Our datasets
The restaurants dataset contains 3,710 English
sentences from the reviews of Ganu et al. (2009).
4
We asked human annotators to tag the aspect terms
of each sentence. In ?The dessert was divine?,
for example, the annotators would tag the aspect
term ?dessert?. In a sentence like ?The restaurant
was expensive, but the menu was great?, the an-
notators were instructed to tag only the explicitly
mentioned aspect term ?menu?. The sentence also
refers to the prices, and a possibility would be to
add ?price? as an implicit aspect term, but we do
not consider implicit aspect terms in this paper.
We used nine annotators for the restaurant re-
views. Each sentence was processed by a single
annotator, and each annotator processed approxi-
mately the same number of sentences. Among the
3,710 restaurant sentences, 1,248 contain exactly
one aspect term, 872 more than one, and 1,590 no
aspect terms. There are 593 distinct multi-word
aspect terms and 452 distinct single-word aspect
terms. Removing aspect terms that occur only
once leaves 67 distinct multi-word and 195 dis-
tinct single-word aspect terms.
The hotels dataset contains 3,600 English sen-
2
See http://2014.eswc-conferences.org/.
3
Our datasets are available upon request. The datasets
of the ABSA task of SemEval 2014 (http://alt.qcri.
org/semeval2014/task4/) are based on our datasets.
4
The original dataset of Ganu et al. contains 3,400 sen-
tences, but some of the sentences had not been properly split.
tences from online customer reviews of 30 hotels.
We used three annotators. Among the 3,600 hotel
sentences, 1,326 contain exactly one aspect term,
652 more than one, and 1,622 none. There are 199
distinct multi-word aspect terms and 262 distinct
single-word aspect terms, of which 24 and 120,
respectively, were tagged more than once.
The laptops dataset contains 3,085 English sen-
tences of 394 online customer reviews. A single
annotator (one of the authors) was used. Among
the 3,085 laptop sentences, 909 contain exactly
one aspect term, 416 more than one, and 1,760
none. There are 350 distinct multi-word and 289
distinct single-word aspect terms, of which 67 and
137, respectively, were tagged more than once.
To measure inter-annotator agreement, we used
a sample of 75 restaurant, 75 laptop, and 100 hotel
sentences. Each sentence was processed by two
(for restaurants and laptops) or three (for hotels)
annotators, other than the annotators used previ-
ously. For each sentence s
i
, the inter-annotator
agreement was measured as the Dice coefficient
D
i
= 2 ?
|A
i
?B
i
|
|A
i
|+|B
i
|
, where A
i
, B
i
are the sets of
aspect term occurrences tagged by the two anno-
tators, respectively, and |S| denotes the cardinal-
ity of a set S; for hotels, we use the mean pair-
wiseD
i
of the three annotators.
5
The overall inter-
annotator agreement D was taken to be the aver-
age D
i
of the sentences of each sample. We, thus,
obtainedD = 0.72, 0.70, 0.69, for restaurants, ho-
tels, and laptops, respectively, which indicate rea-
sonably high inter-annotator agreement.
2.3 Single and multi-word aspect terms
ABSA systems use ATE methods ultimately to ob-
tain the m most prominent (frequently discussed)
distinct aspect terms of the target entity, for dif-
ferent values of m.
6
In a system like the one of
Fig. 1, for example, if we ignore aspect aggrega-
tion, each row will report the average sentiment
score of a single frequent distinct aspect term, and
m will be the number of rows, which may depend
on the display size or user preferences.
Figure 2 shows the percentage of distinct multi-
word aspect terms among themmost frequent dis-
tinct aspect terms, for different values of m, in
5
Cohen?s Kappa cannot be used here, because the annota-
tors may tag any word sequence of any sentence, which leads
to a very large set of categories. A similar problem was re-
ported by Kobayashi et al. (2007).
6
A more general definition of prominence might also con-
sider the average sentiment score of each distinct aspect term.
46
our three datasets and the electronics dataset of Hu
and Liu (2004). There are many more single-word
distinct aspect terms than multi-word distinct as-
pect terms, especially in the restaurant and hotel
reviews. In the electronics and laptops datasets,
the percentage of multi-word distinct aspect terms
(e.g., ?hard disk?) is higher, but most of the dis-
tinct aspect terms are still single-word, especially
for small values of m. By contrast, many ATE
methods (Hu and Liu, 2004; Popescu and Etzioni,
2005; Wei et al., 2010) devote much of their pro-
cessing to identifying multi-word aspect terms.
Figure 2: Percentage of (distinct) multi-word as-
pect terms among the most frequent aspect terms.
3 Evaluation measures
We now discuss previous ATE evaluation mea-
sures, also introducing our own.
3.1 Precision, Recall, F-measure
ATE methods are usually evaluated using preci-
sion, recall, and F -measure (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Kim and Hovy, 2006;
Wei et al., 2010; Moghaddam and Ester, 2010;
Bagheri et al., 2013), but it is often unclear if these
measures are applied to distinct aspect terms (no
duplicates) or aspect term occurrences.
In the former case, each method is expected to
return a set A of distinct aspect terms, to be com-
pared to the set G of distinct aspect terms the hu-
man annotators identified in the texts. TP (true
positives) is |A?G|, FP (false positives) is |A\G|,
FN (false negatives) is |G\A|, and precision (P ),
recall (R), F =
2?P ?R
P+R
are defined as usually:
P =
TP
TP + FP
, R =
TP
TP + FN
(1)
This way, however, precision, recall, and F -
measure assign the same importance to all the dis-
tinct aspect terms, whereas missing, for example, a
more frequent (more frequently discussed) distinct
aspect term should probably be penalized more
heavily than missing a less frequent one.
When precision, recall, and F -measure are ap-
plied to aspect term occurrences (Liu et al., 2005),
TP is the number of aspect term occurrences
tagged (each term occurrence) both by the method
being evaluated and the human annotators, FP is
the number of aspect term occurrences tagged by
the method but not the human annotators, and FN
is the number of aspect term occurrences tagged
by the human annotators but not the method. The
three measures are then defined as above. They
now assign more importance to frequently occur-
ring distinct aspect terms, but they can produce
misleadingly high scores when only a few, but
very frequent distinct aspect terms are handled
correctly. Furthermore, the occurrence-based def-
initions do not take into account that missing sev-
eral aspect term occurrences or wrongly tagging
expressions as aspect term occurrences may not
actually matter, as long as the m most frequent
distinct aspect terms can be correctly reported.
3.2 Weighted precision, recall, AWP
What the previous definitions of precision and re-
call miss is that in practice ABSA systems use
ATE methods ultimately to obtain the m most fre-
quent distinct aspect terms, for a range of m val-
ues. Let A
m
and G
m
be the lists that contain the
m most frequent distinct aspect terms, ordered by
their predicted and true frequencies, respectively;
the predicted and true frequencies are computed
by examining how frequently the ATE method or
the human annotators, respectively, tagged occur-
rences of each distinct aspect term. Differences
between the predicted and true frequencies do not
matter, as long as A
m
= G
m
, for every m. Not
including in A
m
a term of G
m
should be penal-
ized more or less heavily, depending on whether
the term?s true frequency was high or low, respec-
tively. Furthermore, including in A
m
a term not in
G
m
should be penalized more or less heavily, de-
pending on whether the term was placed towards
the beginning or the end of A
m
, i.e., depending on
the prominence that was assigned to the term.
To address the issues discussed above, we in-
troduce weighted variants of precision and recall.
47
For each ATE method, we now compute a single
list A =
?
a
1
, . . . , a
|A|
?
of distinct aspect terms
identified by the method, ordered by decreasing
predicted frequency. For every m value (number
of most frequent distinct aspect terms to show),
the method is treated as having returned the sub-
list A
m
with the first m elements of A. Similarly,
we now take G =
?
g
1
, . . . , g
|G|
?
to be the list of
the distinct aspect terms that the human annotators
tagged, ordered by decreasing true frequency.
7
We
define weighted precision (WP
m
) and weighted
recall (WR
m
) as in Eq. 2?3. The notation 1{?}
denotes 1 if condition ? holds, and 0 otherwise.
By r(a
i
) we denote the ranking of the returned
term a
i
in G, i.e., if a
i
= g
j
, then r(a
i
) = j; if
a
i
?? G, then r(a
i
) is an arbitrary positive integer.
WP
m
=
?
m
i=1
1
i
? 1{a
i
? G}
?
m
i=1
1
i
(2)
WR
m
=
?
m
i=1
1
r(a
i
)
? 1{a
i
? G}
?
|G|
j=1
1
j
(3)
WR
m
counts how many terms of G (gold dis-
tinct aspect terms) the method returned in A
m
,
but weighting each term by its inverse ranking
1
r(a
i
)
, i.e., assigning more importance to terms the
human annotators tagged more frequently. The
denominator of Eq. 3 sums the weights of all
the terms of G; in unweighted recall applied to
distinct aspect terms, where all the terms of G
have the same weight, the denominator would be
|G| = TP + FN (Eq. 1). WP
m
counts how
many gold aspect terms the method returned in
A
m
, but weighting each returned term a
i
by its
inverse ranking
1
i
in A
m
, to reward methods that
return more gold aspect terms towards the begin-
ning of A
m
. The denominator of Eq. 2 sums the
weights of all the terms ofA
m
; in unweighted pre-
cision applied to distinct aspect terms, the denom-
inator would be |A
m
| = TP + FN (Eq. 1).
We plot weighted precision-recall curves by
computingWP
m
,WR
m
pairs for different values
of m, as in Fig. 3 below.
8
The higher the curve
of a method, the better the method. We also com-
pute the average (interpolated) weighted precision
7
In our experiments, we exclude from G aspect terms
tagged by the annotators only once.
8
With supervised methods, we perform a 10-fold cross-
validation for each m, and we macro-average WP
m
,WR
m
over the folds. We provide our datasets partitioned in folds.
(AWP ) of each method over 11 recall levels:
AWP =
1
11
?
r?{0,0.1,...,1}
WP
int
(r)
WP
int
(r) = max
m?{1,...,|A|},WR
m
? r
WP
m
AWP is similar to average (interpolated) precision
(AP ), which is used to summarize the tradeoff be-
tween (unweighted) precision and recall.
3.3 Other related measures
Yu at al. (2011) used nDCG@m (J?arvelin and
Kek?al?ainen, 2002; Sakai, 2004; Manning et al.,
2008), defined below, to evaluate each list of m
distinct aspect terms returned by an ATE method.
nDCG@m =
1
Z
m
?
i=1
2
t(i)
? 1
log
2
(1 + i)
Z is a normalization factor to ensure that a perfect
ranking gets nDCG@m = 1, and t(i) is a reward
function for a term placed at position i of the re-
turned list. In the work of Yu et al., t(i) = 1 if the
term at position i is not important (as judged by
a human), t(i) = 2 if the term is ?ordinary?, and
t(i) = 3 if it is important. The logarithm is used to
reduce the reward for distinct aspect terms placed
at lower positions of the returned list.
The nDCG@mmeasure is well known in rank-
ing systems (e.g., search engines) and it is similar
to our weighted precision (WP
m
). The denomina-
tor or Eq. 2 corresponds to the normalization fac-
tor Z of nDCG@m; the
1
i
factor of in the numer-
ator of Eq. 2 corresponds to the
1
log
2
(1+i)
degra-
dation factor of nDCG@m; and the 1{a
i
? G}
factor of Eq. 2 is a binary reward function, corre-
sponding to the 2
t(i)
? 1 factor of nDCG@m.
The main difference from nDCG@m is that
WP
m
uses a degradation factor
1
i
that is inversely
proportional to the ranking of the returned term
a
i
in the returned list A
m
, whereas nDCG@m
uses a logarithmic factor
1
log
2
(1+i)
, which reduces
less sharply the reward for distinct aspect terms
returned at lower positions in A
m
. We believe
that the degradation factor of WP
m
is more ap-
propriate for ABSA, because most users would in
practice wish to view sentiment scores for only a
few (e.g., m = 10) frequent distinct aspect terms,
whereas in search engines users are more likely to
examine more of the highly-ranked returned items.
It is possible, however, to use a logarithmic degra-
dation factor inWP
m
, as in nDCG@m.
48
Another difference is that we use a binary re-
ward factor 1{a
i
? G} in WP
m
, instead of the
2
t(i)
? 1 factor of nDCG@m that has three pos-
sibly values in the work of Yu at al. (2011). We
use a binary reward factor, because preliminary
experiments we conducted indicated that multi-
ple relevance levels (e.g., not an aspect term, as-
pect term but unimportant, important aspect term)
confused the annotators and led to lower inter-
annotator agreement. The nDCG@m measure
can also be used with a binary reward factor; the
possible values t(i) would be 0 and 1.
With a binary reward factor, nDCG@m in ef-
fect measures the ratio of correct (distinct) aspect
terms to the terms returned, assigning more weight
to correct aspect terms placed closer the top of the
returned list, like WP
m
. The nDCG@m mea-
sure, however, does not provide any indication
of how many of the gold distinct aspect terms
have been returned. By contrast, we also mea-
sure weighted recall (Eq. 3), which examines how
many of the (distinct) gold aspect terms have been
returned in A
m
, also assigning more weight to the
gold aspect terms the human annotators tagged
more frequently. We also compute the average
weighted precision (AWP ), which is a combina-
tion ofWP
m
andWR
m
, for a range of m values.
4 Aspect term extraction methods
We implemented and evaluated four ATE meth-
ods: (i) a popular baseline (dubbed FREQ) that re-
turns the most frequent distinct nouns and noun
phrases, (ii) the well-known method of Hu and Liu
(2004), which adds to the baseline pruning mech-
anisms and steps that detect more aspect terms
(dubbed H&L), (iii) an extension of the previous
method (dubbed H&L+W2V), with an extra prun-
ing step we devised that uses the recently pop-
ular continuous space word vectors (Mikolov et
al., 2013c), and (iv) a similar extension of FREQ
(dubbed FREQ+W2V). All four methods are unsu-
pervised, which is particularly important for ABSA
systems intended to be used across domains with
minimal changes. They return directly a list A of
distinct aspect terms ordered by decreasing pre-
dicted frequency, rather than tagging aspect term
occurrences, which would require computing the
A list from the tagged occurrences before apply-
ing our evaluation measures (Section 3.2).
4.1 The FREQ baseline
The FREQ baseline returns the most frequent (dis-
tinct) nouns and noun phrases of the reviews in
each dataset (restaurants, hotels, laptops), ordered
by decreasing sentence frequency (how many sen-
tences contain the noun or noun phrase).
9
This is a
reasonably effective and popular baseline (Hu and
Liu, 2004; Wei et al., 2010; Liu, 2012).
4.2 The H&L method
The method of Hu and Liu (2004), dubbed H&L,
first extracts all the distinct nouns and noun
phrases from the reviews of each dataset (lines 3?
6 of Algorithm 1) and considers them candidate
distinct aspect terms.
10
It then forms longer can-
didate distinct aspect terms by concatenating pairs
and triples of candidate aspect terms occurring in
the same sentence, in the order they appear in the
sentence (lines 7?11). For example, if ?battery
life? and ?screen? occur in the same sentence (in
this order), then ?battery life screen? will also be-
come a candidate distinct aspect term.
The resulting candidate distinct aspect terms
are ordered by decreasing p-support (lines 12?15).
The p-support of a candidate distinct aspect term t
is the number of sentences that contain t, exclud-
ing sentences that contain another candidate dis-
tinct aspect term t
?
that subsumes t. For example,
if both ?battery life? and ?battery? are candidate
distinct aspect terms, a sentence like ?The battery
life was good? is counted in the p-support of ?bat-
tery life?, but not in the p-support of ?battery?.
The method then tries to correct itself by prun-
ing wrong candidate distinct aspect terms and de-
tecting additional candidates. Firstly, it discards
multi-word distinct aspect terms that appear in
?non-compact? form in more than one sentences
(lines 16?23). Amulti-word term t appears in non-
compact form in a sentence if there are more than
three other words (not words of t) between any
two of the words of t in the sentence. For exam-
ple, the candidate distinct aspect term ?battery life
screen? appears in non-compact form in ?battery
life is way better than screen?. Secondly, if the
p-support of a candidate distinct aspect term t is
smaller than 3 and t is subsumed by another can-
9
We use the default POS tagger of NLTK, and the chun-
ker of NLTK trained on the Treebank corpus; see http:
//nltk.org/. We convert all words to lower-case.
10
Some details of the work of Hu and Liu (2004) were not
entirely clear to us. The discussion here and our implementa-
tion reflect our understanding.
49
didate distinct aspect term t
?
, then t is discarded
(lines 21?23).
Subsequently, a set of ?opinion adjectives? is
formed; for each sentence and each candidate dis-
tinct aspect term t that occurs in the sentence, the
closest to t adjective of the sentence (if there is
one) is added to the set of opinion adjectives (lines
25-27). The sentences are then re-scanned; if a
sentence does not contain any candidate aspect
term, but contains an opinion adjective, then the
nearest noun to the opinion adjective is added to
the candidate distinct aspect terms (lines 28?31).
The remaining candidate distinct aspect terms are
returned, ordered by decreasing p-support.
Algorithm 1 The method of Hu and Liu
Require: sentences: a list of sentences
1: terms = new Set(String)
2: psupport = new Map(String, int)
3: for s in sentences do
4: nouns = POSTagger(s).getNouns()
5: nps = Chunker(s).getNPChunks()
6: terms.add(nouns ? nps)
7: for s in sentences do
8: for t1, t2 in terms s.t. t1, t2 in s ?
s.index(t1)<s.index(t2) do
9: terms.add(t1 + ? ? + t2)
10: for t1, t2, t3 in s.t. t1, t2,t3 in s ?
s.index(t1)<s.index(t2)<s.index(t3) do
11: terms.add(t1 + ? ? + t2 + ? ? + t3)
12: for s in sentences do
13: for t: t in terms ? t in s do
14: if ?? t?: t? in terms ? t? in s ? t in t? then
15: psupport[term] += 1
16: nonCompact = new Map(String, int)
17: for t in terms do
18: for s in sentences do
19: if maxPairDistance(t.words())>3 then
20: nonCompact[t] += 1
21: for t in terms do
22: if nonCompact[t]>1 ? (? t?: t? in terms ? t in t? ?
psupport[t]<3) then
23: terms.remove(t)
24: adjs = new Set(String)
25: for s in sentences do
26: if ? t: t in terms ? t in s then
27: adjs.add(POSTagger(s).getNearestAdj(t))
28: for s in sentences do
29: if ?? t: t in terms ? t in s ? ? a: a in adjs ? a in s
then
30: t = POSTagger(s).getNearestNoun(adjs)
31: terms.add(t)
32: return psupport.keysSortedByValue()
4.3 The H&L+W2V method
We extended H&L by including an additional
pruning step that uses continuous vector space
representations of words (Mikolov et al., 2013a;
Mikolov et al., 2013b; Mikolov et al., 2013c).
The vector representations of the words are pro-
Centroid Closest Wikipedia words
Com. lang. only, however, so, way, because
Restaurants meal, meals, breakfast, wingstreet,
snacks
Hotels restaurant, guests, residence, bed, ho-
tels
Laptops gameport, hardware, hd floppy, pcs, ap-
ple macintosh
Table 1: Wikipedia words closest to the common
language and domain centroids.
duced by using a neural network language model,
whose inputs are the vectors of the words occur-
ring in each sentence, treated as latent variables to
be learned. We used the EnglishWikipedia to train
the language model and obtain word vectors, with
200 features per vector. Vectors for short phrases,
in our case candidate multi-word aspect terms, are
produced in a similar manner.
11
Our additional pruning stage is invoked imme-
diately immediately after line 6 of Algorithm 1. It
uses the ten most frequent candidate distinct as-
pect terms that are available up to that point (fre-
quency taken to be the number of sentences that
contain each candidate) and computes the centroid
of their vectors, dubbed the domain centroid. Sim-
ilarly, it computes the centroid of the 20 most fre-
quent words of the Brown Corpus (news category),
excluding stop-words and words shorter than three
characters; this is the common language centroid.
Any candidate distinct aspect term whose vector is
closer to the common language centroid than the
domain centroid is discarded, the intuition being
that the candidate names a very general concept,
rather than a domain-specific aspect.
12
We use co-
sine similarity to compute distances. Vectors ob-
tained from Wikipedia are used in all cases.
To showcase the insight of our pruning step,
Table 1 shows the five words from the English
Wikipedia whose vectors are closest to the com-
mon language centroid and the three domain cen-
troids. The words closest to the common language
centroid are common words, whereas words clos-
est to the domain centroids name domain-specific
concepts that are more likely to be aspect terms.
11
We use WORD2VEC, available at https://code.
google.com/p/word2vec/, with a continuous bag of
words model, default parameters, the first billion characters
of the English Wikipedia, and the pre-processing of http:
//mattmahoney.net/dc/textdata.html.
12
WORD2VEC does not produce vectors for phrases longer
than two words; thus, our pruning mechanism never discards
candidate aspect terms of more than two words.
50
Figure 3: Weighted precision ? weighted recall curves for the three datasets.
4.4 The FREQ+W2V method
As with H&L+W2V, we extended FREQ by adding
our pruning step that uses the continuous space
word (and phrase) vectors. Again, we produced
one common language and three domain cen-
troids, as before. Candidate distinct aspect terms
whose vector was closer to the common language
centroid than the domain centroid were discarded.
5 Experimental results
Table 2 shows the AWP scores of the methods.
All four methods perform better on the restaurants
dataset. At the other extreme, the laptops dataset
seems to be the most difficult one; this is due to the
fact that it contains many frequent nouns and noun
phrases that are not aspect terms; it also contains
more multi-word aspect terms (Fig. 2).
H&L performs much better than FREQ in all
three domains, and our additional pruning (W2V)
improves H&L in all three domains. By contrast
FREQ benefits from W2V only in the restaurant re-
views (but to a smaller degree than H&L), it bene-
fits only marginally in the hotel reviews, and in the
laptop reviews FREQ+W2V performs worse than
FREQ. A possible explanation is that the list of
candidate (distinct) aspect terms that FREQ pro-
duces already misses many aspect terms in the ho-
tel and laptop datasets; hence, W2V, which can
only prune aspect terms, cannot improve the re-
sults much, and in the case of laptops W2V has a
negative effect, because it prunes several correct
candidate aspect terms. All differences between
AWP scores on the same dataset are statistically
significant; we use stratified approximate random-
ization, which indicates p ? 0.01 in all cases.
13
Figure 3 shows the weighted precision and
weighted recall curves of the four methods. In
the restaurants dataset, our pruning improves
13
See http://masanjin.net/sigtest.pdf.
Method Restaurants Hotels Laptops
FREQ 43.40 30.11 9.09
FREQ+W2V 45.17 30.54 7.18
H&L 52.23 49.73 34.34
H&L+W2V 66.80 53.37 38.93
Table 2: Average weighted precision results (%).
the weighted precision of both H&L and FREQ;
by contrast it does not improve weighted re-
call, since it can only prune candidate as-
pect terms. The maximum weighted precision
of FREQ+W2V is almost as good as that of
H&L+W2V, but H&L+W2V (and H&L) reach
much higher weighted recall scores. In the hotel
reviews, W2V again improves the weighted pre-
cision of both H&L and FREQ, but to a smaller
extent; again W2V does not improve weighted re-
call; also, H&L and H&L+W2V again reach higher
weighted recall scores. In the laptop reviews,
W2V marginally improves the weighted precision
of H&L, but it lowers the weighted precision of
FREQ; again H&L and H&L+W2V reach higher
weighted recall scores. Overall, Fig. 3 confirms
that H&L+W2V is the best method.
6 Conclusions
We constructed and made publicly available three
new ATE datasets from three domains. We also
introduced weighted variants of precision, recall,
and average precision, arguing that they are more
appropriate for ATE. Finally, we discussed how
a popular unsupervised ATE method can be im-
proved by adding a new pruning mechanism that
uses continuous space vector representations of
words and phrases. Using our datasets and eval-
uation measures, we showed that the improved
method performs clearly better than the origi-
nal one, also outperforming a simpler frequency-
based baseline with or without our pruning.
51
References
A. Bagheri, M. Saraee, and F. Jong. 2013. An unsuper-
vised aspect detection model for sentiment analysis
of reviews. In Proceedings of NLDB, volume 7934,
pages 140?151.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, Bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In Pro-
ceedings of ACL, pages 440?447, Prague, Czech Re-
public.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings
of WSDM, pages 231?240, Palo Alto, CA, USA.
G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of WebDB, Providence,
RI, USA.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of KDD, pages
168?177, Seattle, WA, USA.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422?
446.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of SST, pages 1?8, Syd-
ney, Australia.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proceedings of EMNLP-CoNLL,
pages 1065?1074, Prague, Czech Republic.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion ob-
server: analyzing and comparing opinions on the
web. In Proceedings of WWW, pages 342?351,
Chiba, Japan.
B. Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies. Morgan & Claypool.
C. Long, J. Zhang, and X. Zhut. 2010. A review se-
lection approach for accurate feature rating estima-
tion. In Proceedings of COLING, pages 766?774,
Beijing, China.
C. D. Manning, P. Raghavan, and H. Sch?utze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. In Proceedings of Workshop at ICLR.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In
Proceedings of NIPS.
T. Mikolov, W.-T. Yih, and G. Zweig. 2013c. Linguis-
tic regularities in continuous space word representa-
tions. In Proceedings of NAACL HLT.
S. Moghaddam and M. Ester. 2010. Opinion digger:
an unsupervised opinion miner from unstructured
product reviews. In Proceedings of CIKM, pages
1825?1828, Toronto, ON, Canada.
B. Pang and L. Lee. 2005. Seeing stars: exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115?124, Ann Arbor, MI, USA.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT-EMNLP, pages 339?346, Van-
couver, Canada.
T. Sakai. 2004. Ranking the NTCIR systems based
on multigrade relevance. In Proceedings of AIRS,
pages 251?262, Beijing, China.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings
of NAACL, pages 300?307, Rochester, NY, USA.
M. Tsytsarau and T. Palpanas. 2012. Survey on min-
ing subjective data on the web. Data Mining and
Knowledge Discovery, 24(3):478?514.
C.-P. Wei, Y.-M. Chen, C.-S. Yang, and C. C Yang.
2010. Understanding what concerns consumers:
a semantic approach to product feature extraction
from consumer reviews. Information Systems and
E-Business Management, 8(2):149?167.
J. Yu, Z. Zha, M. Wang, and T. Chua. 2011. As-
pect ranking: identifying important product aspects
from online consumer reviews. In Proceedings of
NAACL, pages 1496?1505, Portland, OR, USA.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Group-
ing product features using semi-supervised learning
with soft-constraints. In Proceedings of COLING,
pages 1272?1280, Beijing, China.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering
product features for opinion mining. In Proceedings
of WSDM, pages 347?354, Hong Kong, China.
52
