Proceedings of NAACL HLT 2009: Short Papers, pages 105?108,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Importance of Sub-Utterance Prosody in Predicting Level of Certainty
Heather Pon-Barry
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA
ponbarry@eecs.harvard.edu
Stuart Shieber
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA
shieber@seas.harvard.edu
Abstract
We present an experiment aimed at under-
standing how to optimally use acoustic and
prosodic information to predict a speaker?s
level of certainty. With a corpus of utterances
where we can isolate a single word or phrase
that is responsible for the speaker?s level of
certainty we use different sets of sub-utterance
prosodic features to train models for predict-
ing an utterance?s perceived level of certainty.
Our results suggest that using prosodic fea-
tures of the word or phrase responsible for the
level of certainty and of its surrounding con-
text improves the prediction accuracy without
increasing the total number of features when
compared to using only features taken from
the utterance as a whole.
1 Introduction
Prosody is a fundamental part of human-to-human
spoken communication; it can affect the syntac-
tic and semantic interpretation of an utterance
(Hirschberg, 2003) and it can be used by speakers
to convey their emotional state. In recent years, re-
searchers have found prosodic features to be useful
in automatically detecting emotions such as annoy-
ance and frustration (Ang et al, 2002) and in dis-
tinguishing positive from negative emotional states
(Lee and Narayanan, 2005).
In this paper, we address the problem of predict-
ing the perceived level of certainty of a spoken ut-
terance. Specifically, we have a corpus of utter-
ances where it is possible to isolate a single word
or phrase responsible for the speaker?s level of cer-
tainty. With this corpus we investigate whether us-
ing prosodic features of the word or phrase causing
uncertainty and of its surrounding context improves
the prediction accuracy when compared to using fea-
tures taken only from the utterance as a whole.
This work goes beyond existing research by look-
ing at the predictive power of prosodic features ex-
tracted from salient sub-utterance segments. Pre-
vious work on uncertainty has examined the pre-
dictive power of utterance- and intonational phrase-
level prosodic features (Liscombe et al, 2005) as
well as the relative strengths of correlations between
level of certainty and sub-utterance prosodic fea-
tures (Pon-Barry, 2008). Our results suggest that
we can do a better job at predicting an utterance?s
perceived level of certainty by using prosodic fea-
tures extracted from the whole utterance plus ones
extracted from salient pieces of the utterance, with-
out increasing the total number of features, than by
using only features from the whole utterance.
This work is relevant to spoken language applica-
tions in which the system knows specific words or
phrases that are likely to cause uncertainty. For ex-
ample, this would occur in a tutorial dialogue system
when the speaker answers a direct question (Pon-
Barry et al, 2006; Forbes-Riley et al, 2008), or in
language (foreign or ESL) learning systems and lit-
eracy systems (Alwan et al, 2007) when new vocab-
ulary is being introduced.
2 Previous Work
Researchers have examined certainty in spoken lan-
guage using data from tutorial dialogue systems
(Liscombe et al, 2005) and data from an uncertainty
corpus (Pon-Barry, 2008).
Liscombe et al (2005) trained a decision tree
105
classifier on utterance-level and intonational phrase-
level prosodic features to distinguish between cer-
tain, uncertain, and neutral utterances. They
achieved 76% accuracy, compared to a 66% accu-
racy baseline (choosing the most common class).
We have collected a corpus of utterances spoken
under varying levels of certainty (Pon-Barry, 2008).
The utterances were elicited by giving adult native
English speakers a written sentence containing one
or more gaps, then displaying multiple options for
filling in the gaps and telling the speakers to read
the sentence aloud with the gaps filled in according
to domain-specific criteria. We elicited utterances
in two domains: (1) using public transportation in
Boston, and (2) choosing vocabulary words to com-
plete a sentence. An example is shown below.
Q: How can I get from Harvard to the Silver Line?
A: Take the red line to
a. South Station
b. Downtown Crossing
The term ?context? refers to the fixed part of the re-
sponse (?Take the red line to ?, in this exam-
ple) and the term ?target word? refers to the word or
phrase chosen to fill in the gap.
The corpus contains 600 utterances from 20
speakers. Each utterance was annotated for level
of certainty, on a 5-point scale, by five human
judges who listened to the utterances out of context.
The average inter-annotator agreement (Kappa) was
0.45. We refer to the average of the five ratings as
the ?perceived level of certainty? (the quantity we at-
tempt to predict in this paper).
We computed correlations between perceived
level of certainty and prosodic features extracted
from the whole utterance, the context, and the tar-
get word. Pauses preceding the target word were
considered part of the target word; all segmenta-
tion was done manually. Because the speakers had
unlimited time to read over the context before see-
ing the target words, the target word is considered
to be the source of the speaker?s confidence or un-
certainty; it corresponds to the decision that the
speaker had to make. Our correlation results sug-
gest that while some prosodic cues to level of cer-
tainty were strongest in the whole utterance, others
were strongest in the context or the target word. In
this paper, we extend this past work by testing the
prediction accuracy of models trained on different
subsets of these prosodic features.
3 Prediction Experiments
In our experiments we used 480 of the 600 utter-
ances in the corpus, those which contained exactly
one gap. (Some had two or three gaps.) We ex-
tracted the following 20 prosodic feature-types from
each whole utterance, context, and target word (a to-
tal of 60 features) using WaveSurfer1 and Praat2.
Pitch: minf0, maxf0, meanf0, stdevf0, rangef0, rel-
ative position minf0, relative position maxf0,
absolute slope (Hz), absolute slope (semitones)
Intensity: minRMS, maxRMS, meanRMS, stdev-
RMS, relative position minRMS, relative posi-
tion maxRMS
Temporal: total silence, percent silence, total dura-
tion, speaking duration, speaking rate
These features are comparable to those used in Lis-
combe et al?s (2005) prediction experiments. The
pitch and intensity features were represented as
z-scores normalized by speaker; the temporal fea-
tures were not normalized.
Next, we created a ?combination? set of 20 fea-
tures based on our correlation results. Figure 1 il-
lustrates how the combination set was created: for
each prosodic feature-type (each row in the table) we
chose either the whole utterance feature, the context
feature, or the target word feature, whichever one
had the strongest correlation with perceived level of
certainty. The selected features (highlighted in Fig-
ure 1) are listed below.
Whole Utterance: total silence, total duration,
speaking duration, relative position maxf0, rel-
ative position maxRMS, absolute slope (Hz),
absolute slope (semitones)
Context: minf0, maxf0, meanf0, stdevf0, rangef0,
minRMS, maxRMS, meanRMS, relative posi-
tion minRMS
Target Word: percent silence, speaking rate, rela-
tive position minf0, stdevRMS
1http://www.speech.kth.se/wavesurfer/
2http://www.fon.hum.uva.nl/praat/
106
Feature-type Whole Utterance Context Target Word
min f0 0.107 0.119 0.041
max f0 ?0.073 ?0.153 ?0.045
mean f0 0.033 0.070 ?0.004
stdev f0 ?0.035 ?0.047 ?0.043
range f0 ?0.128 ?0.211 ?0.075
rel. position min f0 0.042 0.022 0.046
rel. position max f0 0.015 0.008 0.001
abs. slope f0 (Hz) 0.275 0.180 0.191
abs. slope f0 (Semi) 0.160 0.147 0.002
min RMS 0.101 0.172 0.027
max RMS ?0.091 ?0.110 ?0.034
mean RMS ?0.012 0.039 ?0.031
stdev RMS ?0.002 ?0.003 ?0.019
rel. position min RMS 0.101 0.172 0.027
rel. position max RMS ?0.039 ?0.028 ?0.007
total silence ?0.643 ?0.507 ?0.495
percent silence ?0.455 ?0.225 ?0.532
total duration ?0.592 ?0.502 ?0.590
speaking duration ?0.430 ?0.390 ?0.386
speaking rate 0.090 0.014 0.136
1
Figure 1: The Combination feature set (highlighted in ta-
ble) was produced by selecting either the whole utterance
feature, the context feature, or the target word feature
for each prosodic feature-type, whichever one was most
strongly correlated with perceived level of certainty.
To compare the prediction accuracies of different
subsets of features, we fit five linear regression mod-
els to the feature sets. The five subsets are: (A)
whole utterance features only, (B) target word fea-
tures only, (C) context features only, (D) all fea-
tures, and (E) the combination feature set. We di-
vided the data into 20 folds (one fold per speaker)
and performed a 20-fold cross-validation for each
set of features. Each experiment fits a model us-
ing data from 19 speakers and tests on the remain-
ing speaker. Thus, when we test our models, we are
testing the ability to classify utterances of an unseen
speaker.
Table 1 shows the accuracies of the models
trained on the five subsets of features. The num-
bers reported are averages of the 20 cross-validation
accuracies. We report results for two cases: 5 pre-
diction classes and 3 prediction classes. We first
computed the prediction accuracy over five classes
(the regression output was rounded to the nearest
integer). Next, in order to compare our results to
those of Liscombe et al (2005), we recoded the
5-class results into 3-class results, following Pon-
Barry (2008), in the way that maximized inter-
annotator agreement. The naive baseline numbers
are the accuracies that would be achieved by always
choosing the most common class.
4 Discussion
Assuming that the target word is responsible for the
speaker?s level of certainty, it is not surprising that
the target word feature set (B) yields higher accura-
cies than the context feature set (C). It is also not sur-
prising that the set of all features (D) yields higher
accuracies than sets (A), (B), and (C).
The key comparison to notice is that the combi-
nation feature set (E), with only 20 features, yields
higher average accuracies than the utterance fea-
ture set (A): a difference of 6.42% for 5 classes
and 5.83% for 3 classes. This suggests that using a
combination of features from the context and target
word in addition to features from the whole utter-
ance leads to better prediction of the perceived level
of certainty than using features from only the whole
utterance.
One might argue that these differences are just
due to noise. To address this issue, we compared
the prediction accuracies of sets (A) and (E) per fold.
This is illustrated in Figure 2. Each fold in our cross-
validation corresponds to a different speaker, so the
folds are not identically distributed and we do not
expect each fold to yield the same prediction accu-
racy. That means that we should compare predic-
tions of the two feature sets within folds rather than
between folds. Figure 2 shows the correlations be-
tween the predicted and perceived levels of certainty
for the models trained on sets (A) and (E). The com-
bination set (E) predictions were more strongly cor-
related than whole utterance set (A) predictions in
16 out of 20 folds. This result supports our claim
that using a combination of features from the con-
text and target word in addition to features from the
whole utterance leads to better prediction of level of
certainty.
Our best prediction accuracy for the 3 class case,
74.79%, was slightly lower than the accuracy re-
ported by Liscombe et al (2005), 76.42%. However,
our difference from the naive baseline was 18.54%
where Liscombe et al?s was 10.42%. Liscombe et
al. randomly divided their data into training and test
sets, so it is unclear whether they tested on seen or
unseen speakers. Further, they ran one experiment
rather than a cross-validation, so their reported ac-
curacy may not be indicative of the entire data set.
We also trained support vector models on these
subsets of features. The main result was the same:
107
Table 1: Average prediction accuracies for the linear regression models trained on five subsets of prosodic features.
The models trained on the Combination feature set and the All feature set perform better than the other three models
in both the 3- and 5-class settings.
Feature Set Num Features Accuracy (5 classes) Accuracy (3 classes)
Naive Baseline N/A 31.46% 56.25%
(A) Utterance 20 39.00% 68.96%
(B) Target Word 20 43.13% 68.96%
(C) Context 20 37.71% 67.50%
(D) All 60 48.54% 74.58%
(E) Combination 20 45.42% 74.79%
Fold UTT COMBI13 0.60742805 0.74174205 1 0.13431410 0.71345083 0.84506209 1 0.131611272 0.65645441 0.7745844 1 0.1181299920 0.59862684 0.69875998 1 0.1001331419 0.61302941 0.70460363 1 0.091574223 0.67823016 0.75606366 1 0.07783355 0.54426476 0.61711862 1 0.072853864 0.74102672 0.81252066 1 0.0714939417 0.71910042 0.77176522 1 0.05266486 0.78220835 0.82806993 1 0.0458615818 0.66737245 0.71009756 1 0.0427251115 0.66996149 0.70962379 1 0.039662319 0.63477603 0.66365739 1 0.028881371 0.7359401 0.7631083 1 0.027168217 0.71645922 0.73071498 1 0.0142557512 0.78824649 0.79491313 1 0.0066666411 0.39557157 0.39381644 0 -0.0017551316 0.62168851 0.61984036 0 -0.0018481514 0.6971148 0.67762751 0 -0.019487298 0.82685033 0.80103581 0 -0.0258145216
00.20.4
0.60.81
0 2 4 6 8 10 12 14 16 18 20FoldC
orrelation C
oeff (R) CombinationUtterance
Figure 2: Correlations with perceived level of certainty
per fold for the Combination (O) and the Utterance (X)
feature set predictions, sorted by the size of the difference.
In 16 of the 20 experiments, the correlation coefficients
for the Combination feature set are greater than those of
the Utterance feature set.
the set of all features (D) and the combination set
(E) had better prediction accuracies than the utter-
ance feature set (A). In addition, the combination set
(E) had the best prediction accuracies (of all models)
in both the 3- and 5-class settings. The raw accura-
cies were approximately 5% lower than those of the
linear regression models.
5 Conclusion and Future Work
The results of our experiments suggest a better pre-
dictive model of level of certainty for systems where
words or phrases likely to cause uncertainty are
known ahead of time. Without increasing the total
number of features, combining select prosodic fea-
tures from the target word, the surrounding context
and the whole utterance leads to better prediction of
level of certainty than using features from the whole
utterance only. In the near future, we plan to exper-
iment with prediction models of the speaker?s self-
reported level of certainty.
Acknowledgments
This work was supported by a National Defense Sci-
ence and Engineering Graduate Fellowship.
References
Abeer Alwan, Yijian Bai, Matthew Black, et al 2007. A
system for technology based assessment of language
and literacy in young children: the role of multiple in-
formation sources. Proc. of IEEE International Work-
shop on Multimedia Signal Processing, pp. 26?30,
Chania, Greece.
Jeremy Ang, Rajdip Dhillon, Ashley Krupski, et al
2002. Prosody-based automatic detection of annoy-
ance and frustration in human-computer dialog. Proc.
of ICSLP 2002, pp. 2037?2040, Denver, CO.
Kate Forbes-Riley, Diane Litman, and Mihai Rotaru.
2008. Responding to student uncertainty during com-
puter tutoring: a preliminary evaluation. Proc. of the
9th International Conference on Intelligent Tutoring
Systems, Montreal, Canada.
Julia Hirschberg. 2003. Intonation and pragmatics. In
L. Horn and G. Ward (ed.), Handbook of Pragmatics,
Blackwell.
Chul Min Lee and Shrikanth Narayanan. 2005. Towards
detecting emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, 13(2):293?303.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. Proceedings of Eurospeech 2005, Lisbon,
Portugal.
Heather Pon-Barry, Karl Schultz, Elizabeth Bratt, Brady
Clark, and Stanley Peters. 2006. Responding to stu-
dent uncertainty in spoken tutorial dialogue systems.
International Journal of Artificial Intelligence in Edu-
cation 16:171-194.
Heather Pon-Barry. 2008. Prosodic manifestations of
confidence and uncertainty in spoken language. Proc.
of Interspeech 2008, pp. 74?77, Brisbane, Australia.
108
Automated Tutoring Dialogues for Training in Shipboard
Damage Control
John Fry, Matt Ginzton, Stanley Peters, Brady Clark & Heather Pon-Barry
Stanford University
Center for the Study of Language Information
Stanford CA 94305-4115 USA
{fry,mginzton,peters,bzack,ponbarry}@csli.stanford.edu
Abstract
This paper describes an application
of state-of-the-art spoken language
technology (OAA/Gemini/Nuance)
to a new problem domain: engaging
students in automated tutorial dia-
logues in order to evaluate and im-
prove their performance in a train-
ing simulator.
1 Introduction
Shipboard damage control refers to the task of
containing the effects of fire, explosions, hull
breaches, flooding, and other critical events
that can occur aboard Naval vessels. The
high-stakes, high-stress nature of this task, to-
gether with limited opportunities for real-life
training, make damage control an ideal target
for AI-enabled educational technologies like
training simulators and tutoring systems.
This paper describes the spoken dialogue
system we developed for automated critiquing
of student performance on a damage control
training simulator. The simulator is DC-
Train (Bulitko and Wilkins, 1999), an im-
mersive, multimedia training environment for
damage control. DC-Train?s training sce-
narios simulate a mixture of physical phenom-
ena (e.g., fire, flooding) and personnel issues
(e.g., casualties, communications, standard-
ized procedures). Our current tutoring sys-
tem is restricted fire damage scenarios only,
and in particular to the twelve fire scenar-
ios available in DC-Train version 2.5, but
in future versions we plan to support post-
session critiques for all of the damage phe-
nomena that will be modeled by DC-Train
4.0: fire, flooding, missile damage, and wall
or firemain ruptures.
2 Previous Work
Eliciting self-explanation from a student has
been shown to be a highly effective tutoring
method (Chi et al, 1994). For this reason,
a number of automated tutoring systems cur-
rently use NLP techniques to engage students
in reflective dialogues. Three notable exam-
ples are the medical Circsim tutor (Zhou et
al., 1999); the Basic Electricity and Electron-
ics (BE&E) tutor (Rose? et al, 1999); and
the computer literacy AutoTutor (Wiemer-
Hastings et al, 1999).
Our system shares several features with
these three tutoring systems:
A knowledge base Our system encodes
all domain knowledge relevant to supporting
intelligent tutoring feedback into a structure
called an Expert Session Summary (Section
4). These expert summaries encode causal
relationships between events on the ship as
well as the proper and improper responses to
shipboard crises.
Tutoring strategies In our system, as in
those above, the flow of dialogue is controlled
by (essentially) a finite-state transition net-
work (Fig. 1).
An interpretation component In our
system, the student?s speech is recognized and
parsed into logical forms (Section 3). A dia-
logue manager inspects the current dialogue
information state to determine how best to
incorporate each new utterance into the dia-
logue (Lemon et al, 2001).
Prompt
student review
of actions
Correct
student?s
report Prompt for
reflection on
START
END
continue"
"OK, let?s
event N...
Summary
of damage
main points
Review
performance
student?s
Evaluate
reflections
Correct
student?s
"You handled
this one well"
event 1
of damage
Summary
Brief
summary of
session
errors
Figure 1: Post-session dialogue move graph (simplified)
However, an important difference is that
the three systems above are entirely text-
based, whereas ours is a spoken dialogue sys-
tem. Our speech interface offers greater natu-
ralness than keyboard-based input. In this re-
spect, our system is similar to cove (Roberts,
2000), a training simulator for conning Navy
ships that uses speech to interact with the
student. But whereas cove uses short conver-
sational exchanges to coach the student dur-
ing the simulation, our system engages in ex-
tended tutorial dialogues after the simulation
has ended. Besides being more natural, spo-
ken language systems are also better suited to
multimodal interactions (viz., one can point
and click while talking but not while typing).
An additional significant difference between
our system and a number of other automated
tutoring systems is our use of ?deep? process-
ing techniques. While other systems utilize
?shallow? statistical approaches like Latent Se-
mantic Analysis (e.g. AutoTutor), our system
utilizes Gemini, a symbolic grammar. This
approach enables us to provide precise and
reliable meaning representations.
3 Implementation
To facilitate the implementation of multi-
modal, mixed-initiative tutoring interactions,
we decided to implement our system within
the Open Agent Architecture (OAA) (Martin
et al, 1999). OAA is a framework for coor-
dinating multiple asynchronous communicat-
ing processes. The core of OAA is a ?facilita-
tor? which manages message passing between
a number of software agents that specialize
in certain tasks (e.g., speech recognition or
database queries). Our system uses OAA to
coordinate the following five agents:
1. The Gemini NLP system (Dowding et
al., 1993). Gemini uses a single unifi-
cation grammar both for parsing strings
of words into logical forms (LFs) and for
generating sentences from LF inputs.
2. A Nuance speech recognition server,
which converts spoken utterances to
strings of words. The Nuance server re-
lies on a language model, which is com-
piled directly from the Gemini grammar,
ensuring that every recognized utterance
is assigned an LF.
3. The Festival text-to-speech system,
which ?speaks? word strings generated by
Gemini.
4. A Dialogue Manager which coordi-
nates inputs from the user, interprets the
user?s dialogue moves, updates the dia-
logue context, and delivers speech and
graphical outputs to the user.
5. A Critique Planner, described below
in Section 4.
Agents 1-3 are reusable, ?off-the-shelf? dia-
logue system components (apart from the
Gemini grammar, which must be modified for
each application). We implemented agents 4
and 5 in Java specifically for this application.
Variants of this OAA/Gemini/Nuance ar-
chitecture have been deployed successfully in
other dialogue systems, notably SRI?s Com-
mandTalk (Stent et al, 1999) and an un-
Figure 2: Screen shot of post-session tutorial dialogue system
manned helicopter interface developed in our
laboratory (Lemon et al, 2001).
4 Planning the dialogue
Each student session with DC-Train pro-
duces a session transcript, i.e. a time-stamped
record of every event (both computer- and
student-initiated) that occurred during the
simulation. These transcripts serve as the
input to our post-session Critique Planner
(CP).
The CP plans a post-session tutorial di-
alogue in two steps. In the first step, an
Expert Session Summary (ESS) is cre-
ated from the session transcript. The ESS
is a tree whose parent nodes represent dam-
age events and whose leaves represent actions
taken in response to those damage events.
Each student-initiated action in the ESS is
evaluated as to its timeliness and conformance
to damage control doctrine. Actions that the
student should have taken but did not are also
inserted into the ESS and flagged as such.
Each action node in the ESS therefore falls
into one of three classes: (i) correct actions;
(ii) errors of commission (e.g., the student
sets fire containment boundaries incorrectly);
and (iii) errors of omission (e.g., the student
fails to secure permission from the captain be-
fore flooding certain compartments).
Our current tutoring system covers scenar-
ios generated by DC-Train 2.5, which covers
fire scenarios only. Future versions will use
scenarios generated by DC-Train 4.0, which
covers damage control scenarios involving fire,
smoke, flooding, pipe and hull ruptures, and
equipment deactivation. Our current tutor-
ing system is based on an ESS graph that is
generated by an expert model that consists
of an ad-hoc set of firefighting rules. Future
versions will be based on an ESS graph that
is generated by an successor to the Minerva-
DCA expert model (Bulitko and Wilkins,
1999), an extended Petri Net envisionment-
based reasoning system. The new expert
model is designed to produce an ESS graph
during the course of problem solving that con-
tains nodes for all successful and unsuccessful
plan and goal achievement events, along with
an explanation structure for each graph node.
The second step in planning the post-
session tutorial dialogue is to produce a di-
alogue move graph (Fig. 1). This is a di-
rected graph that encodes all possible configu-
rations of dialogue structure and content that
can be handled by the system.
Generating an appropriate dialogue move
graph from an ESS requires pedagogical
knowledge, and in particular a tutoring strat-
egy. The tutoring strategy we adopted is
based on our analysis of videotapes of fifteen
actual DC-Train post-session critiques con-
ducted by instructors at the Navy?s Surface
Warfare Officer?s School in Newport, RI. The
strategy we observed in these critiques, and
implemented in our system, can be outlined
as follows:
1. Summarize the results of the simulation
(e.g., the final condition of the ship).
2. For each major damage event in the ESS:
(a) Ask the student to review his ac-
tions, correcting his recollections as
necessary.
(b) Evaluate the correctness of each stu-
dent action.
(c) If the student committed errors,
ask him how these could have been
avoided, and evaluate the correct-
ness of his responses.
3. Finally, review each type of error that
arose in step (2c).
A screen shot of the tutoring system in
action is shown in Fig. 2. As soon as a
DC-Train simulation ends, the dialogue sys-
tem starts up and the dialogue manager be-
gins traversing the dialogue move graph. As
the dialogue unfolds, a graphical representa-
tion of the ESS is revealed to the student in
piecemeal fashion as depicted in the top right
frame of Fig. 2.
Acknowledgments
This work is supported by the Depart-
ment of the Navy under research grant
N000140010660, a multidisciplinary univer-
sity research initiative on natural language in-
teraction with intelligent tutoring systems.
References
V V. Bulitko and D C. Wilkins. 1999. Automated
instructor assistant for ship damage control. In
Proceedings of AAAI-99, Orlando, FL, July.
M. T. H. Chi, N. de Leeuw, M. Chiu, and C.
LaVancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science,
18(3):439?477.
J. Dowding, J. Gawron, D. Appelt, J. Bear, L.
Cherny, R. C. Moore, and D. Moran. 1993.
Gemini: A natural language system for spoken-
language understanding. In Proceedings of the
ARPA Workshop on Human Language Technol-
ogy.
O. Lemon, A. Bracy, A. Gruenstein, and S. Pe-
ters. 2001. A multi-modal dialogue system for
human-robot conversation. In Proceedings of
NAACL 2001.
D. Martin, A. Cheyer, and D. Moran. 1999.
The Open Agent Architecture: a framework for
building distributed software systems. Applied
Artificial Intelligence, 13(1-2).
B. Roberts. 2000. Coaching driving skills in
a shiphandling trainer. In Proceedings of the
AAAI Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. P. Rose?, B. Di Eugenio, and J. D. Moore. 1999.
A dialogue based tutoring system for basic elec-
tricity and electronics. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED?99), pages 759?
761. IOS Press, Amsterdam.
A. Stent, J. Dowding, J. Gawron, E. O. Bratt, and
R. C. Moore. 1999. The CommandTalk spoken
dialogue system. In Proceedings of ACL ?99,
pages 183?190, College Park, MD.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelli-
gent tutor?s comprehension of students with la-
tent semantic analysis. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED?99), pages 535?
542. IOS Press, Amsterdam.
Y. Zhou, R. Freedman, M. Glass, J. A. Michael,
A. A. Rovick, and M. W. Evens. 1999. Deliver-
ing hints in a dialogue-based intelligent tutoring
system. In Proceedings of AAAI-99, Orlando,
FL, July.
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 28?35,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Interactive Question Answering and Constraint Relaxation
in Spoken Dialogue Systems
Sebastian Varges
CSLI
Stanford University
Stanford, CA 94305, USA
varges@stanford.edu
Fuliang Weng, Heather Pon-Barry
Research and Technology Center
Robert Bosch Corporation
4009 Miranda Ave, Palo Alto, CA, USA
fuliang.weng, heather.pon-barry
@rtc.bosch.com
Abstract
We explore the relationship between ques-
tion answering and constraint relaxation in
spoken dialog systems. We develop dia-
logue strategies for selecting and present-
ing information succinctly. In particular,
we describe methods for dealing with the
results of database queries in information-
seeking dialogs. Our goal is to structure
the dialogue in such a way that the user is
neither overwhelmed with information nor
left uncertain as to how to refine the query
further. We present evaluation results ob-
tained from a user study involving 20 sub-
jects in a restaurant selection task.
1 Introduction
Information presentation is an important issue
when designing a dialogue system. This is espe-
cially true when the dialogue system is used in a
high-stress environment, such as driving a vehi-
cle, where the user is already occupied with the
driving task. In this paper, we explore efficient
dialogue strategies to address these issues, and
present implemented knowledge management, di-
alogue and generation components that allow cog-
nitively overloaded users ? see (Weng et al, 2004),
for example ? to obtain information from the di-
alogue system in a natural way. We describe a
knowledge manager that provides factual and on-
tological information, a content optimizer that reg-
ulates the amount of information, and a genera-
tor that realizes the selected content. The domain
data is divided between domain-specific ontolo-
gies and a database back-end. We use the system
for both restaurant selection and MP3 player tasks,
and conducted experiments with 20 subjects.
There has been substantial previous work on
information presentation in spoken dialogue sys-
tems. (Qu and Green, 2002) also present a
constraint-based approach to cooperative informa-
tion dialogue. Their experiments focus on over-
constrained queries, whereas we also deal with un-
derconstrained ones. Moreover, we guide the user
through the dialogue by making suggestions about
query refinements, which serve a similar ro?le to
the conditional responses of (Kruijff-Korbayova et
al., 2002). (Hardy et al, 2004) describe a dialogue
system that uses an error-correcting database man-
ager for matching caller-provided information to
database entries. This allows the system to se-
lect the most likely database entry, but, in contrast
to our approach, does not modify constraints at
a more abstract level. In contrast to all the ap-
proaches mentioned above, our language gener-
ator uses overgeneration and ranking techniques
(Langkilde, 2000; Varges and Mellish, 2001).
This facilitates variation and alignment with the
user utterance.
A long-standing strand of research in NLP is
in natural language access to databases (Androut-
sopoulos et al, 1995). It mainly focused on map-
ping natural language input to database queries.
Our work can be seen as an extension of this work
by embedding it into a dialogue system and al-
lowing the user to refine and relax queries, and
to engage in clarification dialogs. More recently,
work on question answering (QA) is moving to-
ward interactive question answering that gives the
user a greater role in the QA process (HLT, forth-
coming). QA systems mostly operate on free text
whereas we use a relational database. (Thus, one
needs to ?normalize? the information contained in
free text to use our implemented system without
further adaption.)
28
In the following section, we give an overview
of the dialogue system. We then describe the
knowledge management, dialogue and generation
components in separate sections. In section 6 we
present evaluation results obtained from a user
study. This is followed by a discussion section and
conclusions.
2 System architecture
Our dialogue system employs the following archi-
tecture: the output of a speech recognizer (Nu-
ance, using a statistical language model) is ana-
lyzed by both a general-purpose statistical depen-
dency parser and a (domain-specific) topic classi-
fier. Parse trees and topic labels are matched by
the ?dialogue move scripts? of the dialogue man-
ager (Mirkovic and Cavedon, 2005; Weng et al,
2005). The scripts serve to license the instantia-
tion of dialogue moves and their integration into
the ?dialogue move tree.? The use of dialogue
move scripts is motivated by the need to quickly
tailor the system to new domains: only the scripts
need to be adapted, not the underlying machinery
implemented in Java. The scripts define short se-
quences of dialog moves, for example a command
move (?play song X?) may be followed either by
a disambiguation question or a confirmation that
the command will be executed. A dialogue pro-
ceeds by integrating such scripted sequences into
the dialogue move tree, yielding a relatively ?flat?
dialogue structure.
Query constraints are built by dialogue move
scripts if the parse tree matches input patterns
specified in the scripts. These query constraints
are the starting point for the processing strategies
described in this paper. The dialogue system is
fully implemented and has been used in restau-
rant selection and MP3 player tasks. There are 41
task-independent, generic dialogue move scripts,
52 restaurant selection scripts and 89 MP3 player
scripts. The examples in this paper are mostly
taken from the restaurant selection task.
3 Knowledge and Content management
The Knowledge Manager (KM) controls access to
domain knowledge that is structured according to
domain-dependent ontologies. The KM makes use
of OWL, a W3C standard, to represent the onto-
logical relationships between domain entities. The
knowledge base can be dynamically updated with
new instances at any point. In a typical interac-
tion, the Dialog Manager converts a user?s query
into a semantic frame (i.e., a set of semantic con-
straints) and sends this to the KM via the content
optimizer. For example, in the Restaurant domain,
a request such as ?I want to find an inexpensive
Japanese restaurant that takes reservations? results
in the semantic frame below, where Category is a
system property, and the other constraints are in-
herited properties of the Restaurant class:
(1) system:Category = restaurant:Restaurant
restaurant:PriceLevel = 0-10
restaurant:Cuisine = restaurant:japanese
restaurant:Reservations = yes
In addition to the KM module, we employ a
Content Optimization (CO) module that acts as
an intermediary between dialogue and knowledge
management during the query process. It receives
semantic frames from the Dialogue Manager, re-
vises the semantic frames if necessary (see below),
and queries the Knowledge Manager.
The content optimizer also resolves remaining
ambiguities in the interpretation of constraints.
For example, if the user requests an unknown cui-
sine type, the otherwise often accurate classifier
will not be able to provide a label since it oper-
ates under a closed-world assumption. In contrast,
the general purpose parser may be able to pro-
vide an accurate syntactic analysis. However, the
parse still needs to be interpreted by the content
optimizer which has the domain-specific knowl-
edge to determine that ?Montenegrin restaurant?
is a cuisine constraint rather than a service level
constraint, for example. (See also section 7).
Depending on the items in the query result set,
configurable properties, and (potentially) a user
model, the CO module selects and performs an ap-
propriate optimization strategy. To increase porta-
bility, the module contains a library of domain-
independent strategies and makes use of external
configuration files to tailor it to specific domains.
The CO module can modify constraints de-
pending on the number of items in the result
set, the system ontology, and information from
a user model. Constraints can be relaxed, tight-
ened, added or removed. The manner in which
a constraint is modified depends on what kind of
values it takes. For example, for the Cuisine
constraint, values are related hierarchically (e.g.,
Chinese, Vietnamese, and Japanese are all sub-
types of Asian), whereas PriceLevel values are
linear (e.g., cheap, moderate, expensive), and
acceptsCreditCards values are binary (e.g., ac-
29
cepted or not accepted).
If the original query returns no results, the con-
tent optimizer selects a constraint to modify and
then attempts to relax the constraint value. If re-
laxation is impossible, it removes the constraint
instead. Constraint relaxation makes use of the
ontological relationships in the knowledge base.
For example, relaxing a Cuisine constraint entails
replacing it with its parent-concept in the domain
ontology. Relaxing a linear constraint entails re-
placing the current value with an adjacent value.
Relaxing a binary constraint entails replacing the
current value with its opposite value.
Based on the ontological structures, the content
optimizer also calculates statistics for every set of
items returned by the knowledge manager in re-
sponse to a user?s query. If the result set is large,
these figures can be used by the dialogue manager
to give meaningful responses (e.g., in the MP3 do-
main, ?There are 85 songs. Do you want to list
them by a genre such as Rock, Pop, or Soul??).
The content optimizer also produces constraints
that represent meta-knowledge about the ontology,
for example, in response to a user input ?What
cuisines are there??:
(2) rdfs:subClassOf = restaurant:Cuisine
The processing modules described in the next
sections can use meta-level constraints in similar
ways to object-level constraints (see (1)).
4 Dialogue strategies for dealing with
query results
In the following two sections, we describe how
our dialogue and generation strategies tie in with
the choices made by the content optimizer. Con-
sider the following discourse-initial interaction for
which the semantic frame (1) is constructed:
(3)
U: i want to find an inexpensive Japanese
restaurant that takes reservations
S: I found 9 inexpensive Japanese
restaurants that take reservations .
Here are the first few :
S: GINZA JAPANESE RESTAURANT
S: OKI SUSHI CAFE
S: YONA SUSHI
S: Should I continue?
The example query has a relatively small result
set which can be listed directly. This is not always
the case, and thus we need dialogue strategies that
deal with different result set sizes. For example, it
does not seem sensible to produce ?I found 2500
restaurants. Here are the first few: ...?. At what
point does it become unhelpful to list items? We
do not have a final answer to this question ? how-
ever, it is instructive that the (human) wizard in
our data collection experiments did not start list-
ing when the result set was larger than about 10
items. In the implemented system, we define di-
alogue strategies that are activated at adjustable
thresholds.
Even if the result set is large and the system
does not list any result items, the user may still
want to see some example items returned for the
query. This observation is based on comments
by subjects in experimental dry-runs that in some
cases it was difficult to obtain any query result at
all. For example, speech recognition errors may
make it difficult to build up a sufficiently complex
query. In response to this, we always give some
example items even if the result set is large. (An
alternative would be to start listing items after a
certain number of dialogue turns.) Furthermore,
the system should encourage the user to refine the
query by suggesting constraints that have not been
used yet. This is done by maintaining a list of con-
straints in the generator that is used up as the di-
alogue progresses. This list is roughly ordered by
how likely the constraint will be useful. For exam-
ple, using cuisine type is suggested before propos-
ing to ask for information about reservations or
credit cards.
In our architecture, information flows from the
CO module to the generator (see section 5) via the
dialogue move scripts of the dialogue manager.
These are conditioned on the size of the final re-
sult set and whether or not any modifications were
performed. Table 1 summarizes the main dialogue
strategies. These dialogue strategies represent im-
plicit confirmations and are used if NLU has a high
confidence in its analysis of the user utterance (see
(Varges and Purver, 2006) for more details on our
handling of robustness issues). Small result sets
up to a threshold t1 are listed in a single sentence.
For medium-sized result sets up to a threshold t2,
the system starts listing immediately. For large re-
sult sets, the generator shows example items and
makes suggestions as to what constraint the user
may use next. If the CO module performs any con-
straint modification, the first, constraint realizing
sentence of the system turns reflects the modifica-
tion. (?NP-original? and ?NP-optimized? in table 1
are used for brevity and are explained in the next
section.)
30
|resultfinal| mod example realization fexp
s1a 0 no I?m sorry but I found no restaurants on Mayfield Road that serve Mediterranean food. 0
s1b 0 yes I?m sorry but I found no [NP-original]. I did not even find any [NP-optimized]. 0
s2a small: no There are 2 cheap Thai restaurants in Lincoln in my database: Thai Mee Choke and 61
> 0, < t1 Noodle House.
s2b small yes I found no cheap Greek restaurants that have a formal dress code but there are 0
4 inexpensive restaurants that serve other Mediterranean food and have a formal
dress code in my database: ... .
s3a medium: no I found 9 restaurants with a two star rating and a formal dress code that are open 212
>= t1, < t2 for dinner and serve French food. Here are the first ones: ... .
s3b medium yes I found no [NP-original]. However, there are N [NP-optimized]. Here are the first few: ... . 5
s4a large: no I found 258 restaurants on Page Mill Road, for example Maya Restaurant , 300
>= t2 Green Frog and Pho Hoa Restaurant. Would you like to try searching by cuisine?
s4b large yes I found no [NP-original]. However, there are N [NP-optimized]. Would you like to try 16
searching by [Constraint]?
Table 1: Dialogue strategies for dealing with query results (last column explained in sec. 6)
5 Generation
The generator produces turns that verbalize the
constraints used in the database query. This is
important since the system may miss or misinter-
pret constraints, leading to uncertainty for the user
about what constraints were used. For this rea-
son, a generic system response such as ?I found 9
items.? is not sufficient.
The input to the generator consists of the name
of the dialogue move and the relevant instantiated
nodes of the dialogue move tree. From the in-
stantiated move nodes, the generator obtains the
database query result including information about
query modifications. The core of the generator is
a set of productions1 written in the Java Expert
System Shell (Friedman-Hill, 2003). We follow
the bottom-up generation approach for production
systems described in (Varges, 2005) and perform
mild overgeneration of candidate moves, followed
by ranking. The highest-ranked candidate is se-
lected for output.
Productions map individual database con-
straints to phrases such as ?open for lunch?,
?within 3 miles? and ?a formal dress code?, and
recursively combine them into NPs. This includes
the use of coordination to produce ?restaurants
with a 5-star rating and a formal dress code?,
for example. The NPs are integrated into sen-
tence templates, several of which can be combined
to form an output candidate turn. For example,
a constraint realizing template ?I found no [NP-
1Productions are ?if-then? rules that operate over a shared
knowledge base of facts.
original] but there are [NUM] [NP-optimized] in
my database? (see below for further explanation)
can be combined with a follow-up sentence tem-
plate such as ?You could try to look for [NP-
constraint-suggestion]?.
The selection of which sentence template to use
is determined by the dialogue move scripts. Typ-
ically, a move-realizing production produces sev-
eral alternative sentences. On the other hand, the
NP generation rules realize constraints regardless
of the specific dialogue move at hand. This al-
lows us to also use them for clarification ques-
tions based on constraints constructed from classi-
fier information if the parser and associated parse-
matching patterns fail; all that is required is a new
sentence template, for example ?Are you looking
for [NP]??. We currently use 102 productions
overall in the restaurant and MP3 domains, 38 of
them to generate NPs that realize 19 possible input
constraints (for both domains).
The decision of the CO module to relax or re-
move constraints also affects the generator: there
are two sets of constraints, an ?original? one di-
rectly constructed from the user utterance, and an
?optimized? one used by the KM module to obtain
the query result (see section 3). In case of con-
straint modifications, these two sets are not identi-
cal but often overlapping. To avoid generating sep-
arate sets of NPs independently for the two con-
straint sets, we assign unique indices to the con-
straints and hand the generator two index sets as
targets of NP generation. We overgenerate NPs
and check their index sets before integrating them
into sentence templates.
31
turn speaker utterance
1.1 User i need a restaurant with a five star food rating excellent service
1.2 System I found 425 restaurants with a five star rating and excellent service , for example
EL CAPITAN FISH MARKET , K-NINE COUNTRY CLUB , and KAISERHOF GERMAN
AMERICAN . You could try to look for cuisine type .
1.3 User which restaurants take reservations
1.4 System I found 353 restaurants that have a five star rating , have excellent service and
take reservations , for example GRAND CENTRAL CAFE , CHASIN CHICKEN , and
CHON WOO RESTAURANT . Would you like to try searching by price ?
Figure 1: Example dialogue 1 from system evaluation using restaurant selection task
Ranking of candidate output moves is done by
using a combination of factors. First, the ranker
computes an alignment score for each candidate,
based on its ngram-based overlap with the user
utterance. For example, this allows us to prefer
?restaurants that serve Chinese food? over ?Chi-
nese restaurants? if the user used a wording more
similar to the first. We note that the Gricean
Maxim of Brevity, applied to NLG in (Dale and
Reiter, 1995), suggests a preference for the sec-
ond, shorter realization. However, if the user
thought it necessary to use ?serves?, maybe to
avoid confusion of constraints or even to correct an
earlier mislabeling, then the system should make
it clear that it understood the user correctly by
using those same words, thus preferring the first
realization. Mild overgeneration combined with
alignment also allows us to map the constraint
PriceLevel=0-10 in example (1) above to both
?cheap? and ?inexpensive?, and use alignment to
?play back? the original word choice to the user.
As these examples show, using alignment for rank-
ing in NLG allows one to employ overgeneration
techniques even in situations where no corpus data
is available.2
Second, ranking uses a variation score to ?cycle?
over sentence-level paraphrases. In the extreme
case of repeated identical user inputs, the system
simply chooses one paraphrase after the other, and
starts over when all paraphrases have been used.
Third, we use an ngram filter based on bad
examples ngrams, removing, for example, ?Chi-
nese cheap restaurants? but keeping ?cheap Chi-
nese restaurant.? For generalization, we replace
constraint realizations with semantic tags derived
from the constraint names (except for the head
noun), for example the trigram ?CUISINE PRICE
restaurants?. An alternative is to use a more com-
2However, we do have wizard-of-oz data to inform the
system design (see section 7).
plex grammar formalism to prevent ungrammati-
cal candidate moves.
6 Evaluation
We conducted experimental studies involving 20
subjects in a MP3 player task and 20 subjects in a
restaurant selection task. In the following, we con-
centrate on the restaurant selection task because it
is more challenging for constraint handling and in-
formation presentation.
Each subject in the restaurant selection task
was given 9 scenario descriptions involving 3 con-
straints. Subjects were instructed to use their own
words to find a fitting restaurant. We use a back-
end database of 2500 restaurants containing the
following information for each restaurant: restau-
rant name, cuisine type, city and street names,
service level, rating, whether they accept credit
cards or reservations, price level, open hours, dress
code, additional information (e.g. vegetarian food)
and distance from current location. Some of these
constraints have a fixed number of values (e.g.
service level and dress code), whereas those tak-
ing named entities as values are obviously open-
ended.
Figures 1 and 2 show two of the dialogues from
the experiments. To exemplify the challenges
the system is faced with, user turns are shown
in the form of the output of the speech recog-
nizer (we currently use the best ASR hypothesis
as input to the parser). The task of the first dia-
logue (figure 1) was to find a restaurant that has
the following constraints: [Reservations=yes, Ser-
viceLevel:excellent, Rating=5star]. Despite some
minor speech recognition errors, the system cor-
rectly builds a query for the two constraints pro-
vided by the user in (1.1.) and verbalizes these in
(1.2) to show its understanding of the user input.
It also gives some example items and makes a sug-
gestion. This suggestion is not taken up by the user
32
turn speaker utterance
2.1 User breakfast
2.2 System Are you looking for a restaurant that is open for breakfast ?
2.3 User yes located on bower street
2.4 System I found 71 restaurants on bower street that serve breakfast , for example ,
SOUPLANTATION-TO-GO , RAMAYANI WESTWOOD , and , PHILLY WEST .
You could try to look for cuisine type .
2.5 User which
2.6 System I?m sorry, I do not understand what you mean by: which
2.7 System Please try rephrasing that.
2.8 User casual dress
2.9 System There are 51 restaurants on bower street that serve breakfast and have a casual
dress code in my database , for example , BERTY?S , LABOTETEGA , and ,
JAVAN RESTAURANT . Would you like to try searching by price ?
2.10 User british
2.11 System I found no restaurants on bower street that serve british food , have a casual
dress code and are open for breakfast but there are 8 restaurants on bower street that are open for
breakfast , serve European food and have a casual dress code in my database . Here are the first few :
2.12 System HOTEL BEL-AIR
Figure 2: Example dialogue 2 from system evaluation using restaurant selection task
? in fact, we find that suggestions are generally not
taken up by the user. We believe this is due to the
nature of the tasks, which specified exactly which
criteria to match. On the other hand, in more open
application scenarios, where users may not know
what questions can be asked, suggestions may be
useful. In (1.3) the user issues a sub-query that
further constrains the result set. By again summa-
rizing the constraints used, the system confirms in
(1.4) that it has interpreted the new constraint as a
revision of the previous query. The alternative is
to start a new query, which would be wrong in this
context.
The task of the second dialogue, figure 2, was to
find a restaurant that meets the constraints [Busi-
nessHours:breakfast, StreetName=?bower street?,
DressCode=casual]. This user tends to give
shorter, keyword-style input to the system (2.1,
2.8). In (2.3), the user reacts to a clarification
question and adds another constraint which the
system summarizes in (2.4). (2.5) is an ASR er-
ror which the system cannot handle (2.6, 2.7). The
user constraint of (2.8) is correctly used to revise
the query (2.9), but ?british? (2.10) is another ASR
error that leads to a cuisine constraint not intended
in the scenario/by the user. This additional con-
straint yields an empty result set, from which the
system recovers automatically by relaxing the hi-
erarchically organized cuisine constraint to ?Eu-
ropean food?. In (2.11) the system uses dialogue
strategy s3b for medium-sized result sets with con-
straint modifications (section 4). The result of
both dialogues is that all task constraints are met.
We conducted 20 experiments in the restaurant
domain, 2 of which were restarted in the middle.
Overall, 180 tasks were performed involving 1144
user turns and 1818 system turns. Two factors con-
tributing to the higher number of system turns are
a) some system turns are counted as two turns,
such as 2.6, 2.7 in figure 2, and b) restaurants in
longer enumerations of result items are counted as
individual turns. On average, user utterances are
significantly shorter than system utterances (4.9
words, standard deviation ? = 3.82 vs 15,4 words,
? = 13.53). This is a result of the ?constraint sum-
maries? produced by the generator. The high stan-
dard deviation of the system utterances can be ex-
plained by the above-mentioned listing of individ-
ual result items (e.g. utterance (2.12) in figure 2).
We collected usage frequencies for the dia-
logue strategies presented in section 4: there was
no occurrence of empty final result sets (strat-
egy s1a/b) because the system successfully re-
laxed constraints if it initially obtained no results.
Strategy s2a (small result sets without modifica-
tions) was used for 61 inputs, i.e. constraint sets
constructed from user utterances. Strategy s3a/b
(medium-sized result sets) was used for 217 times
and required constraint relaxations in 5 cases.
Strategy s4a/b (large result sets) was used for
33
316 inputs and required constraint relaxations in
16 cases. Thus, the system performed constraint
modifications in 21 cases overall. All of these
yielded non-empty final result sets. For 573 in-
puts, no modification was required. There were no
empty final result set despite modifications.
On average, the generator produced 16 output
candidates for inputs of two constraints, 160 can-
didates for typical inputs of 3 constraints and 320
candidates for 4 constraints. Such numbers can
easily be handled by simply enumerating candi-
dates and selecting the ?best? one.
Task completion in the experiments was high:
the subjects met al target constraints in 170 out of
180 tasks, i.e. completion rate was 94.44%. An
error analysis revealed that the reasons for only
partially meeting the task constraints were varied.
For example, in one case a rating constraint (?five
stars?) was interpreted as a service constraint by
the system, which led to an empty result set. The
system recovered from this error by means of con-
straint relaxation but the user seems to have been
left with the impression that there are no restau-
rants of the desired kind with a five star rating.
7 Discussion
Based on wizard-of-oz data, the system alter-
nates specific and unspecific refinement sugges-
tions (?You could search by cuisines type? vs ?Can
you refine your query??). Furthermore, many of
the phrases used by the generator are taken from
wizard-of-oz data too. In other words, the sys-
tem, including the generator, is informed by em-
pirical data but does not use this data directly (Re-
iter and Dale, 2000). This is in contrast to genera-
tion systems such as the ones described in (Langk-
ilde, 2000) and (Varges and Mellish, 2001).
Considering the fact that the domain ontology
and database schema are known in advance, it is
tempting to make a closed world assumption in
the generator (which could also help system de-
velopment and testing). However, this seems too
restrictive: assume, for example, that the user has
asked for Montenegrin food, which is an unknown
cuisine type, and that the statistical parser com-
bined with the parse-matching patterns in the di-
alogue manager has labeled this correctly. The
content optimization module will remove this con-
straint since there is no Montenegrin restaurant in
the database. If we now want to generate ?I did not
find any restaurants that serve Montenegrin food
...?, we do need to be able to use generation input
that uses unseen attribute-value pairs. The price
one has to pay for this increased robustness and
flexibility is, of course, potentially bad output if
NLU mislabels input words. More precisely, we
find that if any one of the interpretation modules
makes an open-world assumption, the generator
has to do as well, at least as long as we want to
verbalize the output of that module.
7.1 Future work
Our next application domain will be in-car naviga-
tion dialogues. This will involve dialogues that de-
fine target destinations and additional route plan-
ning constraints. It will allow us to explore the
effects of cognitive constraints due to changing
driving situations on dialogue behavior. The nav-
igation domain may also affect the point of inter-
action between dialogue system and external de-
vices: we may query a database to disambiguate
proper names such as street names as soon as these
are mentioned by the user, but start route planning
only when all planning constraints are collected.
An option for addressing the current lack of a
user model is to extend the work in (Cheng et al,
2004). They select the level of detail to be com-
municated to the user by representing the driver?s
route knowledge to avoid repeating known infor-
mation.
Another avenue of future research is to automat-
ically learn constraint relaxation strategies from
(appropriately annotated) evaluation data. User
modeling could be used to influence the order in
which refinement suggestions are given and deter-
mine the thresholds for the information presenta-
tion moves described in section 4.
One could handle much larger numbers of gen-
eration candidates either by using packing (Langk-
ilde, 2000) or by interleaving rule-based genera-
tion with corpus-based pruning (Varges and Mel-
lish, 2001) if complexity should become an issue
when doing overgeneration.
8 Conclusions
We described strategies for selecting and present-
ing succinct information in spoken dialogue sys-
tems. Verbalizing the constraints used in a query is
crucial for robustness and usability ? in fact, it can
be regarded as a special case of providing feed-
back to the user about what the system has heard
and understood (see (Traum, 1994), for example).
34
The specific strategies we use include ?backing-
off? to more general constraints (by the system)
or suggesting query refinements (to be requested
explicitly by the user). Our architecture is config-
urable and open: it can be parametrized by em-
pirically derived values and extended by new con-
straint handling techniques and dialogue strate-
gies. Constraint relaxation techniques have widely
been used before, of course, for example in syn-
tactic and semantic processing. The presented pa-
per details how these techniques, when used at the
content determination level, tie in with dialogue
and generation strategies. Although we focussed
on the restaurant selection task, our approach is
generic and can be applied across domains, pro-
vided that the dialogue centers around accessing
and selecting potentially large amounts of factual
information.
Acknowledgments This work is supported by
the US government?s NIST Advanced Technology
Program. Collaborating partners are CSLI, Robert
Bosch Corporation, VW America, and SRI Inter-
national. We thank the many people involved in
system design, development and evaluation, and
the reviewers of this paper.
References
Ion Androutsopoulos, G.D. Ritchie, and P. Thanisch.
1995. Natural Language Interfaces to Databases ?
An Introduction. Natural Language Engineering,
1(1):29?81.
Hua Cheng, Lawrence Cavedon, and Robert Dale.
2004. Generating Navigation Information Based on
the Driver?s Route Knowledge. In Proceedings of
the Coling 2004 Workshop on Robust and Adap-
tive Information Processing for Mobile Speech In-
terfaces, pages 31?38, Geneva, Switzerland.
Robert Dale and Ehud Reiter. 1995. Computational
Interpretations of the Gricean Maxims in the Gener-
ation of Referring Expressions. Cognitive Science,
19:233?263.
Ernest Friedman-Hill. 2003. Jess in Action: Java
Rule-Based Systems. Manning Publications.
Hilda Hardy, Tomek Strzalkowski, Min Wu, Cristian
Ursu, Nick Webb, Alan Biermann, R. Bryce Inouye,
and Ashley McKenzie. 2004. Data-driven strategies
for an automated dialogue system. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
71?78, Barcelona, Spain, July.
forthcoming. Proceedings of the workshop on Interac-
tive Question Answering at HLT-NAACL 2006.
Ivana Kruijff-Korbayova, Elena Karagjosova, and Stef-
fan Larsson. 2002. Enhancing collaboration with
conditional responses in information-seeking dia-
logues. In Proc. of 6th workshop on the semantics
and pragmatics of dialogue (EDILOG-02).
Irene Langkilde. 2000. Forest-based Statistical Sen-
tence Generation. In Proc NAACL-00, pages 170?
177.
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING).
Yan Qu and Nancy Green. 2002. A Constraint-based
Approach for Cooperative Information-Seeking Di-
alogue. In Proceedings of the International Work-
shop on Natural Language Generation (INLG-02).
Ehud Reiter and Robert Dale. 2000. Building Applied
Natural Language Generation Systems. Cambridge
University Press, Cambridge, UK.
David Traum. 1994. A Computational Theory
of Grounding in Natural Language Conversation.
Ph.D. thesis, Computer Science Dept., U. Rochester.
Sebastian Varges and Chris Mellish. 2001. Instance-
based Natural Language Generation. In Proc.
NAACL-01.
Sebastian Varges and Matthew Purver. 2006. Ro-
bust language analysis and generation for spoken di-
alogue systems (short paper). In Proceedings of the
ECAI 06 Workshop on the Development and Evalu-
ation of Robust Spoken Dialogue Systems.
Sebastian Varges. 2005. Chart generation using pro-
duction systems (short paper). In Proc. of 10th Eu-
ropean Workshop On Natural Language Generation.
Fuliang Weng, L. Cavedon, B. Raghunathan,
D. Mirkovic, H. Cheng, H. Schmidt, H. Bratt,
R. Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg,
and C. Bergmann. 2004. Developing a conversa-
tional dialogue system for cognitively overloaded
users. In Proceedings of the International Congress
on Intelligent Transportation Systems (ICSLP).
Fuliang Weng, Lawrence Cavedon, Badri Raghu-
nathan, Danilo Mirkovic, Ben Bei, Heather Pon-
Barry, Harry Bratt, Hua Cheng, Hauke Schmidt, Ro-
hit Mishra, Brian Lathrop, Qi Zhang, Tobias Schei-
deck, Kui Xu, Tess Hand-Bender, Stanley Peters,
Liz Shriberg, and Carsten Bergmann. 2005. A Flex-
ible Conversational Dialog System for MP3 Player.
In demo session of HLT-EMNLP 2005.
35
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 23?27,
Baltimore, Maryland, USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Finding Eyewitness Tweets During Crises
Fred Morstatter
1
, Nichola Lubold
1
, Heather Pon-Barry
1
, J ?urgen Pfeffer
2
, and Huan Liu
1
1
Arizona State University, Tempe, Arizona, USA
2
Carnegie Mellon University, Pittsburgh, Pennsylvania, USA
{fred.morstatter, nlubold, ponbarry, huan.liu}@asu.edu, jpfeffer@cs.cmu.edu
Abstract
Disaster response agencies incorporate so-
cial media as a source of fast-breaking in-
formation to understand the needs of peo-
ple affected by the many crises that oc-
cur around the world. These agencies look
for tweets from within the region affected
by the crisis to get the latest updates on
the status of the affected region. However
only 1% of all tweets are ?geotagged? with
explicit location information. In this work
we seek to identify non-geotagged tweets
that originate from within the crisis region.
Towards this, we address three questions:
(1) is there a difference between the lan-
guage of tweets originating within a crisis
region, (2) what linguistic patterns differ-
entiate within-region and outside-region
tweets, and (3) can we automatically iden-
tify those originating within the crisis re-
gion in real-time?
1 Introduction
Due to Twitter?s massive popularity, it has become
a tool used by first responders?those who provide
first-hand aid in times of crisis?to understand cri-
sis situations and identify the people in the most
dire need of assistance (United Nations, 2012). To
do this, first responders can survey ?geotagged?
tweets: those where the user has supplied a ge-
ographic location. The advantage of geotagged
tweets is that first responders know whether a per-
son is tweeting from within the affected region or
is tweeting from afar. Tweets from within this
region are more likely to contain emerging top-
ics (Kumar et al., 2013) and tactical, actionable,
information that contribute to situational aware-
ness (Verma et al., 2011).
A major limitation of surveying geotagged
tweets is that only 1% of all tweets are geo-
tagged (Morstatter et al., 2013). This leaves the
first responders unable to tap into the vast major-
ity of the tweets they collect. This limitation leads
to the question driving this work: can we discover
whether a tweet originates from within a crisis re-
gion using only the language used of the tweet?
We focus on the language of a tweet as the
defining factor of location for three major reasons:
(1) the language of Twitter users is dependent on
their location (Cheng et al., 2010), (2) the text is
readily available in every tweet, and (3) the text al-
lows for real-time analysis. Due to the short time
window presented by most crises, first responders
need to be able to locate users quickly.
Towards this goal, we examine tweets from two
recent crises: the Boston Marathon bombing and
Hurricane Sandy. We show that linguistic differ-
ences exist between tweets authored inside and
outside the affected regions. By analyzing the text
of individual tweets we can predict whether the
tweet originates from within the crisis region, in
real-time. To better understand the characteristics
of crisis-time language on Twitter, we conclude
with a discussion of the linguistic features that our
models find most discriminative.
2 Language Differences in Crises
In order for a language-based approach to be able
to distinguish tweets inside of the crisis region, the
language used by those in the region during cri-
sis has to be different from those outside. In this
section, we verify that there are both regional and
temporal differences in the language tweeted. To
start, we introduce the data sets we use throughout
the rest of this paper. We then measure the differ-
ence in language, finding that language changes
temporally and regionally at the time of the crisis.
2.1 Twitter Crisis Datasets
The Twitter data used in our experiments comes
from two crises: the Boston Marathon bombing
and Hurricane Sandy. Both events provoked a sig-
nificant Twitter response from within and beyond
23
Table 1: Properties of the Twitter crisis datasets.
Property Boston Sandy
Crisis Start 15 Apr 14:48 29 Oct 20:00
Crisis End 16 Apr 00:00 30 Oct 01:00
Epicenter 42.35, ?71.08 40.75, ?73.99
Radius 19 km 20 km
|IR| 11,601 5,017
|OR| 541,581 195,957
|PC-IR| 14,052 N/A
|PC-OR| 228,766 N/A
the affected regions.
The Boston Marathon Bombing occurred at
the finish line of the Boston Marathon on April
15th, 2013 at 14:48 Eastern. We collected geo-
tagged tweets from the continental United States
from 2013-04-09 00:00 to 2013-04-22 00:00 uti-
lizing Twitter?s Filter API.
Hurricane Sandy was a ?superstorm? that
ravaged the Eastern United States in October,
2012. Utilizing Twitter?s Filter API, we collected
tweets based on several keywords pertaining to the
storm. Filtering by keywords, this dataset contains
both geotagged and non-geotagged data beginning
from the day the storm made landfall (2012-10-29)
to several days after (2012-11-02).
2.2 Data Partitioning
For the Boston Bombing and Hurricane Sandy
datasets, we partitioned the tweets published dur-
ing the crisis time into two distinct parts based on
location: (1) inside the crisis region (IR), and (2)
outside the crisis region (OR).
For the Boston Bombing dataset, we are able to
extract two additional groups: (1) pre-crisis tweets
(posted before the time of the crisis) from inside
the crisis region (PC-IR) and (2) pre-crisis tweets
from outside the crisis region (PC-OR). We take a
time-based sample from 10:00?14:48 Eastern on
April 15th, 2013 to obtain PC-IR and PC-OR.
Because the bombing was an abrupt event with
no warning, we choose a time period immediately
preceding its onset. The number of tweets in each
dataset partition is shown in Table 1.
2.3 Pre-Crisis vs. During-Crisis Language
For the Boston dataset, we compare the words
used hour by hour between 10:00?19:00 on April
15th. For each pair of hours, we compute the
Jensen-Shannon (J-S) divergence (Lin, 1991) of
the probability distributions of the words used
(a) Temporal lan-
guage differences.
(b) Geographic lan-
guage differences:
tranquil time.
(c) Geographic lan-
guage differences:
crisis time.
Figure 1: Temporal and geographic differences of
language (calculated using Jensen-Shannon diver-
gence); darker shades represent greater difference.
To illustrate geographic differences, we compare
Boston with three other major U.S. cities.
within those hours. Figure 1(a) shows these J-S
divergence values. We see an abrupt change in
language in the hours before the bombing (10:00?
14:00) and those after the bombing (15:00?19:00).
We also note that the tranquil hours are relatively
stable. This suggests that language models trained
on tweets from tranquil time are less informative
for modeling crisis-time langauge.
2.4 IR vs. OR Language
We verify that the tweets authored inside of the
crisis use different words from those outside the
region. We compare the difference in Boston (B)
to three other major U.S. cities: Chicago (C), Los
Angeles (L), and Miami (M). To obtain a base-
line, we compare the cities during tranquil times
using PC-IR and PC-OR datasets. The results are
shown in Figure 1. The tranquil time comparison,
shown in Figure 1(b), displays a low divergence
between all pairs of cities. In contrast, Figure 1(c)
shows a wider divergence between the same cities,
with Boston displaying the greatest divergence.
3 Linguistic Features
As Twitter is a conversational, real-time, mi-
croblogging site, the structure of tweets offers
many opportunities for extracting different types
of features that represent the different linguistic
properties of informal text. Our approach is to
compare the utility, in classifying tweets as IR or
OR, of several linguistic features. We preprocess
the tweets by extracting tokens using the CMU
Twitter NLP tokenizer (Owoputi et al., 2013).
Unigrams and Bigrams We extract the raw fre-
quency counts of the word unigrams and bigrams.
POS Tags We extract part-of-speech tags for
each word in the tweet using the CMU Twitter
NLP POS tagger (Owoputi et al., 2013). We con-
24
sider CMU ARK POS tags, developed specifically
for the dynamic and informal nature of tweets, as
well as Penn Treebank (PTB) style POS tags. The
ARK POS tags are coarser than the PTB tags and
can identify Twitter-specific entities in the data
like hashtags. By comparing both tag sets, we can
measure the effectiveness of both the fine-grained
versus coarse-grained tag sets.
Shallow Parsing In addition to the POS tags,
we extract shallow parsing tags along with the
headword associated with the tag using the tool
provided by Ritter et al. (2011). For example,
in the noun phrase ?the movie? we would ex-
tract the headword ?movie? and represent it as
[...movie...]
NP
. The underlying motivation is that
this class may give more insight into the syntactic
differences of IR tweets versus OR tweets.
Crisis-Sensitive (CS) Features We create a
mixed-class of ?crisis sensitive? features com-
posed of word-based, part of speech, and syntac-
tic constituent attributes. These are based on our
analysis of the Boston Marathon data set. We later
apply these features to the Hurricane Sandy data
set to validate whether the features are generaliz-
able across crises and discuss this in the results.
?We extract ?in? prepositional phrases of the
form [in ... /N]
PP
. For example, ?in Boston.? The
motivation is this use of ?in,? such as with a lo-
cation or a nonspecific time, may be indicative of
crisis language.
? We extract verbs in relationship to the exis-
tential there. As the existential there is usually
the grammatical subject and describes an abstrac-
tion, it may be indicative of situational awareness
messages within the disaster region.
? Part-of-Speech tag sequences that are fre-
quent in IR tweets (from our development set) are
given special consideration. We find sequences
which are used more widely during the time of this
disaster. Some of the ARK tag sequences include:
?N R?, ?L A?, ?N P?, ?P D N?, ?L A !?, ?A N P?.
4 Experiments
Here, we assess the effectiveness of our linguistic
features at the task of identifying tweets originat-
ing from within the crisis region. To do this we
use a Na??ve Bayes classifier configured with an
individual set of feature classes. Each of our fea-
tures are represented as raw frequency counts of
the number of times they occur within the tweet.
The output is a prediction of whether the tweet
is inside region (IR) or outside region (OR). We
Table 2: Top Feature Combinations: Unigrams
(Uni), Bigrams (Bi) and Crisis-Sensitive (CS)
combinations have the best results.
Top Feature Combos Prec. Recall F1
Boston Bombing
Uni + Bi 0.853 0.805 0.828
Uni + Bi + Shallow Parse 0.892 0.771 0.828
Uni + Bi + CS 0.857 0.806 0.831
All Features 0.897 0.742 0.812
Hurricane Sandy
Uni + Bi 0.942 0.820 0.877
Uni + Bi + Shallow Parse + CS 0.956 0.803 0.873
Uni + Bi + CS 0.947 0.826 0.882
All Features 0.960 0.786 0.864
identify the features that can differentiate the two
classes of users, and we show that this process can
indeed be automated.
4.1 Experiment Procedure
We ensure a 50/50 split of IR and OR instances
by sampling the OR dataset. Using the classifier
described above, we perform 3? 5-fold cross val-
idation on the data. Because of the 50/50 split,
a ?select-all? baseline that labels all tweets as IR
will have an accuracy of 50%, a precision of 50%,
and a recall of 100%. All precision and recall val-
ues are from the perspective of the IR class.
4.2 Feature Class Analysis
We compare all possible combinations of individ-
ual feature classes and we report precision, recall,
and F1-scores for the best combinations in Table 2.
In both crises all of the top performing fea-
ture combinations contain both bigram and uni-
gram feature classes. However, our top perform-
ing feature combinations demonstrate that bigrams
in combination with unigrams have added util-
ity. We also see that the crisis-sensitive features
are present in the top performing combinations for
both data sets. The CS feature class was derived
from Boston Bombing data, so its presence in the
top groups from Hurricane Sandy is an indication
that these features are general, and may be useful
for finding users in these and future crises.
4.3 Most Informative Linguistic Features
To see which individual features within the classes
give the best information, we make a modification
to the experiment setup described in Section 4.1:
we replace the Na??ve Bayes classifier with a Logis-
tic Regression classifier to utilize the coefficients
it learns as a metric for feature importance. We re-
port the top three features of each class label from
each feature set in Table 3.
The individual unigram and bigram features
with the most weight have a clear semantic rela-
25
Table 3: Top 3 features indicative of each class within each feature set for both crises.
Feature Set (Class) Boston Marathon Bombing Hurricane Sandy
Unigram (IR) #prayforboston, boston, explosion @kiirkobangz, upset, staying
Unigram (OR) money, weather, gone #tomyfuturechildren, #tomyfutureson, bye
Bigram (IR) ?in boston?, ?the marathon?, ?i?m safe? ?railroad :?, ?evacuation zone?, ?storm warning?
Bigram (OR) ?i?m at?, ?s/o to?, ?, fl? ?you will?, ?: i?ve?, ?hurricane ,?
ARK POS (IR) ?P $ ? ?, ?L !?, ?! R P? ?P #?, ?? ? A?, ?@ @ #?
ARK POS (OR) ?O #?, ?! N O?, ?L P R? ?P V $?, ?A ? ??, ?N L A?
PTB POS (IR) ?CD NN JJ?, ?CD VBD?, ?JJS NN TO? ?USR DT JJS?, ?VB TO RB?, ?IN RB JJ?
PTB POS (OR) ?NNP -RRB-?, ?. JJ JJ?, ?JJ NN CD? ?NNS IN NNS?, ?PRP JJ PRP?, ?JJ NNP NNP?
Shallow Parse (IR) [...explosion...]
NP
, [...marathon...]
NP
,
[...bombs...]
NP
[...bomb...]
NP
, [...waz...]
V P
,
[...evacuation...]
NP
Shallow Parse (OR) [...school...]
NP
, [...song...]
NP
,
[...breakfast...]
NP
[...school...]
NP
, [...head...]
NP
, [...wit...]
PP
CS (IR) [in boston/N]
PP
, [for boston/N]
PP
,
?i?m/L safe/A?
?while/P a/D hurricane/N?, ?of/P my/D
house/N?, [in http://t.co/UxkKJLoX/N]
PP
CS (OR) ?to/P the/D beach/N?, [at la/N]
PP
,
[in love/N]
PP
[like water/N]
PP
, ?shutdowns/N on/P?,
?prayer/N for/P?
tionship to the crisis. Comparing the two crises,
the top features for Hurricane Sandy are more con-
cerned with user-user communication. For ex-
ample, the heavily-weighted ARK POS trigram
?@ @ #? is highly indicative of users spreading
information between each other. One explanation
is that the concern with communication could be
a result of the warning that came from the storm.
The bigram ?hurricane ,? is the 3rd most in-
dicative of a tweet originating from outside the re-
gion. This is likely because the word occurs in the
general discussion outside of the crisis region.
5 Related Work
Geolocation: Eisenstein et al. (2010) first looked
at the problem of using latent variables to ex-
plain the distribution of text in tweets. This prob-
lem was revisited from the perspective of geodesic
grids in Wing and Baldridge (2011) and further
improved by flexible adaptive grids (Roller et al.,
2012). Cheng et al. (2010) employed an approach
that looks at a user?s tweets and estimates the
user?s location based on words with a local geo-
graphical scope. Han et al. (2013) combines tweet
text with metadata to predict a user?s location.
Mass Emergencies: De Longueville et al.
(2009) study Twitter?s use as a sensor for crisis
information by studying the geographical proper-
ties of users? tweets. In Castillo et al. (2011),
the authors analyze the text and social network
of tweets to classify their newsworthiness. Kumar
et al. (2013) use geotagged tweets to find emerg-
ing topics in crisis data. Investigating linguistic
features, Verma et al. (2011) show the efficacy
of language features at finding crisis-time tweets
that contain tactical, actionable information, con-
tributing to situational awareness. Using a larger
dataset, we automatically discover linguistic fea-
tures that can help with situational awareness.
6 Conclusion and Future Work
This paper addresses the challenge of finding
tweets that originate from a crisis region using
only the language of each tweet. We find that the
tweets authored from within the crisis region do
differ, from both tweets published during tranquil
time periods and from tweets published from other
geographic regions. We compare the utility of sev-
eral linguistic feature classes that may help to dis-
tinguish the two classes and build a classifier based
on these features to automate the process of iden-
tifying the IR tweets. We find that our classifier
performs well and that this approach is suitable for
attacking this problem.
Future work includes incorporating the wealth
of tweets preceding the disaster for better predic-
tions. Preliminary tests have shown positive re-
sults; for example we found early, non-geotagged
reports of flooding in the Hoboken train tunnels
during Hurricane Sandy
1
. Future work may also
consider additional features, such as sentiment.
Acknowledgments
This work is sponsored in part by the Office
of Naval Research, grants N000141010091 and
N000141110527, and the Ira A. Fulton Schools of
Engineering, through fellowships to F. Morstatter
and N. Lubold. We thank Alan Ritter and the ARK
research group at CMU for sharing their tools.
1
An extended version of this paper is available at: http://
www.public.asu.edu/
?
fmorstat/paperpdfs/lang loc.pdf.
26
References
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information Credibility on Twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675?684. ACM.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You Are Where You Tweet: A Content-Based
Approach to Geo-locating Twitter Users. In Pro-
ceedings of the 19th ACM International Conference
on Information and Knowledge Management, pages
759?768. ACM.
Bertrand De Longueville, Robin S Smith, and Gian-
luca Luraschi. 2009. ?OMG, from here, I can
see the flames!?: a use case of mining Location
Based Social Networks to acquire spatio-temporal
data on forest fires. In Proceedings of the 2009 In-
ternational Workshop on Location Based Social Net-
works, pages 73?80. ACM.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A Latent Variable Model
for Geographic Lexical Variation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1277?1287. Asso-
ciation for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. A
Stacking-based Approach to Twitter User Geoloca-
tion Prediction. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013): System Demonstrations, pages
7?12.
Shamanth Kumar, Fred Morstatter, Reza Zafarani, and
Huan Liu. 2013. Whom Should I Follow?: Identi-
fying Relevant Users During Crises. In Proceedings
of the 24th ACM Conference on Hypertext and So-
cial Media, HT ?13, pages 139?147, New York, NY,
USA. ACM.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Fred Morstatter, J?urgen Pfeffer, Huan Liu, and Kath-
leen M Carley. 2013. Is the Sample Good Enough?
Comparing Data from Twitter?s Streaming API with
Twitter?s Firehose. Proceedings of The Interna-
tional Conference on Weblogs and Social Media.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved Part-of-Speech Tagging for
Online Conversational Text with Word Clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In EMNLP.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised Text-Based Geolocation using Language Mod-
els on an Adaptive Grid. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1500?1510. Association
for Computational Linguistics.
United Nations. 2012. Humanitarianism in the Net-
work Age. United Nations Office for the Coordina-
tion of Humanitarian Affairs.
Sudha Verma, Sarah Vieweg, William J Corvey, Leysia
Palen, James H Martin, Martha Palmer, Aaron
Schram, and Kenneth Mark Anderson. 2011. Natu-
ral Language Processing to the Rescue? Extracting
?Situational Awareness? Tweets During Mass Emer-
gency. In ICWSM.
Benjamin Wing and Jason Baldridge. 2011. Simple
Supervised Document Geolocation with Geodesic
Grids. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL
2011), pages 955?964.
27
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 28?32,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Discourse Analysis of User Forums in an Online Weight Loss Application
Lydia Manikonda
1
, Heather Pon-Barry
1
, Subbarao Kambhampati
1
, Eric Hekler
2
David W. McDonald
3
1
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University
2
School of Nutrition and Health Promotion, Arizona State University
3
The Information School, University of Washington
{lmanikon, ponbarry, rao, ehekler}@asu.edu, dwmc@uw.edu
Abstract
Online social communities are becoming
increasingly popular platforms for people
to share information, seek emotional sup-
port, and maintain accountability for los-
ing weight. Studying the language and
discourse in these communities can offer
insights on how users benefit from using
these applications. This paper presents a
preliminary analysis of language and dis-
course patterns in forum posts by users
who lose weight and keep it off versus
users with fluctuating weight dynamics.
Our results reveal differences about how
the types of posts, polarity of sentiments,
and semantic cohesion of posts made by
users vary along with their weight loss pat-
tern. To our knowledge, this is the first
discourse-level analysis of language and
weight loss dynamics.
1 Introduction and Related Work
Obesity is a major public health problem; the
number of people suffering from obesity has
risen globally in the last decade (Das and Fax-
vaag, 2014). Many of these people are trying to
lose weight as the multifactorial diseases such as
metabolic syndromes, respiratory problems, coro-
nary heart disease, and psychological challenges
are all closely associated with obesity (Rippe et
al., 1998; Must et al., 1999). More obese peo-
ple are trying to lose weight by using weight-
loss applications and other people interested in
using these applications are trying to avoid gain-
ing weight. Many internet services are becoming
increasingly popular for supporting weight loss
as they provide users with the opportunities to
seek information by asking questions, answering
questions, sharing their experiences and provid-
ing emotional support. Also, the internet provides
many attributes that can help people feel more
comfortable with openly expressing their prob-
lems and concerns (Ballantine and Stephenson,
2011; Hwang et al., 2010).
Most of the existing studies (Saperstein et al.,
2007; Johnson and Wardle, 2011; Hwang et al.,
2010; Ballantine and Stephenson, 2011; Leahey et
al., 2012; Das and Faxvaag, 2014) focused on why
people participate in online weight loss discus-
sion forums and how the social support can help
them to lose weight. These studies are conducted
from the perspective of medical and psychology
domains, where the data are collected via inter-
views or a small set of online forum data that are
manually analyzed by human experts. Their pri-
mary focus is on measuring the social support by
collecting views/opinions of people through sur-
veys; less attention is given to understanding the
natural language aspects of users? posts on these
online communities. Unlike choosing a small sub-
set of a dataset, our work is novel in automat-
ing the process of language analysis that can han-
dle a larger dataset. Automating the process can
also help classify the user type based on the lan-
guage efficiently. This work also considers weekly
check-in weights of users along with the study of
their language.
In this paper, we study the user?s language in
correlation with their weight loss dynamics. To
this end, we analyze a corpus of forum posts gen-
erated by users on the forum of a popular weight
loss application. The forum from which we ob-
tained the data is divided into several threads
where each thread consists of several posts made
by different users. From the overall dataset we
identify two preliminary patterns of weight dy-
namics: (1) users who lose weight and success-
fully maintain the weight loss (i.e., from one week
to the next, weight is lost or weight remains the
same) and (2) users whose weight pattern fluc-
tuates (i.e., from one week to the next, weight
28
changes are erratic or inconsistent). While there
are many possible groupings that we could have
utilized, we chose this grouping because of the
known problems with ?yo-yo? dieting compared
to a more steady weight-loss. We study how the
user?s language in these two groups varies by mea-
suring the semantic cohesion and sentiment of
posts made by them.
Our main contributions include understanding
the types of posts users make on different threads
with a main focus on question-related posts, the
type of language they use by measuring the se-
mantic cohesion and sentiment by correlating with
users? weight loss patterns. From the empirical
analysis we find that users who lose weight in
a fluctuating manner are very active on the dis-
cussion forums compared to the users who fol-
low a non-increasing weight loss pattern. We also
find that users of non-increasing weight loss pat-
tern mostly reply to the posts made by other users
and fluctuating users post more questions compar-
atively. Both the users from these two clusters dif-
fer in terms of the way their posts cohere with pre-
vious posts in the threads and also in terms of the
sentiment associated with their posts.
2 Dataset
We obtain a text corpus of online discussion fo-
rums from Lose It!, a popular mobile and web-
based weight loss application. Along with the text
corpus, we also obtain weekly weight check-in
data for a subset of users. The entire corpus con-
sists of eight different forums that are subdivided
into conversation topic threads. Each thread con-
sists of several posts made by different users. The
forum data in our corpus consists of 884 threads,
with a median length of 20 posts per thread. The
posts were made between January 1, 2010 and July
1, 2012. We identify the subset of users for whom
we have weight check-in data and who made at
least 25 weight check-ins during this time period.
This results in a total of 2,270 users.
The interesting feature of this weight loss appli-
cation is that users are encouraged to set goals to
regularly log their weight, diet, and exercise. For
a subset of users, Lose It! has provided a weekly
weight ?check-in?, an average of the user?s weight
check-ins during the week, for the January 1, 2010
through July 1, 2012 period. This allows us to jux-
tapose the weekly weights of the users with their
posts on the discussion forums.
Figure 1: Example weight loss patterns from two individ-
ual users: non-increasing (bottom line), and fluctuating (top
line). The x-axis ranges from the 1st through the 80th weekly
check-in; the y-axis shows the weight, measured in lbs.
We partition the users into two groups based
on their dynamic weight loss patterns: a non-
increasing group and a fluctuating group.
1. Non-increasing: For each week j, the user?s
check-in weight w
j
is less than or equal to
their past week?s weight w
j?1
, within a small
margin ?. That is, w
j
? (1 + ?)w
j?1
.
2. Fluctuating: If the difference between two
consecutive weekly check-in weights do not
follow the non-increasing constraint, users
are grouped into this category.
We empirically set ? = 0.04 to divide the
users in our dataset into two groups of similar size.
To illustrate the two patterns of weight change,
Figure 1 shows the weekly weight check-ins of
two individual users, one from each group. This
grouping is coarse, but is motivated by studies
(Kraschnewski et al., 2010; Wing and Phelan,
2005) acknowledging that approximately 80% of
people who set out to lose weight are successful
at long-term weight loss maintenance, where suc-
cessful maintenance is defined as losing 10% or
more of the body weight and maintaining that for
at least an year. In the future for further analysis,
we aim to separate users less coarsely, e.g., users
who maintain their weight neither gaining nor los-
ing weight, users who lose weight and maintain it
and finally, users who gain weight.
2.1 Characteristics of Online Community
The Lose It! application helps users set a person-
alized daily calorie budget, track the food they
are eating, and their exercise. It also helps users
to stay motivated by providing an opportunity to
29
connect with other users who want to lose weight
and support each other. Example snippets (para-
phrased) from forum threads are shown below.
The ?Can?t lose weight!? thread demonstrates
users supporting each other and offering advice.
The ?Someday I will? thread highlights the com-
plex relationship between text, semantics, and mo-
tivation in the forums.
Example thread: ?Can?t lose weight!?
User 1: ?I gained over 30 lbs in the last
year and am stressed about losing it. I
eat 1600 calories a day and burn more
than that in exercise, but I havent lost
any weight. I am so confused.?
User 2: ?You?ve only been a member for
less than 2 months. I suggest you relax.
Set your program to 1 pound weight loss
a week. Adjust your habits to something
you can live with. . . long term.?
User 3: ?You sound just like me. I
think your exercise is good but maybe
you are eating more than you think. Try
diligently logging everything you con-
sume.?
User 1: ?Thanks for the suggestions! I
am going to get back to my logging.?
Example thread: ?Someday I will. . . ?
User 1: ?Do a pull-up :-)?
User 2: ?. . . actually enjoy exercising.?
User 3: ?Someday I will stop participat-
ing in the lose it forums, but obviously
not today.?
User 4: ?I hope you fail :-)?
3 Empirical Analysis
In this section, we present preliminary observa-
tions on how the language and discourse patterns
of forum posts vary with respect to weight loss dy-
namics. As an initial step, part-of-speech (POS)
tagging is performed on all forum posts using the
Stanford POS Tagger (Toutanova et al., 2003).
From the weekly check-in data we identified the
number of users and the number of posts from
each weight-loss pattern cluster which are shown
in Table 1. We see that the average number of
posts by fluctuating users is greater than the av-
erage number of posts by non-increasing users.
Weight Pattern
Non-increasing Fluctuating
# Total users 1127 1143
# Forum users 29 68
# Forum posts 99 1279
Posts per user 3.5 18.2
Words per post 49.1 77.3
Table 1: Statistics of users and forum posts.
This suggests that fluctuating users are more ac-
tive in participation. Our data also suggest that
posts made by non-increasing users are shorter
compared to those made by fluctuating users.
3.1 Asking Questions
Previous studies (Bambina, 2007; Langford et al.,
1997) revealed that people on online health com-
munities mainly engage in two activities: (i) seek-
ing information, and (ii) getting emotional sup-
port. People usually ask questions to other com-
munity members or just browse through the com-
munity forums to get information while seeking
information. Below is an example (paraphrased)
showing how a users ask and respond to questions.
Example thread: ?New user?
User 1: ?Did anyone upgrade to the pre-
mium app? What do you like about it??
User 2: ?I upgraded to the premium.
I LOVE the functionality to log food in
advance. I can track and set goals that
are not related to weight like how much
I sleep, how much water I drink, etc.?
User 3: ?I upgraded my account to pre-
mium too. I really liked the added fea-
tures because it helped me keep track of
my steps and participate in challenges.?
We are interested in knowing whether users in
the two clusters are actively involved in posting
questions. We deem a forum post to be a question
if it meets one of these two conditions:
1. Wh-question words: If a sentence in the post
starts with a question word: Wh-Determiner
(WDT), Wh-pronoun (WP), Possessive wh-
pronoun (WP$), Wh-adverb (WRB).
2. Punctuation: If the post contains a question
mark symbol (???).
30
Figure 2: Proportion of sentiments for the two weight-loss
patterns. For non-increasing users, percentage of posts with
Positive, Neutral and Negative sentiments are: 22%, 46.5%
and 31.5% respectively. For fluctuating users, the percentage
of posts with Positive, Neutral and Negative sentiments are:
20.9%, 37.6% and 41.5% respectively.
We computed the ratio of question-oriented
posts made by each user in the two clusters. Af-
ter averaging these ratio values across all the users
in each cluster separately, we found that on aver-
age, 32.6% of the posts made by non-increasing
users were questions (SE = 0.061). And, 37.7%
of the posts made by fluctuating users were ques-
tions (SE = 0.042). This shows that on an aver-
age fluctuating users post relatively more number
of questions than the non-increasing users.
3.2 Sentiment of Posts
Analyzing the sentiment of user posts in the fo-
rums can provide a suprisingly meaningful sense
of how the loss of weight impacts the sentiment of
user?s post. In this analysis, we report our initial
results on extracting the sentiments of user?s posts.
In order to achieve this, we utilized the Stanford
Sentiment Analyzer (Socher et al., 2013). This an-
alyzer classifies a text input into one of five senti-
ment categories?from Very Positive to Very Nega-
tive. We merge the five classes into three: Positive,
Neutral and Negative. In future, we may consider
specific (health and nutrition) sentiment lexicons.
We analyzed the sentiment of posts contributed
by the users from the two clusters. As shown
in Figure 2, posts of users belonging to the non-
increasing cluster are more neutral whereas the
posts made by users from the fluctuating clus-
ter are mainly of negative sentiment. This gives
an interesting intuition that the fluctuating group
of users might require more emotional support as
they use more negative sentiment in their posts.
3.3 Cohesion with Previous Posts
Cohesion is the property of a well-written docu-
ment that links together sentences in the same con-
text. Several existing models measure the cohe-
sion of a given text with applications to topic seg-
mentation or multi-document summarization (El-
sner and Charniak, 2011; Barzilay and Lapata,
2005; Soricut and Marcu, 2006). In this analy-
sis, we want to find out if there is any correlation
between the cohesiveness of posts made by users
and their pattern of weight loss. We are mainly in-
terested in measuring the similarity of a user?s post
with respect to the previous posts in a thread. This
can help identify users who elaborate on previous
post versus those who shift the topic.
We focus on content words: verbs and
nouns (part-of-speech tags VB, VBZ, VBP, VBD,
VBN, VBG, NN, NNP, NNPS). Next, we use
WordNet (Miller, 1995) to identify synonyms of
the content words. Then, we compute similar-
ity between the current post and previous posts of
other users in the thread, in terms of commonly
shared verbs and nouns including synonyms. In
our current, preliminary analysis, we consider this
similarity score to be the measure of cohesion.
In this step, we consider all posts that are not
thread-initial. To approximate whether a post is
cohesive, we compare the nouns and verbs of the
current post to the list of nouns and verbs (plus
synonyms) obtained from the previous posts of the
thread. Our analysis finds that posts made by fluc-
tuating users have an average cohesion score of
0.42 (SE = 0.008), whereas posts made by non-
increasing users have an average cohesion score
of 0.51 (SE = 0.027). This suggests that non-
increasing users may be more focused when par-
ticipating in forums whereas the fluctuating users
are more prone to make posts that have less in
common with the previous posts in a thread.
4 Conclusions and Future Work
In this paper, we analyze how the language
changes based on the weight loss dynamics of
users who participate in the forum of a popular
weight-loss application. Specifically, this analy-
sis revealed four interesting insights about the two
types of users who lose weight in a non-increasing
manner and who lose weight in a fluctuating man-
ner. Firstly, fluctuating users are more active in
participation compared to the other set of users.
Secondly, fluctuating users post more question-
31
oriented posts compared to the non-increasing
users. Thirdly, non-increasing users contribute
posts that are more cohesive with respect to the
previous posts in a given thread. Fourthly, posts
contributed by fluctuating users have more neg-
ative sentiment compared to the posts made by
non-increasing users. This observation hints that
fluctuating users may need more emotional sup-
port to continue using this weight loss application
and lose weight in an effective manner.
While this work is preliminary, our analyses
provide a valuable early ?proof of concept? for
providing insights on how user behavior within
online weight loss forums might impact weight
outcomes. These sorts of analyses, particularly
when replicated, could provide valuable insights
for developing refined online weight loss forums
that might facilitate more effective interactions for
weight loss. It could also provide valuable insights
for improving behavioral theories about behavior
change (Hekler et al., 2013).
In the future, we plan to focus on a larger cor-
pus from an extended time period, aligned more
closely with weekly check-in weight data. Other
directions for consideration are the temporal as-
pect of forum posts and gender-based analyses of
user behavior.
Acknowledgments
We would like to thank Fit Now, Inc., makers of
Lose It!, for providing us with the data to conduct
this research. We thank the anonymous review-
ers for their helpful suggestions. This research is
supported in part by the ARO grant W911NF-13-
1-0023, the ONR grants N00014-13-1-0176 and
N0014-13-1-0519, and a Google Research Grant.
References
Paul W. Ballantine and Rachel J. Stephenson. 2011. Help
me, I?m fat! Social support in online weight loss networks.
Journal of Consumer Behaviour, 10(6):332?337.
Antonina D. Bambina. 2007. Online Social Support: The In-
terplay of Social Networks and Computer-Mediated Com-
munication. Cambria Press.
Regina Barzilay and Mirella Lapata. 2005. Modeling local
coherence: An entity-based approach. In Proceedings of
the Association for Computational Linguistics, ACL ?05,
pages 141?148.
Anita Das and Arild Faxvaag. 2014. What influences patient
participation in an online forum for weight loss surgery?
Interactive Journal of Medical Research, 3(1).
Micha Elsner and Eugene Charniak. 2011. Disentangling
chat with local coherence models. In Proceedings of the
Association for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 1179?
1189.
Eric Hekler, Predrag Klasnja, Jon E. Froehlich, and
Matthew P. Buman. 2013. Mind the theoretical gap: In-
terpreting, using, and developing behavioral theory in HCI
research. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems.
Kevin O. Hwang, Allison J. Ottenbacher, Angela P. Green,
M. Roseann Cannon-Diehl, Oneka Richardson, Elmer V.
Bernstam, and Eric J. Thomas. 2010. Social support in an
internet weight loss community. I. J. Medical Informatics,
79(1):5?13.
Fiona Johnson and Jane Wardle. 2011. The association
between weight loss and engagement with a web-based
food and exercise diary in a commercial weight loss pro-
gramme: a retrospective analysis. International Journal
of Behavioral Nutrition and Physical Activity, 8(1):1?7.
J L Kraschnewski, J Boan, J Esposito, N E Sherwood, E B
Lehman, D K Kephart, and C N Sciamanna. 2010. Long-
term weight loss maintenance in the united states. Inter-
national Journal of Obesity, 34(11):1644?1654.
Catherine Penny Hinson Langford, Juanita Bowsher,
Joseph P. Maloney, and Patricia P. Lillis. 1997. Social
support: A conceptual analysis. Journal of Advanced
Nursing, 25(1):145?151.
Tricia M. Leahey, Rajiv Kumar, Brad M. Weinberg, and
Rena R. Wing. 2012. Teammates and social influence
affect weight loss outcomes in a team-based weight loss
competition. Obesity, 20(7):1413?1418.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
Aviva Must, Jennifer Spadano, Eugenie H. Coakley, Ali-
son E. Field, Graham Colditz, and Dietz William H. 1999.
The disease burden associated with overweight and obe-
sity. JAMA, 282(16):1523?1529.
James M. Rippe, Suellyn Crossley, and Rhonda Ringer.
1998. Obesity as a chronic disease: Modern medical and
lifestyle management. Journal of the American Dietetic
Association, 98(10, Supplement):S9 ? S15.
S. L. Saperstein, N. L. Atkinson, and R. S. Gold. 2007. The
impact of internet use for weight loss. Obesity Reviews,
8(5):459?465.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng, and Christopher
Potts. 2013. Recursive deep models for semantic compo-
sitionality over a sentiment treebank. In Proceedings of
the EMNLP, pages 1631?1642, October.
Radu Soricut and Daniel Marcu. 2006. Discourse generation
using utility-trained coherence models. In Proceedings of
the COLING/ACL on Main Conference Poster Sessions,
COLING-ACL ?06, pages 803?810.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
NAACL HLT - Volume 1, pages 173?180.
Rena R. Wing and Suzanne Phelan. 2005. Long-term weight
loss maintenance. The American Journal of Clinical Nu-
trition, 82(suppl):222S?5S.
32
