Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 996?1004, Prague, June 2007. c?2007 Association for Computational Linguistics
 
Learning to Find English to Chinese Transliterations on the Web 
Jian-Cheng Wu 
 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, Taiwan 
d928322@oz.nthu.edu.tw 
Jason S. Chang 
 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, Taiwan 
jschang@cs.nthu.edu.tw 
Abstract 
We present a method for learning to find 
English to Chinese transliterations on the 
Web. In our approach, proper nouns are 
expanded into new queries aimed at maxi-
mizing the probability of retrieving trans-
literations from existing search engines. 
The method involves learning the sublexi-
cal relationships between names and their 
transliterations. At run-time, a given name 
is automatically extended into queries with 
relevant morphemes, and transliterations in 
the returned search snippets are extracted 
and ranked. We present a new system, 
TermMine, that applies the method to find 
transliterations of a given name. Evaluation 
on a list of 500 proper names shows that 
the method achieves high precision and re-
call, and outperforms commercial machine 
translation systems. 
1 Introduction 
Increasingly, short passages or web pages are be-
ing translated by desktop machine translation soft-
ware or are submitted to machine translation ser-
vices on the Web every day. These texts usually 
contain some proportion of proper names (e.g., 
place and people names in ?The cities of Mesopo-
tamia prospered under Parthian and Sassanian 
rule.?), which may not be handled properly by a 
machine translation system. Online machine trans-
lation services such as Google Translate1 or Yahoo! 
Babelfish2 typically use a bilingual dictionary that 
is either manually compiled or learned from a par-
                                                 
1 Google Translate: translate.google.com/translate_t 
2 Yahoo! Babelfish: babelfish.yahoo.com 
allel corpus. However, such dictionaries often have 
insufficient coverage of proper names and techni-
cal terms, leading to poor translation performance 
due to out of vocabulary (OOV) problem.  
Handling name transliteration is also important 
for cross language information retrieval (CLIR) 
and terminology translation (Quah 2006). There 
are also services on the Web specifically targeting 
transliteration aimed at improving CLIR, including 
CHINET (Kwok et al 2005) and LiveTrans (Lu, 
Chien, and Lee 2004). 
The OOV problems of machine translation (MT) 
or CLIR can be handled more effectively by learn-
ing to find transliteration on the Web. Consider the 
sentence in Example (1), containing three proper 
names. Google Translate produces the sentence in 
Example (2) and leaves ?Parthian? and ?Sas-
sanian? not translated. A good response might be a 
translation like Example (3) with appropriate 
transliterations (underlined). 
(1) The cities of Mesopotamia prospered under 
Parthian and Sassanian rule. 
(2) ????? parthian ????sassanian ?
?? 
(3) ??????3??????4???5???
????? 
These transliterations can be more effectively 
retrieved from mixed-code Web pages by extend-
ing each of the proper names into a query. Intui-
tively, by requiring one of likely transliteration 
morphemes (e.g., ???(Ba) or ???(Pa) for names 
beginning with the prefix ?par-?), we can bias the 
search engine towards retrieving the correct trans-  
                                                 
3 ??????(Meisuobudamiya) is the transliteration of 
?Mesopotamia.? 
4 ???(Badiya) is the transliteration of ?Parthian.? 
5 ??(Sashan) is the transliteration of ?Sassanian.? 
996
 Figure 1. An example of TermMine search for transliterations of the name ?Parthian? 
 
literations (e.g., ???? ?(Badiya) and ???
??(Patiya)) in snippets of many top-ranked docu-
ments.  
This approach to terminology translation by 
searching is a strategy increasingly adopted by 
human translators. Quah (2006) described a mod-
ern day translator would search for the translation 
of a difficult technical term such as ??????
??????? by expanding the query with the 
word ?film? (back transliteration of the component 
?????? of the term in question). This kind of 
query expansion (QE) indeed increases the chance 
of finding the correct translation ?anisotropic con-
ductive film? in top-ranked snippets. However, the 
manual process of expanding query, sending 
search request, and extracting transliteration is te-
dious and time consuming. Furthermore, unless the 
query expansion is done properly, snippets con-
taining answers might not be ranked high enough 
for this strategy to be the most effective.  
We present a new system, TermMine, that auto-
matically learns to extend a given name into a 
query expected to retrieve and extract translitera-
tions of the proper name. An example of machine 
transliteration of ?Parthian? is shown in Figure 1. 
TermMine has determined the best 10 query ex-
pansions (e.g., ?Parthian ? ,? ?Parthian ? ?). 
TermMine learns these effective expansions auto-
matically during training by analyzing a collection 
of place names and their transliterations, and deriv-
ing cross-language relationships of prefix and post-
fix morphemes. For instance, TermMine learns that 
a name that begins with the prefix ?par-? is likely 
to have a transliteration beginning with ??? or 
???). We describe the learning process in Section 
3. 
This prototype demonstrates a novel method for 
learning to find transliterations of proper nouns on 
the Web based on query expansion aimed at 
maximizing the probability of retrieving translit-
erations from existing search engines. Since the 
method involves learning the morphological rela-
tionships between names and their transliterations, 
we refer to this IR-based approach as morphologi-
cal query expansion approach to machine translit-
eration. This novel approach is general in scope 
and can also be applied to back transliteration and 
to translation with slight modifications, even 
though we focus on transliteration in this paper. 
The remainder of the paper is organized as fol-
lows. First, we give a formal statement for the 
problem (Section 2). Then, we present a solution to 
the problem by proposing new transliteration prob-
ability functions, describing the procedure for es-
timating parameters for these functions (Section 3) 
and the run-time procedure for searching and ex-
 
997
 tracting transliteration via a search engine (Section 
4). As part of our evaluation, we carry out two sets 
of experiments, with or without query expansion, 
and compare the results. We also evaluate the re-
sults against two commercial machine translation 
online services (Section 5). 
2 Problem Statement 
Using online machine translation services for name 
transliteration does not work very well. Searching 
in the vicinity of the name in mixed-code Web 
pages is a good strategy. However, query expan-
sion is needed for this strategy to be effective. 
Therefore, to find transliterations of a name, a 
promising approach is to automatically expand the 
given name into a query with the additional re-
quirement of some morpheme expected to be part 
of relevant transliterations that might appear on the 
Web. 
 
Table 1. Sample name-transliteration pairs from the 
training collection. 
Name Transliteration Name Transliteration
Aabenraa ???   Aarberg ??? 
Aabybro ???? Aarburg ??? 
Aachen ?? Aardenburg ??? 
Aalesund ??? Aargau ?? 
Aaley ?? Aars ??? 
Aalten ??? Aba ?? 
Aarau ?? Abacaxis ????? 
 
Now, we formally state the problem we are deal-
ing with: 
 
While a proper name N is given. Our goal 
is to search and extract the transliteration 
T of N from Web pages via a general-
purpose search engine SE. For that, we 
expand N into a set of queries q1, q2, ?, 
qm, such that the top n document snippets 
returned by SE for the queries are likely to 
contain some transliterations T of the 
given name N. 
 
In the next section, we propose using a probabil-
ity function to model the relationships between 
names and transliterations and describe how the 
parameters in this function can be estimated.  
3 Learning Relationships for QE  
We attempt to derive cross-language morpho-
logical relationships between names and translit-
erations and use them to expand a name into an 
effective query for searching and extracting trans-
literations. For the purpose of expanding the given 
name, N, into effective queries to search and ex-
tract transliterations T, we define a probabilistic 
function for mapping prefix syllable from the 
source to the target languages. The prefix translit-
eration function P(TP | NP) is the probability of T 
has a prefix TP under the condition that the name N 
has a prefix NP. 
P (TP | NP) = Count (TP,NP) / Count (NP)       (1) 
where  Count (TP,NP) is the number of TP and NP 
co-occurring in the pairs of training set 
(see Table 1), and Count(NP) is the num-
ber of NP occurring in training set.  
 
Similarly, we define the function P (TS | NS) for 
postfixes TS and NS: 
P (TS | NS) = Count (TS,NS) / Count (NS)        (2) 
The prefixes and postfixes are intended as a syl-
lable in the two languages involved, so the two 
prefixes correspond to each other (See Table 2&3). 
Due to the differences in the sound inventory, the 
Roman prefix corresponding to a syllabic prefix in 
Chinese may vary, ranging from a consonant, a 
vowel, or a consonant followed by a vowel (but not 
a vowel followed by a consonant). So, it is likely 
such a Roman prefix has from one to four letters. 
On the contrary, the prefix syllable for a name 
written in Chinese is readily identifiable. 
 
Table 2. Sample cross-language morphological relation-
ships between prefixes. 
Name 
Prefix (NP)
Transliteration
Prefix (TP) 
NP 
Count 
TP 
Count
Co-occ.
Count
a- ?(A) 1,456 854 854
a- ?(Ya) 1,456 267 264
ab- ?(A) 77 854 45
ab- ?(Ya) 77 267 32
b- ?(Bu) 2,319 574 566
b- ?(Ba) 2,319 539 521
ba- ?(Ba) 650 574 452
bu- ?(Bu) 299 539 182
 
998
 Table 3. Sample cross-language morphological relation-
ships between postfixes. 
Name 
Postfix (Ns) 
Transliteration
Postfix (Ts) 
Ns 
Count 
Ts 
Count
Co-occ.
Count 
-a ?(La) 4,774 1,044 941
-a ?(Ya) 4,774 606 568
-la ?(La) 461 1,044 422
-ra ?(La) 534 1,044 516
-ia ?(Ya) 456 606 391
-nia ?(Ya) 81 606 77
-burg ?(Bao) 183 230 175
 
We also observe that a preferred prefix (e.g., 
???(Ai)) is often used for a Roman prefix (e.g., 
?a-? or ?ir-?), while occasionally other homo-
phonic characters are used (e.g., ???(Ai)). The 
skew distribution creates problems for reliable es-
timation of transliteration functions. To cope with 
this data sparseness problem, we use homophone 
classes and a function CL that maps homophonic 
characters to the same class number. For instance, 
??? and ??? are homophonic, and both are as-
signed the same class identifier(see Table 4 for 
more samples). 
Therefore, we have  
CL (???) = CL (???) = 275. 
Table 4. Some examples of classes of homophonic 
characters. The class ID of each class is assigned arbi-
trarily. 
Class 
ID 
Transl. 
char 
Pronun- 
ciation 
Class 
ID 
Transl.
char  
Pronun-
ciation 
1 ? Ba  2 ? Bo 
1 ? Ba 275 ? Ai 
1 ? Ba 275 ? Ai 
1 ? Ba 275 ? Ai 
1 ? Ba 276 ? Ao 
1 ? Ba 276 ? Ao 
2 ? Bo 276 ? Ao 
2 ? Bo ? ? ? 
 
With homophonic classes of transliteration mor-
phemes, we define class-based transliteration prob-
ability as follows 
PCL(C | NP) = Count(TP,NP) / Count(NP)       (3) 
where CL(TP) = C            
PCL(C | NS) = Count(TS,NS) / Count(NS)       (4) 
where CL(TS) = C              
and then we rewrite P (TP | NP) and P (TS | NS) as  
  
P (TP | NP) = PCL(CL(TP ) | NP)                        (5) 
P (TS | NS) = PCL(CL(TS ) | NS)                        (6) 
With class-based transliteration probabilities, we 
are able to cope with difficulty in estimating pa-
rameters for rare events which are under repre-
sented in the training set. Table 5 shows that ??? 
belongs to a homophonic class co-occurring with 
?a-? for 46 times, even when only one instance of 
(???, ?a-?). 
After cross-language relationships for prefixes 
and postfixes are automatically trained, the prefix 
relationships are stored as prioritized query expan-
sion rules. In addition to that, we also need a trans-
literation probability function to rank candidate 
transliterations at run-time (Section 4). To cope 
with data sparseness, we consider names (or trans-
literations) with the same prefix (or postfix) as a 
class. With that in mind, we use both prefix and 
postfix to formulate an interpolation-based estima-
tor for name transliteration probability:  
P(T | N)=max ?1P(TP | NP)+?2P(TS | NS)        (7) 
               NP, NS  
where ?1 + ?2 = 1 and NP, NS, TP, and TS are the 
prefix and postfix of the given name N 
and transliteration T. 
 
For instance, the probability of ??????
? ?(Meisuobudamiya) as a transliteration of 
?Mesopotamia? is estimated as follows 
 
 P (?????? | ?Mesopotamia?)  
= ?1P (??? | ?me-?)+ ?2 P (??? | ?-a?) 
 
(1) For each entry in the bilingual name list, pair 
up prefixes and postfixes in names and trans-
literations. 
(2) Calculate counts of these affixes and their co-
occurrences. 
(3) Estimate the prefix and postfix transliteration 
functions 
(4) Estimate class-based prefix and postfix trans-
literation functions  
Figure 2. Outline of the process used to train the 
TermMine system. 
 
The system follows the procedure shown in Fig-
ure 2 to estimate these probabilities. In Step (1), 
999
 the system generates all possible prefix pairs for 
each name-transliteration pair. For instance, con-
sider the pair, (?Aabenraa,? ?????), the system 
will generate eight pairs: 
(a-, ?-), (aa-, ?-), (aab-, ?-), (aabe-, ?-), 
(-a, -?), (-aa, -?), (-raa, -?), and (-nraa, -?). 
Finally, the transliteration probabilities are esti-
mated based on the counts of prefixes, postfixes, 
and their co-occurrences. The derived probabilities 
embody a number of relationships:  
(a) Phoneme to syllable relationships (e.g., ?b? vs. 
? ? ? as in ?Brooklyn? and ? ? ? ?
??(Bulukelin)),  
(b) Syllable to syllable relationships (e.g., ?bu? vs. 
???),  
(c) Phonics rules  (e.g., ?br-? vs. ??? and ??? vs. 
?cl-?). The high probability of P(??? | ?cl-?) 
amounts to the phonics rule that stipulates ?c? 
be pronounced with a ?k? sound in the context 
of ?l.? 
4 Transliteration Search and Extraction 
At run-time, the system follows the procedure in 
Figure 3 to process the given name. In Step (1), the 
system looks up in the prefix relationship table to 
find the n best relationships (n = MaxExpQueries) 
for query expansion with preference for relation-
ships with higher probabilistic value. For instance, 
to search for transliterations of ?Acton,? the system 
looks at all possible prefixes and postfixes of ?Ac-
ton,? including a-, ac-, act-, acto-, -n, -on, -ton, 
and -cton, and determines the best query expan-
sions: ?Acton ?,? ?Acton ?,? ?Acton ?,? ?Ac-
ton ?,? ?Acton ?,? etc. These effective expan-
sions are automatically derived during the training 
stage described in Section 3 by analyzing a large 
collection of name-transliteration pairs.  
In Step (2), the system sends off each of these 
queries to a search engine to retrieve up to 
MaxDocRetrieved document snippets. In Step (3), 
the system discards snippets that have too little 
proportion of target-language text. See Example (4) 
for a snippet that has high portion of English text 
and therefore is less likely to contain a translitera-
tion. In Step (4), the system considers the sub-
strings in the remaining snippets. 
 
 
(1) Look up the table for top MaxExpQueries 
prefix and posfix relationships relevant to 
the given name and use the target mor-
phemes in the relationship to form ex-
panded queries  
(2) Search for Web pages with the queries and 
filter out snippets containing at less than 
MinTargetRate portion of target language 
text 
(3) Evaluate candidates based on class-based 
transliteration probability (Equation 5) 
(4) Output top one candidate for evaluation 
Figure 3. Outline of the steps used to search, extract, 
and rank transliterations. 
 
Table 5. Sample data for class-based morphological 
transliteration probability of prefixes, where # of NP 
denotes the number of the name prefix NP; # of C, NP 
denotes the number of all TP belonging to the class C 
co-occurring with the NP; # TP, NP denotes the number 
of transliteration prefix TP co-occurs with the NP; P(C|NP) 
denotes the probability of all TP belonging to C co-
occurring with the NP; P(TP|NP) denotes the probability 
of the Tp co-occurs with the NP. 
NP Class
ID 
TP # of NP # of 
C,NP 
# of 
TP,NP 
P(C|NP) P(TP|NP)
a- 275 ? 1456 46 28 0.032 0.019
a- 275 ? 1456 46 17 0.032 0.012
a- 275 ? 1456 46 1 0.032 0.000
a- 276 ? 1456 103 100 0.071 0.069
a- 276 ? 1456 103 2 0.071 0.001
a- 276 ? 1456 103 1 0.071 0.000
ba- 2 ? 652 5 3 0.008 0.005
ba- 2 ? 652 5 1 0.008 0.002
ba- 2 ? 652 5 1 0.008 0.002
 
Table 6. Sample data for class-based morphological 
transliteration probability of postfixes. Notations are 
similar to those for Table 5. 
Ns Class 
ID 
Ts # of Ns # of  
C,Ns 
# of 
Ts,Ns 
P(C|Ns) P(Ts|Ns)
-li 103 ? 142 140 85 0.986 0.599
-li 103 ? 142 140 52 0.986 0.366
-li 103 ? 142 140 2 0.986 0.014
-li 103 ? 142 140 1 0.986 0.007
-li 103 ? 142 140 0 0.986 0.000
-raa 112 ? 4 1 1 0.250 0.250
-raa 112 ? 4 1 0 0.250 0.000
-raa 112 ? 4 1 0 0.250 0.000
-raa 112 ? 4 1 0 0.250 0.000
 
1000
 For instance, Examples (5-7) shows remaining 
snippets that have high proportion of Chinese text. 
The strings ?????(Akedun) is a transliteration 
found in snippet shown in Example (5), a candi-
date beginning with the prefix ??? and ending 
with the postfix  ??? and is within the distance of 
1 of the instance ?Acton,? separated by a punctua-
tion token. The string ????? (Aikedun) found 
in Example (6) is also a legitimate transliteration 
beginning with a different prefix ??,? while ??
???(Aiketeng) in Example (7) is a transliteration 
beginning with yet another prefix ??.? Translit-
eration ????? appears at a distance of 3 from 
?Acton,? while two instances of ????? appear 
at the distances of 1 and 20 from the nearest in-
stances of ?Acton.?  
 
(4) Acton moive feel pics!! - ?? 
????: ??? > ???? > ?? > Acton 
moive feel pics!! Hop Hero - Acton moive feel 
pics!! 
http://www.hkmassive.com/forum/viewthread.php?
tid=2368&fpage=1 Watch the slide show! ... 
(5) New Home Alert - Sing Tao New Homes 
Please select, Acton ???, Ajax ???, Allis-
ton ????, Ancaster ????, Arthur ??, 
Aurora ???, Ayr ??, Barrie ??, Beamsville, 
Belleville ... 
(6) STS-51-F ? Wikipedia 
????????????????????
? ... ?????? (Karl Henize ???? STS-51-
F ??)?????; ?????? (Loren Acton?
??? STS-51-F??)???????; ??-?
???? (John-David F. ... 
(7) ?????-00-Acton-Australia.htm 
Acton Systems is a world leading manufacturer 
supplying stuctured cabling systems suited to the 
Australian and New Zealand marketplace. ???
????????????????, ????
???????? Custom made leads are now 
available ... 
 
The occurrence counts and average distance 
from instances of the given name are tallied for 
each of these candidates. Candidates with a low 
occurrence count and long average distance are 
excluded from further consideration.  Finally, all 
candidates are evaluated and ranked using Equa-
tion (7) given in Section 3. 
5 Evaluation 
In the experiment carried out to assess the feasibil-
ity to the proposed method, a data set of 23,615 
names and transliterations was used. This set of 
place name data is available from NICT, Taiwan 
for training and testing. There are 967 distinct Chi-
nese characters presented in the data, and more 
details of training data are available in Table 7. 
The English part consists of Romanized versions 
of names originated from many languages, includ-
ing Western and Asian languages. Most of the time, 
the names come with a Chinese counterpart based 
solely on transliteration. But occasionally, the Chi-
nese counterpart is part translation and part trans-
literation. For instance, the city of ?Southampton? 
has a Chinese counterpart consisting of ?? ? 
(translation of ?south?) and ????? (translitera-
tion of ?ampton?). 
 
Table 7. Training data and statistics  
Type of Data Used in Experiment Number
Name-transliteration pairs 23,615
Training data 23,115
Test data 500
Distinct transliteration morphemes 967
Distinct transliteration morphemes  
(80% coverage) 100
Names with part translation and 
 part transliteration (estimated) 300 
Cross-language prefix relationships 21,016 
Cross-language postfix relationships 26,564 
 
We used the set of parameters shown in Table 8 
to train and run System TermMine. A set of 500 
randomly selected were set aside for testing. We 
paired up the prefixes and postfixes in the remain-
ing 23,116 pairs, by taking one to four leading or 
trailing letters of each Romanized place names and 
the first and last Chinese transliteration character 
to estimate P (TP | NP) and P (TS | NS). 
 
Table 8. Parameters for training and testing 
Parameter Value Description 
MaxPrefixLetters 4 Max number of let-ters in a prefix 
MaxPostfixLetters 4 Max number of let-ters in a postfix 
MaxExpQueries 10 Max number of ex-panded queries 
MaxDocRetrieved 1000 Max number of document retrieved
1001
 MinTargetRate 0.5 Min rate of target text in a snippet 
MinOccCount 1 
Min number of co-
occurrence of query 
and transliteration 
candidate in snippets
MaxAvgDistance 4 Max distance be-tween N and T 
WeightPrefixProb 0.5 
Weight of Prefix 
probability (?1) 
WeightPostfixProb 0.5 
Weight of Postfix 
probability (?2) 
 
We carried out two kinds of evaluation on Sys-
tem TermMine, with and without query expansion. 
With QE option off, the name itself was sent off as 
a query to the search engine, while with QE option 
turned on, up to 10 expanded queries were sent for 
each name. We also evaluated the system against 
Google Translate and Yahoo! Babelfish. We dis-
carded the results when the names are returned un-
translated. After that, we checked the correctness 
of all remaining results by hand. Table 9 shows a 
sample of the results produced by the three systems. 
In Table 10, we show performance differences 
of system TermMine in query expansion option. 
Without QE, the system returns transliterations 
(applicability) less than 50% of the time. Neverthe-
less, there are enough snippets for extracting and 
ranking of transliterations. The precision rate of the 
top-ranking transliterations is 88%.  With QE 
turned on, the applicability rate increases signifi-
cantly to 60%. The precision rate also improved 
slightly to 0.89. 
The performance evaluation of three systems is 
shown in Table 11. For the test set of 500 place 
names, Google Translate returned 146 translitera-
tions and Yahoo! Babelfish returned only 44, while 
TermMine returned 300. Of the returned translit-
erations, Google Translate and Yahoo! Babelfish 
achieved a precision rate around 50%, while 
TermMine achieved a precision rate almost as high 
as 90%. The results show that System TermMine 
outperforms both commercial MT systems by a 
wide margin, in the area of machine transliteration 
of proper names.  
 
Table 9. Sample output by three systems evaluated. The 
stared transliterations are incorrect. 
Name TermMine Google Translate 
Yahoo! 
Babelfish
Arlington  ???  ???  ??? 
Toledo  ???  ??? - 
Palmerston  ????  ???? - 
Cootamundra  ?????  ????? - 
Bangui  ??  ?? - 
Australasia  ????? *???  ????? 
Wilson  ???  ???  ??? 
Mao *???  ?  ? 
Inverness  ???? *??  ???? 
Cyprus  ????  ????  ???? 
Rostock  ????  ????  ???? 
Bethel  ???  ??? *?? 
Arcade  ??? *?? *?? 
Lomonosov  ?????  ????? - 
Oskaloosa  ?????  ????? - 
 
Table 10. Performance evaluation of TermMine 
 Method
Evaluation 
TermMine 
QE- 
TermMine 
QE+ 
# of cases performed 238  300
Applicability  0.48  0.60
# Correct Answers    209    263
Precision  0.88  0.89
Recall  0.42  0.53
F-measure 0.57 0.66
 
Table 11. Performance evaluation of three systems 
Method
Evaluation  
TermMine 
QE+ 
Google 
Translate 
Yahoo! 
Babelfish
# of cases done  300  146  44
# of correct  
answers 
  263  67  23
Applicability  0.60  0.29  0.09
Precision  0.89  0.46  0.52
Recall  0.53  0.13 0.05
F-measure 0.66    0.21 0.08
6 Comparison with Previous Work 
Machine transliteration has been an area of active 
research. Most of the machine transliteration 
method attempts to model the transliteration proc-
ess of mapping between graphemes and phonemes. 
Knight and Graehl (1998) proposed a multilayer 
model and a generate-and-test approach to perform 
back transliteration from Japanese to English based 
on the model. In our work we address an issue of 
producing transliteration by way of search.  
Goto et al (2003), and Li et al (2004) proposed 
a grapheme-based transliteration model. Hybrid 
transliteration models were described by Al-
Onaizan and Knight (2002), and Oh et al (2005).  
1002
 Recently, some of the machine transliteration study 
has begun to consider the problem of extracting 
names and their transliterations from parallel cor-
pora (Qu and Grefenstette 2004, Lin, Wu and 
Chang 2004; Lee and Chang 2003, Li and Grefen-
stette 2005).  
Cao and Li (2002) described a new method for 
base noun phrase translation by using Web data. 
Kwok, et al (2001) described a system called 
CHINET for cross language name search. Nagata 
et al (2001) described how to exploit proximity 
and redundancy to extract translation for a given 
term. Lu, Chien, and Lee (2002) describe a method 
for name translation based on mining of anchor 
texts. More recently, Zhang, Huang, and Vogel 
(2005) proposed to use occurring words to expand 
queries for searching and extracting transliterations. 
Oh and Isahara (2006) use phonetic-similarity to 
recognize transliteration pairs on the Web. 
In contrast to previous work, we propose a sim-
ple method for extracting transliterations based on 
a statistical model trained automatically on a bilin-
gual name list via unsupervised learning. We also 
carried out experiments and evaluation of training 
and applying the proposed model to extract trans-
literations by using web as corpus.  
7 Conclusion and Future Work 
Morphological query expansion represents an in-
novative way to capture cross-language relations in 
name transliteration. The method is independent of 
the bilingual lexicon content making it easy to 
adopt to other proper names such person, product, 
or organization names. This approach is useful in a 
number of machine translation subtasks, including 
name transliteration, back transliteration, named 
entity translation, and terminology translation.  
Many opportunities exist for future research and 
improvement of the proposed approach. First, the 
method explored here can be extended as an alter-
ative way to support such MT subtasks as back 
transliteration (Knight and Graehl 1998) and noun 
phrase translation (Koehn and Knight 2003). Fi-
nally, for more challenging MT tasks, such as han-
dling sentences, the improvement of translation 
quality probably will also be achieved by combin-
ing this IR-based approach and statistical machine 
translation. For example, a pre-processing unit may 
replace the proper names in a sentence with trans-
literations (e.g., mixed code text ?The cities of ?
????? prospered under ??? and ?? 
rule.? before sending it off to MT for final transla-
tion. 
References 
GW Bian, HH Chen. Cross-language information access 
to multilingual collections on the internet. 2000. 
Journal of American Society for Information Science 
& Technology (JASIST), Special Issue on Digital Li-
braries, 51(3), pp.281-296, 2000. 
Y. Cao and H. Li. Base Noun Phrase Translation Using 
Web Data and the EM Algorithm. 2002. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics (COLING?02), pp.127-133, 2002. 
PJ. Cheng, JW. Teng, RC. Chen, JH. Wang, WH. Lu, 
and LF. Chien. Translating unknown queries with 
web corpora for cross-language information retrieval. 
2004. In Proceedings of the 27th ACM International 
Conference on Research and Development in Infor-
mation Retrieval (SIGIR04), pp. 146-153, 2004. 
I. Goto, N. Kato, N. Uratani, and T. Ehara. Translitera-
tion considering context information based on the 
maximum entropy method. In Proceedings of Ninth 
Machine Translation Summit, pp.125-132, 2003. 
F. Huang, S. Vogel, and A. Waibel. Automatic extrac-
tion of named entity translingual equivalence based 
on multi-feature cost minimization. In Proceeding of 
the 41st ACL, Workshop on Multilingual and Mixed-
Language Named Entity Recognition, Sapporo, 2003. 
A. Kilgarriff and Grefenstette, G. 2003. Introduction to 
the Special Issue on the Web as Corpus.  Computa-
tional Linguistics 29(3), pp. 333-348, 2003. 
K. Knight, J. Graehl. Machine Transliteration. 1998.  
Computational Linguistics 24(4), pp.599-612, 1998. 
P. Koehn, K. Knight. 2003. Feature-Rich Statistical 
Translation of Noun Phrases. In Proceedings of the 
41st Annual Meeting on Association for Computa-
tional Linguistics, pp. 311-318, 2003. 
J. Kupiec. 1993. An Algorithm for Finding Noun Phrase 
Correspondences in Bilingual Corpora. In Proceed-
ings of the 31st Annual Meeting of the Association 
for Computational Linguistics, pp. 17-22, 1993. 
KL Kwok. 2001. NTCIR-2 Chinese, Cross Language 
Retrieval Experiments Using PIRCS. In Proceedings 
of NTCIR Workshop Meeting, pp.111-118, 2001. 
KL Kwok, P Deng, N Dinstl, HL Sun, W Xu, P Peng, 
and Doyon, J. 2005. CHINET: a Chinese name finder 
system for document triage. In Proceedings of 2005 
1003
 International Conference on Intelligence Analysis, 
2005. 
C.J. Lee, and Jason S. Chang. 2003. Acquisition of Eng-
lish-Chinese Transliterated Word Pairs from Parallel- 
Aligned Texts using a Statistical Machine Translit-
eration Model, In Proceedings of HLT-NAACL 2003 
Workshop, pp. 96-103, 2003. 
H. Li, M. Zhang, and J. Su. 2004. A joint source-
channel model for machine transliteration. In Pro-
ceedings of the 42nd Annual Meeting on Association 
for Computational Linguistics, pp.159-166, 2004. 
Y. Li, G. Grefenstette. 2005. Translating Chinese Ro-
manized name into Chinese idiographic characters 
via corpus and web validation. In Proceedings of  
CORIA 2005, pp. 323-338, 2005. 
T. Lin, J.C. Wu, and J. S. Chang. 2004. Extraction of 
Name and Transliteration in Monolingual and Paral-
lel Corpora. In Proceedings of AMTA 2004, pp.177-
186, 2004. 
WH. Lu, LF. Chien, and HJ. Lee. 2002. Translation of 
web queries using anchor text mining. ACM Transac-
tions on Asian Language Information Processing, 
1(2):159?172, 2002. 
WH Lu, LF Chien, HJ Lee. Anchor text mining for 
translation of Web queries: A transitive translation 
approach. ACM Transactions on Information Systems 
22(2), pp. 242-269, 2004. 
M. Nagata, T. Saito, and K. Suzuki. Using the Web as a 
bilingual dictionary. 2001. In Proceedings of 39th. 
ACL Workshop on Data-Driven Methods in Machine 
Translation, pp. 95-102, 2001. 
J.-H Oh, and H. Isahara. 2006. Mining the Web for 
Transliteration Lexicons: Joint-Validation Approach, 
In IEEE/WIC/ACM International Conference on Web 
Intelligence, pp. 254-261, 2006.  
J.-H. Oh and K.-S. Choi. 2005. An ensemble of graph-
eme and phoneme for machine transliteration. In Pro-
ceedings of IJCNLP05, pp.450?461, 2005. 
Y. Qu, and G. Grefenstette. 2004. Finding Ideographic 
Representations of Japanese Names Written in Latin 
Script via Language Identification and Corpus Vali-
dation. In Proceedings of the 42nd Annual Meeting of 
the Association for Computational Linguistics, 
pp.183-190, 2004. 
CK Quah. 2006. Translation and Technology, Palgrave 
Textbooks in Translation and Interpretation, Pal-
grave MacMillan. 
Y Zhang, F Huang, S Vogel. 2005. Mining translations 
of OOV terms from the web through cross-lingual 
query expansion. In Proceedings of the 28th Annual 
International ACM SIGIR, pp.669-670, 2005.  
Y. Zhang and P. Vines. 2004. Detection and translation 
of oov terms prior to query time. In Proceedings of 
the 27th annual international ACM SIGIR conference 
on Research and development in information re-
trieval, pp.524-525, 2004. 
1004
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 519 ? 529, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Web-Based Unsupervised Learning for Query 
Formulation in Question Answering 
Yi-Chia Wang1, Jian-Cheng Wu2, Tyne Liang1, and Jason S. Chang2 
1
 Dep. of Computer and Information Science, National Chiao Tung University,  
1001 Ta Hsueh Rd., Hsinchu, Taiwan 300, R.O.C. 
rhyme.cis92g@nctu.edu.tw, tliang@cis.nctu.edu.tw 
2
 Dep. of Computer Science, National Tsing Hua University,  
101, Section 2 Kuang Fu Road, Hsinchu, Taiwan 300, R.O.C. 
d928322@oz.nthu.edu.tw, jschang@cs.nthu.edu.tw 
Abstract. Converting questions to effective queries is crucial to open-domain 
question answering systems. In this paper, we present a web-based 
unsupervised learning approach for transforming a given natural-language 
question to an effective query. The method involves querying a search engine 
for Web passages that contain the answer to the question, extracting patterns 
that characterize fine-grained classification for answers, and linking these 
patterns with n-grams in answer passages. Independent evaluation on a set of 
questions shows that the proposed approach outperforms a naive keyword-
based approach in terms of mean reciprocal rank and human effort. 
1   Introduction 
An automated question answering (QA) system receives a user?s natural-language 
question and returns exact answers by analyzing the question and consulting a large 
text collection [1, 2]. As Moldovan et al [3] pointed out, over 60% of the QA errors 
can be attributed to ineffective question processing, including query formulation and 
query expansion.  
A naive solution to query formulation is using the keywords in an input question as 
the query to a search engine. However, it is possible that the keywords may not appear 
in those answer passages which contain answers to the given question. For example, 
submitting the keywords in ?Who invented washing machine?? to a search engine like 
Google may not lead to retrieval of answer passages like ?The inventor of the automatic 
washer was John Chamberlain.? In fact, by expanding the keyword set (?invented?, 
?washing?, ?machine?) with ?inventor of,? the query to a search engine is effective in 
retrieving such answer passages as the top-ranking pages. Hence, if we can learn how to 
associate a set of questions (e.g. (?who invented ???) with effective keywords or 
phrases (e.g. ?inventor of?) which are likely to appear in answer passages, the search 
engine will have a better chance of retrieving pages containing the answer. 
In this paper, we present a novel Web-based unsupervised learning approach to 
handling question analysis for QA systems. In our approach, training-data questions 
are first analyzed and classified into a set of fine-grained categories of question 
520 Y.-C. Wang et al 
patterns. Then, the relationships between the question patterns and n-grams in answer 
passages are discovered by employing a word alignment technique. Finally, the best 
query transforms are derived by ranking the n-grams which are associated with a 
specific question pattern. At runtime, the keywords in a given question are extracted 
and the question is categorized. Then the keywords are expanded according the 
category of the question. The expanded query is the submitted to a search engine in 
order to bias the search engine to return passages that are more likely to contain 
answers to the question. Experimental results indicate the expanded query indeed 
outperforms the approach of directly using the keywords in the question. 
2   Related Work 
Recent work in Question Answering has attempted to convert the original input 
question into a query that is more likely to retrieve the answers. Hovy et al [2] utilized 
WordNet hypernyms and synonyms to expand queries to increase recall. Hildebrandt et 
al. [4] looked up in a pre-compiled knowledge base and a dictionary to expand a 
definition question. However, blindly expanding a word using its synonyms or 
dictionary gloss may cause undesirable effects. Furthermore, it is difficult to determine 
which of many related word senses should be considered when expanding the query.  
Radev et al [5] proposed a probabilistic algorithm called QASM that learns the best 
query expansion from a natural language question. The query expansion takes the 
form of a series of operators, including INSERT, DELETE, REPLACE, etc., to 
paraphrase a factual question into the best search engine query by applying 
Expectation Maximization algorithm. On the other hand, Hermjakob et al [6] 
described an experiment to observe and learn from human subjects who were given a 
question and asked to write queries which are most effective in retrieving the answer 
to the question. First, several randomly selected questions are given to users to 
?manually? generate effective queries that can bias Web search engines to return 
answers. The questions, queries, and search results are then examined to derive seven 
query reformulation techniques that can be used to produce queries similar to the ones 
issued by human subjects. 
In a study closely related to our work, Agichtein et al [7] presented Tritus system 
that automatically learns transforms of wh-phrases (e.g. expanding ?what is? to 
?refers to?) by using FAQ data. The wh-phrases are restricted to sequences of 
function word beginning with an interrogative, (i.e. who, what, when, where, why, 
and how).  These wh-phrases tend to coarsely classify questions into a few types. 
Tritus uses heuristic rules and thresholds of term frequencies to learn transforms. 
In contrast to previous work, we rely on a mathematical model trained on a set of 
questions and answers to learn how to transform the question into an effective query. 
Transformations are learned based on a more fine-grained question classification 
involving the interrogative and one or more content words. 
3   Transforming Question to Query 
The method is aimed at automatically learning of the best transforms that turn a given 
natural language question into an effective query by using the Web as corpus. To that 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 521 
end, we first automatically obtain a collection of answer passages (APs) as the 
training corpus from the Web by using a set of (Q, A) pairs. Then we identify the 
question pattern for each Q by using statistical and linguistic information. Here, a 
question pattern Qp is defined as a question word plus one or two keywords that are 
related to the question word. Qp represents the question intention and it can be treated 
as a preference indicative for fine-grained type of named entities. Finally, we decide 
the transforms Ts for each Qp by choosing those phrases in the APs that are 
statistically associated with Qp and adjacent to the answer A. 
Table 1. An example of converting a question (Q) with its answer (A) to a SE query and 
retrieving answer passages (AP) 
(Q, A) AP 
Bungalow For Rent in Islamabad, Capital 
Pakistan. Beautiful Big House For ? What is the capital of Pakistan?  Answer:( Islamabad) 
(k1, k2,?, kn, A) 
Islamabad is the capital of Pakistan. Current 
time, ? 
capital, Pakistan, Islamabad ?the airport which serves Pakistan's capital Islamabad, ? 
3.1   Search the Web for Relevant Answer Passages 
For training purpose, a large amount of question/answer passage pairs are mined from 
the Web by using a set of question/answer pairs as seeds.  
More formally, we attempt to retrieve a set of (Q, AP) pairs on the Web for training 
purpose, where Q stands for a natural language question, and AP is a passage 
containing at least one keyword in Q and A (the answer to Q). The seed data (Q, A) 
pairs can be acquired from many sources, including trivia game Websites, TREC QA 
Track benchmarks, and files of Frequently Asked Questions (FAQ). The output of 
this training-data gathering process is a large collection of (Q, AP) pairs. We describe 
the procedure in details as follows: 
1. For each (Q, A) pair, the keywords k1, k2,?, kn are extracted from Q by removing 
stopwords. 
2. Submit (k1, k2,?, kn, A) as a query to a search engine SE. 
3. Download the top n summaries returned by SE. 
4. Separate sentences in the summaries, and remove HTML tags, URL, special 
character references (e.g., ?&lt;?). 
5. Retain only those sentences which contain A and some ki. 
Consider the example of gathering answer passages from the Web for the (Q, A) 
pair where Q = ?What is the capital of Pakistan?? and A = ?Islamabad.? See Table 1 
for the query submitted to a search engine and potential answer passages returned. 
3.2   Question Analysis 
This subsection describes the presented identification of the so-called ?question 
pattern? which is critical in categorizing a given question and transforming the 
question into a query. 
522 Y.-C. Wang et al 
Formally, a ?question pattern? for any question is defined as following form: 
question-word  head-word+ 
where ?question-word? is one of the interrogatives (Who/What/Where/When/How) 
and the ?head-word? represents the headwords in the subsequent chunks that tend to 
reflect the intended answer more precisely. If the first headword is a light verb, an 
additional headword is needed. For instance, ?who had hit? is a reasonable question 
pattern for ?Who had a number one hit in 1984 with ?Hello???, while ?who had? 
seems to be too coarse. 
In order to determine the appropriate question pattern for each question, we 
examined and analyzed a set of questions which are part-of-speech (POS) tagged and 
phrase-chunked. With the help of a set of simple heuristic rules based on POS and 
chunk information, fine-grained classification of questions can be carried out 
effectively. 
Question Pattern Extraction 
After analyzing recurring patterns and regularity in quizzes on the Web, we designed 
a simple procedure to recognize question patterns. The procedure is based on a small 
set of prioritized rules. 
The question word which is one of the wh-words (?who,? ?what,? ?when,? 
?where,? ?how,? or ?why?) tagged as determiner or adverbial question word. 
According to the result of POS tagging and phrase chunking, we further decide the 
main verb and the voice of the question. Then, we apply the following expanded rules 
to extract words to form question patterns: 
Rule 1: Question word in a chunk of length more than one (see Example (1) in Table 2). 
Qp = question word + headword in the same chunk 
Rule 2: Question word followed by a light verb and Noun Phrase(NP) or 
Prepositional Phrase(PP) chunk (Example (2)). 
Qp = question word + light verb +headword in the following NP or PP chunk 
Rule 3: Question word followed immediately by a verb (Example (3)).  
Qp = question word + headword in the following Verb Phrase(VP) or NP chunk 
Rule 4: Question word followed by a passive VP (Example (4)).  
Qp = Question word + ?to be? + headword in the passive VP chunk 
Rule 5: Question word followed by the copulate ?to be? and an NP (Example (5)).  
Qp = Question word + ?to be? + headword in the next NP chunk 
Rule 6: If none of the above rules are applicable, the question pattern is the question 
word. 
By exploiting linguistic information of POS and chunks, we can easily form the 
question pattern. These heuristic rules are intuitive and easy to understand. Moreover, 
the fact that these patterns which tend to recur imply that they are general and it is 
easy to gather training data accordingly. These question patterns also indicate a 
preference for the answer to be classified with a fine-grained type of proper nouns. In 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 523 
the next section, we describe how we exploit these patterns to learn the best question-
to-query transforms. 
Table 2. Example questions and question patterns (of words shown in bold) 
(1) Which female singer performed the first song on Top of the Pops? 
(2) Who in 1961 made the first space flight? 
(3) Who painted ?The Laughing Cavalier?? 
(4) What is a group of geese called? 
(5) What is the second longest river in the world? 
3.3   Learning Best Transforms 
This section describes the procedure for learning transforms Ts which convert the 
question pattern Qp into bigrams in relevant APs. 
Word Alignment Across Q and AP 
We use word alignment techniques developed for statistical machine translation to 
find out the association between question patterns in Q and bigrams in AP. The reason 
why we use bigrams in APs instead of unigrams is that bigrams tend to have more 
unique meaning than single words and are more effective in retrieving relevant 
passages. 
We use Competitive Linking Algorithm [8] to align a set of (Q, AP) pairs. The 
method involves preprocessing steps for each (Q, AP) pair so as to filter useless 
information: 
1. Perform part-of-speech tagging on Q and AP. 
2. Replace all instances of A with the tag <ANS> in APs to indicate the location of 
the answers. 
3. Identify the question pattern, Qp and keywords which are not a named entity. We 
denote the question pattern and keywords as q1, q2, ..., qn. 
4. Convert AP into bigrams and eliminate bigrams with low term frequency (tf) or 
high document frequency (df). Bigrams composed of two function words are also 
removed, resulting in bigrams a1, a2, ..., am. 
We then align q?s and a?s via Competitive Linking Algorithm (CLA) procedure as 
follows: 
Input: A collection C of (Q; A) pairs, where (Q; A) = (q1 = Qp , q2, q3, ..., qn ; a1, a2, 
..., am) 
Output: Best alignment counterpart a?s for all q?s in C 
1. For each pair of (Q; A) in C and for all qi and aj in each pair of C, calculate LLR(qi, 
aj), logarithmic likelihood ratio (LLR) between qi and aj, which reflects their 
statistical association. 
2. Discard (q, a) pairs with a LLR value lower than a threshold. 
524 Y.-C. Wang et al 
3. For each pair of (Q; A) in C and for all qi and aj therein, carry out Steps 4-7: 
4. Sort list of (qi, aj) in each pair of (Q ; A) by decreasing LLR value. 
5. Go down the list and select a pair if it does not conflict with previous selection. 
6. Stop when running out of pairs in the list. 
7. Produce the list of aligned pairs for all Qs and APs. 
8. Tally the counts of aligning (q, a). 
9. Select top k bigrams, t1, t2, ..., tk, for every question pattern or keyword q. 
The LLR statistics is generally effective in distinguishing related terms from 
unrelated ones. However, if two terms occur frequently in questions, their alignment 
counterparts will also occur frequently, leading to erroneous alignment due to indirect 
association. CLA is designed to tackle the problem caused by indirect association. 
Therefore, if we only make use of the alignment counterpart of the question pattern, 
we can keep the question keywords in Q so as to reduce the errors caused by indirect 
association. For instance, the question ?How old was Bruce Lee when he died?? Our 
goal is to learn the best transforms for the question pattern ?how old.? In other words, 
we want to find out what terms are associated with ?how old? in the answer passages. 
However, if we consider the alignment counterparts of ?how old? without considering 
those keyword like ?died,? we run the risk of getting ?died in? or ?is dead? rather than 
?years old? and ?age of.? If we have sufficient data for a specific question pattern like 
?how long,? we will have more chances to obtain alignment counterparts that are 
effective terms for query expansion. 
Distance Constraint and Proximity Ranks 
In addition to the association strength implied with alignment counts and co-
occurrence, the distance of the bigrams to the answer should also be considered. We 
observe that terms in the answer passages close to the answers intuitively tend to be 
useful in retrieving answers. Thus, we calculate the bigrams appearing in a window of 
three words appearing on both sides of the answers to provide additional constraints 
for query expansion. 
Combing Alignment and Proximity Ranks 
The selection of the best bigrams as the transforms for a specific question pattern is 
based on a combined rank of alignment count and proximity count. It takes the 
average of these two counts to re-rank bigrams. The average rank of a bigram b is  
Rankavg (b) = (Rankalign (b)+ Rankprox (b))/2, 
where Rankalign (b) is the rank of b?s alignment count and Rankprox (b) is the rank of 
b?s proximity count. The n top-ranking bigrams  for a specific type of question will be 
chosen to transform the question pattern into query terms. For the question pattern 
?how old,? the candidate bigrams with alignment ranks, co-occurring ranks, and 
average ranks are shown in Table 3. 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 525 
Table 3. Average rank calculated from for the bigram counterparts of ?how old? 
Bigrams Alignment Rank Proximity Rank Avg. Rank Final Rank 
age of 1 1 1 1 
years old 2 2 2 2 
ascend the 3 - - - 
throne in 4 3 3.5 3 
the youngest 3 - - - 
? ? ? ? ? 
3.4   Runtime Transformation of Questions 
At runtime, a given question Q submitted by a user is converted into one or more 
keywords and a question pattern, which is subsequently expanded in to a sequence of 
query terms based on the transforms obtained at training. 
We follow the common practice of keyword selection in formulating Q into a 
query: 
? Function words are identified and discarded. 
? Proper nouns that are capitalized or quoted are treated as a single search term with 
quotes. 
Additionally, we expand the question patterns based on alignment and proximity 
considerations: 
? The question pattern Qp is identified according to the rules (in Section 3.2) and is 
expanded to be a disjunction (sequence of ORs) of Qp?s headword and n top-
ranking bigrams (in section 3.3) 
? The query will be a conjunction (sequence of ANDs) of expanded Qp, proper 
names, and remaining keywords. Except for the expanded Qp, all other proper 
names and keywords will be in the original order in the given question for the best 
results. 
Table 4. An example of transformation from question into query 
Question 
How old was Bruce Lee when he died? 
Question pattern Proper noun Keyword 
how old 
Transformation 
age of, years old 
?Bruce Lee? died 
Expanded query 
Boolean query: ( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died? 
Equivalent Google query: (old || ?age of? || ?years old?) ?Bruce Lee? died 
526 Y.-C. Wang et al 
For example, formulating a query for the question ?How old was Bruce Lee when 
he died?? will result in a question pattern ?how old.? Because there is a proper noun 
?Bruce Lee? in the question and a remaining keyword ?died,? the query becomes  
?( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died.?? Table 4 lists the 
query formulating for the example question.  
4   Experiments and Evaluation 
The proposed method is implemented by using the Web search engine, Google, as the 
underlying information retrieval system. The experimental results are also justified 
with assessing the effectiveness of question classification and query expansion. 
We used a POS tagger and chunker to perform shallow parsing of the questions 
and answer passages. The tagger was developed using the Brown corpus and 
WordNet. The chunker is built from the shared CoNLL-2000 data provided by 
CoNLL-2000. The shared task CoNLL-2000 provides a set of training and test data 
for chunks. The chunker we used produces chunks with an average precision rate of 
about 94%. 
4.1   Evaluation of Question Patterns 
The 200 questions from TREC-8 QA Track provide an independent evaluation of how 
well the proposed method works for question pattern extraction works. We will also 
give an error analysis. 
Table 5. Evaluation results of question pattern extraction 
 Two ?good? labels At least one ?good? label 
Precision (%) 86 96 
Table 6. The first five questions with question patterns and judgment 
Question Question pattern Judgment 
Who is the author of the book, "The Iron 
Lady: A Biography of Margaret Thatcher"? Who-author good 
What was the monetary value of the Nobel 
Peace Prize in 1989? What value good  
What does the Peugeot company manufacture? What do 
manufacture good 
How much did Mercury spend on advertising 
in 1993?      How much good 
What is the name of the managing director of 
Apricot Computer? What name bad 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 527 
Two human judges both majoring in Foreign Languages were asked to assess the 
results of question pattern extraction and give a label to each extracted question 
pattern. A pattern will be judged as ?good? if it clearly expresses the answer 
preference of the question; otherwise, it is tagged as ?bad.? The precision rate of 
extraction for these 200 questions is shown in Table 5. The second column indicates 
the precision rate when both of two judges agree that an extracted question pattern is 
?good.? In addition, the third column indicates the rate of those question patterns that 
are found to be ?good? by either judge. The results imply that the proposed pattern 
extraction rules are general, since they are effective even for questions independent of 
the training and development data. Table 6 shows evaluation results for ?two ?good? 
labels? of the first five questions. 
We summarize the reasons behind these bad patterns: 
? Incorrect part-of-speech tagging and chunking 
? Imperative questions such as ?Name the first private citizen to fly in space.? 
? Question patterns that are not specific enough 
For instance, the system produces ?what name? for ?What is the name of the 
chronic neurological autoimmune disease which ? ??, while the judges suggested 
that ?what disease.?. Indeed, some of the patterns extracted can be modified to meet 
the goal of being more fine-grained and indicative of a preference to a specific type of 
proper nouns or terminology. 
4.2   Evaluation of Query Expansion 
We implemented a prototype of the proposed method called Atlas (Automatic 
Transform Learning by Aligning Sentences of question and answer). To develop the 
system of Atlas, we gathered seed training data of questions and answers from a trivia 
game website, called QuizZone1. We collected the questions posted in June, 2004 on 
QuizZone and obtained 3,851 distinct question-answer pairs. We set aside the first 45 
questions for testing and used the rest for training. For each question, we form a query 
with question keywords and the answer and submitted the query to Google to retrieve 
top 100 summaries as the answer passages. In all, we collected 95,926 answer passages.  
At training time, we extracted a total of 338 distinct question patterns from 3,806 
questions. We aligned these patterns and keywords with bigrams in the 95,926 answer 
passages, identified the locations of the answers, and obtained the bigrams appearing 
within a distance of 3 of the answers. At runtime, we use the top-ranking bigram to 
expand each question pattern. If no such bigrams are found, we use only the keyword 
in the question patterns. The expanded terms for question pattern are placed at the 
beginning of the query.  
We submitted forty-five keyword queries and the same number of expanded 
queries generated by Atlas for the test questions to Google and obtained ten returned 
summaries for evaluation. For the evaluation, we use three indicators to measure the 
performance. The first indicator is the mean reciprocal rank (MRR) of the first 
relevant document (or summary) returned. If the r-th document (summary) returned is 
the one with the answer, then the reciprocal rank of the document (summary) is 1/r. 
                                                          
1
 QuizZone (http://www.quiz-zone.co.uk) 
528 Y.-C. Wang et al 
The mean reciprocal rank is the average reciprocal rank of all test questions. The 
second indicator of effective query is the recall at R document retrieved (Recall at R). 
The last indicator measures the human effort (HE) in finding the answer. HE is 
defined as the least number of passages needed to be viewed for covering all the 
answers to be returned from the system. 
The average length of these test questions is short. We believe the proposed 
question expansion scheme helps those short sentences, which tend to be less 
effective in retrieving answers. We evaluated the expanded queries against the same 
measures for summaries returned by simple keyword queries. Both batches of 
returned summaries for the forty-five questions were verified by two human judges. 
As shown in Table 7, the MRR produced by keyword-based scheme is slightly lower 
than the one yielded by the presented query expansion scheme. Nevertheless, such 
improvement is encouraging by indicating the effectiveness of the proposed method. 
Table 8 lists the comparisons in more details. It is found that our method is 
effective in bringing the answers to the top 1 and top 2 summaries as indicated by the 
high Recall of 0.8 at R = 2. In addition, Table 8 also shows that less user?s efforts are 
needed by using our approach. That is, for each question, the average of summaries 
required to be viewed by human beings goes down from 2.7 to 2.3. 
In the end, we found that those bigrams containing a content word and a function 
word  turn out to be very effective. For instance, our method tends to favor transforms 
Table 7. Evaluation results of MRR 
Performances MRR 
GO (Direct keyword query for Google) 0.64 
AT+GO (Atlas expanded query for Google) 0.69 
Table 8. Evaluation Result of Recall at R and Human Effort 
Rank count Recall at R Rank GO AT+GO GO AT+GO 
1 25 26 0.56 0.58 
2 6 10 0.69 0.80 
3 5 3 0.80 0.87 
4 0 1 0.80 0.89 
5 1 1 0.82 0.91 
6 2 0 0.87 0.91 
7 1 0 0.89 0.91 
8 2 0 0.93 0.91 
9 0 1 0.93 0.93 
10 0 0 0.93 0.93 
No answers 3 3 
Human Effort 122 105 
 
# of questions 45 45 
HE per question 2.7 2.3 
 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 529 
such as ?who invented? to bigrams such as ?invented by,? ?invent the,? and ?inventor 
of.? This contrasts to conventional wisdom of using a stoplist of mostly function 
words and excluding them from consideration in a query. Our experiment also shows 
a function word as part of a phrasal term seems to be very effective, for it indicate an 
implied relation with the answer.  
5   Conclusion and Future Work 
In this paper, we introduce a method for learning query transformations that improves 
the ability to retrieve passages with answers using the Web as corpus. The method 
involves question classification and query transformations using a learning-based 
approach. We also describe the experiment with over 3,000 questions indicates that 
satisfactory results were achieved. The experimental results show that the proposed 
method provides effective query expansion that potentially can lead to performance 
improvement for a question answering system. 
A number of future directions present themselves. First, the patterns learned from 
answer passages acquired on the Web can be refined and clustered to derive a 
hierarchical classification of questions for more effective question classification. Second, 
different question patterns, like ?who wrote? and ?which author?, should be treated as the 
same in order to cope with data sparseness and improve system performance. On the 
other hand, an interesting direction is the generating pattern transformations that contain 
the answer extraction patterns for different types of questions.  
References 
1. Ittycheriah, A., Franz, M., Zhu, W.-J., and Rathaparkhi, A. 2000. IBM?s statistical 
question answering system. In Proceedings of the TREC-9 Question Answering Track, 
Gaithersburg, Maryland. 
2. Hovy, E., Gerber, L., Hermjakob, U., Junk, M., and Lin, C.-Y. 2000. Question answering 
in Webclopedia. In Proceedings of the TREC-9 Question Answering Track, Gaithersburg, 
Maryland. 
3. Moldovan D., Pasca M., Harabagiu S., & Surdeanu M. 2002. Performance Issues and error 
Analysis in an Open-Domain Question Answering System. In Proceedings of the 40th 
Annual Meeting of ACL, Philadelphia, Pennsylvania. 
4. Hildebrandt, W., Katz, B., & Lin, J. 2004. Answering definition questions with multiple 
knowledge sources. In Proceedings of the 2004 Human Language Technology Conference 
and the North American Chapter of the Association for Computational. 
5. Radev, D. R., Qi, H., Zheng, Z., Blair-Goldensohn, S., Fan, Z. Z. W., and Prager, J. M. 
2001. Mining the web for answers to natural language questions. In Proceedings of the 
International Conference on Knowledge Management (CIKM-2001), Atlanta, Georgia. 
6. Hermjakob, U., Echihabi, A., and Marcu, D. 2002. Natural Language Based 
Reformulation Resource and Web Exploitation for Question Answering. In Proceeding of 
TREC-2002, Gaithersburg, Maryland. 
7. Agichtein, E., Lawrence, S., and Gravano, L. Learning to find answers to questions on the 
Web. 2003. In ACM Transactions on Internet Technology (TOIT), 4(2):129-162. 
8. Melamed, I. D. 1997. A Word-to-Word Model of Translational Equivalence. In 
Proceedings of the 35st Annual Meeting of ACL, Madrid, Spain. 
9. Yi-Chia Wang, Jian-Cheng Wu, Tyne Liang, and Jason S. Chang. 2004. Using the Web as 
Corpus for Un-supervised Learning in Question Answering, Proceedings of Rocling 2004, 
Taiwan. 
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254?262,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Bilingual Linguistic Reordering Model for Statistical  
Machine Translation 
 
 
Han-Bin Chen, Jian-Cheng Wu and Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Guangfu Road, Hsinchu, Taiwan 
{hanbin,d928322,jschang}@cs.nthu.edu.tw 
 
 
 
Abstract 
In this paper, we propose a method for learn-
ing reordering model for BTG-based statisti-
cal machine translation (SMT). The model 
focuses on linguistic features from bilingual 
phrases. Our method involves extracting reor-
dering examples as well as features such as 
part-of-speech and word class from aligned 
parallel sentences. The features are classified 
with special considerations of phrase lengths. 
We then use these features to train the maxi-
mum entropy (ME) reordering model. With 
the model, we performed Chinese-to-English 
translation tasks. Experimental results show 
that our bilingual linguistic model outper-
forms the state-of-the-art phrase-based and 
BTG-based SMT systems by improvements of 
2.41 and 1.31 BLEU points respectively. 
1 Introduction 
Bracketing Transduction Grammar (BTG) is a spe-
cial case of Synchronous Context Free Grammar 
(SCFG), with binary branching rules that are either 
straight or inverted. BTG is widely adopted in 
SMT systems, because of its good trade-off be-
tween efficiency and expressiveness (Wu, 1996). 
In BTG, the ratio of legal alignments and all possi-
ble alignment in a translation pair drops drastically 
especially for long sentences, yet it still covers 
most of the syntactic diversities between two lan-
guages. 
It is common to utilize phrase translation in 
BTG systems. For example in (Xiong et al, 2006), 
source sentences are segmented into phrases. Each 
sequences of consecutive phrases, mapping to cells 
in a CKY matrix, are then translated through a bi-
lingual phrase table and scored as implemented in 
(Koehn et al, 2005; Chiang, 2005). In other words, 
their system shares the same phrase table with 
standard phrase-based SMT systems.  
 
 3 ? ?   3 ? ?  
three   after  2A  
years 1A   three   
ago  2A  years 1A    
(a) (b) 
Figure 1: Two reordering examples, with straight 
rule applied in (a), and inverted rule in (b). 
 
 
On the other hand, there are various proposed 
BTG reordering models to predict correct orienta-
tions between neighboring blocks (bilingual 
phrases). In Figure 1, for example, the role of reor-
dering model is to predict correct orientations of 
neighboring blocks A1 and A2. In flat model (Wu, 
1996; Zens et al, 2004; Kumar and Byrne, 2005), 
reordering probabilities are assigned uniformly 
during decoding, and can be tuned depending on 
different language pairs. It is clear, however, that 
this kind of model would suffer when the dominant 
rule is wrongly applied. 
Predicting orientations in BTG depending on 
context information can be achieved with lexical 
features. For example, Xiong et al (2006) pro-
posed MEBTG, based on maximum entropy (ME) 
classification with words as features. In MEBTG, 
first words of blocks are considered as the fea-
tures, which are then used to train a ME model 
254
for predicting orientations of neighboring blocks. 
Xiong et al (2008b) proposed a linguistically an-
notated BTG (LABTG), in which linguistic fea-
tures such as POS and syntactic labels from 
source-side parse trees are used. Both MEBTG 
and LABTG achieved significant improvements 
over phrase-based Pharaoh (Koehn, 2004) and 
Moses (Koehn et al, 2007) respectively, on Chi-
nese-to-English translation tasks. 
 
 ?    ?   ?? 
Nes  Nf    Nv 
?   ?? 
DE   Na 
the  details  of 
   14      49     50 
 
2A  
the  plan 
   14    18 1A  
 
Figure 2: An inversion reordering example, with 
POS below source words, and class numbers below 
target words. 
 
 
However, current BTG-based reordering meth-
ods have been limited by the features used.  Infor-
mation might not be sufficient or representative, if 
only the first (or tail) words are used as features. 
For example, in Figure 2, consider target first-word 
features extracted from an inverted reordering ex-
ample (Xiong et al, 2006) in MEBTG, in which 
first words on two blocks are both "the". This kind 
of feature set is too common and not representative 
enough to predict the correct orientation. Intui-
tively, one solution is to extend the feature set by 
considering both boundary words, forming a more 
complete boundary description. However, this 
method is still based on lexicalized features, which 
causes data sparseness problem and fails to gener-
alize. In Figure 2, for example, the orientation 
should basically be the same, when the 
source/target words "??/plan" from block A1 is 
replaced by other similar nouns and translations 
(e.g. "plans", "events" or "meetings"). However, 
such features would be treated as unseen by the 
current ME model, since the training data can not 
possibly cover all such similar cases. 
In this paper we present an improved reorder-
ing model based on BTG, with bilingual linguistic 
features from neighboring blocks. To avoid data 
sparseness problem, both source and target words 
are classified; we perform part-of-speech (POS) 
tagging on source language, and word classifica-
tion on target one, as shown in Figure 2. Addition-
ally, features are extracted and classified 
depending on lengths of blocks in order to obtain a 
more informed model. 
The rest of this paper is organized as follows. 
Section 2 reviews the related work. Section 3 de-
scribes the model used in our BTG-based SMT 
systems. Section 4 formally describes our bilingual 
linguistic reordering model. Section 5 and Section 
6 explain the implementation of our systems. We 
show the experimental results in Section 7 and 
make the conclusion in Section 8. 
2 Related Work 
In statistical machine translation, reordering model 
is concerned with predicting correct orders of tar-
get language sentence given a source language one 
and translation pairs. For example, in phrase-based 
SMT systems (Koehn et al, 2003; Koehn, 2004), 
distortion model is used, in which reordering prob-
abilities depend on relative positions of target side 
phrases between adjacent blocks. However, distor-
tion model can not model long-distance reordering, 
due to the lack of context information, thus is diffi-
cult to predict correct orders under different cir-
cumstances. Therefore, while phrase-based SMT 
moves from words to phrases as the basic unit of 
translation, implying effective local reordering 
within phrases, it suffers when determining phrase 
reordering, especially when phrases are longer than 
three words (Koehn et al, 2003). 
There have been much effort made to improve 
reordering model in SMT. For example, research-
ers have been studying CKY parsing over the last 
decade, which considers translations and orienta-
tions of two neighboring block according to 
grammar rules or context information. In hierar-
chical phrase-based systems (Chiang, 2005), for 
example, SCFG rules are automatically learned 
from aligned bilingual corpus, and are applied in 
CKY style decoding. 
As an another application of CKY parsing tech-
nique is BTG-based SMT. Xiong et al (2006) and 
Xiong et al (2008a) developed MEBTG systems, 
in which first or tail words from reordering exam-
ples are used as features to train ME-based reorder-
ing models. 
Similarly, Zhang et al (2007) proposed a model 
similar to BTG, which uses first/tail words of 
phrases, and syntactic labels (e.g. NP and VP) 
255
from source parse trees as features. In their work, 
however, inverted rules are allowed to apply only 
when source phrases are syntactic; for non-
syntactic ones, blocks are combined straight with a 
constant score.  
More recently, Xiong et al (2008b) proposed 
LABTG, which incorporates linguistic knowledge 
by adding features such as syntactic labels and 
POS from source trees to improve their MEBTG. 
Different from Zhang's work, their model do not 
restrict non-syntactic phrases, and applies inverted 
rules on any pair of neighboring blocks. 
Although POS information is used in LABTG 
and Zhang's work, their models are syntax-oriented, 
since they focus on syntactic labels. Boundary POS 
is considered in LABTG only when source phrases 
are not syntactic phrases. 
In contrast to the previous works, we present a 
reordering model for BTG that uses bilingual in-
formation including class-level features of POS 
and word classes. Moreover, our model is dedi-
cated to boundary features and considers different 
combinations of phrase lengths, rather than only 
first/tail words. In addition, current state-of-the-art 
Chinese parsers, including the one used in LABTG 
(Xiong et al, 2005), lag beyond in inaccuracy, 
compared with English parsers (Klein and Man-
ning, 2003; Petrov and Klein 2007). In our work, 
we only use more reliable information such as 
Chinese word segmentation and POS tagging (Ma 
and Chen, 2003). 
3 The Model 
Following Wu (1996) and Xiong et al (2006), we 
implement BTG-based SMT as our system, in 
which three rules are applied during decoding: 
 ? ?21 AAA ?   (1) 
21 AAA ?    (2) 
yxA /?    (3) 
 
where A1 and A2 are blocks in source order. Straight 
rule (1) and inverted rule (2) are reordering rules. 
They are applied for predicting target-side order 
when combining two blocks, and form the reorder-
ing model with the distributions 
 
reoorderAA ?)(P ,,reo 21  
 
where order ?{straight, inverted}. 
In MEBTG, a ME reordering model is trained 
using features extracted from reordering examples 
of aligned parallel corpus. First words on neighbor-
ing blocks are used as features. In reordering ex-
ample (a), for example, the feature set is 
 
{"S1L=three", "S2L=ago", "T1L=3", "T2L=?"} 
 
where "S1" and "T1" denote source and target 
phrases from the block A1. 
Rule (3) is lexical translation rule, which trans-
lates source phrase x into target phrase y. We use 
the same feature functions as typical phrase-based 
SMT systems (Koehn et al, 2005): 
 
654
321
ee)|(
)|()|()|()|(Ptrans
???
???
y
lw
lw
xyp
yxpxypyxpyx
???
???  
 
where 43 )|()|( ?? xypyxp lwlw ? , 5e? and 6e ?y  
are lexical translation probabilities in both direc-
tions, phrase penalty and word penalty. 
During decoding, the blocks are produced by 
applying either one of two reordering rules on two 
smaller blocks, or applying lexical rule (3) on 
some source phrase. Therefore, the score of a block 
A is defined as 
 
reolm orderAAAA
AAA
?? ),,(P),(P
)P()P()P(
reo21lm
21
21???
??  
 
or 
 
)|(P)(P)P( translm yxAA lm ?? ?  
 
where lmA ?)(Plm  and lmAA ?),(P 21lm?  are respec-
tively the usual and incremental score of language 
model. 
To tune all lambda weights above, we perform 
minimum error rate training (Och, 2003) on the 
development set described in Section 7. 
Let B be the set of all blocks with source side 
sentence C. Then the best translation of C is the 
target side of the block A , where 
 
256
)P(argmaxA A
BA?
?  
4 Bilingual Linguistic Model 
In this section, we formally describe the problem 
we want to address and the proposed method. 
4.1 Problem Statement 
We focus on extracting features representative of 
the two neighboring blocks being considered for 
reordering by the decoder, as described in Section 
3. We define S(A) and T(A) as the information on 
source and target side of a block A. For two 
neighboring blocks A1 and A2, the set of features 
extracted from information of them is denoted as 
feature set function F(S(A1), S(A2), T(A1), S(A2)). In 
Figure 1 (b), for example, S(A1) and T(A1) are sim-
ply the both sides sentences "3 ? " and "three 
years", and F(S(A1), S(A2), T(A1), S(A2)) is 
 
{"S1L=three", "S2L=after", "T1L=3", "T2L=?"} 
 
where "S1L" denotes the first source word on the 
block A1, and "T2L" denotes the first target word 
on the block A2. 
Given the adjacent blocks A1 and A2, our goal 
includes (1) adding more linguistic and representa-
tive information to A1 and A2 and (2) finding a fea-
ture set function F' based on added linguistic 
information in order to train a more linguistically 
motivated and effective model. 
4.2 Word Classification 
As described in Section 1, designing a more com-
plete feature set causes data sparseness problem, if 
we use lexical features. One natural solution is us-
ing POS and word class features.  
In our model, we perform Chinese POS tagging 
on source language. In Xiong et al (2008b) and 
Zhang et al (2007), Chinese parsers with Penn 
Chinese Treebank (Xue et al, 2005) style are used 
to derive source parse trees, from which source-
side features such as POS are extracted. However, 
due to the relatively low accuracy of current Chi-
nese parsers compared with English ones, we in-
stead use CKIP Chinese word segmentation system 
(Ma and Chen, 2003) in order to derive Chinese 
tags with high accuracy. Moreover, compared with 
the Treebank Chinese tagset, the CKIP tagset pro-
vides more fine-grained tags, including many tags 
with semantic information (e.g., Nc for place 
nouns, Nd for time nouns), and verb transitivity 
and subcategorization (e.g., VA for intransitive 
verbs, VC for transitive verbs, VK for verbs that 
take a clause as object). 
On the other hand, using the POS features in 
combination with the lexical features in target lan-
guage will cause another sparseness problem in the 
phrase table, since one source phrase would map to 
multiple target ones with different POS sequences. 
As an alternative, we use mkcls toolkit (Och, 
1999), which uses maximum-likelihood principle 
to perform classification on target side. After clas-
sification, the toolkit produces a many-to-one 
mapping between English tokens and class num-
bers. Therefore, there is no ambiguity of word 
class in target phrases and word class features can 
be used independently to avoid data sparseness 
problem and the phrase table remains unchanged. 
As mentioned in Section 1, features based on 
words are not representative enough in some cases, 
and tend to cause sparseness problem. By classify-
ing words we are able to linguistically generalize 
the features, and hence predict the rules more 
robustly. In Figure 2, for example, the target words 
are converted to corresponding classes, and form 
the more complete boundary feature set 
 
{"T1L=14", "T1R=18", "T2L=14", "T2R=50"}  (4) 
 
In the feature set (4), #14 is the class containing 
"the", #18 is the class containing "plans", and #50 
is the class containing "of." Note that we add last-
word features "T1R=18" and "T2R=50". As men-
tioned in Section 1, the word "plan" from block A1 
is replaceable with similar nouns. This extends to 
other nominal word classes to realize the general 
rule of inverting "the ... NOUN" and "the ... of". 
It is hard to achieve this kind of generality using 
only lexicalized feature. With word classification, 
we gather feature sets with similar concepts from 
the training data. Table 1 shows the word classes 
can be used effectively to cope with data sparse-
ness. For example, the feature set (4) occurs 309 
times in our training data, and only 2 of them are 
straight, with the remaining 307 inverted examples, 
implying that similar features based on word 
classes lead to similar orientation. Additional ex-
amples of similar feature sets with different word 
classes are shown in Table 1. 
257
class X T1R = X    straight/inverted
9 graph, government 2/488 
18 plans, events 2/307 
20 bikes, motors 0/694 
48 day, month, year 4/510 
Table 1: List of feature sets in the form of 
{"T1L=14", "T1R=X", "T2L=14", "T2R=50"}. 
 
4.3 Feature with Length Consideration 
Boundary features using both the first and last 
words provide more detailed descriptions of 
neighboring blocks. However, we should take the 
special case blocks with length 1 into consideration. 
For example, consider two features sets from 
straight and inverted reordering examples (a) and 
(b) in Figure 3. There are two identical source fea-
tures in both feature set, since first words on block 
A1 and last words on block A2 are the same: 
 
{"S1L=P","S2R=Na"}?F(S(A1),S(A2),T(A1), S(A2)) 
 
Therefore, without distinguishing the special case, 
the features would represent quite different cases 
with the same feature, possibly leading to failure to 
predict orientations of two blocks.  
We propose a method to alleviate the problem of 
features with considerations of lengths of two ad-
jacent phrases by classifying both the both source 
and target phrase pairs into one of four classes: M, 
L, R and B, corresponding to different combina-
tions of phrase lengths. 
Suppose we are given two neighboring blocks 
A1 and A2, with source phrases P1 and P2 respec-
tively. Then the feature set from source side is 
classified into one of the classes as follows. We 
give examples of feature set for each class accord-
ing to Figure 4. 
 
 ?? P 
?? ?? 
Neqa  Na   
? ?? 
P    Nc 
?? ??
VC    Na
 hold meeting  2A  for 1A    
these 
reasons  2A   
in 
jordan 1A   
(a) (b) 
Figure 3: Two reordering examples with ambigu-
ous features on source side. 
 
A1 A2  A1  A2 
? 
Nh 
?? 
VE 
 ?? 
P 
 ??  ?? 
    Neqa    Na 
I think  for  these  reasons
              (a)                                     (b) 
M class                             L class 
 
A1 A2  A1  A2 
??  ? 
Na  Caa 
?? 
Na 
 ?  ?? 
P     Nc 
 ??  ??
      VC      Na
technology and equipment in  Jordan  hold  meeting
                (c)                                        (d) 
              R class                                 B class 
Figure 4:   Examples of different length combina-
tions, mapping to four classes. 
 
 
1. M class. The lengths of P1 and P2 are both 1. In 
Figure 4 (a), for example, the feature set is 
 
{"M1=Nh", "M2=VE"} 
 
2. L class. The length of P1 is 1, and the length of 
P2 is greater than 1. In Figure 4 (b), for exam-
ple, the feature set is 
 
{"L1=P", "L2=Neqa", "L3=Na"} 
 
3. R class. The length of P1 is greater than 1, and 
the length of P2 is 1. In Figure 4 (c), for exam-
ple, the feature set is 
 
{"R1=Na", "R2=Caa", "R3=Na"} 
 
4. B class. The lengths of P1 and P2 are both 
greater than 1. In Figure 4 (d), for example, the 
feature set is 
 
{"B1=P", "B2=Nc", "B3=VC", "B4=Na"} 
 
We use the same scheme to classify the two tar-
get phrases. Since both source and target words are 
classified as described in Section 4.2, the feature 
sets are more representative and tend to lead to 
consistent prediction of orientation. Additionally, 
the length-based features are easy to fit into mem-
ory, in contrast to lexical features in MEBTG. 
To summarize, we extract features based on 
word lengths, target-language word classes, and 
fine-grained, semantic oriented parts of speech. To 
illustrate, we use the neighboring blocks from Fig-
258
ure 2 to show an example of complete bilingual 
linguistic feature set: 
 
{"S.B1=Nes", "S.B2=Nv", "S.B3=DE", 
"S.B4=Na", "T.B1=14", "T.B2=18", "T.B3=14", 
"T.B4=50"} 
 
where "S." and "T." denote source and target sides. 
In the next section, we describe the process of 
preparing the feature data and training an ME 
model. In Section 7, we perform evaluations of this 
ME-based reordering model against standard 
phrase-based SMT and previous work based on 
ME and BTG. 
5 Training 
In order to train the translation and reordering 
model, we first set up Moses SMT system (Koehn 
et al, 2007). We obtain aligned parallel sentences 
and the phrase table after the training of Moses, 
which includes running GIZA++ (Och and Ney, 
2003), grow-diagonal-final symmetrization and 
phrase extraction (Koehn et al, 2005). Our system 
shares the same translation model with Moses, 
since we directly use the phrase table to apply 
translation rules (3). 
On the other side, we use the aligned parallel 
sentences to train our reordering model, which in-
cludes classifying words, extracting bilingual 
phrase samples with orientation information, and 
training an ME model for predicting orientation. 
To perform word classification, the source sen-
tences are tagged and segmented before the Moses 
training. As for target side, we ran the Moses 
scripts to classify target language words using the 
mkcls toolkit before running GIZA++. Therefore, 
we directly use its classification result, which gen-
erate 50 classes with 2 optimization runs on the 
target sentences. 
To extract the reordering examples, we choose 
sentence pairs with top 50% alignment scores pro-
vided by GIZA++, in order to fit into memory. 
Then the extraction is performed on these aligned 
sentence pairs, together with POS tags and word 
classes, using basically the algorithm presented in 
Xiong et al (2006). However, we enumerate all 
reordering examples, rather than only extract the 
smallest straight and largest inverted examples. 
Finally, we use the toolkit by Zhang (2004) to train 
the ME model with extracted reordering examples. 
6 Decoding 
We develop a bottom-up CKY style decoder in our 
system, similar to Chiang (2005). For a Chinese 
sentence C, the decoder finds its best translation on 
the block with entire C on source side. The decoder 
first applies translation rules (3) on cells in a CKY 
matrix. Each cell denotes a sequence of source 
phrases, and contains all of the blocks with possi-
ble translations. The longest length of source 
phrase to be applied translations rules is restricted 
to 7 words, in accordance with the default settings 
of Moses training scripts. 
To reduce the search space, we apply threshold 
pruning and histogram pruning, in which the block 
scoring worse than 10-2 times the best block in the 
same cell or scoring worse than top 40 highest 
scores would be pruned. These pruning techniques 
are common in SMT systems. We also apply re-
combination, which distinguish blocks in a cell 
only by 3 leftmost and rightmost target words, as 
suggested in (Xiong et al, 2006). 
7 Experiments and Results 
We perform Chinese-to-English translation task 
on NIST MT-06 test set, and use Moses and 
MEBTG as our competitors.  
The bilingual training data containing 2.2M sen-
tences pairs from Hong Kong Parallel Text 
(LDC2004T08) and Xinhua News Agency 
(LDC2007T09), with length shorter than 60, is 
used to train the translation and reordering model. 
The source sentences are tagged and segmented 
with CKIP Chinese word segmentation system (Ma 
and Chen, 2003). 
About 35M reordering examples are extracted 
from top 1.1M sentence pairs with higher align-
ment scores. We generate 171K features for lexi-
calized model used in MEBTG system, and 1.41K 
features for our proposed reordering model. 
For our language model, we use Xinhua news 
from English Gigaword Third Edition 
(LDC2007T07) to build a trigram model with 
SRILM toolkit (Stolcke, 2002). 
Our development set for running minimum error 
rate training is NIST MT-08 test set, with sentence 
lengths no more than 20. We report the experimen-
tal results on NIST MT-06 test set. Our evaluation 
metric is BLEU (Papineni et al, 2002) with case-
insensitive matching from unigram to four-gram. 
259
System BLEU-4 
Moses(distortion) 22.55 
Moses(lexicalized) 23.42 
MEBTG 23.65 
WC+LC 24.96 
Table 2: Performances of various systems. 
 
 
The overall result of our experiment is shown in 
Table 2. The lexicalized MEBTG system proposed 
by Xiong et al (2006) uses first words on adjacent 
blocks as lexical features, and outperforms phrase-
based Moses with default distortion model and en-
hanced lexicalized model, by 1.1 and 0.23 BLEU 
points respectively. This suggests lexicalized 
Moses and MEBTG with context information out-
performs distance-based distortion model. Besides, 
MEBTG with structure constraints has better 
global reordering estimation than unstructured 
Moses, while incorporating their local reordering 
ability by using phrase tables.  
The proposed reordering model trained with 
word classification (WC) and length consideration 
(LC) described in Section 4 outperforms MEBTG 
by 1.31 point. This suggests our proposed model 
not only reduces the model size by using 1% fewer 
features than MEBTG, but also improves the trans-
lation quality. 
We also evaluate the impacts of WC and LC 
separately and show the results in Table 3-5. Table 
3 shows the result of MEBTG with word classified 
features. While classified MEBTG only improves 
0.14 points over original lexicalized one, it drasti-
cally reduces the feature size. This implies WC 
alleviates data sparseness by generalizing the ob-
served features. 
Table 4 compares different length considerations, 
including boundary model demonstrated in Section 
4.2, and the proposed LC in Section 4.3. Although 
boundary model describes features better than us-
ing only first words, which we will show later, it 
suffers from data sparseness with twice feature size 
of MEBTG. The LC model has the largest feature 
size but performs best among three systems, sug-
gesting the effectiveness of our LC. 
In Table 5 we show the impacts of WC and LC 
together. Note that all the systems with WC sig-
nificantly reduce the size of features compared to 
lexicalized ones. 
 
System Feature size BLEU-4
MEBTG 171K 23.65 
WC+MEBTG 0.24K 23.79 
Table 3: Performances of lexicalized and word 
classified MEBTG. 
 
 
System Feature size BLEU-4
MEBTG 171K 23.65 
Boundary 349K 23.42 
LC 780K 23.86 
Table 4: Performances of BTG systems with dif-
ferent representativeness. 
 
 
System Feature size BLEU-4
MEBTG 171K 23.65 
WC+MEBTG 0.24K 23.79 
WC+Bounary 0.48K 24.29 
WC+LC 1.41K 24.96 
Table 5: Different representativeness with word 
classification. 
 
 
While boundary model is worse than first-word 
MEBTG in Table 4, it outperforms the latter when 
both are performed WC. We obtain the best result 
that outperforms the baseline MEBTG by more 
than 1 point when we apply WC and LC together.  
Our experimental results show that we are able 
to ameliorate the sparseness problem by classifying 
words, and produce more representative features 
by considering phrase length. Moreover, they are 
both important, in that we are unable to outperform 
our competitors by a large margin unless we com-
bine both WC and LC. In conclusion, while de-
signing more representative features of reordering 
model in SMT, we have to find solutions to gener-
alize them. 
8 Conclusion and Future Works 
We have proposed a bilingual linguistic reordering 
model to improve current BTG-based SMT sys-
tems, based on two drawbacks of previously pro-
posed reordering model, which are sparseness and 
representative problem. 
First, to solve the sparseness problem in previ-
ously proposed lexicalized model, we perform 
word classification on both sides. 
260
Secondly, we present a more representative fea-
ture extraction method. This involves considering 
length combinations of adjacent phrases. 
The experimental results of Chinese-to-English 
task show that our model outperforms baseline 
phrase-based and BTG systems. 
We will investigate more linguistic ways to clas-
sify words in future work, especially on target lan-
guage. For example, using word hierarchical 
structures in WordNet (Fellbaum, 1998) system 
provides more linguistic and semantic information 
than statistically-motivated classification tools. 
Acknowledgements 
This work was supported by National Science 
Council of Taiwan grant NSC 95-2221-E-007-182-
MY3. 
References   
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL 2005, pp. 263-270. 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, 
Massachusetts. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL 2003. 
Philipp Koehn. 2004. Pharaoh: a Beam Search  Decoder 
for Phrased-Based Statistical Machine Translation 
Models. In Proceedings of AMTA 2004. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In International Workshop on Spoken Language 
Translation. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan,Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source 
toolkit for statistical machine translation. In Proceed-
ings of ACL 2007, Demonstration Session. 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of ACL 2003. 
Shankar Kumar and William Byrne. 2005. Local phrase 
reordering models for statistical machine translation. 
In Proceedings of HLT-EMNLP 2005. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. Introduction 
to CKIP Chinese Word Segmentation System for the 
First International Chinese Word Segmentation 
Bakeoff. In Proceedings of ACL, Second SIGHAN 
Workshop on Chinese Language Processing, pp168-
171.  
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL ?99: Ninth 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 71?76, 
Bergen, Norway, June. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29:19-51. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL 2003, pages 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the 40th Annual Meeting of the ACL, pages 311?318. 
Slav Petrov and Dan Klein. 2007. Improved Inference-
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL 2007. 
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing, 
volume 2, pages 901?904. 
Dekai Wu. 1996. A Polynomial-Time Algorithm for 
Statistical Machine Translation. In Proceedings of 
ACL 1996. 
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and 
Yueliang Qian. 2005. Parsing the Penn Chinese tree-
bank with semantic knowledge. In Proceedings of 
IJCNLP 2005, pages 70-81. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
ACL-COLING 2006. 
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu, 
and Shouxun Liu. 2008a. Refinements in BTG-based 
statistical machine translation. In Proceedings of 
IJCNLP 2008, pp. 505-512. 
Deyi Xiong, Min Zhang, Ai Ti Aw, and Haizhou Li. 
2008b. Linguistically Annotated BTG for Statistical 
Machine Translation. In Proceedings of COLING 
2008. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
261
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.  
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. 
Reordering Constraints for Phrase-Based Statistical 
Machine Translation. In Proceedings of CoLing 2004, 
Geneva, Switzerland, pp. 205-211.  
Le Zhang. 2004. Maximum Entropy Modeling Toolkit 
for Python and C++. Available at http://homepa 
ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html. 
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou. 
2007. Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT. In Proceedings of EMNLP-
CoNLL 2007. 
 
262
TotalRecall: A Bilingual Concordance for Computer Assisted Translation and 
Language Learning
 
Jian-Cheng Wu , Kevin C. Yeh 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 
300, Taiwan, ROC 
g904307@cs.nthu.edu.tw 
Thomas C. Chuang 
Department of Computer Science 
Van Nung Institute of Technology 
No. 1 Van-Nung Road 
Chung-Li Tao-Yuan, Taiwan, ROC 
tomchuang@cc.vit.edu.tw 
Wen-Chi Shei , Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, 
Taiwan, ROC 
jschang@cs.nthu.edu.tw 
Abstract 
This paper describes a Web-based Eng-
lish-Chinese concordance system, Total-
Recall, developed to promote translation 
reuse and encourage authentic and idio-
matic use in second language writing. We 
exploited and structured existing high-
quality translations from the bilingual Si-
norama Magazine to build the concor-
dance of authentic text and translation. 
Novel approaches were taken to provide 
high-precision bilingual alignment on the 
sentence, phrase and word levels. A 
browser-based user interface (UI) is also 
developed for ease of access over the 
Internet. Users can search for word, 
phrase or expression in English or Chi-
nese. The Web-based user interface facili-
tates the recording of the user actions to 
provide data for further research. 
1 Introduction 
A concordance tool is particularly useful for study-
ing a piece of literature when thinking in terms of a 
particular word, phrase or theme. It will show ex-
actly how often and where a word occurs, so can 
be helpful in building up some idea of how differ-
ent themes recur within an article or a collection of 
articles. Concordances have been indispensable for 
lexicographers and increasingly considered useful 
for language instructor and learners. A bilingual 
concordance tool is like a monolingual concor-
dance, except that each sentence is followed by its 
translation counterpart in a second language. It 
could be extremely useful for bilingual lexicogra-
phers, human translators and second language 
learners. Pierre Isabelle, in 1993, pointed out: ?ex-
isting translations contain more solutions to more 
translation problems than any other existing re-
source.? It is particularly useful and convenient 
when the resource of existing translations is made 
available on the Internet. A web based bilingual 
system has proved to be very useful and popular. 
For example, the English-French concordance sys-
tem, TransSearch (Macklovitch et al 2000). Pro-
vides a familiar interface for the users who only 
need to type in the expression in question, a list of 
citations will come up and it is easy to scroll down 
until one finds one that is useful. TotalRecall 
comes with an additional feature making the solu-
tion more easily recognized. The user not only get 
all the citations related to the expression in ques-
tion, but also gets to see the translation counterpart 
highlighted. 
 
TotalRecall extends the translation memory 
technology and provide an interactive tool intended 
for translators and non-native speakers trying to 
find ideas to properly express themselves. Total-
Recall empower the user by allow her to take the 
initiative in submitting queries for searching au-
thentic, contemporary use of English. These que-
ries may be single words, phrases, expressions or 
even full sentence, the system will search a sub-
stantial and relevant corpus and return bilingual 
citations that are helpful to human translators and 
second language learners. 
2 Aligning the corpus 
Central to TotalRecall is a bilingual corpus and a 
set of programs that provide the bilingual analyses 
to yield a translation memory database out of the 
bilingual corpus. Currently, we are working with a 
collection of Chinese-English articles from the Si-
norama magazine. A large bilingual collection of 
Studio Classroom English lessons will be provided 
in the near future.  That would allow us to offer 
bilingual texts in both translation directions and 
with different levels of difficulty. Currently, the 
articles from Sinaroma seems to be quite usefully 
by its own, covering a wide range of topics, re-
flecting the personalities, places, and events in 
Taiwan for the past three decade.  
 
The concordance database is composed of bi-
lingual sentence pairs, which are mutual transla-
tion. In addition, there are also tables to record 
additional information, including the source of 
each sentence pairs, metadata, and the information 
on phrase and word level alignment. With that ad-
ditional information, TotalRecall provides various 
functions, including 1. viewing of the full text of 
the source with a simple click. 2. highlighted 
translation counterpart of the query word or phrase. 
3. ranking that is pedagogically useful for transla-
tion and language learning. 
 
We are currently running an experimental pro-
totype with Sinorama articles, dated mainly from 
1995 to 2002. There are approximately 50,000 bi-
lingual sentences and over 2 million words in total. 
We also plan to continuously updating the database 
with newer information from Sinorama magazine 
so that the concordance is kept current and relevant 
to the . To make these up to date and relevant.  
 
The bilingual texts that go into TotalRecall 
must be rearranged and structured. We describe the 
main steps below: 
2.1 Sentence Alignment 
After parsing each article from files and put them 
into the database, we need to segment articles into 
sentences and align them into pairs of mutual 
translation. While the length-based approach 
(Church and Gale 1991) to sentence alignment 
produces surprisingly good results for the close 
language pair of French and English at success 
rates well over 96%, it does not fair as well for 
distant language pairs such as English and Chinese. 
Work on sentence alignment of English and Chi-
nese texts (Wu 1994), indicates that the lengths of 
English and Chinese texts are not as highly corre-
lated as in French-English task, leading to lower 
success rate (85-94%) for length-based aligners.  
 
Table 1  The result of Chinese collocation candi-
dates extracted. The shaded collocation pairs are 
selected based on competition of whole phrase log 
likelihood ratio and word-based translation prob-
ability. Un-shaded items 7 and 8 are not selected 
because of conflict with previously chosen bilin-
gual collocations, items 2 and 3. 
 
Simard, Foster, and Isabelle (1992) pointed out 
cognates in two close languages such as English 
and French can be used to measure the likelihood 
of mutual translation. However, for the English-
Chinese pair, there  are no  orthographic,  phonetic 
or semantic cognates readily recognizable by the 
computer. Therefore, the cognate-based approach 
is not applicable to the Chinese-English tasks.  
 
At first, we used the length-based method for 
sentence alignment. The average precision of 
aligned sentence pairs is about 95%. We are now 
switching to a new alignment method based on 
punctuation statistics. Although the average ratio 
of the punctuation counts in a text is low (less than 
15%), punctuations provide valid additional evi-
dence, helping to achieve high degree of alignment 
precision. It turns out that punctuations are telling 
evidences for sentence alignment, if we do more 
than hard matching of punctuations and take into 
consideration of intrinsic sequencing of punctua-
tion in ordered comparison. Experiment results 
show that the punctuation-based approach outper-
forms the length-based approach with precision 
rates approaching 98%.  
2.2 Phrase and Word Alignment 
After sentences and their translation counterparts 
are identified, we proceeded to carry out finer-
grained alignment on the phrase and word levels. 
We employ  part of speech patterns  and  statistical  
Figure 1. The results of searching for ?hard+? with default ranking. 
 
analyses to extract bilingual phrases/collocations 
from a parallel corpus. The preferred syntactic pat-
terns are obtained from idioms and collocations in 
the machine readable English-Chinese version of 
Longman Dictionary of Contemporary of English. 
 
Phrases matching the patterns are extract from 
aligned sentences in a parallel corpus. Those 
phrases are subsequently matched up via cross lin-
guistic statistical association. Statistical association 
between the whole phrase as well as words in 
phrases are used jointly to link a collocation and its 
counterpart collocation in the other language. See 
Table 1 for an example of extracting bilingual col-
locations. The word and phrase level information is 
kept in relational database for use in processing 
queries, hightlighting translation counterparts, and 
ranking citations. Sections 3 and 4 will give more 
details about that. 
3 The Queries 
The goal of the TotalRecall System is to allow a 
user to look for instances of specific words or ex-
pressions. For this purpose, the system opens up 
two text boxes for the user to enter queries in any 
one of the languages involved or both. We offer 
some special expressions for users to specify the 
following queries: 
 
? Exact single word query - W. For instance, 
enter ?work? to find citations that contain 
?work,? but not ?worked?, ?working?, 
?works.? 
? Exact single lemma query ? W+. For in-
stance, enter ?work+? to find citations that 
contain ?work?, ?worked?, ?working?, 
?works.? 
? Exact string query. For instance, enter ?in 
the work? to find citations that contain the 
three words, ?in,? ?the,? ?work? in a row, 
but not citations that contain the three words 
in any other way. 
? Conjunctive and disjunctive query. For in-
stance, enter ?give+ advice+? to find cita-
tions that contain ?give? and ?advice.? It is 
also possible to specify the distance between 
?give? and ?advice,? so they are from a VO 
construction. Similarly, enter ?hard | diffi-
cult | tough? to find citations that involve 
difficulty to do, understand or bear some-
thing, using any of the three words. 
Once a query is submitted, TotalRecall dis-
plays the results on Web pages. Each result ap-
pears as a pair of segments, usually one sentence 
each in English and Chinese, in side-by-side for-
mat. The words matching the query are high-
lighted, and a ?context? hypertext link is included 
in each row. If this link is selected, a new page ap-
pears displaying the original document of the pair. 
If the user so wishes, she can scroll through the 
following or preceding pages of context in the 
original document. 
4 Ranking 
It is well known that the typical user usual has no 
patient to go beyond the first or second pages re-
turned by a search engine. Therefore, ranking and 
putting the most useful information in the first one 
or two is of paramount importance for search en-
gines. This is also true for a concordance.  
 
Experiments with a focus group indicate that 
the following ranking strategies are important: 
 
? Citations with a translation counterpart 
should be ranked first. 
? Citations with a frequent translation coun-
terpart appear before ones with less frequent 
translation 
? Citations with same translation counterpart 
should be shown in clusters by default. The 
cluster can be called out entirely on demand. 
? Ranking by nonlinguistic features should 
also be provided, including date, sentence 
length, query position in citations, etc. 
 
With various ranking options available, the users 
can choose one that is most convenient and 
productive for the work at hand. 
5 Conclusion 
In this paper, we describe a bilingual concordance 
designed as a computer assisted translation and 
language learning tool. Currently, TotalRecall 
uses Sinorama Magazine corpus as the translation 
memory and will be continuously updated as new 
issues of the magazine becomes available. We 
have already put a beta version on line and ex-
perimented with a focus group of second language 
learners. Novel features of TotalRecall include 
highlighting of query and corresponding transla-
tions, clustering and ranking of search results ac-
cording translation and frequency. 
 
TotalRecall enable the non-native speaker who 
is looking for a way to express an idea in English 
or Chinese. We are also adding on the basic func-
tions to include a log of user activities, which will 
record the users? query behavior and their back-
ground. We could then analyze the data and find 
useful information for future research. 
Acknowledgement  
We acknowledge the support for this study through 
grants from National Science Council and Ministry 
of Education, Taiwan (NSC 90-2411-H-007-033-
MC and MOE EX-91-E-FA06-4-4) and a special 
grant for preparing the Sinorama Corpus for distri-
bution by the Association for Computational Lin-
guistics and Chinese Language Processing. 
References 
Chuang, T.C. and J.S. Chang (2002), Adaptive Sentence 
Alignment Based on Length and Lexical Information, ACL 
2002, Companion Vol. P. 91-2. 
Gale, W. & K. W. Church, "A Program for Aligning Sen-
tences in Bilingual Corpora" Proceedings of the 29th An-
nual Meeting of the Association for Computational 
Linguistics, Berkeley, CA, 1991. 
Macklovitch, E., Simard, M., Langlais, P.: TransSearch: A 
Free Translation Memory on the World Wide Web. Proc. 
LREC 2000 III, 1201--1208 (2000). 
Nie, J.-Y., Simard, M., Isabelle, P. and Durand, R.(1999) 
Cross-Language Information Retrieval based on Parallel 
Texts and Automatic Mining of Parallel Texts in the Web. 
Proceedings of SIGIR ?99, Berkeley, CA. 
Simard, M., G. Foster & P. Isabelle (1992), Using cognates to 
align sentences in bilingual corpora. In Proceedings of 
TMI92, Montreal, Canada, pp. 67-81. 
Wu, Dekai (1994), Aligning a parallel English-Chinese corpus 
statistically with lexical criteria. In The Proceedings of the 
32nd Annual Meeting of the Association for Computational 
Linguistics, New Mexico, USA, pp. 80-87. 
Wu, J.C. and J.S. Chang (2003), Bilingual Collocation Extrac-
tion Based on Syntactic and Statistical Analyses, ms. 
Yeh, K.C., T.C. Chuang, J.S. Chang (2003), Using Punctua-
tions for Bilingual Sentence Alignment- Preparing Parallel 
Corpus for Distribution by the ACLCLP, ms. 
Subsentential Translation Memory for  
Computer Assisted Writing and Translation 
Jian-Cheng Wu 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, 
Taiwan, ROC 
D928322@oz.nthu.edu.tw 
Thomas C. Chuang 
Department of Computer Science 
Van Nung Institute of Technology
No. 1 Van-Nung Road 
Chung-Li Tao-Yuan, Taiwan, ROC
tomchuang@cc.vit.edu.tw 
Wen-Chi Shei , Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road, Hsinchu, 300, 
Taiwan, ROC 
jschang@cs.nthu.edu.tw 
 
Abstract 
This paper describes a database of translation 
memory, TotalRecall, developed to encourage 
authentic and idiomatic use in second 
language writing. TotalRecall is a bilingual 
concordancer that support search query in 
English or Chinese for relevant sentences and 
translations. Although initially intended for 
learners of English as Foreign Language (EFL) 
in Taiwan, it is a gold mine of texts in English 
or Mandarin Chinese. TotalRecall is 
particularly useful for those who write in or 
translate into a foreign language. We exploited 
and structured existing high-quality 
translations from bilingual corpora from a 
Taiwan-based Sinorama Magazine and 
Official Records of Hong Kong Legislative 
Council to build a bilingual concordance. 
Novel approaches were taken to provide high-
precision bilingual alignment on the 
subsentential and lexical levels. A browser-
based user interface was developed for ease of 
access over the Internet. Users can search for 
word, phrase or expression in English or 
Mandarin. The Web-based user interface 
facilitates the recording of the user actions to 
provide data for further research. 
1 Introduction 
Translation memory has been found to be more 
effective alternative to machine translation for 
translators, especially when working with batches 
of similar texts. That is particularly true with so-
called delta translation of the next versions for 
publications that need continuous revision such as 
an encyclopaedia or user?s manual. On another 
area of language study, researchers on English 
Language Teaching (ELT) have increasingly 
looked to concordancer of very large corpora as a 
new re-source for translation and language learning. 
Concordancers have been indispensable for 
lexicographers. But now language teachers and 
students also embrace the concordancer to foster 
data-driven, student-centered learning.  
A bilingual concordance, in a way, meets the 
needs of both communities, the computer assisted 
translation (CAT) and computer assisted language 
learning (CALL). A bilingual concordancer is like 
a monolingual concordance, except that each 
sentence is followed by its translation counterpart 
in a second language. ?Existing translations 
contain more solutions to more translation 
problems than any other existing resource.? 
(Isabelle 1993). The same can be argued for 
language learning; existing texts offer more 
answers for the learner than any teacher or 
reference work do.   
However, it is important to provide easy access 
for translators and learning writers alike to find the 
relevant and informative citations quickly. For in-
stance, the English-French concordance system, 
TransSearch provides a familiar interface for the 
users (Macklovitch et al 2000). The user type in 
the expression in question, a list of citations will 
come up and it is easy to scroll down until one 
finds translation that is useful much like using a 
search engine. TransSearch exploits sentence 
alignment techniques (Brown et al1990; Gale and 
Church 1990) to facilitate bilingual search at the 
granularity level of sentences.     
In this paper, we describe a bilingual 
concordancer which facilitate search and 
visualization with fine granularity. TotalRecall 
exploits subsentential and word alignment to 
provide a new kind of bilingual concordancer. 
Through the interactive interface and clustering of 
short subsentential bi-lingual citations, it helps 
translators and non-native speakers find ways to 
translate or express them-selves in a foreign 
language. 
2 Aligning the corpus 
Central to TotalRecall is a bilingual corpus and a 
set of programs that provide the bilingual analyses 
to yield a translation memory database out of the 
bilingual corpus. Currently, we are working with 
  
A: Database selection B: English query C: Chinese query D: Number of items per page 
E: Normal view F: Clustered summary according to translation G: Order by counts or lengths 
H: Submit bottom I: Help file J: Page index K: English citation L: Chinese citation M: Date and title 
N: All citations in the cluster O: Full text context P: Side-by-side sentence alignment 
Figure 2. The results of searching for ?hard? 
 
bilingual corpora from a Taiwan-based Sinorama 
Magazine and Official Records of Hong Kong 
Legislative Council. A large bilingual collection of 
Studio Classroom English lessons will be provided 
in the near future.  That would allow us to offer 
bilingual texts in both translation directions and 
with different levels of difficulty. Currently, the 
articles from Sinorama seems to be quite usefully 
by its own, covering a wide range of topics, 
reflecting the personalities, places, and events in 
Taiwan for the past three decades.  
The concordance database is composed of bi-
lingual sentence pairs, which are mutual translation. 
In addition, there are also tables to record 
additional information, including the source of 
each sentence pairs, metadata, and the information 
on phrase and word level alignment. With that 
additional information, TotalRecall provides 
various functions, including 1. viewing of the full 
text of the source with a simple click. 2. 
highlighted translation counterpart of the query 
word or phrase. 3. ranking that is pedagogically 
useful for translation and language learning. 
We are currently running an operational system 
with Sinorama Magazine articles and HK LEGCO 
records. These bilingual texts that go into 
TotalRecall must be rearranged and structured. We 
describe the main steps below: 
2.1 Subsentential alignment 
While the length-based approach (Church and 
Gale 1991) to sentence alignment produces very 
good results for close language pairs such as 
French and English at success rates well over 96%, 
it does not fair as well for disparate language pairs 
such as English and Mandarin Chinese. Also 
sentence alignment tends to produce pairs of a long 
Chinese sentence and several English sentences. 
Such pairs of mutual translation make it difficult 
for the user to read and grasp the answers 
embedded in the retrieved citations.  
We develop a new approach to aligning English 
and Mandarin texts at sub-sentential level in 
parallel corpora based on length and punctuation 
marks. 
The subsentential alignment starts with parsing 
each article from corpora and putting them into the 
database. Subsequently articles are segmented into 
subsentential segments. Finally, segments in the 
two languages which are mutual translation are 
aligned. 
Sentences and subsentenial phrases and clauses 
are broken up by various types of punctuation in 
the two languages. For fragments much shorter 
than sentences, the variances of length ratio are 
larger leading to unacceptably low precision rate 
for alignment. We combine length-based and 
punctuation-based approach to cope with the 
difficulties in subsentential alignment. 
Punctuations in one language translate more or less 
consistently into punctuations in the other language. 
Therefore the information is useful in 
compensating for the weakness of length-based 
approach. In addition, we seek to further improve 
the accuracy rates by employing cognates and 
lexical information. We experimented with an 
implementation of the pro-posed method on a very 
large Mandarin-English parallel corpus of records 
of Hong Kong Legislative Council with 
satisfactory results. Experiment results show that 
the punctuation-based approach outperforms the 
length-based approach with precision rates 
approaching 98%. 
 
 
Figure 1  The result of subsentential alignment 
and collocation alignment. 
 
2.2 Word and Collocation Alignment 
After sentences and their translation counterparts 
are identified, we proceeded to carry out finer-
grained alignment on the word level. We employed 
the Competitive Linking Algorithm (Melamed 
2000) produce high precision word alignment. We 
also extract English collocations and their transla-
tion equivalent based on the result of word align-
ment. These alignment results were subsequently 
used to cluster citations and highlight translation 
equivalents of the query. 
 
3 Aligning the corpus 
TotalRecall allows a user to look for instances of 
specific words or expressions and its translation 
counterpart. For this purpose, the system opens up 
two text boxes for the user to enter queries in any 
or both of the two languages involved. We offer 
some special expressions for users to specify the 
following queries: 
 
? Single or multi-word query ? spaces be-
tween words in a query are considered as ?and.?  
For disjunctive query, use ?||? to de-note ?or.? 
? Every word in the query will be expanded 
to all surface forms for search. That includes 
singular and plural forms, and various tense of the 
verbs.  
? TotalRecall automatically ignore high fre-
quency words in a stoplist such as ?the,? ?to,? and 
?of.? 
? It is also possible to ask for exact match by 
submitting query in quotes. Any word within the 
quotes will not be ignored. It is useful for 
searching named entities. 
Once a query is submitted, TotalRecall displays 
the results on Web pages. Each result appears as a 
pair of segments in English and Chinese, in side-
by-side format. A ?context? hypertext link is in-
cluded for each citation. If this link is selected, a 
new page appears displaying the original document 
of the pair. If the user so wishes, she can scroll 
through the following or preceding pages of con-
text in the original document. TotalRecall present 
the results in a way that makes it easy for the user 
to grasp the information returned to her: 
? When operating in the monolingual mode, 
TotalRecall presents the citation according to 
lengths. 
? When operating in the bilingual mode, To-
talRecall clusters the citations according to the 
translation counterparts and presents the user with 
a summary page of one example each for different 
translations. The query words and translation 
counterparts are high-lighted. 
4 Conclusion 
In this paper, we describe a bilingual 
concordance designed as a computer assisted 
translation and language learning tool. Currently, 
TotalRecll uses Sinorama Magazine and 
HKLEGCO corpora as the databases of translation 
memory. We have already put a beta version on 
line and experimented with a focus group of 
second language learners. Novel features of 
TotalRecall include highlighting of query and 
corresponding translations, clustering and ranking 
of search results according translation and 
frequency. 
TotalRecall enable the non-native speaker who is 
looking for a way to express an idea in English or 
Mandarin. We are also adding on the basic func-
tions to include a log of user activities, which will 
record the users? query behavior and their back-
ground. We could then analyze the data and find 
useful information for future research. 
Subsentential alignment results 
 
From 1983 to 1991, the average rate of wage growth for all trades 
and industries was only 1.6%.  
???????????????????? 1.6%? 
This was far lower than the growth in labour productivity, which 
averaged 5.3%. 
????????????? 5.3%??? 
But, it must also be noted that the average inflation rate was as 
high as 7.7% during the same period.  
???????????? 7.7%? 
As I have said before, even when the economy is booming, the 
workers are unable to share the fruit of economic success. 
??????????????????????????
??? 
Acknowledgement  
We acknowledge the support for this study 
through grants from National Science Council and 
Ministry of Education, Taiwan (NSC 91-2213-E-
007-061 and MOE EX-92-E-FA06-4-4) and a 
special grant for preparing the Sinorama Corpus 
for distri-bution by the Association for 
Computational Lin-guistics and Chinese Language 
Processing. 
References  
Brown P., Cocke J., Della Pietra S., Jelinek F., 
Lafferty J., Mercer R., & Roossin P. (1990). A 
statistical approach to machine translation. 
Computational Linguistics, vol. 16. 
Gale, W. & K. W. Church, "A Program for 
Aligning Sen-tences in Bilingual Corpora" 
Proceedings of the 29th An-nual Meeting of the 
Association for Computational Linguistics, 
Berkeley, CA, 1991. 
Isabelle, Pierre, M. Dymetman, G. Foster, J-M. 
Jutras, E. Macklovitch, F. Perrault, X. Ren and 
M. Simard. 1993. Translation Analysis and 
Translation Automation. In Pro-ceedings of the 
Fifth International Conference on Theoreti-cal 
and Methodological Issues in Machine 
Translation, Kyoto, Japan, pp. 12-20. 
I. Dan Melamed. 2000. Models of translational 
equivalence among words. Computational 
Linguistics, 26(2):221?249, June. 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 37?40, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Learning Source-Target Surface Patterns for  
Web-based Terminology Translation 
 
Jian-Cheng Wu 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
D928322@oz.nthu.edu.tw 
Tracy Lin 
Dep. of Communication Eng. 
National Chiao Tung University 
1001, Ta Hsueh Road,  
Hsinchu, 300, Taiwan 
tracylin@cm.nctu.edu.tw 
Jason S. Chang 
Department of Computer Science 
National Tsing Hua University 
101, Kuangfu Road,  
Hsinchu, 300, Taiwan 
jschang@cs.nthu.edu.tw 
 
Abstract 
This paper introduces a method for learn-
ing to find translation of a given source 
term on the Web. In the approach, the 
source term is used as a query and part of 
patterns to retrieve and extract transla-
tions in Web pages. The method involves 
using a bilingual term list to learn source-
target surface patterns. At runtime, the 
given term is submitted to a search engine 
then the candidate translations are ex-
tracted from the returned summaries and 
subsequently ranked based on the surface 
patterns, occurrence counts, and translit-
eration knowledge. We present a proto-
type called TermMine that applies the 
method to translate terms. Evaluation on a 
set of encyclopedia terms shows that the 
method significantly outperforms the 
state-of-the-art online machine translation 
systems. 
1 Introduction 
Translation of terms has long been recognized as 
the bottleneck of translation by translators. By re-
using prior translations a significant time spent in 
translating terms can be saved. For many years 
now, Computer-Aided Translation (CAT) tools 
have been touted as very useful for productivity 
and quality gains for translators. CAT tools such as 
Trados typically require up-front investment to 
populate multilingual terminology and translation 
memory. However, such investment has proven 
prohibitive for many in-house translation depart-
ments and freelancer translators and the actual 
productivity gains realized have been insignificant 
except for a few, very repetitive types of content. 
Much more productivity gain could be achieved by 
providing translation service of terminology. 
Consider the job of translating a textbook such 
as ?Artificial Intelligence ? A Modern Approach.? 
The best practice is probably to start by translating 
the indexes (Figure 1). It is not uncommon for 
these repetitive terms to be translated once and 
applied consistently throughout the book. For ex-
ample, A good translation F = "????" for the 
given term E = "acoustic model," might be avail-
able on the Web due to the common practice of 
including the source terms (often in brackets,  see 
Figure 2)  when   using  a   translated term (e.g. 
"???????????Acoustic Model? ?
????  ?"). The surface patterns of co-
occurring source and target terms (e.g., "F?E") 
can be learned by using the Web as corpus. Intui-
tively, we can submit E and F to a search engine  
 
Figure 1. Some index entries in ?Artificial intelli-
gence ? A Modern Approach? page 1045. 
academy award, 458 
accessible, 41 
accusative case, 806 
Acero, A., 580, 1010 
Acharya, A., 131, 994 
achieves, 389 
Ackley, D. H., 133, 987 
acoustic model, 568 
 
Figure 2. Examples of web page summaries with 
relevant translations returned by Google for some 
source terms in Figure 1. 
1. ... ???? Academy Awards. ???? Berlin International 
Film Festival. ... 
2. ... ?????????(inherent Case)???????
(accusative Case)???? ... 
3. ... ??????????(Alfred H. Ackley) ??????
??????????????.. 
4. ..?????? ??????????????????
?????????Acoustic Model??????... 
37
and then extract the strings beginning with F and 
ending with E (or vice versa) to obtain recurring 
source-target patterns. At runtime, we can submit 
E as query, request specifically for target-language 
web-pages. With these surface patterns, we can 
then extract translation candidates Fs from the 
summaries returned by the search engine. Addi-
tional information of occurrence counts and trans-
literation patterns can be taken into consideration 
to rank Fs. 
 
Table 1. Translations by the machine translation 
system Google Translate and TermMine. 
Terms Google Translate TermMine
academy award *???? ???? 
accusative case *???? ?? 
Ackley - ??? 
acoustic model *???? ???? 
 
For instance, among many candidate translations, 
we will pick the translations "????" for "acous-
tic model" and "???" for "Ackley, " because 
they fit certain surface-target surface patterns and 
appears most often in the relevant webpage sum-
maries. Furthermore, the first morpheme "?" in "
???" is consistent with prior transliterations of 
"A-" in "Ackley" (See Table 1). 
We present a prototype system called TermMine, 
that automatically extracts translation on the Web 
(Section 3.3) based on surface patterns of target 
translation and source term in Web pages auto-
matically learned on bilingual terms (Section 3.1). 
Furthermore, we also draw on our previous work 
on machine transliteration (Section 3.2) to provide 
additional evidence. We evaluate TermMine on a 
set of encyclopedia terms and compare the quality 
of translation of TermMine (Section 4) with a 
online translation system. The results seem to indi-
cate the method produce significantly better results 
than previous work. 
2 Related Work 
There is a resurgent of interested in data-intensive 
approach to machine translation, a research area 
started from 1950s. Most work in the large body of 
research on machine translation (Hutchins and 
Somers, 1992), involves production of sentence-
by-sentence translation for a given source text. In 
our work, we consider a more restricted case where 
the given text is a short phrase of terminology or 
proper names (e.g., ?acoustic model? or ?George 
Bush?).  
A number of systems aim to translate words and 
phrases out of the sentence context. For example, 
Knight and Graehl (1998) describe and evaluate a 
multi-stage method for performing backwards 
transliteration of Japanese names and technical 
terms into English by the machine using a genera-
tive model. In addition, Koehn and Knight (2003) 
show that it is reasonable to define noun phrase 
translation without context as an independent MT 
subtask and build a noun phrase translation subsys-
tem that improves statistical machine translation 
methods. 
Nagata, Saito, and Suzuki (2001) present a sys-
tem for finding English translations for a given 
Japanese technical term by searching for mixed 
Japanese-English texts on the Web. The method 
involves locating English phrases near the given 
Japanese term and scoring them based on occur-
rence counts and geometric probabilistic function 
of byte distance between the source and target 
terms. Kwok also implemented a term translation 
system for CLIR along the same line. 
Cao and Li (2002) propose a new method to 
translate base noun phrases. The method involves 
first using Web-based method by Nagata et al, and 
if no translations are found on the Web, backing 
off to a hybrid method based on dictionary and 
Web-based statistics on words and context vectors. 
They experimented with noun-noun NP report that 
910 out of 1,000 NPs can be translated with an av-
erage precision rate of 63%.  
In contrast to the previous research, we present a 
system that automatically learns surface patterns 
for finding translations of a given term on the Web 
without using a dictionary. We exploit the conven-
tion of including the source term with the transla-
tion in the form of recurring patterns to extract 
translations. Additional evident of data redundancy 
and transliteration patterns is utilized to validate 
translations found on the Web. 
3 The TermMine System 
In this section we describe a strategy for searching 
the Web pages containing translations of a given 
term (e.g., ?Bill Clinton? or ?aircraft carrier?) and 
extracting translations therein. The proposed 
method involves learning the surface pattern 
38
knowledge (Section 3.1) necessary for locating 
translations. A transliteration model automatically 
trained on a list of proper name and transliterations 
(Section 3.2) is also utilized to evaluate and select 
transliterations for proper-name terms. These 
knowledge sources are used in concert to search, 
rank, and extract translations (Section 3.3). 
3.1 Source and Target Surface patterns 
With a set of terms and translations, we can learn 
the co-occurring patterns of a source term E and its 
translation F following the procedure below: 
(1) Submit a conjunctive query (i.e. E AND F) for 
each pair (E, F) in a bilingual term list to a 
search engine. 
(2) Tokenize the retrieved summaries into three 
types of tokens: I. A punctuation II. A source 
word, designated with the letter "w" III. A 
maximal block of target words (or characters in 
the case of language without word delimiters 
such as Mandarin or Japanese). 
(3) Replace the tokens for E?s instances with the 
symbol ?E? and the type-III token containing 
the translation F with the symbol ?F?. Note the 
token denoted as ?F? is a maximal string cover-
ing the given translation but containing no 
punctuations or words in the source language. 
(4) Calculate the distance between E and F by 
counting the number of tokens in between.  
(5) Extract the strings of tokens from E to F (or the 
other way around) within a maximum distance 
of d (d is set to 3) to produce ranked surface 
patterns P. 
 
For instance, with the source-target pair ("Califor-
nia," "??") and a retrieved summary of "...??
??. ??? Northern California. ...," the surface 
pattern "FwE" of distance 1 will be derived. 
3.2 Transliteration Model  
TermMine also relies on a machine transliteration 
model (Lin, Wu and Chang 2004) to confirm the 
transliteration of proper names. We use a list of 
names and transliterations to estimate the translit-
eration probability function P(? | ?), for any given 
transliteration unit (TU) ? and transliteration char-
acter (TC) ?. Based on the Expectation Maximiza-
tion (EM) algorithm. A TU for an English name 
can be a syllable or consonants which corresponds 
to a character in the target transliteration. Table 2 
shows some examples of sub-lexical alignment 
between proper names and transliterations. 
Table 2. Examples of aligned transliteration units. 
Name transliteration Viterbi alignment 
Spagna ???? s-? pag-? n-? a-?
Kohn ?? Koh-? n-? 
Nayyar ?? Nay-? yar-? 
Rivard ??? ri-? var-? d-? 
Hall ?? ha-? ll-? 
Kalam ?? ka-? lam-? 
Figure 3. Transliteration probability trained on 
1,800 bilingual names (? denotes an empty string). 
? ? P(?|?) ? ? P(?|?) ? ? P(?|?)
? .458 b ? .700 ye ? .667 
? .271  ? .133  ? .333 
? .059  ? .033 z ? .476 
a 
? .051  ? .033  ? .286 
? .923 an ? .923  ? .095 an 
? .077  ? .077  ? .048 
3.3 Finding and locating translations 
At runtime, TermMine follows the following steps 
to translate a given term E: 
(1) Webpage retrieval. The term E is submitted to 
a Web search engine with the language option 
set to the target language to obtain a set of 
summaries.  
(2) Matching patterns against summaries. The 
surface patterns P learned in the training phase 
are applied to match E in the tokenized summa-
ries, to extract a token that matches the F sym-
bol in the pattern. 
(3) Generating candidates. We take the distinct 
substrings C of all matched Fs as the candidates. 
(4) Ranking candidates. We evaluate and select 
translation candidates by using both data re-
dundancy and the transliteration model. Candi-
dates with a count or transliteration probability 
lower than empirically determined thresholds 
are discarded. 
I. Data redundancy. We rank translation candi-
dates by numbers of instances it appeared in the 
retrieved summaries. 
II. Transliteration Model. For upper-case E, we 
assume E is a proper name and evaluate each 
candidate translation C by the likelihood of C as 
the transliteration of E using the transliteration 
model described in (Lin, Wu and Chang 2004).  
39
Figure 4. The distribution of distances between 
source and target terms in Web pages. 
0
1000
2000
3000
4000
5000
Distance
Co
un
t
Count 63 111 369 2182 4961 2252 718 91 34
-4 -3 -2 -1 0 1 2 3 4
Figure 5. The distribution of distances between 
source and target terms in Web pages. 
Pattern Count Acc. Percent Example distance
FE 3036 28.1% ???? ATLAS 0 
EF 1925 45.9% Elton John???? 0 
E(F 1485 59.7% Austria(??? -1 
F?E 1251 71.2% ?????Atlas 1 
F(E 361 74.6% ????(Atlas 1 
F.E 203 76.5% Peter Pan. ??? 1 
EwF 197 78.3% ?? Northern California -1 
E,F 153 79.7% Mexico, ??? -1 
F??E 137 81.0% ??????Titanic 2 
F??E 119 82.1% ??????Atlas 2 
 
 
(5) Expanding the tentative translation. Based 
on a heuristics proposed by Smadja (1991) to 
expand bigrams to full collocations, we extend 
the top-ranking candidate with count n on both 
sides, while keeping the count greater than n/2 
(empirically determined). Note that the con-
stant n is set to 10 in the experiment described 
in Section 4. 
(6) Final ranking. Rank the expanded versions of 
candidates by occurrence count and output the 
ranked list. 
4 Experimental results 
We took the answers of the first 215 questions on a 
quiz Website (www.quiz-zone.co.uk) and hand-
translations as the training data to obtain a of sur-
face patterns. For all but 17 source terms, we are 
able to find at least 3 instances of co-occurring of 
source term and translation. Figure 4 shows distri-
bution of the distances between co-occurring 
source and target terms. The distances tend to con-
centrate between - 3 and + 3 (10,680 out of 12,398 
instances, or 86%). The 212 surface patterns ob-
tained from these 10,860 instances, have a very 
skew distribution with the ten most frequent sur-
face patterns accounting for 82% of the cases (see 
Figure 5). In addition to source-target surface pat-
terns, we also trained a transliteration model (see 
Figure 3) on 1,800 bilingual proper names appear-
ing in Taiwanese editions of Scientific American 
magazine.  
Test results on a set of 300 randomly selected 
proper names and technical terms from Encyclope-
dia Britannica indicate that TermMine produces 
300 top-ranking answers, of which 263 is the exact 
translations (86%) and 293 contain the answer key 
(98%). In comparison, the online machine transla-
tion service, Google translate produces only 156 
translations in full, with 103 (34%) matching the 
answer key exactly, and 145 (48%) containing the 
answer key. 
5 Conclusion 
We present a novel Web-based, data-intensive ap-
proach to terminology translation from English to 
Mandarin Chinese. Experimental results and con-
trastive evaluation indicate significant improve-
ment over previous work and a state-of-sate 
commercial MT system.  
References 
Y. Cao and H. Li. (2002). Base Noun Phrase Translation Us-
ing Web Data and the EM Algorithm, In Proc. of COLING 
2002, pp.127-133. 
W. Hutchins and H. Somers. (1992). An Introduction to Ma-
chine Translation. Academic Press. 
K. Knight, J. Graehl. (1998). Machine Transliteration. In 
Journal of Computational Linguistics 24(4), pp.599-612. 
P. Koehn, K. Knight. (2003). Feature-Rich Statistical Transla-
tion of Noun Phrases. In Proc. of ACL 2003, pp.311-318. 
K. L. Kwok, The Chinet system. (2004). (personal 
communication). 
T. Lin, J.C. Wu, J. S. Chang. (2004). Extraction of Name and 
Transliteration in Monolingual and Parallel Corpora. In 
Proc. of AMTA 2004, pp.177-186. 
M. Nagata, T. Saito, and K. Suzuki. (2001). Using the Web as 
a bilingual dictionary. In Proc. of ACL 2001 DD-MT 
Workshop, pp.95-102. 
F. A. Smadja. (1991). From N-Grams to Collocations: An 
Evaluation of Xtract. In Proc. of ACL 1991,  pp.279-284. 
40
Proceedings of the ACL 2010 Conference Short Papers, pages 115?119,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Collocation Suggestion in Academic Writing 
 Jian-Cheng Wu1 Yu-Chia Chang1,* Teruko Mitamura2 Jason S. Chang1 1 National Tsing Hua University Hsinchu, Taiwan {wujc86, richtrf, jason.jschang} @gmail.com 
2 Carnegie Mellon University  Pittsburgh, United States            teruko@cs.cmu.edu 
 Abstract In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speak-ers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classifica-tion. The system generates and ranks suggestions to assist learners? collocation usages in their academic writing with sat-isfactory results. * 1 Introduction The notion of collocation has been widely dis-cussed in the field of language teaching for dec-ades. It has been shown that collocation, a suc-cessive common usage of words in a chain, is important in helping language learners achieve native-like fluency. In the field of English for Academic Purpose, more and more researchers are also recognizing this important feature in academic writing. It is often argued that colloca-tion can influence the effectiveness of a piece of writing and the lack of such knowledge might cause cumulative loss of precision (Howarth, 1998). Many researchers have discussed the function of collocations in the highly conventionalized and specialized writing used within academia. Research also identified noticeable increases in the quantity and quality of collocational usage by                                                            * Corresponding author: Yu-chia Chang (Email address: richtrf@gmail.com) 
native speakers (Howarth, 1998). Granger (1998) reported that learners underuse native-like collo-cations and overuse atypical word combinations. This disparity in collocation usage between na-tive and non-native speakers is clear and should receive more attention from the language tech-nology community. To tackle such word usage problems, tradi-tional language technology often employs a da-tabase of the learners' common errors that are manually tagged by teachers or specialists (e.g. Shei and Pain, 2000; Liu, 2002). Such system then identifies errors via string or pattern match-ing and offer only pre-stored suggestions. Com-piling the database is time-consuming and not easily maintainable, and the usefulness is limited by the manual collection of pre-stored sugges-tions. Therefore, it is beneficial if a system can mainly use untagged data from a corpus contain-ing correct language usages rather than the error-tagged data from a learner corpus. A large corpus of correct language usages is more readily avail-able and useful than a small labeled corpus of incorrect language usages. For this suggestion task, the large corpus not only provides us with a rich set of common col-locations but also provides the context within which these collocations appear. Intuitively, we can take account of such context of collocation to generate more suitable suggestions. Contextual information in this sense often entails more lin-guistic clues to provide suggestions within sen-tences or paragraph. However, the contextual information is messy and complex and thus has long been overlooked or ignored. To date, most fashionable suggestion methods still rely upon the linguistic components within collocations as well as the linguistic relationship between mis-used words and their correct counterparts (Chang et al, 2008; Liu, 2009).  In contrast to other research, we employ con-textual information to automate suggestions for verb-noun lexical collocation. Verb-noun collo-cations are recognized as presenting the most 
115
challenge to students (Howarth, 1996; Liu, 2002). More specifically, in this preliminary study we start by focusing on the word choice of verbs in collocations which are considered as the most difficult ones for learners to master (Liu, 2002; Chang, 2008). The experiment confirms that our collocation writing assistant proves the feasibility of using machine learning methods to automatically prompt learners with collocation suggestions in academic writing.  2 Collocation Checking and Suggestion This study aims to develop a web service, Collo-cation Inspector (shown in Figure 1) that accepts sentences as input and generates the related can-didates for learners. In this paper, we focus on automatically pro-viding academic collocation suggestions when users are writing up their abstracts. After an ab-stract is submitted, the system extracts linguistic features from the user?s text for machine learning model. By using a corpus of published academic texts, we hope to match contextual linguistic clues from users? text to help elicit the most rele-vant suggestions. We now formally state the problem that we are addressing: Problem Statement: Given a sentence S writ-ten by a learner and a reference corpus RC, our goal is to output a set of most probable sugges-tion candidates c1, c2, ... , cm. For this, we train a classifier MC to map the context (represented as feature set f1, f2, ..., fn) of each sentence in RC to the collocations. At run-time, we predict these collocations for S as suggestions. 2.1 Academic Collocation Checker Train-ing Procedures Sentence Parsing and Collocation Extraction: We start by collecting a large number of ab-stracts from the Web to develop a reference cor-pus for collocation suggestion. And we continue to identify collocations in each sentence for the subsequent processing. Collocation extraction is an essential step in preprocessing data. We only expect to extract the collocation which comprises components having a syntactic relationship with one another. How-ever, this extraction task can be complicated. Take the following scholarly sentence from the reference corpus as an example (example (1)): 
(1) We introduce a novel method for learning to find documents on the web. 
 Figure 1. The interface for the collocation suggestion  
nsubj (introduce-2, We-1) det (method-5, a-3) amod (method-5, novel-4) dobj (introduce-2, method-5) prepc_for (introduce-2, learning-7) aux (find-9, to-8) ? ? Figure 2. Dependency parsing of Example (1)  Traditionally, through part-of-speech tagging, we can obtain a tagged sentence as follows (ex-ample (2)). We can observe that the desired col-location ?introduce method?, conforming to ?VERB+NOUN? relationship, exists within the sentence. However, the distance between these two words is often flexible, not necessarily rigid. Heuristically writing patterns to extract such verb and noun might not be effective. The patterns between them can be tremendously varied. In addition, some verbs and nouns are adjacent, but they might be intervened by clause and thus have no syntactic relation with one another (e.g. ?pro-pose model? in example (3)). 
(2) We/PRP  introduce/VB  a/DT  novel/JJ  method/NN  for/IN  learning/VBG  to/TO  find/VB  documents/NNS  on/IN  the/DT  web/NN  ./.  
(3) We proposed that the web-based model would be more ef-fective than corpus-based one. A natural language parser can facilitate the ex-traction of the target type of collocations. Such parser is a program that works out the grammati-cal structure of sentences, for instance, by identi-fying which group of words go together or which 
116
word is the subject or object of a verb. In our study, we take advantage of a dependency parser, Stanford Parser, which extracts typed dependen-cies for certain grammatical relations (shown in Figure 2). Within the parsed sentence of example (1), we can notice that the extracted dependency ?dobj (introduce-2, method-4)? meets the crite-rion.  Using a Classifier for the Suggestion task: A classifier is a function generally to take a set of attributes as an input and to provide a tagged class as an output. The basic way to build a clas-sifier is to derive a regression formula from a set of tagged examples. And this trained classifier can thus make predication and assign a tag to any input data. The suggestion task in this study will be seen as a classification problem. We treat the colloca-tion extracted from each sentence as the class tag (see examples in Table 1). Hopefully, the system can learn the rules between tagged classes (i.e. collocations) and example sentences (i.e. schol-arly sentences) and can predict which collocation is the most appropriate one given attributes ex-tracted from the sentences. Another advantage of using a classifier to automate suggestion is to provide alternatives with regard to the similar attributes shared by sentences. In Table 1, we can observe that these collocations exhibit a similar discourse function and can thus become interchangeable in these sentences. Therefore, based on the outputs along with the probability from the classifier, we can provide more than one adequate suggestions.  Feature Selection for Machine Learning: In the final stage of training, we build a statistical machine-learning model. For our task, we can use a supervised method to automatically learn the relationship between collocations and exam-ple sentences. We choose Maximum Entropy (ME) as our train-ing algorithm to build a collocation suggestion classifier. One advantage of an ME classifier is that in addition to assigning a classification it can provide the probability of each assignment. The ME framework estimates probabilities based on the principle of making as few assumptions as possible. Such constraints are derived from the training data, expressing relationships between features and outcomes.  Moreover, an effective feature selection can increase the precision of machine learning. In our study, we employ the contextual features which  
Table 1. Example sentences and class tags (colloca-tions) Example Sentence  Class tag   We introduce a novel method for learning to find documents on the web.  introduce  We presented a method of improving Japa-nese dependency parsing by using large-scale statistical information.  present  In this paper, we will describe a method of identifying the syntactic role of antece-dents, which consists of two phases  describe  In this paper, we suggest a method that automatically constructs an NE tagged cor-pus from the web to be used for learning of NER systems.  suggest   consist of two elements, the head and the ngram of context words:  Head: Each collocation comprises two parts, collocate and head. For example, in a given verb-noun collocation, the verb is the collocate as well as the target for which we provide suggestions; the noun serves as the head of collocation and convey the essential meaning of the collocation. We use the head as a feature to condition the classifier to generate candidates relevant to a given head.  Ngram: We use the context words around the target collocation by considering the correspond-ing unigrams and bigrams words within the sen-tence. Moreover, to ensure the relevance, those context words, before and after the punctuation marks enclosing the collocation in question, will be excluded. We use the parsed sentence from previous step (example (2)) to show the extracted context features1 (example (4)): 
(4) CN=method UniV_L=we UniV_R=a UniV_R=novel UniN_L=a UniN_L=novel UniN_R=for UniN_R=learn BiV_R=a_novel BiN_L=a_novel BiN_R=for_learn BiV_I=we_a BiN_I=novel_for  
                                                           1 CN refers to the head within collocation. Uni and Bi indi-cate the unigram and bigram context words of window size two respectively. V and N differentiate the contexts related to verb or noun. The ending alphabets L, R, I show the posi-tion of the words in context, L = left, R = right, and I = in between. 
117
2.2 Automatic Collocation Suggestion at Run-time After the ME classifier is automatically trained, the model is used to find out the best collocation suggestion. Figure 3 shows the algorithm of pro-ducing suggestions for a given sentence. The input is a learner?s sentence in an abstract, along with an ME model trained from the reference corpus.  In Step (1) of the algorithm, we parse the sen-tence for data preprocessing. Based on the parser output, we extract the collocation from a given sentence as well as generate features sets in Step (2) and (3). After that in Step (4), with the trained machine-learning model, we obtain a set of likely collocates with probability as predicted by the ME model. In Step (5), SuggestionFilter singles out the valid collocation and returns the best collocation suggestion as output in Step (6). For example, if a learner inputs the sentence like Example (5), the features and output candidates are shown in Table 2. 
(5) There are many investiga-tions about wireless network communication, especially it is important to add Internet transfer calculation speeds.  3 Experiment From an online research database, CiteSeer, we have collected a corpus of 20,306 unique ab-stracts, which contained 95,650 sentences. To train a Maximum Entropy classifier, 46,255 col-locations are extracted and 790 verbal collocates are identified as tagged classes for collocation suggestions. We tested the classifier on scholarly sentences in place of authentic student writings which were not available at the time of this pilot study. We extracted 364 collocations among 600 randomly selected sentences as the held out test data not overlapping with the training set. To automate the evaluation, we blank out the verb collocates within these sentences and treat these verbs directly as the only correct suggestions in question, although two or more suggestions may be interchangeable or at least appropriate. In this sense, our evaluation is an underestimate of the performance of the proposed method.    While evaluating the quality of the suggestions provided by our system, we used the mean recip-rocal rank (MRR) of the first relevant sugges-tions returned so as to assess whether the sugges-tion list contains an answer and how far up the answer is in the list as a quality metric of the sys-  
Procedure CollocationSuggestion(sent, MEmodel)   (1)   parsedSen = Parsing(sent) (2)   extractedColl = CollocationExtraction(parsedSent) (3)   features = AssignFeature(ParsedSent)   (4)   probCollection = MEprob(features, MEmodel)    (5)   candidate = SuggestionFilter(probCollection) (6)   Return candidate  Figure 3. Collocation Suggestion at Run-time  Table 2. An example from learner?s sentence Extracted Collocation Features Ranked Candidates 
add speed 
CN=speed UniV_L=important UniV_L=to UniV_R=internet UniV_R=transfer UniN_L=transfer UniN_L=calculation BiV_L=important_to BiV_R=internet_transfer BiN_L=transfer_calcula-tion BiV_I=to_intenet 
improve increase determine maintain ? ? 
 Table 3. MRR for different feature sets Feature Sets Included In Classifier MRR  Features of HEAD 0.407 Features of CONTEXT 0.469 Features of HEAD+CONTEXT 0.518  tem output. Table 3 shows that the best MRR of our prototype system is 0.518. The results indi-cate that on average users could easily find an-swers (exactly reproduction of the blanked out collocates) in the first two to three ranking of suggestions. It is very likely that we get a much higher MMR value if we would go through the lists and evaluate each suggestion by hand. Moreover, in Table 3, we can further notice that contextual features are quite informative in com-parison with the baseline feature set containing merely the feature of HEAD. Also the integrated feature set of HEAD and CONTEXT together achieves a more satisfactory suggestion result. 4 Conclusion Many avenues exist for future research that are important for improving the proposed method. For example, we need to carry out the experi-ment on authentic learners? texts. We will con-duct a user study to investigate whether our sys-tem would improve a learner?s writing in a real setting. Additionally, adding classifier features based on the translation of misused words in learners? text could be beneficial  (Chang et al, 
118
2008). The translation can help to resolve preva-lent collocation misuses influenced by a learner's native language. Yet another direction of this research is to investigate if our methodology is applicable to other types of collocations, such as AN and PN in addition to VN dealt with in this paper. In summary, we have presented an unsuper-vised method for suggesting collocations based on a corpus of abstracts collected from the Web. The method involves selecting features from the reference corpus of the scholarly texts. Then a classifier is automatically trained to determine the most probable collocation candidates with regard to the given context. The preliminary re-sults show that it is beneficial to use classifiers for identifying and ranking collocation sugges-tions based on the context features.  Reference Y. Chang, J. Chang, H. Chen, and H. Liou. 2008. An automatic collocation writing assistant for Taiwan-ese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language Learn-ing, 21(3), pages 283-299. S. Granger. 1998. Prefabricated patterns in advanced EFL writing: collocations and formulae. In Cowie, A. (ed.) Phraseology: theory, analysis and applica-tions. Oxford University Press, Oxford, pages 145-160. P. Howarth. 1996. Phraseology in English Academic Writing. T?bingen: Max Niemeyer Verlag. P. Howarth. 1998. The phraseology of learner?s aca-demic writing. In Cowie, A. (ed.) Phraseology: theory, analysis and applications. Oxford Univer-sity Press, Oxford, pages 161-186. D. Hawking and N. Craswell. 2002. Overview of the TREC-2001 Web track. In Proceedings of the 10th Text Retrieval Conference (TREC 2001), pages 25-31. L. E. Liu. 2002. A corpus-based lexical semantic in-vestigation of verb-noun miscollocations in Taiwan learners? English. Unpublished master?s thesis, Tamkang University, Taipei, January. A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated suggestions for miscollocations. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 47-50. C. C. Shei and H. Pain. 2000. An ESL writer?s collo-cational aid. Computer Assisted Language Learn-ing, 13, pages 167-182. 
119
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 139?144,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linggle: a Web-scale Linguistic Search Engine for Words in Context    Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract 
In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and ad-ditional regular expression (RE) operators. In our approach, we incorporate inverted file in-dexing, PoS information from BNC, and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T. The method in-volves parsing the query to transforming it in-to several keyword retrieval commands. Word chunks are retrieved with counts, further filter-ing the chunks with the query as a RE, and fi-nally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided.  In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehen-sive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major lan-guages in the world. 1 Introduction As a non-native speaker writing in English, one encounters many problems. Doubts concerning the usage of a preposition, the mandatory presen-ce of a determiner, the correctness of the associa-tion of a verb with an object, or the need for syn-onyms of a term in a given context are issues that arise frequently. Printed collocation dictionaries and reference tools based on compiled corpora offer limited coverage of word usage while knowledge of collocations is vital to acquire a 
good level of linguistic competency. We propose to address these limitations with a comprehen-sive system aimed at helping the learners ?know a word by the company it keeps? (Firth, 1957). Linggle (linggle.com). The system based on Web-scaled datasets is designed to be a broad coverage language reference tool for English Second Language learners (ESL). It is conceived to search information related to word usage in context under various conditions. First, we build an inverted file index for the Google Web 1T n-grams to support queries with RE-like patterns including PoS and synonym matches. For example, for the query ?$V $D +important role?, Linggle retrieves 4-grams that start with a verb and a determiner followed by a synonym of important and the keyword role (e.g., play a significant role 202,800). A natural lan-guage interface is also available for users who are less familiar with pattern-based searches. For example, the question ?How can I describe a beach?? would retrieve two word chunks such as ?sandy beach 413,300? and ?rocky beach 16,800?. The n-gram search implementation is achieved through filtering, re-indexing, populat-ing an HBase database with the Web 1T n-grams and augmenting them with the most frequent PoS for words (without disambiguation) derived from the British National Corpus (BNC).   The n-grams returned for a query can then be linked to examples extracted from the New York Times Corpus (Sandhaus, 2008) in order to provide full sentential context for more effective learning.  In some situations, the user might need to search for words in a specific syntactic relation (e.g., Verb-Object collocation). The query absorb $N in n-grams display mode returns all the nouns that follow the verb ordered by decreasing n-gram counts. Some of these nouns might not be objects of the verb absorb. In contrast, the same 
139
query in cluster display mode will control that two words have been labeled verb-object by a parser. Moreover, n-grams grouped by object topic/domain give the learner an overview of the usage of the verb. For example the verb absorb takes clusters of objects related to the topics liq-uid, energy, money, knowledge, and population.  
  Figure 1. An example Linggle search for the que-ry ?absorb $N.?  This tendency of predicates to prefer certain classes of arguments is defined by Wilks (1978) as selectional preferences and widely reported in the literature. Erk and Pad? (2010) extend exper-iments on selectional preference induction to in-verse selectional preference, considering the re-striction imposed on predicates. Inverse sectional preference is also implemented in linggle (e.g. ?$V apple?). Linggle presents clusters of synonymous col-locates (adjectives, nouns and verbs) of a query keyword. We obtained the clusters by building on Lin and Pantel?s (2002) large-scale repository of dependencies and word similarity scores. Us-ing the method proposed by Ritter and Etzioni (2010) we induce selectional preference with a Latent Dirichlet Allocation (LDA) model to seed the clusters. The rest of the paper is organized as follows. We review the related work in the next section. Then we present the syntax of the queries and the functionalities of the system (Section 3). We de-scribe the details of implementation including the indexing of the n-grams and the clustering algo-rithm (Section 4) and draw perspective of devel-opment of Web scale search engines (Section 5). 2 Related work Web-scale Linguistic Search Engine (LSE) has been an area of active research. Recently, the state-of-the-art in LSE research has been re-
viewed in Fletcher (2012). We present in this paper a linguistic search engine that provides a more comprehensive and powerful set of query features.  Kilgarriff et al (2001) describe the implemen-tation of the linguistic search engine Word Sketch (2001) that displays collocations and de-pendencies acquired from a large corpus such as the BNC. Word Sketch is not as flexible as typi-cal search engines, only supporting a fixed set of queries.  Recently, researchers have been attempting to go one step further and work with Web scale da-tasets, but it is difficult for an academic institute to crawl a dataset that is on par with the datasets built by search engine companies. In 2006, Google released the Web 1T for several major languages of the world (trillion-word n-gram da-tasets for English, Japanese, Chinese, and ten European languages), to stimulate NLP research in many areas.  In 2008, Chang described a pro-totype that enhances Google Web 1T bigrams with PoS tags and supports search in the dataset by wildcards (wild-PoS), to identify recurring collocations. Wu, Witten and Franken (2010) describe a more comprehensive system (FLAX) that combines filtered Google data with text ex-amples from the BNC for several learning activi-ties.  In a way similar to Chang (2008) and Wu, Witten and Franken (2010), Stein, Potthast, and Trenkmann (2010) describe the implementation and application of NetSpeak, a system that pro-vides quick access to the Google Web 1T n-gram with RE-like queries (alternator ?|?, one arbitrary word ?*?, arbitrary number of words between two specified words ???). In contrast to Linggle, NetSpeak does not support PoS wildcard or con-ceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distribu-tional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies ex-tracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to or-ganize the n-grams into semantic classes.  More recently, Ritter and Etzioni (2010) pro-pose to apply an LDA model (Blei et al 2003) to 
140
the problem of inducing selectional preference. The idea is to consider the verbs in a corpus as the documents of a traditional LDA model. The arguments of the verb that are encountered in the corpus are treated as the words composing a document in the traditional model. The model seems to successfully infer the semantic classes that correspond to the preferred arguments of a verb. The topics are semi-automatically labeled with WordNet classes to produce a repository of human interpretable class-based selectional pref-erence. This choice might be due to the fact that if most LDA topic heads are usually reasonable upon human inspection, some topics are also in-coherent (Newman 2010) and lower frequency words are not handled as successfully. We con-trol the coherence of the topics and rearrange them into human interpretable clusters using a distributional similarity measure.  Microsoft Sempute Project (Sempute Team 2013) also explores core technologies and appli-cations of semantic computing. As part of Sempute project, NeedleSeek is aimed at auto-matically extracting data to support general se-mantic Web searches. While Linggle focuses on n-gram information for language learning, NeedleSeek also uses LDA to support question answering (e.g., What were the Capitals of an-cient China?) . In contrast to the previous research in Web scale linguistic search engines, we present a sys-tem that supports queries with keywords, wild-card words, POS, synonyms, and additional regular expression (RE) operators and displays the results according the count, similarity, and topic with clusters of synonyms or conceptually related words. We exploit and combine the power of both LDA analysis and distributional similarity to provide meaningful semantic classes that are constrained with members of high simi-larity. Distributional similarity (Lin 1998) and LDA topics become two angles of attack to view language usage and corpus patterns. 3 Linggle Functionalities The syntax of Linggle queries involves basic regular expression of keywords enriched with wildcard PoS and synonyms. Linggle queries can be either pattern-based commands or natural lan-guage questions. The natural language queries are currently handled by simple string matching based on a limited set of questions and command pairs provided by a native speaker informant.  
3.1 Natural language queries The handling of queries formulated in natural language has been implemented with handcrafted patterns refined from a corpus of questions found on various websites. Additionally, we asked both native and non-native speakers to use the system for text edition and to write down all the ques-tions that arise during the exercise.  Linggle transforms a question into commands for further processing based on a set of canned texts (e.g., ?How to describe a beach?? will be converted to ?$A beach?). We are in the process of gathering more examples of language-related question and answer pairs from Answers.com to improve the precision, versatility, and coverage. 3.2 Syntax of queries The syntax of the patterns for n-grams is shown in Table 1. The syntax supports two types of que-ry functions: basic keyword search with regular expression capability and semantic search.  Basic search operators enable the users to que-ry zero, one or more arbitrary words up to five words. For example, the query ?set off ? $N? is intended to search for all nouns in the right con-text of set off, within a maximum distance of three words.  In addition, the ??? operator in front of a word represents a search for n-grams with or without the word. For example, a user wanting to deter-mine whether to use the word to between listen and music can formulate the query ?listen ?to music.? Yet another operation ?|? is provided to search for information related to word choice. For ex-ample the query ?build | construct ... dream? can be used to reveal that people build a dream much more often than they construct a dream. A set of PoS symbols (shown in Table 2) is defined to support queries that need more preci-sion than the symbol *. More work might be needed to resolve PoS ambiguity for n-grams. Currently, any word that has been labeled with the requested PoS in the BNC more than 5% of the time is displayed.  The ?+? operator is provided to support se-mantic queries. Placed in front of a word, it is intended to search for synonyms in the context. For example the query ?+sandy beach? would generate rocky beach, stony beach, barren beach in the top three results. The query ?+abandoned beach? generates deserted, destroyed and empty beach at the top of the list. To support conceptual clustering of collocational n-grams, we need to 
141
identify synonyms related to different senses of a given word. Table 3 shows an example of the result obtained for the ambiguous word bank as a unigram query. We can see the two main senses of the word (river bank and institution) as clus-ters.  Operators  Description * Any Word ? With/without the word ? Zero or more words | Alternator $ Part of speech + Synonyms Table 1: Operators in the Linggle queries   Part of speech  Description N Noun V Verb A Adjective R Adverb PP Preposition NP Proper Noun PR Pronoun D Determiner Table 2: Part-of-speech in the Linggle queries   A cluster button on the interface activates or cancels conceptual clustering. When Linggle is switched into a cluster display mode, adjective-nouns, verb-objects and subject-verb relations can be browsed based on the induced conceptual clusters (see Figure 1). The New York Times Example Base In order to display complete sentence examples for users, the New York Times Corpus sentences are indexed by word. When the user searches for words in a specific syntactic relation, morpho-logical query expansion is performed and pat-terns are used to increase both the coverage and the precision of the provided examples. For ex-ample, the bi-gram kill bacteria will be associat-ed with the example sentence ?The bacteria are killed by high temperatures.?. 3.3 Semantic Clusters Two types of semantic clusters are provided in Linggle: selectional preference and clusters of synonyms. Selectional preference expresses for example that an apple is more likely to be eaten or cooked than to be killed or hanged. Different classes of arguments for a predicate (or of predi-cates for an argument) can be found automatical-ly. The favorite class of objects for the verb drink 
is LIQUID with the noun water ranked at the top. Less frequent objects belonging to the same class include liquor in the tail of the list. We aim at grouping arguments and predicates into semantic clusters for better readability.  valley mountain river lake hill bay plain north ridge coast city district town area community municipality country village land region route highway road railway bridge crossing canal railroad junction stream creek tributary  organization business institution company industry organisation agency school department university government court board channel network affiliate outlet supplier manufacturer distributor vendor retailer in-vestor broker provider lender owner creditor share-holder customer employer Table 3: First two level-one clusters of synonyms for the word ?bank? We produce clusters with a two-layer structure. Level one represents loose topical relatedness roughly corresponding to broad domains, while level two is aimed at grouping together closely similar words. For example, among the objects of the verb cultivate, the nouns tie and contact belong to the same level-two cluster. Attitude and spirit belong to another level-two cluster but both pairs are in the same level-one cluster. The nouns fruit and vegetable are clustered together in another level-one cluster. This double-layer representation is a solution to express at once close synonymy and topic relatedness. The clus-ters of symonyms displayed in Table 3 follow the same representation. 4 Implementation of the system In this section, we describe the implementation of Linggle, including how to index and store n-grams for a fast access (Section 4.1) and construction of the LDA models (Section 4.2). We will describe the clustering method in more details in section 5.  4.1 N-grams preprocessing The n-grams are first filtered keeping only the words that are in WordNet and in the British Na-tional Corpus, and then indexed by word and position in the n-gram, in a way similar to the rotated n-gram approach proposed by Lin et. al. (2010). The files are then stored in an Apache 
142
HBase NoSQL base. The major advantages of using a NoSQL database is the excellent perfor-mance in querying the ability of storing large amounts of data across several servers and the capability to scale up when we have additional entries in the dataset, or additional datasets to add to the system. 4.2 LDA models computations Two types of LDA models are calculated for Linggle. The first type is a selectional preference model between heads and modifiers. Six models are calculated in total for the subject-verb, the verb-object and the adjective-noun relations done in a similar way to Ritter and Etzioni?s (2010) model with binary relations instead of triples. The second is a word/synonyms model in which a word is considered as a document in LDA and its synonyms as the words of the document. This second model has the effect of splitting the syno-nyms of a word into different topics, as shown in Table 3.  Seeds                                             parameter: s1 1. Consider the m first topics for a verb v ac-cording to the LDA per document-topic dis-tribution (?) 2. Consider S = o1,?,on, a set of n objects of v.  3. Split S into m classes C1,..,Cm according to their LDA per topic-word probability: oi  is assigned to the topic in which it has the highest probability. 4. For each class Ci, move every object oj that is not similar to any other ok of Ci , according to a similarity threshold s1 into a new created class. Level 2                                           parameter: s2  While (Argmaxci ,cj Sim( ci , cj ) > s2):            Merge Argmaxci ,cj Sim( ci , cj ) into one class. Level 1                                           parameter: s3  While (Argmaxci ,cj Sim(ci , cj ) > s3):            Group Argmaxci ,cj Sim( ci ,cj ) under the             same level 1 cluster. Table 4:  Clustering Algorithm for the object of a giv-en verb  The hyperparameters alpha, eta, that affect the sparsity of the document-topic (theta) and the topic-word (lambda) distributions are both set to 0.5 and the number of topics is set to 300. More research would be necessary to optimize the val-ue for the parameters in the perspective of the clustering algorithm, as quickly discussed in the next section.  
 Sim (ci, cj): 1. Build the Cartesian product C = ci ? cj 2. Get P the set of the similarity between all word pairs in C 3. Return Sim(ci,cj) the mean of the scores in P  Table 5:  Similarity between two classes ti and tj 5 Clustering algorithm The clustering algorithm combines topic model-ing results and a semantic similarity measure. We use Pantel?s dependencies repository to compute LDA models for subject-verbs, verbs-objects and adjective-nouns relations in both di-rections. Currently, we also use Pantel?s similari-ty measure. It has a reasonable precision partly because it relies on parser information instead of bag of words windows. However the coverage of the available scores is lower than what would be needed for Linggle. We will address this issue in the near future by extending it with similarity scores computed from the n-grams. We combine the two distributional semantics approaches in a simple manner inspired by clus-tering by committee algorithm  (CBC). The simi-larity measure is used to refine the LDA topics and to generate finer grain clusters. Conversely, LDA topics can also be seen as the seeds of our clustering algorithm. This algorithm intends to constrain the words that belong to a final cluster more strictly than LDA does in order to obtain clearly interpretable clusters. The exact same algorithm is applied to synonym models, for synonyms of nouns, adjec-tives and verbs (shown in Table 3). Table 4 shows the algorithm for constructing double layer clusters for a set S of objects of a verb v. The objects are first roughly split into classes, attributing a single topic to every object oi. The topic of a word oi is determined accord-ing to its per topic-word probability. More exper-iments could be done using the product of the per document-topic and the per topic-word LDA probabilities instead, in order to take into account the specific verb when assigning a topic to the object. Such a way of assigning topics should also be more sensitive to the LDA hyperparame-ters.  At this stage, some classes are incoherent and that low frequency words that do not appear in the head of any topic are often misclassified. Words are rearranged between the classes and create new classes if necessary using the simi-larity measure. If any word of a class is not simi-
143
lar to any other word in this class (the threshold is set to s1 = 0.09), a new class is created for it. Any two classes are then merged if their simi-larity (computer accordingly to Table 5) is above s2=0.06, forming the level 2 clusters. Classes are then grouped together if the similarity between them is above s3 = 0.02 forming the level 1 clus-ters. Finally, the classes that contain less than three words are not displayed in Linggle and the predi-cate-arguments counts in the Web 1T are re-trieved using a few hand crafted RE and morpho-logical expansion of the nouns and the verbs.  This algorithm appears to generate interpreta-ble semantic classes and to be quite robust re-garding the threshold parameters. More tests and rigorous evaluation are left to future work.   6 Conclusion There are many different directions in which Linggle will be improved. The first one is to al-low users to work with word forms and with multiword expressions. The second one concerns the extension of the coverage of the example base with several large corpora such as Wikipe-dia and the extension of the coverage of the simi-larity measure. The third direction concerns the development of automatic suggestions for text edition, such as suggesting a better adjective or a different preposition in the context of a sentence. Finally, Linggle is currently being extended to Chinese. We presented a prototype that gives access to Web Scale collocations. Linggle displays both word usage and word similarity information. Depending on the type of the input query, the results are displayed under the form of lists or clusters of n-grams. The system is designed to become a multilingual platform for text edition and can also become a valuable resource for natural language processing research. References  David Blei, A. Ng, and M. Jordan. 2003. Latent Di-richlet alocation. Journal of Machine Learning Research, 3:993?1022, January 2003. Jason S. Chang, 2008. Linggle: a web-scale language reference search engine. Unpublished manuscript. Katrin Erk and Sebastian Pad?. 2010. A Flexible, Corpus-Driven Model of Regular and Inverse Se-lectional Preferences. In Proceedings of ACL 2010.  Christiane Fellbaum. 2010. WordNet. MIT Press, Cambridge, MA. 
John Rupert Firth. 1957. The Semantics of Linguistics Science. Papers in linguistics 1934-1951. London: Oxford University Press. William H Fletcher. 2012. Corpus analysis of the world wide web." In The Encyclopedia of Applied Linguistics. Adam Kilgarriff , and David Tugwell. 2001. Word sketch: Extraction and display of significant collo-cations for lexicography. In Proceedings of COL-LOCTION: Computational Extraction, Analysis and Exploitation workshop, 39th ACL and 10th EACL, pp. 32-38. Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th inter-national conference on Computational linguistics, volume 2. Association for Computational Linguis-tics, pp. 768-774.  Dekang Lin, and Patrick Pantel. 2002. Concept Dis-covery from Text. In Proceedings of Conference on Computational Linguistics (COLING-02). pp. 577-583. Taipei, Taiwan. Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of LREC. David Newman, Jey Han Lau, Karl Grieser and Timo-thy Baldwin (2010). Automatic Evaluation of Topic Coherence. In Proceedings of Human Lan-guage Technologies, 11th NAACL HLT, Los Ange-les, USA, pp. 100?108. Evan Sandhaus. 2008. "New york times corpus: Cor-pus overview." LDC catalogue LDC2008T19.  Sempute Team. 2013. What is NeedleSeek? http://needleseek.msra.cn/readme.htm Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving customary Web language to assist writers. Advances in Information Retrieval. Springer Berlin Heidelberg, pp. 631-635.  Martin Potthast, Martin Trenkmann, and Benno Stein. Using Web N-Grams to Help Second-Language Speakers .2010. SIGIR 10 Web N-Gram Workshop, pages 49-49. Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-guistics (July 2010), pp. 424-434. Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence 11(3), pp. 197-223.  Shaoqun Wu, Ian H. Witten and Margaret Franken (2010). Utilizing lexical data from a web-derived corpus to expand productive collocation knowledge. ReCALL, 22(1), 83?102. 
144
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 295?301,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
Helping Our Own: NTHU NLPLAB System Description 
  
Jian-Cheng Wu+, Joseph Z. Chang*, Yi-Chun Chen+, Shih-Ting Huang+, Mei-Hua Chen*, 
Jason S. Chang+ 
  * Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 30013 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 30013 
{wujc86, bizkit.tw, pieyaaa, koromiko1104, chen.meihua, 
jason.jschang}@gmail.com 
  
  
Abstract 
Grammatical error correction has been an active 
research area in the field of Natural Language 
Processing. In this paper, we integrated four 
distinct learning-based modules to correct 
determiner and preposition errors in leaners? 
writing. Each module focuses on a particular 
type of error. Our modules were tested in 
well-formed data and learners? writing. The 
results show that our system achieves high 
recall while preserves satisfactory precision. 
1. Introduction 
Researchers have demonstrated that prepositions 
and determiners are the two most frequent error 
types for language learners (Leacock et al 2010). 
According to Swan and Smith (2001), preposition 
errors might result from L1 interference. Chen and 
Lin (2011) also reveal that prepositions are the 
most perplexing problem for Chinese-speaking 
EFL learners mainly because there are no clear 
preposition counterparts in Chinese for learners to 
refer to. On the other hand, Swan and Smith (2001) 
predict that the possibility of determiner errors 
depends on learners? native language. The 
Cambridge Learners Corpus illustrates that 
learners of Chinese, Japanese, Korean, and Russian 
might have a poor command of determiners.  
In view of the fact that a large number of 
grammatical errors appear in non-native speakers? 
writing, more and more research has been directed 
towards the automated detection and correction of 
such errors to help improve the quality of that 
writing (Dale and Kilgarriff, 2010). In recent years, 
preposition error detection and correction has 
especially been an area of increasingly active 
research (Leacock et al 2010). The HOO 2012 
shared task also focuses on error detection and 
correction in the use of prepositions and 
determiners (Dale et al, 2012).  
Many studies have been done at correcting 
errors using hybrid modules: implementing distinct 
modules to correct errors of different types. In 
other word, instead of using a general module to 
correct any kind of errors, using different modules 
to deal with different error types seems to be more 
effective and promising. In this paper, we propose 
four distinct modules to deal with four kinds of 
determiner and preposition errors (inserting 
missing determiner, replacing erroneous 
determiner, inserting missing preposition, and 
replacing erroneous prepositions). Four 
learning-based approaches are used to detect and 
correct the errors of prepositions and determiners.   
In this paper, we describe our methods in the 
next section. Section 3 reports the evaluation 
results. Then we conclude this paper in Section 4.  
2. System Description 
2.1 Overview 
In this sub-section, we give a general view of our 
system. Figure 1 shows the architecture of the 
integrated error detection and error correction 
system. The input of the system is a sentence in a 
learner?s writing. First, the data is pre-processed 
using the GeniaTagger tool (Tsuruoka et al, 2005), 
which provides the base forms, part-of-speech tags, 
chunk tags and named entity tags. The tag result of 
295
  
the sample sentence ?This virus affects the defense 
system.? is shown in Table 1. The determiner error 
detection module then directly inserts the missing 
determiners and deletes the unnecessary 
determiners. Meanwhile, the error determiners are 
replaced with predicted answers by the determiner 
error correction module. After finishing the 
determiner error correction, the preposition error 
detection and correction module detects and 
corrects the preposition errors of the modified 
input sentence.  
In the following subsections, we first introduce 
the training and testing of the determiner error 
detection and correction modules (Section 3.2). 
Then in section 3.3 we focus on the training and 
testing of the preposition error detection and 
correction modules. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. System Architecture (Run-Time) 
 
 
Word Base form POS Chunk NE 
This This DT B-NP O 
virus virus NN I-NP O 
affects affect VBZ B-VP O 
the the DT B-NP O 
defence defence NN I-NP O 
system system NN I-NP O 
. . . O O 
Table 1. The tag result of sample sentence. 
2.2 Determiners 
In this section, we investigate the performance of 
two maximum entropy classifiers (Ratnaparkhi, 
1997), one for determining whether a noun phrase 
has a determiner or not and the other for selecting 
the appropriate determiner if one is needed.  
 From the British National Corpus (BNC), we 
extract 22,552,979 noun phrases (NPs). For 
determining which features are useful for this task, 
all NPs are divided into two sets, 20 million cases 
as a training set and the others as a validation set.  
For the classifier (named the DetClassifier 
hereafter) trained for predicting whether a NP has a 
determiner or not, the label set contains two labels: 
?Zero? and ?DET.? On the other hand, for the 
classifier (named the SelClassifier hereafter) which 
predicts appropriate determiners, the label set 
contains 9 labels: the, a, an, my, your, our, one, 
this, their. (In the training data, there are 7,249,218 
cases with those labels.) 
Both of the classifiers use contextual and 
syntactic information as features to predict the 
labels. The features include single features such as 
the headword of the NP, the part of speech (PoS) 
of the headword, the words and  PoSs in the 
chunks before or after the NP (pre-NP, post-NP), 
and all words and PoSs in the NP (excluding the 
determiner if there was one), etc. We also combine 
the single features to form more specific features 
for better performance. 
At run time, the given data are also tagged and 
all features for each NP in the data are extracted 
for classification. For testing, all determiners at the 
beginning of the NPs are ignored if they exist. At 
first, the DetClassifier is used to determine 
whether a NP needs a determiner or not. If the 
classifier predicts that the NP should not have a 
determiner but it does, there is an ?UD? 
(Unnecessary determiner) type mistake. In contrast, 
Preposition Error 
Choice 
Determiner Error 
Detection 
Determiner 
Choice 
Preposition Error 
Detection 
Input 
sentence 
Tagger & Parser 
Determiner 
Preposition 
Output 
296
  
if the classifier predicts that the NP should have a 
determiner but it does not, there is a ?MD? type 
mistake. For both ?MD? (Missing determiner) and 
?RD? (Replace determiner) mistake types, we 
would use the SelClassifier to predict which 
determiner is more appropriate for the given NP.  
2.3 Prepositions 
2.3.1 Preposition Error Detection 
In solving other problems in natural language 
processing, supervised training methods suffers 
from the difficulty of acquiring manually labeled 
data. This may not be the case with grammatical 
language error correction. Although high quality 
error learner?s corpora are not currently available 
to the public to provide negative cases, any 
ordinary corpus can used as positive cases at 
training time. 
In our method, we use an ordinary corpus to 
train a Conditional Random Field (CRF) tagger to 
identify the presence of a targeted lexical category. 
The input of the tagger is a sentence with all words 
in the targeting lexical category removed. The 
tagger will tag every word with a positive or 
negative tag, predicting the presence of a word in 
the targeted lexical category. In this paper, we 
choose the top 13 most frequent prepositions: of, to, 
in, for, on, with, as, at, by, from, about, like, since. 
Conditional Random Field 
The sequence labeling is the task of assigning 
labels from a finite set of categories sequentially to 
a set of observation sequences. This problem is 
encountered not only in the field of computational 
linguistics, but also many others, including 
bioinformatics, speech recognition, and pattern 
recognition. 
Traditionally sequence labeling problems are 
solved using the Hidden Markov Model (HMM). 
HMM is a directed graph model in which every 
outcome is conditioned on the corresponding 
observation node and only the previous outcomes. 
Conditional Random Field (CRF) is considered 
the state-of-the-art sequence labeling algorithm. 
One of the major differences of CRF is that it is 
modeled as a undirected graph. CRF also obeys the 
Markov property, with respect to the undirected 
graph, every outcome is conditioned on its 
neighboring outcomes and potentially the entire 
observation sequence. 
 
 
Figure 2. Simplified view of HMM and CRF 
 
Supervised Training 
Obtaining labeled training data is relatively easy 
for this task, that is, it requires no human labeler. 
For this task, we will use this method to target the 
lexical category preposition. To produce training 
data, we simply use an ordinary English corpus 
and use the presence of prepositions as the 
outcome, and remove all prepositions. For example, 
the sentence  
 
?Miss Hardbroom ?s eyes bored into Mildred 
like    a    laser-beam    the    moment    
they    came into view .? 
 
will produce  
 
?Miss _Hardbroom _?s _eyes _bored +Mildred 
_like _a _laser-beam _the _moment _they 
_came  +view .?  
 
where the underscores indicate no preposition 
presence and the plus signs indicate otherwise. 
Combined with additional features described in 
following sections, we use the CRF model to train 
a preposition presence detection tagger. Features 
additional to the words in the sentence are their 
corresponding lemmas, part-of-speech tags, upper 
or lower case, and word suffix. 
At runtime, we first remove all prepositional 
words in the user input sentence, generate 
additional features, and use the trained tagger to 
predict the presence of prepositions in the altered 
sentence. By comparing the tagged result with the 
original sentence, the system can output insertion 
and/or deletion of preposition suggestions. 
The process of generating features is identical to 
producing the training set. To generate 
297
  
part-of-speech tag features at runtime, one simple 
approach is to use an ordinary POS tagger to 
generate POS tags to the tokens in the altered 
sentences, i.e. English sentences without any 
prepositions. A more sophisticated approach is to 
train a specialized POS tagger to tag English 
sentences with their prepositions removed. A 
state-of-the-art part-of-speech tagger can achieve 
around 95% precision. In our implementation, we 
find that using an ordinary POS tagger to tag 
altered sentences yield near 94% precision, 
whereas a specialized POS tagger performed 
around 1% higher precision. 
We used a small portion of the British National 
Corpus (BNC) to train and evaluate our tagger (1M 
and 10M tokens, i.e. words and punctuation marks). 
The British National Corpus contains over 100 
million words of both written (90%) and spoken 
(10%) British English. The written part of the BNC 
is sampled from a wide variety of sources, 
including newspapers, journals, academic books, 
fictions, letter, school and university essays. A 
separate portion of the BNC is selected to evaluate 
the performance of the taggers. The test set 
contains 322,997 tokens (31,916 sentences). 
 
2.3.2 Preposition Error Correction 
Recently, the problem of preposition error 
correction has been viewed as a word sense 
disambiguation problem and all prepositions are 
considered as candidates of the intended senses. In 
previous studies, well-formed corpora and learner 
corpora are both used in training the classifiers. 
However, due to the limited size of learner corpora, 
it is difficult to use the learner corpora to train a 
classifier. A more feasible approach is to use a 
large well-formed corpus to train a model in 
choosing prepositions. Similar to the determiner 
error correction, we choose the maximum entropy 
model as our classifier to choose appropriate 
prepositions underlying certain contexts. In order 
to cover a large variety of genres in learners? 
writing, we use a balanced well-formed corpus, the 
BNC, to train a maximum entropy model.  
Our context features include four feature 
categories which are introduced as follows.  
? Word feature (f1): Word features include a 
window of five content words to the left and 
right with their positions. 
? Head feature (f2): We select two head words 
in the left and right of prepositions with their 
relative orders as head features. For example, 
in Table 2, we select the first head word, face, 
with its relative order, Rh1, as one of the 
head features of preposition, to. More 
specifically, ?Rh1=face? denotes first head 
word, face, right of the preposition, to. 
? Head combine feature (f3): Combine any 
two head features described above to get six 
features. For example, L1R2 denotes two 
head words surrounding the preposition. 
? Phrase combine feature (f4): Combine the 
head words of noun phrase and verb phrase 
where the preposition is between the phrases. 
For example, V_N feature denotes the head 
words of verb phrase and noun phrase where 
the preposition is followed by noun phrase 
and is preceded by verb phrase. 
   
 
Word Feature 
(f1) 
Lw1=leaving, Rw1=face,  
Rw2= chronic, Rw3= condition 
Head Feature 
(f2) 
Lh1=them, Lh2=leaving, 
Rh1=face, Rh2=condition 
Head Combine 
Feature (f3) 
L1L2= them_leaving,  
L1R1= them_face,  
L1R2= them_condition, ? 
Phrase Combine 
Feature (f4) 
N_N= them_condition,  
V_N= leaving_condition,  
N_V= them_face,  
V_V= leaving_face 
Table 2. Features example for leaving them to face this 
chronic condition 
At run time, we extract the features of each 
preposition in learners? writings and ask the model 
to predict the preposition. The preposition error 
detection model described in section 2.3.1 first 
removes all prepositions from test sentences and 
then marks the ?presence? and ?absence? labels in 
every blank of a sentence. For each blank labeled 
?presence?, the correction model predicts the 
preposition which best fits the blank underlying the 
contexts. The correction model does not predict 
when the blanks are labeled ?absence?. Although 
some blanks labeled ?absence? may still 
correspond to prepositions, we decide to reduce 
some recall score to ensure the accuracy of the 
results. 
298
  
3. Experimental Results 
In this section, we present the experimental results 
of the determiner and preposition modules 
respectively.   
3.1 Determiners 
Table 3 shows the performance of the 
DetClassifier of individual feature and Table 4 
shows the performance of the SelClassifier. We 
also wonder how the size of training data 
influences the performance of the models. Table 5 
and 6 show the precision of modes of different 
sizes of training data with the best feature ?whole 
words in NP and last word of pre-NP.? Because the 
performance converges while using more than 5 
million training cases, we use only 1 million 
training cases to investigate the performance of 
using multiple features.  When using all features, 
the precision increases from 84.8% to 85.8% for 
DetClassifier, and from 39.8% to 56.0% for 
SelClassifier. 
We also implement another data-driven model 
for determiner selection (including zero) by using 
the 5gram of Web 1T corpus. The basic concept of 
the model is to use the frequency of determiners 
which fit the context of the given test data to 
choose the determiner candidates. If the frequency 
of the determiner using in the given NP is lower 
than other candidate determiners, we would use the 
most frequent one as the suggestion. However, 
according to our observation during testing, we 
find that the model tends to cause false alarms. To 
reduce the probability of false alarm, we set a high 
threshold for the ratio f1/f2 where f1 is the frequency 
of the used determiner and f2 is the frequency of 
the most frequent determiner. The suggestion is 
accepted only when the ratio exceeds the threshold.  
The major limitation of the proposed method is 
that some errors are ignored due to parsing errors. 
For example, the given data ?the them? should be 
considered as one NP with the ?UD? type error. 
However, the parser would give the chunk result 
?the [B-NP] them [B-NP]? and the error would not 
be recognized. It might need some rules to handle 
these exceptions. Another weakness of the 
proposed methods is that the less frequently used 
determiners are usually considered as errors and 
suggested to be replaced with more frequently used 
ones. For example, possessives such as ?my? 
and ?your?, are usually replaced with ?the.? We 
need to integrate more informative features to 
improve performance. 
 
Features Precision 
head/PoS 79.1% 
word/PoS of pre-NP 70.0% 
word/PoS of all words in NP 85.9% 
PoS of all words in NP 77.8% 
word/PoS of post-NP 71.8% 
whole words in NP 87.2% 
last word/PoS of pre-NP and head/PoS 92.3% 
whole words in NP and last word of 
pre-NP 
96.8% 
Table 3. Precision of features used in the DetClassifier 
 
Features Precision 
head/PoS 55.2% 
word/PoS of pre-NP 49.5% 
word/PoS of all words in NP 53.9% 
PoS of all words in NP 45.3% 
word/PoS of post-NP 46.1% 
whole words in NP 60.4% 
last word/PoS of pre-NP and head/PoS 65.3% 
whole words in NP and last word of 
pre-NP 
70.8% 
Table 4. Precision of features used in the SelClassifier 
 
Size Precision 
1,000,000 84.8% 
5,000,000 96.8% 
10,000,000 96.8% 
15,000,000 96.8% 
20,000,000 96.8% 
Table 5. Precision of different training size for the 
DetClassifier 
 
Size Precision 
1,000,000 39.8% 
3,000,000 43.2% 
5,000,000 44.5% 
7,000,000 61.6% 
7,249,218 70.8% 
Table 6. Precision of different training size for the  
 SelClassifier 
 
 
 
299
  
3.2 Prepositions 
Two sets of evaluation were carried out for 
detection. First, we use a randomly-selected 
portion of the BNC containing 1 million tokens to 
train our tokenizer targeting the 34 highest 
frequency prepositions. Second, we use a larger 
training corpus containing 10 million tokens, also 
randomly selected from the BNC, and target a 
smaller set of the 13 highest frequency 
prepositions, due to the fact that these 13 
prepositions can cover over 90% of the preposition 
errors found in the development set. 
We evaluate the trained taggers using two 
different metrics. First we evaluate the overall 
tagging precision, which is defined as 
 
Poverall   =  # of correctly tagged words  / # of 
all words  
Ppresence =  # correctly tagged PRESENCE / #  
all words labeled with PRESENCE 
 
Since most answer tags are Non-presence, 
Poverall is not informative, we therefore focus on 
Ppresense, and further evaluate the recall of presence, 
defined as: 
 
Rpresence = # correctly tagged PRESENCE  / # 
word should be tagged with PRESENCE  
 
We then evaluate on Precision and Recall of the 
PRESENCE tag using different probabilities to 
threshold the CRF tagging results. Then we show 
the result of two evaluation sets. On the left is the 
tagger train with 1 million tokens, targeting 34 
prepositions. On the right is the tagger trained with 
10 million tokens, targeting 13 prepositions. Only 
the latter tagger is used for producing the 
submitted runs. 
We used the development data released as part 
of HOO 2012 Shared Task as the gold standard for 
the evaluation of our preposition correction module. 
In order to observe the effect of different feature 
sets in training, we first extracted the MT and RT 
instances marked by the gold standard and then ask 
the correction module to correct these prepositions 
directly. Table 7 shows the precision of the models 
trained on different feature sets. The definition of 
precision is the same as the definition in the HOO 
2012 Shared Task. The results shows that the 
model trained using four feature sets achieved 
higher precision.   
Features Precision 
MT RT MT+RT 
f1 43.62% 39.15% 40.48% 
f1+f2 52.58% 43.47% 46.18% 
f1+f2+f3 55.20% 46.77% 49.27% 
f1+f2+f3+f4 55.11% 47% 49.41% 
Table 7. The feature selection and accuracy of the 
preposition correction module. 
 
In addition to the evaluation on the effect of 
different feature sets, we also conducted an 
evaluation done on the development data of HOO 
2012 Shared Task to observe the performance of 
the correction model when combined with the 
detection model. The correction model corrected 
three different types of preposition errors, MT, RT 
and MT+RT simultaneously (Table 8). 
 
 
  MT RT MT+RT 
Precision 1.16% 3.80% 4.96% 
Recall 29.86% 41.14% 37.79% 
  
Table 8. Precision and recall scores of the correction 
modules when combined with the detection module.  
 
Note that when we only corrected the 
preposition errors marked MT by preposition error 
detection module, the precision and recall are both 
lower than that of RT. The amount of false alarm 
instances of detection module in MT seems to be 
too high, thus in this paper, we won?t correct the 
instance marked MT to insure the higher precision 
of overall preposition correction. 
 
4. Conclusion 
In this paper, we integrate four learning-based 
methods in determiner and preposition error 
detection and correction. The integrated system 
simply parses and tags the test sentences and then 
corrects determiners and prepositions step by step. 
The training of our system relies on well-formed 
corpora and thus seems to be easier to 
re-implement it. The large well-formed corpus 
might also insure higher recall.  
In the future, we plan to integrate the system in 
a more flexible way. The detection modules could 
300
  
pass probabilities to the correction modules. The 
correction modules thus could decide whether to 
correct the instances or not. In addition, we plan to 
reduce the false alarm rate of the detection module. 
Besides, a more considerable evaluation would be 
conducted in the near future. 
Acknowledgements 
We would acknowledge the funding support 
from the Project (NSC 100-2627-E-007-001) 
and the help of the participants. Thanks also go to 
the comments of anonymous reviewers on this 
paper. 
References  
Mei-Hua Chen and Maosung Lin, 2011. Factors and 
Analyses of Common Miscollocations of College 
Students in Taiwan. Studies in English Language and 
Literature, 28, pp. 57-72. 
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving 
prepositions. In Proceedings of the Fourth 
ACL-SIGSEM Workshop on Prepositions, pp.  
25-30. 
Robert Dale and Adam Kilgarriff. 2010. Helping Our 
Own: Text massaging for computational linguistics 
as a new shared task. In Proceedings of the 6th 
International Natural Language Generation 
Conference, pp. 261?266. 
Robert Dale, Ilya Anisimoff and George Narroway 
(2012) HOO 2012: A Report on the Preposition and 
Determiner Error Correction Shared Task. In 
Proceedings of the Seventh Workshop on Innovative 
Use of NLP for Building Educational Applications. 
Rachele De Felice and Stephen G. Pulman. 2007. 
Automatically acquiring models of preposition use. 
In Proceedings of the Fourth ACL-SIGSEM 
Workshop on Prepositions, pp. 45-50. 
Claudia Leacock, Martin Chodorow, Michael Gamon, 
and Joel Tetreault. 2010. Automated Grammatical 
Error Detection for Language Learners. Synthesis 
Lectures on Human Language Technologies. Morgan 
and Claypool. 
Adwait Ratnaparkhi. 1997. A linear observed time 
statistical parser based on maximum entropy models. 
In Proceedings of the Second Conference on 
Empirical Methods in Natural Language Processing, 
Brown University, Providence, Rhode Island. 
Michael Swan and Bernard Smith, editors. Learner 
English: A teacher?s guide to interference and other 
problems. Cambridge University Press, 2 edition, 
2001. DOI: 10.1017/CBO9780511667121 19, 23, 91 
Tsuruoka Y, Tateishi Y, Kim JD, Ohta T, McNaught J, 
Ananiadou S, Tsujii J. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In 
Advances in Informatics, 10th Panhellenic 
Conference on Informatics; 11-13 November 2005 
Volos, Greece. Springer; pp. 382-392. 
 
301
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 20?25,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
CoNLL-2013 Shared Task: Grammatical Error Correction NTHU System Description  Ting-Hui Kao+, Yu-Wei Chang*, Hsun-Wen Chiu*, Tzu-Hsi Yen+,  Joanne Boisson*, Jian-Cheng Wu+, Jason S. Chang+
* Institute of Information Systems and Applications + Department of Computer Science National Tsing Hua University  HsinChu, Taiwan, R.O.C. 30013 { maxis1718, teer1990, chiuhsunwen, joseph.yen,  Joanne.boisson, wujc86, jason.jschang} @gmail.com    Abstract 
Grammatical error correction has been an active research area in the field of Natural Language Processing. This paper describes the grammatical error correction system developed at NTHU in participation of the CoNLL-2013 Shared Task. The system consists of four modules in a pipeline to correct errors related to determiners, prepositions, verb forms and noun number. Although more types of errors are involved that than last year?s Shared Task, leading to more complicated problem this year, our system still obtain higher F-score as compared to last year. We received an overall F-measure score of 0.325, which put our system in second place among 17 systems evaluated. 1 Introduction Grammatical error correction is a task involving automatically detecting and correcting grammatical errors and improper choices. Grammatical error correction in writing of English as a second language (L2) or foreign language (EFL) is an important issue, for there are 375 million L2 speakers and 750 million EFL speakers around the world (Graddol, 2006). Most of these non-native speakers tend to make many kinds of error in their writing. An error correction system has the short-term benefit of helping writers improve the quality of writing. In the long run, non-native writers might learn from the corrections and thus gradually gain better command of grammar and word choice. The HOO shared task of 2012 is aimed at detecting and correcting misuse of determiners and prepositions, two types of errors accounting 
for only 38% of all errors. Therefore, there are a lot more errors related to other parts of speech that we have to address in this year?s shared task. In this paper, we describe the system submission from NTHU. The system reads and processes a given sentence through a pipeline of four distinct modules dealing with determiners, prepositions, verb forms and noun plurality. The output of one module feeds into the next module as input. The system finally produces possibly corrected sentences. The rest of the article is organized as follows. Section 2 describes detection and correction approach of each module in detail. Section 3 describes experiment setting and results. Then in Section 4, we discuss strengths and limitations of the proposed system and directions of future work. We conclude in Section 5.  2 System Description The system is designed to read a sentence and process each type of errors in terms and finally produce a corrected sentence. In Section 2.1, we give an overview of the system. Then, in Sections 2.2-2.5, we describe how to correct errors related to noun number, determiner, verb tense, and preposition.   
  Figure 1. System Architecture 
20
Table 1. Moving windows of ?location? Moving Window n-grams MW5 track based on the location based on the location of  on the location of cell the location of cell phone location of cell phone by MW4 based on the location on the location of the location of cell location of cell phone MW3 on the location the location of location of cell MW2 the location location of 2.1 Overview In this section, we give an overview of our system. Figure 1 shows the architecture of the error correction system. In this study, we focus on five different grammatical error types, including the improper usage of Determiner (ArtOrDet), Noun Number (Nn), Verb-Tenses (Vform), Subject-Verb Agreement (SVA), and Preposition (Prep).  In order to deal with these different types of errors systematically, we propose a back-off model based on the moving window approach.  Moving Window  A moving window MW of certain word wi is defined as below. (Leacock et al, 2010; Rozovskaya et al, 2010)   ???,?(?) = {???? ,? ,????? ???  ?, ? = 0, ? ? 1 ?}  (1)  where i denotes the position of word, k the window size, and w the original or replacement word at position i. In our approach, the window size is set to 2 to 5 words.  For example, consider the target word ?location? in the sentence, ?Children can easily be track based on the location of cell phone by parents.? The n-grams in moving windows of related to ?location? of sizes 2 to 5 are shown in Table 1.  Back-off Model To determine whether the target word needs to be changed to a different form (e.g, from ?location? to ?locations?), we first replace the target word with its variant forms (e.g., ?locations? for ?location?) in all MW n-grams and 
Table 2. Trigram information of ?location? and ?locations? in back-off model MW3 n-gram Freq. S3 location on the location the location of location of cell 304,400 3,794,400 1,400 4 M locations on the locations the locations of locations of cell 18,200 374,000 200 0.04 M  then measure the ratio of the counts of the original and replaced n-grams in a corpus. The frequency counts are obtained by querying a linguistic search engine Linggle (Joanne Boisson et al 2013), a web-scale linguistic search engine based on Google Web1T (Brants and Franz, 2006). The sum of n-gram counts, Sk with  the word w (original or replacement) in the ith position is defined as        ??,?(?)  ?=  ? ?????(?????)????  ?? ? ? ?(?)      (2)  With MW and S, we design a Replace function to determine whether is necessary to replace wi with its variant form, w' :  
Figure 2. The function Replace for determining whether to replace a word in location i using moving windows of size k.  The parameters ? and ? in Replace are set empirically.  For instance, in the given sentence ?Children can easily be track based on the location of cell phone by parents?, the target word wi is ?location? and the candidate is ?locations? for the Nn type error. According to Equation 2, the sums S9,3(?location?) of the original trigrams is about 4 million, whereas S9,3(?locations?) of the replaced trigrams is only 0.4 million (see Table 2 for more details). The value of r is 0.096, and depending on the threshold, Replace either returns False or back off to consider again the ratio r of S9,2(?location?) of the original bigrams and S9,2(?locations?) of the replacement bigrams for confidence in replacing the word ?location.?  
function Replace(i, k, w?) r = Si,k(w?)/Si,k(wi) if r > ? return True else if k > 2 and r > ?: return Replace (i, k-1, w?) else: return False 
21
2.2 The number module The number module is designed to correct error related to noun number (i.e., Nn). Two types of error are included, errors of singular noun and plural noun. To correct errors, we identify heads of base noun phrase (i.e., NP consisting of maximal contiguous sequence of tokens without containing another noun phrase or clause) in the given sentence by using part-of-speech tags and GeniaTagger (Tsuruoka et al, 2006), then use the Replace function to replace the original nouns (either singular or plural) to a different form (i.e., singular to plural, or plural to singular). We use two methods in the number module: combining voting with back-off, and using dependency relations.   Combining voting with back-off   Each n-gram in a moving window of various sizes described in Section 2.1 gets to cast a vote. When the sum of frequency counts related to the original noun is higher than that related to the replacement noun, the original noun gets one vote and vise versa. Voting method determines whether to replace the noun based on majority of the votes. For example, all of the 14 replacement n-grams (MWi, k , k = 2, 5) in Table 1 get a vote, because the n-gram with ?location? has higher frequency count that the same n-gram replaced with ?locations?. Intuitively, we should be confident enough to decide to stay with the original noun, i.e., ?location.? Back-off model described in Section 2.1 make a decision to permit the Replace module to change the original noun depend on threshold ?. Both of voting and back-off model need to show that alternative noun number is better. For the scheme of voting and back-off model, we also require the top count ratio and absolute count of 0.95 and 60,000 based on empirical evidence.  Using dependency relations  In some cases, the noun number depends on subject-verb agreement. We use part-of-speech information of subject and governing verb obtained from a tagger to handle such cases. For that, we use 3rd person singular present (i.e., VBZ) and other verb forms (e.g., VBP) to detect noun number mistakes.  Consider the sentence, ?In the society today, there are many ideas or concept that are 
currently in the stages of research and development.?, where ?concept? is a singular noun, but should be plural according to syntactic dependency information. The dependency parser typically produces nsubj(are-7, concept-11) among other relations and the word ?are? is tagged as VBP. Accordingly, we can replace the original noun, ?concept? to its plural form, ?concepts.?  2.3 Determiners module  The determiner is aimed at correcting determiner errors (i.e., errors annotated as ArtOrDet ). Given a sentence, we first identify the base noun phrases and their determiners (or lack of determiner) and using the moving window approach to decide whether there is an error and which alternative form to use. For determiner errors, the variant form of a base NP with a determiner is simple the same NP with determiner removed, while the variant form of a base NP without a determiner is simple the same NP with a determiner added. In addition to the moving window and back-off model, we also use dependency relations to check if a determiner is required for a base noun phrase.  Frequency of n-grams  We adopt the moving window approach and combine it with the back-off model mentioned in Section 2.1 with slight modification for the cases specific to determiner errors. When the head of given Base-NP is the last word of the n-gram, (as in ?Prepare meals for the elderly is my duty.?), the head can often be used as an modifier (as in ?for elderly people? leading to higher counts unrelated to the our case of the word being used as the head.   Therefore, while we adopting the moving window approach, the count of such n-gram is not counted. We set the threshold in the Replace function empirically: ?=5 and ?=0.35.   Dependency  In some cases, the frequency information of n-grams provides limited evidence for identifying mistakes. Therefore, we use more effective rules based on dependency relations to recognize the determiner errors in a way similar to the number module. 
22
Table 3. Verb form n-grams with PMIs. 
Verb Form n-grams PMI Sum happening crash happening happening at 21.5 38.2 59.7 happen crash happen happen at 24.0 35.7 59.7 happened crash happened happened at air crash happened happened at Miami crash happened at 
30.5 43.0 36.2 31.8 43.2 
184.7 
happens crash happens happens at crash happens at 
27.9 42.4 37.0 
107.3 
 We remove a determiner from a noun phrase with a plural head and an existing determiner. Otherwise, this module adds an appropriate determiner before the current noun phrase. For a conjunction (i.e., X and/or Y) of two base NPs, the rules favor adding a determiner such that both NPs have the same kind of determiner. 2.4 The verb-tense module In this section, we mainly concentrate on providing more proper verb tenses. Besides moving window, we introduce accumulated point-wise mutual information (PMI) (Church and Hanks, 1990) to improve the performance of this module. Applying PMI to this topic is based on the hypothesis that an appropriate verb form has a higher PMI measure with the context. To achieve more flexibility than the standard PMI, we use the modified PMI, which is an extension of standard PMI allowing an n-gram s of arbitrary length as input   ???(?) = log ?(?|?)?(??)????                               (3) where wi denotes the i-th word in s, k = | s |, and P(wi) the probability of wi estimated using a very large corpus. P(s|k) is the probability based on maximal likelihood estimation:   ?(?|?) = ???? (?)???? (?)???                               (4)  where S denotes all n-grams of length k. The PMI value of n-grams related to the original and alternative tense forms of a give verb are then calculated to attempt to correct the verb in question with a decision in favor of highest PMI. 
Table 4. Sample search results of ?being ?$PP a dangerous situation? * 
N-gram Count being in a dangerous situation 161 being a dangerous situation 0 being at a dangerous situation 0 being on a dangerous situation 0 ? 0 being about a dangerous situation 0 * Note:? denotes option word and $PP denotes wildcard prepositions  With this extended notion of PMI, we proceed as follows. First, we select each verb in a sentence and extract n-grams in moving window method as described in Section 2.2. Next, we generate more alternative n-grams by substituting all the related verb forms for the selected verb. After that, for all these n-grams, we calculate PMIs and accumulate the measures for each group of verb forms. Finally, if the accumulated PMI of the original verb is lower than the mean value of PMI of all verb forms, the verb in question will be replaced with the verb form associated the highest PMI value. Consider the sentence, ?In late nineteenth century, there was a severe air crash happening at Miami international airport.? We attempt to correct the verbs ?was? and ?happening? in the sentence. Table 3 shows n-grams and corresponding PMIs of each verb form. The accumulated PMI of ?happened? has the maximum value. So, the module changes ?happening? to ?happened.?  2.5 The prepositions module For preposition, we attempt to handle the two types of error: DELETE and REPLACE, and leave the INSERT errors for future work. For DELETE errors, the preposition in question should be deleted from the given sentence, whereas for REPLACE errors the preposition should be replaced with a more appropriate alternative. The third error type of preposition, INSERT, is left for future study. The proposed solution is based on the hypothesis that the usage of preposition often depends on the collocation relation of verb or noun. Therefore, we propose a back-off model, which utilizes the dependency relations to identify the related words of the preposition in question. We proceed as follows: For a target preposition in a given sentence, we extract the n-gram containing the preposition, its prepositional object, and the content word before the 
23
preposition. For example, the n-gram ?being in a dangerous situation? is extracted from the sentence ?This can protect the students from being in a dangerous situation in particularly for the small children who are studying in nursery.? The n-gram ?being in a dangerous situation? is then transformed into a query for a linguistic search engine (e.g., Linggle as described in Joanne et al 2013) to obtain the counts of all preposition variant forms, including NULL (for DELETE) or other prepositions (for REPLACE). The transformation process is very simple involving changing the proposition to a wild part of speech symbol. For example, ?being in a dangerous situation? is transformed to ?being ?$PP a dangerous situation.? The sample search results are shown in Table 4. From the results, we could confirm that the preposition ?in? is used correctly.  Although we use the web-scale n-gram for validation of usage of preposition, however, data sparseness still poses a problem. Furthermore, we cannot obtain information for n-grams with length more than 5, since the Web 1T we used only contains 1 to 5-grams. In order to cope with the data sparseness problem, we transform a query into a more general form, if no result could be obtained in the first round of search. To generalize the query, we remove the modifiers of the prepositional object one after another. Additionally, we also attempt to change the modifiers with the most frequent modifier of the object. Consider the n-gram ?in modern digit world.? The generalized n-grams ?in digit world? and ?in new world? will then be transformed into queries in turns until the results are sufficient for the model to make a decision. To avoid false alarm, empirically determined thresholds are used to measure the ratio of count of a preposition variant form to the original preposition. 3 Experiment To assess the effectiveness of the proposed method, we used the official training and testing data of CoNLL-2013 Shared Task. We also exploited several tools including Linggle, Stanford Parser and Geniatagger in the proposed system. Linggle supports flexible linguistic queries with wild part of speech and returns matching n-grams counts in Google Web 1T 5gram. Stanford Parser and Geniatagger produce syntactical information including dependency relations, 
part-of-speech tags, and phrase boundary. The evaluation scorer, which computes precision, recall, and F-score, is provided by National University of Singapore, the organizer of CoNLL-2013 Shared Task. On the test data, our system obtained the precision, recall and F-score of .3057, 0.346, and .3246, which put us in first place in term of recall and second place in term of F-score. 4 Discussion In this section, we discuss the strengths and limitations of our system and propose approaches to overcome current limitations. The module of noun numbers, moving window and syntactic dependency for correcting errors cannot handle well some ambiguous cases. For example, in this case "In conclusion, what I have mentioned above, we have to agree, tracking system has many benefits?.", according to the gold-standard annotations, ?system has? is corrected to ?systems have?.  However, this module keep the original word because of the 3rd person singular present verb, ?has?. Before ?has? being corrected to ?have?, there was no sufficient evidence to support that ?systems? is a good replacement. In cases like this, it is often difficult to suggest a correction using only the sentential context and n-gram frequency and dependency relations. To correct such an error, we may need to consider the context of the discourse or combine the module of different error types such as noun numbers and verb tense, which is beyond the scope of the current system. We handle the determiner errors with threshold ?  and ?  empirically derived, but it would be more effective if we could use some form of minimal error rate tuning (MERT) to set the parameters. Besides, we found that applying the dependency criteria and moving window method in parallel leads to high recall but low precision. However, the moving window method often fails because of insufficient evidence. In such case, the system can perform better in both precision and recall by favoring the dependency model output. For our system, the performance of correcting verb form errors is severely limited by the lengths of n-gram. The failure related to verb forms correction are mostly caused by the limitation of n-gram length of Web 1T. There is a large portion of sentences where the subject (or the adverbs) and the verb are so far apart, that 
24
they are not within windows of five words. So, it is difficult to use the noun number of the subject to select the correct verb form. Another major area of limitations of handling verb form errors has to do with rare words which lead to unseen n-grams even in a very large dataset like Web 1T. These rare words are mostly name entities that have insufficient coverage when combined other words in n-grams. Intuitively, we can generalize the n-gram matching process as in the case of handling preposition errors. In this study, we use the preposition and object relation (POBJ) to determine whether the use of the preposition is correct. The relation is useful for generalizing the queries and in correcting preposition errors. However, many preposition errors are unrelated to POBJ. For example, in the sentence ?Surveillance technology will help to prevent the family to loss their member...?, the two words ?to loss? should be replace with ?from losing.? Unfortunately, the current system cannot correct such an error in the absence of POBJ relation. In order to correct this kind of error, we have to consider composed relations such as noun-preposition-verb, which is crucial to the capability of correcting such multiple consecutive errors (i.e., preposition plus verb). 5 Conclusion In this paper, we build four modules in determiner, noun number, verb form, and preposition for error detection and correction. For different types of errors, we have developed modules independently in accordance with their features. The constructed modules rely on both moving windows and back-off model to improve grammatical error correction. Additionally, for verb form errors, we introduce point-wise mutual information for higher precision and recall.  We plan to integrate all the modules in a more flexible way than the current pipeline scheme. Yet another direction for future research is to consider the discourse context. 6 Acknowledgements We would like to acknowledge the funding supports from Delta Electronic Corp and National Science Council, Taiwan (contract no: NSC 100-2511-S-007-005-MY3). We are also thankful for helpful comments from the anonymous reviewers.  
References  Joanne Boisson, Ting-Hui Kao, Jian-Cheng Wu, Tzu-Hsi Yen and Jason S. Chang. 2013. Linggle: a Web-scale Linguistic Search Engine for Words in Context. In proceedings of Association for Computational Linguistics demonstrations. (ACL 2013) Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1.LDC2006T13 Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics 16(1) (1990) 22?29 Leacock Claudia et al 2010. Automated grammatical error detection for language learners. Synthesis Lectures on Human Language Technologies, 3(1) 1?134. Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013). Daniel Dahlmeier and Hwee Tou Ng. 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012). pp. 568 ? 572 David Graddol. 2006. English next: Why global English may mean the end of ?English as a Foreign Language.? UK: British Council. John Lee and Stephanie Seneff. 2006. Automatic Grammar Correction for Second-Language Learners. In INTERSPEECH ICSLP. Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto and Joel Tetreault. 2013. The CoNLL-2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Alla Rozovskaya and Dan Roth. 2010. Generating confusion sets for context-sensitive error correction. In Proceedings of EMNLP, pp. 961?970. Yoshimasa Tsuruoka et al Developing a Robust Part-of-Speech Tagger for Biomedical Text. In Advances in Informatics - 10th Panhellenic Conference on Informatics, pp 382?392. 
25
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 91?95,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
NTHU at the CoNLL-2014 Shared Task
Jian-Cheng Wu*, Tzu-Hsi Yen*, Jim Chang*, Guan-Cheng Huang*,
Jimmy Chang*, Hsiang-Ling Hsu+, Yu-Wei Chang+, Jason S. Chang*
* Department of Computer Science
+ Institute of Information Systems and Applications
National Tsing Hua University
HsinChu, Taiwan, R.O.C. 30013
{wujc86, joseph.yen, cwebb.tw, a2961353,
rocmewtwo, ilovecat6717, teer1990, jason.jschang}@gmail.com
Abstract
In this paper, we describe a system for cor-
recting grammatical errors in texts written
by non-native learners. In our approach, a
given sentence with syntactic features are
sent to a number of modules, each focuses
on a specific error type. A main program
integrates corrections from these modules
and outputs the corrected sentence. We
evaluated our system on the official test
data of the CoNLL-2014 shared task and
obtained 0.30 in F-measure.
1 Introduction
Millions of non-native learners are using English
as their second language (ESL) or foreign lan-
guage (EFL). These learners often make different
kinds of grammatical errors and are not aware of
it. With a grammatical error corrector applies rules
or statistical learning methods, learners can use the
system to improve the quality of writing, and be-
come more aware of the common errors. It may
also help learners improve their writing skills.
The CoNLL-2014 shared task is aimed at pro-
moting research on correcting grammatical errors.
Types of errors handled in the shared task are ex-
tended from the five types in the previous shared
task to include all common errors present in an es-
say.
In this paper, we focus on the following errors
made by ESL writers:
? Spelling and comma
? Article and determiner
? Preposition
? Preposition + verb (interactive)
? Noun number
? Word form
? Subject-verb-agreement
For each error type, we developed and tuned a
module based on the official development data. A
main program combines the correction hypotheses
from these modules and produces the final correc-
tion. If multiple modules propose different cor-
rections to the same word/phrase, the correction
proposed by the module with the highest precision
will be chosen.
2 Method
2.1 Spelling and Comma module
In this section, we correct comma errors and
spelling errors, including missing/extraneous hy-
phens. For simplicity, we adopt Aspell
1
and
GingerIt
2
to detect spelling errors and generate
possible replacements, considered as confusable
words, which might contain the word with cor-
rect spelling. Then, we replace the word being
checked with confusable words to generate sen-
tences. Language models trained on well-formed
texts are used to measure the probability of these
1
http://aspell.net/
2
https://pypi.python.org/pypi/gingerit
We use GingerIt only for correcting missing/extraneous
hyphens
91
candidates. Candidate with the highest probability
is chosen as correction.
Omitted commas form a large proportion of
punctuation errors. We apply the CRF model pro-
posed by Israel, et. al. (2012) with some mod-
ification. We replace distance features with syn-
tactic features. More specifically, we do not use
features such as distances to the start of sentence
or last comma. And we add two features, one in-
dicates whether a word is at the end of a clause,
and the other indicates whether the current clause
starts with a prepositional phrase.
2.2 Subject-verb-agreement module
This module corrects subject-verb-agreement er-
rors in a given sentence. Consider the sentence
?The boy in blue pants are my brother?. The cor-
rect sentence should be ?The boy in blue pants is
my brother?. Since a verb could be far from it?s
subject, using ngram counts may fail to detect and
correct such an error.
We use a rule-based method in this module.
In the first stage, we identify the subject of each
clause by using information from the parser. Both
the dependency relation and syntactic structure are
used in this stage. Dependency relations such as
nsubj and rcmod indicate subjects of subject-
verb relation. If there is a verb that has not been
assigned a subject via dependency relations, head
of noun phrase in the same clause will be used in-
stead. And in the second stage, we check whether
subject and verbs agree, for each clause in the sen-
tence.
Here we explain our correction process in more
detail. For each clause, the singular and plural
forms of verbs in the clause must be consistent
with the subject of the clause unless the subject
is a quantifier. Consider the following sentences:
The number of cats is ten.
A number of cats are playing.
Since our judgment only depends on the subject
number, it?s hard to tell whether should we use
a plural verb or not in this case. The quantifiers
we do not handle are listed as follow: number, lot,
quantity, variety, type, amount, neither.
2.3 Number module and Forms module
We correct noun number error in two stages. In
the first stage, we generate a confusion set for each
word. While constructing confusion set for noun
number, both of the singular form and plural form
are included in the set. While constructing con-
fusion set for word forms, we use the word fam-
ilies in Academic Word List (AWL)
3
and British
National Corpus (BNC4000)
4
. Given a content
word, all the words in the same family except
antonyms are entered into the confusion set. How-
ever, comparative form and superlative form of an
adjective are eliminated from the confusion set,
since replacing an adjective with these forms is a
semantic rather than syntactic decision. The fol-
lowing examples illustrate what kinds of alterna-
tives are eliminated:
antonyms: misleading for the word lead
semantics: higher for the word high
Additionally, in the forms module, a correc-
tion is ignored, if it is actually correcting a verb
tense, subject-verb-agreement, or noun number er-
ror. We use part-of-speech (POS) tag to check this.
More specifically, any corrections that changes a
word with a VBZ tag to a word with a VBP or
VBD tag is ignored, and vice versa. And any cor-
rections that switches a noun between it?s singular
form and plural form is also ignored.
With the confusion sets, we proceed to the
second stage. In this stage, we use words in
the confusion set to attempt to replace potential
errors. Language models trained on well-formed
text are used to validate the replacement decisions.
Given a word w, If there is an alternative w? that
fits in the context better, w is flagged as an error
and w? is returned as a correction.
Here is our formula for correcting errors
P (O) =
P
ngram
(O) + P
rnn
(O)
2
P (R) =
P
ngram
(R) + P
rnn
(R)
2
Promotion =
P (R)? P (O)
|O|
While checking a content word w, we replace
w in the original sentence O with alternatives and
generate candidates C. We then generate the can-
didate R with the highest probability among all
3
http://www.victoria.ac.nz/lals/
resources/academicwordlist/sublists
4
http://simple.wiktionary.org/wiki/
Wiktionary:BNC_spoken_freq
92
candidates. We use an interpolated probability
5
of
ngram language model P
ngram
and recurrent neu-
ral network language model P
rnn
. Promotion in-
dicates the increase in probability per word after
we replace sentence O with the candidate R. We
use word number to normalize Promotion follow-
ing Dahlmeier, et al. (2012). Corrections are made
only if Promotion is higher than a empirically de-
termined threshold.
6
2.4 Article and Determiner module
In this subsection, we describe how we correct er-
rors of omitting a determiner or adding a spuri-
ous determiner. The language models mentioned
in the last subsection are also used in this module.
We tune our thresholds for making corrections on
development data
7
, and found that deleting a de-
terminer should have a lower threshold while in-
serting one should have a higher one, so we set
different thresholds accordingly.
8
To cope with the situation where a deter-
miner/article is far ahead of the head of the noun
phrase, we apply another constraint while making
correction decision.
First, we calculate statistics on the head of noun
phrases. We extract the most frequent 100,000
terms in Web-1T 5-gram corpus. These terms are
used to search their definitions in Wikipedia (usu-
ally at the first paragraph). The characteristic of a
definition is that it has no prior context and most
of the noun phrases with a determiner are unique
or known to the general public. Heads of these
nouns phrases are likely to always appear with a
determiner.
Heads that tend to appear with a determiner
the help us to decide whether a determiner should
be added to a noun phrase. We add a determiner
using two constraints. We only insert a determiner
or an article, if the statistics indicate that head of
a noun phrase tends to have a determiner, or the
promotion of log probability is much higher than
the threshold. A similar constraint is also applied,
for deleting a determiner or an article.
5
the probabilities present in the formula are log probabil-
ities
6
the threshold for noun number module is 0.035 and 0.050
for word form module. These threshold were set empirically
after testing on development data
7
test data of the CoNLL-2013 shared task
8
the threshold for insertion is 0.035 and 0.040 for deletion
2.5 Preposition module
For preposition errors, we focus on handling two
types of errors: REPLACE and DELETE. A
preposition, which should be deleted from the
given sentence, is regarded as a DELETE error,
whereas for a preposition, which should be re-
placed with a more appropriate alternative, is re-
garded as a REPLACE error. In this work, we
correct the two types of errors based on the as-
sumption that the usage of preposition often de-
pends on the collocation relations with a verb or
noun. Therefore, we use the dependency relations
such as dobj and pobj, and prep to identify
the related words, and then we validate the usage
of prepositions, and correct the preposition errors.
A dependency-based model is proposed in this
paper to handle the preposition errors. The model
consists of two stages: detecting the possible
preposition errors and correcting the errors.
In the first stage, we use the Stanford depen-
dency parser (Klein and Manning, 2003) to extract
the dependency relations for each preposition. The
relation tuples, which contain the preposition, verb
or noun, and prepositional object. For example,
the tuple of verb-prep-object (listen, to, music),
or the tuple of noun-prep-object (point, of, view)
are extracted for validation. We identify a preposi-
tion containing as an error, if the tuple containing
the preposition does not occur in a reference list
built using a reference corpus. In order to resolve
the data sparseness and false alarm problems, we
need a sufficiently large list of validated tuples.
In this study, the reference tuple lists are gener-
ated from the Google Books Syntactic N-grams
(Goldberg and Orwant, 2013)
9
. For example, we
can find (come, to, end, 236864) and (lead, to, re-
sult, 57632) in the verb-preposition-object refer-
ence list. We have generated 21,773,752 different
dependency tuples for our purpose.
In the second stage, we attempt to correct all
potential preposition errors. At first, a list of can-
didate tuples is generated by substituting the orig-
inal preposition in the error tuple with alterna-
tive prepositions. For example, the generated can-
didate tuples for the error tuple (join, at, party)
will include (join, in, party), (join, on, party), etc.
On the other hand, the tuple, (join, party), which
9
Data sets available from http://
commondatastorage.googleapis.com/books/
syntactic-ngrams/index.html
93
deletes the preposition, is also taken into consid-
eration. All candidates are ranked according to
the frequency provided by the reference lists. The
preposition in the tuple with the highest frequency
is returned as the correction suggestion.
Figure 1: Sample annotated trigrams
Figure 2: Sample trigram group
Figure 3: Sample phrase translation for a trigram
group
2.6 Interactive errors module
This module uses a new method for correcting
serial grammatical errors in a given sentence in
learners writing. A statistical translation model is
generated to attempt to translate the input with se-
rial and interactive errors into a grammatical sen-
tence. The method involves automatically learn-
ing translation models based on Web-scale n-
gram. The model corrects trigrams containing se-
rial preposition-verb errors via translation. Eval-
uation on a set of sentences in a learner corpus
shows that the method corrects serial errors rea-
sonably well.
Consider an error sentence ?I have difficulty to
understand English.? The correct sentence for this
should be ?I have difficulty in understanding En-
glish.? It is hard to correct these two errors one by
one, since the errors are dependent on each other.
Intuitively, by identifying difficulty to understand
as containing serial errors and correct it to diffi-
culty in understanding, we can handle this kind of
problem more effectively.
First, we generate translation phrase table as
follows. We begin by selecting trigrams related to
serial errors and correction from Web 1T 5-gram.
Figure 1 shows some sample annotation trigrams.
Then, we group the selected trigrams by the first
and last word in the trigrams. See Figure 2 for a
sample VPV group of trigrams. Finally, we gener-
ate translation phrase table for each group. Figure
3 shows a sample translation phrase table.
At run time, we tag the input sentence with part
of speech information in order to find trigrams
that fit the type of serial errors. Then, we search
phrase table and generate translations for the
input phrases in a machine translation decoder to
produce a corrected sentence.
3 Experiment
Two types of trigram language models, ngram
model and recurrent neural network (RNN) model,
are used in correcting spelling, noun number, word
form, and determiner errors. We trained the ngram
language model on English Gigaword and BNC
corpus, using the SRILM tool (Stolcke, 2002).
We train the RNN model with RNNLM toolkit
(Mikolov et al., 2011). Complexity of training the
RNN language model is much higher, so we train
it on a smaller corpus, the British National Corpus
(BNC).
We used the Stanford Parser (Klein and Christo-
pher D. Manning, 2003) to obtain dependency re-
lations in the preposition module, and to obtain
POS tags for the word form module. The subject-
verb-agreement module also uses dependency re-
lations contained in test data. Dependency rela-
tions in Google Books Syntactic N-grams (Gold-
94
berg and Orwant, 2013) were also used to develop
our dendepency-based model in the preposition
module.
To assess the effectiveness of the proposed
method, we used the official training, develop-
ment, and test data of the CoNLL-2014 shared
task. On the test data, our system obtained the pre-
cision, recall and F
0.5
score of 0.351, 0.189, and
0.299. The following table shows the performance
breakdown by module.
Figure 4: The performance breakdown by module.
(Displayed in %)
In the spelling and hyphen module, candidates
from Aspell include words that only differ from
the original word in one character, s. Language
models are then used to choose the candidate with
highest probability as our correction. The module
therefore gives some corrections about noun num-
bers or subject-verb-agreement. As a result, some
corrections made by this module overlap with cor-
rections made by the noun numbers module and
the subject-verb-agreement module, which makes
the recall of correcting spelling and hyphen errors,
4.11%, overestimated.
4 Conclusion
In this work, we have built several modules for er-
ror detection and correction. For different types
of errors, we developed modules independently
using different features and thresholds. If mul-
tiple modules propose different corrections to a
word/phrase, the one proposed by the module with
higher precision will be chosen. Many avenues
for future work present themselves. We plan to
integrate modules in a more flexible way. When
faced with different corrections made by different
modules, the decision would better be based on
the confidence of each correction with a uniform
standard, but not on the confidence of modules.
Acknowledgments
We would like to acknowledge the funding sup-
ports from Delta Electronic Corp and National
Science Council (contract no: NSC 100-2511-S-
007-005-MY3), Taiwan. We are also thankful to
anonymous reviewers and the organizers of the
shared task.
References
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng
2012. NUS at the HOO 2012 Shared Task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, Association for
Computational Linguistics, June 7.
Daniel Dahlmeier and Hwee Tou Ng, 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2012).,568-672
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English. In
Proceedings of the 8th Workshop on Innovative Use
of NLP for Building Educational Applications(BEA
2013)
Yoav Goldberg and Jon Orwant 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Proceedings of the Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, GA, 2013.
Ross Israel, Joel Tetreault, and Martin Chodorow
2012. Correcting Comma Errors in Learner Essays,
and Restoring Commas in Newswire Text. In Pro-
ceeding of the 2012 Conference of the North Amer-
ica Chapter of the Association for Computational
Linguistics: Human Language Technologies,284-
294, Montreal Canada, June. Association for Com-
putational Linguistics
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, 423-430.
Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky 2011. Strategies
for Training Large Scale Neural Network Language
Models Proceedings of ASRU 2011
Andreas Stolcke 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, vol 2, 901-904
95
