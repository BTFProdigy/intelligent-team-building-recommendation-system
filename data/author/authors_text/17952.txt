Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 854?859,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Does the Phonology of L1 Show Up in L2 Texts?
Garrett Nicolai and Grzegorz Kondrak
Department of Computing Science
University of Alberta
{nicolai,gkondrak}@ualberta.ca
Abstract
The relative frequencies of character bi-
grams appear to contain much information
for predicting the first language (L1) of the
writer of a text in another language (L2).
Tsur and Rappoport (2007) interpret this
fact as evidence that word choice is dic-
tated by the phonology of L1. In order to
test their hypothesis, we design an algo-
rithm to identify the most discriminative
words and the corresponding character bi-
grams, and perform two experiments to
quantify their impact on the L1 identifica-
tion task. The results strongly suggest an
alternative explanation of the effectiveness
of character bigrams in identifying the na-
tive language of a writer.
1 Introduction
The task of Native Language Identification (NLI)
is to determine the first language of the writer of a
text in another language. In a ground-breaking pa-
per, Koppel et al (2005) propose a set of features
for this task: function words, character n-grams,
rare part-of-speech bigrams, and various types of
errors. They report 80% accuracy in classifying a
set of English texts into five L1 languages using a
multi-class linear SVM.
The First Shared Task on Native Language
Identification (Tetreault et al, 2013) attracted sub-
missions from 29 teams. The accuracy on a set
of English texts representing eleven L1 languages
ranged from 31% to 83%. Many types of fea-
tures were employed, including word length, sen-
tence length, paragraph length, document length,
sentence complexity, punctuation and capitaliza-
tion, cognates, dependency parses, topic mod-
els, word suffixes, collocations, function word n-
grams, skip-grams, word networks, Tree Substi-
tution Grammars, string kernels, cohesion, and
passive constructions (Abu-Jbara et al, 2013; Li,
2013; Brooke and Hirst, 2013; Cimino et al, 2013;
Daudaravicius, 2013; Goutte et al, 2013; Hender-
son et al, 2013; Hladka et al, 2013; Bykh et al,
2013; Lahiri and Mihalcea, 2013; Lynum, 2013;
Malmasi et al, 2013; Mizumoto et al, 2013; Nico-
lai et al, 2013; Popescu and Ionescu, 2013; Swan-
son, 2013; Tsvetkov et al, 2013). In particular,
word n-gram features appear to be particularly ef-
fective, as they were used by the most competitive
teams, including the one that achieved the highest
overall accuracy (Jarvis et al, 2013). Furthermore,
the most discriminative word n-grams often con-
tained the name of the native language, or coun-
tries where it is commonly spoken (Gebre et al,
2013; Malmasi et al, 2013; Nicolai et al, 2013).
We refer to such words as toponymic terms.
There is no doubt that the toponymic terms
are useful for increasing the NLI accuracy; how-
ever, from the psycho-linguistic perspective, we
are more interested in what characteristics of L1
show up in L2 texts. Clearly, L1 affects the L2
writing in general, and the choice of words in par-
ticular, but what is the role played by the phonol-
ogy? Tsur and Rappoport (2007) observe that lim-
iting the set of features to the relative frequency of
the 200 most frequent character bigrams yields a
respectable 66% accuracy on a 5-language classi-
fication task. The authors propose the following
hypothesis to explain this finding: ?the choice of
words [emphasis added] people make when writ-
ing in a second language is strongly influenced by
the phonology of their native language?. As the
orthography of alphabetic languages is at least par-
tially representative of the underlying phonology,
character bigrams may capture these phonological
preferences.
In this paper, we provide evidence against the
above hypothesis. We design an algorithm to iden-
tify the most discriminative words and the char-
acter bigrams that are indicative of such words,
854
and perform two experiments to quantify their im-
pact on the NLI task. The results of the first ex-
periment demonstrate that the removal of a rela-
tively small set of discriminative words from the
training data significantly impairs the accuracy of
a bigram-based classifier. The results of the sec-
ond experiment reveal that the most indicative bi-
grams are quite similar across different language
sets. We conclude that character bigrams are ef-
fective in determining L1 of the author because
they reflect differences in L2 word usage that are
unrelated to the phonology of L1.
2 Method
Tsur and Rappoport (2007) report that character
bigrams are more effective for the NLI task than
either unigrams or trigrams. We are interested in
identifying the character bigrams that are indica-
tive of the most discriminative words in order to
quantify their impact on the bigram-based classi-
fier.
We follow both Koppel et al (2005) and Tsur
and Rappoport (2007) in using a multi-class SVM
classifier for the NLI task. The classifier computes
a weight for each feature coupled with each L1
language by attempting to maximize the overall
accuracy on the training set. For example, if we
train the classifier using words as features, with
values representing their frequency relative to the
length of the document, the features correspond-
ing to the word China might receive the following
weights:
Arabic Chinese Hindi Japanese Telugu
-770 1720 -276 -254 -180
These weights indicate that the word provides
strong positive evidence for Chinese as L1, as op-
posed to the other four languages.
We propose to quantify the importance of each
word by converting its SVM feature weights into
a single score using the following formula:
WordScore
i
=
?
?
?
?
N
?
j=1
w
ij
2
where N is the number of languages, and w
ij
is the feature weight of word i in language j.
The formula assigns higher scores to words with
weights of high magnitude, either positive or neg-
ative. We use the Euclidean norm rather than the
Algorithm 1 Computing the scores of words and
bigrams in the data.
1: create list of words in training data
2: train SVM using words as features
3: for all words i do
4: WordScore
i
=
?
?
N
j=1
w
ij
2
5: end for
6: sort words by WordScore
7: NormValue = WordScore
200
8: create list of 200 most frequent bigrams
9: for bigrams k = 1 to 200 do
10: BigramScore
k
=
?
k?i
WordScore
i
NormV alue
11: end for
12: sort character bigrams by BigramScore
sum of raw weights because we are interested in
the discriminative power of the words.
We normalize the word scores by dividing them
by the score of the 200th word. Consequently,
only the top 200 words have scores greater than
or equal to 1.0. For our previous example, the
200
th
word has a word score of 1493, while China
has a word score of 1930, which is normalized to
1930/1493 = 1.29. On the other hand, the 1000
th
word gets a normalized score of 0.43.
In order to identify the bigrams that are indica-
tive of the most discriminative words, we promote
those that appear in the high-scoring words, and
downgrade those that appear in the low-scoring
words. Some bigrams that appear often in the
high-scoring words may be very common. For ex-
ample, the bigram an occurs in words like Japan,
German, and Italian, but also by itself as a deter-
miner, as an adjectival suffix, and as part of the
conjunction and. Therefore, we calculate the im-
portance score for each character bigram by multi-
plying the scores of each word in which the bigram
occurs.
Algorithm 1 summarizes our method of identi-
fying the discriminative words and indicative char-
acter bigrams. In line 2, we train an SVM on the
words encountered in the training data. In lines 3
and 4, we assign the Euclidean norm of the weight
vector of each word as its score. Starting in line
7, we determine which character bigrams are rep-
resentative of high scoring words. In line 10, we
calculate the bigram scores.
855
3 Experiments
In this section, we describe two experiments aimed
at quantifying the importance of the discriminative
words and the indicative character bigrams that are
identified by Algorithm 1.
3.1 Data
We use two different NLI corpora. We follow the
setup of Tsur and Rappoport (2007) by extracting
two sets, denoted I1 and I2 (Table 1), from the
International Corpus of Learner English (ICLE),
Version 2 (Granger et al, 2009). Each set con-
sists of 238 documents per language, randomly se-
lected from the ICLE corpus. Each of the docu-
ments corresponds to a different author, and con-
tains between 500 and 1000 words. We follow the
methodology of the paper in performing 10-fold
cross-validation on the sets of languages used by
the authors.
For the development of the method described in
Section 2, we used a different corpus, namely the
TOEFL Non-Native English Corpus (Blanchard et
al., 2013). It consists of essays written by native
speakers of eleven languages, divided into three
English proficiency levels. In order to maintain
consistency with the ICLE sets, we extracted three
sets of five languages apiece (Table 1), with each
set including both related and unrelated languages:
European languages that use Latin script (T1),
non-European languages that use non-Latin scripts
(T2), and a mixture of both types (T3). Each sub-
corpus was divided into a training set of 80%, and
development and test sets of 10% each. The train-
ing sets are composed of approximately 700 docu-
ments per language, with an average length of 350
words per document. There are over 5000 word
types per language, and over 1000 character bi-
grams in total. The test sets include approximately
90 documents per language. We report results on
the test sets, after training on both the training and
development sets.
3.2 Setup
We replicate the experiments of Tsur and Rap-
poport (2007) by limiting the features to the 200
most frequent character bigrams.
1
The feature val-
ues are set to the frequency of the character bi-
1
Our development experiments suggest that using the full
set of bigrams results in a higher accuracy of a bigram-based
classifier. However, we limit the set of features to the 200
most frequent bigrams for the sake of consistency with previ-
ous work.
ICLE:
I1 Bulgarian Czech French Russian Spanish
I2 Czech Dutch Italian Russian Spanish
TOEFL:
T1 French German Italian Spanish Turkish
T2 Arabic Chinese Hindi Japanese Telugu
T3 French German Japanese Korean Telugu
Table 1: The L1 language sets.
grams normalized by the length of the document.
We use these feature vectors as input to the SVM-
Multiclass classifier (Joachims, 1999). The results
are shown in the Baseline column of Table 2.
3.3 Discriminative Words
The objective of the first experiment is to quantify
the influence of the most discriminative words on
the accuracy of the bigram-based classifier. Using
Algorithm 1, we identify the 100 most discrimi-
native words, and remove them from the training
data. The bigram counts are then recalculated, and
the new 200 most frequent bigrams are used as
features for the character-level SVM. Note that the
number of the features in the classifier remains un-
changed.
The results are shown in the Discriminative
Words column of Table 2. We see a statistically
significant drop in the accuracy of the classifier
with respect to the baseline in all sets except T3.
The words that are identified as the most discrim-
inative include function words, punctuation, very
common content words, and the toponymic terms.
The 10 highest scoring words from T1 are: indeed,
often, statement, : (colon), question, instance, . . .
(ellipsis), opinion, conclude, and however. In ad-
dition, France, Turkey, Italian, Germany, and Italy
are all found among the top 70 words.
For comparison, we attempt to quantify the ef-
fect of removing the same number of randomly-
selected words from the training data. Specifically,
we discard all tokens that correspond to 100 word
types that have the same or slightly higher fre-
quency as the discriminative words. The results
are shown in the Random Words column of Ta-
ble 2. The decrease is much smaller for I1, I2, and
T1, while the accuracy actually increases for T2
and T3. This illustrates the impact that the most
discriminative words have on the bigram-based
classifier beyond simple reduction in the amount
of the training data.
856
Set Baseline Random Discriminative Random Indicative
Words Words Bigrams Bigrams
I1 67.5 ?0.2 ?3.6 ?1.0 ?2.2
I2 66.9 ?2.5 ?5.5 ?0.7 ?2.8
T1 60.7 ?3.3 ?7.7 ?2.5 ?3.9
T2 60.6 +0.5 ?3.8 ?1.1 ?5.9
T3 62.2 +0.3 ?0.0 ?0.5 ?4.1
Table 2: The impact of subsets of word types and bigram features on the accuracy of a bigram-based NLI
classifier.
3.4 Indicative Bigrams
Using Algorithm 1, we identify the top 20 charac-
ter bigrams, and replace them with randomly se-
lected bigrams. The results of this experiment are
reported in the Indicative Bigrams column of Ta-
ble 2. It is to be expected that the replacement of
any 20 of the top bigrams with 20 less useful bi-
grams will result in some drop in accuracy, regard-
less of which bigrams are chosen for replacement.
For comparison, the Random Bigrams column of
Table 2 shows the mean accuracy over 100 trials
obtained when 20 bigrams randomly selected from
the set of 200 bigrams are replaced with random
bigrams from outside of the set.
The results indicate that our algorithm indeed
identifies 20 bigrams that are on average more im-
portant than the other 180 bigrams. What is really
striking is that the sets of 20 indicative character
bigrams overlap substantially across different sets.
Table 3 shows 17 bigrams that are common across
the three TOEFL corpora, ordered by their score,
together with some of the highly scored words in
which they occur. Four of the bigrams consist
of punctuation marks and a space.
2
The remain-
ing bigrams indicate function words, toponymic
terms like Germany, and frequent content words
like take and new.
The situation is similar in the ICLE sets, where
likewise 17 out of 20 bigrams are common. The
inter-fold overlap is even greater, with 19 out of
20 bigrams appearing in each of the 10 folds. In
particular, the bigrams fr and bu can be traced
to both the function words from and but, and the
presence of French and Bulgarian in I1. However,
the fact that the two bigrams are also on the list for
2
It appears that only the relatively low frequency of most
of the punctuation bigrams prevents them from dominating
the sets of the indicative bigrams. When using all bigrams
instead of the top 200, the majority of the indicative bigrams
contain punctuation.
Bigram Words
,
,
.
.
u you Telugu
f of
ny any many Germany
yo you your
w now how
i I
y you your
ew new knew
kn know knew
ey they Turkey
wh what why where etc.
of of
ak make take
Table 3: The most indicative character bigrams in
the TOEFL corpus (sorted by score).
the I2 set, which does not include these languages,
suggests that their importance is mostly due to the
function words.
3.5 Discussion
In the first experiment, we showed that the re-
moval of the 100 most discriminative words from
the training data results in a significant drop in the
accuracy of the classifier that is based exclusively
on character bigrams. If the hypothesis of Tsur
and Rappoport (2007) was true, this should not be
the case, as the phonology of L1 would influence
the choice of words across the lexicon.
In the second experiment, we found that the ma-
jority of the most indicative character bigrams are
shared among different language sets. The bi-
grams appear to reflect primarily high-frequency
function words. If the hypothesis was true, this
857
should not be the case, as the diverse L1 phonolo-
gies would induce different sets of bigrams. In
fact, the highest scoring bigrams reflect punctu-
ation patterns, which have little to do with word
choice.
4 Conclusion
We have provided experimental evidence against
the hypothesis that the phonology of L1 strongly
affects the choice of words in L2. We showed
that a small set of high-frequency function words
have disproportionate influence on the accuracy of
a bigram-based NLI classifier, and that the major-
ity of the indicative bigrams appear to be indepen-
dent of L1. This suggests an alternative explana-
tion of the effectiveness of a bigram-based classi-
fier in identifying the native language of a writer
? that the character bigrams simply mirror differ-
ences in the word usage rather than the phonology
of L1.
Our explanation concurs with the findings of
Daland (2013) that unigram frequency differences
in certain types of phonological segments between
child-directed and adult-directed speech are due to
a small number of word types, such as you, what,
and want, rather than to any general phonological
preferences. He argues that the relative frequency
of sounds in speech is driven by the relative fre-
quency of words. In a similar vein, Koppel et al
(2005) see the usefulness of character n-grams as
?simply an artifact of variable usage of particular
words, which in turn might be the result of differ-
ent thematic preferences,? or as a reflection of the
L1 orthography.
We conclude by noting that our experimental re-
sults do not imply that the phonology of L1 has ab-
solutely no influence on L2 writing. Rather, they
show that the evidence from the Native Language
Identification task has so far been inconclusive in
this regard.
Acknowledgments
We thank the participants and the organizers of
the shared task on NLI at the BEA8 workshop for
sharing their reflections on the task. We also thank
an anonymous reviewer for pointing out the study
of Daland (2013).
This research was supported by the Natural
Sciences and Engineering Research Council of
Canada and the Alberta Innovates Technology Fu-
tures.
References
Amjad Abu-Jbara, Rahul Jha, Eric Morley, and
Dragomir Radev. 2013. Experimental results on
the native language identification shared task. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 82?88.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Julian Brooke and Graeme Hirst. 2013. Using other
learner corpora in the 2013 NLI shared task. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 188?196.
Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, and
Detmar Meurers. 2013. Combining shallow and
linguistically motivated features in native language
identification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 197?206.
Andrea Cimino, Felice Dell?Orletta, Giulia Venturi,
and Simonetta Montemagni. 2013. Linguistic pro-
filing based on general?purpose features and na-
tive language identification. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 207?215.
Robert Daland. 2013. Variation in the input: a case
study of manner class frequencies. Journal of Child
Language, 40(5):1091?1122.
Vidas Daudaravicius. 2013. VTEX system descrip-
tion for the NLI 2013 shared task. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 89?95.
Binyam Gebrekidan Gebre, Marcos Zampieri, Peter
Wittenburg, and Tom Heskes. 2013. Improving na-
tive language identification with TF-IDF weighting.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 216?223.
Cyril Goutte, Serge L?eger, and Marine Carpuat. 2013.
Feature space selection and combination for na-
tive language identification. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 96?100.
Sylvaine Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. INTERNATIONAL
CORPUS OF LEARNER ENGLISH: VERSION 2.
John Henderson, Guido Zarrella, Craig Pfeifer, and
John D. Burger. 2013. Discriminating non-native
English with 350 words. In Proceedings of the
Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 101?110.
858
Barbora Hladka, Martin Holub, and Vincent Kriz.
2013. Feature engineering in the NLI shared task
2013: Charles University submission report. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 232?241.
Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013.
Maximizing classification accuracy in native lan-
guage identification. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 111?118.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184. MIT Press.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 624?
628, Chicago, IL. ACM.
Shibamouli Lahiri and Rada Mihalcea. 2013. Using n-
gram and word network features for native language
identification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 251?259.
Baoli Li. 2013. Recognizing English learners. na-
tive language from their writings. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 119?123.
Andr?e Lynum. 2013. Native language identification
using large scale lexical features. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 266?269.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. NLI shared task 2013: MQ submis-
sion. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 124?133.
Tomoya Mizumoto, Yuta Hayashibe, Keisuke Sak-
aguchi, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at the NLI 2013 shared task. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 134?139.
Garrett Nicolai, Bradley Hauer, Mohammad Salameh,
Lei Yao, and Grzegorz Kondrak. 2013. Cognate
and misspelling features for natural language iden-
tification. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 140?145.
Marius Popescu and Radu Tudor Ionescu. 2013. The
story of the characters, the DNA and the native lan-
guage. In Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 270?278.
Ben Swanson. 2013. Exploring syntactic representa-
tions for native language identification. In Proceed-
ings of the Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
146?151.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A Report on the First Native Language Iden-
tification Shared Task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications.
Oren Tsur and Ari Rappoport. 2007. Using Classi-
fier Features for Studying the Effect of Native Lan-
guage on the Choice of Written Second Language
Words. In Proceedings of the Workshop on Cog-
nitive Aspects of Computational Language Acquisi-
tion, pages 9?16, Prague, Czech Republic.
Yulia Tsvetkov, Naama Twitto, Nathan Schneider,
Noam Ordan, Manaal Faruqui, Victor Chahuneau,
Shuly Wintner, and Chris Dyer. 2013. Identifying
the L1 of non-native writers: the CMU-Haifa sys-
tem. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 279?287.
859
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 140?145,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Cognate and Misspelling Features for Natural Language Identification
Garrett Nicolai, Bradley Hauer, Mohammad Salameh, Lei Yao, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, Canada
{nicolai,bmhauer,msalameh,lyao1,gkondrak}@ualberta.ca
Abstract
We apply Support Vector Machines to differ-
entiate between 11 native languages in the
2013 Native Language Identification Shared
Task. We expand a set of common language
identification features to include cognate inter-
ference and spelling mistakes. Our best results
are obtained with a classifier which includes
both the cognate and the misspelling features,
as well as word unigrams, word bigrams, char-
acter bigrams, and syntax production rules.
1 Introduction
As the world becomes more inter-connected, an in-
creasing number of people devote effort to learn-
ing one of the languages that are dominant in the
global community. English, in particular, is stud-
ied in many countries across the globe. The goal is
often related to increasing one?s chances to obtain
employment and succeed professionally. The lan-
guage of work-place communication is often not a
speaker?s native language (L1) but their second lan-
guage (L2). Speakers and writers of the same L1
can sometimes be identified by similar L2 errors.
The weak Contrastive Analysis Hypothesis (Jarvis
and Crossley, 2012) suggests that these errors may
be a result of L1 causing linguistic interference; that
is, common tendencies of a speaker?s L1 are super-
imposed onto their L2. Native Language Identifi-
cation, or NLI, is an attempt to exploit these errors
in order to identify the L1 of the speaker from texts
written in L2.
Our group at the University of Alberta was unfa-
miliar with the NLI research prior to the announce-
ment of a shared task (Tetreault et al, 2013). How-
ever, we saw it as an opportunity to apply our exper-
tise in character-level NLP to a new task. Our goal
was to propose novel features, and to combine them
with other features that have been previously shown
to work well for language identification.
In the end, we managed to define two feature sets
that are based on spelling errors made by L2 writers.
Cognate features relate a spelling mistake to cognate
interference with the writer?s L1. Misspelling fea-
tures identify common mistakes that may be indica-
tive of the writer?s L1. Both feature sets are meant
to exploit the Contrastive Analysis Hypothesis, and
benefit from the writer?s L1 influence on their L2
writing.
2 Related Work
Koppel et al (2005b) approach the NLI task using
Support Vector Machines (SVMs). They experi-
ment with features such as function-word unigrams,
rare part-of-speech bigrams, character bigrams, and
spelling and syntax errors. They report 80% accu-
racy across 5 languages. We further investigate the
role of word unigrams and spelling errors in native
language identification. We consider not only func-
tion words, but also content words, as well as word
bigrams. We also process spell-checking errors with
a text aligner to find common spelling errors among
writers with the same L1.
Tsur and Rappoport (2007) also use SVMs on the
NLI task, but limit their feature set to character bi-
grams. They report 65% accuracy on 5 languages,
and hypothesize that the choice of words when writ-
ing in L2 is strongly affected by the phonology of
140
their L1. We also consider character bigrams in our
feature set, but combine them with a number of other
features.
Wong and Dras (2011) opt for a maximum en-
tropy classifier, and focus more on syntax errors than
lexical errors. They find that syntax tree production
rules help their classifier in a seven language clas-
sification task. They only consider non-lexicalized
rules, and rules with function words. In contrast, we
consider both lexicalized and non-lexicalized pro-
duction rules, and we include content words.
Bergsma et al (2012) consider the NLI task as a
sub-task of the authorship attribution task. They fo-
cus on the following three questions: (1) whether the
native language of the writer of a paper is English,
(2) what is the gender of the writer, and (3) whether
a paper is a conference or workshop paper. The au-
thors conclude that syntax aids the native language
classification task, further motivating our decision to
use part-of-speech n-grams and production rules as
features for our classifier. Furthermore, the authors
suggest normalizing text to reduce sparsity, and im-
plement several meta-features that they claim aid the
classification.
3 Classifier
Following Koppel et al (2005b) and others, we
perform classification with SVMs. We chose the
SVM-Multiclass package, a version of the SVM-
light package(Joachims, 1999) specifically modified
for multi-class classification problems. We use a lin-
ear kernel, and two hyperparameters that were tuned
on the development set: the c soft-margin regular-
ization parameter, which measures the tradeoff be-
tween training error and the size of the margin, and
, which is used as a stopping criterion for the SVM.
C was tuned to a value of 5000, and epsilon to a
value of 0.1.
4 Features
As features for our SVM, we used a combination of
features common in the literature and new features
developed specifically for this task. The features are
listed in the following section.
4.1 Word n-grams
Following previous work, we use word n-grams as
the primary feature set. We normalize the text before
selecting n-grams using the method of Bergsma et
al. (2012). In particular, all digits are replaced with
a representative ?0? character; for example, ?22? and
?97? are both represented as ?00?. However, unlike
Koppel et al (2005b), we incorporate word bigrams
in addition to word unigrams, and utilize both func-
tion words and content words.
4.1.1 Function Words
Using a list of 295 common function words, we
reduce each document to a vector of values repre-
senting their presence or absence in a document. All
other tokens in the document are ignored. When
constructing vectors of bigrams, any word that is not
on the list of function words is converted to a place-
holder token. Thus, most of our function-word bi-
grams consist of a single function word preceded or
followed by a placeholder token.
4.1.2 Content Words
Other than the normalization mentioned in Sec-
tion 4.1, all tokens in the documents are allowed as
possible word unigrams. No spelling correction is
used for reducing the number of word n-grams. Fur-
thermore, we consider all token unigrams that occur
in the training data, regardless of their frequency.
An early concern with token bigrams was that
they were both large in number, and sparse. In an
attempt to reduce the number of bigrams, we con-
ducted experiments on the development set with dif-
ferent numbers of bigrams that exhibited the highest
information gain. It was found that using all combi-
nations of word bigrams improved predictive accu-
racy the most, and did not lead to a significant cost
to the SVM. Thus, for experiments on the test set, all
token bigrams that were encountered in the training
set were used as features.
4.2 Character n-grams
Following Tetreault et al (2012), we utilize all char-
acter bigrams that occur in the training data, rather
than only the most frequent ones. However, where
the literature uses either binary indicators or relative
frequency of bigrams as features, we use a modi-
fied form of the relative frequency in our classifier.
141
In a pre-processing step, we calculate the average
frequency of each character bigram across all train-
ing documents. Then, during feature extraction, we
again determine the relative frequency of each char-
acter bigram across documents. We then use bi-
nary features to indicate if the frequency of a bigram
is higher than the average frequency. Experiments
conducted on the development set showed that al-
though this modified frequency was out-performed
by the original relative frequency on its own, our
method performed better when further features were
incorporated into the classifier.
4.3 Part-of-speech n-grams
All documents are tagged with POS tags using the
Stanford parser (Klein and Manning, 2003), From
the documents in the training data, a list of all POS
bigrams was generated, and documents were repre-
sented by binary indicators of the presence or ab-
sence of a bigram in the document. As with char-
acter bigrams, we did not simply use the most com-
mon bigrams, but rather considered all bigrams that
appeared in the training data.
4.4 Syntax Production Rules
After generating syntactic parse trees with the Stan-
ford Parser. we extract all possible production rules
from each document, including lexicalized rules.
The features are binary; if a production rule occurs
in an essay, its value is set to 1, and 0 otherwise. For
each language, we use information gain for feature
selection to select the most informative production
rules as suggested by Wong and Dras (2011). Ex-
periments on the development set indicated that the
information gain is superior to raw frequency for the
purpose of syntax feature selection. Since the accu-
racy increased as we added more production rules,
the feature set for final testing includes all produc-
tion rules encountered in the training set. The ma-
jority of the rules are of the form POS? terminal.
We hypothesized that most of the information con-
tained in these rules may be already captured by the
word unigram features. However, experiments on
the development set suggested that the lexicalized
rules contain information that is not captured by the
unigrams, as they led to an increase in predictive ac-
curacy.
4.5 Spelling Errors
Koppel et al (2005a) suggested spelling errors
could be helpful as writers might be affected by
the spelling convention in their native languages.
Moreover, spelling errors also reflect the pronun-
ciation characteristics of the writers? native lan-
guages. They identified 8 types of spelling errors
and collected the statistics of each error type as
their features. Unlike their approach, we focus on
the specific spelling errors made by the writers be-
cause 8 types may be insufficient to distinguish the
spelling characteristics of writers from 11 differ-
ent languages. We extract the spelling error fea-
tures from character-level alignments between the
misspelled word and the intended word. For ex-
ample, if the word abstract is identified as the in-
tended spelling of a misspelling abustruct, the char-
acter alignments are as follows:
a bu s t ru ct
| | | | | |
a b s t ra ct
Only the alignments of the misspelled parts, i.e.
(bu,b) and (ru,ra) in this case, are used as fea-
tures. The spell-checker we use is aspell1, and the
character-level alignments are generated by m2m-
aligner (Jiampojamarn et al, 2007).
4.6 Cognate Interference
Cognates are words that share their linguistic origin.
For example, English become and German bekom-
men have evolved from the same word in a com-
mon ancestor language. Other cognates are words
that have been transfered between languages; for ex-
ample, English system comes from the Greek word
??????? via Latin and French. On average, pairs
of cognates exhibit higher orthographic similarity
than unrelated translation pairs (Kondrak, 2013).
Cognate interference may cause an L1-speaker
to use a cognate word instead of a correct English
translation (for example, become instead of get).
Another instance of cognate interference is mis-
spelling of an English word under the influence of
the L1 spelling (Table 1).
We aim to detect cognate interference by identi-
fying the cases where the cognate word is closer to
1http://aspell.net
142
Misspelling Intended Cognate
developped developed developpe? (Fre)
exemple example exemple (Fre)
organisation organization organisation (Ger)
conzentrated concentrated konzentrierte (Ger)
comercial commercial comercial (Spa)
sistem system sistema (Spa)
Table 1: Examples of cognate interference in the data.
the misspelling than to the intended word (Figure 1).
We define one feature to represent each language L,
for which we could find a downloadable bilingual
English-L dictionary. We use the following algo-
rithm:
1. For each misspelled English word m found in
a document, identify the most likely intended
word e using a spell-checking program.
2. For each language L:
(a) Look up the translation f of the intended
word e in language L.
(b) Compute the orthographic edit distance D
between the words.
(c) If D(e, f) < t then f is assumed to be a
cognate of e.
(d) If f is a cognate and D(m, f) < D(e, f)
then we consider it as a clue that L = L1.
We use a simple method of computing ortho-
graphic distance with threshold t = 0.58 defined
as the baseline method by Bergsma and Kondrak
(2007). However, more accurate methods of cog-
nate identification discussed in that paper could also
be used.
Misspellings can betray cognate interference even
if the misspelled word has no direct cognate in
language L1. For example, a Spanish speaker
might spell the word quick as cuick because of
the existence of numerous cognates such as ques-
tion/cuestio?n. Our misspelling features can detect
such phenomena at the character level; in this case,
qu:cu corresponds to an individual misspelling fea-
ture.
4.7 Meta-features
We included a number of document-specific meta-
features as suggested by Bergsma et al (2012): the
conzentratedconcentrated konzentrierte0.30.4
Figure 1: A cognate word influencing the spelling.
average number of words per sentence, the average
word length, as well as the total number of char-
acters, words, and sentences in a document. We
reasoned that writers from certain linguistic back-
grounds may prefer many short sentences, while
other writers may prefer fewer but longer sentences.
Similarly, a particular linguistic background may in-
fluence the preference for shorter or longer words.
5 Results
The dataset used for experiments was the TOEFL11
Non-Native English Corpus (Blanchard et al, 2013).
The dataset was split into three smaller datasets: the
Training set, consisting of 9900 essays evenly dis-
tributed across 9 languages, the Development set,
which contained a further 1100 essays, and the Test
set, which also contained 1100 essays. As the data
had a staggered release, we used the data differently.
We further split the Training set, with a split of 80%
for training, and 10% for development and testing.
We then used the Development set as a held-out test
set. For held-out testing, the classifier was trained on
all data in the Training set, and for final testing, the
classifier was trained on all data in both the Training
and Development sets.
We used four different combinations of features
for our task submissions. The results are shown in
Table 2. We include the following accuracy values:
(1) the results that we obtained on the Development
set before the Test data release, (2) the official Test
set results provided by the organizers (Tetreault et
al., 2013), (3) the actual Test set results, and (4) the
mean cross-validation results (for submissions 1 and
3). The difference between the official and the ac-
tual Test set results is attributed to two mistakes in
our submissions. In submission 1, the feature lists
used for training and testing did not match. In sub-
missions 3 and 4, only non-lexicalized syntax pro-
duction rules were used, whereas our intention was
to use all of them.
143
No. Features Dev Org Test CV
1 Base 82.0 61.2 80.4 58.2
2 ? cont. words 67.4 68.7 68.7 ?
3 + char 81.4 80.3 81.7 58.5
4 + char + meta 81.2 80.0 80.8 ?
Table 2: Accuracy of our submissions.
All four submissions used the following base
combination of features:
? word unigrams
? word bigrams
? error alignments
? syntax production rules
? word-level cognate interference features
In addition, submission 3 includes character bi-
grams, while submission 4 includes both character
bigrams and meta-features. In submission 2, only
function words are used, with the exclusion of con-
tent words.
Our best submission, which achieves 81.73% ac-
curacy on the Test set, includes all features discussed
in Section 4 except POS bigrams. Early tests in-
dicated that any gains obtained with POS bigrams
were absorbed by the production rules, so they were
excluded form the final experiments. Character bi-
grams help on the Test set but not on the Devel-
opment set. The meta-features decrease accuracy
on both sets. Finally, the content words dramati-
cally improve accuracy. The reason we included a
submission which did not use content words is that
it is a common practice in previous work. In our
analysis of the data, we found content words that
were highly indicative of the language of the writer.
Particularly, words and phrases which contained the
speaker?s home country were useful in predicting the
language. It should be noted that this correspon-
dence may be dependent upon the prompt given to
the writer. Furthermore, it may lead to false posi-
tives for L1 speakers who live in multi-lingual coun-
tries.
5.1 Confusion Matrix
We present the confusion matrix for our best submis-
sion in Table 5.1. The highest number of incorrect
A C F G H I J K S T Tu
ARA 83 0 0 0 2 2 2 1 4 5 1
CHI 1 81 2 0 1 0 8 6 1 0 0
FRE 6 0 82 2 1 3 0 0 1 0 5
GER 1 0 0 90 1 1 1 0 2 0 4
HIN 1 2 2 0 76 1 0 0 0 16 2
ITA 1 1 0 1 0 89 1 0 5 1 1
JPN 2 1 1 1 0 1 86 6 0 0 2
KOR 1 8 0 0 0 0 11 78 0 1 1
SPA 2 2 7 0 3 5 0 2 75 0 4
TEL 2 0 0 2 15 0 0 0 1 80 0
TUR 4 3 2 1 0 1 1 5 2 2 79
Table 3: Confusion Matrix for our best classifier.
Features Test
Full system 81.7
w/o error alignments 81.3
w/o word unigrams 81.1
w/o cognate features 81.0
w/o production rules 80.6
w/o character bigrams 80.4
w/o word bigrams 76.7
Table 4: Accuracy of various feature combinations.
classifications are between languages that are either
linguistically or culturally related (Jarvis and Cross-
ley, 2012). For example, Korean is often misclassi-
fied as Japanese or Chinese. The two languages are
not linguistically related to Korean, but both have
historically had cultural ties with Korean. Likewise,
while Hindi and Telugu are not related linguistically,
they are both spoken in the same geographic area,
and speakers are likely to have contact with each
other.
5.2 Ablation Study
Table 4 shows the results of an ablation experiment
on our best-performing submission. The word bi-
grams contribute the most to the classification; their
removal increases the relative error rate by 27%. The
word unigrams contribute much less., This is un-
surprising, as much of the information contained in
the word unigrams is also contained in the bigrams.
The remaining features are also useful. In particu-
lar, our cognate interference features, despite apply-
ing to only 4 of 11 languages, reduce errors by about
4%.
144
6 Conclusions and Future Work
We have described the system that we have devel-
oped for the NLI 2013 Shared Task. The system
combines features that are prevalent in the litera-
ture with our own novel character-level spelling fea-
tures and word cognate interference features. Most
of the features that we experimented with appear
to increase the overall accuracy, which contradicts
the view that simple bag-of-words usually perform
better than more complex feature sets (Sebastiani,
2002).
Our cognate features can be expanded by includ-
ing languages that do not use the Latin script, such
as Russian and Greek, as demonstrated by Bergsma
and Kondrak (2007). We utilized bilingual dictio-
naries representing only four of the eleven languages
in this task2; yet our cognate interference features
still improved classifier accuracy. With more re-
sources and with better methods of cognate identi-
fication, the cognate features have the potential to
further contribute to native language identification.
Our error-alignment features can likewise be fur-
ther investigated in the future. Currently, after ana-
lyzing texts with a spell-checker, we automatically
accept the first suggestion as the correct one. In
many cases, this leads to faulty corrections, and mis-
leading alignments. By using context sensitive spell-
checking, we can choose better corrections, and ob-
tain information which improves classification.
This shared task was a wonderful introduction
to Native Language Identification, and an excellent
learning experience for members of our group,
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 656?663.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 327?337,
Montre?al, Canada.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
2French, Spanish, German, and Italian.
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Scott Jarvis and Scott Crossley, editors. 2012. Approach-
ing Language Transfer Through Text Classification:
Explorations in the Detection-based Approach, vol-
ume 64. Multilingual Matters Limited, Bristol, UK.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and HMMs to letter-to-phoneme conversion. In Pro-
ceedings of NAACL-HLT, pages 372?379.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in ker-
nel methods, pages 169?184. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430.
Grzegorz Kondrak. 2013. Word similarity, cognation,
and translational equivalence. To appear.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1?47.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, pages 9?16, Prague,
Czech Republic.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK.
145
