:Incr en ntal, Event- oneeptua fization.:and 
Natural Language Generation in Monitoring Environments 
Markus GUHE, Christopher HABEL, Heike TAPPE 
Research Group Knowledge and Language Processing (WSV), 
Department of Informatics, University of Hamburg 
Vogt-KGlln-Strage 30 
. . . . . . . . . . .  :225.27..iJamburg,.Gerrna~ay, D,22527 - 
{guhe, habel, tappe}@informatik.uni-hamburg.de 
Abstract 
In this paper we present a psycholinguistically 
motivated architecture and its prototypical 
implementation for an incremental conceptu- 
alizer, which monitors dynamic hanges in the 
world and simultaneously generates warnings 
for (possibly) safety-critical developments. It 
does so by conceptualizing events and build- 
ing up a hierarchical knowledge representa- 
tion of the perceived states of affairs. If it 
detects a safety problem, it selects suitable 
elements from the representation for a warn- 
ing, brings them into an appropriate order, and 
generates incremental preverbal messages 
(propositional structures) from them, which 
can be taken by a subsequent component to 
encode them linguistically. 
1 Introduction 
Systems that generate natural language descrip- 
tions of what happens in a dynamically changing 
world can be improved substantially by working 
incrementally. Incrementality enhances the overall 
quality of the systems for three reasons: (1) The 
dynamic nature of a continuous stream of input in- 
formation can be handled more directly and, there- 
fore, easier. (2) Incremental systems are capable 
of producing fluent speech, i.e. speech without ar- 
tificial auditory gaps. (3) Parallelism that comes 
with incrementality makes better use of the avail- 
able resources. 
Furthermore, Reiter (1994), who reviews the 
? architecture of some models of natural language 
generation, shows that psycholinguistic and engi- 
neering approaches often result in systems, which 
are similar in crucial respects. In this paper we 
ground on two of these common aspects, namely 
the distinction between what-to-say and how-to- 
say (De Smedt, Horacek & Zock, 1996) and the 
use of a pipeline architecture, which divides the 
generation process "into multiple modules, with 
information flowing in a 'pipeline' fashion from 
one module to the next" (Reiter, 1994). Reiter 
states that these architectures do not require mod- 
ules to work in parallel; if parallelism is used one 
has an incremental model, cf. De Smedt & Kern- 
pen (I 987), Ferreira (1996). 
The primary research topic of the ConcEv ~ 
project is the what-to-say component, in which the 
content of utterances i planned (Reiter's content 
planning, in contrast to the language specific sen- 
tence planning component). We use the terminol- 
ogy of Levelt (1989), who calls the first 
component the conceptualizer, the second the 
formulator. These modules interact via preverbal 
messages, which are propositional, non-verbal 
representations of the utterance built up by the 
conceptualizer. They are transformed by the for- 
mulator into linguistic structures for spoken or 
written output. Besides considering high level 
communicative goals (macroplanning), which are 
in the focus of most computational pproaches to 
the what-to-say component, e.g. De Smedt & 
Kempen (1987), McKeown (1985), Hovy (1993), 
Chu-Carroll & Carberry (1998), Radev & McKe- 
own (1998), the type of information to be verbal- 
ized also determines the processes of  
conceptualization the level of microplanning, 
cf. Levelt (1989). Thus, the traditional top-down 
approaches have to be combined with bottom-up 
data-driven approaches of text planning (Marcu, 
1997~. The conceptualizer that is described in de- 
tail in section 3 fits the pipeline architecture on a 
coarse level, but integrates on finer levels the 
ideas of functional modules (Cahill et al, 1999). 
In the present paper we focus on the task of 
I ConcEv (Conceptualizing Events) is supported by the 
DFG (German Science Foundation) in the priority pro- 
gram 'Language Production' under grant Ha-1237/10 
to Christopher Habel. 
85 
generating verbal descriptions of  continuously in-. 
coming input from a changing physical world (see 
section 2, for similar settings cf. Neumann & No- 
vak (1983) and Andr6, Herzog & Rist (1988)). 
This specific task requires an incremental pipeline 
architecture--as there are certain steps that have 
to be carried out in a specific order--and, addi- 
tionally, these steps can be organized in such a 
.... insights .into _psy.cho.lJnguistic.aspects _of natural 
language processing. Our implementation thus 
simulates aspects of behavior, e.g. the effects dif- 
fering time pressure has on verbalizations. 
2 Conceptual iz ing Events 
If the system has to produce descriptions about 
what it 'sees', the main conceptual task is building 
way that a sequence of clear-cut modules arises, 
in aralle\[ .P~allel rocessin has up conceptual entities representing spatio- 
? .which can work ..p . . . . .  :~  " " " g -~ ......... ~eml6oral- constellations -of'theexternai world, i:e. 
the advantage that several tasks can be done si- 
multaneously; thus, while utterances are generated 
for some input, subsequent input can already be 
taken in and processed. 
Simultaneous conceptualization can be used as 
the basis of systems producing verbal messages 
when they detect a (possibly) safety-critical de- 
velopment while monitoring a safety-critical sys- 
tem, like intensive care units, nuclear power 
plants, or airports. A module for the generation of 
natural language can be an effective nhancement 
for monitoring for mainly two reasons: first, in 
most cases operators are busy observing many 
displays. Here the auditory presentation of  infor- 
mation can make use of idle cognitive resources of 
the operators and, thus, reduce their workload in 
directing their attention to a development that may 
lead to hazardous ituations. 2 Second, the essential 
piece of information can be extracted from a 
highly complex set of multimodal information and 
presented by the system in a crisp way. Language 
is the best conceivable means to transfer informa- 
tion as pointedly as possible. Moreover, taking the 
dynamics of the permanently changing world into 
account has the advantage that safety-critical 
situations can be anticipated earlier and much 
more reliably. Conventional systems, in contrast, 
just compare actual measurements with allowed 
values and give a warning or an alarm when a 
violation occurs. But it is more useful, e.g. for a 
nurse if the system tells her that a patient's blood 
pressure is rapidly dropping than that his blood 
pressure is already dangerously low. 
We see the proposed implementation of an in= 
cremental conceptualizer also as a means to gain 
2 The multimodal monitoring environment proposed 
here reflects the division of labor between the compo- 
nents in working memory (Baddeley, 1986), especially 
between the visuospatial sketchpad (VSSP) and the 
phonological loop. Since the observation of multiple 
display units puts a heavy strain on the VSSP, spoken 
natural anguage as input of critical information would 
use that subcomponent of working memory, namely the 
phonological loop, which is less strained. 
event conceptualization. Events emerge from dy- 
namic input data, which are segmented by the 
conceptual system into meaningful units (Avra- 
hami & Kareev, 1994). They are therefore internal 
representations rather than external entities: "\[...\] 
events arise in the perception of observers" 
(Zacks, 1997). Consequently, a language produc- 
tion system designed for verbalizing what the 
system perceives has to deal with information 
stemming from multiple modalities, e.g. auditory 
and spatial. In particular, a continuous multimodal 
'perceptual stream' has to be translated into dis- 
crete propositional output (preverbal messages) 
that can be encoded linguistically, cf. Levelt 
(1989). To meet such demands, three subtasks 
have to be solved: (1) The input stream has to be 
subdivided into 'perceptual units'; (2)conceptual 
representations have to be built up from these 
'percepts', which (3) have to be combined to pre- 
verbal messages. For the time being, we take the 
input stream to be strictly sequential, but later ver- 
sions of our model will compute simultaneous 
events, as well. 
According to Habel & Tappe (1999) the func- 
tion of the conceptualizer can be subdivided into 
the following processes: segmentation & group- 
ing, structuring, selection, and linearization. The 
first process operates on the (perceptual) input that 
is segmented into meaningful basic units (seg- 
mentation), and-- i f  possible--two or more of 
these units are grouped together to form more 
complex entities (grouping). The structuring pro- 
cess builds .up multilevel hierarchical structures 
from these meaningful basic units. 
To exemplify these steps we use the scenario 
o,f: monitoring the :taxiing of an aircraft, ~iz. the 
movements of an aircraft from the terminal to its 
assigned runway and vice versa. Air traffic con- 
trollers who guide the movements of aircraft on 
the ground (surface movement controllers, SMC) 
have to rely mainly on visual information---either 
looking out of the window of the control tower or 
getting information from a airfield control moni- 
86 
? :tor--and on communicatiom.with ,.the.,aircraft .......... e.vent,,~,and,:on,-,the.,other~,hand of  some~groupings..: 
crews. Yet, in some conditions, e.g. in low- The movement from position B to position C, for 
visibility, this method is not failsafe (although re- example, contains--at least--three sub-phases 
liable). It forces the crews to decrease speed--and corresponding to a straight, a curved and a second 
such increases number and duration of  de- straight section of the trajectory. These three 
lays--but it also results in greater safety risks) A phases can be distinguished by segmentation, but 
supporting system that monitors the occurrences are combined by a grouping. Furthermore, the 
on the taxiway can mitigate these effects., structuring process has to build up the different 
? phases to form the.TAXI event, cf. Figure 3. 
J .2r._. Lc.L:_,~J-.28. L_: ........ ~".-..The:,~:third,of:~he ~,above.~mentioned: sub-  
c@ @D Echo processes, selection, has two functions: first, it 
-'--\],~,! detects that there is a conflict or a (possibly) 
safety-critical development or situation and de- 
cides that a warning has to be generated. Second, 
it selects the required information for a suitable 
- - -~A warning or alert. Since the verbal warning can be 
given on different levels of detail, it is necessary 
to select appropriate vents from the event hierar- 
chy for further verbalization. On the one hand, it 
Figure I. Monitoring theTaxiingofanAircrafl: Phases of a would not be adequate to produce a general 
Complex Event 
warning like "Taxiing problem"--except erhaps 
We will demonstrate the workings of our when there is not enough time or no more infor- 
model of an incremental conceptualizer, which mation available at that moment---on the other 
produces natural anguage messages for the SMC, hand, it would not be suitable to give an in-depth 
with the example depicted in Figure 1. The flight description of each part of the taxiing. Finally, the 
with the number CK-314 shall taxi from the ter- selected items are brought into an appropriate or- 
minal via taxiway Echo to runway 27. The initial de r by the forth process, linearization. 
position of the airplane is A. It then starts to move Before we describe the internal structure of the 
until it reaches position 13 right before a junction conceptualizer in section 3, we want to discuss the 
where it has to stop and wait until the way is clear core idea of 'event conceptualization' in more 
before moving on. It then starts again and contin- detail. The first question to be answered is how a 
ues (C) but its velocity is too high at point D. continuous (perceptual) input stream can be seg- 
Consequently the plane might not be able to mented into separate vents. According to the cut 
branch off at the junction, where it is supposed to hypothesis of Avrahami and Kareev (1994, p. 
turn left into runway 27. At that point the moni- 239), "A sub-sequence of stimuli is cut out of a 
toring system generates a warning that the plane is sequence to become a cognitive entity if it has 
in danger of missing the junction. (If the plane in- been experienced many times in different con- 
deed misses the junction, an alert is generated, but texts." This segmentation takes place in the 'eye 
our example does not include this.) of the observer'. Hence, event conceptualization 
For this task two kinds of information have to partly depends on individual as well as on con- 
be available: the planned movements of the plane textual factors. 
and its actual movements. While the former in- The idea of the cut hypothesis implies the ex- 
formation could be handleddirectly by the con- istence of basic events, which ,are the building 
ceptualizer because they are inherently discrete, blocks in our experience used to trigger segmen- 
the latter are information about a continuously tation. They are minimal conceptual entities hu- 
changing world. Here the perpetual continuous in- ? : ~ .man observers ~ascribe a-beginning and,an:end to. 
put stream has to be transformed into discrete Thus, they are perceptual wholes--although t ey 
items. This process consists, on the one hand, of may have an internal structure--, and are there- 
segmentations into discrete units, e.g. a STOP fore the basis for the interface between perception 
and cognition. Basic events can be grouped to- 
gether to form complex events, e.g. assuming that 
3 The reports of incidents and accidents of the Austra- the four basic events GRIP THE HANDLE OF A 
lian Bureau of Air Safety. Investigation is a rich source 
of occurrences that should not happen in civil aircraft WINDOW, TURNING THE HANDLE, PULL, and LET 
operations. GO THE HANDLE are perceived, the complex event 
87 
OPENING A WINDOW. can-b? .bu i i taap .  Furthermore, 
subsequent events of opening all windows of a 
room can be grouped to AIRING. But events can 
not only be grouped but also segmented: if the 
event OPENING A WINDOW iS perceived, it can be 
segmented into the respective sub-events. We as- 
sume that hierarchical event structures, which are 
based on knowledge about he internal structure of 
prototypical events, e.g. in the format of scripts 
:Schank & Abeison (1977)?are.~exepresentational 
backbone of event conceptualization, cf Habel & 
Tappe (I 999). 
3 An Incremental Conceptualizer 
Incremental processing is the 'piecemeal' and par- 
allel processing of a sequential information 
stream. It is a specific kind of parallel processing 
in that the processes have a fixed order, which De 
Smedt & Kempen (1987) describe as a 'cascade of 
processes', in analogy to a water cascade. This 
metaphor means that, for example, the grammati- 
cal encoding--including lexical access--of  an 
utterance segment cannot take place until the in- 
formation 'splashes down' from the conceptual 
encoding process. Figure 2 sketches uch a cas- 
cade of dependent parallel processes in our model 
of the conceptualizer: The cascade consists of the 
processes construction, selection, linearization, 
and pvm-generation (preverbal-message-genera- 
tion). These processes also constitute a pipeline in 
Reiter's (1994) sense, but they do work in parallel. 
One central parameter of incremental process- 
ing, which is highly relevant for the format of pre- 
verbal messages, is the size of the increments. 
Assume that a description (no warning this time) 
of the turning of the flight number CK-314 into 
taxiway Echo shall be given. This could be done 
by a proposition like turn(ok314, goal(tw-echo)), 
which is a potential increment for a preverbal 
message. Yet, such a proposition would have to be 
built up completely, before the subsequent com- 
ponents can begin forming it into a sentence like 
'Flight CK 314 turns into taxiway Echo.' Hence 
the formulator coulcL not start processing the first 
element, say turn, as soon as it is received from 
the conceptualizer. In:contrast tothis, we opt for 
an architecture, in which the selection of appropri- 
ate lemmas from the lexicon can start for parts of 
a preverbal message, before other entities are built 
up on the preverbal message level. 
As a consequence, the dynamics in incremental 
processing demands a modified notion of prever- 
bal messages. We conceive of them no longer as 
-.,eomplete,.propositions~as~:i,s~mosfly t.he~.oase. in 
approaches combining Levelt's ideas with con- 
ceptual semantics--but as sequences of well- 
formed propositional structures~on a.sub-proposi- 
tional level; in logical terminology: predicate 
symbols, functional expressions, terms, etc. The 
incremental formulator SYNPHONICS,  which takes 
specific .well-formed parts of propositions as in- 
put, follows these principles (Abb et al 1996). 
~ . . J  \[ Chanoe ~ - -  ~Search 
I ~ in CCR r I . . . . . .  
I Add ~ Chanqei . . . .  I 
I '~  I RpJ~_c:tic~n \] Traveme i uoncem . 
... . . . . . . .  J 
I CCR 
: ~ { PVM-Generation \] . . . . . . .  " ,Azcess to 1,J Element 
Travevze Preverbal Messaqes 
Figure 2. Model of  an Incremental Conceptualizer 
3.1 Coarse Architecture 
In short, our conceptualizer performs the task 
'Give warnings about (possibly) safety-critical 
developments and situations!' It operates on two 
different input streams: a discrete one, which 
contains the plans for the movements of the air- 
craft on the ground, and a continuous one, which 
originates in the sensors distributed over the taxi- 
way. Since the conceptualizer cannot directly op- 
erate on the continuous input stream, these input 
information must be converted into a stream of 
discrete basic entities, which are basic events in 
this case. In our example a basic event is induced 
by sensoric data sent to the monitoring system. 
e.g. that a particular aircraft passes its position. 
Since the other input stream is already discrete, it 
simply has to be adapted to the required input 
format of the conceptualizer, i.e. it has to be con- 
vetted into basic events, as well. We will neglect 
this process and concentrate on the continuous in- 
put stream. 
..... Based-on Habel :&-T~appe ~(1999) we propose a 
model of the conceptualizer as depicted in Fig- 
ure 2. It consists mainly of four incremental (cas- 
caded) processes that work on the blackboard-like 
current conceptual structure (CCR). At first sight, 
the use of a data structure, to which more than one 
process has access, seems to collide with the no- 
tion of a cascaded information stream. These 
88 
-processes are interdependent,in ~sucha ~way,. how, 
ever, that they indeed behave incrementally; e.g. 
the selection process cannot select anything that 
has not been inserted into the CCR (constructed). 
The CCR can be seen as a shared memory unit 
with a common data structure. A third kind of in- 
formation is needed for a representation f the 
state of affairs: the constellation.of the terminals, 
taxiways, runways, and the participating object(s), 
:are: . 
( 1 ) construction 
(2) selection 
(3) linearization 
(4) pyre-generation 
The first process comprises the processes eg- 
mentation & grouping as well as structuring of 
Habel & Tappe (1999), apart from the segmenta- 
tions that are already done in. the pre-processing 
or, more generally:~the.,spatial~arrmngement,of the 
world and information about objects in it. For ex- 
ample, there is one node that stands for flight CK- 
314, and all the nodes shown in Figure 3 are 
linked to it via an actor relation. Since this type of 
information is not in the focus of the present pa- 
per, we will not discuss it the following. 
In addition to the cascaded processes there is a 
concept lexicon, accessible via a concept matcher: 
these modules, which are called by the construc- 
tion process, find best matches for structures that 
can either be subsumed by a more complex con- 
cept or may represent still incomplete concepts. 
The first is necessary to build up hierarchical 
structures at all. The second is needed for the gen- 
eration of expectations about developments in the 
near future. When, for example, flight CK-314 is 
at position D, the expectation is generated that it 
will go on straight at the next junction or that it 
will be unable to turn left at the next junction 
when keeping the current velocity. 4 On the other 
hand, after the two nodes STARTi and CHPOS~ 
(Figure 3) are constructed, these are given to the 
concept matcher for a subsumption test, which 
consists of trying to match the nodes onto more 
complex concepts. This yields that they can be 
joined together to a MOVE node (MOVEr). Thus, it 
informs the construction process that a STOP event 
(STOPs) will probably occur in the near future, 
which illustrates the second function of the 
matcher: the generation of expectations. (Even the 
last MOVE of a sequence of MOVE events contains 
a STOP event, because aircraft stop at the begin- 
ning of the runway, which is the last event of the 
taxiing, before they commence the takeoff.) The 
construction process inserts these two new nodes 
together with the information .that he. STOPi node 
is just a hypothesis up to now, nothing actually 
perceived. 
The four cascaded processes that constitute the 
'heart' of the conceptualizer and that will be de- 
scribed in more detail in the following sections 
4 The computation of the velocity is easily, done from 
the sensoric data. 
....... step: ~hes~lectiorr.and thet  inearization processes 
correlate to the ones in Habel & Tappe (1999), 
thus, the first selects nodes for verbalizations, 
while the second brings them into an appropriate 
order. The pyre-generation is an additional proc- 
ess and guarantees that the selection as well as the 
linearization have some time to change (the order 
of) the selected nodes, before they are passed on 
to the formulator. We call this time span the la- 
tency time. 
For the implementation f this architecture and 
(a first version of) the algorithms we use a for- 
malism called referential nets (Habel, 1986), 
which was developed to represent linguistic as 
well as common sense knowledge. Entities are 
represented by referential objects (refOs), which 
can be connected via relations, so that a network 
structure arises. The basic entities the pre-proces- 
sing component produces already contain some in- 
formation about what attributes (e.g. which sort) 
have to be ascribed to a refO. In the following we 
use symbolic onstants to refer to refOs. These are 
just arbitrary labels; the important point is that the 
refOs can be related to suitable refOs of subse- 
quent processes, which, for example, stand for 
lexical items. 
3.2 Construction 
The construction process takes basic entities as 
input and builds up a hierarchical knowledge rep- 
resentation of the perceived states of affairs in the 
CCR. In the domain we discuss here, three rela- 
tions are especially relevant for the representation 
of events: (temporal) inclusion. (_), temporal 
precedence (-<), and the match of planned events 
onto actual events (g). For the example described 
. . . .  above::the sub-net- of  the:a'eferential: :net ~that con- 
tains the actual events (the ones that have sort A- 
Event) is depicted in Figure 3 (the velocity prob- 
lem is just detected). MOVE2, for example, is tem- 
porally included in the event TAXI (MOVE2 E 
TAXI), the event MOVEt is the temporal predeces- 
sor of  MOVE2 (MOVEi -< MOVE?), a matching be- 
tween a planned and an actual event is p.(MOVE~, 
? MOVErs), where MOVt!~ stands for the planned 
89 
TAXl 
= J 
MOVE 1_................. 
STARTi _ CHPOS~ -- SSTOPi - START2 ~, ~CHPOS2 ~ ~ _..~SqOP2 / 
F b-'.. I F 
Sl S2 $3  S4  $5 $6' S7  S8 59 SIO-SII -SI2 S13 S14 SIS SI6 SI7 518 
-.< -< .< -< -< -< -'< .K -< "< "< -K '< -< < -< -.K 
Figure 3. The Knowledge Representation f r the Example (STOP2 is only expected) 
movement from position A to B. 
MOVE is a label for complex events that con- 
sists o f  maximal ly  three sub-events,  namely 
START, CHPOS (CHANGE OF POSITION), and STOP, 
where the first and the last sub-event are optional 
and the middle event can be any kind o f  move- 
ment along a trajectory. START, CHPOS, and STOP 
nodes contain the sensor data nodes S. Temporal 
inclusion relates also TAXI and the basic events 
and MOVE and basic events, but are left: out in the 
figure to keep it readable. The precedence rela- 
tion, though, exists explicitly only between nodes 
of  the same granularity, i.e. between the basic 
events and between the nodes of  each intermediate 
level, e.g. CHPOS~ -< STOPi '< START2; the implicit 
precedences have to be derived from these. The It 
relation is not included in Figure 3. We use four 
sorts to subdivide the CCR into four sub-nets, 
each consisting of  refOs of  one sort: planned event 
(P-Event), actual  event  (A-Event), problem 
(Problem), and real-world object (R-Object). 
The construction process, which builds up the 
representation f the actually registered events and 
to detect problems, can be realized by the follow- 
ing algorithm 
1. Generate a new node for the basic entity that is pro- 
vided by the pre-processing unit. Link it to other 
nodes according to the information (i.e., attributes) 
coming with the basic entity. 
2. Take the new node, possibly with related nodes-- 
especially those that stand in the ~ relation to this 
one--and hand them over to the concept matcher to 
find the best matching complex concept hat con- 
tains these nodes--if there is any~and to find (pos- 
sible) problems. 
3. In case there is a problem, create a new node for it, 
and link it to the involved nodes. Continue with step 
2. (In step 2 the 'new' node is the 'old" one, not the 
problem node!) In case of a complex concept, create 
a new node for it, together with the simpler nodes 
that are still lacking (generation of expectations), 
and link it to the basic nodes it subsumes. 
4. Tr3.' to find relations to other complex nodes with 
which links can be established. 
5. Continue with step 2 to try to find more complex 
concepts and problems, until the next new basic en- 
tity enters the system or until there are no more 
complex concepts. Then proceed with step 1. 
In the following application o f  this algorithm to 
the example, the It relation is left out until the 
problem is identified. Up to that point, each node, 
except for the S nodes, has a corresponding one in 
the sub-structure of  planned events, related by It. 
a. s l is read by the construction process as a basic 
event and inserted into the (up to now empty) 
knowledge representation. (step 1) 
b. This node is handed over to the concept matcher 
(there are no related nodes, yet), which responds 
that this is a START event. (2) 
c. STARTi is generated and linked via E to s~. (3) 
d. Steps 4 and 5 yield no results. 
e. $2, the next basic event, is inserted. A .< relation 
between the two s nodes is established. (1) 
f. Because of their temporal vicinity the two s nodes 
are taken and given to the concept matcher, which 
computes a CHPOS event. (2) 
g. CHPOSt iS generated and linked via .G to S2. (3) (It 
would equally be possible to take $2 and S~ to be 
parts of CHPOSt. The only reason we chose this pos- 
sibility is that it preserves the tree structure--at 
least for the moment.) 
h. STARTi and CHPOSi are linked by -<. (4) 
i. Now STARTi and CHPOS~ are given to the concept 
matcher, which joins them together in a complex 
event MOVE (5, 2). 
MOVE1 and.the 'expectation' . ode STOPi are gener- 
ated and MOVEi iS linked via m to its elements (3). 
Step 4 yields no result. 
.MOVE1 is just,part o f  a-more:complex vent TAXI. 
(5, 2) 
TAXI is inserted and linked via E to MOVEi. (3) 
Node Ss is read and subsumed to CHPOS~. In this 
step the attributes of CHPOS~ are updated. (Espe- 
cially interesting for our example: the velocity at- 
tribute.) 
o. Nodes 54 to s7 are integrated and subsumed to 
CHPOS~. 
j, 
k. 
L 
nl .  
n. 
90 

.essary changes(replacements)in the ,traverse, be- 
cause the pvm-generation already took the ele- 
ments that would otherwise have been replaced. 
After the latency time of a node is elapsed the 
pvm-generation passes it on to the subsequent 
process of the formulator. This means--as we ar- 
gued above--that propositions exist only 'piece- 
: . . . . .  ._ wise'. Complete propositions exist, then, on a 
higher level of abstraction. When the PROBLEMv 
SOCCER, 449-4,54. P rac. of the 8tttECAL Munich. 
Australian Bureau of Air Safety Investigation. 
http://www.basi.gov.au 
Avrahami, J. & Kareev, Y. (1994) The emergence of 
events. Cognition, 53,239-261. 
Baddeley, A. (! 986) Working Memory. Oxford: Oxford 
University Press. 
Cahill, L.; Doran, Ch.; Evans, R.; Mellish, Ch.; Pava, 
D.; Reape, M.; Scott, D. & Tipper, N. (1999) In 
search of a reference architecture-for NLG systems. 
EWNLG-1999, Toulouse. 
node is sel ected.as~ a pr~.x~er.bal ~'messages~informa~ ~,~v.:.~.~:~3hu~Catrolk. J:t~O.~(2arberazy:,.:S,.~( 1998 ) ~o|taborative. re- 
tion about what is the problematic event (moving 
too fast), the object involved (the aircraft with the 
flight number CK-314) and about the location of 
the movement (taxiway Echo) have to be con- 
veyed. This is done by handing on not only the 
PROBLEMv node but also further nodes that contain 
information about he event (in the CHPOS2 node), 
the flight (in a FLIGHT node), and about the loca- 
tion (in a TW-ECHO node). Thus, the pvm- 
generation considers ome important relations to 
other nodes before the PROBLEMv node is passed 
on, and checks what other nodes are needed for 
the verbalization and passes them on, as well. 
The formulator can establish interrelations 
between the refOs by the ascribed relations, e.g. 
the PROBLEMv node contains an actor relation to 
the FLIGHT node, which enables the formulator to 
look up all necessary information at the relevant 
nodes. Taken together they are now equivalent to 
a proposition like problem(ck-314, veloc- 
ity(tooHigh), tw-Echo, goal(rw27). 
4 Conclusion 
We presented a psycholinguistically motivated ar- 
chitecture of an incremental conceptualizer to- 
gether with some remarks on its prototypical 
implementation a d how this implementation can 
be used for monitoring purposes. The conceptual- 
izer watches dynamic changes in the world and 
generates on-line propositional, preverbal struc- 
tures that can serve as input to a subsequent com- 
ponent, which encodes these structures linguisti- 
cally. 
References 
Abb, B.; Gfinther, C.; Herweg, M.; Lebeth, K:; Maien- 
born, M. & Schopp, A. (1996) Incremental gram- 
matical encoding--an outline of the SYNPHONICS 
formulator. In G. Adorni & M. Zock, eds., Trends in 
natural language generation: An artificial intelli- 
gence perspective, 277-299, Berlin: Springer. 
Andr6, E.; Herzog, G. & Rist, T. (1988) On the simul- 
taneous interpretation f real world image sequences 
and-their natural anguage description: The system 
sponse generation i  planning dialogues. Computa- 
tional Linguistics, 24, 355-400. 
De Smedt, K.; Horacek, H. & Zock, M. (1996) Archi- 
tectures for natural anguage generation: Problems 
and perspectives. In G. Adorni & M. Zock, eds., 
Trends in natural language generation, Berlin: 
Springer. 
De Smedt, K. & Kempen, G. (1987) Incremental sen- 
tence production: Self-correction and coordination. 
In G. Kempen, ed., Natural language generation, 
365-76, Boston: Martinus Nijhoff. 
Ferreira, F. (1996) Is it better to give than donate? 
Syntactic flexibility in language production. Journal 
of Memory and Language, 35,724-755. 
Habel, Ch. (1986) Prinzipien der Referentialitdt: Un- 
tersuchungen zur propositionalen Reprdsentation 
yon Wissen. Berlin: Springer. 
Habel, Ch. & Tappe, H. (1999) Processes of segmenta- 
tion and linearization i describing events. In R. Kla- 
bunde & C. von Stutterheim, eds., Representations 
and Processes in Language Production, 117-153, 
Wiesbaden: Deutscher Universit~ts-Verlag. 
Hovy, E. H. (1993) Automated iscourse generation 
using discourse structure relations. Artoqcial Intelli- 
gence, 63,341-385. 
Levelt, W.J.M. (1989) Speaking: From intention to ar- 
ticulation. Cambridge, MA: MIT Press. 
Marcu, D. (1997) From local to global coherence: A 
bottom-up approach to text planning. Proc. AAAI 97, 
629-635. 
McKeown, K. (1985) Text generation. Cambridge: 
Cambridge University Press. 
Neumann, B. & Novak, H. -J. (1983). Event models for 
recognition and natural language. IJCAI-83,724-726. 
Radev, D. R. & McKeown, K. (1998) Generating natu- 
ral language summaries from multiple on-line 
sources. Computational Linguistics, 24,469-500. 
Reiter E. (1994) Has a consensus NL generation archi- 
tecture appeared, and is it psycholinguistically p au- 
sible? IWNLG-1994, J63-170,Kennebunkport, ME. 
Reithinger, N. (1992) The Performance of an incre- 
mental generation component for multi-modal dialog 
contributions. In: R. Dale, E. Hovy, D. R6sner & O. 
Stock, eds.~ Aspects of automated-natural language 
generation, 263-276, Berlin: Springer. 
Schank, R. C. & Abelson, R. P. (1977) Scripts, plans, 
goals and understanding: An inquit T into human 
knowledge structures. Hitlsdale: Lawrence Erlbaum. 
Zacks, J. (1997) Seeing the structure in events. Manu- 
script, Stanford University. 
92 
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1158?1167,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Subsequent Reference in Shared Visual Scenes:
Computation vs. Re-Use
Jette Viethen1,2
jette.viethen@mq.edu.au
1TiCC
University of Tilburg
Tilburg, The Netherlands
Robert Dale2
robert.dale@mq.edu.au
2Centre for Language Technology
Macquarie University
Sydney, Australia
Markus Guhe3
m.guhe@ed.ac.uk
3School of Informatics
University of Edinburgh
Edinburgh, UK
Abstract
Traditional computational approaches to re-
ferring expression generation operate in a de-
liberate manner, choosing the attributes to be
included on the basis of their ability to dis-
tinguish the intended referent from its dis-
tractors. However, work in psycholinguis-
tics suggests that speakers align their refer-
ring expressions with those used previously in
the discourse, implying less deliberate choice
and more subconscious reuse. This raises the
question as to which is a more accurate char-
acterisation of what people do. Using a cor-
pus of dialogues containing 16,358 referring
expressions, we explore this question via the
generation of subsequent references in shared
visual scenes. We use a machine learning ap-
proach to referring expression generation and
demonstrate that incorporating features that
correspond to the computational tradition does
not match human referring behaviour as well
as using features corresponding to the process
of alignment. The results support the view that
the traditional model of referring expression
generation that is widely assumed in work on
natural language generation may not in fact
be correct; our analysis may also help explain
the oft-observed redundancy found in human-
produced referring expressions.
1 Introduction
Computational work on referring expression genera-
tion (REG) has an extensive history, and a wide vari-
ety of algorithms have been proposed, dealing with
various facets of what is recognised to be a com-
plex problem. Almost all of this work sees the task
as being concerned with choosing those attributes
of an intended referent that distinguish it from the
other entities with which it might be confused (see,
for example, Dale (1989), Dale and Reiter (1995),
Krahmer et al (2003), van Deemter and Krahmer
(2007), Gardent and Striegnitz (2007)). Indepen-
dently, an alternative way of thinking about refer-
ence has arisen within the psycholinguistics com-
munity: there is now a long tradition of work that
explores how a dialogue participant?s forms of ref-
erence are influenced by those previously used for
a given entity. Most recently, this line of work has
been discussed in terms of the notions of alignment
(Pickering and Garrod, 2004) and conceptual pacts
(Clark and Wilkes-Gibbs, 1986; Brennan and Clark,
1996).
We suspect that neither approach tells the full
story, and so we are interested in exploring whether
the two perspectives should be integrated. Using a
large corpus of referring expressions in task-oriented
dialogues, this paper presents a machine learning
approach that allows us to combine features corre-
sponding to the two perspectives. Our results show
that models based on the alignment perspective out-
perform models based on traditional REG considera-
tions, as well as a number of simpler baselines.
The paper is structured as follows. In Section 2,
we outline the two perspectives on subsequent ref-
erence, and summarise related work. In Section 3,
we describe the iMAP Corpus and the referring ex-
pressions it contains. In Section 4, we describe the
approach we take to learning models of referential
behaviour using this data, and in Section 5 we dis-
cuss the results of a number of experiments based
1158
on this approach, followed by an error analysis in
Section 6. Section 7 draws some conclusions and
discusses future work.
2 Related Work
2.1 The Algorithmic Approach
We use the term algorithmic approach here to re-
fer to the perspective that is common to the consid-
erable body of work within computational linguis-
tics on the problem of referring expression gener-
ation developed over the last 20 years. Much of
this work takes as its starting point the characterisa-
tion of the problem expressed in (Dale, 1989). This
work has focused on the design of algorithms which
take into account the context of reference in order to
decide what properties of an entity should be men-
tioned in order to distinguish that entity from others
with which it might be confused. Early work was
concerned with subsequent reference in discourse,
inspired by Grosz and Sidner?s (1986) observations
on how the attentional structure of a discourse made
particular referents accessible at any given point.
More recently, attention has shifted to initial ref-
erence in visual domains, driven in large part by
the availability of the TUNA dataset and the shared
tasks that make use of it (Gatt et al, 2008). The con-
struction of distinguishing descriptions has consis-
tently been a key consideration in this body of work.
Scenarios that require the generation of references
in multi-turn dialogues that concern visual scenes
are likely to be among the first where we can ex-
pect computational approaches to referring expres-
sion generation to be practically useful. Surpris-
ingly, however, the more recent work on initial refer-
ence in visual domains and the earlier work on sub-
sequent reference in discourse remain somewhat dis-
tinct and separate from each other, despite much the
same algorithms having been used in both. There
is very little work that brings these two strands to-
gether by looking at both initial and subsequent ref-
erences in dialogues that concern visual scenes. An
exception here is the machine learning approach de-
veloped by Stoia et al (2006), who aimed at building
a dialogue system for a situated agent giving instruc-
tions in a virtual 3D world. However, their approach
was concerned with choosing the type of reference
to use (definite or indefinite, pronominal, bare or
modified head noun), and not with the content of the
reference; and their data set consisted of only 1242
referring expressions.
2.2 The Alignment Approach
Meanwhile, starting with the early work of Carroll
(1980), a quite distinct strand of research in psy-
cholinguistics has explored how a speaker?s form of
reference to an entity is impacted by the way that en-
tity has been previously referred to in the discourse
or dialogue. The general idea behind what we will
call the alignment approach is that a conversational
participant will often adopt the same semantic, syn-
tactic and lexical alternatives as the other party in a
dialogue. This perspective is most strongly associ-
ated with the work of Pickering and Garrod (2004).
With respect to reference in particular, speakers are
said to form conceptual pacts in their use of lan-
guage (Clark and Wilkes-Gibbs, 1986; Brennan and
Clark, 1996). Although there is disagreement about
the exact mechanisms that enable alignment and
conceptual pacts, the implication of much of this
work is that one speaker introduces an entity by
means of some description, and then (perhaps after
some negotiation) both conversational participants
share this form of reference, or a form of reference
derived from it, when they subsequently refer to that
entity.
Recent work by Goudbeek and Krahmer (2010)
supports the view that subconscious alignment does
indeed take place at the level of content selection for
referring expressions. The participants in their study
were more likely to use a dispreferred attribute to
describe a target referent if this attribute had recently
been used in a description by a confederate.
There is some work within natural language gen-
eration that attempts to model the process of align-
ment (Buschmeier et al, 2009; Janarthanam and
Lemon, 2009), but this is predominantly concerned
with what we might think of as the ?lexical perspec-
tive?, focussing on lexical choice rather than the se-
lection of appropriate semantic content for distin-
guishing descriptions.
2.3 Combined Models
This paper is not the first to look at how the algorith-
mic approach and the alignment approach might be
integrated in REG. An early machine learning ap-
1159
Figure 1: An example pair of maps.
proach to content selection was presented by Jor-
dan and Walker (2000; 2005); they were also in-
terested in an exploration of the validity of differ-
ent psycholinguistic models of reference produc-
tion, including Grosz and Sidner?s (1986) model
of discourse structure, the conceptual pacts model
of Clark and colleagues, and the intentional influ-
ences model developed by Jordan (2000). However,
their data set consists of only 393 referring expres-
sions, compared to our 16,358, and these expres-
sions had functions other than identification; most
importantly, the entities referred to were not part of
a shared visual scene as is the case in our data.
Gupta and Stent (2005) instantiated Dale and Re-
iter?s (1995) Incremental Algorithm with a prefer-
ence ordering that favours the attributes that were
used in the previous mention of the same referent. In
a second variant, they even require these attributes
to be included in a subsequent reference. Differ-
ently from most other work on REG, they extended
the task to include ordering of the attributes in the
surface form. They therefore create a special evalu-
ation metric that takes ordering into account, which
makes it hard to compare the performance they re-
port to that of any system that is not concerned with
attribute ordering, such as ours. Their evaluation set
was also considerably smaller than ours: they used
1294 and 471 referring expressions from two differ-
ent corpora, compared to our test set of 4947 refer-
ring expressions.
More recently in (Viethen et al, 2010), we pre-
sented a rule-based system that addressed a specific
instance of the problem we consider here, using the
same corpus as we do: we singled out 2579 first ref-
erences to landmarks by the second speaker (?second
speaker initial references?) and attempted to repro-
duce these using a system based on Dale and Re-
iter?s (1995) Incremental Algorithm. Although the
data set was a subset of the one used here, the system
did not reach the same performance (see Section 5).
3 Referring Expressions in the iMAP
Corpus
The iMAP Corpus (Louwerse et al, 2007) is a col-
lection of 256 dialogues between 32 participant-
pairs who contributed 8 dialogues each. Both par-
ticipants had a map of the same environment, but
one participant?s map showed a route winding its
way between the landmarks on the map; see Fig-
ure 1. The task was for this participant (the in-
struction giver, IG) to describe this route in such a
way that their partner (the instruction follower, IF)
could draw it onto their map; this was complicated
by some discrepancies between the two maps, such
1160
as missing landmarks, the unavailability of colour in
some regions due to ink stains, and small differences
between some landmarks.
The landmarks differ from each other in type,
colour, and one other attribute, which is different
for each type of landmark. For example, there are
different kinds of birds (eagle, ostrich, penguin . . . );
fish differ by their patterns (dotted, checkered, plain
. . . ), aliens have different shapes (circular, hexago-
nal . . . ), and bugs appear in small clusters of differ-
ing numbers. In addition to these inherent attributes
of the landmarks, participants used spatial relations
to other items on the map. Each referring expression
in the corpus is annotated with a unique identifier
corresponding to the landmark that it describes and
the semantic values of the attributes that it contains.
This collection of annotations forms the basic data
we use in our experiments.
For each landmarkR referred to in a dialogue, we
view the sequence of references to this landmark as
a coreference chain, notated ?R1, R2, . . . , Rn?. By
convention, R1 is termed the initial reference, and
all other references in the chain are subsequent ref-
erences. From the corpus as a whole we extracted
34,127 referring expressions in 9558 chains. The av-
erage length of a chain is 4.74; and the longest coref-
erence chain contains 43 references. References
may be contributed to a chain by either speaker, and
can be arbitrarily far apart: in the data, 4201 refer-
ences are in the utterance immediately following the
preceding reference in the chain, but the distance be-
tween references in a chain can be as high as 423
utterances.
We removed from the data any annotation that
was not concerned with the four landmark attributes,
type, colour, relation, or the landmark?s other dis-
tinguishing attribute. For example, ?semantically
empty? head nouns such as thing or set. Ordi-
nal numbers that were annotated as the use of the
number attribute were re-tagged as spatial relations,
as these usually described the position of the target
within a line of landmarks.
As a result of the removal of annotations not per-
taining to the use of the four landmark attributes,
2785 referring expressions had no annotation left;
we removed these instances from the final data set.
We also do not attempt to replicate the remaining
5552 plural referring expressions or the 3062 pro-
Content Pattern Count Proportion
?other? 5893 36.0%
?other, type? 3684 22.5%
?other, colour? 1630 10.0%
?other, colour, type? 1021 6.2%
?colour? 969 5.9%
?relation? 777 4.7%
?other, relation? 587 3.6%
?type? 574 3.5%
?colour, type? 434 2.7%
?other, relation, type? 312 1.9%
?relation, type? 236 1.4%
?colour, relation? 99 0.6%
?other, colour, relation? 81 0.5%
?other, colour, relation, type? 44 0.3%
?colour, relation, type? 17 0.1%
Total 16,358
Table 1: The 15 content patterns by frequency.
nouns found in the corpus.1 However, we do in-
clude all of these instances in the feature extraction
step, on the assumption that they might impact on
the content of subsequent references. Similarly, we
filter out 6369 initial references after we have ex-
tracted features from them, since we focus here on
the generation of subsequent reference only. The re-
maining 16,358 referring expressions form the data
which we use in our experiments.
Contrary to findings from other corpora, in which
colour was used much more frequently (Gatt, 2007;
Viethen and Dale, 2008), the colour attribute was
used in only 26.3% of the referring expressions in
our data set. This is probably due to the often low
reliability of colour in this task caused by the ink
stains. The proportion of referring expressions men-
tioning the target?s type might, at 38.7%, also seem
low. This can be explained by the fact that one quar-
ter of the landmarks, namely birds and buildings, are
more likely to be described in terms of their specific
kind than in terms of their generic type. This also
helps explain why the overall use of the other at-
tribute, which for some landmarks was their kind,
was used in 81.0% of all instances. Spatial relations
were used in 13.16% of the referring expressions,
comparable to other corpora in the literature.
1The additional issues that arise in generating plural refer-
ences and deciding when to use pronouns considerably compli-
cate the problem; see (Gatt, 2007).
1161
We can think of each referring expression as be-
ing a linguistic realisation of a content pattern: this
is the collection of attributes that are used in that
instance. The attributes can be derived from the
property-level annotation given in the corpus. So,
for example, if a particular reference appears as the
noun phrase the blue penguin, annotated seman-
tically as ?blue, penguin?, then the corresponding
content pattern is ?colour, kind?. Our aim is to repli-
cate the content pattern of each referring expression
in the corpus. Table 1 lists the 15 content patterns
that occur in our data set in order of frequency.
4 Modelling Referential Behaviour
4.1 The Two Perspectives
Our task is defined simply as follows: for each sub-
sequent referenceR in the corpus, can we predict the
content pattern that will be used in that reference?
As we noted at the outset of the paper, the literature
would appear to suggest two distinct approaches to
this problem. What we have characterised as the al-
gorithmic approach can be summarised thus:
At the point where a reference is required,
a speaker determines the relevant features
of other entities in the context, then com-
putes the content of a referring expression
which distinguishes the intended referent
from the other entities.
The alignment approach, on the other hand, can be
summarised thus:
Speakers align the forms of reference they
use to be similar or identical to references
that have been used before. In particular,
once a form of reference to the intended
referent has been established, they tend to
re-use that form of reference, or perhaps
an abbreviated version of it.
The alignment approach would appear to be prefer-
able on the grounds of computational cost: we
would expect that retrieving a previously-used refer-
ring expression, or parts thereof, generally requires
less computation than building a new referring ex-
pression from scratch.
On the other hand, if the context has changed
in any way, then a previously-used form of ref-
erence may no longer be effective in identifying
Map Features
Main Map type most frequent type of LM on this map
Main Map other other attribute if the most frequent type of LM
Mixedness are other LM types present on this map?
Ink Orderliness shape of the ink blot(s) on the IF?s map
Lmprop Features
other Att type of the other attribute of the target
[att] Value value for each att of target
[att] Difference was att of target different between the two
maps?
Missing was target missing one of the maps?
Inked Out was target inked] out on the IG?s map?
Speaker Features
Dyad ID ID of the pair of participant-pair
Speaker ID ID of the person who uttered this RE
Speaker Role was the speaker the IG or the IF?
Table 2: The Ind feature set.
the intended referent, and recomputation may be
required.2 This is precisely the consideration on
which the initial work on referring expression gen-
eration was based, inspired by Grosz and Sidner?s
(1986) observations about how the changing atten-
tional structure of a discourse moves different en-
tities in and out of focus. However, a straightfor-
ward recomputation of reference based on the cur-
rrent context carries the risk that the most effective
set of properties to use may change quite radically;
if no account is taken of the history of previous ref-
erences to the entity, it?s conceivable that one could
produce a description that is so different from the
previous description that they are virtually unreco-
gisable as descriptions of the same entity. Ideally,
what we want to do is modify a previous description
to do the job.
These observations suggest that, in order to
choose the most appropriate form of reference for an
entity, we need to simultaneously take account of:
? the other entities from which it must be distin-
guished, both in the visual context and in the
preceding discourse (in other words, exactly
the information that traditional algorithmic ap-
proaches consider);
? how this entity, and perhaps other entities, have
been referred to in the past (precisely the infor-
mation that the alignment approach considers).
2Unfortunately, determining what counts as a change of con-
text, especially in visual scenes, is fraught with difficulty.
1162
TradREG Features (Visual)
Count Vis Distractors number of visual distractors
Prop Vis Same [att] proportion of visual distractors with
same att
Dist Closest distance to the closest visual distrac-
tor
Closest Same [att] has the closest distractor the same
att?
Dist Closest Same [att] distance to the closest distractor of
same att as target
Cl Same type Same [att] has the closest distractor of the same
type also the same att?
TradREG Features (Discourse)
Count Intervening LMs number of other LMs mentioned since
the last mention of the target
Prop Intervening [att] proportion of intervening LMs for
which att was used AND which have
the same att as target
Table 3: The TradREG feature set.
The set of features we describe next attempts to cap-
ture these two aspects of the problem.
4.2 Features
The number of factors that can be hypothesised as
having an impact on the form of a referring expres-
sion in a dialogic setting associated with a visual do-
main is very large. Attempting to incorporate all of
these factors into parameters for rule-based systems,
and then experimenting with different settings for
these parameters, is prohibitively complex. Instead,
we here capture a wide range of factors as features
that can be used by a machine learning algorithm to
automatically induce from the data a classifier that
predicts for a given set of features the attributes that
should be used in a referring expression.
The features we extracted from the data set are
listed in Tables 2?4.3 They fall into five subsets.
Map Features capture design characteristics of the
maps the current dialogue is about; Speaker Fea-
tures capture the identity and role of the partici-
pants; and LMprop Features capture the inherent
visual properties of the target referent. For our ex-
periments, we group the Map, LMprop and Speaker
feature sets into one theory-independent set (Ind).
Most importantly for our present considerations,
3In these tables, att is an abbreviatory variable that is instan-
tiated once for each of the four attributes type, colour, relation,
and the other distinguishing attribute of the landmark. The ab-
breviation LM stands for landmark
Alignment Features (Recency)
Last Men Speaker Same who made the last mention of target?
Last Mention [att] was att used in the last mention of
target?
Dist Last Mention Utts distance to the last mention of target
in utterances
Dist Last Mention REs distance to the last mention of target
in REs
Dist Last [att] LM Utts distance in utterances to last use of
att for target
Dist Last [att] LM REs distance in REs to last use of att for
target
Dist Last [att] Dial Utts distance in utterances to last use of
att
Dist Last [att] Dial REs distance in REs to last use of att
Dist Last RE Utts distance to last RE in utterances
Last RE [att] was att mentioned in the last RE?
Alignment Features (Frequency)
Count [att] Dial how often has att been used in the dialogue?
Count [att] LM how often has att been used for target?
Quartile quartile of the dialogue the RE was uttered in
Dial No number of dialogues already completed +1
Mention No number of previous mentions of target +1
Table 4: The Alignment feature set.
TradREG Features capture factors that the tradi-
tional computational approaches to referring expres-
sion generation take account of, in particular prop-
erties of the discourse and visual distractors; and
Alignment Features capture factors that we would
expect to play a role in the psycholinguistic models
of alignment and conceptual pacts.
4.3 The Models
For the experiments described here, we used a 70?30
split to divide the data into a training set (11,411 in-
stances) and a test set (4,947 instances). In addition
to the main prediction class content pattern, the split
was stratified for Speaker ID and Quartile to ensure
that training and test set contained the same pro-
portion of descriptions from each speaker and each
quartile of the dialogues. We used the J48 algorithm
implemented in the Weka toolkit (Witten and Frank,
2005) to train decision trees with the task of judging,
based on the given features, which content pattern
should be used.
First, we have three separate baseline models:
HeadNounOnly generates only the property that is
the most likely head noun for the target, which
is kind for birds and buildings and type for all
1163
other landmarks. This is a form of ?reduced
reference? strategy.
RepeatLast represents a very simplistic alignment
approach. It generates the same content pattern
that was used in the previous mention of the
target referent.
MajorityClass generates the content pattern most
commonly used in the training set.
We then have a number of models that use subsets
of the features described above:
AllFeatures is a decision tree trained on all fea-
tures;
TradREG is a decision tree trained on the
TradREG features only;
Alignment is a decision tree trained on the Align-
ment features only;
Ind is a decision tree trained on the Ind features
only;
Alignment+Ind is a decision tree trained on all but
the TradREG features;
TradREG+Ind is a decision tree trained on all but
the Alignment features; and
TradREG+Alignment is a decision tree trained on
all but the Ind features.
5 Results
In this section we report how the models described
in the previous section performed on the held-out
test set in comparison to each other and to the three
baselines.
We use Accuracy and average DICE score for our
comparisons; these are the most commonly used
measures in the REG literature (see, for example,
Gatt et al, 2008). Given two sets of attributes, A
and B, DICE is computed as
(1) DICE = 2? |A ?B||A|+ |B| .
This gives some measure of the overlap between two
referring expressions, assigning a partial score if the
two sets share attributes but are not identical. The
Accuracy of a system is the proportion of test in-
stances for which it achieves a DICE score of 1, sig-
nifying a perfect match.
col other type rel Comb. Pattern
Acc Acc Acc Acc Acc DICE
HeadOnly n/a n/a n/a n/a 23.1 0.49
RepLast n/a n/a n/a n/a 38.4 0.55
Majority 73.8 81.0 61.7 86.8 36.0 0.65
predicts no yes no no ?other?
Trad 74.6 84.8 77.1 87.0 47.3 0.73
Align 83.6 84.1 80.7 87.5 54.6 0.78
Ind 81.9 82.8 81.4 88.0 52.7 0.78
Align+Ind 86.1 85.3 82.4 88.7 58.2 0.81
Trad+Ind 82.2 84.1 81.1 87.1 52.5 0.78
Trad+Align 84.1 84.0 80.1 86.8 53.9 0.78
AllFeatures 86.2 85.8 83.2 88.5 58.8 0.81
Table 5: Performance of our models compared to the
baselines. Model names are abbreviated for space rea-
sons. The Accuracy (given in %) of all models is signifi-
cantly better than that of the highest performing baseline
at p<.01 according to the ?2 statistic.
We tested two different ways of generating con-
tent patterns based on the different feature sets de-
scribed above: PatternAtOnce builds a decision
tree that chooses one of the 15 content patterns that
occur in our data set; whereas CombinedPattern
builds attribute-specific decision trees (one for each
of the four attributes that occur in the data: colour,
other, type, and relation), and then combines their
predictions into a complete content pattern. We
found that CombinedPattern slightly outperformed
PatternAtOnce, although the difference is not statis-
tically significant for all feature sets. For space rea-
sons, we report in what follows only on the slightly
better-performing CombinedPattern model.
Table 5 compares the performances of the three
baselines and the decision trees based on the five fea-
ture subsets for each of the individual attributes and
for the combined content pattern; note that the Head-
NounOnly and RepeatLast baselines do not make
attribute-specific predictions. The table shows that
the learned systems outperform all three baselines
for the individual attributes as well as for the com-
bined content pattern.
A comparison of the Alignment feature set and
the TradREG feature set shows that the former out-
performs the latter for the attribute-specific trees
which predict the use of the colour attribute and the
1164
use of relation, and that the combined patterns re-
sulting from the Alignment trees are a better match
of the human-produced patterns both in terms of Ac-
curacy (p<.01 for all three categories, using ?2) and
DICE. Interestingly, even the theory-independent
Ind features outperform the TradREG features.
The comparison between TradREG+Ind and
Alignment+Ind again shows a clear advantage for
the Alignment features: dropping them from the
complete feature set significantly hurts performance
compared to AllFeatures (?2=80.5, p<.01), while
dropping the TradREG features has no significant
impact. Also consistent with the results of the three
individual feature sets, dropping the Ind features
hurts performance more than dropping the TradREG
features, but less than dropping the Alignment fea-
tures. Training on the complete feature set (All-
Features) achieves the highest performance, which
is significantly better than that of all other features
sets (p<.01 using ?2) except Alignment+Ind.
These results suggest that considerations at the
heart of traditional REG approaches do not play as
important a role as those postulated by alignment-
based models for the selection of semantic content
for subsequent referring expressions.
We also note that the Accuracy scores achieved
by our learned systems are similar to the best num-
bers previously reported in the REG literature. While
Jordan and Walker?s (2005) data set is not directly
comparable, they achieved a maximum of 59.9%
Accuracy, against our 58.8%. Stoia et al?s (2006)
best Accuracy was 31.2%, albeit on a slightly dif-
ferent task. Even in the arguably much simpler
non-dialogic domains of the REG competitions con-
cerned with pure content selection, the best perform-
ing system achieved only 53% Accuracy (see Gatt et
al., 2008). The most comparable approach, the rule-
based system we presented in (Viethen et al, 2010)
for a subset of the data used here, was not able to
outperform a RepeatLast baseline at 40.2% Accu-
racy and an average DICE score of 0.67.
6 Error Analysis
An important question to ask is how wrong the mod-
els really are when they do not succeed in perfectly
matching a human-produced reference in our test
set. It might be that they choose a completely dif-
Acc Dice Super Sub Inter Noover
Trad 47.3 0.75 14.4 22.2 5.5 10.5
Align 54.6 0.78 16.0 16.1 3.9 9.4
Ind 52.7 0.78 17.1 17.2 3.9 9.0
Align+Ind 58.2 0.81 16.0 14.8 3.1 7.9
Trad+Ind 52.5 0.78 17.4 17.5 3.8 8.8
Trad+Align 53.9 0.78 17.1 15.6 4.3 9.0
AllFeature 58.8 0.81 16.5 14.5 3.1 7.2
Table 6: The proportions of test instances for which each
model produced a subset, a superset, some other form of
intersection or no-overlap to the human reference.
ferent set of attributes from those included by the
human speaker; however, the Accuracy score also
counts as incorrect any set that only partly overlaps
with the reference found in the test set.
The DICE score gives us a partial answer to this
question, as it assigns a score that is based on the
size of the overlap between the attribute set cho-
sen by the model and that included by the human
speaker. A DICE score that is equal to the Accuracy
score would mean that each referring expression was
either reproduced perfectly, or that a set of attributes
was chosen that did not overlap with the original
one at all. The fact that all our models achieved
DICE scores much higher than their Accuracy scores
shows that they only rarely got it completely wrong.
Table 6 gives a more fine-grained picture by list-
ing, for each model, what percentage of the refer-
ring expressions it produced contained a subset of
the attributes included in the human reference, what
percentage were a superset, what percentage had
another form of partial intersection, and what per-
centage had no commonality with the human refer-
ence. Interestingly, a large number of the referring
expressions produced by the model trained only on
TradREG features are subsets of the human refer-
ence. This indicates that human speakers tend to in-
clude more attributes than are strictly speaking nec-
essary to distinguish the landmark.4 The Alignment
model does not as often produce a subset of the gold
standard content pattern, suggesting that it might be
alignment considerations that account for some of
4That humans often produce ?redundant? descriptions, in op-
position to the target behaviour of some of the early REG algo-
rithms, is of course an oft-observed fact.
1165
both both 1st 2nd either pot.
corr. wrong corr. corr. corr. Acc
Trad vs Ind 1797 1794 545 811 3153 63.7
Trad vs Align 1742 1647 600 958 3300 66.7
Trad vs Align+Ind 1849 1574 493 1031 3373 68.2
Align vs Trad+Ind 1908 1557 792 690 3390 68.5
Align vs Ind 1872 1511 828 736 3436 69.5
Ind vs Trad+Align 1840 1511 768 828 3436 69.5
Table 7: Comparison of the predictions for the combined
content pattern between the models trained on mutually
exclusive feature sets.
the apparent redundancy that human-produced refer-
ring expressions contain.
A second important question is whether the differ-
ent feature sets are doing the same work, or whether
they complement each other. Table 7 lists for those
pairings of our learned models which were based on
mutually exclusive feature sets how many referring
expressions both models predicted correctly, how
many both failed to predict, and how many were pre-
dicted correctly by either of the two models.
Note the high numbers in the columns listing the
counts of instances which both models got either
correct or wrong: these show that there is con-
siderable overlap between all pairings. The small-
est agreement lies at 3424 instances (68.2%) be-
tween TradREG (the least successful model) and
Alignment+Ind (the most successful model). How-
ever, they also each predict correct solutions that the
other misses: 493 (10.0%) for TradREG and 1031
(20.8%) for Alignment+Ind.
The last two columns of Table 7 show the number
of instances that at least one of the two models in
each pairing got correct and the proportion out of
all test instances that this number represents. This
proportion is the maximum Accuracy that could be
achieved by a model that combines the two models
in a pairing and then correctly chooses which one to
use in each instance. The maximum Accuracies that
could be achieved in this way on our data set lie just
below 70%, significantly higher than any numbers
reported in the literature on the task of generating
subsequent reference.
7 Conclusions
Using the largest corpus of referring expressions
to date, we have shown how both the traditional
computational view of REG and the alternative psy-
cholinguistic alignment approach can be captured
via a large set of features for machine learning. Ad-
ditionally, we defined a number of theory indepen-
dent features. Using this approach we have pre-
sented three main findings.
First, we have demonstrated that a model using all
these features to predict content patterns in subse-
quent references in shared visual scenes delivers an
Accuracy of 58.8% and a DICE score of 0.81, out-
performing models based only on features inspired
by one of the two approaches. However, we found
that the features based on traditional REG considera-
tions do not contribute as much to this score as those
based on the alignment approach, and that dropping
the traditional REG features does not significantly
hurt the performance of a model based on alignment
and theory-independent features.
Second, our error analysis showed that the main
reason for the low performance of a model based
on traditional algorithmic features is that it often
chooses too few attributes. The fact that the model
based on the alignment features does not make this
mistake so frequently suggests that it may be the
psycholinguistic considerations incorporated in our
alignment features that lead people to add those ad-
ditional attributes.
Finally, while the different models make the same
correct predictions about the content of referring ex-
pressions in many cases, there are also a consider-
able number of cases where the models based on
either the traditional algorithmic features (10.0%)
or the alignment and independent features (20.8%)
alone make correct predictions that the other gets
wrong; this suggests that a system with the ability
to choose the correct model in each of those cases
(perhaps based on a hypothesis as to whether or not
the relevant context has changed) could reach an ac-
curacy of almost 70% on our data set. In future work
we plan to identify further features that will allow us
to inform this choice so that we can move towards
this level of performance.
1166
References
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 22:1482?1493.
Hendrik Buschmeier, Kirsten Bergmann, and Stefan
Kopp. 2009. An alignment-capable microplanner for
natural language generation. In Proceedings of the
12th European Workshop on Natural Language Gen-
eration, pages 82?89, Athens, Greece.
John M. Carroll. 1980. Naming and describing in social
communication. Language and Speech, 23:309?322.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22(1):1?
39.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, Vancouver B.C.,
Canada.
Claire Gardent and Kristina Striegnitz. 2007. Generat-
ing bridging definite descriptions. In Harry C. Bunt
and Reinhard Muskens, editors, Computing Meaning,
volume 3, pages 369?396. Kluwer, Dordrecht, The
Netherlands.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA
Challenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation, pages 198?206, Salt
Fork OH, USA.
Albert Gatt. 2007. Generating Coherent Reference to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, UK.
Martijn Goudbeek and Emiel Krahmer. 2010. Pref-
erences versus adaptation during referring expression
generation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 55?59, Uppsala, Sweden.
Barbara J. Grosz and Candance L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Surabhi Gupta and Amanda Stent. 2005. Automatic
evaluation of referring expression generation using
corpora. In Proceedings of the Workshop on Using
Corpora for Natural Language Generation, pages 1?
6, Brighton, UK.
Srinivasan Janarthanam and Oliver Lemon. 2009. Learn-
ing lexical alignment policies for generating referring
expressions for spoken dialogue systems. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation (ENLG 2009), pages 74?
81, Athens, Greece, March. Association for Compu-
tational Linguistics.
Pamela W. Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting on As-
sociation for Computational Linguistics, Hong Kong,
China.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
Pamela W. Jordan. 2000. Intentional Influences on Ob-
ject Redescriptions in Dialogue: Evidence from an
Empirical Study. Ph.D. thesis, University of Pitts-
burgh, Pittsburgh PA, USA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Lingustics, 29(1):53?72.
Max M. Louwerse, Nick Benesh, Mohammed E. Hoque,
Patrick Jeuniaux, Gwyneth Lewis, Jie Wu, and Megan
Zirnstein. 2007. Multimodal communication in face-
to-face computer-mediated conversations. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society, pages 1235?1240.
Martin J. Pickering and Simon Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behavioral and
Brain Sciences, 27(2):169?226.
Laura Stoia, Darla M. Shockley, Donna K. Byron, and
Eric Fosler-Lussier. 2006. Noun phrase generation
for situated dialogs. In Proceedings of the 4th Interna-
tional Conference on Natural Language Generation,
pages 81?88, Sydney, Australia, July.
Kees van Deemter and Emiel Krahmer. 2007. Graphs
and Booleans: On the generation of referring expres-
sions. In Harry C. Bunt and Reinhard Muskens, edi-
tors, Computing Meaning, volume 3, pages 397?422.
Kluwer, Dordrecht, The Netherlands.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of the 5th International Conference on Natu-
ral Language Generation, pages 59?67, Salt Fork OH,
USA.
Jette Viethen, Simon Zwarts, Robert Dale, and Markus
Guhe. 2010. Dialogue reference in a visual domain.
In Proceedings of the 7th International Conference on
Language Resources and Evaluation, Valetta, Malta.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco CA, USA.
1167
