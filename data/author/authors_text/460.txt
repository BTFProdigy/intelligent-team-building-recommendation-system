Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1493?1502,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Towards Discipline-Independent Argumentative Zoning:
Evidence from Chemistry and Computational Linguistics
Simone Teufel
Computer Laboratory
Cambridge University
sht25@cl.cam.ac.uk
Advaith Siddharthan
Computer Laboratory
Cambridge University
as372@cl.cam.ac.uk
Colin Batchelor
Royal Society of Chemistry
Cambridge, UK
batchelorc@rsc.org
Abstract
Argumentative Zoning (AZ) is an anal-
ysis of the argumentative and rhetorical
structure of a scientific paper. It has been
shown to be reliably used by independent
human coders, and has proven useful for
various information access tasks. Annota-
tion experiments have however so far been
restricted to one discipline, computational
linguistics (CL). Here, we present a more
informative AZ scheme with 15 categories
in place of the original 7, and show that
it can be applied to the life sciences as
well as to CL. We use a domain expert
to encode basic knowledge about the sub-
ject (such as terminology and domain spe-
cific rules for individual categories) as part
of the annotation guidelines. Our results
show that non-expert human coders can
then use these guidelines to reliably an-
notate this scheme in two domains, chem-
istry and computational linguistics.
1 Introduction
Teufel et al (1999) define the task of Argumenta-
tive Zoning (AZ) as a sentence-by-sentence clas-
sification with mutually exclusive categories from
the annotation scheme given in Fig. 1. The reason-
ing behind the categories is inspired by the notion
of a knowledge claim (Myers, 1992; Luukkonen,
1992): the act of writing a paper corresponds to
an attempt of claiming ownership for a new piece
of knowledge, which is to be integrated into the
repository of scientific knowledge in the authors?
field by the process of peer review and publica-
tion. In the cause of this process, the authors
have to convince the reviewers that the knowledge
claim of the paper is valid (Swales, 1990; Hy-
land, 1998). What AZ aims to model, then, are
some of the relevant stages in this argument. We
divide the paper into zones, OTHER, OWN and
BACKGROUND. These are defined on the basis
of who owns the knowledge claim in the corre-
sponding segment. There are also two categories
which are defined by their relationship to existing
work, BASIS and CONTRAST. That means that
parts of the AZ scheme are similar to citation func-
tion classification schemes from the area of cita-
tion content analysis (Garfield, 1965; Weinstock,
1971; Spiegel-Ru?sing, 1977), and to automatic
citation function classification (Nanba and Oku-
mura, 1999; Garzone and Mercer, 2000; Teufel
et al, 2006). The remaining categories, AIM and
TEXTUAL, fulfil different rhetorical functions for
the presentation of the paper. AIM points out the
paper?s main knowledge claim, a rhetorical move
which may be repeated in the conclusion and the
introduction. TEXTUAL explains the physical lo-
cation of information, e.g., by giving a section
overview or presenting a summary of a subsec-
tion. On the basis of human-annotated training
material, AZ can be automatically classified using
supervised machine learning.
Category Description
AIM Statement of research goal.
BACKGROUND Description of generally accepted
background knowledge.
BASIS Existing KC provides basis for new
KC.
CONTRAST An existing KC is contrasted, com-
pared, or presented as weak.
OTHER Description of existing KC.
OWN Description of any other aspect of
new KC.
TEXTUAL Indication of paper?s textual
structure.
Figure 1: AZ Annotation Scheme (Teufel et al
1999).
Rhetorical information marking is useful for
1493
many novel information access tasks. For in-
stance, information retrieval can profit from
rhetorical information in the form of paradigm
shift statements (Chichester et al, 2005), as papers
containing such statements have a high impact in
an area. 75% of the ?Faculty of 1000 Biology?
papers (which are chosen by experts for their spe-
cial importance) contain paradigm shift sentences
(Agnes Sandor, personal communication).
AZ annotation allows the construction of multi-
and single document summaries which concen-
trate on differences and similarities to related
(cited) work. AZ can also be used for search in
a data base of scientific articles, in particular for
enhanced citation indexing. This has been pre-
viously explored in a task-based evaluation, were
users were asked to list positive and negative cita-
tions they would expect in a paper, given a short
extract (Teufel, 2001). In that task, AZ-based ex-
tracts outperformed other document surrogates.
Feltrim et al (2005) present a writing support
system which analyses students? drafts of sum-
maries for their PhD theses, performs an AZ anal-
ysis on them and critiques the rhetorical structure
of the students? draft on the basis of it.
The definition of the AZ categories is based
on rhetorical principles and should be decidable,
in principle, without specific domain knowledge
about what is discussed in detail in the paper. We
present here the first evidence that AZ categories
can be reliably recognised across scientific disci-
plines, using chemistry and computational linguis-
tics as our model disciplines for these experiments.
The categories just introduced are abstract and
depend on the annotators? interpretation of a
rhetorical argument. This means that there is
no guarantee that several independent annotators
would annotate similarly. It is therefore crucial
that all annotations at a high level of interpreta-
tion are backed up by human annotation with more
than one annotator. However, annotations of cita-
tion function classification typically use only the
untested annotation of a single human annotator
as gold standard, who is typically the designer of a
scheme (Spiegel-Ru?sing, 1977; Weinstock, 1971;
Nanba and Okumura, 1999; Garzone and Mercer,
2000). Teufel et al (2006) are the only exception
who test their citation function scheme using mod-
ern corpus-linguistic annotation methodology.
A study of human agreement on AZ annotation
exists (Teufel et al, 1999), but this uses articles
from only one discipline, namely computational
linguistics. In this paper, we use a similar method-
ology to Teufel et al, but with data from two disci-
plines. The preliminary conclusion from these ex-
periments is that annotation with chemistry papers
has resulted in higher agreement than annotation
with computational linguistics papers.
We extend the AZ annotation scheme to make
further distinctions, as will be discussed in sec-
tion 2. We also created an environment in which
domain knowledge that an annotator might have
about the science in a paper is systematically dis-
regarded. We will describe how this was done in
section 3, and then present the annotation experi-
ment itself in section 4.
2 Changes to the AZ Scheme
Argumentative Zoning II (AZ-II) is a new annota-
tion scheme, which is an elaboration of the orig-
inal AZ scheme. It is presented in Fig. 2. Our
annotation guidelines are 111 sides of A4 and con-
tain a decision tree, detailed description of the se-
mantics of the 15 categories, 75 rules for pairwise
distinction of the categories and copious examples
from both chemistry and computational linguis-
tics. During guideline development, 70 chemistry
papers and 20 CL papers were used, which are dis-
tinct from the ones used for annotation. It took 3
months part-time-work to prepare the guidelines
for CL, and substantially less time to adapt them
for chemistry. We have made them available at
www.cl.cam.ac.uk/research/nl/sciborg.
The differences between the original AZ and
AZ-II are as follows:
? Category AIM remained the same.
? Category BACKGROUND was renamed
CO GRO, or common ground.
? Category OTHER was split into other peo-
ple?s work (OTHR) and the authors? own pre-
vious work (PREV OWN).
? Category BASIS was split into usage (USE)
and support (SUPPORT).
? Category CONTRAST was split into neu-
tral comparison (CODI), contradiction
(ANTISUPP), and a category combining
research gaps with criticism (GAP WEAK).
? Category OWN was split into description of
method (OWN MTHD), results (OWN RES)
and conclusions (OWN CONC), and a cate-
gory which specifies recoverable errors made
by the authors (OWN FAIL).
1494
Category Description Category Description
AIM Statement of specific research goal, or
hypothesis of current paper
OWN CONC Findings, conclusions (non-measurable)
of own work
NOV ADV Novelty or advantage of own approach CODI Comparison, contrast, difference to
other solution (neutral)
CO GRO No knowledge claim is raised (or knowl-
edge claim not significant for the paper)
GAP WEAK Lack of solution in field, problem with
other solutions
OTHR Knowledge claim (significant for paper)
held by somebody else. Neutral descrip-
tion
ANTISUPP Clash with somebody else?s results or
theory; superiority of own work
PREV OWN Knowledge claim (significant) held by
authors in a previous paper. Neutral de-
scription.
SUPPORT Other work supports current work or is
supported by current work
OWN MTHD New Knowledge claim, own work:
methods
USE Other work is used in own work
OWN FAIL A solution/method/experiment in the pa-
per that did not work
FUT Statements/suggestions about future
work (own or general)
OWN RES Measurable/objective outcome of own
work
Figure 2: AZ-II Annotation Scheme.
? Category TEXTUAL was discontinued, be-
cause it is less informative than the other cat-
egories.
? Two new categories were introduced,
NOV ADV (advantages of the new knowl-
edge claim) and FUT (declaration of
limitations or future work).
Our AZ-II categories are more fine-grained than
the original AZ categories. The reasons for this are
twofold: To bring AZ closer to contemporary cita-
tion function schemes, and to incorporate distinc-
tions recently found useful by other researchers.
For instance, Chichester et al (2005) argue that
ANTISUPP is particularly important. The finer
grain in AZ-II has been accomplished purely by
splitting existing AZ categories; hence, the coarser
AZ categories are recoverable (with the exception
of the TEXTUAL category). Annotation examples
are given in the appendix.
As in AZ, citations are an important but not nec-
essarily decisive cue for a sentence to belong to
a particular zone. The guidelines mention cita-
tions as one factor in deciding whether a knowl-
edge claim holds, and citations occur in several
examples, so it is likely that the presence of ci-
tations would have influenced annotators in their
decision.
Of the changes, the distinction which is likely
to have the greatest impact on the annotation is
the split of OWN according to the stage of the au-
thors? problem solving process ? into methods, re-
sults, conclusion or local failure. In most life sci-
ences, descriptions of research as a problem solv-
ing process are a dominant phenomenon, whereby
problem-solving descriptions can be of differing
length and embeddedness. For instance, in syn-
thetic chemistry, the starting compound for the
main synthesis in the paper may first have to be
synthesised itself (if it is not commercially avail-
able, for instance). In that case, arriving at the
compound is an intermediate, smaller problem-
solving process which enables the larger problem-
solving process that represents the new KC.
The original AZ scheme didn?t mark the dis-
tinction, possibly because it is not as easily ob-
servable in CL as it is in the life sciences, and
because problem-solving stages were not part of
the main analytic interest of AZ, which focused
on how scientific argumentation is related to de-
scriptions of own and other work. Also, neither of
the traditional AZ applications (summarisation or
citation indexing) had any direct use for the subdi-
vided categories. But in the life sciences, there
are applications which would make use of such
a subdivision. For instance, in chemistry there
is a niche for search applications which guide
searchers directly to the method and/or result sec-
tions in papers. Specifically, the OWN FAIL cat-
egory is motivated by the failure?and?recovery
search. In text, OWN FAILmarks cases where the
authors helpfully mention in passing steps which
were found not to work during a long synthetic
procedure (often the ?total synthesis? of a com-
pound which is found in nature). Such cases hap-
pen frequently, and are generally followed by a
?recovery? statement which explains how the prob-
lem can be avoided. Another possible applica-
tion that calls for a subdivision is Feltrim et al?s
1495
(2005) rhetorical writing system for novice writ-
ers. It trains novices in writing rhetorically well-
formed abstracts and therefore must have a way of
distinguishing, for instance, between methods and
results.
Note that several of the applications based on
AZ and AZ-II in general rely on the rare categories
much more than they rely on the more frequent
categories. OWN FAIL is an example of a rare but
important category, and so is AIM, which is central
to summarisation applications. The comparative
and contrastive categories CODI ANTISUPP and
GAP WEAK, on the other hand, are particularly
useful to citation-based search applications.
Other AZ-like schemes for scientific discourse
created for the biomedical domain (Mizuta and
Collier, 2004) and for computer science (Feltrim
et al, 2005) also made the decision to subdivide
OWN, in similar ways to how we propose here.
The current work, however, is the first experimen-
tal proof that humans can make this distinction ?
and the others encoded in AZ-II ? reliably, and in
two quite distinct disciplines.
3 Discipline-Independent Non-Expert
Annotation
An important principle of AZ is that its categories
can be decided without domain knowledge. This
rule is anchored in the guidelines: when choosing
a category, no reasoning about the scientific facts
is allowed. The avoidance of domain-knowledge
has its motivation in a strategy for a hypotheti-
cal automatic text-understanding system for unre-
stricted texts. Given the state of the art in text pro-
cessing and knowledge representation, text under-
standing systems should in our opinion use gen-
eral, rhetorical, and logical aspects of the text,
rather than attempting to recognise or represent the
scientific knowledge contained in the text. What
the human annotation ? the gold standard ? should
then do is to simulate the best possible output that
such a system could theoretically create.
Annotators may use only general, rhetorical or
linguistic knowledge; knowledge which is shared
by all proficient speakers of a language. The
guidelines spell out what is meant by these general
principles. For instance, one can use lexical and
syntactic parallelism in a text to infer that the au-
thors were setting up a comparison between them-
selves and some other approach.
There is, however, a problem with annotator ex-
pertise and with the exact implementation of the
?no domain knowledge? principle. This problem
does not become apparent until one starts work-
ing with disciplines where at least some of the an-
notators or guideline developers are not domain
experts (chemistry, in our case). Domain experts
naturally use scientific knowledge and inference
when they make annotation decisions. It would
be unrealistic to expect them to be able to disre-
gard their domain knowledge simply because they
were instructed to do so. Additionally, when all
annotators/scheme developers are domain experts,
it is hard to even notice the cases where they ?ac-
cidentally? use domain knowledge during anno-
tation. We therefore artificially created a situa-
tion where all annotators are ?semi-informed non-
experts?, which forces them to comply with the
principle, namely by the following rules:
Justification: Annotators have to justify all an-
notation decisions by pointing to some text-based
evidence, and by giving the section heading in the
guidelines that describes the particular reason for
assigning the category. General discipline-specific
knowledge an annotator may happen to have is ex-
cluded as justification. Annotators? justifications
have to be typed into the annotation tool and are
open to challenge during the training phase. Much
of the allowable justification comes in the form
of general and linguistic principles, e.g., an ex-
plicit cue phrase, the title, or the structural simi-
larity of textual strings. For instance, annotators
are allowed to infer that process-VPs in the title
are likely to be the contribution (knowledge of the
actual concrete contribution of a paper is a require-
ment for annotation of AIM).
Discipline-specific Generics: The guidelines
contain a section with high-level facts about the
general research practices in the discipline. These
generics constitute the only scientific knowledge
which is acceptable as a justification, and are
aimed to help non-expert annotators recognise
how a paper might relate to already established
scientific knowledge, so that they will be able
to avoid common mistakes about the knowledge
claim status of a certain fact. For instance, the bet-
ter they are able to distinguish what is commonly
known from what is newly claimed by the authors,
the more consistent their annotation will be.
Annotation with expert-trained non-expert an-
notators means that a domain expert must be avail-
able initially, during the development of the anno-
1496
tation scheme and the guidelines, either as a co-
developer or as an informant. The domain expert?s
job is to describe scientific knowledge in that do-
main in a general way, in as far as it is neces-
sary for the scheme?s distinctions, and to write the
domain-specific rules for the individual categories,
including the choice of example sentences. This
means that the guidelines are split into a domain-
general and a domain-specific part.
The discipline-specific generics in chemistry
come in the form of a ?chemistry primer?, a 10-
page collection of high-level scientific domain
knowledge. It contains: a glossary of words a non-
chemist would not have heard about or would not
necessarily recognise as chemical terminology; a
list of possible types of experiments performed
in chemistry; a list of commonly used machin-
ery; a list of non-obvious negative characterisa-
tions of experiments and compounds (?sluggish?,
?inert?); and a list of possible types of knowledge
claims. For instance, in chemistry each chemi-
cal substance mentioned can have in principle a
knowledge claim associated with its discovery or
invention ? with the exception of water, rock salt,
the metals known in prehistory and a few others.
If a compound or process is however considered to
be so commonly used that it is in the ?general do-
main? (e.g., ?the Stern?Volmer equation? or ?the
Grignard reaction?), it is no longer associated with
somebody?s knowledge claim, and as a result its
usage is not to be marked with category USE.
Descriptions of individual categories can have
domain-specific subsections, as well as the gen-
eral ones. For instance, if the text states that the
authors could not replicate a published result, the
guidelines describe the cases when this is the au-
thors? fault (OWN FAIL) in contrast to the cases
where this is an indirect accusation of the previ-
ous experiment (ANTISUPP).
Another potentially unclear distinction is
between results (OWN RES) and conclusions
(OWN CONC). The difference is defined on
the basis of how much reasoning is necessary
to be able to make the statement concerned. If
all the authors did was to read a measurement
off an instrument, the label OWN RES applies.
Reasoning points to OWN CONC; it is some-
times linguistically marked (?therefore?, ?we
conclude?, ?this means that?), but in many cases,
domain knowledge may be required to decide
whether reasoning was necessary to make a
certain statement. Possible OWN RES statements,
according to the chemistry primer, include: state-
ments of simple numerical result; descriptions of
graphs; descriptions of atoms? positions in three-
dimensional space; statements of trends, unless
a reason for these results is given; comparisons
of results of more than one experiment, unless a
reason for these results is given.
The chemistry primer also lists phenomena
which in a typical experiment would be read off
chemical machinery (e.g., ?Stark effect?). This list
gives the non-expert annotator an objective crite-
rion to answer the question how likely it is that a
certain statement by the authors was arrived at by
inference. We also found that our list of phenom-
ena which can be read off machinery, which was
compiled from the first 30 papers, generalised well
to the other 40 papers considered.
The chemistry primer is not an attempt to sum-
marise all methods and experimentation types in
chemistry; this would be impossible to do, cer-
tainly in a few pages. Rather, it tries to answer
many of the high-level questions a non-expert
would have to an expert, in the framework of AZ.
This methodology allows to hire expert and
non-expert annotators and bring them in line with
each other. We believe it could be expanded rel-
atively easily into many other disciplines, using
domain experts which create similar primers for
genetics, experimental physics, cell biology, but
re-using the bulk of the guidelines.
4 Annotation Experiments
The annotators were the co-developers of the an-
notation scheme and the authors of this paper.
Whereas all three annotators have good back-
ground knowledge in CL, the largest difference be-
tween them concerns their expertise in chemistry:
Annotator A is a PhD-level chemist, Annotator B
has two years? of undergraduate training in chem-
istry and can therefore be considered a chemical
semi-expert, and Annotator C has no specialised
chemistry knowledge.
As agreement measure we choose the Kappa
coefficient ? (Fleiss, 1971; Siegel and Castellan,
1988), the agreement measure predominantly used
in natural language processing research (Carletta,
1996). ? corrects raw agreement P (A) for agree-
ment by chance P (E):
? =
P (A)?P (E)
1?P (E)
1497
No matter how many items or annotators, or
how the categories are distributed, ? = 0 when
there is no agreement other than what would be
expected by chance, and ? = 1 when agreement
is perfect. If two annotators agree less than ex-
pected by chance, ? can also be negative. Chance
agreement P (E) is defined as the level of agree-
ment which would be reached by random anno-
tation using the same distribution of categories as
the real annotators. All work done here is reported
in terms of Fleiss? ?. 1 ? is also designed to ab-
stract over the number of annotators as its formula
relies on the proportion of expected vs. observed
pairwise agreements possible in a pool. That is,
? for k annotators will be an average of the val-
ues of ? taking all possible m-tuples of annota-
tors from the annotator pool (with m < k). As a
side effect of its definition of random agreement,
? treats agreement in a rare category as more sur-
prising, and rewards such agreement more than an
agreement in a frequent category. This is a desir-
able property, because we are more interested in
the performance of the rare rhetorical categories
than we are in the performance of the more fre-
quent zone categories.
4.1 Data
For chemistry, 30 random-sampled papers from
journals published between 2004 and 2007 by the
Royal Society of Chemistry were used for anno-
tation2. The papers cover all areas of chemistry
and some areas close to chemistry, such as climate
modelling, process engineering, and a double-
blind medical trial. The data used for the exper-
iment contains a total of 3745 sentences.
For computational linguistics, 9 papers were an-
notated, with a total of 1629 sentences. The papers
were published between 1998 and 2001 at ACL,
EACL or EMNLP conferences, and were taken
from the Computation and Language archive.
Both chemistry and CL papers were automatically
sentence-split, with manual correction of errors;
acknowledgement sections were disregarded. A
1Artstein and Poesio (2008) observe that there are several
version of ? which differ in how P (E) is calculated. In par-
ticular, Fleiss? (1971) ? calculates P (E) as the average ob-
served distribution of all annotators, whereas Cohen?s (1960)
? calculates P (E) only on the basis of the other annotator(s).
2100 papers across a spread of disciplines from the Jan-
uary 2004 issues of the RSC were selected blindly (but with
an attempt to cover most areas of chemistry). 30 out of these
were random sampled for annotation; the rest were used for
annotation development.
Category Chem CL Category Chem CL
OWN MTHD 25.4 55.6 SUPPORT 1.5 0.7
OWN RES 24.0 5.6 GAP WEAK 1.1 1.0
OWN CONC 15.1 10.7 FUT 1.0 1.4
OTHR 8.3 10.0 NOV ADV 1.0 0.8
USE 7.9 2.7 CODI 0.8 1.2
CO GRO 6.7 5.7 OWN FAIL 0.8 0.1
PREV OWN 3.4 1.7 ANTISUPP 0.5 0.6
AIM 2.3 1.8
Figure 3: Frequency of AZ-II Categories (in %).
web-based annotation tool was used for guideline
definition and for annotation.
Our choice of which data sets to use was ef-
fected by the relative length of papers more than
by the journal/conference distinction. Average
article length between chemistry journal articles
(3650 words/paper) and CL conference articles
(4219 words/paper) is comparable, so conference
articles in CL seem a much better choice for com-
parative work than journal publications, which are
often very long in CL. Additionally, conferences
have a high profile in CL, and we found the con-
ference publications to be of high editorial quality.
We are nevertheless interested in the structure of
longer journal articles, and plan to investigate CL
journals in the future.
The annotations were done using a web-based
annotation tool. Every sentence is assigned a cat-
egory. No communication between the annotators
was allowed.
4.2 Results
The inter-annotator agreement for chemistry
was ? = 0.71 (N=3745,n=15,k=3). For CL,
the inter-annotator agreement was ? = 0.65
(N=1629,n=15,k=3). For comparison, the
inter-annotator agreement for the original, CL-
specific AZ with 7 categories was ? = 0.71
(N=3420,n=7,k=3). Given the subjective nature
of the task and the fact that AZ-II introduces ad-
ditional distinctions, the AZ-II agreement can be
considered acceptable for CL and relatively high
for chemistry. Additionally, chemistry annota-
tion used one non-expert annotator, who had no
chemistry-specific domain knowledge apart from
that in the chemistry primer.
The distribution of categories for the two disci-
plines is given in Fig. 3. As expected, there is a
large discrepancy in frequency between the (rare)
rhetorical categories and the (much more fre-
quent) zone categories OWN MTHD, OWN RES,
1498
OWN CONC, OTHR and CO GRO. For supervised
learning, too few examples of any category can be
a problem. There are methods which attempt to re-
duce the annotation effort by using a trained clas-
sifier to suggest possible cases to a human. How-
ever, the classifier can only find examples similar
to the ones that have already been manually clas-
sified, when the real problem is a recall-problem,
i.e., the challenge is to find more new examples in
the multitude of possible sentences. To solve this
in a fundamentally sound way, there seems to be
no other way than to annotate more texts, at the
cost of more human effort.
If we consider the differences across disci-
plines, the most striking ones concern the distri-
bution of OWN MTHD, which is more than twice
as common in CL (56% v. 25%), and OWN RES,
which is far more common in chemistry overall
(24% v 5.6%). Usage of other people?s knowl-
edge claims or materials also seems to be more
common in chemistry, or at least more explicitly
expressed (7.9% vs 2.7%). With respect to the
shorter, rarer categories, there is a marked dif-
ference in OWN FAIL (0.1% in CL, but 0.8% in
chemistry3 and SUPPORT, which is more common
in chemistry (1.5% vs 0.7%). However, this effect
is not present for ANTISUPP (contradiction of re-
sults), the ?reverse? category to SUPPORT, (0.6%
in CL vs 0.5% in chemistry).
As far as the chemistry annotation is con-
cerned, it is interesting to find out whether Annota-
tor A was influenced during annotation by domain
knowledge which Annotator C did not have, and
Annotator B had to a lower degree4. We there-
fore calculated pairwise agreement, which was
?
AC
= 0.66, ?
BC
= 0.73 and ?
AB
= 0.73 (all:
N=3745,n=15,k=2). That means that the largest
disagreements were between the non-expert (C)
and the expert (A), though the differences are
modest. This might point to the fact that Anno-
tators A and B might have used a certain amount
of domain-knowledge which the chemistry primer
in the guidelines does not yet, but should, cover.
In an attempt to determine how well cate-
gories are defined, we first consider the binary dis-
3These are not large differences in absolute terms ? 55
items identified as OWN FAIL by at least one annotator in
chemistry, vs. 7 such items in CL, the relative difference is
large and confirms that in chemistry papers, particularly de-
scriptions of synthesis procedures, OWN FAIL cases appear
relatively frequently.
4This question does not arise in the case of CL, as all an-
notators can be considered experts in this respect.
tinction between zone categories (OWN MTHD,
OWN RES, OWN CONC, OWN FAIL, OTHR,
PREV OWN and CO GRO) and rhetorical cate-
gories (the other 8). This shows an inter-annotator
agreement of ?
binary
= 0.78 (N=3745, n=2, k=3)
for chemistry and ?
binary
= 0.65 (N=1629, n=2,
k=3) for CL, indicating that annotators find it rel-
atively easy (chemistry) or at least not more dif-
ficult than the overall distinction (CL) to distin-
guish these two types of categories. We next per-
form Krippendorff?s (1980) category distinctions
(Fig. 4). Here, all categories apart from the one
diagnosed are collapsed, and what is reported is
the difference of inter-annotator agreement when
compared to the overall distinctiveness (?=0.71
for chemistry, ?=0.65 for CL). Where the differ-
ence is positive, the annotators could distinguish
the given category better than they could distin-
guish all categories, and where they are negative,
correspondingly worse.5
The results confirm that categories USE, AIM,
OWN MTHD, OWN RES and FUT are particularly
well distinguished in both disciplines. This is a
positive result, as these categories are important
for several types of searches. In these cases the
guidelines seem to fully suffice for their descrip-
tion, but then again good performance of AIM,
FUT and USE is not that surprising, as they are
signalled clearly by linguistic and non-linguistic
cues. However, there are three categories with
particularly low distinguishability in both disci-
plines: ANTISUPP, OWN FAIL and PREV OWN.
As ANTISUPP and OWN FAIL are crucial for the
envisaged downstream tasks, the problems with
their definition should be identified and fixed. We
are in the process of systematically troubleshoot-
ing the guidelines for those categories.
The table also shows that category definition
has discipline-specific problems. For instance,
we believe that the fact that distinctiveness for
OWN FAIL is so bad for CL must be due to the
fact that we only encountered very few potential
OWN FAIL cases in this domain. The definition
of the categories SUPPORT and NOV ADV also
seem to be substantially more confusing for CL
than for chemistry. However, CODI is a category
which shows average distinctiveness for CL, but
much worse distinctiveness for chemistry. We be-
lieve this is due to the fact that comparisons of
5All ? values for chemistry were measured with N=3745,
n=2, k=3; for CL with N=1629, n=2, k=3.
1499
methods and approaches are more common in CL
and are clearly expressed, whereas in chemistry
the objects that are involved in comparisons are
more varied and at a lower grade of abstraction
(e.g., compounds, properties of compounds, coef-
ficients, etc.), which obviously has a negative ef-
fect on the distinctiveness of this category.
Category Chem CL Category Chem CL
USE +0.12 +0.00 NOV ADV -0.07 -0.23
AIM +0.09 +0.08 OWN CONC -0.08 -0.13
OWN MTHD +0.05 +0.05 GAP WEAK -0.08 -0.16
OWN RES +0.02 +0.04 PREV OWN -0.11 -0.15
FUT +0.01 +0.06 OWN FAIL -0.19 -0.43
CO GRO -0.01 -0.03 ANTISUPP -0.35 -0.32
SUPPORT -0.04 -0.12 CODI -0.36 +0.00
OTHR -0.06 +0.07
Figure 4: Krippendorff?s Diagnostics for Category
Distinction (?, relative to Overall Distinctiveness).
We also provide a direct comparison of our an-
notation results with those from the original AZ
scheme. Comparisons between two similar anno-
tation schemes can be made by collapsing those
categories in each scheme which are not distin-
guished in the other scheme. Such a comparison
can of course only ever approximate the smallest
common denominator between two schemes.
The AZ-II categories were collapsed into a set
of six categories that closely resemble AZ cate-
gories, as described in section 2 (with OWN simu-
lated by the union of OWN FAIL, OWN MTHD,
OWN RES, OWN CONC, FUT, and NOV ADV).
This created a 6-category AZ annotation.
As TEXTUAL is not marked up in AZ-II, the
original AZ annotation was also collapsed, by in-
corporating TEXTUAL examples into OWN. The
two 6-pronged AZ-annotations are now more di-
rectly comparable. Inter-annotator agreement for
the collapsed AZ-II showed ? = 0.75 (N=3745,
n=6, k=3). This compares favourably to the col-
lapsed AZ?s agreement of ? = 0.71 (N=3420, n=6,
k=3); but when comparing the raw numerical re-
sults one should consider that different data from
different disciplines is used (chemistry in AZ-II,
CL in AZ).
These results should be interpreted as a pos-
itive result for the domain-independence of AZ,
and also for the feasibility of using trained non-
experts as annotators. The additional work that
went into the guidelines has produced annotation
of a high consistency, even though AZ-II provides
more distinctions (15 categories vs. 7 in AZ).
There is also the faint possibility that discourse
annotation of chemistry is intrinsically easier than
discourse annotation of CL, because it is a more
established discipline and not despite of it. For
instance, it is likely that the problem-solving cat-
egories OWN FAIL, OWN MTHD, OWN RES and
OWN CONC are easier to describe in a discipline
with an established methodology (such as chem-
istry), than they are in a younger, developing dis-
cipline such as computational linguistics.
5 Conclusion
Argumentative Zoning is an analysis of the rhetor-
ical progression of the scientific argument in a pa-
per. In this paper, we have made the following
contributions to this analysis:
? We have presented a more informative
scheme, which additionally recognises the
structure of an experiment in terms of prob-
lem solving (method ? results ? conclusions)
and makes more fine-grained distinctions in
some of the sentiment-inspired relational cat-
egories (e.g., criticism and comparisons to
other approaches).
? We introduced an annotation methodology
which attempts to systematically exclude the
use of annotators? extraneous domain knowl-
edge from the annotation.
? We have experimentally shown that human
coders can independently annotate this new
AZ scheme in two distinct disciplines. Our
results show inter-annotator agreements of
?=0.65 and ?=0.71 for computational lin-
guistics and chemistry, respectively.
Overall, the outcome of this work indicates
that the phenomena described in AZ can be de-
fined in a domain-independent way. The experi-
ment also tested how realistic the ?expert-trained
non-expert? approach to domain-knowledge free
annotation is. The fact that the agreement be-
tween three annotators (an expert, a semi-expert,
and a non-expert) is acceptable overall vindicate
our task definition as domain-knowledge free (us-
ing the tools of justification and domain-specific
generic knowledge). However, the agreements in-
volving the semi-expert are higher than the agree-
ment between expert and non-expert. This prob-
ably means that the chemistry generics were not
fully adequate to ensure that the non-expert un-
derstood enough of the chemistry to achieve the
highest-possible agreement.
1500
The automation of AZ-annotation is underway.
This requires adaptation of the high-level features
used in AZ (Teufel and Moens, 2002) to chemistry.
We are also preparing an annotation experiment
with naive annotators. Another research avenue
is the expansion of the guidelines to other disci-
plines such as bio-medicine, and to longer journal
articles, e.g., in computational linguistics.
6 Acknowledgements
This work was funded by EPSRC project Sciborg
(EP/C010035/1).
Appendix: Annotation Examples6
AIM We now describe in this paper a synthetic route for the
functionalisation of the framework of mesoporous organosil-
ica by free phosphine oxide ligands, which can act as a tem-
plate for the introduction of lanthanide ions. (b514878b)
AIM The aim of this paper is to examine the role that train-
ing plays in the tagging process . . . (9410012)
NOV ADV Moreover, the simplicity and ease of application
of the electrochemical method [...] should also be emphasised
and makes it an interesting and valuable synthetic tool.
(b513402a)
NOV ADV Other than the economic factor, an important ad-
vantage of combining morphological analysis and error detec-
tion/correction is the way the lexical tree associated with the
analysis can be used to determine correction possibilities.
(9504024)
CO GRO A wide range of organosulfur compounds are bi-
ologically active and some find commercial application as
fungicides and bactericides1?4 . (b514441h)
CO GRO It has often been stated that discourse is an inher-
ently collaborative process . . . (9504007)
OTHR In their system, antibody immobilized on a solid sub-
strate reacts with antigen, which binds with another antibody
labelled with peroxidase. (b313094k)
OTHR But in Moortgat?s mixed system all the different re-
source management modes of the different systems are left in-
tact in the combination and can be exploited in different parts
of the grammar. (9605016)
PREV OWN As a program aimed at the applications of
imines(2a,g,5) we have studied the formation of carbanions
from imines and their subsequent reactions. (b200198e)
PREV OWN Earlier work of the author (Feldweg 1993;
Feldweg 1995a) within the framework of a project on corpus
based development of lexical knowledge bases (ELWIS) has
produced LIKELY . . . (9502038)
OWN MTHD In order for it to be useful for our purposes,
the following extensions must be made: (0102021)
OWN MTHD On the other hand, a tertiary amide can be an
excellent linking functional group. (b201987f)
6Corpus examples are taken from our chemistry and CL
data sets; indicated by their respective file numbers.
OWN FAIL Initial attempts to improve the dehydration of 4
via chemical or thermal means were unsuccessful; similarly,
attempts to couple the chlorosilane (Me3Si)2 (Me2ClSi)CH
with Ag2O failed. (b510692c)
OWN FAIL When the ABL algorithms try to learn with two
completely distinct sentences, nothing can be learned.
(0104006)
OWN RES While the acid 1a readily coupled to the olefin,
the corresponding boronic ester was surprisingly inert under
the reaction conditions. (b311492a)
OWN RES All the curves have a generally upward trend but
always lie far below backoff (51% error rate). (0001012)
OWN CONC It is unlikely that every VOC emit ted by plants
serves an ecological or physiological role . . . (b507589k)
OWN CONC Unless grammar size takes on proportionately
much more significance for such longer inputs, which seems
implausible, it appears that in fact the major problems do not
lie in the area of grammar size, but in input length. (9405033)
GAP WEAK Various methods of preparation have been de-
veloped, but they often suffer from low yield and tedious
separation.[16,17,28,31] (b200888m)
GAP WEAK Here, we will produce experimental evidence
suggesting that this simple model leads to serious overesti-
mates of system error rates. . . (9407009)
CODI However, the measured values of the dielectric con-
stant (? = 310) are lower than the values reported by Ganguli
and coworkers(21) for BSTO pellets sintered at 1100 degC . . .
(b506578j)
CODI Unlike most research in pragmatics that focuses on
certain types of presuppositions or implicatures, we provide a
global framework in which one can express all these types of
pragmatic inferences. (9504017)
SUPPORT This is in line with the findings of Martin and Illas
for inorganic solids (84,85) . (b515732c)
SUPPORT Work similar to that described here has been car-
ried out by Merialdo (1994), with broadly similar conclusions.
(9410012)
USE The diamine 10 was prepared following a previously
published procedure(4d) . (b110865b)
USE We use the framework for the allocation and transfer
of control of Whittaker and Stenton (1988). (9504007)
FUT Our further efforts are directed towards the above
goal,. . . and overcoming limitations pertaining to the electron-
poor arylboronic acids. (b311492a)
FUT An important area for future research is to develop
principled methods for identifying distinct speaker strategies
pertaining to how they signal segments. (9505025)
ANTISUPP Although purification of 8b to a de of 95percent
has been reported elsewhere[31], in our hands it was always
obtained as a mixture of the two [EQN]-diastereomers.
(b310767a)
ANTISUPP This result challenges the claims of recent dis-
course theories (Grosz and Sidner 1986, Reichman 1985)
which argue for a the close relation between cue words and
discourse structure. (9504006)
1501
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational Lin-
guistics, 34(4):555?596.
Jean Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Christine Chichester, Frdrique Lisacek, Aaron Kaplan, and
Agnes Sandor. 2005. Discovering paradigm shift patterns
in biomedical abstracts: application to neurodegenerative
diseases. In Proceedings of First International Sympo-
sium on Semantic Mining in Biomedicine.
Jacob Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measurement,
20:37?46.
V. Feltrim, Simone Teufel, Gracas Nunes, and S. Alu-
sio. 2005. Argumentative zoning applied to critiquing
novices? scientific abstracts. In Janyce Wiebe James
G. Shanahan, Yan Qu, editor, Computing Attitude and Af-
fect in Text: Theory and Applications, pages 233?245.
Springer, Dordrecht, The Netherlands.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76:378?381.
Eugene Garfield. 1965. Can citation indexing be automated?
In M. et al Stevens, editor, Statistical Association Meth-
ods for Mechanical Documentation (NBS Misc. Pub. 269).
National Bureau of Standards, Washington.
Mark Garzone and Robert E. Mercer. 2000. Towards an au-
tomated citation classifier. In Proceedings of the 13th Bi-
ennial Conference of the CSCI/SCEIO (AI-2000), pages
337?346.
Ken Hyland. 1998. Persuasion and context: The pragmat-
ics of academic metadiscourse. Journal of Pragmatics,
30(4):437?455.
Klaus Krippendorff. 1980. Content Analysis: An Introduc-
tion to its Methodology. Sage Publications, Beverly Hills,
CA.
Terttu Luukkonen. 1992. Is scientists? publishing behaviour
reward-seeking? Scientometrics, 24:297?319.
Yoko Mizuta and Nigel Collier. 2004. An annotation scheme
for rhetorical analysis of biology articles. In Proceedings
of LREC?2004.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4):295?
313.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference information.
In Proceedings of the XXth International Joint Conference
on Artificial Intelligence (IJCAI-99), pages 926?931.
Sidney Siegel and N. John Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-Hill,
Berkeley, CA, 2nd edition.
Ina Spiegel-Ru?sing. 1977. Bibliometric and content analy-
sis. Social Studies of Science, 7:97?113.
John Swales, 1990. Genre Analysis: English in Academic
and Research Settings. Chapter 7: Research articles in
English, pages 110?176. Cambridge University Press,
Cambridge, UK.
Simone Teufel and Marc Moens. 2002. Summarising scien-
tific articles ? experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999. An
annotation scheme for discourse-level argumentation in
research articles. In Proceedings of the 9th Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL-99), pages 110?117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
Automatic classification of citation function. In Proceed-
ings of EMNLP-06.
Simone Teufel. 2001. Task-based evaluation of summary
quality: Describing relationships between scientific pa-
pers. In Proceedings of NAACL-01 Workshop ?Automatic
Text Summarization?, Pittsburgh, PA.
Melvin Weinstock. 1971. Citation indexes. In Encyclopedia
of Library and Information Science, volume 5, pages 16?
40. Dekker, New York, NY.
1502
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 45?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic enrichment of journal articles using chemical named entity
recognition
Colin R. Batchelor
Royal Society of Chemistry
Thomas Graham House
Milton Road
Cambridge
UK CB4 0WF
batchelorc@rsc.org
Peter T. Corbett
Unilever Centre for Molecular Science Informatics
University Chemical Laboratory
Lensfield Road
Cambridge
UK CB2 1EW
ptc24@cam.ac.uk
Abstract
We describe the semantic enrichment of journal
articles with chemical structures and biomedi-
cal ontology terms using Oscar, a program for
chemical named entity recognition (NER). We
describe how Oscar works and how it can been
adapted for general NER. We discuss its imple-
mentation in a real publishing workflow and pos-
sible applications for enriched articles.
1 Introduction
The volume of chemical literature published has ex-
ploded over the past few years. The crossover between
chemistry and molecular biology, disciplines which of-
ten study similar systems with contrasting techniques and
describe their results in different languages, has also in-
creased. Readers need to be able to navigate the literature
more effectively, and also to understand unfamiliar termi-
nology and its context. One relatively unexplored method
for this is semantic enrichment. Substructure and simi-
larity searching for chemical compounds is a particularly
exciting prospect.
Enrichment of the bibliographic data in an article with
hyperlinked citations is now commonplace. However,
the actual scientific content has remained largely unen-
hanced, this falling to secondary services and experimen-
tal websites such as GoPubMed (Delfs et al, 2005) or
EBIMed (Rebholz-Schuhmann et al, 2007). There are
a few examples of semantic enrichment on small (a few
dozen articles per year) journals such as Nature Chemi-
cal Biology being an example, but for a larger journal it
is impractical to do this entirely by hand.
This paper concentrates on implementing semantic
enrichment of journal articles as part of a publishing
workflow, specifically chemical structures and biomedi-
cal terms. In the Motivation section, we introduce Oscar
as a system for chemical NER and recognition of ontol-
ogy terms. In the Implementation section we will discuss
how Oscar works and how to set up ontologies for use
with Oscar, specifically GO. In the Case study section we
describe how the output of Oscar can be fed into a pub-
lishing workflow. Finally we discuss some outstanding
ambiguity problems in chemical NER. We also compare
the system to EBIMed (Rebholz-Schuhmann et al, 2007)
throughout.
2 Motivation
There are three routes for getting hold of chemical
structures from chemical text?from chemical compound
names, from author-supplied files containing connection
tables, and from images. The preferred representation
of chemical structures is as diagrams, often annotated
with curly arrows to illustrate the mechanisms of chem-
ical reactions. The structures in these diagrams are typ-
ically given numbers, which then appear in the text in
bold face. However, because text-processing is more ad-
vanced in this regard than image-processing, we shall
concentrate on NER, which is performed with a sys-
tem called Oscar. A preliminary overview of the sys-
tem was presented by Corbett and Murray-Rust (2006).
Oscar is open source and can be downloaded from
http://oscar3-chem.sourceforge.net/
As a first step in representing biomedical content, we
identify Gene Ontology (GO) terms in full text.1 (The
Gene Ontology Consortium, 2000) We have chosen a rel-
atively simple starting point in order to gain experience
in implementing useful semantic markup in a publishing
workflow without a substantial word-sense disambigua-
tion effort. GO terms are largely compositional (Mungall,
2004), hence incomplete matches will still be useful, and
that there is generally a low level of semantic ambiguity.
For example, there are only 133 single-word GO terms,
which significantly reduces the chance of polysemy for
the 20000 or so others. In contrast, gene and protein
1We also use other OBO ontologies, specifically those for
nucleic acid sequences (SO) and cell type (CL).
45
(.*) activity$ ? (\1)
(.*) formation$ ? ?
(.*) synthesis$ ? ?
ribonuclease ? RNAse
? ribonuclease
?alpha- (etc.) ? ?- (etc.)
? alpha- (etc.)
pluralize nouns
stopwords ? ?
Table 1: Example rules from ?Lucinda?, used for generat-
ing recogniser input from OBO files
names are generally short, non-compositional and often
polysemous with ordinary English words such as Cat or
Rat.
3 Implementation
Oscar is intended to be a component in larger workflows,
such as the Sciborg system (Copestake et al, 2006). It
is a shallow named-entity recogniser and does not per-
form deeper parsing. Hence there is no analysis of the
text above the level of the term, with the exception of
acronym matching, which is dealt with below, and some
treatment of the boldface chemical compound numbers
where they appear in section headings. It is optimized
for chemical NER, but can be extended to handle general
term recognition. The EBIMed system, in contrast, is a
pipeline, and lemmatizes words as part of a larger work-
flow.
To identify plurals and other variants of non-chemical
NEs we have a ruleset, nicknamed Lucinda, outlined in
Table 1, for generating the input for the recogniser from
external data. We use the plain-text OBO 1.2 format,
which is the definitive format for the dissemination of the
OBO ontologies.
We strive to keep this ruleset as small as possible, with
the exception of determining plurals and a few other reg-
ular variants. The reason for keeping plurals outside the
ontology is that plurals in ordinary text and in ontologies
can have quite different meanings.
There is also a short stopword list applied at this stage,
which is different from Oscar?s internal stopword han-
dling, described below.
3.1 Named entity recognition and resolution
Oscar has a recogniser to identify chemical names and
ontology terms, and a resolver which matches NEs to on-
tology IDs or chemical structures. The recogniser classi-
fies NEs according to the scheme in Corbett et al (2007).
The classes which are relevant here are CM, which iden-
tifies a chemical compound, either because it appears in
Oscar?s chemical dictionary, which also contains struc-
2 5 8 5 \s
4 5 8 0 \s
X
1
6
2
6 2 2 \s X 1 6
3X 1 6 4
Figure 1: Cartoon of part of the recogniser. The mapping
between this automaton and example GO terms is given
in Table 2.
GO term Regex pair
bud neck 2585\s4580\s
2585\s4580\sX162
bud neck polarisome 2585\s4580\s622\s
2585\s4580\s622\sX163
polarisome 622\s
622\sX164
Table 2: Mapping in Fig. 1. The regexes are purely il-
lustrative. IDs 162, 163 and 164 map on to GO:0005935,
GO:0031560 and GO:0000133 respectively.
tures and InChIs,2 or according to Oscar?s n-gram model,
regular expressions and other heuristics and ASE, a sin-
gle word ending in ?-ase? or ?-ases? and representing an
enzyme type. We add the class ONT to these, to cover
terms found in ontologies that do not belong in the other
classes, and STOP, which is the class of stopwords.
We sketch the recogniser in Fig. 1. To build the recog-
niser: Each term in the input data is tokenized and the
tokens converted into a sequence of digits followed by a
space. These new tokens are concatenated and converted
into a pair of regular expressions. One of these expres-
sions has X followed by a term ID appended to it. These
regex?regex pairs are converted into finite automata, the
union of which is determinized. The resulting DFA is ex-
amined for accept states. For each accept state for which
a transition to X is also present, the sequences of digits
after the X is used to build a mapping of accept states to
ontology IDs (Table 2).
To apply the recogniser: The input text is tokenized,
and for each token a set of representations is calculated
which map to sequences of digits as above. We then make
an empty set of DFA instances (a pointer to the DFA,
2An InChI is a canonical identifier for a chemical com-
pound. http://www.iupac.org/inchi/
46
which state it?s in and which tokens it has matched so
far), and for each token, add a new DFA instance for each
DFA, and for each representation of the token, clone the
DFA instance. If it does not accept the digit-sequence
representation of the token, throw it away. If it is in an
accept state, note which tokens it has matched, and if the
accept state maps to an ontology ID (ontID), we have an
NE which can be annotated with the ontID.
Take all of the potential NEs. For all NEs that have the
same sequence of tokens, share all of the ontIDs. Assign
its class according to a priority list where STOP comes
first and CM precedes ASE and ONT. For the system in
Fig. 1, the phrase ?bud neck polarisome? matches three
IDs. We choose the longest?leftmost sequence. If the
resolver generates an InChI for an NE, we look up this
InChI in ChEBI (de Matos et al, 2006), a biochemical
ontology, and take the ontology ID. This has the effect
of aligning ChEBI with other databases and systematic
nomenclature.
3.2 Gene Ontology
In working out how to mine the literature for GO terms,
we have taken our lead from the domain experts, the GO
editors and the curators of the Gene Ontology Annotation
(GOA) database.
The Functional Curation task in the first BioCreative
exercise (Blaschke et al, 2005) is the closest we have
found to a systematic evaluation of GO term identifica-
tion. The brief was to assign GO annotations to human
proteins and recover supporting text. The GOA curators
evaluated the results (Camon et al, 2005) and list some
common mistakes in the methods used to identify GO
terms. These include annotating to obsolete terms, pre-
dicting GO terms on too tenuous a link with the original
text, for example in one case the phrase ?pH value? was
annotated to ?pH domain binding? (GO:0042731), diffi-
culties with word order, and choosing too much support-
ing text, for example an entire first paragraph of text.
So at the suggestion of the GO editors, Oscar works on
exact matches to term names (as preprocessed above) and
their exact (within the OBO syntax) synonyms.
The most relevant GO terms to chemistry concern en-
zymes, which are proteins that catalyse chemical pro-
cesses. Typically their names are multiword expressions
ending in ?-ase?. The enzyme A B Xase will often be
represented by GO terms ?A B Xase activity?, a descrip-
tion of what the enzyme does, and ?A B Xase complex?,
a cellular component which consists of two or more pro-
tein subunits. In general the bare phrase ?A B Xase? will
refer to the activity, so the ruleset in Table 1 deletes the
word ?activity? from the GO term.
We shall briefly compare our method with the algo-
rithms in EBIMed and GoPubMed. The EBIMed algo-
rithm for GO term identification is very similar to ours,
except for the point about lemmatization listed above, and
its explicit variation of character case, which is handled
in Oscar by its case normalization algorithm. In contrast,
the algorithm in GoPubMed works by matching short
?seed? terms and then expanding them. This copes with
cases such as ?protein threonine/tyrosine kinase activity?
(GO:0030296) where the full term is unlikely to be found
in ordinary text; the words ?protein? and ?activity? are
generally omitted. However, the approach in (Delfs et
al., 2005) cannot be applied blindly; the authors claim for
example that ?biosynthesis? can be ignored without com-
promising the reader?s understanding. In chemistry jour-
nal articles most mentions of a chemical compound will
not refer to how it is formed in nature; they will refer to
the compound itself, its analogues or other processes. In
fact, our ruleset in Table 1 explicitly disallows GO term
synonyms ending in ? synthesis? or ? formation? since
they do not necessarily represent biological processes. It
is also not clear from Delfs et al (2005) how robust the
algorithm is to the sort of errors identified by Camon et
al. (2005).
4 Case study
The problem is to take a journal article, apply meaningful
and useful annotations, connect them to stable resources,
allow technical editors to check and add further annota-
tions, and disseminate the article in enriched form.
Most chemical publishers use XML as a stable format
for maintaining their documents for at least some stages
of the publication process. The Sciborg project (Copes-
take et al, 2006) and the Royal Society of Chemistry
(RSC) use SciXML (Rupp et al, 2006) and RSC XML
respectively. For the overall Sciborg workflow, standoff
annotation is used to store the different sets of annota-
tions. For the purposes of this paper, however, we make
use of the inline output of Oscar, which is SciXML with
<ne> elements for the annotations.
Not all of the RSC XML need be mined for NEs;
much of it is bibliographic markup which can confuse
parsers. Only the useful parts are converted into SciXML
and passed to Oscar, where they are annotated. These
SciXML annotations are then pasted back into the RSC
XML, where they can be checked by technical editors.
In running text, NEs are annotated with an ID local
to the XML file, which refers to <compound> and
<annotation> elements in a block at the end, which
contain chemical structure information and ontology IDs.
This is a lightweight compromise between pure standoff
and pure inline annotation.
We find useful annotations by aggressive threshold-
ing. The only classes which survive are ONTs, and those
CMs which have a chemical structure found by the re-
solver. This enables the chemical NER part of Oscar
to be tuned for high recall even as part of a publishing
47
workflow. Only CMs which correspond to an unambigu-
ous molecule or molecular ion are treated as a chemical
compound; everything else is referred to an appropriate
ontology. We use the InChI as a stable representation for
chemical structure, and the curated OBO ontologies for
biomedical terms.
The role of technical editors is to remove faulty anno-
tations, add new compounds to the chemical dictionary,
based on chemical structures supplied by authors, sug-
gest new GO terms to the ontology curators, and extend
the stopword lists of both Oscar and Lucinda as appropri-
ate. At present (May 2007), this happens after publication
of articles on the web, but is intended to become part of
the routine editing process in the course of 2007.
This enriched XML can then be converted into HTML
and RSS by means of XSL stylesheets and database
lookups, as in the RSC?s Project Prospect.3 The imme-
diate benefits of this work are increased readability of ar-
ticles for readers and extensive cross-linking with other
articles that have been enhanced in the same way. Fu-
ture developments could easily involve structure-based
searching, ontology-based search of journal articles, and
finding correlations between biological processes and
small molecule structures.
5 Ambiguity in chemical NER
One important omission is disambiguating the exact ref-
erent of a chemical name, which is not always clear with-
out context. For example ?the pyridine 6?, is a class de-
scription, but the phrase ?the pyridine molecule? refers to
a particular compound. ChEBI, which contains an ontol-
ogy of molecular structure, uses plurals to indicate chem-
ical classes, for example ?benzenes?, which is often, but
not always, what ?benzenes? means in text. Currently
Oscar does not distinguish between singular and plural.
Amino acids and saccharides are particularly trouble-
some on account of homochirality. Unless otherwise
specified, ?histidine? and ?ribose? specify the molecules
with the chirality found in nature, or to be precise,
L-histidine and D-ribose respectively. What is even
worse is that ?histidine? seldom refers to the independent
molecule; it usually means the histidine residue, part of a
larger entity.
6 Acknowledgements
We thank Dietrich Rebholz-Schuhmann for useful dis-
cussions. CRB thanks Jane Lomax, Jen Clark, Amelia
Ireland and Midori Harris for extensive cooperation and
help, and Richard Kidd, Neil Hunter and Jeff White at
the RSC. PTC thanks Ann Copestake and Peter Murray-
Rust for supervision. This work was funded by EPSRC
(EP/C010035/1).
3http://www.projectprospect.org/
References
Christian Blaschke, Eduardo Andres Leon, Martin
Krallinger and Alfonso Valencia. 2005. Evaluation
of BioCreAtIvE assessment of task 2 BMC Bioinfor-
matics 6(Suppl 1):S16
Evelyn B. Camon, Daniel G. Barrell, Emily C. Dimmer,
Vivian Lee, Michele Magrane, John Maslen, David
Binns and Rolf Apweiler. 2005. An evaluation of GO
annotation retrieval for BioCreAtIvE and GOA BMC
Bioinformatics 6(Suppl 1):S17
Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J.
Rupp, Advaith Siddharthan, Simone Teufel and Ben
Waldron. 2006. An Architecture for Language Tech-
nology for Processing Scientific Texts. In Proceedings
of the 4th UK E-Science All Hands Meeting. Notting-
ham, UK.
Peter Corbett, Colin Batchelor and Simone Teufel. 2007.
Annotation of Chemical Named Entities. In Proceed-
ings of BioNLP in ACL (BioNLP?07).
Peter T. Corbett and Peter Murray-Rust. 2006. High-
throughput identification of chemistry in life science
texts. LNCS, 4216:107?118.
P. de Matos, M. Ennis, M. Darsow, M. Guedj, K. Degt-
yarenko, and R. Apweiler. 2006. ChEBI - Chemical
Entities of Biological Interest Nucleic Acids Research,
Database Summary Paper 646.
The Gene Ontology Consortium. 2000. Gene Ontology:
Tool for the Unification of Biology Nature Genetics,
25:25?29.
Ralph Delfs, Andreas Doms, Alexander Kozlenkov and
Michael Schroeder. 2004. GoPubMed: Exploring
PubMed with the GeneOntology. Proceedings of Ger-
man Bioinformatics Conference, 169?178.
Christopher J. Mungall. 2004. Obol: integrating lan-
guage and meaning in bio-ontologies. Comparative
and Functional Genomics, 5:509?520.
Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel
Arregui, Sylvain Gaudan, Mark Riethoven and Peter
Stoehr. 2007. EBIMed?text crunching to gather
facts for proteins from Medline. Bioinformatics,
23(2):e237?e244.
C. J. Rupp, Ann Copestake, Simone Teufel and Benjamin
Waldron. 2006. Flexible Interfaces in the Application
of Language Technology to an eScience Corpus. In
Proceedings of the 4th UK E-Science All Hands Meet-
ing. Nottingham, UK.
48
BioNLP 2007: Biological, translational, and clinical language processing, pages 57?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotation of Chemical Named Entities
Peter Corbett
Cambridge University
Chemical Laboratory
Lensfield Road
Cambridge
UK CB2 1EW
ptc24@cam.ac.uk
Colin Batchelor
Royal Society of Chemistry
Thomas Graham House
Milton Road
Cambridge
UK CB4 0WF
batchelorc@rsc.org
Simone Teufel
Natural Language and
Information Processing Group
Computer Laboratory
University of Cambridge
UK CB3 0FD
sht25@cam.ac.uk
Abstract
We describe the annotation of chemical
named entities in scientific text. A set of an-
notation guidelines defines 5 types of named
entities, and provides instructions for the
resolution of special cases. A corpus of full-
text chemistry papers was annotated, with an
inter-annotator agreement
 
score of 93%.
An investigation of named entity recogni-
tion using LingPipe suggests that
 
scores
of 63% are possible without customisation,
and scores of 74% are possible with the ad-
dition of custom tokenisation and the use of
dictionaries.
1 Introduction
Recent efforts in applying natural language pro-
cessing to natural science texts have focused on
the recognition of genes and proteins in biomedi-
cal text. These large biomolecules are?mostly?
conveniently described as sequences of subunits,
strings written in alphabets of 4 or 20 letters. Ad-
vances in sequencing techniques have lead to a boom
in genomics and proteomics, with a concomitant
need for natural language processing techniques to
analyse the texts in which they are discussed.
However, proteins and nucleic acids provide only
a part of the biochemical picture. Smaller chemical
species, which are better described atom-by-atom,
play their roles too, both in terms of their inter-
actions with large biomolecules like proteins, and
in the more general biomedical context. A num-
ber of resources exist to provide chemical infor-
mation to the biological community. For example,
the National Center For Biotechnology Information
(NCBI) has added the chemical database PubChem1
to its collections of bioinformatics data, and the on-
tology ChEBI (Chemical Entities of Biological In-
terest) (de Matos et al, 2006) has been added to the
Open Biological Ontologies (OBO) family.
Small-molecule chemistry also plays a role in
biomedical natural language processing. PubMed
has included abstracts from medicinal chemistry
journals for a long time, and is increasingly carry-
ing other chemistry journals too. Both the GENIA
corpus (Kim et al, 2003) and the BioIE cytochrome
P450 corpus (Kulick et al, 2004) come with named
entity annotations that include a proportion of chem-
icals, and at least a few abstracts that are recognis-
able as chemistry abstracts.
Chemical named entity recognition enables a
number of applications. Linking chemical names to
chemical structures, by a mixture of database lookup
and the parsing of systematic nomenclature, allows
the creation of semantically enhanced articles, with
benefits for readers. An example of this is shown in
the Project Prospect2 annotations by the Royal So-
ciety of Chemistry (RSC). Linking chemical NER
to chemical information retrieval techniques allows
corpora to be searched for chemicals with similar
structures to a query molecule, or chemicals that
contain a particular structural motif (Corbett and
Murray-Rust, 2006). With information extraction
techniques, chemicals could be linked to their prop-
erties, applications and reactions, and with tradi-
tional gene/protein NLP techniques, it could be pos-
1http://pubchem.ncbi.nlm.nih.gov/
2http://www.projectprospect.org/
57
sible to discover new links between chemical data
and bioinformatics data.
A few chemical named entity recognition (Cor-
bett and Murray-Rust, 2006; Townsend et al, 2005;
Vasserman, 2004; Kemp and Lynch, 1998; Sun et
al., 2007) or classification (Wilbur et al, 1999) sys-
tems have been published. A plugin for the GATE
system3 will also recognise a limited range of chem-
ical entities. Other named entity recognition or
classification systems (Narayanaswamy et al, 2003;
Torii et al, 2004; Torii and Vijay-Shanker, 2002;
Spasic and Ananiadou, 2004) sometimes include
chemicals as well as genes, proteins and other bio-
logical entities. However, due to differences in cor-
pora and the scope of the task, it is difficult to com-
pare them. There has been no chemical equivalent
of the JNLPBA (Kim et al, 2004) or BioCreAtIvE
(Yeh et al, 2005) evaluations. Therefore, a corpus
and a task definition are required.
To find an upper bound on the levels of perfor-
mance that are available for the task, it is necessary
to study the inter-annotator agreement for the man-
ual annotation of the texts. In particular, it is useful
to see to what extent the guidelines can be applied by
those not involved in their development. Producing
guidelines that enable a highly consistent annotation
may raise the quality of the results of any machine-
learning techniques that use training data applied to
the guidelines, and producing guidelines that cover
a broad range of subdomains is also important (Din-
gare et al, 2005).
2 Annotation Guidelines
We have prepared a set of guidelines for the an-
notation of the names of chemical compounds and
related entities in scientific papers. These guide-
lines grew out of work on PubMed abstracts, and
have since been developed with reference to organic
chemistry journals, and later a range of journals en-
compassing the whole of chemistry.
Our annotation guidelines focus on the chemicals
themselves; we believe that these represent the ma-
jor source of rare words in chemistry papers, and
are of the greatest interest to end-users. Further-
more, many chemical names are formed systemat-
ically or semi-systematically, and can be interpreted
3http://www.gate.ac.uk/
without resorting to dictionaries and databases. As
well as chemical names themselves, we also con-
sider other words or phrases that are formed from
chemical names.
The various types are summarised in Table 1.
Type Description Example
CM chemical compound citric acid
RN chemical reaction 1,3-dimethylation
CJ chemical adjective pyrazolic
ASE enzyme methylase
CPR chemical prefix 1,3-
Table 1: Named entity types
The logic behind the classes is best explained with
an example drawn from the corpus described in the
next section:
In addition, we have found in previous
studies that the Zn  

?Tris system is also
capable of efficiently hydrolyzing other

-
lactams, such as clavulanic acid, which
is a typical mechanism-based inhibitor of
active-site serine

-lactamases (clavulanic
acid is also a fairly good substrate of the
zinc-

-lactamase from B. fragilis).
Here, ?clavulanic acid? is a specific chemical com-
pound (a CM), referred to by a trivial (unsystem-
atic) name, and ?  -lactams? is a class of chemi-
cal compounds (also a CM), defined by a particu-
lar structural motif. ?Zn  

?Tris? is another CM (a
complex rather than a molecule), and despite be-
ing named in an ad hoc manner, the name is com-
positional and it is reasonably clear to a trained
chemist what it is. ?Serine? (another CM) can be
used to refer to an amino acid as a whole compound,
but in this case refers to it as a part of a larger
biomolecule. The word ?hydrolyzing? (an RN) de-
notes a reaction involving the chemical ?water?. ?

-
lactamases? (an ASE) denotes a class of enzymes
that process

-lactams, and ?zinc-

-lactamase? (an-
other ASE) denotes a  -lactamase that uses zinc.
By our guidelines, the terms ?mechanism-based in-
hibitor? or ?substrate? are not annotated, as they de-
note a chemical role, rather than giving information
about the structure or composition of the chemicals.
58
The full guidelines occupy 31 pages (including a
quick reference section), and contain 93 rules. Al-
most all of these have examples, and many have sev-
eral examples.
A few distinctions need to be explained here. The
classes RN, CJ and ASE do not include all reactions,
adjectives or enzymes, but only those that entail
specific chemicals or classes of chemicals?usually
by being formed by the modification of a chemical
name?for example, ?

-lactamases? in the example
above is formed from the name of a class of chem-
icals. Words derived from Greek and Latin words
for ?water?, such as ?aqueous? and ?hydrolysis?, are
included when making these annotations.
The class CPR consists of prefixes, more often
found in systematic chemical names, giving details
of the geometry of molecules, that are attached to
normal English words. For example, the chemi-
cal 1,2-diiodopentane is a 1,2-disubstituted pentane,
and the ?1,2-? forms the CPR in ?1,2-disubstituted?.
Although these contructions sometimes occur as in-
fixes within chemical names, we have only seen
these used as prefixes outside of them. We believe
that identifying these prefixes will be useful in the
adaptation of lexicalised parsers to chemical text.
The annotation task includes a small amount of
word sense disambiguation. Although most chemi-
cal names do not have non-chemical homonyms, a
few do. Chemical elements, and element symbols,
give particular problems. Examples of this include
?lead?, ?In? (indium), ?As? (arsenic), ?Be? (beryl-
lium), ?No? (nobelium) and ?K? (potassium?this is
confusable with Kelvin). These are only annotated
when they occur in their chemical sense.
2.1 Related Work
We know of two publicly available corpora that also
include chemicals in their named-entity markup. In
both of these, there are significant differences to
many aspects of the annotation. In general, our
guidelines tend to give more importance to concepts
regarding chemical structure, and less importance to
biological role, than the other corpora do.
The GENIA corpus (Kim et al, 2003) in-
cludes several different classes for chemi-
cals. Our class CM roughly corresponds to
the union of GENIA?s atom, inorganic,
other organic compound, nucleotide
and amino acid monomer classes, and also
parts of lipid and carbohydrate (we ex-
clude macromolecules such as lipoproteins and
lipopolysaccharides). Occasionally terms that
match our class RN are included as other name.
Our CM class also includes chemical names
that occur within enzyme or other protein names
(e.g. ?inosine-5   -monophosphate? in ?inosine-5   -
monophosphate dehydrogenase?) whereas the
GENIA corpus (which allows nesting) typically
does not. The GENIA corpus also sometimes
includes qualifiers in terms, giving ?intracellular
calcium? where we would only annotate ?calcium?,
and also includes some role/application terms such
as ?antioxidant? and ?reactive intermediate?.
The BioIE P450 corpus (Kulick et al, 2004), by
contrast, includes chemicals, proteins and other sub-
stances such as foodstuffs in a single category called
?substance?. Again, role terms such as ?inhibitor? are
included, and may be merged with chemical names
to make entities such as ?fentanyl metabolites? (we
would only mark up ?fentanyl?). Fragments of
chemicals such as ?methyl group? are not marked up;
in our annotations, the ?methyl? is marked up.
The BioIE corpus was produced with extensive
guidelines; in the GENIA corpus, much more was
left to the judgement of the annotators. These lead
to inconsistencies, such as whether to annotate ?an-
tioxidant? (our guidelines treat this as a biological
role, and do not mark it up). We are unaware of an
inter-annotator agreement study for either corpus.
Both of these corpora include other classes of
named entities, and additional information such as
sentence boundaries.
3 Inter-annotator Agreement
3.1 Related Work
We are unaware of any studies of inter-annotator
agreement with regards to chemicals. However, a
few studies of gene/protein inter-annotator agree-
ment do exist. Demetriou and Gaizauskas (2003)
report an
 
score of 89% between two domain ex-
perts for a task involving various aspects of protein
science. Morgan et al (2004) report an   score of
87% between a domain expert and a systems devel-
oper for D. melanogaster gene names. Vlachos and
Gasperin (2006) produced a revised version of the
59
guidelines for the task, and were able to achieve an
 
score of 91%, and a kappa of 0.905, between a
computational linguist and a domain expert.
3.2 Subjects
Three subjects took part in the study. Subject A
was a chemist and the main author of the guidelines.
Subject B was another chemist, highly involved in
the development of the guidelines. Subject C was a
PhD student with a chemistry degree. His involve-
ment in the development of guidelines was limited to
proof-reading an early version of the guidelines. C
was trained by A, by being given half an hour?s train-
ing, a test paper to annotate (which satisfied A that C
understood the general principles of the guidelines),
and a short debriefing session before being given the
papers to annotate.
3.3 Materials
The study was performed on 14 papers (full pa-
pers and communications only, not review articles
or other secondary publications) published by the
Royal Society of Chemistry. These were taken from
the journal issues from January 2004 (excluding a
themed issue of one of the journals). One paper was
randomly selected to represent each of the 14 jour-
nals that carried suitable papers. These 14 papers
represent a diverse sample of topics, covering areas
of organic, inorganic, physical, analytical and com-
putational chemistry, and also areas where chemistry
overlaps with biology, environmental science, mate-
rials and mineral science, and education.
From these papers, we collected the title, section
headings, abstract and paragraphs, and discarded the
rest. To maximise the value of annotator effort, we
also automatically discarded the experimental sec-
tions, by looking for headers such as ?Experimen-
tal?. This policy can be justified thus: In chemistry
papers, a section titled ?Results and Discussion? car-
ries enough information about the experiments per-
formed to follow the argument of the paper, whereas
the experimental section carries precise details of the
protocols that are usually only of interest to people
intending to replicate or adapt the experiments per-
formed. It is increasingly common for chemistry pa-
pers not to contain an experimental section in the
paper proper, but to include one in the supporting
online information. Furthermore, experimental sec-
tions are often quite long and tedious to annotate,
and previous studies have shown that named-entity
recognition is easier on experimental sections too
(Townsend et al, 2005).
A few experimental sections (or parts thereof)
were not automatically detected, and instead were
removed by hand.
3.4 Procedure
The papers were hand-annotated using our in-house
annotation software. This software displays the text
so as to preserve aspects of the style of the text such
as subscripts and superscripts, and allows the anno-
tators to freely select spans of text with character-
level precision?the text was not tokenised prior to
annotation. Spans were not allowed to overlap or to
nest. Each selected span was assigned to exactly one
of the five available classes.
During annotation the subjects were allowed to
refer to the guidelines (explained in the previous sec-
tion), to reference sources such as PubChem and
Wikipedia, and to use their domain knowledge as
chemists. They were not allowed to confer with
anyone over the annotation, nor to refer to texts an-
notated during development of the guidelines. The
training of subject C by A was completed prior to A
annotating the papers involved in the exercise.
3.5 Evaluation Methodology
Inter-annotator agreement was measured pairwise,
using the
 
score. To calculate this, all of the ex-
act matches were found and counted, and all of the
entities annotated by one annotator but not the other
(and vice versa) were counted. For an exact match,
the left boundary, right boundary and type of the an-
notation had to match entirely. Thus, if one anno-
tator had annotated ?hexane?ethyl acetate? as a sin-
gle entity, and the other had annotated it as ?hexane?
and ?ethyl acetate?, then that would count as three
cases of disagreement and no cases of agreement.
We use the
 
score as it is a standard measure in the
domain?however, as a measure it has weaknesses
which will be discussed in the next subsection.
Given the character-level nature of the annotation
task, and that the papers were not tokenised, the task
cannot sensibly be cast as a classification problem,
and so we have not calculated any kappa scores.
60
Overall results were calculated using two meth-
ods. The first method was to calculate the total lev-
els of agreement and disagreement across the whole
corpus, and to calculate a total   score based on that.
The second method was to calculate   scores for in-
dividual papers (removing a single paper that con-
tained two named entities?neither of which were
spotted by subject B?as an outlier), and to calculate
an unweighted mean, standard deviation and 95%
confidence intervals based on those scores.
3.6 Results and Discussion
Subjects   (corpus)   (average) std. dev.
A?B 92.8% 92.9%   3.4% 6.2%
A?C 90.0% 91.4%   3.1% 5.7%
B?C 86.1% 87.6%   3.1% 5.7%
Table 2: Inter-annotator agreement results.   values
are 95% confidence intervals.
The results of the analysis are shown in Table 2.
The whole-corpus
 
scores suggest that high levels
of agreement (93%) are possible. This is equivalent
to or better than quoted values for biomedical inter-
annotator agreement. However, the poorer agree-
ments involving C would suggest that some of this is
due to some extra information being communicated
during the development of the guidelines.
A closer analysis shows that this is not the case. A
single paper, containing a large number of entities, is
notable as a major source of disagreement between
A and C, and B and C, but not A and B. Looking
at the annotations themselves, the paper contained
many repetitions of the difficult entity ?Zn  

?Tris?,
and also of similar entities. If the offending paper is
removed from consideration, the agreement between
A and C exceeds the agreement between A and B.
This analysis is confirmed using the per-paper  
scores. Two-tailed, pairwise t-tests (excluding the
outlier paper) showed that the difference in mean  
scores between the A?B and A?C agreements was
not statistically significant at the 0.05 significance
level; however, the differences between B?C and A?
B, and between B?C and A?C were.
A breakdown of the inter-annotator agreements
by type is shown in Table 3. CM and RN, at least,
seem to be reliably annotated. The other classes are
less easy to assess, due to their rarity, both in terms
Type
 
Number
CM 93% 2751
RN 94% 79
CJ 56% 20
ASE 96% 25
CPR 77% 10
Table 3: Inter-annotator agreement, by type.  
scores are corpus totals, between Subjects A and C.
The number is the number of entities of that class
found by Subject A.
of their total occurrence in the corpus and the num-
ber of papers that contain them.
We speculate that the poorer B?C agreement may
be due to differing error rates in the annotation. In
many cases, it was clear from the corpus that errors
were made due to failing to spot relevant entities, or
by failing to look up difficult cases in the guidelines.
Although it is not possible to make a formal analy-
sis of this, we suspect that A made fewer errors, due
to a greater familiarity with the task and the guide-
lines. This is supported by the results, as more er-
rors would be involved in the B?C comparison than
in comparisons involving A, leading to higher levels
of disagreement.
We have also examined the types of disagree-
ments made. There were very few cases where two
annotators had annotated an entity with the same
start and end point, but a different type; there were
2 cases of this between A and C, and 3 cases in each
of the other two comparisons. All of these were con-
fusions between CM and CJ.
In the A?B comparison, there were 415 entities
that were annotated by either A or B that did not
have a corresponding exact match. 183 (44%) of
those were simple cases where the two annotators
did not agree as to whether the entity should be
marked up or not (i.e. the other annotator had not
placed any entity wholly or partially within that
span). For example, some annotators failed to spot
instances of ?water?, or disagreed over whether ?fat?
(as a synonym for ?lipid?) was to be marked up.
The remainder of those disagreements are due
to disagreements of class, of where the boundaries
should be, of how many entities there should be in
a given span, and combinations of the above. In all
61
of these cases, the fact that the annotators produce at
least one entity each for a given case means that dis-
agreements of this type are penalised harshly, and
therefore are given disproportionate weight. How-
ever, it is also likely that disagreements over whether
to mark an entity up are more likely to represent a
simple mistake than a disagreement over how to in-
terpret the guidelines; it is easy to miss an entity that
should be marked up when scanning the text.
A particularly interesting class of disagreement
concerns whether a span of text should be anno-
tated as one entity or two. For example, ?Zn  

?Tris?
could be marked up as a single entity, or as ?Zn  

?
and ?Tris?. We looked for cases where one annota-
tor had a single entity, the left edge of which cor-
responded to the left edge of an entity annotated by
the other annotator, and the right edge corresponded
to the right edge of a different entity. We found 43
cases of this. As in each of these cases, at least three
entities are involved, this pattern accounts for at least
30% of the inter-annotator disagreement. Only 17 of
these cases contained whitespace?in the rest of the
cases, hyphens, dashes or slashes were involved.
4 Analysis of the Corpus
To generate a larger corpus, a further two batches of
papers were selected and preprocessed in the manner
described for the inter-annotator agreement study
and annotated by Subject A. These were combined
with the annotations made by Subject A during the
agreement study, to produce a corpus of 42 papers.
Type Entities Papers
CM 6865 94.1% 42 100%
RN 288 4.0% 23 55%
CJ 60 0.8% 20 48%
ASE 31 0.4% 5 12%
CPR 53 0.7% 9 21%
Table 4: Occurrence of entities in the corpus, and
numbers of papers containing at least one entity of a
type.
From Table 4 it is clear that CM is by far the most
common type of named entity in the corpus. Obser-
vation of the corpus shows that RN is common in
certain genres of paper (for example organic synthe-
sis papers), and generally absent from other genres.
ASE, too, is a specialised category, and did not occur
much in this corpus.
A closer examination of CM showed more than
90% of these to contain no whitespace. However,
this is not to say that there are not significant num-
bers of multi-token entities. The difficulty of to-
kenising the corpus is illustrated by the fact that
1114 CM entities contained hyphens or dashes, and
388 CM entities were adjacent to hyphens or dashes
in the corpus. This means that any named entity
recogniser will have to have a specialised tokeniser,
or be good at handling multi-token entities.
Tokenising the CM entities on whitespace and
normalising their case revealed 1579 distinct
words?of these, 1364 only occurred in one paper.
There were 4301 occurrences of these words (out of
a total of 7626). Whereas the difficulties found in
gene/protein NER with complex multiword entities
and polysemous words are less likely to be a prob-
lem here, the problems with tokenisation and large
numbers of unknown words remain just as pressing.
As with biomedical text (Yeh et al, 2005), cases
of conjunctive and disjunctive nomenclature, such
as ?benzoic and thiophenic acids? and ?bromo- or
chlorobenzene? exist in the corpus. However, these
only accounted for 27 CM entities.
5 Named-Entity Recognition
To establish some baseline measures of perfor-
mance, we applied the named-entity modules from
the toolkit LingPipe,4 which has been success-
fully applied to NER of D. melanogaster genes
(e.g. by Vlachos and Gasperin (2006)). Ling-
Pipe uses a first-order HMM, using an enriched
tagset that marks not only the positions of the
named entities, but the tokens in front of and
behind them. Two different strategies are em-
ployed for handling unknown tokens. The
first (the TokenShapeChunker) replaces un-
known or rare tokens with a morphologically-
based classification. The second, newer module
(the CharLmHmmChunker) estimates the prob-
ability of an observed word given a tag us-
ing language models based on character-level   -
grams. The LingPipe developers suggest that the
TokenShapeChunker typically outperforms the
4http://www.alias-i.com/lingpipe/
62
CharLmHmmChunker. However, the more so-
phisticated handling of unknown words by the
CharLmHmmChunker suggests that it might be a
good fit to the domain.
As well as examining the performance of Ling-
Pipe out of the box, we were also able to make some
customisations. We have a custom tokeniser, con-
taining several adaptations to chemical text. For ex-
ample, our tokeniser will only remove brackets from
the front and back of tokens, and only if that would
not cause the brackets within the token to become
unbalanced. For example, no brackets would be re-
moved from ?(R)-acetoin?. Likewise, it will only
tokenise on a hyphen if the hyphen is surrounded
by two lower-case letters on either side (and if the
letters to the left are not common prehyphen com-
ponents of chemical names), or if the string to the
right has been seen in the training data to be hy-
phenated with a chemical name (e.g. ?derived? in
?benzene-derived?). By contrast, the default Ling-
Pipe tokeniser is much more aggressive, and will to-
kenise on hyphens and brackets wherever they occur.
The CharLmHmmChunker?s language models
can also be fed dictionaries as additional training
data?we have experimented with using a list of
chemical names derived from ChEBI (de Matos et
al., 2006), and a list of chemical elements. We have
also made an extension to LingPipe?s token classi-
fier, which adds classification based on chemically-
relevant suffixes (e.g. -yl, -ate, -ic, -ase, -lysis), and
membership in the aforementioned chemical lists, or
in a standard English dictionary.
We analysed the performance of the different
LingPipe configurations by 3-fold cross-validation,
using the 42-paper corpus described in the previous
section. In each fold, 28 whole papers were used as
training data, holding out the other 14 as test data.
The results are shown in Table 5.
From Table 5, we can see that the character   -
gram language models offer clear advantages over
the older techniques, especially when coupled to a
custom tokeniser (which gives a boost to   of over
7%), and trained with additional chemical names.
The usefulness of character-based   -grams has also
been demonstrated elsewhere (Wilbur et al, 1999;
Vasserman, 2004; Townsend et al, 2005). Their use
here in an HMM is particularly apt, as it allows the
token-internal features in the language model to be
Configuration
    
TokenShape 67.0% 52.9% 59.1%
+  71.2% 62.3% 66.5%
+  67.4% 52.5% 59.0%
+  +  73.3% 62.5% 67.4%
CharLm 62.7% 63.4% 63.1%
+  59.8% 68.8% 64.0%
+  71.1% 70.0% 70.5%
+  +  75.3% 73.5% 74.4%
Table 5: LingPipe performance using different con-
figurations.  = custom token classifier,  = chemical
name lists,  = custom tokeniser
combined with the token context.
The impact of custom tokenisation upon
the older TokenShapeChunker is less dra-
matic. It is possible that tokens that contain
hyphens, brackets and other special characters are
more likely to be unknown or rare tokens?the
TokenShapeChunker has previously been
reported to make most of its mistakes on these
(Vlachos and Gasperin, 2006), so tokenising them
is likely to make less of an impact. It is also
possible that chemical names are more distinctive
as a string of subtokens rather than as one large
token?this may offset the loss in accuracy from
getting the start and end positions wrong. The
CharLmHmmChunker already has a mecha-
nism for spotting distinctive substrings such as
?N,N?-? and ?-3-?, and so the case for having long,
well-formed tokens becomes much less equivocal.
It is also notable that improvements in tokenisa-
tion are synergistic with other improvements?the
advantage of using the CharLmHmmChunker is
much more apparent when the custom tokeniser is
used, as is the advantage of using word lists as addi-
tional training data. It is notable that for the unmod-
ified TokenShapeChunker, using the custom to-
keniser actually harms performance.
6 Conclusion
We have produced annotation guidelines that enable
the annotation of chemicals and related entities in
scientific texts in a highly consistent manner. We
have annotated a corpus using these guidelines, an
analysis of which, and the results of using an off-
63
the-shelf NER toolkit, show that finding good ap-
proaches to tokenisation and the handling of un-
known words is critical in the recognition of these
entities. The corpus and guidelines are available by
contacting the first author.
7 Acknowledgements
We thank Ann Copestake and Peter Murray-Rust
for supervision, Andreas Vlachos and Advaith Sid-
dharthan for valuable discussions, and David Jessop
for annotation. We thank the RSC for providing the
papers, and the UK eScience Programme and EP-
SRC (EP/C010035/1) for funding.
References
Peter T. Corbett and Peter Murray-Rust. 2006. High-
Throughput Identification of Chemistry in Life Sci-
ence Texts. CompLife, LNBI 4216:107?118.
P. de Matos, M. Ennis, M. Darsow, M. Guedj, K. Degt-
yarenko and R. Apweiler. 2006. ChEBI ? Chemi-
cal Entities of Biological Interest. Nucleic Acids Res,
Database Summary Paper 646.
George Demetriou and Rob Gaizauskas. 2003. Cor-
pus resources for development and evaluation of a bi-
ological text mining system. Proceedings of the Third
Meeting of the Special Interest Group on Text Mining,
Brisbane, Australia, July.
Shipra Dingare, Malvina Nissim, Jenny Finkel, Christo-
pher Manning and Claire Grover. 2005. A system for
identifying named entities in biomedical text: how re-
sults from two evaluations reflect on both the system
and the evaluations. Comparative and Functional Ge-
nomics, 6(1-2),77-85.
Nick Kemp and Michael Lynch. 1998. Extraction of In-
formation from the Text of Chemical Patents. 1. Iden-
tification of Specific Chemical Names. J. Chem. Inf.
Comput. Sci., 38:544-551.
J.-D. Kim, T. Ohta, Y. Tateisi and J. Tsujii. 2003. GE-
NIA corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl 1):i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi and Nigel Collier. 2004. Introduction
to the Bio-Entity Recognition Task at JNLPBA. Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications, 70-75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein and
Lyle Ungar. 2004. Integrated Annotation for Biomed-
ical Information Extraction HLT/NAACL BioLINK
workshop, 61-68.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal of Biomedical
Informatics, 37(6):396-410.
Meenakshi Narayanaswamy, K. E. Ravikumar and K.
Vijay-Shanker. 2003. A Biological Named Entity
Recogniser. Pac. Symp. Biocomput., 427-438.
Irena Spasic and Sophia Ananiadou. 2004. Using
Automatically Learnt Verb Selectional Preferences
for Classification of Biomedical Terms. Journal of
Biomedical Informatics, 37(6):483-497.
Bingjun Sun, Qingzhao Tan, Prasenjit Mitra and C. Lee
Giles. 2007. Extraction and Search of Chemical For-
mulae in Text Documents on the Web. The 16th In-
ternational World Wide Web Conference (WWW?07),
251-259.
Manabu Torii and K. Vijay-Shanker. 2002. Using Unla-
beled MEDLINE Abstracts for Biological Named En-
tity Classification. Genome Informatics, 13:567-568.
Manabu Torii, Sachin Kamboj and K. Vijay-Shanker.
2004. Using name-internal and contextual features to
classify biological terms. Journal of Biomedical Infor-
matics, 37:498-511.
Joe A. Townsend, Ann A. Copestake, Peter Murray-Rust,
Simone H. Teufel and Christopher A. Waudby. 2005.
Language Technology for Processing Chemistry Pub-
lications. Proceedings of the fourth UK e-Science All
Hands Meeting, 247-253.
Alexander Vasserman. 2004. Identifying Chemical
Names in Biomedical Text: An Investigation of the
Substring Co-occurence Based Approaches. Pro-
ceedings of the Student Research Workshop at HLT-
NAACL. 7-12.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and Evaluating Named Entity Recognition
in the Biomedical Domain. Proceedings of BioNLP in
HLT-NAACL. 138-145.
W. John Wilbur, George F. Hazard, Jr., Guy Divita,
James G. Mork, Alan R. Aronson and Allen C.
Browne. 1999. Analysis of Biomedical Text for
Chemical Names: A Comparison of Three Methods.
Proc. AMIA Symp. 176-180.
Alexander Yeh, Alexander Morgan, Marc Colosimo and
Lynette Hirschman. 2005. BioCreAtIvE Task IA:
gene mention finding evaluation. BMC Bioinformat-
ics 6(Suppl I):S2.
64
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747?757,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A discourse-driven content model for summarising scientific articles
evaluated in a complex question answering task
Maria Liakata
University of Warwick/
EMBL-EBI, UK
M.Liakata@warwick.ac.uk
Simon Dobnik
University of Gothenburg, Sweden
simon.dobnik@gu.se
Shyamasree Saha
EMBL-EBI, UK
saha@ebi.ac.uk
Colin Batchelor
Royal Society of Chemistry, UK
batchelorc@rsc.org
Dietrich Rebholz-Schuhmann
University of Zurich, Switzerland/
EMBL-EBI, UK
rebholz@ebi.ac.uk
Abstract
We present a method which exploits auto-
matically generated scientific discourse an-
notations to create a content model for the
summarisation of scientific articles. Full pa-
pers are first automatically annotated using the
CoreSC scheme, which captures 11 content-
based concepts such as Hypothesis, Result,
Conclusion etc at the sentence level. A content
model which follows the sequence of CoreSC
categories observed in abstracts is used to pro-
vide the skeleton of the summary, making a
distinction between dependent and indepen-
dent categories. Summary creation is also
guided by the distribution of CoreSC cate-
gories found in the full articles, in order to
adequately represent the article content. Fi-
nally, we demonstrate the usefulness of the
summaries by evaluating them in a complex
question answering task. Results are very en-
couraging as summaries of papers from auto-
matically obtained CoreSCs enable experts to
answer 66% of complex content-related ques-
tions designed on the basis of paper abstracts.
The questions were answered with a precision
of 75%, where the upper bound for human
summaries (abstracts) was 95%.
1 Introduction
The publication boom of the last few years, espe-
cially in the life sciences, has highlighted the need
to facilitate automatic access to the information con-
tent of articles. Researchers, curators, reviewers all
need to process a continuously expanding flow of
articles whether the purpose is to follow the state of
the art, curate large knowledge bases or have a good
working knowledge of their own and related disci-
plines to assess progress in research. While a lot
of effort has concentrated on information extraction
of particular types of entities and relations from the
scientific literature (Cohen and Hersh, 2005; Kim
et al, 2009; Ananiadou et al, 2010; Kim et al,
2011), with a view to support scientists in obtain-
ing relevant information from scientific articles and
abstracts, less work has focussed on automatically
combining such information in the form of a co-
hesive summary which preserves the context. Re-
searchers rely to a great extent on author-written ab-
stracts, but the latter suffer from a number of prob-
lems; they are less structured, vary significantly in
terms of length, are often not self-contained and
have been written independently of the main doc-
ument (Teufel, 2010, p.83).
Teufel (2001; 2010), (Teufel and Moens, 2002)
identify argumentative zones within scientific arti-
cles and use them to create use-targeted extractive
summaries. Argumentative zones are annotations
which designate the type of knowledge claim and
rhetorical status for a sentence and how these relate
to the communicative function of the entire paper.
A selection of various combinations of argumenta-
tive zones are chosen for the use-targeted extractive
summaries (rhetorical extracts), each of which ful-
fills a different role. For instance, purpose-oriented
extracts less than 10 sentences long are generated
containing a predetermined number of AIM, SOLU-
TION and BACKGROUND zones. As the emphasis
of this approach was the identification of the argu-
mentative zones, less attention was given to the sen-
tence selection criteria for the extractive summaries.
747
The sentences chosen for the rhetorical extracts were
either all sentences of a particular category (in the
case of rare categories) (Teufel and Moens, 2002),
selected according to a classifier trained on a rele-
vance gold standard (Teufel and Moens, 2002), man-
ually or randomly selected (Teufel, 2010, p.60).
More recently Contractor et al (2012) have used
automatically annotated argumentative zones (Guo
et al, 2011) to guide the creation of extractive sum-
maries of scientific articles. Here argumentative
zones are used as features for the summariser, along
with verbs, tf-idf values and sentence location. They
use a standard approach to summarisation, with a bi-
nary classification recognising candidate sentences
which are then fed into a clustering mechanism. Ex-
tracts can be created to summarise the entire paper or
focus on specific user-specified aspects. The num-
ber of sentences to include in the summary is pre-
specified (either directly or using a compression ra-
tio).
Our approach also makes use of the scientific dis-
course for summarisation purposes. We use the sci-
entific discourse to create a content model for ex-
tractive summarisation, with a focus on represent-
ing the content of the full paper, while keeping the
cohesion of the narrative. We first automatically
annotate the articles with a scheme which captures
fine-grained aspects of the content and conceptual
structure of the papers, namely the Core Scientific
Concepts (CoreSC) scheme (Liakata et al, 2010; Li-
akata et al, 2012). The CoreSC scheme is ?uniquely
suited to recovering common types of scientific ar-
guments about hypotheses, explanations, and evi-
dence? (White et al, 2011), which are not read-
ily identifiable by other annotation schemes. Also,
when compared to argumentative zoning and more
specifically its extension for chemistry papers, AZ-
II (Teufel et al, 2009), it was shown to provide a
greater level of detail in terms of categories denot-
ing objectives, methods and outcomes whereas AZ-
II focusses on the attribution of knowledge claims
and the relation with previous work (Liakata et al,
2010).
We then use the distribution of CoreSC categories
observed in abstracts to create a content model
which provides a skeleton for extractive summaries.
The reasoning behind this is to try to preserve co-
hesion within the summaries and we hypothesise
that the sequence of CoreSC categories is a good
proxy for cohesion (see section 3.1). In creating
the summary, instantiating the content model, we
identify independent categories and dependent cate-
gories, and we argue that in order to preserve the co-
hesion of the text the independent categories should
be determined first (see section 3.2). We also pre-
serve in the summary the distribution of CoreSC cat-
egories found in the corresponding full paper.
Finally, we evaluate the extractive summaries in
a complex real world question-answering task, in
which we assess the usefulness of the summaries as
well as to what extent the generated CoreSC sum-
maries represent the content of the original arti-
cle. Experts are presented with different types of
summaries and are asked to answer article-specific
questions on the basis of the summaries (see sec-
tion 4.1). Our results show that automatically gen-
erated CoreSC summaries can answer 66% of com-
plex questions with 75% precision, outperforming
a baseline of microsoft autosummarise summaries
(See section 4.2).
We have also peformed an intrinsic evaluation of
the summaries using ROUGE and automatic mea-
sures for summary informativeness, such as the
Jensen-Shannon divergence, yielding positive re-
sults (See section 4.2). However, as such measures
have not yet reached maturity and are harder to in-
terpret, we consider the user-based evaluation to be
a more reliable measure of summary quality.
Code for generating the summaries can be ob-
tained by contacting the first author and/or visiting
http://www.sapientaproject.com/software.
2 Related work
The Core Scientific Concepts (CoreSC) Scheme:
The CoreSC scheme consists of three layers; the first
layer corresponds to eleven concepts (Background
(BAC), Hypothesis (HYP), Motivation (MOT), Goal
(GOA), Object (OBJ), Method (MET), Model
(MOD), Experiment (EXP), Observation (OBS),
Result (RES) and Conclusion (CON)); the second
layer corresponds to properties of the concepts (e.g.
New/Old) and the third layer provides identifiers
which link instances of the same category. Liakata
et al (2010) created a corpus of 265 full scientific
articles from chemistry and biochemistry annotated
748
with this scheme and trained classifiers using SVMs
and CRFs in (Liakata et al, 2012), with an accu-
racy of >51% across the 11 concepts. Their data
and CoreSC classification system are available on-
line and can provide a good benchmark for com-
parison. Louis & Nenkova (2012) have successfully
used the CoreSC corpus for evaluating syntax-based
coherence models, which indicates the strong con-
nection between coherence and discourse structure.
Summarisation for scientific articles: A lot of
the work on summarising scientific articles has fo-
cussed on citation-based summaries. Qazvinian &
Radev (2008) use sentences from papers citing the
article to be summarised. Sentences are clustered to-
gether creating a topic, with the combination of clus-
ters forming a citation summary network. Qazvinian
& Radev (2010), (Qazvinian et al, 2010) also make
use of citation sentences in other scientific papers to
summarize the contributions of a paper. The draw-
back of citation summaries is that a paper must be
already cited, so this type of summary will not be
useful to a paper reviewer. Also, citations of articles
will have been influenced by other citations rather
than the paper itself.
Document models for summarisation: Our con-
tent model has some similarities with content mod-
elling using global sentence ordering (Barzilay and
Lee, 2004; Chen et al, 2009). In (Barzilay and
Lee, 2004) unsupervised methods are used to cre-
ate HMM topic sequence models for newswire text
articles. Topics are assigned to texts according to
the content model and extracts of fixed length are
created by selecting the topics most likely to occur
in summaries. While we use supervised methods to
annotate papers with a fixed set of topics (CoreSCs)
in scientific papers, our summary content model for
extracts shares similar principles such as global or-
dering of sentences and non-recurrence. However,
their evaluation involved newspaper articles and ex-
tracts which are a lot shorter (15 and 6 sentences,
respectively).
It is not clear whether unsupervised topic mod-
elling such as (Chen et al, 2009) can be applied to
scientific articles (over 100 sentences long), which
by nature include repetition of topics. It would be
interesting to make comparisons with summaries us-
ing content models learnt from our data automati-
cally, following a similar approach to (Sauper et al,
2010) which learns a content model jointly with a
particular supervised task in web-based documents.
3 Extractive Summarisation using
CoreSCs
In this section we describe how we use CoreSC dis-
course categories annotated at the sentence level to
create extractive summaries of full papers, which we
subsequently evaluate in a question answering task
in section 4.
To generate summaries we follow classic text ex-
traction techniques while making use of a document
content model based on CoreSCs. Our aim is for
the content model to reflect both the distribution of
CoreSCs in the paper as well as the discourse model
of human summaries, as the latter is indicated by the
generic ordering of CoreSC categories in abstracts
encountered in a corpus of 265 annotated full pa-
pers (Liakata and Soldatova, 2009; Liakata et al,
2012). While we do not consider abstracts to be ade-
quate summaries, we at least consider them to be co-
herent summaries, which is why the content model
reflects the distribution of CoreSCs in the abstracts.
To create our summaries, we employed automat-
ically generated CoreSC annotations, which are the
output of the classifiers described in (Liakata et al,
2012). These classifiers assign CoreSC categories
to sentences on the basis of features local to a sen-
tence, such as significant n-grams, verbs and word
triples, as well as global features such as the posi-
tion of the sentence within the document and within
a paragraph and section headers. The following sub-
sections give details about the creation of extractive
summaries from CoreSC categories.
3.1 A content model for CoreSC extractive
summaries
Building an extractive summary using a computa-
tional model of document structure is an idea shared
by many previous approaches, whether the model is
hand-crafted, based on rhetorical elements (McKe-
own, 1985; Teufel and Moens, 2002) or rhetorical
relations (Marcu, 1998b; Marcu, 1998a) or whether
it is a content model, learnt automatically from text
as in (Barzilay and Lee, 2004), focussing on the lo-
cal content or a combination of the local content and
global structure (Sauper et al, 2010).
749
Our document content model is primarily based
on the global discourse of the article as provided by
the type and number of CoreSC categories. How-
ever, unlike (Teufel and Moens, 2002), who take a
fixed number of AZ categories of specific type to
create rhetorical extracts, the number of categories
used from each CoreSC category depends on their
distribution in the original article. Any and all types
of CoreSC category could potentially appear in a
summary, as our summaries are meant to be repre-
sentative of the entire content of the paper. Also, the
ordering of the categories in the summary is learnt
to reflect the ordering of categories observed in ab-
stracts of papers from the same domain.
Our model also caters for local discourse depen-
dencies. For example, the selection of a particu-
lar ?Method? sentence for inclusion in the summary
should influence the choice of ?Experiment? sen-
tences, which refers to particular experimental pro-
cedures performed. This is not an issue of concern
to (Teufel and Moens, 2002), but relates to the no-
tion of NUCLEUS and SATELLITE clauses, which
form the foundation of Rhetorical Structure The-
ory (Mann and Thompson, 1998), and guides the
summarisation paradigm of (Marcu, 1998a; Marcu,
1998b). However, the difference here is that we
define a-priori certain categories to be independent
(have the property of playing the role of nucleus in
the discourse) and specify their relation with partic-
ular types of dependent categories. Thus, nuclearity
becomes a property of the CoreSC category, which
is indirectly inherited by the sentence.
Therefore, when creating the CoreSC content
model for summaries we addressed the following is-
sues: (i) summary length; (ii) number of sentences
from each CoreSC, (iii) the ordering in which sen-
tences from each CoreSC category should appear
and (iv) the extraction of sentences according to in-
dependent and dependent categories.
? Summary length: While the literature (Teufel,
2010, p.45) suggests that 20?30% of the original
document is required for an adequately informa-
tive summary, (Teufel, 2010, p.55) assumes this
is too long for scientific papers. For this reason
and to allow better comparison between papers
of varying lengths, we fixed our summary length
to 20 sentences. This is reasonable considering
we have 11 CoreSCs, any and all of which can
appear in both abstracts and full papers.
? Number of sentences from each category: To
reflect the content of the paper, the distribution
of the CoreSC categories in the extract follows
the distribution of CoreSCs in the full paper.
For each CoreSC we determine the number of
sentences to be selected (n(selected(C))) by
multiplying the ratio of that category in the paper
by 20. A difficulty arises if the ratio of a partic-
ular concept in the paper is very low (? 0.05)
in which case we prefer to include one sentence.
If a particular concept is not at all present in the
paper, the number of selected sentences for that
category will be 0.
? Ordering of CoreSC categories in the sum-
mary: According to a study of empirical sum-
maries (Liddy, 1991), sentences of a particular
textual type appear in a particular order. Since
paper abstracts were the closest approximation
of human summaries available to us, CoreSC
category transitions found in abstracts have been
adopted in our content model for extracts. The
transitions were derived semi-empirically. First,
we extracted initial, medium and final bi-grams
of categories from paper abstracts together with
transition probabilities.
Using this information we manually constructed
transitions of the CoreSC categories that best fit
the observed frequencies and our own intuitions.
This gave us the following sequence: MOT >
(HYP) > OBJ > GOA > BAC > MOD > MET
> EXP > OBS > (HYP) > RES > CON. HYP
appears twice in the sequence as annotators had
distinguished two types of hypotheses, global
hypotheses (stated together with other objec-
tives) and hypotheses about particular observa-
tions. The model provides an amalgamated rep-
resentation of CoreSC concepts in abstracts. In-
terestingly, our semi-empirically derived model
closely follows the content model for abstracts
described in (Liddy, 1991). It would be interest-
ing to see how this compares to a Markov model
of CoreSC categories learnt from the annotated
abstracts.
750
3.2 Sentence extraction based on independent
and dependent categories
Sentence extraction involves selecting the most rel-
evant sentences to include in a summary. Typically,
this entails ranking the sentences according to some
measure of salience and selecting the top n-best
sentences. For example, a sentence will be repre-
sented by a number of features associated with it,
such as whether it contains certain high frequency
words or cue phrases, its location in the document,
location in a paragraph (Brandow et al, 1995; Ku-
piec et al, 1995). Other methods include clustering
based on sentence similarity and choosing the cen-
troids (Erkan and Radev, 2004) or choosing the best
connected sentences (Mihalcea and Tarau, 2004).
When sentences are classified according to
CoreSC categories features such as the ones de-
scribed above for text extraction are taken into
account. Liakata et al (2012) report that the
most salient features for classifying CoreSC cat-
egories are overall n-grams, verbs and direct ob-
jects whereas other features such as the location of
the sentence, the neighbouring section headings and
whether a sentence contains citations play an impor-
tant role for some of the categories. Thus, classi-
fication into CoreSC categories already provides a
selection bias for sentence extraction.
As explained in section 3.1, the number of
CoreSC categories in the summaries is determined
according to their distribution in the paper and the
order of the categories is specified in the content
model. Salience for sentence extraction in this case
is determined by the need to select the most repre-
sentative sentences for a category. There isn?t much
point, for example, in identifying that we need to in-
clude a Method sentence (MET) and that this should
be followed by an Experiment sentence (EXP), if we
are not sure that those are indeed the categories of
the sentences we are about to select.
We therefore rank sentences according to the clas-
sifier confidence score (probability) with which they
were assigned a CoreSC category in (Liakata et al,
2012). The intuition behind this is that sentences
with high classifier confidence will be less noisy,
high precision cases and more representative of a
particular category. Indeed, (Liakata et al, 2012)
report statistical significance for the correlation be-
tween high classifier confidence and agreement be-
tween manual and automatic classification
However, as mentioned in section 3.1, there is
inter-dependence between sentences in the text,
which is in turn inherited by the categories assigned
to them. For example, the highest ranking MET
sentence will be related to an Experiment (EXP) or
Background (BAC) sentence, which may not be the
ones with the highest confidence score in their cate-
gory.
In order to preserve discourse cohesion it is im-
portant to select related sentences from different
categories. We resolve this by distinguising the
CoreSCs into independent categories, which by def-
inition are expected to show nucleus behaviour, and
dependent categories. We also specify the rela-
tion between independent and dependent categories.
The independent categories include the categories
with the lowest percentage of sentences in scien-
tific articles as reported in (Liakata et al, 2012),
namely: Motivation (MOT) (1%), Goal (GOA)(1%),
Hypothesis (HYP)(2%), Object (OBJ)(3%), Model
(MOD)(9%), Conclusion (CON)(9%) and Method
(MET)(11%). Categories whose sentence selec-
tion semantically depends on the former are Exper-
iment (EXP)(10%), Background (BAC)(19%), Re-
sult (RES)(21%) and Observation (OBS)(14%). The
independent categories also have higher precision
than recall, in contrast to the dependent categories.
While MET and EXP are almost equally represented
in the CoreSC corpus, EXP by definition provides
the detailed steps of an experimental method and
thus it is semantically dependent on some MET cat-
egory. More specifically, the dependencies are con-
sidered to be as follows: EXP, BAC depend on MET,
RES depends on CON and OBS depends on RES
(OBS is double-dependent).
Sentence extraction is driven by first identifying
the independent categories based on classifier con-
fidence scores and then choosing the corresponding
dependent categories on the basis of both related-
ness to the independent categories and classifier con-
fidence. We use sentence proximity (defined below)
as a measure for relatedness and combine it with
classifier confidence during sentence extraction.
The mechanism to select sentences for inclusion
in the summary, which considers category depen-
dencies, proceeds as follows:
751
? For an independent category CatI, order sentences by
decreasing order of confidence score. The confidence
score is the average confidence score of the SVM and
CRF classifiers reported in (Liakata et al, 2012) for
a sentence.
? For a dependent category Cat, for which we need n
sentences, given the selected sentences m from the
corresponding independent category CatI we do the
following:
? If m = 0, then treat Cat as independent category
for this case.
? Otherwise, for each selected sentence ti in CatI,
calculate its proximity score to every sentence cj
of the dependent category Cat. Proximity is de-
fined as 1?Distance where Distance is an ab-
solute difference in sentence ids between cj and
ti normalised by the maximum absolute distance
found between all cj and ti pairs.
? The classifier prediction score for each cj is mul-
tiplied by the Proximity(cj , ti) score and the
sentences are re-ranked according to the new
scores, where only the n highest ranking cjs are
kept. The last two steps result in an m?n matrix.
? If m = 1, then the choice for the n sentences for
Cat is straightforward.
? Otherwise, we pick the n highest ranking cjs,
proceeding row-wise. Thus, the highest ranking
cjs for the highest ranking independent sentences
ti are given priority and any cj is chosen at most
once.
Once the sentence ids are selected for each inde-
pendent and each dependent category we plug them
into the content model. Sentence order is preserved
within each CoreSC category. For example, if two
Result sentences are selected, the order in which
they appear in the paper will be preserved in the
summary.
4 Summary evaluation via question
answering
4.1 Task Description and experimental setup
We evaluate the extractive CoreSC summaries in
terms of how well they enable 12 chemistry ex-
perts/evaluators (with at least a Masters degree in
chemistry) to answer complex questions about the
papers. Our test corpus consists of 28 papers held
out from the ART/CoreSC corpus, roughly 1/9,
which were annotated automatically with the SVM
and CRF classifiers described in (Liakata et al,
2012) trained on the remaining 8/9 of the corpus.
For each of the 28 papers in the test corpus, we gen-
erated CoreSC summaries automatically using the
method described in section 3. We compare the
performance of the experts on a question answer-
ing (Q-A) task when given the CoreSC summaries
and two other types of summary, amounting to a
total of three experimental conditions (A,B,C). The
other two types of summary are the original paper
abstracts (summaries A), in the absence of human
summaries, and summaries generated by Microsoft
Office Word 2007 AutoSummarize (summaries B).
Microsoft Office Word 2007 AutoSummarize
(MA) is a widely available commercial system with
reportedly good results (Garcia-Hernandez et al,
2009) and performance equivalent to TextRank (Mi-
halcea and Tarau, 2004). MA works by assigning a
score to each word in a sentence depending on its
frequency in the document and sentences are ranked
and extracted according to the combination of scores
of the words they contain. MA therefore follows
classic lexicalised text extraction techniques, is do-
main independent and is completely agnostic of the
discourse. For the latter reason, we considered MA
to be a suitable baseline the comparison with which
would illustrate the effect of using CoreSC cate-
gories on the summary and the merits of having a
discourse based model for summarisation.
Neither the paper title nor section headings were
available to any of the summarising systems as our
extractive system does not make direct use of them
and we were not sure how they would influence MA.
To ensure that each evaluator considered only one
type of summary per paper, so as to avoid bias from
previous stimuli, and to make sure all experts were
exposed to all papers and all types of summary, the
12 experts were assigned to four groups (G1-G4)
and were allocated 28 summaries each according to
the Latin Square design in Table 1.1.
The experimental setup follows the paradigm
of (Teufel, 2001). However, while (Teufel, 2001) de-
veloped a Q-A task to evaluate summaries showing
the contribution of a scientific article in relation to
previous work, the purpose of the Q-A task at hand
1Initially we had four experimental conditions but one was
dropped, so is not presented in this context
752
is to show the usefulness of the extracted summaries
in answering questions on the paper, and how they
compare to a discourse-agnostic baseline. In the
case of (Teufel, 2001) the task consists of a fixed set
of five questions, the same for all articles tuned par-
ticularly to the relation of current and previous work.
By contrast, the current Q-A task aims to show how
well the summaries represent the content of the en-
tire paper, which means that questions are individ-
ual to each paper and required domain knowledge to
create.
Each of the 12 experts answered three content-
based questions per summary, where the questions
were individual to each paper. An example of the
questions and the corresponding answers for a given
paper can be found below.
Example 4.1.1
? Q:What do DNJ imino sugars inhibit the action of?
A: They inhibit glycosidases and ceramide glucosyl-
transferases.
? Q:What methods do the authors use to study the confor-
mation of N-benzyl-DNJ?
A: They use resonant two-photon ionization (R2PI),
ultraviolet?ultraviolet (UV?UV) hole burning, and in-
frared (IR) ion-dip spectroscopies in conjunction with
electronic structure theory calculations.
? Q:What is the conformation of the exocyclic hydrox-
ymethyl group?
A: The exocyclic hydroxymethyl group is axial to the
piperidine ring (gauche- to the ring nitrogen).
As one can see, the questions are complex wh-
questions and correspond to answers with multiple
components. Questions were complex, to minimise
the likelihood of correct random answers. They
were designed by a senior chemistry expert with
knowledge of linguistics, so that they could be an-
swered based on the abstracts (A). For this purpose,
the senior expert chose abstracts that were at least
three sentences long. Ideally, the questions and an-
swers should have been set on the basis of the en-
tire paper, but this was not possible given our time-
frame for the experiment.The underlying assump-
tion is that a good summary should cover most of the
main points of the paper. One of the merits of set-
ting the questions on the basis of the abstracts was
that the answers to be identified were deemed suf-
ficiently important to be expressed in the humanly
created abstract. However, automatic summaries
created in the way proposed here could potentially
answer questions beyond the scope of the abstract
and in cases of very short abstracts be much more
informative.
Experts were told that summaries were automati-
cally generated with no details about different types
of summary; it is assumed that none of them is com-
pletely familiar with the work mentioned in the 28
papers.
On average, it took experts less than 10 minutes to
read a summary and answer the three content-based
questions.
Papers (28)
Evaluator groups 1?7 8?14 15?21 22-28
G1 A B - C
G2 C A B -
G3 - C A B
G4 B - C A
Table 1: Distribution of summaries to evaluators
4.2 Results and Discussion
We compared each evaluator?s answers obtained af-
ter reading a summary against the model answers
set by the senior expert, the author of the questions,
based on the abstract (A) of the corresponding pa-
per. If an evaluator?s answer is identical to a model
answer, then this counts as ?matched?.
For instance in example 4.1.1 above, ?axial to
the piperidine ring?, ?gauche- to the ring nitrogen?
and ?The OH6 group is axial (Gauche) to the ring
nitrogen? were all considered correct, fully matched
answers to the question ?What is the conformation
of the exocyclic hydroxymethyl group??. In the
case of the second question in the same example all
of the following were considered correct and fully
matched: ?Resonant two-photon ionization (R2PI),
UV/UV hole-burn, and IR ion-dip spectroscopies in
conjunction with electronic structure theory calcula-
tions?, ?R2PI UV/UV hole-burn IR ion-dip e- struc-
ture theory calculations? and ?a combination of res-
onant two-photon ionization (R2PI), UV/UV hole-
burn, and IR ion-dip spectroscopies in conjunction
with electronic structure theory calculations?.
If the answer requires listing more than one item
(as is the case with questions one and two of ex-
ample 4.1.1), all of the items have to be matched.
Partially matched answers are counted as ?partially
matched?. Non-matching answers can be of two
753
types. If an un-matched answer coincided with the
answer the senior expert would have given after
reading that particular summary, then it was marked
as ?un-matched:justified?: Such answers were cor-
rect given the particular summary, but are not nec-
essarily correct with respect to the paper and do
not count as alternative answers. If the answer
was un-matched and also unjustified given the con-
tent of the summary, then it was marked as ?un-
matched:unjustified? . These are cases of evalua-
tor error. Similarly, cases where the evaluator gave
?N/A? as an answer were marked as ?justified? or
?unjustified? according to whether the senior expert
could find the answer in the summary or not. The
results from marking answers are shown in Table 2.
Number of A B C
Matched 240 126 135
Partially matched 0 4 3
Un-matched:justified 0 25 15
N/A:justified 0 71 71
Un-matched:unjustified 5 11 17
N/A:unjustified 7 15 11
All answers 252 252 252
Table 2: Matches between summary-based answers and
model answers
Micro-AVG Macro-AVG
S. types R P F R P F
A 1 0.95 0.98 1 0.95 0.97
B 0.64 0.70 0.67 0.64 0.64 0.60
C 0.66 0.75 0.70 0.64 0.70 0.65
Table 3: Precision, Recall and F-score for answering
questions using the four types of summary. A: abstracts,
B: autosummarize, C:automatic CoreSC summaries.
We report Precision, Recall and F-score (P-R-F)
for answering questions given each type of sum-
mary (Table 3). To calculate these we define TP as
matched answers, FN as N/A:justified and FP every-
thing else (partially matched + un-matched:justified
+ un-matched:unjustified + N/A:unjustified). Here,
the standard definition of recall (TP/(TP+FN))
demonstrates how many questions can be answered
using the summary (summary coverage) and Preci-
sion (TP/(TP+FP)) how well the questions are an-
swered (summary clarity).
We consider the F-measure to be an overall indi-
cator of the summary usefulness. Micro-averaging
is obtained by adding all answers from all papers to
calculate TP, FN and FP whereas macro-averaging
calculates P-R-F first per paper and then averages
over all papers.
The rankings remain consistent regardless of the
averaging method. Condition A (abstracts) shows
perfect Recall (the evaluators are able to answer all
the questions) whereas Precision is affected by un-
justified failed matches (Table 2). The perfect recall
is hardly surprising as the questions are designed
on the basis of the abstract but provides a sanity
check for the experiment. The precision sets an up-
per bound for precision with automatic summaries.
Summaries of condition C provide answers to more
questions (Recall) and with greater accuracy (Pre-
cision) than summaries B. When macro-averaging,
the Recall score of summaries C is tied with that for
summaries B but Precision is 6% higher.
To verify the statistical significance for the dif-
ference in precision and recall for summaries B and
C respectively, we performed Monte Carlo sampling
10000 times, for the populations of answers for sum-
maries B and C. During each iteration of sampling,
precision and recall were calculated, creating popu-
lations of 10000 recalls and 10000 precisions propa-
gated to be representative of the original population
of answers. A t-test performed on the population
of precision and the population of recalls showed
statistical significance at 95% in both cases, with
summaries C having a precision of 5% higher and
a recall of 1.4-1.6% higher than summaries B (see
Table 4). Therefore, we can say that CoreSC sum-
maries C are overall better for answering questions
than summaries B.
Comparison between B and C (B-C)
precision recall
t = -105.90 t = -32.52
df = 19959.79 df = 19994.40
p-value < 2.2e-16 p-value < 2.2e-16
alternative hypothesis: true difference in means 6= 0
95% confidence interval: 95% confidence interval:
-0.051 -0.049 -0.016 -0.014
sample estimates: sample estimates:
mean of x mean of y mean of x mean of y
0.696 0.746 0.639 0.655
Table 4: Test for statistical significance betwen sum-
maries B (microsoft) and C (CoreSC)
The difference in precision between summaries
B and C shows the advantage of having a con-
754
tent model: summaries C are significantly clearer.
We had also expected CoreSC summaries to have a
much higher coverage than summaries B, and there-
fore significantly higher recall. However, this dif-
ference was less pronounced perhaps because au-
tosummarize favours shorter sentences, which are
more likely to be found in the abstracts. We expect
that a refinement in the sentence selection criterion,
which would also take sentence length into account,
will help to showcase further the benefits of using a
CoreSC-based content model.
Analysis using ROUGE showed that while sum-
maries C had a slightly higher ROUGE-1 measure
than summaries B (0.75 vs 0.73), with respect to ab-
stracts, ROUGE-L was the same for the two (0.70).
In table 5 we also report measurements on sum-
mary informativeness based on divergence (Kull-
back Leibler (KL) divergence and Jensen Shannon
(JS) divergence), as in (Louis and Nenkova, 2013).
KL divergence is asymmetric and reflects the aver-
age number of bits wasted by coding samples of a
distribution P using another distribution Q. JS diver-
gence is an information-theoretic measure, reflect-
ing the average distance of the KL divergence be-
tween summary and input (the full paper in our case)
from the mean vocabulary distributions. Compared
to other measures, JS divergence has been found
to produce the best predictions of summary qual-
ity (Louis and Nenkova, 2013). In practice, what JS
divergence tells us is how ?different?/divergent the
summary is from the original paper. Low divergence
scores are indicative of greater overlap between the
summaries and the original paper and are considered
positive in terms of the summary information con-
tent.
type KLI-S KLS-I UnJSD SJSD
B 1.66 0.70 0.21 0.19
C 1.40 0.62 0.18 0.17
random 1.61 0.79 0.21 0.19
Table 5: Macro-averaged divergence scores for the 28
test summaries. B: Autosummarize, C: CoreSC, random:
random summaries each 20 sentences long for each paper.
KLI-S: Average Kullback Leibler divergence between in-
put and summary. KLS-I: Kullback Leibler divergence
between summary and input, since KL divergence is not
symmetric. UnJSD: Jensen Shannon divergence between
input and summary. No smoothing. SJSD:A version with
smoothing.
One can see the that CoreSC summaries have con-
sistently lower divergence (both KL and JS) than mi-
crosoft autosummarise summaries and random sum-
maries of the same length. This is a positive out-
come but since such automatic measures of sum-
mary quality have not yet reached maturity and are
harder to interpret, we consider the manual evalua-
tion a more reliable indicator of summary informa-
tiveness and usefulness. Note that it is not appropri-
ate to use divergence to assess the abstracts as this
measure is influenced by the length of a text, which
varies dramatically in the case of abstracts.
5 Conclusions and future work
We have shown how a content model based on
the scientific discourse as annotated by the CoreSC
scheme can be used to produce extractive sum-
maries. These summaries can be generated as al-
ternatives to abstracts. Since they preserve the dis-
tribution of CoreSCs in the paper and are not pro-
duced independently of it, as is the case with many
abstracts, they are potentially more representative of
abstracts than the full article. We have tested the use-
fulness CoreSC based summaries in answering com-
plex questions relating to the content of scientific
papers. Extracts from automated CoreSCs are infor-
mative, outperform microsoft autosummarise sum-
maries, in both intrinsic and extrinsic evaluation, and
enable experts to answer 66% of complex questions
with a precision of 75%.
In the future we would like to experiment further
with refining the sentence selection method so as
to consider criteria for local cohesion, such as lex-
ical chains. We would also like to perform com-
parisons with automatically induced content mod-
els and check their viability for scientific articles.
We also would like to perform a human based eval-
uation of coherence and explore the full potential
of these summaries as alternatives to author-written
abstracts. This work constitutes a very important
step in producing automatic summaries of scientific
papers and enabling experts to extract information
from the papers, a major requirement for resource
curation, which is dependent on constant reviewing
of the literature.
755
Acknowledgements
This work has been funded by an Early Career
Leverhulme Trust Fellowship to Dr Liakata and by
EMBL-EBI, UK. The authors would like to thank
Annie Louis, Yufan Guo, Simone Teufel, Stephen
Clark and the anonymous reviewers for their valu-
able comments. We would also like to thank Mo
Abrahams for the python version of the summarisa-
tion code and the cafe summary toolkit.
References
S. Ananiadou, Pyysalo S., and J. Tsujii. 2010. Event
extraction for systems biology by text mining the liter-
ature. Trends in Biotechnology, 28(7):381?390.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120. Best paper award.
R. Brandow, K. Mitze, and L. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing and Management,
31:675?685.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using latent
permutations. J. Artif. Int. Res., 36:129?163, Septem-
ber.
A. M. Cohen and W. R. Hersh. 2005. A survey of current
work in biomedical text a survey of current work in
biomedical text mining. Briefings in Bioinformatics,
6:57?71.
Danish Contractor, Yufan Guo, and Anna Korhonen.
2012. Using argumentative zones for extractive sum-
marization of scientific articles. In COLING, pages
663?678.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22:2004.
R.A. Garcia-Hernandez, Y. Ledeneva, G.M. Mendoza,
A.H. Dominguez, J. Chavez, A. Gelbukh, and J.L.T.
Fabela. 2009. Comparing commercial tools and state-
of-the-art methods for generating text summaries.
In Artificial Intelligence, 2009. MICAI 2009. Eighth
Mexican International Conference on, pages 92?96,
November.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 273?283. Association for
Computational Linguistics.
T. Kim, J.and Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of bionlp?09 shared task on event ex-
traction. In Proceedings of the Workshop on BioNLP:
Shared Task, pages 1?9, Boulder, Colorado.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
756
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
the 18th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?95, pages 68?73, New York, NY, USA. ACM.
M. Liakata and L.N. Soldatova. 2009. The ART Corpus.
Technical report, Aberystwyth University.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation, Valetta,Malta.
M. Liakata, S. Saha, S. Dobnik, C. Batchelor, and
Rebholz-Schuhmann D. 2012. Automatic recognition
of conceptualisation zones in scientific articles and
two life science applications. Bioinformatics, 28:991?
1000.
Elizabeth DuRoss Liddy. 1991. The discourse-level
structure of empirical abstracts: an exploratory study.
Inf. Process. Manage., 27:55?81, February.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168. Asso-
ciation for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically as-
sessing machine summary content without a gold stan-
dard. Computational Linguistics, 39(2):267?300.
W. C. Mann and S. A. Thompson. 1998. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1998a. Improving summarization
through rhetorical parsing tuning. In Proceedings of
The Sixth Workshop on Very Large Corpora, pages
206?215, Montreal,Canada.
Daniel C. Marcu. 1998b. The rhetorical parsing,
summarization, and generation of natural language
texts. Ph.D. thesis, Toronto, Ont., Canada, Canada.
AAINQ35238.
Kathleen R. McKeown. 1985. Text generation: using
discourse strategies and focus constraints to generate
natural language text. Cambridge University Press,
New York, NY, USA.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 689?696, Morristown, NJ, USA.
Association for Computational Linguistics.
Vahed Qazvinian and Dragomir R Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th annual
meeting of the association for computational linguis-
tics, pages 555?564. Association for Computational
Linguistics.
Vahed Qazvinian, Dragomir R Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 895?903. Association for Computational Lin-
guistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In EMNLP?10, pages 377?387.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28:409?445, De-
cember.
Simone Teufel, Advaith Siddharthan, and Colin Batche-
lor. 2009. Towards discipline-independent argumen-
tative zoning: Evidence from chemistry and computa-
tional linguistics. In Proceedings of EMNLP-09, Sin-
gapore.
Simone Teufel. 2001. Task-based evaluation of sum-
mary quality: Describing relationships between sci-
entific papersworkshop ?automatic summarization?,
naacl-2001. In NAACL-01 Workshop ?Automatic Text
Summarisation?, Pittsburgh, PA.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. CSLI Studies in Computational Linguistics.
Center for the Study of Language and Information,
Stanford, California.
Elizabeth White, K. Bretonnel Cohen, and Larry Hunter.
2011. Hypothesis and evidence extraction from full-
text scientific journal articles. In Proceedings of
BioNLP 2011 Workshop, pages 134?135, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
757
Proceedings of the First Celtic Language Technology Workshop, pages 60?65,
Dublin, Ireland, August 23 2014.
gdbank: The beginnings of a corpus of dependency structures and
type-logical grammar in Scottish Gaelic
Colin Batchelor
Royal Society of Chemistry, Cambridge, UK CB4 0WF
batchelorc@rsc.org
Abstract
We present gdbank, a small handbuilt corpus of 32 sentences with dependency structures and
categorial grammar type assignments. The sentences have been chosen to illustrate as broad a
range of the unusual features of Scottish Gaelic as possible, particularly nouns being used to
represent psychological states where more thoroughly-studied languages such as English and
French would prefer a verb, and prepositions marking aspect, as is also seen in Welsh and, for
example, Irish Gaelic. We provide hand-built dependency trees, building on previous work on
Irish Gaelic and using the Universal Dependency Scheme. We also provide a tentative categorial
grammar account of the words in the sentences, based largely on previous work on English.
1 Introduction
Scottish Gaelic (usually hereafter Gaelic) is a Celtic language, rather closely related to Irish, with around
59,000 speakers as of the last UK census in 2011. As opposed to the situation for Irish Gaelic (Lynn et
al., 2012a; Lynn et al., 2012b; Lynn et al., 2013; Lynn et al., 2014) there are no treebanks or tagging
schemes for Scottish Gaelic, although there are machine-readable dictionaries and databases available
from Sabhal M`or Ostaig. A single paper in the ACL Anthology (Kessler, 1995) mentions Scottish Gaelic
in the context of computational dialectology of Irish. There is also an LREC workshop paper (Scan-
nell, 2006) on machine translation between Irish and Scottish Gaelic. Elsewhere in the Celtic languages,
Welsh has an LFG grammar (Mittendorf and Sadler, 2005) but no treebanks. For Breton there is a small
amount of work on morphological analysis and Constraint-Grammar-based machine translation (Tyers,
2010). Recent work on the grammar of Scottish Gaelic (for example (Adger and Ramchand, 2003; Adger
and Ramchand, 2005), but there are many more examples) has largely focussed on theoretical syntac-
tic issues somewhat distant from the more surfacy approaches popular in the field of natural language
processing. This paper explores grammatical issues in Scottish Gaelic by means of dependency tagging
and combinatory categorial grammar (CCG), which we see as complementary approaches. As such it
is explicitly inspired by CCGbank (Hockenmaier and Steedman, 2007), which consists of dependency
structures and CCG derivations for over 99% of the Penn Treebank. It is hoped that this corpus will be
a useful adjunct to currently on-going work in developing a part-of-speech tagset and tagger for Scottish
Gaelic.
Section 2 describes how the corpus was prepared, sections 3 and 4 give some context for the depen-
dency scheme and categorial grammar annotations respectively, and the main part of the paper is section
5, which deals with language-specific features of the corpus.
2 Preparing the corpus
The corpus consists of a small handbuilt selection of sentences from the transcripts of An Litir Bheag,
which is a weekly podcast from the BBC written by a native speaker and aimed at Gaelic learners,
example sentences from (Lamb, 2003), the BBC?s online news in Gaelic and the Gaelic column in the
Scotsman newspaper. In order to illustrate as much of the interesting points of Scottish Gaelic as possible,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
60
Dependency Example Gloss GR
det gach latha (det latha gach) every day det
dobj Ithidh i `?m (dobj Ithidh
`?m) She eats butter dobj
adpmod Tha piseag agam (adpmod Tha agam) I have a kitten ncmod
adpobj `as an eilean (adpobj
`as eilean) from the island dobj
nsubj Tha mi a? dol (Tha mi) I am coming ncsubj
prt Chan eil (prt eil chan) is not ncmod
xcomp Tha mi ag iarraidh (xcomp Tha iarraidh) I want xcomp
acomp Tha i breagha (xcomp Tha breagha) It is fine xcomp
ccomp bheachd gun tigeadh e (ccomp bheachd tigeadh) thought he would come ccomp
mark gun tigeadh e (mark tigeadh gun) that he would come ncmod
Table 1: Examples of the UDS-based scheme in this paper mapped to the Briscoe and Carroll scheme.
we looked in particular for sentences describing psychological states and made sure that a reasonable
number of the sentences used each verb for ?to be?, which we will illustrate in section 5.
The sentences are tokenized by hand using the following rules: (1) Punctuation which never forms
part of a lexical item such as the comma, the full stop, the colon and the semicolon is always separated
out from the previous word. (2) Strings connected by a hyphen, for example h-Alba in Banca na h-Alba
(Bank of Scotland) or t-
`
Oban as in an t-
`
Oban (the town of Oban) are always kept together. (3) The
apostrophe is kept together with the copula where it proceeds it, for example in ?S fhearr leam (I like).
(4) Because the past tense particle do is reduced to dh? before a vowel and before f, and this is always
typographically closed up, we separate out past-tense dh? as its own token. These rules work for the
small dataset described here but would clearly need to be expanded for work in the wild.
In this preliminary work the dependencies and types have been determined by a single, non-native
speaker, annotator, according to a set of guidelines which were built up during the annotation process.
This is clearly less than ideal, however, the guidelines are available along with the corpus and we hope
to be able to get the input of a native speaker, not least for interannotator studies.
We use the CoNLL-X format (Buchholz and Marsi, 2006), leaving the POS and projective dependency
fields empty and store the categorial grammar type under field 6, FEATS.
3 Dependency scheme
There are four dependency schemes that we consulted while preparing the corpus. The initial inspiration
was provided by the C&C parser (Curran et al., 2007), which in addition to providing categorial gram-
mar derivations for sentences provides a dependency structure in the GR (Grammatical Representation)
scheme due to (Briscoe and Carroll, 2000; Briscoe and Carroll, 2002). This contains 23 types and was
developed originally for parser evaluation. Another popular scheme is the Stanford Dependency scheme
(de Marneffe and Manning, 2008; de Marneffe and Manning, 2013), which is more finely-grained with
over twice the number of dependency types to deal specifically with noisy data and to make it more
accessible to non-linguists building information extraction applications. A very important scheme is the
Dublin scheme for Irish (Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013), which is of a similar
size to the Stanford scheme, but the reason for its size relative to GR is that it includes a large num-
ber of dependencies intended to handle grammatical features found in Irish but not in English. Lastly
we mention the Universal Dependency Scheme developed in (McDonald et al., 2013), which we have
adopted, despite its being coarser-grained than the Dublin scheme, on account of its simplicity and utility
for cross-lingual comparisons and cross-training (Lynn et al., 2014).
Table 1 gives examples of the dependency relations used along with their mapping to the GR scheme.
4 Categorial grammar
Combinatory categorial grammar (CCG) is a type-logical system which was developed to represent nat-
ural languages such as English but has subsequently been extended to other systems such as chord se-
61
quences in jazz (Granroth-Wilding and Steedman, 2012). For a full description the reader is referred to
(Steedman and Baldridge, 2003), but in order to follow the rest of this paper you merely need to know
that the type N/N is a function which takes an argument of N to its right, returning N, and that the type
N\N is a function expecting an argument of N to its left and that these are combined by application,
composition, where A/B combines with B/C to yield A/C, and type-raising where N is converted to
T/(N\T). Attractive features of CCG for modelling a less-well-studied language include that it is a lex-
ical theory in which it is the lexicon contains the rules for how words are combined to make sense rather
than an external grammar, that it allows all manner of unconventional constituents, which is particularly
powerful for parsing coordinated structures in English, that it is equivalent to a weakly context-sensitive
grammar and hence has the power of a real natural language. In Steedman and Baldridge (2003) there are
examples of the application of multimodal CCG to Irish Gaelic. However, to the best of our knowledge
this paper is the first application of CCG to Scottish Gaelic.
In gdbank, there is a single hand-built CCG derivation for every sentence. The notation is based on that
in CCGbank with a small number of adaptations for Gaelic (see next section). The basic units that can be
assembled into types are S (clauses), N (nouns), conj (conjugations), and PP (prepositional phrases).
For subcategorization purposes and to help keep things clear for the annotator and the reader we mark
prepositional phrases with the dictionary form of the preposition.
We have not yet investigated overgeneration and ungrammatical sentences, hence there is only one
kind of modality in gdbank; however restricting the way words can combine to the way in which they
actually do combine in Gaelic is an obvious and essential next step.
5 Language-specific features
Prepositional phrases in Gaelic are often single-word, fused preposition?pronouns, a part-of-speech
found across the Celtic languages. An ambiguous case of this is the token ris, which can be either ri with
the pronoun e, hence taking the CCG type PP[ri], or the pre-determiner form of ri, hence PP[ri]/N[b].
The other class of fused preposition?pronoun we need to consider is that in sentences like Tha mi gad
chluinntinn, ?I can hear you?, where gad is ag fused with do ?your?. In this case it has type PP[ag]/S[n].
Adjectives as in CCGbank are treated as clauses, S[adj]. The verbal noun is labelled S[n] by analogy
with Hockenmaier and Steedman (2007). In addition to declarative and interrogative clauses, S[dcl]
and S[q], we take our lead from the fourfold division of preverbal particles and add negative clauses
S[neg], usually introduced by cha or chan, and negative interrogative clauses, S[negq], introduced
by nach.
There are two verbs for ?to be? in Scottish Gaelic, bi and is. Bi is used for predicate statements
about nouns, to forming the present tense and to describe some psychological states. It does not usually
equate two NPs, with an exception we will come to. In the Dublin scheme the prepositional phrase
headed by ag in T?a s?e ag iascaireacht (?He is fishing.?) is treated as being an externally-controlled
complement of T?a (Gaelic tha) and we carry this analysis over into Scottish Gaelic where this is the
most common way of expressing the present tense. Figure 1 demonstrates this, where dhachaigh is a
non-clausal modifier of dol, the verbal noun for ?to go?. Is can be used as the copula between two NPs,
and to express psychological states such as liking and preference. To say ?I am a teacher?, the Gaelic
is ?S e tidsear a th? annam. This, at least on the surface, equates pronoun e, with a noun described by a
relative clause including the verb bi. Fig. 1 shows our dependency tree for this. Note that this is different
from the scheme in Lynn et al. (2012b) because of a difference between the two languages. They treat
the analogous sentence Is tusa an m?uinteoir ?You are the teacher? as having a subject, ?the teacher?, and
a clausal predicate, tusa, ?you indeed?.
The most straightforward way of expressing a preference is the assertive is followed by an adjective
or noun, a PP marking the preferrer, and then the object. If you dislike music, you might say Is beag orm
ce`ol. There are exactly analogous constructions in Irish with is + adjective + PP[le] + object, for example
Is maith liom... ?I like...?, which in (U?? Dhonnchadha, 2009) is treated as having the prepositional
phrase as the subject and the adjective as predicate. We modify this to use adpmod as in the Universal
Dependency Scheme as shown in Fig. 1.
62
(a)
Tha mi a? dol dhachaigh
nsubj
xcomp
prt
advmod
(b)
?S e tidsear a th? annam
nsubj
dobj
xcomp
prt
adpmod
(c)
Is beag orm ce`ol
adpmod
acomp
dobj
Figure 1: Dependency trees for (a) ?I am going home?, (b) ?I am a teacher? and (c) ?I hate music?.
Type Count Notes Type Count Notes
N 104 noun N\N 13 adjective/genitive noun
PP/N 41 preposition PP/S[n] 10 ag/a?/air etc.
N/N 38 determiner S[dep]/PP/N 8 bi, is (after particle)
. 31 . (N\N)/S[dcl] 7 relative
S[dcl]/PP/N 25 bi, is S[n] 7 intransitive verbal noun
PP 18 PP (N\N)/(N\N) 7 genitive article
Table 2: Counts for most common types found in corpus. PP[air], PP[aig] and so on have been
merged.
6 Conclusions and future work
In this paper we have presented a small handbuilt corpus of Scottish Gaelic sentences, their dependency
structures and their CCG derivations. To the best of our knowledge this represents the first attempt to
handle a range of real-life Scottish Gaelic sentences in such a way. gdbank itself and the guidelines
used to build it are available from https://code.google.com/p/gdbank/ and we welcome
feedback. We have of course only been able to illustrate a small number of constructions. Tables 2 and
3 list counts for the categorial types and dependency relations used. In 32 sentences there are a total of
406 tokens.
We have not yet on the other hand attempted to deal with the morphology of Scottish Gaelic, for
example lenition and slenderization, beyond drawing the attention of the human annotator to these phe-
nomena when they may affect the correct parsing of a sentence. Clearly for automated natural-language
processing of Gaelic these will need to be treated programmatically. We also disregard case and gender,
although we expect that these will be dealt with as part of a rather more ambitious project, that of the
Lamb group at the University of Edinburgh to build a part-of-speech tagset and tagged corpus which we
look forward to seeing.
Acknowledgements
The anonymous referees for their very constructive comments.
Relation Count Relation Count Relation Count
adpmod 58 mark 23 amod 11
nsubj 47 nmod 18 advmod 9
adpobj 38 ccomp 17 acomp 7
det 34 prt 14 cc 6
p 33 dobj 13 rcmod 4
ROOT 32 xcomp 13 appos 2
Table 3: Counts for dependency relations in gdbank. Note the high number of adpmod relations which
is significantly larger than adpobj because of fused preposition?pronouns in Gaelic.
63
References
David Adger and Gillian Ramchand. 2003. Predication and equation. Linguistic Enquiry, 34:325?359.
David Adger and Gillian Ramchand. 2005. Psych nouns and predications. In Proceedings of the 36th Annual
Meeting of the North East Linguistic Society, Amherst, MA, October.
Ted Briscoe and John Carroll. 2000. Grammatical relation annotation. Online at
http://www.sussex.ac.uk/Users/johnca/grdescription/index.html.
Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of
the Third International Conference on Language Resources and Evaluation (LREC?02), Las Palmas, Canary
Islands, Spain, May.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Natural Language Learning, New York, NY, June.
James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&C and
Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions, pages 33?36, Prague, Czech Republic, June. Association
for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1?8, Manchester, UK, August. Coling 2008 Organizing Committee.
Marie-Catherine de Marneffe and Christopher D. Manning. 2013. Stanford typed dependencies manual. Online
at http://nlp.stanford.edu/software/dependencies manual.pdf.
Mark Granroth-Wilding and Mark Steedman. 2012. Statistical parsing for harmonic analysis of jazz chord se-
quences. In Proceedings of the International Computer Music Conference, pages 478?485. International Com-
puter Music Association, September.
Julia Hockenmaier and Mark Steedman. 2007. CCGBank: A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computational Linguistics, 33:355?356.
Brett Kessler. 1995. Computational dialectology in Irish Gaelic. In Seventh Conference of the European Chapter
of the Association for Computational Linguistics, page 60, Dublin, Ireland, March.
William Lamb. 2003. Scottish Gaelic, 2nd edn. Lincom Europa, Munich, Germany.
Teresa Lynn, Ozlem Cetinoglu, Jennifer Foster, Elaine U?? Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing: A preliminary evaluation. In Nicoletta Calzolari, Khalid Choukri, Thierry
Declerck, Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,
Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),
pages 1939?1946, Istanbul, Turkey, May. European Language Resources Association (ELRA). ACL Anthology
Identifier: L12-1189.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U?? Dhonnchadha. 2012b. Active learning and the irish
treebank. In Proceedings of the Australasian Language Technology Association Workshop 2012, pages 23?32,
Dunedin, New Zealand, December.
Teresa Lynn, Jennifer Foster, and Mark Dras. 2013. Working with a small dataset - semi-supervised dependency
parsing for Irish. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 1?11, Seattle, Washington, USA, October. Association for Computational Linguistics.
Teresa Lynn, Jennifer Foster, Mark Dras, and Lamia Tounsi. 2014. Cross-lingual transfer parsing for low-
resourced languages: An Irish case study. In Proceedings of Celtic Language Technology Workshop 2014,
Dublin, Ireland, August.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92?97, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Ingo Mittendorf and Louisa Sadler. 2005. The Welsh PARGRAM grammar. In 12th Welsh Syntax Workshop,
Gregynog, Wales, July.
64
Kevin P. Scannell. 2006. Machine translation for closely related language pairs. In Proceedings of the LREC 2006
Workshop on Strategies for developing machine translation for minority languages, pages 103?107, Genoa,
Italy, May.
Mark Steedman and Jason Baldridge. 2003. Combinatory Categorial Grammar. Online at
http://homepages.inf.ed.ac.uk/steedman/papers/ccg/SteedmanBaldridgeNTSyntax.pdf.
F. M. Tyers. 2010. Rule-based Breton to French machine translation. In Proceeedings of the 14th Annual Con-
ference of the European Association of Machine Translation, EAMT10, pages 174?181, Saint-Rapha?el, France,
May.
Elaine U?? Dhonnchadha. 2009. Part-of-speech tagging and partial parsing for Irish using finite-state transducers
and constraint grammar. Ph.D. thesis, Dublin City University.
65
