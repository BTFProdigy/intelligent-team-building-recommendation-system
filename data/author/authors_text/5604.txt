Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 9?12,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LeXFlow: a System for Cross-fertilization of Computational Lexicons 
Maurizio Tesconi and Andrea Marchetti 
CNR-IIT 
Via Moruzzi 1, 56024 Pisa, Italy 
{maurizio.tesconi,andrea.marchetti}@iit.cnr.it 
Francesca Bertagna and Monica Monachini and Claudia Soria and Nicoletta Calzolari 
CNR-ILC 
Via Moruzzi 1, 56024 Pisa, Italy 
{francesca.bertagna,monica.monachini, 
claudia.soria,nicoletta.calzolari}@ilc.cnr.it 
 
  
Abstract 
This demo presents LeXFlow, a work-
flow management system for cross-
fertilization of computational lexicons. 
Borrowing from techniques used in the 
domain of document workflows, we 
model the activity of lexicon manage-
ment as a set of workflow types, where 
lexical entries move across agents in the 
process of being dynamically updated. A 
prototype of LeXFlow has been imple-
mented with extensive use of XML tech-
nologies (XSLT, XPath, XForms, SVG) 
and open-source tools (Cocoon, Tomcat, 
MySQL). LeXFlow is a web-based ap-
plication that enables the cooperative and 
distributed management of computational 
lexicons. 
1 Introduction 
LeXFlow is a workflow management system 
aimed at enabling the semi-automatic manage-
ment of computational lexicons. By management 
we mean not only creation, population and vali-
dation of lexical entries but also integration and 
enrichment of different lexicons.  
A lexicon can be enriched by resorting to 
automatically acquired information, for instance 
by means of an application extracting informa-
tion from corpora. But a lexicon can be enriched 
also by resorting to the information available in 
another lexicon, which can happen to encode 
different types of information, or at different lev-
els of granularity. LeXFlow intends to address 
the request by the computational lexicon com-
munity for a change in perspective on computa-
tional lexicons: from static resources towards 
dynamically configurable multi-source entities, 
where the content of lexical entries is dynami-
cally modified and updated on the basis of the 
integration of knowledge coming from different 
sources (indifferently represented by human ac-
tors, other lexical resources, or applications for 
the automatic extraction of lexical information 
from texts). 
This scenario has at least two strictly related 
prerequisites: i) existing lexicons have to be 
available in or be mappable to a standard form 
enabling the overcoming of their respective dif-
ferences and idiosyncrasies, thus making their 
mutual comprehensibility a reality; ii) an archi-
tectural framework should be used for the effec-
tive and practical management of lexicons, by 
providing the communicative channel through 
which lexicons can really communicate and 
share the information encoded therein. 
For the first point, standardization issues obvi-
ously play the central role. Important and exten-
sive efforts have been and are being made to-
wards the extension and integration of existing 
and emerging open lexical and terminological 
standards and best practices, such as EAGLES, 
ISLE, TEI, OLIF, Martif (ISO 12200), Data 
Categories (ISO 12620), ISO/TC37/SC4, and 
LIRICS. An important achievement in this re-
spect is the MILE, a meta-entry for the encoding 
of multilingual lexical information (Calzolari et 
al., 2003); in our approach we have embraced the 
MILE model.  
As far as the second point is concerned, some 
initial steps have been made to realize frame-
works enabling inter-lexica access, search, inte-
gration and operability. Nevertheless, the general 
impression is that little has been made towards 
the development of new methods and techniques 
9
for the concrete interoperability among lexical 
and textual resources. The intent of LeXFlow is 
to fill in this gap.  
2 LeXFlow Design and Application 
LeXFlow is conceived as a metaphoric extension 
and adaptation to computational lexicons of 
XFlow, a framework for the management of 
document workflows (DW, Marchetti et al, 
2005).  
A DW can be seen as a process of cooperative 
authoring where the document can be the goal of 
the process or just a side effect of the coopera-
tion. Through a DW, a document life-cycle is 
tracked and supervised, continually providing 
control over the actions leading to document 
compilation In this environment a document 
travels among agents who essentially carry out 
the pipeline receive-process-send activity.  
Each lexical entry can be modelled as a docu-
ment instance (formally represented as an XML 
representation of the MILE lexical entry), whose 
behaviour can be formally specified by means of 
a document workflow type (DWT) where differ-
ent agents, with clear-cut roles and responsibili-
ties, act over different portions of the same entry 
by performing different tasks.  
Two types of agents are envisaged: external 
agents are human or software actors which per-
form activities dependent from the particular 
DWT, and internal agents are software actors 
providing general-purpose activities useful for 
any DWT and, for this reason, implemented di-
rectly into the system. Internal agents perform 
general functionalities such as creat-
ing/converting a document belonging to a par-
ticular DWT, populating it with some initial data, 
duplicating a document to be sent to multiple 
agents, splitting a document and sending portions 
of information to different agents, merging du-
plicated documents coming from multiple agents, 
aggregating fragments, and finally terminating 
operations over the document. An external agent 
executes some processing using the document 
content and possibly other data, e.g. updates the 
document inserting the results of the preceding 
processing, signs the updating and finally sends 
the document to the next agent(s). 
The state diagram in Figure 1 describes the 
different states of the document instances. At the 
starting point of the document life cycle there is 
a creation phase, in which the system raises a 
new instance of a document with information 
attached.  
Figure 1. Document State Diagram. 
 
The document instance goes into pending 
state. When an agent gets the document, it goes 
into processing state in which the agent compiles 
the parts under his/her responsibility. If the 
agent, for some reason, doesn?t complete the in-
stance elaboration, he can save the work per-
formed until that moment and the document in-
stance goes into freezing state. If the elaboration 
is completed (submitted), or cancelled, the in-
stance goes back into pending state, waiting for a 
new elaboration. 
Borrowing from techniques used in DWs, we 
have modelled the activity of lexicon manage-
ment as a set of DWT, where lexical entries 
move across agents and become dynamically 
updated.  
3 Lexical Workflow General Architec-
ture 
As already written, LeXFlow is based on XFlow 
which is composed of three parts: i) the Agent 
Environment, i.e. the agents participating to all 
DWs; ii) the Data, i.e. the DW descriptions plus 
the documents created by the DW and iii) the 
Engine. Figure 2 illustrates the architecture of the 
framework. 
Figure 2. General Architecture. 
 
The DW environment is the set of human and 
software agents participating to at least one DW. 
10
The description of a DW can be seen as an ex-
tension of the XML document class. A class of 
documents, created in a DW, shares the schema 
of their structure, as well as the definition of the 
procedural rules driving the DWT and the list of 
the agents attending to it. Therefore, in order to 
describe a DWT, we need four components:  
? a schema of the documents involved in the 
DWT; 
? the agent roles chart, i.e. the set of the ex-
ternal and internal agents, operating on the 
document flow. Inside the role chart these 
agents are organized in roles and groups in 
order to define who has access to the 
document. This component constitutes the 
DW environment; 
? a document interface description used by 
external agents to access the documents. 
This component also allows checking ac-
cess permissions to the document; 
? a document workflow description defining 
all the paths that a document can follow in 
its life-cycle, the activities and policies for 
each role.  
The document workflow engine constitutes the 
run-time support for the DW, it implements the 
internal agents, the support for agents? activities, 
and some system modules that the external agents 
have to use to interact with the DW system. 
Also, the engine is responsible for two kinds of 
documents useful for each document flow: the 
documents system logs and the documents system 
metadata. 
4 The lexicon Augmentation Workflow 
Type 
In this section we present a first DWT, called 
?lexicon augmentation?, for dynamic augmenta-
tion of semantic MILE-compliant lexicons. This 
DWT corresponds to the scenario where an entry 
of a lexicon A becomes enriched via basically 
two steps. First, by virtue of being mapped onto 
a corresponding entry belonging to a lexicon B, 
the entry(A) inherits the semantic relations avail-
able in the mapped entry(B). Second, by resorting 
to an automatic application that acquires infor-
mation about semantic relations from corpora, 
the acquired relations are integrated into the en-
try and proposed to the human encoder. 
In order to test the system we considered the 
Simple/Clips (Ruimy et al, 2003) and ItalWord-
Net (Roventini et al, 2003) lexicons.  
An overall picture of the flow is shown in Fig-
ure 3, illustrating the different agents participat-
ing to the flow. Rectangles represent human ac-
tors over the entries, while the other figures 
symbolize software agents: ovals are internal 
agents and octagons external ones. The function-
ality offered to human agents are: display of 
MILE-encoded lexical entries, selection of lexi-
cal entries, mapping between lexical entries be-
longing to different lexicons1, automatic calcula-
tions of new semantic relations (either automati-
cally derived from corpora and mutually inferred 
from the mapping) and manual verification of the 
newly proposed semantic relations.  
5 Implementation Overview 
Our system is currently implemented as a web-
based application where the human external 
agents interact with system through a web 
browser. All the human external agents attending 
the different document workflows are the users 
of system. Once authenticated through username 
and password the user accesses his workload 
area where the system lists all his pending docu-
ments (i.e. entries) sorted by type of flow. 
The system shows only the flows to which the 
user has access. From the workload area the user 
                                                 
1 We hypothesize a human agent, but the same role could be 
performed by a software agent. To this end, we are investi-
gating the possibility of automatically exploiting the proce-
dure described in (Ruimy and Roventini, 2005). 
Figure 3. Lexicon Augmentation Workflow. 
 
11
can browse his documents and select some op-
erations  
 
Figure 4. LeXFlow User Activity State Diagram. 
 
such as: selecting and processing pending docu-
ment; creating a new document; displaying a 
graph representing a DW of a previously created 
document; highlighting the current position of 
the document. This information is rendered as an 
SVG (Scalable Vector Graphics) image. Figure 5 
illustrates the overall implementation of the sys-
tem. 
5.1 The Client Side: External Agent Inter-
action 
The form used to process the documents is ren-
dered with XForms. Using XForms, a browser 
can communicate with the server through XML 
documents and is capable of displaying the 
document with a user interface that can be de-
fined for each type of document. A browser with 
XForms capabilities will receive an XML docu-
ment that will be displayed according to the 
specified template, then it will let the user edit 
the document and finally it will send the modi-
fied document to the server. 
5.2 The Server Side 
The server-side is implemented with Apache 
Tomcat, Apache Cocoon and MySQL. Tomcat is 
used as the web server, authentication module 
(when the communication between the server 
and the client needs to be encrypted) and servlet 
container. Cocoon is a publishing framework that 
uses the power of XML. The entire functioning 
of Cocoon is based on one key concept: compo-
nent pipelines. The pipeline connotes a series of 
events, which consists of taking a request as in-
put, processing and transforming it, and then giv-
ing the desired response. MySQL is used for 
storing and retrieving the documents and the 
status of the documents. 
Each software agent is implemented as a web-
service and the WSDL language is used to define 
its interface.  
References 
Nicoletta Calzolari, Francesca Bertagna, Alessandro 
Lenci and Monica Monachini, editors. 2003. Stan-
dards and Best Practice for Multilingual Computa-
tional Lexicons. MILE (the Multilingual ISLE 
Lexical Entry). ISLE Deliverable D2.2 & 3.2. Pisa. 
Andrea Marchetti, Maurizio Tesconi, and Salvatore 
Minutoli. 2005. XFlow: An XML-Based Docu-
ment-Centric Workflow. In Proceedings of WI-
SE?05, pages 290- 303, New York, NY, USA. 
Adriana Roventini, Antonietta Alonge, Francesca 
Bertagna, Nicoletta Calzolari, Christian Girardi, 
Bernardo Magnini, Rita Marinelli, and Antonio 
Zampolli. 2003. ItalWordNet: Building a Large 
Semantic Database for the Automatic Treatment of 
Italian. In Antonio Zampolli, Nicoletta Calzolari, 
and Laura Cignoni, editors, Computational Lingui-
stics in Pisa, Istituto Editoriale e Poligrafico Inter-
nazionale, Pisa-Roma, pages 745-791. 
Nilda Ruimy, Monica Monachini, Elisabetta Gola, 
Nicoletta Calzolari, Cristina Del Fiorentino, Marisa 
Ulivieri, and Sergio Rossi. 2003. A Computational 
Semantic Lexicon of Italian: SIMPLE. In Antonio 
Zampolli, Nicoletta Calzolari, and Laura Cignoni, 
editors, Computational Linguistics in Pisa, Istituto 
Editoriale e Poligrafico Internazionale, Pisa-Roma, 
pages 821-864. 
Nilda Ruimy and Adriana Roventini. 2005. Towards 
the linking of two electronic lexical databases of 
Italian. In  Proceedings of L&T'05 - Language 
Technologies as a Challenge for Computer Science 
and Linguistics, pages 230-234, Poznan, Poland.
Figure 5. Overall System Implementation. 
12
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LEXICAL MARKUP FRAMEWORK (LMF)  
FOR NLP MULTILINGUAL RESOURCES 
Gil Francopoulo1, Nuria Bel2, Monte George3, Nicoletta Calzolari4, 
Monica Monachini5, Mandy Pet6, Claudia Soria7
 
1INRIA-Loria: gil.francopoulo@wanadoo.fr 
2UPF: nuria.bel@upf.edu 
3ANSI: dracalpha@earthlink.net 
4CNR-ILC: glottolo@ilc.cnr.it 
5CNR-ILC: monica.monachini@ilc.cnr.it 
6MITRE: mpet@mitre.org 
7CNR-ILC: claudia.soria@ilc.cnr.it 
 
Abstract 
Optimizing the production, maintenance 
and extension of lexical resources is one 
the crucial aspects impacting Natural 
Language Processing (NLP). A second 
aspect involves optimizing the process 
leading to their integration in applica-
tions. With this respect, we believe that 
the production of a consensual specifica-
tion on multilingual lexicons can be a 
useful aid for the various NLP actors. 
Within ISO, one purpose of LMF (ISO-
24613) is to define a standard for lexi-
cons that covers multilingual data. 
1 Introduction 
Lexical Markup Framework (LMF) is a model 
that provides a common standardized framework 
for the construction of Natural Language Proc-
essing (NLP) lexicons. The goals of LMF are to 
provide a common model for the creation and 
use of lexical resources, to manage the exchange 
of data between and among these resources, and 
to enable the merging of a large number of indi-
vidual electronic resources to form extensive 
global electronic resources. 
Types of individual instantiations of LMF can 
include monolingual, bilingual or multilingual 
lexical resources. The same specifications are to 
be used for both small and large lexicons. The 
descriptions range from morphology, syntax, 
semantic to translation information organized as 
different extensions of an obligatory core pack-
age. The model is being developed to cover all 
natural languages. The range of targeted NLP 
applications is not restricted. LMF is also used to 
model machine readable dictionaries (MRD), 
which are not within the scope of this paper. 
2 History and current context 
In the past, this subject has been studied and de-
veloped by a series of projects like GENELEX 
[Antoni-Lay], EAGLES, MULTEXT, PAROLE, 
SIMPLE, ISLE and MILE [Bertagna]. More re-
cently within ISO1 the standard for terminology 
management has been successfully elaborated by 
the sub-committee three of ISO-TC37 and pub-
lished under the name "Terminology Markup 
Framework" (TMF) with the ISO-16642 refer-
ence. Afterwards, the ISO-TC37 National dele-
gations decided to address standards dedicated to 
NLP. These standards are currently elaborated as 
high level specifications and deal with word 
segmentation (ISO 24614), annotations 
(ISO 24611, 24612 and 24615), feature struc-
tures (ISO 24610), and lexicons (ISO 24613) 
with this latest one being the focus of the current 
paper. These standards are based on low level 
specifications dedicated to constants, namely 
data categories (revision of ISO 12620), lan-
guage codes (ISO 639), script codes 
(ISO 15924), country codes (ISO 3166), dates 
(ISO 8601) and Unicode (ISO 10646). 
 
This work is in progress. The two level organiza-
tion will form a coherent family of standards 
with the following simple rules: 
1) the low level specifications provide standard-
ized constants; 
                                                 
1 www.iso.org 
1
2) the high level specifications provide struc-
tural elements that are adorned by the standard-
ized constants. 
3 Scope and challenges 
The task of designing a lexicon model that satis-
fies every user is not an easy task. But all the 
efforts are directed to elaborate a proposal that 
fits the major needs of most existing models. 
In order to summarise the objectives, let's see 
what is in the scope and what is not. 
 
LMF addresses the following difficult chal-
lenges: 
? Represent words in languages where 
multiple orthographies (native scripts or 
transliterations) are possible, e.g. some 
Asian languages. 
? Represent explicitly (i.e. in extension) 
the morphology of languages where a de-
scription of all inflected forms (from a list 
of lemmatised forms) is manageable, e.g. 
English. 
? Represent the morphology of languages 
where a description in extension of all in-
flected forms is not manageable (e.g. Hun-
garian). In this case, representation in in-
tension is the only manageable issue. 
? Easily associate written forms and spo-
ken forms for all languages. 
? Represent complex agglutinating com-
pound words like in German. 
? Represent fixed, semi-fixed and flexible 
multiword expressions. 
? Represent specific syntactic behaviors, 
as in the Eagles recommendations. 
? Allow complex argument mapping be-
tween syntax and semantic descriptions, as 
in the Eagles recommendations. 
? Allow a semantic organisation based on 
SynSets (like in WordNet) or on semantic 
predicates (like in FrameNet). 
? Represent large scale multilingual re-
sources based on interlingual pivots or on 
transfer linking. 
LMF does not address the following topics: 
? General sentence grammar of a language 
? World knowledge representation 
In other words, LMF is mainly focused on the 
linguistic representation of lexical information. 
4 Key standards used by LMF 
LMF utilizes Unicode in order to represent the 
orthographies used in lexical entries regardless of 
language. 
Linguistic constants, like /feminine/ or 
/transitive/, are not defined within LMF but are 
specified in the Data Category Registry (DCR) 
that is maintained as a global resource by 
ISO TC37 in compliance with ISO/IEC 11179-
3:2003. 
The LMF specification complies with the 
modeling principles of Unified Modeling Lan-
guage (UML) as defined by OMG2 [Rumbaugh 
2004]. A model is specified by a UML class dia-
gram within a UML package: the class name is 
not underlined in the diagrams. The various ex-
amples of word description are represented by 
UML instance diagrams: the class name is under-
lined.  
5 Structure and core package 
LMF is comprised of two components: 
1) The core package consists of a structural 
skeleton that describes the basic hierarchy of in-
formation in a lexical entry. 
2) Extensions to the core package are ex-
pressed in a framework that describes the reuse 
of the core components in conjunction with addi-
tional components required for the description of 
the contents of a specific lexical resource. 
In the core package, the class called Database 
represents the entire resource and is a container 
for one or more lexicons. The Lexicon class is 
the container for all the lexical entries of the 
same language within the database. The Lexicon 
Information class contains administrative infor-
mation and other general attributes. The Lexical 
Entry class is a container for managing the top 
level language components. As a consequence, 
the number of representatives of single words, 
multi-word expressions and affixes of the lexicon 
is equal to the number of lexical entries in a 
given lexicon. The Form and Sense classes are 
parts of the Lexical Entry. Form consists of a text 
string that represents the word. Sense specifies or 
identifies the meaning and context of the related 
form. Therefore, the Lexical Entry manages the 
relationship between sets of related forms and 
their senses. If there is more than one orthogra-
                                                 
2 www.omg.org 
2
phy for the word form (e.g. transliteration) the 
Form class may be associated with one to many 
Representation Frames, each of which contains a 
specific orthography and one to many data cate-
gories that describe the attributes of that orthog-
raphy. 
The core package classes are linked by the re-
lations as defined in the following UML class 
diagram: 
 
Representation Frame
Lexicon Information
Form Sense
Entry Relation
Sense Relation
Lexical Entry
Database
Lexicon
0..* 0..*
0..*1
0..* 0..*
0..*1
1
0..*
1
1
1
0..*
1
1..*
1
0..*
1
1..*
1..*
1
 
 
Form class can be sub-classed into Lemmatised 
Form and Inflected Form class as follows: 
 
Lemmatised Form Inflected Form
Form
 
 
A subset of the core package classes are ex-
tended to cover different kinds of linguistic data. 
All extensions conform to the LMF core package 
and cannot be used to represent lexical data in-
dependently of the core package. From the point 
of view of UML, an extension is a UML pack-
age. Current extensions for NLP dictionaries are: 
NLP Morphology3, NLP inflectional paradigm, 
NLP Multiword Expression pattern, NLP Syntax, 
NLP Semantic and Multilingual notations, which 
is the focus of this paper. 
6 NLP Multilingual Extension 
The NLP multilingual notation extension is 
dedicated to the description of the mapping be-
tween two or more languages in a LMF database. 
The model is based on the notion of Axis that 
links Senses, Syntactic Behavior and examples 
pertaining to different languages. "Axis" is a 
                                                 
3 Morphology, Syntax and Semantic packages are 
described in [Francopoulo]. 
3
term taken from the Papillon4 project [S?rasset 
2001] 5 . Axis can be organized at the lexicon 
manager convenience in order to link directly or 
indirectly objects of different languages.  
 
6.1 Considerations for standardizing multi-
lingual data  
The simplest configuration of multilingual 
data is a bilingual lexicon where a single link is 
used to represent the translation of a given 
form/sense pair from one language into another. 
But a survey of actual practices clearly reveals 
other requirements that make the model more 
complex. Consequently, LMF has focused on the 
following ones: 
 
(i) Cases where the relation 1-to-1 is impos-
sible because of lexical differences among lan-
guages. An example is the case of English word 
?river? that relates to French words ?rivi?re? and 
?fleuve?, where the latter is used for specifying 
that the referent is a river that flows into the sea. 
The bilingual lexicon should specify how these 
units relate. 
 
(ii) The bilingual lexicon approach should 
be optimized to allow the easiest management of 
large databases for real multilingual scenarios. In 
order to reduce the explosion of links in a multi-
bilingual scenario, translation equivalence can be 
managed through an intermediate "Axis". This 
object can be shared in order to contain the num-
ber of links in manageable proportions. 
 
(iii) The model should cover both transfer 
and pivot approaches to translation, taking also 
into account hybrid approaches. In LMF, the 
pivot approach is implemented by a ?Sense 
Axis?. The transfer approach is implemented by 
a ?Transfer Axis?. 
 
(iv) A situation that is not very easy to deal 
with is how to represent translations to languages 
that are similar or variants. The problem arises, 
for instance, when the task is to represent transla-
tions from English to both European Portuguese 
and Brazilian Portuguese. It is difficult to con-
                                                 
4 www.papillon-dictionary.org  
5 To be more precise, Papillon uses the term "axie" 
from "axis" and "lexie". In the beginning of the LMF 
project, we used the term "axie" but after some bad 
comments about using a non-English term in a stan-
dard, we decided to use the term "axis". 
sider them as two separate languages. In fact, one 
is a variant of the other. The differences are mi-
nor: a certain number of words are different and 
some limited phenomena in syntax are different. 
Instead of managing two distinct copies, it is 
more effective to manage one lexicon with some 
objects that are marked with a dialectal attribute. 
Concerning the translation from English to Por-
tuguese: a limited number of specific Axis in-
stances record this variation and the vast major-
ity of Axis instances is shared. 
 
(v) The model should allow for representing 
the information that restricts or conditions the 
translations. The representation of tests that 
combine logical operations upon syntactic and 
semantic features must be covered. 
6.2 Structure 
The model is based on the notion of Axis that 
link Senses, Syntactic Behavior and examples 
pertaining to different languages. Axis can be 
organized at the lexicon manager convenience in 
order to link directly or indirectly objects of dif-
ferent languages. A direct link is implemented by 
a single axis. An indirect link is implemented by 
several axis and one or several relations. 
The model is based on three main classes: 
Sense Axis, Transfer Axis, Example Axis. 
6.3 Sense Axis 
Sense Axis is used to link closely related 
senses in different languages, under the same 
assumptions of the interlingual pivot approach, 
and, optionally, it can also be used to refer to one 
or several external knowledge representation sys-
tems.  
The use of the Sense Axis facilitates the repre-
sentation of the translation of words that do not 
necessarily have the same valence or morpho-
logical form in one language than in another. For 
example, in a language, we can have a single 
word that will be translated by a compound word 
into another language: English ?wheelchair? to 
Spanish ?silla de ruedas?. Sense Axis may have 
the following attributes: a label, the name of an 
external descriptive system, a reference to a spe-
cific node inside an external description. 
6.4 Sense Axis Relation 
Sense Axis Relation permits to describe the 
linking between two different Sense Axis in-
stances. The element may have attributes like 
label, view, etc. 
4
6.6 Transfer Axis Relation 
Transfer Axis Relation links two Transfer Axis 
instances. The element may have attributes like: 
label, variation. 
The label enables the coding of simple inter-
lingual relations like the specialization of 
?fleuve? compared to ?rivi?re? and ?river?. It is 
not, however, the goal of this strategy to code a 
complex system for knowledge representation, 
which ideally should be structured as a complete 
coherent system designed specifically for that 
purpose. 
6.7 Source Test and Target Test 
Source Test permits to express a condition on 
the translation on the source language side while 
Target Test does it on the target language side. 
Both elements may have attributes like: text and 
comment. 
6.5 Transfer Axis 
Transfer Axis is designed to represent multi-
lingual transfer approach. Here, linkage refers to 
information contained in syntax. For example, 
this approach enables the representation of syn-
tactic actants involving inversion, such as (1): 
6.8 Example Axis  
Example Axis supplies documentation for 
sample translations. The purpose is not to record 
large scale multilingual corpora. The goal is to 
link a Lexical Entry with a typical example of 
translation. The element may have attributes like: 
comment, source. 
 
(1) fra:?elle me manque? => 
eng:?I miss her? 
 
Due to the fact that a lexical entry can be a 
support verb, it is possible to represent transla-
tions that start from a plain verb to a support verb 
like (2) that means "Mary dreams": 
6.9 Class Model Diagram 
The UML class model is an UML package. The 
diagram for multilingual notations is as follows:  
(2)  fra:?Marie r?ve? =>  
  jpn:"Marie wa yume wo miru"  
Transfer Axis Relation
Sense Axis Relation
Syntactic Behavior
SenseExample
Transfer Axis
Example Axis
Source Test
Sense Axis
Target Test
SynSet
Sense
0..*
0..*
0..*
0..*
1
0..*
0..* 0..*
0..*
0..*
1
0..*
1
0..1
1
0..*
0..1
1
1
0..*
1
0..*
1
0..*
5
7 Three examples 
7.1 First example 
The first example is about the interlingual ap-
proach with two axis instances to represent a 
near match between "fleuve" in French and 
"river" in English. In the diagram, French is lo-
cated on the left side and English on the right 
side. The axis on the top is not linked directly to 
any English sense because this notion does not 
exist in English.  
: Sense Axis Relation
comment = flows into the sea
label = more precise
: Sense
label = eng:riverlabel = fra:rivi?re
: Sense
: Sense
label = fra:fleuve
: Sense Axis
: Sense Axis
 
 
7.2 Second example 
Let's see now an example about the transfer 
approach about slight variations between vari-
ants. The example is about English on one side 
and European Portuguese and Brazilian on the 
other side. Due to the fact that these two last 
variants have a very similar syntax, but with 
some local exceptions, the goal is to avoid a full 
and dummy duplication. For instance, the nomi-
native forms of the third person clitics are largely 
preferred in Brazilian rather than the oblique 
form as in European Portuguese. The transfer 
axis relations hold a label to distinguish which 
axis to use depending on the target object. 
 
: Transfer Axis Relation
label = European Portuguese
: Transfer Axis Relation
label = Brazilian
: Syntactic Behavior
label = let me see
: Syntactic Behavior
label = Deixa eu ver
: Syntactic Behavior
label = Deixa-me ver
: Transfer Axis
: Transfer Axis
: Transfer Axis
 
7.3 Third example 
A third example shows how to use the Trans-
fer Axis relation to relate different information in 
a multilingual transfer lexicon. It represents the 
translation of the English ?develop? into Italian 
and Spanish. Recall that the more general sense 
links ?eng:develop? and ?esp:desarrollar?. Both, 
Spanish and Italian, have restrictions that should 
6
be tested in the source language: if the second 
argument of the construction refers to certain 
elements (picture, mentalCreation, building) it 
should be translated into specific verbs.  
 
: Source Test
semanticRestriction = eng:mentalCreation
syntacticArgument = 2
: Source Test
semanticRestriction = eng:picture
syntacticArgument = 2
: Source Test
semanticRestriction = eng:building
syntacticArgument = 2
: Transfer Axis Relation
: Transfer Axis Relation
: Transfer Axis Relation
: Syntactic Behavior
label = esp:revelar
: Syntactic Behavior
label = ita:sviluppare
: Syntactic Behavior
label = ita:costruire
: Syntactic Behavior
label = eng:develop
: Syntactic Behavior
label = esp:construir
: Syntactic Behavior
label = esp:desarrollar
: Transfer Axis
: Transfer Axis
: Transfer Axis
: Transfer Axis
 
8 LMF in XML  
During the last three years, the ISO group fo-
cused on the UML specification. In the last ver-
sion of the LMF document [LMF 2006] a DTD 
has been provided as an informative annex. The 
following conventions are adopted: 
? each UML attribute is transcoded as a 
DC (for Data Category) element 
? each UML class is transcoded as an 
XML element 
? UML aggregations are transcoded as 
content inclusion 
? UML shared associations (i.e. associa-
tions that are not aggregations) are 
transcoded as IDREF(S) 
The first example (i.e. "river") can be represented 
with the following XML tags: 
 
 
<Database> 
<!?   French section ? 
<Lexicon> 
<LexiconInformation 
<DC att="name" val=?French Extract?/> 
<DC att="language" val="fra"/> 
</LexiconInformation> 
<LexicalEntry > 
<DC att="partOfSpeech" val=?noun?/> 
<LemmatisedForm> 
<DC att="writtenForm" val=?fleuve?/> 
</LemmatisedForm> 
<Sense id=?fra.fleuve1?> 
 <SemanticDefinition> 
                  <DC att="text" 
val=?Grande rivi?re lorsqu'elle aboutit ? la mer?/> 
<DC att="source" val=?Le Petit Robert 2003?/> 
</SemanticDefinition> 
</Sense> 
</LexicalEntry> 
<LexicalEntry> 
<DC att="partOfSpeech" val=?noun?/> 
<LemmatisedForm> 
  <DC att="writtenForm" val=?rivi?re?/> 
</LemmatisedForm> 
<Sense id=?fra.riviere1?> 
 <SemanticDefinition> 
<DC att="text"  
val=?Cours d'eau naturel de moyenne importance?/> 
<DC att="source" val=?Le Petit Robert 2003?/> 
</SemanticDefinition> 
</Sense> 
</LexicalEntry> 
</Lexicon> 
<!?                                                 Multilingual section ? 
<SenseAxis id=?A1? senses="fra.fleuve1"> 
7
<SenseAxisRelation targets="A2"> 
 <DC att="comment" val="flows into the sea"/> 
 <DC att="label" val="more precise"/> 
</SenseAxisRelation> 
</SenseAxis> 
<SenseAxis id=?A2? senses="fra.riviere1 eng.river1"/> 
<!?                                                English section ? 
<Lexicon> 
<LexiconInformation> 
<DC att="name" val=?English Extract?/> 
<DC att="language" val="eng"/> 
</LexiconInformation> 
<LexicalEntry> 
<DC att="partOfSpeech" val=?noun?/> 
<LemmatisedForm> 
<DC att="writtenForm" val=?river?/> 
</LemmatisedForm> 
<Sense id=?eng.river1?> 
 <SemanticDefinition> 
<DC att="text" 
val=?A natural and continuous flow of water in a long 
line across a country into the sea?/> 
<DC att="source" val=?Longman DCE 2005?/> 
</SemanticDefinition> 
</Sense> 
</LexicalEntry> 
</Lexicon> 
</Database> 
 
 
9 Comparison 
A serious comparison with previously existing 
models is not possible in this current paper due 
to the lack of space. We advice the interested 
colleague to consult the technical report "Ex-
tended examples of lexicons using LMF" located 
at:  "http://lirics.loria.fr" in the document area. 
The report explains how to use LMF in order to 
represent OLIF-2, Parole/Clips, LC-Star, Word-
Net, FrameNet and BD?f. 
10 Conclusion 
In this paper we presented the results of the 
ongoing research activity of the LMF ISO stan-
dard. The design of a common and standardized 
framework for multilingual lexical databases will 
contribute to the optimization of the use of lexi-
cal resources, specially their reusability for dif-
ferent applications and tasks. Interoperability is 
the condition of a effective deployment of usable 
lexical resources. 
In order to reach a consensus, the work done 
has paid attention to the similarities and differ-
ences of existing lexicons and the models behind 
them. 
Acknowledgements 
The work presented here is partially funded by 
the EU eContent-22236 LIRICS project 6 , par-
tially by the French TECHNOLANGUE 7 + 
OUTILEX8 programs. 
References 
Antoni-Lay M-H., Francopoulo G., Zaysser L. 1994 
A generic model for reusable lexicons: the 
GENELEX project. Literary and linguistic comput-
ing 9(1) 47-54 
Bertagna F., Lenci A., Monachini M., Calzolari N. 
2004 Content interoperability of lexical resources, 
open issues and MILE perspectives LREC Lisbon 
Francopoulo G., George M., Calzolari N., Monachini 
M., Bel N., Pet M., Soria C. 2006 Lexical Markup 
Framework (LMF) LREC Genoa. 
LMF 2006 Lexical Markup Framework ISO-
CD24613-revision-9, ISO Geneva 
Rumbaugh J., Jacobson I.,Booch G. 2004 The unified 
modeling language reference manual, second edi-
tion, Addison Wesley 
S?rasset G., Mangeot-Lerebours M. 2001 Papillon 
Lexical Database project: monolingual dictionaries 
& interlingual links NLPRS Tokyo 
                                                 
6 http://lirics.loria.fr 
7 www.technolangue.net 
8 www.at-lci.com/outilex/outilex.html 
8
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Agent-based Cross-lingual Interoperability of Distributed    
Lexical Resources 
Claudia Soria* Maurizio Tesconi? Andrea Marchetti?
Francesca Bertagna* Monica Monachini*
Chu-Ren Huang?    Nicoletta Calzolari*
*CNR-ILC and ?CNR-IIT 
Via Moruzzi 1, 56024 Pisa 
Italy 
{firstname.lastname@ilc.cnr.it} 
{firstname.lastname@iit.cnr.it} 
?Academia Sinica  
Nankang, Taipei  
Taiwan 
churen@gate.sinica.edu.tw 
 
  
 
Abstract 
In this paper we present an application 
fostering the integration and interopera-
bility of computational lexicons, focusing 
on the particular case of mutual linking 
and cross-lingual enrichment of two wor-
dnets, the ItalWordNet and Sinica BOW 
lexicons. This is intended as a case-study 
investigating the needs and requirements 
of semi-automatic integration and inter-
operability of lexical resources. 
1 Introduction 
In this paper we present an application fostering 
the integration and interoperability of computa-
tional lexicons, focusing on the particular case of 
mutual linking and cross-lingual enrichment of 
two wordnets. The development of this applica-
tion is intended as a case-study and a test-bed for 
trying out needs and requirements posed by the 
challenge of semi-automatic integration and en-
richment of practical, large-scale multilingual 
lexicons for use in computer applications. While 
a number of lexicons already exist, few of them 
are practically useful, either since they are not 
sufficiently broad or because they don?t cover 
the necessary level of detailed information. 
Moreover, multilingual language resources are 
not as widely available and are very costly to 
construct: the work process for manual develop-
ment of new lexical resources or for tailoring 
existing ones is too expensive in terms of effort 
and time to be practically attractive.  
The need of ever growing lexical resources for 
effective multilingual content processing has 
urged the language resource community to call 
for a radical change in the perspective of lan-
guage resource creation and maintenance and the 
design of a ?new generation? of LRs: from static, 
closed and locally developed resources to shared 
and distributed language services, based on open 
content interoperability standards. This has often 
been called a ?change in paradigm? (in the sense 
of Kuhn, see Calzolari and Soria, 2005; Calzolari 
2006). Leaving aside the tantalizing task of 
building on-site resources, the new paradigm 
depicts a scenario where lexical resources are 
cooperatively built as the result of controlled co-
operation of different agents, adopting the para-
digm of accumulation of knowledge so success-
ful in more mature disciplines, such as biology 
and physics (Calzolari, 2006).  
According to this view (or, better, this vision), 
different lexical resources reside over distributed 
places and can not only be accessed but choreo-
graphed by agents presiding the actions that can 
be executed over them. This implies the ability to 
build on each other achievements, to merge re-
sults, and to have them accessible to various sys-
tems and applications. 
At the same time, there is another argument in 
favor of distributed lexical resources: language 
resources, lexicons included, are inherently dis-
tributed because of the diversity of languages 
distributed over the world. It is not only natural 
that language resources to be developed and 
maintained in their native environment. Since 
language evolves and changes over time, it is not 
possible to describe the current state of the lan-
17
guage away from where the language is spoken. 
Lastly, the vast range of diversity of languages 
also makes it impossible to have one single uni-
versal centralized resource, or even a centralized 
repository of resources. 
Although the paradigm of distributed and in-
teroperable lexical resources has largely been 
discussed and invoked, very little has been made 
in comparison for the development of new meth-
ods and techniques for its practical realization. 
Some initial steps are made to design frame-
works enabling inter-lexica access, search, inte-
gration and operability. An example is the Lexus 
tool (Kemps-Snijders et al, 2006), based on the 
Lexical Markup Framework (Romary et al, 
2006), that goes in the direction of managing the 
exchange of data among large-scale lexical re-
sources. A similar tool, but more tailored to the 
collaborative creation of lexicons for endangered 
language, is SHAWEL (Gulrajani and Harrison, 
2002). However, the general impression is that 
little has been made towards the development of 
new methods and techniques for attaining a con-
crete interoperability among lexical resources. 
Admittedly, this is a long-term scenario requiring 
the contribution of many different actors and ini-
tiatives (among which we only mention stan-
dardisation, distribution and international coop-
eration).  
Nevertheless, the intent of our project is to 
contribute to fill in this gap, by exploring in a 
controlled way the requirement and implications 
posed by new generation multilingual lexical 
resources. The paper is organized as follows: 
section 2 describes the general architectural de-
sign of our project; section 3 describes the mod-
ule taking care of cross-lingual integration of 
lexical resources, by also presenting a case-study 
involving an Italian and Chinese lexicons. Fi-
nally, section 4 presents our considerations and 
lessons learned on the basis of this exploratory 
testing. 
2 An Architecture for Integrating Lexi-
cal Resources 
 LeXFlow (Soria et al, 2006) was developed 
having in mind the long-term goal of lexical re-
source interoperability. In a sense, LeXFlow is 
intended as a proof of concept attempting to 
make the vision of an infrastructure for access 
and sharing of linguistic resources more tangible. 
LeXFlow is an adaptation to computational 
lexicons of XFlow, a cooperative web applica-
tion for the management of document workflows 
(DW, Marchetti et al, 2005). A DW can be seen 
as a process of cooperative authoring where a 
document can be the goal of the process or just a 
side effect of the cooperation. Through a DW, a 
document life-cycle is tracked and supervised, 
continually providing control over the actions 
leading to document compilation. In this envi-
ronment a document travels among agents who 
essentially carry out the pipeline receive-process-
send activity.  
There are two types of agents: external agents 
are human or software actors performing activi-
ties dependent from the particular Document 
Workflow Type; internal agents are software 
actors providing general-purpose activities useful 
for many DWTs and, for this reason, imple-
mented directly into the system. Internal agents 
perform general functionalities such as creat-
ing/converting a document belonging to a par-
ticular DW, populating it with some initial data, 
duplicating a document to be sent to multiple 
agents, splitting a document and sending portions 
of information to different agents, merging du-
plicated documents coming from multiple agents, 
aggregating fragments, and finally terminating 
operations over the document. External agents 
basically execute some processing using the 
document content and possibly other data; for 
instance, accessing an external database or 
launching an application.  
LeXFlow was born by tailoring XFlow to 
management of lexical entries; in doing so, we 
have assumed that each lexical entry can be 
modelled as a document instance, whose behav-
iour can be formally specified by means of a 
lexical workflow type (LWT). A LWT describes 
the life-cycle of a lexical entry, the agents al-
lowed to act over it, the actions to be performed 
by the agents, and the order in which the actions 
are to be executed. Embracing the view of coop-
erative workflows, agents can have different 
rights or views over the same entry: this nicely 
suits the needs of lexicographic work, where we 
can define different roles (such as encoder, anno-
tator, validator) that can be played by either hu-
man or software agents. Other software modules 
can be inserted in the flow, such as an automatic 
acquirer of information from corpora or from the 
web. Moreover, deriving from a tool designed 
for the cooperation of agents, LeXFlow allows to 
manage workflows where the different agents 
can reside over distributed places.  
LeXFlow thus inherits from XFlow the gen-
eral design and architecture, and can be consid-
ered as a specialized version of it through design 
18
of specific Lexical Workflow Types and plug-in 
of dedicated external software agents. In the next 
section we briefly illustrate a particular Lexical 
Workflow Type and the external software agents 
developed for the purpose of integrating different 
lexicons belonging to the same language. Since it 
allows the independent and coordinated sharing 
of actions over portions of lexicons, LeXFlow 
naturally lends itself as a tool for the manage-
ment of distributed lexical resources. 
Due to its versatility, LeXFlow is both a gen-
eral framework where ideas on automatic lexical 
resource integration can be tested and an infra-
structure for proving new methods for coopera-
tion among lexicon experts. 
2.1 Using LeXFlow for Lexicon Enrichment 
In previous work (Soria et al, 2006),  the LeX-
Flow framework has been tested for integration 
of lexicons with differently conceived lexical 
architectures and diverging formats. It was 
shown how interoperability is possible between 
two Italian lexicons from the SIMPLE and 
WordNet families, respectively, namely the 
SIMPLE/CLIPS (Ruimy et al, 2003) and Ital-
WordNet (Roventini et al, 2003) lexicons.  
In particular, a Lexical Workflow Type was 
designed where the two different monolingual 
semantic lexicons interact by reciprocally enrich-
ing themselves and moreover integrate informa-
tion coming from corpora. This LWT, called 
?lexicon augmentation?, explicitly addresses dy-
namic augmentation of semantic lexicons. In this 
scenario, an entry of a lexicon A becomes en-
riched via basically two steps. First, by virtue of 
being mapped onto a corresponding entry be-
longing to a lexicon B, the entryA inherits the 
semantic relations available in the mapped en-
tryB. Second, by resorting to an automatic appli-
cation that acquires information about semantic 
relations from corpora, the acquired relations are 
integrated into the entry and proposed to the hu-
man encoder. 
B
An overall picture of the flow is shown in 
Figure 1, illustrating the different agents partici-
pating in the flow. Rectangles represent human 
actors over the entries, while the other figures 
symbolize software agents: ovals are internal 
agents and octagons external ones. The two ex-
ternal agents involved in this flow are the ?rela-
tion calculator? and the ?corpora extractor?. The 
first is responsible for the mapping between the 
sets of semantic relations used by the different 
lexicons. The ?corpora extractor? module in-
vokes an application that acquires information 
about part-of relations by identifying syntactic 
constructions in a vast Italian corpus. It then 
takes care of creating the appropriate candidate 
semantic relations for each lemma that is pro-
posed by the application. 
Figure 1. Lexicons Augmentation Workflow 
Type. 
A prototype of LeXFlow has been imple-
mented with an extensive use of XML technolo-
gies (XML Schema, XSLT, XPath, XForms, 
SVG) and open-source tools (Cocoon, Tomcat, 
mySQL). It is a web-based application where 
human agents interact with the system through 
an XForms browser that displays the document 
to process as a web form whereas software 
agents interact with the system via web services. 
3 Multilingual WN Service 
In the Section above we have illustrated the gen-
eral architecture of LeXFlow and showed how a 
Lexical Workflow Type can be implemented in 
order to enrich already existing lexicons belong-
ing to the same language but realizing different 
models of lexicon encoding. In this section we 
move to a cross-lingual perspective of lexicon 
integration. We present a module that similarly 
addresses the issue of lexicon augmentation or 
enrichment focusing on mutual enrichment of 
two wordnets in different languages and residing 
at different sites. 
This module, named ?multilingual WN Ser-
vice? is responsible for the automatic cross-
lingual fertilization of lexicons having a Word-
19
Net-like structure. Put it very simply, the idea 
behind this module is that a monolingual word-
net can be enriched by accessing the semantic 
information encoded in corresponding entries of 
other monolingual wordnets.  
Since each entry in the monolingual lexicons 
is linked to the Interlingual Index (ILI, cf. Sec-
tion 3.1), a synset of a WN(A) is indirectly 
linked to another synset in another WN(B). On 
the basis of this correspondence, a synset(A) can 
be enriched by importing the relations that the 
corresponding synset(B) holds with other syn-
sets(B), and vice-versa. Moreover, the enrich-
ment of WN(A) will not only import the relations 
found in WN(B), but it will also propose target 
synsets in the language(A) on the basis of those 
found in language(B). 
The various WN lexicons reside over distrib-
uted servers and can be queried through web ser-
vice interfaces. The overall architecture for mul-
tilingual wordnet service is depicted in Figure 2. 
 
 
Figure 2. Multilingual Wordnet Service Archi-
tecture. 
 
Put in the framework of the general LeXFlow 
architecture, the Multilingual wordnet Service 
can be seen as an additional external software 
agent that can be added to the augmentation 
workflow or included in other types of lexical 
flows. For instance, it can be used not only to 
enrich a monolingual lexicon but to bootstrap a 
bilingual lexicon. 
3.1 Linking Lexicons through the ILI  
The entire mechanism of the Multilingual WN 
Service is based on the exploitation of Interlin-
gual Index (Peters et al, 1998), an unstructured 
version of WordNet used in EuroWordNet 
(Vossen et al, 1998) to link wordnets of different 
languages; each synset in the language-specific 
wordnet is linked to at least one record of the ILI 
by means of a set of equivalence relations 
(among which the most important is the 
EQ_SYNONYM, that expresses a total, perfect 
equivalence between two synsets).  
Figure 6 describes the schema of a WN lexical 
entry. Under the root ?synset? we find both in-
ternal relations (?synset relations?) and ILI Rela-
tions, which link to ILI synsets. 
Figure 3 shows the role played by the ILI as 
set of pivot nodes allowing the linkage between 
concepts belonging to different wordnets.  
 
 
Figure 3. Interlingual Linking of Language-
specific Synsets. 
 
In the Multilingual WN Service, only equiva-
lence relations of type EQ_SYNONYM and 
EQ_NEAR_SYNONYM have been taken into ac-
count, being them the ones used to represent a 
translation of concepts and also because they are 
the most exploited (for example, in IWN, they 
cover about the 60% of the encoded equivalence 
relations). The EQ_SYNONYM relation is used to 
realize the one-to-one mapping between the lan-
guage-specific synset and the ILI, while multiple 
EQ_NEAR_SYNONYM relations (because of their 
nature) might be encoded to link a single lan-
guage-specific synset to more than one ILI re-
cord. In Figure 4 we represented the possible 
relevant combinations of equivalence relations 
that can realize the mapping between synsets 
belonging to two languages. In all the four cases, 
a synset ?a? is linked via the ILI record to a syn-
set ?b? but a specific procedure has been fore-
seen in order to calculate different ?plausibility 
scores? to each situation. The procedure relies on 
different rates assigned to the two equivalence 
relations (rate ?1? to EQ_NEAR_SYNONYM rela-
tion and rate ?0? to the EQ_SYNONYM). In this 
way we can distinguish the four cases by assign-
ing respectively a weight of ?0?, ?1?, ?1? and 
?2?. 
20
  
Figure 4. Possible Combinations of Relations 
between two Lexicons A and B and the ILI. 
 
The ILI is a quite powerful yet simple method 
to link concepts across the many lexicons be-
longing to the WordNet-family. Unfortunately, 
no version of the ILI can be considered a stan-
dard and often the various lexicons exploit dif-
ferent version of WordNet as ILI 1 . This is a 
problem that is handled at web-service level, by 
incorporating the conversion tables provided by 
(Daud? et al, 2001). In this way, the use of dif-
ferent versions of WN does not have to be taken 
into consideration by the user who accesses the 
system but it is something that is resolved by the 
system itself2. This is why the version of the ILI 
is a parameter of the query to web service (see 
Section below). 
3.2 Description of the Procedure 
On the basis of ILI linking, a synset can be en-
riched by importing the relations contained in the 
corresponding synsets belonging to another 
wordnet. 
In the procedure adopted, the enrichment is 
performed on a synset-by-synset basis. In other 
words, a certain synset is selected from a word-
net resource, say WN(A). The cross-lingual mod-
ule identifies the corresponding ILI synset, on 
the basis of the information encoded in the syn-
set. It then sends a query to the WN(B) web ser-
vice providing the ID of ILI synset together with 
the ILI version of the starting WN. The WN(B) 
web service returns the synset(s) corresponding 
to the WN(A) synset, together with reliability 
scores. If WN(B) is based on a different ILI ver-
sion, it can carry out the mapping between ILI 
versions (for instance by querying the ILI map-
ping web service). The cross-lingual module then 
analyzes the synset relations encoded in the 
                                                 
1 For example, the Chinese and the Italian wordnets consid-
ered as our case-study use respectively versions 1.6 and 1.5. 
2 It should be noted, however, that the conversion between 
different WN versions could not be accurate so the mapping 
is always proposed with a probability score.
WN(B) synset and for each of them creates a 
new synset relation for the WN(A) synset. 
If the queried wordnets do not use the same set 
of synset relations, the module must take care of 
the mapping between different relation sets. In  
our case-study no mapping was needed, since the 
two sets were completely equivalent.   
Each new relation is obtained by substituting 
the target WN(B)  synset  with the corresponding 
synset WN(A), which again is found by querying 
back the WN(A) web service (all these steps 
through the ILI). The procedure is formally de-
fined by the following formula: 
 
 
 
 
 
Figure 5. Finding New Relations. 
 
Every local wordnet has to provide a web ser-
vice API  with the following methods: 
 
1. GetWeightedSynsetsByIli(ILIid, ILIversion) 
2. GetSynsetById(sysnsetID) 
3. GetSynsetsByLemma(lemma) 
 
21
The returned synsets of each method must be 
formatted in XML following the schema de-
picted in Figure 6: 
 
Figure 6. Schema of Wordnet Synsets Returned 
by WN Web Services. 
 
The scores returned by the method ?Get-
WeightedSynsetsByIli? are used by our module 
to calculate the reliability rating for each new 
proposed relation. 
3.3 A Case Study: Cross-fertilization be-
tween Italian and Chinese Wordnets. 
We explore this idea with a case-study involving 
the ItalianWordNet (Roventini et al, 2003) and 
the Academia Sinica Bilingual Ontological 
Wordnet (Sinica BOW, Huang et al, 2004).  
The BOW integrates three resources: Word-
Net, English-Chinese Translation Equivalents 
Database (ECTED), and SUMO (Suggested Up-
per Merged Ontology). With the integration of 
these three key resources, Sinica BOW functions 
both as an English-Chinese bilingual wordnet 
and a bilingual lexical access to SUMO. Sinica 
Bow currently has two bilingual versions, corre-
sponding to WordNet 1.6. and 1.7. Based on 
these bootstrapped versions, a Chinese Wordnet 
(CWN, Huang et al 2005) is under construction 
with handcrafted senses and lexical semantic re-
lations. For the current experiment, we have used 
the version linking to WordNet 1.6. 
ItalWordNet was realized as an extension of 
the Italian component of EuroWordNet. It com-
prises a general component consisting of about 
50,000 synsets and terminological wordnets 
linked to the generic wordnet by means of a spe-
cific set of relations. Each synset of ItalWordNet 
is linked to the Interlingual-Index (ILI). 
The two lexicons refer to different versions of 
the ILI (1.5 for IWN and 1.6 for BOW), thus 
making it necessary to provide a mapping be-
tween the two versions. On the other hand, no 
mapping is necessary for the set of synset rela-
tions used, since both of them adopt the same set. 
For the purposes of evaluating the cross-
lingual module, we have developed two web-
services for managing a subset of the two re-
sources.  
The following Figure shows a very simple ex-
ample where our procedure discovers and pro-
poses a new meronymy relation for the Italian 
synset {passaggio,strada,via}. This synset is 
equivalent to the ILI ?road,route? that is ILI-
connected with BOW synset ???,? ,?? (da-
o_lu, dao, lu) (Figure 7, A) . The Chinese synset 
has a meronymy relation with the synset ???
??? (wan) (B). This last  synset is equivalent 
to the ILI ?bend, crook, turn? that is ILI-
connected with Italian WordNet synset ?curva-
tura, svolta, curva? (C). Therefore the procedure 
will propose a new candidate meronymy relation 
between the two Italian WordNet synsets (D). 
 
 
Figure 7. Example of a New Proposed Mero-
nymy Relation for Italian. 
3.4 Considerations and Lessons Learned 
Given the diversity of the languages for which 
wordnets exist, we note that it is difficult to im-
plement an operational standard across all typo-
logically different languages. Work on enriching 
and merging multilingual resources presupposes 
that the resources involved are all encoded with 
the same standard. However, even with the best 
efforts of the NLP community, there are only a 
small number of language resources encoded in 
any given standard. In the current work, we pre-
suppose a de-facto standard, i.e. a shared and 
conventionalized architecture, the WordNet one. 
Since the WordNet framework is both conven-
tionalized and widely followed, our system is 
22
able to rely on it without resorting to a more sub-
stantial and comprehensive standard. In the case, 
for instance, of integration of lexicons with dif-
ferent underlying linguistic models, the availabil-
ity of the MILE (Calzolari et al, 2003) was an 
essential prerequisite of our work. Nevertheless, 
even from the perspective of the same model, a 
certain degree of standardization is required, at 
least at the format level. 
From a more general point of view, and even 
from the perspective of a limited experiment 
such as the one described in this paper, we must 
note that the realization of the new vision of dis-
tributed and interoperable language resources is 
strictly intertwined with at least two prerequi-
sites. On the one side, the language resources 
need to be available over the web; on the other, 
the language resource community will have to 
reconsider current distribution policies, and to 
investigate the possibility of developing an 
?Open Source? concept for LRs. 
4 Conclusion 
Our proposal to make distributed wordnets inter-
operable has the following applications in proc-
essing of lexical resources: 
 
? Enriching existing resources: informa-
tion is often not complete in any given 
wordnet: by making two wordnets inter-
operable, we can bootstrap semantic rela-
tions and other information from other 
wordnets. 
? Creation of new resources: multilingual 
lexicons can be bootstrapped by linking 
different language wordnets through ILI. 
? Validation of existing resources: seman-
tic relation information and other synset 
assignments can be validated when it is re-
inforced by data from a different wordnet. 
In particular, our work can be proposed as a 
prototype of a web application that would sup-
port the Global WordNet Grid initiative 
(www.globalwordnet.org/gwa/gwa_grid.htm).  
Any multilingual process, such as cross-
lingual information retrieval, must involve both 
resources and tools in a specific language and 
language pairs. For instance, a multilingual query 
given in Italian but intended for querying Eng-
lish, Chinese, French, German, and Russian 
texts, can be send to five different nodes on the 
Grid for query expansion, as well as performing 
the query itself. In this way, language specific 
query techniques can be applied in parallel to 
achieve best results that can be integrated in the 
future. As multilingualism clearly becomes one 
of the major challenges of the future of web-
based knowledge engineering, WordNet emerges 
as one leading candidate for a shared platform 
for representing a lexical knowledge model for 
different languages of the world. This is true 
even if it has to be recognized that the wordnet 
model is lacking in some important semantic in-
formation (like, for instance, a way to represent 
the semantic predicate). However, such knowl-
edge and resources are distributed. In order to 
create a shared multi-lingual knowledge base for 
cross-lingual processing based on these distrib-
uted resources, an initiative to create a grid-like 
structure has been recently proposed and pro-
moted by the Global WordNet Association, but 
until now has remained a wishful thinking. The 
success of this initiative will depend on whether 
there will be tools to access and manipulate the 
rich internal semantic structure of distributed 
multi-lingual WordNets. We believe that our 
work on LeXFlow offers such a tool to provide 
inter-operable web-services to access distributed 
multilingual WordNets on the grid. 
This allows us to exploit in a cross-lingual 
framework the wealth of monolingual lexical 
information built in the last decade. 
5 References 
Nicoletta Calzolari, Francesca Bertagna, Alessandro 
Lenci and Monica Monachini, editors. 2003. Stan-
dards and Best Practice for Multilingual Computa-
tional Lexicons. MILE (the Multilingual ISLE 
Lexical Entry). ISLE CLWG Deliverable D2.2 & 
3.2. Pisa. 
Nicoletta Calzolari and Claudia Soria. 2005. A New 
Paradigm for an Open Distributed Language Re-
source Infrastructure: the Case of Computational 
Lexicons. In Proceedings of the AAAI Spring Sym-
posium ?Knowledge Collection from Volunteer 
Contributors (KCVC05)?, pages 110-114, Stan-
ford, CA. 
Nicoletta Calzolari. 2006. Technical and Strategic 
issues on Language Resources for a Research In-
frastructure In Proceedings of the International 
Symposium on Large-scale Knowledge Resources 
(LKR2006), pages 53-58, Tokyo, Tokyo Institute 
of Technology. 
Jordi Daud?, Lluis Padr? and German Rigau. 2001. A 
Complete WN1.5 to WN1.6 Mapping. In Proceed-
ings of NAACL Workshop "WordNet and Other 
Lexical Resources: Applications, Extensions and 
23
Customizations", pages 83-88, Pittsburg, PA, USA, 
Association for Computational Linguistics.  
Greg Gulrajani and David Harrison. 2002. SHAWEL: 
Sharable and Interactive Web-Lexicons. In Pro-
ceedings of the LREC2002 Workshop on Tools and 
Resources in Field Linguistics, pages 1-4, Las 
Palmas, Canary Islands, Spain. 
Chu-Ren Huang, Ru-Yng Chang,  and Shiang-Bin 
Lee. 2004. Sinica BOW (Bilingual Ontological 
Wordnet): Integration of Bilingual WordNet and 
SUMO. In Proceedings of LREC2004, pages 1553-
1556, Lisbon, Portugal. 
Chu-Ren Huang, Chun-Ling Chen, Cui-Xia Weng, 
Hsiang-Ping Lee, Yong-Xiang Chen and Keh-jiann 
Chen. 2005. The Sinica Sense Management Sys-
tem: Design and Implementation. Computational 
Linguistics and Chinese Language Processing. 
10(4): 417-430. 
Marc Kemps-Snijders, Mark-Jan Nederhof, and Peter 
Wittenburg. 2006. LEXUS, a web-based tool for 
manipulating lexical resources. Accepted for publi-
cation in Proceedings of LREC2006, Genoa, Italy. 
Andrea Marchetti, Maurizio Tesconi, and Salvatore 
Minutoli. 2005. XFlow: An XML-Based Docu-
ment-Centric Workflow. In Proceedings of 
WISE?05, pages 290-303, New York, NY, USA. 
Wim Peters, Piek Vossen, Pedro Diez-Orzas, and 
Geert Adriaens. 1998. Cross-linguistic Alignment 
of Wordnets with an Inter-Lingual-Index. In Nancy 
Ide, Daniel Greenstein, and Piek Vossen, editors, 
Special Issue on EuroWordNet, Computers and the 
Humanities, 32(2-3): 221-251. 
Laurent Romary, Gil Francopoulo, Monica Monachi-
ni, and Susanne Salmon-Alt 2006. Lexical Markup 
Framework (LMF): working to reach a consensual 
ISO standard on lexicons. Accepted for publication 
in Proceedings of LREC2006, Genoa, Italy. 
Adriana Roventini, Antonietta Alonge, Francesca 
Bertagna, Nicoletta Calzolari, Christian Girardi, 
Bernardo Magnini, Rita Marinelli, and Antonio 
Zampolli. 2003. ItalWordNet: Building a Large 
Semantic Database for the Automatic Treatment of 
Italian. In Antonio Zampolli, Nicoletta Calzolari, 
and Laura Cignoni, editors, Computational Lingui-
stics in Pisa, IEPI, Pisa-Roma, pages 745-791. 
Nilda Ruimy, Monica Monachini, Elisabetta Gola, 
Nicoletta Calzolari, Cristina Del Fiorentino, Marisa 
Ulivieri, and Sergio Rossi. 2003. A Computational 
Semantic Lexicon of Italian: SIMPLE. In Antonio 
Zampolli, Nicoletta Calzolari, and Laura Cignoni, 
editors, Computational Linguistics in Pisa, IEPI, 
Pisa-Roma, pages 821-864. 
Claudia Soria, Maurizio Tesconi, Francesca Bertagna, 
Nicoletta Calzolari, Andrea Marchetti, and Monica 
Monachini. 2006. Moving to Dynamic Computa-
tional Lexicons with LeXFlow. Accepted for pu-
blication in Proceedings of LREC2006, Genova, I-
taly.  
Piek Vossen. 1998. Introduction to EuroWordNet. In 
Nancy Ide, Daniel Greenstein, and Piek Vossen, 
editors, Special Issue on EuroWordNet, Computers 
and the Humanities, 32(2-3): 73-89. 
 
 
 
 
24
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
Taipei, Taiwan
shukai@ntnu.edu.tw
Maurizio Tesconi
IIT
CNR
Pisa, Italy
maurizio.tesconi@iit.cnr.it
Monica Monachini
ILC
CNR
Pisa, Italy
monica.monachini@ilc.cnr.it
Piek Vossen, Roxanne Segers
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl,roxane.segers@gmail.com
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambigua-
tion systems present new challenges. The
difficulties found by supervised systems to
adapt might change the way we assess the
strengths and weaknesses of supervised
and knowledge-based WSD systems. Un-
fortunately, all existing evaluation datasets
for specific domains are lexical-sample
corpora. This task presented all-words
datasets on the environment domain for
WSD in four languages (Chinese, Dutch,
English, Italian). 11 teams participated,
with supervised and knowledge-based sys-
tems, mainly in the English dataset. The
results show that in all languages the par-
ticipants where able to beat the most fre-
quent sense heuristic as estimated from
general corpora. The most successful ap-
proaches used some sort of supervision in
the form of hand-tagged examples from
the domain.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in previous Senseval and SemEval competitions
(Kilgarriff, 2001; Mihalcea et al, 2004; Snyder
and Palmer, 2004; Pradhan et al, 2007). Spe-
cific domains pose fresh challenges to WSD sys-
tems: the context in which the senses occur might
change, different domains involve different sense
distributions and predominant senses, some words
tend to occur in fewer senses in specific domains,
the context of the senses might change, and new
senses and terms might be involved. Both super-
vised and knowledge-based systems are affected
by these issues: while the first suffer from differ-
ent context and sense priors, the later suffer from
lack of coverage of domain-related words and in-
formation.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain. All
datasets and related information are publicly avail-
able from the task websites
1
.
This task was designed in the context of Ky-
oto (Vossen et al, 2008)
2
, an Asian-European
project that develops a community platform for
modeling knowledge and finding facts across lan-
guages and cultures. The platform operates as a
Wiki system with an ontological support that so-
cial communities can use to agree on the mean-
ing of terms in specific domains of their interest.
Kyoto focuses on the environmental domain be-
cause it poses interesting challenges for informa-
tion sharing, but the techniques and platforms are
1
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
2
http://www.kyoto-project.eu/
75
independent of the application domain.
The paper is structured as follows. We first
present the preparation of the data. Section 3 re-
views participant systems and Section 4 the re-
sults. Finally, Section 5 presents the conclusions.
2 Data preparation
The data made available to the participants in-
cluded the test set proper, and background texts.
Participants had one week to work on the test set,
but the background texts where provided months
earlier.
2.1 Test datasets
The WSD-domain comprises comparable all-
words test corpora on the environment domain.
Three texts were compiled for each language by
the European Center for Nature Conservation
3
and
Worldwide Wildlife Forum
4
. They are documents
written for a general but interested public and in-
volve specific terms from the domain. The docu-
ment content is comparable across languages. Ta-
ble 1 shows the numbers for the datasets.
Although the original plan was to annotate mul-
tiword terms, and domain terminology, due to time
constraints we focused on single-word nouns and
verbs. The test set clearly marked which were
the words to be annotated. In the case of Dutch,
we also marked components of single-word com-
pounds. The format of the test set followed that of
previous all-word exercises, which we extended to
accommodate Dutch compounds. For further de-
tails check the datasets in the task website.
The sense inventory was based on publicly
available wordnets of the respective languages
(see task website for details). The annotation pro-
cedure involved double-blind annotation by ex-
perts plus adjudication, which allowed us to also
provide Inter Annotator Agreement (IAA) figures
for the dataset. The procedure was carried out us-
ing KAFnotator tool (Tesconi et al, 2010). Due
to limitations in resources and time, the English
dataset was annotated by a single expert annota-
tor. For the rest of languages, the agreement was
very good, as reported in Table 1.
Table 1 includes the results of the random base-
line, as an indication of the polysemy in each
dataset. Average polysemy is highest for English,
and lowest for Dutch.
3
http://www.ecnc.org
4
http://www.wwf.org
Total Noun Verb IAA Random
Chinese 3989 754 450 0.96 0.321
Dutch 8157 997 635 0.90 0.328
English 5342 1032 366 n/a 0.232
Italian 8560 1340 513 0.72 0.294
Table 1: Dataset numbers, including number of
tokens, nouns and verbs to be tagged, Inter-
Annotator Agreement (IAA) and precision of ran-
dom baseline.
Documents Words
Chinese 58 455359
Dutch 98 21089
English 113 2737202
Italian 27 240158
Table 2: Size of the background data.
2.2 Background data
In addition to the test datasets proper, we also pro-
vided additional documents on related subjects,
kindly provided by ECNC and WWF. Table 2
shows the number of documents and words made
available for each language. The full list with the
urls of the documents are available from the task
website, together with the background documents.
3 Participants
Eleven participants submitted more than thirty
runs (cf. Table 3). The authors classified their runs
into supervised (S in the tables, three runs), weakly
supervised (WS, four runs), unsupervised (no runs)
and knowledge-based (KB, the rest of runs)
5
. Only
one group used hand-tagged data from the domain,
which they produced on their own. We will briefly
review each of the participant groups, ordered fol-
lowing the rank obtained for English. They all par-
ticipated on the English task, with one exception
as noted below, so we report their rank in the En-
glish task. Please refer to their respective paper in
these proceedings for more details.
CFILT: They participated with a domain-
specific knowledge-based method based on Hop-
field networks (Khapra et al, 2010). They first
identify domain-dependant words using the back-
ground texts, use a graph based on hyponyms in
WordNet, and a breadth-first search to select the
most representative synsets within domain. In ad-
dition they added manually disambiguated around
one hundred examples from the domain as seeds.
5
Note that boundaries are slippery. We show the classifi-
cations as reported by the authors.
76
English
Rank Participant System ID Type P R R nouns R verbs
1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.047
2 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.043
3 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.041
4 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.041
5 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044
- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.043
6 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.045
7 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.048
8 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.046
9 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.044
10 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.039
11 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.039
12 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.046
13 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.043
14 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.043
15 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.044
16 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.044
17 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.038
18 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.049
19 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.043
20 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.044
21 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.043
22 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.047
23 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.037
24 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.049
25 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.030
26 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.037
27 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.044
28 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.040
29 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041
- - Random baseline - 0.232 0.232 0.253 0.172
Chinese
Rank Participant System ID Type P R R nouns R verbs
- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.039
1 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.039
2 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038
- - Random baseline - 0.321 0.321 0.326 0.312
4 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.038
3 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.040
5 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031
Dutch
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.034
2 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034
- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.025
3 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033
- - Random baseline - 0.328 0.328 0.350 0.293
Italian
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.038
2 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.035
3 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037
- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035
- - Random baseline - 0.294 0.294 0.308 0.257
Table 3: Overall results for the domain WSD datasets, ordered by recall.
This is the only group using hand-tagged data
from the target domain. Their best run ranked 1st.
IIITTH: They presented a personalized PageR-
ank algorithm over a graph constructed from
WordNet similar to (Agirre and Soroa, 2009),
with two variants. In the first (IIITH1), the vertices
of the graph are initialized following the rank-
ing scores obtained from predominant senses as in
(McCarthy et al, 2007). In the second (IIITH2),
the graph is initialized with keyness values as in
77
0.3 0.35 0.4 0.45 0.5 0.55
Rel. Cliques
Rel. Sem. Trees-2
Rel. Sem. Trees
NLEL-WSD
RACAI-Lexical-Chains
NLEL-WSD-PDB
BLC20BG
Kyoto-1
UCF-WS-domain.noPropers
IIITH2-d.r.l.ppv.05
IIITH1-d.l.ppv.05
RACAI-2MFS-BOW
IIITH1-d.l.baseline.05
IIITH2-d.r.l.baseline.05
UCF-WS-domain
HIT-CIR-DMFS
UCF-WS
RACAI-MFS
Treematch-3
Kyoto-2
Treematch-2
Treematch
CFILT-3
BLC20SC
BLC20SCBG
IIITH2-d.l.ppr.05
IIITH1-d.l.ppr.05
CFILT-1
CFILT-2
MFS
Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond
to one system (denoted in axis Y) according each recall and confidence interval (axis X ). Systems are
ordered depending on their rank.
(Rayson and Garside, 2000). Some of the runs
use sense statistics from SemCor, and have been
classified as weakly supervised. They submitted a
total of six runs, with the best run ranking 3rd.
BLC20(SC/BG/SCBG): This system is super-
vised. A Support Vector Machine was trained us-
ing the usual set of features extracted from con-
text and the most frequent class of the target word.
Semantic class-based classifiers were built from
SemCor (Izquierdo et al, 2009), where the classes
were automatically obtained exploiting the struc-
tural properties of WordNet. Their best run ranked
5th.
Treematch: This system uses a knowledge-
based disambiguation method that requires a dic-
tionary and untagged text as input. A previously
developed system (Chen et al, 2009) was adapted
to handle domain specific WSD. They built a
domain-specific corpus using words mined from
relevant web sites (e.g. WWF and ECNC) as
seeds. Once parsed the corpus, the used the de-
pendency knowledge to build a nodeset that was
used for WSD. The background documents pro-
vided by the organizers were only used to test how
exhaustive the initial seeds were. Their best run
ranked 8th.
Kyoto: This system participated in all four
languages, with a free reimplementation of
the domain-specific knowledge-based method for
WSD presented in (Agirre et al, 2009). It
uses a module to construct a distributional the-
saurus, which was run on the background text, and
a disambiguation module based on Personalized
PageRank over wordnet graphs. Different Word-
Net were used as the LKB depending on the lan-
guage. Their best run ranked 10th. Note that this
team includes some of the organizers of the task.
A strict separation was kept, in order to keep the
test dataset hidden from the actual developers of
the system.
RACAI: This participant submitted three differ-
ent knowledge-based systems. In the first, they use
the mapping to domains of WordNet (version 2.0)
in order to constraint the domains of the content
words of the test text. In the second, they choose
among senses using lexical chains (Ion and Ste-
fanescu, 2009). The third system combines the
previous two. Their best system ranked 12th.
HIT-CIR: They presented a knowledge-based
system which estimates predominant sense from
raw test. The predominant senses were calculated
with the frequency information in the provided
background text, and automatically constructed
78
thesauri from bilingual parallel corpora. The sys-
tem ranked 14.
UCFWS: This knowledge-based WSD system
was based on an algorithm originally described in
(Schwartz and Gomez, 2008), in which selectors
are acquired from the Web via searching with lo-
cal context of a given word. The sense is cho-
sen based on the similarity or relatedness between
the senses of the target word and various types
of selectors. In some runs they include predom-
inant senses(McCarthy et al, 2007). The best run
ranked 13th.
NLEL-WSD(-PDB): The system used for the
participation is based on an ensemble of different
methods using fuzzy-Borda voting. A similar sys-
tem was proposed in SemEval-2007 task-7 (Bus-
caldi and Rosso, 2007). In this case, the com-
ponent method used where the following ones:
1) Most Frequent Sense from SemCor; 2) Con-
ceptual Density ; 3) Supervised Domain Relative
Entropy classifier based on WordNet Domains;
4) Supervised Bayesian classifier based on Word-
Net Domains probabilities; and 5) Unsupervised
Knownet-20 classifiers. The best run ranked 24th.
UMCC-DLSI (Relevant): The team submitted
three different runs using a knowledge-based sys-
tem. The first two runs use domain vectors and
the third is based on cliques, which measure how
much a concept is correlated to the sentence by
obtaining Relevant Semantic Trees. Their best run
ranked 27th.
(G)HR: They presented a Knowledge-based
WSD system, which make use of two heuristic
rules (Li et al, 1995). The system enriched the
Chinese WordNet by adding semantic relations for
English domain specific words (e.g. ecology, en-
vironment). When in-domain senses are not avail-
able, the system relies on the first sense in the Chi-
nese WordNet. In addition, they also use sense
definitions. They only participated in the Chinese
task, with their best system ranking 1st.
4 Results
The evaluation has been carried out using the stan-
dard Senseval/SemEval scorer scorer2 as in-
cluded in the trial dataset, which computes preci-
sion and recall. Table 3 shows the results in each
dataset. Note that the main evaluation measure is
recall (R). In addition we also report precision (P)
and the recall for nouns and verbs. Recall mea-
sures are accompanied by a 95% confidence in-
terval calculated using bootstrap resampling pro-
cedure (Noreen, 1989). The difference between
two systems is deemed to be statistically signifi-
cant if there is no overlap between the confidence
intervals. We show graphically the results in Fig-
ure 1. For instance, the differences between the
highest scoring system and the following four sys-
tems are not statistically significant. Note that this
method of estimating statistical significance might
be more strict than other pairwise methods.
We also include the results of two baselines.
The random baseline was calculated analytically.
The first sense baseline for each language was
taken from each wordnet. The first sense baseline
in English and Chinese corresponds to the most
frequent sense, as estimated from out-of-domain
corpora. In Dutch and Italian, it followed the in-
tuitions of the lexicographer. Note that we don?t
have the most frequent sense baseline from the do-
main texts, which would surely show higher re-
sults (Koeling et al, 2005).
5 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. With
this paper we have motivated the creation of an
all-words test dataset for WSD on the environ-
ment domain in several languages, and presented
the overall design of this SemEval task.
One of the goals of the exercise was to show
that WSD systems could make use of unannotated
background corpora to adapt to the domain and
improve their results. Although it?s early to reach
hard conclusions, the results show that in each of
the datasets, knowledge-based systems are able to
improve their results using background text, and
in two datasets the adaptation of knowledge-based
systems leads to results over the MFS baseline.
The evidence of domain adaptation of supervised
systems is weaker, as only one team tried, and the
differences with respect to MFS are very small.
The best results for English are obtained by a sys-
tem that combines a knowledge-based system with
some targeted hand-tagging. Regarding the tech-
niques used, graph-based methods over WordNet
and distributional thesaurus acquisition methods
have been used by several teams.
79
All datasets and related information are publicly
available from the task websites
6
.
Acknowledgments
We thank the collaboration of Lawrence Jones-Walters, Amor
Torre-Marin (ECNC) and Karin de Boom (WWF), com-
piling the test and background documents. This work
task is partially funded by the European Commission (KY-
OTO ICT-2007-211423), the Spanish Research Department
(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL09), pages 33?
41. Association for Computational Linguistics.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
Davide Buscaldi and Paolo Rosso. 2007. Upv-wsd : Com-
bining different wsd methods by means of fuzzy borda
voting. In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007), pages
434?437.
P. Chen, W. Ding, and D. Brown. 2009. A fully unsupervised
word sense disambiguation method and its evaluation on
coarse-grained all-words task. In Proceeding of the North
American Chapter of the Association for Computational
Linguistics (NAACL09).
Radu Ion and Dan Stefanescu. 2009. Unsupervised word
sense disambiguation with lexical chains and graph-based
context formalization. In Proceedings of the 4th Language
and Technology Conference: Human Language Technolo-
gies as a Challenge for Computer Science and Linguistics,
pages 190?194.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense dis-
ambiguation. In EACL ?09: Proceedings of the 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 389?397, Morristown,
NJ, USA. Association for Computational Linguistics.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense dis-
ambiguation combining corpus based and wordnet based
parameters. In Proceedings of the 5th International Con-
ference on Global Wordnet (GWC2010).
A. Kilgarriff. 2001. English Lexical Sample Task Descrip-
tion. In Proceedings of the Second International Work-
shop on evaluating Word Sense Disambiguation Systems,
Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
6
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
Natural Language Processing. HLT/EMNLP, pages 419?
426, Ann Arbor, Michigan.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A
wordnet-based algorithm for word sense disambiguation.
In Proceedings of The 14th International Joint Conference
on Artificial Intelligence (IJCAI95).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2007. Unsupervised acquisition of predominant
word senses. Computational Linguistics, 33(4).
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Eric W. Noreen. 1989. Computer-Intensive Methods for Test-
ing Hypotheses. John Wiley & Sons.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague, Czech Republic.
Paul Rayson and Roger Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the workshop
on Comparing corpora, pages 1?6.
Hansen A. Schwartz and Fernando Gomez. 2008. Acquir-
ing knowledge from the web to be used as selectors for
noun sense disambiguation. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CONLL08).
B. Snyder and M. Palmer. 2004. The English all-words task.
In Proceedings of the 3rd ACL workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
M. Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, and
A. Marchetti. 2010. Kafnotator: a multilingual seman-
tic text annotation tool. In In Proceedings of the Second
International Conference on Global Interoperability for
Language Resources.
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Christiane
Fellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-
hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-
chini, Federico Neri, Remo Raffaelli, German Rigau,
Maurizio Tescon, and Joop VanGent. 2008. Kyoto: a
system for mining, structuring and distributing knowl-
edge across languages and cultures. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
80
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
Kyoto: An Integrated System for Specific Domain WSD
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle
University of the Basque Country
a.soroa@ehu.es
Monica Monachini
Istituto di Linguistica Computazionale
monica.monachini@ilc.cnr.it
Jessie Lo, Shu-Kai Hsieh
National Taiwan Normal University
shukai@ntnu.edu.tw
Wauter Bosma, Piek Vossen
Vrije Universiteit
{p.vossen,w.bosma}@let.vu.nl
Abstract
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
1 Introduction
In this paper we describe the participation of the
integrated Kyoto system on the ?SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain? task (Agirre et al,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project
1
. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al, 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
1
http://www.kyoto-project.eu
results. Finally, the conclusions are presented.
2 The Kyoto System for Domain Specific
WSD
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
2.1 UKB
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V,E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node?s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
417
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
2.2 Tybots
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion
2
. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al, 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
2.3 Lexical Knowledge bases
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al, 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie
3
. Cornetto com-
prises taxonomic relations and equivalence rela-
2
http://kyoto.let.vu.nl/svn/kyoto/trunk
3
http://www.inl.nl/nl/lexica/780
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al, 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public
4
(Tsai et al,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
3 Experimental setting
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
3.1 UKB parameters
We use UKB with the default parameters. In par-
ticular, we don?t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It?s also worth mentioning that in the
default setting parts of speech were not used.
4
http://cwn.ling.sinica.edu.tw
418
RANK RUN P R R-NOUN R-VERB
Chinese
- 1sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- 1sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- 1sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- 1sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
Table 2: Overall results of our runs, including pre-
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
3.2 Run1: UKB using context
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
3.3 Run2: UKB using related words
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al, 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. ?biodiversity, agri-
culture, ecosystem, nature, life, climate, . . .?). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al, 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 10
5
. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
3.4 Run3: UKB using related words and
bilingual graphs
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
4 Results
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
5
In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
419
our previous work on English (Agirre et al,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
?flat? structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don?t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
5 Conclusions
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
Acknowledgments
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33?41. Association for Computational Linguistics.
E. Agirre, O. L?opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
E. Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
?02: Proceedings of the 11th international conference on
WWW, pages 517?526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745?791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485?506.
420
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 1?10,
Beijing, August 2010
KYOTO: an open platform for mining facts
Piek Vossen
VU University Amsterdam
p.vossen@let.vu.nl
German Rigau
Eneko Agirre
Aitor Soroa
University of the Basque 
Country
german.rigau/e.a-
girre/a.soroa@ehu.es
Monica Monachini
Roberto Bartolini
Istituto di Linguistica 
Computazionale, CNR
monica.monachini/r
oberto.bartolin-
i@ilc.cnr.it
Abstract
This  document  describes  an  open 
text-mining  system  that  was  developed 
for the Asian-European project KYOTO. 
The  KYOTO system uses  an  open text 
representation format and a central onto-
logy to  enable  extraction  of  knowledge 
and facts  from large volumes of text  in 
many different languages. We implemen-
ted a semantic tagging approach that per-
forms off-line reasoning. Mining of facts 
and  knowledge  is  achieved  through  a 
flexible pattern matching module that can 
work in much the same way for different 
languages,  can  handle  efficiently  large 
volumes of documents and is not restric-
ted to a specific domain. We applied the 
system to an English database on estuar-
ies.
1 Introduction
Traditionally, Information Extraction (IE) is the 
task of filling template information from previ-
ously unseen text which belongs to a predefined 
domain (Peshkin & Pfeffer 2003). Most systems 
in  the  Message  Understanding  Conferences 
(MUC,  1987-1998)  and the  Automatic  Content 
Extraction  program  (ACE)1 use  a  pipeline  of 
tools to achieve this, ranging from sophisticated 
NLP tools (like deep parsing) to shallower text-
processing (e.g. FASTUS (Appelt 1995)).
Standard  IE  systems  are  based  on  lan-
guage-specific  pattern  matching  (Kaiser  & 
1http://www.itl.nist.gov/iad/mig//tests/ace  
Miksch 2005), where each pattern consists of a 
regular  expression  and  an  associated  mapping 
from syntactic to logical form. In general, the ap-
proaches can be categorized into two groups: (1) 
the Knowledge Engineering approach (Appelt et 
al.1995), and (2) the learning approach, such as 
AutoSlog  (Appelt  et  al.  1993),  SRV  (Freitag 
1998), or RAPIER (Califf & R. Mooney 1999). 
Another  important  system  is  GATE (Cunning-
ham et al2002), which is a platform for creating 
IE systems. It uses regular expressions, but it can 
also  use  ontologies  to  perform semantic  infer-
ences  to  constrain  linguistic  patterns  semantic-
ally. The use of ontologies in IE is an emerging 
field (Bontcheva & Wilks 2004): linking text in-
stances with elements belonging to the ontology, 
instead of consulting flat gazetteers.
The major disadvantage of traditional IE sys-
tems is that they focus on satisfying precise, nar-
row, pre-specified requests from small homogen-
eous corpora (e.g., extract information about ter-
rorist events). Likewise, they are not flexible, are 
limited to specific types of knowledge and need 
to be built by knowledge engineers for each spe-
cific application and language. In fact most text 
mining  systems are  developed for  a  single  do-
main and a single language, and are not able to 
handle  knowledge  expressed  in  different  lan-
guages  or  expressed  and conceptualized  differ-
ently across cultures.
In this paper we describe an open platform for 
text-mining  or  IE that  can  be applied  to many 
different  languages  in  the  same  way  using  an 
open text representation system and a central on-
1
tology that  is  shared across  languages.  Ontolo-
gical implications are inserted in the text through 
off-line  reasoning and ontological  tagging.  The 
events and facts are extracted from large amounts 
of text using a flexible pattern-matching module, 
as specified by profiles  which comprise  ontolo-
gical and shallow linguistic patterns. The system 
is  developed  in  the  Asian-European  project 
KYOTO2.
In the next section,  we describe the general 
architecture of the KYOTO system. In section 3, 
we specify the knowledge structure that is used. 
Section  4,  describes  the  off-line  reasoning  and 
ontological tagging. In section 5, we describe the 
module for mining knowledge from the text that 
is enriched with ontological  statements.  Finally 
in section 6, we describe the first results of ap-
plying the system to databases on Estuaries.
2 KYOTO overview
The  KYOTO  project  allows  communities  to 
model terms and concepts in their domain and to 
use this knowledge to apply text mining on docu-
ments. The knowledge cycle in the KYOTO sys-
tem starts  with a set  of  source  documents pro-
duced by the community, such as PDFs and web-
sites.  Linguistic  processors  apply  tokenization, 
segmentation, morpho-syntactic analysis and  se-
mantic  processing  to  the  text  in  different  lan-
guages. The semantic processing involves the de-
tection of named-entities (persons, organizations, 
places,  time-expressions)  and  determining  the 
meaning of  words  in  the  text  according to  the 
given wordnet.  
The  output  of  the  linguistic  processors is 
stored in an XML annotation format that  is the 
same for  all  the languages,  called  the KYOTO 
Annotation  Format  (KAF,  Bosma  et  al  2009). 
This format incorporates standardized proposals 
for the linguistic annotation of text and represents 
them in an easy-to-use layered structure, which is 
compatible with the Linguistic Annotation Frame-
work  (LAF,  Ide  and  Romary  2003).  In  KAF, 
words, terms, constituents and syntactic depend-
encies  are  stored  in  separate  layers  with  refer-
ences across the structures. This makes it easier 
to harmonize the output of  linguistic processors 
2 Http://www.kyoto-project.eu
for different languages and to add new semantic 
layers to the basic output, when needed (Bosma 
et al 2009, Vossen et al 2010). All modules in 
KYOTO draw their input from these structures. 
In fact, the word-sense disambiguation process is 
carried out to the same KAF annotation in differ-
ent languages and is therefore the same for all the 
languages (Agirre et al 2009). In the current sys-
tem,  there  are  processors  for  English,  Dutch, 
Italian, Spanish, Basque, Chinese and Japanese.
The KYOTO system proceeds in 2 cycles (see 
Figure 1). In the 1st cycle, the Tybot (Term Yield-
ing Robot) extracts the most relevant terms from 
the documents. The Tybot is another generic pro-
gram that  can  do  this  for  all  the  different  lan-
guages in much the same way. The terms are or-
ganized as a structured hierarchy and, wherever 
possible,  related  to  generic  semantic  databases, 
i.e. wordnets for each language. In the left part of 
Figure 1, we show those terms in the input docu-
ment and their classification in wordnet. Terms in 
italics are present in the original wordnet, while 
underlined terms correspond to terms which were 
not in the original wordnet but were automatic-
ally discovered and linked to wordnet by Tybots. 
Straight  terms  correspond  to hyperonyms  in 
wordnet that do not necessarily occur in the text 
but are linked to ontological classes. The result of 
this  1st cycle  is a domain wordnet  for the target 
language.
The 2nd cycle of the system involves the actu-
al extraction of factual knowledge from the docu-
ments by the Kybots  (Knowledge Yielding Ro-
bots). Kybots use a collection of profiles that rep-
resent patterns of information of interest. In the 
profile, conceptual relations are expressed using 
ontological  and morpho-syntactic linguistic pat-
terns. Since the semantics is defined through the 
ontology,  it  is  possible  to  detect  similar  data 
across documents in different languages, even if 
expressed differently. In Figure 1, we give an ex-
ample of a conceptual pattern that relates organ-
isms that live in habitats. The Kybot can combine 
morpho-syntactic and semantic patterns. When a 
match is detected, the instantiation of the pattern 
is saved in a formal representation, either in KAF 
or in RDF. Since the wordnets in different lan-
guages are mapped to the same ontology and the 
text in these languages is represented in the same 
KAF,  similar  patterns  can  easily  be  applied  to 
multiple languages.
2
3 Ontological  and  lexical  background 
knowledge
As a semantic background model, we defined a 
3-layered  knowledge  architecture  following the 
principle  of  the  division  of  labour  (Putnam 
1975). In this model, the ontology does not need 
to be the central hub for all terms in a domain in 
all  languages.  Following the division  of labour 
principle, we can state that a computer does not 
need  to  distinguish  between  instances  of  a 
European Tree Frog and a Glass Tree frog. We 
assume  that  rigid  concepts  (as  defined  by 
Guarino and Welty 2002) are known to the do-
main experts and do not need to be defined form-
ally in the ontology but can remain in the avail-
able  background  resources,  such  as   databases 
with millions of species.  Terms in the documents 
are mostly non-rigid, e.g.  endangered frogs,  in-
vasive  frogs.  Such  non-rigid  terms  refer  to  in-
stances  of  species  in  contextual  circumstances. 
The processes and states are the important pieces 
of  information  that  matter  to the users  and are 
useful for mining text. The model therefore dis-
tinguishes between background vocabularies, do-
main terms,  wordnets and the central  ontology. 
The  background  vocabularies  are  automatically 
aligned  to  wordnet,  where  we  assume  that 
hyponymy relations to rigid synsets in wordnet 
declare those subconcepts as rigid subtypes too, 
without the necessity to include them in the onto-
logy.  For  non-rigid  terms,  we  defined  a  set  of 
mapping relations to the ontology through which 
we express their non-rigid involvement in these 
processes and states. Likewise, the ontology has 
been extended with processes and states for the 
domain  and  verbs  and  adjectives  have  been 
mapped to be able to detect expressions in text.
The  3-layered  knowledge  model  combines  the 
efforts from 3 different communities:
1.Domain  experts  in  social  communities  that 
continuously build background vocabularies;
2.Wordnet  specialists  that  define  the  basic  se-
mantic model for general concepts for a lan-
guage
3.Semantic Web specialists that define top-level 
and domain-specific ontologies that capture 
formal definitions of concepts;
We formalized the relations between these repos-
itories so that they can developed separately but 
combined within KYOTO to form a coherent and 
formal model.
3.1 Ontology
The KYOTO ontology currently consists of 1149 
classes divided over three layers. The top layer is 
based  on  DOLCE  (DOLCE-Lite-Plus  version 
3.9.7,  Masolo  et  al  2003)  and  OntoWordNet. 
This layer of the ontology has been modified for 
our purposes (Herold et. al. 2009).  The second 
layer consists of so-called Base Concepts (BCs) 
derived  from various  wordnets  (Vossen  1998, 
Izquierdo  et  al. 2007).  Examples  of  BCs  are: 
building,  vehicle,  animal,  plant,  change,  move,  
size, weight. The BCs are those synsets in Word-
Net 3.0 that have the most relations with other 
synsets in the wordnet hierarchies and are selec-
ted in a way that ensures complete coverage of 
the nominal and verbal part of WordNet. This has 
been  completed  for  the  nouns  (about  500 
synsets).  The ontology has also been adapted to 
include important concepts in the domain. Spe-
cial attention has been paid to represents the pro-
cesses  (perdurants)  in  which  objects  (endur-
ants)  of  the domain are  involved and qualities 
they may have. This is typically the information 
that is found in documents on the environment. 
We thus added 40 new event classes for repres-
enting  important  verbs  (e.g.  pollute, absorb, 
damage, drain) and 115 new qualities and qual-
ity-regions for representing important adjectives 
(e.g. airborne, acid, (un)healthy, clear). The full 
Figure 1: Two Cycles of processing in KYOTO
3
ontology can be downloaded from the KYOTO 
website, free for use. A considerable set of gener-
al verbs and adjectives (relevant for for the do-
main)  have  then  been  mapped  to  ontological 
classes: 189  verbal  synsets  and  222  adjectival 
synsets.
The  500  nominal  BCs  are  connected  to  the 
complete  WordNet  hierarchy,  whereas  the  189 
verbs represent 5,978 more specific verbal syn-
sets and the 222 adjectives represent  1,081 ad-
jectival synsets through the wordnet relations.
This basic ontology and the mapping to Word-
Net  are  used  to  model  the  shared  and  lan-
guage-neutral  concepts  and  relations  in the do-
main. Instances are excluded from the ontology. 
Instances will be detected in the documents and 
will be mapped to the ontology through instance 
to ontology relations (see below).  Likewise, we 
make a clear separation between the ontological 
model and the instantiation of the model as de-
scribed in the text.
3.2 Wordnet to ontology mappings
In addition to the ontology, we have wordnets for 
each language in the domain. In addition to the 
regular synset to synset relations in the wordnet, 
we will have a specific set of relations for map-
ping the synsets  to the ontology,  which are  all 
prefixed with sc_ standing for synset-to-concept. 
We differentiate between rigid and non-rigid con-
cepts in the wordnets through the mapping rela-
tions:
? sc_equivalenceOf: the synset is fully equi-
valent to the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_  subclassOf: the synset is a proper sub-
class of the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_domainOf: the synset is not a proper sub-
class  of  the  ontology  Type  &  is  not  disjoint 
(therefore orthogonal) with other synsets that are 
mapped to the same Type either through sc_sub-
classOf or sc_domainOf; the synset is non-Rigid 
but still inherits all properties of the target onto-
logy Type;  the synset  is  also related to a Role 
with a sc_playRole relation
? sc_playRole:  the  synset  denotes  instances 
for  which  the  context  of  the  Role  applies  for 
some period of time but this is not essential for 
the existence of the instances, i.e. if the context 
ceases to exist then the instances may still exist 
(Mizoguchi et al 2007).3
? sc_participantOf:  instances of the concept 
(denoted by the synset) participate in some en-
durant, where the specific role relation is indic-
ated by the playRole mapping. 
? sc_hasState: instances of the concept are in 
a particular state which is not essential and can 
be changed. There is no need to represent the role 
for a stative perdurant.
This model  extends  existing  WordNet  to  onto-
logy mappings.  For  instance,  in  the  SUMO to 
Wordnet mapping (Niles and Pease 2003), only 
the  sc_equivalenceOf and  sc_subclassOf rela-
tions  are  used,  represented  by  the symbols  ?=? 
and ?+? respectively. The SUMO-Wordnet map-
ping likewise does not systematically distinguish 
rigid from non-rigid  synsets.  In our  model,  we 
separate the linguistically and culturally specific 
vocabularies from the shared ontology while us-
ing the ontology  to interface  the concepts used 
by the various communities.
Using these mapping relations, we can express 
that the synset for  duck (which has a hypernym 
relation to the synset  bird, which, in its turn, has 
an  equivalence  relation  to  the  ontology  class 
bird) is  thus  a  proper  subclassOf  the  ontology 
class bird:
wn:duck hypernym wn:bird
wn:bird  sc_equivalenceOf ont:bird
For a concept such as migratory bird, which is 
also  a  hyponym of  bird in  wordnet  but  not  a 
proper subclass as a non-rigid concept, we thus 
create the following mapping:
wn:migratory bird 
? sc_domainOf ont:bird
? sc_playRole ont:done-by
? sc_participantOf ont:migration
This mapping indicates that the synset is used to 
refer to instances of endurants (not subclasses!), 
where the domain is restricted to birds. Further-
more, these instances participate in the process of 
3 Some terms involve more than one role,  e.g.  gas-
powered-vehicle.  Secondary  participants  are  related 
through  sc_hasCoParticipant and sc_playCoRole 
mappings.
4
migration in the role of  done-by. The properties 
of  the  process  migration are  further  defined  in 
the  ontology,  which  indicates  that  it  is  a  act-
ive-change-of-location  done-by  some  endurant, 
going from a source, via a path to some destina-
tion. The mapping relations from the wordnet to 
the ontology, need to satisfy the constraints of the 
ontology, i.e. only roles can be expressed that are 
compatible with the role-schema of the process 
in which they participate.
For  implied  non-essential  states,  we  use  the 
sc_hasState relation to express that a synset such 
as wild dog refers to instances of dogs that life in 
the wild but can stop being wild:
wn:wild dog ? sc_domainOf ont:dog
wn:wild dog ? sc_hasState ont:wild
Ideally, all processes and states that can be ap-
plied to endurants should be defined in the onto-
logy. This may hold for most verbs and adject-
ives in languages, which do not tend to extend in 
specific  domains  and  are  part  of  the  general 
vocabulary  (e.g.  to  pollute,  to  reduce,  wild). 
However, domain specific text contain many new 
nominal terms that refer to domain-specific pro-
cesses and states, e.g. air pollution, nitrogen pol-
lution,  nitrogen  reduction.  These  terms  are 
equally relevant as their counter-parts that refer 
to endurants involved in similar  processes, e.g. 
polluted air, polluting nitrogen or reduced nitro-
gen. We therefore use the reverse participant and 
role mappings to be able to define such terms for 
processes  as  subclasses  of  more  general  pro-
cesses  involving  specific  participants  in  a  spe-
cified role:
wn:air pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:air
? sc_hasRole ont:patient
wn:nitrogen pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:nitrogen
? sc_hasRole ont:done-by
 
Further  mapping  relations  are  described  in  the 
documentation on the KYOTO website. Through 
the mapping relations, we can keep the ontology 
relatively small and compact whereas we can still 
define  the  richness  of  the  vocabularies  of  lan-
guages in a precise way. The classes in the onto-
logy can be defined using rich axioms that model 
precise implications for inferencing. The wordnet 
to synset mappings can be used to define rather 
basic relations relative to the given ontology that 
still  captures  the  semantics  of  the  terms. The 
term definitions capture both relevance and per-
spective  (those  relations  that  matter  from  the 
point of the view of the term), on the one hand, 
and some semantics with respect to the concepts 
that are involved and their (role) relation on the 
other  hand.  Likewise,  the  KYOTO  system can 
model the linguistic and cultural diversity of lan-
guages in a domain but at the same time keep a 
firm anchoring to a basic and compact ontology.
3.3 Domain wordnet
We selected 3 representative documents on estu-
aries to extract relevant terms for the domain us-
ing the Tybot module. The terms have been re-
lated  through  structural  relations,  e.g.  nitrogen 
pollution is a hyponym of pollution, and through 
WordNet synsets that are assigned through WSD 
of the text.  We extracted 3950 candidate  terms 
form the KAF representations of the documents. 
Most of these are nouns (2818 terms). The nom-
inal  terms matched for 40% with wordnet syn-
sets, the verbs and adjectives for 98% and 85% 
respectively. For the domain wordnet, we restric-
ted ourselves to the nouns. From the new nomin-
al  terms,  environmentalists selected  390  terms 
that they deem to be important. These terms are 
connected to parent terms, which ultimately are 
connected to wordnet synsets.  The final domain 
wordnet contains 659 synsets: 197 synsets from 
the generic wordnet and 462 new synsets connec-
ted to the former.  The domain wordnet synsets 
got 990 mappings to the ontology, using the rela-
tions described in the previous section. There are 
86 synsets that have a sc_domainOf mapping, in-
dicating  that  they  are  non-rigid.  Note  that 
hyponyms of these synsets are also non-rigid by 
definition. These non-rigid synsets have complex 
mappings to processes and states in which  they 
are involved. The domain wordnet can be down-
loaded from the KYOTO website, free for use.
5
4 Off-line reasoning and ontological tag-
ging 
The ontological tagging represents the last phase 
in the KYOTO Linguistic  Processor  annotation 
pipeline.  It  consists  of  a three-step module  de-
vised to enrich the KAF documents with know-
ledge derived from the ontology. For each synset 
connected to a term, the first step   adds the Base 
Concepts to which the synset is related through 
the wordnet taxonomical relations. Then, through 
the synset to ontology mapping, it  adds the cor-
responding ontology type with appropriate rela-
tions. Once each synset is specified as to its onto-
logy type,  the  last  ontotagging  step  inserts  the 
full  set  of  ontological  implications  that  follow 
from the explicit ontology. The explicit ontology 
is a new data  structure consisting of a table with 
all  ontology nodes and all  ontological  implica-
tions expressed. The main purpose is to optimize 
<term lemma="pollution" pos="N" tid="t13444" type="open">
  <externalReferences>
   <externalRef reference="eng-30-00191142-n" reftype="baseConcept" resource="wn30g"/>
   <externalRef reference="Kyoto#change-eng-3.0-00191142-n" reftype="sc_subClassOf" resource="ontology">
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#contamination_pollution"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#accomplishment" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#event" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#part" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#specific-constant-constituent" reference="DOLCE-Lite.owl#perdurant" 
status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-quality" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#spatio-temporal-particular" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#participant" reference="DOLCE-Lite.owl#endurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-location_q" status="im-
plied"/>
    <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#particular" status="implied"/>
    </externalRef>
  </externalReferences>
</term>
Figure 2: An example of an OntoTagged output
<kprofile>
 <variables>
<var name="x" type="term" pos="N"/>
  <var name="y" type="term" 
       lemma="produce | generate | release | ! create"/>
  <var name="z" type="term"
       reference="DOLCE-Lite.owl#contamination_pollution"
       reftype="SubClassOf"/>
 </variables>
 <relations>
  <root span="y"/>
  <rel span="x" pivot="y" direction="preceding"/>
  <rel span="z" pivot="y" direction="following"/>
 </relations>
 <events>
  <event target="$y/@tid" lemma="$y/@lemma" pos="$y/@pos"/>
  <role target="$x/@tid" rtype="agent" lemma="$x/@lemma"/>
  <role target="$z/@tid" rtype="patient"lemma="$z/@lemma"/>$
 </events>
</kprofile>
Figure 3: An example of a Kybot profile
<kybotOut>
 <doc name="11767.mw.wsd.ne.onto.kaf">
  <event eid="e1" lemma="generate" pos="V" target="t3504"/>
  <role rid="r1" lemma="industry" rtype="agent" target="t3493" pos="N" event="e1"/>
  <role rid="r2" lemma="pollution" rtype="patient" target="t3495" pos="N" event="e1"/>
 </doc>
 <doc name="16266.mw.wsd.ne.onto.kaf">
  <event eid="e2" lemma="release" pos="V" target="t97"/>
  <role rid="r3" lemma="fuel" rtype="agent" target="t96" pos="N" event="e2"/>
  <role rid="r4" lemma="exhaust_gas" rtype="patient" target="t101" pos="V" event="e2"/>
 </doc>
</kybotOut>
Figure 4: An example of a Kybot output
6
the performance of the mining module over large 
quantities of documents. The advantage for Ky-
bots from ontotagging are many. First of all, they 
are  able  to  run  and  apply  pattern-matching  to 
Base  Concepts  and  ontological  classes  rather 
than just to words or synsets. Moreover, by mak-
ing explicit  the  implicit  ontological  statements, 
Kybots are able to find the same relations hidden 
in  different  expressions  with  different  surface 
realizations:  fish migration,  migratory  fish,  mi-
gration of fish, fishes that migrate, that directly 
or indirectly express the same relations. With on-
totagging,  they  share  the  same ontological  im-
plications which will allow Kybots to apply the 
same patterns and perform the extraction of facts. 
The implications will be represented in the same 
way across different languages, thus facilitating 
cross-lingual extraction of facts. Lastly, ontotag-
ging is a kind of off-line ontological reasoning: 
without  doing reasoning over concepts,  Kybots 
substantially  improve their  performance.  Figure 
2 shows the result of onto-tagging for the term 
pollution.
5 Event and fact extraction
Kybots (Knowledge Yielding Robots) are  com-
puter  programs  that  use  the  mined 
concepts and the generic  concepts  already con-
nected to the language wordnets and the KYOTO 
ontology to extract actual concept instances and 
relations in KAF documents. Kybots incorporate 
technology  for  the  extraction  of  relationships, 
either eventual or not, relative to the general or 
domain concepts already captured by the Tybots. 
That is, the extraction of factual knowledge is be-
ing carried out by the Kybot server by processing 
Kybot profiles on the linguistically enriched doc-
uments.
Kybots  are  defined  following  a  declarative 
format,  the  so  called  Kybot  
profiles, which describe general morpho-syntact-
ic  and  semantic  conditions  on  sequences  of 
terms. Profiles are compiled to generate the Ky-
bots, which scan over KAF documents searching 
for the patterns and extract the relevant informa-
tion from each matching.
Linguistic  patterns  include morphologic  con-
straints and also semantic conditions the matched 
terms must hold.  Kybot are thus able to search 
for term lemmas or part-of-speech tags but also 
for terms linked to ontological process and states 
using  the  mappings  described  in  Section  3.2. 
Thus, it is possible to detect similar eventual in-
formation  across  documents  in  different  lan-
guages, even if expressed differently.
5.1 Example of a Kybot Profile
Kybot Profiles are described using XML syn-
tax.  Figure 3 presents an example of a profile. 
Kybot profiles consist of three main parts: 
?Variable  declaration (<variables> element): 
In this section the search entities are defined. The 
example  defines  three  variables:  x (denoting 
terms  whose  part-of-speech is  noun),  y (which 
are  terms whose lemma is ?release?, ?produce? 
or  ?generate?  but   not  ?create?)  and  z (terms 
linked to  the  ontological  endurant  ?DOLCE-L-
ite.owl#contamination_pollution?, meaning ``be-
ing contaminated with harmful  substances''). 
?Declarations  of  the  relations  among  variables 
(<rel> element): specify the relations among the 
previously  defined variables.  The example pro-
file specifies y  as the main pivot, and states that 
variable  x must  be  preceding  variable  y in  the 
same sentence, and that variable  z must be fol-
lowing variable  y.  Thus,  the Kybot will  search 
for patterns like 'x ? y ? z' in a sentence.
?Output template (<events> element): describes 
the output to be produced on every matching. In 
the example, each match generates a new event 
targeting term  y,  which becomes the main term 
of the event. It also fills two roles of the event, 
the 'agent' role filled by term x and 'patient' role, 
filled by z. 
Figure  4  presents  the  output  of  the  Kybot 
when applied against the benchmark documents.
The Kybot output follows the stand-off architec-
ture when producing new information, and it thus 
forms  a  new KAF layer  on  the  original  docu-
ments.
6 Experimental results
We applied the KYOTO system and resources to 
English documents on estuaries. We collected 50 
URLs for two English estuaries: the Humber Es-
tuary in Hull (UK) and the Chesapeake Bay estu-
ary in the US and for background documents on 
bird  migration,  sedimentation,  habitat  destruc-
tion,  and  climate  change.  In  addition  to  the 
webpages, we extracted 815 PDF files from the 
sites. In total, 4625 files have been extracted. All 
7
the documents have been processed by the lin-
guistic  processor  for  English,  which  generated 
KAF representations for all the documents. From 
this  database,  3  documents  were  selected  for 
benchmarking.
The  documents  were  processed  by  applying 
multiword  tagging,  word-sense-disambiguation, 
named-entity-recognition  and  the  ontological 
tagging to the 3 documents and to the complete 
database; This was done twice: once without the 
domain model and once with the domain model. 
We thus created 4 datasets:  3 benchmark docu-
ments  processed  with  and  without  the  domain 
model; the complete database processed with and 
without the domain model.
Furthermore, we created Kybot profiles based 
on the type of information represented in the do-
main model. We applied the Kybots to all 4 data 
sets. We generate the following data files through 
an WN-LMF export of the domain wordnet:
1. a set of domain multiwords for the multi-
word tagger
2. an extension of the lexicon and the graph 
of  concepts  that  is  used  by  the  WSD 
module
3. an extension of the wordnet-to-ontology 
mappings for the ontotagger
In addition, we constructed mapping lists for all 
WordNet 3.0 synsets to Base Concepts and to ad-
jective and verbs that are matched to the onto-
logy.  These mappings provide the generic  con-
ceptual model based on wordnet and on the onto-
logy. 
Table 1 shows the effects of using the domain 
model for the first 3 modules. We can see that the 
domain  model  has  a  clear  effect  on  the multi-
word  detection  in  the  3  evaluation  documents. 
Using the domain model,  600 multiwords have 
been detected, against 145 with just the generic 
wordnet. This is obvious since the terms are ex-
tracted  from  the  same  documents.  However, 
when applying it  to the complete  database,  we 
see that  still  over 2,300 more multiwords have 
been  detected  using  the domain wordnet.  Note 
that the domain wordnet has only 97 multiwords 
and the generic wordnet has 19,126 multiwords. 
So 0.5% of the multiwords in the domain word-
net add 1.5 times more multiword tokens in the 
database. The third row specifies the number of 
synsets that have been assigned. We can see that 
for the domain model almost 400 more synsets 
have been detected. In the case of the full estuary 
database, we see that relatively few more have 
been detected, almost 1,500 while the database is 
80 times as big. If we look more closely at the 
numbers of actual  domain synsets detected,  we 
see the following results. In the benchmark docu-
ments  637 (or 5%) of  the synsets  is  a  domain 
wordnet  synset,  whereas  5,353 synsets  are  do-
main synsets in the full estuary database, which 
is only 0.52%. Note that in KAF multiwords are 
represented both as a single terms and in terms of 
their elements. The WSD module assigns synsets 
to  both.  The  domain  model  can  thus  only  add 
synsets compared to the processing without the 
domain. 
Finally, if we look at the named-entity-recogni-
tion module, we see a slight negative effect for 
the detection of named-entities due to the domain 
model.  The  named-entity-recognition  module 
does not consider the elements of multiwords but 
just  the multiword terms as a whole. Grouping 
terms  as  multiwords  thus  leads  to  less  named-
entities being detected. This is not necessarily a 
bad things, since the detection heavily over-gen-
erates and could have now more precision.
Table 1: Statistics on processing the estuary documents with and without domain model
bench mark documents (3) estuary documents (4742)
No Domain Domain No Domain Domain
terms 22,204 22,204 2,419,839 2,419,839
multiwords 145 600 4,389 6,671
12,526 12,910 1,021,598 1,023,017
158 126 41,681 40,714
67 66 10,288 10,233
synsets
ne location
ne date
8
Table 2 shows the effect of inserting ontologic-
al  implications  into  the  text  representation.For 
the benchmark documents, we see that more than 
half a million ontological implications have been 
inserted.  Of  these, 82% are implied references, 
that are extracted from the explicit ontology on 
the  basis  of  a  direct  mapping to  the  ontology. 
About  8% of  the  mappings  are  synset-to-onto-
logy mappings (sc) and 9.5% are mappings rep-
resenting the subclass hierarchy. The differences 
between using the domain model and not-using 
the domain model are minimal. For the complete 
database, the implications are 80 times as much 
but the proportions are similar.
Table 3 shows the type of sc-relations that oc-
cur.  Obviously,  sc_subClassOf  and  sc_equival-
entOf  are  the  most  frequent.  Nevertheless,  we 
still  find  about  500  mappings  that  present  the 
participation in a process or state. 
 
     30  reftype="sc_playCoRole"
     32  reftype="sc_hasCoParticipant"
     42  reftype="sc_partOf"
     59  reftype="sc_stateOf"
     92  reftype="sc_playRole"
     94  reftype="sc_hasRole"
     97  reftype="sc_participantOf"
   105  reftype="sc_hasParticipant"
   128  reftype="sc_domainOf"
   169  reftype="sc_hasState"
   312  reftype="sc_hasPart"
 3637  reftype="sc_equivalentOf"
42048  reftype="sc_subClassOf"
Table 3: Type of relations for the wordnet to ontology  
mappings using the domain model
The table clearly shows the impact of role rela-
tions  that  are  encoded  in  the  domain  wordnet. 
When  we  extract  the  mappings  for  the  files 
without the domain model (ony using the map-
pings to the generic wordnet), we get only equi-
valence and subclass mappings.
Finally to complete the knowledge cycle, we cre-
ated a few Kybot profiles for extracting events 
from the  onto-tagged  documents.  As  an  initial 
test, 3 profiles have been created:
1. events of destruction
2. destructions of locations
3. destruction of objects
Using  these  profiles,  we  extracted  211  events 
from the 3 benchmark documents with 396 roles. 
The profiles are created to run over the ontolo-
gical  types  inserted  by  the  ontotagger,  e.g.  re-
stricted to events and change_of_integrity.  Des-
pite the generality of the profiles, we still see a 
clear signature of the domain in the output. This 
is a good indication that we will be able to ex-
tract valuable events from the data, even though 
the  ontotagger  generates  a  massive  amount  of 
implications.  Especially  events  that  combine 
multiple  roles  appear  to  give  rich  information. 
For example, the following sentence:
"One of the greatest challenges to restoration is con-
tinued population growth and development, which 
destroys forests, wetlands and other natural areas"
yielded the following output:
   <event target="t1471" lemma="destroy" pos="V" 
eid="e74"/>
   <role target="t1477" rtype="patient" lemma="area" 
pos="N" event="e74" rid="r138"/>
   <role target="t1472" rtype="patient" 
lemma="forest" pos="N" event="e74" rid="r151"/>
   <role target="t1469" rtype="actor" lemma="devel-
opment" pos="N" event="e74" rid="r180"/>
Running the full set of profiles on the complete data-
base with almost 60 million ontological statements 
took about 2 hours. This shows that our approach is 
scalable and efficient.
Table 2: Ontological implications for the four data sets
bench mark documents (3) estuary documents (4272
No Domain Domain Domain
ontology references 555,677 576,432 48,708,300
implied ontology references 457,332 82.30% 474,916 82.39% 40,523,452 83.20%
direct ontology references 53,178 9.57% 54,769 9.50% 4,377,814 8.99%
45,167 8.13% 46,747 8.11% 3,807,034 7.82%domain synset to ontology mappings
9
7 Conclusions
In this paper, we described an open platform for 
text-mining  using wordnets  and a central  onto-
logy.  The  system  can  be  used  across  different 
languages and can be tailored to mine any type of 
conceptual relations. It can handle semantic im-
plications that are expressed in very different lin-
guistic expressions and yield systematic output. 
As future work, we will carry out benchmarking 
and testing of the mining of events, both for Eng-
lish and for the other languages in the KYOTO 
project.
Acknowledgements
The KYOTO project is co-funded by EU - FP7 
ICT Work Programme 2007 under Challenge 4 - 
Digital  libraries  and  Content,  Objective  ICT-
2007.4.2  (ICT-2007.4.4):  Intelligent  Contsent 
and Semantics  (challenge 4.2).  The Asian part-
ners from Tapei and Kyoto are funded from na-
tional funds. This work has been also supported 
by  Spanish  project  KNOW-2 (TIN2009-14715-
C04-01).
References
Agirre, E., & Soroa, A. (2009) Personalizing PageR-
ank for Word Sense Disambiguation. Proceedings 
of the 12th EACL, 2009. Athens, Greece. 
Agirre, E., Lopez de Lacalle, O., & Soroa, A. (2009) 
Knowledge-based WSD and specific domains: per-
forming over supervised WSD. Proceedings of IJ-
CAI. Pasadena, USA. http://ixa.si.ehu.es/ukb
?lvez J., Atserias J., Carrera J., Climent S., Laparra 
E., Oliver A. and Rigau G. (2008) Complete and 
Consistent  Annotation of  WordNet  using the Top 
Concept Ontology. Proceedings of LREC'08, Mar-
rakesh, Morroco. 2008.
Appelt Douglas E., Jerry R. Hobbs, John Bear, David 
Israel, Megumi Kameyama, Andrew Kehler, David 
Martin,  Karen Myers and Mabry Tyson. Descrip-
tion of the FASTUS System Used for MUC-6. In 
Proceedings  of  MUC-6,  pages  237?248.  San 
Mateo, Morgan Kaufmann, 1995.
Auer A., C. Bizer, G. Kobilarov, J. Lehmann, R. Cy-
ganiak and Z. Ives. DBpedia: A Nucleus for a Web 
of  Open  Data.  In  Proceedings  of  the
International  Semantic  Web  Conference  (ISWC), 
volume 4825 of  Lecture Notes  in Computer Sci-
ence, pages 722-735. 2007.
Bosma, W., Vossen, P., Soroa, A. , Rigau, G., Tesconi, 
M., Marchetti, A., Monachini, M., & Apiprandi, C. 
(2009) KAF: a generic semantic annotation format. 
In Proceedings of the 5th International Conference 
on Generative Approaches to the Lexicon Sept 17-
19, 2009, Pisa, Italy.
Fellbaum,  C.  (Ed.)  (1998)  WordNet:  An  Electronic 
Lexical Database. Cambridge, MA: MIT Press.
Freitag, D. (1998) Information extraction from html: 
Application  of  a  general  machine  learning  ap-
proach.  In  Proceedings  of  the  Fifteenth  National 
Conference on Artificial Intelligence, 1998.
Gangemi  A.,  Guarino  N.,  Masolo  C.,  Oltramari  A., 
Schneider  L.  (2002)  Sweetening  Ontologies  with 
DOLCE. Proceedings of EKAW. 2002
Ide, N. and L. Romary. 2003. Outline of the inter- na-
tional standard Linguistic Annotation Framework. 
In Proceedings of ACL 2003 Workshop on Lin-
guistic Annotation: Getting the Model Right, pages 
1?5.
Izquierdo R., Su?rez A. & Rigau G. Exploring the 
Automatic Selection of Basic Level Concepts. Pro-
ceedings of RANLP'07, Borovetz, Bulgaria. 
September, 2007.
Masolo, C., Borgo, S., Gangemi, A.,  Guarino, N. & 
Oltramari, A. (2003) WonderWeb Deliverable D18: 
Ontology Library, ISTC-CNR, Trento, Italy.
Mizoguchi R., Sunagawa E., Kozaki K. & Kitamura 
Y. (2007 A Model of Roles within an Ontology De-
velopment  Tool:  Hozo.  Journal  of  Applied  Onto-
logy, Vol.2, No.2, 159-179.
Niles, I. & Pease, A. (2001) Formal Ontology in In-
formation Systems. Proceedings of the internation-
al Conference on Formal Ontology in Information 
Systems ? Vol. 2001 Ogunquit, Maine,  USA
Niles, I. and A. Pease. Linking lexicons and ontolo-
gies:  Mapping  WordNet  to  the  Suggested  Upper 
Merged Ontology. In Proc. IEEE IKE, pages 412?
416, 2003.
Vossen, P. (Ed.) (1998) EuroWordNet: a multilingual 
database  with  lexical  semantic  networks  for 
European Languages. Kluwer, Dordrecht.
Vossen P., W. Bosma, E. Agirre, G. Rigau, A. Soroa 
(2010) A full Knowledge Cycle for Semantic Inter-
operability.  Proceedings  of  the  5th  Joint  ISO-
ACL/SIGSEM Workshop on Interoperable Semant-
ic Annotation, (ICGL 2010) Hong Kong, 2010.
10
