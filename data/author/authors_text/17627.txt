Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170?1179,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Feature Noising for Log-linear Structured Prediction
Sida I. Wang?, Mengqiu Wang?, Stefan Wager?,
Percy Liang, Christopher D. Manning
Department of Computer Science, ?Department of Statistics
Stanford University, Stanford, CA 94305, USA
{sidaw, mengqiu, pliang, manning}@cs.stanford.edu
swager@stanford.edu
Abstract
NLP models have many and sparse features,
and regularization is key for balancing model
overfitting versus underfitting. A recently re-
popularized form of regularization is to gen-
erate fake training data by repeatedly adding
noise to real data. We reinterpret this noising
as an explicit regularizer, and approximate it
with a second-order formula that can be used
during training without actually generating
fake data. We show how to apply this method
to structured prediction using multinomial lo-
gistic regression and linear-chain CRFs. We
tackle the key challenge of developing a dy-
namic program to compute the gradient of the
regularizer efficiently. The regularizer is a
sum over inputs, so we can estimate it more
accurately via a semi-supervised or transduc-
tive extension. Applied to text classification
and NER, our method provides a >1% abso-
lute performance gain over use of standard L2
regularization.
1 Introduction
NLP models often have millions of mainly sparsely
attested features. As a result, balancing overfitting
versus underfitting through good weight regulariza-
tion remains a key issue for achieving optimal per-
formance. Traditionally, L2 or L1 regularization is
employed, but these simple types of regularization
penalize all features in a uniform way without tak-
ing into account the properties of the actual model.
An alternative approach to regularization is to
generate fake training data by adding random noise
to the input features of the original training data. In-
tuitively, this can be thought of as simulating miss-
?Both authors contributed equally to the paper
ing features, whether due to typos or use of a pre-
viously unseen synonym. The effectiveness of this
technique is well-known in machine learning (Abu-
Mostafa, 1990; Burges and Scho?lkopf, 1997; Simard
et al, 2000; Rifai et al, 2011a; van der Maaten
et al, 2013), but working directly with many cor-
rupted copies of a dataset can be computationally
prohibitive. Fortunately, feature noising ideas often
lead to tractable deterministic objectives that can be
optimized directly. Sometimes, training with cor-
rupted features reduces to a special form of reg-
ularization (Matsuoka, 1992; Bishop, 1995; Rifai
et al, 2011b; Wager et al, 2013). For example,
Bishop (1995) showed that training with features
that have been corrupted with additive Gaussian
noise is equivalent to a form of L2 regularization in
the low noise limit. In other cases it is possible to
develop a new objective function by marginalizing
over the artificial noise (Wang and Manning, 2013;
van der Maaten et al, 2013).
The central contribution of this paper is to show
how to efficiently simulate training with artificially
noised features in the context of log-linear struc-
tured prediction, without actually having to gener-
ate noised data. We focus on dropout noise (Hinton
et al, 2012), a recently popularized form of artifi-
cial feature noise where a random subset of features
is omitted independently for each training example.
Dropout and its variants have been shown to out-
perform L2 regularization on various tasks (Hinton
et al, 2012; Wang and Manning, 2013; Wan et al,
2013). Dropout is is similar in spirit to feature bag-
ging in the deliberate removal of features, but per-
forms the removal in a preset way rather than ran-
domly (Bryll et al, 2003; Sutton et al, 2005; Smith
et al, 2005).
1170
Our approach is based on a second-order approx-
imation to feature noising developed among others
by Bishop (1995) and Wager et al (2013), which al-
lows us to convert dropout noise into a form of adap-
tive regularization. This method is suitable for struc-
tured prediction in log-linear models where second
derivatives are computable. In particular, it can be
used for multiclass classification with maximum en-
tropy models (a.k.a., softmax or multinomial logis-
tic regression) and for the sequence models that are
ubiquitous in NLP, via linear chain Conditional Ran-
dom Fields (CRFs).
For linear chain CRFs, we additionally show how
we can use a noising scheme that takes advantage
of the clique structure so that the resulting noising
regularizer can be computed in terms of the pair-
wise marginals. A simple forward-backward-type
dynamic program can then be used to compute the
gradient tractably. For ease of implementation and
scalability to semi-supervised learning, we also out-
line an even faster approximation to the regularizer.
The general approach also works in other clique
structures in addition to the linear chain when the
clique marginals can be computed efficiently.
Finally, we extend feature noising for structured
prediction to a transductive or semi-supervised set-
ting. The regularizer induced by feature noising
is label-independent for log-linear models, and so
we can use unlabeled data to learn a better regu-
larizer. NLP sequence labeling tasks are especially
well suited to a semi-supervised approach, as input
features are numerous but sparse, and labeled data
is expensive to obtain but unlabeled data is abundant
(Li and McCallum, 2005; Jiao et al, 2006).
Wager et al (2013) showed that semi-supervised
dropout training for logistic regression captures a
similar intuition to techniques such as entropy regu-
larization (Grandvalet and Bengio, 2005) and trans-
ductive SVMs (Joachims, 1999), which encourage
confident predictions on the unlabeled data. Semi-
supervised dropout has the advantage of only us-
ing the predicted label probabilities on the unlabeled
data to modulate an L2 regularizer, rather than re-
quiring more heavy-handed modeling of the unla-
beled data as in entropy regularization or expecta-
tion regularization (Mann and McCallum, 2007).
In experimental results, we show that simulated
feature noising gives more than a 1% absolute boost
yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)
Figure 1: An illustration of dropout feature noising
in linear-chain CRFs with only transition features
and node features. The green squares are node fea-
tures f(yt, xt), and the orange squares are edge fea-
tures f(yt?1, yt). Conceptually, given a training ex-
ample, we sample some features to ignore (generate
fake data) and make a parameter update. Our goal is
to train with a roughly equivalent objective, without
actually sampling.
in performance over L2 regularization, on both text
classification and an NER sequence labeling task.
2 Feature Noising Log-linear Models
Consider the standard structured prediction problem
of mapping some input x ? X (e.g., a sentence)
to an output y ? Y (e.g., a tag sequence). Let
f(y, x) ? Rd be the feature vector, ? ? Rd be the
weight vector, and s = (s1, . . . , s|Y|) be a vector of
scores for each output, with sy = f(y, x) ? ?. Now
define a log-linear model:
p(y | x; ?) = exp{sy ?A(s)}, (1)
where A(s) = log
?
y exp{sy} is the log-partition
function. Given an example (x,y), parameter esti-
mation corresponds to choosing ? to maximize p(y |
x; ?).
The key idea behind feature noising is to artifi-
cially corrupt the feature vector f(y, x) randomly
1171
into some f?(y, x) and then maximize the average
log-likelihood of y given these corrupted features?
the motivation is to choose predictors ? that are ro-
bust to noise (missing words for example). Let s?,
p?(y | x; ?) be the randomly perturbed versions cor-
responding to f?(y, x). We will also assume the
feature noising preserves the mean: E[f?(y, x)] =
f(y, x), so that E[s?] = s. This can always be done
by scaling the noised features as described in the list
of noising schemes.
It is useful to view feature noising as a form of
regularization. Since feature noising preserves the
mean, the feature noising objective can be written as
the sum of the original log-likelihood plus the dif-
ference in log-normalization constants:
E[log p?(y | x; ?)] = E[s?y ?A(s?)] (2)
= log p(y | x; ?)?R(?, x), (3)
R(?, x)
def
= E[A(s?)]?A(s). (4)
Since A(?) is convex, R(?, x) is always positive by
Jensen?s inequality and can therefore be interpreted
as a regularizer. Note that R(?, x) is in general non-
convex.
Computing the regularizer (4) requires summing
over all possible noised feature vectors, which can
imply exponential effort in the number of features.
This is intractable even for flat classification. Fol-
lowing Bishop (1995) and Wager et al (2013), we
approximate R(?, x) using a second-order Taylor
expansion, which will allow us to work with only
means and covariances of the noised features. We
take a quadratic approximation of the log-partition
function A(?) of the noised score vector s? around
the the unnoised score vector s:
A(s?) u A(s) +?A(s)>(s?? s) (5)
+
1
2
(s?? s)>?2A(s)(s?? s).
Plugging (5) into (4), we obtain a new regularizer
Rq(?, x), which we will use as an approximation to
R(?, x):
Rq(?, x) =
1
2
E[(s?? s)>?2A(s)(s?? s)] (6)
=
1
2
tr(?2A(s) Cov(s?)). (7)
This expression still has two sources of potential in-
tractability, a sum over an exponential number of
noised score vectors s? and a sum over the |Y| com-
ponents of s?.
Multiclass classification If we assume that the
components of s? are independent, then Cov(s?) ?
R|Y|?|Y| is diagonal, and we have
Rq(?, x) =
1
2
?
y?Y
?y(1? ?y)Var[s?y], (8)
where the mean ?y
def
= p?(y | x) is the model prob-
ability, the variance ?y(1??y) measures model un-
certainty, and
Var[s?y] = ?
>Cov[f?(y, x)]? (9)
measures the uncertainty caused by feature noising.1
The regularizerRq(?, x) involves the product of two
variance terms, the first is non-convex in ? and the
second is quadratic in ?. Note that to reduce the reg-
ularization, we will favor models that (i) predict con-
fidently and (ii) have stable scores in the presence of
feature noise.
For multiclass classification, we can explicitly
sum over each y ? Y to compute the regularizer,
but this will be intractable for structured prediction.
To specialize to multiclass classification for the
moment, let us assume that we have a separate
weight vector for each output y applied to the same
feature vector g(x); that is, the score sy = ?y ? g(x).
Further, assume that the components of the noised
feature vector g?(x) are independent. Then we can
simplify (9) to the following:
Var[s?y] =
?
j
Var[gj(x)]?
2
yj . (10)
Noising schemes We now give some examples of
possible noise schemes for generating f?(y, x) given
the original features f(y, x). This distribution af-
fects the regularization through the variance term
Var[s?y].
? Additive Gaussian:
f?(y, x) = f(y, x) + ?, where ? ?
N (0, ?2Id?d).
1Here, we are using the fact that first and second derivatives
of the log-partition function are the mean and variance.
1172
In this case, the contribution to the regularizer
from noising is Var[s?y] =
?
j ?
2?2yj .
? Dropout:
f?(y, x) = f(y, x)  z, where  takes the el-
ementwise product of two vectors. Here, z is
a vector with independent components which
has zi = 0 with probability ?, zi = 11?? with
probability 1 ? ?. In this case, Var[s?y] =
?
j
gj(x)2?
1?? ?
2
yj .
? Multiplicative Gaussian:
f?(y, x) = f(y, x)  (1 + ?), where
? ? N (0, ?2Id?d). Here, Var[s?y] =?
j gj(x)
2?2?2yj . Note that under our second-
order approximation Rq(?, x), the multiplica-
tive Gaussian and dropout schemes are equiva-
lent, but they differ under the original regular-
izer R(?, x).
2.1 Semi-supervised learning
A key observation (Wager et al, 2013) is that
the noising regularizer R (8), while involving a
sum over examples, is independent of the output
y. This suggests estimating R using unlabeled
data. Specifically, if we have n labeled examples
D = {x1, x2, . . . , xn} and m unlabeled examples
Dunlabeled = {u1, u2, . . . , un}, then we can define a
regularizer that is a linear combination the regular-
izer estimated on both datasets, with ? tuning the
tradeoff between the two:
R?(?,D,Dunlabeled) (11)
def
=
n
n+ ?m
( n?
i=1
R(?, xi) + ?
m?
i=1
R(?, ui)
)
.
3 Feature Noising in Linear-Chain CRFs
So far, we have developed a regularizer that works
for all log-linear models, but?in its current form?
is only practical for multiclass classification. We
now exploit the decomposable structure in CRFs to
define a new noising scheme which does not require
us to explicitly sum over all possible outputs y ? Y .
The key idea will be to noise each local feature vec-
tor (which implicitly affects many y) rather than
noise each y independently.
Assume that the output y = (y1, . . . , yT ) is a se-
quence of T tags. In linear chain CRFs, the feature
vector f decomposes into a sum of local feature vec-
tors gt:
f(y, x) =
T?
t=1
gt(yt?1, yt, x), (12)
where gt(a, b, x) is defined on a pair of consecutive
tags a, b for positions t? 1 and t.
Rather than working with a score sy for each
y ? Y , we define a collection of local scores
s = {sa,b,t}, for each tag pair (a, b) and posi-
tion t = 1, . . . , T . We consider noising schemes
which independently set g?t(a, b, x) for each a, b, t.
Let s? = {s?a,b,t} be the corresponding collection of
noised scores.
We can write the log-partition function of these
local scores as follows:
A(s) = log
?
y?Y
exp
{
T?
t=1
syt?1,yt,t
}
. (13)
The first derivative yields the edge marginals under
the model, ?a,b,t = p?(yt?1 = a, yt = b | x), and
the diagonal elements of the Hessian ?2A(s) yield
the marginal variances.
Now, following (7) and (8), we obtain the follow-
ing regularizer:
Rq(?, x) =
1
2
?
a,b,t
?a,b,t(1? ?a,b,t)Var[s?a,b,t],
(14)
where ?a,b,t(1? ?a,b,t) measures model uncertainty
about edge marginals, and Var[s?a,b,t] is simply the
uncertainty due to noising. Again, minimizing the
regularizer means making confident predictions and
having stable scores under feature noise.
Computing partial derivatives So far, we have
defined the regularizer Rq(?, x) based on feature
noising. In order to minimize Rq(?, x), we need to
take its derivative.
First, note that log?a,b,t is the difference of a re-
stricted log-partition function and the log-partition
function. So again by properties of its first deriva-
tive, we have:
? log?a,b,t = Ep?(y|x,yt?1=a,yt=b)[f(y, x)] (15)
? Ep?(y|x)[f(y, x)].
1173
Using the fact that ??a,b,t = ?a,b,t? log?a,b,t and
the fact that Var[s?a,b,t] is a quadratic function in ?,
we can simply apply the product rule to derive the
final gradient?Rq(?, x).
3.1 A Dynamic Program for the Conditional
Expectation
A naive computation of the gradient ?Rq(?, x) re-
quires a full forward-backward pass to compute
Ep?(y|yt?1=a,yt=b,x)[f(y, x)] for each tag pair (a, b)
and position t, resulting in a O(K4T 2) time algo-
rithm.
In this section, we reduce the running time to
O(K2T ) using a more intricate dynamic program.
By the Markov property of the CRF, y1:t?2 only de-
pends on (yt?1, yt) through yt?1 and yt+1:T only
depends on (yt?1, yt) through yt.
First, it will be convenient to define the partial
sum of the local feature vector from positions i to
j as follows:
Gi:j =
j?
t=i
gt(yt?1, yt, x). (16)
Consider the task of computing the feature expecta-
tion Ep?(y|yt?1=a,yt=b)[f(y, x)] for a fixed (a, b, t).
We can expand this quantity into
?
y:yt?1=a,yt=b
p?(y?(t?1:t) | yt?1 = a, yt = b)G1:T .
Conditioning on yt?1, yt decomposes the sum into
three pieces:
?
y:yt?1=a,yt=b
[gt(yt?1 = a, yt = b, x) + F
a
t +B
b
t ],
where
F at =
?
y1:t?2
p?(y1:t?2 | yt?1 = a)G1:t?1, (17)
Bbt =
?
yt+1:T
p?(yt+1:T | yt = b)Gt+1:T , (18)
are the expected feature vectors summed over the
prefix and suffix of the tag sequence, respectively.
Note that F at and B
b
t are analogous to the forward
and backward messages of standard CRF inference,
with the exception that they are vectors rather than
scalars.
We can compute these messages recursively in the
standard way. The forward recurrence is
F at =
?
b
p?(yt?2 = b | yt?1 = a)
[
gt(yt?2 = b, yt?1 = a, x) + F
b
t?1
]
,
and a similar recurrence holds for the backward mes-
sages Bbt .
Running the resulting dynamic program takes
O(K2Tq) time and requires O(KTq) storage,
where K is the number of tags, T is the sequence
length and q is the number of active features. Note
that this is the same order of dependence as normal
CRF training, but there is an additional dependence
on the number of active features q, which makes
training slower.
4 Fast Gradient Computations
In this section, we provide two ways to further im-
prove the efficiency of the gradient calculation based
on ignoring long-range interactions and based on ex-
ploiting feature sparsity.
4.1 Exploiting Feature Sparsity and
Co-occurrence
In each forward-backward pass over a training ex-
ample, we need to compute the conditional ex-
pectations for all features active in that example.
Naively applying the dynamic program in Section 3
is O(K2T ) for each active feature. The total com-
plexity has to factor in the number of active fea-
tures, q. Although q only scales linearly with sen-
tence length, in practice this number could get large
pretty quickly. For example, in the NER tagging ex-
periments (cf. Section 5), the average number of
active features per token is about 20, which means
q ' 20T ; this term quickly dominates the compu-
tational costs. Fortunately, in sequence tagging and
other NLP tasks, the majority of features are sparse
and they often co-occur. That is, some of the ac-
tive features would fire and only fire at the same lo-
cations in a given sequence. This happens when a
particular token triggers multiple rare features.
We observe that all indicator features that only
fired once at position t have the same conditional ex-
pectations (and model expectations). As a result, we
can collapse such a group of features into a single
1174
feature as a preprocessing step to avoid computing
identical expectations for each of the features. Do-
ing so on the same NER tagging experiments cuts
down q/T from 20 to less than 5, and gives us a 4
times speed up at no loss of accuracy. The exact
same trick is applicable to the general CRF gradient
computation as well and gives similar speedup.
4.2 Short-range interactions
It is also possible to speed up the method by re-
sorting to approximate gradients. In our case, the
dynamic program from Section 3 together with the
trick described above ran in a manageable amount
of time. The techniques developed here, however,
could prove to be useful on larger tasks.
Let us rewrite the quantity we want to compute
slightly differently (again, for all a, b, t):
T?
i=1
Ep?(y|x,yt?1=a,yt=b)[gi(yi?1, yi, x)]. (19)
The intuition is that conditioned on yt?1, yt, the
terms gi(yi?1, yi, x) where i is far from t will be
close to Ep?(y|x)[gi(yi?1, yi, x)].
This motivates replacing the former with the latter
whenever |i? k| ? r where r is some window size.
This approximation results in an expression which
only has to consider the sum of the local feature vec-
tors from i?r to i+r, which is captured byGi?r:i+r:
Ep?(y|yt?1=a,yt=b,x)[f(y, x)]? Ep?(y|x)[f(y, x)]
? Ep?(y|yt?1=a,yt=b,x)[Gt?r:t+r] (20)
? Ep?(y|x)[Gt?r:t+r].
We can further approximate this last expression by
letting r = 0, obtaining:
gt(a, b, x)? Ep?(y|x)[gt(yt?1, yt, x)]. (21)
The second expectation can be computed from the
edge marginals.
The accuracy of this approximation hinges on the
lack of long range dependencies. Equation (21)
shows the case of r = 0; this takes almost no addi-
tional effort to compute. However, for some of our
experiments, we observed a 20% difference with the
real derivative. For r > 0, the computational savings
are more limited, but the bounded-window method
is easier to implement.
Dataset q d K Ntrain Ntest
CoNLL 20 437906 5 204567 46666
SANCL 5 679959 12 761738 82405
20news 81 62061 20 15935 3993
RCV14 76 29992 4 9625/2 9625/2
R21578 47 18933 65 5946 2347
TDT2 130 36771 30 9394/2 9394/2
Table 1: Description of datasets. q: average number
of non-zero features per example, d: total number
of features, K: number of classes to predict, Ntrain:
number of training examples, Ntest: number of test
examples.
5 Experiments
We show experimental results on the CoNLL-2003
Named Entity Recognition (NER) task, the SANCL
Part-of-speech (POS) tagging task, and several doc-
ument classification tasks.2 The datasets used are
described in Table 1. We used standard splits when-
ever available; otherwise we split the data at ran-
dom into a test set and a train set of equal sizes
(RCV14, TDT2). CoNLL has a development set
of size 51578, which we used to tune regulariza-
tion parameters. The SANCL test set is divided into
3 genres, namely answers, newsgroups, and
reviews, each of which has a corresponding de-
velopment set.3
5.1 Multiclass Classification
We begin by testing our regularizer in the simple
case of classification where Y = {1, 2, . . . ,K} for
K classes. We examine the performance of the nois-
ing regularizer in both the fully supervised setting as
well as the transductive learning setting.
In the transductive learning setting, the learner
is allowed to inspect the test features at train time
(without the labels). We used the method described
in Section 2.1 for transductive dropout.
2The document classification data are available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvmtools/datasets and http://www.cad.
zju.edu.cn/home/dengcai/Data/TextData.html
3The SANCL dataset has two additional genres?emails and
weblogs?that we did not use, as we did not have access to
development sets for these genres.
1175
Dataset K None L2 Drop +Test
CoNLL 5 78.03 80.12 80.90 81.66
20news 20 81.44 82.19 83.37 84.71
RCV14 4 95.76 95.90 96.03 96.11
R21578 65 92.24 92.24 92.24 92.58
TDT2 30 97.74 97.91 98.00 98.12
Table 2: Classification performance and transduc-
tive learning results on some standard datasets.
None: use no regularization, Drop: quadratic ap-
proximation to the dropout noise (8), +Test: also use
the test set to estimate the noising regularizer (11).
5.1.1 Semi-supervised Learning with Feature
Noising
In the transductive setting, we used test data
(without labels) to learn a better regularizer. As an
alternative, we could also use unlabeled data in place
of the test data to accomplish a similar goal; this
leads to a semi-supervised setting.
To test the semi-supervised idea, we use the same
datasets as above. We split each dataset evenly into
3 thirds that we use as a training set, a test set and an
unlabeled dataset. Results are given in Table 3.
In most cases, our semi-supervised accuracies are
lower than the transductive accuracies given in Table
2; this is normal in our setup, because we used less
labeled data to train the semi-supervised classifier
than the transductive one.4
5.1.2 The Second-Order Approximation
The results reported above all rely on the ap-
proximate dropout regularizer (8) that is based on a
second-order Taylor expansion. To test the validity
of this approximation we compare it to the Gaussian
method developed by Wang and Manning (2013) on
a two-class classification task.
We use the 20-newsgroups alt.atheism vs
soc.religion.christian classification task;
results are shown in Figure 2. There are 1427 exam-
4The CoNNL results look somewhat surprising, as the semi-
supervised results are better than the transductive ones. The
reason for this is that the original CoNLL test set came from a
different distributions than the training set, and this made the
task more difficult. Meanwhile, in our semi-supervised experi-
ment, the test and train sets are drawn from the same distribu-
tion and so our semi-supervised task is actually easier than the
original one.
Dataset K L2 Drop +Unlabeled
CoNLL 5 91.46 91.81 92.02
20news 20 76.55 79.07 80.47
RCV14 4 94.76 94.79 95.16
R21578 65 90.67 91.24 90.30
TDT2 30 97.34 97.54 97.89
Table 3: Semisupervised learning results on some
standard datasets. A third (33%) of the full dataset
was used for training, a third for testing, and the rest
as unlabeled.
10?6 10?4 10?2 100 102
0.78
0.8
0.82
0.84
0.86
0.88
0.9
L2 regularization strength (?)
Ac
cu
rac
y
 
 
L2 only
L2+Gaussian dropout
L2+Quadratic dropout
Figure 2: Effect of ? in ????22 on the testset perfor-
mance. Plotted is the test set accuracy with logis-
tic regression as a function of ? for the L2 regular-
izer, Gaussian dropout (Wang and Manning, 2013)
+ additional L2, and quadratic dropout (8) + L2 de-
scribed in this paper. The default noising regularizer
is quite good, and additional L2 does not help. No-
tice that no choice of ? in L2 can help us combat
overfitting as effectively as (8) without underfitting.
ples with 22178 features, split evenly and randomly
into a training set and a test set.
Over a broad range of ? values, we find that
dropout plus L2 regularization performs far better
than using just L2 regularization for any value of
?. We see that Gaussian dropout appears to per-
form slightly better than the quadratic approxima-
tion discussed in this paper. However, our quadratic
approximation extends easily to the multiclass case
and to structured prediction in general, while Gaus-
sian dropout does not. Thus, it appears that our ap-
proximation presents a reasonable trade-off between
1176
computational efficiency and prediction accuracy.
5.2 CRF Experiments
We evaluate the quadratic dropout regularizer in
linear-chain CRFs on two sequence tagging tasks:
the CoNLL 2003 NER shared task (Tjong Kim Sang
and De Meulder, 2003) and the SANCL 2012 POS
tagging task (Petrov and McDonald, 2012) .
The standard CoNLL-2003 English shared task
benchmark dataset (Tjong Kim Sang and De Meul-
der, 2003) is a collection of documents from
Reuters newswire articles, annotated with four en-
tity types: Person, Location, Organization, and
Miscellaneous. We predicted the label sequence
Y = {LOC, MISC, ORG, PER, O}T without con-
sidering the BIO tags.
For training the CRF model, we used a compre-
hensive set of features from Finkel et al (2005) that
gives state-of-the-art results on this task. A total
number of 437906 features were generated on the
CoNLL-2003 training dataset. The most important
features are:
? The word, word shape, and letter n-grams (up to
6gram) at current position
? The prediction, word, and word shape of the pre-
vious and next position
? Previous word shape in conjunction with current
word shape
? Disjunctive word set of the previous and next 4
positions
? Capitalization pattern in a 3 word window
? Previous two words in conjunction with the word
shape of the previous word
? The current word matched against a list of name
titles (e.g., Mr., Mrs.)
The F?=1 results are summarized in Table 4. We
obtain a 1.6% and 1.1% absolute gain on the test
and dev set, respectively. Detailed results are bro-
ken down by precision and recall for each tag and are
shown in Table 6. These improvements are signifi-
cant at the 0.1% level according to the paired boot-
strap resampling method of 2000 iterations (Efron
and Tibshirani, 1993).
For the SANCL (Petrov and McDonald, 2012)
POS tagging task, we used the same CRF framework
with a much simpler set of features
? word unigrams: w?1, w0, w1
? word bigram: (w?1, w0) and (w0, w1)
F?=1 None L2 Drop
Dev 89.40 90.73 91.86
Test 84.67 85.82 87.42
Table 4: CoNLL summary of results. None: no reg-
ularization, Drop: quadratic dropout regularization
(14) described in this paper.
F?=1 None L2 Drop
newsgroups
Dev 91.34 91.34 91.47
Test 91.44 91.44 91.81
reviews
Dev 91.97 91.95 92.10
Test 90.70 90.67 91.07
answers
Dev 90.78 90.79 90.70
Test 91.00 90.99 91.09
Table 5: SANCL POS tagging F?=1 scores for the 3
official evaluation sets.
We obtained a small but consistent improvement
using the quadratic dropout regularizer in (14) over
the L2-regularized CRFs baseline.
Although the difference on SANCL is small,
the performance differences on the test sets of
reviews and newsgroups are statistically sig-
nificant at the 0.1% level. This is also interesting
because here is a situation where the features are ex-
tremely sparse, L2 regularization gave no improve-
ment, and where regularization overall matters less.
6 Conclusion
We have presented a new regularizer for learning
log-linear models such as multiclass logistic regres-
sion and conditional random fields. This regularizer
is based on a second-order approximation of fea-
ture noising schemes, and attempts to favor mod-
els that predict confidently and are robust to noise
in the data. In order to apply our method to CRFs,
we tackle the key challenge of dealing with feature
correlations that arise in the structured prediction
setting in several ways. In addition, we show that
the regularizer can be applied naturally in the semi-
supervised setting. Finally, we applied our method
to a range of different datasets and demonstrate con-
sistent gains over standard L2 regularization. Inves-
1177
Precision Recall F?=1
LOC 91.47% 91.12% 91.29
MISC 88.77% 81.07% 84.75
ORG 85.22% 84.08% 84.65
PER 92.12% 93.97% 93.04
Overall 89.84% 88.97% 89.40
(a) CoNLL dev. set with no regularization
Precision Recall F?=1
92.05% 92.84% 92.44
90.51% 83.52% 86.87
88.35% 85.23% 86.76
93.12% 94.19% 93.65
91.36% 90.11% 90.73
(b) CoNLL dev. set with L2 reg-
ularization
Precision Recall F?=1
93.59% 92.69% 93.14
93.99% 81.47% 87.28
92.48% 84.61% 88.37
94.81% 95.11% 94.96
93.85% 89.96% 91.86
(c) CoNLL dev. set with dropout
regularization
Tag Precision Recall F?=1
LOC 87.33% 84.47% 85.87
MISC 78.93% 77.12% 78.02
ORG 78.70% 79.49% 79.09
PER 88.82% 93.11% 90.92
Overall 84.28% 85.06% 84.67
(d) CoNLL test set with no regularization
Precision Recall F?=1
87.96% 86.13% 87.03
77.53% 79.30% 78.41
81.30% 80.49% 80.89
90.30% 93.33% 91.79
85.57% 86.08% 85.82
(e) CoNLL test set with L2 reg-
ularization
Precision Recall F?=1
86.26% 87.74% 86.99
81.52% 77.34% 79.37
88.29% 81.89% 84.97
92.15% 92.68% 92.41
88.40% 86.45% 87.42
(f) CoNLL test set with dropout
regularization
Table 6: CoNLL NER results broken down by tags and by precision, recall, and F?=1. Top: development
set, bottom: test set performance.
tigating how to better optimize this non-convex reg-
ularizer online and convincingly scale it to the semi-
supervised setting seem to be promising future di-
rections.
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their comments. We gratefully acknowl-
edge the support of the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, or the US government. S. Wager is
supported by a BC and EJ Eaves SGF Fellowship.
References
Yaser S. Abu-Mostafa. 1990. Learning from hints in
neural networks. Journal of Complexity, 6(2):192?
198.
Chris M. Bishop. 1995. Training with noise is equiva-
lent to Tikhonov regularization. Neural computation,
7(1):108?116.
Robert Bryll, Ricardo Gutierrez-Osuna, and Francis
Quek. 2003. Attribute bagging: improving accuracy
of classifier ensembles by using random feature sub-
sets. Pattern recognition, 36(6):1291?1302.
Chris J.C. Burges and Bernhard Scho?lkopf. 1997. Im-
proving the accuracy and speed of support vector ma-
chines. In Advances in Neural Information Processing
Systems, pages 375?381.
Brad Efron and Robert Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd annual meeting of
the Association for Computational Linguistics, pages
363?370.
Yves Grandvalet and Yoshua Bengio. 2005. Entropy
regularization. In Semi-Supervised Learning, United
Kingdom. Springer.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R. Salakhutdinov.
2012. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved se-
quence segmentation and labeling. In Proceedings of
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 209?216.
Thorsten Joachims. 1999. Transductive inference for
1178
text classification using support vector machines. In
Proceedings of the International Conference on Ma-
chine Learning, pages 200?209.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Arti-
ficial Intelligence - Volume 2, AAAI?05, pages 813?
818.
Gideon S. Mann and Andrew McCallum. 2007. Sim-
ple, robust, scalable semi-supervised learning via ex-
pectation regularization. In Proceedings of the Inter-
national Conference on Machine Learning.
Kiyotoshi Matsuoka. 1992. Noise injection into inputs
in back-propagation learning. Systems, Man and Cy-
bernetics, IEEE Transactions on, 22(3):436?440.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Ben-
gio, and Xavier Muller. 2011a. The manifold tangent
classifier. Advances in Neural Information Processing
Systems, 24:2294?2302.
Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal
Vincent. 2011b. Adding noise to the input of a model
trained with a regularized objective. arXiv preprint
arXiv:1104.3250.
Patrice Y. Simard, Yann A. Le Cun, John S. Denker, and
Bernard Victorri. 2000. Transformation invariance in
pattern recognition: Tangent distance and propagation.
International Journal of Imaging Systems and Tech-
nology, 11(3):181?197.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages 18?
25. Association for Computational Linguistics.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2005. Feature bagging: Preventing weight un-
dertraining in structured discriminative learning. Cen-
ter for Intelligent Information Retrieval, U. of Mas-
sachusetts.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, CONLL ?03,
pages 142?147.
Laurens van der Maaten, Minmin Chen, Stephen Tyree,
and Kilian Q. Weinberger. 2013. Learning with
marginalized corrupted features. In Proceedings of the
International Conference on Machine Learning.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. arXiv
preprint:1307.1493.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and
Rob Fergus. 2013. Regularization of neural networks
using dropconnect. In Proceedings of the Interna-
tional Conference on Machine learning.
Sida Wang and Christopher D. Manning. 2013. Fast
dropout training. In Proceedings of the International
Conference on Machine Learning.
1179
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 90?94,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Baselines and Bigrams: Simple, Good Sentiment and Topic Classification
Sida Wang and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{sidaw,manning}@stanford.edu
Abstract
Variants of Naive Bayes (NB) and Support
Vector Machines (SVM) are often used as
baseline methods for text classification, but
their performance varies greatly depending on
the model variant, features used and task/
dataset. We show that: (i) the inclusion of
word bigram features gives consistent gains on
sentiment analysis tasks; (ii) for short snippet
sentiment tasks, NB actually does better than
SVMs (while for longer documents the oppo-
site result holds); (iii) a simple but novel SVM
variant using NB log-count ratios as feature
values consistently performs well across tasks
and datasets. Based on these observations, we
identify simple NB and SVM variants which
outperform most published results on senti-
ment analysis datasets, sometimes providing
a new state-of-the-art performance level.
1 Introduction
Naive Bayes (NB) and Support Vector Machine
(SVM) models are often used as baselines for other
methods in text categorization and sentiment analy-
sis research. However, their performance varies sig-
nificantly depending on which variant, features and
datasets are used. We show that researchers have
not paid sufficient attention to these model selec-
tion issues. Indeed, we show that the better variants
often outperform recently published state-of-the-art
methods on many datasets. We attempt to catego-
rize which method, which variants and which fea-
tures perform better under which circumstances.
First, we make an important distinction between
sentiment classification and topical text classifica-
tion. We show that the usefulness of bigram features
in bag of features sentiment classification has been
underappreciated, perhaps because their usefulness
is more of a mixed bag for topical text classifica-
tion tasks. We then distinguish between short snip-
pet sentiment tasks and longer reviews, showing that
for the former, NB outperforms SVMs. Contrary to
claims in the literature, we show that bag of features
models are still strong performers on snippet senti-
ment classification tasks, with NB models generally
outperforming the sophisticated, structure-sensitive
models explored in recent work. Furthermore, by
combining generative and discriminative classifiers,
we present a simple model variant where an SVM is
built over NB log-count ratios as feature values, and
show that it is a strong and robust performer over all
the presented tasks. Finally, we confirm the well-
known result that MNB is normally better and more
stable than multivariate Bernoulli NB, and the in-
creasingly known result that binarized MNB is bet-
ter than standard MNB. The code and datasets to
reproduce the results in this paper are publicly avail-
able. 1
2 The Methods
We formulate our main model variants as linear clas-
sifiers, where the prediction for test case k is
y(k) = sign(wTx(k) + b) (1)
Details of the equivalent probabilistic formulations
are presented in (McCallum and Nigam, 1998).
Let f (i) ? R|V | be the feature count vector for
training case i with label y(i) ? {?1, 1}. V is the
1http://www.stanford.edu/?sidaw
90
set of features, and f (i)j represents the number of oc-
currences of feature Vj in training case i. Define
the count vectors as p = ? +
?
i:y(i)=1 f
(i) and
q = ? +
?
i:y(i)=?1 f
(i) for smoothing parameter
?. The log-count ratio is:
r = log
(
p/||p||1
q/||q||1
)
(2)
2.1 Multinomial Naive Bayes (MNB)
In MNB, x(k) = f (k), w = r and b = log(N+/N?).
N+, N? are the number of positive and negative
training cases. However, as in (Metsis et al, 2006),
we find that binarizing f (k) is better. We take x(k) =
f? (k) = 1{f (k) > 0}, where 1 is the indicator func-
tion. p?, q?, r? are calculated using f? (i) instead of f (i)
in (2).
2.2 Support Vector Machine (SVM)
For the SVM, x(k) = f? (k), and w, b are obtained by
minimizing
wTw +C
?
i
max(0, 1? y(i)(wT f? (i) + b))2 (3)
We find this L2-regularized L2-loss SVM to work
the best and L1-loss SVM to be less stable. The LI-
BLINEAR library (Fan et al, 2008) is used here.
2.3 SVM with NB features (NBSVM)
Otherwise identical to the SVM, except we use
x(k) = f? (k), where f? (k) = r? ? f? (k) is the elemen-
twise product. While this does very well for long
documents, we find that an interpolation between
MNB and SVM performs excellently for all docu-
ments and we report results using this model:
w? = (1? ?)w? + ?w (4)
where w? = ||w||1/|V | is the mean magnitude of w,
and ? ? [0, 1] is the interpolation parameter. This
interpolation can be seen as a form of regularization:
trust NB unless the SVM is very confident.
3 Datasets and Task
We compare with published results on the following
datasets. Detailed statistics are shown in table 1.
RT-s: Short movie reviews dataset containing one
sentence per review (Pang and Lee, 2005).
Dataset (N+, N?) l CV |V | ?
RT-s (5331,5331) 21 10 21K 0.8
CR (2406,1366) 20 10 5713 1.3
MPQA (3316,7308) 3 10 6299 0.8
Subj. (5000,5000) 24 10 24K 0.8
RT-2k (1000,1000) 787 10 51K 1.5
IMDB (25k,25k) 231 N 392K 0.4
AthR (799,628) 345 N 22K 2.9
XGraph (980,973) 261 N 32K 1.8
BbCrypt (992,995) 269 N 25K 0.5
Table 1: Dataset statistics. (N+, N?): number of
positive and negative examples. l: average num-
ber of words per example. CV: number of cross-
validation splits, or N for train/test split. |V |: the
vocabulary size. ?: upper-bounds of the differences
required to be statistically significant at the p < 0.05
level.
CR: Customer review dataset (Hu and Liu, 2004)
processed like in (Nakagawa et al, 2010).2
MPQA: Opinion polarity subtask of the MPQA
dataset (Wiebe et al, 2005).3
Subj: The subjectivity dataset with subjective re-
views and objective plot summaries (Pang and
Lee, 2004).
RT-2k: The standard 2000 full-length movie re-
view dataset (Pang and Lee, 2004).
IMDB: A large movie review dataset with 50k full-
length reviews (Maas et al, 2011).4
AthR, XGraph, BbCrypt: Classify pairs of
newsgroups in the 20-newsgroups dataset with
all headers stripped off (the third (18828) ver-
sion5), namely: alt.atheism vs. religion.misc,
comp.windows.x vs. comp.graphics, and
rec.sport.baseball vs. sci.crypt, respectively.
4 Experiments and Results
4.1 Experimental setup
We use the provided tokenizations when they exist.
If not, we split at spaces for unigrams, and we filter
out anything that is not [A-Za-z] for bigrams. We do
2http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html
3http://www.cs.pitt.edu/mpqa/
4http://ai.stanford.edu/?amaas/data/sentiment
5http://people.csail.mit.edu/jrennie/20Newsgroups
91
not use stopwords, lexicons or other resources. All
results reported use ? = 1, C = 1, ? = 0.25 for
NBSVM, and C = 0.1 for SVM.
For comparison with other published results, we
use either 10-fold cross-validation or train/test split
depending on what is standard for the dataset. The
CV column of table 1 specifies what is used. The
standard splits are used when they are available.
The approximate upper-bounds on the difference re-
quired to be statistically significant at the p < 0.05
level are listed in table 1, column ?.
4.2 MNB is better at snippets
(Moilanen and Pulman, 2007) suggests that while
?statistical methods? work well for datasets with
hundreds of words in each example, they cannot
handle snippets datasets and some rule-based sys-
tem is necessary. Supporting this claim are examples
such as not an inhumane monster6, or killing cancer
that express an overall positive sentiment with nega-
tive words.
Some previous work on classifying snippets in-
clude using pre-defined polarity reversing rules
(Moilanen and Pulman, 2007), and learning com-
plex models on parse trees such as in (Nakagawa et
al., 2010) and (Socher et al, 2011). These works
seem promising as they perform better than many
sophisticated, rule-based methods used as baselines
in (Nakagawa et al, 2010). However, we find that
several NB/SVM variants in fact do better than these
state-of-the-art methods, even compared to meth-
ods that use lexicons, reversal rules, or unsupervised
pretraining. The results are in table 2.
Our SVM-uni results are consistent with BoF-
noDic and BoF-w/Rev used in (Nakagawa et al,
2010) and BoWSVM in (Pang and Lee, 2004).
(Nakagawa et al, 2010) used a SVM with second-
order polynomial kernel and additional features.
With the only exception being MPQA, MNB per-
formed better than SVM in all cases.7
Table 2 show that a linear SVM is a weak baseline
for snippets. MNB (and NBSVM) are much better
on sentiment snippet tasks, and usually better than
other published results. Thus, we find the hypothe-
6A positive example from the RT-s dataset.
7We are unsure, but feel that MPQA may be less discrimi-
native, since the documents are extremely short and all methods
perform similarly.
Method RT-s MPQA CR Subj.
MNB-uni 77.9 85.3 79.8 92.6
MNB-bi 79.0 86.3 80.0 93.6
SVM-uni 76.2 86.1 79.0 90.8
SVM-bi 77.7 86.7 80.8 91.7
NBSVM-uni 78.1 85.3 80.5 92.4
NBSVM-bi 79.4 86.3 81.8 93.2
RAE 76.8 85.7 ? ?
RAE-pretrain 77.7 86.4 ? ?
Voting-w/Rev. 63.1 81.7 74.2 ?
Rule 62.9 81.8 74.3 ?
BoF-noDic. 75.7 81.8 79.3 ?
BoF-w/Rev. 76.4 84.1 81.4 ?
Tree-CRF 77.3 86.1 81.4 ?
BoWSVM ? ? ? 90.0
Table 2: Results for snippets datasets. Tree-CRF:
(Nakagawa et al, 2010) RAE: Recursive Autoen-
coders (Socher et al, 2011). RAE-pretrain: train on
Wikipedia (Collobert and Weston, 2008). ?Voting?
and ?Rule?: use a sentiment lexicon and hard-coded
reversal rules. ?w/Rev?: ?the polarities of phrases
which have odd numbers of reversal phrases in their
ancestors?. The top 3 methods are in bold and the
best is also underlined.
sis that rule-based systems have an edge for snippet
datasets to be false. MNB is stronger for snippets
than for longer documents. While (Ng and Jordan,
2002) showed that NB is better than SVM/logistic
regression (LR) with few training cases, we show
that MNB is also better with short documents. In
contrast to their result that an SVM usually beats
NB when it has more than 30?50 training cases, we
show that MNB is still better on snippets even with
relatively large training sets (9k cases).
4.3 SVM is better at full-length reviews
As seen in table 1, the RT-2k and IMDB datasets
contain much longer reviews. Compared to the ex-
cellent performance of MNB on snippet datasets,
the many poor assumptions of MNB pointed out
in (Rennie et al, 2003) become more crippling for
these longer documents. SVM is much stronger
than MNB for the 2 full-length sentiment analy-
sis tasks, but still worse than some other published
results. However, NBSVM either exceeds or ap-
proaches previous state-of-the art methods, even the
92
Our results RT-2k IMDB Subj.
MNB-uni 83.45 83.55 92.58
MNB-bi 85.85 86.59 93.56
SVM-uni 86.25 86.95 90.84
SVM-bi 87.40 89.16 91.74
NBSVM-uni 87.80 88.29 92.40
NBSVM-bi 89.45 91.22 93.18
BoW (bnc) 85.45 87.8 87.77
BoW (b?t?c) 85.8 88.23 85.65
LDA 66.7 67.42 66.65
Full+BoW 87.85 88.33 88.45
Full+Unlab?d+BoW 88.9 88.89 88.13
BoWSVM 87.15 ? 90.00
Valence Shifter 86.2 ? ?
tf.?idf 88.1 ? ?
Appr. Taxonomy 90.20 ? ?
WRRBM ? 87.42 ?
WRRBM + BoW(bnc) ? 89.23 ?
Table 3: Results for long reviews (RT-2k and
IMDB). The snippet dataset Subj. is also included
for comparison. Results in rows 7-11 are from
(Maas et al, 2011). BoW: linear SVM on bag of
words features. bnc: binary, no idf, cosine nor-
malization. ?t?: smoothed delta idf. Full: the
full model. Unlab?d: additional unlabeled data.
BoWSVM: bag of words SVM used in (Pang and
Lee, 2004). Valence Shifter: (Kennedy and Inkpen,
2006). tf.?idf: (Martineau and Finin, 2009). Ap-
praisal Taxonomy: (Whitelaw et al, 2005). WR-
RBM: Word Representation Restricted Boltzmann
Machine (Dahl et al, 2012).
ones that use additional data. These sentiment anal-
ysis results are shown in table 3.
4.4 Benefits of bigrams depends on the task
Word bigram features are not that commonly used
in text classification tasks (hence, the usual term,
?bag of words?), probably due to their having mixed
and overall limited utility in topical text classifica-
tion tasks, as seen in table 4. This likely reflects that
certain topic keywords are indicative alone. How-
ever, in both tables 2 and 3, adding bigrams always
improved the performance, and often gives better
results than previously published.8 This presum-
ably reflects that in sentiment classification there are
8However, adding trigrams hurts slightly.
Method AthR XGraph BbCrypt
MNB-uni 85.0 90.0 99.3
MNB-bi 85.1 +0.1 91.2 +1.2 99.4 +0.1
SVM-uni 82.6 85.1 98.3
SVM-bi 83.7 +1.1 86.2 +0.9 97.7 ?0.5
NBSVM-uni 87.9 91.2 99.7
NBSVM-bi 87.7 ?0.2 90.7 ?0.5 99.5 ?0.2
ActiveSVM ? 90 99
DiscLDA 83 ? ?
Table 4: On 3 20-newsgroup subtasks, we compare
to DiscLDA (Lacoste-Julien et al, 2008) and Ac-
tiveSVM (Schohn and Cohn, 2000).
much bigger gains from bigrams, because they can
capture modified verbs and nouns.
4.5 NBSVM is a robust performer
NBSVM performs well on snippets and longer doc-
uments, for sentiment, topic and subjectivity clas-
sification, and is often better than previously pub-
lished results. Therefore, NBSVM seems to be an
appropriate and very strong baseline for sophisti-
cated methods aiming to beat a bag of features.
One disadvantage of NBSVM is having the inter-
polation parameter ?. The performance on longer
documents is virtually identical (within 0.1%) for
? ? [?, 1], while ? = ? is on average 0.5% better
for snippets than ? = 1. Using ? ? [?,?] makes
the NBSVM more robust than more extreme values.
4.6 Other results
Multivariate Bernoulli NB (BNB) usually performs
worse than MNB. The only place where BNB is
comparable to MNB is for snippet tasks using only
unigrams. In general, BNB is less stable than MNB
and performs up to 10% worse. Therefore, bench-
marking against BNB is untrustworthy, cf. (McCal-
lum and Nigam, 1998).
For MNB and NBSVM, using the binarized MNB
f? is slightly better (by 1%) than using the raw count
feature f . The difference is negligible for snippets.
Using logistic regression in place of SVM gives
similar results, and some of our results can be
viewed more generally in terms of generative vs.
discriminative learning.
93
References
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of ICML.
George E. Dahl, Ryan P. Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. arXiv:1202.5695v1 [cs.LG].
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874, June.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings ACM SIGKDD,
pages 168?177.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In Proceedings
of NIPS, pages 897?904.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of ACL.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In Pro-
ceedings of ICWSM.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In AAAI-98 Workshop, pages 41?48.
Vangelis Metsis, Ion Androutsopoulos, and Georgios
Paliouras. 2006. Spam filtering with naive bayes -
which naive bayes? In Proceedings of CEAS.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP, pages 378?
382, September 27-29.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Proceedings of
ACL:HLT.
Andrew Y Ng and Michael I Jordan. 2002. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. In Proceedings of
NIPS, volume 2, pages 841?848.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL.
Jason D. Rennie, Lawrence Shih, Jaime Teevan, and
David R. Karger. 2003. Tackling the poor assump-
tions of naive bayes text classifiers. In Proceedings of
ICML, pages 616?623.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In Pro-
ceedings of ICML, pages 839?846.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
94
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311?321,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Adaptive Online Training of Feature-Rich Translation Models
Spence Green, Sida Wang, Daniel Cer, and Christopher D. Manning
Computer Science Department, Stanford University
{spenceg,sidaw,danielcer,manning}@stanford.edu
Abstract
We present a fast and scalable online
method for tuning statistical machine trans-
lation models with large feature sets. The
standard tuning algorithm?MERT?only
scales to tens of features. Recent discrimi-
native algorithms that accommodate sparse
features have produced smaller than ex-
pected translation quality gains in large
systems. Our method, which is based on
stochastic gradient descent with an adaptive
learning rate, scales to millions of features
and tuning sets with tens of thousands of
sentences, while still converging after only
a few epochs. Large-scale experiments on
Arabic-English and Chinese-English show
that our method produces significant trans-
lation quality gains by exploiting sparse fea-
tures. Equally important is our analysis,
which suggests techniques for mitigating
overfitting and domain mismatch, and ap-
plies to other recent discriminative methods
for machine translation.
1 Introduction
Sparse, overlapping features such as words and n-
gram contexts improve many NLP systems such as
parsers and taggers. Adaptation of discriminative
learning methods for these types of features to sta-
tistical machine translation (MT) systems, which
have historically used idiosyncratic learning tech-
niques for a few dense features, has been an active
research area for the past half-decade. However, de-
spite some research successes, feature-rich models
are rarely used in annual MT evaluations. For exam-
ple, among all submissions to theWMT and IWSLT
2012 shared tasks, just one participant tuned more
than 30 features (Hasler et al, 2012a). Slow uptake
of these methods may be due to implementation
complexities, or to practical difficulties of configur-
ing them for specific translation tasks (Gimpel and
Smith, 2012; Simianer et al, 2012, inter alia).
We introduce a new method for training feature-
rich MT systems that is effective yet comparatively
easy to implement. The algorithm scales to millions
of features and large tuning sets. It optimizes a lo-
gistic objective identical to that of PRO (Hopkins
and May, 2011) with stochastic gradient descent, al-
though other objectives are possible. The learning
rate is set adaptively using AdaGrad (Duchi et al,
2011), which is particularly effective for the mixture
of dense and sparse features present in MT models.
Finally, feature selection is implemented as efficient
L1 regularization in the forward-backward splitting
(FOBOS) framework (Duchi and Singer, 2009). Ex-
periments show that our algorithm converges faster
than batch alternatives.
To learn good weights for the sparse features,
most algorithms?including ours?benefit from
more tuning data, and the natural source is the train-
ing bitext. However, the bitext presents two prob-
lems. First, it has a single reference, sometimes of
lower quality than the multiple references in tun-
ing sets from MT competitions. Second, large bi-
texts often comprise many text genres (Haddow and
Koehn, 2012), a virtue for classical dense MT mod-
els but a curse for high dimensional models: bitext
tuning can lead to a significant domain adaptation
problem when evaluating on standard test sets. Our
analysis separates and quantifies these two issues.
We conduct large-scale translation quality exper-
iments on Arabic-English and Chinese-English. As
baselines we use MERT (Och, 2003), PRO, and
the Moses (Koehn et al, 2007) implementation
of k-best MIRA, which Cherry and Foster (2012)
recently showed to work as well as online MIRA
(Chiang, 2012) for feature-rich models. The first
experiment uses standard tuning and test sets from
the NIST OpenMT competitions. The second ex-
periment uses tuning and test sets sampled from the
large bitexts. The new method yields significant
improvements in both experiments. Our code is
included in the Phrasal (Cer et al, 2010) toolkit,
which is freely available.
311
2 Adaptive Online Algorithms
Machine translation is an unusual machine learning
setting because multiple correct translations exist
and decoding is comparatively expensive. When we
have a large feature set and therefore want to tune
on a large data set, batch methods are infeasible.
Online methods can converge faster, and in practice
they often find better solutions (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia).
Recall that stochastic gradient descent (SGD),
a fundamental online method, updates weights w
according to
wt = wt?1 ? ??`t(wt?1) (1)
with loss function1 `t(w) of the tth example,
(sub)gradient of the loss with respect to the param-
eters ?`t(wt?1), and learning rate ?.
SGD is sensitive to the learning rate ?, which is
difficult to set in an MT system that mixes frequent
?dense? features (like the language model) with
sparse features (e.g., for translation rules). Further-
more, ? applies to each coordinate in the gradient,
an undesirable property in MT where good sparse
features may fire very infrequently. We would in-
stead like to take larger steps for sparse features and
smaller steps for dense features.
2.1 AdaGrad
AdaGrad is a method for setting an adaptive learn-
ing rate that comes with good theoretical guaran-
tees. The theoretical improvement over SGD is
most significant for high-dimensional, sparse fea-
tures. AdaGrad makes the following update:
wt = wt?1 ? ??1/2t ?`t(wt?1) (2)
??1t = ??1t?1 +?`t(wt?1)?`t(wt?1)>
=
t?
i=1
?`i(wi?1)?`i(wi?1)> (3)
A diagonal approximation to ? can be used for a
high-dimensional vector wt. In this case, AdaGrad
is simple to implement and computationally cheap.
Consider a single dimension j, and let scalars vt =
wt,j , gt = ?j`t(wt?1), Gt = ?ti=1 g2i , then theupdate rule is
vt = vt?1 ? ? G?1/2t gt (4)
Gt = Gt?1 + g2t (5)
Compared to SGD, we just need to storeGt = ??1t,jjfor each dimension j.
1We specify the loss function for MT in section 3.1.
2.2 Prior Online Algorithms in MT
AdaGrad is related to two previous online learning
methods for MT.
MIRA Chiang et al (2008) described an adaption
of MIRA (Crammer et al, 2006) to MT. MIRA
makes the following update:
wt = arg min
w
1
2??w ? wt?1?
2
2 + `t(w) (6)
The first term expresses conservativity: the weight
should change as little as possible based on a sin-
gle example, ensuring that it is never beneficial to
overshoot the minimum.
The relationship to SGD can be seen by lineariz-
ing the loss function `t(w) ? `t(wt?1) + (w ?
wt?1)>?`t(wt?1) and taking the derivative of (6).
The result is exactly (1).
AROW Chiang (2012) adapted AROW (Cram-
mer et al, 2009) to MT. AROW models the current
weight as a Gaussian centered at wt?1 with covari-
ance ?t?1, and does the following update upon
seeing training example xt:
wt,?t =
arg min
w,?
1
?DKL(N (w,?)||N (wt?1,?t?1))
+ `t(w) +
1
2?x
>
t ?xt (7)
The KL-divergence term expresses a more general,
directionally sensitive conservativity. Ignoring the
third term, the ? that minimizes the KL is actu-
ally ?t?1. As a result, the first two terms of (7)
generalize MIRA so that we may be more conser-
vative in some directions specified by ?. To see
this, we can write out the KL-divergence between
two Gaussians in closed form, and observe that the
terms involving w do not interact with the terms
involving ?:
wt = arg min
w
1
2? (w ? wt?1)
>??1t?1(w ? wt?1)
+ `t(w) (8)
?t = arg min
?
1
2? log
( |?t?1|
|?|
)
+ 12?Tr
(
??1t?1?
)
+ 12?x
>
t ?xt (9)
The third term in (7), called the confidence term,
gives us adaptivity, the notion that we should have
smaller variance in the direction v as more data xt
312
is seen in direction v. For example, if ? is diagonal
and xt are indicator features, the confidence term
then says that the weight for a rarer feature should
have more variance and vice-versa. Recall that for
generalized linear models?`t(w) ? xt; if we sub-
stitute xt = ?t?`t(w) into (9), differentiate and
solve, we get:
??1t = ??1t?1 + xtx>t
= ??10 +
t?
i=1
?2i?`i(wi?1)?`i(wi?1)>
(10)
The precision ??1t generally grows as more datais seen. Frequently updated features receive an espe-
cially high precision, whereas the model maintains
large variance for rarely seen features.
If we substitute (10) into (8), linearize the loss
`t(w) as before, and solve, then we have the lin-
earized AROW update
wt = wt?1 ? ??t?`t(wt?1) (11)
which is also an adaptive update with per-coordinate
learning rates specified by ?t (as opposed to ?1/2tin AdaGrad).
2.3 Comparing AdaGrad, MIRA, AROW
Compare (3) to (10) and observe that if we set
??10 = 0 and ?t = 1, then the only differencebetween the AROW update (11) and the AdaGrad
update (2) is a square root. Under a constant gradi-
ent, AROW decays the step size more aggressively
(1/t) compared to AdaGrad (1/?t), and it is sensi-
tive to the specification of ??10 .Informally, SGD can be improved in the conser-
vativity direction using MIRA so the updates do
not overshoot. Second, SGD can be improved in
the adaptivity direction using AdaGrad where the
decaying stepsize is more robust and the adaptive
stepsize allows better weight updates to features
differing in sparsity and scale. Finally, AROW com-
bines both adaptivity and conservativity. For MT,
adaptivity allows us to deal withmixed dense/sparse
features effectively without specific normalization.
Why do we choose AdaGrad over AROW?
MIRA/AROW requires selecting the loss function
`(w) so that wt can be solved in closed-form, by
a quadratic program (QP), or in some other way
that is better than linearizing. This usually means
choosing a hinge loss. On the other hand, Ada-
Grad/linearized AROW only requires that the gradi-
ent of the loss function can be computed efficiently.
Algorithm 1 Adaptive online tuning for MT.
Require: Tuning set {fi, e1:ki }i=1:M1: Set w0 = 02: Set t = 1
3: repeat
4: for i in 1 . . .M in random order do
5: Decode n-best list Ni for fi6: Sample pairs {dj,+, dj,?}j=1:s from Ni7: Compute Dt = {?(dj,+)? ?(dj,?)}j=1:s8: Set gt = ?`(Dt; wt?1)}
9: Set ??1t = ??1t?1 + gtg>t . Eq. (3)
10: Update wt = wt?1 ? ??1/2t gt . Eq. (2)11: Regularize wt . Eq. (15)12: Set t = t+ 1
13: end for
14: until convergence
Linearized AROW, however, is less robust than Ada-
Grad empirically2 and lacks known theoretical guar-
antees. Finally, by using AdaGrad, we separate
adaptivity from conservativity. Our experiments
suggest that adaptivity is actually more important.
3 Adaptive Online MT
Algorithm 1 shows the full algorithm introduced in
this paper. AdaGrad (lines 9?10) is a crucial piece,
but the loss function, regularization technique, and
parallelization strategy described in this section are
equally important in the MT setting.
3.1 Pairwise Logistic Loss Function
Algorithm 1 lines 5?8 describe the gradient com-
putation. We cast MT tuning as pairwise ranking
(Herbrich et al, 1999, inter alia), which Hopkins
and May (2011) applied to MT. The pairwise ap-
proach results in simple, convex loss functions suit-
able for online learning. The idea is that for any
two derivations, the ranking predicted by the model
should be consistent with the ranking predicted by
a gold sentence-level metric G like BLEU+1 (Lin
and Och, 2004).
Consider a single source sentence f with asso-
ciated references e1:k. Let d be a derivation in an
n-best list of f that has the target e = e(d) and the
feature map ?(d). Let M(d) = w ? ?(d) be the
model score. For any derivation d+ that is better
than d? under G, we desire pairwise agreement
such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
2According to experiments not reported in this paper.
313
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011).
We compute difference vectors Dt = {x1:s+ } (Al-gorithm 1 line 7) from s pairs (d+, d?) for source
sentence ft. We use the familiar logistic loss:
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(12)
Choosing the hinge loss instead of the logistic
loss results in the 1-class SVM problem. The 1-
class separation problem is equivalent to the binary
classification problem with x+ = ?(d+)? ?(d?)
as positive data and x? = ?x+ as negative data,
which may be plugged into an existing logistic re-
gression solver.
We find that Algorithm 1 works best with mini-
batches instead of single examples. In line 4 we
simply partition the tuning set so that i becomes a
mini-batch of examples.
3.2 Updating and Regularization
Algorithm 1 lines 9?11 compute the adaptive learn-
ing rate, update the weights, and apply regulariza-
tion. Section 2.1 explained the AdaGrad learn-
ing rate computation. To update and regularize
the weights we apply the Forward-Backward Split-
ting (FOBOS) (Duchi and Singer, 2009) framework,
which separates the two operations. The two-step
FOBOS update is
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (13)
wt = arg min
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(14)
where (13) is just an unregularized gradient descent
step and (14) balances the regularization term r(w)
with staying close to the gradient step.
Equation (14) permits efficient L1 regulariza-
tion, which is well-suited for selecting good features
from exponentially many irrelevant features (Ng,
2004). It is well-known that feature selection is very
important for feature-rich MT. For example, sim-
ple indicator features like lexicalized re-ordering
classes are potentially useful yet bloat the the fea-
ture set and, in the worst case, can negatively impact
Algorithm 2 ?Stale gradient? parallelization
method for Algorithm 1.
Require: Tuning set {fi, e1:ki }i=1:M1: Initialize threadpool p1, . . . , pj2: Set t = 1
3: repeat
4: for i in 1 . . .M in random order do
5: Wait until any thread p is idle
6: Send (fi, e1:ki , t) to p . Alg. 1 lines 5?87: while ? p? done with gradient gt? do . t? ? t8: Update wt = wt?1 ? ?gt? . Alg. 1 lines 9?119: Set t = t+ 1
10: end while
11: end for
12: until convergence
search. Some of the features generalize, but many
do not. This was well understood in previous work,
so heuristic filtering was usually applied (Chiang
et al, 2009, inter alia). In contrast, we need only
select an appropriate regularization strength ?.
Specifically, when r(w) = ??w?1, the closed-
form solution to (14) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(15)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls
below the threshold ?t?1?. It is straightforward to
adapt this to AdaGrad with diagonal ? by setting
each dimension of ?t?1,j = ??
1
2
t,jj and by takingelement-wise products.
We find that?`t?1(wt?1) only involves several
hundred active features for the current example
(or mini-batch). However, naively following the
FOBOS framework requires updating millions of
weights. But a practical benefit of FOBOS is that
we can do lazy updates on just the active dimensions
without any approximations.
3.3 Parallelization
Algorithm 1 is inherently sequential like standard
online learning. This is undesirable in MT where
decoding is costly. We therefore parallelize the algo-
rithm with the ?stale gradient? method of Langford
et al (2009) (Algorithm 2). A fixed threadpool of
workers computes gradients in parallel and sends
them to a master thread, which updates a central
weight vector. Crucially, the weight updates need
not be applied in order, so synchronization is unnec-
essary; the workers only idle at the end of an epoch.
The consequence is that the update in line 8 of Al-
gorithm 2 is with respect to gradient gt? with t? ? t.
Langford et al (2009) gave convergence results for
314
stale updating, but the bounds do not apply to our
setting since we use L1 regularization. Neverthe-
less, Gimpel et al (2010) applied this framework
to other non-convex objectives and obtained good
empirical results.
Our asynchronous, stochastic method has practi-
cal appeal for MT. During a tuning run, the online
method decodes the tuning set under many more
weight vectors than a MERT-style batch method.
This characteristic may result in broader exploration
of the search space, and make the learner more ro-
bust to local optima local optima (Liang and Klein,
2009; Bottou and Bousquet, 2011, inter alia). The
adaptive algorithm identifies appropriate learning
rates for the mixture of dense and sparse features.
Finally, large data structures such as the language
model (LM) and phrase table exist in shared mem-
ory, obviating the need for remote queries.
4 Experiments
We built Arabic-English and Chinese-English MT
systems with Phrasal (Cer et al, 2010), a phrase-
based system based on alignment templates (Och
and Ney, 2004). The corpora3 in our experiments
(Table 1) derive from several LDC sources from
2012 and earlier. We de-duplicated each bitext ac-
cording to exact string match, and ensured that no
overlap existed with the test sets. We produced
alignments with the Berkeley aligner (Liang et al,
2006b) with standard settings and symmetrized via
the grow-diag heuristic.
For each language we used SRILM (Stolcke,
2002) to estimate 5-gram LMs with modified
Kneser-Ney smoothing. We included the monolin-
gual English data and the respective target bitexts.
4.1 Feature Templates
The baseline ?dense? model contains 19 features:
the nine Moses baseline features, the hierarchical
lexicalized re-ordering model of Galley and Man-
ning (2008), the (log) count of each rule, and an
indicator for unique rules.
To the dense features we add three high di-
mensional ?sparse? feature sets. Discrimina-
3We tokenized the English with packages from the Stan-
ford Parser (Klein and Manning, 2003) according to the Penn
Treebank standard (Marcus et al, 1993), the Arabic with the
Stanford Arabic segmenter (Green and DeNero, 2012) accord-
ing to the Penn Arabic Treebank standard (Maamouri et al,
2008), and the Chinese with the Stanford Chinese segmenter
(Chang et al, 2008) according to the Penn Chinese Treebank
standard (Xue et al, 2005).
Bilingual Monolingual
Sentences Tokens Tokens
Ar-En 6.6M 375M 990MZh-En 9.3M 538M
Table 1: Bilingual and monolingual corpora used
in these experiments. The monolingual English
data comes from the AFP and Xinhua sections of
English Gigaword 4 (LDC2009T13).
tive phrase table (PT): indicators for each rule
in the phrase table. Alignments (AL): indica-
tors for phrase-internal alignments and deleted
(unaligned) source words. Discriminative re-
ordering (LO): indicators for eight lexicalized re-
ordering classes, including the six standard mono-
tone/swap/discontinuous classes plus the two sim-
pler Moses monotone/non-monotone classes.
4.2 Tuning Algorithms
The primary baseline is the dense feature set tuned
with MERT (Och, 2003). The Phrasal implemen-
tation uses the line search algorithm of Cer et al
(2008), uniform initialization, and 20 random start-
ing points.4 We tuned according to BLEU-4 (Pap-
ineni et al, 2002).
We built high dimensional baselines with two dif-
ferent algorithms. First, we tuned with batch PRO
using the default settings in Phrasal (L2 regulariza-
tion with ?=0.1). Second, we ran the k-best batch
MIRA (kb-MIRA) (Cherry and Foster, 2012) imple-
mentation in Moses. We did implement an online
version of MIRA, and in small-scale experiments
found that the batch variant worked just as well.
Cherry and Foster (2012) reported the same result,
and their implementation is available in Moses. We
ran their code with standard settings.
Moses5 also contains the discriminative phrase
table implementation of (Hasler et al, 2012b),
which is identical to our implementation using
Phrasal. Moses and Phrasal accept the same phrase
table and LM formats, so we kept those data struc-
tures in common. The two decoders also use the
same multi-stack beam search (Och and Ney, 2004).
For our method, we used uniform initialization,
16 threads, and a mini-batch size of 20. We found
that ?=0.02 and ?=0.1 worked well on development
sets for both languages. To compute the gradients
4Other system settings for all experiments: distortion limit
of 5, a maximum phrase length of 7, and an n-best size of 200.
5v1.0 (28 January 2013)
315
Model #features Algorithm Tuning Set MT02 MT03 MT04 MT09
Dense 19 MERT MT06 45.08 51.32 52.26 51.42 48.44
Dense 19 This paper MT06 44.19 51.42 52.52 50.16 48.13
+PT 151k kb-MIRA MT06 42.08 47.25 48.98 47.08 45.64
+PT 23k PRO MT06 44.31 51.06 52.18 50.23 47.52
+PT 50k This paper MT06 50.61 51.71 52.89 50.42 48.74
+PT+AL+LO 109k PRO MT06 44.87 51.25 52.43 50.05 47.76
+PT+AL+LO 242k This paper MT06 57.84 52.45 53.18 51.38 49.37
Dense 19 MERT MT05/6/8 49.63 51.60 52.29 51.73 48.68
+PT+AL+LO 390k This paper MT05/6/8 58.20 53.61 54.99 52.79 49.94
(Chiang, 2012)* 10-20k MIRA MT04/6 ? ? ? ? 45.90
(Chiang, 2012)* 10-20k AROW MT04/6 ? ? ? ? 47.60
#sentences 728 663 1,075 1,313
Table 2: Ar-En results [BLEU-4 % uncased] for the NIST tuning experiment. The tuning and test sets
each have four references. MT06 has 1,717 sentences, while the concatenated MT05/6/8 set has 4,213
sentences. Bold indicates statistical significance relative to the best baseline in each block at p < 0.001;
bold-italic at p < 0.05. We assessed significance with the permutation test of Riezler and Maxwell (2005).
(*) Chiang (2012) used a similar-sized bitext, but two LMs trained on twice as much monolingual data.
Model #features Algorithm Tuning Set MT02 MT03 MT04
Dense 19 MERT MT06 33.90 35.72 33.71 34.26
Dense 19 This paper MT06 32.60 36.23 35.14 34.78
+PT 105k kb-MIRA MT06 29.46 30.67 28.96 30.05
+PT 26k PRO MT06 33.70 36.87 34.62 34.80
+PT 66k This paper MT06 33.90 36.09 34.86 34.73
+PT+AL+LO 148k PRO MT06 34.81 36.31 33.81 34.41
+PT+AL+LO 344k This paper MT06 38.99 36.40 35.07 34.84
Dense 19 MERT MT05/6/8 32.36 35.69 33.83 34.33
+PT+AL+LO 487k This paper MT05/6/8 37.64 37.81 36.26 36.15
#sentences 878 919 1,597
Table 3: Zh-En results [BLEU-4 % uncased] for the NIST tuning experiment. MT05/6/8 has 4,103
sentences. OpenMT 2009 did not include Zh-En, hence the asymmetry with Table 2.
we sampled 15 derivation pairs for each tuning ex-
ample and scored them with BLEU+1.
4.3 NIST OpenMT Experiment
The first experiment evaluates our algorithm when
tuning and testing on standard test sets, each with
four references. When we add features, our algo-
rithm tends to overfit to a standard-sized tuning set
like MT06. We thus concatenated MT05, MT06,
and MT08 to create a larger tuning set.
Table 2 shows the Ar-En results. Our algorithm
is competitive with MERT in the low dimensional
?dense? setting, and compares favorably to PRO
with the PT feature set. PRO does not benefit
from additional features, whereas our algorithm im-
proves with both additional features and data. The
underperformance of kb-MIRA may result from
a difference between Moses and Phrasal: Moses
MERT achieves only 45.62 on MT09. Moses PRO
with the PT feature set is slightly worse, e.g., 44.52
on MT09. Nevertheless, kb-MIRA does not im-
prove significantly over MERT, and also selects an
unnecessarily large model.
The full feature set PT+AL+LO does help. With
the PT feature set alne, our algorithm tuned on
MT05/6/8 scores well below the best model, e.g.
316
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 45.08 4 39.28 51.42
+PT 72k This paper MT05/6/8 51.29 4 39.50 50.60
+PT 79k This paper bitext5k 44.79 1 43.85 45.73
+PT+AL+LO 647k This paper bitext15k 45.68 1 43.93 45.24
Table 4: Ar-En results [BLEU-4 % uncased] for the bitext tuning experiment. Statistical significance is
relative to the Dense baseline. We include MT04 for comparison to the NIST genre.
Model #features Algorithm Tuning Set #refs bitext5k-test MT04
Dense 19 MERT MT06 33.90 4 33.44 34.26
+PT 97k This paper MT05/6/8 34.45 4 35.08 35.19
+PT 67k This paper bitext5k 36.26 1 36.01 33.76
+PT+AL+LO 536k This paper bitext15k 37.57 1 36.30 34.05
Table 5: Zh-En results [BLEU-4 % uncased] for the bitext tuning experiment.
48.56 BLEU on MT09. For Ar-En, our algorithm
thus has the desirable property of benefiting from
more and better features, and more data.
Table 3 shows Zh-En results. Somewhat sur-
prisingly our algorithm improves over MERT in
the dense setting. When we add the discrimina-
tive phrase table, our algorithm improves over kb-
MIRA, and over batch PRO on two evaluation sets.
With all features and the MT05/6/8 tuning set, we
improve significantly over all other models. PRO
learns a smaller model with the PT+AL+LO fea-
ture set which is surprising given that it applies L2
regularization (AdaGrad uses L1). We speculate
that this may be an consequence of stochastic learn-
ing. Our algorithm decodes each example with
a new weight vector, thus exploring more of the
search space for the same tuning set.
4.4 Bitext Tuning Experiment
Tables 2 and 3 show that adding tuning examples
improves translation quality. Nevertheless, even
the larger tuning set is small relative to the bitext
from which rules were extracted. He and Deng
(2012) and Simianer et al (2012) showed significant
translation quality gains by tuning on the bitext.
However, their bitexts matched the genre of their
test sets. Our bitexts, like those of most large-scale
systems, do not. Domain mismatch matters for the
dense feature set (Haddow and Koehn, 2012). We
show that it also matters for feature-rich MT.
Before aligning each bitext, we randomly sam-
pled and sequestered 5k and 15k sentence tuning
sets, and a 5k test set. We prevented overlap be-
DA DB |A| |B| |A ?B|
MT04 MT06 70k 72k 5.9k
MT04 MT568 70k 96k 7.6k
MT04 bitext5k 70k 67k 4.4k
MT04 bitext15k 70k 310k 10.5k
5ktest bitext5k 82k 67k 5.6k
5ktest bitext15k 82k 310k 14k
Table 6: Number of overlapping phrase table (+PT)
features on various Zh-En dataset pairs.
tween the tuning sets and the test set. We then
tuned a dense model with MERT on MT06, and
feature-rich models on both MT05/6/8 and the bi-
text tuning set. Table 4 shows the Ar-En results.
When tuned on bitext5k the translation quality gains
are significant for bitext5k-test relative to tuning on
MT05/6/8, which has multiple references. However,
the bitext5k models do not generalize as well to the
NIST evaluation sets as represented by the MT04
result. Table 5 shows similar trends for Zh-En.
5 Analysis
5.1 Feature Overlap Analysis
How many sparse features appear in both the tun-
ing and test sets? In Table 6, A is the set of phrase
table features that received a non-zero weight when
tuned on datasetDA (same forB). ColumnDA lists
several Zh-En test sets used and column DB lists
tuning sets. Our experiments showed that tuning
on MT06 generalizes better to MT04 than tuning
317
on bitext5k, whereas tuning on bitext5k general-
izes better to bitext5k-test than tuning on MT06.
These trends are consistent with the level of fea-
ture overlap. Phrase table features in A ? B are
overwhelmingly short, simple, and correct phrases,
suggesting L1 regularization is effective for feature
selection. It is also important to balance the number
of features with how well weights can be learned
for those features, as tuning on bitext15k produced
higher coverage for MT04 but worse generalization
than tuning on MT06.
5.2 Domain Adaptation Analysis
To understand the domain adaptation issue we com-
pared the non-zero weights in the discriminative
phrase table (PT) for Ar-En models tuned on bi-
text5k and MT05/6/8. Table 7 illustrates a statisti-
cal idiosyncrasy in the data for the American and
British spellings of program/programme. The mass
is concentrated along the diagonal, probably be-
cause MT05/6/8 was prepared by NIST, an Amer-
ican agency, while the bitext was collected from
many sources including Agence France Presse.
Of course, this discrepancy is consequential for
both dense and feature-rich models. However, we
observe that the feature-rich models fit the tuning
data more closely. For example, the MT05/6/8
model learns rules like l .?A 	KQK. 	?? 	?JK
 ? program
includes, l .?A 	KQK. ? program of, and l .?A 	KQ. ? @ ? 	Y 	?A 	K?
program window. Crucially, it does not learn the
basic rule l .?A 	KQK. ? program.
In contrast, the bitext5k model contains ba-
sic rules such l .?A 	KQK. ? programme, l .?A 	KQ. ? @ @ 	Y?
? this programme, and l .?A 	KQ. ? @ ?? 	X ? that pro-
gramme. It also contains more elaborate rules such
as l .?A 	KQ. ? @ HA? 	? 	K I	KA? ? programme expenses
were and ????A?? @ ?J
KA 	? 	?? @ HCgQ?@ l .?@QK.?manned
space flight programmes. We observed similar
trends for ?defense/defence?, ?analyze/analyse?, etc.
This particular genre problem could be addressed
with language-specific pre-processing, but our sys-
tem solves it in a data-driven manner.
5.3 Re-ordering Analysis
We also analyzed re-ordering differences. Arabic
matrix clauses tend to be verb-initial, meaning that
the subject and verb must be swapped when translat-
ing to English. To assess re-ordering differences?
if any?between the dense and feature-rich models,
we selected all MT09 segments that began with one
# bitext5k # MT05/6/8
programme 185 0
program 19 449
PT rules w/ programme 353 79
PT rules w/ program 9 31
Table 7: Top: comparison of token counts in two
Ar-En tuning sets for programme and program. Bot-
tom: rule counts in the discriminative phrase table
(PT) for models tuned on the two tuning sets. Both
spellings correspond to the Arabic l .?A 	KQK. .
of seven common verbs: ?A? qaal ?said?, hQ?? SrH
?declared?, PA ?@ ashaar ?indicated?, 	?A? kaan ?was?,
Q?
	
X dhkr ?commented?, 	?A 	?@ aDaaf ?added?, 	???@
acln ?announced?. We compared the output of the
MERT Dense model to our method with the full
feature set, both tuned on MT06. Of the 208 source
segments, 32 of the translation pairs contained dif-
ferent word order in the matrix clause. Our feature-
rich model was correct 18 times (56.3%), Dense
was correct 4 times (12.5%), and neither method
was correct 10 times (31.3%).
(1) ref: lebanese prime minister , fuad siniora ,
announced
a. and lebanese prime minister fuad siniora
that
b. the lebanese prime minister fouad siniora
announced
(2) ref: the newspaper and television reported
a. she said the newspaper and television
b. television and newspaper said
In (1) the dense model (1a) drops the verb while the
feature-rich model correctly re-orders and inserts
it after the subject (1b). The coordinated subject
in (2) becomes an embedded subject in the dense
output (2a). The feature-rich model (2b) performs
the correct re-ordering.
5.4 Runtime Comparison
Table 8 compares our method to standard implemen-
tations of the other algorithms. MERT parallelizes
easily but runtime increases quadratically with n-
best list size. PRO runs (single-threaded) L-BFGS
to convergence on every epoch, a potentially slow
procedure for the larger feature set. Moreover, both
318
epochs min.
MERT Dense 22 180
PRO +PT 25 35
kb-MIRA* +PT 26 25
This paper +PT 10 10
PRO +PT+AL+LO 13 150
This paper +PT+AL+LO 5 15
Table 8: Epochs to convergence (?epochs?) and
approximate runtime per epoch in minutes (?min.?)
for selected Zh-En experiments tuned on MT06.
All runs executed on the same dedicated system
with the same number of threads. (*) Moses and
kb-MIRA are written in C++, while all other rows
refer to Java implementations in Phrasal.
the Phrasal and Moses PRO implementations use
L2 regularization, which regularizes every weight
on every update. kb-MIRA makes multiple passes
through the n-best lists during each epoch. The
Moses implementation parallelizes decoding but
weight updating is sequential.
The core of our method is an inner product be-
tween the adaptive learning rate vector and the gra-
dient. This is easy to implement and is very fast
even for large feature sets. Since we applied lazy
regularization, this inner product usually involves
hundred-dimensional vectors. Finally, our method
does not need to accumulate n-best lists, a practice
that slows down the other algorithms.
6 Related Work
Our work relates most closely to that of Hasler et al
(2012b), who tuned models containing both sparse
and dense features with Moses. A discriminative
phrase table helped them improve slightly over a
dense, online MIRA baseline, but their best results
required initialization with MERT-tuned weights
and re-tuning a single, shared weight for the dis-
criminative phrase table with MERT. In contrast,
our algorithm learned good high dimensional mod-
els from a uniform starting point.
Chiang (2012) adapted AROW to MT and ex-
tended previous work on online MIRA (Chiang et
al., 2008; Watanabe et al, 2007). It was not clear if
his improvements came from the novel Hope/Fear
search, the conservativity gain from MIRA/AROW
by solving the QP exactly, adaptivity, or sophis-
ticated parallelization. In contrast, we show that
AdaGrad, which ignores conservativity and only
capturing adaptivity, is sufficient.
Simianer et al (2012) investigated SGD with a
pairwise perceptron objective. Their best algorithm
used iterative parameter mixing (McDonald et al,
2010), which we found to be slower than the stale
gradient method in section 3.3. They regularized
once at the end of each epoch, whereas we regular-
ized each weight update. An empirical comparison
of these two strategies would be an interesting fu-
ture contribution.
Watanabe (2012) investigated SGD and even ran-
domly selected pairwise samples as we did. He
considered both softmax and hinge losses, observ-
ing better results with the latter, which solves a QP.
Their parallelization strategy required a line search
at the end of each epoch.
Many other discriminative techniques have been
proposed based on: ramp loss (Gimpel, 2012);
hinge loss (Cherry and Foster, 2012; Haddow et
al., 2011; Arun and Koehn, 2007); maximum en-
tropy (Xiang and Ittycheriah, 2011; Ittycheriah and
Roukos, 2007; Och and Ney, 2002); perceptron
(Liang et al, 2006a); and structured SVM (Till-
mann and Zhang, 2006). These works use radically
different experimental setups, and to our knowl-
edge only (Cherry and Foster, 2012) and this work
compare to at least two high dimensional baselines.
Broader comparisons, though time-intensive, could
help differentiate these methods.
7 Conclusion and Outlook
We introduced a new online method for tuning
feature-rich translation models. The method is
faster per epoch than MERT, scales to millions of
features, and converges quickly. We used efficient
L1 regularization for feature selection, obviating
the need for the feature scaling and heuristic filter-
ing common in prior work. Those comfortable with
implementing vanilla SGD should find our method
easy to implement. Even basic discriminative fea-
tures were effective, so we believe that our work
enables fresh approaches to more sophisticated MT
feature engineering.
Acknowledgments We thank John DeNero for helpful com-
ments on an earlier draft. The first author is supported by a
National Science Foundation Graduate Research Fellowship.
We also acknowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily
reflect the view of the DARPA or the US government.
319
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In MT Summit XI.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351?368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
P-C. Chang, M. Galley, and C. D. Manning. 2008.
Optimizing Chinese word segmentation for machine
translation performance. In WMT.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
HLT-NAACL.
D. Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In HLT-
NAACL.
K. Gimpel, D. Das, and N. A. Smith. 2010. Distributed
asynchronous online learning for natural language
processing. In CoNLL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Language Technologies Institute, Carnegie
Mellon University.
S. Green and J. DeNero. 2012. A class-based agree-
ment model for generating accurately inflected trans-
lations. In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect
of out-of-domain data on SMT systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleR-
ank training for phrase-basedmachine translation. In
WMT.
E. Hasler, P. Bell, A. Ghoshal, B. Haddow, P. Koehn,
F. McInnes, et al 2012a. The UEDIN systems for
the IWSLT 2012 evaluation. In IWSLT.
E. Hasler, B. Haddow, and P. Koehn. 2012b. Sparse
lexicalised features and topic adaptation for SMT. In
IWSLT.
X. He and L. Deng. 2012. Maximum expected BLEU
training of phrase and lexicon translation models. In
ACL.
R. Herbrich, T. Graepel, and K. Obermayer. 1999.
Support vector learning for ordinal regression. In
ICANN.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In HLT-NAACL.
P. Liang, A. Bouchard-C?t?, D. Klein, and B. Taskar.
2006a. An end-to-end discriminative approach to
machine translation. In ACL.
P. Liang, B. Taskar, and D. Klein. 2006b. Alignment
by agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanc-
ing the Arabic Treebank: A collaborative effort to-
ward new annotation guidelines. In LREC.
320
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
R.McDonald, K. Hall, andG.Mann. 2010. Distributed
training strategies for the structured perceptron. In
NAACL-HLT.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization (MTSE).
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-
scale discriminative training in SMT. In ACL.
A Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In ICSLP.
C. Tillmann and T. Zhang. 2006. A discriminative
global training algorithm for statistical MT. In ACL-
COLING.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In EMNLP-CoNLL.
T. Watanabe. 2012. Optimized online rank learning
for machine translation. In HLT-NAACL. Associa-
tion for Computational Linguistics.
B. Xiang and A. Ittycheriah. 2011. Discriminative
feature-tied mixture modeling for statistical machine
translation. In ACL-HLT.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
321
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation Task
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John Bauer
Sida Wang, Natalia Silveira?, Julia Neidert and Christopher D. Manning
Computer Science Department, Stanford University
*Center for East Asian Studies, Stanford University
?Department of Linguistics, Stanford University
{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.edu
Abstract
We describe the Stanford University NLP
Group submission to the 2013 Workshop
on Statistical Machine Translation Shared
Task. We demonstrate the effectiveness of a
new adaptive, online tuning algorithm that
scales to large feature and tuning sets. For
both English-French and English-German,
the algorithm produces feature-rich mod-
els that improve over a dense baseline and
compare favorably to models tuned with
established methods.
1 Introduction
Green et al (2013b) describe an online, adaptive
tuning algorithm for feature-rich translation mod-
els. They showed considerable translation quality
improvements over MERT (Och, 2003) and PRO
(Hopkins and May, 2011) for two languages in a
research setting. The purpose of our submission to
the 2013 Workshop on Statistical Machine Trans-
lation (WMT) Shared Task is to compare the algo-
rithm to more established methods in an evaluation.
We submitted English-French (En-Fr) and English-
German (En-De) systems, each with over 100k fea-
tures tuned on 10k sentences. This paper describes
the systems and also includes new feature sets and
practical extensions to the original algorithm.
2 Translation Model
Our machine translation (MT) system is Phrasal
(Cer et al, 2010), a phrase-based system based on
alignment templates (Och and Ney, 2004). Like
many MT systems, Phrasal models the predictive
translation distribution p(e|f ;w) directly as
p(e|f ;w) = 1Z(f) exp
[
w>?(e, f)
]
(1)
where e is the target sequence, f is the source se-
quence, w is the vector of model parameters, ?(?)
is a feature map, and Z(f) is an appropriate nor-
malizing constant. For many years the dimension
of the feature map ?(?) has been limited by MERT,
which does not scale past tens of features.
Our submission explores real-world translation
quality for high-dimensional feature maps and as-
sociated weight vectors. That case requires a more
scalable tuning algorithm.
2.1 Online, Adaptive Tuning Algorithm
FollowingHopkins andMay (2011) we castMT tun-
ing as pairwise ranking. Consider a single source
sentence f with associated references e1:k. Let d
be a derivation in an n-best list of f that has the
target e = e(d) and the feature map ?(d). Define
the linear model scoreM(d) = w ? ?(d). For any
derivation d+ that is better than d? under a gold
metric G, we desire pairwise agreement such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011). Suppose that we sample
s pairs for source sentence ft to compute a set of
difference vectors Dt = {x1:s+ }. Then we optimize
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(2)
which is the familiar logistic loss. Hopkins and
May (2011) optimize (2) in a batch algorithm
that alternates between candidate generation (i.e.,
n-best list or lattice decoding) and optimization
(e.g., L-BFGS). We instead use AdaGrad (Duchi
148
et al, 2011), a variant of stochastic gradient de-
scent (SGD) in which the learning rate is adapted
to the data. Informally, AdaGrad scales the weight
updates according to the geometry of the data ob-
served in earlier iterations. Consider a particu-
lar dimension j of w, and let scalars vt = wt,j ,
gt = ?j`t(wt?1), and Gt = ?ti=1 g2i . The Ada-Grad update rule is
vt = vt?1 ? ? G?1/2t gt (3)
Gt = Gt?1 + g2t (4)
In practice,Gt is a diagonal approximation. IfGt =
I , observe that (3) is vanilla SGD.
In MT systems, the feature map may generate
exponentially many irrelevant features, so we need
to regularize (3). The L1 norm of the weight vec-
tor is known to be an effective regularizer in such
a setting (Ng, 2004). An efficient way to apply
L1 regularization is the Forward-Backward split-
ting (FOBOS) framework (Duchi and Singer, 2009),
which has the following two-step update:
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (5)
wt = argmin
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(6)
where (5) is just an unregularized gradient descent
step and (6) balances the regularization term r(w)
with staying close to the gradient step.
For L1 regularization we have r(w) = ?||w||1
and the closed-form solution to (6) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(7)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls below
the threshold ?t?1?.
Online algorithms are inherently sequential; this
algorithm is no exception. If we want to scale the
algorithm to large tuning sets, then we need to par-
allelize the weight updates. Green et al (2013b)
describe the parallelization technique that is imple-
mented in Phrasal.
2.2 Extensions to (Green et al, 2013b)
Sentence-Level Metric We previously used the
gold metric BLEU+1 (Lin and Och, 2004), which
smoothes bigram precisions and above. This metric
worked well with multiple references, but we found
that it is less effective in a single-reference setting
like WMT. To make the metric more robust, Nakov
et al (2012) extended BLEU+1 by smoothing both
the unigram precision and the reference length. We
found that this extension yielded a consistent +0.2
BLEU improvement at test time for both languages.
Subsequent experiments on the data sets of Green
et al (2013b) showed that standard BLEU+1 works
best for multiple references.
Custom regularization parameters Green et al
(2013b) showed that large feature-rich models over-
fit the tuning sets. We discovered that certain fea-
tures caused greater overfitting than others. Custom
regularization strengths for each feature set are one
solution to this problem. We found that technique
largely fixed the overfitting problem as shown by
the learning curves presented in section 5.1.
Convergence criteria Standard MERT imple-
mentations approximate tuning BLEU by re-
ranking the previous n-best lists with the updated
weight vector. This approximation becomes infeasi-
ble for large tuning sets, and is less accurate for algo-
rithms like ours that do not accumulate n-best lists.
We approximate tuning BLEU by maintaining the
1-best hypothesis for each tuning segment. At the
end of each epoch, we compute corpus-level BLEU
from this hypothesis set. We flush the set of stored
hypotheses before the next epoch begins. Although
memory-efficient, we find that this approximation
is less dependable as a convergence criterion than
the conventional method. Whereas we previously
stopped the algorithm after four iterations, we now
select the model according to held-out accuracy.
3 Feature Sets
3.1 Dense Features
The baseline ?dense? model has 19 features: the
nine Moses (Koehn et al, 2007) baseline features, a
hierarchical lexicalized re-ordering model (Galley
and Manning, 2008), the (log) bitext count of each
translation rule, and an indicator for unique rules.
The final dense feature sets for each language
differ slightly. The En-Fr system incorporates a
second language model. The En-De system adds a
future cost component to the linear distortion model
(Green et al, 2010).The future cost estimate allows
the distortion limit to be raised without a decrease
in translation quality.
149
3.2 Sparse Features
Sparse features do not necessarily fire on each hy-
pothesis extension. Unlike prior work on sparseMT
features, our feature extractors do not filter features
based on tuning set counts. We instead rely on the
regularizer to select informative features.
Several of the feature extractors depend on
source-side part of speech (POS) sequences and
dependency parses. We created those annotations
with the Stanford CoreNLP pipeline.
Discriminative Phrase Table A lexicalized in-
dicator feature for each rule in a derivation. The
feature weights can be interpreted as adjustments
to the associated dense phrase table features.
Discriminative Alignments A lexicalized indi-
cator feature for the phrase-internal alignments in
each rule in a derivation. For one-to-many, many-to-
one, and many-to-many alignments we extract the
clique of aligned tokens, perform a lexical sort, and
concatenate the tokens to form the feature string.
Discriminative Re-ordering A lexicalized indi-
cator feature for each rule in a derivation that ap-
pears in the following orientations: monotone-with-
next, monotone-with-previous, non-monotone-
with-next, non-monotone-with-previous. Green
et al (2013b) included the richer non-monotone
classes swap and discontinuous. However, we found
that these classes yielded no significant improve-
ment over the simpler non-monotone classes. The
feature weights can be interpreted as adjustments
to the generative lexicalized re-ordering model.
Source Content-Word Deletion Count-based
features for source content words that are ?deleted?
in the target. Content words are nouns, adjectives,
verbs, and adverbs. A deleted source word is ei-
ther unaligned or aligned to one of the 100 most
frequent target words in the target bitext. For each
deleted word we increment both the feature for the
particular source POS and an aggregate feature for
all parts of speech. We add similar but separate
features for head content words that are either un-
aligned or aligned to frequent target words.
Inverse Document Frequency Numeric fea-
tures that compare source and target word frequen-
cies. Let idf(?) return the inverse document fre-
quency of a token in the training bitext. Suppose
a derivation d = {r1, r2, . . . , rn} is composed of
n translation rules, where e(r) is the target side of
the rule and f(r) is the source side. For each rule
Bilingual Monolingual
Sentences Tokens Tokens
En-Fr 5.0M 289M 1.51B
En-De 4.4M 223M 1.03B
Table 1: Gross corpus statistics after data selection
and pre-processing. The En-Fr monolingual counts
include French Gigaword 3 (LDC2011T10).
r that translates j source tokens to i target tokens
we compute
q =
?
i
idf(e(r)i)?
?
j
idf(f(r)j) (8)
We add two numeric features, one for the source and
another for the target. When q > 0 we increment
the target feature by q; when q < 0 we increment
the target feature by |q|. Together these features
penalize asymmetric rules that map rare words to
frequent words and vice versa.
POS-based Re-ordering The lexicalized dis-
criminative re-ordering model is very sparse, so we
added re-ordering features based on source parts of
speech. When a rule is applied in a derivation, we
extract the associated source POS sequence along
with the POS sequences from the previous and next
rules. We add a ?with-previous? indicator feature
that is the conjunction of the current and previous
POS sequences; the ?with-next? indicator feature is
created analogously. This feature worked well for
En-Fr, but not for En-De.
4 Data Preparation
Table 1 describes the pre-processed corpora from
which our systems are built.
4.1 Data Selection
We used all of the monolingual and parallel En-
De data allowed in the constrained condition. We
incorporated all of the French monolingual data,
but sampled a 5M-sentence bitext from the approx-
imately 40M available En-Fr parallel sentences.
To select the sentences we first created a ?target?
corpus by concatenating the tuning and test sets
(newstest2008?2013). Then we ran the feature
decay algorithm (FDA) (Bi?ici and Yuret, 2011),
which samples sentences that most closely resem-
ble the target corpus. FDA is a principled method
for reducing the phrase table size by excluding less
relevant training examples.
150
4.2 Tokenization
We tokenized the English (source) data according
to the Penn Treebank standard (Marcus et al, 1993)
with Stanford CoreNLP. The French data was to-
kenized with packages from the Stanford French
Parser (Green et al, 2013a), which implements a
scheme similar to that used in the French Treebank
(Abeill? et al, 2003).
German is more complicated due to pervasive
compounding. We first tokenized the data with the
same English tokenizer. Then we split compounds
with the lattice-based model (Dyer, 2009) in cdec
(Dyer et al, 2010). To simplify post-processing we
added segmentation markers to split tokens, e.g.,
?berschritt? ?ber #schritt.
4.3 Alignment
We aligned both bitexts with the Berkeley Aligner
(Liang et al, 2006) configured with standard set-
tings. We symmetrized the alignments according
to the grow-diag heuristic.
4.4 Language Modeling
We estimated unfiltered 5-gram language models
using lmplz (Heafield et al, 2013) and loaded them
with KenLM (Heafield, 2011). For memory effi-
ciency and faster loading we also used KenLM to
convert the LMs to a trie-based, binary format. The
German LM included all of the monolingual data
plus the target side of the En-De bitext. We built
an analogous model for French. In addition, we
estimated a separate French LM from the Gigaword
data.1
4.5 French Agreement Correction
In French verbs must agree in number and person
with their subjects, and adjectives (and some past
participles) must agree in number and gender with
the nouns they modify. On their own, phrasal align-
ment and target side language modeling yield cor-
rect agreement inflection most of the time. For
verbs, we find that the inflections are often accurate:
number is encoded in the English verb and subject,
and 3rd person is generally correct in the absence
of a 1st or 2nd person pronoun. However, since En-
glish does not generally encode gender, adjective
inflection must rely on language modeling, which
is often insufficient.
1The MT system learns significantly different weights for
the two LMs: 0.086 for the primary LM and 0.044 for the
Gigaword LM.
To address this problem we apply an automatic
inflection correction post-processing step. First, we
generate dependency parses of our system?s out-
put using BONSAI (Candito and Crabb?, 2009),
a French-specific extension to the Berkeley Parser
(Petrov et al, 2006). Based on these dependencies,
we match adjectives with the nouns they modify
and past participles with their subjects. Then we
use Lefff (Sagot, 2010), a machine-readable French
lexicon, to determine the gender and number of the
noun and to choose the correct inflection for the
adjective or participle.
Applied to our 3,000 sentence development set,
this correction scheme produced 200 corrections
with perfect accuracy. It produces a slight (?0.014)
drop in BLEU score. This arises from cases where
the reference translation uses a synonymous but
differently gendered noun, and consequently has
different adjective inflection.
4.6 German De-compounding
Split German compounds must be merged after
translation. This process often requires inserting
affixes (e.g., s, en) between adjacent tokens in the
compound. Since the German compounding rules
are complex and exception-laden, we rely on a dic-
tionary lookup procedure with backoffs. The dic-
tionary was constructed during pre-processing. To
compound the final translations, we first lookup
the compound sequence?which is indicated by
segmentation markers?in the dictionary. If it is
present, then we use the dictionary entry. If the com-
pound is novel, then for each pair of words to be
compounded, we insert the suffix most commonly
appended in compounds to the first word of the pair.
If the first word itself is unknown in our dictionary,
we insert the suffix most commonly appended after
the last three characters. For example, words end-
ing with ung most commonly have an s appended
when they are used in compounds.
4.7 Recasing
Phrasal includes an LM-based recaser (Lita et al,
2003), which we trained on the target side of the
bitext for each language. On the newstest2012 de-
velopment data, the German recaser was 96.8% ac-
curate and the French recaser was 97.9% accurate.
5 Translation Quality Experiments
During system development we tuned on
newstest2008?2011 (10,570 sentences) and tested
151
#iterations #features tune newstest2012 newstest2013?
Dense 10 20 30.26 31.12 ?
Feature-rich 11 207k 32.29 31.51 29.00
Table 2: En-Fr BLEU-4 [% uncased] results. The tuning set is newstest2008?2011. (?) newstest2013 is
the cased score computed by the WMT organizers.
#iterations #features tune newstest2012 newstest2013?
Dense 10 19 16.83 18.45 ?
Feature-rich 13 167k 17.66 18.70 18.50
Table 3: En-De BLEU-4 [% uncased] results.
on newstest2012 (3,003 sentences). We compare
the feature-rich model to the ?dense? baseline.
The En-De system parameters were: 200-best
lists, a maximum phrase length of 8, and a distortion
limit of 6 with future cost estimation. The En-Fr
system parameters were: 200-best lists, a maximum
phrase length of 8, and a distortion limit of 5.
The online tuning algorithm used a default learn-
ing rate ? = 0.03 and a mini-batch size of 20. We
set the regularization strength ? to 10.0 for the dis-
criminative re-ordering model, 0.0 for the dense
features, and 0.1 otherwise.
5.1 Results
Tables 2 and 3 show En-Fr and En-De results, re-
spectively. The ?Feature-rich? model, which con-
tains the full complement of dense and sparse fea-
tures, offers ameager improvement over the ?Dense?
baseline. This result contrasts with the results
of Green et al (2013b), who showed significant
translation quality improvements over the same
dense baseline for Arabic-English and Chinese-
English. However, they had multiple target refer-
ences, whereas the WMT data sets have just one.
We speculate that this difference is significant. For
example, consider a translation rule that rewrites
to a 4-gram in the reference. This event can in-
crease the sentence-level score, thus encouraging
the model to upweight the rule indicator feature.
More evidence of overfitting can be seen in Fig-
ure 1, which shows learning curves on the devel-
opment set for both language pairs. Whereas the
dense model converges after just a few iterations,
the feature-rich model continues to creep higher.
Separate experiments on a held-out set showed that
generalization did not improve after about eight
iterations.
6 Conclusion
We submitted a feature-rich MT system to WMT
2013. While sparse features did offer a measur-
able improvement over a baseline dense feature set,
the gains were not as significant as those shown
by Green et al (2013b). One important difference
between the two sets of results is the number of ref-
erences. Their NIST tuning and test sets had four
references; the WMT data sets have just one. We
speculate that sparse features tend to overfit more
in this setting. Individual features can greatly in-
fluence the sentence-level metric and thus become
large components of the gradient. To combat this
phenomenon we experimented with custom reg-
ularization strengths and a more robust sentence-
level metric. While these two improvements greatly
reduced the model size relative to (Green et al,
2013b), a generalization problem remained. Nev-
ertheless, we showed that feature-rich models are
now competitive with the state-of-the-art.
Acknowledgments This work was supported by the Defense
Advanced Research Projects Agency (DARPA) Broad Opera-
tional Language Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of DARPA or the US government.
References
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building
a treebank for French, chapter 10. Kluwer.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
M. Candito and B. Crabb?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT.
152
ll l l l l
l l l l
l
l
l l
l l
l l
l l
29
30
31
32
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(a) En-Fr tuning
l
l
l l l l l l
l l
l
l
l l l
l l
l l l
7.5
10.0
12.5
15.0
17.5
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(b) En-De tuning
Figure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In NAACL.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In HLT-NAACL.
S. Green, M-C. de Marneffe, and C. D. Manning.
2013a. Parsing models for identifying multiword
expressions. Computational Linguistics, 39(1):195?
227.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL.
B. Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In LREC.
153
