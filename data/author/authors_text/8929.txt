Aggressive Morphology for Robust Lexical Coverage 
Will iam A. Woods 
Sun Microsystems Laborator ies  
1 Network Drive 
Bur l ington,  MA 01803 
Wi l l iam.Woods@east .sun.corn 
Abstract 
This paper describes an approach to providing lex- 
ical information for natural anguage processing in 
unrestricted domains. A system of approximately 
1200 morphological rules is used to extend acore lex- 
icon of 39,000 words to provide lexical coverage that 
exceeds that of a lexicon of 80,000 words or 150,000 
word forms. The morphological system is described, 
and lexical coverage is evaluated for random words 
chosen from a previously unanalyzed corpus. 
1 Motivation 
Many applications of natural language processing 
have a need for a large vocabulary lexicon. How- 
ever, no matter how large a lexicon one starts with, 
most applications will encounter terms that are not 
covered. This paper describes an approach to the 
lexicon problem that emphasizes recognition of mor- 
phological structure in unknown words in order to 
extend a relatively small core lexicon to allow ro- 
bust natural language processing in unrestricted do- 
mains. This technique, which extends functionality 
originally developed for the Lunar system (Woods et 
al., 1972), has been most recently applied in a con- 
ceptual indexing and retrieval system (Woods, 1997; 
Ambroziak and Woods, 1998; Woods et ai., 2000). 
The system described here uses a collection of 
approximately 1200 knowledge-based morphologi- 
cal rules to extend a core lexicon of approximately 
39,000 words to give coverage that exceeds that of an 
English lexicon of more than 80,000 base forms (or 
150,000 base plus inflected forms). To illustrate the 
need for a robust extensible l xicon, a random sam- 
ple of 100 words from the vocabulary of the million- 
word Brown corpus (Kucera and Francis, 1967), con- 
tained 24 words that were not included in a 300,000- 
word list of English word forms. This suggests that 
approximately 25% of the words in the Brown cor- 
pus would not be covered by an independent lexicon 
of even 300,000 words. 
In a recent experiment, 54% of approximately 
34,000 word types (numbers and hyphenated words 
excluded) from a 3.1-million-word corpus of techni- 
cal literature would not be covered by our hypothet- 
ical 300,O00-word lexicon. Many of these are special 
forms (e.g., Nb203 and Ti/tin), and some are ap- 
parent misspellings (e.g., auniprocessor and sychro- 
nized), but the following are a sampling of fairly nor- 
mal words that were not in the 300,O00-word list: 
busmaster 
copyline 
hereabove 
preprocessing 
uniprocessors 
unreacted 
2 Integrated, Preferential, Heuristic 
Morpho logy  
There are a number of systems that have been used 
to describe natural language morphology for compu- 
tational use. The most popular of these is perhaps 
the finite-state Kimmo system (Koskenniemi, 1983). 
Other approaches are described in (Sproat, 1992). 
The system described here differs from other systems 
in a number of dimensions. First, it is integrated 
with an extensive lexicon, a semantic ontology, and 
a syntactic analysis ystem, which it both consults 
and augments. For example, subsumption relation- 
ships in the semantic ontology enable the system to 
determine whether a proposed root is a container or 
a mental attitude, so that cupful is interpreted as a 
unit of measure (a kind of noun), while hopeful is 
interpreted as an adjective. 
Second, it uses ordered preferential rules that at- 
tempt to choose a small number of correct analy- 
ses of a word (usually 1-3) from the many potential 
analyses that might be found. Finally, it uses rules 
that are heuristic in that they are not guaranteed to
give correct analyses, but rather are designed to deal 
with various tates of lack of knowledge and to make 
plausible inferences in the face of uncertainty. The 
focus is to use what it knows (or can infer) to de- 
termine a usable set of part-of-speech lassifications 
for the word and to determine any root-plus-affix 
or internal compound structure that is apparent. If 
possible, it also assigns a semantic ategorization to 
the word. It deals with unknown as well as known 
218
roots, and it indicates relative confidences in its clas- 
sifications when its rules indicate uncertainty in the 
result. 
The role of the morphological nalysis component 
in this system is to construct lexical entries for words 
that do not already have entries, so that subsequent 
encounters with the same word will find an already 
existing lexical entry. Thus, morphological nalysis 
happens only once for each encountered word type 
that is not already in the core lexicon. The resulting 
lexical entries can be saved in a supplementary lex- 
icon that is constructed as a side-effect of analyzing 
text. The rules of the morphological analysis sys- 
tem can ask syntactic and semantic questions about 
potential base forms. The system handles prefixes, 
suffixes, and lexical compounds (e.g., bitmap and re- 
plybuffer). It also handles multiword lexical items 
and many special forms, including Roman numer- 
als, dates, and apparent phone numbers. 
2.1 Morpho log ica l  rules and the  lexicon 
The morphological nalysis system makes use of a 
number of different kinds of morphological rules, ap- 
plied in the following preferential order to words that 
are not already in the lexicon: 
1. Morph-precheck for special forms 
2. Phase one pass with suffix rules (allow only 
"known" roots in phase one) 
3. Prefix rules 
4. Lexical compound rules 
5. Check of name lists and city lists for words not 
yet recognized 
6. Phase two pass with suffix rules (allow unknown 
roots and default rules) 
Generally, the rules are ordered in decreasing or- 
der of specificity, confidence and likelihood. Very 
specific tests are applied in Step 1 to identify and 
deal with "words" that are not ordinary sequences 
of alphabetic haracters. These include numbers, 
alphanumeric sequences, and expressions involving 
special characters. Falling this, an ordered sequence 
of suffix rules is applied in Step 2 in a first pass that 
will allow a match only if the proposed root word is 
"known." The same list of rules will be applied later 
in a second pass without this known-root condition 
if an earlier analysis does not succeed. This issue of 
"known" roots is a subtle one that can involve con- 
sulting external lists of known words as well as words 
already in the lexicon, and can also consider certain 
derived forms of known roots to be "known," even 
when they have not been previously encountered. 
For example, if.fish is a known word, then fishing is 
as good as known, so is considered a "known" root 
for this purpose. In general, suffix rules applied to 
"known" roots are more reliable than applications of 
rules to unknown roots or to words with no identifi- 
able root. 
If no phase-one suffix rules apply, prefix rules are 
tried in Step 3 to see if an interpretation of this word 
as a prefix combined with some other "known" word 
is possible. Falling this, a set of lexical compound 
rules is tried, in Step 4, to see if the word is inter- 
pretable as a compound of two or more words, and 
failing that, lists of first and last names of people 
and names of cities are checked in Step 5. All of 
steps 3-5 are considered more reliable if they suc- 
ceed than the phase-two pass of the suffix rules that 
comes in Step 6. This ordering allows prefixes and 
compounding to be tried before less confident suffix 
analyses are attempted, and avoids applying weak 
suffix analyses to known names. Various other ways 
to order these rules have been tried, but this order 
has been found to be the most effective. 
2.2 Special  fo rm tests  
Before trying pattern-based rules for suffixes, pre- 
fixes, and lexical compounds, the morphological n- 
alyzer makes a number of tests for special forms that 
require idiosyncratic treatment. These tests include 
the following: 
? number (including integer, floating, and expo- 
nential notations, including numbers too large 
to be represented internally as numbers in the 
machine), 
? Roman numeral (vii, mcm), 
? ordinal (1st, 2nd, twenty-third), 
? alphanum (Al203, 79D), 
? letter (b, x), 
? initial (B.), 
? phone number (123-4567), 
? hyphenated adjective (all-volunteer), 
? ra t io  (s/S, V/R), 
? multiword lexical item (snake_in_the_grass), 
? special proper nouns (gls@mit.edu, /usr/bin, 
http://www.sun.com, C+ + ) 
2.3 Pat tern -act ion  rules 
Suffix rules in this system are pattern-action rules 
that specify: 
1. a pattern of characters to match at the end of 
the word to be analyzed, 
2. possibly a number of characters to remove 
and/or a sequence of characters to add to form 
a root (or base form), 
3. a sequence of tests and action clauses indicating 
possible interpretations of a word matching this 
pattern. 
'21_q 219
These rules are organized into blocks that are typi- 
cally indexed by a shared final letter, and are applied 
in order within a block until a rule is encountered 
that generates one or more interpretations. At that 
point, no further rules are tried, and the interpreta- 
tions generated by that rule are used to construct a
lexical entry for the analyzed word. 
The following is an example of a fairly specific, 
but productive, knowledge-rich morphological suffix 
rule: 
((f i s h) (kill 4) 
(test (plausible-root root)) 
(cat nmsp 
(is-root-of-cat root '(adj n)) 
eval (progu (mark-dict lex 
(mark-dict 
(mark-dict 
(mark-d ict  
' -es))) 
'false-root 
root t t) 
lex 
'kindof 
'fish t t )  
lex 
'has-prefix 
root t t )  
l ex  
'root 
'fish t t) 
This rule matches a word that ends in fish and 
removes four letters from the end (the fish part) to 
produce a root word which it then tests to see if it 
is a plausible root (e.g., does it at least have a vowel 
in it?). If it gets this fax, the rule will construct 
a category nmsp interpretation (a kind of noun), if 
the condition ( i s - root -o f -cat  root  ' (adj n))  is 
true (i.e., if the root is a known adjective or noun). 
This rule deals with words like hagfish and goatfish 
and comes before the rules that handle words with 
ish as a suffix, like doltish and oafish. Incidentally, 
this rule doesn't apply to oafish because the hypoth- 
esized root on, which would result from removing 
four letters, is not known to be an adjective or noun. 
When this rule succeeds, it specifies that the word 
will be assigned the category nmsp, a category indi- 
cating a word that has a mass sense, a singular count 
sense, and can also be used as a plural (e.g., Goatfish 
are 1%nny-looking.). (The category nmsp comes from 
a collection of 91 syntactic ategories, organized in 
a hierarchy based on generality, so that, for exam- 
ple, nm subsumes nmsp.) The action part of this rule 
specifies that (contrary to the usual case) the "root" 
obtained by removing characters from the end of the 
word (e.g., goat) is in this case a false root. The real 
root is fish, and the false root (goat) is actually a 
prefix. The rule also specifies that the word refers 
to a kind of fish and that the inflectional paradigm 
for this word is -es (thus allowing goatfishes as an 
alternative plural). 
The rules within a block are ordered in decreasing 
order of confidence and specificity. Thus, rules with 
conditions that check explicit inflectional paradigms 
of known roots are ordered before rules that guess 
the inflectional paradigm from the spelling of the 
root, and rules with more specific conditions are or- 
dered before rules with less specific conditions so 
that the latter can assume that the former will al- 
ready have been tested and rejected. The rules 
within a block of suffix rules will typically try for 
interpretations in roughly the following order: 
1. inflected form of a known root satisfying a 
named inflectional paradigm (paradigmatic) 
2. inflected form of a known word in right category 
with unknown inflectional paradigm 
3. apparent inflected form of a known word of 
some other category 
4. apparent inflected form of an unknown word 
5. apparent derived form of a known root of the 
right category 
6. apparent derived form of a known root regard- 
less of category 
7. apparent derived form of an unknown root 
8. word with apparent syntactic ategory and per- 
haps suffix, without identifiable root 
9. guessed noun (and perhaps verb also, if core 
vocabulary is not comprehensive) 
The last rule in this sequence is a default guess- 
ing rule that depends on a flag that tells it whether 
it is running with a core lexicon that is believed to 
contain most nonobvious verbs. If so, then only the 
noun part-of-speech is assigned, but with a smaller 
core lexicon, the guessing rules would also assign a 
less likely interpretation as a verb, in order to pro- 
vide a way for unknown verbs to be parsed correctly 
in sentences. 
Prefix rules are similar in structure to suffix rules, 
except hat the pattern is matched at the beginning 
of the word, and the rule blocks are indexed by the 
initial letter of the word. Lexical compound rules 
have a slightly different format and are called by a 
specialized interpreter that looks for places to divide 
a word into two pieces of sufficient size. The points 
of potential decomposition are searched from right 
to left, and the first such point that has an interpre- 
tation is taken, with the following exception: The 
morph compound analyzer checks for special cases 
where, for example, the first word is plural and ends 
in an s, but there is an alternative segmentation i  
which the singular of the first word is followed by a 
220 220
word starting with the s. In such cases, the decom- 
position using the singular first word is preferred 
over the one using the plural. For example, the 
word minesweeper will be analyzed as mine+sweeper 
rather than mines+weeper. This preference heuris- 
tic is specific to English and might be different for 
other languages. 
2.4 Recurs ive  app l icat ion  of  rules 
When attempting to apply a rule to a word, the 
morphological analyzer can be applied recursively 
to analyze the hypothesized root. A simple caching 
technique is used to control the potential for com- 
binatoric explosion and to block looping. This is 
sufficiently effective that the time required for mor- 
phological analysis is a negligible part of the time 
required for processing large amounts of natural an- 
guage text. Protection against looping is especially 
important for a kind of morphological rule that de- 
rives one word from another without either of them 
being a root of the other in the usual sense (e.g., de- 
riving communist from communism or external from 
internal). Operating in a loop-safe nvironment al- 
lows rules like these to identify the relationship be- 
tween a new word and a known word in either di- 
rection, whichever of the two forms is encountered 
first. 
3 Eva luat ion  
Since analyzing a word is done once per unknown 
word type and consumes a negligible fraction of the 
overall text-processing time, speed of operation is 
not considered a factor for evaluation. The inter- 
esting dimension of evaluation deals with the cov- 
erage of the rules and the kinds of errors that are 
made. This was tested by applying the system to 
two word lists randomly selected from the Brown 
corpus and provided to me by Philip Resnik, using 
some sampling tools that he developed. The first of 
these (the token sample) consists of 100 word tokens 
selected randomly, without eliminating duplicates, 
and the second (the type sample) consists of 100 
distinct word types selected randomly from the vo- 
cabulary of the Brown corpus. Prior to a single test 
run on each of these samples, neither the lexicon nor 
the morphological rule system had any exposure to 
the Brown corpus, nor had either of these word lists 
been looked at by the experimenter. Consequently, 
the results are a fair evaluation of the expected per- 
formance of this system on an unknown domain. 
3.1 Grad ing  ru le  per fo rmance  
Since different syntactic category errors have dif- 
ferent consequences for parsing text, it is useful to 
grade the syntactic ategory assignments of the ana- 
lyzer on an A-B-C-D-F scale according to the sever- 
ity of any mistakes. Grades are assigned to a lexical 
entry as follows: 
A if all appropriate syntactic ategories are assigned 
and no incorrect categories are assigned 
B if all categories are correct, allowing for catego- 
rizing an adjective or a name as a noun or a 
noun as a name 
C if an entry has at least one correct category and 
is correct except for missing a noun category or 
having a single extra category 
D if there is more than one extra category or if there 
is a missing category other than one of the above 
cases, provided that there is at least one correct 
category 
F if there are no correct categories 
Both A and B grades are considered acceptable 
assignments for the sake of evaluation, since cate- 
gory B errors would allow a reasonable parse to be 
found. This is because the grammar used for pars- 
ing sentences and phrases allows a noun to be used 
as an adjective modifier and a proper noun to be 
used in place of a noun. One parser/grammar that 
uses this lexicon also allows any other categoo; to be 
used as a noun, at the expense of a penalty, so that 
a C grade will still enable a parse, although with a 
penalty and a substantial likelihood that other false 
parses might score better. Similarly, a D grade in- 
creases the likelihood that a false parse might score 
better. 
Separately, we measure whether count/mass dis- 
tinctions are made correctly (for nouns only), 
and whether roots of derived and inflected forms 
are identified correctly. We are interested in 
the count/mass distinction because, like the com- 
mon/proper noun distinction, it affects the gram- 
maticality and likelihood of a noun phrase interpre- 
tation for a singular noun in absence of an explicit 
determiner. 
3.2 Sampl ing  rule per fo rmance  
The morphological nalyzer has been applied to the 
words from the two sample word lists that were not 
already in its core lexicon. There were 17 such 
words from the token sample and 72 such words 
from the type sample. Of the 17 unknown token- 
sample words, 100% were graded B or better (88% 
A and 12% B); 85% of the roots were identified cor- 
rectly (all but one); 85% of the count noun senses 
were found (all but one); and 100% of the mass noun 
senses were found. Token-sample performance is not 
a very challenging test for a morphological nalyzer 
because it is biased toward a relatively small number 
of frequently occurring word types. Token-sample 
performance is used to assess the per-token error rate 
that one would expect in analyzing large amounts of 
running text. In contrast, type-sample performance 
@@1 221
Table 1: Syntactic ate ;ory performance of the analyzer. 
Category Grade A B C D F B or better 
Number 62 8 1 0 1 70 
Percent 86% 11% 1.5% 0% 1.5% 97% 
Table 2: Count/mass distinction performance of the analyzer. 
Count/mass Good count Extra count Good mass Missing mass 
Number 39 1 14 1 
Percent 100% 2.6% 93% 6.7% 
Table 3: Root identification performance of the analyzer. 
Detect root Good Wrong Debatable Missing Extra 
Number 57 1 1 0 1 
Percent 95% 1.7% 1.7% 0 1.7% 
gives a measure of the expected performance on new 
words the analyzer is likely to encounter. 
For the 72 words in the type sample that are not 
covered by the lexicon, Tables 1-3 show the syntactic 
category performance of the analyzer and its abilities 
to make count/mass distinctions and identify roots. 
Notes on incorrect or debatable analyses: 
1. One N (noun) for a probable name (Tonio), 
counted as B. 
2. Two NPR(proper name) for abbreviations; 
(A. V. may be ADJ, W.B. is correct), counted 
as one B and one A. 
3. One wrong root when suffix ism was identified 
as root of hooliganism in a hypothesized com- 
pound hooligan+ism (arguably justifiable as a 
kind of ism, which is known in the lexicon, but 
counted as an error anyway). Reanalyzing this 
word after hooligan is a known word gets the 
correct interpretation. 
4. One debatable root in the hyphenated phrase 
reference-points whose root was listed as points 
rather than reference-point. This is due to a 
bug that caused the hyphenated word rules to 
incorrectly identify this as a verb, rather than 
a noun (counted as F for syntax). 
5. One extra root for embouchure from embouche 
(but a correct form of the French root?). 
6. One missing category N for bobbles, which was 
given category V but not N because the core 
lexicon incorrectly listed bobble only as a verb 
(counted as C for syntax). This is corrected by 
adding the missing category to the lexical entry 
for bobble. 
4 Conclus ions 
We have described an approach to robust lexical cov- 
erage for unrestricted text applications that makes 
use of an aggressive set of morphological rules to 
supplement a core lexicon of approximately 39,000 
words to give lexical coverage that exceeds that of a 
much larger lexicon. This morphological nalyzer 
is integrated with an extensive lexicon, an ontol- 
ogy, and a syntactic analysis ystem, which it both 
consults and augments. It uses ordered preferential 
rules that attempt to choose a small number of cor- 
rect analyses of a word and are designed to deal with 
various states of lack of knowledge. When applied 
to 72 unknown words from a random sample of 100 
distinct word types from the Brown corpus, its syn- 
tactic category assignments received a grade of B or 
better (using a grading system explained herein) for 
97% of the words, and it correctly identified 95% 
of the root words. This performance demonstrates 
that one can obtain robust lexical coverage for natu- 
ral language processing applications in unrestricted 
domains, using a relatively small core lexicon and an 
aggressive collection of morphological rules. 
References 
Jacek Ambroziak and William A. Woods. 1998. 
Natural anguage technology in precision content 
retrieval. In International Conference on Natural 
Language Processing and Industrial Applications, 
Moncton, New Brunswick, Canada, August. 
www.sun.com/research/techrep/1998/abstract- 
69.html. 
Kimmo Koskenniemi. 1983. Two-level model for 
morphological nalysis. In Proceedings of the In- 
ternational Joint Conference on Artificial Intelli- 
gence, pages 683-685, Los Angelos, CA. Morgan 
Kauffmann. 
H. Kucera and W. Francis. 1967. Computa- 
tional Analysis of Present-Day American English. 
Brown University Press. 
222
Richard Sproat. 1992. Morphology and Computa- 
tion. MIT Press, Cambridge, MA. 
William A. Woods, Ronald M. Kaplan, and Bon- 
nie L. Nash-Webber. 1972. The lunar sciences 
natural anguage information system: Final re- 
port. Technical Report BBN Report No. 2378, 
Bolt Beranek and Newman Inc, Cambridge, MA, 
June. (available from NTIS as N72-28984). 
William A. Woods, Lawrence A. Bookman, Ann C. 
Houston, Robert J. Kuhns, Paul A. Martin, and 
Stephen Green. 2000. Linguistic knowledge can 
improve information retrieval. In (these proceed- 
ings). 
William A. Woods. 1997. Conceptual indexing: 
A better way to organize knowledge. Technical 
Report SMLI TR-97-61, Sun Microsystems 
Laboratories, Mountain View, CA, April. 
www.sun.com/research/techrep/1997/abstract- 
61.html. 
223 223
Linguistic Knowledge can Improve Information Retrieval 
Wi l l iam A .  Woods  and Lawrence  A .  Bookman*  and Ann Houston  and 
Rober t  J .  Kuhns  and Pau l  Mar t in  and Stephen Green  
Sun Microsystems Laborator ies 
1 Network Drive 
Bur l ington,  MA 01803 
{Wi l l iam.Woods ,Ann.Houston,Robert .Kuhns ,Pau l .Mart in ,Stephen.Green}@east .sun.com 
Abstract 
This paper describes the results of some experiments 
using a new approach to information access that 
combines techniques from natural language process- 
ing and knowledge representation with a penalty- 
based technique for relevance estimation and passage 
retrieval. Unlike many attempts to combine natural 
language processing with information retrieval, these 
results how substantial benefit from using linguistic 
knowledge. 
1 In t roduct ion  
An online information seeker often fails to find what 
is wanted because the words used in the request are 
different from the words used in the relevant mate- 
rial. Moreover, the searcher usually spends a signifi- 
cant amount of time reading retrieved material in or- 
der to determine whether it contains the information 
sought. To address these problems, a system has 
been developed at Sun Microsystems Laboratories 
(Ambroziak and Woods, 1998) that uses techniques 
from natural language processing and knowledge 
representation, with a technique for dynamic pas- 
sage selection and scoring, to significantly improve 
retrieval performance. This system is able to locate 
specific passages in the indexed material where the 
requested information appears to be, and to score 
those passages with a penalty-based score that is 
highly correlated with the likelihood that they con- 
tain relevant information. This ability, which we call 
"Precision Content Retrieval" is achieved by com- 
bining a system for Conceptual Indexing with an 
algorithm for Relaxation-Ranking Specific Passage 
Retrieval. 
In this paper, we show how linguistic knowledge is
used to improve search effectiveness in this system. 
This is of particular interest, since many previous at- 
tempts to use linguistic knowledge to improve infor- 
mation retrieval have met with little or mixed suc- 
cess (Fagan, 1989; Lewis and Sparck Jones, 1996; 
Sparck Jones, 1998; Varile and Zampolli, 1997; 
Voorhees, 1993; Mandala et al, 1999) (but see the 
latter for some successes as well). 
* Lawrence  Bookman is now at  Tor rent  Systems,  Inc.  
2 Conceptua l  Indexing 
The conceptual indexing and retrieval system used 
for these experiments automatically extracts words 
and phrases from unrestricted text and organizes 
them into a semantic network that integrates syn- 
tactic, semantic, and morphological relationships. 
The resulting conceptual taxonomy (Woods, 1997) is 
used by a specific passage-retrieval algorithm to deal 
with many paraphrase r lationships and to find spe- 
cific passages of text where the information sought is 
likely to occur. It uses a lexicon containing syntac- 
tic, semantic, and morphological information about 
words, word senses, and phrases to provide a base 
source of semantic and morphological relationships 
that are used to organize the taxonomy. In addi- 
tion, it uses an extensive system of knowledge-based 
morphological rules and functions to analyze words 
that are not already in its lexicon, in order to con- 
struct new lexical entries for previously unknown 
words (Woods, 2000). In addition to rules for han- 
dling derived and inflected forms of known words, 
the system includes rules for lexical compounds and 
rules that are capable of making reasonable guesses 
for totally unknown words. 
A pilot version of this indexing and retrieval 
system, implemented in Lisp, uses a collection of 
approximately 1200 knowledge-based morphologi- 
cal rules to extend a core lexicon of approximately 
39,000 words to give coverage that exceeds that of an 
English lexicon of more than 80,000 base forms (or 
150,000 base plus inflected forms). Later versions 
of the conceptual indexing and retrieval system, im- 
plemented in C++, use a lexicon of approximately 
150,000 word forms that is automatically generated 
by the Lisp-based morphological nalysis from its 
core lexicon and an input word list. The base lexicon 
is extended further by an extensive name dictionary 
and by further morphological nalysis of unknown 
words at indexing time. This paper will describe 
some experiments u ing several versions of this sys- 
tem. In particular, it will focus on the role that the 
linguistic knowledge sources play in its operation. 
The lexicon used by the conceptual indexing sys- 
tem contains yntactic information that can be used 
262 
for the analysis of phrases, as well as morphologi- 
cal and semantic information that is used to relate 
more specific concepts to more general concepts in 
the conceptual taxonomy. This information is inte- 
grated into the conceptual taxonomy by considering 
base forms of words to subsume their derived and 
inflected forms ("root subsumption") and more gen- 
eral terms to subsume more specific terms. The sys- 
tem uses these relationships as the basis for infer- 
ring subsumption relationships between more gen- 
eral phrases and more specific phrases according to 
the intensional subsumption logic of Woods (Woods, 
1991). 
The largest base lexicon used by this system cur- 
rently contains semantic subsumption information 
for something in excess of 15,000 words. This infor- 
mation consists of basic "kind of" and "instance of" 
information such as the fact that book is a kind of 
document and washing is a kind of cleaning. The 
lexicon also records morphological roots and affixes 
for words that are derived or inflected forms of other 
words, and information about different word senses 
and their interrelationships. For example, the con- 
ceptual indexing system is able to categorize becomes 
black as a kind of color change because becomes i  an 
inflected form of become, become is a kind of change, 
and black is a color. Similarly, color disruption is 
recognized as a kind of color change, because the 
system recognizes disruption as a derived form of 
disrupt, which is known in the lexicon to be a kind 
of damage, which is known to be a kind of change. 
When using root subsumption as a technique for 
information retrieval, it is important o have a core 
lexicon that knows correct morphological analyses 
for words that the rules would otherwise analyze in- 
correctly. For example, the following are some ex- 
amples of words that could be analyzed incorrectly if
the correct interpretations were not specified in the 
lexicon: 
de legate  (de4.1eg4.ate) take the legs from 
caress (car + ess) female car 
cashier  (cashy 4. er) more wealthy 
daredev i l  (dared + evil) serious risk 
lacerate (lace 4. rate) speed of tatting 
pant ry  (pant + ry) heavy breathing 
p igeon (pig + eon) the age of peccaries 
ratify (rat 4- ify) infest with rodents 
in fant ry  (infant + ry) childish behavior 
Although they are not always as humorous as the 
above examples, there are over 3,000 words in the 
core lexicon of 39,000 English words that would re- 
ceive false morphological nalyses like the above ex- 
amples, if the words were not already in the lexicon. 
3 Re laxat ion  Rank ing  and  Spec i f i c  
Passage  Ret r ieva l  
The system we are evaluating uses a technique called 
"relaxation ranking" to find specific passages where 
as many as possible of the different elements of 
a query occur near each other, preferably in the 
same form and word order and preferably closer 
together. Such passages are ranked by a penalty 
score that measures the degree of deviation from an 
exact match of the requested phrase, with smaller 
penalties being preferred. Differences in morpholog- 
ical form and formal subsumption of index terms 
by query terms introduce small penalties, while in- 
tervening words, unexplained permutations of word 
order, and crossing sentence boundaries introduce 
more significant penalties. Elements of a query that 
cannot be found nearby introduce substantial penal- 
ties that depend on the syntactic ategories of the 
missing words. 
When the conceptual indexing system is presented 
with a query, the relaxation-ranking retrieval algo- 
rithm searches through the conceptual taxonomy for 
appropriately related concepts and uses the posi- 
tions of those concepts in the indexed material to 
find specific passages that are likely to address the 
information eeds of the request. This search can 
find relationships from base forms of words to de- 
rived forms and from more general terms to more 
specific terms, by following paths in the conceptual 
taxonomy. 
For example, the following is a passage retrieved 
by this system, when applied to the UNIX ? operat- 
ing system online documentation (the "man pages"): 
Query: print a message from the mail tool 
6. -2.84 print mail mail mai l tool  
Print sends copies of all the selected mail 
items to your default printer. If there are 
no selected items, mailtool sends copies of 
those items you axe currently... 
The indicated passage is ranked 6th in a returned 
list of found passages, indicated by the 6 in the above 
display. The number -2.84 is the penalty score as- 
signed to the passage, and the subsequent words 
print, mail, mail, and mailtool indicate the words 
in the text that are matched to the corresponding 
content words in the input query. In this case, print 
is matched to print, message to mail, mail to mail, 
and tool to mailtool, respectively. This is followed 
by the content of the actual passage located. The 
information provided in these hit displays gives the 
information seeker a clear idea of why the passage 
was retrieved and enables the searcher to quickly 
skip down the hit list with little time spent looking 
at irrelevant passages. In this case, it was easy to 
263 
identify that the 6th ranked hit was the best one and 
contained the relevant information. 
The retrieval of this passage involved use of a se- 
mantic subsumption relationship to match message 
to mail, because the lexical entry for mail recorded 
that it was a kind of message. It used a morpho- 
logical root subsumption to match tool to mailtool 
because the morphological nalyzer analyzed the un- 
known word mailtool as a compound of mail and tool 
and recorded that its root was tool and that it was 
a kind of tool modified by mail. Taking away the 
ability to morphologically analyze unknown words 
would have blocked the retrieval of this passage, 
as would eliminating the lexical subsumption entry 
that recorded mail as a kind of message. 
Like other approaches to passage retrieval 
(Kaszkiel and Zobel, 1997; Salton et al, 1993; 
Callan, 1994), the relaxation-ranking retrieval algo- 
rithm identifies relevant passages rather than simply 
identifying whole documents. However, unlike ap- 
proaches that involve segmenting the material into 
paragraphs or other small passages before indexing, 
this algorithm dynamically constructs relevant pas- 
sages in response to requests. When responding to 
a request, it uses information i  the index about po- 
sitions of concepts in the text to identify relevant 
passages. In response to a single request, identified 
passages may range in size from a single word or 
phrase to several sentences or paragraphs, depend- 
ing on how much context is required to capture the 
various elements of the request. 
In a user interface to the specific passage retrieval 
system, retrieved passages are reported to the user 
in increasing order of penalty, together with the rank 
number, penalty score, information about which tar- 
get terms match the corresponding query terms, and 
the content of the identified passage with some sur- 
rounding context as illustrated above. In one version 
of this technology, results are presented in a hyper- 
text interface that allows the user to click on any 
of the presented items to see that passage in its en- 
tire context in the source document. In addition, 
the user can be presented with a display of portions 
of the conceptual taxonomy related to the terms in 
the request. This frequently reveals useful gener- 
alizations of the request hat would find additional 
relevant information, and it also conveys an under- 
standing of what concepts have been found in the 
material that will be matched by the query terms. 
For example, in one experiment, searching the on- 
line documentation for the Emacs text editor, the 
request jump to end of file resulted in feedback show- 
ing that jump was classified as a kind of move in the 
conceptual taxonomy. This led to a reformulated 
request, move to end of file, which successfully re- 
trieved the passage 9o to end of buffer. 
4 Experimental Eva luat ion  
In order to evaluate the effectiveness of the above 
techniques, a set of 90 queries was collected from 
a naive user of the UNIX operating system, 84 of 
which could be answered from the online documen- 
tation known as the man pages. A set of "correct" 
answers for each of these 84 queries was manually de- 
termined by an independent UNIX operating system 
expert, and a snapshot of the man pages collection 
was captured and indexed for retrieval. In order 
to compare this methodology with classical docu- 
ment retrieval techniques, we assign a ranking score 
to each document equal to the ra~king score of the 
best ranked passage that it contains. 
In rating the performance of a given method, we 
compute average recall and precision values at 10 
retrieved ocuments, and we also compute a "suc- 
cess rate" which is simply the percentage of queries 
for which an acceptable answer occurs in the top 
ten hits. The success rate is the principal factor 
on which we base our evaluations, ince for this ap- 
plication, the user is not interested in subsequent 
answers once an acceptable answer has been found, 
and finding one answer for each of two requests i  a 
substantially better esult than finding two answers 
to one request and none for another. 
These experiments were conducted using an ex- 
perimental retrieval system that combined a Lisp- 
based language processing stage with a C++ im- 
plementation of a conceptual indexer. The linguis- 
tic knowledge sources used in these experiments in- 
cluded a core lexicon of approximately 18,000 words, 
a substantial set of morphological rules, and spe- 
cialized morphological algorithms covering inflec- 
tions, prefixes, suffixes, lexical compounding, and 
a variety of special forms, including numbers, ordi- 
nals, Roman numerals, dates, phone numbers, and 
acronyms. In addition, they made use of a lexical 
subsumption taxonomy of approximately 3000 lex- 
ical subsumption relations, and a small set of se- 
mantic entailment axioms (e.g., display entails see, 
but is not a kind of see). This system is described 
in (Woods, 1997). The database was a snapshot of 
the local man pages (frozen at the time of the ex- 
periment so that it wouldn't change during the ex- 
periment), consisting of approximately 1800 files of 
varying lengths and constituting a total of approxi- 
mately 10 megabytes of text. 
Table 1 shows the results of comparing three ver- 
sions of this technology with a textbook implementa- 
tion of the standard tfid\] algorithm (Salton, 1989) 
and with the SearchItWMsearch application devel- 
oped at Sun Microsystems, Inc., which combines a
264 
Table 1: A comparison of different retrieval techniques. 
System 
tfidf 
SearchIt system 
Recall II 
w/o morph 
w/o knowledge 
Success Rate 
28.6% 
44.0% 
60.7% 
50.0% 
42.9% 
Recall 
(10 docs) 
14.8% 
28.5% 
38.6% 
not measured 
not measured 
Precision 
(10 docs) 
2.9% 
7.4% 
7.3% 
not measured 
not measured 
simple morphological query expansion with a state- 
of-the-art commercial search engine. In the table, 
Recall II refers to the full conceptual indexing and 
search system with all of its knowledge sources and 
rules. The line labeled "w/o morph" refers to this 
system with its dynamic morphological rules turned 
off, and the line labeled "w/o knowledge" refers to 
this system with all of its knowledge sources and 
rules turned off. The table presents the success 
rate and the measured recall and precision values 
for 10 retrieved ocuments. We measured recall and 
precision at the 10 document level because inter- 
nal studies of searching behavior had shown that 
users tended to give up if an answer was not found 
in the first ten ranked hits. We measured success 
rate, rather than recall and precision, for our ab- 
lation studies, because standard recall and precision 
measures are not sensitive to the distinction between 
finding multiple answers to a single request versus 
finding at least one answer for more requests. 
5 D iscuss ion  
Table 1 shows that for this task, the relaxation- 
ranking passage retrieval algorithm without its sup- 
plementary knowledge sources (Recall II w/o knowl- 
edge) is roughly comparable in performance (42.9% 
versus 44.0% success rate) to a state-of-the-art com- 
mercial search engine (SearchIt) at the pure docu- 
ment retrieval task (neglecting the added benefit of 
locating the specific passages). Adding the knowl- 
edge in the core lexicon (which includes morpho- 
logical relationships, emantic subsumption axioms, 
and entailment relationships), but without morpho- 
logical analysis of unknown words (Recall II w/o 
morph), significantly improves these results (from 
42.9% to 50.0%). Further adding the morphologi- 
cal analysis capability that automatically analyzes 
unknown words (deriving additional morphological 
relationships and some semantic subsumption rela- 
tionships) significantly improves that result (from 
50.0% to 60.7%). In contrast, we found that adding 
the same semantic subsumption relationships to the 
commercial search engine, using its provided the- 
saurus capability degraded its results, and results 
were still degraded when we added only those facts 
that we knew would help find relevant documents. 
It turned out that the additional relevant documents 
found were more than offset by additional irrelevant 
documents hat were also ranked more highly. 
6 Anecdota l  Eva luat ion  o f  Spec i f i c  
Passage  Ret r ieva l  Benef i t s  
As mentioned above, comparing the relaxation- 
ranking algorithm with document retrieval systems 
measures only a part of the benefit of the specific 
passage retrieval methodology. Fully evaluating the 
quality and ranking of the retrieved passages in- 
volves a great many subtleties. However, two in- 
formal evaluations have been conducted that :shed 
some light on the benefits. 
The first of these was a pilot study of the tech- 
nology at a telecommunications company. In that 
study, one user found that she could use a single 
query to the conceptual indexing system to find both 
of the items of information ecessary to complete a
task that formerly required searching two separate 
databases. The conclusion of that study was that the 
concept retrieval technology performs well enough to 
be useful to a person talking live with a customer. 
It was observed that the returned hits can be com- 
pared with one another easily and quickly by eye, 
and attention is taken directly to the relevant con- 
tent of a large document: The automatic indexing 
was considered a plus compared with manual meth- 
ods of content indexing. It was observed that an area 
of great potential may be in a form of knowledge 
management that involves organizing and providing 
intelligent access to small, unrelated "nuggets" of 
textual knowledge that are not amenable to conven- 
tional database archival or categorization. 
A second experiment was conducted by the Hu- 
man Resources Webmaster of a high-tech company, 
an experienced user of search engines who used this 
technology to index his company's internal HR web 
site. He then measured the time it took him to pro- 
cess 15 typical HR requests, first using conventional 
search tools that he had available, and then using 
the Conceptual Indexing technology. In both cases, 
he measured the time it took him to either find the 
answer or to conclude that the answer wasn't in 
the indexed material. His measured times for the 
total suite were 55 minutes using the conventional 
265 
tools and 11 minutes using the conceptual index- 
ing technology. Of course, this was an uncontrolled 
experiment, and there is some potential that infor- 
mation learned from searching with the traditional 
tools (which were apparently used first) might have 
provided some benefit when using the conceptual in- 
dexing technology. However, the fact that he found 
things with the latter that he did not find with the 
former and the magnitude of the time difference sug- 
gests that there is an effect, albeit perhaps not as 
great as the measurements. As a result of this ex- 
perience, he concluded that he would expect many 
users to take much longer to find materials or give 
up, when using the traditional tools. He anticipated 
that after finding some initial materials, more time 
would be required, as users would end up having 
to call people for additional information. He esti- 
mated that users could spend up to an hour trying 
to get the information they needed...having to call 
someone, wait to make contact and finally get the 
information they needed. Using the conceptual in- 
dexing search engine, he expected that these times 
would be at least halved. 
7 Conc lus ion  
We have described some experiments using lin- 
guistic knowledge in an information retrieval sys- 
tem in which passages within texts are dynami- 
cally found in response to a query and are scored 
and ranked based on a relaxation of constraints. 
This is a different approach from previous meth- 
ods of passage retrieval and from previous attempts 
to use linguistic knowledge in information retrieval. 
These experiments show that linguistic knowledge 
can significantly improve information retrieval per- 
formance when incorporated into a knowledge-based 
relaxation-ranking algorithm for specific passage re- 
trieval. 
The linguistic knowledge considered here includes 
the use of morphological relationships between 
words, taxonomic relationships between concepts, 
and general semantic entailment relationships be- 
tween words and concepts. We have shown that the 
combination of these three knowledge sources can 
significantly improve performance in finding appro- 
priate answers to specific queries when incorporated 
into a relaxation-ranking algorithm. It appears that 
the penalty-based relaxation-ranking algorithm fig- 
ures crucially in this success, since the addition of 
such linguistic knowledge to traditional information 
retrieval models typically degrades retrieval perfor- 
mance rather than improving it, a pattern that was 
borne out in our own experiments. 
Acknowledgments  
Many other people have been involved in creating 
the conceptual indexing and retrieval system de- 
scribed here. These include: Gary Adams, Jacek 
Ambroziak, Cookie Callahan, Chris Colby, Jim 
Flowers, Ellen Hays, Patrick Martin, Peter Norvig, 
Tony Passera, Philip Resnik, Robert Sproull, and 
Mark Torrance. 
Sun, Sun Microsystems, and SearchIt are trade- 
marks or registered trademarks of Sun Microsys- 
tems, Inc. in the U.S. and other countries. 
UNIX is a registered trademark in the United 
States and other countries, exclusively licensed 
through X/Open Company, Ltd. UNIX est une 
marque enregistree aux Etats-Unis et dans d'autres 
pays et licenci~e xclusivement par X/Open Com- 
pany Ltd. 
Re ferences  
Jacek Ambroziak and William A. Woods. 1998. 
Natural language technology in precision content 
retrieval. In International Conference on Natural 
Language Processing and Industrial Applications, 
Moncton, New Brunswick, Canada, August. 
www.stm.com/research/techrep/1998/abstract- 
69.html. 
Jamie P. Callan. 1994. Passage-level vidgnce in 
document retrieval. SIGIR, pages 302-309. 
J. L. Fagan. 1989. The effectiveness of a nonsyntac- 
tic approach to automatic phrase indexing for doc- 
ument retrieval. Journal of the American Society 
for Information Science, 40(2):115-132, March. 
Martin Kaszkiel and Justin Zobel. 1997. Passage 
retrieval revisited. SIGIR, pages 302-309. 
David D. Lewis and Karen Sparck Jones. 1996. Nat- 
ural language processing for information retrieval. 
CACM, 39(1):92-101. 
Rila Mandala, Takenobu Tokunaga, and Hozumi 
Tanaka. 1999. Combining multiple evidence from 
different ypes of thesaurus for query expansion. 
In Proceedings on the 22nd annual international 
A CM SIGIR conference on Research and develop- 
ment in information retrieval. ACM-SIGIR. 
Gerald Salton, James Allan, and Chris Buckley. 
1993. Approaches to passage retrieval in full text 
information systems. SIGIR, pages 49-58. 
Gerard Salton. 1989. Automatic Text Processing. 
Addison Wesley, Reading, MA. 
Karen Sparck Jones. 1998. A look back and a look 
forward. SIGIR, pages 13-29. 
Giovanni Varile and Antonio Zampolli, editors. 
1997. Survey of the State of the Art in Human 
Language Technology. Cambridge Univ. Press. 
Ellen M. Voorhees. 1993. Using wordnet o disam- 
biguate word senses for text retrieval. In Pro- 
ceedings of 16th ACM SIGIR Conference. ACM- 
SIG1R. 
William A. Woods. 1991. Understanding subsump- 
tion and taxonomy: A framework for progress. 
In John Sowa, editor, Principles of Semantic 
2ti f f  
Networks: Explorations in the Representation o/
Knowledge, pages 45-94. Morgan Kaufmann, San 
Mateo, CA. 
William A. Woods. 1997. Conceptual indexing: 
A better way to organize knowledge. Technical 
Report SMLI TR-97-61, Sun Microsystems 
Laboratories, Mountain View, CA, April. 
www.sun.com/research/techrep/1997/abstract- 
61.html. 
William A. Woods. 2000. Aggressive morphology 
for robust \]exical coverage. In (these proceedings). 
267 
ACL Lifetime Achievement Award
The Right Tools: Reflections on Computation
and Language
William A. Woods?
ITA Software, Inc.
1. Introduction
Good morning. I want to thank the ACL for awarding me the 2010 Lifetime Achieve-
ment Award. I?m honored to be included in the ranks of my respected colleagues who
have received this award previously. I want to talk to you this morning about the
evolution of some ideas that I think are important, with a little bit of historical and
biographical context thrown in. I hope you?ll find in what I say not only an appreciation
for some of the ideas and where they came from, but also a trajectory that continues
forward and suggests some solutions to problems not yet solved.
1.1 Space: The First Frontier
Figure 1 is a picture of a moon rock. It has been argued that the modern era of com-
puters, and specifically the creation of DARPA and the subsequent invention of the
Internet, were all stimulated and driven by the race to conquer space. Coincidentally,
the beginnings of my own career in computing and computational linguistics are also
tied to space.
On October 4, 1957, the Soviet Union astounded the world by launching earth?s
first artificial satellite. Soon after the Sputnik launch, the Smithsonian Astrophysical
Observatory contacted an astronomer, Joseph Brady, at the University of California?s
Lawrence Radiation Laboratory in Livermore, California. They wanted to know if the
lab would track Sputnik and predict what was going to happen to it. Joe Brady had
the right tools for the job. He and his colleagues had been computing planetary orbits
in the solar system, and they could directly apply their programs to satellite orbits.
Joe was able to successfully predict the exact day that Sputnik was last seen in the
night skies over Washington, DC (December 1, 1957), 58 days after it was launched.
Joe Brady and his fellow astronomers were meticulous about accuracy. (Incidentally,
the Vanguard I satellite that the U.S. launched five months later is still up there and was
still transmitting signals in 1964. It was the first satellite to use solar cells, clearly the
right tool for that job!) Three years later, Joe Brady and his colleague Nevin Sherman
would initiate me into the small group of people who could program computers.
When Sputnik went up, I was a freshman in high school and just learning algebra,
which I thought was the greatest thing since sliced bread. When I graduated from high
? ITA Software, Inc., 141 Portland Street, Cambridge, MA 02139, USA. E-mail: wwoods@itasoftware.com.
This article is an extended version of the talk given on receipt of the ACL?s Lifetime Achievement Award
at the Association?s annual meeting in Uppsala, Sweden, on July 13, 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
Figure 1
A photograph of lunar rock, sample 10046.
school, I went to work for Joe Brady at the Livermore Laboratory as a programming
intern, before starting college at Ohio Wesleyan University. I began my professional
computing career by proofreading 300 years of Mars observations. I worked at the
Livermore Labs for four summers and learned a lot of things from that job, not the
least of which were patience, diligence, and thoughtful programming. I first learned to
program in numerical machine language on an IBM 650, the machine that Donald Knuth
credits as the origin of linked lists. Because the memory on the IBM 650 was a rotating
drum, latency could be optimized by distributing instructions through memory and
linking them together with next-instruction pointers. If you located your instructions
carefully, the next instruction would be rolling up to the read heads just as it was needed.
Later I programmed the IBM 709 and the Univac LARC. The LARC (which stood
for Livermore Advanced Research Computer) was one of the first supercomputers, and
was designed specifically for the Livermore Laboratory to perform scientific computa-
tions. One of the programs I wrote for the LARC was a sine and cosine routine that
was accurate to half a decimal digit in the 12th decimal place and was subsequently
incorporated into the Labs? Fortran library.
Twelve years later, on July 20, 1969, the United States astounded the world by
visiting the moon and subsequently returning with 47.5 pounds of lunar rocks. The
following summer, Jeffrey Warner, from the NASA Manned Spacecraft Center Lunar
Receiving Laboratory, visited me at Harvard University, where I was a recently minted
Assistant Professor. He wanted to know if I could build a question-answering system
that would allow lunar scientists to directly access the data that he had collected. It
turned out that I had the right tools for the job. I had just built a system for answering
natural English questions, using a methodology that could be applied to arbitrary
domains. I had already applied that system to airline flight schedules and to questions
about states, arcs, and paths in an ATN grammar.
At the invitation of Danny Bobrow, then at Bolt Beranek and Newman, Inc. (BBN),
I moved to BBN in 1970 and pursued this endeavor as my first project there. (However,
I continued to teach my courses at Harvard for several years thereafter and returned
602
Woods The Right Tools
to Harvard later as a Gordon McKay Professor of the Practice of Computer Science.)
With the help of Ron Kaplan, Bonnie Webber, and others at BBN, I was able to apply
my system to the lunar rocks domain and demonstrate the Lunar Sciences Natural Lan-
guage Information System, answering live questions from lunar scientists, at the Second
Annual Lunar Science Conference in January 1971. This demonstration, six months after
the project began, proved the validity of my claim that this methodology could be
applied easily to new domains. The basic elements, a general-purpose ATN grammar
for English and a procedural-semantics framework for semantic interpretation, were
powerful tools. I kept track of the questions that were asked at the conference and the
answers that the system produced, and I analyzed the things it got wrong. I cataloged
the successes and the failures (most of which were due to simple errors or missing
lexical entries or rules), and presented the results at the AFIPS Fall Joint Computer
Conference in 1973 (Woods 1973).
So this talk is going to be about tools: a bit about tools in general, but mostly about
the tools of our trade. First, however, I want to tell you a little bit about myself that may
help illustrate where I?m coming from and how I approach problems.
1.2 Beginnings
I consider myself fortunate to have grown up in West Virginia at a place and time when
dogs could run free, and so could little boys (see Figure 2). When I was in the second
grade, I showed my first interest in linguistics when I told my teacher that the English
pronoun system would work better if the pronoun you were confined to the singular
and you all was used for the plural. When I was about the same age, my father taught
Figure 2
A photograph of me sitting on a rock in West Virginia.
603
Computational Linguistics Volume 36, Number 4
me to use basic hand tools. I have been building things ever since. In high school, I built
my own computer. My algebra teacher had given me a handout that showed how a
mechanical relay could represent the binary numbers 0 and 1, and based on that and
a book called The Basics of Digital Computers, I built a binary adder and complementer
and entered it in a local science fair. I made my own relays for that computer, using
metal that I salvaged from used dog-food cans and wire that I unwound from cast-off
steering yokes from TV picture tubes. Figure 3 is a picture of me with my computer at
the science fair. I won first prize in the mathematics division and second place overall
in the fair. I have no doubt that this adventure was instrumental in my getting the job at
the Livermore Laboratory.
In college, I majored in Math and Physics and fell in love with the machine shop
in the Physics lab. Some of my physics experiments involved building the equipment I
needed to do the experiment. I learned to use a drill press and a metal lathe. I liked the
metal lathe so much that when I graduated, I bought one of my own. The thing about
lathes is that they are more of a platform than a tool. They hold a piece of material
and rotate it against a cutting tool that cuts away material until you?re left with the
shape you want. You can make almost any shape that is rotationally symmetric. On
a wood lathe, you select various gouges and cutting tools and hold them against the
work to make the cuts. On a metal lathe, the tool is mounted on a carriage that is moved
automatically along the work, while an operator (or a gear box, or a computer program)
makes precision adjustments. For many jobs, you make a custom cutting tool by taking
a bar of tool steel and grinding away metal to produce exactly the tool you need for the
job. Figure 4 shows some of the cutting tools I made and one of the resulting products.
You can see that I enjoyed the work.
What I learned from the lathe is that you don?t have to confine yourself to an
existing set of tools. You can make custom tools that are tailored to exactly the job you
Figure 3
A photograph of me in front of my science fair computer.
604
Woods The Right Tools
Figure 4
A photograph of some lathe tools I made and a miniature canon I made with them.
want to do. This is a perspective that I?ve carried into my research, and one that I want
to share with you today.
2. Language and Computation
I want to start with the observation that language is fundamentally computational.
Computational linguistics has a more intimate relationship with computers than many
other disciplines that use computers as a tool. When a computational biologist simulates
population dynamics, no animals die. When a meteorologist simulates the weather,
nothing gets wet. But computers really do parse sentences. Natural language question-
answering systems really do answer questions. The actions of language are in the
same space as computational activities, or alternatively, these particular computational
activities are in the same space as language and communication.
2.1 The Tools of Our Trade
So when I talk about tools of our trade you might think first of the computer, but that?s
not exactly what I have in mind. I?m thinking more of abstract things like theories,
algorithms, formalisms, and methodologies. In my graduate training, I learned many
such tools, including finite-state machines, context-free grammars, linear bounded au-
tomata, Turing machines, circuit theory, lattice theory, first-order logic, and algorithms
for parsing, search, optimization, and theorem-proving. I was fortunate to take Sheila
Greibach?s course in Formal Language Theory the year before she discovered abstract
families of languages. When I took it, she was teaching constructive proofs for things
like converting context-free grammars to Greibach normal form, constructing determin-
istic minimal state finite-state machines, and intersecting context-free grammars with
regular sets. These algorithms have been staples of my subsequent research. Tools of our
trade also include principles, notations, architectures, and resources such as dictionaries
and corpora.
605
Computational Linguistics Volume 36, Number 4
3. Formalisms
The first tool I want to talk about is the ATN grammar formalism, but first some
background.
The year that Sputnik went up was the same year that Noam Chomsky pub-
lished Syntactic Structures. When I started graduate school at Harvard, Transformational
Grammar was all the rage, and I attended lectures and seminars by linguists such as
Barbara Hall, George Lakoff, and Haj Ross. Two things impressed me about George
Lakoff. One of them was the meticulous examples that he would muster to support
his theories. The other was the fact that he would completely abandon a theory and
look for a new one as soon as he encountered a key example that his previous theory
couldn?t handle. Initially, these linguists were trying to write specific sequences of trans-
formational rules in an attempt to capture the regularities of English. Even as I watched,
however, this proved too difficult, and the effort shifted to proposing constraints on
such rule sets, without attempting to specify complete sequences of rules.
At the time, there were people trying to write parsers for transformational gram-
mars, but the transformational grammar formalism didn?t lend itself to efficient parsing
algorithms. Stanley Petrick at IBM worked for many years on a parsing system for
transformational grammars, and it was never very fast.
In contrast, the Harvard Predictive Analyzer developed by Kuno, Oettinger, and
Greibach for machine translation used a form of context-free grammar that became
known as Greibach normal form. With this formalism, they could parse real sentences
from real documents in reasonable times. In Greibach normal form, the right-hand side
of every rule started with a terminal symbol, so the operation of the parser was to
simply take the next word from the input, pair it with the symbol on the top of the
stack, and look for matching rules to replace the stack symbol with a sequence of new
symbols. Despite this efficiency, this was an exponential parsing algorithm, unlike the
CKY algorithm which was an n-cubed algorithm.
The problem with these context-free grammars, however, was that expressing the
detailed knowledge of real English syntax in a context-free grammar formalism could
require thousands of rules and generate hundreds of parses for real sentences. This
was because each time you wanted to account for a feature, such as number agreement
between a subject and object, or constraints between a category of verb and the kinds
of complements it could take, it would require copying a portion of the grammar to
produce specialized versions for each different case. This generally involved doubling
the size of some portion of the grammar for each new feature.
For example, consider the following expansions:
(1) S ? NP V (NP)
(2) S ? NP Vi
S ? NP Vt NP
(3) S ? NPs Vis
S ? NPs Vts NP
S ? NPp Vip
S ? NPp Vtp NP
In order to capture the constraint between transitive verbs and direct objects, we have
to make two copies of the original rule. In order to capture number agreement between
singular and plural subjects and singular and plural verbs, we have to make two copies
606
Woods The Right Tools
of each of these. After a number of such doublings, the size of the grammar can become
quite large and unmanageable.
3.1 ATN Grammars
In my doctoral thesis, which was devoted to semantic interpretation of natural lan-
guage questions, I presented a transformational grammar for a subset of English as
an existence proof that English sentences could be mapped into the kinds of syntactic
structures that I wanted for input to my semantic interpreter. Later, when I wanted
to implement a parser that would actually do it, neither context-free grammars nor
transformational grammars were good tools for the job. I wanted a tool that could do
everything that a transformational grammar could do, but do it efficiently.
I developed ATN grammars to meet this need. ATN grammars are represented by
state transition diagrams similar to finite-state transition diagrams, except that they
allow transitions to be labeled with phrase categories as well as individual word cat-
egories, and each phrase category has a corresponding transition diagram that specifies
how to recognize (or generate) phrases of that kind. In addition, the transitions can be
augmented with conditions and actions that can record information about constituents
parsed and can be tested to determine whether the transition is to be allowed or not.
Final states in the diagram are represented by POP arcs that can also have conditions
and can specify the structure to be returned for the constituent recognized. Transitions
are ordered, so that the grammar can express preferences about the relative order of
possible parses. Actions can include setting a weight that can also express preferences
among alternative parsings. These ATN grammars were able to do the kind of deep-
structure analyses of English sentences that transformational grammars could do, but
do them efficiently. I published my ATN paper in the Communications of the ACM in 1970
(Woods 1970).
I found that ATN diagrams were good tools for evolving a grammar as I uncovered
more and more of the patterns and regularities of natural English. Unlike context-free
grammars, I could set features for number agreement and test them later. I could test
whether a verb could be transitive before permitting a transition to pick up a direct
object. I found that after my ATN grammar reached a certain maturity, the changes
needed to handle new phenomena correctly were generally small and focused. When
I encountered a sentence that should have parsed, but didn?t, or one that didn?t parse
correctly, usually it was only necessary to tighten or relax a condition, or add a new
transition to pick up something and then join back into the main flow at a later state.
This was in marked contrast to the context-free grammar formalism, where changes
that were conceptually small might involve copying large portions of the grammar and
introducing a host of name variations to express a new feature. Figure 5 shows a portion
of the ATN grammar for the LUNAR system (Woods, Kaplan, and Nash-Webber 1972). I
found that something like 50 states and 200 transitions could account for quite a diverse
range of natural English.
3.1.1 Making Sense of Shorthands. ATNs developed from my observation that common
notations that linguists used to write compact rules with optional, repeatable, and alter-
native constituent sequences, like the following, were the same operations that defined
regular expressions:
(4) S ? NP V (NP) PP?
607
Computational Linguistics Volume 36, Number 4
Figure 5
A portion of the ATN grammar for the LUNAR system.
(5) S ? NP {Vi | Vt NP} PP?
(6) S ? NP {AUX Vtu NP | AUX Viu | Vt NP | Vi} (PP)?
Regular expressions, I knew, were equivalent to finite-state machines, which were typ-
ically represented by finite-state transition diagrams. So what if we replaced the right-
hand sides of these rules with transition diagrams? I realized that one could consider
such transition diagrams to be a form of pushdown-store automaton. Thus, we could
generalize finite-state transition diagrams to pushdown-store automata by allowing
nonterminal symbols on transitions. This provided a natural parsing algorithm for
grammars that used these notations, and would provide a better semantics for the use
of such notations than just thinking of them as a shorthand for a possibly infinite set of
ordinary context-free grammar rules.
This basic generalization of finite-state machines is now known as a recursive tran-
sition network or RTN. RTNs are essentially ATNs without the augmentations. RTNs
are functionally equivalent to context-free grammars, but they have a capability that
ordinary context-free grammars do not. With an RTN (or an ATN) it is possible to factor
together common parts of different context-free rules while maintaining the constituent
structure of the original. In an RTN (and in ATNs), one can distinguish a finite-state
portion of the machinery and a constituent-structure portion of the machinery and
keep the two distinct. One can apply factoring transformations to the finite-state part
without changing the constituent structure. Figure 6 illustrates a simple factoring of
608
Woods The Right Tools
common parts of rules in the transformation of an ordinary context-free grammar into
an equivalent RTN.
3.1.2 Augmenting the Transition Network. ATNs add augmentations to the basic recursive
transition network of an RTN. Associated with the states of the grammar are a set of
registers that can be carried forward through an analysis of a sentence. Conditions and
actions on the transitions (or ?arcs?) of the grammar can set and test these registers.
Transitions are blocked if the conditions fail, and the information carried in the registers
can be used to build the structure returned by the transition network when it completes
a phrase. The completion of a phrase is indicated by a POP arc, which also can have a
condition, and which specifies how to build the structure to be returned for the phrase.
PUSH arcs indicate transitions that can be made if a phrase of the specified type is
parsed. This can be done by pushing the current configuration onto a stack and invoking
the transition diagram for a phrase. When the phrase that was pushed for is completed,
the result is returned as the value of the suspended transition. Parsing then continues
from the configuration that is popped from the stack. This is the most direct algorithm
for parsing with an ATN, but, as we shall see, it is not the only one.
ATN actions can also pass parameters into and out of constituents, and a special
HOLD list can carry information for long-distance dependencies equivalent to the traces
of transformational grammar theory. A special SYSCONJ meta operator was imple-
mented to handle conjunctions, including reduced conjunction constructions, without
requiring special grammar rules for conjunction. This machinery could handle construc-
tions like:
(7) The man drove his car through and completely demolished a plate glass
window
Here, common parts of an underlying conjunction are factored out and appear only
once in the surface structure, so that we appear to be conjoining the fragments drove
his car through and completely demolished. The SYSCONJ machinery would automatically
handle this kind of construction and build an appropriate deep structure. This kind of
Figure 6
Factoring the common parts of rules.
609
Computational Linguistics Volume 36, Number 4
conjunction would otherwise seem to require something like a combinatory categorical
grammar.
My ATN grammar parser could also use a special kind of POP arc called an SPOP
that would assign a movable modifier to a preferred scope by searching up the stack
for alternative locations for that modifier and then picking the scope that made the
most semantic sense. This feature, called ?selective modifier placement? would parse a
sentence like:
(8) Does American have a flight from an East coast city to Chicago?
with to Chicago attached as a modifier to flight instead of the immediately preceding city,
because there was a semantic rule that would allow flight to use to Chicago, although
there was no such rule for city.
Unlike context-free grammars, ATNs lend themselves to representing regularities
in so-called ?free word-order languages.? Whenever the regularities in the language are
not well expressed by the order of constituents, these regularities can be moved out of
the state transitions and into the registers. Suppose, for example, you can have subject,
verb, and object in any order, except for the sequence ?object, subject, verb?, which is
not permitted. To model this, you can simply have three self-looping transitions on a
single state, with a condition on the subject arc that blocks it if the object is already
set. All three transitions will be blocked if they have already been followed once. If the
language requires at least a verb, but possibly no subject or object, then the POP arc will
have a condition that requires a verb.
3.1.3 ATN Transformations. As I said, RTNs and ATNs permit transformations that share
common parts of rules without changing the constituent structure. This sharing can
reduce the number of configurations that have to be enumerated and explored by a
parsing algorithm. If you are willing to change the constituent structure assigned by
the grammar and accept a weakly equivalent grammar (one that accepts the same
strings, but doesn?t necessarily assign them the same structures), then you can obtain
even more efficiency. For example, you can replace any nonterminal symbol with the
transition network that defines it, as long as the symbol doesn?t itself occur in its own
transition diagram. If you do this until you can?t do it anymore, you get a grammar
whose only non-terminals are recursive symbols. This gets more of the grammar into the
state transition diagram, where it can be optimized by standard finite-state optimization
procedures.
Further, you can replace any left and right recursive symbols with jump transitions
and turn these recursions into iterations. This is illustrated in Figure 7. First you elimi-
nate left recursion by removing the transition that pushes for S from state S and adding
instead a jump transition from each final state (in this case, state 5) to the state that the
left-recursive transition went to (in this case, state 3). These two operations are marked
with (1) in the figure. Then you eliminate right recursion by removing any transition
that pushes for an S to reach a final state and replacing it with a jump transition that
goes back to the start state. These two operations are marked (2) in the figure.
This gets even more of the grammar into the transition diagram where you can
optimize it. In fact, one can argue that this gets as much as possible of the grammar
into finite-state machinery. If you have done all of these transformations, then you
have a grammar whose only nonterminal symbols are self-embedding recursive sym-
bols. There is a theorem, called the Chomsky?Schu?tzenberger Theorem, that shows
self-embedding recursion to be the essential element that distinguishes context-free
610
Woods The Right Tools
Figure 7
Turning recursions into iterations.
languages from finite-state languages. Specifically, it shows that a language is properly
context-free, as opposed to finite-state, only if every context-free grammar for it con-
tains at least one self-embedding recursive nonterminal.
So by these transformations, we can essentially move as much of the grammar as
possible into finite-state machinery, where it can be optimized with finite-state opti-
mization algorithms. Figure 8 illustrates the result of applying these transformations
followed by this optimization.
3.1.4 Earley?s Algorithm. The same year that my ATN paper was published in CACM, Jay
Earley?s algorithm was also published in CACM (Earley 1970). When Earley?s algorithm
came out, I was very excited by it. Originally designed for extensible programming lan-
guages, Earley?s algorithm had the remarkable property that not only did it parse any
context-free grammar in at most n-cubed time, but it automatically achieved the best
known computation bounds on all of the subclasses of context-free grammar that were
known to have time bounds less than n-cubed. By indexing things carefully, and doing
just what was needed, Earley?s algorithm automatically achieved n-squared bounds
on linear grammars and unambiguous grammars and linear bounds on deterministic
grammars, without having to be told what kind of grammar it was being given. Prior to
Earley?s algorithm, special parsing algorithms were used to obtain those tighter bounds.
Figure 8
A highly optimized RTN equivalent to Figure 6.
611
Computational Linguistics Volume 36, Number 4
Earley?s algorithm achieved its basic n-cubed result by getting rid of the stack used
by pushdown-store automata, and instead leaving the equivalent of the stack partially
represented and threaded through a network of ?triples? stored in columns correspond-
ing to positions in the input string. It achieved the tighter n-squared and linear bounds
by indexing the triples in such a way that no unnecessary searching was required at any
point. Earley kept track of progress through a rule with what he called ?dotted rules.?
These were copies of the original context-free grammar rule with a dot somewhere
in the right-hand side to mark the position in the rule that the parser had reached so
far. A dotted rule with the dot at the beginning of the right-hand side represented the
beginning point for processing a rule, and one at the end represented a rule that had
been completely matched. These dots marked the progress of the algorithm through the
rule. The algorithm operated on triples consisting of the number of the rule, the position
of the dot, and the column where the current constituent was begun.
3.1.5 Earley?s Algorithm and ATN Grammars. It turned out that Earley?s algorithm was a
natural for ATN grammars. All you had to do was use the states from the ATN in place
of the dotted rule (i.e., the rule number and the position of the dot). Looked at this way,
the dotted rules are simply states in an ATN. Earley?s three operations of prediction,
scanning, and completion are simply the PUSH, CAT, and POP arcs of the ATN. Earley?s
algorithm can be applied directly to RTNs and to certain restricted classes of ATNs.
Generalizatons of Earley?s algorithm can be applied to more general ATNs.
By transforming a context-free grammar into an ATN, you can combine the com-
mon parts of different rules and achieve a reduction in the number of states in the
parsing computation. Further, you can apply the transformations described earlier and
produce a very compact, minimally branching, minimal state diagram, such as the one
in Figure 8, and apply Earley?s algorithm to that. Figure 9 illustrates a normal Earley
algorithm parse of the context-free grammar in Figure 6, and Figure 10 illustrates the
equivalent parse for the optimized equivalent RTN grammar of Figure 8. The result of
the optimizations is a nearly deterministic parse that needs to consider only 12 state
configurations compared to 50 state configurations for the original.
Figure 9
An Earley parse of the grammar in Figure 6.
612
Woods The Right Tools
Figure 10
The Earley parse for the grammar in Figure 8.
Adapting Earley?s algorithm to more general ATN grammars is more complicated
than for simple RTNs, and beyond the scope of this talk, but I?d be happy to discuss it
with anyone who is interested.
3.1.6 Theoretical Properties of ATNs. Viewed as automata, ATNs have a number of in-
teresting theoretical properties. For example, without some kind of restriction on the
conditions and actions on the arcs, ATN grammars would be Turing complete. In
particular, any single action on an arc could be Turing complete. This has the advantage
of assuring us that whatever linguistic phenomena we encounter can be captured in this
formalism, but it could also make the outcome of an ATN computation semi-decidable.
Clearly, as a linguist, you?d like your grammar formalism to be at least decidable.
To address this issue, I was able to prove that, with certain restrictions, parsing a
sentence with an ATN grammar would be not only decidable, but primitive recursive.
The only necessary restrictions were that the conditions and actions on the arcs be
primitive recursive, and there must be no arbitrarily repeatable chains of singleton self-
embeddings or jump transitions that could be followed in loops without consuming any
input. Because any reasonable grammar would automatically satisfy these restrictions,
these restrictions are effectively no restriction at all. So, parsing with any reasonable
ATN grammar will be at worst primitive recursive.
As automata that are capable of assigning deep structures to natural English sen-
tences with a primitive recursive algorithm, ATNs answered a theoretical question
about the computational complexity of natural language parsing that was concerning
some transformational grammarians at the time. Noam Chomsky wanted the transfor-
mational grammar formalism to express exactly the computational power required by
the human language facility, which he assumed and wanted to be weaker than a Turing
machine. But the transformational grammar formalism as he invented it was shown to
be Turing complete, and even when constraints were added to try to avoid this, Joyce
Friedman was able to prove that the result was still Turing complete.
So showing that the kinds of transformational accounts of English syntax that
the transformational grammarians were developing could be expressed in an ATN
formalism that was less powerful than a Turing machine and whose computations
were decidable provided an answer to this question about the necessary computational
power of the human language facility.
Of course you?d like your parsing algorithm to be much more efficient than an
arbitrary primitive recursive computation, and in fact you?d like it to be better than
exponential in the length of the input string. Applying Earley?s algorithm to ATNs
shows that there are at least subclasses of ATN that are n-cubed, n-squared, and even
linear.
613
Computational Linguistics Volume 36, Number 4
3.1.7 Applications. ATNs have been applied to a variety of problems. I originally devel-
oped them in the context of a question-answering system for airline flight schedules
and then applied that framework to answering questions about states, edges, and
paths in ATN grammars. The same grammar and interpretation framework could easily
handle constructions from either domain, and in fact could handle the union of the two
domains. You could ask this system things like the following:
(9) Does American have a flight from Boston to Chicago?
(10) Is there a Jump arc from state S/ to S/NP?
The second of these was actually asking about the very grammar used to parse both of
them.
Moon Rocks. The first major test of the ATN grammar formalism was the LUNAR
system that my colleagues and I developed at BBN for the NASA Manned Spacecraft
Center to answer questions about the Apollo 11 moon rocks (Woods, Kaplan, and Nash-
Webber 1972). Bonnie Webber and Ron Kaplan were part of that team, among others.
The LUNAR domain was interesting in that it involved fluent natural English with a
technical vocabulary. The lunar scientists even had some linguistic constructions that
one doesn?t encounter in ordinary speech. They thought of samples as partitioned into
?phases? and would ask for things like:
(11) nickel concentrations in the olivine phase in sample S10046
Here, the noun olivine is actually a parameter to the operator phase, occurring as a
prenominal modifier. Parameters to such operators are more commonly presented as
postnominal prepositional phrases.
Other examples of questions LUNAR could handle are:
(12) In how many samples has apatite been identified?
(13) Give me analyses of the olivine phase of each breccia.
(14) What is the average Be concentration in breccias?
(15) What are the Be concentrations in each breccia?
(16) What samples contain silicon and do not contain sodium?
One of the interesting things about this domain was that many of the common English
function words were also the names of chemical elements (As, At, Be, I, In, and He)
as were the common names Al and Mo. This put a burden on the ATN to deal with
the ambiguity of these terms. (LUNAR was implemented at a time when computer
input was typically all upper case, so capitalization was not available as a clue.) In this
domain, these words could not be used as reliable syntactic anchors as they were in
many other grammars of the time.
In addition to parsing sentences, the ATN parser in LUNAR could be run in a mode
in which it would keep track of every step of the parse and could display a browsable
tree structure of the entire parse search space. At any point in this tree, you could
reenter the parsing process and follow the computation step-by-step from that point.
This facility was invaluable for debugging grammars and developing an ATN grammar
614
Woods The Right Tools
with large coverage. Tools such as this, which can help a developer understand what a
system is doing, can be extremely important.
Continuous Speech Understanding. After the LUNAR project, BBN became involved in
the first DARPA speech understanding project (1971?1976). In 1971, no one had any
idea how to do speech understanding. Pierre Vicens and Raj Reddy had done a tiny
pilot using Terry Winograd?s blocks-world domain to show that high level constraints
from syntax and semantics could make up for ambiguity at the acoustic level. To do
that, however, they restricted themselves to sentences containing the word block. The
algorithm knew that and searched for that word as an anchor. The word block was a
good anchor because it begins and ends with plosives, which show up as ?notches? in
a spectrogram and are thus easy to locate. Clearly that algorithm wouldn?t generalize.
I organized a workshop at Harvard that pulled together experts in acoustics, signal
processing, speech recognition, linguistic phonology, computational linguistics, and
artificial intelligence, and we shared what we knew. The outcome of this conference
and other discussions led to the ?Newell Report? (Newell et al 1973), on the basis of
which DARPA decided to fund a program in Continuous Speech Understanding. BBN
was one of the contractors, John Makhoul was our speech and signal processing expert,
and I was the principal investigator and language expert. Other participants were Lynn
Bates, Geoff Brown, Chip Bruce, Craig Cook, Jack Klovstad, Bonnie Webber, Richard
Schwartz, Jerry Wolf, and Victor Zue. Dennis Klatt consulted on the project. Ultimately,
we produced a system called HWIM, for Hear What I Mean (Woods et al 1976). HWIM
understood sentences in the context of an interactive trip-planning and travel-budget-
management system we called TRIPSYS.
One of the things we discovered about speech was that people generally don?t
pronounce all of the words in a sentence with equally clear articulation. In unstressed
parts of a sentence, words are typically pronounced with little effort and their acoustic
recognition is less reliable. In stressed portions of the utterance, words are more clearly
pronounced and can be more reliably recognized. This led to an algorithm I developed,
called the shortfall density algorithm. This algorithm could work outward from these
islands of reliability until they collided with each other and could be combined into
larger islands. This posed an interesting challenge for the syntactic component of the
system, which was required to judge whether an arbitrary sequence of words could be
extended to a complete well-formed sentence and, if possible, to predict the classes of
words that would be compatible with such extensions at each end of the island.
I developed a parsing algorithm that could take an ATN grammar, index it by the
constituents on the arcs of the grammar, and for any word that was hypothesized in an
utterance, find all of the arcs in the grammar that could use that word. I could then test,
for any two adjacent words, which of the arcs compatible with the first word could be
connected to one or more of the arcs of the second by some combination of pushes, pops,
and jumps. I could then combine sequences of words into islands and record a compact,
factored record of all of the possible paths that the grammar could follow through that
sequence of words. The arcs at the ends of these islands could be used to predict the
classes of words that the lexical recognizer should look for at those ends. When a word
was added to one end, the algorithm could propagate constraints through the island to
the other end to possibly narrow the predicted possibilities at the other end.
For the internal structure of this ?island? parse, I used a trick similar to Earley?s
to leave the details of a chain of pushes unspecified until needed. However, I did it for
both pushes and pops and for jumps as well. A resulting path through the island had the
structure of a ?stile? (a set of steps used to cross a fence by going up one side and down
615
Computational Linguistics Volume 36, Number 4
the other). At the top was a word or word sequence, which in general could be popped
to by something on the left and could push for something on the right. The details of
whether that pop or push was direct or indirect were left unspecified until more context
was available to determine the answer. Figure 11 gives an example of such a stile path
covering a fragment. The wavy lines in the figure represent the indefiniteness of the
details of a pop or push transition. Although I developed this algorithm in the context
of speech, I also considered it useful for processing ill-formed utterances. If someone
makes an error in an utterance such as doubling or dropping a word or inserting the
wrong word at some point, an island-driven parser can parse the rest of the utterance
into partially overlapping islands and look for places where a small change could make
a connection in or around the overlap area. The shortfall density algorithm could even
be used to rank the likelihoods of alternative repairs and find the most likely correction.
One of the interesting experiments that we never got to complete with the HWIM
system was to annotate the transition network grammar with information about
prosodic signatures that should be present if certain syntactic transitions are followed.
For example, the sentenceHave any people done chemical analyses on this rockwas misheard
by one version of our system as Give any people done chemical analyses on this rock, which
astoundingly parsed and passed all of the semantic and pragmatic filters. One parse
was equivalent to Give [me] any people-done chemical analyses on this rock. However, if that
was the correct parse, then there should have been a very salient prosodic signature
on the phrase people-done, and the main verb give should have been stressed. Because
neither was the case for the given utterance, this false interpretation would have been
rejected, and the correct interpretation would have been found. We got as far as making
the annotations in the grammar, but had not yet been able to test them when the project
ended.
Conversational Discourse Structure. One of my students, Rachel Reichman, used an inter-
esting kind of ATN to express models of discourse structure she called context spaces
(Reichman 1981). Reichman studied real conversations and found consistent, regular
Figure 11
A stile path covering a fragment.
616
Woods The Right Tools
structure above the sentence level across a variety of kinds of discourse and various
topics. She was able to use the conditions and actions on ATN arcs to characterize how
discourse moves depended on context and could track focus, topic, and role through
the discourse. She could even detect when a speaker switched sides in a debate.
Unlike other discourse grammars, which tend to think of discourse structure in
terms of sequences of categories, Reichman could use the conditions on the ATN
transitions to capture discourse moves such as ?Speaker 2 utters a declarative sentence
that is truth-functionally inconsistent with the previous statement by speaker 1?. She
called this transition a ?challenge.? The content in these discourse ATNs was all in the
conditions. The category labels on the transitions played a very minor role, if any.
One of Reichman?s contentions was that there is a structure of possible moves in
discourse that is orthogonal to the intentions of the speaker, and from which the speaker
has to choose, just as a chess player chooses possible moves from among the legal moves
of chess. In this way, her work complemented that of Perrault, Cohen, Allen, and Bruce,
dealing with the beliefs, desires, and intentions of speakers and hearers. It seemed
to me that her work also provided a missing link for David McDonald?s low-level
tactics of sentence generation. Reichman?s discourse ATNs could provide the tracking
mechanism for the parameters that McDonald?s system required to decide things like
whether to passivize a sentence or not. Reichman?s discourse ATNs could also track the
?reference times? required for Reichenbach?s account of perfect and progressive tenses.
It had previously been a mystery to me where these parameters were expected to come
from.
Other Applications. ATNs and related automata have been applied to a variety of other
problems:
 ATN grammars, without a computer, have been used for field linguistics
to record the evolving understanding of grammars of native languages
(Joseph Grimes, personal communication).
 ATNs were used by Joyce Friedman and David Warren to represent the
grammar of Montague?s Proper Treatment of Quantification in English
(PTQ) and thus provide a parsing algorithm for Montague Grammar
(Friedman and Warren 1978).
 ATNs were used for other kinds of perception. For example, ATNs were
used to recognize patterns of chromosomes (Chou and Fu 1975).
3.2 Generalized Transition Networks
A Generalized Transition Network (GTN) is a generalization of an ATN that can be
used for generalized perception. In GTNs, the transitions are augmented with actions
of sensory perception that can be directed to any point in a perceptual space, replacing
the ATN?s implicit parsing of sequences of constituents in left-to-right-order. State
transitions in GTNs represent moving to more complete states of knowledge as a
result of additional measurements, without any assumption that those measurements
are being applied to a provided sequence of inputs. In a GTN, the results of previous
measurements can determine where the next measurement is to be taken. Students in
my classes at Harvard used GTNs to recognize basic strokes in images of cuneiform
617
Computational Linguistics Volume 36, Number 4
tablets and gliders and other basic patterns in simulations of cellular automata running
Conway?s game of Life.
3.3 Cascaded ATN Grammars
A Cascaded ATN Grammars (CATN) (Woods 1980) is a cooperating sequence of ATN
transducers, each feeding its output to the next stage. One example of a cascade of
ATNs arose in the speech understanding project. The lexical analyzer in HWIM knew
about coarticulation effects between phonemes that could reduce the pronunciation of
a phrase like hand label by suppressing the d in the context between n and l. This was
captured by an algorithm that could be viewed as an ATN transducer that nondeter-
ministically transcribed a sequence of input phonemes into a sequence of words. In this
transducer, the transmission of the words and the consumption of the input phonemes
could be somewhat asynchronous. The ATN transducer needed to consume the l of the
next word label, before it could confirm the appropriate context for the missing d in hand.
So by the time it transmitted the word hand, it was already working on the input for the
next word label. Registers in the ATN could remember the hypothesized current word
while the ATN entered a state that dealt with the coarticulation effects, which could
be shared by all words ending in nd. Then when the confirming l was consumed, the
remembered word was transmitted.
Another example of an ATN cascade would consist of an ATN transducer to recog-
nize syntactic structure, followed by an ATN that performs a semantic interpretation.
Subsequent stages could do pragmatic interpretation, discourse structure, and plan
recognition. Cascading provides benefits similar to sequential decomposition of finite-
state machines: It reduces the overall combinatorics and produces a simpler model. By
factoring these stages apart, we can reduce the extent to which phenomena have to
be learned and recorded multiple times in different contexts. For example, we don?t
have to learn the basic structure of a noun phrase, including the English system of
quantifiers and determiners, repeatedly, for all of the different semantic classes of head
noun that we might want to distinguish. If we?re running a machine learning algorithm,
this means that we don?t have to encounter examples of all of these combinations in
order to learn the correct model. Also, evidence for syntactic structure will be pooled
over different semantic variations and vice versa. Finally, what we learn for the syntactic
stage can carry over to other domains and other applications.
Separating syntactic and semantic processing into two stages of a cascade can also
optimize the flow of information between the two. For example, when parsing a noun
phrase, the syntactic stage can parse the determiner and any adjectives up to the head
noun before transmitting anything to the next stage. Then it can transmit the head
noun and any premodifiers. This allows the semantic interpretation stage, which is
largely head-driven, to start its processing with the most important information. Then
the syntactic stage can parse any subsequent PPs or other postnominal modifiers and
transmit them, followed finally by transmitting the determiner information. After the
semantic stage has interpreted the meanings of the head noun and its modifiers, it can
share the portion of the semantic stage that analyzes determiners.
4. Factoring
This discussion of cascading illustrates a principle I call factoring: the sharing of com-
mon parts of grammars and hypotheses. Factoring is a key element of what makes ATNs
efficient, both for grammar development and for syntactic processing. I distinguish two
618
Woods The Right Tools
kinds of factoring: conceptual factoring and hypothesis factoring. Conceptual factoring
is the sharing of common parts of grammars or other formal models to help a linguist
or a grammar-learning algorithm to capture generalities. Hypothesis factoring is the
sharing of common parts of alternative hypotheses that arise in parsing, perception, or
learning. Hypothesis factoring helps a parsing algorithm or learning algorithm operate
more efficiently by generating fewer cases that need to be considered independently.
Earley?s trick to avoid enumerating different alternative stack contexts while processing
the transitions in a constituent is an example of factoring, as is my generalization of this
trick to handle middle-out island parsing for speech. It?s also the purpose of the RTN op-
timization algorithm I described earlier, and one of the principal benefits of cascading.
Usually factoring transformations that merge the common parts of grammar rules
provide both kinds of factoring benefits, although occasionally these two principles
could compete with each other. In general, one should look for conceptual factoring is-
sues at the level of how a linguist might interact with a formalism to understand models
and theories. Hypothesis factoring optimizations can then be applied behind the scene
by compilers that transform models into efficient structures for processing. For example,
if a linguist prefers to formulate grammars as sets of rules, these could be transformed
to ATNs behind the scene and optimized to obtain efficient parsing algorithms. I seem
to recall that Martin Kay compiled some of his chart-parsing algorithms into something
very similar to ATNs before parsing.
5. Procedural Semantics
I want to turn now to the topic of Procedural Semantics.
In a term paper that led to my thesis on ?Semantics for a Question-Answering
System? (Woods 1967, 1979), I proposed procedural semantics to resolve the dilemma
I had about how to get a computer to answer questions about a database. According
to my dictionary, ?semantics? was the relationship between signs and symbols and the
things they denote or mean. So what, I wanted to know, is meaning? For my application,
semantics had to be more than just assigning features and calling them semantic, or
drawing diagrams and calling them semantic networks. I wanted to know what we
could store in our presumably finite brains that could conceivably play the roles we
attribute to meanings. The best I could find in the philosophy literature was a quote
from Carnap: ?To know the truth conditions of a sentence is to know what is asserted
by it?in usual terms, its ?meaning?.? The only thing I knew that could finitely represent
the truth conditions of a sentence (which is an infinite set of assignments of truth values
to propositions in all possible worlds) was some form of procedure: a Turing machine,
a Post production system, or a computer program.
The idea of procedural semantics is that the semantics of natural language sentences
can be characterized in a formalism whose meanings are defined by abstract procedures
that a computer (or a person) can either execute or reason about. In this theory:
 The meaning of a noun is a procedure for recognizing or generating
instances.
 The meaning of a proposition is a procedure for determining if it?s true.
 The meaning of an action is the ability to do it or to tell if it has been done.
This theory can be thought of either as an alternative to the standard Tarskian semantics
for formal systems or as an extension of them. The idea is that the computational
619
Computational Linguistics Volume 36, Number 4
primitives consisting of represented symbols, the ordered pair, the assignment of values
to variables, conditional branching and iteration, addition and subtraction, and the
subroutine call, together with sensorimotor operators that interact with a real world,
constitute a stronger (and more well-understood) foundation on which to build a theory
of meaning than does set theory and the logical operations of universal and existential
quantification over an all-inclusive infinite universe. These latter theories have their
own paradoxes and incompleteness issues, which we usually ignore, due to familiarity.
For more about the limitations of classical logic alone as a basis for meaning, see Woods
(1987).
A procedural semantics foundation allows for a definition of the standard logical
operators as well as extensions of them to deal with generalized quantifiers, as well as
questions and imperative operations. Building on computational primitives, like the
ordered pair and conditional iteration, seemed to me to be at least as well understood,
and more grounded, than building on the logical operations of universal and existential
quantification over an infinite universe and then having to make some kind of extension
to handle the meanings of questions and imperatives. Moreover, because procedural
specifications can be installed in a machine, where they can be physically executed,
they can interact with sensors like keyboards and cameras and with output devices like
printers and manipulators, so that this approach allows meanings that actually interact
with a physical world, something that no previous theory of meaning had been able to
achieve.
5.1 Semantics for a Question-Answering System
In the case of question answering, procedural semantics allows one to decouple the
parsing and semantic interpretation of English sentences from the details of the storage
and representational conventions of the information in the database. The procedural
semantics approach allows a computer to understand, in a single, uniform way, the
meanings of conditions to be tested, questions to be answered, and actions to be carried
out. Moreover, it permits a general-purpose system for language understanding to be
used with different databases, and even combinations of databases that may have differ-
ent representational conventions and different data structures, and it allows questions
to be answered by results that are computed from the data in the database without being
explicitly stored.
In ?Semantics for a Question-Answering System? (Woods 1967), I formulated a the-
ory of procedural semantics and a methodology for assigning semantic interpretations
to parsed sentences of English. This amounted to compiling the English sentence into an
equivalent abstract procedure, expressed in a meaning representation language (MRL)
that was an extension of conventional predicate calculus notations. This MRL extended
the logical inventory with commands and actions in addition to propositions, and it
introduced generalized and typed quantifiers. The range of quantification for a typed
quantifier was specified by an abstract procedure for enumerating elements of the class,
and the class could be parametrically specified.
This can be illustrated by an example from the LUNAR system discussed earlier.
The semantic interpretation of the questionWhat is the average concentration of Aluminum
in each breccia?was as follows:
(17) (FOR EVERY X5 / (SEQ TYPECS) : T ;
(PRINTOUT (AVGCOMP X5 (QUOTE OVERALL) (QUOTE AL2O3))))
620
Woods The Right Tools
This semantic interpretation can be read as follows:
For every x5 in the class TYPECS such that the universally true condition T is true, print
out the value computed by the averaging function AVGCOMP for the sample x5 for the
overall concentration of Al2O3, where TYPECS is the name used in the database for
?Type-C rocks? (that?s how breccias are encoded in the database) and Al2O3 is the
chemical name for Aluminum Oxide (which is how Aluminum is stored in the
database).
Note that answering this question involves the computation of an average that was not
explicitly stored in the database, and that the grounding for the semantics of terms such
as Aluminum and breccia is in a database whose structure and content and encoding
conventions were previously and independently defined.
LUNAR was influential not only as an example of a successful natural language
question-answering system and an exemplar of ATN parsing and procedural semantics,
but as a stimulus for a lot of subsequent intellectual development. For example, Ron
Kaplan built on the things he learned from working with the LUNAR ATN grammar
to develop his formalism of Lexical Functional Grammars, which in turn influenced the
development of Head-driven Phrase Structure Grammars. Danny Bobrow proposed the
idea of spaghetti stacks (Bobrow and Wegbreit 1973) in response to my description of
a Lisp feature that would make it easier to implement ATN parsers. Spaghetti stacks
are now used to implement continuations in modern Lisp dialects such as Scheme.
And many commercial systems have followed LUNAR as a model when developing
question-answering systems for practical applications.
5.2 Formal Meaning Representation Language
LUNAR?s meaning representation language was an extension of the Predicate Calculus
with generalized quantifiers and imperative operators. Some examples of the schemata
for these quantifiers and operators are:
(18) (FOR <quant> <vbl> / <class> : <condition> : <command>)
(19) (FOR <quant> <vbl> / <class> : <condition> : <condition>)
(20) (TEST <condition>)
(21) (PRINTOUT <designator>)
For example, Example (18) above can be read ?For <quant> <vbl> in the class <class>
such that <condition> is true, do <command>.? This is the schema for a quantified com-
mand. Example (19) can be read: ?For <quant> <vbl> in the class <class> such that the
first <condition> is true, the second <condition> is also true.? This is the schema for a
quantified proposition. The quantifiers <quant> in these schemata include not only the
traditional universal and existential quantifiers EVERY and SOME, but also nontraditional
quantifiers like THE and (MORETHAN <number>) and a generic quantifier GEN which corre-
sponds to the use in English of undetermined noun phrases with a plural noun (e.g.,
birds have wings). The paradigm can accommodate numerical quantifiers as exotic as
an even number of or a prime number of and all sorts of typically, often, rarely, and ?unless-
contradicted? probabilistic quantifiers as well.
621
Computational Linguistics Volume 36, Number 4
The schema (TEST <condition>) is a command to test a condition and print out Yes
or No according to the result, and (PRINTOUT <designator>) is a command to print out a
name or description of the referent of the specified designator.
The fact that LUNAR?s meaning representation language uses typed quantifiers and
provides for additional restrictions on the range of quantification permits a uniform
treatment of different quantifiers and different kinds of head nouns and modifiers
in English noun phrases. For example, Some tall men play basketball has the following
interpretation:
(22) (FOR SOME X / MAN : (TALL X) ; (PLAY X BASKETBALL))
The sentence All long flights to Boston serve meals has the following interpretation:
(23) (FOR EVERY X / (FLIGHT-TO BOSTON) : (LONG X) ; (SERVE-MEAL X))
If we try mapping English directly to classical logic, we need different treatments for
noun phrases with universal versus existential quantifiers. For example, for Some tall
men play basketball, the three predicates (MAN X), (TALL X), and (PLAY X BASKETBALL) are
all conjoined under the quantifier (for some x, x is a man and x is tall and x plays
basketball), whereas for All tall men play basketball, the first two conditions would be
conjoined as the antecedent of an implication whose consequent is the third condition
(for all x, if x is a man and x is tall, then x plays basketball). For nonstandard quantifiers,
such as the or rarely, mapping to classical logic is different still.
5.3 Reasoning with Meanings
The procedural semantics framework allows procedural interpretations to be treated in
two ways. In the simplest way, the semantic interpretation is simply executed as a pro-
gram to compute an answer to a question. However, in a more general case, the system
can take the interpretation as an object to be reasoned about and possibly modified. For
example in the following query from my Airline flight schedules application:
(24) (FOR EVERY X / FLIGHT :
(AND (OPERATOR X AA) (CONNECT X BOSTON CHICAGO)) ;
(PRINTOUT X))
the system can reason that it could get the same result more efficiently by using a special
enumeration function that uses a database index to enumerate only the flights that go
from Boston to Chicago to specify the class of the quantifier, resulting in:
(25) (FOR EVERY X / (FLIGHT-FROM-TO BOSTON CHICAGO) :
(OPERATOR X AA) ; (PRINTOUT X))
This is an example of what I called ?smart quantifiers,? quantifiers that apply reasoning
to their range of quantification and any specified filters on that range to see if there are
more efficient ways to enumerate the same effective range.
There are lots of other reasons why a system might want to reason about the inter-
pretation of a request before acting on it: it may not be appropriate to take literally, or
there might be some additional helpful actions to take, or the request might be estimated
to be unduly expensive, or there may be a more efficient way to do it, or the user may
622
Woods The Right Tools
not be authorized to do what is being requested, or the system may have reasons not to
do the thing that is requested, and so forth. So, we would like semantic representations
to be useful both for execution as a procedure and as objects of mechanical reasoning.
5.4 Answering Questions about Paths in an ATN
When I first implemented my semantic interpreter for the airline flight schedules do-
main, I had no actual database of flight schedules (other than a paper copy of theOfficial
Airline Guide). This was before there existed a multiplicity of on-line databases. What I
did have, though, was my implemented ATN parser with its ATN grammar, which
consisted of a set of states with transitions to other states. So to test out my theory,
and to illustrate that I could apply the theory to a database whose structure had been
previously determined, I decided to make the ATN grammar itself the object of my
requests. I wrote semantic interpretation rules to allow me to ask questions like Is there
a jump arc from S/ to S/NP? or Is there a non-looping path from S/NP to S/POP?. These rules
coexisted with my rules for airline schedules and both shared the same grammar for
natural language input. One could ask the system Is there a connection from S/NP to S/V?
or Is there a connection from Boston to Chicago? and the semantic rules would disambiguate
which domain was intended.
Being able to query paths was especially interesting, because they didn?t actually
exist as objects in the grammar, but had to be constructively enumerated by the enumer-
ation functions for the path class. Moreover, because there were a potentially infinite
number of such paths, the formulation of quantifier classes defined by generators
turned out to be essential. I used smart quantifiers to infer when I could use specialized
generators for non-looping paths, paths rooted at a given start state, paths between two
end points, and so on. I used a resolution theorem prover to prove that the conditions
for one of my specialized generators were implied by the filters on the quantifier,
and then used instantiated variables from this proof to provide the parameters for
the resulting generator. Because I was only using the theorem prover to seek a more
efficient generator for the paths, if the theorem prover took longer to come up with an
answer than an estimate of how much would be saved by using the result, then it would
abandon the effort and use the original quantifier.
5.5 Procedural Semantics in LUNAR
When Jeff Warner approached me from NASA about answering questions about the
Apollo 11 moon rocks, I already had the machinery in place for parsing and answering
English questions and a rule-based system for adding more domains. All I had to do was
build a dictionary for the new vocabulary, write the appropriate semantic interpretation
rules, oh, and by the way, implement a database system, import their data into it, write
the procedures to define the semantics, and do all this in 256 K words of memory on a
PDP-10! The PDP-10 in question was a DEC machine that BBN had modified to support
virtual memory, multiple forks, and time-sharing. I was programming in Lisp and my
language-processing program took up one 256-K fork all by itself, so Danny Bobrow
enlisted Lisp wizard Alice Hartley to add the ability for Lisp to create and control
subforks, and I implemented the database and database retrieval programs in a sepa-
rate fork.
At NASA, Jeff Warner had extracted all of the data from the conference proceed-
ings of the First Annual Lunar Science Conference, normalized the units, and cross-
referenced the data with the articles from which it came. He could generate answers
623
Computational Linguistics Volume 36, Number 4
to questions by having his Fortran programmer write programs to query this database.
He wanted to know if he could get his Fortran programmer out of the loop. He had
collected a set of queries to illustrate the kinds of questions he wanted to be able to
answer, and a vocabulary of 3,000 words that included all of the chemical elements, a
large vocabulary of mineral names, and every word that appeared in the proceedings
of the First Annual Lunar Science Conference. For each word, he had recorded its
most common syntactic category in English. I had to take this list and expand it with
additional categories for ambiguous words and add in syntactic and semantic features
and semantic interpretation rules for the words that needed them.
The procedural semantics approach worked fine, and I learned some interesting
things about semantics and quantifiers from this experience (Woods 1978). One of the
interesting discoveries was how the ?average? operator interacted with quantification
and treated the generic quantifier differently from a quantifier like each. If the quantifier
is each you get a separate average for each value, but if the quantifier is generic or all,
the average is computed over all of the values. LUNAR answered 78% of the queries
asked of it at the Second Annual Lunar Science Conference, and 90% of those queries
fell within its scope. However, LUNAR was far from being a complete solution. If you
asked LUNAR, What is a breccia?, it would reply S10046. S10046 was indeed a breccia,
and LUNAR was programmed to give you what you asked for. If you asked it What
is S10046?, it would reply S10046, since that was a sample that was equal to S10046.
LUNAR simply found referents of referring expressions and gave you their names. It
had no model of the purpose behind the user?s question or of different kinds of answers
for different purposes.
6. Knowledge and Language
I want to shift now and talk about knowledge.
It doesn?t take much thought to realize that background knowledge and contextual
knowledge is essential to language interpretation. For example, knowledge is necessary
to resolve the ambiguity (and get the joke) in Groucho Marx?s famous Time flies like
an arrow (but fruit flies like a banana). Here, both syntactic and semantic ambiguity are
resolved by knowing that there are fruit flies, but not time flies, that arrows fly but
bananas don?t, and that flies can be used metaphorically for moving swiftly.
Consider a spoken utterance that could be segmented either as his wheat germ
and honey or his sweet German honey. One would need to know something about the
context of this utterance in order to venture a prediction as to which interpretation was
intended. Here, we need knowledge even to know what words we are hearing.
In the HWIM system, the sentence Show me Bill?s trips to Washington was misheard
as Show me Bell?s trips to Washington in the context of a travel planning system that knew
travel plans for a group of people that included Bill Woods and Alan Bell. There is a min-
imal difference of one phoneme between these two sentences (one letter in the written
orthography), and there is only one feature difference between these two vowels. The
acoustic scores of the two hypotheses were virtually identical, and the correct choice
happened to come second. However, the system could easily have resolved the choice
by using a semantic interpretation to check the trip database to learn that Bill Woods
was scheduled to go to Washington, while Alan Bell was not.
I once called home on the telephone and asked my young son, who answered, Is
your mother there? He said, Yes. I said Can I speak to her? He said, Yes. Finally, I said, Tell
her to come to the phone. OK, he said. We know that interpreting speech acts depends on
beliefs, desires, and intentions, but how do we manage and acquire all of the knowledge
624
Woods The Right Tools
it takes to correctly infer those beliefs and desires and intentions and do so efficiently at
the right time?
6.1 Requirements for Knowledge Representation
We need a system that can organize and use large amounts of world knowledge and
facilitate the efficient associative access to that knowledge during the analysis of sen-
tences. My experiences in a variety of natural language applications have convinced
me that understanding and using knowledge is the bottleneck in both speech and
natural language processing (Woods 2007). A key problem is how to find the pieces of
knowledge relevant to a problem from among all of the knowledge in a large knowledge
base.
We need a representation system that can satisfy two requirements:
1. It should be expressively adequate to represent all of the necessary
elements of natural language questions, commands, assertions, conditions,
and designators.
2. It should be structured to support semantic interpretation, retrieval, and
inference.
6.2 Links and Logic, KL-One et al
The KL-One project at BBN (?1977?1983) attempted to develop a representation to
meet these conditions. KL-One was a knowledge representation system developed
as part of a research project on Knowledge Representation for Natural Language
Understanding. A number of people worked on this contract, including: me, Madeline
Bates, Rusty Bobrow, Ron Brachman, Bertram Bruce, Eugene Ciccarelli, Phil Cohen,
Brad Goodman, Norton Greenfeld, Andrew Haas, Robert Ingria, David Israel, Jack
Klovstad, David McAllester, Ray Reiter, James Schmolze, Candace Sidner, Marc Vilain,
Bonnie Webber, Martin Yonke, and Frank Zdybel. The project began as an attempt to
develop a knowledge representation system suitable to represent and deliver all of the
knowledge required for human-level reasoning (Sidner et al 1981). In it, we sought to
combine the best features of two traditions:
1. logical reasoning, which is rigorous and formal, but often counterintuitive,
and which has algorithms that match expressions, substitute values for
variables, and invoke rules, and
2. associative networks, which are structured and intuitive, but typically
informal; however, they support efficient algorithms that follow paths
through links to draw conclusions.
We wanted the associativity of link-based representations, in order to exploit efficient
path-following algorithms, but we also needed representations with a clean and
well-understood semantics. A key element of our approach was based on Ron
Brachman?s thesis on ?Structured Inheritance Networks? (Brachman 1977). Ron was
one of my thesis students at Harvard, and his thesis arose from my challenge to figure
out how to index material at the sentence level so that one could find where particular
things were said. Structured inheritance networks not only related concepts to each
other by generality, but also aligned corresponding roles of those concepts.
625
Computational Linguistics Volume 36, Number 4
One of the achievements of the KL-One project was the creation of a knowledge
representation system whose semantics were sufficiently well defined that an algorithm
could automatically place new concepts at the correct position in a conceptual taxon-
omy. I wrote the first algorithm to do this, which I called the MSS algorithm (for Most
Specific Subsumer). This algorithm would automatically find the most specific concepts
in an existing taxonomy that subsumed a new concept (i.e., were more general than or
equivalent to the new concept). The new concept could then be added to the taxonomy
directly under those concepts. An analogous algorithm, the MGS algorithm (for Most
General Subsumee), could find the most general concepts that were subsumed by the
new concept. James Schmolze wrote subsequent ?classifiers? for KL-One.
KL-One began a wave of research in knowledge representation, inspired a vast
number of complexity results, initiated the field now known as Description Logic, and
spawned a family of related systems (Woods and Schmolze 1992).
6.3 Understanding Subsumption and Taxonomy
Although the original KL-One was focused on the structure of concepts, most of the
subsequent work it inspired adopted a declarative approach, based on first-order logic.
In this work, subsumption was identified with logical implication and set inclusion.
While most of this flurry of activity was going on, I was involved in a couple of startup
companies and watching all this from the sidelines. However, I felt that the declara-
tive semantics approach had thrown out the baby with the bathwater by eliminating
all of the intuitions for how the structure of links can support efficient algorithms.
I also felt that the extensional subsumption criterion was a mistake. In 1990, during
an interim appointment at Harvard University, I started to revisit the original goals
of KL-ONE in light of where the field had gotten, under sponsorship from the Kapor
Family Foundation. I wanted a representational system that would be an efficient and
principled methodology for organizing knowledge, and I came to focus on a different
criterion for subsumption that I called ?intensional? rather than ?extensional.? The
result was my 1991 paper on ?Understanding Subsumption and Taxonomy? (Woods
1991).
The idea of intensional subsumption is that for one concept to subsume another,
there must be a direct and recognizable relationship between the meanings of the
concepts. It is not sufficient merely to have a set inclusion of their logical extensions.
Because it takes a theorem to prove that context-free languages are the same set of
languages as the languages accepted by a pushdown-store automaton, these two con-
cepts must have different meanings, even though they have the same extension. Both
meanings are essentially procedural. The first says that the language is accepted by a
context-free grammar. The second says that the language is accepted by a pushdown-
store automaton. The proof involves showing that these two procedures produce the
same results. If it takes this much reasoning to determine that one concept implies the
other, then that?s not intensional subsumption.
My definition of intensional subsumption was that each part of the more general
concept subsumes some part of the more specific concept. Thus [a man with a pet]
subsumes [a man with a dog], because pet subsumes dog and a man is a man. Both
of these subsume [a man with a dog and a cat].
I was able to extend this notion of subsumption to include ?gap? predicates, which
have sufficient conditions and necessary conditions that are not equivalent, leaving an
undefined gap in between. So-called ?natural kinds,? like [chair], which are supposedly
not definable, can often be modeled with such gap predicates. For example, one can
626
Woods The Right Tools
specify some necessary conditions for being a chair (you can sit on it, it was intended to
be sat on, it has a back), and some additional conditions that are collectively sufficient
for being a chair (four legs, one seat, one back), while leaving odd cases undefined
(sitting on a log, in a crook of a tree).
The idea of intensional subsumption proved to be both more expressive than the
extensional semantics approach, and also computationally more tractable. I was able to
show that under certain assumptions about the structure of a conceptual taxonomy, a
new concept could be assimilated by an MSS algorithm in sublinear time (on the order
of the log of the size of the taxonomy). The MGS algorithm was less well-behaved, but
could be expected to take this same sublinear time on the average.
With this machinery in hand, I was able to return to KL-One?s original goals, seeking
a knowledge representation structure to organize everything necessary for human-
level reasoning. My thesis is that we use a conceptual taxonomy, based on something
like intensional subsumption, to organize everything we know. In it, we can record
what do do about different situations, efficiently find the most specific applicable rules,
record rules for acquiring more information, record alternatives to consider, and record
priorities and procedures for doing things. An example of this is the famous Nixon
diamond, shown in Figure 12. The classic conundrum is based on the fact that Nixon is
a Republican, and Republicans are usually Hawks, but Nixon is also a Quaker, and
Quakers are usually Doves. So which is Nixon? Many AI researchers have felt that
some inheritance principle should answer such questions. My opinion is that the job
of the knowledge representation system is to identify the greatest-lower-bound concept
[Republican Quaker] as the locus of the issue, classify it under both Republican and
Quaker, classify Nixon under that, and propose that some other component figure
out the political leanings of Republican Quakers (via a poll?). Then for any future
person, if they are classified under both Republican and Quaker, the MSS algorithm will
place them under the more specific concept [Republican Quaker] and they will inherit
whatever answer this poll recorded there.
Figure 12
The Nixon Diamond.
627
Computational Linguistics Volume 36, Number 4
6.4 A Practical Application of Conceptual Taxonomy
In 1991, I joined the new Sun Microsystems Laboratories in Burlington, Massachusetts,
carrying with me my new theory of conceptual taxonomy. My focus was to be on
improving search technology. The rationale was that if we could improve the ability
to find specific information in text, it would have wide applicability. The goal was
to understand phrases, handle paraphrase variations, find specific passages, and help
people find specific information quickly. In addition to these goals, I was also interested
in having a laboratory for gaining experience with subsumption technology on large
populations of natural concepts. The population in this case was all of the words and
phrases extracted from unrestricted text.
My Knowledge Technology Group at Sun Labs developed a search engine we
called Nova that truly delivered on these goals. It contained a universal lexicon and
morphology for general English that enabled me to apply Nova to any new sub-
ject matter with no initial preparation. It had a core lexicon of about 40,000 words,
which we expanded by rule to a lexicon of approximately 150,000 word forms. The
expansion was based on a known word list and a set of approximately 1,200 mor-
phological rules that could analyze an unknown word and produce a complete lexical
entry for future use. These lexical entries included syntactic word categories, semantic
features, and preferences among word senses. The lexicon knew semantic subsumption
facts for approximately 20,000 words. The indexer contained a scanning ATN grammar
that could extract basic phrases, which were then automatically classified into a
conceptual taxonomy that was automatically created for each collection we indexed.
In addition to finding specific passages, Nova allowed you to browse in the conceptual
taxonomy to get ideas about how to generalize a query and to understand what
paraphrases exist in the material that have already been covered by what you asked.
One experiment showed a five times speedup in human search productivity using
Nova, compared to conventional document retrieval technology.
This search technology was incorporated into several Sun products and deployed
internally to search Sun?s e-mail archives. Among the people who worked on this
project that you might know are Phil Resnik, Paul Martin, and Peter Norvig. Steve
Green and I, with help from Paul Martin, implemented the final Java version of this
project.
6.5 Generalized Perception
Since working on Nova, I?ve been thinking about generalized perception at a level
that subsumes natural language understanding, speech understanding, visual scene
recognition, and general situation awareness. This is in some sense the opposite of text
search. It?s more like you have a huge taxonomy of queries and only one text. When you
are presented with a situation, you want all of the queries that it would satisfy to wake
up, and you?d like to be alerted to the most specific ones, which will in turn provide you
with information about what to do or expect in that situation. The ?queries? in this case
are the concepts in your taxonomy. In addition, the taxonomy should serve as a kind
of ?grammar? that can analyze the elements of a situation and characterize how they
relate. Such a structure, I believe, is at the core of intelligent behavior, including natural
language use.
Imagine such a conceptual taxonomy inserted as a stage in a CATN at the position
where semantic interpretation is to occur. The taxonomy would find the most specific
subsumers of each partial interpretation as it accrues, and notify the earlier stage if the
628
Woods The Right Tools
pieces don?t make sense. This stage would turn phrases into concepts, relate them to
other concepts, and provide associated information such as interesting specializations
and other elements to expect. I?d like to find some good applications to explore these
ideas in a context where the result could actually help people do things.
7. Methodology
Before I close, I?d like to say a few words about a methodology I call Directed Research.
This is how I approach problems, and I recommend it for your consideration. The idea
is to understand real problems that one would like to solve, and to do it with the
standards of the highest quality research. This combines the best features of ?applied
research? and ?basic research.? I?ve always found it productive to look at the details of
real problems. Real problems often reveal issues that you wouldn?t think of otherwise.
It?s important to look at the details. Try to understand what would be necessary to solve
the whole problem. At this point, don?t settle for approximations. If you have a practical
job to do, and it?s important to get it done quickly as well as possible, and you can only
do that by partially solving the problem, then by all means do that. That?s practical
engineering, and I do that with my Engineer?s hat on. But that?s not going to advance
the science, and with my Scientist?s hat on, I?ll keep worrying the problem, trying to
discover what it takes to really do the job.
It?s really useful here to have an arsenal of intellectual tools to try to fit to the
problem, but pay attention to the fit. Don?t restrict yourself to existing tools, however
nice they are. If the fit is not good, look for tools that can really do the job. Modify old
ones or invent new ones as necessary. That?s my message about ?the right tools.?
I was asked how I decide to stop working on one problem and work on a new one.
For me that is easy: Once I understand a technology well enough to know its strengths
and weaknesses and what it can and can?t do, I start working on the next problem. I
always have a queue of problems I want to work on, and I can usually find a match
between one of them and something that someone needs.
8. The Best Is Yet to Come
In closing, I want to observe that there seems to have been an evolution in the things
I?ve worked on that is moving closer and closer to the goal of truly effective person?
computer communication. We still haven?t begun to try some of the most interesting
applications of language and computation. I?d like to make more progress on the
following goals:
 genuine interchange of knowledge between a person and a machine;
 cooperative problem-solving and decision making with a machine partner;
and
 a machine that can understand what you need and respond appropriately.
I believe that doing this will require integrating all of the tools I?ve just presented plus
some machine learning and some extraction technology. And I fully expect to have to
invent a few more tools as well.
You can find links to some of my papers and follow my progress on my personal
Web page: http://parsecraft.com.
Thank you.
629
Computational Linguistics Volume 36, Number 4
References
Bobrow, D. G. and B. Wegbreit. 1973. A
model and stack implementation of
multiple environments. Communications
of the ACM, 16(10):591?603.
Brachman, R. A. 1977. A Structural Paradigm
for Representing Knowledge. Ph.D. thesis,
Harvard University.
Chou, S. M. and K. S. Fu. 1975. Transition
networks for pattern recognition. Technical
Report TR-EE 75-39, School of Electrical
Engineering, Purdue University, Indiana.
Earley, J. 1970. An efficient context-free
parsing algorithm. Communications of
the ACM, 13(2):94?102.
Friedman, J. and D. S. Warren. 1978. A
parsing method for Montague Grammars.
Linguistics and Philosophy, 2(3):347?372.
Newell, A., J. Barnett, J. Forgie, C. Green,
D. Klatt, J. C. R. Licklider, M. Munson,
R. Reddy, and W. Woods. 1973. Speech
Understanding Systems: Final Report
of a Study Group. North-Holland,
American Elsevier.
Reichman, R. 1981. Modeling informal
debates. In IJCAI?81: Proceedings of the
7th International Joint Conference on
Artificial intelligence, pages 19?24,
San Francisco, CA.
Sidner, C. L., M. Bates, R. J. Bobrow,
J. Schmolze, R. J. Brachman, P. R. Cohen,
D. J. Israel, B. L. Webber, and W. A. Woods.
1981. Research in knowledge
representation for natural language
understanding: Annual report. BBN
Technical Report 4785, Bolt Beranek
and Newman, Inc., Cambridge, MA.
Woods, W. A. 1967. Semantics for a
Question-Answering System. Ph.D. thesis,
Division of Engineering and Applied
Physics, Harvard University. Also
available as Report NSF-19, Computation
Laboratory, Harvard University,
September 1967. Republished in
Garland Publishing?s Outstanding
Dissertations in Computer Science
series, Garland Publishing, 1979.
Woods, W. A. 1970. Transition network
grammars for natural language
analysis. Communications of the ACM,
13(10):591?606. Reprinted in Yoh-Han Pao
and George W. Ernest (eds.), Tutorial:
Context-Directed Pattern Recognition
and Machine Intelligence Techniques for
Information Processing. IEEE Computer
Society Press, Silver Spring, MD, 1982.
Also reprinted in Barbara Grosz, Karen
Sparck Jones, and Bonnie Webber (eds.),
Readings in Natural Language Processing,
San Mateo, CA, Morgan Kaufmann,
1986, pages 71?87.
Woods, W. A. 1973. Progress in natural
language understanding: an application
to lunar geology. In AFIPS ?73: Proceedings
of the June 4-8, 1973, National Computer
Conference and Exposition, pages 441?450,
New York.
Woods, W. A. 1978. Semantics and
quantification in natural language
question answering. In M. Yovits,
editor, Advances in Computers. Academic
Press. Reprinted in Barbara Grosz, Karen
Sparck Jones, and Bonnie Webber (eds.),
Readings in Natural Language Processing,
San Mateo, CA, Morgan Kaufmann,
1986, pages 205?248.
Woods, W. A. 1979. Semantics for a Question
Answering System. New York, Garland
Publishing.
Woods, W. A. 1980. Cascaded ATN
grammars. American Journal of
Computational Linguistics, 6(1):1?12.
Woods, W. A. 1987. Don?t blame the tool.
Computational Intelligence, 3(1):228?237.
Woods, W. A. 1991. Understanding
subsumption and taxonomy: A
framework for progress. In John Sowa,
editor, Principles of Semantic Networks:
Explorations in the Representation of
Knowledge. San Mateo, CA, Morgan
Kaufmann, pages 45?94.
Woods, W. A. 2007. Meaning and links:
A semantic odyssey. AI Magazine,
28(4):71?92.
Woods, W. A., M. Bates, G. Brown, B. Bruce,
C. Cook, J. Klovstad, J. Makhoul,
B. Nash-Webber, R. Schwartz, J. Wolf,
and V. Zue. 1976. Speech understanding
systems: Final technical progress report,
volumes i?v. BBN Technical Report 3848,
Bolt Beranek and Newman Inc.,
Cambridge, MA.
Woods, W. A., R. M. Kaplan, and B. L.
Nash-Webber. 1972. The lunar sciences
natural language information system:
Final report. BBN Report No. 2378,
Bolt Beranek and Newman Inc.,
Cambridge, MA. Available from NTIS
as N72-28984.
Woods, W. A. and J. G. Schmolze. 1992. The
KL-ONE family. Computers & Mathematics
with Applications, 23(2-5):133?177.
630
