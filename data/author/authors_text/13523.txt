Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 14?15,
Vancouver, October 2005.
Prague Dependency Treebank as an exercise book of Czech
Barbora Hladka? and Ondr?ej Kuc?era
Institute of Formal and Applied Linguistics
Charles University
Malostransk?e n?am. 25
118 00 Prague, Czech Republic
hladka@ufal.mff.cuni.cz, ondrej.kucera@centrum.cz
Abstract
There was simply linguistics at the begin-
ning. During the years, linguistics has
been accompanied by various attributes.
For example corpus one. While a name
corpus is relatively young in linguistics,
its content related to a language - collec-
tion of texts and speeches - is nothing new
at all. Speaking about corpus linguistics
nowadays, we keep in mind collecting of
language resources in an electronic form.
There is one more attribute that comput-
ers together with mathematics bring into
linguistics - computational. The progress
from working with corpus towards the
computational approach is determined by
the fact that electronic data with the ?un-
limited? computer potential give opportu-
nities to solve natural language processing
issues in a fast way (with regard to the pos-
sibilities of human being) on a statistically
significant amount of data.
Listing the attributes, we have to stop for
a while by the notion of annotated cor-
pora. Let us build a big corpus including
all Czech text data available in an elec-
tronic form and look at it as a sequence of
characters with the space having dominat-
ing status ? a separator of words. It is very
easy to compare two words (as strings), to
calculate how many times these two words
appear next to each other in a corpus, how
many times they appear separately and so
on. Even more, it is possible to do it
for every language (more or less). This
kind of calculations is language indepen-
dent ? it is not restricted by the knowl-
edge of language, its morphology, its syn-
tax. However, if we want to solve more
complex language tasks such as machine
translation we cannot do it without deep
knowledge of language. Thus, we have
to transform language knowledge into an
electronic form as well, i.e. we have to
formalize it and then assign it to words
(e.g., in case of morphology), or to sen-
tences (e.g., in case of syntax). A cor-
pus with additional information is called
an annotated corpus.
We are lucky. There is a real annotated
corpus of Czech ? Prague Dependency
Treebank (PDT). PDT belongs to the top
of the world corpus linguistics and its sec-
ond edition is ready to be officially pub-
lished (for the first release see (Hajic? et al,
2001)). PDT was born in Prague and had
arisen from the tradition of the successful
Prague School of Linguistics. The depen-
dency approach to a syntactical analysis
with the main role of verb has been ap-
plied. The annotations go from the mor-
phological level to the tectogrammatical
level (level of underlying syntactic struc-
ture) through the intermediate syntactical-
analytical level. The data (2 mil. words)
have been annotated in the same direction,
i.e., from a more simple level to a more
14
complex one. This fact corresponds to
the amount of data annotated on a partic-
ular level. The largest number of words
have been annotated morphologically (2
mil. words) and the lowest number of
words tectogramatically (0.8 mil. words).
In other words, 0.8 million words have
been annotated on all three levels, 1.5 mil.
words on both morphological and syntac-
tical level and 2 mil. words on the lowest
morphological level.
Besides the verification of ?pre-PDT? the-
ories and formulation of new ones, PDT
serves as training data for machine learn-
ing methods. Here, we present a system
Styx that is designed to be an exercise
book of Czech morphology and syntax
with exercises directly selected from PDT.
The schoolchildren can use a computer to
write, to draw, to play games, to page en-
cyclopedia, to compose music - why they
could not use it to parse a sentence, to de-
termine gender, number, case, . . . ? While
the Styx development, two main phases
have been passed:
1. transformation of an academic ver-
sion of PDT into a school one. 20
thousand sentences were automati-
cally selected out of 80 thousand
sentences morphologically and syn-
tactically annotated. The complex-
ity of selected sentences exactly cor-
responds to the complexity of sen-
tences exercised in the current text-
books of Czech. A syntactically an-
notated sentence in PDT is repre-
sented as a tree with the same num-
ber of nodes as is the number of the
words in the given sentence. It dif-
fers from the schemes used at schools
(Grepl and Karl??k, 1998). On the
other side, the linear structure of PDT
morphological annotations was taken
as it is ? only morphological cate-
gories relevant to school syllabuses
were preserved.
2. proposal and implementation of ex-
ercises. The general computer facil-
ities of basic and secondary schools
were taken into account while choos-
ing a potential programming lan-
guage to use. The Styx is imple-
mented in Java that meets our main
requirements ? platform-independent
system and system stability.
At least to our knowledge, there is no
such system for any language corpus that
makes the schoolchildren familiar with an
academic product. At the same time, our
system represents a challenge and an op-
portunity for the academicians to popular-
ize a field devoted to the natural language
processing with promising future.
A number of electronic exercises of Czech
morphology and syntax were created.
However, they were built manually, i.e.
authors selected sentences either from
their minds or randomly from books,
newspapers. Then they analyzed them
manually. In a given manner, there is no
chance to build an exercise system that
reflects a real usage of language in such
amount the Styx system fully offers.
References
Jan Hajic?, Eva Hajic?ova?, Barbora Hladka?, Petr Pajas,Jarmila Panevova?, and Petr Sgall. 2001. Prague
Dependency Treebank 1.0 (Final Production Label)CD-ROM, CAT: LDC2001T10, ISBN 1-58563-212-0,Linguistic Data Consortium.
Miroslav Grepl and Petr Karl??k 1998. Skladba c?es?iny.
[Czech Langauge.] Votobia, Praha.
15
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 209?212,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Play the Language: Play Coreference
Barbora Hladk
?
a and Ji
?
r?? M??rovsk?y and Pavel Schlesinger
Charles University in Prague
Institute of Formal and Applied Linguistics
e-mail: {hladka, mirovsky, schlesinger@ufal.mff.cuni.cz}
Abstract
We propose the PlayCoref game, whose
purpose is to obtain substantial amount of
text data with the coreference annotation.
We provide a description of the game de-
sign that covers the strategy, the instruc-
tions for the players, the input texts selec-
tion and preparation, and the score evalua-
tion.
1 Introduction
A collection of high quality data is resource-
demanding regardless of the area of research and
type of the data. This fact has encouraged a
formulation of an alternative way of data col-
lection, ?Games With a Purpose? methodology
(GWAP), (van Ahn and Dabbish, 2008). The
GWAP methodology exploits the capacity of Inter-
net users who like to play on-line games. The on-
line games are being designed to generate data for
applications that either have not been implemented
yet, or have already been implemented with a per-
formance lower than human. Moreover, the play-
ers work simply by playing the game - the data are
generated as a by-product of the game. If the game
is enjoyable, it brings human resources and saves
financial resources. The game popularity brings
more game sessions and thus more annotated data.
The GWAP methodology was formulated in
parallel with design and implementation of the
on-line games with images (van Ahn and Dab-
bish, 2004) and subsequently with tunes (Law
et al, 2007),
1
in which the players try to agree
on a caption of the image/tune. The popularity of
the games is enormous so the authors have suc-
ceeded in the basic requirement that the annota-
tion is generated in a substantial amount. Then
the Onto games appeared (Siorpaes and Hepp,
1
www.gwap.org
2008), bringing a new type of input data to GWAP,
namely video and text.
2
The situation with text seems to be slightly dif-
ferent. One has to read a text in order to identify
its topics, which takes more time than observing
images, and the longer text, the worse. Since the
game must be of a dynamic character, it is unimag-
inable that the players will spend minutes reading
an input text. Therefore, the text must be opened
to the players ?part? by ?part?.
So far, besides the Onto games, two more games
with texts have been designed: What did Shan-
non say?
3
, the goal of which is to help the speech
recognizer with difficult-to-recognize words, and
Phrase Detectives
4
(Kruschwitz, Chamberlain,
Poesio, 2009), the goal of which is to identify re-
lationships between words and phrases in a text.
Motivated by the GWAP portal, the LGame por-
tal
5
has been established. Seven key properties
that any game on the LGame portal will satisfy
were formulated ? see Table 1.
The LGame portal has been opened with the
Shannon game, a game of intentionally hidden
words in the sentence, where players guess them,
and the Place the Space game, a game of word
segmentation.
Within a systematic framework established at
the LGame portal, the games PlayCoref, PlayNE,
PlayDoc devoted to the linguistic phenomena
dealing with the contents of documents, namely
coreference, named-entitites, and document la-
bels, respectively, are being designed in parallel
but implemented subsequently since the GWAPs
are open-ended stories the success of which is hard
to estimate in advance. These games are designed
for Czech and English by default. However, the
game rules are language independent.
2
www.ontogame.org
3
lingo.clsp.jhu.edushannongame.html
4
www.phrasedetectives.org
5
www.lgame.cz
209
1. During the game, the data are collected for the natural
language processing tasks that computers cannot solve
at all or not well enough.
2. Playing the game only requires a basic knowledge
of the grammar of the language of the game. No extra
linguistic knowledge is required.
3. The game rules are designed independently of the
language of the game.
4. The game is designed for Czech and English by de-
fault.
5. During the game, the players have at least a general
idea of what their opponent(s) do.
6. The game is designed for at least two players (also a
computer can be an opponent).
7. The game offers several levels of difficulty (to fit a
vast range of players).
Table 1: Key properties of the games on the LGame portal.
We have decided to implement the PlayCoref
first. Coreference crosses the sentence boundaries
and playing coreference offers a great opportunity
to test players? willingness to read a text part by
part, e.g. sentence by sentence. In this paper, we
discuss various aspects of the PlayCoref design.
2 Coreference
Coreference occurs when several referring expres-
sions in a text refer to the same entity (e.g. per-
son, thing, reality). A coreferential pair is marked
between subsequent pairs of the referring expres-
sions. A sequence of coreferential pairs referring
to the same entity in a text forms a coreference
chain.
Various projects on the coreference annotation
by linguists are running. We mention two of
them ? the Prague Dependency Treebank 2.0 and
the coreference task for the sixth Message Under-
standing Conference.
Prague Dependency Treebank 2.0 (PDT 2.0)
6
is the only corpus establishing the coreference
annotation on a layer of meaning, so-called tec-
togrammatical layer (t-layer). The annotation in-
cludes grammatical and textual coreference. Ex-
tended textual coreference (covering additional
categories) is being annotated in PDT 2.0 in an on-
going project (Nedoluzhko, 2007).
Sixth Message Understanding Conference ? the
coreference task (MUC-6)
7
operates on a sur-
face layer. The coreferential pairs are marked be-
tween pairs of the categories nouns, noun phrases,
and pronouns.
6
ufal.mff.cuni.cz/pdt2.0
7
cs.nyu.edu/faculty/grishman/muc6.html
3 The PlayCoref Game
Motivation The PDT 2.0 coreference annota-
tion (including the annotation scheme design,
training of the annotators, technical and linguistic
support, and annotation corrections) spanned the
period from summer 2002 till autumn 2004. Each
of two annotators annotated one half out of 3,165
documents. We are aware that coreferential pairs
marked in the PlayCoref sessions may differ from
the PDT 2.0 coreference annotation. However,
the following estimates reinforce our motivation
to use the GWAP technology on texts: assuming
that (1) the PlayCoref is designed as a two-player
game, (2) at least one document is being present
in each session, (3) the session lasts up to 5 min-
utes and (4) the players play half an hour a day,
then at least 6 documents will be processed a day
by two players. This means that 3,165 documents
will be annotated by two players in 528 days, by
eight players in 132 days, by 32 players in 33 days
etc., and by 128 players in 9 days.
Strategy The game is designed for two players.
The game starts with several first sentences of the
document displayed in the players? sentence win-
dow. According to the restrictions put on the mem-
bers of the coreferential pairs, parts of the text are
unlocked while the other parts are locked. Only
unlocked parts of the text are allowed to become
a member of the coreferential pair. In our case,
only nouns and selected pronouns are unlocked.
8
In Table 2, we provide a list of the locked pro-
noun?s sub-part-of-speech classes (as designed in
the Czech positional tag system). Pronouns of
the other sub-part-of-speech classes are unlocked.
The selection of the locked pronoun?s sub-part-of-
speech classes is based on the fact that some types
of pronouns usually corefer with parts of the text
larger than one word. This type of coreference
cannot be annotated without a linguistic knowl-
edge and without training. Therefore it must be
omitted for the purposes of the PlayCoref game.
The players mark coreferential pairs between
the unlocked words in the text (no phrases are al-
lowed). They mark the coreferential pairs as undi-
rected links.
9
After the session, the coreference
8
A tagging procedure is used to get the part-of-speech
classes of the words.
9
This strategy differs from the general conception of
coreference being understood as either the anaphoric or cat-
aphoric relation depending on ?direction? of the link in the
text. We believe that the players will benefit from this sim-
210
Locked pronouns: subPOS and its description
D Demonstrative (?ten?, ?onen?, ..., lit. ?this?, ?that?, ?that?, ...
?over there?, ... )
E Relative ?co?z? (corresponding to English which in subordinate
clauses referring to a part of the preceding text)
L Indefinite ?v?sechen?, ?s?am? (lit. ?all?, ?alone?)
O ?sv?uj?, ?nesv?uj?, ?tentam? alone (lit. ?own self?, ?not-in-mood?,
?gone?)
Q Relative/interrogative ?co?, ?copak?, ?co?zpak? (lit. ?what?, ?isn?t-
it-true-that?)
W Negative (?nic?, ?nikdo?, ?nijak?y?, ??z?adn?y?, ..., lit. ?nothing?,
?nobody?, ?not-worth-mentioning?, ?no?/?none?)
Y Relative/interrogative ?co? as an enclitic (after a preposition)
(?o?c?, ?na?c?, ?za?c?, lit. ?about what?, ?on?/?onto? ?what?, ?af-
ter?/?for what?)
Z Indefinite (?n?ejak?y?, ?n?ekter?y?, ??c??koli?, ?cosi?, ..., lit. ?some?,
?some?, ?anybody?s?, ?something?)
Table 2: List of the pronoun?s sub-part-of-speech classes in
the Czech positional tag system locked for the PlayCoref.
chains are automatically reconstructed from the
coreferential pairs marked.
During the session, the number of words the
opponent has linked into the coreferential pairs is
displayed to the player. The number of sentences
with at least one coreferential pair marked by the
opponent is displayed to the player as well. Re-
vealing more information about the opponent?s ac-
tions would affect the independency of the play-
ers? decisions.
If the player finishes pairing all the related
words in a visible part of the document (visible
to him), he asks for the next sentence of the docu-
ment. It appears at the bottom of his sentence win-
dow. The player can remove pairs created before
at any time and can make new pairs in the sen-
tences read so far. The session goes on this way
until the end of the session time.
Instructions for the Players Instructions for the
players must be as comprehensible and concise as
possible. To mark a coreferential pair, no linguis-
tic knowledge is required. It is all about the text
comprehension ability.
Input Texts In the first stage of the project, doc-
uments from PDT 2.0 and MUC-6 will be used in
the sessions, so that the quality of the game data
can be evaluated against the manual coreference
annotation.
Since the PDT 2.0 coreference annotation oper-
ates on the tectogrammatical layer and PlayCoref
on the surface layer, the coreferential pairs of the t-
layer must be projected to the surface first. The ba-
sic steps of the projection are depicted in Figure 1.
Going from the t-layer, some of the coreferential
plification and that the quality of the game data will not be
decreased.
pairs get lost because their members do not have
their counterparts on surface.
10
From the remain-
ing coreferential pairs, those between nouns and
unlocked pronouns are selected. In the final game
documents, the difference between the grammat-
ical, textual and extended textual coreference is
omitted, because the players will not be asked to
distinguish them. Table 3 shows the number of
coreferential pairs in various stages of the projec-
tion.
DEEPSURFGRAM DEEPSURFTEXT
DEEPSURFGRAM DEEPSURFTEXT DEEPSURF
EX TT EE XN TD
PDT 2.0 PDT 2.0+ ext. textualcoreference surfacesubset
GRAMSURFunlockedTEXTSURFunlockedEXTENDTEXTSURFunlocked
PlayCorefdatalockedunlockedG  SR  UA  RM  F lockedunlockedT  SE  UX  RT  F lockedunlockedE    T  SE  UX  RT  F
Figure 1: Projection of the PDT coreference annotation to
the surface layer. The first step depicts the annotation of the
extended textual coreference. Pairs that have no surface coun-
terparts are marked DEEP, pairs with surface counterparts
are marked SURF. Pairs suitable for the game are marked un-
locked.
Data from the coreference task on the sixth
Message Understanding Conference can be used
in a much more straightforward way. Coreference
is annotated on the surface and no projection is
needed. The links with noun phrases are disre-
garded.
PDT 2.0 PDT 2.0 surface PlayCoref
+ ext. subset
# coref. pairs 45 96 70 33
Table 3: Number of coreferential pairs (in thousands) in
various stages of projection. Counts in the second, third and
fourth columns are extrapolated on the basis of data anno-
tated so far, which is about 200 thousand word tokens in 12
thousand sentences (out of 833 thousand tokens in 49 thou-
sand sentences in PDT 2.0). Type of the coreferential pairs,
either grammatical or textual one, is not distinguished.
Scoring The players get points for their coref-
erential pairs according to the equation pts
A
=
w
1
?ICA(A, acr)+w
2
?ICA(A,B) where A and
B are the players, acr is an automatic coreference
resolution procedure, weights 0 ? w
1
, w
2
? 1,
w
1
, w
2
? R are set empirically, and ICA stands for
the inter-coder agreement that we can simultane-
ously express either by the F-measure or Krippen-
10
Czech is a ?pro-drop? language, in which the subject pro-
noun on ?he? has a zero form (also in feminine, plural, etc.).
211
D E P
Figure 2: Player ?1? pairs (A,C) ? the dotted curve; player
?2? pairs (A,B) and (B,C) ? the solid lines; player ?3? pairs
(A,B) and (A,C) ? the dashed curves. Although players ?1?
and ?2? do not agree on the coreferential pairs at all, ?1? and
?3? agree only on (A,C) and ?2? and ?3? agree only on (A,B),
for the purposes of the coreference chains reconstruction, the
players? agreement is higher: players ?1? and ?2? agree on two
members of the coreferential chain: A and C, players ?1? and
?3? agree on A and C as well, and players ?2? and ?3? achieved
agreement even on all three members: A, B, and C.
dorff?s ? (Artstein and Poesio, 2008). The score
is calculated at the end of the session and no run-
ning score is being presented during the session.
Otherwise, the players might adjust their decisions
according to the changes in the score. Obviously,
it is undesirable.
Assigning a score to the players deals with the
coreferential pairs. However, motivated by (Pas-
sonneau, 2004) and others, the evaluation handles
the coreferential pairs in a way demonstrated in
Figure 2.
PlayCoref vs. PhraseDetectives At least to
our knowledge, there are no other GWAPs deal-
ing with the relationship among words in a text
like PhraseDetectives and PlayCoref. Neverthe-
less, there are many differences between these two
games ? the main ones are enumerated in Table 4.
PlayCoref PhraseDetectives
detection of coreference
chains
anaphora resolution
two-player game one-player game
a document presented sen-
tence by sentence
a paragraph presented at
once
? checking the pairs marked
in the previous sessions
pairing not restricted to the
position in the text
the closest antecedent
simple instructions players training
scoring with respect to the
automatic coreference reso-
lution and to the opponent?s
pairs
scoring with respect to the
players that play with the
same document before
coreferential pairs correc-
tion
no corrections allowed
Table 4: PlayCoref vs. PhraseDetectives.
4 Conclusion
We propose the PlayCoref game, a concept of a
GWAP with texts that aims at getting the docu-
ments with the coreference annotation in substan-
tially larger volume than can be obtained from
experts. In the proposed game, we introduce
coreference to the players in a way that no lin-
guistic knowledge is required from them. We
present the game rules design, the preparation of
the game documents and the evaluation of the
players? score. A short comparison with a simi-
lar project is also provided.
Acknowledgments
We gratefully acknowledge the support of the
Czech Ministry of Education (grants MSM-
0021620838 and LC536), the Czech Grant
Agency (grant 405/09/0729), and the Grant
Agency of Charles University in Prague (project
GAUK 138309).
References
Ron Artstein, Massimo Poesio. 2008. Inter-Coder Agree-
ment for Computational Linguistics. Computational Lin-
guistics, December 2008, vol. 34, no. 4, pp. 555?596.
Udo Kruschwitz, Jon Chamberlain, Massimo Poesio. 2009.
(Linguistic) Science Through Web Collaboration in the
ANAWIKI project. In Proceedings of the WebSci?09: So-
ciety On-Line, Athens, Greece, in press.
Lucie Ku?cov?a, Eva Haji?cov?a. 2005. Coreferential Relations
in the Prague Dependency Treebank. In Proceedings of
the 5th International Conference on Discourse Anaphora
and Anaphor Resolution, San Miguel, Azores, pp. 97?102.
Edith. L. M. Law et al 2007. Tagatune: A game for music
and sound annotation. In Proceedings of the Music In-
formation Retrieval Conference, Austrian Computer Soc.,
pp. 361?364.
Anna Nedoluzhko. 2007. Zpr?ava k anotov?an?? roz?s???ren?e
textov?e koreference a bridging vztah?u v Pra?zsk?em
z?avoslostn??m korpusu (Annotating extended coreference
and bridging relations in PDT). Technical Report, UFAL,
MFF UK, Prague, Czech Republic.
Rebecca J. Passonneau. 2004. Computing Reliability for
Coreference. Proceedings of LREC, vol. 4, pp. 1503?
1506, Lisbon.
Katharina Siorpaes and Martin Hepp. 2008. Games with a
purpose for the Semantic Web. IEEE Intelligent Systems
Vol. 23, number 3, pp. 50?60.
Luis van Ahn and Laura Dabbish. 2004. Labelling images
with a computer game. In Proceedings of the SIGHI Con-
ference on Human Factors in Computing Systems, ACM
Press, New York, pp. 319?326.
Luis van Ahn and Laura Dabbish. 2008. Designing Games
with a Purpose. Communications of the ACM, vol. 51,
No. 8, pp. 58?67.
212
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 36?43,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Annotated Corpus Outside Its Original Context: A Corpus-Based
Exercise Book
Barbora Hladka? and Ondr?ej Kuc?era
Institute of Formal and Applied Linguistics, Charles University
Malostranske? na?m. 25
118 00 Prague
Czech Republic
hladka@ufal.mff.cuni.cz, ondrej.kucera@centrum.cz
Abstract
We present the STYX system, which is de-
signed as an electronic corpus-based exercise
book of Czech morphology and syntax with
sentences directly selected from the Prague
Dependency Treebank, the largest annotated
corpus of the Czech language. The exercise
book offers complex sentence processing with
respect to both morphological and syntactic
phenomena, i. e. the exercises allow students
of basic and secondary schools to practice
classifying parts of speech and particular mor-
phological categories of words and in the pars-
ing of sentences and classifying the syntactic
functions of words. The corpus-based exer-
cise book presents a novel usage of annotated
corpora outside their original context.
1 Introduction
Schoolchildren can use a computer to chat with their
friends, to play games, to draw, to browse the Inter-
net or to write their own blogs - why should they not
use it to parse sentences or to determine the mor-
phological categories of words? We do not expect
them to practice grammar as enthusiastically as they
do what is mentioned above, but we believe that an
electronic exercise book could make the practicing,
which they need to do anyway, more fun.
We present the procedure of building an exercise
book of the Czech language based on the Prague
Dependency Treebank. First (in Section 2) we
present the motivation for building an exercise book
of Czech morphology and syntax based on an an-
notated corpus ? the Prague Dependency Treebank
(PDT). Then we provide a short description of the
PDT itself in Section 3. Section 4 is the core of
our paper. Section 4.1 is devoted to the filtering of
the PDT sentences in such a way that the complex-
ity of sentences included in the exercise book ex-
actly corresponds to the complexity of sentences ex-
ercised in traditional Czech textbooks and exercise
books. Section 4.2 documents the transformation of
the sentences ? more precisely a transformation of
their annotations into the school analysis scheme as
recommended by the official framework of the ed-
ucational programme for general secondary educa-
tion (Jer?a?bek and Tupy?, 2005). The evaluation of the
system is described in Section 4.3. Section 5 sum-
marizes this paper and plans for the future work.
2 Motivation
From the very beginning, we had an idea of us-
ing an annotated corpus outside its original context.
We recalled our experience from secondary school,
namely from language lessons when we learned
morphology and syntax. We did it ?with pen and pa-
per? and more or less hated it. Thus we decided to
build an electronic exercise book to learn and prac-
tice the morphology and the syntax ?by moving the
mouse around the screen.?
In principle, there are two ways to build an ex-
ercise book - manually or automatically. A manual
procedure requires collecting sentences the authors
usually make up and then process with regard to the
chosen aspects. This is a very demanding, time-
consuming task and therefore the authors manage
to collect only tens (possibly hundreds) of sentences
that simply cannot fully reflect the real usage of a
language. An automatic procedure is possible when
an annotated corpus of the language is available.
Then the disadvantages of the manual procedure dis-
36
appear. It is expected that the texts in a corpus are
already selected to provide a well-balanced corpus
reflecting the real usage of the language, the hard an-
notation work is also done and the size of such cor-
pus is thousands or tens of thousands of annotated
sentences. The task that remains is to transform the
annotation scheme used in the corpus into the sen-
tence analysis scheme that is taught in schools. In
fact, a procedure based on an annotated corpus that
we apply is semi-automatic, since the annotation
scheme transformation presents a knowledge-based
process designed manually - no machine-learning
technique is used.
We browsed the Computer-Assisted Language
Learning (CALL) approaches, namely those con-
centrated under the teaching and language cor-
pora interest group (e.g. (Wichmann and Fligel-
stone (eds.), 1997), (Tribble, 2001), (Murkherjee,
2004), (Schultze, 2003), (Scott, Tribble, 2006)).
We realized that none of them actually employs
manually annotated corpora ? they use corpora as
huge banks of texts without additional linguistic
information (i.e. without annotation). Only one
project (Keogh et al, 2004) works with an automat-
ically annotated corpus to teach Irish and German
morphology.
Reviewing the Czech electronic exercise books
available (e.g. (Terasoft, Ltd., 2003)), none of them
provides the users with any possibility of analyzing
the sentence both morphologically and syntactically.
All of them were built manually.
Considering all the facts mentioned above, we
find our approach to be novel one. One of the most
exciting aspects of corpora is that they may be used
to a good advantage both in research and teach-
ing. That is why we wanted to present this system
that makes schoolchildren familiar with an academic
product. At the same time, this system represents a
challenge and an opportunity for academics to pop-
ularize a field with a promising future that is devoted
to natural language processing.
3 The Prague Dependency Treebank
The Prague Dependency Treebank (PDT) presents
the largest annotated corpus of Czech, and its second
edition was published in 2006 (PDT 2.0, 2006). The
PDT had arisen from the tradition of the successful
Prague School of Linguistics. The dependency ap-
proach to syntactic analysis with the main role of
a verb has been applied. The annotations go from
the morphological layer through to the intermedi-
ate syntactic-analytical layer to the tectogrammati-
cal layer (the layer of an underlying syntactic struc-
ture). The texts have been annotated in the same
direction, i. e. from the simplest layer to the most
complex. This fact corresponds with the amount of
data annotated on each level ? 2 million words have
been annotated on the lowest morphological layer,
1.5 million words on both the morphological and the
syntactic layer, and 0.8 million words on all three
layers.
Within the PDT conceptual framework, a sen-
tence is represented as a rooted ordered tree with la-
beled nodes and edges on both syntactic (Hajic?ova?,
Kirschner and Sgall, 1999) and tectogrammatical
(Mikulova? et al, 2006) layers. Thus we speak about
syntactic and tectogrammatical trees, respectively.
Representation on the morphological layer (Hana et
al., 2005) corresponds to a list of (word token and
morphological tag) pairs. Figure 1 illustrates the
syntactic and morphological annotation of the sam-
ple sentence Rozd??l do regulovane? ceny byl hrazen
z dotac??. [The variation of the regulated price was
made up by grants.] One token of the morphological
layer is represented by exactly one node of the tree
(rozd??l [variation], do [of], regulovane? [regulated],
ceny [price], byl [was], hrazen [made up], z [by],
dotac?? [grants], ?.?) and the dependency relation be-
tween two nodes is captured by an edge between
them, i. e. between the dependent and its governor.
The actual type of the relation is given as a func-
tion label of the edge, for example the edge (rozd??l,
hrazen) is labeled by the function Sb (subject) of the
node rozd??l. Together with a syntactic function, a
morphological tag is displayed (rozd??l, NNIS1-----A-
---).
Since there is m:n correspondence between the
number of nodes in syntactic and tectogrammati-
cal trees, it would be rather confusing to display
the annotations on those layers all together in one
tree. Hence we provide a separate tree visualizing
the tectogrammatical annotation of the sample sen-
tence ? see Figure 2. A tectogrammatical lemma
and a functor are relevant to our task, thus we dis-
play them with each node in the tectogrammatical
37
DFPSUSProceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 52?55,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Designing a Language Game for Collecting Coreference Annotation
Barbora Hladka? and Jir??? M??rovsky? and Pavel Schlesinger
Charles University in Prague
Institute of Formal and Applied Linguistics
e-mail: {hladka, mirovsky, schlesinger@ufal.mff.cuni.cz}
Abstract
PlayCoref is a concept of an on-line lan-
guage game designed to acquire a substan-
tial amount of text data with the corefer-
ence annotation. We describe in detail var-
ious aspects of the game design and dis-
cuss features that affect the quality of the
annotation.
1 Introduction
Creating a collection of high quality data is
resource-demanding regardless of the area of re-
search and type of the data. This fact has encour-
aged a formulation of an alternative way of data
collection, ?Games With a Purpose? methodology
(GWAP), (van Ahn and Dabbish, 2008). The
GWAP methodology exploits the capacity of In-
ternet users who like to play on-line games. The
games are designed to generate data for applica-
tions that either have not been implemented yet,
or have already been implemented with a perfor-
mance lower than human. The players work sim-
ply by playing the game - the data are generated as
a by-product of the game. The more enjoyable the
game is, the more users play it and the more data
is acquired.
The GWAP methodology was first used for on-
line games with images (van Ahn and Dabbish,
2004) and later with tunes (Law et al, 2007),1
in which the players try to agree on a caption of
the image/tune. The popularity of these games is
enormous and generates a huge amount of data.
Onto games (Siorpaes and Hepp, 2008) brought
another type of input data to GWAP ? video and
text.2
The situation with text is slightly different. One
has to read a text in order to identify its topics.
1www.gwap.org
2www.ontogame.org
Reading texts takes more time than observing im-
ages and the longer text, the worse. Since the
game must be of a dynamic character, it is unimag-
inable that the players would spend minutes read-
ing an input text. Therefore, it must be opened to
the players ?part? by ?part?.
So far, besides the Onto games, two more games
with texts have appeared: What did Shannon
say?3, the goal of which is to help the speech
recognizer with difficult-to-recognize words, and
Phrase Detectives4 (Kruschwitz, Chamberlain,
Poesio, 2009), the goal of which is to identify re-
lationships between words and phrases in a text.
No information about their popularity has been
published yet.
Motivated by the GWAP portal, the LGame por-
tal5 dedicated to language games has been estab-
lished. The LGame portal has been opened with
the Shannon game, a game of intentionally hidden
words in the sentence, where players guess them,
and the Place the Space game, a game of word
segmentation.
2 Coreference
Coreference occurs when several referring expres-
sions in a text refer to the same entity (e.g. per-
son, thing, fact). A coreferential pair is marked
between subsequent pairs of the referring expres-
sions. A sequence of coreferential pairs referring
to the same entity in a text forms a coreference
chain. The coreferential pairs and the coreference
chains cover only the identity relation.
Many projects for various languages on the
coreference annotation by linguists are running.
The annotated data serve as a basis for further
linguistic study of coreference, and most impor-
tantly also to train and test procedures for auto-
matic coreference resolution, which is a task that
3lingo.clsp.jhu.edushannongame.html
4www.phrasedetectives.org
5www.lgame.cz
52
many other applications can benefit from, e.g. text
summarization, question answering, and informa-
tion retrieval.
Manual annotation is costly and time consum-
ing. We propose a design of the PlayCoref game
? to appear at the LGame portal ? as an alternative
way of the coreference annotation collection, and
most importantly, of a substantially larger volume
than any expert annotation can ever achieve.
3 The PlayCoref Game
3.1 Game Design
We prepare the game for Czech and English first.
However, PlayCoref can be played in any lan-
guage.
The game is designed for two players. The
game starts with several first sentences of the doc-
ument displayed in the players? sentence window.
According to the restrictions put on the members
of the coreferential pairs, parts of the text are un-
locked (i.e. they are active) while the other parts
are locked (i.e. they are inactive); both of them
are graphically distinguished. In our case, only
nouns and selected pronouns are unlocked. The
players mark coreferential pairs between the in-
dividual unlocked words in the text (no phrases
are allowed). They mark the coreferential pairs as
undirected links.
During the session, the number of words the
opponent has linked into the coreferential pairs is
displayed to the player. The number of sentences
with at least one coreferential pair marked by the
opponent is displayed to the player as well. Re-
vealing more information about the opponent?s ac-
tions would affect the independency of the play-
ers? decisions.
If the player finishes pairing all the related
words in the visible part of the document (visible
to him), he asks for the next sentence of the docu-
ment. It appears at the bottom of the player?s sen-
tence window. The player can remove pairs cre-
ated before at any time and can make new pairs in
the sentences read so far. The session goes on this
way until the end of the session time. More than
one document can be present in the session.
After the session, the players? scores are calcu-
lated and displayed.
Instructions for the Players Instructions for the
players must be as comprehensible and concise as
possible. To mark a coreferential pair, no linguis-
tic knowledge is required, thus no extensive anno-
tation guidelines need to be formulated. It is all
about the text comprehension ability.
3.2 Game Data
Any textual data can be used in the game, but the
following pre-processing steps are necessary.
Tagging Most importantly, the morphological
tagging (usually preceded by tokenization) is
required to recognize part-of-speech categories
(and sub-part-of-speech categories), in order to
lock/unlock individual words for the game. For
most languages, tagging is a well solved problem
(e.g. for Czech: the MORC?E tagger6, for English:
TnT tagger7).
Text Parts Locking In the game, we work with
coreferential links between the individual words
only. The coreferential pairs that link larger text
parts consisting of clauses or even several sen-
tences are disregarded. Their marking requires lin-
guistic knowledge and extensive training.
Our research shows that pronouns that are usu-
ally members of such ?undesirable? links can
be detected automatically in advance (at least in
Czech). They will get locked, so the players will
not consider them at all during the sessions.
Automatic Coreference Resolution According
to the way we calculate the players scores (see be-
low), an automatic procedure for coreference res-
olution is required. If this procedure works on a
different layer than the surface layer, further auto-
matic processing of the data may be needed.
4 Data Quality
4.1 Players? Score
We want to obtain a large volume of data so we
must first attract the players and motivate them
to play the game more and more. As a reward
for their effort we present scoring. We hope that
the players? appetite to win, to confront with their
opponents and to place well in the long-term top
scores tables correlates with our research aims and
objectives.
Our goal is to ensure the highest quality of the
annotation. The scoring function should reflect
the game data quality and thus motivate the play-
ers to produce the right data. An agreement with
6ufal.mff.cuni.cz/morce
7www.coli.uni-saarland.de/?thorsten/
tnt/
53
the manual expert annotation would be a perfect
scoring function. But the manual annotation is not
available for all languages and above all, it is not
our goal to annotate already annotated data.
An automatic coreference resolution procedure
serves as a first approximation for the scoring
function. Since the procedure does not work for
?100%?, we need to add another component. We
suppose that most of the players will mark the
coreferential pairs reliably. Then an agreement
between the players? pairs indicates correctness,
even if the pair differs from the output of auto-
matic coreference resolution procedure. There-
fore, the inter-player agreement will become the
second component of the scoring function. To mo-
tivate the players to ask for more parts of the text
(and not only ?tune? links in the initially displayed
sentences), the third component of the scoring
function will award number of created coreferen-
tial links.
The players get points for their coreferential
pairs according to the equation ptsA = w1 ?
ICA(A, acr) + w2 ? ICA(A,B) + w3 ? N(A)
whereA andB are the players, acr is an automatic
coreference resolution procedure, ICA stands for
the inter-coder agreement that we can simultane-
ously express either by the F-measure or Krippen-
dorff?s ? (Krippendorf, 2004), N is a contribu-
tion of the number of created links, and weights
0 ? w1, w2 ? 1, w1, w2, w3 ? R (summing to 1)
are set empirically.
The score is calculated at the end of the ses-
sion and no running score is being presented dur-
ing the session. From the scientific point of view,
the scores serve for the long term quality control
of the players? annotation.
4.2 Interactivity Issues
The degree of a player-to-player interactivity con-
tributes to the attractiveness of the game. From the
player?s point of view, the more interactivity, the
better. For example, knowing both his and the op-
ponent?s running score would be very stimulating
for the mutual competitiveness. From the linguis-
tics? point of view, once any kind of interaction is
allowed, statistically pure independency between
the players? decisions is lost. A reasonable trade-
off between the interactivity and the independency
must be achieved. Interactivity that would lead to
cheating and decreasing the quality of the game
data must be avoided.
Allowing the players to see their own running
score would lead to cheating. The players might
adjust their decisions according to the changes in
the score. Another possible extension of interac-
tivity that would lead to cheating is highlighting
words that the opponent used in the coreferential
pairs. The players might then wait for the oppo-
nent?s choice and again, adjust their decisions ac-
cordingly. Such game data would be strongly bi-
ased. However, we still believe that a slight idea of
what the opponent is doing can boost inter-coder
agreement and yet avoid cheating. Revealing the
information about the opponent?s number of pairs
and number of sentences with at least one pair of-
fers not zero but low interactivity, yet it will not
harm the quality of the data.
4.3 Post-Processing
The players mark the coreferential links undi-
rected. This strategy differs from the general con-
ception of coreference being understood as either
the anaphoric or cataphoric relation depending on
the ?direction? of the link in the text. We believe
that the players will benefit from this simplifica-
tion and so will the data quality. After the ses-
sion, the coreference chains are automatically re-
constructed from the coreferential pairs.
4.4 Evaluation
Data with manually annotated coreference will be
used to measure the game data quality. We will
also study how much the scoring function suffers
from the difference between the output of the au-
tomatic coreference resolution procedure and the
manual annotation (gold standard). For Czech, we
will use the data from PDT 2.0, for English from
MUC-6.
PDT 2.0 8 contains the annotation of grammat-
ical and pronominal textual coreference. Nomi-
nal textual coreference is being annotated in PDT
2.0 in an ongoing project (Nedoluzhko, 2007).
Since the PDT 2.0 coreference annotation oper-
ates on the so-called tectogrammatical layer (layer
of meaning) and PlayCoref plays on the surface
layer, the coreferential pairs must be projected to
the surface first. The process consists of several
steps and only a part of the coreferential pairs is
actually projectable to the surface (links between
nodes that have no surface counterpart get lost).
8ufal.mff.cuni.cz/pdt2.0
54
MUC-6 9 operates on the surface layer. This
data can be used in a much more straightforward
way. The coreferential pairs are marked between
nouns, noun phrases, and pronouns and no projec-
tion is needed. The links with noun phrases are
disregarded.
Evaluation Methods For the game data evalu-
ation, well established methods for calculating an
inter-annotator agreement in the coreference anno-
tation will be employed. These methods consider
a coreference chain to be a set of words and they
measure the agreement on the membership of the
individual words in the sets (Passonneau, 2004).
Weighted agreement coefficients such as Krippen-
dorf?s ? (Krippendorf, 2004) need to be used -
sets of words can differ only partially, which does
not mean a total disagreement.
5 Further Work
Acquisition Evaluation Process The quality of
the game annotation undergoes standard evalua-
tion. Apart from collecting, assuming the game
reaches sufficient popularity, long-term monitor-
ing of the players? outputs can bring into question
new issues concerning the game data quality: How
much can we benefit from presenting a document
into more sessions? Should we prefer the output of
more reliable and experienced players during the
evaluation? Should we omit the output of ?not-so-
reliable? players?
Named Entity Recognition The step of the
named entity recognition will be applied in the
subsequent stages of the project. Multi-word ex-
pressions that form a named entity (e.g. ?Czech
National Bank?) will be presented to the players
as a single unit of annotation. We also plan to im-
plement a GWAP for named entity recognition.
6 Conclusion
We have presented the concept of the PlayCoref
game, a proposed language game that brings a
novel approach to collecting coreference annota-
tion of texts using the enormous potential of In-
ternet users. We have described the design of the
game and discussed the issues of interactivity of
the players and measuring the player score ? is-
sues that are crucial both for the attractiveness of
the game and for the quality of the game data. The
9cs.nyu.edu/faculty/grishman/muc6.html
game can be applied on any textual data in any lan-
guage, providing certain basic tools also discussed
in the paper exist. The GWAPs are open-ended
stories so until the game is released, it is hard to
say if the players will find it attractive enough. If
so, we hope to collect a large volume of data with
coreference annotation at extremely low costs.
Acknowledgments
We gratefully acknowledge the support of the
Czech Ministry of Education (grants MSM-
0021620838 and LC536), the Czech Grant
Agency (grant 405/09/0729), and the Grant
Agency of Charles University in Prague (project
GAUK 138309).
References
Klaus Krippendorf. 2004. Content Analysis: An Introduc-
tion to Its Methodology, second edition, chapter 11, Sage,
Thousand Oaks, CA.
Udo Kruschwitz, Jon Chamberlain, Massimo Poesio. 2009.
(Linguistic) Science Through Web Collaboration in the
ANAWIKI project. In Proceedings of the WebSci?09: So-
ciety On-Line, Athens, Greece, in press.
Lucie Kuc?ova?, Eva Hajic?ova?. 2005. Coreferential Relations
in the Prague Dependency Treebank. In Proceedings of
the 5th International Conference on Discourse Anaphora
and Anaphor Resolution, San Miguel, Azores, pp. 97?102.
Edith. L. M. Law et al 2007. Tagatune: A game for music
and sound annotation. In Proceedings of the Music In-
formation Retrieval Conference, Austrian Computer Soc.,
pp. 361?364.
Anna Nedoluzhko. 2007. Zpra?va k anotova?n?? rozs???r?ene?
textove? koreference a bridging vztahu? v Praz?ske?m
za?voslostn??m korpusu (Annotating extended coreference
and bridging relations in PDT). Technical Report, UFAL,
MFF UK, Prague, Czech Republic.
Rebecca J. Passonneau. 2004. Computing Reliability for
Coreference. Proceedings of LREC, vol. 4, pp. 1503?
1506, Lisbon.
Katharina Siorpaes and Martin Hepp. 2008. Games with a
purpose for the Semantic Web. IEEE Intelligent Systems
Vol. 23, number 3, pp. 50?60.
Luis van Ahn and Laura Dabbish. 2004. Labelling images
with a computer game. In Proceedings of the SIGHI Con-
ference on Human Factors in Computing Systems, ACM
Press, New York, pp. 319?326.
Luis van Ahn and Laura Dabbish. 2008. Designing Games
with a Purpose. Communications of the ACM, vol. 51, No.
8, pp. 58?67.
Marc Vilain et al 1995. A Model-Theoretic Coreference
Scoring Scheme. Proceedings of the Sixth Message Un-
derstanding Conference, pp. 45?52, Columbia, MD.
55
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 90?98,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Syntactic annotation of spoken utterances: 
A case study on the Czech Academic Corpus 
 
 
Barbora Hladk? and Zde?ka Ure?ov? 
 
Charles University in Prague 
Institute of Formal and Applied Linguistics  
{hladka, uresova}@ufal.mff.cuni.cz 
 
Abstract 
Corpus annotation plays an important 
role in linguistic analysis and computa-
tional processing of both written and 
spoken language. Syntactic annotation 
of spoken texts becomes clearly a topic 
of considerable interest nowadays, 
driven by the desire to improve auto-
matic speech recognition systems by 
incorporating syntax in the language 
models, or to build language under-
standing applications. Syntactic anno-
tation of both written and spoken texts 
in the Czech Academic Corpus was 
created thirty years ago when no other 
(even annotated) corpus of spoken texts 
has existed. We will discuss how much 
relevant and inspiring this annotation is 
to the current frameworks of spoken 
text annotation. 
1 Motivation 
The purpose of annotating corpora is to cre-
ate an objective evidence of the real usage 
of the language. In general, it is easier to anno-
tate written text ? speech must be recorded and 
transcribed to process it whilst texts are avail-
able ?immediately?; moreover, written texts 
usually obey standard grammar rules of the 
language in questions, while a true transcript of 
spoken utterances often does not. 
The theoretical linguistic research considers 
the language to be a system of layers 
(e.g. the Government and Binding theory 
(Chomsky, 1993), the Functional-Generative 
Description of the language (Sgall, Haji?ov?, 
Panevov? 1986)). In order to be a valuable 
source of linguistic knowledge, the corpus an-
notation should respect this point of view. 
The morphological and syntactic layers 
of annotation represent a standard in today?s 
text corpora, e.g. the Penn Treebank, 
the family of the Prague Dependency Tree-
banks, the Tiger corpus for German, etc. Some 
corpora contain a semantic annotation, such as 
the Penn Treebank enriched by PropBank and 
Nombank, the Prague Dependency Treebank in 
its highest layer, the Penn Chinese or the 
the Korean Treebanks. The Penn Discourse 
Treebank contains discourse annotation. 
It is desirable that syntactic (and higher) an-
notation of spoken texts respects the written-
text style as much as possible, for obvious rea-
sons: data ?compatibility?, reuse of tools etc. 
A number of questions arise immediately: 
How much experience and knowledge ac-
quired during the written text annotation can 
we apply to the spoken texts? Are the annota-
tion instructions applicable to transcriptions in 
a straightforward way or some modifications 
of them must be done? Can transcriptions be 
annotated ?as they are? or some transformation 
of their inner structure into a written text struc-
ture must precede the annotation? The Czech 
Academic Corpus will help us to find out the 
answers. 
2 Introduction 
The first attempts to syntactically annotate 
spoken texts date back to the 1970s and 1980s 
when the Czech Academic Corpus ? CAC 
(Kr?l?k, Uhl??ov?, 2007) and the Swedish Tal-
banken (Nilsson, Hall, Nivre, 2005) appeared. 
Talbanken was annotated with partial phrase 
structures and grammatical functions, CAC 
with dependency-based structures and analyti-
cal functions. Thus both corpora can be re-
garded as belonging to the pioneers in corpus 
linguistics, together with the paper-only ?Quirk 
corpus? (Svartvik, Quirk, 1980; computerized 
later as the London-Lund Corpus).1  
                                                 
1
 When these annotation projects began in the 1960s, 
there were only two computerized manually annotated 
corpora available: the Brown Corpus of American Eng-
90
 During the last twenty years the work on 
creating new treebanks has increased consid-
erably and so CAC and Talbanken have been 
put in a different light, namely with regard to 
their internal formats and annotation schemes. 
Given that, transformation of them became 
necessary: while the Talbanken?s transforma-
tion concerned only the internal format, trans-
formation of CAC concerned both internal for-
mat and annotation scheme. 
Later, more annotated corpora of spoken 
texts have appeared, like the British Compo-
nent of the International Corpus of English 
(ICE-GB, Greenbaum, 1996), the Fisher Cor-
pus for English (Cieri et al, 2004), the Childes 
database 2 , the Switchboard part of the Penn 
Treebank (Godfrey et al, 1992), Corpus 
Gesproken Nederlands (Hoekstra et al, 2001) 
and the Verbmobil corpora.3 The syntactic an-
notation in these corpora is mostly automatic 
using tools trained on written corpora or on a 
small, manually annotated part of spoken cor-
pora. 
The aim of our contribution is to answer 
the question whether it is possible to annotate 
speech transcriptions syntactically according to 
the guidelines originally designed for text cor-
pora. We will show the problems that arise in 
extending an explicit scheme of syntactic an-
notation of written Czech into the domain of 
spontaneous speech (as found in the CAC).  
Our paper is organized as follows. In Sec-
tion 3, we give a brief description of the past 
and present of the Czech Academic Corpus. 
The compatibility of the original CAC syntac-
tic annotation with a present-day approach 
adopted by the Prague Dependency Treebank 
project is evaluated in Section 4. Section 5 is 
the core of our paper. We discuss phenomena 
typical for spoken texts making impossible to 
annotate them according to the guidelines for 
written texts. We explore a trade-off between 
leaving the original annotation aside and anno-
tating from scratch, and an upgrade of the 
original annotation. In addition, we briefly 
compare the approach adopted for Czech and 
those adopted for other languages. 
                                                                       
lish and the LOB Corpus of British English. Both contain 
written texts annotated for part of speech. Their size is 1 
mil. tokens. 
2
 http://childes.psy.cmu.edu/grasp/ 
3
 http://verbmobil.dfki.de/ 
3 The Czech Academic Corpus: past 
and present (1971-2008) 
The idea of the Czech Academic Corpus 
(CAC) came to life between 1971 and 1985 
thanks to the Department of Mathematical 
Linguistics within the Institute of Czech Lan-
guage. The discussion on the concept of aca-
demic grammar of Czech, i.e. on the concept 
of CAC annotation, finally led to the tradi-
tional, systematic, and well elaborated concept 
of morphology and dependency syntax (?mi-
lauer, 1972). By the mid 1980s, a total of 
540,000 words of CAC were morphologically 
and syntactically manually annotated.  
The documents originally selected for the 
CAC are articles taken from a range of media. 
The sources included newspapers and maga-
zines, and transcripts of spoken language from 
radio and TV programs, covering administra-
tive, journalistic and scientific fields. 
The original CAC was on par with it peers at 
the time (such as the Brown corpus) in size, 
coverage, and annotation; it surpassed them in 
that it contained (some) syntactic annotation. 
CAC was used in the first experiments of sta-
tistical morphological tagging of Czech (Haji?, 
Hladk?, 1997). 
After the Prague Dependency Treebank 
(PDT) has been built (Haji? et al, 2006), a 
conversion from the CAC to the PDT format 
has started. The PDT uses three layers of anno-
tation: morphological, syntactic and ?tecto-
grammatical? (or semantic) layers (henceforth 
m-layer, a-layer and t-layer, respectively).  
The main goal was to make the CAC and the 
PDT compatible at the m-layer and the a-layer, 
and thus to enable integration of the CAC into 
the PDT. The second version of the CAC pre-
sents such a complete conversion of the inter-
nal format and the annotation schemes. The 
overall statistics on the CAC 2.0 are presented 
in Table 1. 
Annotation transformation is visualized in 
Figure 1. In the areas corresponding to the cor-
pora, the morphological annotation is symbol-
ized by the horizontal lines and syntactical an-
notation by the vertical lines.  
Conversion of the originally simple textual 
comma-separated values format into the Pra-
gue Markup Language (Pajas, ?t?p?nek, 2005) 
was more or less straightforward. 
Morphological analysis of Czech in the 
CAC and in the PDT is almost the same, ex-
cept that the morphological tagset of CAC is 
91
 slightly more detailed. Semi-automatic conver-
sion of the original morphological annotation 
into the Czech positional morphological tagset 
was executed in compliance with the morpho-
logical annotation of PDT (Hana et al, 2005). 
Figure 1 shows that morphological annotation 
conversion of both written and spoken texts 
was done. 
The only major problem in this conversion 
was that digit-only tokens and punctuation 
were omitted from the original CAC since they 
were deemed linguistically ?uninteresting?, 
which is certainly true from the point of view 
of the original CAC?s purpose to give quantita-
tive lexical support to a new Czech dictionary. 
Since the sources of the CAC documents were 
no longer available, missing tokens had to in-
serted and revised manually. 
Syntactic conversion of CAC was more de-
manding than the morphological one. In a pilot 
study, (Ribarov et al, 2006) attempt to answer 
a question whether an automatic transforma-
tion of the CAC annotation into the PDT for-
mat (and subsequent manual corrections) is 
more effective than to leave the CAC annota-
tion aside and process the CAC?s texts by a 
statistical parser instead (again, with subse-
quent manual correction). In the end, the latter 
variant was selected (with regrets). No distinc-
tion in strategy of written and spoken texts an-
notation transformation was made. However, 
spoken texts were eventually excluded from 
the CAC 2.0 (Figure 1). Reasons for this are 
explained in detail in the following two sec-
tions. 
4 Syntax in the CAC and the PDT 
4.1 Syntactic annotation in the CAC 
The syntactic analysis of Czech in the CAC 
and in the PDT is very much alike, but there 
are phenomena in which the CAC syntactic 
annotation scenario differs from the PDT, even 
though both corpora are based on the same 
linguistic theory (?milauer, 1969), i.e. on the 
dependency grammar notion common to the 
?Prague school? of linguists since the 1930s.  
However, the syntactic annotation differs 
between the two corpora. The CAC works with 
a single syntactic layer, whereas the PDT 
works with two independent (although inter-
linked) syntactic layers: an analytical (syntac-
tic) one and a tectogrammatical one (a-layer 
and t-layer, respectively). In this paper, we are 
referring to the a-layer of the PDT in our com-
parisons unless specifically noted for those 
elements of the tectogrammatical annotation 
that do have some counterpart in the CAC. 
 
 
Figure 1 Overall scheme of the CAC conver-
sion 
 
Style form4 #docs #sntncs 
(K) 
#tokens 
(K) 
Journalism w 52 10 189 
Journalism s 8 1 29 
Scientific w 68 12 245 
Scientific s 32 4 116 
administrative w 16 3 59 
administrative s 4 2 14 
Total w 135 25 493 
Total s 44 7 159 
Total w&s 180 32 652 
Table 1 Size of the CAC 2.0 parts 
The CAC annotation scheme makes a sub-
stantial distinction between two things: surface 
syntactic relations within a single clause as 
well as syntactic relations between clauses in a 
complex sentence. These two types of syntac-
tic information are captured by two types of 
syntactic tags. 
(a) Word-level (intra-clausal) syntactic tag is 
a 6-position tag assigned to every non-
auxilliary (?autosemantic?) word within a 
single clause, representing the intra-
clausal dependency relations.  
(b) Clause-level (intra-sentential) syntactic 
tag is a 8-position tag assigned to the first 
token of each clause in a complex sen-
tence, representing the status (and possi-
ble dependency) of the given clause 
within the given (complex) sentence. 
                                                 
4
 Either written (w) or spoken (s) texts. 
written  
spoken  
written  
C
A
C 
P
D
T 
2.
0 
written  
spoken  
C
A
C 
2.
0 
theory  theory  
guidelines  guidelines  
92
 The CAC thus annotates not only depend-
ency relations within a single clause but also 
dependency relations within a complex sen-
tence. 
A description of the 6-position and the 8-
position tags is given in Tables 2 and 3, respec-
tively. (Ribarov et al, 2006) gives a detailed 
description. 
4.2 Syntactic annotation in the PDT 
The PDT a-layer annotates two main things: 
a dependency structure of the sentence and 
types of these dependencies.  
Representation of a structure of the sentence 
is rendered in a form of a dependency tree, the 
nodes of which correspond to the tokens 
(words and punctuation) that form the sen-
tence. The type of dependency (subject, object, 
adverbial, complement, attribute, etc.) is repre-
sented by a node attribute called an ?analytical 
function? (afun for short; the most frequent 
values of this attribute are listed in Table 4).  
4.3 CAC vs. PDT 
Comparing the CAC and the PDT syntactic 
annotation scenarios, we can see that the anno-
tation of the major syntactic relations within a 
sentence is very similar, from similar adapta-
tions of the theoretical background down to the 
high-level corpus markup conventions. For 
example, in both corpora the predicate is the 
clausal head and the subject is its dependent, 
unlike the descriptions we can find in the tradi-
tional Czech syntactic theory (?milauer, 1969). 
Another (technical) similarity can be found in 
the way the dependency types are encoded. In 
both corpora, the dependency type label is 
stored at the dependent. No confusion arises 
since the link from a dependent to its governor 
is unique. 
However, the list of differences is actually 
quite long. Some are minor and technical: for 
example, in the PDT an ?overarching? root of 
the sentence tree (marked AuxS) is always 
added, so that all other nodes appear as if they 
depend on it. Some differences are more pro-
found and are described below. 
We are not going to list all the differences in 
individual syntactic labels - they can be found 
easily by confronting Tables 2 and 4, but we 
would like to draw the readers? attention to the 
main dissimilarities between the CAC?s and 
the PDT?s syntactic annotation scenarios. 
 
Punctuation 
The first difference can be observed at first 
glance: in CAC no punctuation marks can be 
found (as mentioned in Section 3). While some 
might question whether punctuation should 
ever be part of syntax, in computational ap-
proaches punctuation is certainly seen as a 
very important part of written-language syntax 
and is thus taken into account in annotation 
(for important considerations about punctua-
tion in spoken corpora, see Section 5). 
Digits 
CAC leaves out digital tokens, even though 
they are often a valid part of the syntactic 
structure and can plausibly get various syntac-
tic labels as we can see in the PDT annotation, 
where nothing is left out of the syntactic tree 
structure. 
Prepositions and function words 
The next most significant difference is in the 
treatment of prepositions (or function words in 
general, see also the next paragraphs on con-
junctions and other auxiliaries). Whereas CAC 
neither labels them nor even includes them in 
the dependency tree, PDT at the a-layer, re-
flecting the surface shape of the sentence, 
makes them the head of the autosemantic 
nodes they ?govern? (and labels them with the 
AuxP analytical function tag). The CAC way 
of annotation (rather, non-annotation) of 
prepositions is, in a sense, closer to the annota-
tion scenario of the underlying syntactic layer 
(the t-layer) of the PDT, It is also reflected in 
the adverbial types of labels (column 2 in Ta-
ble 2) ? these would all be labeled only as Adv 
at the (surface-syntactic) a-layer of the PDT, 
but at the (deep) t-layer, they get a label from a 
mix of approx. 70 functional, syntactic and 
semantic labels. Unfortunately, only seven 
such labels are used in the CAC, resulting in 
loss of information in some cases (adverbials 
of aim, accompaniment, attitude, beneficiary, 
etc.); the same is true for certain subtypes of 
time and location adverbials, since they are not 
distinguished in terms of direction, location 
designation (on/under/above/next to and many 
other), duration, start time vs. end time, etc. 
Conjunctions 
Further, subordinating as well as coordinat-
ing conjunctions get only a sentential syntactic 
tag in the CAC (if any), i.e. they are labeled by 
93
 the 9-position tag but not by the word-level, 
intra-clausal syntactic tag. In PDT, subordinat-
ing and coordinating conjunctions get assigned 
the analytical function value AuxC and Co-
ord, respectively, and they are always in-
cluded in the syntactic tree. For subordinating 
conjunctions, the CAC approach is again in 
some ways similar to the annotation scenario 
of the tectogrammatical layer of PDT ? de-
pendencies between clauses are annotated but 
the set of labels is much smaller than that of t- 
layer of the PDT, again resulting in a loss of 
information. For coordination and apposition, 
the difference is structural; while CAC marks 
an a coordination element with a specific label 
(value ?1? in the column 6 of a word-level tag 
and the same value in column 8 of the clause-
level tag, see Tables 2 and 3), PDT makes a 
node corresponding to the coordination (appo-
sition) a virtual head of the members of the 
coordination or apposition (whether phrasal or 
clausal). CAC thus cannot annotate hierarchy 
in coordination and apposition without another 
loss of information, while PDT can. 
Reflexive particles 
In CAC, reflexive particles se/si are often left 
unannotated, while PDT uses detailed labels 
for all occurrences. Lexicalized reflexives 
(AuxT in the PDT), particles (AuxO) and re-
flexive passivization (AuxR) and also certain 
(yet rare) adverbial usages (Adv) are not anno-
tated in the CAC at all. The only case where 
CAC annotates them is in situations where 
they can be considered objects (accusative or 
dative case of the personless reflexive pronoun 
sebe). 
Analytic verb forms 
In CAC, no syntactic relation is indicated for 
auxiliary verbs, loosing the reference to the 
verb they belong to; in the PDT, they are put as 
dependents onto their verb, and labeled AuxV 
to describe their function. 
Special adverbs and particles 
In PDT, there are also syntactic labels for 
certain type of ?special? adverbials and parti-
cles, such as rad?ji [better], z?ejm? [probably], 
tak? [also], p?ece [surely], jedin? [only]. In 
CAC, dependencies and syntactic tags for 
these tokens are missing. 
Other differences in both syntactic scenarios 
will be described in the next section since they 
are related to spoken language annotation.  
5 CAC syntactic annotation of spoken 
utterances  
Current Czech syntactic theory is based al-
most entirely on written Czech but spoken lan-
guage often differs strikingly from the written 
one (M?llerov?, 1994). 
In the CAC guidelines, only the following 
word-level markup specifically aimed at the 
spoken utterance structure is described: 
? non-identical reduplication of a word 
(value ?7? in column 6), 
? identical reduplication of a word (value 
?8? in column 6), 
? ellipsis (value ?9? or ?0? in column 6). 
Let?s take this spoken utterance from CAC: 
CZ: A to jsou tro?ku, jedna je, jedna m? sv?tlou 
budovu a druh? m? tmavou budovu, ony jsou um?s-
t?ny v jednom, v jednom are?le, ale ta, to centrum, 
pat?ilo t?, bylo to v bloku Univerzity vl?msk?, a j? 
jsem se ptala na univerzit?, na, v Univerzit? svo-
bodn?, ?e, no a to p?eci oni nev?d?, to nanejv??, to 
prost? jedin?, kdy? je to Univerzita vl?msk?, tak o 
tom oni p?ece nemohou nic v?d?t, a nic. 
(Lit.: And they are a bit, one is, one has a light 
building and the second has a dark building, they 
are placed in one, in one campus, but the, the cen-
ter, it belonged to the, it was in a bloc of the Flem-
ish University, and I asked  at the University, in, at 
the Free University, that, well, and that surely they 
don?t know, it at most, it simply only, if it is the 
Flemish University, so they surely cannot know 
anything, and nothing.) 
Words jsou [are] and ta [the] represent a non-
identical reduplication of a word; that is why 
they have been assigned the value ?7? (as de-
scribed above), while je [is], jednom [one], to 
[the] and nic [nothing] represent an identical 
reduplication of a word, i.e. they get the value 
?8? (?identical reduplication of a word?). The 
description does not quite correspond to what a 
closer look at the data reveals: ?7? is used to 
mark a reparandum (part of the sentence that 
was corrected by the speaker later), while ?8? is 
used to mark the part that replaces the reparan-
dum (cf. also the ?EDITED? nonterminal and 
the symbols ?[?, ?+? and ?\? in the Penn Tree-
bank Switchboard annotation (Godfrey et al, 
1992). Ellipsis (the value ?9?) was assigned to 
the words tro?ku [a bit] and t? [to the].  
94
 However, our sample sentence contains 
more phenomena typical for spoken language 
than CAC attempts to annotate, for example:  
- unfinished sentences (fragments), with 
apparent ellipsis: A to je tro?ku? [And 
they are a bit?], 
-  false beginnings (restarts): jedna je, 
jedna m? [one is, one has], 
- repetition of words in the middle 
of sentence:  jsou um?st?ny v jednom, 
jednom are?le [they are placed in one, in 
one campus], 
- redundant and ungrammatically used 
words: ony jsou um?st?ny v jednom?, 
univerzit?, na,v Univerzit? svobodn?,? 
[, they are placed in one? at the Univer-
sity, in, at the Free University, ], 
- redundant deictic words: ?ale ta, to cen-
trum? [?but the, the center?], 
- intonation fillers:  no [well], 
- question tags: na Univerzit? svobodn?, 
?e [at the Free University, that], 
- redundant conectors: kdy? je to Uni-
verzita vl?msk?, tak to o tom [if it is the 
Flemish University, so they surely can-
not know anything], 
- broken coherence of utterance, ?teared? 
syntactic scheme of proposition: ale ta, 
to centrum, bylo to v bloku [but the, the 
center, it belonged to the, it was in a 
bloc], 
- syntactic errors, anacoluthon:  p?eci 
nemohu nic v?d?t, a nic. [surely (I) can-
not know anything, and nothing]. 
The CAC syntactic scenario does not cover 
these phenomena in the guidelines (and tag 
tables), and even if some of them would easily 
fall in the reparandum/repair category (such as 
the phrase jedna je, jedna m? [one is, one 
has]), which is seemingly included, it does not 
annotate them as such. Moreover, these are just 
some of the spoken language phenomena, 
taken from just one random utterance; a thor-
ough look at the spoken part of the CAC re-
veals that most of the well-known spoken lan-
guage phenomena, e.g. grammatically incoher-
ent utterances, grammatical additions spoken 
as an afterthought, redundant co-references or 
phrase-connecting errors (Shriver, 1994, Fitz-
gerald, 2009), are present in the texts but left 
unnoticed.  
In comparison, however, the PDT covers 
none of these typical spoken structures in the 
text annotation guidelines (the main reason 
being that it does not contain spoken material 
in the first place). Thus, at the surface-
syntactic layer (the a-layer) of the PDT, there 
are only limited means for capturing such spo-
ken phenomena.  
For example, words playing the role of fill-
ers could get the analytical function AuxO de-
signed mostly for a redundant (deictic or emo-
tive) constituent.  
Many phenomena typical for spoken lan-
guage would get, according to the PDT guide-
lines, the analytical function ExD (Ex-
Dependent), which just ?warns? of such type 
of incomplete utterance structure where a gov-
erning word is missing, i.e. it is such ellipsis 
where the dependent is present but its govern-
ing element is not. 
In Figure 2, we present an attempt to anno-
tate the above spoken utterance using the stan-
dard PDT guidelines. The ?problematic? 
nodes, for which we had to adopt some arbi-
trary annotation decisions due to the lack of 
proper means in the PDT annotation guide-
lines, are shown as dark squares. For compari-
son, we have used dashed line for those de-
pendency edges that were annotated in the 
CAC by one of the spoken-language specific 
tags (values ?7?, ?8?, ?9? in the column 6 of the 
original annotation, see above at the beginning 
of Sect. 5), 
Most of the square-marked nodes do corre-
spond well to the PDT labels for special cases 
which are used for some of the peripheral lan-
guage phenomena (ExD, Apos and its mem-
bers, several AuxX for extra commas, AuxY 
for particles etc.).  
It can also be observed that the dashed lines 
(CAC spoken annotation labels) correspond to 
some of the nodes with problematic markup in 
the PDT, but they are used only in clear cases 
and therefore they are found much more spar-
ingly in the corpus. 
6 Conclusion 
Courage of the original CAC project?s team 
deserves to be reminded. Having the experi-
ence with the present spoken data processing, 
we do appreciate the initial attempts with the 
syntactic annotation of spoken texts. 
95
 Given the main principles of the a-layer of 
PDT annotation (no addition/deletion of to-
kens, no word-order changes, no word correc-
tions), one would have to introduce arbitrary, 
linguistically irrelevant rules for spoken mate-
rial annotation with a doubtful use even if ap-
plied consistently to the corpus. Avoiding that, 
transcriptions currently present in the CAC 
could not be syntactically annotated using 
the annotation guidelines of the PDT. 
However, in the future, we plan to complete 
the annotation of the spoken language tran-
scriptions, using the scheme of the so-called 
?speech reconstruction? project (Mikulov? et 
al., 2008), running now within the framework 
of the PDT (for both Czech and English)5. This 
project will enable to use the text-based guide-
lines for syntactic annotation of spoken mate-
rial by introducing a separate layer 
of annotation, which allows for ?editing? of the 
original transcript and transforming it thus into 
a grammatical, comprehensible text. The ?ed-
ited? layer is in addition to the original tran-
script and contains explicit links between them 
at the word granularity, allowing in turn for 
observations of the relation between the origi-
nal transcript and its syntactic annotation 
(made ?through? the edited text) without any 
loss. The scheme picks up the threads of the 
speech reconstruction approach developed for 
English by Erin Fitzgerald (Fitzgerald, Jelinek, 
2008). Just for a comparison see our sample 
sentence (analyzed in Sect. 5) transformed into 
a reconstructed sentence (The bold marking 
means changes, and parentheses indicate ele-
ments left out in the reconstructed sentence.).  
CZ: A (to) jsou tro?ku rozd?ln?,(jedna je,) jedna 
m? sv?tlou budovu a druh? m? tmavou budovu.(, 
ony) Jsou um?st?ny (v jednom,) v jednom are?le, 
ale (ta,) to centrum (, pat?ilo t?,) bylo (to) v bloku 
Univerzity vl?msk?(,) a j? jsem se ptala na (univer-
zit?, na, v) Univerzit? svobodn?.(, ?e, no a to p?eci 
oni nev?d?, to nanejv??, to prost? jedin?,) Kdy? je 
to Univerzita vl?msk?, tak o tom oni p?ece nemo-
hou nic v?d?t (, a nic). 
(Lit.: And they are a bit different, one has a light 
building and the second has a dark building. They 
are placed in one campus, but the center (, it be-
longed to the, it) was in a bloc of the Flemish Uni-
versity, and I asked at the (University, in, at the) 
Free University.(, that, well, and that surely they 
don?t know, it at most, it simply only,) If it is the 
Flemish University, so they surely cannot know 
anything(, and nothing).) 
                                                 
5
 http://ufal.mff.cuni.cz/pdtsl 
 
Figure 2. A syntactic annotation attempt 
(PDT-guidelines based) at the sample CAC 
sentence. The dashed edges are the only ones 
containing some spoken-language specific 
CAC annotation, the others correspond as 
close as possible to the PDT annotation sce-
nario. Square-shaped nodes mark the problem-
atic parts (phenomena with no explicit support 
in the PDT guidelines). 
96
  
 
Governor Dependency 
relation 
Dependency 
subtypes Direction Offset 
Other 
1 2 3 4 5 6 
Tag Desc.  Tag Desc.  Tag Desc. 
1 Subject + Right 1-6 Coordination 
types 
2 Predicate - Left 
 
7,8 Repetitions  
(for the 
spoken part) 
3 Attribute   9, 0 Ellipses 
4 Object     
5 Adverbial     
6 Clause core     
7 Trans. type     
8 Independent 
clause member 
    
9 Parenthesis 
 
Values 
specific 
to the 
dependency 
relation 
(see  
column 1) 
  
 
Distance between 
words (two digit 
string: for ex. 01 
denotes 
neighboring 
word) 
  
Table 2 Main word-level syntactic tags in the Czech Academic Corpus 
 
Governing clause/word Clause ID Clause Type Subordination 
(dep.) type Gov. noun Gov. 
clause 
Clausal relation 
1 2 3 4 5 6 7 8 
 Tag Desc. Tag Desc.   Tag Desc. 
1 
 
Simple    1 Coordination 
2 Main   2 Parenthesis 
1 Subject 3 Direct Speech 
2 Predicate 5 Parenthesis in 
direct speech 
3 Attribute 6 Introductory 
clause 
4 Object 8 Parenthesis, in-
troductory clause 
5 Local ! Structural error 
Two-digit id 
(unique 
within a 
sentence: for 
ex. 91 de-
notes the  
first sentence 
3 Sub-
ordinated 
... ?. 
One-digit 
relative posi-
tion of a noun 
modified by 
the clause 
Attributive 
clauses only 
Two-digit 
id of the 
governing 
clause 
... etc. 
Table 3 Clause-level syntactic tags in the Czech Academic Corpus 
 
Analytic function Description 
Pred Predicate  
Sb Subject 
Obj Object 
Adv Adverbial 
Atr Attribute 
Pnom Nominal predicate, or nom. part of predicate with copula to be 
AuxV Auxiliary verb to be 
Coord Coordination node 
Apos Apposition (main node) 
AuxT Reflexive tantum 
AuxR Reflexive,neither Obj nor AuxT (passive reflexive) 
AuxP Primary preposition, parts of a secondary preposition 
AuxC Conjunction (subordinate) 
AuxO Redundant or emotional item, ?coreferential? pronoun 
ExD A technical value for a node depending on a deleted item (ellipsis with dependents) 
Aux.., Atv(V),.. Other auxiliary tags, verbal complements, other special syntactic tags 
Table 4 Dependency relation tags in the Prague Dependency Treebank 
 
97
 Acknowledgement 
We gratefully acknowledge the support of 
the Czech Ministry of Education through the 
grant No. MSM-0021620838 and ME 838 and 
the Grant Agency of Charles University in 
Prague through the grant No. GAUK 
52408/2008. 
We wish to thank Jan Haji?, whose com-
ments stimulated us to make our paper better. 
We are grateful to Petr Pajas for Figure 2 pre-
senting a wide dependency tree. 
References  
Christopher Cieri, David Miller, Kevin Walker. 
2004. The Fisher Corpus: a Resource for the 
Next Generations of Speech-to-Text. In Pro-
ceedings of the 4th LREC, Lisbon, Portugal, pp. 
69-71. 
John J. Godfrey, Edward C. Holliman, Jane 
McDaniel. 1992. SWITCHBOARD: Telephone 
speech corpus for research and development, 
IEEE ICASSP, pp. 517-520.  
Erin Fitzgerald. 2009. Reconstructing spontaneous 
speech. PhD thesis, Baltimore, Maryland. 
Erin Fitzgerald, Frederick Jelinek. 2008. Linguistic 
resources for reconstructing spontaneous speech 
text. In LREC Proceedings, Marrakesh, Mo-
rocco, pp. 1?8. 
Sidney Greenbaum (ed.). 1996. Comparing English 
Worldwide: The International Corpus of English. 
Oxford: Clarendon Press. 
Jan Haji?, Barbora Hladk?. 1997. Tagging of inflec-
tive languages: a comparison. In Proceedings of 
ANLP'97, Washington, DC, pp. 136--143. 
Jan Haji? et al 2006. The Prague Dependency 
Treebank 2.0, (Linguistic Data Consortium, 
Philadelphia, PA, USA), Cat. No. LDC2006T01. 
Ji?? Hana, Daniel Zeman, Jan Haji?, Hana Hanov?, 
Barbora Hladk?, Emil Je??bek. 2005. Manual for 
Morphological Annotation. TR-2005-27, ?stav 
form?ln? a aplikovan? lingvistiky, MFF UK. 
Heleen Hoekstra, Michael Moortgat, Ineke Schuur-
man, Ton van der Wouden 2001. Syntactic An-
notation for the Spoken Dutch Corpus Project. In 
Daelemans, W.; Simaan, K.; Veenstra. J.; Zavrel, 
J. (eds.): Computational Linguistics in the Neth-
erlands 2000. Amsterdam/New York, Rodopi, 
pp. 73-87.  
Noam Chomsky. 1993. Lectures on Government 
and Binding: The Pisa Lectures. Holland: Foris 
Publications, 1981. Reprint. 7th Edition. Berlin 
and New York: Mouton de Gruyter. 
Jan Kr?l?k, Ludmila Uhl??ov?. 2007. The Czech 
Academic Corpus (CAC), its history and pres-
ence, In Journal of quantitative linguistics. 14 
(2-3): 265-285. 
Marie Mikulov?. 2008. Rekonstrukce standard-
izovan?ho textu z mluven? ?e?i v Pra?sk?m z?vis-
lostn?m korpusu mluven? ?e?tiny. Manu?l pro 
anot?tory. TR-2008-38, Institute of Formal and 
Applied Linguistics, MFF UK. 
Olga M?llerov?. 1994. Mluven? text a jeho syntak-
tick? v?stavba. Academia, Praha.  
Jens Nilsson, Johan Hall, Joakim Nivre. 2005. 
MAMBA meets TIGER: Reconstructing a Tree-
bank from Antiquity. In Proceedings of 
NODALIDA 2005 Special Session on Treebanks 
for Spoken and Discourse, Copenhagen Studies 
in Language 32, Joensuu, Finland, pp. 119-132 
Petr Pajas, Jan ?t?p?nek. 2005. A Generic XML-
based Format for Structured Linguistic Annota-
tion and its Application to the Prague Depend-
ency Treebank 2.0. TR-2005-29, Institute of 
Formal and Applied Linguistics, MFF UK.  
Kiril Ribarov, Alevtina B?mov?, Barbora Hladk?. 
2006. When a statistically oriented parser was 
more efficient than a linguist: A case of treebank 
conversion, In Prague Bulletin of Mathematical 
Linguistics, 1 (86):21-38.  
Petr Sgall, Eva Haji?ov?, Jarmila Panevov?. 1986. 
The meaning of the sentence in its semantic and 
pragmatic aspects, ed. by J. Mey. Reidel, 
Dordrecht; Academia, Praha.  
Elisabeth Shriberg. 1994. Preliminaries to a Theory 
of Speech Disfluencies. PhD thesis, University of 
California, Berkeley. 
Jan Svartvik and Randolph Quirk. 1980. A Corpus 
of English Conversation. Lund. 
Vladim?r ?milauer. 1972. Nauka o ?esk?m jazyku. 
Praha. 
Vladim?r ?milauer. 1969. Novo?esk? skladba. 
St?tn? pedagogick? nakladatelstv?. Praha. 
 
98
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 232?241,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Feature Engineering in the NLI Shared Task 2013:
Charles University Submission Report
Barbora Hladka?, Martin Holub and Vincent Kr??z?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Prague, Czech Republic
{hladka, holub,kriz}@ufal.mff.cuni.cz
Abstract
Our goal is to predict the first language (L1)
of English essays?s authors with the help of
the TOEFL11 corpus where L1, prompts (top-
ics) and proficiency levels are provided. Thus
we approach this task as a classification task
employing machine learning methods. Out
of key concepts of machine learning, we fo-
cus on feature engineering. We design fea-
tures across all the L1 languages not making
use of knowledge of prompt and proficiency
level. During system development, we experi-
mented with various techniques for feature fil-
tering and combination optimized with respect
to the notion of mutual information and infor-
mation gain. We trained four different SVM
models and combined them through majority
voting achieving accuracy 72.5%.
1 Introduction
Learner corpora are collections of texts written by
second language (L2) learners, e.g. English as L2
? ICLE (Granger et al, 2009), Lang-8 (Tajiri et al,
2012), Cambridge Learner Corpus,1 German as L2
? FALKO (Reznicek et al, 2012), Czech as L2 ?
CzeSL (Hana et al, 2010). They are a valuable
resource for second language acquisition research,
identifying typical difficulties of learners of a cer-
tain proficiency level (e.g. low/medium/high) or
learners of a certain native language (L1 learners of
L2). Research on the learner corpora does not con-
centrate on text collections only. Studying the er-
rors in learner language is undertaken in the form
1http://www.cambridge.org/gb/elt
of error annotation like in the projects (Hana et al,
2012), (Boyd et al, 2012), (Rozovskaya and Roth,
2010), (Tetreault and Chodorow, 2008). Once the
errors and other relevant data are recognized in the
learner corpora, automatic procedures for e.g. error
correction, author profiling, native language identi-
fication etc. can be designed.
Our attention is focused on the task of automatic
Native Language Identification (NLI), namely with
English as L2.
In this report, we summarize the involment of the
Charles University team in the first shared task in
NLI co-located with the 8th Workshop on Innova-
tive Use of NLP for Building Educational Appli-
cations in June 2013 in Atlanta, USA. The report
is organized as follows: we briefly review related
works in Section 2. The data sets to experiment with
are characterized in Section 3. Section 4 lists the
main concepts we pursue during the system devel-
opment. Our approach is entirely focused on feature
engineering and thus Section 5 is the most impor-
tant one. We present there our main motivation for
making such a decision, describe patterns according
to which the features are generated and techniques
that manipulate the features. We revise our ideas ex-
perimentally as documented in Section 6. In total,
we submitted five systems to the sub-task of closed-
training. In Sections 7 and 8, we describe these sys-
tems and discuss their results in detail. We summa-
rize our two month effort in the shared task in Sec-
tion 9.
232
2 Related work
We understand the task of native language identifica-
tion as a subtask of natural language processing and
we consider it as still a young task since the very
first attempt to address it occurred eight years ago in
2005, as evident from the literature, namely (Koppel
et al, 2005b), (Koppel et al, 2005a).
We appreciate all the previous work concerned
with the given topic but we focus on the latest three
papers only, all of them published at the 24th In-
ternational Conference on Computational Linguis-
tics held in December 2012 in Bombay, India,
namely (Brooke and Hirst, 2012), (Bykh and Meur-
ers, 2012), and (Tetreault et al, 2012). They provide
a comprehensive review of everything done since the
very first attempts. We do not want to replicate their
chapters. Rather, we summarize them from the as-
pects we consider the most important ones in any
machine learning system, namely the data, the fea-
ture design, the feature manipulation, and the ma-
chine learning methods - see Table 1.
3 Data sets
A new publicly available corpus of non-native En-
glish writing called TOEFL112 consists of essays on
eight different topics written by non-native speakers
of three proficiency levels (low/medium/high); the
essays? authors have 11 different native languages.
The corpus contains 1,100 essays per language with
an average of 348 word tokens per essay. A corpus
description and motivation to build such corpus can
be found in (Blanchard et al, 2013).
The texts from TOEFL11 were released for the
purpose of the shared task as three subsets, namely
Train for training, DevTest for testing while sys-
tem development, and EvalTest for final testing.
The texts were already tokenized and we processed
them with the Standford POS tagger (Toutanova et
al., 2003).
4 System settings
1. Task: Having a collection of English essays
written by non-native speakers, the goal is to
predict a native language of the essays? authors.
2Source: Derived from data provided by ETS. Copyright c?
2013 ETS. www.ets.org.
Languages L1 are known in advance. Since we
have a collection of English essays for which
L1 is known (TOEFL11) at our disposal, we
formulate this task as a classification task ad-
dressed by using supervised machine learning
methods.
2. Feature set: A setA = {A1, A2, ..., Am} ofm
features where m changes as we perform var-
ious feature combinations and filtering steps.
We prefer to work with binary features. We
do not include two extra features, proficiency
level and prompt, provided with the data. In
addition, we design features across all 11 lan-
guages, i.e. we do not design features sepa-
rately for a particular L1. Doing so, we ad-
dress the task of predicting L1 from the text
only, without any additional knowledge.
3. Input data: A set X of instances being texts
from TOEFL11 corpus represented as feature
vectors, x = ?x1, x2, ..., xm? ? X,xi ? Ai.
4. Output classes: A set C of L1 languages, C
= {ARA, CHIN, FRE, GER, HIN, ITA, JPN,
KOR, SPA, TEL, TUR}, |C| = 11.
5. True prediction: A set D = {< x, y >:
x ? X , y ? C}, |D| = 12, 100 and its pairwise
disjoint subsets Train, DevTest, EvalTest
where Train ? DevTest ? EvalTest = D,
|Train| = 9, 900, |DevTest| = 1, 100,
|EvalTest| = 1, 100.
6. Training data: Train ? DevTest. No other
type of training data is used.
7. Learning mechanism: Since we focus on fea-
ture engineering, we do not study appropriate-
ness of particular machine learning methods to
our task in details. Instead, reviewing the re-
lated works, we selected the Support Vector
Machine algorithm to experiment with.
8. Evaluation: 10-fold cross-validation with the
sample Train ? DevTest. Accuracy, Pre-
cision, Recall. Proficiency-based evaluation.
Topic-based evaluation.
233
PAPER DATA FEATURE FEATURE ML
DESIGN MANIPULATION METHOD
[1] Lang-8,
ICLE,
Cambridge
Learner
Corpus
function words, charac-
ter n-grams, POS n-grams,
POS/function n-grams, context-
free-grammar productions,
dependencies, word n-grams
frequency-based
feature selection
SVM, MaxEnt
[2] ICLE binary features spanning word-
based recurring n-grams, func-
tion words, recurring POS based
n-grams and combination of
them
no special feature
treatment
logistic regression
[3] ICLE,
TOEFL11
character n-grams, function
words, POS, spelling errors,
writing quality
no special feature
treatment
logistic regression
Table 1: A summary of latest related works [1](Brooke and Hirst, 2012), [2](Bykh and Meurers, 2012), [3](Tetreault
at al., 2012)
5 Feature engineering
We split the process of feature engineering into two
mutually interlinked steps. The first step aims at an
understanding of the task projected into features de-
scribing properties of entities we experiment with.
These experiments represent the second step where
we find out how the features interact with each other
and how they interact with a chosen machine learn-
ing algorithm.
We compose a feature family as a group of pat-
terns that are relevant for a particular task. The fea-
tures are then extracted from the data according to
them. Since we experiment with English texts writ-
ten by non-native speakers, we have to search for
specific and identifiable text properties, i.e. tenden-
cies of certain first language writers, based on the
errors caused by the difference between L1 and L2.
In addition, we look for phenomena that are not nec-
essarily incorrect in written English but they provide
clear evidence of characteristics typical for L1. Our
feature family is built from chunks of various length
in the texts, formally lexically and part-of-speech
based n-grams. In total, the feature family contains
eight patterns described in Table 2 - six for binary
features l,n,p,s1,s2,sp and two for continuous fea-
tures a,r. Outside the feature family, its patterns can
be combined into joint patterns, like l+sp, n+sp+r.
Considering the key issues of machine learning,
Figure 1: Feature engineering
we mainly pay attention to overfitting. We are aware
of many aspects that may cause overfitting, like
complexity of the model trained, noise in training
data, a small amount of training data. Features can
lead to overfitting as well, thus we address it us-
ing elaborated feature engineering visualised in Fig-
ure 1. We can see there the data components and the
process components having the features in common.
The scheme can be traced either with individual pat-
terns from the feature family or with joint patterns.
Both basic feature filtering and advanced feature
manipulation apply selected concepts from informa-
234
FEATURE DESCRIPTION EXAMPLES
FAMILY n=1,2,3
PATTERN
l n-grams of lemmas picture; to see; you, be, not
n n-grams of words picture; to see; you, are, not
p n-grams of function words and POS tags of content
words, i.e. nouns, verbs, adjectives, cardinal num-
bers
not; PRP; you, VBP; JJ, to, VB
s1 skipgrams of words: bigram wi?2, wi and trigrams
wi?3, wi?1, wi, wi?3, wi?2, wi extracted from a se-
quence of words wi?3 wi?2 wi?1 wi
you,not; able, see; to, see,in; to
things, in
s2 skipgrams of words: bigrams wi?3, wi, wi?4, wi
and trigrams wi?4, wi?3, wi, wi?4, wi?2, wi,
wi?4, wi?1, wi extracted from a sequence of words
wi?4 wi?3 wi?2 wi?1 wi
are,see; you,see; you,are,see;
you,able,see; you,to,see;
sp n-grams of function words and shrunken POS tags
of content words: POS tags N* are shrunken into a
tag N, V* into V, J* into J
not; PRP; you V; J to V
a relative frequency of POS tags and function words
r relative frequency of POS tags
Table 2: A feature family. Examples are taken from the file 498.txt, namely from the sentence You are not able to
see things in a big picture. tagged as follows: (You/you/PRP are/be/VBP not/not/RB able/able/JJ to/to/TO see/see/VB
things/thing/NNS in/in/IN a/a/DT big/big/JJ picture/picture/NN ././.)
tion theory.
5.1 Concepts from information theory
Consider a random variable A having two possible
values 0 and 1 where the probability of 1 is p and
0 is 1 ? p. A degree of uncertainty we deal with
when predicting the value of the variable depends
on p. If p is close to zero or one, then we are almost
confident about the value and our uncertainty is low.
If the values are equally likely (i.e. p = 0.5), our
uncertainty is maximal.
The entropy H(A) measures the uncertainty. In
other words, it quantifies the amount of information
needed to predict the value of the variable. The for-
mula 1 for the entropy treats variables with N ? 1
possible values.
H(A) = ?
N?
i=1
p(A = ai) log2 p(A = ai) (1)
The conditional entropy H(A|B) quantifies the
amount of information needed to predict the value
of the random variable A given that the value of an-
other random variable B is known, see Formula 2.
Then H(A|B) ? H(A) holds.
H(A|B) =
?
b?B
p(B = b)H(A|B = b) (2)
The amount H(A) ? H(A|B) by which H(A)
decreases reflects additional information about A
provided by B and is called mutual information
I(A;B) - see Formula 3. In other words, I(A;B)
quantifies the mutual dependence of two random
variables A and B.
I(A;B) = H(A)?H(A|B) (3)
Proceeding from statistics to machine learning,
independent random variables correspond to fea-
tures. Thus we can directly speak about the entropy
of a feature, the conditional entropy of a feature
given another feature and the mutual information of
two features.
235
Information gain of feature Ak - IG(Ak) - mea-
sures the expected reduction in entropy caused by
partitioning the data set Data according to the val-
ues of the feature Ak (Quinlan, 1987):
IG(Ak) = H(Data)?
c?
i=j
|Dvj |
|Data|
H(Dvj ), (4)
where Avk = {v1, v2, ..., vc} is a set of possible val-
ues of feature Ak and Dvi is a subset of Data con-
tainig instances with the feature value xk = vj .
C being a target feature, H(Data) = H(C).
Thus the mutual information between C and Ak -
I(C;Ak) - is the information gain of the feature Ak,
i.e.
I(C;Ak) = IG(Ak). (5)
All mentioned concepts are visualized in Figure 2
for our settings:
? Our target feature C has eleven possible val-
ues (i.e. L1 languages). These values are
uniformly distributed in the data D, thus
H(C) = ?
?11
i=1
1
11 log2
1
11 = log2 11
.=
3.46. Sample features (only for illustration)
A1, A2, A3, A4 ? A are binary features so
H(Ai) ? 1 < H(C) = 3.46, i = 1, ..., 4.
The circle areas correspond to the entropy of
features.
? The black areas correspond to mutual informa-
tion I(Ai;Ak).
? The striped areas correspond to the mutual in-
formation I(C;Ak) between C and Ak.
? Features A1 and A3 are independent, so
I(A1;A3) = 0.
? A2 has the highest mutual dependence with C,
? H(A2) = H(A3) and IG(A2) > IG(A3)
In addition to the concepts from information the-
ory, we introduce another measure to quantify fea-
tures: the document frequency of feature Ak ?
df(Ak) is the number of texts in which Ak occurs,
i.e. df(Ak) ? 0.
Figure 2: Information gain and mutual information visu-
alization
5.2 Discussion on features
We impose a fundamental requirement on features:
they should be both informative (i.e. useful for the
classification task) and robust (i.e. not sensitive to
training data). We control the criterion of being in-
formative by information gain maximization. The
criterion of being robust is quantified by document
frequency. If df(Ak) is high enough, then we can
expect that Ak will occur in test data frequently. We
propose two techniques to increase df : (i) filtering
out features with low df ; (ii) feature combination
driven by IG.
The fulfillment of both criteria is always depen-
dent on training data, i.e. the final feature set tends
to fit training data and our goal is to weaken this ten-
dency in order to get a more robust feature set. Both
basic feature filtering and advanced feature combi-
nation help us to address this issue.
5.3 Basic feature filtering
We obtained the feature setA0 by extracting features
according to the feature family patterns) from the
training data. Basic feature filtering removes fea-
tures from A0 in two steps that result in a primary
feature set A1:
1. Remove binary feature Ak if df(Ak) < ?df .
Remove continous feature Ak if
relative frequency(Ak) < ?rf or
df(relative frequency(Ak) ? ?rf ) < ?df .
2. Remove binary feature Ak if IG(Ak) ? ?IG.
236
5.4 Advanced feature manipulation
The process of advanced feature manipulation han-
dles m input features from the primary feature set
A1 in two different ways, filter them and combine
them, in order to generate a final feature set Af
ready to train the model:
? Filter them. We use Fast Correlation-Based
Filter (FCBF; (Fleuret, 2004), (Yu and Liu,
2003)) that addresses the correlation between
features. It first ranks the features accord-
ing to their information gain, i.e. IG(A1) ?
IG(A2) ? ... ? IG(Am). In the second step,
it iteratively removes any featureAk if there ex-
ists a feature Aj such that IG(Aj) ? IG(Ak)
and I(Ak;Aj) ? IG(Ak), i.e. Aj is bet-
ter as a predictor of C and Ak is more sim-
ilar to Aj than to C. In the situation visu-
alized in Figure 2, the feature A4 will be fil-
tered out because there is a featureA3 such that
IG(A3) ? IG(A4) and I(A3;A4) ? IG(A4)
? Combine them. We combine (COMB) binary
features using logical operations (AND, OR,
XOR, AND NOT, etc.) getting a new binary
feature.
For example, if we combine two features A1
and A2 using the OR operator, we get a new
binary feature Y = A1 OR A2 for which the
inequalities df(Y ) > df(A1) and df(Y ) >
df(A2) hold. Thus we get a feature that is
more robust than the two input features. To
know whether it is more informative, we need
to know how high IG(Y ) is with respect to
IG(A1) and IG(A2). Without loss of gen-
erality, assume that IG(A1) > IG(A2). If
IG(Y ) > IG(A1) > IG(A2), then Y is more
informative than A1 and A2, but both of these
features could be informative enough as well.
It depends on the threshold we set up for being
informative. We can easily iterate this process -
let Y1 = A1 ORA2 and Y2 = A3 ORA4. Then
we can combine Y3 = Y1 OR A5 or Y4 = Y1
OR Y2, etc.
Then, advanced feature manupilation runs ac-
cording to scenarios formed as a series of FCBF
and COMB, for example A1 ? FCBF ? COMB
? FCBF? Af or A1 ? COMB? FCBF? Af .
6 System development
During system development, we formulated hy-
potheses how to avoid overfitting and get features ro-
bust and informative enough. In parallel, we run the
experiments with parameters using which we con-
trolled this requirement.
Basic feature filtering We set the thresholds ?df ,
?IG, ?rf empirically to the values 4, 0 and 0.02, re-
spectively. Table 3 shows the changes in the size of
the initial feature set after the basic feature filtering.
It is evident that even such trivial filtering reduces
the number of features substantially.
FEATURE INITIAL AFTER AFTER
FAMILY FEATURE df IG
PATTERN SET FILTERING FILTERING
(i.e. |A0|) (i.e. |A1|)
l 2,078,105 156,722 2,827
n 2,411,516 163,939 2,840
p 1,116,986 161,681 2,467
s1 4,794,702 242,969 1,877
s2 7,632,011 382,881 4,566
sp 781,018 123,431 933
a 181 111 111
r 48 48 48
Table 3: Volumes of initial feature sets extracted from
Train ? DevTest (1st column). Volumes of primary
feature sets after basic filtering of A0 (3rd column)
.
Learning mechanisms Originally, we started
with two learning algorithms, Random Forests (RF)
and Support Vector Machines (SVM), running them
in the R system.3
The Random forests4 algorithm joins random-
ness with classification decision trees. They iterate
the process of two random selections and training a
decision tree k-times on a subset ofm features. Each
of them classifies a new input instance x and the
class with the most votes becomes the output class
of x.
Support Vector Machines (Vapnik, 1995) effi-
ciently perform both linear and non-linear classi-
fication employing different Kernel functions and
3http://www.r-project.org
4http://www.stat.berkeley.edu/?breiman/
237
avoiding the overfitting by two parameters, cost and
gamma.
We run a number of initial experiments with the
following settings: the feature family pattern n; the
basic feature filtering, RF with different values of
parameters k and m, SVM with different values of
parameters kernel, gamma and cost
Cross-validation on the data set Train performed
with SVM showed significantly better results than
those obtained with RF. We were quite suprised that
RF ran with low performance so that we decided
to stop experimenting with this algorithm. Step by
step, we added patterns into the feature family and
carried out experiments with SVM only on the data
set Train ? DevTest. We fixed the values of the
SVM parameters kernel, degree, gamma, cost after
several experiments as follows kernel = polynomial,
degree = 1, gamma = 0.0004, cost = 1. Then we
included the advanced feature manipulation into the
experiments according to the scenariosA1 ? FCBF
? COMB ? FCBF ? Af and A1 ? COMB ?
FCBF? Af . COMB was composed using the OR
operator only. Unfortunately, none of them outper-
formed the initial experiments with the basic filter-
ing only.
Table 4 contains candidates for the final submis-
sion. The highlighted candidates were finally se-
lected for the submission.
FEATURE CROSS-VALIDATION Acc (%)
PATTERNS on Train on DevTest
l + a 72.97 ? 0.76 71.09
n + a 72.45 ? 0.98 63.00
l + sp + a 72.00 ? 0.72 70.64
l+sp 71.09 ? 0.72 71.45
n+sp 70.38 ? 0.69 52.27
l 71.67 ? 0.57 70.18
n 71.27 ? 0.84 68.72
l+p 71.17 ? 2.41 71.27
n+s1 69.90 ? 1.04 66.72
n+s2 68.75 ? 1.50 67.63
n+s1+s2 67.97 ? 0.96 66.81
Table 4: Candidates for the final submission. Candidates
in bold were submitted.
MODEL FEATURE FAMILY Acc
PATTERN (%)
CUNI-closed-1 majority voting
of CUNI-closed-[2-5] 72.5
CUNI-closed-2 l+a 71.6
CUNI-closed-3 l+p 71.6
CUNI-closed-5 l+sp+a 71.1
CUNI-closed-4 l+sp 69.7
Table 5: An overview of models submitted.
MODEL Acc (%)
CUNI-closed-1 74.2
CUNI-closed-2 73.4
CUNI-closed-3 73.9
CUNI-closed-4 73.1
CUNI-closed-5 72.9
Table 6: Cross-validation results for all submitted CUNI-
closed systems.
7 Submission to the shared task
In total, we submitted five systems to the closed-
training sub-task - see their overview in Table 5. The
results correspond to our expectations that we made
based on the results of cross-validation presented in
Table 4. The best system, CUNI-closed-1, was the
outcome of majority voting of the remaining four
systems. The performance of this system per lan-
guage is presented in Table 7.
Table 6 reports accuracy results when doing 10-
fold cross-validation on Train ? DevTest. The
folds for this experiment were provided by the or-
ganizers to get more reliable comparison of the NLI
systems.
It is interesting to analyse the complementarity of
the CUNI-closed-[2-5] systems that affects the per-
formance of CUNI-closed-1. In Table 8, we list the
numerical characteristics of five possible situations
that can occur when comparing the outputs of two
systems i and j. Situations 2 and 3 capture how
complementary the systems are. The numbers for
our systems are presented in Table 9.
We grouped languages according to the thresholds
of F-measure. First we did it across the data, no mat-
ter what the proficiency level and prompt are - see
the first row of Table 10. Second we did grouping
238
Acc(%) P(%) R(%) F(%)
ARA 72 67 72 69,6
CHI 78 71 78 74,3
FRE 73 74 73 73,7
GER 83 83 83 83,0
HIN 75 68 75 71,4
ITA 83 85 83 83,8
JPN 70 65 70 67,6
KOR 64 70 64 67,0
SPA 66 70 66 68,0
TEL 68 72 68 69,7
TUR 65 72 65 68,4
Table 7: CUNI-closed-1 on EvalTest: Acc, P, R, F
1. the number of instances both systems pre-
dicted correctly;
2. the number of instances both systems pre-
dicted incorrectly;
3. the number of instances the systems pre-
dicted differently: i system correctly and j
system incorrectly;
4. the number of instance the systems pre-
dicted differently: i system incorrectly and j
system correctly;
5. the number of instances the systems pre-
dicted differently and both incorrectly.
Table 8: Pair of two systems i and j and their predictions.
pair of CUNI-closed-i
and CUNI-closed-j systems
2-3 2-4 2-5 3-4 3-5 4-5
1 707 717 745 701 710 732
2 161 215 242 183 181 250
3 81 71 43 87 78 35
4 81 50 37 66 72 50
5 70 47 33 63 59 33
Table 9: CUNI-closed-[2-5]: complementary rates.
? 90% ? 80% ? 70% < 70%
overall GER,
ITA
CHI,
FRE,
HIN
TEL,
ARA,
TUR,
SPA,
JPN,
KOR
high GER,
ITA
CHI,
HIN,
FRE
KOR,
TUR,
SPA,
TEL,
ARA,
JPN
medium ITA,
GER,
FRE,
TEL
CHI,
ARA,
SPA,
TUR
JPN,
KOR,
HIN
low GER ITA,
FRE,
JPN
ARA KOR,
TEL,
HIN,
TUR,
SPA,
CHI,
FRE
Table 10: CUNI-closed-1 on EvalTest: Groups of lan-
guages sorted according to F-measure w.r.t. proficiency
level.
for a particular proficiency level - see the remaining
rows in Table 10. We can see that both GER and
ITA are languages with the highest F-measure on all
levels. Third we grouped by a particular prompt -
see Table 11. We can see there diversed numbers for
L1 languages despite the fact that prompts are for-
mulated generally. Even more, we observe a topic
similarity between prompts P2, P3, and P8, between
P4 and P5, and between P1 and P7.
8 Future plans
In our future research, w want to elaborate ideas that
concern the feature engineering. We plan to work
with the feature family that we designed in our ini-
tial experiments. However, we will think about more
specific patterns in the essays, like the average count
of tokens/punctuation/capitalized nouns/articles per
sentence. As Table 12 shows, there is only one can-
didate, namely the number of tokens in sentence, to
be taken into considerations since there is the largest
difference between minimum and maximum.
We confronted Ken Lackman,5 an English
teacher, with the task of manual native language
identification by English teachers. He says: ?I think
5http://kenlackman.com
239
? 90% ? 80% ? 70% < 70%
P1 GER,
ITA
FRE,
HIN,
ARA,
TEL
CHI,
KOR,
TUR
SPA,
JPN
P2 GER,
FRE,
ITA,
TEL
ARA,
HIN,
JPN
SPA,
KOR,
CHI
TUR
P3 GER CHI,
KOR
HIN,
ITA
FRE,
JPN,
TUR,
ARA,
SPA,
TEL
P4 ITA CHI,
TUR,
HIN,
FRE
TEL,
SPA,
GER,
JPN,
ARA,
KOR
P5 ITA TUR,
JPN,
GER
FRE,
TEL,
KOR
HIN,
CHI,
SPA,
ARA
P6 ITA,
CHI,
SPA
KOR,
ARA,
JPN
HIN,
FRE,
TEL,
GER,
TUR
P7 ITA,
CHI,
TUR
SPA,
GER,
HIN,
FRE
ARA,
JPN,
KOR,
TEL
P8 ARA GER,
TEL,
SPA,
ITA
FRE
HIN,
KOR,
JPN,
TUR,
CHI
Table 11: CUNI-closed-1 on EvalTest: Groups of lan-
guages sorted according to F-measure w.r.t. prompt.
AVG COUNT Train
PER MIN (L1) - MAX (L1)
SENTENCE
TOKEN 18 (JPN) -25.8 (SPA)
PUNCTUATION 1.5 (HIN, TEL) - 2.1 (SPA)
CAPITALIZED 0.1 (CHI) - 0.3 (HIN)
NOUN
the 0.6 (KOR) - 1.2 (ITA, SPA, TEL)
a/an 0.3 (JPN, KOR) - 0.7 (ITA, SPA)
Table 12: Data counts on Train.
it?s quite possible to do but you would need a set of
guidelines to supply teachers with. The guidelines
would list tendancies of certain first language writ-
ers, based on errors caused by difference between
L1 and L2. For example, Germans tend to capital-
ize too many nouns, since there are far more nouns
capitalized in their language, Asians tend to leave
out articles and Arab students tend to use the verb
?to be? inappropriately before other verbs.? Look-
ing into the data, we observe the phenomena Ken is
speaking about, but the quantity of them is not sta-
tistically significant to distinguish L1s.
We formulate an idea of a bootstrapped feature
extraction that has not been published yet, at least
to our knowledge. Let us assume a set of opera-
tions that can be performed over a feature set (so far,
we have proposed two possible operations with the
features, filtering them out and their combinations).
Determining whether a condition to perform a given
operation holds is done on the high number of ran-
dom samples. If the condition holds on the majority
of them, then the operation is performed. The only
parameter that must be set up is the majority. In-
stead of setting a threshold that is adjusted for all the
features, bootstrapped feature extraction deals with
fitting the data individually for each feature.
9 Conclusion
It was the very first experience for our team to ad-
dress the task of NLI. We assess it as very stimu-
lating and we understand our participation as setting
the baseline for applying other ideas. An overall ta-
ble of results (Tetreault et al, 2013) for all the teams
involved in the NLI 2013 Shared Task shows that
there is still space for improvement of our baseline.
We really appreciate all the work done by the or-
ganizers. They?ve made an effort to prepare the
high-quality data and set up the framework by which
the use of various NLI systems can be reliably com-
pared.
Acknowledgments
The authors would like to thank Eva Hajic?ova? and
Jirka Hana for their valuable comments. We also
thank Ken Lackman and Leslie Ryan6 for sharing
6http://lesliestreet.cz
240
their teaching experience. This research was sup-
ported by the Czech Science Foundation, grant no.
P103/12/G084 and the Technology Agency of the
Czech Republic, grant no. TA02010182.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Adriane Boyd, Marion Zepf, and Detmar Meurers. 2012.
Informing Determiner and Preposition Error Correc-
tion with Hierarchical Word Clustering. In Proceed-
ings of the 7th Workshop on Innovative Use of NLP
for Building Educational Applications (BEA7), pages
208?215, Montreal, Canada. Association for Compu-
tational Linguistics.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? Inves-
tigating Abstraction and Domain Dependence. In Pro-
ceedings of COLING 2012, pages 425?440, Mumbai,
India, December.
F. Fleuret. 2004. Fast Binary Feature Selection with
Conditional Mutual Information. Journal of Machine
Learning Research (JMLR), 5:1531?1555.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English v2 (Handbook + CD-ROM). Presses
universitaires de Louvain, Louvain-la-Neuve.
Jirka Hana, Alexandr Rosen, Svatava S?kodova?, and
Barbora S?tindlova?. 2010. Error-tagged Learner Cor-
pus of Czech. In Proceedings of the Fourth Lin-
guistic Annotation Workshop (LAW IV), pages 11?
19, Stroudsburg, USA. Association for Computational
Linguistics.
Jirka Hana, Alexandr Rosen, Barbora S?tindlova?, and
Petr Ja?ger. 2012. Building a learner corpus. In
Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC 2012),
I?stanbul, Turkey. European Language Resources As-
sociation.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proceedings of the 11th ACM
SIGKDD, pages 624?628, Chicago, IL. ACM.
John Ross Quinlan. 1987. Simplifying decision trees.
International Journal of ManMachine Studies, 27,
221-234.
Marc Reznicek, Anke Ludeling, Cedric Krummes,
Franziska Schwantuschke, Maik Walter, Karin
Schmidt, Hagen Hirschmann, and Torsten Andreas.
2012. Das Falko-Handbuch. Korpusaufbau und
Annotationen Version 2.01. Technical report, Depart-
ment of German Studies and Linguistics, Humboldt
University, Berlin, Germany.
Alla Rozovskaya and Dan Roth. 2010. Annotating ESL
Errors: Challenges and Rewards. In Proceedings of
the NAACL HLT 2010 Fifth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 28?36, Los Angeles, California, June. Associ-
ation for Computational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction for
ESL Learners Using Global Context. In In Proceed-
ings of the 50th ACL: Short Papers, pages 192?202.
Joel R. Tetreault and Martin Chodorow. 2008. Native
judgments of non-native usage: experiments in prepo-
sition error detection. In Proceedings of the Work-
shop on Human Judgements in Computational Lin-
guistics, HumanJudge ?08, pages 24?32, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. In Proceedings of COL-
ING 2012, pages 2585?2602, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL 2003, pages 252?259.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, New York.
L. Yu and H. Liu. 2003. Feature Selection for High-
Dimensional Data: A Fast Correlation-Based Filter
Solution. In Proceedings of The Twentieth Interna-
tional Conference on Machine Leaning (ICML-03),
pages 856?863, Washington, D.C., USA. Association
for Computational Linguistics.
241
