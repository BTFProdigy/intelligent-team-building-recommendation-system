Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582?590,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
Partha Pratim Talukdar?
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Joseph Reisinger?
University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Deepak Ravichandran
Google Inc.
Mountain View, CA 94043
deepakr@google.com
Rahul Bhagat?
USC Information Sciences Institute
Marina Del Rey, CA 90292
rahul@isi.edu
Fernando Pereira
Google Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
1 Introduction
1.1 Motivation
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding andWeb search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al, 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
?Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
1.2 Contributions
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al, 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas?ca, 2008) is extended in two
ways:
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
582
instances such as Office of War Information),
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas?ca
(2008), while retaining similar precision levels.
2 First Phase Extractors
To show Adsorption?s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas?ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al, 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
2.1 Extraction from Free Text
Van Durme and Pas?ca (2008) produce an open-
domain set of instance clusters C ? C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C ? ? C, C ? 6= C containing
instances with label L is less thanK. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
2.2 Extraction from Structured Text
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al, 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T ? T consists of one or more columns. Each
column g ? T consists of a set of candidate in-
stances i ? g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C ? C as
MC(g)
def
= {i ? I(C) : i ? g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its ?-unique class
coverage, that is, the set of classes that have at least
? unique seeds in g,
Q(g;?)
def
= {C ? C : |MC(g)| ? ?}.
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
score(C, g;?)
def
= |MC(g)|
? ?? ?
seed matches
?
class coherence
? ?? ?
|MC(g)|
|
?
C??Q(g;?) I(C
?)|
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
583
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas?ca, 2008).
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
score(i, C;?)
def
=
1
ZC
?
g?T,T?T
score(C, g;?)
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
3 Graph-Based Extraction
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
? Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
? Extractions frommultiple sources, such asWeb
queries, Web tables, and text patterns can be
represented in a single graph.
? Graphs make explicit the potential paths of in-
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V,E,W ), where V is the set
of nodes, E is the set of edges and W : E ? R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C,w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node?s class is
hypothesized to be semantically similar to the label?s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
584
bob dylan
musician
0.95
johnny cash
0.87
singer
0.73
billy joel
0.82
0.75
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al, 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V,E,W ) and computes for each node v a la-
bel distribution Lv that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al, 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V,Er,Wr) be the edge-reversed version of the
original graph G = (V,E,W ) where (a, b) ?
Er iff (b, a) ? E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q ? V . To es-
timate Lq for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
1. With probability pcontv , continue the ran-
dom walk to a neighbor of v.
2. With probability pabndv , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
3. With probability pinjv , stop the random
walk and emit a label L from Iv.
Lq is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions Lv to assign a probability not
only to each label in C but also to the dummy
label ?, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G? = (V ?, E?,W ?) as
follows. For each v ? V , we define an ini-
tial distribution Iv = L?, where L? is the
dummy distribution with L?(?) = 1, repre-
senting lack of label information for v. In addi-
tion, let Vs ? V be the set of nodes for which
we have some actual label knowledge, and let
V ? = V ? {v? : v ? Vs}, E? = E ? {(v?, v) :
v ? Vs}, and W ?(v?, v) = 1 for v ? Vs,
W ?(u, v) = W (u, v) for u, v ? V . Finally,
let Iv? (seed labels) specify the knowledge about
possible labels for v ? Vs. Less formally, the
v? nodes in G? serve to inject into the graph the
prior label distributions for each v ? Vs.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
585
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al
(2008) to set the random walk probabilities:
? Let cv =
log ?
log(? + expH(v)) where H(v) =
?
?
u puv ? log(puv) with puv =
W (u,v)
P
u
? W (u
? ,v)
.
H(v) can be interpreted as the entropy of v?s
neighborhood. Thus, cv is lower if v has many
neighbors. We set ? = 2.
? jv = (1 ? cv) ?
?
H(v) if Iv 6= L> and 0
otherwise.
? Then let
zv = max(cv + jv, 1)
pcontv = cv/zv
pinjv = jv/zv
pabndv = 1? p
cont
v ? p
abnd
v
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al (2008).
3The number of iterations was set to 10 in the experiments
reported in this paper.
Algorithm 1 Adsorption Algorithm.
Input: G? = (V
?
, E
?
,W ?), Iv (?v ? V ?).
Output: Distributions {Lv : v ? V }.
1: Lv = Iv ?v ? V
?
2:
3: repeat
4: Nv =
?
u W (u, v)
5: Dv = 1Nv
?
u W (u, v)Lu ?v ? V
?
6: for all v ? V
?
do
7: Lv = pcontv ?Dv +p
inj
v ? Iv +pabndv ?L
>
8: end for
9: until convergence
4 Experiments
4.1 Data
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas?ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set ? = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
4.2 Graph Creation
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
586
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
Table 2: Classes and seeds used to initialize Adsorption.
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
5 Evaluation
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion?s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption?s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
Book Publishers Federal Agencies NFL Players Scientific Journals Mammals20
40
60
80
100
 
 Adsorption A8
Book
Publishers
Federal
Agencies
NFL
Players
Scientific
Journals
Mammals
A8 Adsorption
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
5.1 Instance Precision
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation ?i is a C? was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sante?, kidney and blood pressure research,
american journal of physiology?cell physiology
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
5.2 Class Label Recall
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
588
MRR MRR # found
Method (full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
lower MRR scores compared to Adsorption. This
result highlights Adsorption?s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
6 Related Work
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
7 Conclusion
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
Acknowledgments
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
References
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49?56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
589
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207?227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050?1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using theWeb. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
590
Using Encyclopedic Knowledge for Named Entity Disambiguation
Razvan Bunescu
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712-0233
razvan@cs.utexas.edu
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
mars@google.com
Abstract
We present a new method for detecting and
disambiguating named entities in open do-
main text. A disambiguation SVM kernel
is trained to exploit the high coverage and
rich structure of the knowledge encoded
in an online encyclopedia. The resulting
model significantly outperforms a less in-
formed baseline.
1 Introduction
1.1 Motivation
The de-facto web search paradigm defines the re-
sult to a user?s query as roughly a set of links to the
best-matching documents selected out of billions
of items available. Whenever the queries search
for pinpointed, factual information, the burden
of filling the gap between the output granularity
(whole documents) and the targeted information (a
set of sentences or relevant phrases) stays with the
users, by browsing the returned documents in or-
der to find the actually relevant bits of information.
A frequent case are queries about named entities,
which constitute a significant fraction of popu-
lar web queries according to search engine logs.
When submitting queries such as John Williams
or Python, search engine users could also be pre-
sented with a compilation of facts and specific at-
tributes about those named entities, rather than a
set of best-matching web pages. One of the chal-
lenges in creating such an alternative search result
page is the inherent ambiguity of the queries, as
several instances of the same class (e.g., different
people) or different classes (e.g., a type of snake,
a programming language, or a movie) may share
the same name in the query. As an example, the
Work done during a summer internship at Google.
contexts below are part of web documents refer-
ring to different people who share the same name
John Williams:
1. ?John Williams and the Boston Pops con-
ducted a summer Star Wars concert at Tan-
glewood.?
2. ?John Williams lost a Taipei death match
against his brother, Axl Rotten.?
3. ?John Williams won a Victoria Cross for his
actions at the battle of Rorke?s Drift.?
The effectiveness of the search could be greatly
improved if the search results were grouped
together according to the corresponding sense,
rather than presented as a flat, sense-mixed list
of items (whether links to full-length documents,
or extracted facts). As an added benefit, users
would have easier access to a wider variety of re-
sults, whenever the top 10 or so results returned by
the largest search engines happen to refer to only
one particular (arguably the most popular) sense
of the query (e.g., the programming language in
the case of Python), thus submerging or ?hiding?
documents that refer to other senses of the query.
In various natural language applications, signif-
icant performance gains are achieved as a func-
tion of data size rather than algorithm complex-
ity, as illustrated by the increasingly popular use
of the web as a (very large) corpus (Dale, 2003).
It seems therefore natural to try to exploit the web
in order to also improve the performance of re-
lation extraction, i.e. the discovery of useful re-
lationships between named entities mentioned in
text documents. However, if one wants to combine
evidence from multiple web pages, then one needs
again to solve the name disambiguation problem.
9
Without solving it, a relation extraction system an-
alyzing the sentences in the above example could
mistakenly consider the third as evidence that John
Williams the composer fought at Rorke?s Drift.
1.2 Approach
The main goal of the research reported in this pa-
per is to develop a named entity disambiguation
method that is intrinsically linked to a dictionary
mapping proper names to their possible named en-
titiy denotations. More exactly, the method:
1. Detects whether a proper name refers to a
named entity included in the dictionary (de-
tection).
2. Disambiguates between multiple named enti-
ties that can be denoted by the same proper
name (disambiguation).
As a departure from the methodology of previous
approaches, the paper exploits a non-traditional
web-based resource. Concretely, it takes advan-
tage of some of the human knowledge available
in Wikipedia, a free online encyclopedia created
through decentralized, collective efforts of thou-
sands of users (Remy, 2002). We show that the
structure of Wikipedia lends itself to a set of
useful features for the detection and disambigua-
tion of named entities. The remainder of the pa-
per is organized as follows. Section 2 describes
Wikipedia, with an emphasis on the features that
are most important to the entity disambiguation
task. Section 3 describes the extraction of named
entity entries (versus other types of entries) from
Wikipedia. Section 4 introduces two disambigua-
tion methods, which are evaluated experimentally
in Section 5. We conclude with future work and
conclusions.
2 Wikipedia ? A Wiki Encyclopedia
Wikipedia is a free online encyclopedia written
collaboratively by volunteers, using a wiki soft-
ware that allows almost anyone to add and change
articles. It is a multilingual resource - there are
about 200 language editions with varying levels
of coverage. Wikipedia is a very dynamic and
quickly growing resource ? articles about news-
worthy events are often added within days of their
occurrence. As an example, the September 2005
version contains 751,666 articles, around 180,000
more articles than four months earlier. The work
in this paper is based on the English version from
May 2005, which contains 577,860 articles.
Each article in Wikipedia is uniquely identified
by its title ? a sequence of words separated by
underscores, with the first word always capital-
ized. Typically, the title is the most common name
for the entity described in the article. When the
name is ambiguous, it is further qualified with a
parenthetical expression. For instance, the arti-
cle on John Williams the composer has the title
John Williams (composer).
Because each article describes a specific en-
tity or concept, the remainder of the paper some-
times uses the term ?entity? interchangeably to re-
fer to both the article and the corresponding entity.
Also, let E denote the entire set of entities from
Wikipedia. For any entity e2E, e:title is the title
name of the corresponding article, and e:T is the
text of the article.
In general, there is a many-to-many correspon-
dence between names and entities. This relation
is captured in Wikipedia through redirect and dis-
ambiguation pages, as described in the next two
sections.
2.1 Redirect Pages
A redirect page exists for each alternative name
that can be used to refer to an entity in Wikipedia.
The name is transformed (using underscores for
spaces) into a title whose article contains a
redirect link to the actual article for that en-
tity. For example, John Towner Williams is the
full name of the composer John Williams. It
is therefore an alternative name for the com-
poser, and consequently the article with the ti-
tle John Towner Williams is just a pointer to the
article for John Williams (composer). An exam-
ple entry with a considerably higher number of
redirect pages is United States. Its redirect pages
correspond to acronyms (U.S.A., U.S., USA, US),
Spanish translations (Los Estados Unidos, Esta-
dos Unidos), misspellings (Untied States) or syn-
onyms (Yankee land).
For any given Wikipedia entity e2E, let e:R be
the set of all names that redirect to e.
2.2 Disambiguation Pages
Another useful structure is that of disambiguation
pages, which are created for ambiguous names,
i.e. names that denote two or more entities in
Wikipedia. For example, the disambiguation page
for the name John Williams lists 22 associated
10
TITLE REDIRECT DISAMBIG CATEGORIES
Star Wars music, ...
John Williams (composer) John Towner Williams John Williams Film score composers,
20th century classical composers
John Williams (wrestler) Ian Rotten John Williams Professional wrestlers,
People living in Baltimore
John Williams (VC) none John Williams British Army soldiers,
British Victoria Cross recipients
Boston Pops Orchestra Boston Pops, Pops American orchestras,
The Boston Pops Orchestra Massachusetts musicians
United States US, USA, ... US, USA, North American countries,
United States of America United States Republics, United States
Venus, Venus
Venus (planet) Planet Venus Morning Star, Planets of the Solar System,
Evening Star Planets, Solar System, ...
Table 1: Examples of Wikipedia titles, aliases and categories
entities. Therefore, besides the non-ambiguous
names that come from redirect pages, additional
aliases can be found by looking for all disam-
biguation pages that list a particular Wikipedia en-
tity. In his philosophical article ?On Sense and
Reference? (Frege, 1999), Gottlob Frege gave a
famous argument to show that sense and reference
are distinct. In his example, the planet Venus may
be referred to using the phrases ?morning star? and
?evening star?. This theoretical example is nicely
captured in practice in Wikipedia by two disam-
biguation pages, Morning Star and Evening Star,
both listing Venus as a potential referent.
For any given Wikipedia entity e 2 E, let e:D
be the set of names whose disambiguation pages
contain a link to e.
2.3 Categories
Every article in Wikipedia is required to have at
least one category. As shown in Table 1, John
Williams (composer) is associated with a set of
categories, among them Star Wars music, Film
score composers, and 20th century classical com-
posers. Categories allow articles to be placed into
one or more topics. These topics can be further
categorized by associating them with one or more
parent categories. In Table 1 Venus is shown as
both an article title and a category. As a cate-
gory, it has one direct parent Planets of the Solar
System, which in turn belongs to two more gen-
eral categories, Planets and Solar System. Thus,
categories form a directed acyclic graph, allowing
multiple categorization schemes to co-exist simul-
taneously. There are in total 59,759 categories in
Wikipedia.
For a given Wikipedia entity e 2E, let e:C be
the set of categories to which e belongs (i.e. e?s
immediate categories and all their ancestors in the
Wikipedia taxonomy).
2.4 Hyperlinks
Articles in Wikipedia often contain mentions of
entities that already have a corresponding arti-
cle. When contributing authors mention an ex-
isting Wikipedia entity inside an article, they are
required to link at least its first mention to the cor-
responding article, by using links or piped links.
Both types of links are exemplified in the follow-
ing wiki source code of a sentence from the article
on Italy: ?The [[Vatican City|Vatican]] is now an
independent enclave surrounded by [[Rome]]?.
The string from the second link (?Rome?) denotes
the title of the referenced article. The same string
is also used in the display version. If the author
wants another string displayed (e.g., ?Vatican? in-
stead of ?Vatican City?), then the alternative string
is included in a piped link, after the title string.
Consequently, the display string for the aforemen-
tioned example is: ?The Vatican is now an inde-
pendent enclave surrounded by Rome?. As de-
scribed later in Section 4, the hyperlinks can pro-
vide useful training examples for a named entity
disambiguator.
3 A Dictionary of Named Entities
We organize all named entities from Wikipedia
into a dictionary structure D, where each string
entry d 2 D is mapped to the set of entities
d:E that can be denoted by d in Wikipedia. The
first step is to identify named entities, i.e. entities
with a proper name title. Because every title in
Wikipedia must begin with a capital letter, the de-
cision whether a title is a proper name relies on the
following sequence of heuristic steps:
11
1. If e:title is a multiword title, check the cap-
italization of all content words, i.e. words
other than prepositions, determiners, con-
junctions, relative pronouns or negations.
Consider e a named entity if and only if all
content words are capitalized.
2. If e:title is a one word title that contains at
least two capital letters, then e is a named en-
tity. Otherwise, go to step 3.
3. Count how many times e:title occurs in the
text of the article, in positions other than at
the beginning of sentences. If at least 75% of
these occurrences are capitalized, then e is a
named entity.
The combined heuristics extract close to half a
million named entities from Wikipedia. The sec-
ond step constructs the actual dictionary D as fol-
lows:
 The set of entries in D consists of all strings
that may denote a named entity, i.e. if e2E
is a named entity, then its title name e:title,
its redirect names e:R, and its disambigua-
tion names e:D are all added as entries in D.
 Each entry string d2D is mapped to d:E, the
set of entities that d may denote in Wikipedia.
Consequently, a named entity e is included in
d:E if and only if d = e:title, d 2 e:R, or
d2e:D.
4 Named Entity Disambiguation
As illustrated in Section 1, the same proper name
may refer to more than one named entity. The
named entity dictionary from Section 3 and the hy-
perlinks from Wikipedia articles provide a dataset
of disambiguated occurrences of proper names,
as described in the following. As shown in Sec-
tion 2.4, each link contains the title name of an en-
tity, and the proper name (the display string) used
to refer to it. We use the term query to denote the
occurrence of a proper name inside a Wikipedia
article. If there is a dictionary entry matching the
proper name in the query q such that the set of
denoted entities q:E contains at least two entities,
one of them the true answer entity q:e, then the
query q is included in the dataset. More exactly, if
q:E contains n named entities e
1
, e
2
, ..., e
n
, then
the dataset will be augmented with n pairs hq; e
k
i
represented as follows:
hq; e
k
i = [?(e
k
; q:e) j q:T j e
k
:title]
The field q:T contains all words occurring in a
limit length window centered on the proper name.
The window size is set to 55, which is the value
that was observed to give optimum performance
in the related task of cross-document coreference
(Gooi and Allan, 2004). The Kronecker delta
function ?(e
k
; q:e) is 1 when e
k
is the same as
the entity q:e referred in the link. Table 2 lists
the query pairs created for the three John Williams
queries from Section 1.1, assuming only three en-
tities in Wikipedia correspond to this name.
? Query Text Entity Title
1 Boston Pops conduct ... John Williams (composer)
0 Boston Pops conduct ... John Williams (wrestler)
0 Boston Pops conduct ... John Williams (VC)
1 lost Taipei match ... John Williams (wrestler)
0 lost Taipei match ... John Williams (composer)
0 lost Taipei match ... John Williams (VC)
1 won Victoria Cross ... John Williams (VC)
0 won Victoria Cross ... John Williams (composer)
0 won Victoria Cross ... John Williams (wrestler)
Table 2: Disambiguation dataset.
The application of this procedure on Wikipedia
results into a dataset of 1,783,868 disambiguated
queries.
4.1 Context-Article Similarity
Using the representation from the previous sec-
tion, the name entity disambiguation problem can
be cast as a ranking problem. Assuming that an
appropriate scoring function score(q; e
k
) is avail-
able, the named entity corresponding to query q is
defined to be the one with the highest score:
e^ = argmax
e
k
score(q; e
k
) (1)
If e^ = q:e then e^ represents a hit, otherwise e^ is
a miss. Disambiguation methods will then differ
based on the way they define the scoring function.
One ranking function that is evaluated experimen-
tally in this paper is based on the cosine similarity
between the context of the query and the text of
the article:
score(q; e
k
) = cos(q:T; e
k
:T ) =
q:T
kq:Tk
e
k
:T
ke
k
:Tk
The factors q:T and e
k
:T are represented in the
standard vector space model, where each compo-
nent corresponds to a term in the vocabulary, and
the term weight is the standard tf  idf score
(Baeza-Yates and Ribeiro-Neto, 1999). The vo-
cabulary V is created by reading all Wikipedia
12
articles and recording, for each word stem w, its
document frequency df(w) in Wikipedia. Stop-
words and words that are too frequent or too rare
are discarded. A generic document d is then repre-
sented as a vector of length jV j, with a position for
each vocabulary word. If f(w) is the frequency of
word w in document d, and N is the total num-
ber of Wikipedia articles, then the weight of word
w2V in the tf  idf representation of d is:
d
w
= f(w) ln
N
df(w)
(2)
4.2 Taxonomy Kernel
An error analysis of the cosine-based ranking
method reveals that, in many cases, the pair hq; ei
fails to rank first, even though words from the
query context unambiguously indicate e as the ac-
tual denoted entity. In these cases, cue words from
the context do not appear in e?s article due to two
main reasons:
1. The article may be too short or incomplete.
2. Even though the article captures most of the
relevant concepts expressed in the query con-
text, it does this by employing synonymous
words or phrases.
The cosine similarity between q and e
k
can be seen
as an expression of the total degree of correlation
between words from the context of query q and a
given named entity e
k
. When the correlation is too
low because the Wikipedia article for named entity
e
k
does not contain all words that are relevant to
e
k
, it is worth considering the correlation between
context words and the categories to which e
k
be-
longs. For illustration, consider the two queries
for the name John Williams from Figure 1.
To avoid clutter, Figure 1 depicts only two enti-
ties with the name John Williams in Wikipedia: the
composer and the wrestler. On top of each entity,
the figure shows one of their Wikipedia categories
(Film score composers and Professional wrestlers
respectively), together with some of their ances-
tor categories in the Wikipedia taxonomy. The
two query contexts are shown at the bottom of
the figure. In the context on the left, words such
as conducted and concert denote concepts that are
highly correlated with the Musicians, Composers
and Film score composers categories. On the other
hand, their correlation with other categories in
Figure 1 is considerably lower. Consequently, a
Musicians
Composers
Film score composers
People by occupation
People
People known in connection
with sports and hobbies
Wrestlers
Professional wrestlers
high correlationshigh correlations
? ?
conducted
a summer Star Wars
John Williams John Williams
a Taipei death
lost
concert match[...] [...]
John Williams (composer) John Williams (wrestler)
Figure 1: Word-Category correlations.
goal of this paper is to design a disambiguation
method that 1) learns the magnitude of these cor-
relations, and 2) uses these correlations in a scor-
ing function, together with the cosine similarity.
Our intuition is that, given the query context on the
left, such a ranking function has a better chance
of ranking the ?composer? entity higher than the
?wrestler? entity, when compared with the simple
cosine similarity baseline.
We consider using a linear ranking function as
follows:
e^ = argmax
e
k
w (q; e
k
) (3)
The feature vector (q; e
k
) contains a dedicated
feature 
cos
for cosine similarity, and jV j  jCj
features 
w;c
corresponding to combinations of
words w from the Wikipedia vocabulary V and
categories c from the Wikipedia taxonomy C:

cos
(q; e
k
) = cos(q:T; e
k
:T ) (4)

w;c
(q; e
k
) =
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 639?647,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Outclassing Wikipedia in Open-Domain Information Extraction:
Weakly-Supervised Acquisition of Attributes over Conceptual Hierarchies
Marius Pas?ca
Google Inc.
Mountain View, California 94043
mars@google.com
Abstract
A set of labeled classes of instances is ex-
tracted from text and linked into an exist-
ing conceptual hierarchy. Besides a signif-
icant increase in the coverage of the class
labels assigned to individual instances, the
resulting resource of labeled classes is
more effective than similar data derived
from the manually-created Wikipedia, in
the task of attribute extraction over con-
ceptual hierarchies.
1 Introduction
Motivation: Sharing basic intuitions and long-
term goals with other tasks within the area of Web-
based information extraction (Banko and Etzioni,
2008; Davidov and Rappoport, 2008), the task
of acquiring class attributes relies on unstructured
text available on the Web, as a data source for ex-
tracting generally-useful knowledge. In the case
of attribute extraction, the knowledge to be ex-
tracted consists in quantifiable properties of var-
ious classes (e.g., top speed, body style and gas
mileage for the class of sports cars).
Existing work on large-scale attribute extraction
focuses on producing ranked lists of attributes, for
target classes of instances available in the form
of flat sets of instances (e.g., ferrari modena,
porsche carrera gt) sharing the same class label
(e.g., sports cars). Independently of how the input
target classes are populated with instances (man-
ually (Pas?ca, 2007) or automatically (Pas?ca and
Van Durme, 2008)), and what type of textual data
source is used for extracting attributes (Web docu-
ments or query logs), the extraction of attributes
operates at a lexical rather than semantic level.
Indeed, the class labels of the target classes may
be not more than text surface strings (e.g., sports
cars) or even artificially-created labels (e.g., Car-
toonChar in lieu of cartoon characters). More-
over, although it is commonly accepted that sports
cars are also cars, which in turn are also motor ve-
hicles, the presence of sports cars among the input
target classes does not lead to any attributes being
extracted for cars and motor vehicles, unless the
latter two class labels are also present explicitly
among the input target classes.
Contributions: The contributions of this paper
are threefold. First, we investigate the role of
classes of instances acquired automatically from
unstructured text, in the task of attribute extrac-
tion over concepts from existing conceptual hi-
erarchies. For this purpose, ranked lists of at-
tributes are acquired from query logs for various
concepts, after linking a set of more than 4,500
open-domain, automatically-acquired classes con-
taining a total of around 250,000 instances into
conceptual hierarchies available in WordNet (Fell-
baum, 1998). In comparison, previous work
extracts attributes for either manually-specified
classes of instances (Pas?ca, 2007), or for classes of
instances derived automatically but considered as
flat rather than hierarchical classes, and manually
associated to existing semantic concepts (Pas?ca
and Van Durme, 2008). Second, we expand the
set of classes of instances acquired from text, thus
increasing their usefulness in attribute extraction
in particular and information extraction in general.
To this effect, additional class labels (e.g., mo-
tor vehicles) are identified for existing instances
(e.g., ferrari modena) of existing class labels (e.g.,
sports cars), by exploiting IsA relations available
within the conceptual hierarchy (e.g., sports cars
are also motor vehicles). Third, we show that
large-scale, automatically-derived classes of in-
639
stances can have as much as, or even bigger, prac-
tical impact in open-domain information extrac-
tion tasks than similar data from large-scale, high-
coverage, manually-compiled resources. Specif-
ically, evaluation results indicate that the accu-
racy of the extracted lists of attributes is higher
by 8% at rank 10, 13% at rank 30 and 18% at
rank 50, when using the automatically-extracted
classes of instances rather than the comparatively
more numerous and a-priori more reliable, human-
generated, collaboratively-vetted classes of in-
stances available within Wikipedia (Remy, 2002).
2 Attribute Extraction over Hierarchies
Extraction of Flat Labeled Classes: Unstruc-
tured text from a combination of Web documents
and query logs represents the source for deriving
a flat set of labeled classes of instances, which are
necessary as input for attribute extraction experi-
ments. The labeled classes are acquired in three
stages:
1) extraction of a noisy pool of pairs of a
class label and a potential class instance, by ap-
plying a few Is-A extraction patterns, selected
from (Hearst, 1992), to Web documents:
(fruits, apple), (fruits, corn), (fruits, mango),
(fruits, orange), (foods, broccoli), (crops, lettuce),
(flowers, rose);
2) extraction of unlabeled clusters of distribu-
tionally similar phrases, by clustering vectors of
contextual features collected around the occur-
rences of the phrases within Web documents (Lin
and Pantel, 2002):
{lettuce, broccoli, corn, ..},
{carrot, mango, apple, orange, rose, ..};
3) merging and filtering of the raw pairs and un-
labeled clusters into smaller, more accurate sets of
class instances associated with class labels, in an
attempt to use unlabeled clusters to filter noisy raw
pairs instead of merely using clusters to general-
ize class labels across raw pairs (Pas?ca and Van
Durme, 2008):
fruits={apple, mango, orange, ..}.
To increase precision, the vocabulary of class
instances is confined to the set of queries that are
most frequently submitted to a general-purpose
Web search engine. After merging, the resulting
pairs of an instance and a class label are arranged
into instance sets (e.g., {ferrari modena, porsche
carrera gt}), each associated with a class label
(e.g., sports cars).
Linking Labeled Classes into Hierarchies:
Manually-constructed language resources such as
WordNet provide reliable, wide-coverage upper-
level conceptual hierarchies, by grouping together
phrases with the same meaning (e.g., {analgesic,
painkiller, pain pill}) into sets of synonyms
(synsets), and organizing the synsets into concep-
tual hierarchies (e.g., painkillers are a subconcept,
or a hyponym, of drugs) (Fellbaum, 1998). To de-
termine the points of insertion of automatically-
extracted labeled classes into hand-built Word-
Net hierarchies, the class labels are looked up in
WordNet using built-in morphological normaliza-
tion routines. When a class label (e.g., age-related
diseases) is not found in WordNet, it is looked up
again after iteratively removing its leading words
(e.g., related diseases, and diseases) until a poten-
tial point of insertion is found where one or more
senses exist in WordNet for the class label.
An efficient heuristic for sense selection is to
uniformly choose the first (that is, most frequent)
sense of the class label in WordNet, as point of
insertion. Due to its simplicity, the heuristic is
bound to make errors whenever the correct sense is
not the first one, thus incorrectly linking academic
journals under the sense of journals as personal
diaries rather than periodicals, and active volca-
noes under the sense of volcanoes as fissures in
the earth, rather than mountains formed by vol-
canic material. Nevertheless, choosing the first
sense is attractive for three reasons. First, Word-
Net senses are often too fine-grained, making the
task of choosing the correct sense difficult even
for humans (Palmer et al, 2007). Second, choos-
ing the first sense from WordNet is sometimes
better than more intelligent disambiguation tech-
niques (Pradhan et al, 2007). Third, previous ex-
perimental results on linking Wikipedia classes to
WordNet concepts confirm that first-sense selec-
tion is more effective in practice than other tech-
niques (Suchanek et al, 2007). Thus, a class la-
bel and its associated instances are inserted under
the first WordNet sense available for the class la-
bel. For example, silicon valley companies and its
associated instances (apple, hewlett packard etc.)
are inserted under the first of the 9 senses of com-
panies in WordNet, which corresponds to compa-
nies as institutions created to conduct business.
In order to trade off coverage for higher preci-
sion, the heuristic can be restricted to link a class
label under the first WordNet sense available, as
640
before, but only when no other senses are avail-
able at the point of insertion beyond the first sense.
With the modified heuristic, the class label internet
search engines is linked under the first and only
sense of search engines in WordNet, but silicon
valley companies is no longer linked under the first
of the 9 senses of companies.
Extraction of Attributes for Hierarchy Con-
cepts: The labeled classes of instances linked to
conceptual hierarchies constitute the input to the
acquisition of attributes of hierarchy concepts, by
mining a collection of Web search queries. The at-
tributes capture properties that are relevant to the
concept. The extraction of attributes exploits the
sets of class instances rather than the associated
class labels. More precisely, for each hierarchy
concept for which attributes must be extracted, the
instances associated to all class labels linked un-
der the subhierarchy rooted at the concept are col-
lected as a union set of instances, thus exploiting
the transitivity of IsA relations. This step is equiv-
alent to propagating the instances upwards, from
their class labels to higher-level WordNet concepts
under which the class labels are linked, up to the
root of the hierarchy. The resulting sets of in-
stances constitute the input to the acquisition of
attributes, which consists of four stages:
1) identification of a noisy pool of candidate at-
tributes, as remainders of queries that also con-
tain one of the class instances. In the case of the
concept movies, whose instances include jay and
silent bob strike back and kill bill, the query ?cast
jay and silent bob strike back? produces the can-
didate attribute cast;
2) construction of internal vector representa-
tions for each candidate attribute, based on queries
(e.g., ?cast selection for kill bill?) that contain a
candidate attribute (cast) and a class instance (kill
bill). These vectors consist of counts tied to the
frequency with which an attribute occurs with a
given ?templatized? query. The latter replaces spe-
cific attributes and instances from the query with
common placeholders, e.g., ?X for Y?;
3) construction of a reference internal vector
representation for a small set of seed attributes
provided as input. A reference vector is the nor-
malized sum of the individual vectors correspond-
ing to the seed attributes;
4) ranking of candidate attributes with respect
to each concept, by computing the similarity be-
tween their individual vector representations and
the reference vector of the seed attributes.
The result of the four stages, which are de-
scribed in more detail in (Pas?ca, 2007), is a ranked
list of attributes (e.g., [opening song, cast, charac-
ters,...]) for each concept (e.g., movies).
3 Experimental Setting
Textual Data Sources: The acquisition of open-
domain knowledge relies on unstructured text
available within a combination of Web documents
maintained by, and search queries submitted to the
Google search engine. The textual data source
for extracting labeled classes of instances con-
sists of around 100 million documents in En-
glish, as available in a Web repository snapshot
from 2006. Conversely, the acquisition of open-
domain attributes relies on a random sample of
fully-anonymized queries in English submitted by
Web users in 2006. The sample contains about 50
million unique queries. Each query is accompa-
nied by its frequency of occurrence in the logs.
Other sources of similar data are available publicly
for research purposes (Gao et al, 2007).
Parameters for Extracting Labeled Classes:
When applied to the available document col-
lection, the method for extracting open-domain
classes of instances from unstructured text intro-
duced in (Pas?ca and Van Durme, 2008) produces
4,583 class labels associated to 258,699 unique
instances, for a total of 869,118 pairs of a class
instance and an associated class label. All col-
lected instances occur among to the top five mil-
lion queries with the highest frequency within the
input query logs. The data is further filtered by
discarding labeled classes with fewer than 25 in-
stances. The classes, examples of which are shown
in Table 1, are linked under conceptual hierarchies
available within WordNet 3.0, which contains a to-
tal of 117,798 English noun phrases grouped in
82,115 concepts (or synsets).
Parameters for Extracting Attributes: For each
target concept from the hierarchy, given the union
of all instances associated to class labels linked to
the target concept or one of its subconcepts, and
given a set of five seed attributes (e.g., {quality,
speed, number of users, market share, reliabil-
ity} for search engines), the method described
in (Pas?ca, 2007) extracts ranked lists of attributes
from the input query logs. Internally, the rank-
ing of attributes uses Jensen-Shannon (Lee, 1999)
to compute similarity scores between internal rep-
641
Class Label Class Size Class Instances
accounting systems 40 flexcube, myob, oracle financials, peachtree accounting, sybiz
antimicrobials 97 azithromycin, chloramphenicol, fusidic acid, quinolones, sulfa drugs
civilizations 197 ancient greece, chaldeans, etruscans, inca, indians, roman republic
elementary particles 33 axions, electrons, gravitons, leptons, muons, neutrons, positrons
farm animals 61 angora goats, burros, cattle, cows, donkeys, draft horses, mule, oxen
forages 27 alsike clover, rye grass, tall fescue, sericea lespedeza, birdsfoot trefoil
ideologies 179 egalitarianism, laissez-faire capitalism, participatory democracy
social events 436 academic conferences, afternoon teas, block parties, masquerade balls
Table 1: Examples of instances within labeled classes extracted from unstructured text, used as input for
attribute extraction experiments
resentations of seed attributes, on one hand, and
each of the newly acquired attributes, on the other
hand. Depending on the experiments, the amount
of supervision is thus limited to either 5 seed at-
tributes for each target concept, or to 5 seed at-
tributes (population, area, president, flag and cli-
mate) provided for only one of the extracted la-
beled classes, namely european countries.
Experimental Runs: The experiments consist of
four different runs, which correspond to different
choices for the source of conceptual hierarchies
and class instances linked to those hierarchies, as
illustrated in Table 2. In the first run, denoted N,
the class instances are those available within the
latest version of WordNet (3.0) itself via HasIn-
stance relations. The second run, Y, corresponds to
an extension of WordNet based on the manually-
compiled classes of instances from categories in
Wikipedia, as available in the 2007-w50-5 version
of Yago (Suchanek et al, 2007). Therefore, run Y
has the advantage of the fact that Wikipedia cat-
egories are a rich source of useful and accurate
knowledge (Nastase and Strube, 2008), which ex-
plains their previous use as a source for evaluation
gold standards (Blohm et al, 2007). The last two
runs from Table 2, Es and Ea, correspond to the
set of open-domain labeled classes acquired from
unstructured text. In both Es and Ea, class labels
are linked to the first sense available at the point
of insertion in WordNet. In Es, the class labels
are linked only if no other senses are available at
the point of insertion beyond the first sense, thus
promoting higher linkage precision at the expense
of fewer links. For example, since the phrases im-
pressionists, sports cars and painters have 1, 1 and
4 senses available in WordNet respectively, the
class labels french impressionists and sports cars
are linked to the respective WordNet concepts,
whereas the class label painters is not. Compar-
atively, in Ea, the class labels are uniformly linked
Description Source of Hierarchy and Instances
N Y Es Ea
Include instances
? ?
- -
from WordNet?
Include instances -
? ? ?
from elsewhere?
#Instances (?103) 14.3 1,296.5 108.0 257.0
#Class labels 945 30,338 1,315 4,517
#Pairs of a class label 17.4 2,839.8 191.0 859.0
and instance (?103)
Table 2: Source of class instances for various ex-
perimental runs
to the first sense available in WordNet, regardless
of whether other senses may or may not be avail-
able. Thus, Ea trades off potentially lower preci-
sion for the benefit of higher linkage recall, and
results in more of the class labels and their asso-
ciated instances extracted from text to be linked to
WordNet than in the case of run Es.
4 Evaluation
4.1 Evaluation of Labeled Classes
Coverage of Class Instances: In run N, the in-
put class instances are the component phrases of
synsets encoded via HasInstance relations under
other synsets in WordNet. For example, the synset
corresponding to {search engine}, defined as ?a
computer program that retrieves documents or
files or data from a database or from a computer
network?, has 3 HasInstance instances in Word-
Net, namely Ask Jeeves, Google and Yahoo. Ta-
ble 3 illustrates the coverage of the class instances
extracted from unstructured text and linked to
WordNet in runs Es and Ea respectively, relative to
all 945 WordNet synsets that contain HasInstance
instances. Note that the coverage scores are con-
servative assessments of actual coverage, since a
run (i.e., Es or Ea) receives credit for a WordNet
instance only if the run contains an instance that
is a full-length, case-insensitive match (e.g., ask
642
Concept HasInstance Instances within WordNet Cvg
Synset Offset Examples Count Es Ea
{existentialist, existentialist, 10071557 Albert Camus, Beauvoir, Camus, 8 1.00 1.00
philosopher, existential philosopher} Heidegger, Jean-Paul Sartre
{search engine} 06578654 Ask Jeeves, Google, Yahoo 3 1.00 1.00
{university} 04511002 Brown, Brown University, 44 0.61 0.77
Carnegie Mellon University
{continent} 09254614 Africa, Antarctic continent, Europe, 13 0.54 0.54
Eurasia, Gondwanaland, Laurasia
{microscopist} 10313872 Anton van Leeuwenhoek, Anton 6 0.00 0.00
van Leuwenhoek, Swammerdam
Average over all 945 WordNet concepts that have HasInstance instance(s) 18.71 0.21 0.40
Table 3: Coverage of class instances extracted from text and linked to WordNet (used as input in runs Es
and Ea respectively), measured as the fraction of WordNet HasInstance instances (used as input in run
N) that occur among the class instances (Cvg=coverage)
jeeves) of the WordNet instance. On average, the
coverage scores for class instances of runs Es and
Ea relative to run N are 0.21 and 0.40 respectively,
as shown in the last row in Table 3. Comparatively,
the equivalent instance coverage for run Y, which
already includes most of the WordNet instances by
design (cf. (Suchanek et al, 2007)), is 0.59.
Relative Coverage of Class Labels: The link-
ing of class labels to WordNet concepts allows for
the expansion of the set of classes of instances ac-
quired from text, thus increasing its usefulness in
attribute extraction in particular and information
extraction in general. To this effect, additional
class labels are identified for existing instances,
in the form of component phrases of the synsets
that are superconcepts (or hypernyms, in WordNet
terminology) of the synset under which the class
label of the instance is linked in WordNet. For ex-
ample, since the class label sports cars is linked
under the WordNet synset {sports car, sport car},
and the latter has the synset {motor vehicle, auto-
motive vehicle} among its hypernyms, the phrases
motor vehicles and automotive vehicles are col-
lected as new class labels 1 and associated to ex-
isting instances of sports cars from the original
set, such as ferrari modena. No phrases are col-
lected from a selected set of 10 top-level Word-
Net synsets, including {entity} and {object, phys-
ical object}, which are deemed too general to be
useful as class labels. As illustrated in Table 4,
a collected pair of a new class label and an exist-
ing instance either does not have any impact, if the
pair already occurs in the original set of labeled
1For consistency with the original labeled classes, new
class labels collected from WordNet are converted from sin-
gular (e.g., motor vehicle) to plural (e.g., motor vehicles).
Already in original labeled classes:
painters alfred sisley
european countries austria
Expansion of existing labeled classes:
animals avocet
animals northern oriole
scientists howard gardner
scientists phil zimbardo
Creation of new labeled classes:
automotive vehicles acura nsx
automotive vehicles detomaso pantera
creative persons aaron copland
creative persons yoshitomo nara
Table 4: Examples of additional class labels col-
lected from WordNet, for existing instances of the
original labeled classes extracted from text
classes; or expands existing classes, if the class
label already occurs in the original set of labeled
classes but not in association to the instance; or
creates new classes of instances, if the class label
is not part of the original set. The latter two cases
aggregate to increases in coverage, relative to the
pairs from the original sets of labeled classes, of
53% for Es and 304% for Ea.
4.2 Evaluation of Attributes
Target Hierarchy Concepts: The performance of
attribute extraction is assessed over a set of 25 tar-
get concepts also used for evaluation in (Pas?ca,
2008). The set of 25 target concepts includes: Ac-
tor, Award, Battle, CelestialBody, ChemicalEle-
ment, City, Company, Country, Currency, Dig-
italCamera, Disease, Drug, FictionalCharacter,
Flower, Food, Holiday, Mountain, Movie, Nation-
alPark, Painter, Religion, River, SearchEngine,
Treaty, Wine. Each target concept represents ex-
actly one WordNet concept (synset). For instance,
643
one of the target concepts, denoted Country, cor-
responds to a synset situated at the internal off-
set 08544813 in WordNet 3.0, which groups to-
gether the synonymous phrases country, state and
land and associates them with the definition ?the
territory occupied by a nation?. The target con-
cepts exhibit variation with respect to their depths
within WordNet conceptual hierarchies, ranging
from a minimum of 5 (e.g., for Food) to a maxi-
mum of 11 (for Flower), with a mean depth of 8
over the 25 concepts.
Evaluation Procedure: The measurement of re-
call requires knowledge of the complete set of
items (in our case, attributes) to be extracted. Un-
fortunately, this number is often unavailable in in-
formation extraction tasks in general (Hasegawa
et al, 2004), and attribute extraction in particular.
Indeed, the manual enumeration of all attributes
of each target concept, to measure recall, is un-
feasible. Therefore, the evaluation focuses on the
assessment of attribute accuracy.
To remove any bias towards higher-ranked at-
tributes during the assessment of class attributes,
the ranked lists of attributes produced by each run
to be evaluated are sorted alphabetically into a
merged list. Each attribute of the merged list is
manually assigned a correctness label within its
respective class. In accordance with previously
introduced methodology, an attribute is vital if it
must be present in an ideal list of attributes of
the class (e.g., side effects for Drug); okay if it
provides useful but non-essential information; and
wrong if it is incorrect (Pas?ca, 2007).
To compute the precision score over a ranked
list of attributes, the correctness labels are con-
verted to numeric values (vital to 1, okay to 0.5
and wrong to 0). Precision at some rank N in the
list is thus measured as the sum of the assigned
values of the first N attributes, divided by N .
Attribute Accuracy: Figure 1 plots the precision
at ranks 1 through 50 for the ranked lists of at-
tributes extracted by various runs as an average
over the 25 target concepts, along two dimensions.
In the leftmost graphs, each of the 25 target con-
cepts counts towards the computation of precision
scores of a given run, regardless of whether any
attributes were extracted or not for the target con-
cept. In the rightmost graphs, only target con-
cepts for which some attributes were extracted are
included in the precision scores of a given run.
Thus, the leftmost graphs properly penalize a run
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
N
Y
EsEa
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
N
Y
EsEa
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
N
Y
EsEa
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
N
Y
EsEa
Figure 1: Accuracy of the attributes extracted for
various runs, as an average over the entire set of
25 target concepts (left graphs) and as an average
over (variable) subsets of the 25 target concepts
for which some attributes were extracted in each
run (right graphs). Seed attributes are provided as
input for only one target concept (top graphs), or
for each target concept (bottom graphs)
for failing to extract any attributes for some tar-
get concepts, whereas the rightmost graphs do not
include any such penalties. On the other dimen-
sion, in the graphs at the top of Figure 1, seed at-
tributes are provided only for one class (namely,
european countries), for a total of 5 attributes over
all classes. In the graphs at the bottom of the fig-
ure, there are 5 seed attributes for each of the 25
target concepts in the graphs at the bottom of Fig-
ure 1, for a total of 5?25=125 attributes.
Several conclusions can be drawn after inspect-
ing the results. First, providing more supervi-
sion, in the form of seed attributes for all concepts
rather than for only one concept, translates into
higher attribute accuracy for all runs, as shown
by the graphs at the top vs. graphs at the bot-
tom of Figure 1. Second, in the leftmost graphs,
run N has the lowest precision scores, which is in
line with the relatively small number of instances
available in the original WordNet, as confirmed by
the counts from Table 2. Third, in the leftmost
graphs, the more restrictive run Es has lower pre-
cision scores across all ranks than its less restric-
tive counterpart Ea. In other words, adding more
644
Class Precision
@10 @30 @50
N Y Es Ea N Y Es Ea N Y Es Ea
Actor 1.00 1.00 1.00 1.00 0.78 0.85 0.98 0.95 0.62 0.84 0.95 0.96
Award 0.00 0.50 0.95 0.85 0.00 0.35 0.80 0.73 0.00 0.29 0.70 0.69
Battle 0.80 0.90 0.00 0.90 0.76 0.80 0.00 0.80 0.74 0.72 0.00 0.73
CelestialBody 1.00 1.00 1.00 0.40 1.00 1.00 0.93 0.16 0.98 0.89 0.91 0.12
ChemicalElement 0.00 0.65 0.80 0.80 0.00 0.45 0.83 0.63 0.00 0.48 0.84 0.51
City 1.00 1.00 0.00 1.00 0.86 0.80 0.00 0.83 0.78 0.70 0.00 0.76
Company 0.00 1.00 0.90 1.00 0.00 0.90 0.93 0.88 0.00 0.77 0.82 0.80
Country 1.00 0.90 1.00 1.00 0.98 0.81 0.96 0.96 0.97 0.76 0.98 0.97
Currency 0.00 0.90 0.00 0.90 0.00 0.53 0.00 0.83 0.00 0.36 0.00 0.87
DigitalCamera 0.00 0.20 0.85 0.85 0.00 0.10 0.85 0.85 0.00 0.10 0.82 0.82
Disease 0.00 0.60 0.75 0.75 0.00 0.76 0.83 0.83 0.00 0.63 0.87 0.86
Drug 0.00 1.00 1.00 1.00 0.00 0.91 1.00 1.00 0.00 0.88 0.96 0.96
FictionalCharacter 0.80 0.70 0.00 0.55 0.65 0.48 0.00 0.38 0.42 0.41 0.00 0.34
Flower 0.00 0.65 0.00 0.70 0.00 0.26 0.00 0.55 0.00 0.16 0.00 0.53
Food 0.00 0.80 0.90 1.00 0.00 0.65 0.71 0.96 0.00 0.53 0.59 0.96
Holiday 0.00 0.60 0.80 0.80 0.00 0.50 0.48 0.48 0.00 0.37 0.41 0.41
Mountain 1.00 0.75 0.00 0.90 0.96 0.61 0.00 0.86 0.77 0.58 0.00 0.74
Movie 0.00 1.00 1.00 1.00 0.00 0.90 0.80 0.78 0.00 0.85 0.75 0.74
NationalPark 0.90 0.80 0.00 0.00 0.85 0.76 0.00 0.00 0.82 0.75 0.00 0.00
Painter 1.00 1.00 1.00 1.00 0.96 0.93 0.88 0.96 0.92 0.89 0.76 0.93
Religion 0.00 0.00 1.00 1.00 0.00 0.00 1.00 1.00 0.00 0.00 0.92 0.97
River 1.00 0.80 0.00 0.00 0.70 0.60 0.00 0.00 0.61 0.58 0.00 0.00
SearchEngine 0.40 0.00 0.25 0.25 0.23 0.00 0.35 0.35 0.32 0.00 0.43 0.43
Treaty 0.50 0.90 0.80 0.80 0.33 0.65 0.53 0.53 0.26 0.59 0.42 0.42
Wine 0.00 0.30 0.80 0.80 0.00 0.26 0.43 0.45 0.00 0.20 0.28 0.29
Average (over 25) 0.41 0.71 0.59 0.77 0.36 0.59 0.53 0.67 0.32 0.53 0.49 0.63
Average (over non-empty) 0.86 0.78 0.87 0.83 0.75 0.64 0.78 0.73 0.68 0.57 0.73 0.68
Table 5: Comparative accuracy of the attributes extracted by various runs, for individual concepts, as an
average over the entire set of 25 target concepts, and as an average over (variable) subsets of the 25 target
concepts for which some attributes were extracted in each run. Seed attributes are provided as input for
each target concept
restrictions may improve precision but hurts recall
of class instances, which results in lower average
precision scores for the attributes. Fourth, in the
leftmost graphs, the runs using the automatically-
extracted labeled classes (Es and Ea) not only out-
perform N, but one of them (Ea) also outperforms
Y. This is the most important result. It shows
that large-scale, automatically-derived classes of
instances can have as much as, or even bigger,
practical impact in attribute extraction than similar
data from larger (cf. Table 2), manually-compiled,
collaboratively created and maintained resources
such as Wikipedia. Concretely, in the graph on
the bottom left of Figure 1, the precision scores at
ranks 10, 30 and 50 are 0.71, 0.59 and 0.53 for run
Y, but 0.77, 0.67 and 0.63 for run Ea. The scores
correspond to attribute accuracy improvements of
8% at rank 10, 13% at rank 30, and 18% at rank
50 for run Ea over run Y. In fact, in the rightmost
graphs, that is, without taking into account target
concepts without any extracted attributes, the pre-
cision scores of both Es and Ea are higher than for
run Y across most, if not all, ranks from 1 through
50. In this case, it is E1 that produces the most
accurate attributes, in a task-based demonstration
that the more cautious linking of class labels to
WordNet concepts in Es vs. Ea leads to less cov-
erage but higher precision of the linked labeled
classes, which translates into extracted attributes
of higher accuracy but for fewer target concepts.
Analysis: The curves plotted in the two graphs
at the bottom of Figure 1 are computed as av-
erages over precision scores for individual target
concepts, which are shown in detail in Table 5.
Precision scores of 0.00 correspond to runs for
which no attributes are acquired from query logs,
because no instances are available in the subhier-
archy rooted at the respective concepts. For exam-
ple, precision scores for run N are 0.00 for Award
and DigitalCamera, among others concepts in Ta-
ble 5, due to the lack of any HasInstance instances
in WordNet for the respective concepts. The num-
ber of target concepts for which some attributes
are extracted is 12 for run N, 23 for Y, 17 for Es
645
and 23 for Ea. Thus, both run N and run Es exhibit
rather binary behavior across individual classes, in
that they tend to either not retrieve any attributes or
retrieve attributes of relatively higher quality than
the other runs, causing Es and N to have the worst
precision scores in the last but one row of Table 5,
but the best precision scores in the last row of Ta-
ble 5.
The individual scores shown for Es and Ea in
Table 5 concur with the conclusion drawn earlier
from the graphs in Figure 1, that Run Es has lower
precision than Ea as an average over all target con-
cepts. Notable exceptions are the scores obtained
for the concepts CelestialBody and ChemicalEle-
ment, where Es significantly outperforms Ea in Ta-
ble 5. This is due to confusing instances (e.g., kobe
bryant) being associated with class labels (e.g.,
nba stars) that are incorrectly linked under the tar-
get concepts (e.g., Star, which is a subconcept of
CelestialBody in WordNet) in Ea, but not linked at
all and thus not causing confusion in Es.
Run Y performs better than Ea for 5 of the 25
individual concepts, including NationalPark, for
which no instances of national parks or related
class labels are available in run Ea; and River, for
which relevant instances in the labeled classes in
Ea, but they are associated to the class label river
systems, which is incorrectly linked to the Word-
Net concept systems rather than to rivers. How-
ever, run Ea outperforms Y on 12 individual con-
cepts (e.g., Award, DigitalCamera and Disease),
and also as an average over all classes (last two
rows in Table 5).
5 Related Work
Previous work on the automatic acquisition of at-
tributes for open-domain classes from text requires
the manual enumeration of sets of instances and
seed attributes, for each class for which attributes
are to be extracted. In contrast, the current method
operates on automatically-extracted classes. The
experiments reported in (Pas?ca and Van Durme,
2008) also exploit automatically-extracted classes
for the purpose of attribute extraction. However,
they operate on flat classes, as opposed to concepts
organized hierarchically. Furthermore, they re-
quire manual mappings from extracted class labels
into a selected set of evaluation classes (e.g., by
mapping river systems to River, football clubs to
SoccerClub, and parks to NationalPark), whereas
the current method maps class labels to concepts
automatically, by linking class labels and their as-
sociated instances to concepts. Manually-encoded
attributes available within Wikipedia articles are
used in (Wu and Weld, 2008) in order to derive
other attributes from unstructured text within Web
documents. Comparatively, the current method
extracts attributes from query logs rather than
Web documents, using labeled classes extracted
automatically rather than available in manually-
created resources, and requiring minimal supervi-
sion in the form of only 5 seed attributes provided
for only one concept, rather than thousands of at-
tributes available in millions of manually-created
Wikipedia articles. To our knowledge, there is
only one previous study (Pas?ca, 2008) that directly
addresses the problem of extracting attributes over
conceptual hierarchies. However, that study uses
labeled classes extracted from text with a different
method; extracts attributes for labeled classes and
propagates them upwards in the hierarchy, in order
to compute attributes of hierarchy concepts from
attributes of their subconcepts; and does not con-
sider resources similar to Wikipedia, as sources of
input labeled classes for attribute extraction.
6 Conclusion
This paper introduces an extraction framework
for exploiting labeled classes of instances to ac-
quire open-domain attributes from unstructured
text available within search query logs. The link-
ing of the labeled classes into existing conceptual
hierarchies allows for the extraction of attributes
over hierarchy concepts, without a-priori restric-
tions to specific domains of interest and with little
supervision. Experimental results show that the
extracted attributes are more accurate when us-
ing automatically-derived labeled classes, rather
than classes of instances derived from manually-
created resources such as Wikipedia. Current
work investigates the impact of the semantic dis-
tribution of the classes of instances on the overall
accuracy of attributes; the potential benefits of us-
ing more compact conceptual hierarchies (Snow
et al, 2007) on attribute accuracy; and the orga-
nization of labeled classes of instances into con-
ceptual hierarchies, as an alternative to inserting
them into existing conceptual hierarchies created
manually from scratch or automatically by filter-
ing manually-generated relations among classes
from Wikipedia (Ponzetto and Strube, 2007).
646
References
M. Banko and O. Etzioni. 2008. The tradeoffs between open
and traditional relation extraction. In Proceedings of the
46th Annual Meeting of the Association for Computational
Linguistics (ACL-08), pages 28?36, Columbus, Ohio.
S. Blohm, P. Cimiano, and E. Stemle. 2007. Harvesting re-
lations from the web - quantifiying the impact of filter-
ing functions. In Proceedings of the 22nd National Con-
ference on Artificial Intelligence (AAAI-07), pages 1316?
1321, Vancouver, British Columbia.
D. Davidov and A. Rappoport. 2008. Classification of se-
mantic relationships between nominals using pattern clus-
ters. In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08), pages
227?235, Columbus, Ohio.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical
Database and Some of its Applications. MIT Press.
W. Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and H. Hon.
2007. Cross-lingual query suggestion using query logs
of different languages. In Proceedings of the 30th ACM
Conference on Research and Development in Information
Retrieval (SIGIR-07), pages 463?470, Amsterdam, The
Netherlands.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415?
422, Barcelona, Spain.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
International Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
L. Lee. 1999. Measures of distributional similarity. In Pro-
ceedings of the 37th Annual Meeting of the Association of
Computational Linguistics (ACL-99), pages 25?32, Col-
lege Park, Maryland.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference on
Computational linguistics (COLING-02), pages 1?7.
V. Nastase and M. Strube. 2008. Decoding Wikipedia cat-
egories for knowledge acquisition. In Proceedings of
the 23rd National Conference on Artificial Intelligence
(AAAI-08), pages 1219?1224, Chicago, Illinois.
M. Pas?ca and B. Van Durme. 2008. Weakly-supervised ac-
quisition of open-domain classes and class attributes from
web documents and query logs. In Proceedings of the 46th
Annual Meeting of the Association for Computational Lin-
guistics (ACL-08), pages 19?27, Columbus, Ohio.
M. Pas?ca. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of the 16th World Wide Web Con-
ference (WWW-07), pages 101?110, Banff, Canada.
M. Pas?ca. 2008. Turning Web text and search queries into
factual knowledge: Hierarchical class attribute extraction.
In Proceedings of the 23rd National Conference on Arti-
ficial Intelligence (AAAI-08), pages 1225?1230, Chicago,
Illinois.
M. Palmer, H. Dang, and C. Fellbaum. 2007. Making fine-
grained and coarse-grained sense distinctions, both man-
ually and automatically. Natural Language Engineering,
13(2):137?163.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
National Conference on Artificial Intelligence (AAAI-07),
pages 1440?1447, Vancouver, British Columbia.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007.
SemEval-2007 Task-17: English lexical sample, SRL and
all words. In Proceedings of the 4th Workshop on Se-
mantic Evaluations (SemEval-07), pages 87?92, Prague,
Czech Republic.
M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007. Learning
to merge word senses. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP-07), pages 1005?1014, Prague, Czech
Republic.
F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide Web
Conference (WWW-07), pages 697?706, Banff, Canada.
F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the 17th
World Wide Web Conference (WWW-08), pages 635?644,
Beijing, China.
647
Aligning Needles in a Haystack: Paraphrase
Acquisition Across the Web
Marius Pas?ca and Pe?ter Dienes
Google Inc.,
1600 Amphitheatre Parkway,
Mountain View, California, 94043, USA
{mars, dienes}@google.com
Abstract. This paper presents a lightweight method for unsupervised
extraction of paraphrases from arbitrary textual Web documents. The
method differs from previous approaches to paraphrase acquisition in
that 1) it removes the assumptions on the quality of the input data,
by using inherently noisy, unreliable Web documents rather than clean,
trustworthy, properly formatted documents; and 2) it does not require
any explicit clue indicating which documents are likely to encode parallel
paraphrases, as they report on the same events or describe the same sto-
ries. Large sets of paraphrases are collected through exhaustive pairwise
alignment of small needles, i.e., sentence fragments, across a haystack
of Web document sentences. The paper describes experiments on a set
of about one billion Web documents, and evaluates the extracted para-
phrases in a natural-language Web search application.
1 Introduction
The information captured in textual documents frequently encodes semantically
equivalent ideas through different lexicalizations. Indeed, given the generative
power of natural language, different people employ different words or phrases to
convey the same meaning, depending on factors such as background knowledge,
level of expertise, style, verbosity and personal preferences. Two equivalent frag-
ments of text may differ only slightly, as a word or a phrase in one of them
is paraphrased in the other, e.g., through a synonym. Yet even small lexical
variations represent challenges to any automatic decision on whether two text
fragments have the same meaning, or are relevant to each other, since they are
no longer lexically identical. Many natural-language intensive applications make
such decisions internally. In document summarization, the generated summaries
have a higher quality if redundant information has been discarded by detecting
text fragments with the same meaning [1]. In information extraction, extrac-
tion templates will not be filled consistently whenever there is a mismatch in
the trigger word or the applicable extraction pattern [2]. Similarly, a question
answering system could incorrectly discard a relevant document passage based
on the absence of a question phrase deemed as very important [3], even if the
passage actually contains a legitimate paraphrase.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 119?130, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
120 M. Pas?ca and P. Dienes
In information retrieval, deciding whether a text fragment (e.g., a document)
is relevant to another text fragment (i.e., the query) is crucial to the overall out-
put, rather than merely useful within some internal system module. Indeed,
relevant documents or passages may be missed, due to the apparent mismatch
between their terms and the paraphrases occurring in the users? queries. The
previously proposed solutions to the mismatch problem vary with respect to the
source of the data used for enriching the query with alternative terms. In auto-
matic query expansion, the top documents provide additional query terms [4]. An
alternative is to attempt to identify the concepts captured in the queries and find
semantically similar concepts in external resources, e.g., lexical databases [5, 6].
This paper explores a different direction, namely the unsupervised acquisition
of large sets of paraphrases from unstructured text within Web documents, and
their exploitation in natural-language Web search.
We present a lightweight method for unsupervised extraction of paraphrases
from arbitrary, textual Web documents. The method taps the textual contents
provided by millions of anonymous Web document contributors. The remainder
of the paper is structured as follows. After a condensed overview of the para-
phrase acquisition method and a contrast to previous literature in Section 2,
Section 3 presents the method in more detail. Section 4 describes evaluation
results when applying the method to textual documents from a Web repository
snapshot of the Google search engine.
2 Method at a Glance
The proposed acquisition method collects large sets of word and phrase-level
paraphrases via exhaustive pairwise alignment of small needles, i.e., sentence
fragments, across a haystack of Web document sentences. The acquisition of
paraphrases is a side-effect of the alignment.
In the example in Figure 1, if two sentence fragments have common word
sequences at both extremities, then the variable word sequences in the middle
are potential paraphrases of each other. A significant advantage of this extraction
mechanism is that it can acquire paraphrases from sentences whose information
content overlaps only partially, as long as the fragments align. Indeed, the source
sentences of the paraphrase (withdrew from, pulled out of), as well as of (took
effect, came into force), are arguably quite different overall in Figure 1. Moreover,
the sentences are part of documents whose content intersection is very small.
In addition to its relative simplicity when compared to more complex,
sentence-level paraphrase acquisition [7], the method introduced in this paper
is a departure from previous approaches in several respects. First, the para-
phrases are not limited to variations of specialized, domain-specific terms as
in [8], nor are they restricted to a narrow class such as verb paraphrases [9].
Second, as opposed to virtually all previous approaches, the method does not
require high-quality, clean, trustworthy, properly-formatted input data. Instead,
it uses inherently noisy, unreliable Web documents. The source data in [10] is
also a set of Web documents. However, it is based on top search results collected
Aligning Needles in a Haystack 121
Web repository
http://www.rantburg.com/default.asp?D=1/13/2004&C=India?Pakistan
After 1989, when Soviet troops withdrew from Afghanistan, the mujahedeen fought a civil war against
the Afghan government, which devastated the country, Kabul in particular.
D
o c
u m
e n
ts
F r
a g
m
e n
ts 1989, when Soviet troops withdrew from Afghanistan
1989, when Soviet troops pulled out of Afghanistan pulled out of
withdrew from
But Washington has steadily downgraded its involvement in and financial commitment to the region
since 1989, when Soviet troops pulled out of Afghanistan.
http://www.tamil.net/list/2001?09/msg00404.html
www.wvu.edu/~law/wvjolt/Arch/Nevin/Nevin.htm
The Electronic Signatures in Global and National Commerce Act, which took effect in October 2000,
established standards for the use of digital authentication.
S e
n t
e n
c e
s
D
o c
u m
e n
ts
S e
n t
e n
c e
s
F r
a g
m
e n
ts Act, which took effect in October 2000
Act, which came into force in October 2000
took effect
came into force
The United States passed federal e?commerce legislation, the Electronic Signatures in Global and
National Commerce Act, which came into force in October 2000.
www.ahbl.ca/library/High?Tech%20&%20Innovation/lookback?lookforward?mar2001.pdf
Fig. 1. Paraphrase acquisition from unstructured text across the Web
from external search engines, and its quality benefits implicitly from the rank-
ing functions of the search engines. Third, the input documents here are not
restricted to a particular genre, whereas virtually all other recent approaches
are designed for collections of parallel news articles, whether the articles are
part of a carefully-compiled collection [11] or aggressively collected from Web
news sources [12]. Fourth, the acquisition of paraphrases in this paper does not
rely on external clues and attributes that two documents are parallel and must
report on the same or very similar events. Comparatively, previous work has
explicit access to, and relies strongly on clues such as the same or very similar
timestamps being associated to two news article documents [11], or knowledge
that two documents are translations by different people of the same book into
the same language [13].
3 Mining the Web for Paraphrases
The use of the Web as input data source strongly impacts the design of the
method, since the average Web document is much noisier and less reliable than
documents in standard textual collections. Furthermore, the separation of useful
textual information from other items within the document is trivial in standard
122 M. Pas?ca and P. Dienes
collections. In contrast, Web documents contain extraneous html information,
formatting errors, intra- and inter-document inconsistencies, spam and other
adversarial information, and in general they lack any assumptions regarding a
common document structure. Consequently, the acquisition of paraphrases must
be robust, handle Web documents with only minimal linguistic processing, avoid
expensive operations, and scale to billions of sentences.
3.1 Document Pre-processing
As a pre-requisite to the actual acquisition of paraphrases, the Web documents
are converted from raw string representations into more meaningful linguistic
units. After filtering out html tags, the documents are tokenized, split into
sentences and part-of-speech tagged with the TnT tagger [14]. Many of the
candidate sentences are in fact random noise caused by the inconsistent structure
(or complete lack thereof) of Web documents, among other factors. To improve
the quality of the data, sentences are retained for further processing only if
they satisfy the following lightweight sanity checks: 1) they are reasonably sized:
sentences containing less than 5 words or more than 30 words are discarded; 2)
they contain at least one verb that is neither a gerund nor a modal verb; 3) they
contain at least one non-verbal word starting in lower-case; 4) none of the words
is longer than 30 characters; and 5) less than half of the words are numbers.
Since the experiments use a collection of English documents, these checks are
geared towards English.
3.2 Acquisition via Text Fragment Alignment
At Web scale, the number of sentences that pass the fairly aggressive sanity
checks during document pre-processing is still extremely large, easily exceed-
ing one billion. Any brute-force alignment of all pairs of document sentences is
therefore unfeasible. Instead, the acquisition of paraphrases operates at the level
of text fragments (ngrams) as shown in Figure 2.
The extraction algorithm roughly consists of the following three phases:
? Generate candidate ngrams from all sentences (steps 1 through 5 in Figure 2);
? Convert each ngram into a ready-to-align pair of a variable fragment (a
candidate paraphrase) and a constant textual anchor (steps 6 through 13);
? Group the pairs with the same anchors; collect the variable fragments within
each group of pairs as potential paraphrases of one another (steps 14 to 20).
The algorithm starts with the generation of candidate ngrams, by collecting
all possible ngrams such that their length varies within pre-defined boundaries.
More precisely, an ngram starts and ends in a fixed number of words (LC);
the count of the additional (ngram) words in-between varies within pre-defined
limits (MinP and MaxP , respectively).
The concatenation of the fixed-length left (CstL) and right (CstR) extremi-
ties of the ngram forms a textual anchor for the variable fragment (V ar) in the
middle. The variable fragment becomes a potential candidate for a paraphrase:
Aligning Needles in a Haystack 123
Input: 6 For each ngram Ni in {N}
{S} set of sentences 7 LNi = length of Ni
LC length of constant extremities 8 CstL| = subseq [0, LC -1] of Ni
MinP , MaxP paraphrase length bounds 9 CstR = subseq [LNiLC , LNi -1] of Ni
Vars: 10 V ari = subseq [LC , LNi -LC-1] of Ni
{N} set of ngrams with attached info 11 Anchori = concat of CstL| and CstR
{P} set of pairs (anchor, candidate) 12 Anchori = concat of Atti and Anchori
{R} set of paraphrase pairs with freq info 13 Insert pair (Anchori,V ari) into {P}
Output: {R} 14 Sort pairs in {P} based on their anchor
Steps: 15 For each {Pi} ? {P} with same anchor
1 {R} = {N} = {P} = empty set; 16 For all item pairs Pi1 and Pi2 in {Pi}
2 For each sentence Si in {S} 17 V ari1 = variable part of pair Pi1
3 Generate ngrams Nij between length 18 V ari2 = variable part of pair Pi2
2 ? LC + MinP and 2 ? LC + MaxP 19 Incr. count of (V ari1 ,V ari2) in {R}
4 For each Nij , attach addtl. info Attij 20 Incr. count of (V ari2 ,V ari1) in {R}
5 Insert Nij with Attij into {N} 21 Return {R}
Fig. 2. Algorithm for paraphrase acquisition from Web document sentences
(S1) Together they form the Platte River ,which eventually
? ?? ?
CstL
flows
? ?? ?
V ar
into the Gulf
? ?? ?
CstR
of Mexico.
Whenever the anchors of two or more ngrams are the same, their variable frag-
ments are considered to be potential paraphrases of each other, thus implement-
ing a const-var-const type of alignment.
3.3 Alignment Anchors
According to the simplified discussion from above, the algorithm in Figure 2 may
align two sentence fragments ?decided to read the government report published
last month? and ?decided to read the edition published last month? to incorrectly
produce government report and edition as potential paraphrases of each other.
To avoid such alignments, Steps 4 and 12 of the algorithm enrich the anchoring
text around each paraphrase candidate, namely by extending the anchors to in-
clude additional information from the source sentence. By doing so, the anchors
become longer and more specific, and thus closer to expressing the same informa-
tion content. In turn, this reduces the chances of any two ngrams to align, since
ngram alignment requires the complete matching of the corresponding anchors.
In other words, the amount of information captured in the anchors is a trade-off
between coverage (when anchors are less specific) and accuracy of the acquired
paraphrases (when the anchors are more specific). At the low end, less specific
anchors include only immediate contextual information. This corresponds to the
algorithm in Figure 2, when nothing is attached to any of the ngrams in Step
4. At the high end, one could collect all the remaining words of the sentence
outside the ngram, and attach them to more specific anchors in Step 4. This is
equivalent to pairwise alignment of full-length sentences.
We explore three different ways of collecting additional anchoring information
from the sentences:
124 M. Pas?ca and P. Dienes
Table 1. Examples of paraphrase pairs collected from the Web with one of Ngram-
Entity or Ngram-Relative, but not with the other
Only with Ngram-Entity Only with Ngram-Relative
abduction, kidnapping abolished, outlawed
bachelor degree, bachelors degree abolished slavery, freed the slaves
cause, result in causes, results in
indicate, specify carries, transmits
inner product space, vector space died from, succumbed to
kill, murder empties into, flows to
obligations, responsibilities funds, pays for
registered service marks, registered trademarks means, stands for
video poker betting, video poker gambling penned, wrote
x-mas gift, x-mas present seized, took over
? Ngram-Only: The anchor includes only the contextual information assembled
from the fixed-length extremities of the ngram. Nothing else is attached to
the anchor.
? Ngram-Entity: In addition to Ngram-Only, the anchor contains the preced-
ing and following named entities that are nearest to the ngram. Sentences
without such named entities are discarded. The intuition is that the ngram
contains information which relates the two entities to each other.
? Ngram-Relative: On top of Ngram-Only, the anchor includes the remain-
ing words of the adverbial relative clause in which the variable part of the
ngram appears, e.g., ?when Soviet Union troops pulled out of Afghanistan?,
or ?which came into force in 2000? in Figure 1. The clause must modify a
named entity or a date, which is also included in the anchor. Sentences not
containing such clauses are rejected. 1 The intuitive motivation in that the
entity is related to part of the ngram via the adverbial particle.
For illustration, consider the earlier example of the sentence S1 from Sec-
tion 3.2. With Ngram-Entity, Platte River (preceding entity) and Mexico (fol-
lowing entity) are included in the anchor. In comparison, with Ngram-Relative
the additional information combines Platte River (entity) and of Mexico (remain-
der of relative clause). In this example, the difference between Ngram-Entity and
Ngram-Relative happens to be quite small. In general, however, the differences
are more significant. Table 1 illustrates paraphrases collected from the Web by
only one of the two anchoring mechanisms.
To ensure robustness on Web document sentences, simple heuristics rather
than complex tools are used to approximate the additional information attached
to ngrams in Ngram-Entity and Ngram-Relative. Named entities are approxi-
mated by proper nouns, as indicated by part-of-speech tags. Adverbial relative
clauses, together with the entities or dates they modify, are detected according
to a small set of lexico-syntactic patterns which can be summarized as:
?[Date|Entity] [,|-|(|nil] [Wh] RelClause [,|-|)|.]?
1 By discarding many sentences, Ngram-Relative sacrifices recall in favor of precision.
Aligning Needles in a Haystack 125
where Wh is one of who, when, which or where. The patterns are based mainly on
wh-words and punctuation. The matching adverbial clause RelClause must sat-
isfy a few other constraints, which aim at avoiding, rather than solving, complex
linguistic phenomena. First, personal and possessive pronouns are often refer-
ences to other entities. Therefore clauses containing such pronouns are discarded
as ambiguous. Second, appositives and other similar pieces of information are
confusing when detecting the end of the current clause. Consequently, during
pattern matching, if the current clause does not contain a verb, the clause is
either extended to the right, or discarded upon reaching the end of the sentence.
4 Evaluation
The input data for paraphrase acquisition is a collection of 972 million Web
documents, from a Web repository snapshot of the Google search engine taken in
2003. All documents are in English. The parameters controlling the length of the
ngrams and candidate paraphrases, introduced in Figure 2, are LC=3, MinP =1
and MaxP =4. 2 The anchors use additional information from the sentences,
resulting in separate runs and sets of paraphrases extracted with Ngram-Only,
Ngram-Entity and Ngram-Relative respectively. The experiments use a parallel
programming model [15]. The extracted paraphrase pairs that co-occur very
infrequently (i.e., in less than 5 unique ngram pairs) are discarded.
4.1 Quantitative Results
The sanity checks applied in document pre-processing (see Section 3.1) discard
a total of 187 billion candidate sentences from the input documents, with an
average of 3 words per sentence. In the case of Ngram-Only, paraphrases are ex-
tracted from the remaining 9.5 billion sentences, which have 17 words on average.
As explained in Section 3.3, Ngram-Entity and Ngram-Relative apply a set of ad-
ditional constraints as they search the sentences for more anchoring information.
Ngram-Entity discards 72 million additional sentences. In contrast, as many as
9.3 billion sentences are rejected by the constraints encoded in Ngram-Relative.
The number of paraphrase pairs extracted from the Web varies with the
particular kind of anchoring mechanism. The simplest one, i.e., Ngram-Only,
produces 41,763,994 unique pairs that co-occur in at least 5 different ngrams.
With Ngram-Relative, the output consists of 13,930 unique pairs. In comparison,
Ngram-Entity generates 101,040 unique pairs. Figure 3 shows that the number
of acquired paraphrases varies more or less linearly in the size of the input data.
The large majority of the paraphrase pairs contain either two single-word
phrases (40% for Ngram-Entity, and 49% for Ngram-Relative), or one single-
word and one multi-word phrase (22% for Ngram-Entity, and 43% for Ngram-
Relative). Table 2 illustrates the top paraphrase pairs with two multi-word
phrases, after removal of paraphrases containing only stop words, or upper/lower
2 No experiments were performed with higher values for MaxP (to collect longer
paraphrases), or higher/lower values for LC (to use more/less context for alignment).
126 M. Pas?ca and P. Dienes
 0
 40000
 80000
 120000
 10  20  30  40  50  60  70  80  90  100
Co
un
t
Percentage of input data
Paraphrase pairs (Ngram-Entity)
Paraphrase pairs (Ngram-Relative)
Fig. 3. Variation of the number of acquired paraphrase pairs with the input data size
Table 2. Top ranked multi-word paraphrase pairs in decreasing order of frequency of
co-occurrence
# Ngram-Entity Ngram-Relative
1 DVD Movie, VHS Movie became effective, took effect
2 betting is excited, wagering is excited came into force, took effect
3 betting is, wagering is became effective, went into effect
4 betting is excited, gambling is excited became effective, came into force
5 Annual Meeting of, meeting of became effective, came into effect
6 center of, centre of entered into force, took effect
7 betting is, gambling is one hour, two hours
case variation. Top multi-word phrases extracted by Ngram-Relative tend to be
self-contained syntactic units. For instance, entered into force is a verb phrase
in Table 2. In contrast, many of the top paraphrases with Ngram-Entity end in
a linking word, such as the pair (center of, centre of). Note that every time this
pair is extracted, the smaller single-word paraphrase pair that folds the common
linking word into the anchor, e.g., (center, centre), is also extracted.
4.2 Quality of Paraphrases
Table 2 shows that the extracted paraphrases are not equally useful. The pair
(became effective, took effect) is arguably more useful than (one hour, two hours).
Table 3 is a side-by-side comparison of the accuracy of the paraphrases with
Ngram-Only, Ngram-Entity and Ngram-Relative respectively. The values are
the result of manual classification of the top, middle and bottom 100 paraphrase
pairs from each run into 11 categories. The first six categories correspond to
pairs classified as correct. For instance (Univeristy, University) is classified in
class (1); (Treasury, treasury) in (2); (is, are) in (3); (e-mail, email) in (4);
and (can, could) in (5). The pairs in class (6) are considered to be the most
useful; they include (trip, visit), (condition, status), etc. The next three classes
do not contain synonyms but are still useful. The pairs in (7) are siblings rather
than direct synonyms; examples are (twice a year, weekly) and (French, welsh).
Furthermore, modal verbs such as (may, should), numbers, and prepositions like
(up, back) also fall under class (7). Many of the 63 pairs classified as siblings
Aligning Needles in a Haystack 127
Table 3. Quality of the acquired paraphrases
Ngram-Only Ngram-Entity Ngram-Relative
Classification of Pairs Top Mid Low Top Mid Low Top Mid Low
100 100 100 100 100 100 100 100 100
(1) Correct; punct., symbols, spelling 1 5 11 12 6 20 18 11 15
(2) Correct; equal if case-insensitive 0 5 0 27 2 11 9 2 14
(3) Correct; both are stop words 4 0 0 3 0 1 1 0 0
(4) Correct; hyphenation 0 1 4 10 35 8 2 19 43
(5) Correct; morphological variation 8 1 10 9 10 20 20 15 6
(6) Correct; synonyms 16 8 21 5 32 14 33 23 6
Total correct 29 20 46 66 85 74 83 70 84
(7) Siblings rather than synonyms 63 29 19 32 8 15 5 7 7
(8) One side adds an elaboration 0 0 3 0 0 0 1 2 1
(9) Entailment 0 3 2 0 0 1 3 1 0
Total siblings 63 32 24 32 8 16 9 10 8
(10) Incorrect; antonyms 6 0 2 0 1 4 4 3 4
(11) Incorrect; other 2 48 28 2 6 6 4 17 4
Total incorrect 8 48 30 2 7 10 8 20 8
with Ngram-Only in Table 3 are precisely such words. Class (8) contains pairs
in which a portion of one of the elements is a synonym or phrasal equivalent
of the other element, such as (poliomyelitis globally, polio) and (UNC, UNC-
CH), whereas (9) captures what can be thought of as entailment, e.g., (governs,
owns) and (holds, won). Finally, the last two classes from Table 3 correspond to
incorrect extractions, due to either antonyms like (lost, won) and (your greatest
strength, your greatest weakness) in class (10), or other factors in (11).
The aggregated evaluation results, shown in bold in Table 3, suggest that
Ngram-Only leads to paraphrases of lower quality than those extracted with
Ngram-Entity and Ngram-Relative. In particular, the samples from the middle
and bottom of the Ngram-Only paraphrases contain a much higher percentage
of incorrect pairs. The results also show that, for Ngram-Entity and Ngram-
Relative, the quality of paraphrases is similar at different ranks in the paraphrase
lists sorted by the number of different ngrams they co-occur in. For instance, the
total number of correct pairs has comparable values for the top, middle and bot-
tom pairs. This confirms the usefulness of the heuristics introduced in Section 3.3
to discard irrelevant sentences with Ngram-Entity and Ngram-Relative.
4.3 Paraphrases in Natural-Language Web Search
The usefulness of paraphrases in Web search is assessed via an existing experi-
mental repository of more than 8 million factual nuggets associated with a date.
Repositories of factual nuggets are built offline, by matching lightweight, open-
domain lexico-semantic patterns on unstructured text. In the repository used in
this paper, a factual nugget is a sentence fragment from a Web document, paired
with a date extracted from the same document, when the event encoded in the
128 M. Pas?ca and P. Dienes
Table 4. Impact of expansion of the test queries (QH/QL=count of queries with
higher/lower scores than without expansion, NE=Ngram-Entity, NR=Ngram-Relative)
Max. nr. disjunctions QH QL Score
per expanded phrase NE NR NE NR NE NR
1 (no paraphrases) 0 0 0 0 52.70 52.70
2 (1 paraphrase) 17 8 7 6 64.50 57.62
3 (2 paraphrases) 22 13 6 9 70.38 60.46
4 (3 paraphrases) 23 15 6 7 71.42 60.39
5 (4 paraphrases) 26 18 12 5 71.73 63.35
sentence fragment occurred according to the text, e.g., ?1937, Golden Gate was
built?, and ?1947, Bell Labs invented the transistor?.
A test set of temporal queries is used to extract direct results (dates) from
the repository of factual nuggets, by matching the queries against the sentence
fragments, and retrieving the associated dates. The test queries are all queries
that start with either When or What year, namely 207 out of the total count of
1893 main-task queries, from the Question Answering track [16] of past editions
(1999 through 2002). The metric for measuring the accuracy of the retrieved
results is the de-facto scoring metric for fact-seeking queries, that is, the recip-
rocal rank of the first returned result that is correct (in the gold standard) [16].
If there is no correct result among the top 10 returned, the query receives no
credit. Individual scores are aggregated (i.e., summed) over the entire query set.
In a series of parallel experiments, all phrases from the test queries are
expanded into Boolean disjunctions with their top-ranked paraphrases. Query
words with no paraphrase are placed into the expanded queries in their origi-
nal form. The other query words are expanded only if they are single words, for
simplicity. Examples of implicitly-Boolean queries expanded disjunctively, before
removal of stop words and wh-words, are:
? When did Amtrak (begin | start | began | continue | commence) (operations
| operation | activities | Business | operational)?
? When was the De Beers (company | Co. | firm | Corporation | group) (founded
| established | started | created | co-founded)?
Table 4 illustrates the impact of paraphrases on the accuracy of the dates
retrieved from the repository of factual nuggets associated with dates. When
compared to non-expanded queries, paraphrases consistently improve the accu-
racy of the returned dates. Incremental addition of more paraphrases results in
more individual queries with a better score than for their non-expanded ver-
sion, and higher overall scores for the returned dates. The paraphrases extracted
with Ngram-Entity produce scores that are higher than those of Ngram-Relative,
due mainly to higher coverage. Since the temporal queries represent an exter-
nal, objective test set, they provide additional evidence regarding the quality of
paraphrases in a practical application.
Aligning Needles in a Haystack 129
5 Conclusion
The Web has gradually grown into a noisy, unreliable, yet powerful resource of
human knowledge. This knowledge ranges from basic word usage statistics to in-
tricate facts, background knowledge and associated inferences made by humans
reading Web documents. This paper describes a method for unsupervised acqui-
sition of lexical knowledge across the Web, by exploiting the numerous textual
forms that people use to share similar ideas, or refer to common events. Large
sets of paraphrases are collected through pairwise alignment of ngrams occur-
ring within the unstructured text of Web documents. Several mechanisms are
explored to cope with the inherent lack of quality of Web content. The quality of
the extracted paraphrases improves significantly when the textual anchors used
for aligning potential paraphrases attempt to approximate, even at a very coarse
level, the presence of additional information within the sentences. In addition
to the known role of the extracted paraphrases in natural-language intensive
applications, the experiments in this paper illustrate their impact in returning
direct results to natural-language queries.
The final output of the extraction algorithm lacks any distinction among
paraphrases that apply to only one of the several senses or part of speech tags
that a word or phrase may have. For instance, hearts, center and middle mix
the medical and positioning senses of the word heart. Conversely, the extracted
paraphrases may capture only one sense of the word, which may not match
the sense of the same word in the queries. As an example, in the expansion of
one of the test queries, ?Where is the massive North Korean (nuclear|atomic)
(complex|real) (located|situated|found)??, a less-than-optimal paraphrase of com-
plex not only provides a sibling rather than a near synonym, but may incorrectly
shift the focus of the search towards the mathematical sense of the word (com-
plex versus real numbers). Aggregated contextual information from the source
ngrams could provide a means for selecting only some of the paraphrases, based
on the query. As another direction for future work, we plan to revise the need
for language-dependent resources (namely, the part of speech tagger) in the cur-
rent approach, and explore possibilities of minimizing or removing their use for
seamless transfer of the approach to other languages.
References
1. Hirao, T., Fukusima, T., Okumura, M., Nobata, C., Nanba, H.: Corpus and eval-
uation measures for multiple document summarization with multiple sources. In:
Proceedings of the 20th International Conference on Computational Linguistics
(COLING-04), Geneva, Switzerland (2004) 535?541
2. Shinyama, Y., Sekine, S.: Paraphrase acquisition for information extraction. In:
Proceedings of the 41st Annual Meeting of the Association of Computational Lin-
guistics (ACL-03), 2nd Workshop on Paraphrasing: Paraphrase Acquisition and
Applications, Sapporo, Japan (2003) 65?71
3. Pas?ca, M.: Open-Domain Question Answering from Large Text Collections. CSLI
Studies in Computational Linguistics. CSLI Publications, Distributed by the Uni-
versity of Chicago Press, Stanford, California (2003)
130 M. Pas?ca and P. Dienes
4. Mitra, M., Singhal, A., Buckley, C.: Improving automatic query expansion. In:
Proceedings of the 21st ACM Conference on Research and Development in Infor-
mation Retrieval (SIGIR-98), Melbourne, Australia (1998) 206?214
5. Schutze, H., Pedersen, J.: Information retrieval based on word senses. In: Pro-
ceedings of the 4th Annual Symposium on Document Analysis and Information
Retrieval. (1995) 161?175
6. Zukerman, I., Raskutti, B.: Lexical query paraphrasing for document retrieval. In:
Proceedings of the 19th International Conference on Computational Linguistics
(COLING-02), Taipei, Taiwan (2002) 1177?1183
7. Barzilay, R., Lee, L.: Learning to paraphrase: An unsupervised approach using
multiple-sequence alignment. In: Proceedings of the 2003 Human Language Tech-
nology Conference (HLT-NAACL-03), Edmonton, Canada (2003) 16?23
8. Jacquemin, C., Klavans, J., Tzoukermann, E.: Expansion of multi-word terms
for indexing and retrieval using morphology and syntax. In: Proceedings of the
35th Annual Meeting of the Association of Computational Linguistics (ACL-97),
Madrid, Spain (1997) 24?31
9. Glickman, O., Dagan, I.: Acquiring Lexical Paraphrases from a Single Corpus. In:
Recent Advances in Natural Language Processing III. John Benjamins Publishing,
Amsterdam, Netherlands (2004) 81?90
10. Duclaye, F., Yvon, F., Collin, O.: Using the Web as a linguistic resource for learning
reformulations automatically. In: Proceedings of the 3rd Conference on Language
Resources and Evaluation (LREC-02), Las Palmas, Spain (2002) 390?396
11. Shinyama, Y., Sekine, S., Sudo, K., Grishman, R.: Automatic paraphrase acqui-
sition from news articles. In: Proceedings of the Human Language Technology
Conference (HLT-02), San Diego, California (2002) 40?46
12. Dolan, W., Quirk, C., Brockett, C.: Unsupervised construction of large para-
phrase corpora: Exploiting massively parallel news sources. In: Proceedings of
the 20th International Conference on Computational Linguistics (COLING-04),
Geneva, Switzerland (2004) 350?356
13. Barzilay, R., McKeown, K.: Extracting paraphrases from a parallel corpus. In:
Proceedings of the 39th Annual Meeting of the Association for Computational
Linguistics (ACL-01), Toulouse, France (2001) 50?57
14. Brants, T.: TnT - a statistical part of speech tagger. In: Proceedings of the 6th
Conference on Applied Natural Language Processing (ANLP-00), Seattle, Wash-
ington (2000) 224?231
15. Dean, J., Ghemawat, S.: MapReduce: Simplified data processing on large clus-
ters. In: Proceedings of the 6th Symposium on Operating Systems Design and
Implementation (OSID-04), San Francisco, California (2004) 137?150
16. Voorhees, E., Tice, D.: Building a question-answering test collection. In: Pro-
ceedings of the 23rd International Conference on Research and Development in
Information Retrieval (SIGIR-00), Athens, Greece (2000) 200?207
Answering Denition Questions via Temporally-Anchored Text Snippets
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
A lightweight extraction method derives text
snippets associated to dates from the Web.
The snippets are organized dynamically into
answers to definition questions. Experi-
ments on standard test question sets show
that temporally-anchored text snippets allow
for efficiently answering definition ques-
tions at accuracy levels comparable to the
best systems, without any need for complex
lexical resources, or specialized processing
modules dedicated to finding definitions.
1 Introduction
In the field of automated question answering (QA),
a variety of information sources and multiple extrac-
tion techniques can all contribute to producing rele-
vant answers in response to natural-language ques-
tions submitted by users. Yet the nature of the infor-
mation source which is mined for answers, together
with the scope of the questions, have the most sig-
nificant impact on the overall architecture of a QA
system. When compared to the average queries sub-
mitted in a decentralized information seeking envi-
ronment such as Web search, fact-seeking questions
tend to specify better the nature of the information
being sought by the user, whether it is the name of
the longest river in some country, or the name of
the general who defeated the Spanish Armada. In
order to understand the structure and the linguistic
clues encoded in natural-language questions, many
QA systems employ sophisticated techniques, thus
deriving useful information such as terms, relations
among terms, the type of the expected answers (e.g.,
cities vs. countries vs. presidential candidates), and
other semantic constraints (e.g., the elections from
1978 rather than any other year).
One class of questions whose characteristics place
them closer to exploratory queries, rather than stan-
dard fact-seeking questions, are definition questions.
Seeking information about an entity or a concept,
questions such as ?Who is Caetano Veloso?? of-
fer little guidance as to what particular techniques
could be used in order to return relevant information
from a large text collection. In fact, the same user
may choose to submit a definition question or a sim-
pler exploratory query (Caetano Veloso), and still
look for text snippets capturing relevant properties
of the question concept. Various studies (Chen et al,
2006; Han et al, 2006) illustrate the challenges in-
troduced by definition questions. As such questions
have a less irregular form than other open-domain
questions, recognizing their type is relatively eas-
ier (Hildebrandt et al, 2004). Conversely, the iden-
tification of relevant documents and the extraction
of answers to definition questions are more labori-
ous, and the impact on the architecture of QA sys-
tems is quite significant. Indeed, separate, dedicated
modules, or even end-to-end systems are specifi-
cally built for answering definition questions (Kla-
vans and Mures?an, 2001; Hildebrandt et al, 2004;
Greenwood and Saggion, 2004). The importance of
definition questions among other question categories
is confirmed by their inclusion among the evalua-
tion queries from the QA track of TREC evalua-
tions (Voorhees and Tice, 2000).
This paper investigates the impact of temporally-
411
anchored text snippets derived from the Web, in
answering definition questions and, more gener-
ally, exploratory queries. Section 2 describes a
lightweight mechanism for extracting text snippets
and associated dates from sentences in Web docu-
ments. Section 3 assesses the coverage of the ex-
tracted snippets. As shown in Section 4, relevant
events, in which the question concept was involved,
can be captured by matching the queries on the text
snippets, and organizing the snippets around the as-
sociated dates. Section 5 describes discusses the role
of the extracted text snippets in answering two sets
of definition questions.
2 Temporally Anchored Text Snippets
All experiments rely on the unstructured text in ap-
proximately one billion documents in English from a
2003 Web repository snapshot of the Google search
engine. Pre-processing of the documents consists
in HTML tag removal, simplified sentence bound-
ary detection, tokenization and part-of-speech tag-
ging with the TnT tagger (Brants, 2000). No other
tools or lexical resources are employed.
A sequence of sentence tokens represents a po-
tential date if it consists of: single year (four-digit
numbers, e.g., 1929); or simple decade (e.g., 1930s);
or month name and year (e.g., January 1929); or
month name, day number and year (e.g., January
15, 1929). Dates occurring in text in any other for-
mat are ignored. To avoid spurious matches, such
as 1929 people, potential dates are discarded if they
are immediately followed by a noun or noun modi-
fier, or immediately preceded by a noun.
To convert document sentences into a few text
snippets associated with dates, the overall structure
of sentences is roughly approximated. Deep text
analysis may be desirable but simply not feasible on
the Web. As a lightweight alternative, the proposed
extraction method approximates the occurrence and
boundaries of text snippets through the following set
of lexico-syntactic patterns:
(P1): ?Date [,|-|(|nil] [when] Snippet [,|-|)|.]?
(P2): ?[StartSent] [In|On] Date [,|-|(|nil] Snippet [,|-|)|.]?
(P3): ?[StartSent] Snippet [in|on] Date [EndSent]?
(P4): ?[Verb] [OptionalAdverb] [in|on] Date?
The first extraction pattern, P1, targets sentences
with adverbial relative clauses introduced by wh-
adverbs and preceded by a date, e.g.:
?By [Date 1910], when [Snippet Korea was an-
nexed to Japan], the Korean population in America
had grown to 5,008?.
Comparatively, P2 and P3 match sentences that
start or end in a simple adverbial phrase containing
a date. In the case of P4, the occurrence of rele-
vant dates within sentences is approximated by verbs
followed by a simple adverbial phrase containing a
date. P4 marks the entire sentence as a potential
nugget because it lacks the punctuation clues in the
other three patterns.
The patterns must satisfy additional constraints in
order to match a sentence. These constraints con-
stitute heuristics to avoid, rather than solve, com-
plex linguistic phenomena. Thus, a nugget is always
discarded if it does not contain a verb, or contains
any pronoun. Furthermore, the snippets in P2 and
P3 must start with, and the nugget in P4 must con-
tain a noun phrase, which in turn is approximated by
the occurrence of a noun, adjective or determiner.
The combination of patterns and constraints is by
no means definitive or error-free. It is a practical
solution to achieve graceful degradation on large
amounts of data, reduce the extraction errors, and
improve the usefulness of the extracted snippets. As
such, it emphasizes robustness at Web scale, without
taking advantage of existing specification languages
for representing events and temporal expressions oc-
curring in text (Pustejovsky et al, 2003), and forgo-
ing the potential benefits of more complex methods
that extract temporal relations from relatively clean
text collections (Mani et al, 2006).
3 Coverage of Text Snippets
A concept such as a particular actor, country or or-
ganization usually occurs within more than one of
the extracted text snippets. In fact, the set of text
snippets containing the concept, together with the
associated dates, often represents an extract-based,
simple temporal summary of the events in which the
concept has been involved. Starting from this ob-
servation, a task-based evaluation of the coverage
of the extracted text snippets consists in verifying
to what extent they capture the condensed history
of several countries. Since any country must have
been involved in some historical timeline of events,
a reference timeline is readily available in an exter-
412
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
G
am
bi
a
Bu
ru
nd
i
Co
m
or
os
D
jib
ou
ti
Er
itr
ea
Et
hi
op
ia
Ke
ny
a
R
w
an
da
Se
yc
he
lle
s
So
m
al
ia
Ta
nz
an
ia
Ug
an
da
Ca
m
er
oo
n
Eq
ua
to
ria
l G
ui
ne
a
G
ab
on
Al
ge
ria
Ce
ut
a
Eg
yp
t
Li
by
a
M
el
illa
M
or
oc
co
Su
da
n
Tu
ni
si
a
W
es
te
rn
 S
ah
ar
a
Co
un
t/P
er
ce
nt
ag
e
Total reference snippets (count)
Matched reference snippets (percentage)
Figure 1: Percentage of reference snippets with corresponding extracted snippets
nal resource, e.g., encyclopedia, as an excerpt cov-
ering a condensed history of the country. The refer-
ence timeline is compared against the text snippets
containing a country such as Ethiopia. To this ef-
fect, the text snippets containing a given country as
a phrase are retained, ordered in increasing order of
their associated dates, and evaluated against the ref-
erence timeline.
Both the test set of countries and the gold stan-
dard are collected from Wikipedia (Remy, 2002).
The test set comprises countries from Africa. Since
African countries have fewer extracted snippets than
other countries, the evaluation results provide more
useful, lower bounds rather than average or best-
case. Due to limited human resources available for
this evaluation, the test countries are a subset of the
African countries in Wikipedia, selected in the order
in which they are listed on the site. They cover all
Eastern, Central and Northern Africa. The Central
African Republic, the Republic of the Congo, and
Sao Tome and Principe are discarded and Gambia
added, leading to a test set of 24 country names. The
source of the reference timelines is the condensed
history article that is part of the main description
page of each country in Wikipedia.
The evaluation procedure is concerned only with
recall, but is still highly subjective. It requires
the manual division of the reference text into dated
events. In addition, the assessor must decide which
details surrounding an event are significant, and
must be matched into the extracted snippets in order
to get any credit. The actual evaluation consists in
matching each dated event from the reference time-
line into the extracted timeline. During matching,
the extracted snippets are analyzed by hand to decide
which snippets, if any, capture the reference event,
significant details around it, and the time stamp.
On average, 1173 text snippets are returned per
country name, with a median of 733 snippets. Fig-
ure 1 summarizes the comparison of reference snip-
pets and extracted snippets. The continuous line
corresponds to the total number of reference snip-
pets that were manually identified in the reference
timeline; Melilla has the smallest such number (2),
whereas Sudan has the largest (24). The dotted
line in Figure 1 represents the percentage of refer-
ence snippets that have at least one match into the
extracted snippets, thus evaluating recall. An av-
erage of 72% of the reference snippets have such
matches. For 5 queries, there are matches for all
reference snippets. The worst case occurs for Equa-
torial Guinea, for which only two out of the 11 ref-
erence snippets can be matched. Based on the re-
sults, we conclude that the text snippets and the as-
sociated dates provide a good coverage in the case
of information about countries. The snippets can be
retrieved as answers to questions asking about dates
(When, What year) as described in (Pas?ca, 2007), or
as answers to definition questions as discussed be-
low.
413
4 Answering Definition Questions
Input definition questions are uniformly handled as
Boolean queries, after the removal of stop words as
well as question-specific terms (Who etc.). Thus,
questions such as ?Who is Caetano Veloso?? and
?Who won the Nobel Peace Prize?? are consistently
converted into conjunctive queries corresponding to
Caetano Veloso and won Nobel Peace Prize respec-
tively. The score assigned to a matching text snippet
is higher, if the snippet occurs in a larger number
of documents. Similarly, the score is higher if the
snippet contains fewer non-stop terms in addition to
the question term matches, or the average distance
in the snippet between pairs of query term matches
is lower. A side effect of the latter heuristic is to
boost the snippets in which the query terms occur as
a phrase, rather than as scattered term matches.
When they are associated to a common date, re-
trieved snippets transfer their relevance score onto
the date, in the form of the sum of the individual
snippet scores. The dates are ranked in decreas-
ing order of their relevance scores, and those with
the highest scores are returned as responses to the
question, together with the top associated snippets.
Within a set of text snippets associated to a date, the
snippets are also ranked relatively to one another,
such that each returned date is accompanied by its
top supporting snippets. The ranking within a set of
snippets associated to a date is a two-pass procedure.
First, the snippets are scanned to count the number
of occurrences of non-stop unigrams within the en-
tire set. Second, a snippet is weighted with respect
to others based on how many of the unigrams it con-
tains, and the individual scores of those unigrams.
In the output, the snippets act as useful, implicit
text-based justifications of why the dates may be
relevant or not. As such, they implement a practi-
cal method of fusing together bits (snippets) of in-
formation collected from unrelated documents. In
some cases, the snippets show why a returned result
(date) is relevant. For example, 1990 is relevant to
the query Germany unied because ?East and West
Germany were unied? according to the top snippet.
In other cases, the text snippets quickly reveal why
the result is related to the query even though it may
not match the original user?s intent. For instance, a
user may ask the question ?When was the Taj Mahal
built?? with the well-known monument in mind, in
which case the irrelevance of the date 1903 is self-
explanatory based on one of its supporting snippets,
?the lavish Taj Mahal Hotel was built?.
5 Evaluation
The answers returned by the system are ranked in
decreasing order of their scores. By convention, an
answer to a definition question comprises a returned
date, plus the top matching text snippets that pro-
vide support for that date. Ideally, a snippet should
only contain the desired answer and nothing else. In
practice, a snippet is deemed correct if it contains
the ideal answer, although it may contain some other
extraneous information.
5.1 Objective Evaluation
A thorough evaluation of answers to definition ques-
tions would be complex, prone to subjective as-
sessments, and would involve significant human la-
bor (Voorhees, 2003). Therefore, the quality of the
text snippets in the context of definition questions
is tested on a set, DefQa1, containing the 23 ?Who
is/was [ProperName]?? questions from the TREC
QA track from 1999 through 2002. In this case, each
returned answer consists of a date and the first sup-
porting text snippet.
Table 1 contains a sample of the test questions.
The right column shows actual text snippets re-
trieved for the definition questions, together with the
associated date and the rank of that date within the
output. In an objective evaluation strictly based on
the answer keys of the gold standard, the MRR score
over the DefQa1 set is 0.596. The score is quite high,
given that the answer keys prefer the genus of the
question concept, rather than other types of infor-
mation. For instance, the answer keys for the TREC
questions Q222:?Who is Anubis?? and Q253:?Who
is William Wordsworth?? mark poet and ?Egyptian
god? as correct answers respectively, thus empha-
sizing the genus of the question concepts Anubis
and William Wordsworth. This explains the strong
reliance in previous work on hand-written patterns
and dictionary-based techniques for detecting text
fragments encoding the genus and differentia of the
question concept (Lin, 2002; Xu et al, 2004).
414
Question (Rank) Relevant Date: Associated Fact
Q218: Who was (1) 1893: First patented in 1893 by Whitcomb Judson, the Clasp Locker was notoriously unreliable
Whitcomb Judson? and expensive
(2) 1891: the zipper was invented by Whitcomb Judson
Q239: Who is (1) February 21 1936: Barbara Jordan was born in Houston, Texas
Barbara Jordan? (2) January 17 1996: Barbara Jordan died in Austin, Texas, at the age of 59
(4) 1973: Barbara Jordan was diagnosed with multiple sclerosis and was confined to a wheelchair
(5) 1976: Barbara Jordan became the first African-American Woman to deliver a keynote address at
a political convention
(7) 1966: Barbara Jordan became the first black representative since 1883 to win an election to
the Texas legislature
(8) 1972: Barbara Jordan was elected to the US Congress
Q253: Who is (1) 1770: William Wordsworth was born in 1770 in the town of Cockermouth, England
William Wordsworth? (2) April 7 1770: William Wordsworth was born
(4) 1798: Romanticism officially began, when William Wordsworth and Samuel Taylor Coleridge
anonymously published Lyrical Ballads
(5) 1802: William Wordsworth married Mary Hutchinson at Brompton church
(7) 1795: Coleridge met the poet William Wordsworth
(8) April 23 1850: William Wordsworth died
(11) 1843: William Wordsworth (1770-1850) was made Poet Laureate of Britain
Q346: Who is (1) 1902: Langston Hughes was born in Joplin, Missouri
Langston Hughes? (2) May 22 1967: Langston Hughes died of cancer
(5) 1994: The Collected Poems of Langston Hughes was published
Q351: Who is (1) 1927: aviation hero Charles Lindbergh was honored with a ticker-tape parade in New York City
Charles Lindbergh? (2) 1932: Charles Lindbergh?s infant son was kidnapped and murdered
(3) February 4 1902: Charles Lindbergh was born in Detroit
(5) August 26 1974: Charles Lindbergh died
(7) May 21 1927: Charles Lindbergh landed in Paris
(8) May 20 1927: Charles Lindbergh took off from Long Island
(9) May 1927: an airmail pilot named Charles Lindbergh made the first solo flight across the Atlantic
Ocean
Q419: Who was (1) 1977: Goodall founded the Jane Goodall Institute for Wildlife Research
Jane Goodall? (2) April 3 1934: Jane Goodall was born in London, England
(3) 1960: Dr Jane Goodall began studying chimpanzees in east Africa
(8) 1985: Jane Goodall ?s twenty-five years of anthropological and conservation research was
published
Table 1: Temporally-anchored text snippets returned as answers to definition questions
5.2 Subjective Evaluation
Beyond the snippets that happen to contain the genus
of the question concept, the output constitutes sup-
plemental results to what other definition QA sys-
tems may offer. The intuition is that prominent facts
associated with the question concept provide use-
ful, if not direct answers to the corresponding def-
inition question, with the twist of presenting them
together with the associated date. For instance, the
first answer to Q239:?Who is Barbara Jordan?? re-
veals her date of birth and is associated with the
first retrieved date, February 21 1936. In the objec-
tive evaluation, this answer is marked as incorrect.
However, some users may find this snippet useful,
although they may still prefer the seventh or eighth
text snippets from Table 1 as primary answers, as
they mention Barbara Jordan?s election to a state
legislature in 1966, and to the Congress in 1972. As
an alternative evaluation, the top five matching snip-
pets for each of the top ten dates are inspected man-
ually, and answers such as the birth year of a person
are subjectively marked as correct. Overall, 59.1%
of the snippets returned for the DefQa1 questions are
deemed correct, which shows that the answers cap-
ture useful properties of the question concepts.
5.3 Alternative Objective Evaluation
A separate objective evaluation was conducted on
a set, DefQa2, containing the 24 definition ques-
tions asking for information about various people,
from the TREC QA track from 2004. Although cor-
rectness assessments are still subjective, they bene-
fit from a more rigorous evaluation procedure. For
each question, the gold standard consists of sets of
responses classified according to their importance
into two classes, namely vital nuggets, containing
415
information that the assessors feel must be returned
for the overall output to be good, and non-vital, con-
taining information that is acceptable in the output
but not necessary.
Following the official 2004 evaluation proce-
dure (Voorhees, 2004), a returned text snippet is
considered vital, non-vital, or incorrect based on
whether it conceptually matches a vital, non-vital
answer, or none of the answers specified in the gold
standard for that question. The overall recall is
the average of individual recall values per question,
which are computed as the number of returned vi-
tal answers, divided by the number of vital answers
from the gold standard for a given question. In this
case, a returned answer is formed by a date and
its top three associated text snippets. If a vital an-
swer from the gold standard matches any of the three
snippets of a returned answer, then the returned an-
swer is vital.
The overall recall value over DefQa2 is 0.46. The
corresponding F-measure, which gives three times
more importance to recall than to precision as speci-
fied in the official evaluation procedure, is 0.39. The
score measures favorably against the top three F-
measure scores of 0.46, 0.40, and 0.37 reported in
the official 2004 evaluation (Voorhees, 2004). The
two better scores were obtained by systems that rely
extensively on human-generated knowledge from
resources such as WordNet (Zhang et al, 2005) and
specific Web glossaries (Cui et al, 2007). In com-
parison, the text snippets retrieved in this paper pro-
vide relevant answers to definition questions with
the added benefit of providing a temporal anchor
for each answer, and without using any complex lin-
guistic resources and tools.
The scores per question vary widely, with the re-
trieved snippets containing none of the vital answers
for six questions, all vital answers for other six, and
some fraction of the vital answers for the remain-
ing questions. For example, one of the retrieved
text snippets is ?US Air Force Colonel Eileen Marie
Collins was the rst woman to command a space
shuttle mission?. The snippet is classified as vital
for the question about Eileen Marie Collins, since it
conceptually matches a vital answer from the gold
standard, namely ?rst woman space shuttle com-
mander?. Again, even though the standard evalua-
tion does not require a temporal anchor for an an-
swer to be correct, we feel that the dates associated
to the retrieved snippets provide very useful, addi-
tional, condensed information. In the case of Eileen
Marie Collins, the above-mentioned vital answer is
accompanied by the date 1999, when the mission
took place.
6 Related Work
Previous approaches to answering definition ques-
tions from large text collections can be classified
according to the kind of techniques for the extrac-
tion of answers. A significant body of work is ori-
ented towards mining descriptive phrases or sen-
tences, as opposed to other types of semantic in-
formation, for the given question concepts. To this
effect, the use of hand-written lexico-syntactic pat-
terns and regular expressions, targeting the genus
and possibly the differentia of the question concept,
is widespread, whether employed for mining defini-
tions in English (Liu et al, 2003; Hildebrandt et al,
2004) or other languages such as Japanese (Fujii and
Ishikawa, 2004), from local text collections (Xu et
al., 2004) or from the Web (Blair-Goldensohn et al,
2004; Androutsopoulos and Galanis, 2005). Com-
paratively, the small set of patterns used here targets
text snippets that are temporally-anchored. There-
fore the text snippets provide answers to definition
answers without actually employing any specialized
module for seeking specific information such as the
genus of the question concept.
Several studies propose unsupervised extraction
methods as an alternative to using hand-written pat-
terns for definition questions (Androutsopoulos and
Galanis, 2005; Cui et al, 2007). Previous work
often relies on external resources as an important
or even essential guide towards the desired out-
put. Such resources include WordNet (Prager et
al., 2001) for finding the genus of the question con-
cept; large dictionaries such as Merriam Webster,
for ready-to-use definitions (Xu et al, 2004; Hilde-
brandt et al, 2004); and encyclopedias, for collect-
ing words that are likely to occur in potential defini-
tions (Fujii and Ishikawa, 2004; Xu et al, 2004). In
comparison, the experiments reported in this paper
do not require any external lexical resource.
416
7 Conclusion
Without specifically targeting definitions,
temporally-anchored text snippets extracted from
the Web provide very useful answers to definition
questions, as measured on standard test question
sets. Since the snippets tend to capture important
events involving the question concepts, rather than
phrases that describe the question concept, they
can be employed as either standalone answers, or
supplemental results in conjunction with answers
extracted with other techniques.
References
I. Androutsopoulos and D. Galanis. 2005. A practically unsu-
pervised learning method to identify single-snippet answers
to definition questions on the Web. In Proceedings of the
Human Language Technology Conference (HLT-EMNLP-
05), pages 323?330, Vancouver, Canada.
S. Blair-Goldensohn, K. McKeown, and A. Schlaikjer, 2004.
New Directions in Question Answering, chapter Answering
Definitional Questions: a Hybrid Approach, pages 47?58.
MIT Press, Cambridge, Massachusetts.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natural
Language Processing (ANLP-00), pages 224?231, Seattle,
Washington.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers
for definitional QA using language modeling. In Proceed-
ings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL-06), pages 1081?
1088, Sydney, Australia.
H. Cui, M. Kan, and T. Chua. 2007. Soft pattern matching
models for definitional question answering. ACM Transac-
tions on Information Systems, 25(2).
A. Fujii and T. Ishikawa. 2004. Summarizing encyclope-
dic term descriptions on the Web. In Proceedings of the
20th International Conference on Computational Linguistics
(COLING-04), pages 645?651, Geneva, Switzerland.
M. Greenwood and H. Saggion. 2004. A pattern based ap-
proach to answering factoid, list and definition questions.
In Proceedings of the 7th Content-Based Multimedia Infor-
mation Access Conference (RIAO-04), pages 232?243, Avi-
gnon, France.
K. Han, Y. Song, and H. Rim. 2006. Probabilistic model for
definitional question answering. In Proceedings of the 29th
ACM Conference on Research and Development in Informa-
tion Retrieval (SIGIR-06), pages 212?219, Seattle, Washing-
ton.
W. Hildebrandt, B. Katz, and J. Lin. 2004. Answering defini-
tion questions with multiple knowledge sources. In Proceed-
ings of the 2004 Human Language Technology Conference
(HLT-NAACL-04), pages 49?56, Boston, Massachusetts.
J. Klavans and Smaranda Mures?an. 2001. Evaluation of
Definder: A system to mine definitions from consumer-
oriented medical text. In Proceedings of the 1st ACM/IEEE-
CS Joint Conference on Digital Libraries (JCDL-01), pages
201?203, Roanoke, Virginia.
C.Y. Lin. 2002. The effectiveness of dictionary and web-
based answer reranking. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics (COLING-
02), pages 1?7, Taipei, Taiwan.
B. Liu, C. Chin, and H.T. Ng. 2003. Mining topic-specific
concepts and definitions on the Web. In Proceedings of the
12th International World Wide Web Conference (WWW-03),
pages 251?260, Budapest, Hungary.
I. Mani, M. Verhagen, B. Wellner, C. Lee, and J. Pustejovsky.
2006. Machine learning of temporal relations. In Proceed-
ings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL-06), pages 753?
760, Sydney, Australia.
M. Pas?ca. 2007. Lightweight Web-based fact repositories
for textual question answering. In Proceedings of the 16th
ACM Conference on Information and Knowledge Manage-
ment (CIKM-07), Lisboa, Portugal.
J. Prager, D. Radev, and K. Czuba. 2001. Answering what-is
questions by virtual annotation. In Proceedings of the 1st
Human Language Technology Conference (HLT-01), pages
1?5, San Diego, California.
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gaizauskas,
A. Setzer, and G. Katz. 2003. TimeML: Robust specifica-
tion of event and temporal expressions in text. In Proceed-
ings of the 5th International Workshop on Computational Se-
mantics (IWCS-5), Tilburg, Netherlands.
M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.
E.M. Voorhees and D.M. Tice. 2000. Building a question-
answering test collection. In Proceedings of the 23rd In-
ternational Conference on Research and Development in
Information Retrieval (SIGIR-00), pages 200?207, Athens,
Greece.
E. Voorhees. 2003. Evaluating answers to definition questions.
In Proceedings of the 2003 Human Language Technology
Conference (HLT-NAACL-03), pages 109?111, Edmonton,
Canada.
E.M. Voorhees. 2004. Overview of the TREC-2004 Question
Answering track. In Proceedings of the 13th Text REtrieval
Conference (TREC-8), Gaithersburg, Maryland. NIST.
J. Xu, R. Weischedel, and A. Licuanan. 2004. Evaluation of
an extraction-based approach to answering definitional ques-
tions. In Proceedings of the 27th ACM Conference on Re-
search and Development in Information Retrieval (SIGIR-
04), pages 418?424, Sheffield, United Kingdom.
Z. Zhang, Y. Zhou, X. Huang, and L. Wu. 2005. Answering
definition questions using Web knowledge bases. In Pro-
ceedings of the 2nd International Joint Conference on Natu-
ral Language Processing (IJCNLP-05), pages 498?506, Jeju
Island, Korea.
417
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study on Similarity and Relatedness
Using Distributional and WordNet-based Approaches
Eneko Agirre? Enrique Alfonseca? Keith Hall? Jana Kravalova?? Marius Pas?ca? Aitor Soroa?
? IXA NLP Group, University of the Basque Country
? Google Inc.
? Institute of Formal and Applied Linguistics, Charles University in Prague
{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.com
kravalova@ufal.mff.cuni.cz
Abstract
This paper presents and compares WordNet-
based and distributional similarity approaches.
The strengths and weaknesses of each ap-
proach regarding similarity and relatedness
tasks are discussed, and a combination is pre-
sented. Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a super-
vised combination of them yields the best pub-
lished results on all datasets. Finally, we pio-
neer cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.
1 Introduction
Measuring semantic similarity and relatedness be-
tween terms is an important problem in lexical se-
mantics. It has applications in many natural lan-
guage processing tasks, such as Textual Entailment,
Word Sense Disambiguation or Information Extrac-
tion, and other related areas like Information Re-
trieval. The techniques used to solve this problem
can be roughly classified into two main categories:
those relying on pre-existing knowledge resources
(thesauri, semantic networks, taxonomies or ency-
clopedias) (Alvarez and Lim, 2007; Yang and Pow-
ers, 2005; Hughes and Ramage, 2007) and those in-
ducing distributional properties of words from cor-
pora (Sahami and Heilman, 2006; Chen et al, 2006;
Bollegala et al, 2007).
In this paper, we explore both families. For the
first one we apply graph based algorithms to Word-
Net, and for the second we induce distributional
similarities collected from a 1.6 Terabyte Web cor-
pus. Previous work suggests that distributional sim-
ilarities suffer from certain limitations, which make
them less useful than knowledge resources for se-
mantic similarity. For example, Lin (1998b) finds
similar phrases like captive-westerner which made
sense only in the context of the corpus used, and
Budanitsky and Hirst (2006) highlight other prob-
lems that stem from the imbalance and sparseness of
the corpora. Comparatively, the experiments in this
paper demonstrate that distributional similarities can
perform as well as the knowledge-based approaches,
and a combination of the two can exceed the per-
formance of results previously reported on the same
datasets. An application to cross-lingual (CL) sim-
ilarity identification is also described, with applica-
tions such as CL Information Retrieval or CL spon-
sored search. A discussion on the differences be-
tween learning similarity and relatedness scores is
provided.
The paper is structured as follows. We first
present the WordNet-based method, followed by the
distributional methods. Section 4 is devoted to the
evaluation and results on the monolingual and cross-
lingual tasks. Section 5 presents some analysis, in-
cluding learning curves for distributional methods,
the use of distributional similarity to improve Word-
Net similarity, the contrast between similarity and
relatedness, and the combination of methods. Sec-
tion 6 presents related work, and finally, Section 7
draws the conclusions and mentions future work.
2 WordNet-based method
WordNet (Fellbaum, 1998) is a lexical database of
English, which groups nouns, verbs, adjectives and
adverbs into sets of synonyms (synsets), each ex-
pressing a distinct concept. Synsets are interlinked
with conceptual-semantic and lexical relations, in-
cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-
sentation of WordNet, our method has basically two
19
steps: We first compute the personalized PageR-
ank over WordNet separately for each of the words,
producing a probability distribution over WordNet
synsets. We then compare how similar these two dis-
crete probability distributions are by encoding them
as vectors and computing the cosine between the
vectors.
We represent WordNet as a graph G = (V,E) as
follows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets associated
to them by directed edges.
For each word in the pair we first compute a per-
sonalized PageRank vector of graph G (Haveliwala,
2002). Basically, personalized PageRank is com-
puted by modifying the random jump distribution
vector in the traditional PageRank equation. In our
case, we concentrate all probability mass in the tar-
get word.
Regarding PageRank implementation details, we
chose a damping value of 0.85 and finish the calcula-
tion after 30 iterations. These are default values, and
we did not optimize them. Our similarity method is
similar, but simpler, to that used by (Hughes and Ra-
mage, 2007), which report very good results on sim-
ilarity datasets. More details of our algorithm can be
found in (Agirre and Soroa, 2009). The algorithm
and needed resouces are publicly available1.
2.1 WordNet relations and versions
The WordNet versions that we use in this work are
the Multilingual Central Repository or MCR (At-
serias et al, 2004) (which includes English Word-
Net version 1.6 and wordnets for several other lan-
guages like Spanish, Italian, Catalan and Basque),
and WordNet version 3.02. We used all the rela-
tions in MCR (except cooccurrence relations and se-
lectional preference relations) and in WordNet 3.0.
Given the recent availability of the disambiguated
gloss relations for WordNet 3.03, we also used a
version which incorporates these relations. We will
refer to the three versions as MCR16, WN30 and
WN30g, respectively. Our choice was mainly moti-
vated by the fact that MCR contains tightly aligned
1http://http://ixa2.si.ehu.es/ukb/
2Available from http://http://wordnet.princeton.edu/
3http://wordnet.princeton.edu/glosstag
wordnets of several languages (see below).
2.2 Cross-linguality
MCR follows the EuroWordNet design (Vossen,
1998), which specifies an InterLingual Index (ILI)
that links the concepts across wordnets of differ-
ent languages. The wordnets for other languages in
MCR use the English WordNet synset numbers as
ILIs. This design allows a decoupling of the rela-
tions between concepts (which can be taken to be
language independent) and the links from each con-
tent word to its corresponding concepts (which is
language dependent).
As our WordNet-based method uses the graph of
the concepts and relations, we can easily compute
the similarity between words from different lan-
guages. For example, consider a English-Spanish
pair like car ? coche. Given that the Spanish Word-
Net is included in MCR we can use MCR as the
common knowledge-base for the relations. We can
then compute the personalized PageRank for each
of car and coche on the same underlying graph, and
then compare the similarity between both probabil-
ity distributions.
As an alternative, we also tried to use pub-
licly available mappings for wordnets (Daude et al,
2000)4 in order to create a 3.0 version of the Span-
ish WordNet. The mapping was used to link Spanish
variants to 3.0 synsets. We used the English Word-
Net 3.0, including glosses, to construct the graph.
The two Spanish WordNet versions are referred to
as MCR16 and WN30g.
3 Context-based methods
In this section, we describe the distributional meth-
ods used for calculating similarities between words,
and profiting from the use of a large Web-based cor-
pus.
This work is motivated by previous studies that
make use of search engines in order to collect co-
occurrence statistics between words. Turney (2001)
uses the number of hits returned by a Web search
engine to calculate the Pointwise Mutual Informa-
tion (PMI) between terms, as an indicator of syn-
onymy. Bollegala et al (2007) calculate a number
of popular relatedness metrics based on page counts,
4http://www.lsi.upc.es/?nlp/tools/download-map.php.
20
like PMI, the Jaccard coefficient, the Simpson co-
efficient and the Dice coefficient, which are com-
bined with lexico-syntactic patterns as model fea-
tures. The model parameters are trained using Sup-
port Vector Machines (SVM) in order to later rank
pairs of words. A different approach is the one taken
by Sahami and Heilman (2006), who collect snip-
pets from the results of a search engine and repre-
sent each snippet as a vector, weighted with the tf?idf
score. The semantic similarity between two queries
is calculated as the inner product between the cen-
troids of the respective sets of vectors.
To calculate the similarity of two words w1 and
w2, Ruiz-Casado et al (2005) collect snippets con-
taining w1 from a Web search engine, extract a con-
text around it, replace it with w2 and check for the
existence of that modified context in the Web.
Using a search engine to calculate similarities be-
tween words has the drawback that the data used will
always be truncated. So, for example, the numbers
of hits returned by search engines nowadays are al-
ways approximate and rounded up. The systems that
rely on collecting snippets are also limited by the
maximum number of documents returned per query,
typically around a thousand. We hypothesize that
by crawling a large corpus from the Web and doing
standard corpus analysis to collect precise statistics
for the terms we should improve over other unsu-
pervised systems that are based on search engine
results, and should yield results that are competi-
tive even when compared to knowledge-based ap-
proaches.
In order to calculate the semantic similarity be-
tween the words in a set, we have used a vector space
model, with the following three variations:
In the bag-of-words approach, for each word w
in the dataset we collect every term t that appears in
a window centered in w, and add them to the vector
together with its frequency.
In the context window approach, for each word
w in the dataset we collect every window W cen-
tered in w (removing the central word), and add it
to the vector together with its frequency (the total
number of times we saw windowW around w in the
whole corpus). In this case, all punctuation symbols
are replaced with a special token, to unify patterns
like , the <term> said to and ? the <term> said to.
Throughout the paper, when we mention a context
window of size N it means N words at each side of
the phrase of interest.
In the syntactic dependency approach, we parse
the entire corpus using an implementation of an In-
ductive Dependency parser as described in Nivre
(2006). For each word w we collect a template of
the syntactic context. We consider sequences of gov-
erning words (e.g. the parent, grand-parent, etc.) as
well as collections of descendants (e.g., immediate
children, grandchildren, etc.). This information is
then encoded as a contextual template. For example,
the context template cooks <term> delicious could
be contexts for nouns such as food, meals, pasta, etc.
This captures both syntactic preferences as well as
selectional preferences. Contrary to Pado and Lap-
ata (2007), we do not use the labels of the syntactic
dependencies.
Once the vectors have been obtained, the fre-
quency for each dimension in every vector is
weighted using the other vectors as contrast set, with
the ?2 test, and finally the cosine similarity between
vectors is used to calculate the similarity between
each pair of terms.
Except for the syntactic dependency approach,
where closed-class words are needed by the parser,
in the other cases we have removed stopwords (pro-
nouns, prepositions, determiners and modal and
auxiliary verbs).
3.1 Corpus used
We have used a corpus of four billion documents,
crawled from the Web in August 2008. An HTML
parser is used to extract text, the language of each
document is identified, and non-English documents
are discarded. The final corpus remaining at the end
of this process contains roughly 1.6 Terawords. All
calculations are done in parallel sharding by dimen-
sion, and it is possible to calculate all pairwise sim-
ilarities of the words in the test sets very quickly
on this corpus using the MapReduce infrastructure.
A complete run takes around 15 minutes on 2,000
cores.
3.2 Cross-linguality
In order to calculate similarities in a cross-lingual
setting, where some of the words are in a language l
other than English, the following algorithm is used:
21
Method Window size RG dataset WordSim353 dataset
MCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]
WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]
WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]
CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]
2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]
3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]
4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]
5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]
6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]
7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]
BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]
2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]
3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]
4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]
5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]
6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]
7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]
Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]
G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]
G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]
G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]
G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]
G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]
CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]
Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]
4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]
4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]
4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]
4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]
Table 1: Spearman correlation results for the various WordNet-based
models and distributional models. CW=Context Windows, BoW=bag
of words, Syn=syntactic vectors. For Syn, the window size is actually
the tree-depth for the governors and descendants. For examples, G1
indicates that the contexts include the parents and D2 indicates that both
the children and grandchildren make up the contexts. The final grouping
includes both contextual windows (at width 4) and syntactic contexts in
the template vectors. Max scores are bolded.
1. Replace each non-English word in the dataset
with its 5-best translations into English using
state-of-the-art machine translation technology.
2. The vector corresponding to each Spanish word
is calculated by collecting features from all the
contexts of any of its translations.
3. Once the vectors are generated, the similarities
are calculated in the same way as before.
4 Experimental results
4.1 Gold-standard datasets
We have used two standard datasets. The first
one, RG, consists of 65 pairs of words collected by
Rubenstein and Goodenough (1965), who had them
judged by 51 human subjects in a scale from 0.0 to
4.0 according to their similarity, but ignoring any
other possible semantic relationships that might ap-
pear between the terms. The second dataset, Word-
Sim3535 (Finkelstein et al, 2002) contains 353 word
pairs, each associated with an average of 13 to 16 hu-
man judgements. In this case, both similarity and re-
5Available at http://www.cs.technion.ac.il/
?gabr/resources/data/wordsim353/wordsim353.html
Context RG terms and frequencies
ll never forget the * on his face when grin,2,smile,10
he had a giant * on his face and grin,3,smile,2
room with a huge * on her face and grin,2,smile,6
the state of every * will be updated every automobile,2,car,3
repair or replace the * if it is stolen automobile,2,car,2
located on the north * of the Bay of shore,14,coast,2
areas on the eastern * of the Adriatic Sea shore,3,coast,2
Thesaurus of Current English * The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2
wizard,4,glass,4,crane,5,smile,5
implement,5,oracle,2,lad,2
food,3,car,2,madhouse,3,jewel,3
asylum,4,tool,8,journey,6,etc.
be understood that the * 10 may be designed crane,3,tool,3
a fight between a * and a snake and bird,3,crane,5
Table 2: Sample of context windows for the terms in the RG dataset.
latedness are annotated without any distinction. Sev-
eral studies indicate that the human scores consis-
tently have very high correlations with each other
(Miller and Charles, 1991; Resnik, 1995), thus val-
idating the use of these datasets for evaluating se-
mantic similarity.
For the cross-lingual evaluation, the two datasets
were modified by translating the second word in
each pair into Spanish. Two humans translated
simultaneously both datasets, with an inter-tagger
agreement of 72% for RG and 84% for Word-
Sim353.
4.2 Results
Table 1 shows the Spearman correlation obtained on
the RG and WordSim353 datasets, including the in-
terval at 0.95 of confidence6.
Overall the distributional context-window ap-
proach performs best in the RG, reaching 0.89 corre-
lation, and both WN30g and the combination of con-
text windows and syntactic context perform best on
WordSim353. Note that the confidence intervals are
quite large in both RG and WordSim353, and few of
the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of
the glosses and WordNet 3.0 (WN30g) yields the
best results in both datasets. While MCR16 is close
to WN30g for the RG dataset, it lags well behind
on WordSim353. This discrepancy is further ana-
lyzed is Section 5.3. Note that the performance of
WordNet in the WordSim353 dataset suffers from
unknown words. In fact, there are nine pairs which
returned null similarity for this reason. The num-
6To calculate the Spearman correlations values are trans-
formed into ranks, and we calculate the Pearson correlation on
them. The confidence intervals refer to the Pearson correlations
of the rank vectors.
22
Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset. Left: WordSim353 with bag-of-words,
Right: RG with context windows.
Dataset Method overall ? interval
RG MCR16 0.78 -0.05 [0.66, 0.86]
WN30g 0.74 -0.09 [0.61, 0.84]
Bag of words 0.68 -0.23 [0.53, 0.79]
Context windows 0.83 -0.05 [0.73, 0.89]
WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]
WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]
Bag of words 0.53 -0.12 [0.45, 0.61]
Context windows 0.52 -0.11 [0.44, 0.59]
Table 3: Results obtained by the different methods on the Span-
ish/English cross-lingual datasets. The ? column shows the perfor-
mance difference with respect to the results on the original dataset.
ber in parenthesis in Table 1 for WordSim353 shows
the results for the 344 remaining pairs. Section 5.2
shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-
gether terms that can have a similar distribution of
contextual terms. Therefore, terms that are topically
related can appear in the same textual passages and
will get high values using this model. We see this
as an explanation why this model performed better
than the context window approach for WordSim353,
where annotators were instructed to provide high
ratings to related terms. On the contrary, the con-
text window approach tends to group together words
that are exchangeable in exactly the same context,
preserving order. Table 2 illustrates a few exam-
ples of context collected. Therefore, true synonyms
and hyponyms/hyperonyms will receive high simi-
larities, whereas terms related topically or based on
any other semantic relation (e.g. movie and star) will
have lower scores. This explains why this method
performed better for the RG dataset. Section 5.3
confirms these observations.
4.3 Cross-lingual similarity
Table 3 shows the results for the English-Spanish
cross-lingual datasets. For RG, MCR16 and the
context windows methods drop only 5 percentage
points, showing that cross-lingual similarity is feasi-
ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is
the best for this dataset, with the rest of the meth-
ods falling over 10 percentage points relative to the
monolingual experiment. A closer look at the Word-
Net results showed that most of the drop in perfor-
mance was caused by out-of-vocabulary words, due
to the smaller vocabulary of the Spanish WordNet.
Though not totally comparable, if we compute the
correlation over pairs covered in WordNet alne, the
correlation would drop only 2 percentage points. In
the case of the distributional approaches, the fall in
performance was caused by the translations, as only
61% of the words were translated into the original
word in the English datasets.
5 Detailed analysis and system
combination
In this section we present some analysis, including
learning curves for distributional methods, the use
of distributional similarity to improve WordNet sim-
ilarity, the contrast between similarity and related-
ness, and the combination of methods.
5.1 Learning curves for distributional methods
Figure 1 shows that the correlation improves with
the size of the corpus, as expected. For the re-
sults using the WordSim353 corpus, we show the
results of the bag-of-words approach with context
size 10. Results improve from 0.5 Spearman correla-
tion up to 0.65 when increasing the corpus size three
orders of magnitude, although the effect decays at
the end, which indicates that we might not get fur-
23
Method Without similar words With similar words
WN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]
WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]
Table 4: Results obtained replacing unknown words with their most
similar three words (WordSim353 dataset).
Method overall Similarity Relatedness
MCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]
WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]
WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]
BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]
CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]
Table 5: Results obtained on the WordSim353 dataset and on the two
similarity and relatedness subsets.
ther gains going beyond the current size of the cor-
pus. With respect to results for the RG dataset, we
used a context-window approach with context radius
4. Here, results improve even more with data size,
probably due to the sparse data problem collecting
8-word context windows if the corpus is not large
enough. Correlation improves linearly right to the
end, where results stabilize around 0.89.
5.2 Combining both approaches: dealing with
unknown words in WordNet
Although the vocabulary of WordNet is very ex-
tensive, applications are bound to need the similar-
ity between words which are not included in Word-
Net. This is exemplified in the WordSim353 dataset,
where 9 pairs contain words which are unknown to
WordNet. In order to overcome this shortcoming,
we could use similar words instead, as provided by
the distributional thesaurus. We used the distribu-
tional thesaurus defined in Section 3, using context
windows of width 4, to provide three similar words
for each of the unknown words in WordNet. Results
improve for both WN30 and WN30g, as shown in
Table 4, attaining our best results for WordSim353.
5.3 Similarity vs. relatedness
We mentioned above that the annotation guidelines
of WordSim353 did not distinguish between simi-
lar and related pairs. As the results in Section 4
show, different techniques are more appropriate to
calculate either similarity or relatedness. In order to
study this effect, ideally, we would have two ver-
sions of the dataset, where annotators were given
precise instructions to distinguish similarity in one
case, and relatedness in the other. Given the lack
of such datasets, we devised a simpler approach in
order to reuse the existing human judgements. We
manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-
ing synonyms of each other, antonyms, iden-
tical, hyperonym-hyponym, hyponym-hyperonym,
holonym-meronym, meronym-holonym, and none-
of-the-above. The inter-tagger agreement rate was
0.80, with a Kappa score of 0.77. This anno-
tation was used to group the pairs in three cate-
gories: similar pairs (those classified as synonyms,
antonyms, identical, or hyponym-hyperonym), re-
lated pairs (those classified as meronym-holonym,
and pairs classified as none-of-the-above, with a hu-
man average similarity greater than 5), and unrelated
pairs (those classified as none-of-the-above that had
average similarity less than or equal to 5). We then
created two new gold-standard datasets: similarity
(the union of similar and unrelated pairs), and relat-
edness (the union of related and unrelated)7.
Table 5 shows the results on the relatedness and
similarity subsets of WordSim353 for the different
methods. Regarding WordNet methods, both WN30
and WN30g perform similarly on the similarity sub-
set, but WN30g obtains the best results by far on
the relatedness data. These results are congruent
with our expectations: two words are similar if their
synsets are in close places in the WordNet hierarchy,
and two words are related if there is a connection
between them. Most of the relations in WordNet
are of hierarchical nature, and although other rela-
tions exist, they are far less numerous, thus explain-
ing the good results for both WN30 and WN30g on
similarity, but the bad results of WN30 on related-
ness. The disambiguated glosses help find connec-
tions among related concepts, and allow our method
to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some
comments. Given the fact that MCR16 performed
very well on the RG dataset, it comes as a surprise
that it performs so poorly for the similarity subset
of WordSim353. In an additional evaluation, we at-
tested that MCR16 does indeed perform as well as
MCR30g on the similar pairs subset. We believe
that this deviation could be due to the method used to
construct the similarity dataset, which includes some
pairs of loosely related pairs labeled as unrelated.
7Available at http://alfonseca.org/eng/research/wordsim353.html
24
Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatedness
WN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]
WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]
WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]
WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]
Table 6: Results using a supervised combination of several systems. Max values are bolded for each dataset.
Concerning the techniques based on distributional
similarities, the method based on context windows
provides the best results for similarity, and the bag-
of-words representation outperforms most of the
other techniques for relatedness.
5.4 Supervised combination
In order to gain an insight on which would be the up-
per bound that we could obtain when combining our
methods, we took the output of three systems (bag
of words with window size 10, context window with
size 4, and the WN30g run). Each of these outputs is
a ranking of word pairs, and we implemented an or-
acle that chooses, for each pair, the rank that is most
similar to the rank of the pair in the gold-standard.
The outputs of the oracle have a Spearman correla-
tion of 0.97 for RG and 0.92 for WordSim353, which
gives as an indication of the correlations that could
be achieved by choosing for each pair the rank out-
put by the best classifier for that pair.
The previous results motivated the use of a su-
pervised approach to combine the output of the
different systems. We created a training cor-
pus containing pairs of pairs of words from the
datasets, having as features the similarity and rank
of each pair involved as given by the differ-
ent unsupervised systems. A classifier is trained
to decide whether the first pair is more simi-
lar than the second one. For example, a train-
ing instance using two unsupervised classifiers is
0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative
meaning that the similarities given by the first clas-
sifier to the two pairs were 0.001364 and 0.327515
respectively, which ranked them in positions 31 and
64. The second classifier gave them similarities of
0.084805 and 0.109061 respectively, which ranked
them in positions 57 and 59. The class negative in-
dicates that in the gold-standard the first pair has a
lower score than the second pair.
We have trained a SVM to classify pairs of pairs,
and use its output to rank the entries in both datasets.
It uses a polynomial kernel with degree 4. We did
Method Source Spearman (MC) Pearson (MC)
(Sahami et al, 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78]
(Chen et al, 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85]
(Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89]
(Leacock et al, 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91]
(Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90]
(Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92]
(Bollegala et al, 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92]
(Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93]
(Jarmasz, 2003) Roget?s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94]
(Patwardhan et al, 2006) WordNet n/a 0.91
(Alvarez and Lim, 2007) WordNet n/a 0.91
(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96]
(Hughes et al, 2007) WordNet 0.90 n/a
Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a
Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]
Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]
Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]
SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]
Table 7: Comparison with previous approaches for MC.
not have a held-out set, so we used the standard set-
tings of Weka, without trying to modify parameters,
e.g. C. Each word pair is scored with the number
of pairs that were considered to have less similar-
ity using the SVM. The results using 10-fold cross-
validation are shown in Table 6. A combination of
all methods produces the best results reported so far
for both datasets, statistically significant for RG.
6 Related work
Contrary to the WordSim353 dataset, common prac-
tice with the RG dataset has been to perform the
evaluation with Pearson correlation. In our believe
Pearson is less informative, as the Pearson correla-
tion suffers much when the scores of two systems are
not linearly correlated, something which happens
often given due to the different nature of the tech-
niques applied. Some authors, e.g. Alvarez and Lim
(2007), use a non-linear function to map the system
outputs into new values distributed more similarly
to the values in the gold-standard. In their case, the
mapping function was exp (?x4 ), which was chosenempirically. Finding such a function is dependent
on the dataset used, and involves an extra step in the
similarity calculations. Alternatively, the Spearman
correlation provides an evaluation metric that is in-
dependent of such data-dependent transformations.
Most similarity researchers have published their
25
Word pair M&C SVM Word pair M&C SVM
automobile, car 3.92 62 crane, implement 1.68 26
journey, voyage 3.84 54 brother, lad 1.66 39
gem, jewel 3.84 61 car, journey 1.16 37
boy, lad 3.76 57 monk, oracle 1.1 32
coast, shore 3.7 53 food, rooster 0.89 3
asylum, madhouse 3.61 45 coast, hill 0.87 34
magician, wizard 3.5 49 forest, graveyard 0.84 27
midday, noon 3.42 61 monk, slave 0.55 17
furnace, stove 3.11 50 lad, wizard 0.42 13
food, fruit 3.08 47 coast, forest 0.42 18
bird, cock 3.05 46 cord, smile 0.13 5
bird, crane 2.97 38 glass, magician 0.11 10
implement, tool 2.95 55 rooster, voyage 0.08 1
brother, monk 2.82 42 noon, string 0.08 5
Table 8: Our best results for the MC dataset.
Method Source Spearman
(Strube and Ponzetto, 2006) Wikipedia 0.19?0.48
(Jarmasz, 2003) WordNet 0.33?0.35
(Jarmasz, 2003) Roget?s 0.55
(Hughes and Ramage, 2007) WordNet 0.55
(Finkelstein et al, 2002) Web corpus, WN 0.56
(Gabrilovich and Markovitch, 2007) ODP 0.65
(Gabrilovich and Markovitch, 2007) Wikipedia 0.75
SVM Web corpus, WN 0.78
Table 9: Comparison with previous work for WordSim353.
complete results on a smaller subset of the RG
dataset containing 30 word pairs (Miller and
Charles, 1991), usually referred to as MC, making it
possible to compare different systems using differ-
ent correlation. Table 7 shows the results of related
work on MC that was available to us, including our
own. For the authors that did not provide the de-
tailed data we include only the Pearson correlation
with no confidence intervals.
Among the unsupervised methods introduced in
this paper, the context window produced the best re-
ported Spearman correlation, although the 0.95 con-
fidence intervals are too large to allow us to accept
the hypothesis that it is better than all others meth-
ods. The supervised combination produces the best
results reported so far. For the benefit of future re-
search, our results for the MC subset are displayed
in Table 8.
Comparison on the WordSim353 dataset is eas-
ier, as all researchers have used Spearman. The
figures in Table 9) show that our WordNet-based
method outperforms all previously published Word-
Net methods. We want to note that our WordNet-
based method outperforms that of Hughes and Ram-
age (2007), which uses a similar method. Although
there are some differences in the method, we think
that the main performance gain comes from the use
of the disambiguated glosses, which they did not
use. Our distributional methods also outperform all
other corpus-based methods. The most similar ap-
proach to our distributional technique is Finkelstein
et al (2002), who combined distributional similar-
ities from Web documents with a similarity from
WordNet. Their results are probably worse due to
the smaller data size (they used 270,000 documents)
and the differences in the calculation of the simi-
larities. The only method which outperforms our
non-supervised methods is that of (Gabrilovich and
Markovitch, 2007) when based on Wikipedia, prob-
ably because of the dense, manually distilled knowl-
edge contained in Wikipedia. All in all, our super-
vised combination gets the best published results on
this dataset.
7 Conclusions and future work
This paper has presented two state-of-the-art dis-
tributional and WordNet-based similarity measures,
with a study of several parameters, including per-
formance on similarity and relatedness data. We
show that the use of disambiguated glosses allows
for the best published results for WordNet-based
systems on the WordSim353 dataset, mainly due to
the better modeling of relatedness (as opposed to
similarity). Distributional similarities have proven
to be competitive when compared to knowledge-
based methods, with context windows being better
for similarity and bag of words for relatedness. Dis-
tributional similarity was effectively used to cover
out-of-vocabulary items in the WordNet-based mea-
sure providing our best unsupervised results. The
complementarity of our methods was exploited by
a supervised learner, producing the best results so
far for RG and WordSim353. Our results include
confidence values, which, surprisingly, were not in-
cluded in most previous work, and show that many
results over RG and WordSim353 are indistinguish-
able. The algorithm for WordNet-base similarity
and the necessary resources are publicly available8.
This work pioneers cross-lingual extension and
evaluation of both distributional and WordNet-based
measures. We have shown that closely aligned
wordnets provide a natural and effective way to
compute cross-lingual similarity with minor losses.
A simple translation strategy also yields good results
for distributional methods.
8http://ixa2.si.ehu.es/ukb/
26
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proc. of EACL
2009, Athens, Greece.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling
of Semantic Similarity between Words. Proc. of the
Conference on Semantic Computing, pages 355?362.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning multi-
lingual central repository. In Proc. of Global WordNet
Conference, Brno, Czech Republic.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proceedings of WWW?2007.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness.
Computational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of COCLING/ACL 2006.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets using structural information. In Proceedings of
ACL?2000, Hong Kong.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press,
Cambridge, Mass.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Transactions on Information Systems, 20(1):116?131.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proc of IJCAI, pages 6?12.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international con-
ference on World Wide Web, pages 517?526.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
M. Jarmasz. 2003. Roget?s Thesuarus as a lexical re-
source for Natural Language Processing.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, volume 33. Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An Electronic Lexical Database,
49(2):265?283.
D. Lin. 1998a. An information-theoretic definition of
similarity. In Proc. of ICML, pages 296?304, Wiscon-
sin, USA.
D. Lin. 1998b. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of ACL-98.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Nivre. 2006. Inductive Dependency Parsing, vol-
ume 34 of Text, Speech and Language Technology.
Springer.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Pycholinguistics Together,
pages 1?8, Trento, Italy.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. Proc. of IJCAI,
14:448?453.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Using context-window overlapping in Synonym Dis-
covery and Ontology Extension. In Proceedings of
RANLP-2005, Borovets, Bulgaria,.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. Proc. of WWW, pages 377?386.
M. Strube and S.P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the AAAI-2006, pages 1419?1424.
P.D. Turney. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. Lecture Notes in Computer
Science, 2167:491?502.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. of ACL, pages 133?138, Las
Cruces, New Mexico.
D. Yang and D.M.W. Powers. 2005. Measuring semantic
similarity in the taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
27
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809?816,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Names and Similarities on the Web: Fact Extraction in the Fast Lane
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Dekang Lin
Google Inc.
Mountain View, CA 94043
lindek@google.com
Jeffrey Bigham?
University of Washington
Seattle, WA 98195
jbigham@cs.washington.edu
Andrei Lifchits?
University of British Columbia
Vancouver, BC V6T 1Z4
alifchit@cs.ubc.ca
Alpa Jain?
Columbia University
New York, NY 10027
alpa@cs.columbia.edu
Abstract
In a new approach to large-scale extrac-
tion of facts from unstructured text, dis-
tributional similarities become an integral
part of both the iterative acquisition of
high-coverage contextual extraction pat-
terns, and the validation and ranking of
candidate facts. The evaluation mea-
sures the quality and coverage of facts
extracted from one hundred million Web
documents, starting from ten seed facts
and using no additional knowledge, lexi-
cons or complex tools.
1 Introduction
1.1 Background
The potential impact of structured fact reposito-
ries containing billions of relations among named
entities on Web search is enormous. They en-
able the pursuit of new search paradigms, the pro-
cessing of database-like queries, and alternative
methods of presenting search results. The prepa-
ration of exhaustive lists of hand-written extrac-
tion rules is impractical given the need for domain-
independent extraction of many types of facts from
unstructured text. In contrast, the idea of boot-
strapping for relation and information extraction
was first proposed in (Riloff and Jones, 1999), and
successfully applied to the construction of seman-
tic lexicons (Thelen and Riloff, 2002), named en-
tity recognition (Collins and Singer, 1999), extrac-
tion of binary relations (Agichtein and Gravano,
2000), and acquisition of structured data for tasks
such as Question Answering (Lita and Carbonell,
2004; Fleischman et al, 2003). In the context of
fact extraction, the resulting iterative acquisition
?Work done during internships at Google Inc.
framework starts from a small set of seed facts,
finds contextual patterns that extract the seed facts
from the underlying text collection, identifies a
larger set of candidate facts that are extracted by
the patterns, and adds the best candidate facts to
the previous seed set.
1.2 Contributions
Figure 1 describes an architecture geared towards
large-scale fact extraction. The architecture is sim-
ilar to other instances of bootstrapping for infor-
mation extraction. The main processing stages are
the acquisition of contextual extraction patterns
given the seed facts, acquisition of candidate facts
given the extraction patterns, scoring and ranking
of the patterns, and scoring and ranking of the can-
didate facts, a subset of which is added to the seed
set of the next round.
Within the existing iterative acquisition frame-
work, our first contribution is a method for au-
tomatically generating generalized contextual ex-
traction patterns, based on dynamically-computed
classes of similar words. Traditionally, the ac-
quisition of contextual extraction patterns requires
hundreds or thousands of consecutive iterations
over the entire text collection (Lita and Carbonell,
2004), often using relatively expensive or restric-
tive tools such as shallow syntactic parsers (Riloff
and Jones, 1999; Thelen and Riloff, 2002) or
named entity recognizers (Agichtein and Gravano,
2000). Comparatively, generalized extraction pat-
terns achieve exponentially higher coverage in
early iterations. The extraction of large sets of can-
didate facts opens the possibility of fast-growth it-
erative extraction, as opposed to the de-facto strat-
egy of conservatively growing the seed set by as
few as five items (Thelen and Riloff, 2002) after
each iteration.
809
Acquisition of contextual extraction patterns
Distributional similaritiesText collection
Candidate facts
Acquisition of candidate facts
Occurrences of extraction patterns
Validation of candidate facts
Scored extraction patternsScored candidate facts
Scoring and ranking
Validated candidate facts
Seed facts
Occurrences of seed facts Extraction patterns
Validated extraction patterns
Validation of patterns
Generalized extraction patterns
Figure 1: Large-scale fact extraction architecture
The second contribution of the paper is a
method for domain-independent validation and
ranking of candidate facts, based on a similar-
ity measure of each candidate fact relative to the
set of seed facts. Whereas previous studies as-
sume clean text collections such as news cor-
pora (Thelen and Riloff, 2002; Agichtein and Gra-
vano, 2000; Hasegawa et al, 2004), the valida-
tion is essential for low-quality sets of candidate
facts collected from noisy Web documents. With-
out it, the addition of spurious candidate facts to
the seed set would result in a quick divergence of
the iterative acquisition towards irrelevant infor-
mation (Agichtein and Gravano, 2000). Further-
more, the finer-grained ranking induced by simi-
larities is necessary in fast-growth iterative acqui-
sition, whereas previously proposed ranking crite-
ria (Thelen and Riloff, 2002; Lita and Carbonell,
2004) are implicitly designed for slow growth of
the seed set.
2 Similarities for Pattern Acquisition
2.1 Generalization via Word Similarities
The extraction patterns are acquired by matching
the pairs of phrases from the seed set into docu-
ment sentences. The patterns consist of contigu-
ous sequences of sentence terms, but otherwise
differ from the types of patterns proposed in earlier
work in two respects. First, the terms of a pattern
are either regular words or, for higher generality,
any word from a class of similar words. Second,
the amount of textual context encoded in a pat-
tern is limited to the sequence of terms between
(i.e., infix) the pair of phrases from a seed fact that
could be matched in a document sentence, thus ex-
cluding any context to the left (i.e., prefix) and to
the right (i.e., postfix) of the seed.
The pattern shown at the top of Figure 2, which
(Irving Berlin, 1888)
    NNP       NNP       CD
Infix
Aurelio de la Vega was born November 28 , 1925 , in Havana , Cuba .
    FW       FW FW  NNP VBD  VBN      NNP           CD  ,    CD    ,  IN    NNP      ,   NNP    .
foundnot found
Infix
not found
Prefix PostfixInfix
Matching on sentences
Seed fact Infix?only pattern
The poet was born Jan. 13 , several years after the revolution .
not found
British ? native Glenn Cornick of Jethro Tull was born April 23 , 1947 .
   NNP     :      JJ         NNP        NNP       IN    NNP     NNP  VBD  VBN   NNP   CD  ,   CD     .
Infix
foundfound
Chester Burton Atkins was born June 20 , 1924 , on a farm near Luttrell .
   NNP          NNP       NNP     VBD  VBN  NNP  CD  ,   CD     ,  IN DT  NN     IN       NNP       .
Infix
Infix
found
The youngest child of three siblings , Mariah Carey was born March 27 ,
1970 in Huntington , Long Island in New York .
  DT       JJS            NN     IN   CD        NNS       ,    NNP        NNP    VBD  VBN    NNP     CD  ,
  CD    IN       NNP             ,    JJ         NN      IN  NNP    NNP   .
found
foundfound
(S1)
(S2)
(S3)
(S4)
(S5)
(Jethro Tull, 1947)  (Mariah Carey, 1970)  (Chester Burton Atkins, 1924)
Candidate facts
  DT    NN   VBD  VBN  NNP CD ,       JJ           NNS     IN     DT        NN           .
N/A          CL1 born CL2 00 ,              N/A
Figure 2: Extraction via infix-only patterns
contains the sequence [CL1 born CL2 00 .], illus-
trates the use of classes of distributionally similar
words within extraction patterns. The first word
class in the sequence, CL1, consists of words such
as {was, is, could}, whereas the second class in-
cludes {February, April, June, Aug., November}
and other similar words. The classes of words are
computed on the fly over all sequences of terms
in the extracted patterns, on top of a large set of
pairwise similarities among words (Lin, 1998) ex-
tracted in advance from around 50 million news
articles indexed by the Google search engine over
three years. All digits in both patterns and sen-
tences are replaced with a common marker, such
810
that any two numerical values with the same num-
ber of digits will overlap during matching.
Many methods have been proposed to compute
distributional similarity between words, e.g., (Hin-
dle, 1990), (Pereira et al, 1993), (Grefenstette,
1994) and (Lin, 1998). Almost all of the methods
represent a word by a feature vector, where each
feature corresponds to a type of context in which
the word appeared. They differ in how the feature
vectors are constructed and how the similarity be-
tween two feature vectors is computed.
In our approach, we define the features of a
word w to be the set of words that occurred within
a small window of w in a large corpus. The context
window of an instance of w consists of the clos-
est non-stopword on each side of w and the stop-
words in between. The value of a feature w? is de-
fined as the pointwise mutual information between
w? and w: PMI(w?, w) = ? log( P (w,w?)P (w)P (w?)). The
similarity between two different words w1 and w2,
S(w1, w2), is then computed as the cosine of the
angle between their feature vectors.
While the previous approaches to distributional
similarity have only applied to words, we applied
the same technique to proper names as well as
words. The following are some example similar
words and phrases with their similarities, as ob-
tained from the Google News corpus:
? Carey: Higgins 0.39, Lambert 0.39, Payne
0.38, Kelley 0.38, Hayes 0.38, Goodwin 0.38,
Griffin 0.38, Cummings 0.38, Hansen 0.38,
Williamson 0.38, Peters 0.38, Walsh 0.38, Burke
0.38, Boyd 0.38, Andrews 0.38, Cunningham
0.38, Freeman 0.37, Stephens 0.37, Flynn 0.37,
Ellis 0.37, Bowers 0.37, Bennett 0.37, Matthews
0.37, Johnston 0.37, Richards 0.37, Hoffman
0.37, Schultz 0.37, Steele 0.37, Dunn 0.37, Rowe
0.37, Swanson 0.37, Hawkins 0.37, Wheeler 0.37,
Porter 0.37, Watkins 0.37, Meyer 0.37 [..];
? Mariah Carey: Shania Twain 0.38, Christina
Aguilera 0.35, Sheryl Crow 0.35, Britney Spears
0.33, Celine Dion 0.33, Whitney Houston 0.32,
Justin Timberlake 0.32, Beyonce Knowles 0.32,
Bruce Springsteen 0.30, Faith Hill 0.30, LeAnn
Rimes 0.30, Missy Elliott 0.30, Aretha Franklin
0.29, Jennifer Lopez 0.29, Gloria Estefan 0.29,
Elton John 0.29, Norah Jones 0.29, Missy
Elliot 0.29, Alicia Keys 0.29, Avril Lavigne
0.29, Kid Rock 0.28, Janet Jackson 0.28, Kylie
Minogue 0.28, Beyonce 0.27, Enrique Iglesias
0.27, Michelle Branch 0.27 [..];
? Jethro Tull: Motley Crue 0.28, Black Crowes
0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sab-
bath 0.26, Doobie Brothers 0.26, Judas Priest 0.26,
Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24,
Black Flag 0.24, Godsmack 0.24, Grateful Dead
0.24, Grand Funk Railroad 0.24, Smashing Pump-
kins 0.24, Led Zeppelin 0.24, Aerosmith 0.24,
Limp Bizkit 0.24, Counting Crows 0.24, Echo
And The Bunnymen 0.24, Cold Chisel 0.24, Thin
Lizzy 0.24 [..].
To our knowledge, the only previous study that
embeds similarities into the acquisition of extrac-
tion patterns is (Stevenson and Greenwood, 2005).
The authors present a method for computing pair-
wise similarity scores among large sets of poten-
tial syntactic (subject-verb-object) patterns, to de-
tect centroids of mutually similar patterns. By as-
suming the syntactic parsing of the underlying text
collection to generate the potential patterns in the
first place, the method is impractical on Web-scale
collections. Two patterns, e.g. chairman-resign
and CEO-quit, are similar to each other if their
components are present in an external hand-built
ontology (i.e., WordNet), and the similarity among
the components is high over the ontology. Since
general-purpose ontologies, and WordNet in par-
ticular, contain many classes (e.g., chairman and
CEO) but very few instances such as Osasuna,
Crewe etc., the patterns containing an instance
rather than a class will not be found to be simi-
lar to one another. In comparison, the classes and
instances are equally useful in our method for gen-
eralizing patterns for fact extraction. We merge
basic patterns into generalized patterns, regardless
of whether the similar words belong, as classes or
instances, in any external ontology.
2.2 Generalization via Infix-Only Patterns
By giving up the contextual constraints imposed
by the prefix and postfix, infix-only patterns rep-
resent the most aggressive type of extraction pat-
terns that still use contiguous sequences of terms.
In the absence of the prefix and postfix, the outer
boundaries of the fact are computed separately for
the beginning of the first (left) and end of the sec-
ond (right) phrases of the candidate fact. For gen-
erality, the computation relies only on the part-
of-speech tags of the current seed set. Starting
forward from the right extremity of the infix, we
collect a growing sequence of terms whose part-
of-speech tags are [P1+ P2+ .. Pn+], where the
811
notation Pi+ represents one or more consecutive
occurrences of the part-of-speech tag Pi. The se-
quence [P1 P2 .. Pn] must be exactly the sequence
of part of speech tags from the right side of one of
the seed facts. The point where the sequence can-
not be grown anymore defines the boundary of the
fact. A similar procedure is applied backwards,
starting from the left extremity of the infix. An
infix-only pattern produces a candidate fact from
a sentence only if an acceptable sequence is found
to the left and also to the right of the infix.
Figure 2 illustrates the process on the infix-
only pattern mentioned earlier, and one seed fact.
The part-of-speech tags for the seed fact are [NNP
NNP] and [CD] for the left and right sides respec-
tively. The infix occurs in all sentences. How-
ever, the matching of the part-of-speech tags of the
sentence sequences to the left and right of the in-
fix, against the part-of-speech tags of the seed fact,
only succeeds for the last three sentences. It fails
for the first sentence S1 to the left of the infix, be-
cause [.. NNP] (for Vega) does not match [NNP
NNP]. It also fails for the second sentence S2 to
both the left and the right side of the infix, since [..
NN] (for poet) does not match [NNP NNP], and
[JJ ..] (for several) does not match [CD].
3 Similarities for Validation and Ranking
3.1 Revisiting Standard Ranking Criteria
Because some of the acquired extraction patterns
are too generic or wrong, all approaches to iter-
ative acquisition place a strong emphasis on the
choice of criteria for ranking. Previous literature
quasi-unanimously assesses the quality of each
candidate fact based on the number and qual-
ity of the patterns that extract the candidate fact
(more is better); and the number of seed facts ex-
tracted by the same patterns (again, more is bet-
ter) (Agichtein and Gravano, 2000; Thelen and
Riloff, 2002; Lita and Carbonell, 2004). However,
our experiments using many variations of previ-
ously proposed scoring functions suggest that they
have limited applicability in large-scale fact ex-
traction, for two main reasons. The first is that
it is impractical to perform hundreds of acquisi-
tion iterations on terabytes of text. Instead, one
needs to grow the seed set aggressively in each
iteration. Previous scoring functions were im-
plicitly designed for cautious acquisition strate-
gies (Collins and Singer, 1999), which expand the
seed set very slowly across consecutive iterations.
In that case, it makes sense to single out a small
number of best candidates, among the other avail-
able candidates. Comparatively, when 10,000 can-
didate facts or more need to be added to a seed set
of 10 seeds as early as after the first iteration, it
is difficult to distinguish the quality of extraction
patterns based, for instance, only on the percent-
age of the seed set that they extract. The second
reason is the noisy nature of the Web. A substan-
tial number of factors can and will concur towards
the worst-case extraction scenarios on the Web.
Patterns of apparently high quality turn out to pro-
duce a large quantity of erroneous ?facts? such as
(A-League, 1997), but also the more interesting
(Jethro Tull, 1947) as shown earlier in Figure 2, or
(Web Site David, 1960) or (New York, 1831). As
for extraction patterns of average or lower quality,
they will naturally lead to even more spurious ex-
tractions.
3.2 Ranking of Extraction Patterns
The intuition behind our criteria for ranking gen-
eralized pattern is that patterns of higher preci-
sion tend to contain words that are indicative of
the relation being mined. Thus, a pattern is more
likely to produce good candidate facts if its in-
fix contains the words language or spoken if ex-
tracting Language-SpokenIn-Country facts, or the
word capital if extracting City-CapitalOf-Country
relations. In each acquisition iteration, the scor-
ing of patterns is a two-pass procedure. The first
pass computes the normalized frequencies of all
words excluding stopwords, over the entire set of
extraction patterns. The computation applies sep-
arately to the prefix, infix and postfix of the pat-
terns. In the second pass, the score of an extraction
pattern is determined by the words with the high-
est frequency score in its prefix, infix and postfix,
as computed in the first pass and adjusted for the
relative distance to the start and end of the infix.
3.3 Ranking of Candidate Facts
Figure 3 introduces a new scheme for assessing the
quality of the candidate facts, based on the compu-
tation of similarity scores for each candidate rela-
tive to the set of seed facts. A candidate fact, e.g.,
(Richard Steele, 1672), is similar to the seed set if
both its phrases, i.e., Richard Steele and 1672, are
similar to the corresponding phrases (John Lennon
or Stephen Foster in the case of Richard Steele)
from the seed facts. For a phrase of a candidate
fact to be assigned a non-default (non-minimum)
812
...
Lennon
Lambert
McFadden
Bateson
McNamara
Costello
Cronin
Wooley
Baker
...
Foster
Hansen
Hawkins
Fisher
Holloway
Steele
Sweeney
Chris
John
James
Andrew
Mike
Matt
Brian
Christopher
...
John Lennon         1940
Seed facts
Stephen Foster      1826
Brian McFadden           1980
(4)(3)
Robert S. McNamara    1916
(6)(5)
Barbara Steele               1937
(7) (2)
Stan Hansen                  1949
(9)(8)
Similar wordsSimilar words
for: John
Similar words
for: Stephen
for: Lennon
Similar words
for: Foster
...
Stephen
Robert
Michael
Peter
William
Stan
Richard(1)
Barbara
(3)
(5)
(7) (2)
(8)
(9)
(4)
(6)
(2)(1)
Candidate facts
Jethro Tull                     1947
Richard Steele               1672
Figure 3: The role of similarities in estimating the
quality of candidate facts
similarity score, the words at its extremities must
be similar to one or more words situated at the
same positions in the seed facts. This is the case
for the first five candidate facts in Figure 3. For ex-
ample, the first word Richard from one of the can-
didate facts is similar to the first word John from
one of the seed facts. Concurrently, the last word
Steele from the same phrase is similar to Foster
from another seed fact. Therefore Robert Foster
is similar to the seed facts. The score of a phrase
containing N words is:
{
C1 +
?N
i=1 log(1 + Simi) , if Sim1,N > 0
C2 , otherwise.
where Simi is the similarity of the component
word at position i in the phrase, and C1 and C2
are scaling constants such that C2C1. Thus,
the similarity score of a candidate fact aggregates
individual word-to-word similarity scores, for the
left side and then for the right side of a candidate
fact. In turn, the similarity score of a component
word Simi is higher if: a) the computed word-to-
word similarity scores are higher relative to words
at the same position i in the seeds; and b) the com-
ponent word is similar to words from more than
one seed fact.
The similarity scores are one of a linear com-
bination of features that induce a ranking over the
candidate facts. Three other domain-independent
features contribute to the final ranking: a) a phrase
completeness score computed statistically over the
entire set of candidate facts, which demotes candi-
date facts if any of their two sides is likely to be
incomplete (e.g., Mary Lou vs. Mary Lou Retton,
or John F. vs. John F. Kennedy); b) the average
PageRank value over all documents from which
the candidate fact is extracted; and c) the pattern-
based scores of the candidate fact. The latter fea-
ture converts the scores of the patterns extracting
the candidate fact into a score for the candidate
fact. For this purpose, it considers a fixed-length
window of words around each match of a candi-
date fact in some sentence from the text collection.
This is equivalent to analyzing all sentence con-
texts from which a candidate fact can be extracted.
For each window, the word with the highest fre-
quency score, as computed in the first pass of the
procedure for scoring the patterns, determines the
score of the candidate fact in that context. The
overall pattern-based score of a candidate fact is
the sum of the scores over all its contexts of occur-
rence, normalized by the frequency of occurrence
of the candidate over all sentences.
Besides inducing a ranking over the candidate
facts, the similarity scores also serve as a valida-
tion filter over the candidate facts. Indeed, any
candidates that are not similar to the seed set can
be filtered out. For instance, the elimination of
(Jethro Tull, 1947) is a side effect of verifying that
Tull is not similar to any of the last-position words
from phrases in the seed set.
4 Evaluation
4.1 Data
The source text collection consists of three chunks
W1, W2, W3 of approximately 100 million doc-
uments each. The documents are part of a larger
snapshot of the Web taken in 2003 by the Google
search engine. All documents are in English.
The textual portion of the documents is cleaned
of Html, tokenized, split into sentences and part-
of-speech tagged using the TnT tagger (Brants,
2000).
The evaluation involves facts of type Person-
BornIn-Year. The reasons behind the choice of
this particular type are threefold. First, many
Person-BornIn-Year facts are probably available
on the Web (as opposed to, e.g., City-CapitalOf-
Country facts), to allow for a good stress test
for large-scale extraction. Second, either side of
the facts (Person and Year) may be involved in
many other types of facts, such that the extrac-
tion would easily divergence unless it performs
correctly. Third, the phrases from one side (Per-
son) have an utility in their own right, for lexicon
813
Table 1: Set of seed Person-BornIn-Year facts
Name Year Name Year
Paul McCartney 1942 John Lennon 1940
Vincenzo Bellini 1801 Stephen Foster 1826
Hoagy Carmichael 1899 Irving Berlin 1888
Johann Sebastian Bach 1685 Bela Bartok 1881
Ludwig van Beethoven 1770 Bob Dylan 1941
construction or detection of person names.
The Person-BornIn-Year type is specified
through an initial set of 10 seed facts shown in Ta-
ble 1. Similarly to source documents, the facts are
also part-of-speech tagged.
4.2 System Settings
In each iteration, the case-insensitive matching of
the current set of seed facts onto the sentences pro-
duces basic patterns. The patterns are converted
into generalized patterns. The length of the infix
may vary between 1 and 6 words. Potential pat-
terns are discarded if the infix contains only stop-
words.
When a pattern is retained, it is used as an
infix-only pattern, and allowed to generate at most
600,000 candidate facts. At the end of an itera-
tion, approximately one third of the validated can-
didate facts are added to the current seed set. Con-
sequently, the acquisition expands the initial seed
set of 10 facts to 100,000 facts (after iteration 1)
and then to one million facts (after iteration 2) us-
ing chunk W1.
4.3 Precision
A separate baseline run extracts candidate facts
from the text collection following the traditional
iterative acquisition approach. Pattern general-
ization is disabled, and the ranking of patterns
and facts follows strictly the criteria and scoring
functions from (Thelen and Riloff, 2002), which
are also used in slightly different form in (Lita
and Carbonell, 2004) and (Agichtein and Gravano,
2000). The theoretical option of running thou-
sands of iterations over the text collection is not
viable, since it would imply a non-justifiable ex-
pense of our computational resources. As a more
realistic compromise over overly-cautious acqui-
sition, the baseline run retains as many of the top
candidate facts as the size of the current seed,
whereas (Thelen and Riloff, 2002) only add the
top five candidate facts to the seed set after each it-
eration. The evaluation considers all 80, a sample
of the 320, and another sample of the 10,240 facts
retained after iterations 3, 5 and 10 respectively.
The correctness assessment of each fact consists
in manually finding some Web page that contains
clear evidence that the fact is correct. If no such
page exists, the fact is marked as incorrect. The
corresponding precision values after the three iter-
ations are 91.2%, 83.8% and 72.9%.
For the purpose of evaluating the precision of
our system, we select a sample of facts from
the entire list of one million facts extracted from
chunk W1, ranked in decreasing order of their
computed scores. The sample is generated auto-
matically from the top of the list to the bottom, by
retaining a fact and skipping the following consec-
utive N facts, where N is incremented at each step.
The resulting list, which preserves the relative or-
der of the facts, contains 1414 facts. The 115 facts
for which a Web search engine does not return any
documents, when the name (as a phrase) and the
year are submitted together in a conjunctive query,
are discarded from the sample of 1414 facts. In
those cases, the facts were acquired from the 2003
snapshot of the Web, but queries are submitted to
a search engine with access to current Web doc-
uments, hence the difference when some of the
2003 documents are no longer available or index-
able.
Based on the sample set, the average preci-
sion of the list of one million facts extracted from
chunk W1 is 98.5% over the top 1/100 of the list,
93.1% over the top half of the list, and 88.3% over
the entire list of one million facts. Table 2 shows
examples of erroneous facts extracted from chunk
W1. Causes of errors include incorrect approxima-
tions of the name boundaries (e.g., Alma in Alma
Theresa Rausch is incorrectly tagged as an adjec-
tive), and selection of the wrong year as birth year
(e.g., for Henry Lumbar).
In the case of famous people, the extracted facts
tend to capture the correct birth year for several
variations of the names, as shown in Table 3. Con-
versely, it is not necessary that a fact occur with
high frequency in order for it to be extracted,
which is an advantage over previous approaches
that rely strongly on redundancy (cf. (Cafarella et
al., 2005)). Table 4 illustrates a few of the cor-
rectly extracted facts that occur rarely on the Web.
4.4 Recall
In contrast to the assessment of precision, recall
can be evaluated automatically, based on external
814
Table 2: Incorrect facts extracted from the Web
Spurious Fact Context in Source Sentence
(Theresa Rausch, Alma Theresa Rausch was born
1912) on 9 March 1912
(Henry Lumbar, Henry Lumbar was born 1861
1937) and died 1937
(Concepcion Paxety, Maria de la Concepcion Paxety
1817) b. 08 Dec. 1817 St. Aug., FL.
(Mae Yaeger, Ella May/Mae Yaeger was born
1872) 20 May 1872 in Mt.
(Charles Whatley, Long, Charles Whatley b. 16
1821) FEB 1821 d. 29 AUG
(HOLT George W. HOLT (new line) George W. Holt
Holt, 1845) was born in Alabama in 1845
(David Morrish David Morrish (new line)
Canadian, 1953) Canadian, b. 1953
(Mary Ann, 1838) had a daughter, Mary Ann, who
was born in Tennessee in 1838
(Mrs. Blackmore, Mrs. Blackmore was born April
1918) 28, 1918, in Labaddiey
Table 3: Birth years extracted for both
pseudonyms and corresponding real names
Pseudonym Real Name Year
Gloria Estefan Gloria Fajardo 1957
Nicolas Cage Nicolas Kim Coppola 1964
Ozzy Osbourne John Osbourne 1948
Ringo Starr Richard Starkey 1940
Tina Turner Anna Bullock 1939
Tom Cruise Thomas Cruise Mapother IV 1962
Woody Allen Allen Stewart Konigsberg 1935
lists of birth dates of various people. We start by
collecting two gold standard sets of facts. The first
set is a random set of 609 actors and their birth
years from a Web compilation (GoldA). The sec-
ond set is derived from the set of questions used
in the Question Answering track (Voorhees and
Tice, 2000) of the Text REtrieval Conference from
1999 through 2002. Each question asking for the
birth date of a person (e.g., ?What year was Robert
Frost born??) results in a pair containing the per-
son?s name and the birth year specified in the an-
swer keys. Thus, the second gold standard set
contains 17 pairs of people and their birth years
(GoldT ). Table 5 shows examples of facts in each
of the gold standard sets.
Table 6 shows two types of recall scores com-
puted against the gold standard sets. The recall
scores over ?Gold take into consideration only the
set of person names from the gold standard with
some extracted year(s). More precisely, given that
some years were extracted for a person name, it
verifies whether they include the year specified in
the gold standard for that person name. Compar-
atively, the recall score denoted AllGold is com-
Table 4: Extracted facts that occur infrequently
Fact Source Domain
(Irvine J Forcier, 1912) geocities.com
(Marie Louise Azelie Chabert, 1861) vienici.com
(Jacob Shalles, 1750) selfhost.com
(Robert Chester Claggett, 1898) rootsweb.com
(Charoltte Mollett, 1843) rootsweb.com
(Nora Elizabeth Curran, 1979) jimtravis.com
Table 5: Composition of gold standard sets
Gold Set Composition and Examples of Facts
GoldA Actors (Web compilation) Nr. facts: 609
(Andie MacDowell, 1958), (Doris Day,
1924), (Diahann Carroll, 1935)
GoldT People (TREC QA track) Nr. facts: 17
(Davy Crockett, 1786), (Julius Caesar,
100 B.C.), (King Louis XIV, 1638)
puted over the entire set of names from the gold
standard.
For the GoldA set, the size of the ?Gold set of
person names changes little when the facts are ex-
tracted from chunk W1 vs. W2 vs. W3. The re-
call scores over ?Gold exhibit little variation from
one Web chunk to another, whereas the AllGold
score is slightly higher on the W3 chunk, prob-
ably due to a higher number of documents that
are relevant to the extraction task. When the facts
are extracted from a combination of two or three
of the available Web chunks, the recall scores
computed over AllGold are significantly higher as
the size of the ?Gold set increases. In compar-
ison, the recall scores over the growing ?Gold
set increases slightly with larger evaluation sets.
The highest value of the recall score for GoldA
is 89.9% over the ?Gold set, and 70.7% over
AllGold. The smaller size of the second gold stan-
dard set, GoldT , explains the higher variation of
the values shown in the lower portion of Table 6.
4.5 Comparison to Previous Results
Another recent approach specifically addresses the
problem of extracting facts from a similarly-sized
collection of Web documents. In (Cafarella et al,
2005), manually-prepared extraction rules are ap-
plied to a collection of 60 million Web documents
to extract entities of types Company and Country,
as well as facts of type Person-CeoOf-Company
and City-CapitalOf-Country. Based on manual
evaluation of precision and recall, a total of 23,128
company names are extracted at precision of 80%;
the number decreases to 1,116 at precision of 90%.
In addition, 2,402 Person-CeoOf-Company facts
815
Table 6: Automatic evaluation of recall, over two
gold standard sets GoldA (609 person names) and
GoldT (17 person names)
Gold Set Input Data Recall (%)
(Web Chunk) ?Gold AllGold
GoldA W1 86.4 49.4
W2 85.0 50.5
W3 86.3 54.1
W1+W2 88.5 64.5
W1+W2+W3 89.9 70.7
GoldT W1 81.8 52.9
W2 90.0 52.9
W3 100.0 64.7
W1+W2 81.8 52.9
W1+W2+W3 91.6 64.7
are extracted at precision 80%. The recall value is
80% at precision 90%. Recall is evaluated against
the set of company names extracted by the system,
rather than an external gold standard with pairs of
a CEO and a company name. As such, the result-
ing metric for evaluating recall used in (Cafarella
et al, 2005) is somewhat similar to, though more
relaxed than, the recall score over the ?Gold set
introduced in the previous section.
5 Conclusion
The combination of generalized extraction pat-
terns and similarity-driven ranking criteria results
in a fast-growth iterative approach for large-scale
fact extraction. From 10 Person-BornIn-Year facts
and no additional knowledge, a set of one million
facts of the same type is extracted from a collec-
tion of 100 million Web documents of arbitrary
quality, with a precision around 90%. This cor-
responds to a growth ratio of 100,000:1 between
the size of the extracted set of facts and the size
of the initial set of seed facts. To our knowledge,
the growth ratio and the number of extracted facts
are several orders of magnitude higher than in any
of the previous studies on fact extraction based on
either hand-written extraction rules (Cafarella et
al., 2005), or bootstrapping for relation and infor-
mation extraction (Agichtein and Gravano, 2000;
Lita and Carbonell, 2004). The next research steps
converge towards the automatic construction of a
searchable repository containing billions of facts
regarding people.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting
relations from large plaintext collections. In Proceedings
of the 5th ACM International Conference on Digital Li-
braries (DL-00), pages 85?94, San Antonio, Texas.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natural
Language Processing (ANLP-00), pages 224?231, Seattle,
Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the web. In Proceedings of the Human Lan-
guage Technology Conference (HLT-EMNLP-05), pages
563?570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the 1999
Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC-99),
pages 189?196, College Park, Maryland.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics (ACL-03), pages 1?7, Sapporo, Japan.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus
Discovery. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415?
422, Barcelona, Spain.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Annual
Meeting of the Association for Computational Linguistics
(ACL-90), pages 268?275, Pittsburgh, Pennsylvania.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of the 17th International Confer-
ence on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Linguistics
(COLING-ACL-98), pages 768?774, Montreal, Quebec.
L. Lita and J. Carbonell. 2004. Instance-based ques-
tion answering: A data driven approach. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-04), pages 396?403,
Barcelona, Spain.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of english words. In Proceedings of the 31st Annual
Meeting of the Association for Computational Linguistics
(ACL-93), pages 183?190, Columbus, Ohio.
E. Riloff and R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In Pro-
ceedings of the 16th National Conference on Artificial In-
telligence (AAAI-99), pages 474?479, Orlando, Florida.
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to IE pattern induction. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 379?386, Ann Arbor, Michigan.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-02),
pages 214?221, Philadelphia, Pennsylvania.
E.M. Voorhees and D.M. Tice. 2000. Building a question-
answering test collection. In Proceedings of the 23rd
International Conference on Research and Development
in Information Retrieval (SIGIR-00), pages 200?207,
Athens, Greece.
816
Proceedings of ACL-08: HLT, pages 19?27,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Open-Domain Classes and Class
Attributes from Web Documents and Query Logs
Marius Pas?ca
Google Inc.
Mountain View, California 94043
mars@google.com
Benjamin Van Durme?
University of Rochester
Rochester, New York 14627
vandurme@cs.rochester.edu
Abstract
A new approach to large-scale information
extraction exploits both Web documents and
query logs to acquire thousands of open-
domain classes of instances, along with rel-
evant sets of open-domain class attributes at
precision levels previously obtained only on
small-scale, manually-assembled classes.
1 Introduction
Current methods for large-scale information ex-
traction take advantage of unstructured text avail-
able from either Web documents (Banko et al,
2007; Snow et al, 2006) or, more recently, logs of
Web search queries (Pas?ca, 2007) to acquire use-
ful knowledge with minimal supervision. Given a
manually-specified target attribute (e.g., birth years
for people) and starting from as few as 10 seed facts
such as (e.g., John Lennon, 1941), as many as a
million facts of the same type can be derived from
unstructured text within Web documents (Pas?ca et
al., 2006). Similarly, given a manually-specified tar-
get class (e.g., Drug) with its instances (e.g., Vi-
codin and Xanax) and starting from as few as 5 seed
attributes (e.g., side effects and maximum dose for
Drug), other relevant attributes can be extracted for
the same class from query logs (Pas?ca, 2007). These
and other previous methods require the manual spec-
ification of the input classes of instances before any
knowledge (e.g., facts or attributes) can be acquired
for those classes.
?Contributions made during an internship at Google.
The extraction method introduced in this paper
mines a collection of Web search queries and a col-
lection of Web documents to acquire open-domain
classes in the form of instance sets (e.g., {whales,
seals, dolphins, sea lions,...}) associated with class
labels (e.g., marine animals), as well as large sets
of open-domain attributes for each class (e.g., circu-
latory system, life cycle, evolution, food chain and
scientific name for the class marine animals). In
this light, the contributions of this paper are four-
fold. First, instead of separately addressing the
tasks of collecting unlabeled sets of instances (Lin,
1998), assigning appropriate class labels to a given
set of instances (Pantel and Ravichandran, 2004),
and identifying relevant attributes for a given set of
classes (Pas?ca, 2007), our integrated method from
Section 2 enables the simultaneous extraction of
class instances, associated labels and attributes. Sec-
ond, by exploiting the contents of query logs during
the extraction of labeled classes of instances from
Web documents, we acquire thousands (4,583, to
be exact) of open-domain classes covering a wide
range of topics and domains. The accuracy reported
in Section 3.2 exceeds 80% for both instance sets
and class labels, although the extraction of classes
requires a remarkably small amount of supervision,
in the form of only a few commonly-used Is-A ex-
traction patterns. Third, we conduct the first study in
extracting attributes for thousands of open-domain,
automatically-acquired classes, at precision levels
over 70% at rank 10, and 67% at rank 20 as de-
scribed in Section 3.3. The amount of supervision is
limited to five seed attributes provided for only one
reference class. In comparison, the largest previous
19
Knowledge extracted from documents and queries
amino acids={phenylalanine, l?cysteine, tryptophan, glutamic acid, lysine, thr,
marine animals={whales, seals, dolphins, turtles, sea lions, fishes, penguins, squids,
movies={jay and silent bob strike back, romeo must die, we were soldiers, matrix,
zoonotic diseases={rabies, west nile virus, leptospirosis, brucellosis, lyme disease,
movies: [opening song, cast, characters, actors, film review, movie script,
zoonotic diseases: [scientific name, causative agent, mode of transmission,
Open?domain labeled classes of instances
marine animals: [circulatory system, life cycle, evolution, food chain, eyesight,
Open?domain class attributes
(2)
  ornithine, valine, serine, isoleucine, aspartic acid, aspartate, taurine, histidine,...}
  pacific walrus, aquatic birds, comb jellies, starfish, florida manatees, walruses,...}
  kill bill, thelma and louise, mad max, field of dreams, ice age, star wars,...}
  cat scratch fever, foot and mouth disease, venezuelan equine encephalitis,...}
amino acids: [titration curve, molecular formula, isoelectric point, density,
  extinction coefficient, pi, food sources, molecular weight, pka values,...]
  scientific name, skeleton, digestion, gestation period, reproduction, taxonomy,...]
  symbolism, special effects, soundboards, history, screenplay, director,...]
  life cycle, pathology, meaning, prognosis, incubation period, symptoms,...]
Qu
ery
 lo
gs
W
eb
 d
oc
um
en
ts
(1)
(2)
Figure 1: Overview of weakly-supervised extraction of
class instances, class labels and class attributes from Web
documents and query logs
study in attribute extraction reports results on a set
of 40 manually-assembled classes, and requires five
seed attributes to be provided as input for each class.
Fourth, we introduce the first approach to infor-
mation extraction from a combination of both Web
documents and search query logs, to extract open-
domain knowledge that is expected to be suitable
for later use. In contrast, the textual data sources
used in previous studies in large-scale information
extraction are either Web documents (Mooney and
Bunescu, 2005; Banko et al, 2007) or, recently,
query logs (Pas?ca, 2007), but not both.
2 Extraction from Documents and Queries
2.1 Open-Domain Labeled Classes of Instances
Figure 1 provides an overview of how Web docu-
ments and queries are used together to acquire open-
domain, labeled classes of instances (phase (1) in the
figure); and to acquire attributes that capture quan-
tifiable properties of those classes, by mining query
logs based on the class instances acquired from the
documents, while guiding the extraction based on a
few attributes provided as seed examples (phase (2)).
As described in Figure 2, the algorithm for de-
riving labeled sets of class instances starts with the
acquisition of candidate pairs {ME} of a class la-
bel and an instance, by applying a few extraction
patterns to unstructured text within Web documents
{D}, while guiding the extraction by the contents
of query logs {Q} (Step 1 in Figure 2). This is fol-
Input: set of Is-A extraction patterns {E}
. large repository of search queries {Q}
. large repository of Web docs {D}
. weighting parameters J?[0,1] and K?1..?
Output: set of pairs of a class label and an instance {<C,I>}
Variables: {S} = clusters of distributionally similar phrases
. {V} = vectors of contextual matches of queries in text
. {ME} = set of pairs of a class label and an instance
. {CS} = set of class labels
. {X}, {Y} = sets of queries
Steps:
01. {ME} = Match patterns {E} in docs {D} around {Q}
02. {V} = Match phrases {Q} in docs {D}
03. {S} = Generate clusters of queries based on vectors {V}
04. For each cluster of phrases S in {S}
05. {CS} = ?
06. For each query Q of S
07. Insert labels of Q from {ME} into {CS}
08. For each label CS of {CS}
09. {X} = Find queries of S with the label CS in {ME}
10. {Y} = Find clusters of {S} containing some query
10. with the label CS in {ME}
11. If |{X}| > J?|{S}|
12. If |{Y}| < K
13. For each query X of {X}
14. Insert pair <CS ,X> into output pairs {<C,I>}
15. Return pairs {<C,I>}
Figure 2: Acquisition of labeled sets of class instances
lowed by the generation of unlabeled clusters {S} of
distributionally similar queries, by clustering vectors
of contextual features collected around the occur-
rences of queries {Q} within documents {D} (Steps
2 and 3). Finally, the intermediate data {ME} and
{S} is merged and filtered into smaller, more accu-
rate labeled sets of instances (Steps 4 through 15).
Step 1 in Figure 2 applies lexico-syntactic pat-
terns {E} that aim at extracting Is-A pairs of an in-
stance (e.g., Google) and an associated class label
(e.g., Internet search engines) from text. The two
patterns, which are inspired by (Hearst, 1992) and
have been the de-facto extraction technique in previ-
ous work on extracting conceptual hierarchies from
text (cf. (Ponzetto and Strube, 2007; Snow et al,
2006)), can be summarized as:
?[..] C [such as|including] I [and|,|.]?,
where I is a potential instance (e.g., Venezuelan
equine encephalitis) and C is a potential class label
for the instance (e.g., zoonotic diseases), for exam-
ple in the sentence: ?The expansion of the farms
increased the spread of zoonotic diseases such as
Venezuelan equine encephalitis [..]?.
During matching, all string comparisons are case-
insensitive. In order for a pattern to match a sen-
tence, two conditions must be met. First, the class
20
label C from the sentence must be a non-recursive
noun phrase whose last component is a plural-form
noun (e.g., zoonotic diseases in the above sentence).
Second, the instance I from the sentence must also
occur as a complete query somewhere in the query
logs {Q}, that is, a query containing the instance and
nothing else. This heuristic acknowledges the dif-
ficulty of pinpointing complex entities within doc-
uments (Downey et al, 2007), and embodies the
hypothesis that, if an instance is prominent, Web
search users will eventually ask about it.
In Steps 4 through 14 from Figure 2, each clus-
ter is inspected by scanning all labels attached to
one or more queries from the cluster. For each la-
bel CS , if a) {ME} indicates that a large number
of all queries from the cluster are attached to the la-
bel (as controlled by the parameter J in Step 12);
and b) those queries are a significant portion of all
queries from all clusters attached to the same label
in {ME} (as controlled by the parameter K in Step
13), then the label CS and each query with that la-
bel are stored in the output pairs {<C,I>} (Steps
13 and 14). The parameters J and K can be used
to emphasize precision (higher J and lower K) or
recall (lower J and higher K). The resulting pairs
of an instance and a class label are arranged into
sets of class instances (e.g., {rabies, west nile virus,
leptospirosis,...}), each associated with a class label
(e.g., zoonotic diseases), and returned in Step 15.
2.2 Open-Domain Class Attributes
The labeled classes of instances collected automat-
ically from Web documents are passed as input
to phase (2) from Figure 1, which acquires class
attributes by mining a collection of Web search
queries. The attributes capture properties that are
relevant to the class. The extraction of attributes ex-
ploits the set of class instances rather than the asso-
ciated class label, and consists of four stages:
1) identification of a noisy pool of candidate at-
tributes, as remainders of queries that also contain
one of the class instances. In the case of the class
movies, whose instances include jay and silent bob
strike back and kill bill, the query ?cast jay and
silent bob strike back? produces the candidate at-
tribute cast;
2) construction of internal search-signature vector
representations for each candidate attribute, based
on queries (e.g., ?cast selection for kill bill?) that
contain a candidate attribute (cast) and a class in-
stance (kill bill). These vectors consist of counts
tied to the frequency with which an attribute occurs
with a given ?templatized? query. The latter replaces
specific attributes and instances from the query with
common placeholders, e.g., ?X for Y?;
3) construction of a reference internal search-
signature vector representation for a small set of
seed attributes provided as input. A reference vec-
tor is the normalized sum of the individual vectors
corresponding to the seed attributes;
4) ranking of candidate attributes with respect to
each class (e.g., movies), by computing similarity
scores between their individual vector representa-
tions and the reference vector of the seed attributes.
The result of the four stages is a ranked list of
attributes (e.g., [opening song, cast, characters,...])
for each class (e.g., movies).
In a departure from previous work, the instances
of each input class are automatically generated as
described earlier, rather than manually assembled.
Furthermore, the amount of supervision is limited
to seed attributes being provided for only one of
the classes, whereas (Pas?ca, 2007) requires seed at-
tributes for each class. To this effect, the extrac-
tion includes modifications such that only one ref-
erence vector is constructed internally from the seed
attributes during the third stage, rather one such vec-
tor for each class in (Pas?ca, 2007); and similarity
scores are computed cross-class by comparing vec-
tor representations of individual candidate attributes
against the only reference vector available during the
fourth stage, rather than with respect to the reference
vector of each class in (Pas?ca, 2007).
3 Evaluation
3.1 Textual Data Sources
The acquisition of open-domain knowledge, in the
form of class instances, labels and attributes, re-
lies on unstructured text available within Web doc-
uments maintained by, and search queries submitted
to, the Google search engine.
The collection of queries is a random sample of
fully-anonymized queries in English submitted by
Web users in 2006. The sample contains approx-
imately 50 million unique queries. Each query is
21
Found in Count Pct. Examples
WordNet?
Yes 1931 42.2% baseball players,
(original) endangered species
Yes 2614 57.0% caribbean countries,
(removal) fundamental rights
No 38 0.8% agrochemicals, celebs,
handhelds, mangas
Table 1: Class labels found in WordNet in original form,
or found in WordNet after removal of leading words, or
not found in WordNet at all
accompanied by its frequency of occurrence in the
logs. The document collection consists of approx-
imately 100 million Web documents in English, as
available in a Web repository snapshot from 2006.
The textual portion of the documents is cleaned of
HTML, tokenized, split into sentences and part-of-
speech tagged using the TnT tagger (Brants, 2000).
3.2 Evaluation of Labeled Classes of Instances
Extraction Parameters: The set of instances that
can be potentially acquired by the extraction algo-
rithm described in Section 2.1 is heuristically lim-
ited to the top five million queries with the highest
frequency within the input query logs. In the ex-
tracted data, a class label (e.g., search engines) is
associated with one or more instances (e.g., google).
Similarly, an instance (e.g., google) is associated
with one or more class labels (e.g., search engines
and internet search engines). The values chosen
for the weighting parameters J and K from Sec-
tion 2.1 are 0.01 and 30 respectively. After dis-
carding classes with fewer than 25 instances, the ex-
tracted set of classes consists of 4,583 class labels,
each of them associated with 25 to 7,967 instances,
with an average of 189 instances per class.
Accuracy of Class Labels: Built over many years of
manual construction efforts, lexical gold standards
such as WordNet (Fellbaum, 1998) provide wide-
coverage upper ontologies of the English language.
Built-in morphological normalization routines make
it straightforward to verify whether a class label
(e.g., faculty members) exists as a concept in Word-
Net (e.g., faculty member). When an extracted label
(e.g., central nervous system disorders) is not found
in WordNet, it is looked up again after iteratively re-
moving its leading words (e.g., nervous system dis-
Class Label={Set of Instances} Parent in C?
WordNet
american composers={aaron copland, composers Y
eric ewazen, george gershwin,...}
modern appliances={built-in oven, appliances S
ceramic hob, tumble dryer,...}
area hospitals={carolinas medical hospitals S
center, nyack hospital,...}
multiple languages={chuukese, languages N
ladino, mandarin, us english,...}
Table 2: Correctness judgments for extracted classes
whose class labels are found in WordNet only after re-
moval of their leading words (C=Correctness, Y=correct,
S=subjectively correct, N=incorrect)
orders, system disorders and disorders).
As shown in Table 1, less than half of the 4,583
extracted class labels (e.g., baseball players) are
found in their original forms in WordNet. The ma-
jority of the class labels (2,614 out of 4,583) can be
found in WordNet only after removal of one or more
leading words (e.g., caribbean countries), which
suggests that many of the class labels correspond to
finer-grained, automatically-extracted concepts that
are not available in the manually-built WordNet. To
test whether that is the case, a random sample of
200 class labels, out of the 2,614 labels found to
be potentially-useful specific concepts, are manually
annotated as correct, subjectively correct or incor-
rect, as shown in Table 2. A class label is: correct,
if it captures a relevant concept although it could not
be found in WordNet; subjectively correct, if it is
relevant not in general but only in a particular con-
text, either from a subjective viewpoint (e.g., mod-
ern appliances), or relative to a particular tempo-
ral anchor (e.g., current players), or in connection
to a particular geographical area (e.g., area hospi-
tals); or incorrect, if it does not capture any use-
ful concept (e.g., multiple languages). The manual
analysis of the sample of 200 class labels indicates
that 154 (77%) are relevant concepts and 27 (13.5%)
are subjectively relevant concepts, for a total of 181
(90.5%) relevant concepts, whereas 19 (9.5%) of the
labels are incorrect. It is worth emphasizing the im-
portance of automatically-collected classes judged
as relevant and not present in WordNet: caribbean
countries, computer manufacturers, entertainment
companies, market research firms are arguably very
useful and should probably be considered as part of
22
Class Label Size of Instance Sets Class Label Size of Instance Sets
M (Manual) E (Extracted) M E M?EM M (Manual) E (Extracted) M E M?EM
Actor actors 1500 696 23.73 Movie movies 626 2201 30.83
AircraftModel - 217 - - NationalPark parks 59 296 0
Award awards 200 283 13 NbaTeam nba teams 30 66 86.66
BasicFood foods 155 3484 61.93 Newspaper newspapers 599 879 16.02
CarModel car models 368 48 5.16 Painter painters 1011 823 22.45
CartoonChar cartoon 50 144 36 ProgLanguage programming 101 153 26.73
characters languages
CellPhoneModel cell phones 204 49 0 Religion religions 128 72 11.71
ChemicalElem chemicals 118 487 1.69 River river systems 167 118 15.56
City cities 589 3642 50.08 SearchEngine search engines 25 133 64
Company companies 738 7036 26.01 SkyBody constellations 97 37 1.03
Country countries 197 677 91.37 Skyscraper - 172 - -
Currency currencies 55 128 25.45 SoccerClub football clubs 116 101 22.41
DigitalCamera digital cameras 534 58 0.18 SportEvent sports events 143 73 12.58
Disease diseases 209 3566 65.55 Stadium stadiums 190 92 6.31
Drug drugs 345 1209 44.05 TerroristGroup terrorist groups 74 134 33.78
Empire empires 78 54 6.41 Treaty treaties 202 200 7.42
Flower flowers 59 642 25.42 University universities 501 1127 21.55
Holiday holidays 82 300 48.78 VideoGame video games 450 282 17.33
Hurricane - 74 - - Wine wines 60 270 56.66
Mountain mountains 245 49 7.75 WorldWarBattle battles 127 135 9.44
Total mapped: 37 out of 40 classes - - 26.89
Table 3: Comparison between manually-assembled instance sets of gold-standard classes (M ) and instance sets of
automatically-extracted classes (E). Each gold-standard class (M ) was manually mapped into an extracted class (E),
unless no relevant mapping was found. Ratios ( M?EM ) are shown as percentages
any refinements to hand-built hierarchies, including
any future extensions of WordNet.
Accuracy of Class Instances: The computation of
the precision of the extracted instances (e.g., fifth el-
ement and kill bill for the class label movies) relies
on manual inspection of all instances associated to
a sample of the extracted class labels. Rather than
inspecting a random sample of classes, the evalua-
tion validates the results against a reference set of 40
gold-standard classes that were manually assembled
as part of previous work (Pas?ca, 2007). A class from
the gold standard consists of a manually-created
class label (e.g., AircraftModel) associated with a
manually-assembled, and therefore high-precision,
set of representative instances of the class.
To evaluate the precision of the extracted in-
stances, the manual label of each gold-standard class
(e.g., SearchEngine) is mapped into a class label ex-
tracted from text (e.g., search engines). As shown
in the first two columns of Table 3, the mapping into
extracted class labels succeeds for 37 of the 40 gold-
standard classes. 28 of the 37 mappings involve
linking an abstract class label (e.g., SearchEngine)
with the corresponding plural forms among the ex-
tracted class labels (e.g., search engines). The re-
maining 9 mappings link a manual class label with
either an equivalent extracted class label (e.g., Soc-
cerClub with football clubs), or a strongly-related
class label (e.g., NationalPark with parks). No map-
ping is found for 3 out of the 40 classes, namely Air-
craftModel, Hurricane and Skyscraper, which are
therefore removed from consideration.
The sizes of the instance sets available for each
class in the gold standard are compared in the third
through fifth columns of Table 3. In the table, M
stands for manually-assembled instance sets, and E
for automatically-extracted instance sets. For ex-
ample, the gold-standard class SearchEngine con-
tains 25 manually-collected instances, while the
parallel class label search engines contains 133
automatically-extracted instances. The fifth col-
umn shows the percentage of manually-collected in-
stances (M ) that are also extracted automatically
(E). In the case of the class SearchEngine, 16 of the
25 manually-collected instances are among the 133
automatically-extracted instances of the same class,
23
Label Value Examples of Attributes
vital 1.0 investors: investment strategies
okay 0.5 religious leaders: coat of arms
wrong 0.0 designers: stephanie
Table 4: Labels for assessing attribute correctness
which corresponds to a relative coverage of 64%
of the manually-collected instance set. Some in-
stances may occur within the manually-collected set
but not the automatically-extracted set (e.g., zoom-
info and brainbost for the class SearchEngine) or,
more frequently, vice-versa (e.g., surfwax, blinkx,
entireweb, web wombat, exalead etc.). Overall,
the relative coverage of automatically-extracted in-
stance sets with respect to manually-collected in-
stance sets is 26.89%, as an average over the 37
gold-standard classes. More significantly, the size
advantage of automatically-extracted instance sets
is not the undesirable result of those sets contain-
ing many spurious instances. Indeed, the manual
inspection of the automatically-extracted instances
sets indicates an average accuracy of 79.3% over the
37 gold-standard classes retained in the experiments.
To summarize, the method proposed in this paper ac-
quires open-domain classes from unstructured text
of arbitrary quality, without a-priori restrictions to
specific domains of interest and with virtually no su-
pervision (except for the ubiquitous Is-A extraction
patterns), at accuracy levels of around 90% for class
labels and 80% for class instances.
3.3 Evaluation of Class Attributes
Extraction Parameters: Given a target class spec-
ified as a set of instances and a set of five seed at-
tributes for a class (e.g., {quality, speed, number of
users, market share, reliability} for SearchEngine),
the method described in Section 2.2 extracts ranked
lists of class attributes from the input query logs.
Internally, the ranking uses Jensen-Shannon (Lee,
1999) to compute similarity scores between internal
representations of seed attributes, on one hand, and
each of the candidate attributes, on the other hand.
Evaluation Procedure: To remove any possible
bias towards higher-ranked attributes during the as-
sessment of class attributes, the ranked lists of at-
tributes to be evaluated are sorted alphabetically into
a merged list. Each attribute of the merged list is
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Holiday
manually assembled instances
automatically extracted instances
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
manually assembled instances
automatically extracted instances
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Mountain
manually assembled instances
automatically extracted instances
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
manually assembled instances
automatically extracted instances
Figure 3: Accuracy of attributes extracted based on man-
ually assembled, gold standard (M ) vs. automatically ex-
tracted (E) instance sets, for a few target classes (left-
most graphs) and as an average over all (37) target classes
(rightmost graphs). Seed attributes are provided as input
for each target class (top graphs), or for only one target
class (bottom graphs)
manually assigned a correctness label within its re-
spective class. An attribute is vital if it must be
present in an ideal list of attributes of the class; okay
if it provides useful but non-essential information;
and wrong if it is incorrect.
To compute the overall precision score over a
ranked list of extracted attributes, the correctness la-
bels are converted to numeric values as shown in Ta-
ble 4. Precision at some rank N in the list is thus
measured as the sum of the assigned values of the
first N candidate attributes, divided by N .
Accuracy of Class Attributes: Figure 3 plots pre-
cision values for ranks 1 through 50 of the lists of
attributes extracted through several runs over the 37
gold-standard classes described in the previous sec-
tion. The runs correspond to different amounts of
supervision, specified through a particular choice in
the number of seed attributes, and in the source of
instances passed as input to the system:
? number of input seed attributes: seed attributes
are provided either for each of the 37 classes, for a
total of 5?37=185 attributes (the graphs at the top of
Figure 3); or only for one class (namely, Country),
24
Class Precision Top Ten Extracted Attributes
# Class Label={Set of Instances} @5 @10 @15 @20
1 accounting systems={flexcube, 0.70 0.70 0.77 0.70 overview, architecture, interview questions, free
myob, oracle financials, downloads, canadian version, passwords, modules,
peachtree accounting, sybiz,...} crystal reports, property management, free trial
2 antimicrobials={azithromycin, 1.00 1.00 0.93 0.95 chemical formula, chemical structure, history,
chloramphenicol, fusidic acid, invention, inventor, definition, mechanism of
quinolones, sulfa drugs,...} action, side-effects, uses, shelf life
5 civilizations={ancient greece, 1.00 1.00 0.93 0.90 social pyramid, climate, geography, flag,
chaldeans, etruscans, inca population, social structure, natural resources,
indians, roman republic,...} family life, god, goddesses
9 farm animals={angora goats, 1.00 0.80 0.83 0.80 digestive system, evolution, domestication,
burros, cattle, cows, donkeys, gestation period, scientific name, adaptations,
draft horses, mule, oxen,...} coloring pages, p**, body parts, selective breeding
10 forages={alsike clover, rye grass, 0.90 0.95 0.73 0.57 types, picture, weed control, planting, uses,
tall fescue, sericea lespedeza,...} information, herbicide, germination, care, fertilizer
Average-Class (25 classes) 0.75 0.70 0.68 0.67
Table 5: Precision of attributes extracted for a sample of 25 classes. Seed attributes are provided for only one class.
for a total of 5 attributes over all classes (the graphs
at the bottom of Figure 3);
? source of input instance sets: the instance sets
for each class are either manually collected (M from
Table 3), or automatically extracted (E from Ta-
ble 3). The choices correspond to the two curves
plotted in each graph in Figure 3.
The graphs in Figure 3 show the precision over
individual target classes (leftmost graphs), and as an
average over all 37 classes (rightmost graphs). As
expected, the precision of the extracted attributes as
an average over all classes is best when the input in-
stance sets are hand-picked (M ), as opposed to au-
tomatically extracted (E). However, the loss of pre-
cision from M to E is small at all measured ranks.
Table 5 offers an alternative view on the quality
of the attributes extracted for a random sample of
25 classes out of the larger set of 4,583 classes ac-
quired from text. The 25 classes are passed as in-
put for attribute extraction without modifications. In
particular, the instance sets are not manually post-
filtered or otherwise changed in any way. To keep
the time required to judge the correctness of all ex-
tracted attributes within reasonable limits, the eval-
uation considers only the top 20 (rather than 50) at-
tributes extracted per class. As shown in Table 5, the
method proposed in this paper acquires attributes for
automatically-extracted, open-domain classes, with-
out a-priori restrictions to specific domains of inter-
est and relying on only five seed attributes specified
for only one class, at accuracy levels reaching 70%
at rank 10, and 67% at rank 20.
4 Related Work
4.1 Acquisition of Classes of Instances
Although some researchers focus on re-organizing
or extending classes of instances already available
explicitly within manually-built resources such as
Wikipedia (Ponzetto and Strube, 2007) or Word-
Net (Snow et al, 2006) or both (Suchanek et al,
2007), a large body of previous work focuses on
compiling sets of instances, not necessarily labeled,
from unstructured text. The extraction proceeds
either iteratively by starting from a few seed ex-
traction rules (Collins and Singer, 1999), or by
mining named entities from comparable news arti-
cles (Shinyama and Sekine, 2004) or from multilin-
gual corpora (Klementiev and Roth, 2006).
A bootstrapping method (Riloff and Jones, 1999)
cautiously grows very small seed sets of five in-
stances of the same class, to fewer than 300 items
after 50 consecutive iterations, with a final preci-
sion varying between 46% and 76% depending on
the type of semantic lexicon. Experimental results
from (Feldman and Rosenfeld, 2006) indicate that
named entity recognizers can boost the performance
of weakly supervised extraction of class instances,
but only for a few coarse-grained types such as Per-
son and only if they are simpler to recognize in
text (Feldman and Rosenfeld, 2006).
25
In (Cafarella et al, 2005), handcrafted extraction
patterns are applied to a collection of 60 million Web
documents to extract instances of the classes Com-
pany and Country. Based on the manual evaluation
of samples of extracted instances, an estimated num-
ber of 1,116 instances of Company are extracted at
a precision score of 90%. In comparison, the ap-
proach of this paper pursues a more aggressive goal,
by extracting a larger and more diverse number of
labeled classes, whose instances are often more dif-
ficult to extract than country names and most com-
pany names, at precision scores of almost 80%.
The task of extracting relevant labels to describe
sets of documents, rather than sets of instances, is
explored in (Treeratpituk and Callan, 2006). Given
pre-existing sets of instances, (Pantel and Ravichan-
dran, 2004) investigates the task of acquiring appro-
priate class labels to the sets from unstructured text.
Various class labels are assigned to a total of 1,432
sets of instances. The accuracy of the class labels
is computed over a sample of instances, by manu-
ally assessing the correctness of the top five labels
returned by the system for each instance. The result-
ing mean reciprocal rank of 77% gives partial credit
to labels of an evaluated instance, even if only the
fourth or fifth assigned labels are correct. Our eval-
uation of the accuracy of class labels is stricter, as it
considers only one class label of a given instance at a
time, rather than a pool of the best candidate labels.
As a pre-requisite to extracting relations among
pairs of classes, the method described in (Davidov et
al., 2007) extracts class instances from unstructured
Web documents, by submitting pairs of instances as
queries and analyzing the contents of the top 1,000
documents returned by a Web search engine. For
each target class, a small set of instances must be
provided manually as seeds. As such, the method
can be applied to the task of extracting a large set of
open-domain classes only after manually enumerat-
ing through the entire set of target classes, and pro-
viding seed instances for each. Furthermore, no at-
tempt is made to extract relevant class labels for the
sets of instances. Comparatively, the open-domain
classes extracted in our paper have an explicit la-
bel in addition to the sets of instances, and do not
require identifying the range of the target classes
in advance, or providing any seed instances as in-
put. The evaluation methodology is also quite dif-
ferent, as the instance sets acquired based on the in-
put seed instances in (Davidov et al, 2007) are only
evaluated for three hand-picked classes, with preci-
sion scores of 90% for names of countries, 87% for
fish species and 68% for instances of constellations.
Our evaluation of the accuracy of class instances is
again stricter, since the evaluation sample is larger,
and includes more varied classes, whose instances
are sometimes more difficult to identify in text.
4.2 Acquisition of Class Attributes
Previous work on the automatic acquisition of at-
tributes for open-domain classes from text is less
general than the extraction method and experiments
presented in our paper. Indeed, previous evalua-
tions were restricted to small sets of classes (forty
classes in (Pas?ca, 2007)), whereas our evaluations
also consider a random, more diverse sample of
open-domain classes. More importantly, by drop-
ping the requirement of manually providing a small
set of seed attributes for each target class, and rely-
ing on only a few seed attributes specified for one
reference class, we harvest class attributes without
the need of first determining what the classes should
be, what instances they should contain, and from
which resources the instances should be collected.
5 Conclusion
In a departure from previous approaches to large-
scale information extraction from unstructured text
on the Web, this paper introduces a weakly-
supervised extraction framework for mining useful
knowledge from a combination of both documents
and search query logs. In evaluations over labeled
classes of instances extracted without a-priori re-
strictions to specific domains of interest and with
very little supervision, the accuracy exceeds 90%
for class labels, approaches 80% for class instances,
and exceeds 70% (at rank 10) and 67% (at rank 20)
for class attributes. Current work aims at expanding
the number of instances within each class while re-
taining similar precision levels; extracting attributes
with more consistent precision scores across classes
from different domains; and introducing confidence
scores in attribute extraction, allowing for the detec-
tion of classes for which it is unlikely to extract large
numbers of useful attributes from text.
26
References
M. Banko, Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the Web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-07), pages 2670?2676, Hyderabad, India.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), pages 224?231,
Seattle, Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the Web. In Proceedings of the Human
Language Technology Conference (HLT-EMNLP-05),
pages 563?570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceed-
ings of the 1999 Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99), pages 189?196, College
Park, Maryland.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by Web mining. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL-07), pages 232?239, Prague, Czech
Republic.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in Web text. In Proceed-
ings of the 20th International Joint Conference on Ar-
tificial Intelligence (IJCAI-07), pages 2733?2739, Hy-
derabad, India.
R. Feldman and B. Rosenfeld. 2006. Boosting unsu-
pervised relation extraction by using NER. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-ACL-
06), pages 473?481, Sydney, Australia.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
A. Klementiev and D. Roth. 2006. Weakly super-
vised named entity transliteration and discovery from
multilingual comparable corpora. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING-ACL-
06), pages 817?824, Sydney, Australia.
L. Lee. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-99), pages
25?32, College Park, Maryland.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th International
Conference on Computational Linguistics and the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL-98), pages 768?774, Mon-
treal, Quebec.
R. Mooney and R. Bunescu. 2005. Mining knowledge
from text using information extraction. SIGKDD Ex-
plorations, 7(1):3?10.
M. Pas? ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the World Wide Web
of facts - step one: the one-million fact extraction chal-
lenge. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI-06), pages 1400?
1405, Boston, Massachusetts.
M. Pas? ca. 2007. Organizing and searching the World
Wide Web of facts - step two: Harnessing the wisdom
of the crowds. In Proceedings of the 16th World Wide
Web Conference (WWW-07), pages 101?110, Banff,
Canada.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proceedings of the
2004 Human Language Technology Conference (HLT-
NAACL-04), pages 321?328, Boston, Massachusetts.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
National Conference on Artificial Intelligence (AAAI-
07), pages 1440?1447, Vancouver, British Columbia.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
Y. Shinyama and S. Sekine. 2004. Named entity dis-
covery using comparable news articles. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING-04), pages 848?853,
Geneva, Switzerland.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801?808, Sydney, Australia.
F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide
Web Conference (WWW-07), pages 697?706, Banff,
Canada.
P. Treeratpituk and J. Callan. 2006. Automatically la-
beling hierarchical clusters. In Proceedings of the 7th
Annual Conference on Digital Government Research
(DGO-06), pages 167?176, San Diego, California.
27
Proceedings of ACL-08: HLT, pages 994?1002,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining Parenthetical Translations from the Web by Word Alignment   Dekang Lin Shaojun Zhao? Benjamin Van Durme? Marius Pa?ca Google, Inc. University of Rochester University of Rochester Google, Inc. Mountain View Rochester Rochester Mountain View CA, 94043 NY, 14627 NY, 14627 CA, 94043 lindek@google.com zhao@cs.rochester.edu vandurme@cs.rochester.edu mars@google.com   Abstract 
Documents in languages such as Chinese, Japanese and Korean sometimes annotate terms with their translations in English inside a pair of parentheses. We present a method to extract such translations from a large collec-tion of web documents by building a partially parallel corpus and use a word alignment al-gorithm to identify the terms being translated. The method is able to generalize across the translations for different terms and can relia-bly extract translations that occurred only once in the entire web. Our experiment on Chinese web pages produced more than 26 million pairs of translations, which is over two orders of magnitude more than previous re-sults. We show that the addition of the ex-tracted translation pairs as training data provides significant increase in the BLEU score for a statistical machine translation sys-tem.  
1 Introduction In natural language documents, a term (word or phrase) is sometimes followed by its translation in another language in a pair of parentheses. We call these parenthetical translations. The following examples are from Chinese web pages  (we added underlines to indicate what is being translated): (1) ???????????Brookings Institution??? ??????????????????????????Jeremy Shapiro?????...  (2) ????????????????indigestion?????gastritis????????????. (3) ???????????not going to fly?????? (4) ?????????????(linear programming).                                                              ?Contributions made during an internship at Google 
The parenthetically translated terms are typically new words, technical terminologies, idioms, prod-ucts, titles of movies, books, songs, and names of persons, organizations locations, etc. Commonly, an author might use such a parenthetical when a given term has no standard translation (or translit-eration), and does not appear in conventional dic-tionaries.  That is, an author might expect a term to be an out-of-vocabulary item for the target reader, and thus helpfully provides a reference translation in situ. For example, in (1), the name Shapiro was transliterated as ???. The name has many other transliterations in web documents, such as ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ??? ..., where the three Chinese characters corresponds to the three sylla-bles in Sha-pi-ro respectively. Each syllable may be mapped into different characters: 'Sha' into ? or ?, 'pi' into ?, ?, ?, and 'ro' into ?, ?, ?, .... Variation is not limited to the effects of phonetic similarity.  Story titles, for instance, are commonly translated semantically, often leading to a number of translations that have similar meaning, yet differ greatly in lexicographic form. For example, while the movie title Syriana is sometimes phonetically transliterated as ???, ???, it may also be trans-lated semantically according to the plot of the movie, e.g., ??? (mystery in mystery), ?? (real log), ???  (spy against spy), ????  (oil-triggered secret war), ??? (Syria), ?? (mystery journey), ...  The parenthetical translations are extremely valuable both as a stand-alone on-line dictionary and as training data for statistical machine transla-tion systems. They provide fresh data (new words) and cover a much wider range of topics than typi-cal parallel training data for statistical machine translation systems. 
994
The main contribution of this paper is a method for mining parenthetical translations by treating text snippets containing candidate pairs as a par-tially parallel corpus and using a word alignment algorithm to establish the correspondences be-tween in-parenthesis and pre-parenthesis words.  This technique allows us to identify translation pairs even if they only appeared once on the entire web. As a result, we were able to obtain 26.7 mil-lion Chinese-English translation pairs from web documents in Chinese. This is over two orders of magnitude more than the number of extracted translation pairs in the previously reported results (Cao, et al 2007). The next section presents an overview of our al-gorithm, which is then detailed in Sections 3 and 4. We evaluate our results in Section 5 by comparison with bilingually linked Wikipedia titles and by us-ing the extracted pairs as additional training data in a statistical machine translation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern:  (4)             f1f2?fm (e1e2?en) which is a sequence of m non-English words fol-lowed by a sequence of n English words in paren-theses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well.  There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2?en is given and use a search en-gine to retrieve text snippets containing e1e2?en from predominately non-English web pages (Na-gata et al 2001, Kwok et al 2005). Another method (Cao et al 2007) is to go through a non-English corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a prede-fined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al 2005). For example, Table 1 lists a set of Chinese segments (with word-to-word translation underneath) that 
precede the English term Lower Egypt. Owing to the frequency with which ??? appears as a can-didate, and in varying contexts, one has a good reason to believe???is the correct translation of Lower Egypt. ?   ??   ??  ?   ?  ?? downstream  region is  down Egypt ?   ??            ??       ?     ?? center located-at  down Egypt ?   ??   ??     ?       ?     ?? and  so-called of  down Egypt ?   ??       ?     ?? called down Egypt Table 1: Chinese text preceding Lower Egypt Unfortunately, this heuristic does not hold as of-ten as one might imagine.  Consider the candidates for Channel Spacing in Table 2.  The suffix?? (gap) has the highest frequency count. It is none-theless an incomplete translation of Channel Spac-ing. The correct translations in rows c to h occurred with Channel Spacing only once. a ?     ?   ??    ?? ?  is   channel distance b ?  ?        ??          ?? its   channel distance c ?    ??                  ??                    ??          ?? in-addition-to  reducing  wave-passage  distance d ?  ?     ??      ?                 ??   ?? also showed have wave-passage  gap e ?      ?          ?      ?      ??  ?? also  therefore is   channel   gap f ? ?       ??      ?  ?? and  channel   ?s    gap g ?  ??      ??      ??      ?                 ??   ?? an   important property is signal-passage  gap h ?   ??       ??    ??     ??  ?? already  able   reach passage  gap Table 2: Text preceding Channel Spacing The crucial observation we make here is that al-though the words like ?? (in row g) co-occurred with Channel Spacing only once, there are many co-occurrences of ??and Channel in other candi-date pairs, such as: ? ? ? ? ?? ?? (Speech Channel) ? ? ?? ?? ?? (Block Flat Fading Channel) ? ?? B (Channel B) ? ?? ?? ?? (Fiber Channel Probes) 
995
? ?? ?? (Reverse Channel) ? ?? ?? ?? ?? (Reverse Channel) Unlike previous approaches that rely solely on the preceding text of a single English term to de-termine its translation, we treat the entire collection of candidate pairs as a partially parallel corpus and establish the correspondences between the words using a word alignment algorithm.  At first glance, word alignment appears to be a more difficult problem than the extraction of par-enthetical translations. Extraction of parenthetical translations need only determine the first pre-parenthesis word aligned with an in-parenthesis word, whereas word alignment requires the respec-tive linking of all such (pre,in)-parenthesis word pairs. However, by casting the problem as word alignment, we are able to generalize across in-stances involving different in-parenthesis terms, giving us a larger number of, and more varied, ex-ample contexts per word. For the examples in Table 2, the words??(channel), ?? (wave passage), ?? (signal pas-sage), and ?? (passage) are aligned with Channel, and the words??(distance) and ??  (gap) are aligned with Spacing. Given these alignments, the left boundary of the translated Chinese term is simply the leftmost word that is linked to one of the English words.  Our algorithm consists of two steps: Step 1 constructs a partially parallel corpus. This step takes as input a large collection of Chinese web pages and converts the sentences with pa-rentheses containing English text into pairs of candidates. Step 2 uses an unsupervised algorithm to align English and Chinese and identify the term being translated according to the left-most aligned Chinese word. If no word alignments can be es-tablished, the pair is not considered a translation. The next two sections present the details of each of the two steps. 3 Constructing a Partially Parallel Corpus 
3.1 Filtering out non-translations The first step of our algorithm is to extract paren-theticals and then filter out those that are not trans-lations. This filtering is required as parenthetical translations represent only a small fraction of the 
usages for parentheses (see Sec. 5.1).  Table 3 shows some example of parentheses that are not translations. The input to Step 1 is a collection of arbitrary web documents. We used the following criteria to identify candidate pairs: ? The pre-parenthesis text (Tp) is predominantly in Chinese and the in-parenthesis text (Ti) is pre-dominantly in English. ? The concatenation of the digits in Tp must be identical to the concatenation of the digits in Ti. For example, rows a, b and c in Table 3 can be ruled out this way. ? If Tp contains some text in English, the same text must also appear in Ti. This filters out row d. ? Remove the pairs where Ti is part of anchor text. This rule is often applied to instances like row e where the file type tends to be inside a clickable link to a media file. ? The punctuation characters in Tp must also ap-pear in Ti, unless they are quotation marks. The example in row f  is ruled out because ?/? is not found in the pre-parenthesis text.  Examples with translations in italic Function of the in-parenthesis text a ??????1.4~3.0?? (MacArthur, 1967) The range of its values is within 1.4~3.0 (MacArthur, 1967)  
to provide citation 
b ????/??? (VN901 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30)  
flight information 
c ??????255-8FT? sale of pool table (255-8FT)  product Id.  d // ??? // void main  ( void  ) // main program // void main  (void )  
function declaration 
e ????: ??? (DVD) movie title: Thousand Year Lake (DVD) 
DVD is the file type 
f ?? ? ?? ? ?? ( g/L) mass consumed by water sample (g/L) 
measurement unit 
g ?????? (Sensitive) gentle protective facial cream (Sensitive)  
to indicate the type of the cream  
h ????????????? (Ask Jeeves) Evaluation of Nine Main Search Engines in the US: Chapter 4 (Ask Jeeves) 
Chapter 4 is about Ask Jeeves 
Table 3: Other uses of parentheses 
996
The instances in rows g and h cannot be eliminated by these simple rules, and are filtered only later, as we fail to discover a convincing word alignment. 3.2  Constraining term boundaries Similar to (Cao et al 2007), we segmented the pre-parenthesis Chinese text and restrict the term boundary to be one of the segmentation bounda-ries. Since parenthetical translations are mostly translation of terms, it makes sense to further con-strain the left boundary of the Chinese side to be a term boundary. Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al 2003).  We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collec-tion of search engine query logs. Given a Chinese sentence, we first identify all (possibly overlapping) sequences of words in the sentence that match one of the top-5M queries. A matching sequence is called a maximal match if it is not properly contained in another matching se-quence. We then define the potential boundary positions to be the boundaries of maximal matches or words that are not covered by any of the top-5M queries.  3.3 Length-based trimming If there are numerous Chinese words preceding a pair of parentheses containing two English words, it is very unlikely for all but the right-most few Chinese words to be part of the translation of the English words. Including extremely long se-quences as potential candidates introduces signifi-cantly more noise and makes word alignment harder than necessary. We therefore trimmed the pre-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ? 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 
proposed (Brown et al 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in de-scending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of align-ment algorithms and found Competitive Linking to have one of the highest precision scores. A disad-vantage of Competitive Linking, however, is that the alignments are restricted word-to-word align-ments, which implies that multi-word expressions can only be partially linked at best.  4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj.  4.2 Link scoring We used ?2 (Gale and Church, 1991) as the link score in the modified competitive linking algo-rithm, although there are many other possible choices for the link scores, such as ?2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al 2005). The ?2 statistics for a pair of words ei and fj is computed as 
( )
( )( )( )( )dcdbcaba
bcad
++++
?
=
2
2?  where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing  fj; d is the number of sentence pairs containing nei-ther ei nor fj. 
997
The ?2 score ranges from 0 to 1. We set a threshold at 0.001, below which the ?2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a candi-date pair are expected to be translated, there should be a preference for linking the words towards the end of the Chinese text. One advantage of Com-petitive Linking is that it is quite easy to introduce such preferences into the algorithm, by using the word positions to break ties of the ?2 scores when sorting the word pairs. 4.4 Capturing syllable-level regularities Many of the parenthetical translations involve proper names, which are often transliterated ac-cording to the sound. Word alignment algorithms have generally ignored syllable-level regularities in transliterated terms.  Consider again the Shapiro example in the introduction section. There are nu-merous correct transliterations for the same Eng-lish word, some of which are not very frequent. For example, the word ???happens to have a similar ?2 score with Shapiro as the word ?? (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.  Previous approaches to parenthetical translations relied on specialized algorithms to deal with trans-literations (Cao et al 2007; Jiang et al 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not re-quire any additional resources such as pronuncia-tion dictionaries and bilingual dictionaries. In addition to computing the ?2 scores between words, we also compute the ?2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last three bytes. Since we used UTF-8 encoding, the first and last three bytes of a Chi-nese word, except in very rare cases, correspond to the first and last Chinese character of the word. Table 4 lists the English prefixes and suffixes that have the highest ?2 scores with the Chinese prefix?and suffix?. 
 Type Chinese English prefix ? sha, amo, cha, sum, haw, lav, lun, xia, xal, hnl, shy, eve, she, cfh, ? suffix ? rlo, llo, ouh, low, ilo, owe, lol, lor, zlo, klo, gue, ude, vir, row, oro, olo, aro, ulo, ero, iro, rro, loh, lok, ? Table 4: Example prefixes and suffixes with top ?2 In our modified version of the competitive link-ing algorithm, the link score of a pair of words is the sum of the ?2 scores of the words themselves, their prefixes and their suffixes.  In addition to syllable-level correspondences in transliterations, the ?2 scores of prefixes and suf-fixes can also capture correlations in morphologi-cally composed words. For example, the Chinese prefix ? (three) has a relatively high ?2 score with the English prefix tri. Such scores enable word alignments to be made that may otherwise be missed. Consider the following text snippet: ...... ?  ? ??? (triaziflam)  The correct translation for triaziflam is?????.  However, the Chinese term is segmented as ? + ? + ???. The association between? (three) and triaziflam is very weak because ?is a very frequent word, whereas triaziflam is an extremely rare word. With the addition of the ?2 score be-tween ?and tri, we were able to correctly estab-lish the connection between triaziflam and ?. It turns out to be quite effective to assume pre-fixes and suffixes of words consist of three bytes, despite its apparent simplicity. The benefit of ?2 scores for prefixes and suffixes is not limited to morphemes that happen to be three bytes long.  For example, the English morpheme ?du-? corresponds to the Chinese character ? (two). Although the ?2 between du and? won?t be computed, we do find high ?2 scores between? and due and between? and dua. The three letter prefixes account for many of the words with the du- prefix. 5 Experimental Results We extracted from Chinese web pages about 1.58 billion unique sentences with parentheses that con-tain ASCII text. We removed duplicate sentences so that duplications of web documents will not skew the statistics. By applying the filtering algo-rithm in Sec. 3.1, we constructed a partially paral-
998
lel corpus with 126,612,447 candidate pairs (46,791,841 unique), which is about 8% of the number of sentences. Using the word alignment algorithm in Sec. 4, we extracted 26,753,972 trans-lation pairs between 13,471,221 unique English terms and 11,577,206 unique Chinese terms. Parenthetical translations mined from the Web have mostly been evaluated by manual examina-tion of a small sample of results (usually a few hundred entries) or in a Cross Lingual Information Retrieval setup. There does not yet exist a common evaluation data set.  5.1 Evaluation with Wikipedia Our first evaluation is based on translations in Wikipedia, which contains far more terminology and proper names than bilingual dictionaries. We extracted the titles of Chinese and English Wikipe-dia articles that are linked to each other and treated them as gold standard translations. There are 79,714 such pairs. We removed the following types of pairs because they are not translations or are not terms: ? Pairs with identical strings. For example, both English and Chinese versions have an entry ti-tled ?.ch?; ? Pairs where the English term begins with a digit, e.g., ?245?, ?300 BC?, ?1991 in film?; ? Pairs where the English term matches the regu-lar expression ?List of .*?, e.g., ?List of birds?, ?List of cinemas in Hong Kong?; ? Pairs where the Chinese title does not have any non-ASCII code. For example, the English en-try ?Syncfusion? is linked to ?.NET Frame-work? in the Chinese Wikipedia. The resulting data set contains 68,131 transla-tion pairs between 62,581 Chinese terms and 67,613 English terms. Only a small percentage of terms have more than one translation.  Whenever there is more than one translation, we randomly pick one as the answer key. For each Chinese and English word in the Wikipedia data, we first find whether there is a translation for the word in the extracted translation pairs. The Coverage of the Wikipedia data is measured by the percentage of words for which one or more translations are found. We then see whether our most frequent translation is an Exact Match of the answer key in the Wikipedia data.    
  Coverage Exact Match Full 70.8% 36.4% -term 67.1% 34.8% -pre-suffix 67.6% 34.4% IBM 67.6% 31.2% LDC 10.8% 4.8% Table 5: Chinese to English Results    Coverage Exact Match Full 59.6% 27.9% -term 59.6% 27.5% -pre-suffix 58.9% 27.4% IBM 52.4% 13.4% LDC 3.0% 1.4% Table 6: English to Chinese Results  Table 5 and 6 show the Chinese-to-English and English-to-Chinese results for the following sys-tems: Full refers to our system described in Sec. 3 and 4; -term is the system without the use of query logs to restrict potential term boundary posi-tions (Sec. 3.2); -pre-suffix is the system without using the ?2 score of the prefixes and suffixes; IBM refers to a system where we substitute our word alignment algorithm with IBM Model 1 and Model 2 followed by the HMM alignment (Och and Ney 2003), which is a common configuration for the word align-ment components in machine translations systems; LDC refers to the LDC2.0 English to Chinese bilingual dictionary with 161,117 translation pairs. It can be seen that the use of queries to constrain boundary positions and the addition of ?2 scores of prefixes/suffixes improve the percentage of Exact Match. The IBM Model tends to make many more alignments than Completive Linking. While this is often beneficial for machine translation systems, it is not very suitable for creating bilingual dictionar-ies, where precision is of paramount importance. The LDC dictionary was manually compiled from diverse resources within LDC and (mostly) from the Internet. Its coverage of Wikipedia data is ex-tremely low, compared to our method.  
999
English Wikipedia Translation Parenthetical Translation Pumping lemma ??? ??1 Topic-prominent language ?????? ?????1 Yoido Full Gos-pel Church ???????? ??????1 First Bulgarian Empire ???????? ?????????2 Vespid ?? ??????2 Ibrahim Rugova ???????? ???3 Jerry West ?????? ???3 Nicky Butt ????? ??3 Benito Mussolini ???????? ????3 Ecology of Hong Kong ???? ?????* Paracetamol ?????? ????* Thermidor ?? ??* Udo ?? ?? Public opinion ?? ???? Michael Bay ???? ????? Dagestan ??????? ???? Battle of Leyte Gulf ????? ??????? Glock ????? ??? Ergonomics ????? ??? Frank Sinatra ??????? ?????? Zaragoza ????? ???? Komodo ???? ???? Eli Vance ????? ??????? Manitoba ???? ????? Giant Bottlenose Whale ????? ???? Exclusionary rule ?????? ?????? Computer worm ???? ????? Social network ????? ???? Glasgow School of Art ???????? ???????? Dee Hock ????? ????? Bondage ?? ?? The China Post ?????? ???? Rachel ?? ?? John Nash ????? ????? Hattusa ??? ??? Bangladesh ???? ??? Table 7: A random sample of non-exact-matches                                                            1the extracted translation is too short 2the extracted translation is too long 3the extracted translation contains only the last name *the extracted term is completely wrong.   
 Note that Exact Match is a rather stringent crite-rion. Table 7 shows a random sample of extracted parenthetical translations that failed the Exact Match test. Only a small percentage of them are genuine errors. We nonetheless adopted this meas-ure because it has the advantage of automated evaluation and our goal is mainly to compare the relative performances. To determine the upper bound of the coverage of our web data, for each Wikipedia English term we searched within the total set of available paren-thesized text fragments (our English candidate set before filtering as by Step 1).  We discovered 81% of the Wikipedia titles, which is approximately 10% above the coverage of our final output. This indicates a minor loss of recall because of mistakes made in filtering (Sec. 3.1) and/or word alignment.  5.2 Evaluation with term translation requests To evaluate the coverage of output produced by their method, Cao et al(2007) extracted English queries from the query log of a Chinese search en-gine. They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations. Ex-amining our own Chinese query logs, however, the most-frequent English queries appear to be naviga-tional queries instead of translation requests. We therefore used the following regular expression to identify queries that are unambiguously translation requests:  /^[a-zA-Z ]* ???$/ where???means ??s Chinese?. This regular ex-pression matched 1579 unique queries in the logs. We manually judged the translation for 200 of them. A small random sample of the 200 is shown in Table 8. The empty cells indicate that the Eng-lish term is missing from our translation pairs. We use * to mark incorrect translations. When com-pared with the sample queries in (Cao et al, 2007), the queries in our sample seem to contain more phrasal words and technical terminology. It is in-teresting to see that even though parenthetical translations tend to be out-of-vocabulary words, as we have remarked in the introduction, the sheer size of the web means that occasionally transla-tions of common words such as ?use? are some-times included as well. 
1000
We compared our results with translations ob-tained from Google and Yahoo?s translation serv-ices. The numbers of correct translations for the random sample of 200 queries are as follows: Systems Google Yahoo! Mined Mined+G Correct 115 84 116 135 Our system?s outputs (Mined) have the same accuracy as the Google Translate. Our outputs have results for 154 out of the 200 queries. The 46 missing results are considered incorrect. If we combine our results with Google Translate by looking up Google results for missing entries, the accuracy increases from 56% to 68% (Mined+G). If we treat the LDC Chinese-English Dictionary 2.0 as a translator, it only covers 20.5% of the 200 queries.  5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al 2003; Brants et al 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as 
additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms them-selves may be hard to come by.  Cao et al (2007), like us, used a 300GB collec-tion of web documents as input. They used super-vised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learn-ing and does not make a distinction between trans-lations and transliterations. Furthermore, we are able to extract two orders of magnitude more trans-lations from than (Cao et al, 2007). 7 Conclusion We presented a method to apply a word alignment algorithm on a partially parallel corpus to extract translation pairs from the web. Treating the transla-tion extraction problem as a word alignment prob-lem allowed us to generalize across instances involving different in-parenthesis terms. Our algo-rithm extends Competitive Linking to deal with multi-word alignments and takes advantage of word-internal correspondences between transliter-ated words or morphologically composed words. Finally, through our discussion of parallel Wikipe-dia topic titles as a gold standard, we presented the first evaluation of such an extraction system that went beyond manual judgments on small sized samples. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments.  
buckingham palace ???? chinadaily ???? coo ????? diammonium sulfate  emilio pucci ??????? finishing school ???? gloria ???? horny ?????* jam ?? lean six sigma ?????? meiosis ???? near miss ???? pachycephalosaurus ??? pops ???????? recreation vehicle ????? shanghai ethylene cracker complex  stenonychosaurus ??? theanine ??? use ?? with you all the time ??????????? Table 8: A small sample of manually judged query translations 
1001
References  T. Brants, A. Popat, P. Xu, F. Och and J. Dean, Large Language Models for Machine Translation, EMNLP-CoNLL-2007. P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Compu-tational Linguistics, 19(2):263?311. G. Cao, J. Gao and J.Y. Nie. 2007. A system to mine large-scale bilingual dictionaries from monolingual Web pages, MT Summit, pp. 57-64. T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguis-tics 19, 1. W. Gale and K. Church. 1991. Identifying word corre-spondence in parallel text. In Proceedings of the DARPA NLP Workshop. L. Jiang, M. Zhou, L.F. Chien, C. Niu. 2007. Named Entity Translation with Web Mining and Translitera-tion. In Proc. of IJCAI-2007. pp. 1629-1634. P. Koehn, F. Och and D. Marcu, Statistical Phrase-based Translation, In Proc. of HLT-NAACL 2003. K.L. Kwok, P. Deng, N. Dinstl, H.L. Sun, W. Xu, P. Peng, and J. Doyon. 2005. CHINET: a Chinese name finder system for document triage. In Proceedings of 2005 International Conference on Intelligence Analysis. I.D. Melamed. 2000. Models of translational equiva-lence among words. Computational Linguistics, 26(2):221?249. M. Nagata, T. Saito, and K. Suzuki. 2001. Using the Web as a bilingual dictionary. In Proc. of ACL 2001 DD-MT Workshop, pp.95-102. NIST. 2003. The NIST machine translation evaluations. http://www.nist.gov/speech/tests/mt/. F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19?51. I.A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of CICLing-2002, pp 1?15, Mexico City, Mexico. B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-criminative matching approach to word alignment. In Proc. of HLT/EMNLP-05. Vancouver, BC. J. Tiedemann. 2004. Word to word alignment strategies. In Proceedings of the 20th international Conference on Computational Linguistics. Geneva, Switzerland.  
J.C. Wu and J.S. Chang. 2007. Learning to Find English to Chinese Transliterations on the Web. In Proc. of EMNLP-CoNLL-2007.  pp.996-1004. Prague, Czech Republic. Y. Zhang, S. Vogel. 2005 Competitive Grouping in In-tegrated Phrase Segmentation and Alignment Model. in Proceedings of ACL-05 Workshop on Building and Parallel Text. Ann Arbor, MI.  
1002
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 620?628,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Latent Variable Models of Concept-Attribute Attachment
Joseph Reisinger?
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
This paper presents a set of Bayesian
methods for automatically extending the
WORDNET ontology with new concepts
and annotating existing concepts with
generic property fields, or attributes. We
base our approach on Latent Dirichlet Al-
location and evaluate along two dimen-
sions: (1) the precision of the ranked
lists of attributes, and (2) the quality of
the attribute assignments to WORDNET
concepts. In all cases we find that the
principled LDA-based approaches outper-
form previously proposed heuristic meth-
ods, greatly improving the specificity of
attributes at each concept.
1 Introduction
We present a Bayesian approach for simultane-
ously extending Is-A hierarchies such as those
found in WORDNET (WN) (Fellbaum, 1998) with
additional concepts, and annotating the resulting
concept graph with attributes, i.e., generic prop-
erty fields shared by instances of that concept. Ex-
amples of attributes include ?height? and ?eye-
color? for the concept Person or ?gdp? and ?pres-
ident? for Country. Identifying and extracting
such attributes relative to a set of flat (i.e., non-
hierarchically organized) labeled classes of in-
stances has been extensively studied, using a vari-
ety of data, e.g., Web search query logs (Pas?ca and
Van Durme, 2008), Web documents (Yoshinaga
and Torisawa, 2007), and Wikipedia (Suchanek et
al., 2007; Wu and Weld, 2008).
Building on the current state of the art in at-
tribute extraction, we propose a model-based ap-
proach for mapping flat sets of attributes anno-
tated with class labels into an existing ontology.
This inference problem is divided into two main
components: (1) identifying the appropriate par-
ent concept for each labeled class and (2) learning
?Contributions made during an internship at Google.
the correct level of abstraction for each attribute in
the extended ontology. For example, consider the
task of annotating WN with the labeled class re-
naissance painters containing the class instances
Pisanello, Hieronymus Bosch, and Jan van Eyck
and associated with the attributes ?famous works?
and ?style.? Since there is no WN concept for
renaissance painters, the latter would need to be
mapped into WN under, e.g., Painter. Further-
more, since ?famous works? and ?style? are not
specific to renaissance painters (or even the WN
concept Painter), they should be placed at the
most appropriate level of abstraction, e.g., Artist.
In this paper, we show that both of these goals
can be realized jointly using a probabilistic topic
model, namely hierarchical Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003b).
There are three main advantages to using a topic
model as the annotation procedure: (1) Unlike hi-
erarchical clustering (Duda et al, 2000), the at-
tribute distribution at a concept node is not com-
posed of the distributions of its children; attributes
found specific to the concept Painter would not
need to appear in the distribution of attributes for
Person, making the internal distributions at each
concept more meaningful as attributes specific to
that concept; (2) Since LDA is fully Bayesian, its
model semantics allow additional prior informa-
tion to be included, unlike standard models such as
Latent Semantic Analysis (Hofmann, 1999), im-
proving annotation precision; (3) Attributes with
multiple related meanings (i.e., polysemous at-
tributes) are modeled implicitly: if an attribute
(e.g., ?style?) occurs in two separate input classes
(e.g., poets and car models), then that attribute
might attach at two different concepts in the ontol-
ogy, which is better than attaching it at their most
specific common ancestor (Whole) if that ancestor
is too general to be useful. However, there is also
a pressure for these two occurrences to attach to a
single concept.
We use WORDNET 3.0 as the specific test on-
tology for our annotation procedure, and evalu-
620
anticancer drugs: mechanism of action, uses, extrava-
sation, solubility, contraindications, side effects, chem-
istry, molecular weight, history, mode of action
bollywood actors: biography, filmography, age, bio-
data, height, profile, autobiography, new wallpapers, lat-
est photos, family pictures
citrus fruits: nutrition, health benefits, nutritional value,
nutritional information, calories, nutrition facts, history
european countries: population, flag, climate, presi-
dent, economy, geography, currency, population density,
topography, vegetation, religion, natural resources
london boroughs: population, taxis, local newspapers,
mp, lb, street map, renault connexions, local history
microorganisms: cell structure, taxonomy, life cycle,
reproduction, colony morphology, scientific name, vir-
ulence factors, gram stain, clipart
renaissance painters: early life, bibliography, short bi-
ography, the david, bio, painting, techniques, homosexu-
ality, birthplace, anatomical drawings, famous paintings
Figure 1: Examples of labeled attribute sets ex-
tracted using the method from (Pas?ca and Van
Durme, 2008).
ate three variants: (1) a fixed structure approach
where each flat class is attached to WN using
a simple string-matching heuristic, and concept
nodes are annotated using LDA, (2) an extension
of LDA allowing for sense selection in addition to
annotation, and (3) an approach employing a non-
parametric prior over tree structures capable of in-
ferring arbitrary ontologies.
The remainder of this paper is organized as fol-
lows: ?2 describes the full ontology annotation
framework, ?3 introduces the LDA-based topic
models, ?4 gives the experimental setup, ?5 gives
results, ?6 gives related work and ?7 concludes.
2 Ontology Annotation
Input to our ontology annotation procedure con-
sists of sets of class instances (e.g., Pisanello,
Hieronymus Bosch) associated with class labels
(e.g., renaissance painters) and attributes (e.g.,
?birthplace?, ?famous works?, ?style? and ?early
life?). Clusters of noun phrases (instances) are
constructed using distributional similarity (Lin
and Pantel, 2002; Hearst, 1992) and are labeled
by applying ?such-as? surface patterns to raw Web
text (e.g., ?renaissance painters such as Hierony-
mous Bosch?), yielding 870K instances in more
than 4500 classes (Pas?ca and Van Durme, 2008).
Attributes for each flat labeled class are ex-
tracted from anonymized Web search query
logs using the minimally supervised procedure
in (Pas?ca, 2008)1. Candidate attributes are ranked
based on their weighted Jaccard similarity to a
set of 5 manually provided seed attributes for the
1Similar query data, including query strings and fre-
quency counts, is available from, e.g., (Gao et al, 2007)
LDA
?
?
z
?
D
T
w
?
?
?
z
?
D
T
w
?
c
Fixed Structure LDA
?
?
z
?
D
?
w
?
T
c
?
nCRP
T
ww
w
Figure 2: Graphical models for the LDA variants;
shaded nodes indicate observed quantities.
class european countries. Figure 1 illustrates sev-
eral such labeled attribute sets (the underlying in-
stances are not depicted). Naturally, the attributes
extracted are not perfect, e.g., ?lb? and ?renault
connexions? as attributes for london boroughs.
We propose a set of Bayesian generative models
based on LDA that take as input labeled attribute
sets generated using an extraction procedure such
as the above and organize the attributes in WN ac-
cording to their level of generality. Annotating
WN with attributes proceeds in three steps: (1)
attaching labeled attribute sets to leaf concepts in
WN using string distance, (2) inferring an attribute
model using one of the LDA variants discussed in
? 3, and (3) generating ranked lists of attributes for
each concept using the model probabilities (? 4.3).
3 Hierarchical Topic Models
3.1 Latent Dirichlet Allocation
The underlying mechanism for our annotation
procedure is LDA (Blei et al, 2003b), a fully
Bayesian extension of probabilistic Latent Seman-
tic Analysis (Hofmann, 1999). Given D labeled
attribute sets wd, d ? D, LDA infers an unstruc-
tured set of T latent annotated concepts over
which attribute sets decompose as mixtures.2 The
latent annotated concepts represent semantically
coherent groups of attributes expressed in the data,
as shown in Example 1.
The generative model for LDA is given by
?d|? ? Dir(?), d ? 1 . . . D
?t|? ? Dir(?), t ? 1 . . . T
zi,d|?d ? Mult(?d), i ? 1 . . . |wd|
wi,d|?zi,d ? Mult(?zi,d), i ? 1 . . . |wd|
(1)
where ? and ? are hyperparameters smoothing
the per-attribute set distribution over concepts and
per-concept attribute distribution respectively (see
Figure 2 for the graphical model). We are inter-
ested in the case where w is known and we want
2In topic modeling literature, attributes are words and at-
tribute sets are documents.
621
to compute the conditional posterior of the remain-
ing random variables p(z,?,?|w). This distribu-
tion can be approximated efficiently using Gibbs
sampling. See (Blei et al, 2003b) and (Griffiths
and Steyvers, 2002) for more details.
(Example 1) Given 26 labeled attribute sets falling into
three broad semantic categories: philosophers, writers
and actors (e.g., sets for contemporary philosophers,
women writers, bollywood actors), LDA is able to infer
a meaningful set of latent annotated concepts:
quotations
teachings
virtue ethics
philosophies
biography
sayings
new movies
filmography
official website
biography
email address
autobiography
writing style
influences
achievements
bibliography
family tree
short biography
(philosopher) (writer) (actor)
(concept labels manually added for the latent annotated
concepts are shown in parentheses). Note that with a flat
concept structure, attributes can only be separated into
broad clusters, so the generality/specificity of attributes
cannot be inferred. Parameters were ?=1, ?=0.1, T=3.
3.2 Fixed-Structure LDA
In this paper, we extend LDA to model structural
dependencies between latent annotated concepts
(cf. (Li and McCallum, 2006; Sivic et al, 2008));
In particular, we fix the concept structure to cor-
respond to the WN Is-A hierarchy. Each labeled
attribute set is assigned to a leaf concept in WN
based on the edit distance between the concept la-
bel and the attribute set label. Possible latent con-
cepts for this set include the concepts along all
paths from its attachment point to the WN root,
following Is-A relation edges. Therefore, any two
labeled attribute sets share a number of latent con-
cepts based on their similarity in WN: all labeled
attribute sets share at least the root concept, and
may share more concepts depending on their most
specific, common ancestor. Under such a model,
more general attributes naturally attach to latent
concept nodes closer to the root, and more specific
attributes attach lower (Example 2).
Formally, we introduce into LDA an extra set of
random variables cd identifying the subset of con-
cepts in T available to attribute set d, as shown
in the diagram at the middle of Figure 2.3 For
example, with a tree structure, cd would be con-
strained to correspond to the concept nodes in T
on the path from the root to the leaf containing d.
Equation 1 can be adapted to this case if the in-
dex t is taken to range over concepts applicable to
attribute set d.
3Abusing notation, we use T to refer to a structured set of
concepts and to refer to the number of concepts in flat LDA
(Example 2 ) Fixing the latent concept structure to cor-
respond to WN (dark/purple nodes), and attaching each
labeled attribute set (examples depicted by light/orange
nodes) yields the annotated hierarchy:
works
picture
writings
history
biography
philosophy
natural rights
criticism
ethics
law
literary criticism
books
essays
short stories
novels 
tattoos
funeral
filmography
biographies
net worth
person
philosopher writer actor
scholar
intellectual
performer
entertainerliterate
communicator
bollywood
actors
women
writers
contemporary 
philosophers
Attribute distributions for the small nodes are not shown.
Dotted lines indicate multiple paths from the root, which
can be inferred using sense selection. Unlike with the flat
annotated concept structure, with a hierarchical concept
structure, attributes can be separated by their generality.
Parameters were set at ?=1 and ?=0.1.
3.3 Sense-Selective LDA
For each labeled attribute set, determining the ap-
propriate parent concept in WN is difficult since a
single class label may be found in many different
synsets (for example, the class bollywood actors
might attach to the ?thespian? sense of Actor or
the ?doer? sense). Fixed-hierarchy LDA can be
extended to perform automatic sense selection by
placing a distribution over the leaf concepts c, de-
scribing the prior probability of each possible path
through the concept tree. For WN, this amounts
to fixing the set of concepts to which a labeled at-
tribute set can attach (e.g., restricting it to a seman-
tically similar subset) and assigning a probability
to each concept (e.g., using the relative WN con-
cept frequencies). The probability for each sense
attachment cd becomes
p(cd|w, c?d, z) ? p(wd|c,w?d, z)p(cd|c?d),
i.e., the complete conditionals for sense selection.
p(cd|c?d) is the conditional probability for attach-
ing attribute set d at cd (e.g., simply the prior
p(cd|c?d)
def
= p(cd) in the WN case). A closed
form expression for p(wd|c,w?d, z) is derived
in (Blei et al, 2003a).
3.4 Nested Chinese Restaurant Process
In the final model, shown in the diagram on the
right side of Figure 2, LDA is extended hierarchi-
cally to infer arbitrary fixed-depth tree structures
622
from data. Unlike the fixed-structure and sense-
selective approaches which use the WN hierarchy
directly, the nCRP generates its own annotated hi-
erarchy whose concept nodes do not necessarily
correspond to WN concepts (Example 3). Each
node in the tree instead corresponds to a latent an-
notated concept with an arbitrary number of sub-
concepts, distributed according to a Dirichlet Pro-
cess (Ferguson, 1973). Due to its recursive struc-
ture, the underlying model is called the nested Chi-
nese Restaurant Process (nCRP). The model in
Equation 1 is extended with cd|? ? nCRP(?, L),
d ? D i.e., latent concepts for each attribute set are
drawn from an nCRP. The hyperparameter ? con-
trols the probability of branching via the per-node
Dirichlet Process, and L is the fixed tree depth.
An efficient Gibbs sampling procedure is given
in (Blei et al, 2003a).
(Example 3) Applying nCRP to the same three semantic
categories: philosophers, writers and actors, yields the
model:
biography
date of birth
childhood
picture
family
works
books
quotations
critics
poems
teachings
virtue ethics
structuralism
philosophies 
political theory
criticism
short stories
style
poems 
complete works
accomplishments
official website
profile
life story
achievements
filmography
pictures 
new movies
official site
works
(root)
(philosopher) (writer) (actor)
bollywood
actors
women
writers
contemporary 
philosophers
(manually added labels are shown in parentheses). Un-
like in WN, the inferred structure naturally places
philosopher and writer under the same subconcept,
which is also separate from actor. Hyperparameters were
?=0.1, ?=0.1, ?=1.0.
4 Experimental Setup
4.1 Data Analysis
We employ two data sets derived using the pro-
cedure in (Pas?ca and Van Durme, 2008): the full
set of automatic extractions generated in ? 2, and a
subset consisting of all attribute sets that fall under
the hierarchies rooted at the WN concepts living
thing#1 (i.e., the first sense of living thing), sub-
stance#7, location#1, person#1, organization#1
and food#1, manually selected to cover a high-
precision subset of labeled attribute sets. By com-
paring the results across the two datasets we can
measure each model?s robustness to noise.
In the full dataset, there are 4502 input attribute
sets with a total of 225K attributes (24K unique),
of which 8121 occur only once. The 10 attributes
occurring in the most sets (history, definition, pic-
ture(s), images, photos, clipart, timeline, clip art,
types) account for 6% of the total. For the subset,
there are 1510 attribute sets with 76K attributes
(11K unique), of which 4479 occur only once.
4.2 Model Settings
Baseline: Each labeled attribute set is mapped to
the most common WN concept with the closest la-
bel string distance (Pas?ca, 2008). Attributes are
propagated up the tree, attaching to node c if they
are contained in a majority of c?s children.
LDA: LDA is used to infer a flat set of T = 300
latent annotated concepts describing the data. The
concept selection smoothing parameter is set as
?=100. The smoother for the per-concept multi-
nomial over words is set as ?=0.1.4 The effects of
concept structure on attribute precision can be iso-
lated by comparing the structured models to LDA.
Fixed-Structure LDA (fsLDA): The latent con-
cept hierarchy is fixed based on WN (? 3.2), and
labeled attribute sets are mapped into it as in base-
line. The concept graph for each labeled attribute
set wd is decomposed into (possibly overlapping)
chains, one for each unique path from the WN root
to wd?s attachment point. Each path is assigned a
copy wd, reducing the bias in attribute sets with
many unique ancestor concepts.5 The final mod-
els contain 6566 annotated concepts on average.
Sense-Selective LDA (ssLDA): For the sense se-
lective approach (? 3.3), the set of possible sense
attachments for each attribute set is taken to be all
WN concepts with the lowest edit distance to its
label, and the conditional probability of each sense
attachment p(cd) is set proportional to its relative
frequency. This procedure results in 2 to 3 senses
per attribute set on average, yielding models with
7108 annotated concepts.
Arbitrary hierarchy (nCRP): For the arbitrary
hierarchy model (? 3.4), we set the maximum
tree depth L=5, per-concept attribute smoother
?=0.05, concept assignment smoother ?=10 and
nCRP branching proportion ?=1.0. The resulting
4(Parameter setting) Across all models, the main results
in this paper are robust to changes in ?. For nCRP, changes
in ? and ? affect the size of the learned model but have less
effect on the final precision. Larger values for L give the
model more flexibility, but take longer to train.
5Reducing the directed-acyclic graph to a tree ontology
did not significantly affect precision.
623
models span 380 annotated concepts on average.
4.3 Constructing Ranked Lists of Attributes
Given an inferred model, there are several ways to
construct ranked lists of attributes:
Per-Node Distribution: In fsLDA and ssLDA,
attribute rankings can be constructed directly for
each WN concept c, by computing the likelihood
of attribute w attaching to c, L(c|w) = p(w|c) av-
eraged over all Gibbs samples (discarding a fixed
number of samples for burn-in). Since c?s attribute
distribution is not dependent on the distributions
of its children, the resulting distribution is biased
towards more specific attributes.
Class-Entropy (CE): In all models, the inferred
latent annotated concepts can be used to smooth
the attribute rankings for each labeled attribute set.
Each sample from the posterior is composed of
two components: (1) a multinomial distribution
over a set of WN nodes, p(c|wd, ?) for each at-
tribute set wd, where the (discrete) values of c are
WN concepts, and (2) a multinomial distribution
over attributes p(w|c, ?) for each WN concept c.
To compute an attribute ranking for wd, we have
p(w|wd) =
?
c
p(w|c, ?)p(c|wd, ?).
Given this new ranking for each attribute set, we
can compute new rankings for each WN concept
c by averaging again over all the wd that appear
as (possible indirect) descendants of c. Thus, this
method uses LDA to first perform reranking on the
raw extractions before applying the baseline ontol-
ogy induction procedure (? 4.2).6
CE ranking exhibits a ?conservation of entropy?
effect, whereby the proportion of general to spe-
cific attributes in each attribute set wd remains the
same in the posterior. If set A contains 10 specific
attributes and 30 generic ones, then the latter will
be favored over the former in the resulting distri-
bution 3 to 1. Conservation of entropy is a strong
assumption, and in particular it hinders improving
the specificity of attribute rankings.
Class-Entropy+Prior: The LDA-based models
do not inherently make use of any ranking infor-
mation contained in the original extractions. How-
ever, such information can be incorporated in the
form of a prior. The final ranking method com-
bines CE with an exponential prior over the at-
tribute rank in the baseline extraction. For each
attribute set, we compute the probability of each
6One simple extension is to run LDA again on the CE
ranked output, yielding an iterative procedure; however, this
was not found to significantly affect precision.
attribute p(w|wd) = plda(w|wd)pbase(w|wd), as-
suming a parametric form for pbase(w|wd)
def
=
?r(w,wd). Here, r(w,wd) is the rank of w in at-
tribute set d. In all experiments reported, ?=0.9.
4.4 Evaluating Attribute Attachment
For the WN-based models, in addition to mea-
suring the average precision of the reranked at-
tributes, it is also useful to evaluate the assign-
ment of attributes to WN concepts. For this eval-
uation, human annotators were asked to determine
the most appropriate WN synset(s) for a set of gold
attributes, taking into account polysemous usage.
For each model, ranked lists of possible concept
assignments C(w) are generated for each attribute
w, usingL(c|w) for ranking. The accuracy of a list
C(w) for an attribute w is measured by a scoring
metric that corresponds to a modification (Pas?ca
and Alfonseca, 2009) of the mean reciprocal rank
score (Voorhees and Tice, 2000):
DRR = max
1
rank(c)? (1 + PathToGold)
where rank(c) is the rank (from 1 up to 10) of a
concept c in C(w), and PathToGold is the length
of the minimum path along Is-A edges in the con-
ceptual hierarchies between the concept c, on one
hand, and any of the gold-standard concepts man-
ually identified for the attribute w, on the other
hand. The length PathToGold is 0, if the returned
concept is the same as the gold-standard concept.
Conversely, a gold-standard attribute receives no
credit (that is, DRR is 0) if no path is found in
the hierarchies between the top 10 concepts of
C(w) and any of the gold-standard concepts, or if
C(w) is empty. The overalll precision of a given
model is the average of the DRR scores of individ-
ual attributes, computed over the gold assignment
set (Pas?ca and Alfonseca, 2009).
5 Results
5.1 Attribute Precision
Precision was manually evaluated relative to 23
concepts chosen for broad coverage.7 Table 1
shows precision at n and the Mean Average Preci-
sion (MAP); In all LDA-based models, the Bayes
average posterior is taken over all Gibbs samples
7(Precision evaluation) Attributes were hand annotated
using the procedure in (Pas?ca and Van Durme, 2008) and nu-
merical precision scores (1.0 for vital, 0.5 for okay and 0.0 for
incorrect) were assigned for the top 50 attributes per concept.
25 reference concepts were originally chosen, but 2 were not
populated with attributes in any method, and hence were ex-
cluded from the comparison.
624
Model Precision @ MAP
5 10 20 50
Base (unranked) 0.45 0.48 0.47 0.44 0.46
Base (ranked) 0.77 0.77 0.69 0.58 0.67
LDA? -24 ? 105
CE 0.64 0.53 0.52 0.56 0.55
CE+Prior 0.80 0.73 0.74 0.58 0.69
Fixed-structure (fsLDA) -22 ? 105
Per-Node 0.43 0.41 0.42 0.41 0.42
CE 0.75 0.68 0.63 0.55 0.63
CE+Prior 0.78 0.77 0.71 0.59 0.69
Sense-selective (ssLDA) -18 ? 105
Per-Node 0.37 0.44 0.42 0.41 0.42
CE 0.69 0.68 0.65 0.58 0.64
CE+Prior 0.81 0.80 0.72 0.60 0.70
nCRP? -14 ? 105
CE 0.74 0.76 0.73 0.65 0.72
CE+Prior 0.88 0.85 0.81 0.68 0.78
Subset only
Base (unranked) 0.61 0.62 0.62 0.60 0.62
Base (ranked) 0.79 0.82 0.72 0.65 0.72
?WN living thing 0.73 0.80 0.71 0.65 0.69
?WN substance 0.80 0.80 0.69 0.53 0.68
?WN location 0.95 0.93 0.84 0.75 0.84
?WN person 0.75 0.83 0.75 0.77 0.77
?WN organization 0.60 0.70 0.60 0.68 0.63
?WN food 0.90 0.85 0.58 0.45 0.64
Fixed-structure (fsLDA) -77 ? 104
Per-Node 0.64 0.58 0.52 0.56 0.55
CE 0.90 0.83 0.78 0.73 0.78
CE+Prior 0.88 0.86 0.80 0.66 0.78
?WN living thing 0.83 0.88 0.78 0.63 0.77
?WN substance 0.85 0.83 0.78 0.66 0.76
?WN location 0.95 0.95 0.88 0.75 0.85
?WN person 1.00 0.93 0.91 0.76 0.87
?WN organization 0.80 0.70 0.80 0.76 0.75
?WN food 0.80 0.70 0.63 0.40 0.59
nCRP? -45 ? 104
CE 0.88 0.88 0.78 0.71 0.79
CE+Prior 0.90 0.88 0.83 0.67 0.79
Table 1: Precision at n and mean-average preci-
sion for all models and data sets. Inset plots show
log-likelihood of each Gibbs sample, indicating
convergence except in the case of nCRP. ? indi-
cates models that do not generate annotated con-
cepts corresponding to WN nodes and hence have
no per-node scores.
after burn-in.8 The improvements in average pre-
cision are important, given the amount of noise in
the raw extracted data.
When prior attribute rank information (Per-
Node and CE scores) from the baseline extractions
is not incorporated, all LDA-based models outper-
form the unranked baseline (Table 1). In particu-
lar, LDA yields a 17% reduction in error (MAP)
8(Bayes average vs. maximum a-posteriori) The full
Bayesian average posterior consistently yielded higher preci-
sion than the maximum a-posteriori model. For the per-node
distributions, the fsLDA Bayes average model exhibits a 17%
reduction in relative error over the maximum a-posteriori es-
timate and for ssLDA there was a 26% reduction.
Model DRR Scores
all (n) found (n)
Base (unranked) 0.14 (150) 0.24 (91)
Base (ranked) 0.17 (150) 0.21 (123)
Fixed-structure
(fsLDA) 0.31 (150) 0.37 (128)
Sense-selective
(ssLDA) 0.31 (150) 0.37 (128)
Subset only
Base (unranked) 0.15 (97) 0.27 (54)
Base (ranked) 0.18 (97) 0.24 (74)
WN living thing 0.29 (27) 0.35 (22)
WN substance 0.21 (12) 0.32 (8)
WN location 0.12 (30) 0.17 (20)
WN person 0.37 (18) 0.44 (15)
WN organization 0.15 (31) 0.17 (27)
WN food 0.15 (6) 0.22 (4)
Fixed-structure
(fsLDA) 0.37 (97) 0.47 (77)
WN living thing 0.45 (27) 0.55 (22)
WN substance 0.48 (12) 0.64 (9)
WN location 0.34 (30) 0.44 (23)
WN person 0.44 (18) 0.52 (15)
WN organization 0.44 (31) 0.71 (19)
WN food 0.60 (6) 0.72 (5)
Table 2: All measures the DRR score relative to
the entire gold assignment set; found measures
DRR only for attributes with DRR(w)>0; n is the
number of scores averaged.
over the baseline, fsLDA yields a 31% reduction,
ssLDA yields a 33% reduction and nCRP yields
a 48% reduction (24% reduction over fsLDA).
Performance also improves relative to the ranked
baseline when prior ranking information is incor-
porated in the LDA-based models, as indicated
by CE+Prior scores in Table 1. LDA and fsLDA
reduce relative error by 6%, ssLDA by 9% and
nCRP by 33%. Furthermore, nCRP precision
without ranking information surpasses the base-
line with ranking information, indicating robust-
ness to extraction noise. Precision curves for indi-
vidual attribute sets are shown in Figure 3. Over-
all, learning unconstrained hierarchies (nCRP) in-
creases precision, but as the inferred node distri-
butions do not correspond to WN concepts they
cannot be used for annotation.
One benefit to using an admixture model like
LDA is that each concept node in the resulting
model contains a distribution over attributes spe-
cific only to that node (in contrast to, e.g., hierar-
chical agglomerative clustering). Although abso-
lute precision is lower as more general attributes
have higher average precision (Per-Node scores
in Table 1), these distributions are semantically
meaningful in many cases (Figure 4) and further-
more can be used to calculate concept assignment
precision for each attribute.9
9Per-node distributions (and hence DRR) were not evalu-
625
Figure 3: Precision (%) vs. rank plots (log scale) of attributes broken down across 18 labeled test attribute
sets. Ranked lists of attributes are generated using the CE+Prior method.
5.2 Concept Assignment Precision
The precision of assigning attributes to various
concepts is summarized in Table 2. Two scores are
given: all measures DRR relative to the entire gold
assignment set, and found measures DRR only for
attributes with DRR(w)>0. Comparing the scores
gives an estimate of whether coverage or precision
is responsible for differences in scores. fsLDA and
ssLDA both yield a 20% reduction in relative er-
ror (17.2% increase in absolute DRR) over the un-
ranked baseline and a 17.2% reduction (14.2% ab-
solute increase) over the ranked baseline.
5.3 Subset Precision and DRR
Precision scores for the manually selected subset
of extractions are given in the second half of Ta-
ble 1. Relative to the unranked baseline, fsLDA
and nCRP yield 42% and 44% reductions in er-
ror respectively, and relative to the ranked base-
line they both yield a 21.4% reduction. In terms of
absolute precision, there is no benefit to adding in
prior ranking knowledge to fsLDA or nCRP, in-
dicating diminishing returns as average baseline
precision increases (Baseline vs. fsLDA/nCRP CE
scores). Broken down across each of the subhier-
archies, LDA helps in all cases except food.
DRR scores for the subset are given in the lower
half of Table 2. Averaged over all gold test at-
tributes, DRR scores double when using fsLDA.
These results can be misleading, however, due
to artificially low coverage. Hence, Table 2 also
shows DRR scores broken down over each sub-
hierarchy, In this case fsLDA more than doubles
the DRR relative to the baseline for substance and
location, and triples it for organization and food.
ated for LDA or nCRP, because they are not mapped to WN.
6 Related Work
A large body of previous work exists on extend-
ing WORDNET with additional concepts and in-
stances (Snow et al, 2006; Suchanek et al, 2007);
these methods do not address attributes directly.
Previous literature in attribute extraction takes ad-
vantage of a range of data sources and extraction
procedures (Chklovski and Gil, 2005; Tokunaga
et al, 2005; Pas?ca and Van Durme, 2008; Yoshi-
naga and Torisawa, 2007; Probst et al, 2007; Van
Durme et al, 2008; Wu and Weld, 2008). How-
ever these methods do not address the task of de-
termining the level of specificity for each attribute.
The closest studies to ours are (Pas?ca, 2008), im-
plemented as the baseline method in this paper;
and (Pas?ca and Alfonseca, 2009), which relies on
heuristics rather than formal models to estimate
the specificity of each attribute.
7 Conclusion
This paper introduced a set of methods based on
Latent Dirichlet Allocation (LDA) for jointly ex-
tending the WORDNET ontology and annotating
its concepts with attributes (see Figure 4 for the
end result). LDA significantly outperformed a pre-
vious approach both in terms of the concept as-
signment precision (i.e., determining the correct
level of generality for an attribute) and the mean-
average precision of attribute lists at each concept
(i.e., filtering out noisy attributes from the base ex-
traction set). Also, relative precision of the attach-
ment models was shown to improve significantly
when the raw extraction quality increased, show-
ing the long-term viability of the approach.
626
entity
physical entity
bollywood
actors
actor
new wallpapers
upcoming movies
baby pictures
latest wallpapers
performer
filmography
new movies
schedule
new pictures
new pics
entertainer
hairstyle
hairstyles
music videos
songs
new pictures
sexy pictures
person
bio
autobiography
childhood
bibliography
accomplishments
timeline
organism
causal agent
living thing
photos
taxonomy
scientific name
reproduction
life cycle
habitat
whole
object
history
pictures
images
picture
photos
timeline
renaissance 
painters
painter
influenced
impressionist
the life
's paintings
style of
watercolor
artist
self portrait
paintings
famous works
self portraits
painting techniques
famous paintings
creator
influences
artwork
style
work
art
technique
european
countries
European
 country
recreation
national costume
prime minister
political parties
royal family
national parks
country
state codes
zipcodes
country profile
currencies
national anthem
telephone codes
administrative
 district
sights
weather forecast
culture
tourist spots
state map
district
traditional dress
per capita income
tourist spot
cuisine
folk dances
industrial policy
region
population
nightlife
street map
temperature
location
climate
tourist attractions
geography
weather
tourism
economy
drug
danger
half life
ingredients
side effects
withdrawal symptoms
sexual side effects
agent
pharmacokinetics
mechanism of action
long term effects
pharmacology
contraindications
mode of action
substance
matter
chemistry
ingredients
chemical structure
dangers
chemical formula
msds
liquors
liquor
drink mixes
apparitions
pitchers
existence
fantasy art
alcohol
carbohydrates
carbs
calories
alcohol content
pronunciation
glass
beveragedrug of abuse
sugar content
alcohol content
caffeine content
serving temperature
alcohol percentage
shelf life
liquid
food
advertisements
sugar content
adverts
brand
nutrition information
storage temperature
shelf life
nutritional facts
nutrition information
flavors
nutrition
nutritional information
fluid
recepies
gift baskets
receipes
rdi
daily allowance
fondue recipes
substance
density
uses
physical properties
melting point
chemical properties
chemical structure
abstraction
london
boroughs
borough
registry office
school term dates
local history
renault
citizens advice bureau
leisure centres
vegetables
vegetable
pests
nutritional values
music store
essential oil
nutrition value
dna extraction
produce
fiber
electricity
potassium
nutritional values
nutrition value
dna extraction
food
solid
material properties
refractive index
thermal properties
phase diagram
thermal expansion
aneurysm
parasites
parasite
pathogen
phobia
mortality rate
symptoms
treatment
orchestras
orchestra
recordings
broadcasts
recording
christmas
ticket
conductor
musical
 organization
dvorak
recordings
conductor
instrument
broadcasts
hall
organization
careers
ceo
phone number
annual report
london
company
social
group
jobs
website
logo
address
mission statement
president
group
ancient cities
city
port
cost of living
canadian embassy
city
air pollution
cheap hotels
municipality
sightseeing
weather forecast
tourist guide
american school
zoo
hospitals
?
?
?
red wines
wine
grape
vintage chart
grapes
city
food pairings
cheese
Figure 4: Example per-node attribute distribution generated by fsLDA. Light/orange nodes represent
labeled attribute sets attached to WN, and the full hypernym graph is given for each in dark/purple
nodes. White nodes depict the top attributes predicted for each WN concept. These inferred annotations
exhibit a high degree of concept specificity, naturally becoming more general at higher levels of the
ontology. Some annotations, such as for the concepts Agent, Substance, Living Thing and Person have
high precision and specificity while others, such as Liquor and Actor need improvement. Overall, the
more general concepts yield better annotations as they are averaged over many labeled attribute sets,
reducing noise. 627
References
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2003a. Hierarchical topic models and the nested
Chinese restaurant process. In Proceedings of the
17th Conference on Neural Information Process-
ing Systems (NIPS-2003), pages 17?24, Vancouver,
British Columbia.
D. Blei, A. Ng, and M. Jordan. 2003b. Latent dirich-
let alocation. Machine Learning Research, 3:993?
1022.
T. Chklovski and Y. Gil. 2005. An analysis of knowl-
edge collected from volunteer contributors. In Pro-
ceedings of the 20th National Conference on Arti-
ficial Intelligence (AAAI-05), pages 564?571, Pitts-
burgh, Pennsylvania.
R. Duda, P. Hart, and D. Stork. 2000. Pattern Classifi-
cation. John Wiley and Sons.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database and Some of its Applications. MIT
Press.
T. Ferguson. 1973. A bayesian analysis of some non-
parametric problems. Annals of Statistics, 1(2):209?
230.
W. Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and
H. Hon. 2007. Cross-lingual query suggestion using
query logs of different languages. In Proceedings of
the 30th ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR-07), pages
463?470, Amsterdam, The Netherlands.
T. Griffiths and M. Steyvers. 2002. A probabilistic ap-
proach to semantic representation. In Proceedings
of the 24th Conference of the Cognitive Science So-
ciety (CogSci02), pages 381?386, Fairfax, Virginia.
M. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING-92), pages 539?545, Nantes,
France.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of the 22nd ACM Confer-
ence on Research and Development in Information
Retrieval (SIGIR-99), pages 50?57, Berkeley, Cali-
fornia.
W. Li and A. McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning (ICML-06), pages
577?584, Pittsburgh, Pennsylvania.
D. Lin and P. Pantel. 2002. Concept discovery from
text. In Proceedings of the 19th International Con-
ference on Computational linguistics (COLING-02),
pages 1?7, Taipei, Taiwan.
M. Pas?ca and E. Alfonseca. 2009. Web-derived re-
sources for Web Information Retrieval: From con-
ceptual hierarchies to attribute hierarchies. In Pro-
ceedings of the 32nd International Conference on
Research and Development in Information Retrieval
(SIGIR-09), Boston, Massachusetts.
M. Pas?ca and B. Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 19?27, Columbus, Ohio.
M. Pas?ca. 2008. Turning Web text and search
queries into factual knowledge: Hierarchical class
attribute extraction. In Proceedings of the 23rd Na-
tional Conference on Artificial Intelligence (AAAI-
08), pages 1225?1230, Chicago, Illinois.
K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu.
2007. Semi-supervised learning of attribute-value
pairs from product descriptions. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2838?2843, Hyder-
abad, India.
J. Sivic, B. Russell, A. Zisserman, W. Freeman, and
A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition (CVPR-08), pages 1?8, Anchorage, Alaska.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), pages 801?808, Sydney, Aus-
tralia.
F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide
Web Conference (WWW-07), pages 697?706, Banff,
Canada.
K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Au-
tomatic discovery of attribute words from Web doc-
uments. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 106?118, Jeju Island, Korea.
B. Van Durme, T. Qian, and L. Schubert. 2008.
Class-driven attribute extraction. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING-2008), pages 921?928,
Manchester, United Kingdom.
E.M. Voorhees and D.M. Tice. 2000. Building a
question-answering test collection. In Proceedings
of the 23rd International Conference on Research
and Development in Information Retrieval (SIGIR-
00), pages 200?207, Athens, Greece.
F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the
17th World Wide Web Conference (WWW-08), pages
635?644, Beijing, China.
N. Yoshinaga and K. Torisawa. 2007. Open-domain
attribute-value acquisition from semi-structured
texts. In Proceedings of the 6th International Se-
mantic Web Conference (ISWC-07), Workshop on
Text to Knowledge: The Lexicon/Ontology Interface
(OntoLex-2007), pages 55?66, Busan, South Korea.
628
 	
Experiments with Open-Domain Textual Question Answering 
Sanda M. Harabagiu and Marius A. Pa~ca and Steven J. Maiorano 
Department of Computer Science and Engineering 
Southern Methodist University 
Dallas, TX 75275-0122 
{sanda, marius, steve}@renoir, seas. smu. edu 
Abstract 
This paper describes the integration of several 
knowledge-based natural language processing tech- 
niques into a Question Answering system, capable 
of mining textual answers from large collections of 
texts. Surprizing quality is achieved when several 
lightweight knowledge-based NLP techniques com- 
I)lement mostly shallow, surface-based approaclms. 
1 Background 
The last decade has witnessed great advances and 
interest in the area of Information Extraction (IE) 
fi'om real-world texts. Systems that participated in 
the TIPSTER MUC competitions have been quite sue- 
cessflll at extracting information from newswire rues- 
sages and filling templates with inforInation pertain- 
ing to events or situations of interest. Typically, the 
templates model queries regarding who did what to 
wh, om, when and where, and eventually why. 
Recently, a new trend in information processing 
from texts has emerged. Textual Question Answer- 
ing (Q/A) aims at; identit~ying the answer of a ques- 
tion in large collections of on-line documents. In- 
stead of extracting all events of interest and their re- 
lated entities, a Q/A system higtflights only a short 
piece of text, accounting for the answer. Moreover, 
questions are expressed in natural anguage, are not 
constrained to a specific domain and are not limited 
to the six question types sought by IE systems (i.e. 
wh, ol (lid what.2 to whoma, when4 and wh, eres, and 
eventually whya). 
In open-domain Q/A systems, the finite-state 
technology and domain knowledge that made IE sys- 
tems successful are replaced by a combination of (1) 
kuowledge-based question processing, (2) new forms 
of text indexing and (3) lightweight abduction of 
queries. More generally, these systems coml)ine cre- 
atively components of tile NLP basic research ill- 
frastructure developed in the 80s (e.g. the compu- 
tational theory of Q/A reported in (Lehnert 1978) 
and tim theory of abductive interpretation of texts 
reported in (Hobbs et a1.1993)) with other shallow 
teclmiques that make possil)le the open-domain pro- 
cessing on real-world texts. 
Tim idea of building open-domain Q/A systems 
that perform on real-world ocument collections was 
initiated by the eighth Text REtrieval Conference 
(TREC-8), by organizing the first competition of an- 
swering fact-based questions uch as "Who came up 
with the name, El Nino?'. Resisting the tempta- 
tion of merely porting and integrating existing IE 
and IR technologies into Q/A systems, the develop- 
ers of the TREC Q/A systems have not only shalmd 
new processing methods, but also inspired new re- 
search in the challenging iutegration of surface-text- 
based methods with knowledge-based text inference. 
In particular, two clear knowledge processing needs 
are presented: (1) capturing the semantics of open- 
domain questions and (2) justifying the correctness 
of answers. 
In this paper, we present our experiments with 
integrating knowledge-based NLP with shallow pro- 
cessing techniques for these two aspects of Q/A. Our 
research was motivated by the need to enhance the 
precision of an implemented Q/A system and by the 
requirement to preI)are it for scaling to more com- 
plex questions than those t)resented in the TREC 
competition. In the remaining of the paper, we de- 
scribe a Q/A architecture that allows the integra- 
tion of knowledge-based NLP processing with shal- 
low processing and we detail tlmir interactions. Sec- 
tion 2 presents the flmctionality of several knowledge 
processing modules and describes tile NLP tech- 
niques for question and answer processing. Section 
3 explains the semantic and logical interactions of 
processing questions and answers whereas Sectiou 
4 higlllights the inference aspects that inlplement 
the justification option of a Q/A system. Section 
5 presents the results and the evaluations whereas 
Section 6 concludes tim paper. 
2 The NLP Techniques 
Surprising quality for open-donlain textual Q/A can 
be achieved when several lightweight knowledge- 
based NLP techniques eomt)lenmnt mostly shallow, 
surface-based approaches. The processing imposed 
by Q/A systems must be distinguished, oi1 the one 
band, from IR techniques, that locate sets of doc- 
292 
, :  : , .  
Queslion l Ioeumcnls Answer(s) 
Question ' Qul , ~._ k~\[ Semantic Logi 
~ ~  l,"LTransfimnati?n.- )j~J ('rransI 
, - \ [  - (Question ~ 
/ \ / \ / \  ..... " ' /  c/,-,.,..,. \ [!  
/ Ot, estion \ ~ = g ~  
Taxonomics ~/Expanded 
/ V ~.~ Expanded Word Classes \[ Question \ ] ) -  
f-~-:~2q ._1,'~ Expansion 
I / / P ~ ~ ? \ \  k.~J.j~J qSxpandcd 
Knowledge-lhrwd OuestioH l'roces,dng 
) 
tW 
Shallow Document l'roces.d~tg Kmm,ledge-Based Answer l'rocessing 
Figure 1: An architecture tbr knowh;dge-l)ased Question/Answering 
uments ('ontaining the required information, based 
on keywords tech, niques. Q/A systems are presented 
with natural anguage questions, far richer in seman- 
tics than a set of keywords eventually st, ru('flured 
around some, oi)erators. Ihnthernmre, the outtlut 
of Q/A systems is either the actual answer identi- 
fied in a text or small text; fragments containing the 
answer. This eliminates the user's trouble of tind- 
ing the required inibrnlation in sometime.s large sets 
of retrieved o(-uments. Op(m-donmin Q/A systems 
must also l)e distinguished, on the other hand, h'om 
IE syst(;ms that model the inforlnation eed through 
(latal)as(; t(;mt)lates , thus less naturally than a tex- 
tual answer. Moreovei', open-domain IE is still ditli- 
(:ult to achieve, beeause its linguistic t)atCern re(:og- 
nition relies on domain-dependent lexico-semantie 
knowledge. 
To t)e able to satisf~y tile ol)en-donmin constraints, 
textual Q/A systems replace the linguistic pattern 
matching capabilities of IE systems with methods 
that rely (m the recognitioil of tile question type and 
of the e.'rpectcd answer type. Generally, this informa- 
tion is available by accessing a classification based 
on the question stem (i.e. what, how much, who) 
and the head of the first nOml phrase of the ques- 
ti(m. Question 1)rocessing also includes the identifi- 
cation of the question keywords. Empirical methods, 
based on a set of ordered heuristics ot)erating on the 
phrasal parse of the question, extract keywords that 
are passed to the search engine. The overall preci- 
sion of tile Q/A system depends also on th(, recogni- 
tion of the question focus, since the answer extrac- 
tion, suet:ceding the IR phase, is centered around 
the question focus. Unl})rtmmtely, eml)irical ninth- 
ods fl)r t'oeus recognition are hard to develop without 
the availability of richer semantic knowledge. 
S1)eeial requir(nnents are set Oil the documeid; pro- 
(:essing COml)Olmnt of a Q/A system. To speed-u l) 
the answer extraction, the search engine returns only 
those 1)aragrai)hs from a document that contain all 
queried keywords. The paragraphs are ordered to 
1)roInote the, eases when the keywords not only art; as 
close as 1)ossibh~', lint also t)reserve the syntactic de- 
1)enden(:ies re(:ognized in the question. Answers are 
('.xtra(:ted whenever the question topic and the m> 
swer tyI)e are recognized iil a 1)aragraph. Thereafl;er 
the answers :/1(; scored 1)ased on several bag-of-words 
hem'isties. Throughout all this 1)roeessing, the NLP 
te(:hniques are limited to (21) named entity recogni- 
tion; (b) semantic lassification of the question tyt)e, 
l/ased oil information 1)rovided by an off-line ques- 
tion taxonon ly  21.i1(t senmntic lass intbrmation avail- 
able from WordNet (Felll)mml 1998); mid (c) phrasal 
parsing produced by enhancing Brill's part-of-sl)eech 
tagger with some rules tbr phrase tbrmation. 
Ilowever simt/le, this technology surl)asses 75% 
precision on trivia questions, as posed in the TREC- 
8 (:ompetit ion (of. (Moldovan et al1999)). An im- 
pressive improvenmnt of 14% is achieved when more 
knowledge-intensive NLP techniques are ai)plied a.t 
both question and answer processing level. Figure 1 
illustrates the architecture of a system that has en- 
hanced Q/A performance. 
As represented in Figure 1, all three modules 
of the Q/A system preserve the shallow processing 
eomi/onents that determine good t)erformanee. In 
t;t1(', Quest, ion Processing module, the Question Class 
re(:ognizer, working against a taxonomy of questions, 
293 
still constitutes the central processing that takes 
place at this stage. However, a far richer representa- 
tion of the quest;ion classes is employed. To be able 
to classify against the new question taxonomy each 
question is first flflly parsed and transfommd into 
a semantic representation that captures all relation- 
ships between I)hrase heads. 
The recognition of the question class is based on 
the comparison of the question smnantic representa- 
tion with the semantic representation of the nodes 
from tlm question taxonomy. Taxonomy nodes en- 
code also the answer type, the question focus and 
the semantic lass of question keywords. Multiple 
sets of keywords are generated based on their seman- 
tic class, all pertaining to the stone original ques- 
tion. This thature enables the search engine to re- 
trieve multiple sets of documents, pertaining to mul- 
tit)le sets of answers, that are extracted, combined 
and ranked based on several heuristics, reported in 
(Moklovan et a1.1999). This process of obtaining 
multiple sets of answers increases l;he likelihood of 
finding the correct answer. 
However, the big boost in the precision of the 
knowledge-based Q/A system is provided by the op- 
tion of enabling the justification of the extracted an- 
swer. All extracted answers are parsed and trans- 
formed in semantic representations. Thereafter, 
both semantic transformations for questions and an- 
swers are translated into logic forms and presented 
to a simplified theoreln prover. The proof back- 
chains Dora the question to the answer, its trace 
generating a justification. The prover may access 
a set of abduction rules that relax the justification 
process. Whenever an answer cmmot l)e 1)roven, it 
is discarded. This option solves multiple situations 
when the correct answer is not ranked as the first 
return, due to stronger surface-text-based in icators 
in some other answers, which unfortunately are not 
correct. 
This architecture allows for simple integration of 
semantic and axiomatic knowledge sources in a Q/A 
system and determines efficient interaction of text- 
surface-based and knowledge-based NLP techniques. 
3 In teract ions  
Three main interactions between text-surface-based 
and knowledge-based NLP techniques are designed 
in our Q/A architecture: 
1. When multiple sets of question keywords are passed 
to the search engine, increasing the chance of find- 
ing the text paragraph containing the answer. 
2. When the question focus and the answer type, re- 
suiting from the knowledge-based processing of the 
question, are used in the extraction of the answer, 
based on several empirical scores. 
3. When the justification option of the Q/A system is 
available. Instead of returning answers cored by 
some empirical measures, a proof of the correctness 
of the answer is produced, by accessing the logical 
transformations of the question and the answer, as 
well as axioms encoding world knowledge. 
All these interactions depend on two Nctors: (1) 
the l;ransformations of the question or answer into 
semantic or logical representations; and (2) the avail- 
ability of knowledge resources, e.g. the question tax- 
onomy and the world knowledge axioms. The avail- 
ability of new, high-performace parsers that operate 
on real world texts determines the transformation 
into semantic and logic formulae quite simple. In ad- 
dition, the acquisition of question taxonomies i alle- 
viated by machine learning techlfiques inspired from 
bootstrapping methods that learn linguistic patterns 
and semantic dictionaries for IE (of. (Riloff and 
Jones, 1999)). World knowledge axioms can also be 
easily derived by processing the gloss (lefinitions of 
WordNel; (Fellbaunl 1998). 
a.1 Semant ic  and  Logic  T rans format ions  
Semant ic  T ranstbrmat ions  
Instead of t)roducing only a phrasal parse for the 
question and answer, we lnake use of one of the new 
statistical parsers of large real-world text coverage 
(Collins, 1996). The parse trees produced by such a 
parser can be easily translated into a seinantic repre- 
sentation that (1) comprises all the phrase beads and 
(2) captures their int,er-relationships by anonymous 
links. Figure 2 illustrates both the I)arse tree and 
the associated semantic representation f a TI{EC-8 
question. 
Question: Why did I)avid Koresh ask the FBI for a word processor? 
Parse: 
SBAP.Q 
-- - . . . . .  SQ 
/ //------ v,, 
/ /  / /  4 -  - -~  --Pl' 
i / / / \  / 
WRB VBI) NNP NN\[' VB l)T NNI' IN DT NN NN 
I I I I I I I I I l I 
Why did David Koresh ask the Fill for a word processor 
Semantic representation: 
~ a s k  
word ~ . .  
REASON I )av id%/Y  - - -~"~\  processor 
Koresh FBI 
Figure 2: Question semantic transformation 
The actual transformation i to semantic repre- 
sentation of a question or an answer is obtained 
as a by-product of the parse tree traversal. Ini- 
tially, all leaves of the parse tree are classified as 
294 
.@@nodes or no'n-skipnodes. All n(mns, non-auxiliary 
verbs, adjectives a.nd adverl)s are categorized as 
non-skitmodes. All the other h~aves are skipnodes. 
Bottom-u 1) trav('.rsal of tim 1)arse tree (:ntails tlm 
t)roi)agation of leaf labels wh('amver l;hc 1)arcnt nod(; 
has more than one non-skipnod(; child. A rule based 
on the syntactic ategory (it' th(.' father selects one of 
the childr(m to 1)ropagatc its label a,t the next level 
in the tree. The winning node will then be consid- 
ered linked to all the other fornmr siblings thai; al'e 
non-skilmodes. The prot)agation (:ontimms mltil the 
l)arse 1;l'(~.c root receives a label, and thus a scmanti(" 
gral)h is (:rc;tl;(;(l as a 1)y-1)rodu(:t. Part of th('. la- 
bel i)roI)agation, we also (:onsider that whenever all 
('hildr(;n of a non-terminal are skilmo(l(;,% the parent; 
becomes a. skipnode as well. 
Figure 3 rel)r(~'s(;nts the lal)el I)rOl)agation for th(; 
1)arse tree of the question l'el)res(mt(M in Figure 2. 
The labels of Korcsh, ask, FB1 and processor are 
l)rot)agated to tlw, next level. This entails t;hat Ko- 
r'c.s'h is linked to David, ask to I,'BI and procc',s'sor mM 
procc, ssor to word. As a.@ be(:(/m(!s the lab(:l of th(; 
tree to(it, it is a.lso linked to I~ds'A,9ON, the qu(~sti()n 
type (l('~I;(n'lnin(',d l)y tlm question st('m: wh, y. The 
lal)el 1)rot)agation rules are id(mtical t<) the rules fl)r 
mapping from trees to d(~t)(m(hm(:y s\[.rlt(:l;lll.'cs llSC,(l 
my Mi('ha(,q Collins (of. (Collins, 199(5)). These rules 
i(lenti\[}, the head-child, and pl'Ol)agatc its label up 
in the tree. 
/ _  > 
( /  vV (>k) 
wl lm)w'  /~  Nl' (K,,,,,~IO' ' ' i , / '  NI' \[ I:IH,I 
/ / ~ ~ / / \ / ' \ 
WRB VBI) NNP NNP , ', VII I)T NNP, IN I)T NN NN ) 
I I I I / ' ,  I I I~" I I I I ,' 
Wily did l)avid Korcsh ask file \[:BI for a word lUl)ccssiw 
Figur(' .  3: \])~II'S(~ t,l'(~.(, ~ l;ra, v(',rsa\[ 
I'l'( processor ! -  
/ /  NI~ I)rOCCssor I / , . 
Logical  T rans format ions  
The logical formulae in whi(:h questions or answers 
arc translated are inspired \[)y the notal;i(m prol/os('.d 
in (ltobbs, 1.986-1.) and implemented in TACITUS 
(ttobbs, 1986-2). 
Based on the davidsonian tl"eatmenl; of action sen- 
|;(?llC(;S, in which events are tr(~at, ed as individuals, 
every question and every answer are transform('xl in
a tirst-order t)redicatc tbrmula for which (1) verbs 
are mapped in 1)red\[cares vcvb(e,x,y,z,. . .)  with the. 
(:onvention thai; variable e rel)res(;nts the evc'nl;ual- 
i(;y of that acl;ion or even(; (;o take place, wh('a'eas (;lm 
othcu arguments (e.g. z, y, z, ...) repr(~,s(mt l;lm t)rcd- 
icate argmnents of the verb; (2) nouns arc mal)l)ed 
into their lexicalized predicates; raM, (3) mo(lificws 
have the same argument as the predicate they mod- 
i\[y. For  (;Xalnpl('~, l;he qu(~si;ion i l l us t ra ted  in F igure  2 
has  l;he fo l low ing  log ica l  fo rn l  t rans for lnat ion  (LFT)  : 
\[REASON (x) ~David (y) ~Koresh (y) ~ask (e, x, y, z, p) 
~FBI (z) {qproces sot (p) ~word (p) \] 
The process of trmlslat;ing a sema.ntic ret)resenta.- 
tion into a logic form has the following steps: 
1. For each node in the semantic rcprcscntation, 
create a prcdicatc with a distinct argument. 
2.a. If  it noun and (tit ad, jective predicate arc linlccd 
tht:y should have, the sam(" argument. 
2.b. Tltc .qamc fin" verbs and adverbs, pairs of lwuns 
or an adjective and an advcrb. 
3. l,b'r each verb predicate, add aT~lumcnts corrc.sponding 
to each predicate to which it is directly 
linked in the semantic representation. 
Predicate argunmnts (:an be identified because tim 
soma,hi ; i t  l'ol)res(.~nl;al;ion us ing  &no l lymous  re la t ions  
represenl;s uniformly adjuncts mM thematic roles. 
Ilowevel:, sl;e 1) 2 of the l;l"mtsl~d;ion procedure l'eCOg- 
nixes the adjuncts, making predicate argmnenl;s the 
remaining (:()nn(~(:tions of tlm v(n'}) in t;}l(, ~semal~I;ic 
rq)resentation. 
3.2 Quest ion  Taxonomy 
The question taxonomy re l )rcscnl ;s  each  question 
nod(', as a qu in tup le :  (1) a setnan( ; ic  rcpr(~s(!ni;at ion 
of a qucsLion; (2) th(; question type; (3) the m~swer 
typ(,; (4) the question focus an,l (5) the question 
keywoMs. By using over 1500 questions provided by 
\]lcme(lia, as well as otlmr 2000 quest;ions retrieved 
from FAQFinder, we have b(!en abh ~. to learn classiti- 
cation rules mM buihl a (:oinplex question taxonomy. 
\ ..... , \[ \ ] I1SI I I I ICe I 
,,/- ;,,,\__ 
// ,, 
.2_ 
\[Artwo,k jl LI.ocation 1 
Figure 4: A snapshot of the top Question Taxonomy 
Initially, we stm'tcd with a seed hit;rarchy of 25 
question classes, manually built, in which all the se- 
mantic classes of the nodes fl'om the semantic repre- 
sentations were decided oil-line, by a hmnan expert. 
300 questions were processed to create this seed hi- 
erarchy. Figure 4 illustrates some of the nodes of 
295 
the top of this hierarchy. Later, as 500 more ques- 
tions were considered, we started classifying them 
semi-automatically, using the following two steps: 
(1) first a hmnan wonld decide the semantic lass of 
each node in the semantic representation f the new 
question; (2) then a classification procedure would 
decide whether the question belongs to one of the 
existing classes or a new class should be considered. 
To be able to classify a new question in one of the ex- 
isting question classes, two conditions must be satis- 
fled: (a) all nodes fi'om the taxonomy question must 
correspond to new question nodes with the same 
semantic lasses; and (b) unifyable nodes must be 
linked in the same way in both representations. The 
hierarchy grew to 68 question nodes. 
Later, 2700 more questions were classified fully 
automatically. To decide the semantic lasses of the 
nodes, we used the WordNet semantic hierarchies, 
by simply assigning to each semantic representation 
node the same class as that of any other question 
term from its WordNet hiera.rchy. 
The semantic representation, having the same for- 
mat for questions and answers, is a case fi'ame with 
anonymous relations, that allows the unification of 
the answer to the question regardless of the case re- 
lation. Figure 5 illustrates tbur nodes fi'om the ques- 
tion taxonomy, two for the "currency" question tyI)e 
attd two for the "person name" question type. The 
Figure also represents the mappings of four TREC- 
8 questions in these hier~rchy nodes. The mappings 
are represented by dashed arcs. In this Figm'c, the 
nodes front the semantic representations that con- 
rain a question mark are place holders for the ex- 
pected answer type. 
An additional set of classification rules is assoei- 
ated with this taxonomy, hfitially, all rules are based 
on the recognition of the question stem and of the 
answer type, obtained with class intbrmation from 
WordNet. However we could learn new rules when 
inorphologieal nd semantic variations of the seman- 
tic nodes arc allowed. Moreover, along with the new 
rules, we enrich the taxonomy, because often the new 
questions unify only partially with the current tax- 
enemy. All semantic and morphologic variations of 
the semantic representation nodes are grouped into 
word classes. Several of the word classes we used are 
listed in Table 1. 
\[I Word Ulass Words l\] 
Value words "monetary value", "money", "price" 
Expenditure words "spend", "buy", "rent", "invest" 
Creation words "author", "designer", ainvent~... 
Table 1: Examples of word classes. 
The bootstrapping algorithm that learns new 
classification rules and new classes of questions 
is based on an intbrmation extraction measure: 
scorc(rulei)--Ai * lo.q2(Ni), where Ai stands for the 
Class Name: cur rency  
\[ ~ l  O7: What debts did Qintex 
, . . . . . .  . .   rouploav : 
Class Name." I la lne perso l l  
Q32:Who received Ihe Will 
? Rogers Award in 1989? 
conlpctilioll " 
HFI2 
semalltic reprd~ - ~ representation 
Class Nanle: author 
Q92: Who released 1he lnlernet 
'~ worm in the late 1980s? 
el'talc " "  
| . 2 ~  . . . . . .  I 1980s1 
~ c  ~Bf i - )  ~ semanttc representation _ 
Figure 5: Mapping Questions in the Taxonomy 
number of different lexicon entries for the answer 
type of the question, whereas Ni = Ai /F i ,  where Fi 
is the number of different focus categories classified. 
The steps of the bootstrapping algorithm are: 
1. Retrieve concepts morphologically//semantically re ated 
to the semantic representations 
2. Apply the classification rules to all questions that 
contain any newly retrieved concepts. 
3. New_Classificatiton_Rules={} 
MUTUAL BOOTSTRAPPING LOOP 
4. Score. all new classification rules 
5. best_CR=thc highest scoring classification rule 
6. Add bcst_CR to the classification rules 
7. Add the questions classified by best_CR to the taxonomy 
8. Goto step 4 three times. 
9. Discard all new rules but the best scoring three. 
10. Goto 3. until the Q/A performance improves. 
296 
4 The Just i f icat ion Option 
A Q/A system that provides with the option of jus- 
til~ving the answer has the advantage that erroneous 
answers can be ruled out syst(,'matieally. In our quest 
of enh~mcing the precision of a Q /A  system by inco f  
porating additional knowledge, we fount1 this option 
very helpflH. However~ the generation of justifica- 
tions for ol)en-domain textual Q /A  systems poses 
some challenges. First;, we needed to develol) a very 
efficient prover, operating on logical form transfer- 
mat;ions. Our 1)rool'q are backchaining Do\]n the qlles- 
tions through a mixture of axioms. We use thl'ee 
forlllS of axioms: (1) axioms derived fl'om the facts 
stated in I;he l;eXtllal allswel; (2) ~/XiOlllS ro4)resent- 
ing world knowledge,; and (3) axioms (let;ermined by 
coreference resohlt;ion in the atiswer text;. For ex- 
alni)le , some of the axioms eml)loyed 1;o prove the 
answer to the TREC~8 question (26: Why did David 
Kor'csh, ask th, c 1,7\]1 for  a 'wo~'d p~vccssor?" are: 
SET 1 
Mr (71) := nu l l .  Koresh(71) :=nu l l .  word(72) :=nu l l .  
p rocessor (72) :=nu l l ,  sent (77  76  78  71) :=nu l l .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
SET 2 
Dav id (1) :=Mr(1) .  REASON(5) := enab le (5  3 6 ) .  
SET 3 
FB I  (i) : =nu l l .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
The first set represents fa(',l;s extraete(| through 
LFT \]~re(licat(',s of the textual answer: "0'?;(',~' t\]t(: 
'mc(~k(m,d My Kovcsh, .s'c'nt a vcq',,c.st fo'r a 'u;ord proce, s- 
so'r to c',,ablc h,i'H~, to 'record his vt:'~;clatio'~,s". The sec- 
Ol~d set  ret ) r ( ' , sen l ;s  worl(1 knowledge axioms that; we 
acquired senfi-auto\]na.tically. For instance we know 
that David is a male name, thus tlmt person cmt be 
addressed with Mr.. Similarly, events are (real>led 
for some reason. The third set of axioms represent 
the fact that the t,'l~l is in the context of the text 
answer. To be noted that the axioms derived from 
the answer have construct argument, s, relnesentc, d by 
convention with numl)ers larger titan 70. All the 
other arguments are variables. 
Q52 \?ho invented the road trallic cone,? 
Answer ST~iliT~g proudly for the caTl~cras , Gover~tor 
(shallow Pete Wilson, US TraTtsportatioTt Secretary 
methods) l,'edcrico l)eT~a aTtd Mayor l~ichavd l~iovda~t 
removed ~t hal f -  dozeTt phtstic or~t~tgc coTtcs 
f i rm the road~oay aTtd the first ca~'s passed 
I Answer David Mor~la~, the comp~tTty's maTtagiT~ 9 
(kb-based di~'ector t~d iT~?~eltlo~" of the plastic 
methods) co~te cycle, collects them. 
Table 2: Examples of trot)roved answer correctness. 
Tim justification of this answer is t)rovided by the 
r~ ( following proof I;r~lee. \] h ', 1)rover ;tttelllI)tS t;o 1)rove 
the LFT of the question (QLF) corre(:t 1} 3, proving 
from left to right each term of QLF. 
->Answer:0ver the weekend Mr Koresh sent a request for a word 
processor to enable him to record his revelations. 
->~LF:David(l)~Koresh(1)"word(2)^processor(2)'FBI(4) ~ 
ask(3 4 2 i 5) '_REASDN(5)'_PER(1)*0RG(4) 
->ALF:Mr(71)"Koresh(71)'word(72)'processor(72)'revelations(74)" 
record(Z3 74 75)^enable(75 73 76)~request(76)'sent(77 76 78 71) 
^weekend(78)"PEK(71) 'DATE(?8)  
-->Proving:David(1)^Koresh(1)^word(2)'processor(2)^FBI(4)" 
ask(3 4 2 i 5)'REASON(5)'_PER(1)'_0RG(4) 
There are i target axioms. Selected axiom: David(1):= Mr(1). 
Unifying: i to i. Ok 
--> Proving: Mr(1)'Koresh(J)~word(2)^processor(2)'FBl(4) 
^ask(3 4 2 i 5)^_REASDN(5)'_PER(1)^_0RG(4) 
There are i target axioms. Selected axiom: Mr(71):= null. 
Unifying: i to 71. Dk 
--> Proving: Koresh(71)*word(2)-processor(2)~FBl(4) " 
ask(3 4 2 Yl 5)* REASDN(5)^_PER(71)'_0RG(4) 
There are i target axioms. Selected axiom: Koresh(71):= null. 
Unifying: Y1 to 71. Dk 
--> Proving: word(2)'processor(2)^FBl(4)~ask(3 4 2 71 5)* 
_REASON(5)^_PER(71)'0RG(4) 
There are i target axioms. Selected axiom: word(72):= null, 
Unifying: 2 to 72. Ok 
--> Proving: processor(Y2)'FBl(4)^ask(3 4 72 71 5)^REASON(5) 
^_PER(71)^_DRG(4) 
There are i target axioms. Selected axiom: processor(72):= null. 
Unifying: ?2 to 72. Ok 
--> Proving: FBl(4)'ask(3 4 72 71 5)^REASON(5)^_PER(71)'_0RG(4) 
There are i target axioms, Selected axiom: FBI(1):= null. 
Unifying: 4 to i. Ok 
--> Proving: ask(3 4 72 71 5)~REASON(5)^_PER(ZI)^_BRG(4) 
There are 2 target axioms. Selected axiom: ask(l 2 3 4 5):= 
sent(l 6 7 4)'request(5). 
Unifying: i to 2. 3 to i. 5 to 5. 71 to 4. 72 to 3. Ok 
--> Proving: sent(l 6 7 ?I)^request(6)^_REASON(5)^_PER(YI)^_0RG(2) 
There are I target axioms, Selected axiom: sent(Z7 76 78 71):= null. 
Unifying: I to 77. 6 to Y6. Y to 78. 71 to 71. Ok 
--> Proving: request(76)^REASON(5)'_PER(71)^_0RG(2) 
There are i target axioms. Selected axiom: request(76):= null. 
Unifying: 76 to 76. Ok 
--> Proving: _REASON(5)?PER(71)?8}~G(2) 
There are 1 ta rget  axioms. Se lected axiom: _REASON(5):= enable(5 3 6). 
Unifying: 5 to 5. flk 
--> Proving: enable(5 3 6)^_PER(YI)-_flRG(2) 
There are i target axioms. Selected axiom: enable(75 73 76): = null. 
Unifying: 3 to ?3. 5 to 75. 6 to 76. Dk 
--> Proving: _PER(71)^_ORG(2) 
There are 3 target axioms. Selected axiom: _PER(71):= null. 
Unifying: 71 to gl. 0k 
--> Proving: _ORG(2) 
There are i target axioms. Selected axiom: _0RG(1):= FBI(1). 
Unifying: 2 to i. Ok 
--> Proving: nullJl\]J We found:Success. 
.................................................................. 
There are cases when our simple prover fails to 
prove a. correct answer. We have notice(1 that this 
hal)pens 1)ecause in the answer semantic representa- 
tion, st)me concepts that are connected in the ques- 
tion semantic representation are no longer directly 
linked. This is due to the f~mt that there are either 
parser errors or there are new syntactic dependen- 
cies between the two concepts. To acconmmdm;e 
this situation, we allow diflhrent constants that are 
arguments of the sanle predicate to be unifiable. The 
special cases in which this relaxation of the unifica- 
tion i)roeedul'e is allowed constitute our abduction 
r l t les .  
297 
5 Evaluat ion 
Both qualitative and quautitative valuation of the 
integration of surface text-based and knowledge- 
based methods for Q/A is imposed. Quantitatively, 
Tal)le 3 summarizes the scores obtained when only 
shallow methods were emI)loyed, in contrast with 
the results when knowledge-based methods were in- 
tegrated. We have sepm'ately measured the effect 
of tile integration of the knowledge-based methods 
at question processing and answer processing level. 
We have also evaluated the precision of the sys- 
tern when both integrations were implemented. The 
results were the first five answes's returned within 
250 bytes of text, when approximatively half mil- 
lion TREC documents are mined. Wc have used the 
200 questions from TREC-8, mid tile correct answers 
provided by NIST. The performance was measured 
both with the NIST scoring method employed in the 
TREC-8 and by simply assigning a score of 1 tbr the 
question having a correct answer, regardless of its 
position. 
Percentage of NIST score 
correct answers 
in top 5 returns 
Tezt-suTJ'acc-bascd 77.7% 64.5% 
Knowledge-based 83.2% 71.5% 
Question Processing 
(only) 
~l~:rt-su@zce-bascd 77.7% 73% 
only 'with Answer 
Justification 
Knowledge-based 89.5% 84.75% 
Question Pwces.sin9 
with Answer 
Justification 
Table 3: Accuracy t)erformance 
When using the NIST scoring method to eval- 
uate an individual answer, we used only six 
values:(1, .5, .33, .25, .2, 0), representing the score the 
answer's question obtains. If the first answer is cor- 
rect, it obtains a score of 1, if the second one is cor- 
rect, it is scored with .5, if the third one is correct, 
tile score becomes .aa, if the fourth is correct, the 
score is .25 and if the fifth one is correct, the score 
is .2. Otherwise, it is scored with 0. No credit is 
given if multiple answers are correct. Table 3 shows 
that both knowledge-based methods enhanced the 
precision, regardless of the scoring method. 
To further evaluate the contribution of tim justi- 
ficat, ion option, we evaluated separately the preci- 
sion of the prover tbr those questions for which tile 
surface-text-based methods of our system, when op- 
erating alone, emmet find correct answers. We had 
45 TREC-8 questions for which the evaluation of the 
prover was performed. Table 4 summarizes the ac- 
curacy of the prover. 
hlcorrect ~ilswors 
(no knowledge) 
Correct m~swers 
(KB-based) 
Incorrect answers 
(KB-based) 
P rovell 
con'ect 
3 
127 
l'roven Precision 
incorrect 
210 98.5% 
5 96.2% 
38 90.04% 
Table 4: Prover performmme 
Qualitatively, we find that the integration of 
knowledge-based methods is very beneficial. Table 2 
illustrates tile correct answer obtained with these 
methods, in contrast to tile incorrecl, answer pro- 
vided when only the shallow techniques m'e al)plied. 
6 Conc lus ions  
\Ve believe that the lmrfol'nmnce of a Q/A system 
del)ends on the knowledge sources it employs. In 
this paper we trove presented the effect of tile in- 
tegration of knowledge derived from question tax- 
enemies and produced by answer justifications on 
the Q/A precision. Our knowledge-based methods 
are lightweight, since, we do not generate precise se- 
mantic rel)resentations of questions or answo.rs, but  
mere attproximations determismd by syntactic de- 
1)en(lencies. Fm'thermore, our prover operates on 
very simple logical representations, its which syntac- 
tic and semantic ambiguities are completely ignored. 
Nevertheless, we have shown that these approxima- 
tions are functional, since we implemented a prover 
that justifies answers with high precision. Similarly, 
our knowledge-t)ased question l)rocessillg is a nlel'e 
combination of word class information and syntactic 
dep endencies. 
Re ferences  
Michael Collins. A New Statistical Parser Based on Bigram 
Lexical l)et)endencies. In Proceedings of the 2dst Annual 
Mectin9 of the Association for" Computational Lin.quistics, 
ACL-.96~ pages 184 19t, 1996. 
Christlane Fellbaum (Ed). WordNet - An Electronic Lexical 
l)atabase. MIT Press, 1998. 
Jerry R. IIobbs. Discourse and Inference. Unpublished 
malmscrlpt, 1986. 
Jerry R.. Itobbs. Overview of the TACITUS Project;. In Com- 
putational Linguistics, 12:(3), 1986. 
Jerry ltobbs, Mark StickeI, Doug Appelt, and Paul Mm'- 
tin. Interpretation as abduction. Artificial Intelligence, 63, 
pages 69 142, 1993. 
YVendy Lehnert. The processing of question answering. 
Lawrence Erlbaum Publishers, 1978. 
l)an Moldovan, Sanda llarabagiu, Marius Pasta, Rada Mi- 
halcea, Richard Goodrum, Roxana Gh:iu alK1 Vasile ll.us. 
Lasso: a tool for surfing the answer net. In Proceedings of 
TREC-8, 1999. 
Ellen Riloff and Rosie Jones. Learning l)ictionaries for hffor- 
mation Extraction by Multi-Level Bootstrat)plng. In Pro- 
cccdings of the 16th National Conference on Artificial In- 
telligence, AAAI-99, 1999. 
298 
 
			ffThe Role of Lexico-Semantic Feedback in Open-Domain Textual
Question-Answering
Sanda Harabagiu, Dan Moldovan
Marius Pas?ca, Rada Mihalcea, Mihai Surdeanu,
Ra?zvan Bunescu, Roxana G??rju, Vasile Rus and Paul Mora?rescu
Department of Computer Science and Engineering
Southern Methodist University
Dallas, TX 75275-0122
 
sanda  @engr.smu.edu
Abstract
This paper presents an open-domain
textual Question-Answering system
that uses several feedback loops to en-
hance its performance. These feedback
loops combine in a new way statistical
results with syntactic, semantic or
pragmatic information derived from
texts and lexical databases. The paper
presents the contribution of each feed-
back loop to the overall performance of
76% human-assessed precise answers.
1 Introduction
Open-domain textual Question-Answering
(Q&A), as defined by the TREC competitions1 ,
is the task of identifying in large collections of
documents a text snippet where the answer to
a natural language question lies. The answer
is constrained to be found either in a short (50
bytes) or a long (250 bytes) text span. Frequently,
keywords extracted from the natural language
question are either within the text span or in
its immediate vicinity, forming a text para-
graph. Since such paragraphs must be identified
throughout voluminous collections, automatic
and autonomous Q&A systems incorporate an
index of the collection as well as a paragraph
retrieval mechanism.
Recent results from the TREC evaluations
((Kwok et al, 2000) (Radev et al, 2000) (Allen
1The Text REtrieval Conference (TREC) is a series of
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
et al, 2000)) show that Information Retrieval (IR)
techniques alone are not sufficient for finding an-
swers with high precision. In fact, more and more
systems adopt architectures in which the seman-
tics of the questions are captured prior to para-
graph retrieval (e.g. (Gaizauskas and Humphreys,
2000) (Harabagiu et al, 2000)) and used later in
extracting the answer (cf. (Abney et al, 2000)).
When processing a natural language question two
goals must be achieved. First we need to know
what is the expected answer type; in other words,
we need to know what we are looking for. Sec-
ond, we need to know where to look for the an-
swer, e.g. we must identify the question keywords
to be used in the paragraph retrieval.
The expected answer type is determined based
on the question stem, e.g. who, where or how
much and eventually one of the question concepts,
when the stem is ambiguous (for example what),
as described in (Harabagiu et al, 2000) (Radev et
al., 2000) (Srihari and Li, 2000). However finding
question keywords that retrieve all candidate an-
swers cannot be achieved only by deriving some
of the words used in the question. Frequently,
question reformulations use different words, but
imply the same answer. Moreover, many equiv-
alent answers are phrased differently. In this pa-
per we argue that the answer to complex natural
language questions cannot be extracted with sig-
nificant precision from large collections of texts
unless several lexico-semantic feedback loops are
allowed.
In Section 2 we survey the related work
whereas in Section 3 we describe the feedback
loops that refine the search for correct answers.
Section 4 presents the approach of devising key-
word alternations whereas Section 5 details the
recognition of question reformulations. Section 6
evaluates the results of the Q&A system and Sec-
tion 7 summarizes the conclusions.
2 Related work
Mechanisms for open-domain textual Q&A were
not discovered in the vacuum. The 90s witnessed
a constant improvement of IR systems, deter-
mined by the availability of large collections of
texts and the TREC evaluations. In parallel, In-
formation Extraction (IE) techniques were devel-
oped under the TIPSTER Message Understand-
ing Conference (MUC) competitions. Typically,
IE systems identify information of interest in a
text and map it to a predefined, target represen-
tation, known as template. Although simple com-
binations of IR and IE techniques are not practical
solutions for open-domain textual Q&A because
IE systems are based on domain-specific knowl-
edge, their contribution to current open-domain
Q&A methods is significant. For example, state-
of-the-art Named Entity (NE) recognizers devel-
oped for IE systems were readily available to be
incorporated in Q&A systems and helped recog-
nize names of people, organizations, locations or
dates.
Assuming that it is very likely that the answer
is a named entity, (Srihari and Li, 2000) describes
a NE-supported Q&A system that functions quite
well when the expected answer type is one of the
categories covered by the NE recognizer. Un-
fortunately this system is not fully autonomous,
as it depends on IR results provided by exter-
nal search engines. Answer extractions based on
NE recognizers were also developed in the Q&A
presented in (Abney et al, 2000) (Radev et al,
2000) (Gaizauskas and Humphreys, 2000). As
noted in (Voorhees and Tice, 2000), Q&A sys-
tems that did not include NE recognizers per-
formed poorly in the TREC evaluations, espe-
cially in the short answer category. Some Q&A
systems, like (Moldovan et al, 2000) relied both
on NE recognizers and some empirical indicators.
However, the answer does not always belong
to a category covered by the NE recognizer. For
such cases several approaches have been devel-
oped. The first one, presented in (Harabagiu et
al., 2000), the answer type is derived from a large
answer taxonomy. A different approach, based on
statistical techniques was proposed in (Radev et
al., 2000). (Cardie et al, 2000) presents a method
of extracting answers as noun phrases in a novel
way. Answer extraction based on grammatical
information is also promoted by the system de-
scribed in (Clarke et al, 2000).
One of the few Q&A systems that takes into
account morphological, lexical and semantic al-
ternations of terms is described in (Ferret et al,
2000). To our knowledge, none of the cur-
rent open-domain Q&A systems use any feed-
back loops to generate lexico-semantic alterna-
tions. This paper shows that such feedback loops
enhance significantly the performance of open-
domain textual Q&A systems.
3 Textual Q&A Feedback Loops
Before initiating the search for the answer to a
natural language question we take into account
the fact that it is very likely that the same ques-
tion or a very similar one has been posed to the
system before, and thus those results can be used
again. To find such cached questions, we measure
the similarity to the previously processed ques-
tions and when a reformulation is identified, the
system returns the corresponding cached correct
answer, as illustrated in Figure 1.
When no reformulations are detected, the
search for answers is based on the conjecture that
the eventual answer is likely to be found in a
text paragraph that (a) contains the most repre-
sentative question concepts and (b) includes a tex-
tual concept of the same category as the expected
answer. Since the current retrieval technology
does not model semantic knowledge, we break
down this search into a boolean retrieval, based
on some question keywords and a filtering mech-
anism, that retains only those passages containing
the expected answer type. Both the question key-
words and the expected answer type are identified
by using the dependencies derived from the ques-
tion parse.
By implementing our own version of the pub-
licly available Collins parser (Collins, 1996), we
also learned a dependency model that enables the
mapping of parse trees into sets of binary rela-
tions between the head-word of each constituent
and its sibling-words. For example, the parse tree
of TREC-9 question Q210: ?How many dogs pull
a sled in the Iditarod ?? is:
JJ
S
Iditarod
VP
NP
PP
NP
NNPDTINNN
NP
DTVBPNNS
NP
manyHow
WRB
dogs pull a sled in the
For each possible constituent in a parse tree,
rules first described in (Magerman, 1995) and
(Jelinek et al, 1994) identify the head-child and
propagate the head-word to its parent. For the
parse of question Q210 the propagation is:
NP (sled)
DT NN DTIN
manyHow
WRB
dogs
NNSJJ
NP (dogs)
VBP
pull a sled in the Iditarod
NNP (Iditarod)
NP (Iditarod)
PP (Iditarod)
NP (sled)
VP (pull)
S (pull)
When the propagation is over, head-modifier
relations are extracted, generating the following
dependency structure, called question semantic
form in (Harabagiu et al, 2000).
dogs IditarodCOUNT pull sled
In the structure above, COUNT represents the
expected answer type, replacing the question stem
?how many?. Few question stems are unambigu-
ous (e.g. who, when). If the question stem is am-
biguous, the expected answer type is determined
by the concept from the question semantic form
that modifies the stem. This concept is searched
in an ANSWER TAXONOMY comprising several
tops linked to a significant number of WordNet
noun and verb hierarchies. Each top represents
one of the possible expected answer types imple-
mented in our system (e.g. PERSON, PRODUCT,
NUMERICAL VALUE, COUNT, LOCATION). We
encoded a total of 38 possible answer types.
In addition, the question keywords used for
paragraph retrieval are also derived from the ques-
tion semantic form. The question keywords are
organized in an ordered list which first enumer-
ates the named entities and the question quota-
tions, then the concepts that triggered the recogni-
tion of the expected answer type followed by all
adjuncts, in a left-to-right order, and finally the
question head. The conjunction of the keywords
represents the boolean query applied to the doc-
ument index. (Moldovan et al, 2000) details the
empirical methods used in our system for trans-
forming a natural language question into an IR
query.
Answer Semantic Form
No
No
Yes
Lexical 
Alternations
Semantic
Alternations
Question Semantic Form
Answer Logical Form
S-UNIFICATIONS
Expected Answer Type 
Question Logical Form
ABDUCTIVE   PROOF
in paragraph
No
Yes
No
Yes
LOOP 2
Filter out paragraph
Expected Answer Type
Question Keywords
Min<Number Paragraphs<Max No
LOOP 1Index
Yes LOOP 3
Yes
PARSE
	



Retrieval
Cached Questions
Cached Answers
    
Question
REFORMULATION
Figure 1: Feedbacks for the Answer Search.
It is well known that one of the disadvantages
of boolean retrieval is that it returns either too
many or too few documents. However, for ques-
tion answering, this is an advantage, exploited by
the first feedback loop represented in Figure 1.
Feedback loop 1 is triggered when the number of
retrieved paragraphs is either smaller than a min-
imal value or larger than a maximal value deter-
mined beforehand for each answer type. Alterna-
tively, when the number of paragraphs is within
limits, those paragraphs that do not contain at
least one concept of the same semantic category
as the expected answer type are filtered out. The
remaining paragraphs are parsed and their depen-
dency structures, called answer semantic forms,
are derived.
Feedback loop 2 illustrated in Figure 1 is acti-
vated when the question semantic form and the
answer semantic form cannot by unified. The uni-
fication involves three steps:
 Step 1: The recognition of the expected answer
type. The first step marks all possible concepts
that are answer candidates. For example, in the
case of TREC -9 question Q243: ?Where did the
ukulele originate ??, the expected answer type is
LOCATION. In the paragraph ?the ukulele intro-
duced from Portugal into the Hawaiian islands?
contains two named entities of the category LO-
CATION and both are marked accordingly.
 Step 2: The identification of the question con-
cepts. The second step identifies the question
words, their synonyms, morphological deriva-
tions or WordNet hypernyms in the answer se-
mantic form.
 Step 3: The assessment of the similarities of
dependencies. In the third step, two classes of
similar dependencies are considered, generating
unifications of the question and answer semantic
forms:Answer Mining from On-Line Documents
Marius Pas?ca and Sanda M. Harabagiu
Department of Computer Science and Engineering
Southern Methodist University
Dallas, TX 75275-0122
 
mars,sanda  @engr.smu.edu
Abstract
Mining the answer of a natural lan-
guage open-domain question in a large
collection of on-line documents is made
possible by the recognition of the ex-
pected answer type in relevant text pas-
sages. If the technology of retriev-
ing texts where the answer might be
found is well developed, few studies
have been devoted to the recognition of
the answer type.
This paper presents a unified model of
answer types for open-domain Ques-
tion/Answering that enables the discov-
ery of exact answers. The evaluation
of the model, performed on real-world
questions, considers both the correct-
ness and the coverage of the answer
types as well as their contribution to an-
swer precision.
1 Introduction
Answer mining, a.k.a. textual Ques-
tion/Answering (Q/A), represents the task of
discovering the answer to an open-domain nat-
ural language question in large text collections.
Answer mining became a topic of significant
recent interest, partly due to the popularity of In-
ternet Q/A services like AskJeeves and partly due
to the recent evaluations of domain-independent
Q/A systems organized in the context of the
Text REtrieval Conference (TREC)1. The TREC
1The Text REtrieval Conference (TREC) is a series of
evaluations of fully automatic Q/A systems
specified two restrictions: (1) there is at least
one document in the test collection that contains
the answer to a test question; and (2) the answer
length is either 50 contiguous bytes (short an-
swers) or 250 contiguous bytes (long answers).
These two requirements intentionally simplify
the answer mining task, since the identification
of the exact answer is left to the user. However,
given that the expected information is recognized
by inspecting text snippets of relatively small
size, the TREC Q/A task took a step closer
to information retrieval rather than document
retrieval. Moreover, the techniques developed
to extract text snippets where the answers might
lie paved the way to a unified model for answer
mining.
To find the answer to a question several steps
must be taken, as reported in (Abney et al, 2000)
(Moldovan et al, 2000) (Srihari and Li, 2000):
 First, the question semantics needs to be cap-
tured. This translates into identifying (i) the
expected answer type and (ii) the question
keywords that can be used to retrieve text
passages where the answer may be found.
 Secondly, the index of the document collec-
tion must be used to identify the text pas-
sages of interest. The retrieval method either
employs special operators or simply modi-
fies boolean or vector retrieval. Since the
expected answer type is known at the time
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
of the retrieval, the quality of the text pas-
sages is greatly improved by filtering out
those passages where concepts of the same
category as the answer type are not present.
 Thirdly, answer extraction takes place by
combining several features that take into ac-
count the expected answer type.
Since the expected answer type is the only in-
formation used in all the phases of textual Q/A,
its recognition and usage is central to the perfor-
mance of answer mining.
For an open-domain Q/A system, establishing
the possible answer types is a challenging prob-
lem. Currently, most of the systems recognize the
answer type by associating the question stem (e.g.
What, Who, Why or How) and one of the concepts
from the question to a predefined general cate-
gory, such as PERSON, ORGANIZATION, LOCA-
TION, TIME, DATE, MONEY or NUMBER. Since
many of these categories are represented in texts
as named entities, their recognition as possible
answers is enabled by state-of-the-art Named En-
tity (NE) recognizers, devised to work with high
precision in Information Extraction (IE) tasks. To
allow for NE-supported answer mining, a large
number of semantic categories corresponding to
various names must be considered, e.g. names of
cars, names of diseases, names of dishes, names
of boats, etc. Furthermore, a significant number
of entities are not unique, therefore do not bear
names, but are still potential answers to an open-
domain question. Additionally, questions do not
focus only on entities and their attributes; they
also ask about events and their related entities.
In this paper we introduce a model of answer
types that accounts for answers to questions of
various complexity. The model enables several
different formats of the exact answer to open-
domain questions and considers also the situation
when the answer is produced from a number of
different document sources. We define formally
the answer types to open-domain questions and
extend the recognition of answer types beyond the
question processing phase, thus enabling several
feed-back mechanisms derived from the process-
ing of documents and answers.
The main contribution of the paper is in provid-
ing a unified model of answer mining from large
collections of on-line documents that accounts for
the processing of open-domain natural language
questions of varied complexity. The hope is that a
coherent model of the textual answer discovery
could help developing better text mining meth-
ods, capable of acquiring and rapidly prototyping
knowledge from the vast amount of on-line texts.
Additionally, such a model enables the develop-
ment of intelligent conversational agents that op-
erate on open-domain tasks.
We first present a background of Q/A systems
and then define several classes of question com-
plexity. In Section 3 we present the formal answer
type model whereas in Section 4 we show how to
recognize the answer type of open-domain ques-
tions and use it to mine the answer. Section 5
presents the evaluation of the model and summa-
rizes the conclusions.
2 Background
Open-Domain Question/Answering
To search in a large collection of on-line docu-
ments for the answer to a natural language ques-
tion we need to know (1) what we are looking for,
i.e. the expected answer type; and (2) where the
answer might be located in the collection. Fur-
thermore, knowing the answer type and recog-
nizing a text passage where the answer might be
found is not sufficient for extracting the exact an-
swer. We also need to know the dependencies
between the answer type and the other concepts
from the question or the answer. For example, if
the answer type of the TREC question
QT: How many dogs pull a sled in the Iditarod?
is known to be a number, we also need to be aware
that this number must quantify the dogs harnessed
to a sled in the Iditarod games and not the number
of participants in the games.
Capturing question or answer dependencies
can be cast as a straightforward process of
mapping syntactic trees to sets of binary head-
modifier relationships, as first noted in (Collins,
1996). Given a parse tree, the head-child of each
syntactic constituent can be identified based on a
simple set of rules used to train syntactic parsers,
cf. (Collins, 1996). Dependency relations are es-
tablished between each leaf corresponding to the
head child and the leaves of its constituent sib-
VP
SQ
WP
PP
What do most tourists visit in
Question ET1:
Reims
RBSVBP NNS VB IN NNP
NPNPWHNP
SBARQParse:
What visit Reimstouristsmost
Question Dependecies (ET1):
What do most tourists visit in Reims?
(a)
(b)
Figure 1: Example of TREC test question
lings that are not stop words, as illustrated by the
mapping of Figure 1(a) into Figure 1(b). Unlike in
IR systems, question stems are considered content
words. When question dependencies are known
(Harabagiu et al, 2000) proposed a technique of
identifying the answer type based on the semantic
category of the question stem and eventually of its
most connected dependent concept. For example,
in the case of question ET1, illustrated in Figure 1,
the answer type is determined by the ambiguous
question stem what and the verb visit. The answer
type is the object of the verb visit, which is a place
of attraction or entertainment, defined by the se-
mantic category LANDMARK. The answer type
replaces the question stem, generating the follow-
ing dependency graph, that can be later unified
with the answer dependency graph:
mostLANDMARK tourists visit Reims
However syntactic dependencies vary across
question reformulations or equivalent answers
made possible by the productive nature of natural
language. For example, the dependency structure
of ET2, a reformulation of question ET1 differs
from the dependency structure of ET1:
Due to the fact that verbs see and visit are syn-
onyms (cf. WordNet (Miller, 1995)) and pronoun
I can be read a possible visitor, the dependency
Question ET2:
ILANDMARK Reimssee
What could I see in Reims?
structures of ET1 and ET2 can be mapped one into
another. The mapping is produced by unifying the
two structures when lexical and semantic alterna-
tions are allowed. Possible lexical alternations are
synonyms or morphological alternations. Seman-
tic alternations consist of hypernyms, entailments
or paraphrases. The unifying mapping of ET1 and
ET2 shows that the two questions are equivalent
only when I refers to a visitor; other readings of
ET2 being possible when the referent is an investi-
gator or a politician. In each of the other readings,
the answer type of the question would be differ-
ent. The unifying mapping of ET1 and ET2 is:
ReimsI -->tourists see/visitLANDMARK
Similarly, a pair of equivalent answers is rec-
ognized when lexical and semantic alternations of
the concepts are allowed. This observation is cru-
cial for answer mining because:
1. it establishes the dependency relations as the
basic processing level for Q/A; and
2. it defines the search space based on alterna-
tions of the question and answer concepts.
Consequently, lexical and semantic alternations
are incorporated as feedback loops in the architec-
ture of open-domain Q/A systems, as illustrated
in Figure 2.
To locate answers, text passages are retrieved
based on keywords assembled from the question
dependency structure. At the time of the query, it
is unknown which keywords can be unified with
answer dependencies. However, the relevance of
the query is determined by the number of result-
ing passages. If too many passages are gener-
ated, the query was too broad, thus is needs a
specialization by adding a new keyword. If too
few passages were retrieved the query was too
specific, thus one keyword needs to be dropped.
The relevance feedback based on the number of
retrieved passages ends when no more keywords
can be added or dropped. After this, the unifi-
cations of the question and answer dependencies
Answer
Dependencies
KB
KB
Answer Type
Keywords
Index
Relevance Feedback (# Passages)
Lexical Alternations
Semantic Unifications Abductive Justification
Question On-line Documents Answer
Dependencies
Question
Text Passages
Semantic Alternations
Answer Fusion
Lexico
Semantic
Figure 2: A diagram of the feedbacks supporting Open-Domain Q/A
is produced and the lexical alternations imposed
by unifications are added to the list of keywords,
making possible the retrieval of new, unseen text
passages, as illustrated in Figure 2.
The unification of dependency structures al-
lows erroneous answers when the resulting map-
ping is a sparse graph. To justify the correct-
ness of the answer an abductive proof backchain-
ing from the answer to the question must be
produced. Such abductive mechanisms are de-
tailed in (Harabagiu et al, 2000). Moreover, the
proof relies on lexico-semantic knowledge avail-
able from WordNet as well as rapidly formated
knowledge bases generated by mechanisms de-
scribed in (Chaudri et al, 2000). The justification
process brings forward semantic alternations that
are added to the list of keywords, the feedback
destination of all loops represented in Figure 2.
Mining the exact answer does not always end
after extracting the answer type from a correct
text snippet because often they result only in par-
tial answers that need to be fused together. The
fusion mechanisms are dictated by the answer
type.
Question Complexity
Open-Domain natural language questions can
also be of different complexity levels. Gener-
ally, the test questions used in the TREC evalu-
ations were qualified as fact-based questions (cf.
(Voorhees and Tice, 2000)) as they mainly were
short inquiries about attributes or definitions of
some entity or event. Table 1 lists a sample of
TREC test questions.
The TREC test set did not include any question
Where is Romania located? Europe
Who wrote ?Dubliners?? James Joyce
What is the wingspan of a condor? 9 feet
What is the population of Japan? 120 million
What king signed the Magna Carta? King John
Name a flying mammal. bat
Table 1: TREC test questions and their exact an-
swers (boldfaced)
that can be modeled as Information Extraction
(IE) task. Typically, IE templates model queries
regarding who did an event of interest, what was
produced by that event, when and where and even-
tually why. The event of interest is a complex
event, like terrorism in Latin America, joint ven-
tures or management successions. An example of
template-modeled question is:
What management successions occurred at
IBM in 1999?
In addition, questions may also ask about de-
velopments of events or trends that are usually
answered by a text summary. Since data produc-
ing these summaries can be sourced in different
documents, summary fusion techniques as pro-
posed in (Radev and McKeown, 1998) can be em-
ployed. Template-based questions and summary-
asking inquiries cover most of the classes of ques-
tion complexity proposed in (Moldovan et al,
2000). Although the topic of natural language
open-domain question complexity needs further
study, we consider herein the following classes of
questions:
 Class 1: Questions inquiring about entities,
events, entity attributes (including number),
event themes, event manners, event condi-
tions and event consequences.
 Class 2: Questions modeled by templates,
including questions that focus only on one
of the template slots (e.g. ?What managers
were promoted last year at Microsoft??).
 Class 3: Questions asking for a sum-
mary that is produced by fusing template-
based information from different sources
(e.g. ?What happened after the Titanic
sunk??).
Since (Radev and McKeown, 1998) describes the
summary fusion mechanisms, Class 3 of ques-
tions can be reduced in this paper to Class 2,
which deals with the processing of the template.
3 A Model of Answer Types
This section describes a knowledge-based model
of open-domain natural language answer types
(ATs). In particular we formally define the an-
swer type through a quadruple
 
CATEGORY, DEPENDENCY, NUMBER,
FORMAT  .
The CATEGORY is defined as one of the following
possibilities:
1. one of the tops of a predefined ANSWER
TAXONOMY or one of its nodes;
2. DEFINITION;
3. TEMPLATE; or
4. SUMMARY.
For expert Q/A systems, this list of categories can
be extended. The DEPENDENCY is defined as the
question dependency structure when the CATE-
GORY belongs to the ANSWER TAXONOMY or is
a DEFINITION. Otherwise it is a template auto-
matically generated. The NUMBER is a flag indi-
cating whether the answer should contain a single
datum or a list of elements. The FORMAT defines
the text span of the exact answer. For example, if
the CATEGORY is DIMENSION, the FORMAT is
 Number 	  Measuring Unit 	 .
The ANSWER TAXONOMY was created in
three steps:
Step 1 We devise a set of top categories modeled
after the semantic domains encoded in the Word-
Net database, which contains 25 noun categories
and 15 verb categories. The top of each WordNet
hierarchy corresponding to every semantic cate-
gory was manually inspected to select the most
representative nodes and add them to the tops of
he ANSWER TAXONOMY. Furthermore we have
added open semantic categories corresponding to
named entities. For example Table 2 lists the
named entity categories we have considered in
our experiments. Many of the tops of the AN-
SWER TAXONOMY are further categorized, as il-
lustrated in Figure 3. In total, we have considered
33 concepts as tops of the taxonomy.
D
ATER URATIOND
ERCENTAGEP
OUNTCEGREED
VALUEUMERICALN OCATIONL
COUNTRY
IMENSION TOWN
PROVINCE O THER
LOCATION
Figure 3: Two examples of top answer hierar-
chies.
Step 2 The additional categorization of the top
ANSWER TAXONOMY generates a many-to-
many mapping of the Named Entity categories in
the tops of the ANSWER TAXONOMY. Figure 4
illustrates some of the mappings.
date time organization city
product price country money
human disease phone number continent
percent province other location plant
mammal alphabet airport code game
bird reptile university dog breed
number quantity landmark dish
Table 2: Named Entity Categories.
ERSON
MONEY
PEEDS
DURATION
MOUNTA
A NSWER TYPE ENTYTY
P
number
N AMED CATEGORY
human
money
price
quantity
Figure 4: Mappings of answer types in named en-
tity categories.
Step 3: Each leaf from the top of the ANSWER
TAXONOMY is connected to one or several Word-
N ALUEV
dew
point
temperature
body
temperature
zero
absolute perimeter,
girth
size
largeness
bigness
circumference
distance,
length
wingspread
wingspan,
light time
altitude
duration,
UMERICAL
length
N
longevity
longness
ATIONALITY
of the trip from...?
What is the duration
of an active volcano get?
EGREED TEMPERATURE DURATION COUNT SPEED DIMENSION
OCATIONL
What is the wingspan
of a condor? in diameter?
How big is our galaxyHow hot does the inside
Figure 5: Fragment of the ANSWER TAXONOMY.
Net subherarchies. Figure 5 illustrates a fragment
of the ANSWER TAXONOMY comprising several
WordNet subhierarchies.
4 Answer Recognition and Extraction
In this section we show how, given a question and
its dependency structure, we can recognize its an-
swer type and consequently extract the exact an-
swer. Here we describe four representative cases.
Case 1: The CATEGORY of the answer type is
DEFINITION when the question can be matched
by one of the following patterns:
(Q-P1):What   is  are  phrase to define  ?
(Q-P2):What is the definition of  phrase to define  ?
(Q-P3):Who   is was  are were  person name(s)  ?
The format of the DEFINITION answers is sim-
ilarly dependent on a set of patterns, determined
as the head of the  Answer phrase 	 :
(A-P1):   phrase to define    is  are 	
 Answer phrase 
(A-P2):   phrase to define  ,   a  the  an  Answer phrase 	
(A-P3):   phrase to define  ? 
 Answer phrase 
Case 2: The dependency structure of the ques-
tion indicates that a special instance of a concept
is sought. The cues are given either by the pres-
ence of words kind, type, name or by the ques-
tion stems what or which connected to the object
of a verb. Table 3 lists a set of such questions
and their corresponding answers. In this case the
answer type is given by the subhierarchy defined
by the node from the dependency structure whose
adjunct is either kind, type, name or the question
stem. In this situation the CATEGORY does not
belong to the top of the ANSWER TAXONOMY,
but it is rather dynamically created by the inter-
pretation of the dependency graph.
For example, the dynamic CATEGORY bridge,
generated for Q204 from Table 3, contains 14
member instances, including viaduct, rope bridge
and suspension bridge. Similarly, question Q581
generates a dynamic CATEGORY flower, with 470
member instances, comprising orchid, petunia
and sunflower. For dynamic categories all mem-
ber instances are searched in the retrieved pas-
sages during answer extraction to detect candidate
answers.
Case 3: In all other cases, the concept related
to the question stem in the question dependency
graph is searched through the ANSWER TAXON-
OMY, returning the answer type as the top of it
hierarchy. Figure 5 illustrates several questions
and their answer type CATEGORY.
Case 4: Whenever the semantic dependencies of
several correct answers can be mapped one into
another, we change the CATEGORY of the answer
type into TEMPLATE. The slots of the actual tem-
plate are determined by a three step procedure,
that we illustrate with a walk-through example
corresponding to the question What management
successions occurred at IBM in 1999?:
Step 1: For each pair of extracted candidate
Q204: What type of bridge is the Golden Gate Bridge?
Answer: the Seto Ohashi Bridge, consisting of six suspension bridges in the style of Golden Gate Bridge.
Q267: What is the name for clouds that produce rain?
Answer: Acid rain in Cheju Island and the Taean peninsula is carried by rain clouds from China.
Q503: What kind of sports team is the Buffalo Sabres?
Answer: Alexander Mogilny hopes to continue his hockey career with the NHL?s Buffalo Sabres.
Q581: What flower did Vincent Van Gogh paint?
Answer: In March 1987, van Gogh?s ?Sunflowers? sold for $39.9 million at Christie?s in London
Table 3: TREC test questions and their answers. The exact answer is emphasized.
nominate/assignOrganization
(b)
PositionPerson
Organizationresign/leave PositionPerson
(a)
Position
Person1 Person2 Organization Positionreplace/succeed
Person1 Person2 Organization
Figure 6: Dependencies that generate templates.
answers unify the dependency graphs and find
common generalizations whenever possible. Fig-
ure 6(a) illustrates some of the mappings.
Step 2: Identify across mappings the common
categories and the trigger-words that were used
as keywords. In Figure 6(a) the trigger words are
boldfaced.
Step 3: Collect all common categories in a tem-
plate and use their names as slots. Figure 6(b)
illustrates the resulting template.
This procedure is a reverse-engineering of the
mechanisms used generally in Information Ex-
traction (IE), where given a template, linguistic
patterns are acquired to identify the text frag-
ments having relevant information. In the case
of answer mining, the relevant text passages are
known. The dependency graphs help finding the
linguistic rules and are generalized in a template.
To be able to generate the template we also
need to have a way of extracting the text where
the answer dependencies are detected. For this
purpose we have designed a method that em-
ploys a simple machine learning mechanism: the
perceptron. For each text passage retrieved by
the keyword-based query we define the following
seven features:
 	 the number of question words matched in
the same phrase as the answer type CATEGORY;
 
 the number of question words matched in
the same sentence as the answer type CATEGORY;
 	 : a flag set to 1 if the answer type CATE-
GORY is followed by a punctuation sign, and set
to 0 otherwise;
  : the number of question words
matches separated from the answer type CATE-
GORY by at most three words and one comma;
  : the number of question words occur-
ring in the same order in the answer text as in the
question;
 Coling 2010: Poster Volume, pages 819?827,
Beijing, August 2010
Instance Sense Induction from Attribute Sets
Ricardo Martin-Brualla
Google Inc
rmbrualla@gmail.com
Enrique Alfonseca
Google Inc
ealfonseca@google.com
Marius Pasca
Google Inc
mars@google.com
Keith Hall
Google Inc
kbhall@google.com
Enrique Robledo-Arnuncio
Google Inc
era@google.com
Massimiliano Ciaramita
Google Inc
massi@google.com
Abstract
This paper investigates the new problem
of automatic sense induction for instance
names using automatically extracted at-
tribute sets. Several clustering strategies
and data sources are described and eval-
uated. We also discuss the drawbacks of
the evaluation metrics commonly used in
similar clustering tasks. The results show
improvements in most metrics with re-
spect to the baselines, especially for pol-
ysemous instances.
1 Introduction
Recent work on information extraction increas-
ingly turns its attention to the automatic acqui-
sition of open-domain information from large
text collections (Etzioni et al, 2008). The ac-
quired information typically includes instances
(e.g. barack obama or hillary clinton), class la-
bels (e.g. politician or presidential candidate)
and relations and attributes of the instances (e.g.
president-country or date-of-birth) (Sekine, 2006;
Banko et al, 2007).
Within the larger area of relation extraction,
the acquisition of instance attributes (e.g. pres-
ident for instances of countries, or side effects
for instances of drugs) plays an important role,
since attributes may serve as building blocks in
any knowledge base constructed around open-
domain classes of instances. Thus, a variety
of attribute extraction methods mine textual data
sources ranging from unstructured (Tokunaga et
al., 2005) or structured (Cafarella et al, 2008) text
within Web documents, to human-compiled ency-
clopedia (Wu et al, 2008; Cui et al, 2009) and
Web search query logs (Pas?ca and Van Durme,
2007), attempting to extract, for a given class, a
ranked list of attributes that is as comprehensive
and accurate as possible.
Previous work on attribute extraction, however,
does not capture or address attributes of polyse-
mous instances. An instance may have differ-
ent meanings, and the extracted attributes may
not apply to all of them. For example, the
most salient meanings of darwin are the scientist
Charles Darwin, an Australian city, and an op-
erating system, plus many less-known meanings.
For these ambiguous instances, it is common for
the existing procedures to extract mixed lists of
attributes that belong to incompatible meanings,
e.g. {biography, population, hotels, books}.
This paper explores the problem of automati-
cally inducing instance senses from the learned
attribute lists, and describes several clustering so-
lutions based on a variety of data sources. For
that, it brings together research on attribute acqui-
sition and on word sense induction. Results show
that we can generate meaninful groupings of at-
tributes for polysemous instance names, while not
harming much the monosemous instance names
by generating unwanted clusters for them. The
results are much better than for a random base-
line, and are superior to the one-in-all and the all-
singleton baselines.
2 Previous Work
Previous work on attribute extraction uses a va-
riety of types of textual data as sources for mining
attributes. Some methods take advantage of struc-
tured and semi-structured text available within
Web documents. Examples of this are the use of
markup information in HTML documents to ex-
819
tract patterns and clues around attributes (Yoshi-
naga and Torisawa, 2007; Wong and Lam, 2009;
Ravi and Pas?ca, 2008), or the use of articles
within online encyclopedia as sources of struc-
tured text for attribute extraction (Suchanek et al,
2007; Nastase and Strube, 2008; Wu and Weld,
2008). Regarding unstructured text in Web docu-
ments, the method described in (Tokunaga et al,
2005) takes various class labels as input, and ap-
plies manually-created lexico-syntactic patterns to
document sentences to extract candidate attributes
ranked using several frequency statistics. In (Bel-
lare et al, 2007), the extraction is guided by a set
of seed instances and attributes rather than hand-
crafted patterns, with the purpose of generating
training data and extract new instance-attribute
pairs from text.
Web search queries have also been used as a
data source for attribute extraction, using lexico-
syntactic patterns (Pas?ca and Van Durme, 2007) or
seed attributes (Pas?ca, 2007) to guide the extrac-
tion, and leading to attributes of higher accuracy
than those extracted with equivalent techniques
from Web documents (Pas?ca et al, 2007).
Another related area to this work is the field of
word sense induction: the task of identifying the
possible senses of a word in a corpus using unsu-
pervised methods (Yarowsky, 1995), as opposed
to traditional disambiguation methods which rely
on the availability of a finite and static list of pos-
sible meanings. In (Agirre and Soroa, 2007) a
framework is proposed for evaluating such sys-
tems. Word sense induction can be naturally for-
mulated as a clustering task. This introduces
the complication of choosing the right number
of possible senses, hence a Bayesian approach to
WSI was proposed which deals with this problem
within a principled generative framework (Brody
and Lapata, 2009). Another related line of work
Turkey Attributes Darwin Attributes
maps1 capital1 maps1 definition1,3
recipes2 culture1 awards2 jobs1
pictures1,2 history1 shoes1 tourism1
calories2 tourism1 evolution3 biography3
facts1,2 nutrition facts2 theory3 attractions1
nutrition2 beaches1 weather1 hotels1
cooking time2 brands2 pictures1,3 ports4
religion1 language1 quotes3 population1
Table 1: Attributes extracted for the instances
Turkey and Darwin.
is the disambiguation of people names (Mann and
Yarowsky, 2003). In SEMEVAL-1, a shared task
was introduced dedicated to this problem, the Web
People Search task (Artiles et al, 2007; Artiles et
al., 2009). Disambiguating names is also often ap-
proached as a clustering problem. One challenge
shared by word sense induction and name disam-
biguation (and most unsupervised settings), is the
evaluation. In both tasks, simple baselines such as
predicting one single cluster tend to outperform
more sophisticated approaches (Agirre and Soroa,
2007; Artiles et al, 2007).
3 Instance Sense Induction
3.1 Problem description
This paper assumes the existence of an attribute
extraction procedure. Using those attributes, our
aim is to identify the coarse-grained meanings
with which each attribute is associated. As an
example, Table 1 shows the top 16 attributes ex-
tracted using the procedure described in (Pas?ca
and Van Durme, 2007). Salient meanings for
turkey are the country name (labeled as 1 in the
table), and the bird name (labeled as 2). Some at-
tributes are applicable to both meanings (pictures
and facts). The second example, darwin, can re-
fer to a city (sense 1), the Darwin Awards (sense
2), the person (sense 3), and an operating system
(sense 4).
Examples of applications that need to dis-
criminate between the several meanings of in-
stances are user-facing applications requiring the
attributes to be organized logically and informa-
tion extraction pipelines that depend on the ex-
tracted attributes to find values in documents.
The problem we are addressing is the automatic
induction of instance senses from the attribute
sets, by grouping together the attributes that can
be applied to a particular sense. As in related work
on sense induction (Agirre and Soroa, 2007; Ar-
tiles et al, 2007), we approach this as a clustering
problem: finding the right similarity metrics and
clustering procedures to identify sets of related at-
tributes in an instance. We propose a clustering
based on the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977), exploring differ-
ent parameters, similarity sources, and prior dis-
tributions.
820
3.2 Instance and attributes input data
The input data of instances and attributes has been
obtained, in a fully automated way, following
the method described in (Pas?ca and Van Durme,
2007). The input dataset is a set of fully anony-
mized set of English queries submitted to a popu-
lar (anonymized) search engine. The set contains
millions of unique isolated, individual queries that
are independent from one another. Each query
is accompanied by its frequency of occurrence
in the query logs. The sum of frequencies of
all queries in the dataset is hundreds of millions.
Other sources of similar data are available pub-
licly for research purposes (Gao et al, 2007). This
extraction method applies a few patterns (e.g., the
A of I, or I?s A, or A of I) to queries within
query logs, where an instance I is one of the most
frequent 5 million queries from the repository of
isolated queries, and A is a candidate attribute.
For each instance, the method extracts ranked lists
containing zero, one or more attributes, along with
frequency-based scores. For this work, only the
top 32 attributes of each instance were used, in or-
der to have an input set for the clustering with a
reasonable size, but to keep precision at high lev-
els.
3.3 Per-attribute clustering information
For each (instance, attribute) pair, the following
information is collected:
Search results: The top 20 search results (in-
cluding titles and snippets) returned by a popular
search engine for a query created by concatenat-
ing the instance and the attribute. The motivation
for this data source is that the attributes that re-
fer to the same meaning of the instance should
help the search engine in selecting web pages that
refer to that meaning. The titles and snippets of
these search results are expected to contain other
terms related to that meaning. For example, for
the queries [turkey maps] and [turkey culture] the
search results will contain information related to
the country, whereas [turkey recipes] and [turkey
nutritional value] should share many terms about
the poultry.
Query sessions: A query session is a series of
queries submitted by a single user within a small
range of time (Silverstein et al, 1999). Informa-
tion stored in the session logs may include the text
For each (instance, attribute) pair:
? Retrieve all the sessions that contained the query [in-
stance attribute].
? Collect the set of all the queries that appeared in the
same session and which are a superstring of instance.
? Remove instance from each of those queries, and out-
put the resulting set of query words.
Figure 1: Algorithm to collect session phrases as-
sociated to attributes.
of the queries and metadata, such as the time, the
type of query (e.g., using the normal or the ad-
vance form), and user settings such as the Web
browser used (Silverstein et al, 1999).
Users often search for related queries within
a session: queries on the culture of the coun-
try Turkey will tend to be surrounded by queries
about topics related to the country; similarly,
queries about turkey recipes will tend to be sur-
rounded by other queries on recipes. Therefore,
if two attributes refer to the same meaning of the
instance, the distributions of terms that co-occur
with them in the same search sessions is expected
to be similar. To ensure that the user did not
change intent during the session, we also require
the queries from which we extract phrases to con-
tain the instance of interest. The pseudocode of
the procedure is shown in Figure 1.
Class labels: As described in (Pas?ca and Van
Durme, 2008), we collect for each instance (e.g.,
turkey), a ranked list of class labels (e.g., country,
location, poultry, food). The procedure uses a col-
lection of Web documents and applies some IsA
extraction patterns selected from (Hearst, 1992).
Using the (instance, ranked-attributes) and the (in-
stance, ranked-class labels) lists, it is possible to
aggregate the two datasets to obtain, for each at-
tribute, the class labels that are most strongly as-
sociated to it (Figure 2).
3.4 EM clustering
We run a set of EM clusterings separately for the
attributes of each instance. The model imple-
mented is the following: given an instance, let
A = {a1, a2, ..., an} be the set of attributes as-
sociated with that instance. Let T be the vocabu-
lary for the terms found in the search results, S the
vocabulary of session log terms co-occurring with
821
For each attribute:
? Collect all the instances that contain that attribute.
? For each class label, average its ranks for those in-
stances. If an instance does not contain a particular
class label, use as rank the size of the longest list of
class labels plus one.
? Rank the class labels from smaller to larger average
rank.
Figure 2: Algorithm to collect class labels associ-
ated to attributes.
the attribute, and C be the set of all the possible
class labels. Let K be the cluster function which
assigns cluster indexes to the attributes.
We assume that the distributions for snippet
terms, session terms and class labels are condi-
tionally independent given the clustering. Further-
more, we assume that the distribution of terms for
queries in a cluster are also conditionally indepen-
dent given the cluster assignments:
p?(T |K,A) ?
Y
j
p?(tj |K,A)
p?(S|K,A) ?
Y
k
p?(sk|K,A)
p?(C|K,A) ?
Y
l
p?(cl|K,A)
The clustering model for each instance (the ex-
pectation step) is, therefore:
p?(KT SC|A,?) =
N?
i
p?(K|A)p?(T |K,A)p?(S|K,A)p?(C|K,A)
To estimate the parameters of the model, we must
be able to estimate the following distributions dur-
ing the maximization step:
? p?(tj |K,A) = E?(tj ,K|A)E?(K|A)
? p?(sk|K,A) = E?(sk,K|A)E?(K|A)
? p?(cl|K,A) = E?(cl,K|A)E?(K|A)One advantage of this approach is that it allows
using a subset of the available data sources to eval-
uate their relative influence on the clustering qual-
ity. In the experiments we have tried all possible
combinations of the three data sources to find the
settings that give the best results.
3.5 Initialization strategies
The initial assignment of attributes to clusters is
important, since a bad seed clustering can lead
EM to local optima. We have tried the following
two strategies:
Random assignment: the attributes are assigned
to clusters randomly. To make the results repeat-
able, for each instance we use the instance name
as the seed for the random number generator.
K-means: the initial assignments of attributes
to clusters is performed using K-means. In this
model, we use a simple vector-space-model in the
following way:
1. Each attribute is represented with a bag-of-
words of the snippets of the search results for
a concatenation of the instance name and the
attribute. This is the same data already col-
lected for EM.
2. Each of the snippet terms in these bag-of-
words is weighted using the tf ? idf score,
with inverse document frequencies estimated
from an English web corpus with hundreds
of millions of documents.
3. The cosine of the angle of the vectors is used
as the similarity metric between each pair of
attributes.
Several values of K have been tried in our exper-
iments, as mentioned in Section 4.
3.6 Post-processing
EM works with a fixed set of clusters. In order
to decide which is the optimal number of clusters,
we have run all the experiments with a number of
clusters K that is large enough to accommodate
most of the queries in our dataset, and we run a
post-processing step that merges clusters for in-
stances that have less than K meanings.
Since we have, for each attribute, a distribution
of the most likely class labels (Section 3.3), the
post-processing performs as follows:
1. Generate a list of class labels per cluster, by
combining the ranked lists of per-attribute
class labels as was done in Section 3.3.
2. Merge together all the clusters such that their
sets of top k class labels are the same.
The values ofK and k are chosen by doing several
runs with different values on the development set,
as described in Section 4.
822
4 Evaluation and Results
4.1 Evaluation metrics
There does not exist a fully agreed evaluation
metric for clustering tasks in NLP (Geiss, 2009;
Amigo? et al, 2009). Each metric has its own
idiosyncrasies, so we have chosen to compute
six different evaluation metrics as described in
(Amigo? et al, 2009). Empirical results show they
are highly correlated, i.e., tuning a parameter by
hill-climbing on F-score typically also improves
the B3 F-score.
Purity (Zhao and Karypis, 2002): Let C be
the clusters to evaluate, L the set of cate-
gories (the clusters in the gold-standard), and
N the number of clustered items. Purity is
the average of the precision values: Purity =?
i
|Ci|
N maxj Prec(Ci, Lj), where the precisionfor cluster Ci with respect to category Lj is
Prec(Ci, Lj) = |Ci?Lj ||Ci| . Purity is a precision met-ric. Inverting the roles of the categories L and the
clusters C gives a recall metric, inverse purity,
which rewards grouping items together. The two
metrics can be combined in an F-score.
B3 Precision (Bagga and Baldwin, 1998): Let
L(e) and C(e) denote the gold-standard-category
and the cluster of an item e. The correctness of the
relation between e and other element e? is defined
as
Correctness(e, e?) =
?
1 iffL(e) = L(e?)? C(e) = C(e?)
0 otherwise
The B3 Precision of an item is the proportion
of items in its cluster which belong to its cat-
egory, including itself. The total precision is
the average of the item precisions: B3 Prec =
avge[avge?:C(e)=C(e?)Correctness(e, e?)]
B3 Recall: is calculated in a similar way, inverting
the roles of clusters and categories. The B3 F-
score is obtained by combining B3 precision and
B3 recall.
4.2 Gold standards
We have built two annotated sets, one to be used
as a development set for adjusting the parame-
ters, and a second one as a test set. The evalu-
ation settings were chosen without knowledge of
Purity Inv. F-score B3 B3 B3
Purity Precision Recall F-score
0.94 0.95 0.92 0.90 0.92 0.91
Table 2: Inter-judge agreement scores.
Polysemous Main meanings
airplane machine, movie
apple fruit, company
armstrong unit, company, person
chain reaction company, film, band, chemistry
chf airport, currency, heart attack
darwin person, city
david copperfield book, performer, movie
delta letter, airways
Table 3: Examples of polysemous instances.
the test set. Each of the two sets contains 75 in-
stances chosen randomly from the complete set of
instances with ranked attributes (Section 3.2 de-
scribed the input data). For the random sampling,
the instances were weighted with their frequency
in the query logs as full queries, so that more
frequent instances have higher chance to be cho-
sen. This ensures that uncommon instances are
not overrepresented in the gold-standard.
The annotators contributed 50 additional in-
stances (25 for development and 25 for testing)
that they considered interesting to study, e.g., be-
cause of having several salient meanings.
Five human annotators were shown the top 32
attributes for each instance, and they were asked
to cluster them. We decided to start with a sim-
plified version of the problem by considering it a
hard clustering task.
Table 2 shows that the average agreement
scores between judge pairs, measured with the
same evaluation metrics used for the system out-
put, are quite high. In the first three metrics, the
F-score is not an average of precision and recall,
but a weighted average calculated separately for
each cluster, so it may have a value that is not be-
tween the values of precision and recall.
The annotated instances were classified as
monosemous/polysemous, depending on wether
or not they had more than one cluster with enough
(five) attributes. This classification allows to re-
port separate results for the whole set (where in-
stances with just one major sense dominate) and
for the subset of polysemous instances. Table 3
shows examples of polysemous instances. Exam-
823
All instances polysemous instances
Weights Purity Inv. F B3 B3 B3 F Purity Inv. F B3 B3 B3 F
Purity score Prec. Recall score Purity score Prec. Recall score
All-in-one 0.797 1.000 0.766 0.700 1.000 0.797 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242 1.000 0.205 0.266 1.000 0.205 0.333
Random 0.888 0.322 0.451 0.851 0.246 0.373 0.685 0.362 0.447 0.595 0.276 0.373
Random Only snippets 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
Init. Only sessions 0.797 0.948 0.728 0.700 0.944 0.753 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.798 0.983 0.760 0.701 0.969 0.785 0.561 0.990 0.541 0.415 0.981 0.574
No snippets 0.798 0.934 0.723 0.702 0.918 0.744 0.561 0.990 0.541 0.415 0.981 0.574
No sessions 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
No class labels 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
All 0.809 0.380 0.420 0.736 0.316 0.414 0.596 0.430 0.400 0.483 0.361 0.399
K-Means Only snippets 0.844 0.765 0.700 0.771 0.654 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Init. Only sessions 0.798 0.957 0.736 0.702 0.949 0.759 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.824 0.656 0.622 0.747 0.568 0.604 0.641 0.768 0.565 0.519 0.699 0.575
No snippets 0.824 0.655 0.622 0.748 0.562 0.598 0.640 0.768 0.565 0.518 0.698 0.574
No sessions 0.843 0.770 0.701 0.769 0.661 0.677 0.671 0.806 0.587 0.556 0.719 0.611
No class labels 0.844 0.762 0.698 0.771 0.651 0.673 0.671 0.806 0.587 0.556 0.719 0.611
All 0.843 0.767 0.699 0.770 0.657 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Table 4: Scores over all instances and over polysemous instances.
ples of monosemous instances are activision, am-
ctheaters, american airlines, ask.com, bebo, dis-
ney or einstein. 22% of the instances in the devel-
opment set and 13% of the instances in the test set
are polysemous.
4.3 Parameter tuning
We tuned the different parameters of the algorithm
using the development set. We performed several
EM runs including all three data sources, modi-
fying the following parameters: the smoothing 
added to the cluster soft-assignment in the Maxi-
mization step (Manning et al, 2008), the number
K of clusters for K-Means and EM, and the num-
ber k of top ranked class labels that two clusters
need to have in common in order to be merged
at the post-processing step. The best results were
obtained with  = 0.4, K = 5 and k = 1. These
are the values used in the experiments mentioned
from now on.
4.4 EM initialization and data sources
Table 4 shows the results after running EM over
the development set, using every possible combi-
nation of data sources, and the two initialization
strategies (random and K-Means). Several obser-
vations can be drawn from this table:
First, as mentioned in Section 2, the evalua-
tion metrics are biased towards the all-in-one solu-
tion. This is worsened by the fact that the majority
of the instances in our dataset are monosemous.
Therefore, the highest F-scores and B3 F-scores
are obtained by the all-in-one baseline, although
it is not the most useful clustering.
When using only class labels, EM tends to pro-
duce results similar to the all-in-one baseline This
can be explained by the limited class vocabulary
which makes most of the attributes share class la-
bels. The bad results when using only sessions are
caused by the presence of attributes with no ses-
sion terms, due to insufficient data.
The random clustering baseline (third line in
Table 4) tends to give smaller clusters than EM,
because it distributes instances uniformly across
the clusters. This leads to better precision scores,
and much worse recall and F-score metrics.
From these results, we conclude that snippet
terms are the most useful resource for clustering.
The other data sources do not provide a signifi-
cant improvement over it. The best results overall
for the polysemous instances, and the highest re-
sults for the whole dataset (excluding the outliers
that are too similar to the all-in-one baseline) are
obtained using snippet terms. For these configura-
tions, as we expected, the K-Means initialization
does a better job in avoiding local optima during
EM than the random one.
4.5 Post-processing
Table 5 includes the results on the development
set after post-processing, using the best configu-
ration for EM (K-Means initialization and snippet
terms for EM). Post-processing slightly hurts the
B3 F-score for polysemous terms, but it improves
results for the whole dataset, as it merges many
clusters for the monosemous instances.
824
Data Method Purity Inv. Purity F-score B3 Prec. B3 Recall B3 F-score
All instances All-in-one 0.797 1.000 0.766 0.700 1.000 0.797
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242
K-Means + EM (snippets) 0.844 0.765 0.700 0.771 0.654 0.675
K-Means + EM (snippets) + postprocessing 0.825 0.837 0.728 0.743 0.761 0.722
Polysemous All-in-one 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.205 0.266 1.000 0.205 0.333
K-Means + EM (snippets) 0.671 0.806 0.587 0.556 0.719 0.611
K-Means + EM (snippets) + postprocessing 0.644 0.846 0.592 0.518 0.777 0.607
Table 5: Scores only over all and polysemous instances, without and with postprocessing.
K-Means output EM output Post-processing
pictures, family, logo, biography pictures, biography, inauguration pictures, biography, inauguration
inauguration, song, lyrics, foods, song, lyrics, foods, timeline, song, lyrics, goods, timeline,
quotes, timeline, shoes, health care camping, shoes, maps, art, history, camping, shoes, maps, art, history
maps, art, kids, history, speeches official website, facts, speeches official website, facts, speeches
official website, facts, scandal scandal, blog, music scandal, blog, music, family, kids
economy, blog, music, flag, camping approval rating, health care, daughters
approval rating economy approval rating, health care,
daughters family, kids, daughters economy
symbol logo, quotes, symbol, flag logo, quotes, symbol, definition
definition, religion, definition, religion, slogan, books religion, slogan, books, flag
slogan, books
Table 6: Attributes extracted for the monosemous instance obama, using snippet terms for EM.
4.6 Clustering examples
Tables 6 and 7 show examples of clustering results
for three instances chosen as representatives of the
monosemous and the polysemous subsets. These
show that the output of the K-Means initialization
can uncover some meaningful clusters, but tends
to generate a dominant cluster and a few small or
singleton clusters. EM distributes the attributes
more evenly across clusters, combining attributes
that are closely related.
For monosemous instances like obama, EM
generates small clusters of highly related at-
tributes (e.g, family, kids and daughters). Post-
processing merges some of the clusters together,
but it fails to merge all into a single cluster.
For darwin, two of the small clusters given by
K-Means are actually good, as ports is the only at-
tribute of the operating system, and lyrics is one of
the two attributes referring to a song titled Darwin.
EM again redistributes the attributes, creating two
large and mostly correct clusters.
For david copperfield, EM creates two clusters
for the performer, one for the book, one for the
movie, and one for tattoo (off-topic for this in-
stance). The two clusters referring to the per-
former are merged in the post-processing, with
some errors remaining, e.g, trailer and second
wife are in the wrong cluster.
4.7 Results on the test set
Table 8 show the results of the EM clustering and
the postprocessing step when executed on the test
set. The settings are those that produced the best
results on the development set: using EM initial-
ized with K-Means, and using only snippet terms
for the generative model.
As mentioned above, the test set has a higher
proportion of monosemous queries than the de-
velopment set, so the all-in-one baseline pro-
duces better results than before. Still, we can see
the same trend happening: for the whole dataset
the F-score metrics are somewhat worse than the
best baseline, given that the evaluation metrics all
overvalue the all-in-one baseline, but this can be
considered an artifact of the metrics. As with the
development set, using EM produces the best pre-
cision scores (except for the all-singletons base-
line), and the postprocessing improves precision
and F-score over the all-in-one baseline. The
whole system improves considerably the F-score
for the polysemous terms.
5 Conclusions
This paper investigates the new task of inducing
instance senses using ranked lists of attributes as
input. It describes a clustering procedure based
on the EM model, capable of integrating differ-
825
Instance K-Means output EM output Post-processing
Darwin maps, shoes, logo, awards, maps, shoes, logo, maps, shoes, logo,
weather pictures, quotes, weather jobs, tourism weather jobs, tourism
definition, jobs, tourism, hotels, attractions, hotels, attractions,
biography, hotels, beaches, accommodation, beaches, accommodation,
attractions, beaches, tv show, clothing, tv show, clothing,
accommodation, tv show, postcode, music, review postcode, music, review
clothing, postcode, music side effects, airlines, side effects, airlines,
facts, review, history prices, lighting prices, lighting
side effects, airlines, awards, ports definition, population
prices, lighting evolution, theory, quotes awards, ports
ports pictures, biography, evolution, theory, quotes
evolution, theory, books facts, history, books pictures, biography,
lyrics lyrics facts, history, books
population definition, population lyrics
David Copperfield summary, biography, pictures, biography, pictures, quotes, biography, pictures, girlfriend
quotes, strokes, book review, strokes, tricks, tour dates, quotes, strokes, tricks, tattoo
tricks, tour dates, characters, lyrics, dating, logo, tour dates, secrets, lyrics,
lyrics, plot, synopsis, dating, filmography, cast members, wives, music, dating, logo,
logo, themes, author, official website, trailer, filmography, blog, cast members,
filmography, cast members, setting, religion official website, trailer,
official website, trailer, book review, review, house, setting, religion
setting, religion reviews book review, review, house,
house, reviews tattoo reviews
tattoo summary, second wife, summary, second wife,
second wife characters, plot, synopsis, characters, plot, synopsis,
girlfriend, secrets, wives, themes, author themes, author
review, music, blog girlfriend, secrets, wives,
music, blog
Table 7: Attributes extracted for three polysemous instances, using snippet terms for EM.
Set Solution Purity Inverse Purity F-score B3 Precision B3 Recall B3 F-score
All All-in-one 0.907 1.000 0.892 0.858 1.000 0.908
All-singletons 1.000 0.076 0.114 1.000 0.076 0.136
Random 0.936 0.325 0.463 0.914 0.243 0.377
EM 0.927 0.577 0.664 0.896 0.426 0.561
EM+postprocessing 0.919 0.806 0.804 0.878 0.717 0.764
Polysemous All-in-one 0.588 1.000 0.586 0.457 1.000 0.613
All-singletons 1.000 0.141 0.210 1.000 0.141 0.239
Random 0.643 0.382 0.441 0.549 0.288 0.369
EM 0.706 0.631 0.556 0.626 0.515 0.547
EM+postprocessing 0.675 0.894 0.650 0.564 0.842 0.661
Table 8: Scores in the test set.
ent data sources, and explores cluster initializa-
tion and post-processing strategies. The evalu-
ation shows that the most important of the con-
sidered data sources is the snippet terms obtained
from search engine results to queries made by
concatenating the instance and the attribute. A
simple post-processing that merges attribute clus-
ters that have common class labels can improve
recall for monosemous queries. The results show
improvements across most metrics with respect to
a random baseline, and F-score improvements for
polysemous instances.
Future work includes extending the generative
model to be applied across the board, linking the
clustering models of different instances with each
other. We also intend to explore applications of
the clustered attributes in order to perform extrin-
sic evaluations on these data.
References
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007, pages 7?12.
Association for Computational Linguistics.
Amigo?, E., J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Retrieval,
12(4):461?486.
Artiles, Javier, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proceed-
ings of SemEval-2007, pages 64?69.
Artiles, J., J. Gonzalo, and S. Sekine. 2009. Weps 2 evalua-
tion campaign: overview of the web people search cluster-
ing task. In 2nd Web People Search Evaluation Workshop
(WePS 2009), 18th WWW Conference.
Bagga, A. and B. Baldwin. 1998. Entity-based cross-
document co-referencing using the vector space model,
Proceedings of the 17th international conference on Com-
putational linguistics. In Proceedings of ACL-98.
826
Banko, M., Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information extraction
from the Web. In Proceedings of IJCAI-07, pages 2670?
2676, Hyderabad, India.
Bellare, K., P.P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. In NIPS 2007 Workshop
on Machine Learning for Web Search.
Brody, Samuel and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL ?09, pages 103?
111.
Cafarella, M.J., A. Halevy, D.Z. Wang, and Y. Zhang. 2008.
Webtables: Exploring the Power of Tables on the Eeb.
Proceedings of the VLDB Endowment archive, 1(1):538?
549.
Cui, G., Q. Lu, W. Li, and Y. Chen. 2009. Automatic Acqui-
sition of Attributes for Ontology Construction. In Pro-
ceedings of the 22nd International Conference on Com-
puter Processing of Oriental Languages, pages 248?259.
Springer.
Dempster, A.P., N.M. Laird, D.B. Rubin, et al 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society. Series B
(Methodological), 39(1):1?38.
Etzioni, O., M. Banko, S. Soderland, and S. Weld. 2008.
Open Information Extraction from the Web. Communica-
tions of the ACM, 51(12), December.
Gao, W., C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and
H. Hon. 2007. Cross-lingual query suggestion using
query logs of different languages. In Proceedings of
SIGIR-07, pages 463?470, Amsterdam, The Netherlands.
Geiss, J. 2009. Creating a Gold Standard for Sentence Clus-
tering in Multi-Document Summarization. ACL-IJCNLP
2009.
Hearst, M. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of COLING-92, pages
539?545, Nantes, France.
Mann, Gideon S. and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings of
HLT-NAACL 2003, pages 33?40. Association for Compu-
tational Linguistics.
Manning, C.D., P. Raghavan, and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge University
Press New York, NY, USA.
Nastase, V. and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In Proceedings of
AAAI-08, pages 1219?1224, Chicago, Illinois.
Pas?ca, M. and B. Van Durme. 2007. What you seek is what
you get: Extraction of class attributes from query logs. In
Proceedings of IJCAI-07, pages 2832?2837, Hyderabad,
India.
Pas?ca, M. and B. Van Durme. 2008. Weakly-supervised ac-
quisition of open-domain classes and class attributes from
web documents and query logs. In Proceedings of ACL-
08, pages 19?27, Columbus, Ohio.
Pas?ca, M., B. Van Durme, and N. Garera. 2007. The role of
documents vs. queries in extracting class attributes from
text. In Proceedings of CIKM-07, pages 485?494, Lis-
bon, Portugal.
Pas?ca, M. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of WWW-07, pages 101?110,
Banff, Canada.
Ravi, S. and M. Pas?ca. 2008. Using Structured Text for
Large-Scale Attribute Extraction. In CIKM. ACM New
York, NY, USA.
Sekine, S. 2006. On-Demand Information Extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Compu-
tational Linguistics Morristown, NJ, USA.
Silverstein, C., H. Marais, M. Henzinger, and M. Moricz.
1999. Analysis of a very large web search engine query
log. In ACM SIGIR Forum, pages 6?12. ACM New York,
NY, USA.
Suchanek, F., G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of WWW-07, pages 697?706,
Banff, Canada.
Tokunaga, K., J. Kazama, and K. Torisawa. 2005. Au-
tomatic discovery of attribute words from Web docu-
ments. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP-05),
pages 106?118, Jeju Island, Korea.
Wong, T.L. and W. Lam. 2009. An Unsupervised Method
for Joint Information Extraction and Feature Mining
Across Different Web Sites. Data & Knowledge Engi-
neering, 68(1):107?125.
Wu, F. and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of WWW-
08, pages 635?644, Beijing, China.
Wu, F., R. Hoffmann, and D. Weld. 2008. Information
extraction from Wikipedia: Moving down the long tail.
In Proceedings of KDD-08, pages 731?739, Las Vegas,
Nevada.
Yarowsky, David. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
ACL-95, pages 189?196. Association for Computational
Linguistics.
Yoshinaga, N. and K. Torisawa. 2007. Open-Domain
Attribute-Value Acquisition from Semi-Structured Texts.
In Proceedings of the Workshop on Ontolex, pages 55?66.
Zhao, Y. and G. Karypis. 2002. Criterion functions for docu-
ment clustering. Technical report, Experiments and Anal-
ysis University of Minnesota, Department of Computer
Science/Army HPC Research Center.
827
Coling 2010: Poster Volume, pages 955?962,
Beijing, August 2010
The Role of Queries in Ranking Labeled Instances Extracted from Text
Marius Pas?ca
Google Inc.
mars@google.com
Abstract
A weakly supervised method uses
anonymized search queries to induce a
ranking among class labels extracted from
unstructured text for various instances.
The accuracy of the extracted class labels
exceeds that of previous methods, over
evaluation sets of instances associated
with Web search queries.
1 Introduction
Classes pertaining to unrestricted domains (e.g.,
west african countries, science fiction films, slr
cameras) and their instances (cape verde, avatar,
canon eos 7d) play a disproportionately important
role in Web search. They occur prominently in
Web documents and among search queries sub-
mitted most frequently by Web users (Jansen et
al., 2000). They also serve as building blocks in
formal representation of human knowledge, and
are useful in a variety of text processing tasks.
Recent work on offline acquisition of fine-
grained, labeled classes of instances applies
manually-created (Banko et al, 2007; Talukdar et
al., 2008) or automatically-learned (Snow et al,
2006) extraction patterns to large document col-
lections. Although various methods exploit addi-
tional textual resources to increase accuracy (Van
Durme and Pas?ca, 2008) and coverage (Talukdar
et al, 2008), some of the extracted class labels
are inevitably less useful (works) or spurious (car
makers) for an associated instance (avatar). In
Web search, the relative ranking of documents re-
turned for a query directly affects the outcome of
the search. Similarly, the relative ranking among
class labels extracted for a given instance influ-
ences any applications using the labels.
Our paper proposes the use of features other
than those computed over the underlying doc-
ument collection, such as the frequency of co-
occurrence or diversity of extraction patterns pro-
ducing a given pair (Etzioni et al, 2005), to deter-
mine the relative ranking of various class labels,
given a class instance. Concretely, the method
takes advantage of the co-occurrence of a class
label and an instance within search queries from
anonymized query logs. It re-ranks lists of class
labels produced for an instance by standard ex-
traction patterns, to promote class labels that co-
occur with the instance. This corresponds to a soft
ranking approach, focusing on the ranking of can-
didate extractions such as the less relevant ones
are ranked lower, as opposed to removed when
deemed unreliable based on various clues.
By using queries in ranking, the ranked lists
of class labels available for various instances are
instrumental in determining the classes to which
given sets of instances belong. The accuracy of
the class labels exceeds that of previous work,
over evaluation sets of instances associated with
Web search queries. The results confirm the use-
fulness of the extracted IsA repository, which re-
mains general-purpose and is not tailored to any
particular task.
2 Instance Class Ranking
2.1 Extraction of Instances and Classes
The initial extraction of labeled instances relies
on hand-written patterns from (Hearst, 1992),
widely used in work on extracting hierarchies
from text (Snow et al, 2006; Ponzetto and Strube,
955
2007):
?[..] C [such as|including] I [and|,|.]?,
where I is a potential instance (e.g., diderot) and
C is a potential class label (e.g., writers).
Following (Van Durme and Pas?ca, 2008), the
boundaries of potential class labels C are approx-
imated from the part-of-speech tags of the sen-
tence words, whereas the boundaries of instances
I are identified by checking that I occurs as an
entire query in query logs. Since users type many
queries in lower case, the collected data is con-
verted to lower case.
When applied to inherently-noisy Web docu-
ments, the extraction patterns may produce irrele-
vant extractions (Kozareva et al, 2008). Causes of
errors include incorrect detection of possible enu-
merations, as in companies such as Procter and
Gamble (Downey et al, 2007); incorrect estima-
tion of the boundaries of class labels, due to in-
correct attachment as in years from on a limited
number of vehicles over the past few years, includ-
ing the Chevrolet Corvette; subjective (famous ac-
tors) (Hovy et al, 2009), relational (competitors,
nearby landmarks) and otherwise less useful (oth-
ers, topics) class labels; or questionable source
sentences, as in Large mammals such as deer and
wild turkeys can be [..] (Van Durme and Pas?ca,
2008).
As a solution, recent work uses additional evi-
dence, as a means to filter the pairs extracted by
patterns, thus trading off coverage for higher pre-
cision. The repository extracted from a similarly-
sized Web document collection using the same
initial extraction patterns as here, after a weighted
intersection of pairs extracted with patterns and
clusters of distributionally similar phrases, con-
tains a total of 9,080 class labels associated with
263,000 instances in (Van Durme and Pas?ca,
2008). Subsequent extensions of the repository,
using data derived from tables within Web doc-
uments, increase instance coverage and induce a
ranking among class labels of each instance, but
do not increase the number of class labels (Taluk-
dar et al, 2008). Due to aggressive filtering, the
resulting number of class labels is higher than the
often-small sets of entity types studied previously,
but may still be insufficient given the diversity of
Web search queries.
2.2 Ranking of Classes per Instance
As an alternative, the soft ranking approach pro-
posed here attempts to rank better class labels
higher, without necessarily removing class labels
deemed incorrect according to various criteria.
For each instance I , the associated class labels are
ranked in the following stages:
1) Apply the scoring formula below, resulting
in a ranked list of class labels L1(I):
Score(I, C) = Size({Pattern(I,C)})2 ? Freq(I, C)
Thus, a class label C is deemed more relevant
for an instance I if C is extracted by multiple ex-
traction patterns and its original frequency-based
score is higher.
2) For each term within any class label from
L1(I), compute a score equal to the frequency
sum of the term within anonymized queries con-
taining the instance I as a prefix, and the term
anywhere else in the queries. Each class label is
assigned the geometric mean of the scores of its
terms, after ignoring stop words. The class labels
are ranked according to the means, resulting in a
ranked list L2(I). In case of ties, L2(I) preserves
the relative ranking from L1(I). Thus, a class la-
bel is deemed more relevant if its individual terms
occur in popular queries containing the instance.
3) Compute a merged ranked list of class labels
out of the ranked lists L1(I) and L2(I), by sorting
the class labels in decreasing order of the inverse
of the average rank, computed with the following
formula:
MergedScore(C) = 2Rank(C, L1) + Rank(C, L2)
where 2 is the number of input lists of class la-
bels, and Rank(C, Li) is the rank of C in the list
Li of class labels computed for the correspond-
ing input instance. The rank is set to 1000, if C
is not present in the list Li. By using only the
relative ranks of the class labels within the input
lists, and not on their scores, the outcome of the
merging is less sensitive to how class labels of a
given instance are scored within the IsA reposi-
tory. In case of ties, the scores of the class labels
from L1(I) serve as a secondary ranking criterion.
Note that the third stage is introduced because
relying on query logs to estimate the relevance of
956
class labels exposes the ranking method to signifi-
cant noise. On one hand, arguably useful class la-
bels (e.g., authors) may not occur in queries along
with the respective instances (diderot). On the
other hand, for each query containing an instance
and (part of) useful class labels, there are many
other queries containing, e.g., attributes (diderot
biography or diderot beliefs) or the name of a
book in the query diderot the nun. Therefore, the
ranked lists L2(I) may be too noisy to be used di-
rectly as rankings of the class labels for I .
3 Experimental Setting
3.1 Textual Data Sources
The acquisition of the IsA repository relies on un-
structured text available within Web documents
and search queries. The collection of queries is
a sample of 50 million unique, fully-anonymized
queries in English submitted by Web users in
2009. Each query is accompanied by its frequency
of occurrence in the logs. The document col-
lection consists of a sample of 100 million doc-
uments in English. The textual portion of the
documents is cleaned of HTML, tokenized, split
into sentences and part-of-speech tagged using the
TnT tagger (Brants, 2000).
3.2 Experimental Runs
The experimental runs correspond to different
methods for extracting and ranking pairs of an in-
stance and a class:
? as available in the repository from (Talukdar
et al, 2008), which is collected from a docu-
ment collection similar in size to the one used
here plus a collection of Web tables, in a run
denoted Rg;
? from the repository extracted here, with class
labels of an instance ranked based on the fre-
quency and the number of extraction patterns
(see Score(I, C) in Section 2), in run Rs;
? from the repository extracted here, with class
labels of an instance ranked based on the
MergedScore from Section 2, in run Ru.
3.3 Evaluation Procedure
The manual evaluation of open-domain informa-
tion extraction output is time consuming (Banko
et al, 2007). Fortunately, it is possible to im-
plement an automatic evaluation procedure for
ranked lists of class labels, based on existing re-
sources and systems. Assume that a gold stan-
dard is available, containing gold class labels that
are each associated with a gold set of their in-
stances. The creation of such gold standards is
discussed later. Based on the gold standard, the
ranked lists of class labels available within an IsA
repository can be automatically evaluated as fol-
lows. First, for each gold label, the ranked lists
of class labels of individual gold instances are re-
trieved from the IsA repository. Second, the in-
dividual retrieved lists are merged into a ranked
list of class labels, associated with the gold label.
The merged list is computed using an extension
of the MergedScore formula described earlier
in Section 2. Third, the merged list is compared
against the gold label, to estimate the accuracy of
the merged list. Intuitively, a ranked list of class
labels is a better approximation of a gold label, if
class labels situated at better ranks in the list are
closer in meaning to the gold label.
3.4 Evaluation Metric
Given a gold label and a list of class labels, if any,
derived from the IsA repository, the rank of the
highest class label that matches the gold label de-
termines the score assigned to the gold label, in
the form of the reciprocal rank, max(1/rankmatch).
Thus, if the gold label matches a class label at rank
1, 2, 3, 4 or 5 in the computed list, the gold label
receives a score of 1, 0.5, 0.33, 0.25 or 0.2 respec-
tively. The score is 0 if the gold label does not
match any of the top 20 class labels. The overall
score over the entire set of gold labels is the mean
reciprocal rank (MRR) score over all gold labels
from the set. Two types of MRR scores are auto-
matically computed:
? MRRf considers a gold label and a class la-
bel to match if they are identical;
? MRRp considers a gold label and a class la-
bel to match if one or more of their tokens
that are not stop words are identical.
957
During matching, all string comparisons are
case-insensitive, and all tokens are first converted
to their singular form (e.g., european countries
to european country) when available, by using
WordNet?s morphological routines. Thus, insur-
ance carriers and insurance companies are con-
sidered to not match in MRRf scores, but match
in MRRp scores, whereas insurance companies
and insurance company match in both MRRf and
MRRp scores. Note that both MRRf and MRRp
scores fail to give any credit to arguably valid
and useful class labels, such as insurers for the
gold label insurance carriers, or asian nations
for the gold label asia countries. On the other
hand, MRRp scores may give credit to less rele-
vant class labels, such as insurance policies for the
gold label insurance carriers. Therefore, MRRp
is an approximate, and MRRf is a conservative,
lower-bound estimate of the actual usefulness of
the computed ranked lists of class labels as ap-
proximations of the semantics of the gold labels.
4 Evaluation Results
4.1 Evaluation Sets of Queries
A random sample of anonymized, class-seeking
queries (e.g., video game characters or smart-
phone) submitted by Web users to Google
Squared 1 over a 30-day interval is filtered, to re-
move queries for which Google Squared returns
fewer than 10 instances at the time of the evalua-
tion. The resulting evaluation set of queries, de-
noted Qe, contains 807 queries, each associated
with a ranked list of between 10 and 100 instances
automatically extracted by Google Squared.
Since the instances available as input for each
query as part of Qe are automatically extracted,
they may (e.g., acorn a7000) or may not (e.g.,
konrad zuse) be true instances of the respective
queries (e.g., computers). A second evaluation
set Qm is assembled as a subset of 40 queries
from Qe, such that the instances available for each
query in Qm are correct. For this purpose, each
instance returned by Google Squared for the 40
1Google Squared (http://www.google.com/squared) is a
Web search tool taking as input class-seeking queries (e.g.,
insurance companies) and returning lists of instances (e.g.,
allstate, state farm insurance), along with attributes (e.g., in-
dustry, headquarters) and values for each instance.
Query Set: Sample of Queries
Qe (807 queries): 2009 movies, amino acids,
asian countries, bank, board games, buildings,
capitals, chemical functional groups, clothes,
computer language, dairy farms near modesto
ca, disease, egyptian pharaohs, eu countries,
french presidents, german islands, hawaiian is-
lands, illegal drugs, irc clients, lakes, mac-
intosh models, mobile operator india, nba
players, nobel prize winners, orchids, photo
editors, programming languages, renaissance
artists, roller costers, science fiction tv series,
slr cameras, soul singers, states of india, tal-
iban members, thomas edison inventions, u.s.
presidents, us president, water slides
Qm (40 queries): actors, airlines, birds, cars,
celebrities, computer languages, digital cam-
era, dog breeds, drugs, endangered animals,
european countries, fruits, greek gods, hor-
ror movies, ipods, names, netbooks, operat-
ing systems, park slope restaurants, presidents,
ps3 games, religions, renaissance artists, rock
bands, universities, university, vitamins
Table 1: Size and composition of evaluation sets
of queries associated with non-filtered (Qe) or
manually-filtered (Qm) instances
queries from Qm is reviewed by at least three hu-
man annotators. Instances deemed highly rele-
vant (out of 5 possible grades) with high inter-
annotator agreement are retained. As a result, the
40 queries from Qm are associated with between
8 and 33 human-validated instances.
Table 1 shows a sample of the queries from Qe
and queries from Qm. A small number of queries
are slight lexical variations of one another, such as
u.s. presidents and us presidents in Qe, or univer-
sities and university in Qm. In general, however,
the sets cover a wide range of domains of inter-
est, including entertainment for 2009 movies and
rock bands; biology for endangered animals and
amino acids; geography for asian countries and
hawaiian islands; food for fruits; history for egyp-
tian pharaohs and greek gods; health for drugs
and vitamins; and technology for photo editors
and ipods. Some of the queries from Table 1
are specific enough that computing them exactly,
958
Accuracy
IQ 3 5 10 15
CI 5 10 20 5 10 20 5 10 20 5 10 20
MRRf computed over Qe:
Rg 0.106 0.112 0.112 0.121 0.122 0.123 0.131 0.135 0.127 0.134 0.132 0.127
Rs 0.186 0.195 0.198 0.198 0.207 0.210 0.204 0.214 0.218 0.206 0.216 0.221
Ru 0.202 0.211 0.216 0.232 0.238 0.244 0.245 0.255 0.257 0.245 0.252 0.254
MRRp computed over Qe:
Rg 0.390 0.399 0.394 0.420 0.420 0.413 0.443 0.443 0.435 0.439 0.431 0.425
Rs 0.489 0.495 0.495 0.517 0.528 0.529 0.541 0.553 0.557 0.551 0.557 0.557
Ru 0.520 0.531 0.533 0.564 0.573 0.578 0.590 0.601 0.602 0.598 0.603 0.601
MRRf computed over Qm:
Rg 0.284 0.289 0.295 0.305 0.327 0.322 0.320 0.335 0.335 0.334 0.328 0.337
Rs 0.406 0.436 0.442 0.431 0.447 0.466 0.467 0.470 0.501 0.484 0.501 0.554
Ru 0.423 0.426 0.429 0.436 0.483 0.508 0.500 0.526 0.530 0.520 0.540 0.524
MRRp computed over Qm:
Rg 0.507 0.517 0.531 0.495 0.509 0.518 0.555 0.553 0.550 0.563 0.561 0.572
Rs 0.667 0.662 0.660 0.675 0.677 0.699 0.702 0.695 0.716 0.756 0.765 0.787
Ru 0.711 0.703 0.680 0.734 0.731 0.748 0.733 0.797 0.782 0.799 0.834 0.819
Table 2: Accuracy of instance set labeling, as full-match (MRRf ) or partial-match (MRRp) scores over
the evaluation sets of queries associated with non-filtered instances (Qe) or manually-filtered instances
(Qm), for various experimental runs (IQ=number of instances available in the input evaluation sets that
are used for retrieving class labels; CI=number of class labels retrieved from IsA repository per input
instance)
even from a comprehensive, perfect list of ex-
tracted instance, would be very difficult whether
done automatically or manually. Examples of
such queries are dairy farms near modesto ca and
science fiction tv series, but also mobile opera-
tor india (phrase expressed as keywords) in Qe, or
park slope restaurants (specific location) in Qm.
Access to a system such as Google Squared is
useful, but not necessary to conduct the evalua-
tion. Given other sets of queries, it is straightfor-
ward, albeit time consuming, to create evaluation
sets similar to Qm, by manually compiling correct
instances, for each selected query or concept.
Following the general evaluation procedure,
each query from the sets Qe and Qm acts as a gold
class label associated with its set of instances.
Given a query and its instances I from the evalu-
ation sets Qe or Qm, we compute merged, ranked
lists of class labels, by merging the ranked lists of
class labels available in the underlying IsA reposi-
tory for each instance I . The evaluation compares
the merged lists of class labels, on one hand, and
the corresponding queries from Qe or Qm, on the
other hand.
4.2 Accuracy of Class Labels
Table 2 summarizes results from comparative ex-
periments, quantifying a) horizontally, the impact
of alternative parameter settings on the computed
lists of class labels; and b) vertically, the compar-
ative accuracy of the experimental runs over the
query sets. The experimental parameters are the
number of input instances from the evaluation sets
that are used for retrieving class labels, IQ, set to
3, 5, 10 and 15; and the number of class labels
retrieved per input instance, CI , set to 5, 10 and
20.
The scores over Qm are higher than those
over Qe, confirming the intuition that the higher-
quality input set of instances available in Qm rel-
ative to Qe should lead to higher-quality class la-
bels for the corresponding queries. When IQ is
fixed, increasing CI leads to small, if any, score
improvements. Conversely, when CI is fixed,
959
even small values of IQ, such as 3 or 5 (that is,
very small sets of instances provided as input) pro-
duce scores that are competitive with those ob-
tained with a higher value like. This suggests that
useful class labels can be generated even in ex-
treme scenarios, where the number of instances
available as input is as small as 3 or 5.
For most combinations of parameter settings
and on both query sets, run Ru produces the high-
est scores. In particular, when IQ is set to 10 and
CI to 20, run Ru identifies the original query as
an exact match among the top four class labels
returned; and as a partial match among the top
two class labels returned, as an average over the
Qe set. In this case, the original query is iden-
tified at ranks 1, 2, 3, 4 and 5 for 16.8%, 8.7%,
6.1%, 3.7% and 1.7% of the queries, as an ex-
act match; and for 48.8%, 14.2%, 6.1%, 3.6% and
1.9% respectively, as a partial match. The corre-
sponding MRRf score of 0.257 over the Qe set
obtained with run Ru is higher than with run Rs,
and much higher than with run Rg. In all experi-
ments, the higher scores of Ru can be attributed to
higher coverage of class labels, relative to Rg; and
higher-quality lists of class labels, relative to Rs
but also to Rg, despite the fact that Rg combines
high-precision seed data with using both unstruc-
tured and structured text as sources of class labels
(cf. (Talukdar et al, 2008)). Among combinations
of parameter settings described in Table 2, values
around 15 for IQ and 20 for CI give the highest
scores over both Qe and Qm.
5 Related Work
5.1 Extraction of IsA Repositories
Knowledge including instances and classes can be
manually compiled by experts (Fellbaum, 1998)
or collaboratively by non-experts (Singh et al,
2002). Alternatively, classes of instances acquired
automatically from text are potentially less ex-
pensive to acquire, maintain and grow, and their
coverage and scope are theoretically bound only
by the size of the underlying data source. Ex-
isting methods for extracting classes of instances
acquire sets of instances that are each either un-
labeled (Wang and Cohen, 2008; Pennacchiotti
and Pantel, 2009; Lin and Wu, 2009), or as-
sociated with a class label (Pantel and Pennac-
chiotti, 2006; Banko et al, 2007; Wang and Co-
hen, 2009). When associated with a class la-
bel, the sets of instances may be organized as
flat sets or hierarchically, relative to existing hi-
erarchies such as WordNet (Snow et al, 2006) or
the category network within Wikipedia (Wu and
Weld, 2008; Ponzetto and Navigli, 2009). Semi-
structured text was shown to be a complemen-
tary resource to unstructured text, for the purpose
of extracting relations from Web documents (Ca-
farella et al, 2008).
The role of anonymized query logs in Web-
based information extraction has been explored
in the tasks of class attribute extraction (Pas?ca
and Van Durme, 2007) and instance set ex-
pansion (Pennacchiotti and Pantel, 2009). Our
method illustrates the usefulness of queries con-
sidered in isolation from one another, in ranking
class labels in extracted IsA repositories.
5.2 Labeling of Instance Sets
Previous work on generating relevant labels, given
sets or clusters of items, focuses on scenarios
where the items within the clusters are descrip-
tions of, or full-length documents within docu-
ment collections. The documents are available as
a flat set (Cutting et al, 1993; Carmel et al, 2009)
or are hierarchically organized (Treeratpituk and
Callan, 2006). Relying on semi-structured con-
tent assembled manually as part of the struc-
ture of Wikipedia articles, such as article titles
or categories, the method introduced in (Carmel
et al, 2009) derives labels for clusters contain-
ing 100 full-length documents each. In contrast,
our method relies on IsA relations automatically
extracted from unstructured text within arbitrary
Web documents, and computes labels given tex-
tual input that is orders of magnitude smaller, i.e.,
around 10 phrases (instances). The experiments
described in (Carmel et al, 2009) assign labels to
one of 20 sets of newsgroup documents from a
standard benchmark. Each set of documents is as-
sociated with a higher-level, coarse-grained label
used as a gold label against which the generated
labels are compared. In comparison, our experi-
ments compute text-derived class labels for finer-
grained, often highly-specific gold labels.
960
Reducing the granularity of the items to be la-
beled from full documents to condensed docu-
ment descriptions, (Geraci et al, 2006) submits
arbitrary search queries to external Web search en-
gines. It organizes the top 200 returned Web doc-
uments into clusters, by analyzing the text snip-
pets associated with each document in the output
from the search engines. Any words and phrases
from the snippets may be selected as labels for the
clusters, which in general leads to labels that are
not intended to capture any classes that may be as-
sociated to the query. For example, labels of clus-
ters generated in (Geraci et al, 2006) include arm-
strong ceilings, italia, armstrong sul sito and louis
jazz for the query armstrong; and madonnaweb,
music, madonna online and madonna itself for the
query madonna. The amount of text available as
input for the purpose of labeling is at least two or-
ders of magnitude larger than in our method, and
the task of selecting any phrases as labels, as op-
posed to selecting only labels that correspond to
classes, is more relaxed and likely easier.
Another approach specifically addresses the
problem of generating labels for sets of instances,
where the labels are extracted from unstructured
text. In (Pantel and Ravichandran, 2004), given a
collection of news articles that is both cleaner and
smaller than Web document collections, a syn-
tactic parser is applied to document sentences in
order to identify and exploit syntactic dependen-
cies for the purpose of selecting candidate class
labels. Such methods are comparatively less ap-
plicable to Web document collections, due to scal-
ability issues associated with parsing a large set
of Web documents of variable quality. Moreover,
the class labels generated in (Pantel and Ravichan-
dran, 2004) tend to be rather coarse-grained. For
example, the top labels generated for a set of Chi-
nese universities (qinghua university, fudan uni-
versity, beijing university) are university, institu-
tion, stock-holder, college and school.
6 Conclusion
The method presented in this paper produces an
IsA repository whose class labels have higher
coverage and accuracy than with recent meth-
ods operating on document collections. This is
done by injecting useful ranking signals from
inherently-noisy queries, rather than making bi-
nary, coverage-reducing quality decisions on the
extracted data. Current work investigates the use-
fulness of the extracted class labels in the gener-
ation of flat or hierarchical query refinements for
class-seeking queries.
Acknowledgments
The author thanks Randolph Brown for assistance
in assembling the evaluation sets of class-seeking
queries.
References
Banko, M., Michael J Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007. Open infor-
mation extraction from the Web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670?2676, Hyder-
abad, India.
Brants, T. 2000. TnT - a statistical part of speech
tagger. In Proceedings of the 6th Conference on
Applied Natural Language Processing (ANLP-00),
pages 224?231, Seattle, Washington.
Cafarella, M., A. Halevy, D. Wang, E. Wu, and
Y. Zhang. 2008. WebTables: Exploring the power
of tables on the Web. In Proceedings of the 34th
Conference on Very Large Data Bases (VLDB-08),
pages 538?549, Auckland, New Zealand.
Carmel, D., H. Roitman, and N. Zwerding. 2009. En-
hancing cluster labeling using Wikipedia. In Pro-
ceedings of the 32nd ACM Conference on Research
and Development in Information Retrieval (SIGIR-
09), pages 139?146, Boston, Massachusetts.
Cutting, D., D. Karger, and J. Pedersen. 1993.
Constant interaction-time scatter/gather browsing of
very large document collections. In Proceedings of
the 16th ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR-93), pages
126?134, Pittsburgh, Pennsylvania.
Downey, D., M. Broadhead, and O. Etzioni. 2007. Lo-
cating complex named entities in Web text. In Pro-
ceedings of the 20th International Joint Conference
on Artificial Intelligence (IJCAI-07), pages 2733?
2739, Hyderabad, India.
Etzioni, O., M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the Web: an experimental study. Artificial Intelli-
gence, 165(1):91?134.
Fellbaum, C., editor. 1998. WordNet: An Elec-
tronic Lexical Database and Some of its Applica-
tions. MIT Press.
961
Geraci, F., M. Pellegrini, M. Maggini, and F. Sebas-
tiani. 2006. Cluster generation and cluster la-
belling for Web snippets: A fast and accurate hi-
erarchical solution. In Proceedings of the 13th Con-
ference on String Processing and Information Re-
trieval (SPIRE-06), pages 25?36, Glasgow, Scot-
land.
Hearst, M. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the
14th International Conference on Computational
Linguistics (COLING-92), pages 539?545, Nantes,
France.
Hovy, E., Z. Kozareva, and E. Riloff. 2009. Toward
completeness in concept extraction and classifica-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-09), pages 948?957, Singapore.
Jansen, B., A. Spink, and T. Saracevic. 2000. Real
life, real users, and real needs: a study and analysis
of user queries on the Web. Information Processing
and Management, 36(2):207?227.
Kozareva, Z., E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL-08), pages 1048?1056, Columbus,
Ohio.
Lin, D. and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-IJCNLP-09), pages 1030?
1038, Singapore.
Pas?ca, M. and B. Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from
query logs. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-
07), pages 2832?2837, Hyderabad, India.
Pantel, P. and M. Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-
06), pages 113?120, Sydney, Australia.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 2004 Human Language Technology Conference
(HLT-NAACL-04), pages 321?328, Boston, Mas-
sachusetts.
Pennacchiotti, M. and P. Pantel. 2009. Entity extrac-
tion via ensemble semantics. In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-09), pages 238?
247, Singapore.
Ponzetto, S. and R. Navigli. 2009. Large-scale tax-
onomy mapping for restructuring and integrating
Wikipedia. In Proceedings of the 21st International
Joint Conference on Artificial Intelligence (IJCAI-
09), pages 2083?2088, Pasadena, California.
Ponzetto, S. and M. Strube. 2007. Deriving a large
scale taxonomy from Wikipedia. In Proceedings
of the 22nd National Conference on Artificial In-
telligence (AAAI-07), pages 1440?1447, Vancouver,
British Columbia.
Singh, P., T. Lin, E. Mueller, G. Lim, T. Perkins,
and W. Zhu. 2002. Open Mind Common Sense:
Knowledge acquisition from the general public. In
Proceedings of the ODBASE Conference (ODBASE-
02), pages 1223?1237.
Snow, R., D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), pages 801?808, Sydney, Aus-
tralia.
Talukdar, P., J. Reisinger, M. Pas?ca, D. Ravichan-
dran, R. Bhagat, and F. Pereira. 2008. Weakly-
supervised acquisition of labeled class instances us-
ing graph random walks. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-08), pages 582?
590, Honolulu, Hawaii.
Treeratpituk, P. and J. Callan. 2006. Automatically la-
beling hierarchical clusters. In Proceedings of the
7th Annual Conference on Digital Government Re-
search (DGO-06), pages 167?176, San Diego, Cali-
fornia.
Van Durme, B. and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of
labeled instances for open-domain information ex-
traction. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI-08), pages
1243?1248, Chicago, Illinois.
Wang, R. and W. Cohen. 2008. Iterative set expan-
sion of named entities using the web. In Proceed-
ings of the International Conference on Data Min-
ing (ICDM-08), pages 1091?1096, Pisa, Italy.
Wang, R. and W. Cohen. 2009. Automatic set instance
extraction using the Web. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-IJCNLP-09), pages 441?
449, Singapore.
Wu, F. and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the
17th World Wide Web Conference (WWW-08), pages
635?644, Beijing, China.
962
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 403?414,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Open-Domain Fine-Grained Class Extraction from Web Search Queries
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
This paper introduces a method for extract-
ing fine-grained class labels (?countries with
double taxation agreements with india?) from
Web search queries. The class labels are more
numerous and more diverse than those pro-
duced by current extraction methods. Also
extracted are representative sets of instances
(singapore, united kingdom) for the class la-
bels.
1 Introduction
Motivation: As more semantic constraints are
added, concepts like companies become more spe-
cific, e.g., companies that are in the software busi-
ness, and have been started in a garage. The
sets of instances associated with the classes become
smaller; the class labels used to concisely describe
the meaning of more specific concepts tend to be-
come longer. In fact, fine-grained class labels such
as ?software companies started in a garage? are of-
ten complex noun phrases, since they must somehow
summarize multiple semantic constraints. Although
Web users are interested in both coarse (e.g., ?com-
panies?) and fine-grained (e.g., ?software compa-
nies started in a garage?) class labels, virtually all
class labels acquired from text by previous extrac-
tion methods (Etzioni et al, 2005; Van Durme and
Pas?ca, 2008; Kozareva and Hovy, 2010; Snow et
al., 2006) exhibit little syntactic diversity. Indeed,
instances and class labels that are relatively com-
plex nouns are known to be difficult to detect and
pick out precisely from surrounding text (Downey
et al, 2007). This and other challenges associated
with large-scale extraction from Web text (Etzioni
et al, 2011) cause the extracted class labels to usu-
ally follow a rigid modifiers-plus-nouns format. The
format covers nouns (?companies?) possibly pre-
ceded by one or many modifiers (?software com-
panies?, ?computer security software companies?).
Examples of actual extractions include ?european
cities? (Etzioni et al, 2005), ?strong acids? (Pan-
tel and Pennacchiotti, 2006), ?prestigious private
schools? (Van Durme and Pas?ca, 2008), ?aquatic
birds? (Kozareva and Hovy, 2010).
As an alternative to extracting class labels from
text, some methods simply import them from
human-curated resources, for example from the set
of categories encoded in Wikipedia (Remy, 2002).
As a result, class labels potentially exhibit higher
syntactic diversity. The modifiers-plus-nouns for-
mat (?computer security software companies?) is
usually still the norm. But other formats are possi-
ble: ?software companies based in london?, ?soft-
ware companies of the united kingdom?. Vocab-
ulary coverage gaps remain a problem, with many
relevant class labels (?software companies of texas?
?software companies started in a garage?, ?soft-
ware companies that give sap training?) still miss-
ing. There is a need for methods that more ag-
gressively identify fine-grained class labels, beyond
those extracted by previous methods or encoded in
existing, manually-created resources. Such class la-
bels increase coverage, for example in scenarios that
enrich Web search results with instances available
for the class labels specified in the queries.
Contributions: The contributions of this paper are
twofold. First, it proposes a weakly-supervised
403
method to assemble a large vocabulary of class la-
bels from queries. The class labels include fine-
grained class labels (?countries with double taxa-
tion agreements with india?, ?no front license plate
states?) that are difficult to extract from text by
previous methods for open-domain information ex-
traction. Second, the method acquires representa-
tive instances (singapore, united kingdom; arizona,
new mexico) that belong to fine-grained class labels
(?countries with double taxation agreements with
india?, ?no front license plate states?). Both class
labels and their instances are extracted from Web
search queries.
2 Extraction from Queries
2.1 Extraction of Class Labels
Overview: Given a set of arbitrary Web search
queries as input, our method produces a vocabulary
of fine-grained class labels. For this purpose, it: a)
selects an initial vocabulary of class labels, as a sub-
set of input queries that are likely to correspond to
search requests for classes; b) expands the vocabu-
lary, by generating a large, noisy set of other pos-
sible class labels, through replacements of ngrams
within initial class labels with their similar phrases;
c) restricts the generated class labels to those that
match the syntactic structure of class labels within
the initial vocabulary; and d) further restricts the
generated class labels to those that appear within the
larger set of arbitrary Web search queries.
Initial Vocabulary of Class Labels: Out of a set
of arbitrary search queries available as input, the
queries in the format ?list of ..? are selected as the
initial vocabulary of class labels. The prefix ?list
of? is discarded from each query. Thus, the query
?list of software companies that use linux? gives the
class label ?software companies that use linux?.
Generation via Phrase Similarities: As a prerequi-
site to generating class labels, distributionally simi-
lar phrases (Lin and Pantel, 2002; Lin and Wu, 2009;
Pantel et al, 2009) and their scores are collected in
advance. A phrase is represented as a vector of its
contextual features. A feature is a word, collected
from windows of three words centered around the
occurrences of the phrase in sentences across Web
documents (Lin and Wu, 2009). In the contextual
vector of a phrase, the weight of a feature is the
pointwise-mutual information (Lin and Wu, 2009)
between the phrase P and the feature F . The dis-
tributional similarity score between two phrases is
the cosine similarity between the contextual vectors
of the two phrases. The lists of most distribution-
ally similar phrases of a phrase P are thus compiled
offline, by ranking the similar phrases of P in de-
creasing order of their similarity score relative to P .
Each class label from the initial vocabulary is ex-
panded into a set of generated, candidate class la-
bels. To this effect, every ngram P within a given
class label is replaced with each of the distribution-
ally similar phrases, if any, available for the ngram.
As shown later in the experimental section, the ex-
pansion can increase the vocabulary by a factor of
100.
Approximate Syntactic Filtering: The set of gen-
erated class labels is noisy. The set is filtered, by
retaining only class labels whose syntactic structure
matches the syntactic structure of some class label(s)
from the initial vocabulary. The syntactic structure
is loosely approximated at surface rather than syn-
tactic level. A generated class label is retained, if
its sequence of part of speech tags matches the se-
quence of part of speech tags of one of the class la-
bels from the initial vocabulary. As an additional
constraint, the sequence must contain one tag cor-
responding to a common noun in plural form, i.e.,
NNS. Otherwise, the class label is discarded.
Query Filtering: Generated class labels that pass
previous filters are further restricted. They are inter-
sected with the set of arbitrary Web search queries
available as input. Generated class labels that are
not full queries are discarded.
2.2 Extraction of Instances
Overview: Our method mines instances of fine-
grained class labels from queries. In a nutshell, it
identifies queries containing two types of informa-
tion simultaneously. First, the queries contain an in-
stance (marvin gaye) of the more general class labels
(?musicians?) from which the fine-grained class la-
bels (?musicians who have been shot?) can be ob-
tained. Second, the queries contain the constraints
added by the fine-grained class labels (?... shot?) on
top of the more general class labels.
Instances of General Class Labels: Follow-
ing (Ponzetto and Strube, 2007), the Wikipedia cate-
gory network is refined into a hierarchy that discards
404
non-IsA (thematic) edges, and retains only IsA (sub-
sumption) edges from the network (Ponzetto and
Strube, 2007). Instances, i.e., titles of Wikipedia
articles, are propagated upwards to all their ances-
tor categories. The class label ?musicians? would
be mapped into madonna, marvin gaye, jon bon jovi
etc. The mappings from each ancestor category, to
all its descendant instances in the Wikipedia hierar-
chy, represent our mappings from more general class
labels to instances.
Decomposition of Fine-Grained Class Labels: A
fine-grained class label (e.g., ?musicians who have
been shot?) is effectively decomposed into pairs of
two pieces of information. The first piece is a more
general class label (?musicians?), if any occurs in
it. The second piece is a bag of words, collected
from the remainder of the fine-grained class label
after discarding stop words. Note that the standard
set of stop words is augmented with auxiliary verbs
(e.g., does, has, is, would), determiners, conjunc-
tions, prepositions, and question wh-words (Radev
et al, 2005) (e.g., where, how). In the first piece
of each pair, the general class label is then replaced
with each of its instances. This produces multiple
pairs of a candidate instance and a bag of words, for
each fine-grained class label. As an illustration, the
class labels ?musicians who have been shot? and
?automobiles with remote start? are decomposed
into pairs like <madonna, {shot}>, <marvin gaye,
{shot}>; and <buick lacrosse, {remote, start}>,
<nissan versa, {remote, start}>, respectively.
Matching of Candidate Instances: A decomposed
class label is retained, if there are matching queries
that contain the candidate instance, the bag of words,
and optionally stop words. Otherwise, the decom-
posed class label is discarded. The word matching is
performed after word stemming (Porter, 1980). The
aggregated frequency of the matching queries is as-
signed as the score of the candidate instance for the
fine-grained class label:
Score(I, C) =
?
Q
(Freq(Q)|Match(Q,< I,C >)) (1)
For example, the score of the candidate instance
marvin gaye for the class label ?musicians who have
been shot?, is the sum of the frequencies of the
matching queries ?marvin gaye is shot?, ?when was
marvin gaye shot?, ?why marvin gaye was shot?
etc. Similarly, the score of buick lacrosse for ?au-
tomobiles with remote start? is given by the aggre-
gated frequencies of the queries ?buick lacrosse re-
mote start?, ?how to remote start buick lacrosse?,
?remote start for buick lacrosse?. Candidate in-
stances of a class label are ranked in decreasing or-
der of their scores.
3 Experimental Setting
Web Textual Data: The experiments rely on a sam-
ple of 1 billion queries in English submitted by users
of a Web search engine. Each query is accompa-
nied by its frequency of occurrence. Also available
is a sample of around 200 million Web documents
in English.
Phrase Similarities: Web documents are used in
the experiments only to construct a phrase similar-
ity repository following (Lin and Wu, 2009; Pantel
et al, 2009). The repository contains ranked lists
of the top 1000 phrases, computed to be the most
distributionally similar to each of around 16 million
phrases.
Text Pre-Processing: The TnT tagger (Brants,
2000) assigns part of speech tags to words in class
labels.
Instances: To collect mappings from Wikipedia cat-
egories (as more general class labels) to titles of de-
scendant Wikipedia articles (as instances), a snap-
shot of Wikipedia articles was intersected with the
Wikipedia category hierarchy from (Ponzetto and
Strube, 2007). The mappings connect a total of
1,535,083 instances to a total of 108,756 class la-
bels.
4 Evaluation of Class Labels
4.1 Evaluation Procedure
Experimental Runs: Human-compiled information
available within Wikipedia serves as the source of
data for two baseline runs. The set of all categories,
listed in Wikipedia for any of its articles, corre-
sponds to the set of class labels ?acquired? in run
Rwc. Categories used for internal Wikipedia book-
keeping (Ponzetto and Strube, 2007) are discarded.
Their names contain one of the words article(s), cat-
egory(ies), indices, pages, redirects, stubs, or tem-
plates. Similarly, the titles of Wikipedia articles with
the prefix ?List of ..? (e.g., ?List of automobile man-
ufacturers of Germany?) form the set of class labels
405
?acquired? in run Rwl. The prefix ?List of? is dis-
carded.
For completeness, a third baseline run, Rdc, cor-
responds to class labels extracted from Web docu-
ments. The class labels are noun phrases C that fill
extraction patterns equivalent to ?C such as I?. The
patterns are matched to document sentences. The
boundaries of the class labels C are approximated
from part of speech tags of sentence words (Van
Durme and Pas?ca, 2008). The patterns were pro-
posed in (Hearst, 1992). They were employed
widely in subsequent methods (Etzioni et al, 2005;
Kozareva et al, 2008; Wu et al, 2012), which ex-
tract class labels precisely from the set of class la-
bels C produced by the extraction patterns. Even
methods using queries as a textual data source still
extract class labels from documents using the same
extraction patterns (Pas?ca, 2010). Therefore, from
the point of view of evaluating class labels, run Rdc
is a valid representative of previous extraction meth-
ods, including (Etzioni et al, 2005; Kozareva et al,
2008; Van Durme and Pas?ca, 2008; Pas?ca, 2010; Wu
et al, 2012).
Besides the baseline runs, three experimental runs
are considered. In run Rql, the queries starting with
the prefix ?list of? form the set of class labels. The
prefix ?list of? is discarded from each query. In run
Rqg, the class labels are generated via phrase sim-
ilarities, starting from Rql as an initial set of class
labels. Run Rqa represents an ablation experiment.
It is created from Rqg, by limiting the expansion of
a given class label via distributional similarities to
only one, rather than multiple, phrases within the
class label. Note that, by design, none of the class
labels that appear in Rql also appear in runs Rqa or
Rqg. Therefore, the intersection between Rql, on one
hand, and Rqa and Rqg, on the other hand, is the
empty set.
All data, including the class labels extracted in all
experimental runs, is converted to lower case.
4.2 Relative Coverage of Class Labels
Coverage Over Entire Sets: Table 1 illustrates the
overall coverage of the various experimental runs.
The table takes all class labels into account, relative
to the Wikipedia-based runs as reference sets: Rwc
(Wikipedia categories), in the upper part of the table;
and Rwl (Wikipedia List-Of categories), in the lower
Counts Cvg
A B |A| |B| |A?B| |A?B||A|
vs. Wikipedia categories:
Rwc Rdc 295,587 2,884,390 15,011 0.051
Rql 295,587 1,649,261 21,979 0.074
Rqa 295,587 33,073,741 33,502 0.113
Rqg 295,587 134,235,151 43,935 0.148
Rql?Rqg 295,587 135,884,412 65,914 0.222
vs. Wikipedia categories that are queries:
Rwc?Q Rdc 126,318 2,884,390 14,840 0.117
Rql 126,318 1,649,261 21,979 0.173
Rqa 126,318 33,073,741 33,502 0.265
Rqg 126,318 134,235,151 43,935 0.347
Rql?Rqg 126,318 135,884,412 65,914 0.521
vs. Wikipedia List-Of categories:
Rwl Rdc 134,840 2,884,390 8,099 0.060
Rql 134,840 1,649,261 26,446 0.196
Rqa 134,840 33,073,741 16,204 0.120
Rqg 134,840 134,235,151 20,021 0.148
Rql?Rqg 134,840 135,884,412 46,467 0.344
vs. Wikipedia List-Of categories that are queries:
Rwl?Q Rdc 47,442 2,884,390 7,985 0.168
Rql 47,442 1,649,261 24,821 0.523
Rqa 47,442 33,073,741 16,204 0.341
Rqg 47,442 134,235,151 20,021 0.422
Rql?Rqg 47,442 135,884,412 44,842 0.945
Table 1: Coverage of class labels extracted by various
experimental runs, relative to class labels available in
Wikipedia before and after intersecting them with a large
set of arbitrary queries (A = reference set, relative to
which coverage is computed; B = measured set, for which
coverage is computed relative to the reference set; |A| =
size of set A; Q = set of input queries)
part of the table. Note that the number of class labels
extracted by the individual run shown in the second
column (B) is shown in the fourth column (|B|). In
particular, there are around 1.6 million unique ?list
of ..? queries, from which class labels are collected
in run Rql.
During the computation of coverage, the refer-
ence set, and the set for which coverage is being
computed, are intersected. Intersection relies on
strict string matching. All words, including punc-
tuation, must match exactly in order for a class la-
bel to be part of the intersection. The reference
sets are intersected with the set of all Web search
queries Q used in the experiments. Coverage is com-
puted both before and after intersection. Less than
half (126,318 of 295,587) of the class labels, for
406
the reference set Rwc; and about a third (47,442 of
134,840) for Rwl; appear in the set Q of all queries.
Three conclusions can be drawn from the re-
sults. First, query-based runs vastly outperform
Wikipedia-based runs in terms of absolute coverage.
Run Rql contains around 5 and 12 times more class
labels, than Rwc and Rwl respectively. On top of
that, generating class labels via phrase similarities
further increases the class label count by about 20
times for Rqa, and 80 times for Rqg. Second, query-
based runs Rqa and Rqg surpass the document-based
run Rdc. Third, higher class label counts translate
into higher relative coverage. In the upper part of
the table, run Rwl contains 3.9% (relative to Rwc)
and 7.1% (relative to Rwc?Q) of the reference set.
But the relative coverage doubles for Rql at 7.4%
(relative to Rwc) and 17.3% (relative to Rwc?Q).
Coverage again doubles for Rqg at 14.8% (relative
to Rwc) and 34.7% (relative to Rwc?Q). The union
of query-based initial and generated class labels is
Rql?Rqg. The union contains about a quarter (i.e.,
22.2%) or half (52.1%) of the reference set Rwc, de-
pending on whether the reference set is intersected
with the set of all queries or not. In the lower part of
the table, more than 90% of the queries in the refer-
ence set Rwl that are also queries are found among
the class labels collectively extracted in the query-
based runs. Note that, since Rql is disjoint from Rqa
and Rqg, none of the class labels already in Rql can
be ?re-discovered? (generated) again in Rqa or Rqg.
Therefore, by experimental design, relative coverage
scores of Rql may be relatively difficult to surpass by
Rqa or Rqg taken individually.
Diversity: Class labels restricted to those that have
the format ?.. that/which/who ..? are relatively more
specific, e.g., ?grocery stores that double coupons in
omaha?, ?airlines which fly from santa barbara?,
?writers who were doctors?. The most frequent
head phrases of such restricted class labels offer an
idea about how diverse the class labels are. The
counts of class labels for the most frequent head
phrases are in the order of 10?s in the case of Rwl vs.
10,000?s for Rqg. In comparison, none of the class
labels of run Rdc have this format. The lack of such
class labels in run Rdc, and their smaller proportion
in run Rwl vs. Rqg, suggest that class labels extracted
by the proposed method exhibit higher lexical and
syntactic diversity than previous methods do.
Tag (Value): Examples of Class Labels
correct (1.0): angioplasty specialists in kolkata, good
things pancho villa did, eating disorders inpatient units
in the uk nhs specialist services
questionable (0.5): picture framers adelaide cbd, side
effects bicalutamide, different eating disorders, private
hospitals treat kidney stones uk
incorrect (0.0): al hirschfield theatre hours, value of
berkshire hathaway shares, remove spaces in cobol,
dogs with loss of appetite, 1999 majorca open
Table 2: Correctness tags manually assigned to class la-
bels containing one of the (underlined) target phrases, ex-
tracted by various runs
4.3 Precision of Class Labels
Evaluation Metric: Class labels being evaluated are
manually assigned a correctness tag. A class label is
deemed correct, if it is grammatically well-formed
and describes a relevant concept that embodies some
(unspecified) set of instances that share similar prop-
erties; questionable, if it is relevant but not well-
formed; or incorrect. A questionable class label is
not well-formed because it lacks necessary linking
particles (e.g., the prepositions of or for in ?side ef-
fects bicalutamide?), or contains undesirable mod-
ifiers (?different eating disorders?). Examples of
correct and incorrect class labels are ?angioplasty
specialists in kolkata? and ?al hirschfield theatre
hours? respectively.
To compute the precision score, the correctness
tags are converted to numeric values, as shown in
Table 2: correct to 1; questionable to 0.5; and in-
correct to 0. Precision over a list of class labels is
measured as the sum of the correctness values of the
class labels in the list, divided by the size of the list.
Precision Relative to Target Phrases: The preci-
sion of the class labels in each run is determined sim-
ilarly to how relative coverage was computed ear-
lier. More precisely, the precision is computed over
the class labels whose names contain each phrase
from the set of 75 target phrases from (Alfonseca
et al, 2010). For each phrase, and for each run,
a random sample of at most 50 of the class labels
that match the phrase is selected for evaluation. The
samples taken for each run, corresponding to the
same phrase, are combined into a merged list. This
produces one merged list for each phrase, for a total
of 75 merged lists. The precision score over a target
407
phrase is the precision score over its sample of class
labels.
The last two columns of Table 3 capture the pre-
cision scores for the class labels. The scores are
computed in two ways: averaged over the (variable)
subsets of target phrases for which some matching
class label(s) exist, in the last but one column, e.g.,
over 19 of the 75 target phrases for Rwc; and aver-
aged over the entire set of 75 target phrases, in the
last column. The former does not penalize a run
for not being able to extract any class labels con-
taining a particular target phrase, whereas the latter
does penalize. Naturally, precision scores over the
entire set of target phrases decrease when coverage
is lower, for runs Rwc, Rwl and, to a lesser extent,
Rdc and Rql. But even after ignoring target phrases
with no matching class labels, precision scores in
the last but one column in Table 3 reveal important
properties of the experimental runs. First, between
the two Wikipedia-based runs, Rwl has perfect class
labels, whereas as many as 1 in 4 class labels of
run Rwc are marked as incorrect during the evalu-
ation. Second, the class labels collected from ?list
of ..? queries in run Rql correspond to relevant, well-
formed concepts in 80% of the cases. Third, the gen-
eration of class labels via phrase similarities (Rqg)
greatly increases coverage as shown earlier. The in-
crease comes at the expense of lowering precision
from 80% to 72%. However, the phrases from ini-
tial queries that are expanded via distributional sim-
ilarities can be limited from multiple to only one, by
switching from Rqg to Rqa. This gives higher preci-
sion for Rqa than for Rqg.
As a complement to Table 3, the graphs in Fig-
ure 1 offer a more detailed view into the precision
of class labels. The figure covers a Wikipedia-based
run (Rwc) and two query-based runs (Rql, Rqg). The
graphs show the precision scores, over each of the
75 target phrases. Among target phrases for which
some matching class labels exist in the respective
run, the target phrases with the lowest precision
scores are robotics (score of 0.15) and karlsruhe
(0.33), for Rwc; carotid arteries and kidney stones,
both with a score of 0.00 because their matching
class labels are all incorrect, for Rdc; african pop-
ulation and chester arthur, both with a score of 0.00
because their matching class labels are all incorrect,
for Rql; and arlene martel (0.00) and right to vote
Run Target Phrases Precision of Class Labels
Over Target Phrases
All Matched Cvg Over Matched Over All
Rwc 75 19 0.253 0.756 0.191
Rwl 75 15 0.200 1.000 0.200
Rdc 75 35 0.467 0.834 0.389
Rql 75 48 0.640 0.800 0.512
Rqa 75 70 0.933 0.868 0.810
Rqg 75 73 0.973 0.724 0.705
Table 3: Precision of class labels that match (i.e., whose
names contain) each target phrase, computed as an av-
erage over (variable) subsets of target phrases for which
some matching class label(s) exist, and as an average over
the entire set of 75 target phrases
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
aa
a 
  
 1
ad
el
ai
de
 c
bd
   
 5
am
er
ic
an
 fa
sc
ism
  1
0
an
ta
rc
tic
 re
gi
on
  1
5
ba
qu
ba
  2
0
bo
ul
de
r c
ol
or
ad
o 
 2
5
ch
es
te
r a
rth
ur
  3
0
co
n
te
m
po
ra
ry
 a
rt 
 3
5
ea
tin
g 
di
so
rd
er
s  
40
ha
lo
ge
ns
  4
5
jua
n c
arl
os 
 50
lu
ck
y 
al
i  
55
ph
os
ph
or
us
  6
0
ro
u
en
  
65
u
.s
. 
 7
0
w
la
n 
 7
5
Pr
ec
isi
on
Phrase
Per-Phrase Precision for Run Rwc
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
aa
a 
  
 1
ad
el
ai
de
 c
bd
   
 5
am
er
ic
an
 fa
sc
ism
  1
0
an
ta
rc
tic
 re
gi
on
  1
5
ba
qu
ba
  2
0
bo
ul
de
r c
ol
or
ad
o 
 2
5
ch
es
te
r a
rth
ur
  3
0
co
n
te
m
po
ra
ry
 a
rt 
 3
5
ea
tin
g 
di
so
rd
er
s  
40
ha
lo
ge
ns
  4
5
jua
n c
arl
os 
 50
lu
ck
y 
al
i  
55
ph
os
ph
or
us
  6
0
ro
u
en
  
65
u
.s
. 
 7
0
w
la
n 
 7
5
Pr
ec
isi
on
Phrase
Per-Phrase Precision for Run Rql
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
aa
a 
  
 1
ad
el
ai
de
 c
bd
   
 5
am
er
ic
an
 fa
sc
ism
  1
0
an
ta
rc
tic
 re
gi
on
  1
5
ba
qu
ba
  2
0
bo
ul
de
r c
ol
or
ad
o 
 2
5
ch
es
te
r a
rth
ur
  3
0
co
n
te
m
po
ra
ry
 a
rt 
 3
5
ea
tin
g 
di
so
rd
er
s  
40
ha
lo
ge
ns
  4
5
jua
n c
arl
os 
 50
lu
ck
y 
al
i  
55
ph
os
ph
or
us
  6
0
ro
u
en
  
65
u
.s
. 
 7
0
w
la
n 
 7
5
Pr
ec
isi
on
Phrase
Per-Phrase Precision for Run Rdc
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
aa
a 
  
 1
ad
el
ai
de
 c
bd
   
 5
am
er
ic
an
 fa
sc
ism
  1
0
an
ta
rc
tic
 re
gi
on
  1
5
ba
qu
ba
  2
0
bo
ul
de
r c
ol
or
ad
o 
 2
5
ch
es
te
r a
rth
ur
  3
0
co
n
te
m
po
ra
ry
 a
rt 
 3
5
ea
tin
g 
di
so
rd
er
s  
40
ha
lo
ge
ns
  4
5
jua
n c
arl
os 
 50
lu
ck
y 
al
i  
55
ph
os
ph
or
us
  6
0
ro
u
en
  
65
u
.s
. 
 7
0
w
la
n 
 7
5
Pr
ec
isi
on
Phrase
Per-Phrase Precision for Run Rqg
Figure 1: Precision scores for runs Rwc, Rql, Rdc and
Rqg , over class labels that match (i.e., contain) each of
the 75 target phrases
(0.25), for Rqg.
Precision over Samples of Class Labels: The pre-
cision is separately computed over a random sample
of 400 class labels per experimental run. The sam-
ples are selected from the set of all class labels ex-
tracted by the respective run. The precision scores
are: 0.759 for Rwc; 1.000 for Rwl; 0.806 for Rdc;
0.811 for Rql; 0.856 for Rqa; and 0.711 for Rqg. The
scores are in line with scores computed earlier over
the target phrases, in the fourth column of Table 3.
Discussion: As noted in (Ponzetto and Strube,
2007), Wikipedia organizes its articles and cate-
gories into a category network that mixes IsA (sub-
sumption) edges with non-IsA (thematic) edges.
Whenever an edge in Wikipedia is not IsA, the par-
408
Longest Class Labels
Rwl: [japanese army and navy members in military or
politic services in proper japan korea manchuria occu-
pied china and nearest areas in previous times and pa-
cific war epoch(1930-40s), mental disorders as defined
by the diagnostic and statistical manual of mental dis-
orders and the international statistical classification of
diseases and related health problems,..]
Rqg: [differences between transformational lead-
ership and transactional leadership, things to do in
llanfairpwllgwyngyllgogerychwyrndrobwllllantysilio-
gogogoch, philosophical differences between thomas
jefferson and alexander hamilton, musculoskeletal
manifestations of human immunodeficiency virus
infection,..]
Table 4: Longest class labels extracted by runs Rwl and
Rqg
ent category may not be a relevant concept that de-
scribes some set of instances that share similar prop-
erties. Such categories are not good class labels,
and therefore are marked as incorrect. Examples in-
clude the class labels ?austrian contemporary art?,
?1999 majorca open? and ?u.s. route 30?, listed in
Wikipedia as categories of the instances vienna bien-
nale, 1999 majorca open and squirrel hill tunnel re-
spectively. This affects the precision scores for Rwc
in Table 3. It also affects the coverage values rela-
tive to Rwc in Table 1. Ideally, high-precision exper-
imental runs would not extract any incorrect class la-
bels that happen to appear in Rwc, for example ?aus-
trian contemporary art?. But the coverage relative
to Rwc would artificially penalize such runs, for not
extracting the incorrect class labels from Rwc.
As a proxy for estimating class label complexity,
Table 4 shows the longest class labels derived from
Wikipedia (Rwl) vs. generated from queries (Rqg).
Class labels derived from Web search queries may
be semantically overlapping. Examples are ?writers
who killed themselves? vs. ?writers who committed
suicide?. The overlap is desirable, since different
Web users may request the same information via dif-
ferent queries. The same phenomenon has been ob-
served in other information extraction tasks. It also
affects manually-created resources like Wikipedia.
The continuous manual refinements to Wikipedia
content still cannot prevent the occurrence of du-
plicate class labels among Wikipedia List-Of cate-
gories. The duplicates are present in run Rwl. Exam-
Target Class Labels
007 movie actors, .308 weapons, actors with obsessive
compulsive disorder, antibiotics for multiple sclerosis,
astronauts in space station, automobiles with remote
start, beatles songs of love, beetles that bite, compa-
nies with sustainable competitive advantage, countries
with double taxation agreements with india, criminals
who have been executed, daft punk live albums, dal-
las medical companies, direct democracy states, elec-
tronic companies in electronic city bangalore, expen-
sive brands of shoes, eye diseases in cats, f1 car com-
panies, fwd sports cars, garden landscaping maga-
zines, heliskiing resorts, hell in a cell wrestlers, hol-
idays celebrated in sydney, ibf weight classes, ibiza
2011 djs, immunology scientists, jewelry manufactur-
ing companies, kanye west songs on youtube, kingston
upon thames supermarkets, latin military ranks, lud-
hiana newspapers, maastricht treaty countries, mu-
sicians who have been shot, no front license plate
states, non-profit organizations in nashville tennessee,
organic chocolate companies, plants which are used in
homeopathy, programming languages for server side
programming, qatar chemical companies, qld private
schools, real estate companies in virginia beach vir-
ginia, respiratory infection antibiotics, serial killers
with antisocial personality disorder, singers with curly
hair, telecommunications companies in the philip-
pines, trains from la to san diego, visual basic database
management systems, warmblood colors, washington
university basketball players, world heritage sites in
northern ireland
Table 5: Set of 50 class labels, used in the evaluation of
extracted instances
ples are ?formula one drivers that never qualified for
a race? vs. ?formula one drivers who never quali-
fied for a race?; or ?goaltenders who have scored
a goal in a nhl game? vs. ?goaltenders who have
scored a goal in an nhl game?. Some of the lexi-
cal differences among class labels are due to unde-
sirable misspellings. Again, similar problems occa-
sionally affect existing Wikipedia categories: ?no-
bel laureates who endorse barack obama? vs. ?no-
bel laureates who endorse barrack obama?.
5 Evaluation of Instances
5.1 Evaluation Procedure
Target Set of Class Labels: The target set for evalu-
ation is shown in Table 5. Initially, a random sample
of 100 class labels is selected from all class labels in
409
Tag (Value): Examples of Instances
correct (1.0): countries with double taxation agree-
ments with india: thailand; hell in a cell wrestlers:
brock lesnar; ibiza 2011 djs: dimitri from paris; he-
liskiing resorts: valle nevado
questionable (0.5): 007 movie actors: david niven;
kanye west songs on youtube: the good life; holidays
celebrated in sydney: waitangi day
incorrect (0.0): electronic companies in electronic
city bangalore: bank of baroda; garden landscaping
magazines: marquis; immunology scientists: rosalind
franklin
Table 6: Correctness tags manually assigned to instances
extracted from queries for various class labels
run Rqg. Class labels deemed incorrect, as well as
class labels for which no instances are extracted, are
manually removed from the sample. Out of the re-
maining class labels, a smaller random sample of 50
of the remaining class labels is retained, for the pur-
pose of evaluating the quality of instances extracted
for various class labels.
Evaluation Metric: The evaluation computes the
precision of the ranked list of instances extracted for
each target class label. To remove any undesirable
bias towards higher-ranked instances, the ranked list
is sorted alphabetically, then each instance is as-
signed one of the correctness tags from Table 6.
Instances are deemed questionable, if they would
be correct for a rather obscure interpretation of the
class label. For example, david niven is an actor in
one of the spoofs rather than main releases of the
007 movie. Instances that would be correct if a few
words were dropped or added are also deemed ques-
tionable: the good life is not one of the ?kanye west
songs on youtube? but good life is.
To compute the precision score over a ranked list
of instances, the correctness tags are converted to
numeric values. Precision at some rank N in the list
is measured as the sum of the correctness values of
the instances extracted up to rank N, divided by the
number of instances extracted up to rank N.
5.2 Precision of Instances
Precision: Precision scores in Table 7 vary across
target class labels. For some class labels, the ex-
tracted instances are noisy enough that scores are
below 0.50 at ranks 10 and higher. This is the case
for ?electronic companies in electronic city banga-
Target Class Label Precision of Instances
@1 @5 @10 @50
007 movie actors 1.00 1.00 0.85 0.85
actors with obsessive compul-
sive disorder
0.00 0.60 0.70 0.70
antibiotics for multiple sclerosis 0.50 0.60 0.55 0.58
astronauts in space station 1.00 0.70 0.85 0.83
automobiles with remote start 1.00 1.00 0.75 0.75
beatles songs of love 0.00 0.50 0.65 0.52
beetles that bite 1.00 0.80 0.50 0.56
companies with sustainable
competitive advantage
1.00 1.00 0.80 0.88
countries with double taxation
agreements with india
1.00 1.00 1.00 0.90
criminals who have been exe-
cuted
1.00 1.00 0.90 0.82
daft punk live albums 0.50 0.40 0.35 0.35
dallas medical companies 0.00 0.70 0.65 0.54
direct democracy states 1.00 1.00 0.90 0.86
electronic companies in elec-
tronic city bangalore
1.00 0.40 0.40 0.42
expensive brands of shoes 1.00 1.00 0.90 0.92
eye diseases in cats 0.50 0.50 0.35 0.35
f1 car companies 1.00 1.00 0.80 0.30
fwd sports cars 1.00 1.00 1.00 1.00
garden landscaping magazines 0.00 0.10 0.15 0.06
heliskiing resorts 1.00 1.00 1.00 1.00
hell in a cell wrestlers 1.00 1.00 1.00 0.92
holidays celebrated in sydney 1.00 0.70 0.75 0.75
... ... ... ... ...
Average over 50 class labels 0.80 0.80 0.76 0.71
Table 7: Precision at various ranks in the ranked lists of
instances extracted from queries, for various target class
labels and as an average over the entire set of 50 target
class labels
lore? and ?daft punk live albums?, and especially
for ?garden landscaping magazines? which has the
worst precision. On the other hand, instances ex-
tracted for ?companies with sustainable competitive
advantage? or ?criminals who have been executed?
have high precision across all ranks. As an aver-
age over all target class labels, precision is 0.76 at
rank 10, and 0.71 at rank 50. Although there is room
for improvement, we find these accuracy levels to be
encouragingly good, especially at rank 50. As a re-
minder, instances are extracted from noisy queries,
and for class labels as fine-grained as those acquired
and used in our experiments. Some of the extracted
ranked lists of instances are shown in Table 8.
410
Target Class Label Extracted Instances
countries with
double taxation
agreements with
india
[singapore, malaysia, mauritius,
kenya, australia, united king-
dom, cyprus, turkey, thailand, ger-
many,..]
direct democracy
states
[california, oregon, nevada, wis-
consin, louisiana, arizona, ver-
mont, alaska, illinois, michigan,..]
fwd sports cars [scion tc, ford probe, honda pre-
lude, nissan 200sx, lotus elan, mit-
subishi fto, dodge srt-4, mitsubishi
gto, volvo c30, toyota celica,..]
garden landscap-
ing magazines
[front, contemporary, gallery,
edge, view, chelsea, wallpaper,
expo, wizard, sunset,..]
holidays cele-
brated in sydney
[halloween, australia day, anzac
day, independence day, waitangi
day, melbourne cup, hogmanay,
rotuma day, solstice, yule,..]
Table 8: Ranked lists of instances extracted for a sample
of class labels
In additional experiments, the same evaluation
procedure is applied to output from two previous ex-
traction methods. The first method starts by inter-
nally generating a small set of seed instances for a
class label given as input (Wang and Cohen, 2009).
A set expansion module then expands the seed set
into a longer, ranked list of instances. The instances
are extracted from unstructured and semi-structured
text within Web documents. The documents are ac-
cessed via the search interface of a general-purpose
Web search engine (cf. (Wang and Cohen, 2009)
for more details). The second method extracts in-
stances of class labels using the extraction patterns
proposed in (Hearst, 1992). As such, it is similar
to (Kozareva et al, 2008; Van Durme and Pas?ca,
2008; Wu et al, 2012). The method corresponds
to the run Rdc described earlier, where the rela-
tive ranking of instances and class labels uses the
co-occurrence of instances and class labels within
queries (Pas?ca, 2010). For the purpose of the eval-
uation, when no instances are available for a target
class label, the class label is generalized into iter-
atively shorter phrases containing fewer modifiers,
until some instances are available for the shorter
phrase. For example, target class labels like actors
with obsessive compulsive disorder, beatles songs of
love, garden landscaping magazines do not have any
instances extracted by the second method. There-
fore, the instances evaluated for the second method
for these target class labels are collected from the
instances of the more general actors, beatles songs,
landscaping magazines. Without the generalization,
the target class label would receive no credit dur-
ing the evaluation, and the two previous methods
would have lower precision scores. Over the 50 tar-
get class labels, the precision of the two methods is
0.11 and 0.27 at rank 5; 0.06 and 0.25 at rank 10;
0.05 and 0.22 at rank 20; and 0.05 and 0.20 at rank
50. The results confirm that, as explained earlier,
previous methods for open-domain information ex-
traction have limited ability to extract instances of
fine-grained class labels.
Discussion: Earlier errors in the acquisition of the
class label affect the usefulness of any instances that
may be subsequently extracted for them. The ex-
periments require candidate instances to appear in
Wikipedia. This may improve precision, at the ex-
pense of not extracting instances that are not yet in
Wikipedia (Lin et al, 2012).
6 Related Work
Previous methods for extracting classes of instances
from text acquire sets of instances that are each
either unlabeled (Pennacchiotti and Pantel, 2009;
Jain and Pennacchiotti, 2010; Shi et al, 2010),
or associated with a class label (Banko et al,
2007; Wang and Cohen, 2009). The sets of in-
stances and/or class labels may be organized as
flat sets or hierarchically, relative to inferred hier-
archies (Kozareva and Hovy, 2010) or existing hier-
archies such as WordNet (Snow et al, 2006; Davi-
dov and Rappoport, 2009) or the category network
within Wikipedia (Wu and Weld, 2008; Ponzetto
and Navigli, 2009). Semi-structured text from Web
documents is a complementary resource to unstruc-
tured text, for the purpose of extracting relations in
general (Cafarella et al, 2008), and classes and in-
stances in particular (Talukdar et al, 2008; Dalvi et
al., 2012).
With previous methods, the vocabulary of class
labels potentially produced for any instance is con-
fined to a closed set provided manually as in-
put (Wang and Cohen, 2009; Carlson et al, 2010).
The closed set is often derived from resources like
Wikipedia (Talukdar and Pereira, 2010; Lin et al,
411
2012; Hoffart et al, 2013) or Freebase (Pantel et
al., 2012). Alternatively, the vocabulary is not a
closed set, but instead is acquired along with the
instances (Pantel and Pennacchiotti, 2006; Snow
et al, 2006; Banko et al, 2007; Van Durme and
Pas?ca, 2008; Kozareva and Hovy, 2010). In the lat-
ter case, the extracted class labels take the form of
head nouns preceded by modifiers. Examples are
?cities?, ?european cities? (Etzioni et al, 2005);
?artists?, ?strong acids? (Pantel and Pennacchiotti,
2006); ?outdoor activities?, ?prestigious private
schools? (Van Durme and Pas?ca, 2008); ?methate-
rians?, ?aquatic birds? (Kozareva and Hovy, 2010).
In contrast, the class labels extracted in our method
exhibit greater syntactic diversity and are finer-
grained. In addition, they are not constrained to a
particular set of categories available in resources like
Wikipedia.
Fine-grained class labels roughly correspond to
queries submitted in typed search (Demartini et al,
2009) or entity search (Balog et al, 2010) or list-
seeking questions (?name the circuit judges in the
cayman islands that are british?). But our focus is
on generating, rather than answering such queries
or, more generally, attempting to deeply understand
their semantics (Li, 2010). Phrase similarities can
be derived with any methods, using documents (Lin
and Wu, 2009) or search queries (Jain and Pennac-
chiotti, 2010).
Whether Web search queries are a useful textual
data source for open-domain information extraction
has been investigated in several tasks. Examples are
collecting unlabeled sets of similar instances (Jain
and Pennacchiotti, 2010), ranking of class labels
already extracted from text (Pas?ca, 2010), extract-
ing attributes of instances (Alfonseca et al, 2010)
and identifying the occurrences in queries of in-
stances of several types, where the types are de-
fined in a manually-created resource (Pantel et al,
2012). Comparatively, we show that queries are use-
ful in identifying possible class labels, not only re-
ranking them; and even in populating the class labels
with relevant, albeit small, sets of corresponding in-
stances.
As automatically-extracted class labels become
finer-grained, they more clearly illustrate a phe-
nomenon that received little attention. Namely, class
labels of an instance, on one hand, and relations link-
ing the instance with other instances and classes, on
the other hand, are not mutually exclusive pieces
of knowledge. Their extraction does not necessar-
ily require different, dedicated techniques. Quite
the opposite, class labels serve in text as nothing
more than convenient lexical representations, or lex-
ical shorthands, of relations linking instances with
other instances. The class labels ?no front license
plate states? and ?states with no front license plate
requirement? are applicable to arizona. If so, it is
because arizona is a state, and states require the in-
stallation of license plates on vehicles, and the re-
quirement does not apply to the front of vehicles
in the case of arizona. The connection between
class labels and relations has been judiciously ex-
ploited in (Nastase and Strube, 2008). In that study,
relations encoded implicitly within Wikipedia cat-
egories are transformed into explicit relations. As
an example, the explicit relation that deconstruct-
ing harry is directed by woody allen is obtained
from the fact that deconstructing harry is listed un-
der ?movies directed by woody allen? in Wikipedia.
Ours is the first approach to examine the potential
for extracting relations from search queries, where
relations are compactly and loosely folded into the
respective class labels. A variety of methods address
the more general task of acquisition of open-domain
relations from documents, e.g., (Zhu et al, 2009;
Carlson et al, 2010; Fader et al, 2011; Lao et al,
2011).
7 Conclusion
The approach introduced in this paper exploits
knowledge loosely encoded within Web search
queries. It acquires a vocabulary of class labels that
are finer grained than in previous literature. The
class labels have precision comparable to that of
class labels derived from human-created knowledge
repositories. Furthermore, representative instances
are extracted from queries for the fine-grained class
labels, at encouraging levels of accuracy. Current
work explores the use of noisy syntactic features to
increase the accuracy of extracted class labels; the
extraction of instances from evidence in multiple,
rather than single queries; the expansion of extracted
instances into larger sets; and the conversion of fine-
grained class labels into relations among classes.
412
References
E. Alfonseca, M. Pas?ca, and E. Robledo-Arnuncio. 2010.
Acquisition of instance attributes via labeled and re-
lated instances. In Proceedings of the 33rd Interna-
tional Conference on Research and Development in In-
formation Retrieval (SIGIR-10), pages 58?65, Geneva,
Switzerland.
K. Balog, M. Bron, and M. de Rijke. 2010. Category-
based query modeling for entity search. In Proceed-
ings of the 32nd European Conference on Information
Retrieval (ECIR-10), pages 319?331, Milton Keynes,
United Kingdom.
M. Banko, Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the Web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-07), pages 2670?2676, Hyderabad, India.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), pages 224?231,
Seattle, Washington.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on
the Web. In Proceedings of the 34th Conference on
Very Large Data Bases (VLDB-08), pages 538?549,
Auckland, New Zealand.
A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and
T. Mitchell. 2010. Coupled semi-supervised learn-
ing for information extraction. In Proceedings of the
3rd ACM Conference on Web Search and Data Mining
(WSDM-10), pages 101?110, New York.
B. Dalvi, W. Cohen, and J. Callan. 2012. Websets: Ex-
tracting sets of entities from the Web using unsuper-
vised information extraction. In Proceedings of the
5th ACM Conference on Web Search and Data Mining
(WSDM-12), pages 243?252, Seattle, Washington.
D. Davidov and A. Rappoport. 2009. Enhancement
of lexical concepts using cross-lingual Web mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
09), pages 852?861, Singapore.
G. Demartini, T. Iofciu, and A. de Vries. 2009. Overview
of the INEX 2009 Entity Ranking track. In INitiative
for the Evaluation of XML Retrieval Workshop, pages
254?264, Brisbane, Australia.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in Web text. In Proceed-
ings of the 20th International Joint Conference on Ar-
tificial Intelligence (IJCAI-07), pages 2733?2739, Hy-
derabad, India.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web: an experimental study. Artificial Intelligence,
165(1):91?134.
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
Mausam. 2011. Open information extraction: The
second generation. In Proceedings of the 22nd In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-11), pages 3?10, Barcelona, Spain.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP-11), pages
1535?1545, Edinburgh, Scotland.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum.
2013. YAGO2: a spatially and temporally enhanced
knowledge base from Wikipedia. Artificial Intelli-
gence, 194:28?61.
A. Jain and M. Pennacchiotti. 2010. Open entity ex-
traction from Web search query logs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING-10), pages 510?518,
Beijing, China.
Z. Kozareva and E. Hovy. 2010. A semi-supervised
method to learn and construct taxonomies using the
web. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-10), pages 1110?1118, Cambridge, Mas-
sachusetts.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
class learning from the Web with hyponym pattern
linkage graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-08), pages 1048?1056, Columbus, Ohio.
N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk
inference and learning in a large scale knowledge base.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
11), pages 529?539, Edinburgh, Scotland.
X. Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-10), pages 1337?1345, Up-
psala, Sweden.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7, Taipei, Taiwan.
D. Lin and X. Wu. 2009. Phrase clustering for discrim-
inative learning. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-IJCNLP-09), pages 1030?1038, Singapore.
413
T. Lin, Mausam, and O. Etzioni. 2012. No noun phrase
left behind: Detecting and typing unlinkable enti-
ties. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL-12), pages 893?903, Jeju Island, Korea.
V. Nastase and M. Strube. 2008. Decoding Wikipedia
categories for knowledge acquisition. In Proceedings
of the 23rd National Conference on Artificial Intelli-
gence (AAAI-08), pages 1219?1224, Chicago, Illinois.
M. Pas?ca. 2010. The role of queries in ranking la-
beled instances extracted from text. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING-10), pages 955?962, Bei-
jing, China.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL-06), pages 113?120,
Sydney, Australia.
P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-09), pages 938?947, Singapore.
P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity
types from query logs via user intent modeling. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-12), pages
563?571, Jeju Island, Korea.
M. Pennacchiotti and P. Pantel. 2009. Entity extrac-
tion via ensemble semantics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP-09), pages 238?247,
Singapore.
S. Ponzetto and R. Navigli. 2009. Large-scale taxonomy
mapping for restructuring and integrating Wikipedia.
In Proceedings of the 21st International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), pages 2083?
2088, Pasadena, California.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
National Conference on Artificial Intelligence (AAAI-
07), pages 1440?1447, Vancouver, British Columbia.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3):130?137.
D. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. 2005.
Probabilistic question answering on the Web. Journal
of the American Society for Information Science and
Technology, 56(3):571?583.
M. Remy. 2002. Wikipedia: The free encyclopedia. On-
line Information Review, 26(6):434.
S. Shi, H. Zhang, X. Yuan, and J. Wen. 2010.
Corpus-based semantic class mining: Distributional
vs. pattern-based approaches. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING-10), pages 993?1001, Beijing,
China.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801?808, Sydney, Australia.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL-10), pages 1473?1481, Uppsala,
Sweden.
P. Talukdar, J. Reisinger, M. Pas?ca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-supervised
acquisition of labeled class instances using graph ran-
dom walks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-08), pages 582?590, Honolulu, Hawaii.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243?
1248, Chicago, Illinois.
R. Wang and W. Cohen. 2009. Automatic set instance
extraction using the Web. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-IJCNLP-09), pages 441?449, Singa-
pore.
F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the
17th World Wide Web Conference (WWW-08), pages
635?644, Beijing, China.
W. Wu, , H. Li, H. Wang, and K. Zhu. 2012. Probase:
a probabilistic taxonomy for text understanding. In
Proceedings of the 2012 International Conference on
Management of Data (SIGMOD-12), pages 481?492,
Scottsdale, Arizona.
J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. 2009. Stat-
Snowball: a statistical approach to extracting entity re-
lationships. In Proceedings of the 18th World Wide
Web Conference (WWW-09), pages 101?110, Madrid,
Spain.
414
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1081?1091,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Queries as a Source of Lexicalized Commonsense Knowledge
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
The role of Web search queries has been
demonstrated in the extraction of attributes
of instances and classes, or of sets of re-
lated instances and their class labels. This
paper explores the acquisition of open-
domain commonsense knowledge, usu-
ally available as factual knowledge, from
Web search queries. Similarly to previ-
ous work in open-domain information ex-
traction, knowledge extracted from text
- in this case, from queries - takes the
form of lexicalized assertions associated
with open-domain classes. Experimental
results indicate that facts extracted from
queries complement, and have competitive
accuracy levels relative to, facts extracted
from Web documents by previous meth-
ods.
1 Introduction
Motivation: Open-domain information extrac-
tion methods (Etzioni et al., 2005; Pennac-
chiotti and Pantel, 2009; Wang and Cohen, 2009;
Kozareva and Hovy, 2010; Wu et al., 2012) aim
at distilling text into knowledge assertions about
classes, instances and relations among them (Et-
zioni et al., 2011). Ideally, the assertions would
complement or expand upon knowledge avail-
able in popular, human-created resources such as
Wikipedia (Remy, 2002) and Freebase (Bollacker
et al., 2008), reducing costs and scalability is-
sues associated with manual editing, curation and
maintenance of knowledge.
Candidate knowledge assertions extracted from
text for various instances and classes (Banko et al.,
2007; Cafarella et al., 2008; Wu and Weld, 2010)
must satisfy several constraints in order to be use-
ful. First, their boundaries must be correctly iden-
tified within the larger context (e.g., a document
sentence) from which they are extracted. In prac-
tice, this is a challenge with arbitrary Web docu-
ments, where even instances and class labels that
are complex nouns, and thus still shorter than can-
didate assertions, are difficult to precisely detect
and pick out from surrounding text (Downey et
al., 2007). This causes the extraction of assertions
like companies may ?be in the process?, hurri-
canes may ?run from june?, or video games may
?make people? (Fader et al., 2011). Second, the
assertions must be correctly associated with their
corresponding instance or class. In practice, tag-
ging and parsing errors over documents of arbi-
trary quality may cause the extracted assertions to
be associated with the wrong instances or classes.
Examples are video games may ?watch movies?,
or video games may ?read a book?. Third, the as-
sertions, even if true, must refer to relevant prop-
erties or facts, rather than to statements of little
or no practical interest to anyone. In practice,
relevant properties may be difficult to distinguish
from uninteresting statements in Web documents.
Consequently, assertions extracted from Web doc-
uments include the facts that companies may ?say
in a statement?, or that hurricanes may ?be just
around the corner? or may ?be in effect?.
Contributions: This paper explores the use of
Web search queries, as opposed to Web docu-
ments, as a textual source from which knowl-
edge pertaining to open-domain classes can be
extracted. Previous explorations of the role of
queries in information extraction include the ac-
quisition of attributes of instances (Alfonseca
et al., 2010) and of classes (Van Durme and
Pas?ca, 2008); the acquisition of sets of related
1081
instances (Sekine and Suzuki, 2007; Jain and
Pennacchiotti, 2010) and their class labels (Van
Durme and Pas?ca, 2008; Pantel et al., 2012); the
disambiguation of instances mentioned in queries
relative to entries in external knowledge reposito-
ries (Pantel and Fuxman, 2011) and its applica-
tion in query expansion (Dalton et al., 2014); and
the extraction of the most salient of the instances
mentioned in a given Web document (Gamon et
al., 2013). In comparison, this paper shows that
queries also lend themselves to the acquisition of
factual knowledge beyond attributes, like the facts
that companies may ?buy back stock?, hurricanes
may ?need warm water?, and video games may
?come out on tuesdays?.
To extract knowledge assertions for diverse
classes of interest to Web users, the method ap-
plies simple extraction patterns to queries. The
presence of the source queries, from which the as-
sertions are extracted, is in itself deemed evidence
that the Web users who submitted the queries
find the assertions to be relevant and not just ran-
dom statements. Experimental results indicate that
knowledge assertions extracted from queries com-
plement, and have competitive accuracy levels rel-
ative to, knowledge extracted from Web docu-
ments by previous methods.
2 Extraction from Queries
Queries as Knowledge: Users tend to formu-
late their Web search queries based on knowl-
edge that they already possess at the time of the
search (Pas?ca, 2007). Therefore, search queries
play two roles simultaneously: in addition to re-
questing new information, they indirectly convey
knowledge in the process.
A fact corresponds to a property that, together
with other properties, help define the semantics of
the class and its interaction with other classes. The
extraction of factual knowledge from queries starts
from the intuition that, if a fact F is relevant for a
class C, then users are likely to ask for various
aspects of the fact F , in the context of the class
C. If companies may ?pay dividends? or ?get au-
dited?, and such properties are relatively promi-
nent for companies, then users eventually submit
queries to inquire about the facts.
Often, queries will be simple concatenations of
keywords: ?companies pay dividends? or perhaps
?company dividends?, ?audit companies?. Since
there are no restrictions on the linguistic structure
Query logs
Target classes
Disease: {diseases, illnesses, medical conditions, ...}
how does an actor prepare for a role   how do actors get an agent
how do actors get paid   why do actors need to warm up
why are actors left handed   how do actors memorize their lines
Hurricane: {hurricanes, ...}
how is a disease transmitted   how are diseases inherited from parents
how is a disease treated   how is a disease diagnosed
how do diseases enter the body   how does a disease mutate
why does a hurricane weaken over land   how are hurricanes predicted
why does a hurricane lose strength over land   how is a hurricane forecasted
why does a hurricane have an eye   how does a hurricane dissipate
Extracted facts
Actor: {prepare for a role, get an agent, get paid, be left handed,
Disease: {be transmitted, be inherited from parents, be treated,
Hurricane: {weaken over land, be predicted, lose strength over land,
             need to warm up, memorize their lines, ...}
                 be diagnosed, enter the body, mutate, ...}
                    be forecasted, have an eye, dissipate, ...}
Actor: {actors, ...}
Figure 1: Overview of extraction of knowledge
from Web search queries
of keyword-based queries, extracting facts from
such queries would be difficult. But if queries are
restricted to fact-seeking questions, the expected
format of the questions makes it easier to iden-
tify the likely boundaries of the class and the fact
mentioned in the queries. Queries such as ?why
does a (company)
C
(pay dividends)
F
? and ?how
do (companies)
C
(get audited)
F
?, follow the lin-
guistic structure, even if minimal, imposed by for-
mulating the query as a question. This allows one
to approximate the location of the class C, possi-
bly towards the beginning of the query; the start of
the fact F , possibly as the verb immediately fol-
lowing the class; and the end of the fact, which
possibly coincides with the end of the query.
Acquisition from Queries: The extraction
method proposed in this paper takes as input a set
of target classes, each of which is available as a
set of class descriptors, i.e., phrases that describe
the class. It also has access to a set of anonymized
queries. As illustrated in Figure 1, the method se-
lects queries that contain a class descriptor and
what is deemed to be likely a fact. It outputs
ranked lists of facts for each class. The extrac-
tion consists in several stages: 1) the selection of
a subset of queries that refer to a class in a form
that suggests the queries inquire about a fact of the
class; 2) the extraction of facts, from query frag-
ments that describe the property of interest to users
submitting the queries; and 3) the aggregation and
1082
ranking of facts of a class.
Extraction Patterns: In order to determine
whether a query contains a fact for a class, the
query is matched against the extraction patterns
from Table 1.
The use of targeted patterns in relation extrac-
tion has been suggested before (Hearst, 1992;
Fader et al., 2011; Mesquita et al., 2013). Specifi-
cally, in (Tokunaga et al., 2005), the patterns ?A of
D? or ?what is the A of D? extract noun-phrase A
attributes from queries and documents, for phrase
descriptors D of the class. In our case, the pat-
terns are constructed such that they match ques-
tions that likely inquire about the reason why, or
manner in which, a relevant fact F may hold for
a class C. For example, the first pattern from Ta-
ble 1 matches the queries ?why does a company
pay dividends? and ?why do video games come
out on tuesdays?. These queries seek explanations
for why certain properties may hold for companies
and video games respectively.
A class C can be mentioned in queries through
lexicalized, phrase descriptors D that capture its
meaning. The descriptors D of the class C
may be available as non-disambiguated items, i.e.,
as strings (companies, firms, businesses, video
games); or as disambiguated items, that is, as
pointers to knowledge base entries with a disam-
biguated meaning (Company, Video Game). In the
first case, the matching of a query fragment, on
one hand, to the portion of an extraction pattern
corresponding to the class C, on the other hand,
consists in simple string matching with one of the
descriptors D specified for C. In the second case,
the matching requires that the disambiguation of
the query fragment, in the context of the query,
matches the desired disambiguated meaning of C
from the pattern. The subset of queries matching
any of the extraction patterns, for any descriptor
D of a class C, are the queries that contribute to
extracting facts of the class C.
If a pattern from Table 1 employs a form of the
auxiliary verb ?be?, the extracted facts are modi-
fied by having the verb ?be? inserted at their be-
ginning. For example, the fact ?be stored side-
ways? is extracted from the query ?why is wine
stored sideways?. In all patterns, the candidate
fact is required to start with a verb that acts as the
predicate of the query.
Ranking of Facts: Facts of a class C are aggre-
gated from facts of individual class descriptors D.
Extraction Pattern
? Examples of Matched Queries
why [does|did|do] [a|an|the|<nothing>] D F
? why does a (company)
D
(pay dividends)
F
? why do (planes)
D
(take longer to fly west than east)
F
? why do (video games)
D
(come out on tuesdays)
F
why [is|was|were] [a|an|the|<nothing>] D F
? why are (cars)
D
(made of steel)
F
? why is a (newspaper)
D
(written in columns)
F
? why is (wine)
D
(stored sideways)
F
how [does|did|do] [a|an|the|<nothing>] D F
? how does a (company)
D
(use financial statements)
F
? how does (food)
D
(get absorbed)
F
? how do (stadiums)
D
(get cleaned)
F
how [is|was|were] [a|an|the|<nothing>] D F
? how are (hurricanes)
D
(predicted)
F
? how is a (treaty)
D
(ratified)
F
? how is a (cell phone)
D
(unlocked)
F
Table 1: The extraction patterns match queries
likely to inquire about facts of a class (D=a phrase
acting as a class descriptor; F=a sequence of to-
kens whose first token is the head verb of the
query)
A fact F is deemed more relevant for C if the fact
is extracted for more of the descriptors D of the
class C, and for fewer descriptors D that do not
belong to the class C. Concretely, the score of a
fact for a class is the lower bound of the Wilson
score interval (Brown et al., 2001):
Score(F,C) = LowBound(Wilson(N
+
, N
?
))
where:
? the number of positive observations N
+
is
the number of queries for which the fact A is
extracted for some descriptor D of the class C,
|{Query(D,A)}
D?C
|; and
? the number of negative observations N
?
is
the number of queries for which the fact F is ex-
tracted for some descriptors D outside of the class
C, |{Query(D,A)}
D/?C
|.
The scores are internally computed at 95% con-
fidence. Facts of each class are ranked in decreas-
ing order of their scores. In case of ties, facts are
ranked in decreasing order of the frequency sum
of the source queries from which the facts are ex-
tracted.
3 Experimental Setting
Textual Data Sources: The experiments rely
on a random sample of around 1 billion fully-
anonymized Web search queries in English. The
sample is drawn from queries submitted to a
general-purpose Web search engine. Each query
is available independently from other queries, and
is accompanied by its frequency of occurrence in
1083
Target Class (class descriptors to be looked up in queries)
Actor (actors) Mountain (mountains)
Aircraft (planes) Movie (movies)
Award (awards) NationalPark (national parks)
Battle (battles) NbaTeam (nba teams)
Car (cars) Newspaper (newspapers)
CartoonChar Painter
(cartoon characters) (painters)
CellPhone ProgLanguage
(cell phones) (programming languages)
ChemicalElem (elements) Religion (religions)
City (cities) River (rivers)
Company (companies) SearchEngine (search engines)
Country (countries) SkyBody (celestial bodies)
Currency (currencies) Skyscraper (skyscrapers)
DigitalCamera SoccerClub
(digital cameras) (soccer teams)
Disease (diseases) SportEvent (sport events)
Drug (drugs) Stadium (stadiums)
Empire TerroristGroup
(empires) (terrorist groups)
Flower (flowers) Treaty (treaties)
Food (foods) University (universities)
Holiday (holidays) VideoGame (video games)
Hurricane (hurricanes) Wine (wines)
Table 2: Set of 40 target classes used in the evalu-
ation of extracted facts
the query logs.
Target Classes: Table 2 shows the set of 40 tar-
get classes for evaluating the extracted facts. Sim-
ilar evaluation strategies were followed in previ-
ous work (Pas?ca, 2007). As illustrated earlier in
Figure 1, a target class consists in a small set of
phrase descriptors. The phrase descriptors are se-
lected such that they best approximate the mean-
ing of the class. In general, the descriptors can be
selected and expanded with any strategy from any
source. One such possible source might be syn-
onym sets from WordNet (Fellbaum, 1998). Fol-
lowing a stricter strategy, the sets of descriptors
in our experiments contain only one phrase each,
manually selected to match the target class. Ex-
amples are the sets of phrase descriptors {actors}
for the class Actor and {nba teams} for NbaTeam.
The occurrence of a descriptor (nba teams) in
a query (?how do nba teams make money?) is
deemed equivalent to a mention of the correspond-
ing class (NbaTeam) in that query. Each set of de-
scriptors of a class is then expanded (not shown in
Table 2), to also include the singular forms of the
descriptors (e.g., nba team for nba teams). Further
inclusion of additional descriptors would increase
the coverage of the extracted facts.
Experimental Runs: The baseline run R
D
is
the extraction method introduced in (Fader et al.,
2011). The method produces triples of an instance
or a class, a text fragment capturing a fact, and an-
other instance or class. In these experiments, the
second and third elements of each triple are con-
catenated together, giving pairs of an instance or
a class, and a fact applying to it. The baseline
run is applied to around 500 million Web docu-
ments in English. 1 In addition to the baseline run,
the method introduced in this paper constitutes the
second experimental run R
Q
. Facts extracted by
the two experimental runs are directly compara-
ble: both are text snippets extracted from the re-
spective sources of text - documents in the case of
R
D
, or queries in the case of R
Q
.
Parameter Settings: Queries that match any of
the extraction patterns from Table 1 are syntacti-
cally parsed (Petrov et al., 2010), in order to verify
that the first token of an extracted fact is the head
verb of the query. Extracted facts that do not sat-
isfy the constraint are discarded. A positive side
effect of doing so is to avoid extraction from some
of the particularly subjective queries. For exam-
ple, facts extracted from the queries ?why is (A)
evil? or ?why is (B) ugly?, where (A) and (B) are
the name of a company and actress respectively,
are discarded.
4 Evaluation Results
Accuracy: The measurement of recall requires
knowledge of the complete set of items (in our
case, facts) to be extracted. Unfortunately, this
number is often unavailable in information extrac-
tion tasks in general (Hasegawa et al., 2004), and
fact extraction in particular. Indeed, the manual
enumeration of all facts of each target class, to
measure recall, is unfeasible. Therefore, the eval-
uation focuses on the assessment of accuracy.
Following evaluation methodology from prior
work (Pas?ca, 2007), the top 50 facts, from a ranked
lists extracted for each target class, are manually
assigned correctness labels. A fact is marked as
vital, if it must be present among representative
facts of the class; okay, if it provides useful but
non-essential information; and wrong, if it is in-
correct (Pas?ca, 2007). For example, the facts ?run
on kerosene?, ?be delayed? and ?fly wiki? are an-
notated as vital, okay and wrong respectively for
the class Aircraft. To compute the precision score
1At the time when the experiments were conducted, the
facts were extracted by the baseline run from English doc-
uments in the ClueWeb collection, and were accessible at
http://reverb.cs.washington.edu.
1084
Target Class: Sample of Extracted Facts (with Source
Queries)
Target Class: Sample of Extracted Facts (with Source
Queries)
Actor (may): prepare for a role (how does an actor prepare
for a role), get an agent (how do actors get an agent), do love
scenes (how do actors do love scenes), get paid (how do actors
get paid), be left handed (why are actors left handed), need to
warm up (why do actors need to warm up)
Car (may): backfire (why does a car backfire), burn oil (why
do cars burn oil), pull to the right (why do cars pull to the
right), pull to the left (why does a car pull to the left), catch
on fire (how does a car catch on fire), run hot (why do cars
run hot), get repossessed (why do cars get repossessed)
Company (may): buy back stock (how does a company buy
back stock), go public (why does a company go public), buy
back shares (why do companies buy back shares), incorporate
in delaware (why do companies incorporate in delaware), pay
dividends (why does a company pay dividends), merge (how
do companies merge)
Disease (may): be transmitted (how is a disease transmitted),
be inherited from parents (how are diseases inherited from
parents), affect natural selection (how do diseases affect nat-
ural selection), be treated (how is a disease treated), affect the
conquest of the americas (how did diseases affect the conquest
of the americas), be diagnosed (how is a disease diagnosed)
Hurricane (may): weaken over land (why does a hurricane
weaken over land), be predicted (how are hurricanes pre-
dicted), lose strength over land (why does a hurricane lose
strength over land), have an eye (why does a hurricane have
an eye), be forecasted (how is a hurricane forecasted), dissi-
pate (how does a hurricane dissipate), lose strength (how do
hurricanes lose strength)
NbaTeam (may): make money (how does an nba team make
money), communicate to win (how does an nba team commu-
nicate to win), want expiring contracts (why do nba teams
want expiring contracts), make the playoffs (how do nba
teams make the playoffs), get their names (how do nba teams
get their names), do sign and trades (why do nba teams do
sign and trades), lose money (how do nba teams lose money)
Table 3: Examples of facts extracted for various classes by run R
Q
Class Precision Class Precision
@10 @20 @50 @10 @20 @50
R
D
R
Q
R
D
R
Q
R
D
R
Q
R
D
R
Q
R
D
R
Q
R
D
R
Q
Actor 0.60 0.85 0.57 0.85 0.60 0.83 Mountain 0.20 0.75 0.10 0.72 0.05 0.55
Aircraft 0.50 0.95 0.42 0.87 0.47 0.81 Movie 0.40 0.20 0.37 0.20 0.40 0.32
Award 0.50 0.25 0.45 0.25 0.52 0.23 NationalPark 0.40 0.70 0.32 0.72 0.30 0.69
Battle 0.25 0.45 0.42 0.46 0.38 0.44 NbaTeam 0.60 0.75 0.42 0.80 0.20 0.77
Car 0.55 0.80 0.62 0.82 0.52 0.75 Newspaper 0.25 0.80 0.32 0.55 0.44 0.59
CartoonChar 0.25 0.60 0.22 0.57 0.18 0.55 Painter 0.30 0.75 0.40 0.65 0.42 0.61
CellPhone 0.75 0.90 0.75 0.82 0.55 0.82 ProgLanguage 0.20 0.75 0.25 0.72 0.25 0.70
ChemicalElem 0.45 0.90 0.45 0.72 0.54 0.72 Religion 0.10 0.80 0.30 0.70 0.13 0.69
City 0.30 0.80 0.27 0.67 0.27 0.63 River 0.65 0.95 0.70 0.87 0.54 0.57
Company 0.60 0.95 0.57 0.95 0.53 0.91 SearchEngine 0.40 0.70 0.37 0.65 0.38 0.64
Country 0.30 0.85 0.25 0.90 0.20 0.83 SkyBody 0.55 0.00 0.32 0.00 0.28 0.00
Currency 0.40 0.90 0.25 0.85 0.22 0.73 Skyscraper 0.45 0.85 0.37 0.77 0.24 0.78
DigitalCamera 0.30 0.90 0.35 0.85 0.42 0.77 SoccerClub 0.35 0.15 0.37 0.33 0.41 0.31
Disease 0.55 0.90 0.60 0.70 0.64 0.60 SportEvent 0.30 0.00 0.27 0.00 0.32 0.00
Drug 0.20 0.95 0.30 0.87 0.40 0.78 Stadium 0.50 0.85 0.50 0.77 0.47 0.75
Empire 0.15 0.45 0.12 0.52 0.23 0.49 TerroristGroup 0.90 0.55 0.70 0.55 0.55 0.53
Flower 0.60 0.90 0.50 0.80 0.48 0.78 Treaty 1.00 0.75 0.90 0.75 0.77 0.59
Food 0.65 0.80 0.55 0.85 0.43 0.85 University 0.10 0.95 0.05 0.92 0.10 0.70
Holiday 0.30 0.25 0.17 0.22 0.19 0.14 VideoGame 0.20 0.90 0.25 0.85 0.28 0.77
Hurricane 0.40 0.80 0.37 0.77 0.32 0.73 Wine 0.70 1.00 0.60 0.87 0.56 0.70
Average-Class 0.43 0.71 0.40 0.67 0.38 0.63
Table 4: Relative accuracy of facts extracted from documents in run R
D
, vs. facts extracted from queries
in run R
Q
over a set of facts, the correctness labels are con-
verted to numeric values: vital to 1.0, okay to 0.5,
and wrong to 0.0. Precision is the sum of the cor-
rectness values of the facts, divided by the number
of facts. Table 3 shows a sample of facts extracted
from queries by run R
Q
, which are judged to be
vital or okay.
Table 4 provides a comparison of precision at
ranks 10, 20 and 50, for each of the 40 target
classes and as an average over all target classes.
The scores vary from one class to another and be-
tween the two runs, for example 0.22 (R
D
) and
0.73 (R
Q
) for the class Currency at rank 50, but
0.77 (R
D
) and 0.59 (R
Q
) for Treaty. Run R
Q
fails
to extract any facts for two of the target classes,
SkyBody and SportEvent. Therefore, it receives no
credit for those classes during the computation of
precision.
Over all target classes, run R
Q
is superior to run
R
D
, with relative precision boosts of 65% (0.71
vs. 0.43) at rank 10, 67% at rank 20, and 65% at
rank 50. The results show that facts extracted from
1085
Run: [Ranked Facts Extracted from Text for a Sample of Classes]
Class: Actor (may):
R
D
: [do a great job, get the part, play their roles, play their parts, play their characters, be on a theatre, die aged 81, be all
great, deliver their lines, portray their characters, take on a role, be best known for his role, play the role of god, be people,
give great performances, bring the characters to life, wear a mask, be the one, have chemistry, turn director, read the script, ..]
R
Q
: [prepare for a role, get an agent, do love scenes, get paid, be left handed, need to warm up, get started, get paid so much,
memorize their lines, get ripped so fast, remember their lines, make themselves cry, learn their lines, jump out of a window
in times square, lose weight so fast, play dead, be paid, kiss, remember lines, memorize lines, get discovered, get paid for
movies, go uncredited, say break a leg, get their start, have perfect skin, become actors, ..]
Class: Car (may):
R
D
: [get a tax write-off, can be more competitive than airline rates, be in good condition, be first for second hand cars, be in
the shop, relocate to a usa firm, be in motion, come to a stop, hire companies, be in great shape, be for sale, hire service from
spain, ride home, be on fire, use the autos.com, come to a halt, catch fire, be on road, be on display, go on sale, hit a tree, be
available for delivery, stop in front, be a necessity, go off the road, pull out in front, hire services, run out of gas, ..]
R
Q
: [backfire, burn oil, save ostriches from extinction, pull to the right, pull to the left, catch on fire, run hot, sputter, get
repossessed, have a top speed, be called a car, have gears, get impounded, be called cars, go to auction, called whip, made of
steel, get hot in the sun, shake at high speed, changed america, totaled, cut out, cut off while driving, fail emissions, protect
from lightning, run rich, lose oil, become electrically charged, cut off, flip over, know tire pressure, have a maximum speed,
require premium gas, shake at high speeds, stall out, cause acid rain, fog up, get stuck in park, need an oil change, ..]
Class: Company (may):
R
D
: [say in a statement, specialize in local moves, be in the process, go out of business, have been in business, be in business,
do business, file for bankruptcy, make money, be on track, say in a press release, be a place, have cut back on health insurance,
state in a press release, be on the verge, save money, be in talks, have helped thousands of consumers, reduce costs, go bust,
be in the midst, say in a release, be founded in 1999, be in trouble, be founded in 2000, be losing money, ..]
R
Q
: [buy back stock, go public, buy back shares, incorporate in delaware, pay dividends, merge, go global, go international,
use financial statements, verify education, expand internationally, go green, verify employment, need a website, choose to
form as a corporation, do market research, go private, diversify, go into administration, get on angies list, pay dividend, struck
off, buy back their shares, get audited, need a mission statement, repurchase common stock, spin off, get listed on the nyse,
create value, distribute dividends, need a strategic plan, ..]
Class: Mountain (may):
R
D
: [spot fever, meet the sea, be covered with snow, be covered in snow, be the place, come into view, be on fire, be fun,
fly fishing, be volcano, be moved out of their places, enjoy the exhilaration, meet the ocean, be available for hire, keep their
secrets, win the mwc in 2010, ..]
R
Q
: [affect rainfall, affect the climate of an area, affect climate, be measured, be formed, be created, be made, grow, affect
weather, have snow on top, affect solar radiation, affect temperature, be formed ks2, affect the weather, be built, affect people,
look blue, tops cold, affect neighboring climates, be formed video, help shape the development of greek civilization, be made
for kids, occur, affect the climate, be formed, be formed wikipedia, have roots, affect precipitation, exist, affect life on earth,
be formed kids, float in avatar, erode, have snow on the top, affect the political character of greece, help rain form, ..]
Table 5: Comparative top facts extracted for a sample of classes from documents (R
D
) or queries (R
Q
)
queries have higher levels of accuracy.
Facts from Documents vs. Queries: Table 5
compares the top facts extracted by the two exper-
imental runs for a sample of target classes. Most
commonly, erroneous facts are extracted by run
R
D
due to the extraction of relatively uninterest-
ing properties (a Company may ?say in a state-
ment? or ?be in the process?). Other errors in
R
D
are caused by wrong boundary detection of
facts within documents (a Company may ?be in
the midst?), or by the association of a fact with the
wrong instance or class (a Car may ?hire compa-
nies? or ?hire services?).
As for facts extracted by run R
Q
, they are some-
times too informal, due to the more conversa-
tional nature of queries when compared to docu-
ments. Queries may suggest that a Car may ?know
tire pressure?. Occasionally, similarly to facts
from documents, they have wrong boundaries (a
Mountain may ?be made for kids? or ?be formed
wikipedia?); and they may correspond to less in-
teresting, or too specific, properties (a Company
may ?incorporate in delaware?). Lastly, queries
may appear to be questions, but occasionally they
really are not. An example is the query ?why did
the actor jump out of the window in times square?,
which may refer to a joke. When such queries
match one of the extraction patterns, they produce
wrong facts. Overall, Table 5 corroborates the
scores from Table 4. It suggests that a) facts ex-
tracted by either R
D
or R
Q
still need refinement,
before they can capture essential characteristics
of the respective classes and nothing else; and b)
facts extracted in run R
Q
have higher quality than
facts extracted in run R
D
. Indeed, because fact-
seeking queries inquire about the value (or rea-
son, or manner) of some relations of an instance,
the facts themselves tend to be more relevant than
facts extracted from arbitrary document sentences.
An issue related to facts extracted from text
1086
is their ability to capture the kind of ?obvious?
commonsense knowledge (Zang et al., 2013) that
would be essential for machine-driven reasoning.
If it is obvious that ?teachers give lectures?, how
likely is it for such information to be explic-
itly stated in documents or, even more interest-
ingly, inquired about in queries? Anecdotal ev-
idence gathered during experimentation suggests
that queries do produce many commonsense facts,
perhaps even surprisingly so given that a) queries
tend to be shorter and grammatically simpler than
document sentences; and b) the patterns in Ta-
ble 1 are relatively more restrictive than the pat-
terns used in (Fader et al., 2011). Indeed, the pat-
terns in Table 1, when applied to queries like ?why
do teachers give homework?, ?why do teach-
ers give grades?, actually produce commonsense
knowledge that teachers give homework, grades
(to their students). In fact, the quality of equivalent
facts extracted from documents in (Fader et al.,
2011) may be lower. Concretely, facts extracted
in (Fader et al., 2011) state that what teachers give
is students, class, homework and feedback, in this
order. The first two of these extractions are errors,
likely caused by the incorrect detection of com-
plex entities and their inter-dependencies in docu-
ment sentences (Downey et al., 2007).
A necessary condition for the usefulness of ex-
tracted facts is that the source text contain consis-
tent, true information. But both documents and
queries may contain contradictory or false infor-
mation, whether due to unsupported conjectures,
unintended errors or systematic campaigns that
fall under the scope of adversarial information re-
trieval (Castillo and Davison, 2011). The phenom-
ena potentially affect prior work on Web-based
open-domain extraction, and potentially affect the
quality of facts extracted from queries in this pa-
per. For example, facts extracted from queries like
?why do companies like obamacare? and ?why do
companies hate obamacare? would be inconsis-
tent, if not incorrect.
Occasionally, facts extracted from the two text
sources refer to the same properties. For exam-
ple, a VideoGame may ?be good for the hand-eye
coordination?, according to documents; and may
?improve hand eye coordination?, according to
queries. Nevertheless, facts derived from queries
likely serve as a complement, rather than replace-
ment, of facts from documents. In particular, facts
extracted from queries make no attempt to iso-
late the value of the respective properties, whereas
facts extracted from documents usually do.
Stricter Comparison of Data Sources: In the
experiments described so far, distinct sets of pat-
terns are applied in the experimental runs to doc-
uments vs. queries. More precisely, run R
D
ap-
plies the patterns introduced in (Fader et al., 2011)
to document sentences, whereas run R
Q
the pat-
terns shown in Table 1 to queries. To more ac-
curately gauge the role of queries vs. documents
in extracting facts from unstructured text, addi-
tional experiments isolate the effect of extracting
facts from different types of data sources. For
this purpose, the same set of patterns from Ta-
ble 1 is matched against the sentences from around
500 million Web documents. The patterns are ap-
plied to document sentences converted to lower-
case, similarly to how they are applied to queries.
This corresponds to a new experimental run R
DS
,
which employs the same patterns as the earlier run
R
Q
but runs over document sentences instead of
queries.
As an average over the target classes, the pre-
cision of facts extracted by run R
DS
is 0.50, 0.47
and 0.44 at ranks 10, 20 and 50 respectively. Two
conclusions can be drawn from comparing these
scores with the average scores from the earlier Ta-
ble 4. First, the average precision of run R
DS
is
higher than for run R
D
. In other words, when
extracting from document sentences in R
DS
and
R
D
, the patterns proposed in our method give
fewer and more accurate facts than the patterns
from (Fader et al., 2011). Second, although R
DS
is
more accurate than R
D
, it is less accurate than run
R
Q
. Note that, among the top 50 facts extracted
for each target class by runs R
DS
and R
Q
, an aver-
age of 13% of the facts are extracted by both runs.
There are several phenomena contributing to the
difference in precision. While inherently noisy,
queries tend to be more compact, and therefore
more focused. In comparison, document sentences
matching the patterns are often more convoluted
(e.g., ?who do cities keep building stadiums de-
spite study after study showing they do not make
money?, or ?how does a company go from low
associate satisfaction to #15 on the fortune 100
best list in the midst of a crippling recession?).
Furthermore, both queries and sentences may not
be useful questions from which relevant facts can
be extracted, even when they match the extraction
patterns. However, anecdotal evidence suggests
1087
that this happens more frequently with document
sentences than with queries. Examples include
document sentences extracted from sites aggregat-
ing jokes (?why did the cell phone ask to see the
psychologist?). The results confirm that queries
represent an intriguing resource for fact extraction,
providing a useful complement to document sen-
tences for the purpose of extracting facts.
Quantitative Results: From the set of queries
used as input in run R
Q
, 3.8% of all queries start
with why or how. In turn, 13.6% of them match
one of the extraction patterns from Table 1, and
therefore produce a candidate fact in R
Q
. In the
case of run R
DS
, 18.7% of the document sentences
that start with why or how match one of the pat-
terns from Table 1.
Choice of Extraction Patterns: The sets of pat-
terns sometimes employed in relation extraction
from documents (Hearst, 1992) occasionally ben-
efit from the addition of new patterns, or the re-
finement into more specific patterns (Kozareva et
al., 2008). Similarly, the set of patterns proposed
in Table 1, which targets the extraction of facts
from queries, is neither exhaustive nor final. Other
patterns beyond why and how may prove useful,
whether they rely on relatively less frequent when
and where queries, or extract relations contain-
ing underspecified arguments from who or what
queries.
When applied to queries in run R
Q
, the how pat-
terns from Table 1 match 3.3 times more queries
than the why patterns.
In separate experiments, why vs. how patterns
from Table 1 are temporarily disabled. The ra-
tio of facts extracted on average per target class in
run R
Q
diminishes from 100% (with both patterns)
to 30% (with why only) or 70% (with how only).
Overall, no difference in accuracy is observed over
facts extracted by why vs. how patterns.
Choice of Phrase Descriptors: A separate experi-
ment investigates the impact of expanding the sets
of phrase descriptors associated with each target
class. Among many possible strategies, each set of
phrase descriptors associated with a target class is
expanded automatically, using WordNet and dis-
tributional similarities. For this purpose, for each
target class, the set of synonyms and hyponyms of
all senses, if any, available in WordNet for each
phrase descriptor is intersected with the set of the
50 most distributionally similar phrases, if any,
available for each phrase descriptor. The origi-
nal set of phrase descriptors of each target class
is then expanded, to include the phrases from the
intersected set, if any.
A repository of distributionally similar phrases
is collected in advance following (Lin and Wu,
2009; Pantel et al., 2009), from a sample of around
200 million Web documents. Their intersection
with phrases collected from WordNet aims at re-
ducing the noise associated with expansion solely
from either source. For example, for the class
Actor, the set of phrases {player, worker, heavy,
plant, actress, comedian, film star, ..} is collected
from WordNet for the descriptor actors. The set
is intersected with the set of phrases {film stars,
performers, comedians, actresses, ..} most dis-
tributionally similar to actors. Examples of sets
of phrase descriptors after expansion are {actors,
actresses, comedians, players, film stars, ..}, for
the class Actor; and {battles, naval battles, fights,
skirmishes, struggles, ..}, for Battle.
On average, the sets of phrase descriptors as-
sociated with each target class contains 2 vs. 11
phrases, before vs. after expansion. Some of the
sets of phrase descriptors, such as for the target
classes CartoonChar and DigitalCamera, remain
unchanged after expansion. As expected, expan-
sion may introduce noisy phrase descriptors, such
as players for Actor, or diets for Food. The pres-
ence of noisy phrase descriptors lowers the preci-
sion of the extracted facts. After expansion, the
precision scores of R
Q
, as an average over all tar-
get classes, become smaller by 6% (0.71 vs. 0.67),
at rank 10; 6% (0.67 vs. 0.63), at rank 20; and 7%
(0.63 vs. 0.59), at rank 50. Expansion also affects
relative coverage, increasing the average number
of facts extracted by R
Q
per target class by more
than twice (i.e., by a factor of 2.6).
Redundant Facts: Due to lexical variation in
the source text fragments, some of the extracted
facts may be near-duplicates of one another. In
general, the phenomenon affects facts extracted
from text by previous methods (Van Durme and
Pas?ca, 2008; Etzioni et al., 2011; Fader et al.,
2011). In particular, it affects facts extracted from
both documents or queries in our experiments.
For example, the facts extracted from documents
for Actor include ?play their roles?, ?play their
parts?, ?play their characters? and ?portrayed
their characters?. Separately, the facts ?memorize
their lines?, ?remember their lines? and ?learn
their lines? are extracted from queries for the class
1088
Actor. The automatic detection of equivalent facts
would increase the usefulness of facts extracted
from text in general, and of facts extracted by the
method presented here in particular.
5 Related Work
A variety of methods address the more general
task of acquisition of open-domain relations from
text, e.g., (Banko et al., 2007; Carlson et al., 2010;
Wu and Weld, 2010; Fader et al., 2011; Lao et
al., 2011; Mausam et al., 2012; Lopez de La-
calle and Lapata, 2013). In general, relations ex-
tracted from document sentences (e.g., ?Claude
Monet was born in Paris?) are tuples of an argu-
ment (claude monet), a text fragment acting as the
lexicalized relation (was born in), and another ar-
gument (paris) (cf. (Banko et al., 2007; Fader et
al., 2011; Mausam et al., 2012)). For convenience,
the relation and second argument may be concate-
nated into a fact applying to the first argument, as
in ?was born in paris? for claude monet. Rel-
atively shallow tools like part of speech taggers,
or more complex tools like semantic taggers (Van
Durme et al., 2008; Van Durme et al., 2009) can be
employed in order to extract relations from docu-
ment sentences. The former choice scales better
to Web documents of arbitrary quality, whereas
the latter could be more accurate over high-quality
documents such as news articles (Mesquita et
al., 2013). In both cases, document sentences
mentioning an instance or a class may refer to
properties of the instance that people other than
the author of the document are less likely to in-
quire about. Consequently, even top-ranked ex-
tracted relations occasionally include less infor-
mative ones, such as ?come into view? for mount
rainier, ?be on the table? for madeira wine, or
?allow for features? for javascript (Fader et al.,
2011).
Data available within Web documents, from
which relations are extracted in previous work,
includes unstructured (Banko et al., 2007; Fader
et al., 2011), structured (Raju et al., 2008) and
semi-structured text (Yoshinaga and Torisawa,
2007; Pasupat and Liang, 2014), layout format-
ting tags (Wong et al., 2008), itemized lists or ta-
bles (Cafarella et al., 2008). Another source is
human-compiled resources (Wu and Weld, 2010)
including infoboxes and category labels (Nastase
and Strube, 2008; Hoffart et al., 2013; Wang et
al., 2013; Flati et al., 2014) in Wikipedia, or topics
and relations in Freebase (Weston et al., 2013; Yao
and Van Durme, 2014).
Whether Web search queries are a useful tex-
tual data source for open-domain information ex-
traction has been investigated in several tasks. Ex-
amples are collecting unlabeled sets of similar in-
stances (Jain and Pennacchiotti, 2010), extract-
ing attributes of instances (Alfonseca et al., 2010;
Pas?ca, 2014), identifying mentions in queries
of instances defined in a manually-created re-
source (Pantel et al., 2012), and extracting the
most salient of the instances mentioned within
Web documents (Gamon et al., 2013).
Other previous work shares the intuition that the
submission of Web search queries is influenced
by, and indicative of, various relations. Relations
are loosely defined, either by approximating them
via distributional similarities (Alfonseca et al.,
2009), or by exploring the acquisition of untyped,
similarity-based relations from query logs (Baeza-
Yates and Tiberi, 2007). In both cases, the com-
puted relations hold among full-length queries.
Untyped relations can also be identified among
query terms for the purpose of query reformula-
tion (Wang and Zhai, 2008). More generally, the
choice of query substitutions may reveal various
relations among full queries or query terms (Jones
et al., 2006), but requires individual queries to be
connected to one another via query sessions or via
search-result click-through data.
6 Conclusion
Anonymized search queries submitted by Web
users represent requests for knowledge. Collec-
tively, they can also be seen as informal, lexi-
calized knowledge assertions. By asking about a
property of some class, fact-seeking queries im-
plicitly assert the relevance of the property for the
class.
Since Web search queries refer to properties
that Web users are collectively interested in, fac-
tual knowledge extracted from queries tends to be
more relevant than facts extracted from arbitrary
documents using previous methods. Current work
explores the extraction of facts from implicit rather
than explicit fact-seeking questions, that is, from
queries that do not start with a question prefix; and
the combination of queries as a source of more ac-
curate facts, and documents as a source of more
numerous facts.
1089
References
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-scale
computation of distributional similarities for queries. In
Proceedings of the 2009 Conference of the North Amer-
ican Association for Computational Linguistics (NAACL-
HLT-09), Short Papers, pages 29?32, Boulder, Colorado.
E. Alfonseca, M. Pas?ca, and E. Robledo-Arnuncio. 2010.
Acquisition of instance attributes via labeled and related
instances. In Proceedings of the 33rd International Con-
ference on Research and Development in Information Re-
trieval (SIGIR-10), pages 58?65, Geneva, Switzerland.
R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic
relations from query logs. In Proceedings of the 13th
ACM Conference on Knowledge Discovery and Data Min-
ing (KDD-07), pages 76?85, San Jose, California.
M. Banko, Michael J Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction from
the Web. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI-07), pages
2670?2676, Hyderabad, India.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.
2008. Freebase: A collaboratively created graph database
for structuring human knowledge. In Proceedings of the
2008 International Conference on Management of Data
(SIGMOD-08), pages 1247?1250, Vancouver, Canada.
L. Brown, T. Cai, and A. DasGupta. 2001. Interval es-
timation for a binomial proportion. Statistical Science,
16(2):101?117.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on the
Web. In Proceedings of the 34th Conference on Very
Large Data Bases (VLDB-08), pages 538?549, Auckland,
New Zealand.
A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and
T. Mitchell. 2010. Coupled semi-supervised learning for
information extraction. In Proceedings of the 3rd ACM
Conference on Web Search and Data Mining (WSDM-10),
pages 101?110, New York.
C. Castillo and B. Davison. 2011. Adversarial web search.
Journal of Foundations and Trends in Information Re-
trieval, 4(5):377?486.
J. Dalton, L. Dietz, and J. Allan. 2014. Entity query feature
expansion using knowledge base links. In Proceedings of
the 37th International Conference on Research and Devel-
opment in Information Retrieval (SIGIR-14), pages 365?
374, Gold Coast, Queensland, Australia.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locating
complex named entities in Web text. In Proceedings of
the 20th International Joint Conference on Artificial Intel-
ligence (IJCAI-07), pages 2733?2739, Hyderabad, India.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked,
S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised
named-entity extraction from the Web: an experimental
study. Artificial Intelligence, 165(1):91?134.
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
Mausam. 2011. Open information extraction: The second
generation. In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI-11), pages 3?
10, Barcelona, Spain.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proceedings
of the 2011 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-11), pages 1535?1545,
Edinburgh, Scotland.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical
Database and Some of its Applications. MIT Press.
T. Flati, D. Vannella, T. Pasini, and R. Navigli. 2014. Two is
bigger (and better) than one: the Wikipedia Bitaxonomy
project. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL-14),
pages 945?955, Baltimore, Maryland.
M. Gamon, T. Yano, X. Song, J. Apacible, and P. Pantel.
2013. Identifying salient entities in web pages. In Pro-
ceedings of the 22nd International Conference on Infor-
mation and Knowledge Management (CIKM-13), pages
2375?2380, Burlingame, California.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415?
422, Barcelona, Spain.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
International Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. 2013.
YAGO2: a spatially and temporally enhanced knowledge
base from Wikipedia. Artificial Intelligence Journal. Spe-
cial Issue on Artificial Intelligence, Wikipedia and Semi-
Structured Resources, 194:28?61.
A. Jain and M. Pennacchiotti. 2010. Open entity extrac-
tion from Web search query logs. In Proceedings of the
23rd International Conference on Computational Linguis-
tics (COLING-10), pages 510?518, Beijing, China.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gener-
ating query substitutions. In Proceedings of the 15h World
Wide Web Conference (WWW-06), pages 387?396, Edin-
burgh, Scotland.
Z. Kozareva and E. Hovy. 2010. A semi-supervised method
to learn and construct taxonomies using the web. In Pro-
ceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing (EMNLP-10), pages 1110?
1118, Cambridge, Massachusetts.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
class learning from the Web with hyponym pattern link-
age graphs. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL-08),
pages 1048?1056, Columbus, Ohio.
N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk in-
ference and learning in a large scale knowledge base. In
Proceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-11), pages
529?539, Edinburgh, Scotland.
D. Lin and X. Wu. 2009. Phrase clustering for discrimina-
tive learning. In Proceedings of the 47th Annual Meeting
of the Association for Computational Linguistics (ACL-
IJCNLP-09), pages 1030?1038, Singapore.
O. Lopez de Lacalle and M. Lapata. 2013. Unsupervised re-
lation extraction with general domain knowledge. In Pro-
ceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP-13), pages 415?
425, Seattle, Washington.
Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni.
2012. Open language learning for information extraction.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL-12),
pages 523?534, Jeju Island, Korea.
1090
F. Mesquita, J. Schmidek, and D. Barbosa. 2013. Effective-
ness and efficiency of open relation extraction. In Pro-
ceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP-13), pages 447?
457, Seattle, Washington.
V. Nastase and M. Strube. 2008. Decoding Wikipedia cat-
egories for knowledge acquisition. In Proceedings of
the 23rd National Conference on Artificial Intelligence
(AAAI-08), pages 1219?1224, Chicago, Illinois.
M. Pas?ca. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of the 16th World Wide Web Con-
ference (WWW-07), pages 101?110, Banff, Canada.
M. Pas?ca. 2014. Acquisition of noncontiguous class at-
tributes from Web search queries. In Proceedings of the
14th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-14), pages
386?394, Gothenburg, Sweden.
P. Pantel and A. Fuxman. 2011. Jigs and lures: Associating
web queries with structured entities. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics (ACL-11), pages 83?92, Portland, Oregon.
P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and V. Vyas.
2009. Web-scale distributional similarity and entity
set expansion. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing
(EMNLP-09), pages 938?947, Singapore.
P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity types
from query logs via user intent modeling. In Proceedings
of the 50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-12), pages 563?571, Jeju Island,
Korea.
P. Pasupat and P. Liang. 2014. Zero-shot entity extraction
from Web pages. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguistics
(ACL-14), pages 391?401, Baltimore, Maryland.
M. Pennacchiotti and P. Pantel. 2009. Entity extraction via
ensemble semantics. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP-09), pages 238?247, Singapore.
S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing. In
Proceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-10), pages
705?713, Cambridge, Massachusetts.
S. Raju, P. Pingali, and V. Varma. 2008. An unsupervised ap-
proach to product attribute extraction. In Proceedings of
the 31st International Conference on Research and Devel-
opment in Information Retrieval (SIGIR-08), pages 35?42,
Singapore.
M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.
S. Sekine and H. Suzuki. 2007. Acquiring ontological
knowledge from query logs. In Proceedings of the 16th
World Wide Web Conference (WWW-07), Posters, pages
1223?1224, Banff, Canada.
K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Automatic
discovery of attribute words from Web documents. In
Proceedings of the 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05), pages 106?
118, Jeju Island, Korea.
B. Van Durme and M. Pas?ca. 2008. Finding cars, goddesses
and enzymes: Parametrizable acquisition of labeled in-
stances for open-domain information extraction. In Pro-
ceedings of the 23rd National Conference on Artificial In-
telligence (AAAI-08), pages 1243?1248, Chicago, Illinois.
B. Van Durme, T. Qian, and L. Schubert. 2008. Class-
driven attribute extraction. In Proceedings of the 22nd
International Conference on Computational Linguistics
(COLING-08), pages 921?928, Manchester, United King-
dom.
B. Van Durme, P. Michalak, and L. Schubert. 2009. Deriv-
ing generalized knowledge from corpora using Wordnet
abstraction. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computational
Linguistics (EACL-09), pages 808?816, Athens, Greece.
R. Wang and W. Cohen. 2009. Automatic set instance ex-
traction using the Web. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguistics
(ACL-IJCNLP-09), pages 441?449, Singapore.
X. Wang and C. Zhai. 2008. Mining term association pat-
terns from search logs for effective query reformulation.
In Proceedings of the 17th International Conference on In-
formation and Knowledge Management (CIKM-08), pages
479?488, Napa Valley, California.
Z. Wang, Z. Li, J. Li, J. Tang, and J. Pan. 2013. Trans-
fer learning based cross-lingual knowledge extraction for
Wikipedia. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (ACL-13),
pages 641?650, Sofia, Bulgaria.
J. Weston, A. Bordes, O. Yakhnenko, and N. Usunier. 2013.
Connecting language and knowledge bases with embed-
ding models for relation extraction. In Proceedings of the
2013 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13), pages 1366?1371, Seat-
tle, Washington.
T. Wong, W. Lam, and T. Wong. 2008. An unsuper-
vised framework for extracting and normalizing product
attributes from multiple Web sites. In Proceedings of the
31st International Conference on Research and Develop-
ment in Information Retrieval (SIGIR-08), pages 35?42,
Singapore.
F. Wu and D. Weld. 2010. Open information extraction using
Wikipedia. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-10),
pages 118?127, Uppsala, Sweden.
W. Wu, , H. Li, H. Wang, and K. Zhu. 2012. Probase: a prob-
abilistic taxonomy for text understanding. In Proceedings
of the 2012 International Conference on Management of
Data (SIGMOD-12), pages 481?492, Scottsdale, Arizona.
X. Yao and B. Van Durme. 2014. Information extraction
over structured data: Question Answering with Freebase.
In Proceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-14), pages 956?
966, Baltimore, Maryland.
N. Yoshinaga and K. Torisawa. 2007. Open-domain
attribute-value acquisition from semi-structured texts. In
Proceedings of the 6th International Semantic Web Con-
ference (ISWC-07), Workshop on Text to Knowledge: The
Lexicon/Ontology Interface (OntoLex-2007), pages 55?
66, Busan, South Korea.
L. Zang, C. Cao, Y. Cao, Y. Wu, and C. Cao. 2013. A sur-
vey of commonsense knowledge acquisition. Journal of
Computer Science and Technology, 28(4):689?719.
1091
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 386?394,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Acquisition of Noncontiguous Class Attributes from Web Search Queries
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
Previous methods for extracting attributes
(e.g., capital, population) of classes (Em-
pires) from Web documents or search
queries assume that relevant attributes oc-
cur verbatim in the source text. The ex-
tracted attributes are short phrases that
correspond to quantifiable properties of
various instances (ottoman empire, ro-
man empire, mughal empire) of the class.
This paper explores the extraction of non-
contiguous class attributes (manner (it)
claimed legitimacy of rule), from fact-
seeking and explanation-seeking queries.
The attributes cover properties that are
not always likely to be extracted as short
phrases from inherently-noisy queries.
1 Introduction
Motivation: Resources such as Wikipedia (Remy,
2002) and Freebase (Bollacker et al., 2008) aim
at organizing knowledge around classes (Food in-
gredients, Astronomical objects, Religions) and
their instances (wheat flower, uranus, hinduism).
Due to inherent limitations associated with main-
taining and expanding human-curated resources,
their content may be incomplete. For example,
attributes representing the energy (or energy per
100g) or solubility in water are available in both
Wikipedia and Freebase for many instances of
Food ingredients (e.g., for olive oil, honey, fennel).
But the attributes are missing for some instances
(e.g., cornmeal). Moreover, structured informa-
tion about how long (it) lasts unopened or manner
(it) helps in weight loss is generally missing for
Food ingredients, from both resources. Such in-
formation is also often absent from among the at-
tributes acquired from either documents or queries
by previous extraction methods (Pas?ca et al., 2007;
Van Durme et al., 2008). Previously extracted at-
tributes tend to be short, often nominal, phrases
like nutritional value and taste. Even when ex-
tracted attributes are not nominal (Pas?ca, 2012),
they remain relatively short phrases such as good
for skin. As such, previous attributes have limited
ability to capture the finer-grained properties be-
ing asked about in queries such as ?how long does
olive oil last unopened? and ?how does honey help
in weight loss?. The presence of such queries
suggests that such information is relevant to Web
users. Identifying noncontiguous properties, or
attributes of interest to Web users, helps filling
some of the gaps in existing knowledge resources,
which otherwise could not be filled by attributes
extracted with previous methods.
Contributions: The contributions of this paper
are twofold. First, it introduces a method for the
acquisition of noncontiguous class attributes, from
fact or explanation-seeking Web search queries
like ?how long does olive oil last unopened? or
?how does honey help in weight loss?. The re-
sulting attributes are more diverse than, and there-
fore subsume, the scope of attributes extracted
by previous methods. Indeed, previous meth-
ods are unlikely to extract attributes as specific
as length/duration (it) lasts unopened and man-
ner (it) helps in weight loss, for the instances olive
oil and honey of the class Food ingredients. Con-
versely, previously extracted attributes like nutri-
tional value and solubility in water are roughly
equivalent to the finer-grained nutritional value
(it) has and reason (it) dissolves in water, ex-
tracted from the queries ?what nutritional value
does honey have? and ?why does glucose dissolve
in water? respectively. Second, the noncontiguous
attributes can be simultaneously interpreted as bi-
nary relations pertaining to instances and classes.
The relations (helps in weight loss) connect an in-
stance (honey) or, more generally, a class (Food
ingredients), on one hand; and a loosely-typed un-
known argument (manner) whose value is of in-
terest to Web users, on the other hand. Because
386
Web users already inquire about the value of one
of their arguments, the extracted relations are more
likely to be relevant for the respective instances
and classes, than relations extracted from arbitrary
document sentences (Fader et al., 2011).
2 Noncontiguous Attributes
Intuitions: Users tend to formulate their Web
search queries based on knowledge that they al-
ready possess at the time of the search (Pas?ca,
2007). Therefore, search queries play two roles
simultaneously: in addition to requesting new in-
formation, they indirectly convey knowledge in
the process. In particular, attributes correspond
to quantifiable properties of instances and their
classes. The extraction of attributes from queries
starts from the intuition that, if an attribute A is rel-
evant for a class C, then users are likely to ask for
the value of the attribute A, for various instances
I of the class C. If nutritional value and diameter
are relevant attributes of the classes Food ingre-
dients and Astronomical objects respectively, it is
likely that users submit queries to inquire about
the values of the attributes for instances of the
two classes. Such queries could take the form
?what is the (nutritional value)
A
of (olive oil)
I
?
and ?what is the (diameter)
A
of (jupiter)
I
?; or
the more compact ?(nutritional value)
A
of (olive
oil)
I
? and ?(diameter)
A
of (jupiter)
I
?. In this
case, the attributes are relatively short phrases
(nutritional value, diameter), and are expected to
appear as contiguous phrases within queries. Pre-
vious methods on attribute extraction from queries
specifically target this type of attributes. In fact,
some methods apply dedicated extraction patterns
(e.g., A of I) over either queries (Pas?ca et al.,
2007) or documents (Tokunaga et al., 2005). Other
methods expand manually-provided seed sets of
attributes, with other phrases that co-occur with
instances within queries, in similar contexts as the
seed attributes do (Pas?ca, 2007).
While simpler properties are often mentioned in
queries as short, contiguous phrases, finer-grained
properties often are not. Queries seeking the rea-
son for solidification for some Food ingredients
could, but rarely do, contain the attribute ver-
batim (?what is the reason for the solidification
of honey?). Instead, queries are more likely to
inquire about the expected value, while specify-
ing the instance and the properties encoded by
the attribute (?(why)
A
does (honey)
I
(solidify)
A
?).
Readable descriptions (names) of the attributes
can be recovered from the queries, by assembling
the type of the expected value and the proper-
ties together (reason (it) solidifies). Thus, fact
and explanation-seeking queries are an intriguing
source of noncontiguous attributes that are not re-
stricted to short phrases, and are not required to
occur as contiguous phrases in queries.
Acquisition from Queries: The extraction
method proposed in this paper takes as input a set
of target classes, each of which is available as a
set of instances that belong to the class; and a set
of anonymized queries independent from one an-
other. As illustrated in Figure 1, the method se-
lects queries that contain an instance of a class
together with what is deemed to be likely a non-
contiguous attribute, and outputs ranked lists of
attributes for each class. The extraction consists
in several stages:
? selection of a subset of queries that contain
an instance in a form that suggests the queries ask
for the value of a noncontiguous attribute of the
instance;
? extraction of noncontiguous attributes, from
query fragments that describe the property of in-
terest and the type of its expected value;
? aggregation and ranking of attributes of in-
dividual instances of a class, into attributes of a
class.
Extraction Patterns: In order to determine
whether a query contains an attribute of a class,
the query is matched against the extraction pat-
terns from Table 1. The use of patterns in attribute
extraction has been previously suggested in (Pas?ca
et al., 2007; Tokunaga et al., 2005), where the pat-
tern what is the A of I extracts noun-phrase A
attributes of instances I from queries and docu-
ments. In our case, the patterns are constructed
such that they match fact-seeking and explanation-
seeking questions that likely inquire about the
value of a relevant property of an instance I of the
class C. For example, the first pattern from Ta-
ble 1 matches queries such as ?when did everquest
become free to play? and ?when was radon dis-
covered as an element?, which inquire about the
date or time when certain events affected certain
properties of the instances everquest and radon re-
spectively. Instances I of the class C may be avail-
able as non-disambiguated items, that is, as strings
(java) whose meaning is otherwise unknown; or
as disambiguated items, that is, as strings associ-
387
Query logs
who discovered the element iron what family does zinc belong to in the periodic table
when was radon discovered as an element   how does oxygen return to the atmosphere
why does chlorine react with water   what elements does argon combine with
how does javascript run   who created haskell   how does java execute
who invented the programming language cobol   how long does python take to learn
how does java compile   when was c# first released   where does python install to
how does c# differ from c++   how does javascript store dates
when did minecraft come out for xbox 360   when did everquest become free to play
who does the voice in black ops 2   when did league of legends become free to play
when was fable 2 released   how much does world of warcraft cost to play online
Extracted class attributes
Chemical elements: {
who can you unlock in band hero   how many copies did halo reach sell the first day
  date/time (it) was discovered as an element, manner (it) returns to the atmosphere,
  who discovered the element, manner (it) enters the soil, reason (it) reacts with water,
  elements (it) combines with, manner (it) reacts with other elements, 
  family (it) belongs to in the periodic table, number of electrons (it) gains, ...}
Target classes
Programming languages: {
  manner (it) executes, length/duration (it) takes to learn, file extension (it) uses, ...}
  manner (it) differs from c++, manner (it) compiles, manner (it) stores dates,
  who is using (it), location (it) installs to, date/time (it) was first released,
  manner (it) runs, who created (it), who invented the programming language,
  date/time (it) was released, number of copies (it) sold first day,
Video games: {
  date/time (it) came out for xbox 360, date/time it came out for ps2,
  price/quantity/degree (it) costs to play online, date/time (it) became free to play, ...}
  date/time (it) came out for pc, who does the voice in (it), who can you unlock in (it),
Chemical elements: {radon, chlorine, argon, nitrogen, oxygen, carbon,
  hydrogen, iron, zinc, ...}
  cobol, lisp, actionscript, ...}
Video games: {minecraft, black ops II, league of legends, halo reach, everquest,
  fable 2, world of warcraft, band hero, ...}
Programming languages: {c#, javascript, haskell, json, perl, java, python, prolog,
how many electrons does chlorine gain   who is using lisp
how does oxygen interact with other elements   how does nitrogen enter the soil
Figure 1: Overview of extraction of noncontiguous attributes from Web search queries
ated with pointers to knowledge base entries with a
disambiguated meaning (Java (programming lan-
guage)). In the first case, the matching of a query
fragment, on one hand, to the portion of an ex-
traction pattern corresponding to an instance I , on
the other hand, consists in simple string match-
ing. In the second case, the matching requires
that the disambiguation of the query fragment, in
the context of the query, matches the desired dis-
ambiguated meaning of I from the pattern. The
subset of queries matching any of the extraction
patterns, for any instances I of a class C, are the
queries that contribute to extracting noncontigu-
ous attributes of the class C.
Collecting Attributes of Individual Instances:
A small set of rules optionally converts wh-
prefixes into coarse-grained types of the expected
values (e.g., how long into length/duration; or
when into date/time). In the case of what-prefixed
queries, the adjacent noun phrase, if any, is con-
sidered to be the expected type (?what nutritional
value ..? into nutritional value). Similar rules
have been employed for shallow analysis of open-
domain questions (Dumais et al., 2002). The pred-
icate verbs in the remainder of the query are up-
dated, to match the tense specified by the auxil-
iary verb (e.g., ?when did ..?), if any, following
the wh-prefix. Thus, the verb come is converted
to the past tense came, in the case of the query
?when did minecraft come out for xbox 360?. An
388
Extraction Pattern
? Examples of Matched Queries
when [does|did|do|was|were] [a|an|the|<nothing>] I A
? when did everquest become free to play
why [does|did|do|was|were] [a|an|the|<nothing>] I A
? why does chlorine interact with water
where [does|did|do|was|were] [a|an|the|<nothing>] I A
? where does radon occur naturally
how [does|did|do|was|were] [a|an|the|<nothing>] I A
? how does nitrogen enter the soil
who [does|did|do|was|were] [a|an|the|<nothing>] I A
? who did claude monet study under
how A [does|did|do|was|were] [a|an|the|<nothing>] I A
? how fast does oxygen dissolve in water
who A I
? who invented the programming language cobol
(Note: A does not start with [is|are|was|were])
what A [does|did|do|was|were] [a|an|the|<nothing>] I A
? what elements does argon combine with
whichA [does|did|do|was|were] [a|an|the|<nothing>] I A
? which ports does minecraft use
Table 1: The extraction patterns match queries that
are likely to inquire about the value of a noncon-
tiguous attribute of an instance (I=a required in-
stance; A=a required non-empty sequence of arbi-
trary tokens)
attribute is constructed from the concatenation of
the wh-prefix or expected type (date/time); the
slot pronoun it, in lieu of the instance (date/time
(it)); and the query remainder after tense conver-
sion (date/time (it) came out for xbox 360). If the
linking verb following the wh-prefix is a form of
be (e.g., was), then the linking verb is also re-
tained after the slot pronoun, to form a more co-
herent attribute (date/time (it) was first released).
Since constructed attributes are noun phrases, they
are more consistent with, and can be more eas-
ily inserted among, existing attributes in struc-
tured data repositories (infobox entries of articles
in Wikipedia, or property names or topics in Free-
base).
Aggregation into Class Attributes: Attributes of
a class C are aggregated from attributes of indi-
vidual instances I of the class. An attribute A
is deemed more relevant for C if the attribute is
extracted for more of the instances I of the class
C, and for fewer instances I that do not belong to
the class C. Concretely, the score of an attribute
for a class is the lower bound of the Wilson score
interval (Brown et al., 2001) where the number
of positive observations is the number of queries
for which the attribute A is extracted for some in-
stance I in the class C, |{Query(I, A)}
I?C
|; and
the number of negative observations is the num-
ber of queries for which the attribute A is ex-
tracted for some instances I outside of the class
C, |{Query(I, A)}
I /?C
|. The scores are internally
computed at 95% confidence. Attributes of each
class are ranked in decreasing order of their scores.
Reduction of Near-Duplicate Attributes: Due to
lexical variations across queries from which at-
tributes are extracted, some of the attributes are
equivalent or nearly equivalent to one another. For
example, gained independence, won its indepen-
dence and gained its freedom of the class Coun-
tries are roughly equivalent, although they employ
distinct tokens. The diversity and potential useful-
ness of a ranked list of attributes can be increased,
if groups of near-duplicate attributes are identified
in the list, and merged together.
A lower-ranked attribute is marked as a near-
duplicate of a higher-ranked (i.e., earlier) attribute
from the list, if all tokens from the lower-ranked
attribute match either tokens from the higher-
ranked attribute (gained independence vs. won
its independence), or tokens from synonyms of
phrases from the earlier attribute (gained indepen-
dence vs. won its independence; or takes to show
symptoms vs. takes to come out). Stop words,
which include linking verbs, pronouns, determin-
ers, conjunctions, wh-prefixes and prepositions,
are not required to match. Synonyms may be ei-
ther derived from existing lexical resources (e.g.,
WordNet (Fellbaum, 1998)), or mined from large
document collections (Madnani and Dorr, 2010).
Lower-ranked near-duplicate attributes are merged
with the higher-ranked ones from the ranked list,
thus improving the diversity of the list.
3 Experimental Setting
Textual Data Sources: The experiments rely
on a random sample of around 1 billion fully-
anonymized queries in English, submitted to a
general-purpose Web search engine. Each query
is available independently from other queries, and
is accompanied by its frequency of occurrence in
the query logs.
Target Classes: Table 2 shows the set of 40 tar-
get classes for evaluating the attributes extracted
from queries. In an effort to reuse experimental
setup proposed in previous work, each of the 40
manually-compiled classes introduced in (Pas?ca,
2007) is mapped into the Wikipedia category that
best matches it. For example, the evaluation
classes Aircraft Model, Movie, Religion and Ter-
389
Class (Examples of Instances)
Actors (keanu reeves, milla jovovich, ben affleck), Air-
craft (boeing 737, bombardier crj200, embraer 170), An-
imated characters (bugs bunny, pink panther (character),
yosemite sam), Association football clubs (a.s. roma, flu-
minense football club, real madrid), Astronomical objects
(alpha centauri, jupiter, delta corvi), Automobiles (nis-
san gt-r, tesla model s, toyota prius), Awards (grammy
award, justin winsor prize (library), palme d?or), Battles
and operations of world war ii (battle of midway, opera-
tion postmaster, battle of milne bay), Chemical elements
(plutonium, radon, hydrogen), Cities (rio de janeiro, os-
aka, chiang mai), Companies (best buy, aveeno, pep-
sico), Countries (costa rica, rwanda, south korea), Cur-
rencies by country (japanese yen, swiss franc, korean
won), Digital cameras (canon eos 400d, nikon d3000,
pentax k10d), Diseases and disorders (anorexia nervosa,
hyperlysinemia, repetitive strain injury), Drugs (flutica-
sone propionate, phentermine, tramadol), Empires (ot-
toman empire, roman empire, mughal empire), Films (the
fifth element, mockingbird don?t sing, ten thousand years
older), Flowers (trachelospermum jasminoides, lavandula
stoechas, evergreen rose), Food ingredients (carrot, olive
oil, fennel), Holidays (good friday, easter, halloween),
Hurricanes in North America (hurricane katrina, hurri-
cane wilma, hurricane dennis), Internet search engines
(google, baidu, lycos), Mobile phones (nokia n900, htc
desire, samsung s5560), Mountains (mount rainier, cerro
san luis obispo, steel peak), National Basketball Associa-
tion teams (los angeles lakers, cleveland cavaliers, indiana
pacers), National parks (yosemite national park, orang na-
tional park, tortuguero national park), Newspapers (the
economist, corriere del trentino, seattle medium), Organi-
zations designated as terrorist (taliban, shining path, eta),
Painters (claude monet, domingo antonio velasco, tarci-
sio merati), Programming languages (javascript, prolog,
obliq), Religious faiths traditions and movements (con-
fucianism, fudoki, omnism), Rivers (danube, pingo river,
viehmoorgraben), Skyscrapers (taipei 101, 15 penn plaza,
eqt plaza), Sports events (tour de france, 1984 scottish cup
final, rotlewi versus rubinstein), Stadiums (fenway park,
chengdu longquanyi, stade geoffroy-guichard), Treaties
(treaty of versailles, franco-indian alliance, treaty of cor-
doba), Universities and colleges (cornell university, nu-
gaal university, gale college), Video games (minecraft,
league of legends, everquest), Wine (madeira wine, yel-
low tail (wine), port wine)
Table 2: Set of 40 Wikipedia categories used as
target classes in the evaluation of attributes
roristGroup from (Pas?ca, 2007) are mapped into
the Wikipedia categories Aircraft, Films, Religious
faiths traditions and movements and Organiza-
tions designated as terrorist respectively. The
name of the Wikipedia category only serves as a
convenience label for its target class, and is not
otherwise exploited in any way during the evalua-
tion. Instead, a target class consists in a set of titles
of Wikipedia articles, of which sample titles (e.g.,
the Wikipedia article titled nissan gt-r) are shown
in lowercase for each class (e.g., Automobiles) in
Table 2. The set of instances of a class is selected
from all articles listed under the respective cate-
Label Examples of Attributes
vital Astronomical objects: manner (it) generates its
energy
Food ingredients: temperature (it) solidifies
Religion: date/time (it) became a religion
okay Astronomical objects: manner (it) became a
constellation
Food ingredients: reason (it) sparks in the mi-
crowave
Religion: manner (it) feels about abortion
wrong Astronomical objects: reason (it) has arms
Food ingredients: manner (it) cleans pennies
Religion: who owns (it)
Table 3: Correctness labels manually assigned to
attributes extracted for various classes
gory in Wikipedia, or listed under sub-categories
of the respective category.
The target classes contain between 41 (for Na-
tional Basketball Association teams) and 66,934
(for Films) instances, with an average of 10,730
instances per class.
Synonym Repository: A synonym repository ex-
tracted separately from Web documents contains
mappings from each of around 60,000 phrases in
English, to lists of their synonym phrases. For ex-
ample, the top synonyms available for the phrases
turn off and contagious are [switch off, extinguish,
turn out, ..] and [infectious, catching, communica-
ble, ..] respectively.
Parameter Settings: Queries that match any of
the extraction patterns from Table 1 are syntac-
tically parsed (Petrov et al., 2010). As a pre-
requisite, the portion I of the patterns from the
table must match a disambiguated instance from
a query.
A variation of the tagger introduced
in (Cucerzan, 2007) maps query fragments
to their disambiguated, corresponding Wikipedia
instances (i.e., to Wikipedia articles). The tagger
is simplified to select the longest instance men-
tions, and does not use gazetteers or queries for
training. Depending on the sources of textual
data available for training, any taggers (Cucerzan,
2007; Ratinov et al., 2011; Pantel et al., 2012) that
disambiguate text fragments relative to Wikipedia
entries can be employed.
4 Evaluation Results
Attribute Accuracy: The top 50 attributes, from
the ranked lists extracted for each target class, are
manually assigned correctness labels. As shown in
Table 3, an attribute is marked as vital, if it must
be present among representative attributes of the
390
Class Precision of Extracted Attributes
%vital %okay %wrong Score
Awards 29 14 7 0.72
Chemical elements 46 2 2 0.94
Companies 42 1 7 0.85
Food ingredients 31 9 10 0.71
Programming languages 31 7 12 0.69
Stadiums 42 5 3 0.89
Video games 33 14 3 0.80
...
Avg-All-Classes 33 10 7 0.76
Table 4: Accuracy of top 50 class attributes ex-
tracted from fact-seeking and explanation-seeking
queries, over the evaluation set of 40 target classes
class; okay, if it provides useful but non-essential
information; and wrong, if it is incorrect (Pas?ca,
2007). For example, the attributes manner (it) gen-
erates its energy, manner (it) became a constella-
tion and reason (it) has arms are annotated as vital,
okay and wrong respectively for the class Astro-
nomical objects. To compute the precision score
over a set of attributes, the correctness labels are
converted to numeric values: vital to 1.0, okay to
0.5, and wrong to 0.0. Precision is the sum of the
correctness values of the attributes, divided by the
number of attributes.
Table 4 summarizes the precision scores over
the evaluation set of target classes. The scores
vary from one class to another, for example 0.71
for Food ingredients but 0.94 for Chemical el-
ements. The average score is 0.76, indicating
that attributes extracted from fact and explanation-
seeking queries have encouraging levels of accu-
racy. The results already take into account the
detection of near-duplicate attributes. More pre-
cisely, the highest-ranked attribute in each group
of near-duplicate attributes, examples of which are
shown in Table 5, is retained and evaluated; the
lower-ranked attributes from each group are not
considered in the evaluation. Attributes like num-
ber of passengers (it) can hold, number of pas-
sengers it fits and number of passengers it seats
are nearly equivalent, but are still not marked as
near-duplicates for the class Aircraft, when they
should. Conversely, the attribute location (it)
lives is marked as a near-duplicate of location (it)
lives in new york, when it should not. Never-
theless, a significant number of near-duplicates,
which would otherwise crowd the ranked lists of
attributes with redundant information, are identi-
fied and discarded.
Target Class: Group of Near-Duplicate Attributes
Actors: movies (it) plays in, played in, acts in, acted in,
played, played on
Automobiles: date (it) was first manufactured, first pro-
duced, first made
Battles and operations of World War II: reason (it) hap-
pened, took place, occurred
Chemical elements: manner (it) returns to the atmo-
sphere, gets back into the atmosphere, got into the atmo-
sphere, gets into the atmosphere, enters the environment,
enters the atmosphere
Companies: location (it) makes its products, manufac-
tures its products, produces its products, gets its products,
makes its products, manufactures their products
Companies: date/time (it) began outsourcing, started out-
sourcing, outsourced
Countries: date (it) got its independence, gained indepen-
dence, gained its independence, got independence, got
their independence, won its independence, achieved inde-
pendence, received its independence, gained its freedom
Diseases and disorders: length/duration (it) takes to show
symptoms, takes to show up, takes to show, takes to ap-
pear, takes to manifest, takes to come out
Table 5: Groups of near-duplicate attributes iden-
tified for various classes. Attributes within a group
are ranked according to their individual scores.
Removing all but the first attribute of each group,
from the ranked list of attributes of the respective
class, improves the diversity of the list
Discussion: The set of patterns shown in Table 1
is extensible. Moreover, the patterns are subject
to errors. They may cause false matches, resulting
in erroneous extractions. The extent to which this
occurs is indirectly measured in the overall preci-
sion results. The modification of some of the pat-
terns, or the addition of new ones, would likely af-
fect the expected coverage and precision of the ex-
tracted attributes. If a pattern is particularly noisy,
it is likely to cause systematic errors, and therefore
produce attributes of lower quality.
Since attributes in Wikipedia and Freebase are
initially entered manually by human editors, their
correctness is virtually guaranteed. As for at-
tributes extracted automatically, previous compar-
isons indicate that attributes tend to have higher
quality when extracted from queries instead of
documents (Pas?ca, 2007). Indeed, a set of
extraction patterns applied to text produces at-
tributes whose average precision at rank 50 is 0.44
when extracted from documents, vs. 0.63 from
queries (Pas?ca et al., 2007). More importantly,
previously available or extracted attributes are vir-
tually always simple, short noun phrases like nu-
tritional value, taste or solubility in water. Even if
not confined to noun phrases, they are still short,
391
Run: [Ranked Attributes for a Sample of Classes]
Class: Automobiles:
D: [(it) goes on sale, (it) will go on sale, (it) is an en-
gineering playground, (it) will be available in japan, (it)
shows up in japan, (it) is a technical tour de force, (it) un-
veiled at tas 2008, (it) runs a 7:38, (it) is a unique car, (it)
uses a premium midship package, (it) features an all-new
3.8-litre, (it) is one of the fastest cars, (it) made a quick
drive-by, ..]
Q: [price/quantity/degree (it) weights, year (it) was
banned from bathurst, manner (it) launch control
works, engine (it) has, kind of engine (it) has,
price/quantity/degree (it) costs in japan, number of horse-
power (it) has, price/quantity/degree horsepower (it) has,
number of seats (it) has, speed (it) goes, who designed
(it), ..]
Class: Mobile phones:
D: [(it) was announced on september 17 2008, (it) ceased
with version, (it) was scheduled to be released in late
2010, (it) also supports qt (toolkit), (it) supports hardware
capable, (it) can synchronize with microsoft outlook, (it)
also supports python (programming language), ..]
Q: [date/time (it) came out in australia, who carries (it),
reason (it) keeps rebooting, colours (it) comes in, video
format (it) supports, date/time (it) was released, date/time
(it) came out in the uk, length/duration (it)?s battery lasts,
who sells (it), how much (it) costs, ..]
Class: Mountains:
D: [(it) is an active volcano, (it) is in the distance, (it)
is the highest peak in cascade range, (it) is 14,410 feet,
(it) was established in 1899, (it) comes into view, (it) was
established as a national park, ..]
Q: [date/time (it) last erupted, manner (it) erupted in 1882,
manner (it) formed, date/time (it) first became active,
manner (it) got its name, number of eruptions (it) had,
type of magma (it) has, reason (it) became a national park,
kind of animals (it) has, ..]
Table 6: Top relations extracted for a sample of
target classes via open-domain relations from doc-
uments (D) or via attributes from queries (Q)
like vegan, healthy or gluten free (Van Durme et
al., 2008; Pas?ca, 2012). In comparison, attributes
extracted in this paper accommodate properties
that are sometimes awkward or even impossible
to express through short phrases.
Noncontiguous Attributes as Relations: Non-
contiguous attributes extracted from fact-seeking
queries are embodiments of relations linking the
instances mentioned in the queries, on one hand,
and the values being requested by the queries, on
the other hand. Therefore, the method proposed in
this paper can also be regarded as a method for the
acquisition of relevant relations of various classes.
The extracted relations specify the left argument
(i.e., the instance) and the linking relation name
(i.e., the attribute). They only specify the type
of the, but not the actual, right argument (i.e., the
value being requested).
An additional experiment compares the accu-
racy of relations extracted as noncontiguous at-
tributes from queries, vs. relations extracted by a
previous open-domain method (Fader et al., 2011)
from 500 million Web documents. The previous
method, including its extraction patterns and its
ranking scheme, is designed with instances rather
than classes in mind. For fairness to the method
in (Fader et al., 2011), the evaluation procedure
is slightly adjusted. The set of instances associ-
ated with each target class, over which the two
methods are evaluated, is reduced to a single repre-
sentative instance selected a-priori. The instances
are shown as the first instances in parentheses for
each class in the earlier Table 2. Thus, the class
attributes are extracted using only the instances
keanu reeves, boeing 737 and bugs bunny in the
case of the classes Actors, Aircraft and Animated
characters respectively.
Table 6 suggests that noncontiguous attributes
extracted from queries tend to capture higher-
quality relations than arbitrary relations extracted
from documents. Because fact-seeking queries in-
quire about the value of some relations (attributes)
of an instance, the relations themselves tends to
be more relevant than relations extracted from ar-
bitrary document sentences. Nevertheless, rela-
tions derived from queries likely serve as a useful
complement, rather than replacement, of relations
from documents. The former only discover what
relations may be relevant; the latter also identify
their occurrences within text.
5 Related Work
Sources of text from which relations (Zhu et
al., 2009; Carlson et al., 2010; Lao et al.,
2011) and, more specifically, attributes can be
extracted include Web documents and data in
human-compiled encyclopedia. In Web docu-
ments, attributes are available within unstruc-
tured (Tokunaga et al., 2005; Pas?ca et al., 2007),
structured (Raju et al., 2008) and semi-structured
text (Yoshinaga and Torisawa, 2007), layout for-
matting tags (Wong et al., 2008), itemized lists or
tables (Cafarella et al., 2008). In human-compiled
encyclopedia (Wu and Weld, 2010), data relevant
to attribute extraction includes infoboxes and cat-
egory labels (Nastase and Strube, 2008; Hoffart
et al., 2013) associated with Wikipedia articles.
In order to acquire class attributes, a common
strategy is to first acquire attributes of instances,
then aggregate or propagate (Talukdar and Pereira,
392
2010) attributes, from instances to the classes to
which the instances belong. The role of Web
search queries, as an alternative textual data source
to Web documents in open-domain information
extraction, has been investigated in the tasks of at-
tribute extraction (Pas?ca, 2007; Pas?ca, 2012), as
well as in collecting sets of related instances (Jain
and Pennacchiotti, 2010).
To increase diversity within a ranked list of at-
tributes, the extraction method in this paper em-
ploys a synonym vocabulary to approximately
identify groups of near-duplicate attributes. As
reported for previous methods, the resulting lists
may still contain lexically different but semanti-
cally equivalent attributes. Scenarios where de-
tecting all equivalent attributes is important may
benefit from other techniques for paraphrase ac-
quisition (Madnani and Dorr, 2010).
Sophisticated techniques are sometimes em-
ployed to identify the type of the expected an-
swers of open-domain questions (Pinchak et al.,
2009). In comparison, the loose typing of the
values of our noncontiguous attributes is mostly
coarse-grained. It relies on wh-prefixes (when,
how long, where, how) and possibly subsequent
words (what nutritional value) from the queries,
to determine whether the values are expected to
be a date/time, length/duration, location, manner,
nutritional value etc.
Relations extracted from document sentences
(e.g., ?Claude Monet was born in Paris?) are tu-
ples of an instance (claude monet), a text fragment
acting as the lexicalized relation (was born in), and
another instance (paris) (cf. (Fader et al., 2011;
Mausam et al., 2012)). For convenience, the re-
lation and second instance may be concatenated,
as in was born in paris for claude monet. But
document sentences mentioning an instance do not
necessarily refer to properties of the instance that
people other than the author of the document are
likely to inquire about. Consequently, even top-
ranked extracted relations occasionally include
less informative ones, such as comes into view for
mount rainier, is on the table for madeira wine,
or allows for features for javascript (Fader et al.,
2011). Comparatively, relations extracted via non-
contiguous attributes from queries tend to refer to
properties that have values that Web users inquire
about in their search queries. Therefore, the rela-
tions extracted from queries are more likely to re-
fer to salient properties, such as date/time (it) had
its last eruption for mount rainier; length/duration
(it) lasts for madeira wine; and manner (it) stores
date information for javascript.
6 Conclusion
By requesting values for attributes of individual
instances, fact-seeking and explanation-seeking
queries implicitly assert the relevance of the prop-
erties encoded by the attributes, for the respec-
tive instances and their classes. The extracted at-
tributes are not required to take the form of con-
tiguous short phrases in the source queries, thus
allowing for the acquisition of a broader range of
attributes than those extracted by previous meth-
ods. Furthermore, since Web users are interested
in their values, the relations to which the ex-
tracted attributes refer tend to be more relevant
than relations extracted from arbitrary documents
using previous methods. Current work explores
the role of distributional similarities in expanding
extracted attributes for narrow classes; and the ex-
traction of noncontiguous attributes and relations
from natural-language queries without a wh-prefix
(e.g., cars driven by james bond).
Acknowledgments
The author would like to thank Efrat Farkash and
Michael Kleyman for assistance with the synonym
repository.
References
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.
2008. Freebase: A collaboratively created graph database
for structuring human knowledge. In Proceedings of the
2008 International Conference on Management of Data
(SIGMOD-08), pages 1247?1250, Vancouver, Canada.
L. Brown, T. Cai, and A. DasGupta. 2001. Interval es-
timation for a binomial proportion. Statistical Science,
16(2):101?117.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on the
Web. In Proceedings of the 34th Conference on Very
Large Data Bases (VLDB-08), pages 538?549, Auckland,
New Zealand.
A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and
T. Mitchell. 2010. Coupled semi-supervised learning for
information extraction. In Proceedings of the 3rd ACM
Conference on Web Search and Data Mining (WSDM-10),
pages 101?110, New York.
S. Cucerzan. 2007. Large-scale named entity disambigua-
tion based on Wikipedia data. In Proceedings of the 2007
Conference on Empirical Methods in Natural Language
Processing (EMNLP-07), pages 708?716, Prague, Czech
Republic.
393
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: Is more always better? In Pro-
ceedings of the 24th ACM Conference on Research and
Development in Information Retrieval (SIGIR-02), pages
207?214, Tampere, Finland.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proceedings
of the 2011 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-11), pages 1535?1545,
Edinburgh, Scotland.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical
Database and Some of its Applications. MIT Press.
J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. 2013.
YAGO2: a spatially and temporally enhanced knowledge
base from Wikipedia. Artificial Intelligence Journal. Spe-
cial Issue on Artificial Intelligence, Wikipedia and Semi-
Structured Resources, 194:28?61.
A. Jain and M. Pennacchiotti. 2010. Open entity extrac-
tion from Web search query logs. In Proceedings of the
23rd International Conference on Computational Linguis-
tics (COLING-10), pages 510?518, Beijing, China.
N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk in-
ference and learning in a large scale knowledge base. In
Proceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-11), pages
529?539, Edinburgh, Scotland.
N. Madnani and B. Dorr. 2010. Generating phrasal and
sentential paraphrases: a survey of data-driven methods.
Computational Linguistics, 36(3):341?387.
Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni.
2012. Open language learning for information extraction.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL-12),
pages 523?534, Jeju Island, Korea.
V. Nastase and M. Strube. 2008. Decoding Wikipedia cat-
egories for knowledge acquisition. In Proceedings of
the 23rd National Conference on Artificial Intelligence
(AAAI-08), pages 1219?1224, Chicago, Illinois.
M. Pas?ca, B. Van Durme, and N. Garera. 2007. The role of
documents vs. queries in extracting class attributes from
text. In Proceedings of the 16th International Conference
on Information and Knowledge Management (CIKM-07),
pages 485?494, Lisbon, Portugal.
M. Pas?ca. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of the 16th World Wide Web Con-
ference (WWW-07), pages 101?110, Banff, Canada.
M. Pas?ca. 2012. Attribute extraction from conjectural
queries. In Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING-12), Mum-
bai, India.
P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity types
from query logs via user intent modeling. In Proceedings
of the 50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-12), pages 563?571, Jeju Island,
Korea.
S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing. In
Proceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-10), pages
705?713, Cambridge, Massachusetts.
C. Pinchak, D. Lin, and D. Rafiei. 2009. Flexible answer
typing with discriminative preference ranking. In Pro-
ceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics (EACL-
09), pages 666?674, Athens, Greece.
S. Raju, P. Pingali, and V. Varma. 2008. An unsupervised ap-
proach to product attribute extraction. In Proceedings of
the 31st International Conference on Research and Devel-
opment in Information Retrieval (SIGIR-08), pages 35?42,
Singapore.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation to
Wikipedia. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL-11),
pages 1375?1384, Portland, Oregon.
M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.
P. Talukdar and F. Pereira. 2010. Experiments in graph-based
semi-supervised learning methods for class-instance ac-
quisition. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-10),
pages 1473?1481, Uppsala, Sweden.
K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Automatic
discovery of attribute words from Web documents. In
Proceedings of the 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05), pages 106?
118, Jeju Island, Korea.
B. Van Durme, T. Qian, and L. Schubert. 2008. Class-
driven attribute extraction. In Proceedings of the 22nd
International Conference on Computational Linguistics
(COLING-08), pages 921?928, Manchester, United King-
dom.
T. Wong, W. Lam, and T. Wong. 2008. An unsuper-
vised framework for extracting and normalizing product
attributes from multiple Web sites. In Proceedings of the
31st International Conference on Research and Develop-
ment in Information Retrieval (SIGIR-08), pages 35?42,
Singapore.
F. Wu and D. Weld. 2010. Open information extraction using
Wikipedia. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-10),
pages 118?127, Uppsala, Sweden.
N. Yoshinaga and K. Torisawa. 2007. Open-domain
attribute-value acquisition from semi-structured texts. In
Proceedings of the 6th International Semantic Web Con-
ference (ISWC-07), Workshop on Text to Knowledge: The
Lexicon/Ontology Interface (OntoLex-2007), pages 55?
66, Busan, South Korea.
J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. 2009. Stat-
Snowball: a statistical approach to extracting entity rela-
tionships. In Proceedings of the 18th World Wide Web
Conference (WWW-09), pages 101?110, Madrid, Spain.
394
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1200?1209,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Fine-Grained Class Label Markup of Search Queries
Joseph Reisinger?
Department of Computer Sciences
The University of Texas at Austin
Austin, Texas 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
We develop a novel approach to the seman-
tic analysis of short text segments and demon-
strate its utility on a large corpus of Web
search queries. Extracting meaning from short
text segments is difficult as there is little
semantic redundancy between terms; hence
methods based on shallow semantic analy-
sis may fail to accurately estimate meaning.
Furthermore search queries lack explicit syn-
tax often used to determine intent in ques-
tion answering. In this paper we propose a
hybrid model of semantic analysis combin-
ing explicit class-label extraction with a la-
tent class PCFG. This class-label correlation
(CLC) model admits a robust parallel approxi-
mation, allowing it to scale to large amounts of
query data. We demonstrate its performance
in terms of (1) its predicted label accuracy on
polysemous queries and (2) its ability to accu-
rately chunk queries into base constituents.
1 Introduction
Search queries are generally short and rarely contain
much explicit syntax, making query understanding a
purely semantic endeavor. Furthermore, as in noun-
phrase understanding, shallow lexical semantics is
often irrelevant or misleading; e.g., the query [trop-
ical breeze cleaners] has little to do with island va-
cations, nor are desert birds relevant to [1970 road
runner], which refers to a car model.
This paper introduces class-label correlation
(CLC), a novel unsupervised approach to extract-
?Contributions made during an internship at Google.
ing shallow semantic content that combines class-
based semantic markup (e.g., road runner is a car
model) with a latent variable model for capturing
weakly compositional interactions between query
constituents. Constituents are tagged with IsA class
labels from a large, automatically extracted lexicon,
using a probabilistic context free grammar (PCFG).
Correlations between the resulting label?term dis-
tributions are captured using a set of latent produc-
tion rules specified by a hierarchical Dirichlet Pro-
cess (Teh et al, 2006) with latent data groupings.
Concretely, the IsA tags capture the inventory
of potential meanings (e.g., jaguar can be labeled
as european car or large cat) and relevant con-
stituent spans, while the latent variable model per-
forms sense and theme disambiguation (e.g., [jaguar
habitat] would lend evidence for the large cat la-
bel). In addition to broad sense disambiguation, CLC
can distinguish closely related usages, e.g., the use
of dell in [dell motherboard replacement] and [dell
stock price].1 Furthermore, by employing IsA class
labeling as a preliminary step, CLC can account for
common non-compositional phrases, such as big ap-
ple unlike systems relying purely on lexical seman-
tics. Additional examples can be found later, in Fig-
ure 5.
In addition to improving query understanding, po-
tential applications of CLC include: (1) relation ex-
traction (Baeza-Yates and Tiberi, 2007), (2) query
substitutions or broad matching (Jones et al, 2006),
and (3) classifying other short textual fragments
such as SMS messages or tweets.
We implement a parallel inference procedure for
1Dell the computer system vs. Dell the technology company.
1200
CLC and evaluate it on a sample of 500M search
queries along two dimensions: (1) query constituent
chunking precision (i.e., how accurate are the in-
ferred spans breaks; cf., Bergsma and Wang (2007);
Tan and Peng (2008)), and (2) class label assign-
ment precision (i.e., given the query intent, how rel-
evant are the inferred class labels), paying particu-
lar attention to cases where queries contain ambigu-
ous constituents. CLC compares favorably to sev-
eral simpler submodels, with gains in performance
stemming from coarse-graining related class labels
and increasing the number of clusters used to cap-
ture between-label correlations.
(Paper organization): Section 2 discusses relevant
background, Section 3 introduces the CLC model,
Section 4 describes the experimental setup em-
ployed, Section 5 details results, Section 6 intro-
duces areas for future work and Section 7 concludes.
2 Background
Query understanding has been studied extensively
in previous literature. Li (2010) defines the se-
mantic structure of noun-phrase queries as intent
heads (attributes) coupled with some number of in-
tent modifiers (attribute values), e.g., the query [al-
ice in wonderland 2010 cast] is comprised of an in-
tent head cast and two intent modifiers alice in won-
derland and 2010. In this work we focus on seman-
tic class markup of query constituents, but our ap-
proach could be easily extended to account for query
structure as well.
Popescu et al (2010) describe a similar class-
label-based approach for query interpretation, ex-
plicitly modeling the importance of each label for
a given entity. However, details of their implemen-
tation were not publicly available, as of publication
of this paper.
For simplicity, we extract class labels using the
seed-based approach proposed by Van Durme and
Pas?ca (2008) (in particular Pas?ca (2010)) which gen-
eralizes Hearst (1992). Talukdar and Pereira (2010)
use graph-based semi-supervised learning to acquire
class-instance labels; Wang et al (2009) introduce a
similar CRF-based approach but only apply it to a
small number of verticals (i.e., Computing and Elec-
tronics or Clothing and Shoes). Snow et al (2006)
describe a learning approach for automatically ac-
quiring patterns indicative of hypernym (IsA) rela-
tions. Semantic class label lexicons derived from
any of these approaches can be used as input to CLC.
Several authors have studied query clustering in
the context of information retrieval (e.g., Beeferman
and Berger, 2000). Our approach is novel in this
regard, as we cluster queries in order to capture cor-
relations between span labels, rather than explicitly
for query understanding.
Tratz and Hovy (2010) propose a taxonomy for
classifying and interpreting noun-compounds, fo-
cusing specifically on the relationships holding be-
tween constituents. Our approach yields similar top-
ical decompositions of noun-phrases in queries and
is completely unsupervised.
Jones et al (2006) propose an automatic method
for query substitution, i.e., replacing a given query
with another query with the similar meaning, over-
coming issues with poor paraphrase coverage in tail
queries. Correlations mined by our approach are
readily useful for downstream query substitution.
Bergsma and Wang (2007) develop a super-
vised approach to query chunking using 500 hand-
segmented queries from the AOL corpus. Tan and
Peng (2008) develop a generative model of query
segmentation that makes use of a language model
and concepts derived from Wikipedia article titles.
CLC differs fundamentally in that it learns con-
cept label markup in addition to segmentation and
uses in-domain concepts derived from queries them-
selves. This work also differs from both of these
studies significantly in scope, training on 500M
queries instead of just 500.
At the level of class-label markup, our model is
related to Bayesian PCFGs (Liang et al, 2007; John-
son et al, 2007b), and is a particular realization of an
Adaptor Grammar (Johnson et al, 2007a; Johnson,
2010).
Szpektor et al (2008) introduce a model of con-
textual preferences, generalizing the notion of selec-
tional preference (cf. Ritter et al, 2010) to arbitrary
terms, allowing for context-sensitive inference. Our
approach differs in its use of class-instance labels for
generalizing terms, a necessary step for dealing with
the lack of syntactic information in queries.
1201
 ?
C
 
?
L
 
?
L
vinyl windowsbrighton
seaside towns building materials
query clusters
label clusters
label pcfg
query constituents
Figure 1: Overview of CLC markup generation for
the query [brighton vinyl windows]. Arrows denote
multinomial distributions.
3 Latent Class-Label Correlation
Input to CLC consists of raw search queries and a
partial grammar mapping class labels to query spans
(e.g., building materials?vinyl windows). CLC in-
fers two additional latent productions types on top
of these class labels: (1) a potentially infinite set of
label clusters ?Llk coarse-graining the raw input label
productions V , and (2) a finite set of query clusters
?Cci specifying distributions over label clusters; see
Figure 1 for an overview.
Operationally, CLC is implemented as a Hierar-
chical Dirichlet Process (HDP; Teh et al, 2006) with
latent groups coupled with a Probabilistic Context
Free Grammar (PCFG) likelihood function (Figure
2). We motivate our use of an HDP latent class
model instead of a full PCFG with binary produc-
tions by the fact that the space of possible binary
rule combinations is prohibitively large (561K base
labels; 314B binary rules). The next sections discuss
the three main components of CLC: ?3.1 the raw IsA
class labels, ?3.2 the PCFG likelihood, and ?3.3 the
HDP with latent groupings.
3.1 IsA Label Extraction
IsA class labels (hypernyms) V are extracted from
a large corpus of raw Web text using the method
proposed by Van Durme and Pas?ca (2008) and ex-
tended by Pas?ca (2010). Manually specified patterns
are used to extract a seed set of class labels and the
resulting label lists are reranked using cluster purity
measures. 561K labels for base noun phrases are
collected. Table 1 shows an example set of class
labels extracted for several common noun phrases.
Similar repositories of IsA labels, extracted using
other methods, are available for experimental pur-
class label?query span
recreational facilities?jacuzzi
rural areas?wales
destinations?wales
seaside towns?brighton
building materials?vinyl windows
consumer goods?european clothing
Table 1: Example production rules collected using
the semi-supervised approach of Van Durme and
Pas?ca (2008).
poses (Talukdar and Pereira, 2010). In addition to
extracted rules, the CLC grammar is augmented with
a set of null rules, one per unigram, ensuring that
every query has a valid parse.
3.2 Class-Label PCFG
In addition to the observed class-label production
rules, CLC incorporates two sets of latent produc-
tion rules coupled via an HDP (Figure 1). Class
label?query span productions extracted from raw
text are clustered into a set of latent label produc-
tion clusters L = {l1, . . . , l?}. Each label pro-
duction cluster lk defines a multinomial distribution
over class labels V parametrized by ?Llk . Conceptu-
ally, ?Llk captures a set of class labels with similar
productions that are found in similar queries, for ex-
ample the class labels states, northeast states, u.s.
states, state areas, eastern states, and certain states
might be included in the same coarse-grained cluster
due to similarities in their productions.
Each query q ? Q is assigned to a latent query
cluster cq ? C{c1, . . . , c?}, which defines a dis-
tribution over label production clusters L, denoted
?Ccq . Query clusters capture broad correlations be-
tween label production clusters and are necessary for
performing sense disambiguation and capturing se-
lectional preference. Query clusters and label pro-
duction clusters are linked using a single HDP, al-
lowing the number of label clusters to vary over the
course of Gibbs sampling, based on the variance of
the underlying data (Section 3.3). Viewed as a gram-
mar, CLC only contains unary rules mapping labels
to query spans; production correlations are captured
directly by the query cluster, unlike in HDP-PCFG
(Liang et al, 2007), as branching parses over the en-
1202
Indices Cardinality
HDP base measure ? ? GEM(?) - |L| ? ?
Query cluster ?Ci ? DP(?
C ,?) i ? |C| |L| ? ?
Label cluster ?Lk ? Dirichlet(?
L) k ? |L| |V |
Query cluster ind
piq ? Dirichlet(?) q ? |Q| |C|
cq ? piq q ? |Q| 1
Label cluster ind zq,t ? ?
C
cq t ? q, q ? |Q| 1
Label ind lq,t ? ?
L
zq,t t ? q, q ? |Q| 1
 
c
z
?
q
t
l
!
L
?
 
?
 
?
 
?
label clusters
 
!
C
|C|
 
?0
query clusters
 
?
Figure 2: Generative process and graphical model for CLC. The top section of the model is the standard
HDP prior; the middle section is the additional machinery necessary for modeling latent groupings and the
bottom section contains the indicators for the latent class model. PCFG likelihood is not shown.
tire label sparse are intractably large.
Given a query q, a query cluster assignment cq and
a set of label production clustersL, we define a parse
of q to be a sequence of productions tq forming a
parse tree consuming all the tokens in q. As with
Bayesian PCFGs (Johnson, 2010), the probability of
a tree tq is the product of the probabilities of the
production rules used to construct it
P (tq|?L,?C , cq) =
?
r?Rq
P (r|?Llr)P (lr|?
C
cq)
where Rq is the set of production rules used to de-
rive tq, P (r|?Llr) is the probability of r given its label
cluster assignment lr, and P (lr|?Ccq) is the probabil-
ity of label cluster lr in query cluster c.
The probability of a query q is the sum of the
probabilities of the parse trees that can generate it,
P (q|?L,?C , cq) =
?
{t|y(t)=q}
P (t|?L,?C , cq)
where {t|y(t) = q} is the set of trees with q as their
yield (i.e., generate the string of tokens in q).
3.3 Hierarchical Dirichlet Process with Latent
Groups
We complete the Bayesian generative specification
of CLC with an HDP prior linking ?C and ?L. The
HDP is a Bayesian generative model of shared struc-
ture for grouped data (Teh et al, 2006). A set of
base clusters ? ? GEM(?) is drawn from a Dirich-
let Process with base measure ? using the stick-
breaking construction, and clusters for each group k,
? ? HDP-LG base-measure smoother; higher val-
ues lead to more uniform mass over label
clusters.
?C ? Query cluster smoothing; higher values lead
to more uniform mass over label clusters.
?L ? Label cluster smoothing; higher values lead
to more label diversity within clusters.
? ? Query cluster assignment smoothing; higher
values lead to more uniform assignment.
Table 2: CLC-HDP-LG hyperparameters.
?Ck ? DP(?), are drawn from a separate Dirichlet
Process with base measure ?, defined over the space
of label clusters. Data in each group k are condi-
tionally independent given ?. Intuitively, ? defines
a common ?menu? of label clusters, and each query
cluster ?Ck defines a separate distribution over the
label clusters.
In order to account for variable query-cluster as-
signment, we extend the HDP model with latent
groupings piq ? Dir(?) for each query. The re-
sulting Hierarchical Dirichlet Process with Latent
Groups (HDP-LG) can be used to define a set of
query clusters over a set of (potentially infinite) base
label clusters (Figure 2). Each query cluster ?C (la-
tent group) assigns weight to different subsets of the
available label clusters ?L, capturing correlations
between them at the query level. Each query q main-
tains a distribution over query clusters piq, capturing
its affinity for each latent group. The full generative
specification of CLC is shown in Figure 2; hyperpa-
rameters are shown in Table 2.
In addition to the full joint CLC model, we evalu-
1203
ate several simpler models:
1. CLC-BASE ? no query clusters, one label per
label cluster.
2. CLC-DPMM ? no query clusters, DPMM(?C)
distribution over labels.
3. CLC-HDP-LG ? full HDP-LG model with |C|
query clusters over a potentially infinite num-
ber of query clusters.
as well as various hyperparameter settings.
3.4 Parallel Approximate Gibbs Sampler
We perform inference in CLC via Gibbs sampling,
leveraging Multinomial-Dirichlet conjugacy to inte-
grate out pi, ?C and ?L (Teh et al, 2006; Johnson
et al, 2007b). The remaining indicator variables c, z
and l are sampled iteratively, conditional on all other
variable assignments. Although there are an expo-
nential number of parse trees for a given query, this
space can be sampled efficiently using dynamic pro-
gramming (Finkel et al, 2006; Johnson et al, 2007b)
In order to apply CLC to Web-scale data, we
implement an efficient parallel approximate Gibbs
sampler in the MapReduce framework Dean and
Ghemawat (2004). Each Gibbs iteration consists
of a single MapReduce step for sampling, followed
by an additional MapReduce step for computing
marginal counts. 2 Relevant assignments c, z and
l are stored locally with each query and are dis-
tributed across compute nodes. Each node is respon-
sible only for resampling assignments for its local
set of queries. Marginals are fetched opportunisti-
cally from a separate distributed hash server as they
are needed by the sampler. Each Map step computes
a single Gibbs step for 10% of the available data, us-
ing the marginals computed at the previous step. By
resampling only 10% of the available data each it-
eration, we minimize the potentially negative effects
of using the previous step?s marginal distribution.
4 Experimental Setup
4.1 Query Corpus
Our dataset consists of a sample of 450M En-
glish queries submitted by anonymous Web users to
2This approximation and architecture is similar to Smola
and Narayanamurthy (2010).
Query length
de
ns
ity
0.1
0.2
0.3
0.4
2 4 6 8 10 12
Figure 3: Distribution in the query corpus, bro-
ken down by query length (red/solid=all queries;
blue/dashed=queries with ambiguous spans); most
queries contain between 2-6 tokens.
Google. The queries have an average of 3.81 tokens
per query (1.7B tokens). Single token queries are re-
moved as the model is incapable of using context to
disambiguate their meaning. Figure 3 shows the dis-
tribution of remaining queries. During training, we
include 10 copies of each query (4.5B queries total),
allowing an estimate of the Bayes average posterior
from a single Gibbs sample.
4.2 Evaluations
Query markup is evaluated for phrase-chunking pre-
cision (Section 5.1) and label precision (Section 5.2)
by human raters across two different samples: (1)
an unbiased sample from the original corpus, and
(2) a biased sample of queries containing ambigu-
ous spans.
Two raters scored a total of 10K labels from 800
spans across 300 queries. Span labels were marked
as incorrect (0.0), badspan (0.0), ambiguous (0.5),
or correct (1.0), with numeric scores for label pre-
cision as indicated. Chunking precision is measured
as the percentage of labels not marked as badspan.
We report two sets of precision scores depend-
ing on how null labels are handled: Strict evaluation
treats null-labeled spans as incorrect, while Normal
evaluation removes null-labeled spans from the pre-
cision calculation. Normal evaluation was included
since the simpler models (e.g., CLC-BASE) tend to
produce a significantly higher number of null assign-
ments.
Model evaluations were broken down into max-
imum a posteriori (MAP) and Bayes average esti-
mates. MAP estimates are calculated as the single
most likely label/cluster assignment across all query
copies; all assignments in the sample are averaged
1204
% 
clu
ste
r m
ov
es
0.0
0.2
0.4
0.6
0.8
50 100 150 200 250
% 
lab
el 
mo
ve
s
0.25
0.30
0.35
0.40
0.45
0.50
50 100 150 200 250
Gibbs iterations
%
 nu
ll r
ule
s
0.040
0.045
0.050
0.055
0.060
0.065
0.070
50 100 150 200 250
Figure 4: Convergence rates of CLC-
BASE (red/solid), CLC-HDP-LG 100C,40L
(green/dashed), CLC-HDP-LG 1000C,40L
(blue/dotted) in terms of % of query cluster swaps,
label cluster swaps and null rule assignments.
to obtain the Bayes average precision estimate.3
5 Results
A total of five variants of CLC were evaluated with
different combinations of |C| and HDP prior con-
centration ?C (controlling the effective number of
label clusters). Referring to models in terms of their
parametrizations is potentially confusing. There-
fore, we will make use of the fact that models with
?C = 1 yielded roughly 40 label clusters on aver-
age, and models with ?C = 0.1 yielded roughly 200
label clusters, naming model variants simply by the
number of query and label clusters: (1) CLC-BASE,
(2) CLC-DPMM 1C-40L, (3) CLC-HDP-LG 100C-
40L, (4) CLC-HDP-LG 1000C-40L, and (5) CLC-
HDP-LG 1000C-200L. Figure 4 shows the model
convergence for CLC-BASE, CLC-HDP-LG 100C-
40L, and CLC-HDP-LG 1000C-40L.
3We calculate the Bayes average precision estimates at
the top 10 (Bayes@10) and top 20 (Bayes@20) parse trees,
weighted by probability.
5.1 Chunking Precision
Chunking precision scores for each model are
shown in Table 3 (average % of labels not marked
badspan). CLC-HDP-LG 1000C-40L has the high-
est precision across both MAP and Bayes esti-
mates (?93% accuracy), followed by CLC-HDP-LG
1000C-200L (?90% accuracy) and CLC-DPMM 1C-
40L (?85%). CLC-BASE performed the worst by
a significant margin (?78%), indicating that label
coarse-graining is more important than query clus-
tering for chunking accuracy. No significant dif-
ferences in label chunking accuracy were found be-
tween Bayes and MAP inference.
5.2 Predicting Span Labels
The full CLC-HDP-LG model variants obtain higher
label precision than the simpler models, with CLC-
HDP-LG 1000C-40L achieving the highest precision
of the three (?63% accuracy). Increasing the num-
ber of label clusters too high, however, significantly
reduces precision: CLC-HDP-LG 1000C-200L ob-
tains only ?51% accuracy. However, comparing
to CLC-DPMM 1C-40L and CLC-BASE demonstrates
that the addition of label clusters and query clusters
both lead to gains in label precision. These relative
rankings are robust across strict and normal evalua-
tion regimes.
The breakdown over MAP and Bayes posterior
estimation is less clear when considering label pre-
cision: the simpler models CLC-BASE and CLC-
DPMM 1C-40L perform significantly worse than
Bayes when using MAP estimation, while in CLC-
HDP-LG the reverse holds.
There is little evidence for correlation between
precision and query length (weak, not statistically
significant negative correlation using Spearman?s ?).
This result is interesting as the relative prevalence
of natural language queries increases with query
length, potentially degrading performance. How-
ever, we did find a strong positive correlation be-
tween precision and the number of labels produc-
tions applicable to a query, i.e., production rule fer-
tility is a potential indicator of semantic quality.
Finally, the histogram column in Table 3 shows
the distribution of rater responses for each model.
In general, the more precise models tend to have
a significantly lower proportion of missing spans
1205
Model Chunking Label Precision Ambiguous Label Precision Spearman?s ?
Precision normal strict hist normal strict q. len # labels
Class-Label Correlation Base
Bayes@10 78.7?1.1 37.7?1.2 35.8?1.2 35.4?2.0 33.2?1.9 -0.13 0.51?
Bayes@20 78.7?1.1 37.7?1.2 35.8?1.2 35.4?2.0 33.2?1.9 -0.13 0.51?
MAP 76.3?2.2 33.3?2.2 31.8?2.2 36.2?4.0 33.2?3.8 -0.13 0.52?
Class-Label Correlation DPMM 1C 40L
Bayes@10 84.9?0.4 46.6?0.6 44.3?0.5 36.0?1.1 33.7?1.0 -0.05 0.25
Bayes@20 84.8?0.4 47.4?0.5 45.2?0.5 37.8?1.0 35.5?1.0 -0.02 0.23
MAP 84.1?0.8 42.6?1.0 40.5?0.9 11.2?1.3 10.6?1.3 -0.03 0.12
Class-Label Correlation HDP-LG 100C 40L
Bayes@10 83.8?0.4 55.6?0.5 51.0?0.5 55.6?1.0 47.7?1.0 0.03 0.44?
Bayes@20 83.6?0.4 56.9?0.5 52.3?0.5 57.4?1.0 49.8?0.9 0.04 0.41?
MAP 82.7?0.5 58.5?0.5 53.6?0.5 60.4?1.1 51.5?1.0 0.02 0.41?
Class-Label Correlation HDP-LG 1000C 40L
Bayes@10 93.1?0.2 61.1?0.3 60.0?0.3 43.2?0.9 40.2?0.9 -0.06 0.26?
Bayes@20 92.8?0.2 62.6?0.3 61.7?0.3 44.9?0.8 42.2?0.8 -0.10 0.27?
MAP 92.7?0.2 63.7?0.3 62.7?0.3 44.1?0.9 41.1?0.9 -0.12 0.28?
Class-Label Correlation HDP-LG 1000C 200L
Bayes@10 90.3?0.5 50.9?0.8 48.6?0.7 45.8?1.5 42.5?1.3 -0.10 0.13
Bayes@20 89.9?0.5 50.2?0.7 48.0?0.7 44.4?1.4 41.3?1.3 -0.08 0.11
MAP 90.0?0.6 51.0?0.8 48.9?0.8 49.2?1.5 46.0?1.4 -0.07 0.04
Table 3: Chunking and label precision across five models. Confidence intervals are standard error; sparklines
show distribution of precision scores (left is zero, right is one). Hist shows the distribution of human rating
response (log y scale): green/first is correct, blue/second is ambiguous, cyan/third is missing and red/fourth
is incorrect. Spearman?s ? columns give label precision correlations with query length (weak negative corre-
lation) and the number of applicable labels (weak to strong positive correlation); dots indicate significance.
(blue/second bar; due to null rule assignment) in ad-
ditional to more correct (green/first) and fewer in-
correct (red/fourth) spans.
5.3 High Polysemy Subset
We repeat the analysis of label precision on a subset
of queries containing one of the manually-selected
polysemous spans shown in Table 4. The CLC-
HDP-LG -based models still significantly outper-
form the simpler models, but unlike in the broader
setting, CLC-HDP-LG 100C-40L significantly out-
performs CLC-HDP-LG 1000C-40L, indicating that
lower query cluster granularity helps address poly-
semy (Table 3).
5.4 Error Analysis
Figure 5 gives examples of both high-precision and
low-precision queries markups inferred by CLC-
HDP-LG. In general, CLC performs well on queries
with clear intent head / intent modifier structure (Li,
acapella, alamo, apple, atlas, bad, bank, batman,
beloved, black forest, bravo, bush, canton, casino,
champion, club, comet, concord, dallas, diamond,
driver, english, ford, gamma, ion, lemon, man-
hattan, navy, pa, palm, port, put, resident evil,
ronaldo, sacred heart, saturn, seven, solution, so-
pranos, sparta, supra, texas, village, wolf, young
Table 4: Samples from a list of 90 manually se-
lected ambiguous spans used to evaluate model per-
formance under polysemy.
2010). More complex queries, such as [never know
until you try quotes] or [how old do you have to be
a bartender in new york] do not fit this model; how-
ever, expanding the set of extracted labels to also
cover instances such as never know until you try
would mitigate this problem, motivating the use of
n-gram language models with semantic markup.
A large number of mistakes made by CLC are
1206
To
p
 
1
0
%
B
o
t
t
o
m
 
2
0
%
M
i
d
d
l
e
 
2
0
%
Figure 5: Examples of high- and low-precision query markups inferred by CLC-HDP-LG. Black text is the
original query; lines indicate potential spans; small text shows potential labels colored and numbered by
label cluster; small bar shows percentage of assignments to that label cluster.
due to named-entity categories with weak seman-
tics such as rock bands or businesses (e.g., [tropi-
cal breeze cleaners], [cosmic railroad band] or [so-
pranos cigars]). When the named entity is common
enough, it is detected by the rule set, but for the long
tail of named entities this is not the case. One poten-
tial solution is to use a stronger notion of selectional
preference and slot-filling, rather than just relying on
correlation between labels.
Other examples of common errors include inter-
preting weymouth in [weymouth train time table] as
a town in Massachusetts instead of a town in the UK
(lack of domain knowledge), and using lower qual-
ity semantic labels (e.g., neighboring countries for
france, or great retailers for target).
6 Discussion and Future Work
Adding both latent label clusters (DPMM) and la-
tent query clusters (extending to HDP-LG) improve
chunking and label precision over the baseline CLC-
BASE system. The label clusters are important be-
cause they capture intra-group correlations between
class labels, while the query clusters are important
for capturing inter-group correlations. However, the
algorithm is sensitive to the relative number of clus-
ters in each case: Too many labels/label clusters rel-
1207
ative to the number of query clusters make it difficult
to learn correlations (O(n2) query clusters are re-
quired to capture pairwise interactions). Too many
query clusters, on the other hand, make the model
intractable computationally. The HDP automates se-
lecting the number of clusters, but still requires man-
ual hyperparameter setting.
(Future Work) Many query slots have weak se-
mantics and hence are misleading for CLC. For
example [pacific breeze cleaners] or [dale hartley
subaru] should be parsed such that the type of the
leading slot is determined not by its direct content,
but by its context; seeing subaru or cleaners after
a noun-phrase slot is a strong indicator of its type
(dealership or shop name). The current CLC model
only couples these slots through their correlations in
query clusters, not directly through relative position
or context. Binary productions in the PCFG or a dis-
criminative learning model would help address this.
Finally, we did not measure label coverage with
respect to a human evaluation set; coverage is use-
ful as it indicates whether our inferred semantics are
biased with respect to human norms.
7 Conclusions
We introduced CLC, a set of latent variable PCFG
models for semantic analysis of short textual seg-
ments. CLC captures semantic information in the
form of interactions between clusters of automati-
cally extracted class-labels, e.g., finding that place-
names commonly co-occur with business-names.
We applied CLC to a corpus containing 500M search
queries, demonstrating its scalability and straight-
forward parallel implementation using frameworks
like MapReduce or Hadoop. CLC was able to chunk
queries into spans more accurately and infer more
precise labels than several sub-models even across a
highly ambiguous query subset. The key to obtain-
ing these results was coarse-graining the input class-
label set and using a latent variable model to capture
interactions between coarse-grained labels.
References
R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic
relations from query logs. In Proceedings of the 13th
ACM Conference on Knowledge Discovery and Data
Mining (KDD-07), pages 76?85. San Jose, California.
D. Beeferman and A. Berger. 2000. Agglomerative clus-
tering of a search engine query log. In Proceedings of
the 6th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD-00), pages 407?416.
S. Bergsma and Q. Wang. 2007. Learning noun phrase
query segmentation. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-07), pages 819?826. Prague,
Czech Republic.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In Proceedings
of the 6th Symposium on Operating Systems Design
and Implementation (OSDI-04), pages 137?150. San
Francisco, California.
J. Finkel, C. Manning, and A. Ng. 2006. Solving the
problem of cascading errors: Approximate Bayesian
inference for linguistic annotation pipelines. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-06),
pages 618?626. Sydney, Australia.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545. Nantes, France.
M. Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-10), pages 1148?1157. Up-
psala, Sweden.
M. Johnson, T. Griffiths, and S. Goldwater. 2007a. Adap-
tor grammars: a framework for specifying composi-
tional nonparametric bayesian models. In Advances
in Neural Information Processing Systems 19, pages
641?648. Vancouver, Canada.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov Chain
Monte Carlo. In Proceedings of the 2007 Confer-
ence of the North American Association for Computa-
tional Linguistics (NAACL-HLT-07), pages 139?146.
Rochester, New York.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15h
World Wide Web Conference (WWW-06), pages 387?
396. Edinburgh, Scotland.
X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1337?1345. Upp-
sala, Sweden.
1208
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proceedings of the 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
07), pages 688?697. Prague, Czech Republic.
M. Pas?ca. 2010. The role of queries in ranking labeled in-
stances extracted from text. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING-10), pages 955?962. Beijing, China.
A. Popescu, P. Pantel, and G. Mishne. 2010. Seman-
tic lexicon adaptation for use in query interpretation.
In Proceedings of the 19th World Wide Web Confer-
ence (WWW-10), pages 1167?1168. Raleigh, North
Carolina.
A. Ritter, Mausam, and O. Etzioni. 2010. A latent Dirich-
let alocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
424?434. Uppsala, Sweden.
A. Smola and S. Narayanamurthy. 2010. An architec-
ture for parallel topic models. In Proceedings of the
36th Conference on Very Large Data Bases (VLDB-
10), pages 703?710. singapore.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801?808. Sydney, Australia.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-08), pages 683?691. Colum-
bus, Ohio.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1473?1481. Upp-
sala, Sweden.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia.
In Proceedings of the 17th World Wide Web Confer-
ence (WWW-08), pages 347?356. Beijing, China.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
S. Tratz and E. Hovy. 2010. A taxonomy, dataset, and
classifier for automatic noun compound interpretation.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-10), pages
678?687. Uppsala, Sweden.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243?
1248. Chicago, Illinois.
T. Wang, R. Hoffmann, X. Li, and J. Szymanski.
2009. Semi-supervised learning of semantic classes
for query understanding: from the Web and for the
Web. In Proceedings of the 18th International Con-
ference on Information and Knowledge Management
(CIKM-09), pages 37?46. Hong Kong, China.
1209
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1607?1615,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Ranking Class Labels Using Query Sessions
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, California 94043
mars@google.com
Abstract
The role of search queries, as available within
query sessions or in isolation from one an-
other, in examined in the context of ranking
the class labels (e.g., brazilian cities, busi-
ness centers, hilly sites) extracted from Web
documents for various instances (e.g., rio de
janeiro). The co-occurrence of a class la-
bel and an instance, in the same query or
within the same query session, is used to re-
inforce the estimated relevance of the class la-
bel for the instance. Experiments over eval-
uation sets of instances associated with Web
search queries illustrate the higher quality of
the query-based, re-ranked class labels, rel-
ative to ranking baselines using document-
based counts.
1 Introduction
Motivation: The offline acquisition of instances (rio
de janeiro, porsche cayman) and their correspond-
ing class labels (brazilian cities, locations, vehicles,
sports cars) from text has been an active area of re-
search. In order to extract fine-grained classes of
instances, existing methods often apply manually-
created (Banko et al, 2007; Talukdar et al, 2008) or
automatically-learned (Snow et al, 2006) extraction
patterns to text within large document collections.
In Web search, the relative ranking of documents
returned in response to a query directly affects the
outcome of the search. Similarly, the quality of
the relative ranking among class labels extracted for
a given instance influences any applications (e.g.,
query refinements or structured extraction) using the
extracted data. But due to noise in Web data and
limitations of extraction techniques, class labels ac-
quired for a given instance (e.g., oil shale) may fail
to properly capture the semantic classes to which the
instance may belong (Kozareva et al, 2008). In-
evitably, some of the extracted class labels will be
less useful (e.g., sources, mutual concerns) or incor-
rect (e.g., plants for the instance oil shale). In pre-
vious work, the relative ranking of class labels for
an instance is determined mostly based on features
derived from the source Web documents from which
the data has been extracted, such as variations of the
frequency of co-occurrence or diversity of extraction
patterns producing a given pair (Etzioni et al, 2005).
Contributions: This paper explores the role of
Web search queries, rather than Web documents, in
inducing superior ranking among class labels ex-
tracted automatically from documents for various in-
stances. It compares two sources of indirect ranking
evidence available within anonymized query logs:
a) co-occurrence of an instance and its class label
in the same query; and b) co-occurrence of an in-
stance and its class label, as separate queries within
the same query session. The former source is a noisy
attempt to capture queries that narrow the search re-
sults to a particular class of the instance (e.g., jaguar
car maker). In comparison, the latter source nois-
ily identifies searches that specialize from a class
(e.g., car maker) to an instance (e.g., jaguar) or,
conversely, generalize from an instance to a class.
To our knowledge, this is the first study comparing
inherently-noisy queries and query sessions for the
purpose of ranking of open-domain, labeled class in-
stances.
1607
The remainder of the paper is organized as fol-
lows. Section 2 introduces intuitions behind an
approach using queries for ranking class labels of
various instances, and describes associated ranking
functions. Sections 3 and 4 describe the experi-
mental setting and evaluation results over evaluation
sets of instances associated with Web search queries.
The results illustrate the higher quality of the query-
based, re-ranked lists of class labels, relative to alter-
native ranking methods using only document-based
counts.
2 Instance Class Ranking via Query Logs
Ranking Hypotheses: We take advantage of
anonymized query logs, to induce superior ranking
among the class labels associated with various class
instances within an IsA repository acquired from
Web documents. Given a class instance I, the func-
tions used for the ranking of its class labels are cho-
sen following several observations.
? Hypothesis H1: If C is a prominent class of an
instance I, then C and I are likely to occur in text in
contexts that are indicative of an IsA relation.
? Hypothesis H2: If C is a prominent class of an
instance I, and I is ambiguous, then a fraction of
the queries about I may also refer to and contain C.
? Hypothesis H3: If C is a prominent class of an
instance I, then a fraction of the queries about I
may be followed by queries about C, and vice-versa.
Ranking Functions: The ranking functions follow
directly from the above hypotheses.
? Ranking based on H1 (using documents): The
first hypothesis H1 is a reformulation of findings
from previous work (Etzioni et al, 2005). In prac-
tice, a class label is deemed more relevant for an in-
stance if the pair is extracted more frequently and by
multiple patterns, with the scoring formula:
ScoreH1(C, I) = Freq(C, I)? Size({Pattern(C)})2 (1)
where Freq(C, I) is the frequency of extraction of
C for the instance I, and Size({Pattern(C)}) is the
number of unique patterns extracting the class label
C for the instance I. The patterns are hand-written,
following (Hearst, 1992):
?[..] C [such as|including] I [and|,|.]?,
where I is a potential instance (e.g., diderot) and C
is a potential class label (e.g., writers). The bound-
aries are approximated from the part-of-speech tags
of the sentence words, for potential class labels C;
and identified by checking that I occurs as an entire
query in query logs, for instances I (Van Durme and
Pas?ca, 2008).
The application of the scoring formula (1) to can-
didates extracted from the Web produces a ranked
list of class labels LH1(I).
? Ranking based on H2 (using queries): Intu-
itively, Web users searching for information about
I sometimes add some or all terms of C to a search
query already containing I, either to further spec-
ify their query, or in response to being presented
with sets of search results spanning several mean-
ings of an ambiguous instance. Examples of such
queries are happiness emotion and diderot philoso-
pher. Moreover, queries like happiness positive psy-
chology and diderot enlightenment may be consid-
ered to weakly and partially reinforce the relevance
of the class labels positive emotions and enlighten-
ment writers of the instances happiness and diderot
respectively. In practice, a class label is deemed
more relevant if its individual terms occur in pop-
ular queries containing the instance. More precisely,
for each term within any class label from LH1(I),
we compute a score TermQueryScore. The score is
the frequency sum of the term within anonymized
queries containing the instance I as a prefix, and
the term anywhere else in the queries. Terms are
stemmed before the computation.
Each class label C is assigned the geometric mean
of the scores of its N terms Ti, after ignoring stop
words:
ScoreH2(C, I) = (
N
?
i=1
TermQueryScore(Ti))1/N (2)
The geometric mean is preferred to the arithmetic
mean, because the latter is more strongly affected by
outlier values. The class labels are ranked according
to the means, resulting in a ranked list LH2(I). In
case of ties, LH2(I) keeps the relative ranking from
LH1(I).
? Ranking based on H3 (using query sessions):
Given the third hypothesis H3, Web users searching
for information about I may subsequently search for
more general information about one of its classes C.
Conversely, users may specialize their search from
a class C to one of its instances I. Examples of
such queries are happiness followed later by emo-
tions, or diderot followed by philosophers; or emo-
1608
tions followed later by happiness, or philosophers
followed by diderot. In practice, a class label is
deemed more relevant if its individual terms occur as
part of queries that are in the same query session as a
query containing only the instance. More precisely,
for each term within any class label from LH1(I),
we compute a score TermSessionScore, equal to the
frequency sum of the anonymized queries from the
query sessions that contain the term and are: a) ei-
ther the initial query of the session, with the instance
I being one of the subsequent queries from the same
session; or b) one of the subsequent queries of the
session, with the instance I being the initial query
of the same session. Before computing the frequen-
cies, the class label terms are stemmed.
Each class label C is assigned the geometric mean
of the scores of its terms, after ignoring stop words:
ScoreH3(C, I) = (
N
?
i=1
TermSessionScore(Ti))1/N (3)
The class labels are ranked according to the geo-
metric means, resulting in a ranked list LH3(I). In
case of ties, LH3(I) preserves the relative ranking
from LH1(I).
Unsupervised Ranking: Given an instance I, the
ranking hypotheses and corresponding functions
LH1(I), LH2(I) and LH3(I) (or any combination
of them) can be used together to generate a merged,
ranked list of class labels per instance I. The score
of a class label in the merged list is determined by
the inverse of the average rank in the lists LH1(I)
and LH2(I) and LH3(I), computed with the follow-
ing formula:
ScoreH1+H2+H3(C, I) =
N
?N
i Rank(C, LHi)
(4)
where N is the number of input lists of class labels
(in this case, 3), and Rank(C, LHi) is the rank of C
in the input list of class labels LHi (LH1, LH2 or
LH3). The rank is set to 1000, if C is not present in
the list LHi. By using only the relative ranks and not
the absolute scores of the class labels within the in-
put lists, the outcome of the merging is less sensitive
to how class labels of a given instance are numeri-
cally scored within the input lists. In case of ties,
the scores of the class labels from LH1(I) serve as a
secondary ranking criterion. Thus, every instance I
from the IsA repository is associated with a ranked
list of class labels computed according to this rank-
ing formula. Conversely, each class label C from
the IsA repository is associated with a ranked list
of class instances computed with the earlier scoring
formula (1) used to generate lists LH1(I).
Note that the ranking formula can also consider
only a subset of the available input lists. For in-
stance, ScoreH1+H2 would use only LH1(I) and
LH2(I) as input lists; ScoreH1+H3 would use only
LH1(I) and LH3(I) as input lists; etc.
3 Experimental Setting
Textual Data Sources: The acquisition of the
IsA repository relies on unstructured text available
within Web documents and search queries. The
queries are fully-anonymized queries in English sub-
mitted to Google by Web users in 2009, and are
available in two collections. The first collection is
a random sample of 50 million unique queries that
are independent from one another. The second col-
lection is a random sample of 5 million query ses-
sions. Each session has an initial query and a se-
ries of subsequent queries. A subsequent query is a
query that has been submitted by the same Web user
within no longer than a few minutes after the initial
query. Each subsequent query is accompanied by
its frequency of occurrence in the session, with the
corresponding initial query. The document collec-
tion consists of a sample of 100 million documents
in English.
Experimental Runs: The experimental runs corre-
spond to different methods for extracting and rank-
ing pairs of an instance and a class:
? from the repository extracted here, with class
labels of an instance ranked based on the frequency
and the number of extraction patterns (ScoreH1
from Equation (1) in Section 2), in run Rd;
? from the repository extracted here, with class
labels of an instance ranked via the rank-based
merging of: ScoreH1+H2 from Section 2, in run
Rp, which corresponds to re-ranking using co-
occurrence of an instance and its class label in
the same query; ScoreH1+H3 from Section 2, in
run Rs, which corresponds to re-ranking using co-
occurrence of an instance and its class label, as sep-
arate queries within the same query session; and
ScoreH1+H2+H3 from Section 2, in run Ru, which
corresponds to re-ranking using both types of co-
occurrences in queries.
1609
Evaluation Procedure: The manual evaluation of
open-domain information extraction output is time
consuming (Banko et al, 2007). A more practi-
cal alternative is an automatic evaluation procedure
for ranked lists of class labels, based on existing re-
sources and systems.
Assume that there is a gold standard, containing
gold class labels that are each associated with a gold
set of their instances. The creation of such gold stan-
dards is discussed later. Based on the gold standard,
the ranked lists of class labels available within an
IsA repository can be automatically evaluated as fol-
lows. First, for each gold label, the ranked lists of
class labels of individual gold instances are retrieved
from the IsA repository. Second, the individual re-
trieved lists are merged into a ranked list of class
labels, associated with the gold label. The merged
list can be computed, e.g., using an extension of the
ScoreH1+H2+H3 formula (Equation (4)) described
earlier in Section 2. Third, the merged list is com-
pared against the gold label, to estimate the accu-
racy of the merged list. Intuitively, a ranked list of
class labels is a better approximation of a gold label,
if class labels situated at better ranks in the list are
closer in meaning to the gold label.
Evaluation Metric: Given a gold label and a list of
class labels, if any, derived from the IsA repository,
the rank of the highest class label that matches the
gold label determines the score assigned to the gold
label, in the form of the reciprocal rank of the match.
Thus, if the gold label matches a class label at rank
1, 2 or 3 in the computed list, the gold label receives
a score of 1, 0.5 or 0.33 respectively. The score is
0 if the gold label does not match any of the top 20
class labels. The overall score over the entire set of
gold labels is the mean reciprocal rank (MRR) score
over all gold labels from the set. Two types of MRR
scores are automatically computed:
? MRRf considers a gold label and a class label
to match, if they are identical;
? MRRp considers a gold label and a class label
to match, if one or more of their tokens that are not
stop words are identical.
During matching, all string comparisons are case-
insensitive, and all tokens are first converted to their
singular form (e.g., european countries to european
country) using WordNet (Fellbaum, 1998). Thus, in-
surance carriers and insurance companies are con-
Query Set: Sample of Queries
Qe (807 queries): 2009 movies, amino acids, asian
countries, bank, board games, buildings, capitals,
chemical functional groups, clothes, computer lan-
guage, dairy farms near modesto ca, disease, egyp-
tian pharaohs, eu countries, fetishes, french presidents,
german islands, hawaiian islands, illegal drugs, irc
clients, lakes, macintosh models, mobile operator in-
dia, nba players, nobel prize winners, orchids, photo
editors, programming languages, renaissance artists,
roller costers, science fiction tv series, slr cameras,
soul singers, states of india, taliban members, thomas
edison inventions, u.s. presidents, us president, water
slides
Qm (40 queries): actors, actresses, airlines, ameri-
can presidents, antibiotics, birds, cars, celebrities, col-
ors, computer languages, digital camera, dog breeds,
dogs, drugs, elements, endangered animals, european
countries, flowers, fruits, greek gods, horror movies,
idioms, ipods, movies, names, netbooks, operating
systems, park slope restaurants, planets, presidents,
ps3 games, religions, renaissance artists, rock bands,
romantic movies, states, universities, university, us
cities, vitamins
Table 1: Size and composition of evaluation sets of
queries associated with non-filtered (Qe) or manually-
filtered (Qm) instances
sidered to not match in MRRf scores, but match in
MRRp scores. On the other hand, MRRp scores may
give credit to less relevant class labels, such as insur-
ance policies for the gold label insurance carriers.
Therefore, MRRp is an optimistic, and MRRf is a
pessimistic estimate of the actual usefulness of the
computed ranked lists of class labels as approxima-
tions of the gold labels.
4 Evaluation
IsA Repository: The IsA repository, extracted from
the document collection, covers a total of 4.04 mil-
lion instances associated with 7.65 million class la-
bels. The number of class labels available per in-
stance and vice-versa follows a long-tail distribu-
tion, indicating that 2.12 million of the instances
each have two or more class labels (with an average
of 19.72 class labels per instance).
Evaluation Sets of Queries: Table 1 shows sam-
ples of two query sets, introduced in (Pas?ca, 2010)
and used in the evaluation. The first set, denoted Qe,
1610
Query Set Min Max Avg Median
Number of Gold Instances:
Qe 10 100 70.4 81
Qm 8 33 16.9 17
Number of Query Tokens:
Qe 1 8 2.0 2
Qm 1 3 1.4 1
Table 2: Number of gold instances (upper part) and num-
ber of query tokens (lower part) available per query, over
the evaluation sets of queries associated with non-filtered
gold instances (Qe) or manually-filtered gold instances
(Qm)
is obtained from a random sample of anonymized,
class-seeking queries submitted by Web users to
Google Squared. The set contains 807 queries, each
associated with a ranked list of between 10 and 100
gold instances automatically extracted by Google
Squared.
Since the gold instances available as input for
each query as part of Qe are automatically extracted,
they may or may not be true instances of the respec-
tive queries. As described in (Pas?ca, 2010), the sec-
ond evaluation set Qm is a subset of 40 queries from
Qe, such that the gold instances available for each
query in Qm are found to be correct after manual
inspection. The 40 queries from Qm are associated
with between 8 and 33 human-validated instances.
As shown in the upper part of Table 2, the queries
from Qe are up to 8 tokens in length, with an average
of 2 tokens per query. Queries from Qm are com-
paratively shorter, both in maximum (3 tokens) and
average (1.4 tokens) length. The lower part of Ta-
ble 2 shows the number of gold instances available
as input, which average around 70 and 17 per query,
for queries from Qe and Qm respectively. To provide
another view on the distribution of the queries from
evaluation sets, Table 3 lists tokens that are not stop
words, which occur in most queries from Qe. Com-
paratively, few query tokens occur in more than one
query in Qm.
Evaluation Procedure: Following the general eval-
uation procedure, each query from the sets Qe and
Qm acts as a gold class label associated with the
corresponding set of instances. Given a query and
its instances I from the evaluation sets Qe or Qm,
a merged, ranked lists of class labels is computed
out of the ranked lists of class labels available in the
Query Cnt. Examples of Queries Containing
the Token
Token
countries 22 african countries, eu countries,
poor countries
cities 21 australian cities, cities in califor-
nia, greek cities
presidents 18 american presidents, korean
presidents, presidents of the
south korea
restaurants 15 atlanta restaurants, nova scotia
restaurants, restaurants 10024
companies 14 agriculture companies, gas util-
ity companies, retail companies
states 14 american states, states of india,
united states national parks
prime 11 australian prime ministers, in-
dian prime ministers, prime min-
isters
cameras 10 cameras, digital cameras olym-
pus, nikon cameras
movies 10 2009 movies, movies, romantic
movies
american 9 american authors, american
president, american revolution
battles
ministers 9 australian prime ministers, in-
dian prime ministers, prime min-
isters
Table 3: Query tokens occurring most frequently in
queries from the Qe evaluation set, along with the number
(Cnt) and examples of queries containing the tokens
underlying IsA repository for each instance I. The
evaluation compares the merged lists of class labels,
with the corresponding queries from Qe or Qm.
Accuracy of Lists of Class Labels: Table 4 summa-
rizes results from comparative experiments, quanti-
fying a) horizontally, the impact of alternative pa-
rameter settings on the computed lists of class la-
bels; and b) vertically, the comparative accuracy of
the experimental runs over the query sets. The ex-
perimental parameters are the number of input in-
stances from the evaluation sets that are used for re-
trieving class labels, I-per-Q, set to 3, 5, 10; and the
number of class labels retrieved per input instance,
C-per-I, set to 5, 10, 20.
Four conclusions can be derived from the results.
First, the scores over Qm are higher than those over
Qe, confirming the intuition that the higher-quality
1611
Accuracy
I-per-Q 3 5 10
C-per-I 5 10 20 5 10 20 5 10 20
MRRf computed over Qe:
Rd 0.186 0.195 0.198 0.198 0.207 0.210 0.204 0.214 0.218
Rp 0.202 0.211 0.216 0.232 0.238 0.244 0.245 0.255 0.257
Rs 0.258 0.260 0.261 0.278 0.277 0.276 0.279 0.280 0.282
Ru 0.234 0.241 0.244 0.260 0.263 0.270 0.274 0.275 0.278
MRRp computed over Qe:
Rd 0.489 0.495 0.495 0.517 0.528 0.529 0.541 0.553 0.557
Rp 0.520 0.531 0.533 0.564 0.573 0.578 0.590 0.601 0.602
Rs 0.576 0.584 0.583 0.612 0.616 0.614 0.641 0.636 0.628
Ru 0.561 0.570 0.571 0.606 0.614 0.617 0.640 0.641 0.636
MRRf computed over Qm:
Rd 0.406 0.436 0.442 0.431 0.447 0.466 0.467 0.470 0.501
Rp 0.423 0.426 0.429 0.436 0.483 0.508 0.500 0.526 0.530
Rs 0.590 0.601 0.594 0.578 0.604 0.595 0.624 0.612 0.624
Ru 0.481 0.502 0.508 0.531 0.539 0.545 0.572 0.588 0.575
MRRp computed over Qm:
Rd 0.667 0.662 0.660 0.675 0.677 0.699 0.702 0.695 0.716
Rp 0.711 0.703 0.680 0.734 0.731 0.748 0.733 0.797 0.782
Rs 0.841 0.822 0.820 0.835 0.828 0.823 0.850 0.856 0.844
Ru 0.800 0.810 0.781 0.795 0.794 0.779 0.806 0.827 0.816
Table 4: Accuracy of instance set labeling, as full-match (MRRf ) or partial-match (MRRp) scores over the evaluation
sets of queries associated with non-filtered instances (Qe) or manually-filtered instances (Qm), for various experi-
mental runs (I-per-Q=number of gold instances available in the input evaluation sets that are used for retrieving class
labels; C-per-I=number of class labels retrieved from IsA repository per input instance)
input set of instances available in Qm relative to
Qe should lead to higher-quality class labels for
the corresponding queries. Second, when I-per-Q
is fixed, increasing C-per-I leads to small, if any,
score improvements. Third, when C-per-I is fixed,
even small values of I-per-Q, such as 3 (that is, very
small sets of instances provided as input) produce
scores that are competitive with those obtained with
a higher value like 10. This suggests that useful class
labels can be generated even in extreme scenarios,
where the number of instances available as input is
as small as 3 or 5. Fourth and most importantly, for
most combinations of parameter settings and on both
query sets, the runs that take advantage of query logs
(Rp, Rs, Ru) produce the highest scores. In particu-
lar, when I-per-Q is set to 10 and C-per-I to 20, run
Ru identifies the original query as an exact match
among the top three to four class labels returned
(score 0.278); and as a partial match among the top
one to two class labels returned (score 0.636), as an
average over the Qe set. The corresponding MRRf
score of 0.278 over the Qe set obtained with run Ru
is 27% higher than with run Rd.
In all experiments, the higher scores of Rp, Rs and
Ru can be attributed to higher-quality lists of class
labels, relative to Rd. Among combinations of pa-
rameter settings described in Table 4, values around
10 for I-per-Q and 20 for C-per-I give the highest
scores over both Qe and Qm.
Among the query-based runs Rp, Rs and Ru, the
highest scores in Table 4 are obtained mostly for run
Rs. Thus, between the presence of a class label and
an instance either in the same query, or as separate
queries within the same query session, it is the lat-
ter that provides a more useful signal during the re-
ranking of class labels of each instance.
Table 5 illustrates the top class labels from the
ranked lists generated in run Rs for various queries
from both Qe and Qm. The table suggests that the
computed class labels are relatively resistant to noise
and variation within the input set of gold instances.
For example, the top elements of the lists of class la-
1612
Query Query Gold Instances Top Labels Generated Using Top 10 Gold In-
stances
Set Cnt. Sample from Top Gold In-
stances
actors Qe 100 abe vigoda, ben kingsley, bill
hickman
actors, stars, favorite actors, celebrities, movie
stars
Qm 28 al pacino, christopher
walken, danny devito
actors, celebrities, favorite actors, movie stars,
stars
computer
languages
Qe 59 acm transactions on math-
ematical software, apple-
script, c
languages, programming languages, programs,
standard programming languages, computer pro-
gramming languages
Qm 17 applescript, eiffel, haskell languages, programming languages, computer
languages, modern programming languages,
high-level languages
european
countries
Qe 60 abkhazia, armenia, bosnia &
herzegovina
countries, european countries, eu countries, for-
eign countries, western countries
Qm 19 belgium, finland, greece countries, european countries, eu countries, for-
eign countries, western countries
endangered
animals
Qe 98 arkive, arabian oryx,
bagheera
species, animals, endangered species, animal
species, endangered animals
Qm 21 arabian oryx, blue whale, gi-
ant hispaniolan galliwasp
animals, endangered species, species, endan-
gered animals, rare animals
park slope
restaurants
Qe 100 12th street bar & grill, aji bar
lounge, anthony?s
businesses, departments
Qm 18 200 fifth restaurant bar, ap-
plewood restaurant, beet thai
restaurant
(none)
renaissance
artists
Qe 95 michele da verona, andrea
sansovino, andrea del sarto
artists, famous artists, great artists, renaissance
artists, italian artists
Qm 11 botticelli, filippo lippi, gior-
gione
artists, famous artists, renaissance artists, great
artists, italian artists
rock bands Qe 65 blood doll, nightmare, rock-
away beach
songs, hits, films, novels, famous songs
Qm 15 arcade fire, faith no more, in-
digo girls
bands, rock bands, favorite bands, great bands,
groups
Table 5: Examples of gold instances available in the input, and actual ranked lists of class labels produced by run Rs for
various queries from the evaluation sets of queries associated with non-filtered gold instances (Qe) or manually-filtered
gold instances (Qm)
bels generated for computer languages are relevant
and also quite similar for Qe vs. Qm, although the
list of gold instances in Qe may contain incorrect
items (e.g., acm transactions on mathematical soft-
ware). Similarly, the class labels computed for eu-
ropean countries are almost the same for Qe vs. Qm,
although the overlap of the respective lists of 10 gold
instances used as input is not large. The table shows
at least one query (park slope restaurants) for which
the output is less than optimal, either because the
class labels (e.g., businesses) are quite distant se-
mantically from the query (for Qe), or because no
output is produced at all, due to no class labels being
found in the IsA repository for any of the 10 input
gold instances (for Qm). For many queries, how-
ever, the computed class labels arguably capture the
meaning of the original query, although not neces-
sarily in the exact same lexical form, and sometimes
only partially. For example, for the query endan-
gered animals, only the fourth class label from Qm
identifies the query exactly. However, class labels
preceding endangered animals already capture the
notion of animals or species (first and third labels),
or that they are endangered (second label).
1613
     0.062
     0.125
     0.250
     0.500
     1.000
     2.000
     4.000
     8.000
    16.000
    32.000
    64.000
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
(no
t in
to
p 
20
)
Pe
rc
en
ta
ge
 o
f q
ue
rie
s
Rank
Query evaluation set: Qe
Full-match
Partial-match
     0.062
     0.125
     0.250
     0.500
     1.000
     2.000
     4.000
     8.000
    16.000
    32.000
    64.000
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
(no
t in
to
p 
20
)
Pe
rc
en
ta
ge
 o
f q
ue
rie
s
Rank
Query evaluation set: Qm
Full-match
Partial-match
Figure 1: Percentage of queries from the evaluation sets,
for which the earliest class labels from the computed
ranked lists of class labels, which match the queries, oc-
cur at various ranks in the ranked lists returned by run
Rs
Figure 1 provides a detailed view on the distribu-
tion of queries from the Qe and Qm evaluation sets,
for which the class label that matches the query oc-
curs at a particular rank in the computed list of class
labels. In the first graph of Figure 1, for Qe, the
query matches the automatically-generated class la-
bel at ranks 1, 2, 3, 4 and 5 for 18.9%, 10.3%, 5.7%,
3.7% and 1.2% of the queries respectively, with full
string matching, i.e., corresponding to MRRf ; and
for 52.6%, 12.4%, 5.3%, 3.7% and 1.7% respec-
tively, with partial string matching, corresponding to
MRRp. The second graph confirms that higher MRR
scores are obtained for Qm than for Qe. In particu-
lar, the query matches the class label at rank 1 and 2
for 50.0% and 17.5% (or a combined 67.5%) of the
queries from Qm, with full string matching; and for
52.6% and 12.4% (or a combined 67%), with partial
string matching.
Discussion: The quality of lists of items extracted
from documents can benefit from query-driven rank-
ing, particularly for the task of ranking class labels
of instances within IsA repositories. The use of
queries for ranking is generally applicable: it can
be seen as a post-processing stage that enhances the
ranking of the class labels extracted for various in-
stances by any method into any IsA repository.
Open-domain class labels extracted from text and
re-ranked as described in this paper are useful in a
variety of applications. Search tools such as Google
Squared return a set of instances, in response to
class-seeking queries (e.g., insurance companies).
The labeling of the returned set of instances, using
the re-ranked class labels available per instances, al-
lows for the generation of query refinements (e.g.,
insurers). In search over semi-structured data (Ca-
farella et al, 2008), the labeling of column cells is
useful to infer the semantics of a table column, when
the subject row of the table in which the column ap-
pears is either absent or difficult to detect.
5 Related Work
The role of anonymized query logs in Web-based
information extraction has been explored in tasks
such as class attribute extraction (Pas?ca and Van
Durme, 2007), instance set expansion (Pennacchiotti
and Pantel, 2009) and extraction of sets of similar
entities (Jain and Pennacchiotti, 2010). Our work
compares the usefulness of queries and query ses-
sions for ranking class labels in extracted IsA repos-
itories. It shows that query sessions produce better-
ranked class labels than isolated queries do. A task
complementary to class label ranking is entity rank-
ing (Billerbeck et al, 2010), also referred to as rank-
ing for typed search (Demartini et al, 2009).
The choice of search queries and query substitu-
tions is often influenced by, and indicative of, vari-
ous semantic relations holding among full queries or
query terms (Jones et al, 2006). Semantic relations
may be loosely defined, e.g., by exploring the ac-
quisition of untyped, similarity-based relations from
query logs (Baeza-Yates and Tiberi, 2007). In com-
parison, queries are used here to re-rank class labels
capturing a well-defined type of open-domain rela-
tions, namely IsA relations.
6 Conclusion
In an attempt to bridge the gap between informa-
tion stated in documents and information requested
1614
in search queries, this study shows that inherently-
noisy queries are useful in re-ranking class labels ex-
tracted from Web documents for various instances,
with query sessions leading to higher quality than
isolated queries. Current work investigates the im-
pact of ambiguous input instances (Vyas and Pantel,
2009) on the quality of the generated class labels.
References
R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic
relations from query logs. In Proceedings of the 13th
ACM Conference on Knowledge Discovery and Data
Mining (KDD-07), pages 76?85, San Jose, California.
M. Banko, Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the Web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-07), pages 2670?2676, Hyderabad, India.
B. Billerbeck, G. Demartini, C. Firan, T. Iofciu, and
R. Krestel. 2010. Ranking entities using Web search
query logs. In Proceedings of the 14th European
Conference on Research and Advanced Technology for
Digital Libraries (ECDL-10), pages 273?281, Glas-
gow, Scotland.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on
the Web. In Proceedings of the 34th Conference on
Very Large Data Bases (VLDB-08), pages 538?549,
Auckland, New Zealand.
G. Demartini, T. Iofciu, and A. de Vries. 2009. Overview
of the INEX 2009 Entity Ranking track. In INitiative
for the Evaluation of XML Retrieval Workshop, pages
254?264, Brisbane, Australia.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web: an experimental study. Artificial Intelligence,
165(1):91?134.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
A. Jain and M. Pennacchiotti. 2010. Open entity ex-
traction from Web search query logs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING-10), pages 510?518,
Beijing, China.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15h
World Wide Web Conference (WWW-06), pages 387?
396, Edinburgh, Scotland.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
class learning from the Web with hyponym pattern
linkage graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-08), pages 1048?1056, Columbus, Ohio.
M. Pas?ca and B. Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from
query logs. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-07),
pages 2832?2837, Hyderabad, India.
M. Pas?ca. 2010. The role of queries in ranking la-
beled instances extracted from text. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING-10), pages 955?962, Bei-
jing, China.
M. Pennacchiotti and P. Pantel. 2009. Entity extrac-
tion via ensemble semantics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP-09), pages 238?247,
Singapore.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801?808, Sydney, Australia.
P. Talukdar, J. Reisinger, M. Pas?ca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-supervised
acquisition of labeled class instances using graph ran-
dom walks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-08), pages 582?590, Honolulu, Hawaii.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243?
1248, Chicago, Illinois.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of the 2009 Conference
of the North American Association for Computational
Linguistics (NAACL-HLT-09), pages 290?298, Boul-
der, Colorado.
1615
Web Search Queries as a Corpus
Tutorial at the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)
Marius Pa?ca
Google Inc.
mars@google.com
ACL 2011                                                        June 2011
Portland, Oregon _           _
Overview
? Part One: Introduction
? Part Two: Queries as a Corpus
? Part Three: Extraction from Queries
Part One: Introduction
? Open-domain information extraction
? Instances, concepts, relations
Unweaving the World Wide Web of Facts
? The Web is a repository of implicitly-encoded human knowledge
? some text fragments contain easier-to-extract knowledge
? More knowledge leads to better answers
? acquire facts from a fraction of the knowledge on the Web
? exploit available facts during search
? Open-domain information extraction
? extract knowledge (facts, relations) applicable to a wide range,
rather than closed, pre-defined set of domains (e.g., medical, 
financial etc.)
? no need to specify set of concepts and relations of interest in 
advance
? rely on as little manually-created input data as possible
Instances, Concepts and Relations
? A concept (class) is a placeholder for a set of instances 
(objects) that share similar properties
? set of instances
? {matrix, kill bill, ice age, pulp fiction, inception, cidade de deus,...}
? class label
? movies, films
? definition
? a series of pictures projected on a screen in rapid succession with 
objects shown in successive positions slightly changed so as to produce 
the optical effect of a continuous picture in which the objects move
(Merriam Webster)
? a form of entertainment that enacts a story by sound and a sequence of 
images giving the illusion of continuous movement (WordNet)
Instances, Concepts and Relations
? Relations are assertions linking two (binary relation) or more (n-
ary relation) concepts
? actors-act in-movies; cities-capital of-countries
? Facts are instantiations of relations, linking two or more 
instances
? leonardo dicaprio-act in-inception; cairo-capital of-egypt
? Attributes correspond to facts capturing quantifiable 
properties of a class or an instance
? actors --> awards, birth date, height
? movies --> producer, release date, budget
Open-Domain Information
diseases
chemical elements
foods
currencies
countries
drugs
yellow fever, influenza, 
bipolar disorder, rocky 
mountain spotted fever, 
anosmia, myxedema,...
potassium, magnesium, 
gold, sulfur, palladium, 
argon, carbon, borium, 
ruthenium, zinc, lead,...
fish, turkey, rice, milk, 
chicken, cheese, eggs, 
corn, beans, wheat, 
asparagus, grapes,...
euro, won, lire, pounds, 
rand, us dollars, yen, 
pesos, pesetas, kroner, 
escudos, shillings,...
australia, south korea, 
kenya, greece, sudan, 
portugal, argentina, 
mexico, cuba, kuwait,...
paxil, lipitor, ibuprofen, 
prednisone, albuterol, 
effexor, azithromycin, 
fluconazole, advil,...
flag
climate
population density
geography
currency
side effects
dosage
price
withdrawal symptoms
generic equivalent
mass
symbol
lewis dot diagram
atomic number
electron configuration
treatment
symptoms
causes
diagnosis
incidence
size
color
calories
taste
allergies
denominations
country
currency converter
symbol
exchange rate
Open-Domain Information
used in the
treatment of
decay 
product of
depletes the 
body of
worth millions 
of
currency
of
good sources 
of
can reduce 
risk of
brand name 
of
is a
form of
diseases
chemical elements
foods
currencies
countries
drugs
yellow fever, influenza, 
bipolar disorder, rocky 
mountain spotted fever, 
anosmia, myxedema,...
potassium, magnesium, 
gold, sulfur, palladium, 
argon, carbon, borium, 
ruthenium, zinc, lead,...
fish, turkey, rice, milk, 
chicken, cheese, eggs, 
corn, beans, wheat, 
asparagus, grapes,...
euro, won, lire, pounds, 
rand, us dollars, yen, 
pesos, pesetas, kroner, 
escudos, shillings,...
australia, south korea, 
kenya, greece, sudan, 
portugal, argentina, 
mexico, cuba, kuwait,...
paxil, lipitor, ibuprofen, 
prednisone, albuterol, 
effexor, azithromycin, 
fluconazole, advil,...
Terminology and Scope
? Terminology
? concept vs. class: used interchangeably
? instance vs. entity: used interchangeably
? Scope
? discussing methods using queries to extract open-domain 
information
? not discussing methods using queries in other tasks such as Web 
search in general (e.g., query suggestion, spelling correction, 
improving search results)
Sources of Open-Domain Information
? Human-compiled knowledge resources
? resources created by experts
? resources created collaboratively by non-experts
? Sources of textual data
? text documents (unstructured or semi-structured text)
? (Web) search queries
Expert Resources
? WordNet
? [Fel98]: C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press 
1998.
? lexical database of English created by experts
? wide-coverage of upper-level conceptual hierarchies
? replicated or extended to other languages
? Cyc
? [Len95]: D. Lenat. CYC: A Large-Scale Investment in Knowledge 
Infrastructure. Communications of the ACM 1995.
? knowledge base of common-sense knowledge created by experts over 100+ 
person-years
? terms and assertions capturing ground assertions and (inference) rules
Collaborative, Non-Expert Resources
? Wikipedia
? [Rem02]: M. Remy. Wikipedia: The Free Encyclopedia. Journal of Online Information 
Review 2002.
? free online encyclopedia developed collaboratively by Web volunteers
? among top 20 most popular Web sites (according to comScore: Top 50 US Web 
Properties, Aug 2009)
? DBpedia
? [BLK+09] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer et al DBpedia ? A Crystallization 
Point for the Web of Data. Journal of Web Semantics 2009.
? community effort to convert Wikipedia articles into structured data
? manually-created ontology, mappings from subset of Wikipedia infoboxes to ontology, 
mappings from Wikipedia articles to WordNet concepts
? Freebase
? [BEP+08]: K. Bollacker, C. Evans, P. Paritosh et al Freebase: A Collaboratively Created 
Graph Database for Structuring Human Knowledge. SIGMOD-08.
? repository for storing structured data from Wikipedia and other sources, as well as 
from user contributions
? collaboratively created, structured and maintained
? Open Mind
? [SLM+02]: P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins and W. Zhu. Open Mind Common 
Sense: Knowledge Acquisition from the General Public. Lecture Notes In Computer 
Science 2002.
? collect common-sense knowledge from non-expert Web users
? unlike Cyc, collect and represent knowledge in natural language rather than through 
formal assertions
Wikipedia
Wikipedia infobox
Wikipedia article
DBpedia, Freebase
Wikipedia infobox Wikipedia infobox source code
<Sears_Tower, previous_building, World_Trade_Center>
<Sears_Tower, construction_period, 1970-1973>
...
DBpedia entries
Quantitative Comparison of
Human-Compiled Resources
? Wikipedia
? 3.5+ million articles in English
? articles also available in 200+ other languages
? DBpedia
? 2.5+ million instances, 250+ million relations
? Freebase
? 20+ million instances, 300+ million relations
? Cyc
? ResearchCyc: 300,000+ concepts and 3+ million assertions
? OpenCyc 2.0: add mappings from Cyc concepts to Wikipedia articles
? Open Mind
? 800,000+ facts in English
? facts also available in other languages
Sources of Open-Domain Information
? Human-compiled knowledge resources
? resources created by experts
? resources created collaboratively by non-experts
? Sources of textual data
? text documents (unstructured or semi-structured text)
? (Web) search queries
Documents
Semi-structured textUnstructured text
Documents
Semi-structured textSemi-structured text
Alternative to Documents
? Conventionally: data for textual information extraction is 
available as (some sort of) a document collection
? documents capture knowledge, or assertions about the world
? assertions are often ?hidden? in expository text
? the goal is to derive some of that knowledge from text
? Alternatively: textual information extraction may be pursued 
even without a document collection
? to find new knowledge within a document collection, users formulate 
their search queries based on the knowledge that they already 
possess at the time of the search
--> query logs collectively capture knowledge, through requests that may 
be answered by knowledge asserted in document collections
Next Topic
? Part One: Introduction
? Part Two: Queries as a Corpus
? Part Three: Extraction from Queries
Queries as a Corpus
? Structure of queries
? Comparison with other textual sources
? Usage, demographics and privacy
Structure of Queries
? [SW07]: S. Bergsma and Q. Wang. Learning Noun Phrase Query Segmentation. 
EMNLP-07.
? identify segments of contiguous query tokens corresponding to semantic concepts, using 
manually annotated queries as training data
? [TP08]: B. Tan and F. Peng. Unsupervised Query Segmentation Using Generative 
Language Models and Wikipedia. WWW-08.
? identify segments of contiguous query tokens corresponding to semantic concepts, using 
evidence from queries and from Wikipedia documents
? [BJR08]: C. Barr, R. Jones and M. Regelson. The Linguistic Structure of English 
Web-Search Queries. EMNLP-08.
? identify structural characteristics of queries in the task of part of speech tagging
? [ML09]: M. Manshadi and X. Li. Semantic Tagging of Web Search Queries. ACL-
IJCNLP-09.
? classify queries into domains, and identify query fragments corresponding to pre-
specified, per-domain schema of tags
? [GXC+09]: J. Guo and G. Xu and X. Cheng and H. Li. Named Entity Recognition in 
Query. SIGIR-09.
? detect instances within queries, and classify instances into coarse-grained classes
? [Li10]: X. Li. Understanding the Semantic Structure of Noun Phrase Queries. 
ACL-10.
? represent noun phrase queries as a combination of intent heads and intent modifiers, 
and identify those components automatically
Finding Structure in Queries
? [BJR08]: C. Barr, R. Jones and M. Regelson. The Linguistic Structure of 
English Web-Search Queries. EMNLP-08.
Part-of-Speech Tags of Query Tokens
? Task
? investigate the task of part-of-speech (POS) tagging when applied to queries
? Input data
? set of 3.2K (2.5K unique) Web search queries, after automatic spell checking 
and tokenization
? Manual annotation of POS tags of query tokens is unreliable
? inter-annotator agreement: 0.79 (token-level), 0.65 (query-level)
? main cause of annotation errors (70% of cases): actual query ambiguity (e.g., 
download may be a noun or a verb) rather than human annotation mistakes
? POS tags have a different distribution in queries than in documents
? in documents (Brown corpus): ~90 distinct tags, of which 15 for determiners, 
and 35 for verbs
? in queries: ~20 distinct tags are sufficient, of which 1 for determiners and 1 
for verbs
Suggested Part-of-Speech Tags
2.4%getverb
2.5%yunknown
.........
preposition
URI
adjective
common noun
proper noun
Part-of-Speech 
Tag
in
ebay.com
big
pictures
texas
Example 
Token
3.7%
5.9%
7.1%
30.9%
40.2%
Percentage of 
Query Tokens
? Nouns are predominant in queries
? most frequent tags in documents: 13% of tokens are common nouns
? most frequent tags in queries: 40% of tokens are proper nouns, 71% of 
tokens are common nouns or proper nouns
? Verbs are infrequent in queries
? in documents: at least one verb in most sentences
? in queries: less than 3% of tokens
(Courtesy R. Jones)
Part-of-Speech Tagging Experiments
? Use of capitalization in queries is inconsistent
? 17% queries contain capitalization, of which 4% are all-caps
? when a query contains mixed capitalization, first-letter token capitalization 
is indicative of an actual proper noun for 73% of cases
? other uses of capitalization in queries: acronyms, capitalization for first 
token of query, first-letter capitalization for all tokens
--> cannot rely on capitalization to identify proper nouns in queries
tagger trained and evaluated on queries with 
automatically-induced capitalization
tagger trained and evaluated on queries with 
perfect capitalization
tagger trained on annotated queries
tagger trained on annotated documents
tagger that assigns most frequent tag (over 
separate training lexicon) of each token
Experimental Setting
70.9%
89.4%
69.7%
48.2%
65.4%
Per-Token Tagging 
Accuracy
Comparison with Other Textual Sources
? [CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie. A Statistical 
Comparison of Tag and Query Logs. SIGIR-09.
? investigate similarity between vocabularies of tokens from search queries vs. tags 
assigned by users to Web documents
? [GNL+10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang. A Comparative 
Study of Bing Web N-gram Language Models for Web Search and Natural 
Language Processing. SIGIR 2010, Web N-gram Workshop.
? generate a repository of n-grams from Web data, including from queries, and evaluate 
it in various text processing tasks
Characteristics of Documents vs. Queries
2-3 words25 words or moreAverage length
bag of keywordsnatural languageGrammatical style
lowhigh (varies)Average quality
self-containedsurrounding textAvailable context
request info.convey info.Purpose
texttextType of medium
QueriesDocument Sentences
Data SourceCharacteristic
Queries vs. Other Textual Sources
? [CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie. A 
Statistical Comparison of Tag and Query Logs. SIGIR-09.
Queries vs. Tags
? Task
? investigate the similarity between query logs and user-generated tags 
(entered by users to annotate documents)
? Input data
? from query logs containing click-through data, and from Delicious (social 
bookmark) tags, select queries and tags associated with a set of 4K Web 
documents
? each document clicked at least 50 times, and associated with a tag at least 
20 times
? generate respective vocabularies (i.e., sets) of tokens for tags and queries, 
after removing stop words and stemming all tokens with the Porter stemmer
Vocabulary SizeToken OccurrencesMetric
Median
Std deviation
Mean
278.0
6464.7
955.3
Queries
393.0
1533.4
1105.8
Tags
15.0
12.8
17.6
Queries
83.0
137.7
139.6
Tags
Query vs. Tag Vocabulary
? Compute overlap between query tokens and tag tokens
? Optionally, remove low frequency tokens or keep high frequency tokens
? Over more than half of 
documents, overlap ? 0.5
--> query vocabulary is very 
similar to tag vocabulary
(Courtesy M. Carman)
Query vs. Tag vs. Document Vocabulary
? Include vocabulary of Web documents in comparison of relative overlap
? Similarity between query 
and document vocabulary 
is higher than between 
query and tag vocabulary
? since documents are 
clicked search results, 
they are likely to contain 
query tokens
? Similarity is lowest 
between tag and 
document vocabulary
? users do not necessarily 
enter tags that appear in 
document content
(Courtesy M. Carman)
Repositories of Distilled Query Data
? [GNL+10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang. A 
Comparative Study of Bing Web N-gram Language Models for Web 
Search and Natural Language Processing. SIGIR 2010, Web N-gram 
Workshop.
Web N-Gram Collection
QueriesDocumentsN-gram Length
4.6B5.1B2.3B148.5B4-grams
1.3B1.1B464.1M11.7B2-grams
3.1B3.1B1.4B60.0B3-grams
5-grams
1-grams
230.0B
1.2B
Body
N/A
60.3M
Anchor Text
N/A
150M
Title
N/A
251.5M
? Language models found to be more similar between queries and 
document title (and queries and document anchor text) than between 
queries and document body
? Language models of n-grams, from Web documents and search queries
Queries as a Corpus
? Structure of queries
? Comparison with other textual sources
? Usage, demographics and privacy
Usage, Demographics and Privacy
? [MC08]: Q. Mei and K. Church. Entropy of Search Logs: How Hard is Search? With 
Personalization? With Backoff? WSDM-08.
? investigate Web search from the perspective of entropy in search logs, and assess the impact of 
aggregated data about users (e.g., from IP addresses) on the outcome of Web search
? [JBS08]: B. Jansen and D. Booth and A. Spink. Determining the Informational, Navigational, 
and Transactional Intent of Web Queries. Journal of Information Processing and 
Management 2008.
? investigate the distribution of queries from the point of view of intent type (and subtypes), and 
automatically classify queries accordingly
? [JBS09]: B. Jansen, D. Booth and A. Spink. Patterns of Query Reformulation During Web 
Searching. Journal of the American Society for Information Science and Technology 2009.
? develop models to classify various types of query reformulations and identify the most frequent ones 
among Web users
? [WC10]: Ingmar Weber and Carlos Castillo. The Demographics of Web Search. Sigir-10.
? study the impact of various user demographics factors on the users? choice of queries
? [JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins. ?I Know What You did Last Summer?: 
Query Logs and User Privacy. CIKM-07.
? study the possibility of uncovering user identity from query logs, despite attempts to remove basic 
personally identifiable information from queries
? [GBG+10]: S. Goel, A. Broder, E. Gabrilovich and B. Pang. Anatomy of the Long Tail: Ordinary 
People with Extraordinary Tastes. WSDM-10.
? [KKM+09]: A. Korolova, K. Kenthapadi, N. Mishra and A. Ntoulas. Releasing Search Queries 
and Clicks Privately. WWW-09.
? investigate methods to generate modified query log data that preserves user privacy
Query Usage
? Search zeitgeist
? capture ?the general intellectual, moral, and cultural climate of an era?
(Merriam Webster), as reflected in the aggregation of search queries 
submitted by Web users
nokia n900
htc evo 4g
nokia 5530
iphone 4
ipad
Consumer 
Electronics
youtube videos
netflix
eminem
shakira
justin bieber
Entertainment
Top Rising Queries (2010)Top  Global 
Events (2010)
ash cloud
oil spill
haiti earthquake
olympics
world cup
(Google Zeitgeist)
Geographical Distribution
? For: ash cloud
(Google Zeitgeist)
Temporal Distribution
? For: circuit city
(Google Trends)
More queries 
submitted later 
during the year(s) 
(shopping season)
More queries 
submitted, due to 
unusual event with 
high news coverage
Query Demographics
? [WC10]: Ingmar Weber and Carlos Castillo. The Demographics of Web 
Search. Sigir-10.
Query Demographics
? Task
? investigate impact of user demographics on Web search
? Input data
? user profile data (birth year, gender, zip code)
? set of pairs of (query, clicked URL) from query logs
? census demographic data for various zip codes
US
Avg.
Query Log DataFeature
1974
14.0
2.3
5.7
88.1
25.6
10.9
22.4
60%
1982
27.3
5.1
15.5
94.4
37.6
16.5
27.7
80%
17.917.37.94.5Non-English (%)
1974196819661956Year of birth
12.34.02.40.9Afric. Amer. (%)
3.64.01.10.4Asian (%)
61.9
12.8
4.5
16.0
20%
78.8
18.1
7.2
18.9
40%
76.9
25.5
11.1
22.7
Avg.
White (%)
BA degree (%)
Below poverty (%)
Per-capita income ($k)
75.1
24.4
12.4
21.6
(Courtesy I. Weber)
Role of Demographics in Web Search
? Highly-discriminant queries for various user demographics
spencer stuart executive search
insight venture partners
federal circuit
four seasons jackson hole
www.unitnet.com
slaker
kipasa
www.tokbox.com
chris jordan
electric candle warmer
www.popsugar.com
ns4w.org
QueryFeature
BA degree (%)
Below poverty (%)
Per-capita income ($k)
(Courtesy I. Weber)
Role of Demographics in Web Search
? Highly-discriminant queries for various user demographics
sina
big bang lyrics
tvb series
jay chou lyrics
trey songz bio
def jam records address
s2s magazine
madinaonline
pulloff.com
central boiler wood furnace
firewood processors
midwest super cub
QueryFeature
Afric. Amer. (%)
Asian (%)
White (%)
Role of Demographics in Web Search
? Highly-discriminant queries for various user demographics
free teen chatrooms
wet seal
tottaly layouts
photofiltre brushes
www.johnshopkinshealthalerts.com
www.envisionreports.com/vz
yahoo free bridge games
bnymellon.mobular.net/bnymellon/frp
QueryFeature
Year of birth, old
Year of birth, young
Queries and User Privacy
? [JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins. ?I Know What You 
did Last Summer?: Query Logs and User Privacy. CIKM-07.
Queries and User Privacy
? Task
? investigate the vulnerability of narrowing down the identify (demographics) 
of users submitting search queries, even after removal of personally 
identifiable information (names, numbers) from query logs
? Input data
? from user profile data (anonymized id, birth year, gender, zip code), select 
100M profiles
? from query logs, select query sessions issued by users with available profile 
data, for 744K users
? Assessment of vulnerability
? arrange data into buckets by age, gender, zip code
? arrange buckets into bins, by conjunctions of age, gender, zip code
? smaller bin size makes it easier to identify a particular user from the bin 
(especially when additional information, e.g., hobbies, is available about the 
user)
? e.g., if input data is arranged into bins that share gender bucket, age bucket, 
and first 3 of 5 zip code digits (e.g., males, age 25-29, living in zip code 
950xx) --> almost 100K of the 744K users fit into a bin of 100 users or less
Deriving Demographics from Queries
? Identifying user gender and age
? classifiers using bag-of-words features
? gender identification: accuracy of 83.8%
? examples of discriminative features: {bridal, makeup, hair, women?s,..} for women; 
{nfl, poker, male, compusa,..} for men
? age identification: absolute error of 7 years (predicted vs. actual), better 
than always guessing the middle age point
? examples of discriminative features: {myspace, pregnancy, wikipedia, mall,..} for 
lower age; {aarp, lottery, amazon.com, senior, repair,..} for higher age
? if personally identifiable information (names and numbers) are removed from 
queries, both gender and age classification remain about as accurate
? Identifying location (zip code)
? existing classifier for locations: given query as input, output list of locations
? convert list of locations into zip code buckets of known first 3, 4 or 5 digits
? if personally identifiable information (names and numbers) are removed from 
queries, location classification becomes much less accurate
13.1%
6.2%
First 5
54.1%
34.9%
First 3
251.%
13.7%
First 4Known Digits of Zip Code
Correct among top three
Correct at top one
Deriving Queries from Known Information
? Identifying query sessions submitted by a known user
? use demographics, conversations with, lifestyle changes of user, in order to 
guess queries that may have been submitted by user
? as an approximation, manually create a set of guessed queries
bassmaster (388)
skulling (17)
skiing (9618)
football (123802)
Sports
assam (747)pizza (104888)
italian restaurant (4998)
brie (39325)
Food
harry potter (27838)
danielle steele (238)
freakonomics (574)
volkswagen beetle (478)
honda odyssey (1504)
toyota prius (1070)
Common
holly lisle (20)
elizabeth moon (27)
triumph tr23 (23)
e-type jaguar (5)
RareCategory
Books
Cars
? use combinations of guessed queries
Knowing that a 
user submitted 
the query e-
type jaguar 
narrows down 
the identity of 
the user to a 
bin of 5 
possible users
(Courtesy R. Jones)
Deriving Queries from Known Information
1brie, holly lisle, pizza
27harry potter, volkswagen beetle
......
2pizza, triumph tr3
2430football, skiing
1441italian restaurant, pizza
1
4855
Bin SizeQuery Combination
danielle steele, volkswagen beetle
harry potter, pizza
--> even if individual bits of information are far from unique among users, 
putting them together can uniquely identify a user
Next Topic
? Part One: Introduction
? Part Two: Queries as a Corpus
? Part Three: Extraction from Queries
Extraction Methods
? Methods for extraction of:
? instances and concepts
? attributes and relations
Instances and Concepts
diseases
chemical elements
foods
currencies
countries
drugs
yellow fever, influenza, 
bipolar disorder, rocky 
mountain spotted fever, 
anosmia, myxedema,...
potassium, magnesium, 
gold, sulfur, palladium, 
argon, carbon, borium, 
ruthenium, zinc, lead,...
fish, turkey, rice, milk, 
chicken, cheese, eggs, 
corn, beans, wheat, 
asparagus, grapes,...
euro, won, lire, pounds, 
rand, us dollars, yen, 
pesos, pesetas, kroner, 
escudos, shillings,...
australia, south korea, 
kenya, greece, sudan, 
portugal, argentina, 
mexico, cuba, kuwait,...
paxil, lipitor, ibuprofen, 
prednisone, albuterol, 
effexor, azithromycin, 
fluconazole, advil,...
Instances and Concepts
? [Pas07]: M. Pa?ca. Weakly-Supervised Discovery of Named Entities using Web 
Search Queries. CIKM-07.
? expand sets of instances using Web search queries
? [VP08]: B. Van Durme and M. Pa?ca. Finding Cars, Goddesses and Enzymes: 
Parametrizable Acquisition of Labeled Instances for Open-Domain Information 
Extraction. AAAI-08.
? extract labeled sets of instances from Web documents, by merging clusters of 
distributionally similar phrases with IsA pairs extracted with lexico-syntactic patterns
? [PP09]: M. Pennacchiotti and P. Pantel. Entity Extraction via Ensemble Semantics. 
EMNLP-09.
? expand sets of instances using multiple sources of text including queries
? [AHH09]: E. Alfonseca and K. Hall and S. Hartmann. Large-Scale Computation of 
Distributional Similarities for Queries. NAACL-HLT-2009.
? apply vector-space model of distributional similarities to queries rather than documents
? [JP10]: A. Jain and P. Pantel. Open Entity Extraction from Web Search Query 
Logs. COLING-10.
? extract clusters of distributionally similar phrases from Web search queries and click-
through data
Instances and Concepts
? [VP08]: B. Van Durme and M. Pa?ca. Finding Cars, Goddesses and 
Enzymes: Parametrizable Acquisition of Labeled Instances for Open-
Domain Information Extraction. AAAI-08.
Extraction from Documents and Queries
? Input
? target relation, available as a small set of extraction patterns
? e.g., <C [such as|including] I>
? Data sources
? collection of Web documents
? collection of anonymized Web search queries
? Output
? sets of instances, each set associated with a class label
? e.g., marine animals = {whales, seals, dolphins, turtles, sea lions, fishes, 
penguins, squids, pacific walrus, aquatic birds, comb jellies, starfish, 
florida manatees, walruses,...}
? each set alo associated with lists of attributes
Acquisition of Open-Domain Classes
? Define a closed vocabulary of potential class instances, as the 
set of most frequently-submitted Web search queries
? textual data source: Web query logs
? output: noisy set of potential class instances
? Acquire class labels for potential class instances, via hand-
written extraction patterns
? textual data source: Web documents
? <C [such as|including] I>, where C is a potential class label (e.g., 
zoonotic diseases) and I is a potential instance (e.g., brucellosis)
? output: noisy pairs of an instance and a class label
? Organize potential class instances into sets of distributionally
similar phrases
? output: noisy sets of distributionally similar instances
Merge into labeled sets of instances
Extraction of Labeled Instances
Input:   - pairs of an instance and a class label
- unlabeled sets of distributionally similar instances
Output: - sets of instances, each set associated with a class label
For each unlabeled set of distributionally-similar instances S
For each class label L assigned to some instance(s) of set S
A=set of instances of S whose class label is L
B=set of sets that contain some instance(s) whose label is L
If |A| > J?|S|:
If |B| < K:
Collect instances of A, associated with the class label L
tf
idf
? Note: J, K are weighting parameters controlling precision/recall
? J in [0,1); higher J --> higher precision
? K is non-negative integer; lower K --> higher precision
george w. bush
j. carter
bill clinton
nixon
ronald reagan
al sharpton
hillary clinton
gm
volvo
ford
schwinn
toyota
lettuce
corn
broccoli
carrot
apple
orange
rose
banana
mango
benjamin franklin
george washington
paul revere
jefferson
john adams
abe lincoln
Presidents
FruitsCar Companies
Patterns and Distributional Similarities
(Courtesy B. Van Durme)
Instances and Concepts
? [PP09]: M. Pennacchiotti and P. Pantel. Entity Extraction via Ensemble 
Semantics. EMNLP-09.
Extraction from Multiple Sources
? Input
? target classes, available as small sets of seed instances
? e.g., {jodie foster, humphrey bogart, anthony hopkins} for Actor
? target classes, also available as small sets of seed relations with other 
classes
? e.g., < leonardo dicaprio, inception>, <nicole kidman, eyes wide shut> for Actor 
(corresponding to relation Actor-act in-Movie)
? Data sources
? collection of Web documents
? collection of Web search queries
? HTML tables identified within the collection of Web documents
? collection of articles from Wikipedia
? Output
? ranked lists of instances, one per class
? e.g., [gordon tootoosis, rosalind chao, john hawkes, jeffrey dean morgan,...] for 
Actor
Ensemble Semantics
S1
SK
S2
KE
n
KE
2
KE
1
FG1 FG2 FGm
KB
FEATURE GENERATORS
RANKER
KN
O
W
LE
DG
E 
EX
TR
AC
TO
RS
AG
G
RE
G
AT
O
R
MODELER
DECODER
(Courtesy P. Pantel, M. Pennacchiotti)
Extraction Components
? Sources (S1, S2,..., Sk)
? data sources from which instances and their relevant features are 
extracted
? Knowledge extractors (KE1, KE2,..., KEn)
? extract candidate instances from sources, using various algorithms
? Feature generators (FG1, FG2,..., FGm)
? collect evidence/features relevant to deciding whether candidate
instances are correct or not
? Aggregator
? combine evidence available from multiple sources for candidate 
instances
? Ranker
? rank candidate instances extracted by knowledge extractors, based 
on features available from feature generators
Ranking Features
? Collected by feature generators
? 4 feature families: from Web documents, queries, tables, Wikipedia
? 5 feature types: frequency, co-occurrence, distributional, pattern, 
termness (i.e., checking whether extracted terms are well-formed)
(Courtesy P. Pantel, M. Pennacchiotti)
Extraction Results
? Input data = collection of 600 million Web documents; tables identified 
within the documents; one year of queries; 2 million Wikipedia articles
? Evaluate lists of instances extracted for 3 classes: Actor, Athlete and 
Musician
? create gold standard from samples of 500 instances selected randomly for 
each class
? compute precision of extracted lists of instances, relative to and over the 
gold standards
? Average precision: 0.860 (Actor), 0.915 (Athlete), 0.788 (Musician)
? Precision@100: 0.99 (Athlete)
? Estimated precision@22000: 0.97 (Athlete)
Instances and Concepts
? [JP10]: A. Jain and P. Pantel. Open Entity Extraction from Web Search 
Query Logs. COLING-10.
Extraction from Queries
? Data sources
? anonymized search queries along with frequencies and click-through data 
(clicked search results)
? Web documents
? Output
? clusters of similar instances
? e.g., {basic algebra, numerical analysis, discrete math, lattice theory, nonlinear 
physics, ...}, {aaa insurance, roadside assistance, personal liability insurance, 
international driving permits, ...}
? Steps
? collect set of candidate instances from queries
? cluster instances using context in queries or click-through data or both
Similarity in Documents vs. Queries
? Contextual space of Web documents
? an instance is represented by the contexts in which it appears in text 
documents
? instances are modeled ?objectively?, according to descriptions of the world
? Contextual space of Web search queries
? an instance is represented by the contexts in which it appears in a search 
queries
? instances are modeled ?subjectively?, according to users? perception of the 
world
britney spears
celine dion
bruce springsteen
paris hilton
serena williams
britney spears galapagos islands
south america cruise
kauai snorkeling
Contextual space of Web documents Contextual space of Web search queries
galapagos islands
tasmania
guinea
Other singers Other celebritiesOther regions Other island travel topics
Extraction of Instances
? Identify candidate instances
? intuition: in queries composed by copying fragments from Web documents 
and pasting them into queries, capitalization of instances is preserved
? from queries containing capitalization, extract contiguous sequences of 
capitalized tokens as instances
Queries Candidate Instances
Britney Spears new song --> Britney Spears
travel to Italy Roma --> Italy Roma
restaurant Cascal in Mountain View --> Cascal, Mountain View
? Retain set of best candidate instances
? first criterion: promote candidate instances whose capitalization is frequent 
in Web documents
? second criterion: promote candidate instances that occur as full-length 
queries
? retain set of candidate instances that score highly (above some thresholds) 
according to both criteria
(Courtesy A. Jain)
Clustering of Instances
? Induce unlabeled classes of instances, by clustering instances using 
features collected from queries
? as an alternative to collecting features from unstructured text in documents
? for efficiency, no attempt to parse the queries
? Context features
? vector of elements corresponding to contexts, where a context is the prefix 
and postfix around the instance, from queries containing the instance
? Click-through features
? vector of elements corresponding to documents, where a document is one 
that is clicked by a user submitting the instance as a full-length query
? Hybrid features
? normalized combination of context and click-through vectors
Impact of Clustering Features
? Given an instance, manually 
judge each co-clustered 
instance:
? ?If you were interested in 
instance I, would you also 
be interested in instance Ic 
in any intent??
? also, annotate with type of 
relation between instance 
and co-clustered instance
? Compute precision, over a 
set of evaluation instances
? CL-CTX: context
? CL-CLK: click-through
? CL-HYB: hybrid
? CL-Web: context collected 
from Web documents 
rather than queries
0.46CL-CTX
0.85CL-HYB
0.73CL-Web
0.81CL-CLK
PrecisionMethod
MethodRelation
Type
0.020.01-0.01child
0.01
-
0.72
0.27
CL-Web
0.03
0.09
0.43
0.46
CL-CTX
0.12
0.13
0.29
0.46
CL-CLK
0.32sibling
0.16synonym
0.40topic
0.09parent
CL-HYB
Extraction Methods
? Methods for extraction of:
? instances and concepts
? attributes and relations
Attributes and Relations
diseases
chemical elements
foods
currencies
countries
drugs
yellow fever, influenza, 
bipolar disorder, rocky 
mountain spotted fever, 
anosmia, myxedema,...
potassium, magnesium, 
gold, sulfur, palladium, 
argon, carbon, borium, 
ruthenium, zinc, lead,...
fish, turkey, rice, milk, 
chicken, cheese, eggs, 
corn, beans, wheat, 
asparagus, grapes,...
euro, won, lire, pounds, 
rand, us dollars, yen, 
pesos, pesetas, kroner, 
escudos, shillings,...
australia, south korea, 
kenya, greece, sudan, 
portugal, argentina, 
mexico, cuba, kuwait,...
paxil, lipitor, ibuprofen, 
prednisone, albuterol, 
effexor, azithromycin, 
fluconazole, advil,...
flag
climate
population density
geography
currency
side effects
dosage
price
withdrawal symptoms
generic equivalent
mass
symbol
lewis dot diagram
atomic number
electron configuration
treatment
symptoms
causes
diagnosis
incidence
size
color
calories
taste
allergies
denominations
country
currency converter
symbol
exchange rate
Attributes and Relations
used in the
treatment of
decay 
product of
depletes the 
body of
worth millions 
of
currency
of
good sources 
of
can reduce 
risk of
brand name 
of
is a
form of
diseases
chemical elements
foods
currencies
countries
drugs
yellow fever, influenza, 
bipolar disorder, rocky 
mountain spotted fever, 
anosmia, myxedema,...
potassium, magnesium, 
gold, sulfur, palladium, 
argon, carbon, borium, 
ruthenium, zinc, lead,...
fish, turkey, rice, milk, 
chicken, cheese, eggs, 
corn, beans, wheat, 
asparagus, grapes,...
euro, won, lire, pounds, 
rand, us dollars, yen, 
pesos, pesetas, kroner, 
escudos, shillings,...
australia, south korea, 
kenya, greece, sudan, 
portugal, argentina, 
mexico, cuba, kuwait,...
paxil, lipitor, ibuprofen, 
prednisone, albuterol, 
effexor, azithromycin, 
fluconazole, advil,...
Attributes and Relations
? [PV07]: M. Pa?ca and B. Van Durme. What You Seek is What You Get: Extraction 
of Class Attributes from Query Logs. IJCAI-07.
? apply small set of patterns to extract attributes from queries
? [PVG07]: M. Pa?ca, B. Van Durme and N. Garera. The Role of Documents vs. 
Queries in Extracting Class Attributes from Text. CIKM-07.
? apply patterns to extract attributes from unstructured text in documents vs. queries
? [Pas07]: M. Pa?ca. Organizing and Searching the World Wide Web of Facts -
Step Two: Harnessing the Wisdom of the Crowds. WWW-07.
? expand sets of seed attributes using queries
? [LWA09]: X. Li, Y. Wang and A. Acero. Extracting Structured Information from 
User Queries with Semi-Supervised Conditional Random Fields. SIGIR-09.
? detect relevant fields in product-search queries, using click data and document content
? [PER+10]: M. Pa?ca, E. Alfonseca, E. Robledo-Arnuncio, R. Martin-Brualla and K. 
Hall. The Role of Query Sessions in Extracting Instance Attributes from Web 
Search Queries. ECIR-10.
? extract attributes of instances, from sequences of queries within query sessions
? [YTT10]: X. Yin, W. Tan and Y. Tu. Automatic Extraction of Clickable Structured 
Web Contents for Name Entity Queries. WWW-10.
? given a query containing an instance, extract structured data from click data and 
contents of subsequently visited documents
? [SJY11]: A. Das Sarma, A. Jain and C. Yu. Dynamic Relationship and Event 
Discovery. WSDM-11.
? acquire temporally-anchored relations that apply within a given set of instances, using 
queries and (news) documents
Attributes and Relations
? [Pas07]: M. Pa?ca. Organizing and Searching the World Wide Web of 
Facts - Step Two: Harnessing the Wisdom of the Crowds. WWW-07.
Extraction from Queries
? Input
? target classes, available as sets of representative instances
? e.g., {Delphi, Apple Computer, Honda, Oracle, Coca Cola, Toyota, Washington 
Mutual, Delta, Reuters, Target, ...} for Company
? small sets of seed attributes, one per class
? e.g., {headquarters, stock price, ceo, location, chairman} for Company
? Data source
? anonymized search queries along with frequencies
? Output
? ranked (longer) lists of attributes, one per class
? e.g., {headquarters, mission statement, stock price, ceo, code of conduct, stock 
symbol, organizational structure, corporate address, cio, ...} for Company
? Steps
? select candidate attributes, from queries containing an instance
? create internal representation of candidate attributes, from queries 
containing an instance and a candidate attribute
? rank candidate attributes, from similarity between internal representation 
of a candidate attribute and combined internal representation of all seed 
attributes
Class Attribute Extraction
Company: {Delphi, Apple Computer, Honda, Oracle, Coca Cola,
Toyota, Washington Mutual, Delta, Reuters, Target,...}
Company: {headquarters, stock price, ceo, location, chairman}
Seed attributes
Target classes
Company: installing
Company: stock price
Company: accord
Company: headquarters
Company: mission statement
Reference search-signature vectors (one per class)
Company
[ ]      [ ]      [8.1-7 on solaris 8]
prefix    infix                                      postfix
[ ]      [ ]      [cressida water pump]
prefix    infix                                       postfix
[ ]      [company one year]      [target]
prefix                                    infix             postfix
[ ]      [air lines]      [history]
prefix                 infix               postfix
[ ]      [ ]      [1989 sei]
prefix    infix             postfix
[new]      [ ]      [ ]
prefix    infix     postfix
[where is the world]      [for]      [corporation]
prefix        infix       postfix
[ ]    [new]    [impact]
prefix        infix          postfix
[ ]      [for the]      [corporation]
prefix               infix                      postfix
[ ]      [for]      [airlines]
prefix       infix                   postfix
installing toyota cressida water pumporacle 8.1-7 on solaris 8coc  cola company one ye r stock rice targetdel air lines stock price his o yh nda acc rd 1989 sein w honda cordwhere s th  worl headquart s for delphi corporationashington u ual n w h dquarters imp tmissio  s tement for the r cle r orationlta i ines
Query logs
Company: {installing, stock price, accord,
headquarters, mission statement,...}
Pool of candidate attributes
Search-signature vectors (one per candidate attribute)
Company: {headquarters, mission statement, stock price, ceo,
code of conduct, stock symbol, organizational
structure, corporate address, cio,...}
Ranked list of extracted class attributes
Top Extracted Attributes
costume, voice, creator, first appearance, funny pictures, origins, 
cartoon images, cartoon pics, color pages
CartoonChar6
...
7
5
4
3
2
1
......
features, battery life, retail price, mobile review, specification, 
price list, functions, ratings, tips, tricks
CellPhoneModel
transmission, top speed, acceleration, transmission problems, 
owners manual, gas mileage, towing capacity, stalling, maintenance 
schedule, performance parts
CarModel
calories, color, size, allergies, taste, carbs, nutritional 
information, nutrition facts, nutritional value, nutrition
BasicFood
recipients, date, winners list, result, gossip, printable ballot, 
nominees, winners, location, announcements
Award
weight, length, history, fuel consumption, interior photos, 
specifications, photographs, interior pictures, seating 
arrangement, flight deck
AircraftModel
awards, height, age, date of birth, weight, b** ****, birthdate,
birthplace, cause of death, real name
Actor
Top Extracted AttributesClass
Top Extracted Attributes
40
39
38
37
36
35
34
...
date, location, significance, images, importance, timeline, 
summary, pics, maps, photographs
WorldWarBattle
vintage, color, cost, style, taste, vintage chart, pronunciation, 
shelf life, wine ratings, wine reviews
Wine
price, system requirements, creator, official site, official 
website, free game download, concept art, download demo, pc 
cheat codes, reviews
VideoGame
alumni, mascot, dean, economics department, career center, 
graduation 2005, department of psychology, school colors, tuition 
costs, campus map
University
countries, ratification, date, definition, summary, purpose, pros, 
cons, members, picture
Treaty
attacks, leader, goals, meaning, website, leadership, photos, 
images, definition, flag
TerroristGroup
location, seating capacity, architect, address, seating map, 
dimensions, tours, pics, poster, box office
Stadium
......
Top Extracted AttributesClass
Extraction Results
? Input data = 50 million anonymized queries
? Evaluate attributes extracted with hand-written patterns vs. based on 
seeds
40
39
38
37
...
4
3
2
1
0.760.530.850.640.900.72Average (40 Classes)
0.660.000.820.000.850.00WorldWarBattle
0.570.290.870.421.000.40Wine
0.900.440.900.570.900.70VideoGame
0.740.650.850.820.850.90University
.....................
0.860.650.950.901.001.00BasicFood
0.690.240.770.150.950.30Award
0.710.680.850.770.800.80AircraftModel
0.960.741.000.821.000.85Actor
SeedPattSeedPattSeedPatt
@50@20@10
PrecisionClass
Summary
? Do ask, do tell
? if knowledge is prominent, someone will eventually write about it
? if knowledge is prominent, someone will eventually ask about it
? Web search queries are cursory reflections of knowledge encoded 
deeply within unstructured and structured content available in 
documents
? Queries are useful in open-domain information extraction
? each user searches for something; collectively, all users search for 
many (most?) things
? queries often reflect the relative popularity of people, topics,
events etc.
--> useful in the extraction and ranking of instances, classes and
relations
