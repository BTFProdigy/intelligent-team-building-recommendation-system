Proceedings of the 12th Conference of the European Chapter of the ACL, pages 835?842,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Growing Finely-Discriminating Taxonomies from Seeds
of Varying Quality and Size 
Tony Veale
School of Computer Science
University College Dublin
Ireland
tony.veale@ucd.ie
Guofu Li
School of Computer Science
University College Dublin
Ireland
guofu.li@ucd.ie
Yanfen Hao
School of Computer Science
University College Dublin
Ireland
yanfen.hao@ucd.ie
Abstract
Concept taxonomies offer a powerful means 
for organizing knowledge, but this organiza-
tion  must  allow  for  many  overlapping  and 
fine-grained perspectives if a general-purpose 
taxonomy is  to  reflect  concepts  as  they are 
actually employed and reasoned about in ev-
eryday  usage.  We present  here  a  means  of 
bootstrapping  finely-discriminating  tax-
onomies from a variety of different  starting 
points, or seeds, that are acquired from three 
different sources: WordNet, ConceptNet and 
the web at large.  
1 Introduction
Taxonomies  provide  a  natural  and  intuitive 
means of organizing information, from the bio-
logical taxonomies of the Linnaean system to the 
layout of supermarkets and bookstores to the or-
ganizational structure of companies. Taxonomies 
also provide the structural backbone for ontolo-
gies  in  computer  science,  from common-sense 
ontologies like Cyc (Lenat and Guha, 1990) and 
SUMO (Niles and Pease, 2001) to lexical ontolo-
gies like WordNet (Miller  et al, 1990). Each of 
these uses is based on the same root-branch-leaf 
metaphor:  the  broadest  terms  with  the  widest 
scope occupy the highest positions of a taxono-
my, near the root, while specific terms with the 
most local concerns are located lower in the hier-
archy,  nearest  the  leaves.  The  more  interior 
nodes that  a taxonomy possesses,  the finer  the 
conceptual distinctions and the more gradated the 
similarity judgments it can make (e.g., Budanit-
sky and Hirst, 2006).
General-purpose  computational  taxonomies 
are called upon to perform both coarse-grained 
and  fine-grained  judgments.  In  NLP,  for  in-
stance,  the  semantics  of  ?eat?  requires  just 
enough  knowledge  to  discriminate  foods  like 
tofu and cheese from non-foods like  wool  and 
steel, while specific applications in the domain of 
cooking  and  recipes  (e.g.,  Hammond?s  (1986) 
CHEF)  require  enough  discrimination  to  know 
that tofu can be replaced with clotted cheese in 
many recipes because each is a soft, white and 
bland food.  
So while much depends on the domain of us-
age, it remains an open question as to how many 
nodes a good taxonomy should possess. Prince-
ton WordNet,  for  instance,  strives for  as many 
nodes as there are word senses in English, yet it 
also contains a substantial number of composite 
nodes  that  are  lexicalized not  as  single  words, 
but as complex phrases. Print dictionaries intend-
ed for human consumption aim for some econo-
my of structure, and typically do not include the 
meaning  of  phrases  that  can  be  understood  as 
straightforward compositions of the meaning of 
their  parts  (Hanks,  2004).  But  WordNet  also 
serves another purpose, as a lexical knowledge-
base  for  computers,  not  humans,  a  context  in 
which concerns about space seem quaint. When 
space is not a issue, there seems no good reason 
to exclude nodes from a concept taxonomy mere-
ly for being composites of other ideas; the real 
test of entry is whether a given node adds value 
to a taxonomy, by increasing its level of internal 
organization through the systematic dissection of 
overly broad categories into finer, more intuitive 
and manageable clusters.
In this paper we describe a means by which 
finely-discriminating  taxonomies  can  be  grown 
from  a  variety  of  different  knowledge  seeds. 
These taxonomies comprise composite categories 
that  can  be  lexicalized  as  phrases  of  the  form 
?ADJ NOUN?, such as Sharp-Instrument, which 
represents the set of all instruments that are typi-
cally considered sharp, such as knives, scissors, 
chisels and can-openers. While WordNet aleady 
contains  an  equivalent  category,  named  Edge-
835
Tool, which it defines with the gloss ?any cutting 
tool  with a sharp cutting edge?,  it  provides  no 
structural basis for inferring that any member of 
this  category can be considered  sharp.  For  the 
most part, if two ideas (word senses) belong to 
the same semantic category X in WordNet, the 
most we can infer is that both possess the trivial 
property  X-ness.  Our  goal  here  is  to  construct 
taxonomies whose form makes explicit the actual 
properties that accrue from membership in a cat-
egory. 
Past work on related approaches to taxonomy 
creation are discussed in section 2, while section 
3  describes  the  different  knowledge  seeds  that 
serve as the starting point for our bootstrapping 
process. In section 4 we describe the bootstrap-
ping process in more detail;  such processes are 
prone to noise, so we also discuss how the ac-
quired categorizations are validated and filtered 
after each bootstrapping cycle. An evaluation of 
the key ideas is then presented in section 5, to 
determine which seed yields the highest quality 
taxonomy once bootstrapping is completed. The 
paper then concludes with some final remarks in 
section 6.
2 Related Work
Simple pattern-matching techniques can be sur-
prisingly effective for the extraction of lexico-se-
mantic relations from text when those relations 
are expressed using relatively stable and unam-
biguous  syntagmatic  patterns  (Ahlswede  and 
Evens, 1988). For instance, the work of Hearst 
(1992) typifies this surgical approach to relation 
extraction,  in  which a system fishes in  a large 
text for particular word sequences that strongly 
suggest  a  semantic  relationship  such  as  hyper-
nymy  or,  in  the  case  of  Charniak and Berland 
(1999), the part-whole relation. Such efforts offer 
high precision but can exhibit low recall on mod-
erate-sized corpora, and extract just a tiny (but 
very useful) subset of the semantic content of a 
text.  The  KnowItAll system  of  Etzioni  et  al. 
(2004)  employs  the  same  generic  patterns  as 
Hearst (e.g., ?NPs such as NP1, NP2, ??),  and 
more besides, to extract a whole range of facts 
that can be exploited for web-based question-an-
swering.  Cimiano  and  Wenderoth  (2007)  also 
use a range of Hearst-like patterns to find text se-
quences in web-text that are indicative of the lex-
ico-semantic  properties  of  words;  in  particular, 
these  authors  use  phrases  like  ?to  *  a  new 
NOUN? and ?the purpose of NOUN is to *? to 
identify the formal (isa), agentive (made by) and 
telic (used for) roles of nouns.
Snow, Jurafsky and Ng (2004) use supervised 
learning techniques to acquire those syntagmatic 
patterns that prove most useful for extracting hy-
pernym relations from text. They train their sys-
tem using pairs of WordNet terms that exemplify 
the hypernym relation; these are used to identify 
specific sentences in corpora that are most likely 
to express the relation in lexical terms. A binary 
classifier is then trained on lexico-syntactic fea-
tures that are extracted from a dependency-struc-
ture  parse  of  these  sentences.  Kashyap  et  al., 
(2005) experiment with a bootstrapping approach 
to  growing concept  taxonomies  in  the  medical 
domain.  A  gold  standard  taxonomy  provides 
terms that are used to retrieve documents which 
are  then  hierarchically  clustered;  cohesiveness 
measures are used to yield a taxonomy of terms 
that can then further drive the retrieval and clus-
tering cycle. Kozareva  et al (2008) use a boot-
strapping approach that extends the fixed-pattern 
approach  of  Hearst  (1992)  in  two  intriguing 
ways. First, they use a doubly-anchored retrieval 
pattern of the form ?NOUNcat such as NOUNexam-
ple and  *?  to  ground the  retrieval  relative  to  a 
known example of hypernymy,  so that any val-
ues extracted for the wildcard * are likely to be 
coordinate terms of  NOUNexample and even more 
likely to be good examples of NOUNcat. Second-
ly, they construct a graph of terms that co-occur 
within this pattern to determine which terms are 
supported by others,  and by how much.  These 
authors also use two kinds of bootstrapping: the 
first  variation,  dubbed  reckless,  uses the candi-
dates extracted from the double-anchored pattern 
(via *) as exemplars (NOUNexample) for successive 
retrieval cycles; the second variation first checks 
whether a candidate is sufficiently supported to 
be used as an exemplar in future retrieval cycles.
The approach we describe here is most similar 
to that of Kozareva  et al (2008). We too use a 
double-anchored pattern, but place the anchors in 
different  places  to  obtain  the  query  patterns 
?ADJcat NOUNcat such as *? and ?ADJcat * such 
as NOUNexample?. As a result, we obtain a finely-
discriminating  taxonomy  based  on  categories 
that are explicitly annotated with the properties 
(ADJcat)  that  they  bequeath  to  their  members. 
These categories have an obvious descriptive and 
organizational  utility,  but  of  a kind that  one is 
unlikely  to  find  in  conventional  resources  like 
WordNet and Wikipedia. Kozareva et al (2008) 
test their approach on relatively simple and ob-
jective  categories  like  states,  countries (both 
836
closed sets), singers and fish (both open, the for-
mer more so than the latter), but not on complex 
categories in which members are tied both to a 
general category, like food, and to a stereotypical 
property, like  sweet (Veale and Hao, 2007). By 
validating  membership  in  these  complex  cate-
gories  using WordNet-based heuristics,  we  can 
hang these categories and members  on specific 
WordNet senses, and thus enrich WordNet with 
this additional taxonomic structure.
3 Seeds for Taxonomic Growth 
A fine-grained taxonomy can be viewed as a set 
of triples Tijk = <Ci, Dj, Pk>, where Ci denotes a child of the parent term Pk that possesses the dis-
criminating  property  Dj;  in  effect,  each  such 
triple expresses that Ci is a specialization of the 
complex  taxonym  Dj-Pk.  Thus,  the  belief  that 
cola  is  a  carbonated-drink  is  expressed  by the 
triple <cola, carbonated, drink>. From this triple 
we  can  identify  other  categorizations  of  cola 
(such as treat and refreshment) via the web query 
?carbonated * such as cola?, or we can identify 
other similarly fizzy drinks via the query ?car-
bonated  drinks  such  as  *?.  So  this  web-based 
bootstrapping  of  fine-grained  category  hierar-
chies requires that we already possess a collec-
tion  of  fine-grained  distinctions  of  a  relatively 
high-quality.  We  now  consider  three  different 
starting points for this bootstrapping process, as 
extracted from three different resources:  Word-
Net, ConceptNet and the web at large.
3.1 WordNet 
The noun-sense taxonomy of WordNet makes a 
number  of  fine-grained  distinctions  that  prove 
useful in clustering entities into smaller and more 
natural groupings. For instance, WordNet differ-
entiates  {feline,  felid} into  the  sub-categories 
{true_cat,  cat} and  {big_cat,  cat},  the  former 
serving  to  group  domesticated  cats  with  other 
cats of a similar size, the latter serving to cluster 
cats  that  are  larger,  wilder  and  more  exotic. 
However, such fine-grained distinctions are the 
exception rather than the norm in WordNet, and 
not  one of  the  60+ words  of  the  form  Xess in 
WordNet that denote a person (such as huntress,  
waitress, Jewess, etc.) express the defining prop-
erty  female in  explicit  taxonomic  terms. 
Nonetheless, the free-text glosses associated with 
WordNet sense-entries often do state the kind of 
distinctions we would wish to find expressed as 
explicit  taxonyms.  A  shallow  parse  of  these 
glosses  thus  yields  a  sizable  number  of  fine-
grained  distinctions,  such  as  <lioness,  female,  
lion>,   <espresso,  strong,  coffee>  and  both 
<messiah, awaited, king> and <messiah, expect-
ed, deliverer>. 
3.2 ConceptNet 
Despite  its  taxonomic  organization,  WordNet 
owes much to the centralized and authority-pre-
serving  craft  of  traditional  lexicography.  Con-
ceptNet (Liu and Singh, 2004), in contrast, is a 
far less authoritative knowledge-source, one that 
owes more to the workings of the WWW than to 
conventional print dictionaries. Comprising fac-
toids culled from the template-structured contri-
butions of thousands of web users,  ConceptNet 
expresses many relationships that accurately re-
flect  a  public,  common-sense  view on a  given 
topic (from vampires to dentists) and many more 
that are simply bizarre or ill-formed. Looking to 
the relation that interests us here, the IsA rela-
tion,  ConceptNet  tells  us  that  an  espresso is  a 
strong coffee (correctly, like WordNet) but that a 
bagel is a Jewish word (confusing use with men-
tion). Likewise, we find that expressionism is an 
artistic style (correct, though WordNet deems it 
an  artistic movement) but that an  explosion is a 
suicide attack (confusing formal and telic roles). 
Since we cannot trust the content of ConceptNet 
directly, lest we bootstrap from a highly unreli-
able starting point, we use WordNet as a simple 
filter.  While  the  concise  form  of  ConceptNet 
contains over 30,000 IsA propositions, we con-
sider as our seed collection only those that define 
a noun concept (such as ?espresso?) in terms of a 
binary  compound  (e.g.,  ?strong coffee?)  where 
the head of the latter (e.g.,  ?coffee?) denotes a 
WordNet hypernym of some sense of the former. 
This  yields  triples  such  as  <Wyoming,  great,  
state>,  <wreck,  serious,  accident>  and  <wolf,  
wild, animal>.
3.3 Web-derived Stereotypes 
Veale and Hao (2007) also use the observations 
of web-users to acquire common perceptions of 
oft-mentioned ideas, but do so by harvesting sim-
ile expressions of the form ?as ADJ as a NOUN? 
directly from the web.  Their approach hinges on 
the fact that similes exploit stereotypes to draw 
out the salient properties of a target, thereby al-
lowing rich  descriptions of those stereotypes to 
be easily acquired, e.g., that snowflakes are pure 
and unique, acrobats are agile and nimble, knifes 
are  sharp and dangerous,  viruses  are  malicious 
and infectious, and so on. However, because they 
find that almost 15% of their web-harvested sim-
837
iles are ironic (e.g., ?as subtle as a rock?, ?as bul-
letproof as a sponge-cake?, etc.), they filter irony 
from these associations by hand, to yield a siz-
able  database  of  stereotypical  attributions  that 
describes over 6000 noun concepts in terms of 
over  2000  adjectival  properties.  However,  be-
cause Veale and Hao?s data directly maps stereo-
typical properties to simile vehicles, it does not 
provide  a  parent  category  for  these  vehicles. 
Thus, the seed triples derived from this data are 
only partially instantiated;  for  instance,  we ob-
tain <surgeon, skilful, ?>, <virus, malicious, ?> 
and <dog, loyal, ?>.  This does not prove to be a 
serious  impediment,  however,  as  the  missing 
field  of  each triple  is  quickly identified during 
the first cycle of bootstrapping.
3.4 Overview of Seed Resources 
Neither of these three seeds is an entirely useful 
knowledge-base in its own right. The WordNet-
based seed is clearly a representation of conve-
nience,  since  it  contains  only  those  properties 
that can be acquired from the glosses that happen 
to be amenable  to a simple  shallow-parse.  The 
ConceptNet seed is likewise a small collection of 
low-hanging fruit, made smaller still by the use 
of WordNet as a coarse but very necessary noise-
filter.  And while the simile-derived distinctions 
obtained from Veale and Hao paint a richly de-
tailed  picture  of  the  most  frequent  objects  of 
comparison, this seed offers no coverage for the 
majority of concepts that are insufficiently note-
worthy to be found in web similes. A quantita-
tive comparison of all three seeds is provided in 
Table 1 below.
WordNet ConceptNet Simile
# terms 
in total 12,227 1,133 6512
# triples 
in total 51,314 1808 16,688
# triples 
per term 4.12 1.6 2.56
# fea-
tures 2305 550 1172
Table 1:  The size of seed collections yielded from 
different sources. 
We can see that WordNet-derived seed is clearly 
the largest and apparently the most comprehen-
sive knowledge-source of the  three:  it  contains 
the most terms (concepts), the most features (dis-
criminating properties of those concepts), and the 
most triples (which situate those concepts in par-
ent  categories  that  are  further  specialized  by 
these  discriminating  features).  But  size  is  only 
weakly suggestive of quality, and as we shall see 
in  the  next  section,  even such  dramatic  differ-
ences in scale can disappear after several cycles 
of bootstrapping. In section 5 we will then con-
sider  which  of  these  seeds  yields  the  highest 
quality taxonomies after bootstrapping has been 
applied. 
4 Bootstrapping from Seeds
The seeds of the previous section each represent 
a different starting collection of triples. It is the 
goal of the bootstrapping process to grow these 
collections  of  triples,  to  capture  more  of  the 
terms ? and more of the distinctions ? that a tax-
onomy is expected to know about. The expansion 
set  of  a  triple  Tijk =  <Ci,  Dj,  Pk> is  the  set  of 
triples that can be acquired from the web using 
the  following  query  expansions  (*  is  a  search 
wildcard):
1. ?Dj * such as Ci?
2. ?Dj Pk such as *?
In the first query, a noun is sought to yield anoth-
er categorization of Ci, while in the second, other 
members of the fine-grained category Dj-Pk are 
sought to accompany Ci. In parsing the text snip-
pets  returned by these  queries,  we also exploit 
text sequences that match the following patterns:
3. ?* and Dj Pk such as *?
4. ?* and Dj * such as Ci?
These last two patterns allow us to learn new dis-
criminating  features  by  noting  how  these  dis-
criminators are combined to reinforce each other 
in  some  ad-hoc  category  formulations.  For  in-
stance, the phrase ?cold and refreshing beverages 
such  as  lemonade?  allows  us  to  acquire  the 
triples <lemonade, cold, beverage> and <lemon-
ade, refreshing, beverage>. This pattern is neces-
sary if the bootstrapping process is to expand be-
yond  the  limited  vocabulary  of  discriminating 
features  (Dj)  found in  the  original  seed collec-
tions of triples.
We denote the mapping from a triple T to the 
set of additional triples that can be acquired from 
the web using the above queries/patterns as  ex-
pand(T').  We currently implement this function 
using  the  Google  search  API.  Our  experiences 
with each query suggest  that  200 snippets is  a 
good search range for the first query, while 50 is 
usually more than adequate for the second. 
838
We can now denote the knowledge that is ac-
quired when starting from a given seed collection 
S after t cycles of bootstrapping as KtS. Thus, 
K 0S=S
K 1S=K 0S ?
{T ? T '?S ? T?expand ?T ' ?}
K t?1S =K tS ?
{T ? T '?K tS ? T?expand ?T ' ?}
Web queries, and the small snippets of text that 
they return, offer just a keyhole view of language 
as it is used in real documents.  Unsurprisingly, 
the  new triples  acquired from the  web via  ex-
pand(T') are likely to be very noisy indeed. Fol-
lowing Kozareva et al (2008), we can either in-
dulge  in  reckless  bootstrapping,  which  ignores 
the  question  of  noise  until  all  bootstrapping  is 
finished, or we can apply a noise filter after each 
incremental step.  The latter approach has the ad-
ditional advantage of keeping the search-space as 
small as possible, which is a major consideration 
when bootstrapping from sizable seeds. We use a 
simple WordNet-based filter called near-miss:  a 
new triple <Ci,  Dj,  Pk> is accepted if WordNet 
contains  a  sense  of  Ci that  is  a  descendant  of 
some sense of Pk (a hit), or a sense of Ci that is a 
descendant of the direct hypernym of some sense 
of Pk (a near-miss). This allows the bootstrapping 
process to acquire structures that are not simply a 
decorated version of the basic WordNet taxono-
my,  but  to acquire hierarchical  relations whose 
undifferentiated forms are not in WordNet (yet 
are largely compatible with WordNet). This non-
reckless bootstrapping process can be expressed 
as follows:
K t?1S =K tS ? {T ? T '?K tS ?
T? filter near?miss?expand ?T ' ??}
Figure 1 and figure 2 below illustrate the rate of 
growth  of  triple-sets  from  each  of  our  three 
seeds.
Referring again to table 1, we note that while 
the ConceptNet collection is by far the smallest 
of  the three seeds ? more  that  7 times smaller 
than the simile-derived seed, and almost 40 times 
smaller than the WordNet seed ? this difference 
is  size  shrinks  considerably over  the  course  of 
five  bootstrapping  cycles.  The  WordNet  near-
miss filter ensures that the large body of triples 
grown from each  seed  are  broadly  sound,  and 
that  we  are  not  simply  generating  comparable 
quantities of nonsense in each case.
Figure 1: Growth in the number of acquired triples, 
over 5 cycles of bootstrapping from different seeds.
Figure 2: Growth in the number of terms described by 
the acquired triples, over 5 cycles of bootstrapping 
from different seeds.
4.1 An  Example
Consider cola, for which the simile seed has one 
triple: <cola, refreshing, beverage>. After a sin-
gle cycle of bootstrapping, we find that cola can 
now be described as an effervescent beverage, a 
sweet  beverage,  a  nonalcoholic  beverage and 
more. After a second cycle, we find it described 
as a sugary food, a fizzy drink and a dark mixer. 
After a third cycle, it is found to be a  sensitive 
beverage, an  everyday beverage and a  common 
drink. After a fourth cycle, it is also found to be 
an  irritating food and an  unhealthy drink. After 
the  fifth  cycle,  it  is  found to  be  a  stimulating 
drink, a toxic food and a corrosive substance. In 
all, the single cola triple in the simile seed yields 
14 triples after 1 cycle, 43 triples after 2 cycles, 
72 after 3 cycles, 93 after 4 cycles, and 102 after 
5 cycles. During these bootstrapping cycles, the 
description  refreshing beverage additionally be-
comes  associated  with  the  terms  champagne, 
lemonade and beer. 
0 1 2 3 4 5
0
200000
400000
600000
800000
1000000
1200000
1400000
1600000
1800000 WordNet
Simile
ConceptNet
Bootstrapping Cycle
# 
Tri
ple
s
0 1 2 3 4 5
0
50000
100000
150000
200000
250000
300000
350000
WordNet
Simile
ConceptNet
Bootstrapping Cycle
# 
Te
rm
s
839
5 Empirical Evaluation
The WordNet  near-miss filter thus ensures that 
the parent field (Pk) of every triple contains a val-
ue  that  is  sensible  for  the  given  child  concept 
(Ci), but does not ensure that the discriminating 
property  (Dj)  in  each  triple  is  equally  sensible 
and apropos.  To see  whether the bootstrapping 
process  is  simply  padding  the  seed  taxonomy 
with large quantities of noise,  or whether the ac-
quired Dj values do indeed mark out the implicit 
essence of the Ci terms they describe, we need an 
evaluation framework that can quantify the onto-
logical usefulness of these Dj values. For this, we 
use  the  experimental  setup  of  Almuhareb  and 
Poesio  (2005),  who  use  information  extraction 
from the web to acquire attribute values for dif-
ferent terms/concepts, and who then compare the 
taxonomy that can be induced by clustering these 
values  with the  taxonomic  backbone  of  Word-
Net. 
Almuhareb and Poesio first created a balanced 
set  of  402  nouns  from  21  different  semantic 
classes in WordNet. They then acquired attested 
attribute values for these nouns (such as  hot for 
coffee,  red for car, etc.) using the query "(a|an|
the) * Ci  (is|was)" to find corresponding Dj val-
ues for each Ci. Unlike our work, these authors 
did  not seek to acquire hypernyms  for each Ci 
during this search, and did not try to link the ac-
quired attribute values to a particular branching 
point  (Pk) in the taxonomy (they did,  however, 
seek matching attributes for these values, such as 
Temperature for  hot, but that aspect is not rele-
vant here). They acquired 94,989 attribute values 
in all for the 402 test nouns. These values were 
then used as features of the corresponding nouns 
in  a  clustering  experiment,  using  the  CLUTO 
system of Karypis (2002). By using attribute val-
ues  as  a  basis  for  partitioning  the  set  of  402 
nouns  into  21  different  categories,  Almuhareb 
and Poesio attempted to reconstruct the original 
21  WordNet  categories  from which  the  nouns 
were drawn. The more accurate the match to the 
original WordNet clustering, the more these at-
tribute values can be seen (and used) as a repre-
sentation of conceptual structure. In their first at-
tempt, they achieved just a 56.7% clustering ac-
curacy against the original human-assigned cate-
gories of WordNet. But after using a noise-filter 
to remove almost  half of the web-harvested at-
tribute values, they achieve a higher cluster accu-
racy of 62.7%. More specifically, Poesio and Al-
muhareb achieve a cluster purity of 0.627 and a 
cluster entropy of 0.338 using 51,345 features to 
describe and cluster the 402 nouns.1
We?replicate?the?above?experiments?using?the?
same?402?nouns,?and?assess?the?clustering?accur?
acy ? (again ?using ?WordNet ? as ? a ? gold?standard)?
after?each?bootstrapping?cycle.?Recall?that?we?use?
only?the?Dj?fields?of?each?triple?as?features?for?the?
clustering ?process, ? so ? the ?comparison ?with ? the?
WordNet?gold?standard?is?still?a?fair?one.?Once?
again,?the?goal?is?to?determine?how?much?like?the?
human?crafted ? WordNet ? taxonomy ? is ? the ? tax?
onomy? that ? is ?clustered?automatically ?from?the?
discriminating?words?Dj?only.?The?clustering?ac?
curacy?for?all?three?seeds?are?shown?in?Tables?2,?
3?and?4.
Cycle  E  P # Features Coverage
1st .327 .629 907 66%
2nd .253 .712 1,482 77%
3rd .272 .717 2,114 82%
4th .312 .640 2,473 83%
5th .289 .684 2,752 83%
Table 2: Clustering accuracy using the WordNet seed 
collection (E denotes Entropy and P stands for Purity)
Cycle E P # Features Coverage
1st .115 .842 363 41%
2nd .255 .724 787 59%
3rd .286 .694 1,362 74%
4th .279 .694 1,853 79%
5th .299 .673 2,274 82%
Table 3: Clustering accuracy using the ConceptNet 
seed collection
Cycle E P # Features Coverage
1st .254 .716 837 59%
2nd .280 .712 1,338 73%
3rd .289 .693 1,944 79%
4th .313 .660 2,312 82%
5th .157 .843 2,614 82%
Table 4: Clustering accuracy using the Simile seed 
collection
The test-set of 402 nouns contains some low-fre-
quency words, such as casuarina,  cinchona,  do-
decahedron, and  concavity, and Almuhareb and 
1 We use cluster purity as a reflection of clustering accu-
racy. We express accuracy as a percentage; hence a pu-
rity of 0.627 is seen as an accuracy of 62.7%. 
840
Poesio note that one third of their data-set has a 
low-frequency of between 5-100 occurrences in 
the British National Corpus. Looking to the cov-
erage  column  of  each  table,  we  thus  see  that 
there  are  words  in  the  Poesio  and  Almuhareb 
data set for which no triples can be acquired in 5 
cycles  of  bootstrapping.  Interestingly,  though 
each seed is quite different in origin and size (see 
again Table 1), all reach similar levels of cover-
age (~82%) after  5 bootstrapping cycles.   Test 
nouns for which all three seeds fail to reach a de-
scription  include  yesteryear,  nonce (very rare), 
salient (more typically an adjective), jag, droop,  
fluting,  fete,  throb,  poundage,  stinging,  rouble,  
rupee,  riel,  drachma,  escudo,  dinar,  dirham,  
lira, dispensation,  hoard,  airstream (not typical-
ly a solid compound), riverside and curling. Fig-
ures 3 and 4 summarize the key findings in the 
above tables: while bootstrapping from all three 
seeds converges to the same level of coverage, 
the simile seed clearly produces the highest qual-
ity taxonomy. 
Figure 3: Growth in the coverage from different 
seed sources. 
Figure 4: Divergence in the clustering Purity 
achieved using different seed sources. The results of 
Poesio and Almuhareb are shown as the straight line: 
y = 0.627.
Both  the  WordNet  and  ConceptNet  seeds 
achieve comparable accuracies of 68% and 67% 
respectively  after  5  cycles  of  bootstrapping, 
which compares well with the accuracy of 62.7% 
achieved  by  Poesio  and  Almuhareb.  However, 
the simile seed clearly yields the best accuracy of 
84.3%,  which  also  exceeds  the  accuracy  of 
66.4% achieved by Poesio and Almuhareb when 
using both values  and attributes (such as  Tem-
perature, Color, etc.) for clustering, or the accu-
racy of 70.9% they achieve when using attributes 
alone. Furthermore, bootstrapping from the simi-
le seed yields higher cluster accuracy on the 402-
noun data-set than Veale and Hao (2008) them-
selves achieve with their simile data on the same 
test-set (69.85%). 
But most striking of all is the concision of the 
representations that are acquired using bootstrap-
ping. The simile seed yields a high cluster accu-
racy using a pool of just 2,614 fine discrimina-
tors,  while  Poesio  and  Almuhareb  use  51,345 
features even after their feature-set has been fil-
tered  for  noise.  Though starting  from different 
initial scales, each seed converges toward a fea-
ture-set that is roughly twenty times smaller than 
that used by Poesio and Almuhareb. 
6 Conclusions
These experiments reveal that seed knowledge of 
different authoritativeness, quality and size will 
tend to converge toward roughly the same num-
ber  of  finely  discriminating  properties  and  to-
ward much the same coverage after 5 or so cy-
cles of bootstrapping. Nonetheless, quality wins 
out,  and  the  simile-derived  seed  knowledge 
shows itself to be a clearly superior basis for rea-
soning  about  the  structure  and  organization  of 
conceptual  categories.  Bootstrapping  from  the 
simile  seed yields  a slightly smaller  set of  dis-
criminating features than bootstrapping from the 
WordNet  seed,  one that  is  many times  smaller 
than the Poesio and Almuhareb feature set. What 
matters is that they are the right features to dis-
criminate with. 
There appears to be a number of reasons for 
this  significant  difference  in  quality.  For  one, 
Veale and Hao (2007) show that similes express 
highly  stereotypical  beliefs  that  strongly  influ-
ence the affective disposition of a term/concept; 
negatively  perceived  concepts  are  commonly 
used to exemplify negative properties in similes, 
while  positively perceived  concepts  are  widely 
used to exemplify positive properties. Veale and 
Hao (2008) go on to argue that similes offer a 
very concise snapshot of those widely-held be-
liefs that are the cornerstone of everyday reason-
1 2 3 4 5
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
WordNet
Simile
ConceptNet
Bootstrapping Cycle
Co
ve
rag
e
1 2 3 4 5
0.40
0.50
0.60
0.70
0.80
0.90
1.00
WordNet
Simile
ConceptNet
Poesio & Alm.
Bootstrapping Cycle
Pu
rity
841
ing, and which should thus be the corner-stone of 
any general-purpose taxonomy.  In addition, be-
liefs expressed via the ?as Dj as Ci? form of simi-
les  appear  to  lend  themselves  to  re-expression 
via the ?Dj Pk such as  Ci? form; in each case, a 
concept Ci is held up as an exemplar of a salient 
property  Dj.  Since  the  ?such  as?  bootstrapping 
pattern seeks out  expressions of  prototypicality 
on the web, a simile-derived seed set is likely the 
best starting point for this search.
All three seeds appear to suffer the same cov-
erage limitations,  topping out  at  about  82% of 
the words in the Poesio and Almuhareb data-set. 
Indeed,  after  5  bootstrapping  cycles,  all  three 
seeds give rise to taxonomies that overlap on 328 
words from the 402-noun test-set, accounting for 
81.59% of the test-set. In effect then, bootstrap-
ping stumbles over the same core of hard words 
in each case, no matter the seed that is used. As 
such, the problem of coverage lies not in the seed 
collection, but in the queries used to perform the 
bootstrapping.  The  same  coverage  limitations 
will thus apply to other bootstrapping approaches 
to  knowledge acquisition,  such as  Kozareva  et  
al. (2008), which rely on much the same stock 
patterns.  So  while  bootstrapping may not  be  a 
general  solution  for  acquiring  all  aspects  of  a 
general-purpose taxonomy, it is clearly useful in 
acquiring large swathes  of  such a  taxonomy if 
given  a  sufficiently  high-quality  seed  to  start 
from.
References
Ahlswede, T. and Evans, M. (1988). Parsing vs. Text 
Processing in the analysis of dictionary definitions. 
In Proc. of the 26th Annual Meeting of the ACL, pp 
217-224.
Almuhareb,  A.  and  Poesio,  M.  (2005).  Concept 
Learning  and  Categorization  from  the  Web.  In 
Proc. of the annual meeting of the Cognitive  Sci-
ence Society, Italy, July. 
Budanitsky,  A.  and  Hirst,  G. (2006).   Evaluating 
WordNet-based Measures of Lexical Semantic Re-
latedness. Computational Linguistics, 32(1):13-47.
Cimiano, P. and Wenderoth, J. (2007). Automatic Ac-
quisition  of  Ranked  Qualia  Structures  from  the 
Web.  In Proc. of  the 45th Annual Meeting of  the  
ACL, pp 888-895.
Charniak, E. and Berland, M. (1999). Finding parts in 
very  large  corpora.  In  Proc.  of  the  37th Annual  
Meeting of the ACL, pp 57?64.
Etzioni,  O.,  Kok,  S.,  Soderland,  S.,  Cafarella,  M., 
Popescu, A-M., Weld, D., Downey, D., Shaked, T. 
and Yates,  A. (2004).  Web-scale information ex-
traction  in  KnowItAll  (preliminary  results).  In  
Proc. of the 13th WWW Conference, pp 100?109.
Hammond, K. J. (1986). CHEF : A Model of Case--
based Planning.  In Proc. of the 5th National Con-
ference  on  Artificial  Intelligence,  pp  267--271, 
Philadelphia, Pennsylvania.  American Association 
for Artificial Intelligence. 
Hanks, P. (2004). WordNet: What is to be done? In 
Proc. of GWC?2004, the 2nd Global WordNet con-
ference, Masaryk University, Brno.
Hearst,  M. (1992).  Automatic  acquisition  of  hy-
ponyms  from large  text  corpora.  In  Proc.  of  the 
14th Int.  Conf.  on  Computational  Linguistics,  pp 
539?545.
Kashyap,  V.  Ramakrishnan,  C.  and  Sheth,  T.  A. 
(2005). TaxaMiner: an experimentation framework 
for automated taxonomy bootstrapping.  Int. Jour-
nal of Web and Grid Services 1(2), pp 240-266.
Karypis,  G. (2002).  CLUTO:  A  clustering  toolkit. 
Technical Report 02-017, University of Minnesota. 
http://www-users.cs.umn.edu/~karypis/cluto/.
Kozareva, Z., Riloff, E. and Hovy, E. (2008). Seman-
tic  Class  Learning from the Web with Hyponym 
Pattern Linkage Graphs. In Proc. of the 46th Annu-
al Meeting of the ACL.
Lenat, D. B. and Guha, R. V. (1990). Building large 
knowledge-based  systems:  representation  and  in-
ference in the Cyc project. NY: Addison-Wesley.
Liu, H. and Singh, P. (2004), ConceptNet: A Practical 
Commonsense Reasoning Toolkit.  BT Technology 
Journal, 22(4):211-226.
Miller, G., Beckwith,R., Fellbaum, C., Gross, D. and 
Miller,  K.J.  (1990).  Introduction  to  WordNet:  an 
on-line lexical database. Int. Journal of Lexicogra-
phy, 3(4):235 ? 244.
Niles, I. and Pease, A. (2001). Toward a standard up-
per ontology. In Proc. of the 2nd International Con-
ference  on Formal  Ontology  in  Information  Sys-
tems (FOIS-2001).
Snow, R., Jurafsky, D. and Ng, A. Y. (2004). Learn-
ing syntactic patterns for automatic hypernym dis-
covery.  Advances  in Neural Information Process-
ing Systems 17.
Veale,  T.  and Hao,  Y. (2007).  Making Lexical  On-
tologies Functional and Context-Sensitive. In Proc.  
of the 45th Annual Meeting of the ACL, pp 57?64.
Veale, T. and  Hao, Y. (2008).  A Fluid Knowledge 
Representation for  Understanding and Generating 
Creative Metaphors.  In Proc. of Coling 2008, The  
22nd International  Conference  on  Computational  
Linguistics, Manchester.
842
Proceedings of ACL-08: HLT, pages 523?531,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multilingual Harvesting of Cross-Cultural Stereotypes
Tony Veale
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
tony.veale@ucd.ie
Yanfen Hao
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
yanfen.hao@ucd.ie
Guofu Li
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
li.guofu.l@gmail.com
Abstract
People rarely articulate explicitly what a na-
tive speaker of a language is already assumed
to know. So to acquire the stereotypical
knowledge that underpins much of what is
said in a given culture, one must look to what
is implied by language rather than what is
overtly stated. Similes are a convenient ve-
hicle for this kind of knowledge, insofar as
they mark out the most salient aspects of the
most frequently evoked concepts. In this pa-
per we perform a multilingual exploration of
the space of common-place similes, by min-
ing a large body of Chinese similes from the
web and comparing these to the English sim-
iles harvested by Veale and Hao (2007). We
demonstrate that while the simile-frame is in-
herently leaky in both languages, a multilin-
gual analysis allows us to filter much of the
noise that otherwise hinders the knowledge
extraction process. In doing so, we can also
identify a core set of stereotypical descrip-
tions that exist in both languages and accu-
rately map these descriptions onto a multilin-
gual lexical ontology like HowNet. Finally,
we demonstrate that conceptual descriptions
that are derived from common-place similes
are extremely compact and predictive of onto-
logical structure.
1 Introduction
Direct perception of our environment is just one
of the ways we can acquire knowledge of the
world. Another, more distinctly human approach,
is through the comprehension of linguistic descrip-
tions of another person?s perceptions and beliefs.
Since computers have limited means of human-like
perception, the latter approach is also very much
suited to the automatic acquisition of world knowl-
edge by a computer (see Hearst, 1992; Charniak and
Berland, 1999; Etzioni et al, 2004; Vo?lker et al,
2005; Almuhareb and Poesio, 2005; Cimiano and
Wenderoth, 2007; Veale and Hao, 2007). Thus, by
using the web as a distributed text corpus (see Keller
et al, 2002), a multitude of facts and beliefs can
be extracted, for purposes ranging from question-
answering to ontology population.
The possible configurations of different concepts
can also be learned from how the words denoting
these concepts are distributed; thus, a computer can
learn that coffee is a beverage that can be served hot
or cold, white or black, strong or weak and sweet
or bitter (see Almuhareb and Poesio, 2005). But it
is difficult to discern from these facts the idealized
or stereotypical states of the world, e.g., that one ex-
pects coffee to be hot and beer to be cold, so that if
one spills coffee, we naturally infer the possibilities
of scalding and staining without having to be told
that the coffee was hot or black; the assumptions
of hotness and blackness are just two stereotypical
facts about coffee that we readily take for granted.
Lenat and Guha (1990) describe these assumed facts
as residing in the white space of a text, in the body
of common-sense assumptions that are rarely articu-
lated as explicit statements. These culturally-shared
common-sense beliefs cannot be harvested directly
from a single web resource or document set, but
must be gleaned indirectly, from telling phrases that
are scattered across the many texts of the web.
Veale and Hao (2007) argue that the most pivotal
523
reference points of this world-view can be detected
in common-place similes like ?as lazy as a dog?, ?as
fat as a hippo? or ?as chaste as a nun?. To the extent
that this world-view is ingrained in and influenced
by how we speak, it can differ from culture to cul-
ture and language to language. In English texts, for
example, the concept Tortoise is stereotypically as-
sociated with the properties slowness, patience and
wrinkled, but in Chinese texts, we find that the same
animal is a model of slowness, ugliness, and nutri-
tional value. Likewise, because Chinese ?wine? has
a high alcohol content, the dimension of Strength is
much more salient to a Chinese speaker than an En-
glish speaker, as reflected in how the word? is used
in statements such as??????, which means
?as strong as wine?, or literally, ?as wine equally
strong?.
In this paper, we compare the same web-based
approach to acquiring stereotypical concept descrip-
tions from text using two very different languages,
English and Chinese, to determine the extent to
which the same cross-cultural knowledge is un-
earthed for each. In other words, we treat the web as
a large parallel corpus (e.g., see Resnick and Smith,
2003), though not of parallel documents in dif-
ferent languages, but of corresponding translation-
equivalent phrases. By seeking translation equiva-
lence between different pieces of textually-derived
knowledge, this paper addresses the following ques-
tions: if a particular syntagmatic pattern is useful for
mining knowledge in English, can its translated form
be equally useful for Chinese? To what extent does
the knowledge acquired using different source lan-
guages overlap, and to what extent is this knowledge
language- (and culture-) specific? Given that the
syntagmatic patterns used in each language are not
wholly unambiguous or immune to noise, to what
extent should finding the same beliefs expressed in
two different languages increase our confidence in
the acquired knowledge? Finally, what representa-
tional synergies arise from finding these same facts
expressed in two different languages?
Given these goals, the rest of the paper as-
sumes the following structure: in section 2, we
summarize related work on syntagmatic approaches
to knowledge-acquisition; in section 3, we de-
scribe our multilingual efforts in English and Chi-
nese to acquire stereotypical or generic-level facts
from the web, by using corresponding translations
of the commonplace stereotype-establishing pattern
?as ADJ as a NOUN?; and in section 4, we describe
how these English and Chinese data-sets can be uni-
fied using the bilingual ontology HowNet (Dong and
Dong, 2006). This mapping allows us to determine
the meaning overlap in both data sets, the amount
of noise in each data set, and the degree to which
this noise is reduced when parallel translations can
be identified. In section 5 we demonstrate the
overall usefulness of stereotype-based knowledge-
representation by replicating the clustering experi-
ments of Almuhareb and Poesio (2004, 2005) and
showing that stereotype-based representations are
both compact and predictive of ontological classi-
fication. We conclude the paper with some final re-
marks in section 6.
2 Related Work
Text-based approaches to knowledge acquisition
range from the ambitiously comprehensive, in which
an entire text or resource is fully parsed and ana-
lyzed in depth, to the surgically precise, in which
highly-specific text patterns are used to eke out cor-
respondingly specific relationships from a large cor-
pus. Endeavors such as that of Harabagiu et al
(1999), in which each of the textual glosses in Word-
Net (Fellbaum, 1998) is linguistically analyzed to
yield a sense-tagged logical form, is an example of
the former approach. In contrast, foundational ef-
forts such as that of Hearst (1992) typify the latter
surgical approach, in which one fishes in a large text
for word sequences that strongly suggest a particu-
lar semantic relationship, such as hypernymy or, in
the case of Charniak and Berland (1999), the part-
whole relation. Such efforts offer high precision but
low recall, and extract just a tiny (but very useful)
subset of the semantic content of a text. The Know-
ItAll system of Etzioni et al (2004) employs the
same generic patterns as Hearst ( e.g., ?NPs such
as NP1, NP2, ...?), and more besides, to extract a
whole range of facts that can be exploited for web-
based question-answering. Cimiano and Wenderoth
(2007) also use a range of Hearst-like patterns to
find text sequences in web-text that are indicative
of the lexico-semantic properties of words; in par-
ticular, these authors use phrases like ?to * a new
524
NOUN? and ?the purpose of NOUN is to *? to
identify the agentive and telic roles of given nouns,
thereby fleshing out the noun?s qualia structure as
posited by Pustejovsky?s (1990) theory of the gener-
ative lexicon.
The basic Hearst approach has even proven use-
ful for identifying the meta-properties of concepts
in a formal ontology. Vo?lker et al (2005) show
that patterns like ?is no longer a|an NOUN? can
identify, with reasonable accuracy, those concepts
in an ontology that are not rigid, which is to say,
concepts like Teacher and Student whose instances
may at any point stop being instances of these con-
cepts. Almuhareb and Poesio (2005) use patterns
like ?a|an|the * C is|was? and ?the * of the C is|was?
to find the actual properties of concepts as they are
used in web texts; the former pattern is used to iden-
tify value features like hot, red, large, etc., while
the latter is used to identify the attribute features
that correspond to these values, such as tempera-
ture, color and size. Almuhareb and Poesio go on
to demonstrate that the values and attributes that are
found for word-concepts on the web yield a suffi-
ciently rich representation for these word-concepts
to be automatically clustered into a form resembling
that assigned by WordNet (see Fellbaum, 1998).
Veale and Hao (2007) show that the pattern ?as ADJ
as a|an NOUN? can also be used to identify the
value feature associated with a given concept, and
argue that because this pattern corresponds to that
of the simile frame in English, the adjectival fea-
tures that are retrieved are much more likely to be
highly salient of the noun-concept (the simile ve-
hicle) that is used. Whereas Almuhareb and Poe-
sio succeed in identifying the range of potential at-
tributes and values that may be possessed by a par-
ticular concept, Veale and Hao succeed in identi-
fying the generic properties of a concept as it is
conceived in its stereotypical form. As noted by
the latter authors, this results in a much smaller yet
more diagnostic feature set for each concept. How-
ever, because the simile frame is often exploited for
ironic purposes in web texts (e.g., ?as meaty as a
skeleton?), and because irony is so hard to detect,
Veale and Hao suggest that the adjective:noun pair-
ings found on the web should be hand-filtered to re-
move such examples. Given this onerous require-
ment for hand-filtering, and the unique, culturally-
loaded nature of the noise involved, we use the work
of Veale and Hao as the basis for the cross-cultural
investigation in this paper.
3 Harvesting Knowledge from Similes:
English and Chinese
Because similes are containers of culturally-
received knowledge, we can reasonably expect the
most commonly used similes to vary significantly
from language to language, especially when those
languages correspond to very different cultures.
These similes form part of the linguistic currency of
a culture which must be learned by a speaker, and
indeed, some remain opaque even to the most edu-
cated native speakers. In ?A Christmas Carol?, for
instance, Dickens (1943/1984) questions the mean-
ing of ?as dead as a doornail?, and notes: ?I might
have been inclined, myself, to regard a coffin-nail as
the deadest piece of ironmongery in the trade. But
the wisdom of our ancestors is in the simile?.
Notwithstanding the opacity of some instances of
the simile form, similes are very revealing about the
concepts one most encounters in everyday language.
In section 5 we demonstrate that concept descrip-
tions which are harvested from similes are both ex-
tremely compact and highly predictive of ontolog-
ical structure. For now, we turn to the process by
which similes can be harvested from the text of the
web. In section 3.1 we summarize the efforts of
Veale and Hao, whose database of English similes
drives part of our current investigation. In section
3.2 we describe how a comparable database of Chi-
nese similes can be harvested from the web.
3.1 Harvesting English Similes
Veale and Hao (2007) use the Google API in con-
junction with Princeton WordNet (Fellbaum, 1998)
as the basis of their harvesting system. They first
extracted a list of antonymous adjectives, such as
?hot? or ?cold?, from WordNet, the intuition being
that explicit similes will tend to exploit properties
that occupy an exemplary point on a scale. For ev-
ery adjective ADJ on this list, they then sent the
query ?as ADJ as *? to Google and scanned the
first 200 snippets returned for different noun val-
ues for the wildcard *. The complete set of nouns
extracted in this way was then used to drive a sec-
525
ond harvesting phase, in which the query ?as * as
a NOUN? was used to collect similes that employ
different adjectives or which lie beyond the 200-
snippet horizon of the original search. Based on
this wide-ranging series of core samples (of 200 hits
each) from across the web, Veale and Hao report
that both phases together yielded 74,704 simile in-
stances (of 42,618 unique types, or unique adjec-
tive:noun pairings), relating 3769 different adjec-
tives to 9286 different nouns. As often noted by
other authors, such as Vo?lker et al (2005), a pattern-
oriented approach to knowledge mining is prone to
noise, not least because the patterns used are rarely
leak-free (inasmuch as they admit word sequences
that do not exhibit the desired relationship), and be-
cause these patterns look at small text sequences in
isolation from their narrative contexts. Veale and
Hao (2007) report that when the above 42,618 simile
types are hand-annotated by a native speaker, only
12,259 were judged as non-ironic and meaningful
in a null context. In other words, just 29% of the
retrieved pairings conform to what one would con-
sider a well-formed and reusable simile that conveys
some generic aspect of cultural knowledge. Of those
deemed invalid, 2798 unique pairings were tagged
as ironic, insofar as they stated precisely the oppo-
site of what is stereotypically believed to be true.
3.2 Harvesting Chinese Similes
To harvest a comparable body of Chinese similes
from the web, we also use the Google API, in con-
junction with both WordNet and HowNet (Dong and
Dong, 2006). HowNet is a bilingual lexical ontol-
ogy that associates English and Chinese word labels
with an underlying set of approximately 100,000
lexical concepts. While each lexical concept is de-
fined using a unique numeric identifier, almost all of
HowNet?s concepts can be uniquely identified by a
pairing of English and Chinese labels. For instance,
the word ???? can mean both Tortoise and Cuck-
old in Chinese, but the combined label tortoise|??
uniquely picks out the first sense while cuckold|?
? uniquely picks out the second. Though Chi-
nese has a large number of figurative expressions,
the yoking of English to Chinese labels still serves
to identify the correct sense in almost every case.
For instance, ????? is another word for Cuck-
old in Chinese, but it can also translate as ?green
hat? and ?green scarf?. Nonetheless, green hat|?
?? uniquely identifies the literal sense of ???
?? (a green covering) while green scarf|???
and cuckold|??? both identify the same human
sense, the former being a distinctly culture-specific
metaphor for cuckolded males (in English, a dispos-
sessed lover ?wears the cuckold?s horns?; in Chi-
nese, one apparently ?wears a green scarf?).
We employ the same two-phase design as Veale
and Hao: an initial set of Chinese adjectives are
extracted from HowNet, with the stipulation that
their English translations (as given by HowNet) are
also categorized as adjectives in WordNet. We
then use the Chinese equivalent of the English sim-
ile frame ??* ??ADJ? (literally, ?as-NOUN-
equally-ADJ?) to retrieve a set of noun values that
stereotypically embody these adjectival features.
Again, a set of 200 snippets is analyzed for each
query, and only those values of the Google * wild-
card that HowNet categorizes as nouns are accepted.
In a second phase, these nouns are used to create
new queries of the form ??Noun??*? and the re-
sulting Google snippets are now scanned for adjec-
tival values of *.
In all, 25,585 unique Chinese similes (i.e., pair-
ings of an adjective to a noun) are harvested, link-
ing 3080 different Chinese adjectives to 4162 Chi-
nese nouns. When hand-annotated by a native Chi-
nese speaker, the Chinese simile frame reveals it-
self to be considerably less leaky than the corre-
sponding English frame. Over 58% of these pairings
(14,867) are tagged as well-formed and meaning-
ful similes that convey some stereotypical element
of world knowledge. The Chinese pattern ??*?
?*? is thus almost twice as reliable as the English
?as * as a *? pattern. In addition, Chinese speak-
ers exploit the simile frame much less frequently for
ironic purposes, since just 185 of the retrieved sim-
iles (or 0.7%) are tagged as ironic, compared with
ten times as many (or 7%) retrieved English similes.
In the next section we consider the extent to which
these English and Chinese similes convey the same
information.
4 Tagging and Mapping of Similes
In each case, the harvesting processes for English
and for Chinese allow us to acquire stereotypi-
526
cal associations between words, not word senses.
Nonetheless, the frequent use of synonymous terms
introduces a substantial degree of redundancy in
these associations, and this redundancy can be used
to perform sense discrimination. In the case of En-
glish similes, Veale and Hao (2007) describe how
two English similes ?as A as N1? and ?as A as
N2? will be mutually disambiguating if N1 and
N2 are synonyms in WordNet, or if some sense
of N1 is a hypernym or hyponym of some sense
of N2 in WordNet. This heuristic allows Veale
and Hao to automatically sense-tag 85%, or 10,378,
of the unique similes that are annotated as valid.
We apply a similar intuition to the disambiguation
of Chinese similes: though HowNet does not sup-
port the notion of a synset, different word-senses
that have the same meaning will be associated with
the same logical definition. Thus, the Chinese
word ???? can translate as ?celebrated?, ?fa-
mous?, ?well-known? and ?reputable?, but all four
of these possible senses, given by celebrated|??,
famous|??, well-known|?? and reputable|?
?, are associated with the same logical form in
HowNet, which defines them as a specialization of
ReputationValue|???. This allows us to safely
identify ???? with this logical form. Overall, 69%
of Chinese similes can have both their adjective and
noun assigned to specific HowNet meanings in this
way.
4.1 Translation Equivalence Among Similes
Since HowNet represents an integration of English
and Chinese lexicons, it can easily be used to con-
nect the English and Chinese data-sets. For while
the words used in any given simile are likely to
be ambiguous (in the case of one-character Chinese
words, highly so), it would seem unlikely that an
incorrect translation of a web simile would also be
found on the web. This is an intuition that we can
now use the annotated data-sets to evaluate.
For every English simile of the form <Ae as
Ne>, we use HowNet to generate a range of possible
Chinese variations <Ac0 as Nc0>, <Ac1 as Nc0>,
<Ac0 as Nc1>, <Ac1 as Nc1>, ... by using the
HowNet lexical entries Ae|Ac0, Ae|Ac1, ..., Ne|Nc0,
Ne|Nc1, ... as a translation bridge. If the variation
<Aci as Ncj> is found in the Chinese data-set, then
translation equivalence is assumed between <Ae as
Language Precision Recall F1
English 0.76 0.25 0.38
Chinese 0.82 0.27 0.41
Table 1: Automatic filtering of similes using Translation
Equivalence.
Ne> and <Aci as Ncj>; furthermore, Ae|Aci is as-
sumed to be the HowNet sense of the adjectives Ae
and Aci while Ncj is assumed to be the HowNet
sense of the nouns Ne and Ncj . Sense-tagging is
thus a useful side-effect of simile-mapping with a
bilingual lexicon.
We attempt to find Chinese translation equiva-
lences for all 42,618 of the English adjective:noun
pairings harvested by Veale and Hao; this includes
both the 12,259 pairings that were hand-annotated as
valid stereotypical facts, and the remaining 30,359
that were dismissed as noisy or ironic. Using
HowNet, we can establish equivalences from 4177
English similes to 4867 Chinese similes. In those
mapped, we find 3194 English similes and 4019
Chinese similes that were hand-annotated as valid
by their respective native-speaker judges. In other
words, translation equivalence can be used to sep-
arate well-formed stereotypical beliefs from ill-
formed or ironic beliefs with approximately 80%
precision. The precise situation is summarized in
Table 1.
As noted in section 3, just 29% of raw English
similes and 58% of raw Chinese similes that are har-
vested from web-text are judged as valid stereotyp-
ical statements by a native-speaking judge. For the
task of filtering irony and noise from raw data sets,
translation equivalence thus offers good precision
but poor recall, since most English similes appear
not to have a corresponding Chinese variant on the
web. Nonetheless, this heuristic allows us to reliably
identify a sizeable body of cross-cultural stereotypes
that hold in both languages.
4.1.1 Error Analysis
Noisy propositions may add little but empty con-
tent to a representation, but ironic propositions will
actively undermine a representation from within,
leading to inferences that are not just unlikely, but
patently false (as is generally the intention of irony).
Since Veale and Hao (2007) annotate their data-
527
set for irony, this allows us to measure the number
of egregious mistakes made when using translation
equivalence as a simile filter. Overall, we see that
1% of Chinese similes that are accepted via transla-
tion equivalence are ironic, accounting for 9% of all
errors made when filtering Chinese similes. Like-
wise, 1% of the English similes that are accepted are
ironic, accounting for 5% of all errors made when
filtering English similes.
4.2 Representational Synergies
By mapping WordNet-tagged English similes onto
HowNet-tagged Chinese similes, we effectively ob-
tain two representational viewpoints onto the same
shared data set. For instance, though HowNet
has a much shallower hierarchical organization
than WordNet, it compensates by encapsulating the
meaning of different word senses using simple log-
ical formulae of semantic primitives, or sememes,
that are derived from the meaning of common Chi-
nese characters. WordNet and HowNet thus offer
two complementary levels or granularities of gen-
eralization that can be exploited as the context de-
mands.
4.2.1 Adjective Organization
Unlike WordNet, HowNet organizes its adjec-
tival senses hierarchically, allowing one to obtain
a weaker form of a given description by climb-
ing the hierarchy, or to obtain a stronger form by
descending the hierarchy from a particular sense.
Thus, one can go up from kaleidoscopic|???
? to colored|?, or down from colored|? to
any of motley|??, dappled|??, prismatic|??
?? and even gorgeous|??. Once stereotypi-
cal descriptions have been sense-tagged relative to
HowNet, they can easily be further enhanced or
bleached to suit the context of their use. For exam-
ple, by allowing a Chinese adjective to denote any
of the senses above it or below in the HowNet hi-
erarchy, we can extend the mapping of English to
Chinese similes so as to achieve an improved recall
of .36 (though we note that this technique reduces
the precision of the translation-equivalence heuristic
to .75).
As demonstrated by Almuhareb and Poesio
(2004), the best conceptual descriptions combine
adjectival values with the attributes that they fill.
Because adjectival senses hook into HowNet?s up-
per ontology via a series of abstract taxonyms like
TasteValue|???, ReputationValue|??? and
AmountValue|???, a taxonym of the form At-
tributeValue can be identified for every adjective
sense in HowNet. For example, the English ad-
jective ?beautiful? can denote either beautiful|?,
organized by HowNet under BeautyValue|??
?, or beautiful|?, organized by HowNet un-
der gracious|? which in turn is organized under
GraceValue|???. The adjective ?beautiful? can
therefore specify either the Grace or Beauty at-
tributes of a concept. Once similes have been sense-
tagged, we can build up a picture of most salient at-
tributes of our stereotypical concepts. For instance,
?peacock? similes yield the following attributes via
HowNet: Beauty, Appearance, Color, Pride, Be-
havior, Resplendence, Bearing and Grace; likewise
?demon? similes yield the following: Morality, Be-
havior, Temperament, Ability and Competence.
4.2.2 Orthographic Form
The Chinese data-set lacks counterparts to many
similes that one would not think of as culturally-
determined, such ?as red as a ruby?, ?as cruel as
a tyrant? and ?as smelly as a skunk?. One signifi-
cant reason for this kind of omission is not cultural
difference, but obviousness: many Chinese words
are multi-character gestalts of different ideas (see
Packard, 2000), so that these ideas form an explicit
part of the orthography of a lexical concept. For in-
stance, using HowNet, we can see that skunk|??
is actually a gestalt of the concepts smelly|? and
weasel|?, so the simile ?as smelly as a skunk? is
already somewhat redundant in Chinese (somewhat
akin to the English similes ?as hot as a hotdog? or
?as hard as a hardhat?).
Such decomposition can allow us to find those
English similes that are already orthographically ex-
plicit in Chinese word-forms. We simply look for
pairs of HowNet senses of the form Noun|XYZ and
Adj|X, where X and XYZ are Chinese words and the
simile ?as Adj as a|an Noun? is found in the English
simile set. When we do so, we find that 648 English
similes, from ?as meaty as a steak? to ?as resonant
as a cello?, are already fossilized in the orthographic
realization of the corresponding Chinese concepts.
When fossilized similes are uncovered in this way,
528
the recall of translation equivalence as a noise filter
rises to .29, while its precision rises to .84 (see Table
1)
5 Empirical Evaluation: Simile-derived
Representations
Stereotypes persist in language and culture because
they are, more often than not, cognitively useful:
by emphasizing the most salient aspects of a con-
cept, a stereotype acts as a dense conceptual descrip-
tion that is easily communicated, widely shared,
and which supports rapid inference. To demonstrate
the usefulness of stereotype-based concept descrip-
tions, we replicate here the clustering experiments
of Almuhareb and Poesio (2004, 2005), who in turn
demonstrated that conceptual features that are mined
from specific textual patterns can be used to con-
struct WordNet-like ontological structures. These
authors used different text patterns for mining fea-
ture values (like hot) and attributes (like tempera-
ture), and their experiments evaluated the relative ef-
fectiveness of each as a means of ontological cluster-
ing. Since our focus in this paper is on the harvesting
of feature values, we replicate here only their exper-
iments with values.
Almuhareb and Poesio (2004) used as their ex-
perimental basis a sampling of 214 English nouns
from 13 of WordNet?s upper-level semantic cate-
gories, and proceeded to harvest adjectival features
for these noun-concepts from the web using the tex-
tual pattern ?[a | an | the] * C [is |was]?. This pattern
yielded a combined total of 51,045 value features
for these 214 nouns, such as hot, black, etc., which
were then used as the basis of a clustering algorithm
in an attempt to reconstruct the WordNet classifica-
tions for all 214 nouns. Clustering was performed
by the CLUTO-2.1 package (Karypis, 2003), which
partitioned the 214 nouns in 13 categories on the ba-
sis of their 51,045 web-derived features. Compar-
ing these clusters with the original WordNet-based
groupings, Almuhareb and Poesio report a cluster-
ing accuracy of 71.96%. In a second, larger exper-
iment, Almuhareb and Poesio (2005) sampled 402
nouns from 21 different semantic classes in Word-
Net, and harvested 94,989 feature values from the
web using the same textual pattern. They then ap-
plied the repeated bisections clustering algorithm to
Approach accuracy features
Almuhareb + Poesio 71.96% 51,045
Simile-derived stereotypes 70.2% 2,209
Table 2: Results for experiment 1 (214 nouns, 13 WN
categories).
Approach Cluster Cluster features
purity entropy
Almu. + Poesio
(no filtering) 56.7% 38.4% 94,989
Almu. + Poesio
(with filtering) 62.7% 33.8% 51345
Simile-derived
stereotypes
(no filtering) 64.3% 33% 5,547
Table 3: Results for experiment 2 (402 nouns, 21 WN
categories).
this larger data set, and report an initial cluster purity
measure of 56.7%. Suspecting that a noisy feature
set had contributed to the apparent drop in perfor-
mance, these authors then proceed to apply a variety
of noise filters to reduce the set of feature values to
51,345, which in turn leads to an improved cluster
purity measure of 62.7%.
We replicated both of Almuhareb and Poesio?s
experiments on the same experimental data-sets (of
214 and 402 nouns respectively), using instead the
English simile pattern ?as * as a NOUN? to harvest
features for these nouns from the web. Note that
in keeping with the original experiments, no hand-
tagging or filtering of these features is performed, so
that every raw match with the simile pattern is used.
Overall, we harvest just 2209 feature values for the
214 nouns of experiment 1, and 5547 features for the
402 nouns of experiment 2. A comparison of both
sets of results for experiment 1 is shown is Table 2,
while a comparison based on experiment 2 is shown
is Table 3.
While Almuhareb and Poesio achieve marginally
higher clustering on the 214 nouns of experiment 1,
they do so by using over 20 times as many features.
529
In experiment 2, we see a similar ratio of feature
quantities before filtering; after some initial filtering,
Almuhareb and Poesio reduce their feature set to just
under 10 times the size of the simile-derived feature
set.
These experiments demonstrate two key points
about stereotype-based representations. First, the
feature representations do not need to be hand-
filtered and noise-free to be effective; we see from
the above results that the raw values extracted
from the simile pattern prove slightly more effec-
tive than filtered feature sets used by Almuhareb and
Poesio. Secondly, and perhaps more importantly,
stereotype-based representations prove themselves a
much more compact means (by factor of 10 to 20
times) of achieving the same clustering goals.
6 Conclusions
Knowledge-acquisition from texts can be a process
fraught with complexity: such texts - especially
web-based texts - are frequently under-determined
and vague; highly ambiguous, both lexically and
structurally; and dense with figures of speech, hy-
perbolae and irony. None of the syntagmatic frames
surveyed in section 2, from the ?NP such as NP1,
NP2 ...? pattern of Hearst (1992) and Etzioni et al
(2004) to the ?no longer NOUN? pattern of Vo?lker
et al (2005), are leak-free and immune to noise.
Cimiano and Wenderoth (2007) mitigate this prob-
lem somewhat by performing part-of-speech anal-
ysis on all extracted text sequences, but the prob-
lem remains: the surgical, pattern-based approach
offers an efficient and targeted means of knowledge-
acquisition from corpora because it largely ignores
the context in which these patterns occur; yet one
requires this context to determine if a given text se-
quence really is a good exemplar of the semantic re-
lationship that is sought.
In this paper we have described how stereotyp-
ical associations between adjectival properties and
noun concepts can be mined from similes in web
text. When harvested in both English and Chi-
nese, these associations exhibit two kinds of re-
dundancy that can mitigate the problem of noise.
The first kind, within-language redundancy, allows
us to perform sense-tagging of the adjectives and
nouns that are used in similes, by exploiting the
fact that the same stereotypical association can oc-
cur in a variety of synonymous forms. By recog-
nizing synonymy between the elements of different
similes, we can thus identify the underlying senses
(or WordNet synsets) in these similes. The sec-
ond kind, between-language redundancy, exploits
the fact that the same associations can occur in dif-
ferent languages, allowing us to exploit translation-
equivalence to pin these associations to particular
lexical concepts in a multilingual lexical ontology
like HowNet. While between-language redundancy
is a limited phenomenon, with just 26% of Veale
and Hao?s annotated English similes having Chinese
translations on the web, this phenomenon does allow
us to identify a significant core of shared stereotyp-
ical knowledge across these two very different lan-
guages.
Overall, our analysis suggests that a comparable
number of well-formed Chinese and English similes
can be mined from the web (our exploration finds
approx. 12,000 unique examples of each). This
demonstrates that harvesting stereotypical knowl-
edge from similes is a workable strategy in both lan-
guages. Moreover, Chinese simile usage is charac-
terized by two interesting facts that are of some prac-
tical import: the simile frame ??NOUN??ADJ?
is a good deal less leaky and prone to noise than the
equivalent English frame, ?as ADJ as a NOUN?; and
Chinese speakers appear less willing to subvert the
stereotypical norms of similes for ironic purposes.
Further research is needed to determine whether
these observations generalize to other knowledge-
mining patterns.
References
A. Almuhareb and M. Poesio. 2004. Attribute-Based and
Value-Based Clustering: An Evaluation. In proceed-
ings of EMNLP 2004, pp 158?165. Barcelona, Spain.
A. Almuhareb and M. Poesio. 2005. Concept Learning
and Categorization from the Web. In proceedings of
CogSci 2005, the 27th Annual Conference of the Cog-
nitive Science Society. New Jersey: Lawrence Erl-
baum.
C. Dickens. 1843/1981. A Christmas Carol. Puffin
Books, Middlesex, UK.
C. Fellbaum. 1998. WordNet, an electronic lexical
database. MIT Press.
E. Charniak and M. Berland. 1999. Finding parts in
530
very large corpora. In proceedings of the 37th Annual
Meeting of the ACL, pp 57-64.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In proceedings
of EMNLP-02, pp 230-237.
F. Keller, M. Lapata, and O. Ourioupina. 1990. Building
large knowledge-based systems: representation and
inference in the Cyc project. Addison-Wesley.
G. Karypis. 2003. CLUTO: A clustering toolkit. Univer-
sity of Minnesota.
J. L. Packard. 2000. The Morphology of Chinese: A
Linguistic and Cognitive Approach. Cambridge Uni-
versity Press, UK.
J. Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics 17(4), pp 209-441.
J. Vo?lker, D. Vrandecic and Y. Sure. 2005. Automatic
Evaluation of Ontologies (AEON). In Y. Gil, E. Motta,
V. R. Benjamins, M. A. Musen, Proceedings of the 4th
International Semantic Web Conference (ISWC2005),
volume 3729 of LNCS, pp. 716-731. Springer Verlag
Berlin-Heidelberg.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In proceedings of the 14th
intenatinal conference on Computational Linguistics,
pp 539-545.
O. Etzioni, S. Kok, S. Soderland, M. Cafarella, A-M.
Popescu, D. Weld, D. Downey, T. Shaked and A.
Yates. 2004. Web-scale information extraction in
KnowItAll (preliminary results). In proceedings of the
13th WWW Conference, pp 100-109.
P. Cimiano and J. Wenderoth. 2007. Automatic Acqui-
sition of Ranked Qualia Structures from the Web. In
proceedings of the 45th Annual Meeting of the ACL,
pp 888?895.
P. Resnik and N. A. Smith. 2003. The Web as a parallel
corpus. Computational Linguistics, 29(3),pp 349-380.
S. Harabagiu, G. Miller and D. Moldovan. 1999. Word-
Net2 - a morphologically and semantically enhanced
resource. In proceedings of SIGLEX-99, pp 1-8, Uni-
versity of Maryland.
T. Veale and Y. Hao. 2007. Making Lexical Ontologies
Functional and Context-Sensitive. In proceedings of
the 45th Annual Meeting of the ACL, pp 57-64.
Z. Dong and Q. Dong. 2006. HowNet and the Computa-
tion of Meaning. World Scientific: Singapore.
531
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 7?12,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Specifying Viewpoint and Information Need with Affective Metaphors A System Demonstration of the Metaphor Magnet Web App/Service  Tony Veale                 Guofu Li Web Science and Technology Division,   School of Computer Science & Informatics, KAIST, Daejeon, University College Dublin, South Korea. Belfield, Dublin D4, Ireland. Tony.Veale@gmail.com        Yanfen.Hao@UCD.ie  Abstract Metaphors pervade our language because they are elastic enough to allow a speaker to express an affective viewpoint on a topic without committing to a specific meaning. This balance of expressiveness and inde-terminism means that metaphors are just as useful for eliciting information as they are for conveying information. We explore here, via a demonstration of a system for metaphor interpretation and generation called Metaphor Magnet, the practical uses of metaphor as a basis for formulating af-fective information queries. We also con-sider the kinds of deep and shallow stereotypical knowledge that are needed for such a system, and demonstrate how they can be acquired from corpora and the web. 
1 Introduction Metaphor is perhaps the most flexible and adaptive tool in the human communication toolbox. It is suited to any domain of discourse, to any register, and to the description of any concept we desire. Speakers use metaphor to communicate not just meanings, but their feelings about those meanings. The open-ended nature of metaphor interpretation means that we can use metaphor to simultaneously express and elicit opinions about a given topic. Metaphors are flexible conceits that allow us to express a position while seeking elaboration or refutation of this position from others. A metaphor is neither true or false, but a conceptual model that allow speakers to negotiate a common viewpoint. 
Computational models for the interpretation and elaboration of metaphors should allow speakers to exploit the same flexibility of expression with ma-chines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of fig-urative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that agree might respond by elaborating the metaphor and providing substantiating evidence, while those that disagree might refute the metaphor and switch to another of their own choosing. A well-chosen metaphor can provide the talking points for an informed conversation, allowing a speaker to elicit the desired knowledge as a combi-nation of objective and subjective elements. In IR, such a capability should allow searchers to express their information needs subjectively, via affective metaphors like ?X is a cult?. The goal, of course, is not just to retrieve documents that make explicit use of the same metaphor ? a literal match-ing of non-literal texts is of limited use ?  but to 
7
retrieve texts whose own metaphors are consonant with those of the searcher, and which elaborate upon the same talking points. This requires a com-puter to understand the user?s metaphor, to appre-ciate how other metaphors might convey the same affective viewpoint, and to understand the different guises these metaphors might assume in a  text. IR extends the reach of its retrieval efforts by expanding the query it is given, in an attempt to make explicit what the user has left implicit. Meta-phors, like under-specified queries, have rich meanings that are, for the most part, implicit: they imply and suggest much more than they specify. An expansionist approach to metaphor meaning, in which an affective metaphor is interpreted by gen-erating the space of related metaphors and talking points that it implies, is thus very much suited to a more creative vision of IR, as e.g. suggested by Veale (2011). To expand a metaphorical query (like ?company-X is a cult? or ?company-Y is a dinosaur? or ?Z was a tyrant?), a system must first expand the metaphor itself, into a set of plausible construals of the metaphor (e.g. a company that is viewed as a dinosaur will likely be powerful, but also bloated, lumbering and slow). The system described in this paper, Metaphor Magnet, demonstrates this expansionist approach to metaphorical inference. Users express queries in the form of affective metaphors or similes, perhaps using explicit + or ? tags to denote a positive or negative spin on a given concept. For instance, ?Google is as ?powerful as Microsoft? does not look for documents that literally contain this simi-le, but documents that express viewpoints that are implied by this simile, that is, documents that dis-cuss the negative implications of Google?s power, where these implications are first understood in relation to Microsoft. The system does this by first considering the metaphors that are conventionally used to describe Microsoft, focusing only on those metaphors that evoke the property powerful, and which cast a negative light on Microsoft. The im-plications of these metaphors (e.g., dinosaur, bully, monopoly, etc.) are then examined in the context of Google, using the metaphors that are typically used to describe Google as a guide to what is most apt. Thus, since Google is often described as a giant in web texts, the negative properties and behaviors of a stereotypical giant ? like lumbering and sprawl-ing ? will be considered apt and highlighted. To perform this kind of analysis reliably, for a 
wide range of metaphors and an even wider range of topics,  requires a  robustly shallow approach. We exploit the fact that the Google n-grams (Brants and Franz, 2006) contains a great many copula metaphors of the form ?X is a Y? to under-stand how X is typically viewed on the web. We further exploit a large dictionary of affective stere-otypes to provide an understanding of the +/- prop-erties and behaviors of each source concept Y. Combining these resources allows the Metaphor Magnet system to understand the implications of a metaphorical query ?X as Z? in terms of the quali-ties that are typically considered salient for Z and which have been corpus-attested as apt for X. We describe the construction of our lexicon of affective stereotypes in section 2. Each stereotype is associated with a set of typical properties and behaviors (like sprawling for giant, or inspiring for guru), where the overall affect of each stereotype depends on which subset of qualities is activated in a given context (e.g., giant can be construed posi-tively or negatively, as can baby, soldier, etc.). We describe how Metaphor Magnet exploits these ste-reotypes in section 3, before providing a worked example in section 4 and screenshots in section 5. 2 An Affective Lexicon of Stereotypes We construct the lexicon in two stages. In the first stage, a large collection of stereotypical descrip-tions is harvested from the Web. As in Liu et al (2003), our goal is to acquire a lightweight com-mon-sense representation of many everyday con-cepts. In the second stage, we link these common-sense qualities in a support graph that captures how they mutually support each other in their co-description of a stereotypical idea. From this graph we can estimate positive and negative valence scores for each property and behavior, and default averages for the stereotypes that exhibit them. Similes and stereotypes share a symbiotic rela-tionship: the former exploit the latter as reference points for an evocative description, while the latter are perpetuated by their constant re-use in similes. Expanding on the approach in Veale (2011), we use two kinds of query for harvesting stereotypes from the web. The first, ?as ADJ as a NOUN?, ac-quires typical adjectival properties for noun con-cepts; the second, ?VERB+ing like a NOUN? and ?VERB+ed like a NOUN?, acquires typical verb behaviors. Rather than use a wildcard * in both 
8
positions (ADJ and NOUN, or VERB and NOUN), which yields limited results with a search engine like Google, we generate fully instantiated similes from hypotheses generated via the Google n-grams. Thus, from the 3-gram ?a drooling zombie? we generate the query ?drooling like a zombie?, and from the 3-gram ?a mindless zombie? we gen-erate ?as mindless as a zombie?. Only those similes whose queries retrieve one or more web documents via Google are considered to contain promising associations. But this still gives us over 250,000 web-validated simile associ-ations for our stereotypical model. We quickly fil-ter these candidates manually, to ensure that the contents of the lexicon are of the highest quality. As a result, we obtain rich descriptions for many stereotypical ideas, such as Baby, which is de-scribed via 163 typical properties and behaviors like crying, drooling and guileless. After this filter-ing phase, the stereotype lexicon maps 9,479 stere-otypes to a set of 7,898 properties and behaviors, to yield more than 75,000 pairings. We construct the second level of the lexicon by automatically linking these properties and behav-iors to each other in a support graph. The intuition here is that properties which reinforce each other in a single description (e.g. ?as lush and green as a jungle? or ?as hot and humid as a sauna?) are more likely to have a similar affect than properties which do not support each other. We first gather all Google 3-grams in which a pair of stereotypical properties or behaviors X and Y are linked via co-ordination, as in ?hot and humid? or ?kicking and screaming?. A bidirectional link between X and Y is added to the support graph if one or more stereo-types in the lexicon contain both X and Y. If this is not so, we consider whether both descriptors ever reinforce each other in web similes, by posing the web query ?as X and Y as?. If this query has  non-zero hits, we also add a link between X and Y. Let N denote this support graph, and N(p) de-note the set of neighboring terms to p, that is, the set of properties and behaviors that can mutually support p. Since every edge in N represents an af-fective context, we can estimate the likelihood that a property p is ever used in a positive or negative context if we know the positive or negative affect of enough members of N(p). So if we label enough vertices of N as +  or -, we can interpolate a posi-tive/negative valence score for all vertices p in N. To do this, we build a reference set -R of typi-
cally negative words, and a set +R of typically positive words. Given a few seed members of -R (such as sad, disgusting, evil, etc.) and a few seed members of +R (such as happy, wonderful, etc.), we find many other candidates to add to +R and -R by considering neighbors of these seeds in N. After three iterations in this fashion, we populate +R and -R with approx. 2000 words each. For a property p we can now define N+(p) and N-(p) as follows:    (1)        N+(p) = N(p) ? +R    (2)        N-(p) = N(p) ? -R We can now assign positive and negative valence scores to each vertex p  by interpolating from ref-erence values to their neighbors in N:    (3)   pos(p)   =           |N+(p)|   |N+(p) ? N-(p)|    (4)   neg(p)   =        1  -  pos(p) If a term S denotes a stereotypical idea and is de-scribed via a set of typical properties and behaviors typical(S) in the lexicon, then: 
   (5)        pos(S)   =        ?p?typical(S) pos(p)               |typical(S)| 
   (6)        neg(S)   = 1  -  pos(S) Thus, (5) and (6) calculate the mean affect of the properties and behaviors of S, as represented via typical(S). We can now use (3) and (4) to separate typical(S) into those elements that are more nega-tive than positive (putting a negative spin on S) and into those that are more positive than negative (putting a positive spin on S): (7)  posTypical(S)  = {p | p ? typical(S) ? pos(p) > 0.5} (8)  negTypical(S)  = {p | p ? typical(S) ? neg(p) > 0.5} 2.1 Evaluation of Stereotypical Affect In the process of populating +R and -R, we identi-fy a reference set of 478 positive stereotypes (such as saint and hero) and 677 negative stereotypes (such as tyrant and monster). When we use these reference points to test the effectiveness of (5) and (6) ? and thus, indirectly, of (3) and (4) and of the 
9
stereotype lexicon itself ? we find that 96.7% of the positive stereotypes in +R are correctly as-signed a positivity score greater than 0.5 (pos(S) > neg(S)) by (5), while 96.2% of the negative stereo-types in -R are correctly assigned a negativity score greater than 0.5  (neg(S) > pos(S)) by (6). 
3 Expansion/Interpretation of Metaphors  The Google n-grams are a rich source of affective metaphors of the form Target is Source, such as ?politicians are crooks?, ?Apple is a cult?, ?racism is a disease? and ?Steve Jobs is a god?. Let src(T) denote the set of stereotypes that are commonly used to describe T, where commonality is defined as the presence of the corresponding copula meta-phor in the Google n-grams. To find metaphors for proper-named entities like ?Bill Gates?, we also analyze n-grams of the form stereotype First [Middle] Last, such as ?tyrant Adolf Hitler?. Thus:  src(racism)  =   {problem, disease, joke, sin, poi-son, crime, ideology, weapon} src(Hitler) = {monster, criminal, tyrant, idiot, madman, vegetarian, racist, ?} We do not try to discriminate literal from non-literal assertions, nor do we even try to define liter-ality. We simply assume each putative metaphor offers a potentially useful perspective on a topic T.  Let srcTypical(T) denote the aggregation of all properties ascribable to T via metaphors in src(T): 
   (9) srcTypical (T)   =   M?src(T)typical(M) We can also use the posTypical and negTypical variants in (7) and (8) to focus only on metaphors that project positive or negative qualities onto T.  (9) is especially useful when the source S in the metaphor  T is S  is not a known stereotype in the lexicon, as happens when one describes Apple as Scientology. When the set typical(S) is empty, src-Typical(S) may not be, so srcTypical(S) can act as a proxy representation for S in these cases.   The properties and behaviors that are salient to the interpretation of   T is S   are given by:    (10)  salient (T,S)  =  |srcTypical(T) ?  typical(T)|           ?             |srcTypical(S) ?  typical(S)| In the context of T is S, the metaphorical stereotype  
M ? src(S)?src(T)?{S} is an apt vehicle for T if:    (11)   apt(M, T,S)  = |salient(T,S) ?  typical(M)| > 0 and the degree to which M is apt for T is given by:   (12)  aptness(M,T,S)  =     |salient(T, S) ?  typical(M)|                  |typical(M)| We can construct an interpretation for  T is S  by considering not just {S}, but the stereotypes in src(T) that are apt for T in the context of T is S, as well as the stereotypes that are commonly used to describe S ? that is, src(S) ? that are also apt for T:     (13)  interpretation(T, S)        = {M|M ? src(T)?src(S)?{S} ? apt(M, T, S)} In effect then, the interpretation of  T is S  is itself a set of apt metaphors for T that expand upon S. The elements {Mi} of interpretation(T, S) can now be sorted by  aptness(Mi T, S)  to produce a ranked list of interpretations (M1, M2 ? Mn). For any inter-pretation M, the salient features of M are thus:    (14)  salient(M, T,S) = typical(M) ?  salient (T,S)   If  T is S  is a creative IR query ? to find docu-ments that view T as S ? then interpretation(T, S) is an expansion of  T is S  that includes the com-mon metaphors that are consistent with T viewed as S. For any viewpoint Mi, salient(Mi, T, S) is an expansion of Mi that includes all of the qualities that T is likely to exhibit when it behaves like Mi. 4 Metaphor Magnet: A Worked Example Consider the query ?Google is Microsoft?, which expresses a need for documents in which Google exhibits qualities typically associated with Mi-crosoft. Now, both Google and Microsoft are com-plex concepts, so there are many ways in which they can be considered similar or dissimilar, either in a good or a bad light. However, the most salient aspects of Microsoft will be those that underpin our common metaphors for Microsoft, i.e., stereo-types in src(Microsoft). These metaphors will pro-vide the talking points for the interpretation.  The Google n-grams yield up the following metaphors, 57 for Microsoft and 50 for Google: src(Microsoft) = {king, master, threat, bully, giant, leader, monopoly, dinosaur ?} 
? 
10
 src(Google)   = {king, engine, threat, brand, giant, leader, celebrity, religion ?} So the following qualities are aggregated for each: srcTypical(Microsoft) = {trusted, menacing, ruling,  threatening, overbearing,  admired, commanding, ?} srcTypical(Google)  = {trusted, lurking reigning, ruling, crowned, shining, determined, admired ?} Now, the salient qualities highlighted by the meta-phor, namely salient(Google, Microsoft),  are: {celebrated, menacing, trusted, challenging, estab-lished,  threatening, admired, respected, ?} Thus, interpretation(Google, Microsoft) contains: {king, criminal, master, leader, bully,  threatening, giant, threat, monopoly, pioneer, dinosaur, ?} Suppose we focus on the metaphorical expansion ?Google is king?, since king is the most highly ranked element of the interpretation. Now,  sali-ent(king, Google, Microsoft)  contains: {celebrated, revered, admired, respected, ruling, arrogant, commanding, overbearing, reigning, ?} These properties and behaviors are already implicit in our perception of Google, insofar as they are salient aspects of the stereotypes to which Google is frequently compared. The metaphor ?Google is Microsoft? ? and its expansion ?Google is king? ? simply crystalizes these qualities, from perhaps different comparisons, into a single act of ideation. Consider the metaphor ?Google is -Microsoft?. Since -Microsoft is used to impart a negative spin (+ would impart a positive spin), negTypical is here used in place of typical in (9) and (10). Thus:   srcTypical(-Microsoft)  =    {menacing, threatening, twisted, raging, feared, sinister, lurking, domineering, overbearing, ?}   salient(Google, -Microsoft) =    {menacing, bullying, roaring, dreaded?} Now interpretation(Google, -Microsoft) becomes:     {criminal, giant, threat, bully, victim, devil, ?} In contrast, interpretation(Google, +Microsoft) is:      {king, master, leader, pioneer, partner, ?}  
More focus is achieved with the simile query ?Google is as ?powerful as Microsoft?. In explicit similes, we need to focus on just a subset of  the salient properties, using e.g. this variant of (10):  {p |  p ? salient(Google, Microsoft) ? N(powerful)             ? neg(p) > pos(p)} In this -powerful case, the interpretation becomes:    {bully, giant, devil, monopoly, dinosaur, ?}  5 The  Metaphor Magnet Web App Metaphor Magnet is designed to be a lightweight web application that provides both HTML output (for humans) and XML (for client applications).  The system allows users to enter queries such as Google is ?Microsoft, life is a +game, Steve Jobs is Tony Stark, or even Rasputin is Karl Rove (queries are case-sensitive). Each query is expanded into a set of apt metaphors via mappings in the Google n-grams, and each metaphor is expanded into a set of contextually apt qualities. In turn, each quality is then expanded into an IR query that is used to re-trieve relevant hits from Google. In effect, the sys-tem allows users to interface with a search engine like Google using metaphor and other affective language forms. The demonstration system can be accessed using a standard browser at this URL:      http://boundinanutshell.com/metaphor-magnet Metaphor Magnet can exploit the properties and behaviors of its stock of almost 10,000 stereotypes, and can infer salient qualities for many proper-named entities like Karl Rove and Steve Jobs using a combination of copula statements from the Google n-grams (e.g., ?Steve Jobs is a visionary?) and category assignments from Wikipedia. The interpretation of the simile/query ?Google is as -powerful as Microsoft? thus highlights a selec-tion of affective viewpoints on the source concept, Microsoft, and picks out an apt selection of view-points on the target Google. Metaphor Magnet dis-plays both selections as phrase clouds in which each hyperlinked phrase ? a combination of an apt stereotype and a salient quality ? is clickable, to yield linguistic evidence for the selection and cor-responding web-search results (via a Google gadg-et). The phrase cloud representing Microsoft in this simile is shown in the screenshot of Figure 1, while the phrase cloud for Google is shown in Figure 2. 
11
 Figure 1. A screenshot of a phrase cloud for the perspective cast upon the source ?Microsoft? by the simile ?Google is as ?powerful as Microsoft?.  
 Figure 2. A screenshot of a phrase cloud for the perspective cast upon the target term ?Google? by the simile ?Google is as ?powerful as Microsoft?.  Metaphor Magnet demonstrates the potential utili-ty of affective metaphors in human-computer lin-guistic interaction, and acts as a web service from which other NL applications can derive a measure of metaphorical competence. When accessed as a service, Metaphor Magnet returns either HTML or XML data, via simple get requests. For illustrative purposes, each HTML page also provides the URL for the corresponding XML-structured data set. Acknowledgements This research was partly supported by the WCU (World Class University) program under the Na-tional Research Foundation of Korea (Ministry of Education, Science and Technology of Korea, Pro-ject No: R31-30007), and partly funded by Science Foundation Ireland via the Centre for Next Genera-tion Localization (CNGL). References  Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium. 
Dan Fass. 1997. Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science & Tech-nology. New York: Ablex. Hugo Liu, Henry Lieberman and Ted Selker. 2003. A Model of Textual Affect Sensing Using Real-World Knowledge. Proc. of the 8th international conference on Intelligent user interfaces, 125-132. James H. Martin. 1990. A Computational Model of Metaphor Interpretation. NY: Academic Press. Zachary J. Mason. 2004. CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System, Computational Linguistics, 30(1):23-44. Ekaterina Shutova. 2010. Metaphor Identification Using Verb and Noun Clustering. In Proc. of the 23rd Inter-national Conference on Computational Linguistics, 1001-1010 Tony Veale. 2011. Creative Language Retrieval. Crea-tive Language Retrieval: A Robust Hybrid of Infor-mation Retrieval and Linguistic Creativity. In Proc. of ACL?2011, the 49th Annual Meeting of the Asso-ciation for Computational Linguistics. 
12


References
 Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram 
Version 1. Linguistic Data Consortium.
Dan Fass. 1997. Processing Metonymy and Metaphor.
Contemporary Studies in Cognitive Science & Technology. New York: Ablex.
Hugo Liu, Henry Lieberman and Ted Selker. 2003. A 
Model of Textual Affect Sensing Using Real-World 
Knowledge. Proc. of the 8
th
international conference 
on Intelligent user interfaces, 125-132.
James H. Martin. 1990.  A Computational Model of 
Metaphor Interpretation. NY: Academic Press.
Zachary J. Mason. 2004. CorMet: A Computational, 
Corpus-Based Conventional Metaphor Extraction 
System, Computational Linguistics, 30(1):23-44.
Ekaterina Shutova. 2010. Metaphor Identification Using 
Verb and Noun Clustering. In Proc. of the 23
rd
International Conference on Computational Linguistics, 
1001-1010
Tony Veale. 2011. Creative Language Retrieval. Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity. In  Proc. 
of ACL’2011, the 49
th
Annual Meeting of the Association for Computational Linguistics.Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 660?670,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Creating Similarity:  Lateral Thinking for Vertical Similarity Judgments 
  Tony Veale Guofu Li Web Science and Technology Division, School of Computer Science and Informatics, Korean Advanced Institute of Science  University College Dublin, and Technology, Yuseong, South Korea Belfield, Dublin D2, Ireland. Tony.Veale@gmail.com li.guofu.l@gmail.com       Abstract Just as observing is more than just see-ing, comparing is far more than mere matching. It takes understanding, and even inventiveness, to discern a useful basis for judging two ideas as similar in a particular context, especially when our perspective is shaped by an act of linguis-tic creativity such as metaphor, simile or analogy. Structured resources such as WordNet offer a convenient hierarchical means for converging on a common ground for comparison, but offer little support for the divergent thinking that is needed to creatively view one concept as another. We describe such a means here, by showing how the web can be used to harvest many divergent views for many familiar ideas. These lateral views com-plement the vertical views of WordNet, and support a system for idea exploration called Thesaurus Rex. We show also how Thesaurus Rex supports a novel, genera-tive similarity measure for WordNet. 1 Seeing is Believing (and Creating) Similarity is a cognitive phenomenon that is both complex and subjective, yet for practical reasons it is often modeled as if it were simple and objec-tive. This makes sense for the many situations where we want to align our similarity judgments with those of others, and thus focus on the same conventional properties that others are also likely to focus upon. This reliance on the consensus viewpoint explains why WordNet (Fellbaum, 1998) has proven so useful as a basis for compu-tational measures of lexico-semantic similarity 
(e.g. see Pederson et al 2004, Budanitsky & Hirst, 2006; Seco et al 2006). These measures reduce the similarity of two lexical concepts to a single number, by viewing similarity as an objec-tive estimate of the overlap in their salient quali-ties. This convenient perspective is poorly suited to creative or insightful comparisons, but it is sufficient for the many mundane comparisons we often perform in daily life, such as when we or-ganize books or look for items in a supermarket. So if we do not know in which aisle to locate a given item (such as oatmeal), we may tacitly know how to locate a similar product (such as cornflakes) and orient ourselves accordingly.  Yet there are occasions when the recognition of similarities spurs the creation of similarities, when the act of comparison spurs us to invent new ways of looking at an idea. By placing pop tarts in the breakfast aisle, food manufacturers encourage us to view them as a breakfast food that is not dissimilar to oatmeal or cornflakes. When ex-PM Tony Blair published his memoirs, a mischievous activist encouraged others to move his book from Biography to Fiction in bookshops, in the hope that buyers would see it in a new light. Whenever we use a novel meta-phor to convey a non-obvious viewpoint on a topic, such as ?cigarettes are time bombs?, the comparison may spur us to insight, to see aspects of the topic that make it more similar to the vehi-cle (see Ortony, 1979; Veale & Hao, 2007).   In formal terms, assume agent A has an in-sight about concept X, and uses the metaphor X is a Y to also provoke this insight in agent B. To arrive at this insight for itself, B must intuit what X and Y have in common. But this commonality is surely more than a standard categorization of X, or else it would not count as an insight about X. To understand the metaphor, B must place X 
660
in a new category, so that X can be seen as more similar to Y. Metaphors shape the way we per-ceive the world by re-shaping the way we make similarity judgments. So if we want to imbue computers with the ability to make and to under-stand creative metaphors, we must first give them the ability to look beyond the narrow view-points of conventional resources.   Any measure that models similarity as an ob-jective function of a conventional worldview employs a convergent thought process. Using WordNet, for instance, a similarity measure can vertically converge on a common superordinate category of both inputs, and generate a single numeric result based on their distance to, and the information content of, this common generaliza-tion. So to find the most conventional ways of seeing a lexical concept, one simply ascends a narrowing concept hierarchy, using a process de Bono (1970) calls vertical thinking. To find nov-el, non-obvious and useful ways of looking at a lexical concept, one must use what Guilford (1967) calls divergent thinking and what de Bono calls lateral thinking. These processes cut across familiar category boundaries, to simultaneously place a concept in many different categories so that we can see it in many different ways.   de Bono argues that vertical thinking is selec-tive while lateral thinking is generative. Whereas vertical thinking concerns itself with the ?right? way or a single ?best? way of looking at things, lateral thinking focuses on producing alternatives to the status quo. To be as useful for creative tasks as they are for conventional tasks, we need to re-imagine our computational similarity measures as generative rather than selective, ex-pansive rather than reductive, divergent as well as convergent and lateral as well as vertical. Though WordNet is ideally structured to support vertical, convergent reasoning, its comprehensive nature means it can also be used as a solid foun-dation for building a more lateral and divergent model of similarity. Here we will use the web as a source of diverse perspectives on familiar ide-as, to complement the conventional and often narrow views codified by WordNet.   Section 2 provides a brief overview of past work in the area of similarity measurement, be-fore section 3 describes a simple bootstrapping loop for acquiring richly diverse perspectives from the web for a wide variety of familiar ideas. These perspectives are used to enhance a Word-Net-based measure of lexico-semantic similarity in section 4, by broadening the range of informa-tive viewpoints the measure can select from. 
Similarity is thus modeled as a process that is both generative and selective. This lateral-and-vertical approach is evaluated in section 5, on the Miller & Charles (1991) data-set. A web app for the lateral exploration of diverse viewpoints, named Thesaurus Rex, is also presented, before closing remarks are offered in section 6. 2 Related Work and Ideas WordNet?s taxonomic organization of noun-senses and verb-senses ? in which very general categories are successively divided into increas-ingly informative sub-categories or instance-level ideas ? allows us to gauge the overlap in information content, and thus of meaning, of two lexical concepts. We need only identify the deepest point in the taxonomy at which this con-tent starts to diverge. This point of divergence is often called the LCS, or least common subsumer, of two concepts (Pederson et al, 2004). Since sub-categories add new properties to those they inherit from their parents ? Aristotle called these properties the differentia that stop a category sys-tem from trivially collapsing into itself ? the depth of a lexical concept in a taxonomy is an intuitive proxy for its information content. Wu & Palmer (1994) use the depth of a lexical concept in the WordNet hierarchy as such a proxy, and thereby estimate the similarity of two lexical concepts as twice the depth of their LCS divided by the sum of their individual depths.  Leacock and Chodorow (1998) instead use the length of the shortest path between two con-cepts as a proxy for the conceptual distance be-tween them. To connect any two ideas in a hierarchical system, one must vertically ascend the hierarchy from one concept, change direction at a potential LCS, and then descend the hierar-chy to reach the second concept. (Aristotle was also first to suggest this approach in his Poetics). Leacock and Chodorow normalize the length of this path by dividing its size (in nodes) by twice the depth of the deepest concept in the hierarchy; the latter is an upper bound on the distance be-tween any two concepts in the hierarchy. Negat-ing the log of this normalized length yields a corresponding similarity score. While the role of an LCS is merely implied in Leacock and Cho-dorow?s use of a shortest path, the LCS is pivotal nonetheless, and like that of Wu & Palmer, the approach uses an essentially vertical reasoning process to identify a single ?best? generalization.   Depth is a convenient proxy for information content, but more nuanced proxies can yield 
661
more rounded similarity measures. Resnick (1995) draws on information theory to define the information content of a lexical concept as the negative log likelihood of its occurrence in a corpus, either explicitly (via a direct mention) or by presupposition (via a mention of any of its sub-categories or instances). Since the likelihood of a general category occurring in a corpus is higher than that of any of its sub-categories or instances, such categories are more predictable, and less informative, than rarer categories whose occurrences are less predictable and thus more informative. The negative log likelihood of the most informative LCS of two lexical concepts offers a reliable estimate of the amount of infor-mation shared by those concepts, and thus a good estimate of their similarity. Lin (1998) combines the intuitions behind Resnick?s metric and that of Wu and Palmer to estimate the similarity of two lexical concepts as an information ratio: twice the information content of their LCS divided by the sum of their individual information contents.   Jiang and Conrath (1997) consider the con-verse notion of dissimilarity, noting that two lex-ical concepts are dissimilar to the extent that each contains information that is not shared by the other. So if the information content of their most informative LCS is a good measure of what they do share, then the sum of their individual information contents, minus twice the content of their most informative LCS, is a reliable estimate of their dissimilarity.   Seco et al (2006) presents a minor innova-tion, showing how Resnick?s notion of infor-mation content can be calculated without the use of an external corpus. Rather, when using Res-nick?s metric (or that of Lin, or Jiang and Con-rath) for measuring the similarity of lexical concepts in WordNet, one can use the category structure of WordNet itself to estimate infor-mation content. Typically, the more general a concept, the more descendants it will possess. Seco et al thus estimate the information content of a lexical concept as the log of the sum of all its unique descendants (both direct and indirect), divided by the log of the total number of con-cepts in the entire hierarchy. Not only is this in-trinsic view of information content convenient to use, without recourse to an external corpus, Seco et al show that it offers a better estimate of in-formation content than its extrinsic, corpus-based alternatives, as measured relative to average hu-man similarity ratings for the 30 word-pairs in the Miller & Charles (1991) test set.  A similarity measure can draw on other 
sources of information besides WordNet?s cate-gory structures. One might eke out additional information from WordNet?s textual glosses, as in Lesk (1986), or use category structures other than those offered by WordNet. Looking beyond WordNet, entries in the online encyclopedia Wikipedia are not only connected by a dense topology of lateral links, they are also organized by a rich hierarchy of overlapping categories. Strube and Ponzetto (2006) show how Wikipedia can support a measure of similarity (and related-ness) that better approximates human judgments than many WordNet-based measures. Nonethe-less, WordNet can be a valuable component of a hybrid measure, and Agirre et al (2009) use an SVM (support vector machine) to combine in-formation from WordNet with information har-vested from the web. Their best similarity measure achieves a remarkable 0.93 correlation with human judgments on the Miller & Charles word-pair set.   Similarity is not always applied to pairs of concepts; it is sometimes analogically applied to pairs of pairs of concepts, as in proportional analogies of the form A is to B as C is to D (e.g., hacks are to writers as mercenaries are to sol-diers, or chisels are to sculptors as scalpels are to surgeons). In such analogies, one is really as-sessing the similarity of the unstated relationship between each pair of concepts: thus, mercenaries are soldiers whose allegiance is paid for, much as hacks are writers with income-driven loyalties; sculptors use chisels to carve stone, while sur-geons use scalpels to cut or carve flesh. Veale (2004) used WordNet to assess the similarity of A:B to C:D as a function of the combined simi-larity of A to C and of B to D. In contrast, Tur-ney (2005) used the web to pursue a more divergent course, to represent the tacit relation-ships of A to B and of C to D as points in a high-dimensional space. The dimensions of this space initially correspond to linking phrases on the web, before these dimensions are significantly reduced using singular value decomposition.   In the infamous SAT test, an analogy A:B::C:D has four other pairs of concepts that serve as likely distractors (e.g. singer:songwriter for hack:writer) and the goal is to choose the most appropriate C:D pair for a given A:B pair-ing. Using variants of Wu and Palmer (1994) on the 374 SAT analogies of Turney (2005), Veale (2004) reports a success rate of 38?44% using only WordNet-based similarity. In contrast, Tur-ney (2005) reports up to 55% success on the same analogies, partly because his approach aims 
662
to match implicit relations rather than explicit concepts, and in part because it uses a divergent process to gather from the web as rich a perspec-tive as it can on these latent relationships.  2.1 Clever Comparisons Create Similarity Each of these approaches to similarity is a user of information, rather than a creator, and each fails to capture how a creative comparison (such as a metaphor)  can spur a listener to view a topic from an atypical perspective. Camac & Glucks-berg (1984) provide experimental evidence for the claim that ?metaphors do not use preexisting associations to achieve their effects [?] people use metaphors to create new relations between concepts.? They also offer a salutary reminder of an often overlooked fact: every comparison ex-ploits information, but each is also a source of new information in its own right. Thus, ?this cola is acid? reveals a different perspective on cola (e.g. as a corrosive substance or an irritating food) than ?this acid is cola? highlights for acid (such as e.g., a familiar substance)    Veale & Keane (1994) model the role of simi-larity in realizing the long-term perlocutionary effect of an informative comparison. For exam-ple, to compare surgeons to butchers is to en-courage one to see all surgeons as more bloody, crude or careless. The reverse comparison, of butchers to surgeons, encourages one to see butchers as more skilled and precise. Veale & Keane present a network model of memory, called Sapper, in which activation can spread between related concepts, thus allowing one con-cept to prime the properties of a neighbor. To interpret an analogy, Sapper lays down new acti-vation-carrying bridges in memory between ana-logical counterparts, such as between surgeon & butcher, flesh & meat, and scalpel & cleaver. Comparisons can thus have lasting effects on how Sapper sees the world, changing the pattern of activation that arises when it primes a concept.   Veale (2003) adopts a similarly dynamic view of similarity in WordNet, showing how an ana-logical comparison can result in the automatic addition of new categories and relations to WordNet itself. Veale considers the problem of finding an analogical mapping between different parts of WordNet?s noun-sense hierarchy, such as between instances of Greek god and Norse god, or between the letters of different alphabets, such as of Greek and Hebrew. But no structural similarity measure for WordNet exhibits enough discernment to e.g. assign a higher similarity to 
Zeus & Odin (each is the supreme deity of its pantheon) than to a pairing of Zeus and any other Norse god, just as no structural measure will as-sign a higher similarity to Alpha & Aleph or to Beta & Beth than to any random letter pairing.   A fine-grained category hierarchy permits fine-grained similarity judgments, and though WordNet is useful, its sense hierarchies are not especially fine-grained. However, we can auto-matically make WordNet subtler and more dis-cerning, by adding new fine-grained categories to unite lexical concepts whose similarity is not reflected by any existing categories. Veale (2003) shows how a property that is found in the glosses of two lexical concepts, of the same depth, can be combined with their LCS to yield a new fine-grained parent category, so e.g. ?su-preme? + deity = Supreme-deity (for Odin, Zeus, Jupiter, etc.) and ?1st? + letter = 1st-letter (for Alpha, Aleph, etc.) Selected aspects of the textual similarity of two WordNet glosses ? the key to similarity in Lesk (1986) ? can thus be reified into an explicitly categorical WordNet form.  3 Divergent  (Re)Categorization To tap into a richer source of concept properties than WordNet?s glosses, we can use web n-grams. Consider these descriptions of a cowboy from the Google n-grams (Brants & Franz, 2006). The numbers to the right are Google fre-quency counts.  a lonesome cowboy   432  a mounted cowboy   122  a grizzled cowboy     74  a swaggering cowboy     68 To find the stable properties that can underpin a meaningful fine-grained category for cowboy, we must seek out the properties that are so often pre-supposed to be salient of all cowboys that one can use them to anchor a simile, such as "swag-gering like a cowboy? or ?as grizzled as a cow-boy?. So for each property P suggested by Google n-grams for a lexical concept C, we gen-erate a like-simile for verbal behaviors such as swaggering and an as-as-simile for adjectives such as lonesome. Each is then dispatched to Google as a phrasal query. We value quality over size, as these similes will later be used to find diverse viewpoints on the web via bootstrapping. We thus manually filter each web simile, to weed out any that are ill-formed, and those intended to be seen as ironic by their authors. This gives us a body of 12,000+ valid web similes. 
663
 Veale (2011, 2012, 2013) notes that web uses of the pattern ?as P as C? are rife with irony. In contrast, web instances of ?P S such as C? ? where S denotes a superordinate of C ? are rarely ironic. Hao & Veale (2010) exploit this fact to filter ironic comparisons from web similes, by re-expressing each ?as P as C? simile as  ?P * such as C? (using a wildcard * to match any val-ues for S) and looking for attested uses of this new form on the web. Since each hit will also yield a value for S via the wildcard *, and a fine-grained category P-S for C, we use this approach here to harvest fine-grained categories from the web from most of our similes.    Once C is seen to be an exemplary member of the category P-S, such as cola in fizzy-drink, a targeted web search is used to find other mem-bers of P-S, via the anchored query ?P S such as * and C?. For example, ?fizzy drinks such as * and cola? will retrieve web texts in which * is matched to soda or lemonade. Each new member can then be used to instantiate a further query, as in ?fizzy drinks such as * and soda?, to retrieve other members of P-S, such as champagne and root beer. This bootstrapping process runs in successive cycles, using doubly-anchored pat-terns that ? following Kozareva et al (2008) and Veale et al (2009) ? explicitly mention both the category to be populated (P-S) and a recently acquired member of this category (C).   As cautioned by Kozareva et al, it is reckless to bootstrap from members to categories to members again if each enfilade of queries is like-ly to return noisy results. A reliable filter must be applied at each stage, to ensure that any member C that is placed in a category P-S is a sensible member of the category S. Only by filtering in this way can we stop the rapid accumulation of noise. For instance, a WordNet-based filter dis-cards any categorization ?P S such as X and C? where X does not denote a WordNet entry for which S does not denote a valid hypernym. Such a filter offers no creative latitude, however, since it forces every pairing of C and P-S to precisely obey WordNet?s category hierarchy. We thus use instead the near-miss filter described in Veale et al (2009), in which X must denote a descendant of some direct hypernym of some sense of S. The filter does not (and cannot) determine whether P is salient for X. It merely assumes that if P is sa-lient for C, it is salient for X.   Five successive cycles of bootstrapping are performed, using the 12,000+ web similes as a starting point. Consider cola: after 1 cycle, we acquire 14 new categories, such as effervescent-
beverage and sweet-beverage. After 2 cycles we acquire 43 categories; after 3 cycles, 72; after 4 cycles, 93; and after 5 cycles, we acquire 102 fine-grained perspectives on cola, such as stimu-lating-drink and corrosive-substance.  
 Figure 1. Fine-grained perspectives for cola found by Thesaurus Rex on the web. See also Figures 3 and 4. These alternative viewpoints, for a broad array of concepts, are gleaned from the collective intelli-gence of the web. Some are more discerning and informative than others ? see for instance war & divorce in Figure 4 ? though as de Bono (1971) notes, lateral thinking does not privilege a nar-row set of ?correct? viewpoints, rather it gener-ates a broad array of interesting alternatives, none of which are ever ?wrong?, even if some prove more useful than others in a given context.  4 Measuring and Creating Similarity Which perspectives will be most useful and in-formative to a WordNet-based similarity metric? Simply, a perspective M-Cx  for a concept Cy can be coherently added to WordNet iff Cx de-notes a hypernym of some sense of Cy in Word-Net. For purposes of quantifying the similarity of two terms t1 and t2 ? by finding the WordNet senses of these terms that exhibit the highest sim-ilarity ? we can augment WordNet with the per-spectives on t1 and t2 that are coherent with WordNet?s hierarchy. So for t1=cola & t2=acid, corrosive-substance offers a coherent new per-spective on each, slotting in beneath the match-ing WordNet sense of substance.   A category system is a structured feature space. We estimate the similarity of C1 and C2 as the cosine of the angle between the feature vec-tors that are constructed for each. The dimen-sions of these vectors are the atomic hypernyms (direct or indirect) of C1 and C2 in WordNet; the value of a dimension H in a vector is the infor-mation content (IC) of the WordNet hypernym H:  
664
                   size(H)        ?c ? WN  size(c)) Here size(H) is the total number of lexical con-cepts in category H in WordNet, excluding any instance-level concepts, as these illustrative indi-viduals are not evenly distributed across Word-Net categories.   We also want any fine-grained perspective M-H to influence our similarity metric, provided it can be coherently tied into WordNet as a shared hypernym of the two lexical concepts being compared. The absolute information content of a category M-H  that is newly added to WordNet is given by (2): 
                                         size(M-H)    ?m-h ? WN  size(m-h)) where size(M-H) is the number of lexical con-cepts in WordNet for which M-H can be added as a new hypernym. The denominator in (2) de-notes the sum total of the size of all fine-grained categories that can be coherently added to WordNet for any term.     The IC of M-H relative to H is estimated via the geometric mean of ICabs(M-H) and IC(H) is given by (3): (3)  IC(M-H)    =   ? ICabs(M-H) . IC(H) For a shared dimension H in the feature vectors of concepts C1 and C2, if at least one fine-grained perspective M-H has been added to WordNet between H and C1 and between H and C2, then the value of dimension H for C1 and for C2 is given by (4):  (4)  weight(H)   = max(IC(H),  maxM IC(M-H)) When no shared perspective M-H can be added under H, then weight(H) = IC(H). A fine-grained perspective M-H will thus influence a similarity judgment between C1 and C2 only if M-H can be coherently added to WordNet as a hypernym of C1 and C2, and if M-H enriches our view of H. Unlike Resnick (1995), Lin (1998) and Seco et al (2006), this vector-space approach does not hinge on the information content of a single LCS, so any shared hypernym H or perspective M-H can shape a similarity judgment according to its informativeness. 
5 Empirical Evaluation  Many fascinating perspectives on familiar ideas are bootstrapped from the web using similes as a starting point. These perspectives drive an ex-ploratory web-aid to lateral thinking we call The-saurus Rex, while the cosine-distance metric constructed from WordNet and these many fine-grained categories is called, simply, Rex. When Rex provides a numeric estimate of similarity for two ideas, Thesaurus Rex provides an enhanced insight into why these ideas are similar, e.g. by explaining that cola & acid are not just substanc-es, they are corrosive substances.      We evaluate Rex by estimating how closely its judgments correlate with those of human judges on the 30-pair word set of Miller & Charles (M&C), who aggregated the judgments of multi-ple human raters into mean ratings for these pairs. We evaluate three variants of Rex on M&C: Rex-lat, which combines WordNet with all of Thesaurus Rex; Rex-wn, which uses only WordNet, with nothing at all from Thesaurus Rex; and Rex-pop, which enriches WordNet with only popular perspectives from Thesaurus Rex. A perspective is considered popular if it is dis-covered 5 or more times in the bootstrapping process, using 5 different anchors. While corro-sive-substance is a popular category for acid, it not so for cola or juice. Popularity thus approxi-mates what Ortony (1979) calls salience.   Similarity metric r Similarity metric r Wu & Palmer?94* .74 Seco et al ?06* .84 Resnick ?95* .77 Agirre et al ?09 .93 Leacock/Chod?98* .82 Han et al?09 .856 Lin ?98* .80 Rex-wn .84 Jiang/Conrath ?97* -.81 Rex-lat .89 Li et al ?03 .89 Rex-pop .93 Table 1. Product-moment correlations (Pearson?s r) with mean human ratings on all 30 word pairs of the Miller & Charles similarity data-set. * As re-evaluated by Seco et al (2006) for all 30 pairs Table 1 lists coefficients of correlation (Pear-son?s r) with mean human ratings for a range of WordNet-based metrics. Table 1 includes the hybrid WordNet+web+SVM metric of Agirre et al (2009) ? who report a correlation of .93 ? and the Mutual-Information-based PMImax metric of Han et al (2009). The latter achieves good re-sults for 27 of the 30 M&C pairs by enriching a PMI metric with an automatically-generated the-saurus. Yet while informative, this thesaurus is 
         (               ) 
 
         (                          
 
(2)  ICabs(M-H) =  -log 
(1)   IC(H)             =     - log 
665
not organized as an explanatory system of hier-archical categories as it is in Thesaurus Rex.  Rex-wn does no better than Seco et al (2006) on the M&C dataset, suggesting that Rex?s vec-tors of IC-weighted hypernyms are no more dis-cerning than a single informative LCS. However, such vectors also permit Rex to incorporate addi-tional, fine-grained perspectives from Thesaurus Rex, allowing Rex-lat in turn to achieve a com-parable correlation to that of Li et al (2003) ? .89. Yet the formulation in (2) favors unusual or idiosyncratic perspectives that are unlikely to generalize across independent judges. The mean ratings of M&C are the stuff of consensus, not individual creativity, and outside the realm of creative metaphor it often makes sense to safely align our judgments with those of others.   By limiting its use of Thesaurus Rex to the perspectives that other judges are most likely to use, Rex-pop obtains a correlation of .93 with mean human ratings on all 30 M&C pairs. This result is comparable to that reported by Agirre et al (2009), who use SVM-based supervised learning to combine the judgments of two met-rics, one based on WordNet and another on the analysis of web contexts of both input terms. However, Rex has the greater capacity for in-sight, since it augments the structured category system of WordNet with structured categories of its own. At each level of the WordNet hierarchy, Rex finds the fine-grained category that can best inform its judgments. Because Rex makes highly selective use of the diverse products of lateral thinking, this selectivity also produces concise explanations for its judgments. 5.1 Generative Uses of Similarity A similarity metric offers a numerical measure of how closely one idea can cluster with another. It can also indicate how well one object may serve as a substitute for another, as when a letter open-er is used as a knife, or tofu is used instead of meat. This need for substitution can be grist for creativity, yet most similarity metrics can only assess a suggested substitution, rather than sug-gest one for themselves. If they are to actively shape a creative decision, our similarity metrics must be made more generative.   A similarity metric can learn to be generative, by observing how people typically cluster words and ideas that are made similar by their contexts of use. The Google 3-grams contain many in-stances of the clustering pattern ?X+s and Y+s?, as in ?cowboys and pirates? or ?doctors and law-yers?, and so a comprehensive trawl yields many 
insights into the pairings of ideas that we implic-itly see as comparable. We harvest all such Google 3-grams, to build a symmetric compara-bility graph in which any two comparable terms are adjacent nodes. For any node, we can gener-ate a diverse set of comparable ideas just by reading off its adjacent nodes. Thesaurus Rex can be used to find an embracing category for many such pairs of nodes, while Rex estimates the sim-ilarity of any two adjacent nodes. A comparabil-ity graph of 28,000 nodes is produced from the Google 3-grams, with a sparse adjacency matrix of just 1,264,827 (0.16%) non-zero entries.   Is this dense enough for a task requiring gen-erative similarity? Almuhareb & Poesio (2004) describe one such task: they sample 214 words from across 13 WordNet categories, and ask if these 214 words can be partitioned into 13 clus-ters that mirror the WordNet categories from which they were drawn. They then collect tens of thousands of web contexts for these 214 words, to extract a feature representation of each. We instead use Rex to generate, as features, a diverse set of comparable terms for each word. (We also assume that each word is a feature of itself). The Rex comparability graph suggests a pool of 8,300 features for all 214 words. The clustering toolkit CLUTO is used to partition the original 214 words into 13 clusters guided only by these com-parability features. The resulting 13 clusters have an average purity of 93.4% relative to WordNet, suggesting that categorization tasks which re-quire implicit comparability judgments are well served by a generative approach to similarity.   5.2 Learning From Similarity Judgments  Rex augments the narrow worldview of WordNet with the more diverse viewpoints it gleans from the web, not by viewing them as separate knowledge sources, but by actually updating WordNet itself. The relative performance of Rex-pop > Rex-lat > Rex-wn on the M&C da-taset shows that selective use of a divergent per-spective permits WordNet to better serve its popular role as a judge of similarity. It is worth asking then whether these passing additions to WordNet should not be made permanent.   Rex estimates a similarity score for each of the 1,264,827 pairings of comparable terms it finds in the Google 3-grams. These scores are then cached to support generative similarity, and to permit fast lookup of scores for common com-parisons. This lookup table is a lightweight means of using Rex in a range of creative substi-tution or generation tasks. Though the table is 
666
sparse, ?5.1 shows that it implicitly captures key nuances of category structure. The 39,826 unique fine-grained categories added by Rex-pop (ver-sus the 44,238 categories added by Rex-lat) in the course of its 1,264,827 comparisons thus suggest credible enhancements to WordNet. Fig-ure 2 graphs the distribution of new categories and their membership sizes when Rex-pop is used on this scale. 
 Figure 2. The number of new categories (Y-axis) with a given membership size (X-axis) added to WordNet when Rex-pop/lat are used on a large, web scale. The Goldilocks categories are those that are not so small as to lack generality, and not so large as to lack information content. For example, Rex-pop suggests the addition of 15,125 new fine-grained categories to WordNet with membership sizes ranging from 5 to 25. This is a large but manageable number of categories that should be further considered for future addition to Word-Net, or indeed to any similarly curated knowledge resource.  6 Summary and Conclusions de Bono (1970) argues that the best solutions arise from using lateral and vertical thinking in unison. Lateral thinking is divergent and genera-tive, while vertical thinking is convergent and analytical. The former can thus be used to create a pool of interesting candidates for the latter to selectively consider. Thesaurus Rex uses the web to generate a rich pool of alternate perspectives on familiar ideas, and Rex selects from this pool to perform vertical reasoning with WordNet to yield precise similarity judgments. Rex also uses the most informative perspective to concisely explain each comparison, or ? when used in gen-erative mode ? to suggest a creative comparison. For instance, to highlight the potential toxicity of coffee, Thesaurus Rex suggests comparisons with alcohol, tobacco or pesticide, as all have been categorized as toxic substances on the web. A web app based on Thesaurus Rex, to support this 
kind of lateral thinking, is accessible online at this URL: http://boundinanutshell.com/therex2 Screenshots from the Thesaurus Rex application are provided in Figures 3 and 4 overleaf. Be-cause Thesaurus Rex targets the acquisition of fine-grained perspectives, ranging from the off-beat to the obvious, it acquires an order-of-magnitude more categories from the web than can be found in WordNet itself. Rex dips selec-tively into this wealth of perspectives (and Rex-pop is more selective still), though many of Rex?s needs can be anticipated by looking to how ideas are implicitly grouped into ad-hoc catego-ries (Barsalou, 1983) in constructions such as ?X+s and Y+s?. Using the Google n-grams as a source of tacit grouping constructions, we have created a comprehensive lookup table that pro-vides Rex similarity scores for the most common (if often implicit) comparisons.      Comparability is not the same as similarity, and a non-zero similarity score does not mean that two concepts would ever be considered comparable by a human. This poses a problem for the generation of sensible comparisons. However, Rex?s lookup table captures the implic-it pragmatics of comparability, making Rex usa-ble in generative tasks where a metric must both suggest and evaluate comparisons. Human simi-larity mechanisms are evaluative and generative, convergent and divergent. Our computational mechanisms should be no less so. 7 Acknowledgements This research was partly supported by the WCU (World Class University) program under the Na-tional Research Foundation of Korea (Ministry of Education, Science and Technology of Korea, Project no. R31-30007) and partly funded by Science Foundation Ireland via the Centre for Next Generation Localization (CNGL). 
667
 Figure 3.  A screenshot from the web application Thesaurus Rex, showing the fine-grained categories found by Thesaurus Rex for the lexical concept creativity on the web.    
  Figure 4.  A screenshot from the web application Thesaurus Rex, showing the shared overlapping categories found by Thesaurus Rex for the lexical concepts divorce and war.
668
References  Aristotle (translator: James Hutton). 1982. Aristotle?s Poetics. New York: Norton. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca and Aitor Soroa. 2009. Study on Similarity and Relatedness Using Distri-butional and WordNet-based Approaches. In Pro-ceedings of NAACL '09, The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 19?27.  Abdulrahman Almuhareb and Massimo Poesio. 2004. Attribute-Based and Value-Based Clustering: An Evaluation. In Proceedings of the Conference on Empirical Methods in NLP, Barcelona. pp. 158-165.  Lawrence W. Barsalou. 1983. Ad hoc categories. Memory and Cognition, 11:211?227.  Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Ver. 1. Philadelphia: Linguistic Data Consor-tium.  Alexander Budanitsky and Graeme Hirst. 2006.  Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13-47.  Mary K. Camac, and Sam Glucksberg. 1984. Metaphors do not use associations between concepts, they are used to create them. Journal of Psycholinguistic Research, 13, 443-455. de Bono, Edward. 1970. Lateral thinking: creativity step by step. New York: Harper & Row. de Bono, Edward. 1971. Lateral thinking for management: a handbook for creativity. New York: McGraw Hill. Christiane Fellbaum (ed.). 1998. WordNet: An Elec-tronic Lexical Database. MIT Press, Cambridge, MA.  J. Paul Guilford. 1967. The Nature of Human Intelligence. New York: McGraw Hill. Lushan Han, Tim Finin, Paul McNamee, Anupam Joshi and Yelena Yesha. 2012. Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy. IEEE Transactions on Data and Knowledge Engineering (13 Feb. 2012). Yanfen Hao and Tony Veale. 2010. An Ironic Fist in a Velvet Glove: Creative Mis-Representation in the Construction of Ironic Similes. Minds and Machines 20(4), pp. 635?650. Jay Y. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics, pp. 19-33.  
 Zornitsa Kozareva, Eileen Riloff and Eduard Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proc. of the 46th Annual Meeting of the ACL, pp 1048-1056.  Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Fellbaum, C. (ed.), WordNet: An Electronic Lexical Database, 265?283.  Yuhua Li, Zuhair A. Bandar and David McLean. 2003. An Approach for Measuring Semantic Simi-larity between Words Using Multiple Information Sources. IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 4, pp. 871-882. Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th ICML, the International Conference on Machine Learning, Morgan Kaufmann, San Francisco CA, pp. 296? 304. Michael  Lesk. 1986 Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of ACM SigDoc, ACM, 24?26. George A. Miller and Walter. G. Charles. 1991. Con-textual correlates of semantic similarity. Language and Cognitive Processes 6(1):1-28. Andrew Ortony. 1979. Beyond literal similarity. Psy-chological Review, 86, pp. 161-180. Ted Pederson, Siddarth Patwardhan and Jason Michelizzi. 2004. WordNet::Similarity: measuring the relatedness of concepts. In Proceedings of HLT-NAACL?04 (Demonstration Papers) the 2004 annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 38-41. Philip Resnick. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proceedings of IJCAI?95, the 14th International Joint Conference on Artificial Intelligence. Nuno Seco, Tony Veale and Jer Hayes, 2004. An In-trinsic Information Content Metric for Semantic Similarity in WordNet. In Proceedings of ECAI?04, the European Conference on Artificial Intelligence.  Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate! Computing Semantic Relatedness Using Wikipedia. In Proceedings of AAAI-06, the 2006 Conference of the Association for the Advancement of AI, pp. 1419?1424. Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. Proceedings of the 19th International Joint Conference on Artificial Intelli-gence, 1136-1141.  
669
Tony Veale and Mark T. Keane. 1994. Belief Model-ing, Intentionality and Perlocution in Metaphor Comprehension. In Proceedings of the 16th Annual Meeting of the Cognitive Science Society, Atlanta, Georgia. Hillsdale, NJ: Lawrence Erlbaum.  Tony Veale. 2003. The analogical thesaurus: An emerging application at the  juncture of lexical metaphor and information retrieval. In Proceedings of IAAI?03, the 15th International Conference on Innovative Applications of Artificial Intelligence, Mexico. Tony Veale. 2004. WordNet sits the SAT: A knowledge-based approach to lexical analogy. Proceedings of ECAI'04, the European Conference on Artificial Intelligence, 606-612.  Tony Veale and Yanfen Hao. 2007. Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language. In proceedings of AAAI 2007, the 22nd AAAI Con-ference on Artificial Intelligence. Vancouver, Can-ada. Tony Veale, Guofu Li and Yanfen Hao. 2009. Grow-ing Finely-Discriminating Taxonomies from Seeds of Varying Quality and Size.  In Proc. of EACL?09, the 12th Conference of the European Chapter of the Association for Computational Linguistics pp. 835-842.  Tony Veale. 2011. Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Lin-guistic Creativity. In Proceedings of ACL?2011, the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Tony Veale. 2012. Exploding the Creativity Myth: The computational foundations of linguistic crea-tivity. London: Bloomsbury Academic. Tony Veale. 2013. Humorous Similes. Humor: The International Journal of Humor Research, 21(1):3-22. Zhibiao Wu and Martha Palmer. 1994. Verb seman-tics and lexical selection. In Proceedings of ACL?94, 32nd annual meeting of the Association for Computational Linguistics, Las Cruces, New Mexi-co,. pp. 133-138.    
670
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 230?233,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UCD-Goggle: A Hybrid System for Noun Compound Paraphrasing
Guofu Li
School of Computer Science
and Informatics
University College Dublin
guofu.li@ucd.ie
Alejandra Lopez-Fernandez
School of Computer Science
and Informatics
University College Dublin
alejandra.lopez
-fernandez@ucd.ie
Tony Veale
School of Computer Science
and Informatics
University College Dublin
tony.veale@ucd.ie
Abstract
This paper addresses the problem of rank-
ing a list of paraphrases associated with a
noun-noun compound as closely as possi-
ble to human raters (Butnariu et al, 2010).
UCD-Goggle tackles this task using se-
mantic knowledge learnt from the Google
n-grams together with human-preferences
for paraphrases mined from training data.
Empirical evaluation shows that UCD-
Goggle achieves 0.432 Spearman correla-
tion with human judgments.
1 Introduction
Noun compounds (NC) are sequences of nouns
acting as a single noun (Downing, 1977). Re-
search on noun compounds involves two main
tasks: NC detection and NC interpretation. The
latter has been studied in the context of many
natural language applications, including question-
answering, machine translation, information re-
trieval, and information extraction.
The use of multiple paraphrases as a semantic
intepretation of noun compounds has recently be-
come popular (Kim and Baldwin, 2006; Nakov
and Hearst, 2006; Butnariu and Veale, 2008;
Nakov, 2008). The best paraphrases are those
which most aptly characterize the relationship be-
tween the modifier noun and the head noun.
The aim of this current work is to provide a
ranking for a list of paraphrases that best approxi-
mates human rankings for the same paraphrases.
We have created a system called UCD-Goggle,
which uses semantic knowledge acquired from
Google n-grams together with human-preferences
mined from training data. Three major com-
ponents are involved in our system: B-score,
produced by a Bayesian algorithm using seman-
tic knowledge from the n-grams corpus with a
smoothing layer of additional inference; R
t
-score
captures human preferences observed in the tail
distribution of training data; and R
p
-score cap-
tures pairwise paraphrase preferences calculated
from the training data. Our best system for
SemEval-2 task 9 combines all three components
and achieves a Spearman correlation of 0.432 with
human rankings.
This paper is organized as follows: the Bayesian
B-score is introduced in section 2. In section 3
we describe two supervised approaches to mining
the preferences of human raters from training data.
Finally, section 4 presents the results of our empir-
ical evaluation of the UCD-Goggle system.
2 Semantic Approach
2.1 Collecting Data
Google have made their web n-grams, also known
as Web-1T corpus, public via the Linguistic Data
Consortium (Brants and Franz, 2006). This cor-
pus contains sequences of n terms that occur more
than 40 times on the web.
We view the paraphrase task as that of suggest-
ing the right verb phrase for two nouns (But-
nariu and Veale, 2008). Previous work has shown
the n-grams corpus to be a promising resource
for retrieving semantic evidence for this approach.
However, the corpus itself needs to be tailored to
serve our purpose. Since the n-grams corpus is a
collection of raw snippets from the web, together
with their web frequency, certain pre-processing
steps are essential before it can be used as a semi-
structured knowledge base. Following a syntac-
tic pattern approach, snippets in the n-grams that
agree with the following patterns are harvested:
1. Head VP Mod
2. Head VP DET Mod
3. Head [that|which] VP Mod
4. Head [that|which] VP DET Mod
Here, DET denotes any of the determiners (i.e.,
230
the set of {an, a, the} for English), Head and Mod
are nouns for heads and modifiers, and VP stands
for verb-based paraphrases observed in the test
data. It must be highlighted that, when we collect
snippets for the KB, any Head or Mod that falls out
of the range of the dataset are also accepted via a
process of semantic slippage (to be discussed in
Sect. 2.4). The patterns listed above enable us to
collect examples such as:
1. ?bread containing nut?
2. ?pill alleviates the headache?
3. ?novel which is about crimes?
4. ?problem that involves the students?
After a shallow parse, these snippets are formal-
ized into the triple format ?Head, Para,Mod?.
The sample snippets above are represented as:
1. ?bread, contain, nut?
2. ?pill, alleviate, headache?
3. ?novel, be about, crime?
4. ?problem, involve, student?
We use ?Head, Para,Mod? to denote the fre-
quency of ?Head, Para,Mod? in the n-grams.
2.2 Loosely Coupled Compound Analysis
Tens of millions of snippets are harvested and
cleaned up in this way, yet expecting even this
large set to provide decent coverage over the test
data is still unrealistic. We calculated the proba-
bility of an example in the test data to appear in
KB at less than 1%. To overcome the coverage is-
sue, a loosely coupled analysis and representation
of compounds is employed. Despite the fact that
both modifier and head can influence the ranking
of a paraphrase, we believe that either the modifier
or the head is the dominating factor in most cases.
This assumption has been shown to be plausible
by earlier work (Butnariu and Veale, 2008). Thus,
instead of storing complete triples in our KB, we
divide each complete triple into two partial triples
as shown below:
?Head, Para,Mod? ?
{
?Head, Para, ??
??, Para,Mod?
We can also retrieve these partial triples directly
from the n-grams corpus using partial patterns like
?Head Para? and ?Para Mod?. However, just as
shorter incomplete patterns can produce a larger
KB, they also accept much more noise. For in-
stance, single-verb paraphrases are very common
among the test data. In these cases, the partial pat-
tern approach would need to harvest snippets with
the form ?NN VV? or ?VV NN? from 2-grams,
which are too common to be reliable.
2.3 Probabilistic Framework
In the probabilistic framework, we define the B-
score as the conditional probability of a para-
phrase, Para, being suggested for a given com-
pound Comp:
B(Para;Comp) ? P (Para|Comp) (1)
Using the KB, we can estimate this conditional
probability by applying the Bayes theorem:
P (Para|Comp) =
P (Comp|Para)P (Para)
P (Comp)
(2)
The loose-coupling assumption (Sect. 2.2) allows
us to estimate P (Comp) as:
P (Comp) ? P (Mod ?Head). (3)
Meanwhile, a priori probabilities such as
P (Para) can be easily inferred from the KB.
2.4 Inferential Smoothing Layer
After applying the loose-coupling technique de-
scribed in Section 2.2, the coverage of the KB
rises to 31.78% (see Figure 1). To further in-
crease this coverage, an inference layer is added
to the system. This layer aims to stretch the con-
tents of the KB via semantic slippage to the KB, as
guided by the maximization of a fitness function.
A WordNet-based similarity matrix is employed
(Seco et al, 2004) to provide a similarity measure
between nouns (so sim(x, x) is 1). Then, a su-
perset of Head or Mod (denoted as H andM re-
spectively) can be extracted by including all nouns
with similarity greater than 0 to any of them in the
test data. Formally, for Head we have:
H = {h|sim(h,Head) ? 0, Head in dataset}.
(4)
The definition ofM is analogous to that ofH.
A system of equations is defined to produce al-
ternatives for Head and Mod and their smoothed
corpus frequencies (we show only the functions
for head here):
h
0
= Head (5)
fit(h) = sim
2
(h, h
n
)? ?h, p, ?? (6)
h
n+1
= arg max
h?H
fit(h) (7)
231
Here, fit(h) is a fitness function of the can-
didate head h, in the context of a paraphrase p.
Empirically, we use h
1
for Head and fit(h
1
) for
?Head, Para, ?? when calculating the B-score
back in the probabilistic framework (Sect. 2.3). In
theory, we can apply this smoothing step repeat-
edly until convergence is obtained.
Figure 1: Comparison on coverage.
This semantic slippage mechanism allows a
computer to infer the missing parts of the KB, by
building a bridge between the limitations of a fi-
nite KB and the knowledge demands of an appli-
cation. Figure 1 above shows how the coverage of
the system increases when using partial matching
and the smoothing technique, over the use of exact
matching with the KB.
3 Preferences for Paraphrases
3.1 Tail-based Preference
Similar to various types of data studied by social
scientists, the distribution of strings in our corpus
tends to obey Zipf?s law (Zipf, 1936). The same
Zipfian trend was also observed in the compound-
paraphrase dataset: more than 190 out of 250 com-
pounds in the training data have 60% of their para-
phrases in an undiscriminating tail, while 245 of
250 have 50% of their paraphrases in the tail. We
thus assume the existence of a long tail in the para-
phrase list for each compound.
The tail of each paraphrase list can be a valuable
heuristic for modeling human paraphrase prefer-
ences. We refer to this model as the tail-based
preference model. We assume that an occurrence
of a paraphrase is deemed to occur in the tail iff it
is mentioned by the human raters only once. Thus,
the tail preference is defined as the probability that
a paraphrase appears in the non-tail part of the list
for all compounds in the training data. Formally,
it can be expressed as:
R
t
(p) =
?
c?C
?(c, p)f(c, p)
?
c?C
f(c, p)
(8)
where C is the set of all compounds in the training
data and f(c, p) is the frequency of paraphrase p
on compound c as given by the human raters. The
?(c, p) is a filter coefficient as shown below:
?(c, p) =
{
1, f(c, p) > 1,
0, f(c, p) = 1.
(9)
The tail-based preference model is simple but
effective when used in conjunction with seman-
tic ranking via the KB acquired from n-grams.
However, an important drawback is that the tail
model assigns a static preference to paraphrase
(i.e., tail preferences are assumed to be context-
independent). More than that, this preference does
not take information from non-tail paraphrases
into consideration. Due to these downsides, we
use pairwise preferences described below.
3.2 Pairwise Preference
To fully utilize the training data, we employ an-
other preference mining approach called pairwise
preference modeling. This approach applies the
principle of pairwise comparison (David, 1988)
to determine the rank of a paraphrase inside a list.
We build a pairwise comparison matrix ? for
paraphrases using the values of Equation 10 (here
we have assumed that each of the paraphrases has
been mapped into numeric values):
?
i,j
=
{
n(p
i
,p
j
)
n(p
i
,p
j
)+n(p
j
,p
i
)
, n(p
i
, p
j
) > n(p
j
, p
i
),
0, otherwise.
(10)
where n(p
i
, p
j
) is the relative preferability of p
i
to p
j
. To illustrate the logic behind n(x, y), we
imagine a scenario with three compounds shown
in Table 1:
abor. prob. abor. vote arti. desc.
involve 12 8 3
concern 10 9 5
be about 3 9 15
Table 1: An example
1
to illustrate n(x, y)
1
In this example, abor. prob. stands for abortion problem,
abor. vote stands for abortion vote, and arti. desc. stands for
artifact description
232
The relative preferability is given by the number
of times that the frequency of p
i
from human raters
is greater than that of p
j
. Observing that 1 out of
3 times involve is ranked higher than concern, we
can calculate their relative preferability as:
n(involve, concern) = 1
n(concern, involve) = 2
Once the matrix is built, the preference score for
a paraphrase i is calculated as:
R
p
(i; c) =
?
j?P
c
?
i,j
|P
c
|
(11)
whereP
c
is the list of paraphrases for a given com-
pound c in the test data. The pairwise preference
puts a paraphrase in the context of its company, so
that the opinions of human raters can be approxi-
mated more precisely.
4 Empirical Results
We evaluated our system by tackling theSemEval-
2 task 9 test data. We created three systems with
different combinations of the three components
(B, R
t
, R
p
). Table 2 below shows the perfor-
mance of UCD-Goggle for each setting:
System Config Spearman ? Pearson r
I B + R
t
0.380 0.252
II R
p
0.418 0.375
III B + R
t
+ R
p
0.432 0.395
* Baseline 0.425 0.344
Table 2: Evaluation results on different settings of
the UCD-Goggle system.
The first setting is a hybrid system which first
calculates a ranking according to the ngrams cor-
pus and then applies a very simple preference
heuristic (Sect. 2.3 and 3.1). The second setting
simply applies the pairwise preference algorithm
to the training data to learn ranking preferences
(Sect. 3.2). Finally, the third setting integrates
both of these settings in a single approach.
The individual contribution of B-score and R
t
was tested by two-fold cross validation applied to
the training data. The training data was split into
two subsets and preferences were learnt from one
part and then applied to the other. As an unsuper-
vised algorithm, B-score produced Spearman cor-
relation of 0.31 while the R
t
-score gave 0.33. We
noticed that more than 78% of the paraphrases had
0 score by R
t
. This number not only reconfirmed
the existence of the long-tail phenomenon, but also
suggested thatR
t
-score alone could hardly capture
the preference on the non-tail part. On the other
hand, with more than 80% chance we could expect
B to produce a non-zero score for a paraphrase,
even if the paraphrase fell out of the topic. When
combined together, B and R
t
complemented each
other and improved the performance considerably.
However, this combined effort still could not beat
the pairwise preference R
p
or the baseline system,
which had no semantic knowledge involved. The
major limitation of our system is that the seman-
tic approach is totally ignorant of the training data.
In future work, we will intend to use it as a valu-
able resource in both KB construction and ranking
stage.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium.
C. Butnariu and T. Veale. 2008. A concept-centered
approach to noun-compound interpretation. In Proc.
of the 22nd COLING, pages 81?88, Manchester,
UK.
C. Butnariu, S. N. Kim, P. Nakov, D.
?
O S?eaghdha,
S. Szpakowicz, and T. Veale. 2010. Semeval-2 task
9: The interpretation of noun compounds using para-
phrasing verbs and prepositions. In Workshop on
Semantic Evaluation, Uppsala, Sweden.
H. A. David. 1988. The Method of Paired Compar-
isons. Oxford University Press, New York.
P. Downing. 1977. On the creation and use of English
compound nouns. In Language 53, pages 810?842.
S. N. Kim and T. Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics.
In Proc. of the COLING/ACL, pages 491?498, Mor-
ristown, NJ, USA.
P. Nakov and M. A. Hearst. 2006. Using verbs to char-
acterize noun-noun relations. In Proc. of AIMSA,
pages 233?244.
P. Nakov. 2008. Noun compound interpretation using
paraphrasing verbs: Feasibility study. In Proc. of
the 13th AIMSA, pages 103?117, Berlin, Heidelberg.
Springer-Verlag.
N. Seco, T. Veale, and J. Hayes. 2004. An intrinsic
information content metric for semantic similarity
in WordNet. In Proc. of the 16th ECAI, Valencia,
Spain. John Wiley.
G. K. Zipf. 1936. The Psycho-Biology of Language:
An Introdution to Dynamic Philology. Routledge,
London.
233
