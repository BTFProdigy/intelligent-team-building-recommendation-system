Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1512?1523,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
A Joint Graph Model for Pinyin-to-Chinese Conversion
with Typo Correction?
Zhongye Jia and Hai Zhao?
MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
jia.zhongye@gmail.com, zhaohai@cs.sjtu.edu.cn
Abstract
It is very import for Chinese language pro-
cessing with the aid of an efficient input
method engine (IME), of which pinyin-
to-Chinese (PTC) conversion is the core
part. Meanwhile, though typos are in-
evitable during user pinyin inputting, ex-
isting IMEs paid little attention to such big
inconvenience. In this paper, motivated by
a key equivalence of two decoding algo-
rithms, we propose a joint graph model to
globally optimize PTC and typo correction
for IME. The evaluation results show that
the proposed method outperforms both ex-
isting academic and commercial IMEs.
1 Introduction
1.1 Chinese Input Method
The daily life of Chinese people heavily depends
on Chinese input method engine (IME), no matter
whether one is composing an E-mail, writing an
article, or sending a text message. However, ev-
ery Chinese word inputted into computer or cell-
phone cannot be typed through one-to-one map-
ping of key-to-letter inputting directly, but has to
go through an IME as there are thousands of Chi-
nese characters for inputting while only 26 letter
keys are available in the keyboard. An IME is
an essential software interface that maps Chinese
characters into English letter combinations. An ef-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the National Ba-
sic Research Program of China (Grant No.2013CB329401),
the Science and Technology Commission of Shanghai Mu-
nicipality (Grant No.13511500200), and the European Union
Seventh Framework Program (Grant No.247619).
?Corresponding author
ficient IME will largely improve the user experi-
ence of Chinese information processing.
Nowadays most of Chinese IMEs are pinyin
based. Pinyin is originally designed as the pho-
netic symbol of a Chinese character (based on the
standard modern Chinese, mandarin) , using Latin
letters as its syllable notation. For example, the
pinyin of the Chinese character ???(love) is ??i?.
Most characters usually have unique pinyin rep-
resentations, while a few Chinese characters may
be pronounced in several different ways, so they
may have multiple pinyin representations. The ad-
vantage of pinyin IME is that it only adopts the
pronunciation perspective of Chinese characters
so that it is simple and easy to learn. But there
are only less than 500 pinyin syllables in stan-
dard modern Chinese, compared with over 6,000
commonly used Chinese characters, which leads
to serious ambiguities for pinyin-to-charactermap-
ping. Modern pinyin IMEsmostly use a ?sentence-
based? decoding technique (Chen and Lee, 2000)
to alleviate the ambiguities. ?Sentence based?
means that IME generates a sequence of Chinese
characters upon a sequence of pinyin inputs with
respect to certain statistical criteria.
1.2 Typos and Chinese Spell Checking
Written in Chinese characters but not alphabets,
spell checking for Chinese language is quite dif-
ferent from the same task for other languages.
Since Chinese characters are entered via IME,
those user-made typos do not immediately lead to
spelling errors. When a user types a wrong letter,
IME will be very likely to fail to generate the ex-
pected Chinese character sequence. Normally, the
user may immediately notice the inputting error
and then make corrections, which usually means
doing a bunch of extra operations like cursor
1512
movement, deletion and re-typing. Thus there are
two separated sub-tasks for Chinese spell check-
ing: 1. typo checking for user typed pinyin se-
quences which should be a built-in module in
IME, and 2. spell checking for Chinese texts in
its narrow sense, which is typically a module of
word processing applications (Yang et al, 2012b).
These two terms are often confused especially in
IME related works such as (Chen and Lee, 2000)
and (Wu et al, 2009).
Pinyin typos have always been a serious prob-
lem for Chinese pinyin IMEs. The user may fail
to input the completely right pinyin simply be-
cause he/she is a dialect speaker and does not know
the exact pronunciation for the expected character.
This may be a very common situation since there
are about seven quite different dialects in Chinese,
among which being spoken languages, six are far
different from the standard modern Chinese, man-
darin. With the boom of smart-phones, pinyin ty-
pos worsen due to the limited size of soft key-
board, and the lack of physical feedback on the
touch screen. However, existing practical IMEs
only provide small patches to deal with typos such
as Fuzzy Pinyin (Wu and Chen, 2004) and other
language specific errors (Zheng et al, 2011b).
Typo checking and correction has an important
impact on IME performance. When IME fails to
correct a typo and generate the expected sentence,
the user will have to takemuch extra effort to move
the cursor back to the mistyped letter and correct it,
which leads to very poor user experience (Jia and
Zhao, 2013).
2 Related Works
The very first approach for Chinese input with
typo correction was made by (Chen and Lee,
2000), which was also the initial attempt of
?sentence-based? IME. The idea of ?statistical in-
put method? was proposed by modeling PTC con-
version as a hidden Markov model (HMM), and
using Viterbi (Viterbi, 1967) algorithm to decode
the sequence. They solved the typo correction
problem by decomposing the conditional proba-
bility P (H|P ) of Chinese character sequence H
given pinyin sequence P into a language model
P (w
i
|w
i?1
) and a typing model P (p
i
|w
i
). The
typing model that was estimated on real user input
data was for typo correction. However, real user
input data can be very noisy and not very conve-
nient to obtain. As we will propose a joint model
in this paper, such an individual typing model is
not necessarily built in our approach.
(Zheng et al, 2011a) developed an IME sys-
tem with typo correction called CHIME using
noisy channel error model and language-specific
features. However their model depended on a
very strong assumption that input pinyin sequence
should have been segmented into pinyin words by
the user. This assumption does not really hold in
modern ?sentence-based? IMEs. We release this
assumption since our model solves segmentation,
typo correction and PTC conversion jointly.
Besides the common HMM approach for PTC
conversion, there are also variousmethods such as:
support vector machine (Jiang et al, 2007), max-
imum entropy (ME) model (Wang et al, 2006),
conditional random field (CRF) (Li et al, 2009)
and statistical machine translation (SMT) (Yang et
al., 2012a; Wang et al, 2013c; Zhang and Zhao,
2013), etc.
Spell checking or typo checking was first pro-
posed for English (Peterson, 1980). (Mays et al,
1991) addressed that spell checking should be done
within a context, i.e., a sentence or a long phrase
with a certain meaning, instead of only in one
word. A recent spell correction work is (Li et al,
2006), where a distributional similarity was intro-
duced for spell correction of web queries.
Early attempts for Chinese spelling checking
could date back to (Chang, 1994) where charac-
ter tables for similar shape, pronunciation, mean-
ing, and input-method-code characters were pro-
posed. More recently, the 7th SIGHANWorkshop
on Chinese Language Processing (Yu et al, 2013)
held a shared task on Chinese spell checking. Var-
ious approaches were made for the task includ-
ing language model (LM) based methods (Chen
et al, 2013), ME model (Han and Chang, 2013),
CRF (Wang et al, 2013d; Wang et al, 2013a),
SMT (Chiu et al, 2013; Liu et al, 2013), and graph
model (Jia et al, 2013), etc.
3 Pinyin Input Method Model
3.1 From English Letter to Chinese Sentence
It is a rather long journey from the first English
letter typed on the keyboard to finally a completed
Chinese sentence generated by IME. We will first
take an overview of the entire process.
The average length of pinyin syllables is about 3
letters. There are about 410 pinyin syllables used
in the current pinyin system. Each pinyin sylla-
1513
ble has a bunch of corresponding Chinese char-
acters which share the same pronunciation repre-
sented by the syllable. The number of those homo-
phones ranges from 1 to over 300. Chinese char-
acters then form words. But word in Chinese is
a rather vague concept. Without word delimiters,
linguists have argued on what a Chinese word re-
ally is for a long time and that is why there is al-
ways a primary word segmentation treatment in
most Chinese language processing tasks (Zhao et
al., 2006; Huang and Zhao, 2007; Zhao and Kit,
2008; Zhao et al, 2010; Zhao and Kit, 2011; Zhao
et al, 2013). A Chinese word may contain from
1 to over 10 characters due to different word seg-
mentation conventions. Figure 1 demonstrates the
relationship of pinyin andword, from pinyin letters
?nihao? to the word ??? (hello)?. Typically, an
IME takes the pinyin input, segments it into sylla-
bles, looks up corresponding words in a dictionary
and generates a sentence with the candidate words.
nihao ??
ni hao ?? Pinyin syllablesChinese characters
Pinyin word
Chinese word
n i h a o Pinyin characters
Figure 1: Relationship of pinyin and words
3.2 Pinyin Segmentation and Typo
Correction
Non-Chinese users may feel confused or even
surprised if they know that when typing pinyin
through an IME, Chinese IME users will never en-
ter delimiters such as ?Space? key to segment ei-
ther pinyin syllables or pinyin words, but just in-
put the entire un-segmented pinyin sequence. For
example, if one wants to input ????? (Hello
world)?, he will just type ?nihaoshijie? instead of
segmented pinyin sequence ?ni hao shi jie?. Nev-
ertheless, pinyin syllable segmentation is a much
easier problem compared to Chinese word seg-
mentation. Since pinyin syllables have a very lim-
ited vocabulary and follow a set of regularities
strictly, it is convenient to perform pinyin sylla-
ble segmentation by using rules. But as the pinyin
input is not segmented, it is nearly impossible to
adopt previous spell checking methods for English
to pinyin typo checking, although techniques for
English spell checking have been well developed.
A bit confusing but interesting, pinyin typo cor-
rection and segmentation come as two sides of one
problem: when a pinyin sequence is mistyped, it
is unlikely to be correctly segmented; when it is
segmented in an awkward way, it is likely to be
mistyped.
Inspired by (Yang et al, 2012b) and (Jia et al,
2013), we adopt the graph model for Chinese spell
checking for pinyin segmentation and typo correc-
tion, which is based on the shortest path word seg-
mentation algorithm (Casey and Lecolinet, 1996).
The model has two major steps: segmentation and
correction.
3.2.1 Pinyin Segmentation
The shortest path segmentation algorithm is based
on the idea that a reasonable segmentation should
minimize the number of segmented units. For a
pinyin sequence p
1
p
2
. . . p
L
, where p
i
is a letter,
first a directed acyclic graph (DAG) G
S
= (V,E)
is built for pinyin segmentation step. The vertex
set V consists of the following parts:
? Virtual start vertex S
0
and end vertex S
E
;
? Possible legal syllables fetched from dictio-
nary D
p
according to the input pinyin se-
quence:
{S
i,j
|S
i,j
= p
i
. . . p
j
? D
p
};
? The letter itself as a fallback no matter if it is
a legal pinyin syllable or not:
{S
i
|S
i
= p
i
}.
The vertex weights w
S
are all set to 0. The edges
are from a syllable to all syllables next to it:
E = {E(S
i,j
? S
j+1,k
)|S
i,j
, S
j+1,k
? V}.
The edge weight the negative logarithm of con-
ditional probability P (S
j+1,k
|S
i,j
) that a syllable
S
i,j
is followed by S
j+1,k
, which is give by a bi-
gram language model of pinyin syllables:
W
E(S
i,j
?S
j+1,k
)
= ? logP (S
j+1,k
|S
i,j
)
The shortest path P ? on the graph is the path P
with the least sum of weights:
P
?
= argmin
(v,E)?G?(v,E)?P
?
v
w
v
+
?
E
W
E
.
1514
Computing the shortest path from S
0
to S
E
on
G
S
yields the best segmentation. This is the sin-
gle source shortest path (SSSP) problem on DAG
which has an efficient algorithm by preprocessing
the DAG with topology sort, then traversing ver-
tices and edges in topological order. It has the time
complexity of O(|V|+ |E|). For example, one in-
tends to input ????? (Hello world)? by typ-
ing ?nihaoshijie?, but mistyped as ?mihaoshijiw?.
The graph for this input is shown in Figure 2. The
shortest path, i.e., the best segmentation is ?mi hao
shi ji w?. We will continue to use this example in
the rest of this paper.
m i h a o s h i j i w
ha
hao
ao
shi
j i
m i
Figure 2: Graph model for pinyin segmentation
3.2.2 Pinyin Typo Correction
Next in the correction step, for the segmented
pinyin sequence S
1
, S
2
, . . . , S
M
, a graph G
c
is
constructed to perform typo correction. The ver-
tex set V consists of the following parts:
? Virtual start vertex S?
0
and end vertex S?
E
with
vertex weights of 0;
? All possible syllables similar to original syl-
lable in G
s
. If the adjacent syllables can be
merged into a legal syllable, the merged syl-
lable is also added into V:
{S
?
i,j
|S
?
i,j
= S
?
i
. . . S
?
j
? D
p
,
S
?
k
? S
k
, k = i ? j},
where the similarity ? is measured in Lev-
enshtein distance (Levenshtein, 1966). Sylla-
bles with Levenshtein distance under a certain
threshold are considered as similar:
L(S
i
, S
j
) < T ? S
i
? S
j
.
The vertex weight is the Levenshtein distance
multiply by a normalization parameter:
w
S
?
i,j
= ?
j
?
k?i
L(S
?
k
, S
k
).
Similar toG
s
, the edges are from one syllable to all
syllables next to it and edge weights are the condi-
tional probabilities between them. Computing the
shortest path from S?
0
to S?
E
on G
c
yields the best
typo correction result. In addition, the result has
been segmented so far. Considering our running
example, the graph G
c
is shown in Figure 3, and
the typo correction result is ?mi hao shi jie?.
whao shi j i
j ie
m i
ti
ni
m a
hai
hu o
p ao
shu
sai
z hi
j ia
. . .
. . .
. . .
. . . . . .
. . . . . .
. . .
a
e
. . .
shu ai
j u
Figure 3: Graph model for pinyin typo correction
Merely using the above model, the typo cor-
rection result is not satisfying yet, no matter how
much effort is paid. The major reason is that the
basic semantic unit of Chinese language is actu-
ally word (tough vaguely defined) which is usu-
ally composed of several characters. Thus the con-
ditional probability between characters does not
make much sense. In addition, a pinyin syllable
usually maps to dozens or even hundreds of cor-
responding homophonic characters, which makes
the conditional probability between syllablesmuch
more noisy. However, using pinyin words instead
of syllables is not a wise choice because pinyin
word segmentation is not so easy a task as syllable
segmentation. To make typo correction better, we
consider to integrate it with PTC conversion using
a joint model.
3.3 Hidden Markov Model for
Pinyin-to-Chinese Conversion
PTC conversion has long been viewed as a decod-
ing problem using HMM. We continue to follow
this formalization. The best Chinese character se-
quenceW ? for a given pinyin syllable sequence S
is the one with the highest conditional probability
P (W |S) that
W
?
= argmax
W
P (W |S)
= argmax
W
P (W )P (S|W )
P (S)
= argmax
W
P (W )P (S|W )
= argmax
w
1
,w
w
,...,w
M
?
w
i
P (w
i
|w
i?1
)
?
w
i
P (s
i
|w
i
)
1515
In the HMM for pinyin IME, observation states are
pinyin syllables, hidden states are Chinese words,
emission probability is P (s
i
|w
i
), and transition
probability is P (w
i
|w
i?1
). Note the transition
probability is the conditional probability between
words instead of characters. PTC conversion is to
decode the Chinese word sequence from the pinyin
sequence. The Viterbi algorithm (Viterbi, 1967) is
used for the decoding.
The shortest path algorithm for typo correction
and Viterbi algorithm for PTC conversion are very
closely related. It has been strictly proven by (For-
ney, 1973) that the sequence decoding problem on
HMM is formally identical to finding a shortest
path on a certain graph, which can be constructed
in the following manner.
Consider a first order HMM with all possi-
ble observations O = {o
1
, o
2
, . . . , o
M
}, hidden
states S = {s
1
, s
2
, . . . , s
N
}, a special start state
s
0
, emission probabilities (E
s
i
,o
k
) = P (o
k
|s
i
),
transition probabilities (T
s
i
,s
j
) = P (s
j
|s
i
), and
start probabilities (S
s
i
) = P (s
i
|s
0
). For an
observation sequence of T time periods Y =
{y
1
, y
2
, . . . , y
T
|y
t
? O, t = 1, . . . , T}, the de-
coding problem is to find the best corresponding
hidden state sequence X? with the highest proba-
bility, i.e.,
X
?
= argmax
x
1
,x
t
?S
S
x
1
E
x
1
,y
1
T
?
t=2
E
x
t
,y
t
T
x
t?1
,x
t
. (1)
Thenwewill construct a DAGG = (V,E) upon
the HMM. The vertex set V includes:
? Virtual start vertex v
0
and end vertex v
E
with
vertex weight of 0;
? Normal vertices v
x
t
, where t = 1, . . . , T , and
?x
t
? S. The vertex weight is the negative
logarithm of emission probability:
w
v
x
t
= ? log E
x
t
,y
t
.
The edge set E includes:
? Edges from the start vertexE(v
0
? v
x
1
)with
edge weight
W
E(v
0
?v
x
1
)
= ? logS
x
1
,
where ?x
1
? S;
? Edges to the end vertex E(v
x
T
? v
E
) with
vertex weights of 0;
? Edges between adjacent time periods
E(v
x
t?1
? v
x
t
) with edge weight
W
E(v
x
t?1
?v
x
t
)
= ? log T
x
t?1
,x
t
,
where t = 2, . . . , T , and ?x
t
, x
t?1
? S.
The shortest path P ? from v
0
to v
E
is the one with
the least sum of vertex and edge weights, i.e.,
P
?
= argmin
v
x
t
?V
T
?
t=1
(w
v
x
t
+ W
E(v
x
t?1
?v
x
t
)
)
= argmin
v
x
1
,v
x
t
?V
{? logS
x
1
? log E
x
1
,y
1
+
T
?
t=2
(? log E
x
t
,y
t
? log T
x
t?1
,x
t
)}
= argmax
v
x
1
,v
x
t
?V
S
x
1
E
x
1
,y
1
T
?
t=2
E
x
t
,y
t
T
x
t?1
,x
t
. (2)
The optimization goal of P ? in Equation (2) is
identical to that of X? in Equation (1).
3.4 Joint Graph Model For Pinyin IME
Given HMM decoding problem is identical to
SSSP problem on DAG, we propose a joint graph
model for PTC conversion with typo correction.
The joint graph model aims to find the global op-
timal for both PTC conversion and typo correction
on the entire input pinyin sequence. The graph
G = (V,E) is constructed based on graph G
c
for
typo correction in Section 3.2. The vertex set V
consists of the following parts:
? Virtual start vertex V
0
and end vertex V
E
with
vertex weight of 0;
? Adjacent pinyin syllables in G
c
are merged
into pinyin words. Corresponding Chinese
words are fetched from a PTC dictionary D
c
,
which is a dictionary maps pinyin words to
Chinese words, and added as vertices:
{V
i,j
|?V
i,j
? D
c
[S
?
i
. . . S
?
j
], i ? j};
The vertex weight consists of two parts: 1.
the vertex weights of syllables in G
c
, and 2.
the emission probability:
w
V
i,j
=?
j
?
k=i
L(S
?
k
, S
k
)
? ? logP (S?
i
. . . S
?
j
|V
i,j
);
1516
If the corresponding pinyin syllables inG
c
have an
edge between them, the vertices in G also have an
edge:
E = {E(V
i,j
? V
j+1,k
)|E(S
i,j
? S
j+1,k
) ? G
c
}.
The edge weights are the negative logarithm of the
transition probabilities:
W
E(V
i,j
?V
j+1,k
)
= ? logP (V
j+1,k
|V
i,j
)
Although the model is formulated on first order
HMM, i.e., the LM used for transition probabil-
ity is a bigram one, it is easy to extend the model
to take advantage of higher order n-gram LM, by
tracking longer history while traversing the graph.
Computing the shortest path from V
0
to V
E
onG
yields the best pinyin-to-Chinese conversion with
typo correction result. Considering our running
example, the graph G is shown in Figure 4.
ni?hao
whao shi j i
j iem i
ti
ni
m a hai
hu o
p ao
shu
sai
z hi
j ia
a
eshu ai
j u
m i?hu o
??...
z hi' j i
shi' j ie
??? ...
??? ...
?? ...
?? ...
?? ...
?? ...?? ?
...
?? ...
??? ...
??? ...
?? ...
?? ...?? ...
?? ...
??? ...
??? ...
??... ? .?..
??? ...
?? ...
?? ...
??...
Figure 4: Joint graph model
The joint graph is rather huge and density. Ac-
cording to our empirical statistics, when setting
threshold T = 2, for a sentence of M characters,
the joint graph will have |V| = M ? 1, 000, and
|E| = M ? 1, 000, 000.
3.5 K-Shortest Paths
To reduce the scale of graphG, we filter graphG
c
by searching itsK-shortest paths first to getG?
c
and
construct G on top of G?
c
. Figure 5 shows the 3-
shortest paths filtered graphG?
c
and Figure 6 shows
the correspondingG for our running example. The
scale of graph may be thus drastically reduced.
hao shi
j i
j ie
m i
ni
hu o z hi a
Figure 5: K-shortest paths in typo correction
An efficient heap data structure is required in
K-shortest paths algorithm (Eppstein, 1998) for
hao shi
j i
j ie
m i
ni
hu o z hi a
ni?hao
m i?hu o z hi' j i
shi' j ie
?? ...
??...
? .?..
?? ...
??? ...
?? ...
?? ...??? ...
??...
??? ...
?? ...
??...
??? ...
Figure 6: Filtered graph model
backtracking the best paths to current vertex while
traversing. The heap is implemented as a priority
queue of size K sorted according to path length
that should support efficient push and pop opera-
tions. Fibonacci heap (Fredman and Tarjan, 1987)
is adopted for the heap implementation since it has
a push complexity ofO(1) which is better than the
O(K) for other heap structures.
Another benefit provided by K-shortest paths
is that it can be used for generating N -best can-
didates of PTC conversion, which may be helpful
for further performance improvement.
4 Experiments
4.1 Corpora, Tools and Experiment Settings
The corpus for evaluation is the one provided
in (Yang et al, 2012a), which is originally ex-
tracted from the People?s Daily corpus and labeled
with pinyin. The corpus has already been split into
training T????, development D?? and test T???
sets as shown in Table 1.
T???? D?? T???
#Sentence 1M 2K 100K
#character 43,679,593 83,765 4,123,184
Table 1: Data set size
SRILM (Stolcke, 2002) is adopted for lan-
guagemodel training andKenLM (Heafield, 2011;
Heafield et al, 2013) for language model query.
The Chinese part of the corpus is segmented into
words before LM training. Maximum match-
ing word segmentation is used with a large word
vocabulary V extracted from web data provided
by (Wang et al, 2013b). The pinyin part is seg-
mented according to the Chinese part. This vo-
cabulary V also serves as the PTC dictionary. The
original vocabulary is not labeled with pinyin, thus
we use the PTC dictionary of sunpinyin1 which is
an open source Chinese pinyin IME, to label the
1http://code.google.com/p/sunpinyin/
1517
vocabulary V with pinyin. The emission proba-
bilities are estimated using the lexical translation
module of MOSES (Koehn et al, 2007) as ?trans-
lation probability? from pinyin to Chinese.
4.2 Evaluation Metrics
Wewill use conventional sequence labeling evalu-
ation metrics such as sequence accuracy and char-
acter accuracy2.
Chinese characters in a sentence may be sepa-
rated by digits, punctuation and alphabets which
are directly inputted without the IME. We fol-
low the so-called term Max Input Unit (MIU), the
longest consecutive Chinese character sequence
proposed by (Jia and Zhao, 2013). We will mainly
consider MIU accuracy (MIU-Acc) which is the
ratio of the number of completely corrected gen-
erated MIUs over the number of all MIUs, and
character accuracy (Ch-Acc), but the sentence ac-
curacy (S-Acc) will also be reported in evaluation
results.
We will also report the conversion error
rate (ConvER) proposed by (Zheng et al, 2011a),
which is the ratio of the number of mistyped pinyin
word that is not converted to the right Chinese
word over the total number of mistyped pinyin
words3.
4.3 Baseline System without Typo Correction
Firstly we build a baseline system without typo
correction which is a pipeline of pinyin syllable
segmentation and PTC conversion. The baseline
system takes a pinyin input sequence, segments it
into syllables, and then converts it to Chinese char-
acter sequence.
The pinyin syllable segmentation already has
very high (over 98%) accuracy with a trigram LM
using improved Kneser-Ney smoothing. Accord-
ing to our empirical observation, emission prob-
abilities are mostly 1 since most Chinese words
have unique pronunciation. So in this step we set
? = 0. We consider different LM smoothing
methods including Kneser-Ney (KN), improved
Kneser-Ney (IKN), and Witten-Bell (WB). All of
the three smoothing methods for bigram and tri-
gram LMs are examined both using back-off mod-
2We only work on the PTC conversion part of IME, thus
we are unable to use existing evaluation systems (Jia and
Zhao, 2013) for full Chinese IME functions.
3Other evaluation metrics are also proposed by (Zheng et
al., 2011a) which is only suitable for their system since our
system uses a joint model
els and interpolated models. The number of N -
best candidates for PTC conversion is set to 10.
The results on D?? are shown in Figure 7 in which
the ?-i? suffix indicates using interpolated model.
According to the results, we then choose the tri-
gram LM using Kneser-Ney smoothing with inter-
polation.
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
KN KN-i IKN IKN-i WB WB-i
 0.944
 0.946
 0.948
 0.95
 0.952
 0.954
 0.956
 0.958
 0.96
 0.962
 0.964
MIU
-Acc Ch-A
cc
MIU-Acc-bigramCh-Acc-bigramMIU-Acc-trigramCh-Acc-trigram
Figure 7: MIU-Acc and Ch-Acc with different LM
smoothing
The choice of the number of N -best candidates
for PTC conversion also has a strong impact on the
results. Figure 8 shows the results onD??with dif-
ferentNs, of which theN axis is drawn in logarith-
mic scale. We can observe that MIU-Acc slightly
decreases whileN goes up, but Ch-Acc largely in-
creases. We therefore chooseN = 10 as trade-off.
 0.7265
 0.727
 0.7275
 0.728
 0.7285
 0.729
 0.7295
 0.73
 0.7305
 0.731
 0.7315
 0.732
 1  10  100  1000
 0.935
 0.94
 0.945
 0.95
 0.955
 0.96
 0.965
 0.97
 0.975
 0.98
 0.985
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
Figure 8: MIU-Acc and Ch-Acc with differentNs
The parameter ? determines emission probabil-
ity. Results with different ? on D?? is shown in
Figure 9, of which the ? axis is drawn in logarith-
mic scale. ? = 0.03 is chosen at last.
We compare our baseline system with several
practical pinyin IMEs including sunpinyin and
Google Input Tools (Online version)4. The results
on D?? are shown in Table 2.
4http://www.google.com/inputtools/try/
1518
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.001  0.01  0.1  1
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
Figure 9: MIU-Acc and Ch-Acc with different ?
MIU-Acc Ch-Acc S-Acc
Baseline 73.39 96.24 38.00
sunpinyin 52.37 87.51 13.95
Google 74.74 94.81 40.2
Yang-ME - 93.3 30.2
Yang-MT - 95.5 45.4
Table 2: Baseline system compared to other
IMEs (%)
4.4 PTC Conversion with Typo Correction
Based upon the baseline system, we build the joint
system of PTC conversion with typo correction.
We simulate user typos by randomly generating
errors automatically on the corpus. The typo rate
is set according to previous Human-Computer In-
teraction (HCI) studies. Due to few works have
been done on modeling Chinese text entry, we
have to refer to those corresponding results on
English (Wobbrock and Myers, 2006; MacKen-
zie and Soukoreff, 2002; Clarkson et al, 2005),
which show that the average typo rate is about 2%.
(Zheng et al, 2011a) performed an experiment that
2,000 sentences of 11,968 Chinese words were en-
tered by 5 native speakers. The collected data con-
sists of 775 mistyped pinyin words caused by one
edit operation, and 85 caused by two edit opera-
tions. As we observe on T???? that the average
pinyin word length is 5.24, then typo rate in the
experiment of (Zheng et al, 2011a) can be roughly
estimated as:
775 + 85? 2
11968? 5.24
= 1.51%,
which is similar to the conclusion on English. Thus
we generate corpora from D?? with typo rate of
0% (0-P), 2% (2-P), and 5% (5-P) to evaluate the
system.
According to (Zheng et al, 2011a) most
mistyped pinyin words are caused by one edit op-
eration. Since pinyin syllable is much shorter than
pinyin word, this ratio can be higher for pinyin
syllables. From our statistics on T????, with 2%
randomly generated typos, Pr(L(S?, S) < 2) =
99.86%. Thus we set the threshold T for L to 2.
We first set K-shortest paths filter to K = 10
and tune ?. Results with different ? are shown
in Figure 10. With ? = 3.5, we select K. Re-
 0.7
 0.705
 0.71
 0.715
 0.72
 0.725
 0.73
 0.735
 0.74
 0.745
 2  2.5  3  3.5  4  4.5  5
 0.954
 0.956
 0.958
 0.96
 0.962
 0.964
 0.966
 0.968
 0.97
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(a) 0-P
 0.665
 0.67
 0.675
 0.68
 0.685
 0.69
 0.695
 2  2.5  3  3.5  4  4.5  5
 0.932
 0.934
 0.936
 0.938
 0.94
 0.942
 0.944
 0.946
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
(b) 2-P
 0.585
 0.59
 0.595
 0.6
 0.605
 0.61
 0.615
 2  2.5  3  3.5  4  4.5  5
 0.872
 0.874
 0.876
 0.878
 0.88
 0.882
 0.884
 0.886
 0.888
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
(c) 5-P
Figure 10: MIU-Acc and Ch-Acc with different ?
sults with differentK are shown in Figure 11. We
choose K = 20 since there is no significant im-
provement when K > 20.
The selection of K also directly guarantees the
running time of the joint model. With K = 20,
on a normal PC with Intel Pentium Dual-Core
E6700 CPU, the PTC conversion rate is over 2000
characters-per-minute (cpm), which is much faster
than the normal typing rate of 200 cpm.
With all parameters optimized, results on T???
1519
 0.705
 0.71
 0.715
 0.72
 0.725
 0.73
 0.735
 0.74
 0.745
 0  10  20  30  40  50  60  70  80  90  100 0.954
 0.956
 0.958
 0.96
 0.962
 0.964
 0.966
 0.968
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(a) 0-P
 0.655
 0.66
 0.665
 0.67
 0.675
 0.68
 0.685
 0.69
 0.695
 0.7
 0  10  20  30  40  50  60  70  80  90  100 0.92
 0.925
 0.93
 0.935
 0.94
 0.945
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(b) 2-P
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0  10  20  30  40  50  60  70  80  90  100 0.85
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(c) 5-P
Figure 11: MIU-Acc and Ch-Acc with differentK
using the proposed joint model are shown in Ta-
ble 3 and Table 4. Our results are compared to
the baseline system without typo correction and
Google Input Tool. Since sunpinyin does not have
typo correction module and performs much poorer
than our baseline system, we do not include it in
the comparison. Though no direct proofs can be
found to indicate if Google Input Tool has an in-
dependent typo correction component, its outputs
show that such a component is unlikely available.
Since Google Input Tool has to be accessed
through a web interface and the network connec-
tion cannot be guaranteed. we only take a subset
of 10K sentences of T??? to perform the experi-
ments, and the results are shown in Table 3.
The scores reported in (Zheng et al, 2011a) are
not listed in Table 4 since the data set is differ-
ent. They reported a ConvER of 53.56%, which is
given here for reference.
Additionally, to further inspect the robustness of
ourmodel, performance with typo rate ranges from
0% to 5% is shown in Figure 12. Although the per-
formance decreases while typo rate goes up, it is
still quite satisfying around typo rate of 2% which
is assumed to be the real world situation.
MIU-Acc Ch-Acc S-Acc ConvER
Baseline 0-P 79.90 97.47 48.87 -
Baseline 2-P 50.47 90.53 11.12 99.95
Baseline 5-P 30.26 82.83 3.32 99.99
Google 0-P 79.08 95.26 46.83 -
Google 2-P 49.47 61.50 11.08 91.70
Google 5-P 29.18 36.20 3.29 94.64
Joint 0-P 79.90 97.52 49.27 -
Joint 2-P 75.55 95.40 40.69 18.45
Joint 5-P 67.76 90.17 27.86 24.68
Table 3: Test results on 10K sentences from T???
(%)
MIU-Acc Ch-Acc S-Acc ConvER
Baseline 0-P 74.46 96.42 40.50 -
Baseline 2-P 47.25 89.50 9.62 99.95
Baseline 5-P 28.28 81.74 2.63 99.98
Joint 2-P 74.22 96.39 40.34 -
Joint 2-P 69.91 94.14 33.11 21.35
Joint 5-P 62.14 88.49 22.62 27.79
Table 4: Test results on T??? (%)
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0  1  2  3  4  5
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
Figure 12: MIU-Acc and Ch-Acc with different
typo rate (%)
5 Conclusion
In this paper, we have developed a joint graph
model for pinyin-to-Chinese conversion with typo
correction. This model finds a joint global opti-
mal for typo correction and PTC conversion on the
entire input pinyin sequence. The evaluation re-
sults show that our model outperforms both pre-
vious academic systems and existing commercial
products. In addition, the joint model is efficient
enough for practical use.
1520
References
Richard G. Casey and Eric Lecolinet. 1996. A Sur-
vey of Methods and Strategies in Character Segmen-
tation. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 18(7):690?706.
Chao-Huang Chang. 1994. A Pilot Study on Auto-
matic Chinese Spelling Error Correction. Journal of
Chinese Language and Computing, 4:143?149.
Zheng Chen and Kai-Fu Lee. 2000. A New Statis-
tical Approach To Chinese Pinyin Input. In Pro-
ceedings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 241?247,
Hong Kong, October.
Kuan-YuChen, Hung-Shin Lee, Chung-HanLee, Hsin-
Min Wang, and Hsin-Hsi Chen. 2013. A Study
of Language Modeling for Chinese Spelling Check.
In Proceedings of the Seventh SIGHAN Workshop
on Chinese Language Processing, pages 79?83,
Nagoya, Japan, October. Asian Federation of Nat-
ural Language Processing.
Hsun-wen Chiu, Jian-cheng Wu, and Jason S. Chang.
2013. Chinese Spelling Checker Based on Statis-
tical Machine Translation. In Proceedings of the
Seventh SIGHAN Workshop on Chinese Language
Processing, pages 49?53, Nagoya, Japan, October.
Asian Federation of Natural Language Processing.
Edward Clarkson, James Clawson, Kent Lyons, and
Thad Starner. 2005. An Empirical Study of Typ-
ing Rates onmini-QWERTYKeyboards. InCHI ?05
Extended Abstracts onHuman Factors in Computing
Systems, CHI EA ?05, pages 1288?1291, New York,
NY, USA. ACM.
David Eppstein. 1998. Finding the K Shortest Paths.
SIAM Journal on computing, 28(2):652?673.
Jr G. David Forney. 1973. The Viterbi Algorithm.
Proceedings of the IEEE, 61(3):268?278.
Michael L. Fredman and Robert Endre Tarjan. 1987.
Fibonacci Heaps and Their Uses in Improved Net-
work Optimization Algorithms. Journal of the ACM
(JACM), 34(3):596?615, July.
Dongxu Han and Baobao Chang. 2013. A Maxi-
mum Entropy Approach to Chinese Spelling Check.
In Proceedings of the Seventh SIGHAN Workshop
on Chinese Language Processing, pages 74?78,
Nagoya, Japan, October. Asian Federation of Nat-
ural Language Processing.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Changning Huang and Hai Zhao. 2007. Chinese Word
Segmentation: A Decade Review. Journal of Chi-
nese Information Processing, 21(3):8?20.
Zhongye Jia and Hai Zhao. 2013. KySS 1.0: a
Framework for Automatic Evaluation of Chinese In-
put Method Engines. In Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1195?1201, Nagoya, Japan, Octo-
ber. Asian Federation of Natural Language Process-
ing.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Graph
Model for Chinese Spell Checking. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 88?92, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Wei Jiang, Yi Guan, Xiaolong Wang, and BingQuan
Liu. 2007. PinYin-to-Character Conversion Model
based on Support Vector Machines. Journal of Chi-
nese information processing, 21(2):100?105.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Vladimir I. Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions and Reversals. In
Soviet physics doklady, volume 10, page 707.
Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou.
2006. Exploring Distributional Similarity Based
Models for Query Spelling Correction. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
1025?1032, Sydney, Australia, July. Association for
Computational Linguistics.
Lu Li, Xuan Wang, Xiao-Long Wang, and Yan-Bing
Yu. 2009. A Conditional Random Fields Approach
to Chinese Pinyin-to-Character Conversion. Journal
of Communication and Computer, 6(4):25?31.
Xiaodong Liu, Kevin Cheng, Yanyan Luo, Kevin Duh,
and Yuji Matsumoto. 2013. A Hybrid Chinese
Spelling Correction Using LanguageModel and Sta-
tisticalMachine Translation with Reranking. InPro-
ceedings of the Seventh SIGHAN Workshop on Chi-
nese Language Processing, pages 54?58, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
1521
I. Scott MacKenzie and R.William Soukoreff. 2002. A
Character-level Error Analysis Technique for Eval-
uating Text Entry Methods. In Proceedings of the
Second Nordic Conference on Human-computer In-
teraction, NordiCHI ?02, pages 243?246, NewYork,
NY, USA. ACM.
Eric Mays, Fred J Damerau, and Robert L Mercer.
1991. Context Based Spelling Correction. Informa-
tion Processing & Management, 27(5):517?522.
James L. Peterson. 1980. Computer Programs for De-
tecting and Correcting Spelling Errors. Commun.
ACM, 23(12):676?687, December.
Andreas Stolcke. 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the in-
ternational conference on spoken language process-
ing, volume 2, pages 901?904.
Andrew J. Viterbi. 1967. Error Bounds for Con-
volutional Codes and an Asymptotically Optimum
Decoding Algorithm. Information Theory, IEEE
Transactions on, 13(2):260?269.
Xuan Wang, Lu Li, Lin Yao, and Waqas Anwar. 2006.
A Maximum Entropy Approach to Chinese Pin Yin-
To-Character Conversion. In Systems, Man and Cy-
bernetics, 2006. SMC?06. IEEE International Con-
ference on, volume 4, pages 2956?2959. IEEE.
Chun-Hung Wang, Jason S. Chang, and Jian-Cheng
Wu. 2013a. Automatic Chinese Confusion Words
ExtractionUsing Conditional RandomFields and the
Web. In Proceedings of the Seventh SIGHANWork-
shop on Chinese Language Processing, pages 64?
68, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Peilu Wang, Ruihua Sun, Hai Zhao, and Kai Yu.
2013b. A New Word Language Model Evaluation
Metric for Character Based Languages. In Chinese
Computational Linguistics and Natural Language
Processing Based on Naturally Annotated Big Data,
pages 315?324. Springer.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita,
Hai Zhao, and Bao-Liang Lu. 2013c. Converting
Continuous-Space Language Models into N-Gram
Language Models for Statistical Machine Transla-
tion. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 845?850, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Yih-Ru Wang, Yuan-Fu Liao, Yeh-Kuang Wu, and
Liang-Chun Chang. 2013d. Conditional Random
Field-based Parser and Language Model for Tradi-
tional Chinese Spelling Checker. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 69?73, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Jacob O.Wobbrock and Brad A.Myers. 2006. Analyz-
ing the Input Stream for Character- Level Errors in
Unconstrained Text Entry Evaluations. ACM Trans.
Comput.-Hum. Interact., 13(4):458?489, December.
Jun Wu and Liren Chen. 2004. Fault-tolerant Roman-
ized Input Method for Non-roman Characters, Au-
gust 25. US Patent App. 10/928,131.
Jun Wu, Hulcan Zhu, and Hongjun Zhu. 2009. Sys-
tems and Methods for Translating Chinese Pinyin
to Chinese Characters, January 13. US Patent
7,478,033.
Shaohua Yang, Hai Zhao, and Bao-liang Lu. 2012a. A
Machine Translation Approach for Chinese Whole-
Sentence Pinyin-to-Character Conversion. In Pro-
ceedings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 333?
342, Bali,Indonesia, November. Faculty of Com-
puter Science, Universitas Indonesia.
ShaohuaYang, Hai Zhao, XiaolinWang, and Bao-liang
Lu. 2012b. Spell Checking for Chinese. In Interna-
tional Conference on Language Resources and Eval-
uation, pages 730?736, Istanbul, Turkey, May.
Liang-Chih Yu, Yuen-Hsien Tseng, Jingbo Zhu, and
Fuji Ren, editors. 2013. Proceedings of the Seventh
SIGHAN Workshop on Chinese Language Process-
ing. Asian Federation of Natural Language Process-
ing, Nagoya, Japan, October.
Jingyi Zhang and Hai Zhao. 2013. Improving Func-
tion Word Alignment with Frequency and Syntac-
tic Information. In Proceedings of the Twenty-Third
international joint conference on Artificial Intelli-
gence, pages 2211?2217. AAAI Press.
Hai Zhao and Chunyu Kit. 2008. Exploiting Unlabeled
Text with Different Unsupervised Segmentation Cri-
teria for Chinese Word Segmentation. Research in
Computing Science, 33:93?104.
Hai Zhao and Chunyu Kit. 2011. Integrating Unsu-
pervised and Supervised Word Segmentation: The
Role of Goodness Measures. Information Sciences,
181(1):163?183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. In Proceedings of the
Fifth SIGHANWorkshop on Chinese Language Pro-
cessing, pages 162?165, Sydney, Australia, July.
Association for Computational Linguistics.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A Unified Character-Based Tagging
Framework for Chinese Word Segmentation. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 9(2):5.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An Empirical Study on Word
Segmentation for Chinese Machine Translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, pages 248?263. Springer.
1522
Yabin Zheng, Chen Li, and Maosong Sun. 2011a.
CHIME: An Efficient Error-tolerant Chinese Pinyin
Input Method. In Proceedings of the Twenty-Second
International Joint Conference on Artificial Intel-
ligence - Volume Volume Three, IJCAI?11, pages
2551?2556. AAAI Press.
Yabin Zheng, Lixing Xie, Zhiyuan Liu, Maosong Sun,
Yang Zhang, and Liyun Ru. 2011b. Why Press
Backspace? Understanding User Input Behaviors in
Chinese Pinyin Input Method. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Techologies,
pages 485?490, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
1523
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 74?81,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Grammatical Error Correction as Multiclass Classification
with Single Model?
Zhongye Jia, Peilu Wang and Hai Zhao?
MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
{jia.zhongye,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cn
Abstract
This paper describes our system in the shared
task of CoNLL-2013. We illustrate that gram-
matical error detection and correction can be
transformed into a multiclass classification
task and implemented as a single-model sys-
tem regardless of various error types with the
aid of maximum entropy modeling. Our sys-
tem achieves the F1 score of 17.13% on the
standard test set.
1 Introduction and Task Description
Grammatical error correction is the task of auto-
matically detecting and correcting erroneous word
usage and ill-formed grammatical constructions in
text (Dahlmeier et al, 2012). This task could be help-
ful for hundreds of millions of people around the world
that are learning English as a second language. Al-
though there have been much of work on grammatical
error correction, the current approaches mainly focus
on very limited error types and the result is far from
satisfactory.
The CoNLL-2013 shared task, compared with the
previous Help Our Own (HOO) tasks focusing on only
determiner and preposition errors, considers a more
comprehensive list of error types, including determiner,
preposition, noun number, verb form, and subject-
verb agreement errors. The evaluation metric used in
CoNLL-2013 is Max-Matching (M2) (Dahlmeier and
Ng, 2012) precision, recall and F1 between the system
edits and a manually created set of gold-standard ed-
its. The corpus used in CoNLL-2013 is NUS Corpus
of Learner English (NUCLE) of which the details are
described in (Dahlmeier et al, 2013).
In this paper, we describe the system submission
from the team 1 of Shanghai Jiao Tong Univer-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), and the National
Basic Research Program of China (Grant No.2009CB320901
and Grant No.2013CB329401).
?Corresponding author
sity (SJT1). Grammatical error detection and correc-
tion problem is treated as multiclass classification task.
Unlike previous works (Dahlmeier et al, 2012; Ro-
zovskaya et al, 2012; Kochmar et al, 2012) that train
a model upon each error type, we use one single model
for all error types. Instead of the original error type, a
more detailed version of error types is used as class la-
bels. A rule based system generates labels from the
golden edits utilizing an extended version of Leven-
shtein edit distance. We use maximum entropy (ME)
model as classifier to obtain the error types and use
rules to do the correction. Corrections are made us-
ing rules. Finally, the corrections are filtered using lan-
guage model (LM).
2 System Architecture
Our system is a pipeline of grammatical error detection
and correction. We treats grammatical error detection
as a classification task. First all the tokens are relabeled
according to the golden annotation and a sequence of
modified version of error types is generated. This re-
labeling task is rule based using an extended version
of Levenshtein edit distance which will be discussed
in the following section. Then with the modified error
types as the class labels, a classifier using ME model
is trained. The grammatical error correction is also
rule based, which is basically the reverse of the rela-
beling phase. The modefied version of error types that
we used is much more detailed than the original five
types so that it enables us to use one rule to do the cor-
rection for each modified error type. After all, the cor-
rections are filtered by LM, to remove those corrections
that seem much worse than the original sentence.
As typical classification task, we have a training step
and a test step. The training step consists three phases:
? Error types relabeling.
? Training data refinement.
? ME training.
The test step includes three phases:
? ME classification.
74
? Error correction according to lebels.
? LM filtering.
2.1 Rebeling by Levenshtein Edit Distance
with Inflection
In CoNLL-2013 there are 5 error types but they cannot
be used directly as class labels, since they are too gen-
eral for error correction. For example, the verb form
error includes all verb form inflections such as con-
verting a verb to its infinitive form, gerund form, paste
tense, paste participle, passive voice and so on. Previ-
ous approaches (Dahlmeier et al, 2012; Rozovskaya et
al., 2012; Kochmar et al, 2012) manually decompose
each error types to more detailed ones. For example,
in (Dahlmeier et al, 2012), the determinater error is
decomposed into:
? replacement determiner (RD): { a? the }
? missing determiner (MD): { ?? a }
? unwanted determiner (UD): { a? ? }
For limited error types such as merely determinatives
error and preposition error in HOO 2012, manually de-
composition may be sufficient. But for CoNLL-2013
with 5 error types including complicated verb inflec-
tion, an automatic method to decompose error types is
needed. We present an extended version of Levenshtein
edit distance to decompose error types into more de-
tailed class labels and relabel the input with the labels
generated.
The original Levenshtein edit distance has 4 edit
types: unchange (U), addition (A), deletion (D) and
substitution (S). We extend the ?substitution? edit
into two types of edits: inflection (I) and the orig-
nal substitution (S). To judge whether two words can
be inflected from each other, the extended algorithm
needs lemmas as input. If the two words have the
same lemma, they can be inflected from each other.
The extended Levenshtein edit distance with inflec-
tion is shown in Algorithm 1. It takes the source to-
kens toksrc, destination tokens tokdst and their lemmas
lemsrc, lemdst as input and returns the edits E and the
parameters of edits P. For example, for the golden edit
{look? have been looking at}, the edits E returned by
DISTANCE will be {A,A,U ,A}, and the parameters
P of edits returned by DISTANCE will be {have, been,
looking, at}.
Then with the output of DISTANCE, the labels can
be generated by calculating the edits between original
text and golden edits. For those tokens without errors,
we directly assign a special label ??? to them. The
tricky part of the relabeling algorithm is the problem
of the edit ?addition?, A. A new token can only be
added before or after an existing token. Thus for la-
bels with addition, we must find some token that the
label can be assigned to. That sort of token is defined
as pivot. A pivot can be a token that is not changed in
Algorithm 1 Levenshtein edit distance with inflection
1: function DISTANCE(toksrc, tokdst, lemsrc,
lemdst)
2: (lsrc, ldst)? (len(toksrc), len(tokdst))
3: D[0 . . . lsrc][0 . . . ldst]? 0
4: B[0 . . . lsrc][0 . . . ldst]? (0, 0)
5: E[0 . . . lsrc][0 . . . ldst]? ?
6: for i? 1 . . . lsrc do
7: D[i][0]? i
8: B[i][0]? (i? 1, 0)
9: E[i][0]? D
10: end for
11: for j ? 1 . . . ldst do
12: D[0][j]? j
13: B[0][j]? (0, j ? 1)
14: E[0][j]? A
15: end for
16: for i? 1 . . . lsrc; j ? 1 . . . ldst do
17: if toksrc[i? 1] = tokdst[j ? 1] then
18: D[i][j]? D[i? 1][j ? 1]
19: B[i][j]? (i? 1, j ? 1)
20: E[i][j]? U
21: else
22: m ? min(D[i ? 1][j ? 1], D[i ?
1][j], D[i][j ? 1])
23: if m = D[i? 1][j ? 1] then
24: D[i][j]? D[i? 1][j ? 1] + 1
25: B[i][j]? (i? 1, j ? 1)
26: if lemsrc[i ? 1] = lemdst[j ? 1]
then
27: E[i][j]? S
28: else
29: E[i][j]? I
30: end if
31: else if m = D[i? 1][j] then
32: D[i][j]? D[i? 1][j] + 1
33: B[i][j]? (i? 1, j)
34: E[i][j]? D
35: else if m = D[i][j ? 1] then
36: D[i][j]? D[i][j ? 1] + 1
37: B[i][j]? (i, j ? 1)
38: E[i][j]? A
39: end if
40: end if
41: end for
42: (i, j)? (lsrc, ldst)
43: while i > 0 ? j > 0 do
44: insert E[i][j] into head of E
45: insert tokdst[j ? 1] into head of P
46: (i, j)? B[i][j]
47: end while
48: return (E,P)
49: end function
an edit, such as the ?apple? in edit {apple ? an ap-
ple}, or some other types of edit such as the inflection
of ?look? to ?looking? in edit {look? have been look-
75
ing at}. In the CoNLL-2013 task, the addition edits are
mostly adding articles or determinaters, so when gener-
ating the label, adding before the pivot is preferred and
only those trailing edits are added after the last pivot.
At last, the label is normalized to upper case.
The BNF syntax of labels is defined in Figure 1. The
the non-terminal ?inflection-rules? can be substituted
by terminals of inflection rules that are used for cor-
recting the error types of noun number, verb form, and
subject-verb agreement errors. All the inflection rules
are listed in Table 1. The ?stop-word? can be subsi-
tuted by terminals of stop words which contains all ar-
ticles, determinnaters and prepositions for error types
of determiner and preposition, and a small set of other
common stop words. All the stop words are listed in
Table 2.
?label? ::= ?simple-label? | ?compound-label?
?simple-label? ::= ?pivot? | ?add-before? | ?add-after?
?compound-label? ::= ?add-before? ?pivot?
| ?pivot? ?add-after?
| ?add-before? ?pivot? ?add-after?
?pivot? ::= ?inflection? | ?unchange? | ?substitution?
| ?deletion?
?add-before? ::= ?stop-word??
| ?stop-word???add-before?
?add-after? ::= ??stop-word?
| ??stop-word??add-after?
?substitution? ::= ?stop-word?
?inflection? ::= ?inflection-rules?
?unchange? ::= ?
?deletion? ::= ?
Figure 1: BNF syntax of label
Algorithm 2 is used to generate the label from the
extended Levenshtein edits according to the syntax de-
fined in Figure 1. It takes the original tokens, tokorig
and golden edit tokens, tokgold in an annotation and
their lemmas, lemorig, lemgold as input and returns the
generated label L. For our previous example of edit
{looked ? have been looking at}, the L returned by
RELABEL is {HAVE?BEEN?GERUND?AT}. Some
other examples of relabeling are demonstrated in Ta-
ble 3.
The correction step is done by rules according to the
labels. The labels are parsed with a simple LL(1) parser
and edits are made according to labels. A bit of extra
work has to be taken to handle the upper/lower case
problem. For additions and substitutions, the words
added or substituted are normalized to lowercase. For
inflections, original case of words are reserved. Then
a bunch of regular expressions are applied to correct
upper/lower case for sentence head.
Catalog Rules
Noun Number LEMMA, NPLURAL
Verb Number VSINGULAR, LEMMA
Verb Tense LEMMA, GERUND, PAST, PART
Table 1: Inflection rules
Catalog Words
Determinater a an the my your his her its our
their this that these those
Preposition about along among around as at
beside besides between by down
during except for from in inside
into of off on onto outside over
through to toward towards un-
der underneath until up upon with
within without
Modal Verb can could will would shall should
must may might
BE be am is are was were been
HAVE have has had
Other many much
Table 2: Stop words
Tokens Edit Label
to {to reveal?revealing} ?reveal GERUND
a {a woman?women} ?woman NPLURAL
developing {developing world THE?
wold ?the developing world} ?
a {a? ?} ?
in {in?on} ON
apple {apple?an apple} AN?
Table 3: Examples of relabeling
2.2 Training Corpus Refinement
The corpus used to train the grammatical error recog-
nition model is highly imbalanced. The original train-
ing corpus has 57,151 sentences and only 3,716 of
them contain at least one grammatical error, and only
5,049 of the total 1,161K words are needed to be cor-
rected. This characteristic makes it hard to get a well-
performed machine learning model, since the samples
to be recognized are so sparse and those large amount
of correct data will severely affect the machine learn-
ing process as it is an optimization on the global train-
ing data. While developing our system, we found
that only using sentences containing grammatical er-
rors will lead to a notable improvement of the result.
76
Algorithm 2 Relabeling using the extended Leven-
shtein edit distance
1: function RELABEL(tokorig , tokgold, lemorig ,
lemgold)
2: (E,P) ? DISTANCE(tokorig, tokgold,
lemorig , lemgold)
3: pivot? number of edits in E that are not A
4: L? ?
5: L? ??
6: while i < length of E do
7: if E[i] = A then
8: L? L+ label of edit E[i] with P[i]
9: i? i + 1
10: else
11: l? L+ label of edit E[i] with P[i]
12: pivot? pivot? 1
13: if pivot = 0 then
14: i? i + 1
15: while i < length of E do
16: l? l +?+ P[i]
17: i? i + 1
18: end while
19: end if
20: push l into L
21: L? ??
22: end if
23: end while
24: L? upper case of L
25: return L
26: end function
Inspired by this phenomenon, we propose a method to
refine the training corpus which will reduce the error
sparsity of the training data and notably improve the
recall rate.
The refined training corpus is composed of contexts
containing grammatical errors. To keep the informa-
tion which may have effects on the error identification
and classification, those contexts are selected based on
both syntax tree and n-gram, of which the process is
shown in Algorithm 3. For a word with error, its syntax
context of size c is those words in the minimal subtree
in the syntax tree with no less than c leaf nodes; and its
n-gram context of size n is n? 1 words before and af-
ter itself. In the CORPUSREFINE algorithm, the input c
gives the size of syntax context and n provides the size
of the n-gram context. These two parameters decide
the amount of information that may affect the recogni-
tion of errors. To obtain the context, given a sentence
containing a grammatical error, we first get a minimum
syntax tree whose number of terminal nodes exceed the
c threshold, then check whether the achieved context
containing the error word?s n-gram, if not, add the n-
gram to the context. At last the context is returned by
CORPUSREFINE.
An example illustrating this process is presented in
Figure 2. In this example, n and c are both set to 5,
Algorithm 3 Training Corpus Refine Algorithm
1: function CORPUSREFINE(sentence, c, n)
2: context? ?
3: if sentence contains no error then
4: return ?
5: end if
6: build the syntax tree T of sentence
7: enode? the node with error in T
8: e? enode
9: while True do
10: pnode? parent node of e in T
11: cnodes ? all the children nodes of pnode
in T
12: if len(cnodes) > c then
13: context? cnodes
14: break
15: end if
16: e? pnode
17: end while
18: i? the position of enode in context
19: l? size of context
20: if i < n then
21: add (n-i) words before context at the head
of context
22: end if
23: if l-i<n then
24: append (l-i) words after context in
context
25: end if
26: return context
27: end function
the minimal syntax tree and the context decided by it
are colored in green. Since the syntax context in the
green frame does not contain the error word?s 5-gram,
therefore, the 5-gram context in the blue frame is added
to the syntax context and the final achieved context of
this sentence is ?have to stop all works for the develop-
ment?.
2.3 LM Filter
All corrections are filtered using a large LM. Only
those corrections that are not too much worse than the
original sentences are accepted. Perplexity (PPL) of the
n-gram is used to judge whether a sentence is good:
PPL = 2?
?
w?n-gram p(w) log p(w),
We use rPPL, the ratio between the PPL before and
after correction, as a metric of information gain.
rPPL =
PPLcorrected
PPLoriginal
,
Only those corrections with an rPPL lower than a cer-
tain threshold are accepted.
77
N-gram Context
Then the innovators have to stop all works for the development .
S
RB
NP
VP .
DT NNS VBP
S
VP
TO
VP
VB
NP
PP
DT NNS IN
NP
DT NN
Syntax Context
Figure 2: Example of training corpus refinement
3 Features
The single model approach enables us only to optimize
one feature set for all error type in the task, which can
drastically reduce the computational cost in feature se-
lection.
As many previous works have proposed various of
features, we first collected features from different pre-
vious works including (Dahlmeier et al, 2012; Ro-
zovskaya et al, 2012; HAN et al, 2006; Rozovskaya
et al, 2011; Tetreault, Joel R and Chodorow, Martin,
2008). Then experiments with different features were
built to test these features? effectiveness and only those
have positive contribution to the final performance
were preserved. The features we used are presented in
Table 4, where word0 is the word that we are generat-
ing features for, and word and POS is the word itself
and it?s POS tag for various components. NPHead de-
notes the head of the minimum Noun Phrase (NP) in
syntax tree. wordNP?1 represents the word appearing
before NP in the sentence. NC stands for noun com-
pound and is composed of the last n words (n ? 2)
in NP which are tagged as a noun. Verb feature is
the word that is tagged as a verb whose direct object
is the NP containing current word. Adj feature repre-
sents the first word in the NP whose POS is adjective.
Prep feature denotes the preposition word if it imme-
diately precedes the NP. DPHead is the parent of the
current word in the dependency tree, and DPRel is the
dependency relation with parent.
4 Experiments
4.1 Data Sets
The CoNLL-2013 training data consist of 1,397 arti-
cles together with gold-standard annotation. The docu-
ments are a subset of the NUS Corpus of Learner En-
glish (NUCLE) (Dahlmeier et al, 2013). The official
test data consists of 50 new essays which are also from
NUCLE. The first 25 essays were written in response to
one prompt. The remaining 25 essays were written in
response to a second prompt. One of the prompts had
been used for the training data, while the other prompt
is new. More details of the data set are described in (Ng
et al, 2013).
We split the the entire training corpus ALL by article.
For our training step, we randomly pick 90% articles of
ALL and build a training corpus TRAIN of 1,258 arti-
cles. The rest 10% of ALL with 139 articles are for
developing corpus DEV .
For the final submission, we use the entire corpus
ALL for relabeling and training.
78
Feature Example
Lexical features
word0 (current word) the
wordi, (i = ?1,?2,?3) man, stared, at, old, oak,
tree
n words before word0,
(n=2, 3, 4)
stared+at,
man+stared+at. . .
n words after word0, (n=2,
3, 4)
old+oak, old+oak+tree . . .
wordi + POSi, (i =
?1,?2,?3)
stared+VBD, at+IN. . .
First word in NP the
ith word before/after NP,
(i = ?1,?2,?3)
man, stared, at, period . . .
wordNP?1 + NP at + ( the + old + oak +
tree )
Bag of words in NP old+oak+the+tree
NC oak tree
Adj + NC old oak tree
Adj POS + NC JJ+oak tree
POS features
POS0 (current word POS) NNS
POSi, (i = ?1,?2,?3) NN, VBD, IN . . .
POS?n + POS?(n?1) +
? ? ?+POS?1, (n = 2, 3, 4)
VBD + IN, NN + VBD +
IN . . .
POS1 + POS2 + ? ? ? +
POSn, (n = 2, 3, 4)
JJ + NN, JJ + NN + NNS
. . .
Bag of POS in NP DT+JJ+NN+NN
Head word features
NPHead of NP tree
NPHead POS NN
NPHead +
NPHead POS
tree+NN
Bag of POS in NP +
NPHead
DT JJ NN NN+tree
wordNP?1 + NPHead at+tree
Adj + NPHead old+tree
Adj POS + NPHead JJ+tree
wordNP?1 + Adj +
NPHead
at+old+tree
wordNP?1 +Adj POS +
NPHead
at+JJ+tree
Preposition features
Prep at
Prep + NPHead at+tree
Prep + Adj + NPHead at+old+tree
Prep + Adj POS +
NPHead
at+JJ+tree
Prep + NC at+(oak+tree)
Prep + NP at + ( the + old + oak +
tree )
Prep + NPHead POS at+NN
Prep + Adj +
NPHead POS
at+old+NN
Prep + Adj POS +
NPHead POS
at + JJ + NN
Prep + Bag of NP POS at + ( DT + JJ + NN )
Prep + NPHead POS +
NPHead
at + NN + tree
Lemma features
Lemma the
Dependency features
DPHead word tree
DPHead POS NN
DPRel det
Table 4: Features for grammatical error recognition.
The example sentence is:?That man stared at the old
oak tree.? and the current word is ?the? .
Feature Example
Verb features
Verb stared
Verb POS VBD
Verb + NPHead stared+tree
Verb + Adj + NPHead stared+old+tree
Verb + Adj POS +
NPHead
stared+JJ+tree
Verb + NP stared+(the+old+oak+tree)
Verb + Bag of NP POS stared+(DT+JJ+NN)
Table 5: Features Continued
Count Label
1146000 ?
3369 ?
3088 NPLURAL
2766 THE?
1470 LEMMA
706 A?
200?300 IN AN? THE ARE FOR TO OF
100
?200
ON A IS GERUND PAST VSINGULAR
50?100 WITH ?THE AT AN BY THIS INTO
5?50 FROM TO? WAS ABOUT WERE ?A
THESE TO?LEMMA OF? ?OF ARE?
?TO THROUGH BE?PAST AS AMONG
IN? BE? THEIR THE?LEMMA OVER
?ON HAVE? DURING FOR? ?WITH
PART ?IN HAVE WITHIN BE MANY
?AN THE?NPLURAL MUCH IS? WITH?
TOWARDS ?FOR HAVE?PART ?ABOUT
WILL ?UPON THEIR? HAVE?PAST
HAS?PART HAS? HAS BY? BEEN?
BE?? AN?LEMMA THAT? ITS HAD
FROM? BETWEEN A?LEMMA
4 WERE? UPON THOSE ON? MANY?
IS?? ?FROM CAN AS?
3 WILL?LEMMA WILL? TOWARD THIS?
THAT ITS? HAVE?? ?BE AT? ?AS
ABOUT?
2 WOULD WAS? TO?BE? THE??
ONTO IS?PAST IS?GERUND INSIDE
HAVE?BEEN? CAN?LEMMA ?BEEN
?AT
1 WOULD?LEMMA WITHOUT UN-
DER TO?THE TO?THAT?OF
TO?HAVE THIS?WILL? THIS?MAY
THIS?HAS? THESE? THE???OF
THEIR?LEMMA THE?GERUND
THE?A? THAT?HAS?BEEN?
THAT?HAS? SHOULD? ?OVER
OF?THE? OF?THE OF?GERUND OFF
OF?A? MAY? MAY IS?TO IS?LEMMA
INTO? ?INTO IN?THE? HIS HAVE?AN
HAS?PAST HAS?BEEN?GERUND
HAS?BEEN? HAD?? HAD? DURING?
COULD CAN?BE? CAN? BY?GERUND
?BY ?BETWEEN BESIDES BEEN?PART
BEEN AT?AN AT?A AS?THE AS?HAS
AROUND ARE?PAST ARE?A ARE??
A???OF AM?
Table 6: All labels after relabeling
79
4.2 Resources
We use the following NLP resources in our sys-
tem. For relabeling and correction, perl mod-
ule Lingua::EN::Inflect1 (Conway, 1998) is used
for determining noun and verb number and Lin-
gua::EN::VerbTense2 is used for determining verb
tense. A revised and extended version of maxi-
mum entropy model3 is used for ME modeling. For
lemmatization, the Stanford CoreNLP lemma annota-
tor (Toutanova et al, 2003; Toutanova and Manning,
2000) is used. The language model is built by the
SRILM toolkit (Stolcke and others, 2002). The corpus
for building LM is the EuroParl corpus (Koehn, 2005).
The English part of the German-English parallel cor-
pus is actually used. We use such a corpus to build
LM for the following reasons: 1. LM for grammatical
error correction should be trained from corpus that it-
self is grammatically correct, and the EuroParl corpus
has very good quality of writing; 2. the NUCLE cor-
pus mainly contains essays on subjects such as environ-
ment, economics, society, politics and so on, which are
in the same dormain as those of the EuroParl corpus.
4.3 Relabeling the Corpus
There are some complicated edits in the annotations
that can not be represented by our rules, for example
substitution of non-stopwords such as {human? peo-
ple} or {are not short of? do not lack}. The relabel-
ing phase will ignore those ones thus it may not cover
all the edits. After all, we get 174 labels after relabel-
ing on the entire corpus as shown in Table 6. These
labels are generated following the syntax defined in
Figure1 and terminals defined in Table 1 and Table 2.
Directly applying these labels for correction receives an
M2 score of Precission = 91.43%,Recall = 86.92%
and F1 = 89.12%. If the non-stopwords non-inflection
edits are included, of course the labels will cover all the
golden annotations and directly applying labels for cor-
rection can receive a score up to almost F1 = 100%.
But that will get nearly 1,000 labels which is too com-
putationally expensive for a classification task. Cut out
labels with very low frequency such as those exists only
once reduces the training time, but does not give signif-
icant performance improvement, since it hurts the cov-
erage of error detection. So we use all the labels for
training.
4.4 LM Filter Threshold
To choose the threshold for rPPL, we run a series of
tests on the DEV set with different thresholds. From
our empirical observation on those right corrections
and those wrong ones, we find the right ones seldomly
1http://search.cpan.org/?dconway/
Lingua-EN-Inflect-1.89/
2http://search.cpan.org/?jjnapiork/
Lingua-EN-VerbTense-3.003/
3http://www.nactem.ac.uk/tsuruoka/
maxent/
have rPPL > 2, so we test thresholds ranging from 1.5
to 3. The curves are shown in Figure 3.
 0.14
 0.145
 0.15
 0.155
 0.16
 0.165
 0.17
 1.4  1.6  1.8  2  2.2  2.4  2.6  2.8  3
Prec
ision
, Re
call 
and
 F1 c
urve
s
LM Filter Threshold
PrecisionRecallF1
Figure 3: Different thresholds of LM filters
With higher threshold, more correction with lower
information gain will be obtained. Thus the recall
grows higher but the precission grows lower. We can
observe a peak of F1 arround 1.8 to 2.0, and the thresh-
old chosen for final submission is 1.91.
4.5 Evaluation and Result
The evaluation is done by calculating the M2 precis-
sion, recall and F1 score between the system output
and golden annotation. All the error types are evalu-
ated jointly. Only one run of a team is permitted to be
submitted. Table 7 shows our result on our DEV data
set and the official test data set.
Data Set Precission Recall F1
DEV 16.03% 15.88% 15.95%
Official 40.04% 10.89% 17.13%
Table 7: Evaluation Results
5 Conclusion
In this paper, we presented the system from team 1 of
Shanghai Jiao Tong University that participated in the
HOO 2012 shared task. Our system achieves an F1
score of 17.13% on the official test set based on gold-
standard edits.
References
DM Conway. 1998. An algorithmic approach to en-
glish pluralization. In Proceedings of the Second An-
nual Perl Conference. C. Salzenberg. San Jose, CA,
O?Reilly.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 568?572, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
80
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 216?
224, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, Atlanta, Georgia, USA.
NA-RAE HAN, MARTIN CHODOROW, and CLAU-
DIA LEACOCK. 2006. Detecting Errors in En-
glish Article Usage by Non-Native Speakers. Nat-
ural Language Engineering, 12:115?129, 5.
Ekaterina Kochmar, ?istein Andersen, and Ted
Briscoe. 2012. HOO 2012 Error Recognition and
Correction Shared Task: Cambridge University Sub-
mission Report. In Proceedings of the Seventh Work-
shop on Building Educational Applications Using
NLP, pages 242?250, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
For Statistical Machine Translation. In MT summit,
volume 5.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The conll-
2013 shared task on grammatical error correction. In
Proceedings of the Seventeenth Conference on Com-
putational Natural Language Learning.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association for
Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI System in the HOO 2012 Shared Task
on Error Correction. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 272?280, Montre?al, Canada, June.
Association for Computational Linguistics.
Andreas Stolcke et al 2002. SRILM-An Extensible
Language Modeling Toolkit. In Proceedings of the
international conference on spoken language pro-
cessing, volume 2, pages 901?904.
Tetreault, Joel R and Chodorow, Martin. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pages 865?872. Association for Compu-
tational Linguistics.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the Knowledge Sources Used in a Max-
imum Entropy Part-of-Speech Tagger. In Proceed-
ings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics-Volume 13, pages 63?70. Asso-
ciation for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich Part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
81
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 74?82,
Baltimore, Maryland, 26-27 July 2014.
c
?2014 Association for Computational Linguistics
Grammatical Error Detection and Correction
using a Single Maximum Entropy Model
?
Peilu Wang, Zhongye Jia and Hai Zhao
?
Key Laboratory of Shanghai Education Commission for
Intelligent Interaction and Cognitive Engineering,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
{plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn
Abstract
This paper describes the system of Shang-
hai Jiao Tong Unvierity team in the
CoNLL-2014 shared task. Error correc-
tion operations are encoded as a group of
predefined labels and therefore the task
is formulized as a multi-label classifica-
tion task. For training, labels are obtained
through a strict rule-based approach. For
decoding, errors are detected and correct-
ed according to the classification results.
A single maximum entropy model is used
for the classification implementation in-
corporated with an improved feature selec-
tion algorithm. Our system achieved pre-
cision of 29.83, recall of 5.16 and F 0.5
of 15.24 in the official evaluation.
1 Introduction
The task of CoNLL-2014 is grammatical error cor-
rection which consists of detecting and correcting
the grammatical errors in English essays written
by non-native speakers (Ng et al., 2014). The re-
search of grammatical error correction can poten-
tially help millions of people in the world who are
learning English as foreign language. Although
there have been many works on grammatical error
correction, the current approaches mainly focus on
very limited error types and the result is far from
satisfactory.
The CoNLL-2014 shared task, compared with
the previous Help Our Own (HOO) tasks (Dale et
al., 2012) considering only determiner and prepo-
sition errors and the CoNLL-2013 shared task fo-
?
This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the National Ba-
sic Research Program of China (Grant No.2013CB329401),
the Science and Technology Commission of Shanghai Mu-
nicipality (Grant No.13511500200), and the European Union
Seventh Framework Program (Grant No.247619).
?
Corresponding author
cusing on five major types of errors, requires to
correct all 28 types of errors (Ng et al., 2014).
One traditional strategy is designing a system
combined of a set of sub-models, where each sub-
model is specialized for a specific subtask, for ex-
ample, correcting one type of errors. This strat-
egy is computationally efficient and can adopt d-
ifferent favorable features for each subtask. Top
ranked systems in CoNLL-2013 (Rozovskaya et
al., 2013; Kao et al., 2013; Xing et al., 2013;
Yoshimoto et al., 2013; Xiang et al., 2013) are
based on this strategy. However, the division of
the model relies on prior-knowledges and the de-
signing of different features for each sub-model
requires a large amount of manual works. This
shortage is especially notable in CoNLL-2014
shared task, since the number of error types is
much larger and the composition of errors is more
complicated than before.
In contrast, we follow the work in (Jia et al.,
2013a; Zhao et al., 2009a), integrating everything
into one model. This integrated system holds a
merit that a one-way feature selection benefits the
whole system and no additional process is needed
to deal with the conflict or error propagation of ev-
ery sub-models. Here is a glance of this method: A
set of more detailed error types are generated auto-
matically from the original 28 types of errors. The
detailed error type can be regarded as the label of
a word, thus the task of grammatical error detec-
tion is transformed to a multi-label classification
task using maximum entropy model (Berger et al.,
1996; Zhao et al., 2013). A feature selection ap-
proach is introduced to get effective features from
large amounts of feature candidates. Once errors
are detected through word label classification, a
rule-based method is used to make corrections ac-
cording to their labels.
The rest of the paper is organized as follows.
Section 2 describes the system architecture. Sec-
tion 3 introduces the feature selection approach
74
and the features we used. Experiments and result-
s are presented in section 5, followed by conclu-
sion.
2 System Architecture
In our approach, the grammatical error detection
is regarded as a multi-label classification task. At
first, each token in training corpus is assigned a la-
bel according to the golden annotation. The con-
struction of labels is rule based using an extend-
ed version of Levenshtein edit distance algorith-
m which will be discussed in the following sub-
section. Each label maps an edit operation to do
the correction, thus the generated labels are much
more detailed than the originial 28 error types.
Then, a maximum entropy (ME) model is adopted
as the classifier. With the labeled data, the process
of grammatical error correction is just applying the
edit operation mapped by each label, which is ba-
sically the reverse of the labeling phase.
2.1 Data Labeling
In CoNLL-2014 shared task, there are 28 error
types but they can not be used directly as class la-
bels, since these types are too general that they can
hardly be corrected by applying one rule-based
edit. For example, the correction of Vform (ver-
b form) error type includes all verb form inflec-
tions such as converting a verb to its infinitive for-
m, gerund form, past form and past participle and
so on. Previous works (Dahlmeier et al., 2012;
Rozovskaya et al., 2012; Kochmar et al., 2012)
manually decompose each error types to more de-
tailed subtypes. For example, in (Dahlmeier et al.,
2012), the determinater errors are decomposed in-
to:
? replacement determiner (RD): { a ? the }
? missing determiner (MD): { ? ? a }
? unwanted determiner (UD): { a ? ? }
For a task with a few error types such as merely
determinative and preposition error in HOO 2012,
manually decomposition may be sufficient. How-
ever, for CoNLL-2014, all 28 error types are re-
quired to be corrected and some of these types
such as Rloc- (Local redundancy) and Um (Un-
clear meaning) are quite complex that the manu-
al decomposition is time consuming and requires
lots of grammatical knowledges. Therefore, an au-
tomatica decomposition method is proposed. It is
extended from the Levenshtein edit distance algo-
rithm and can divide error types into more detailed
subtypes that each subtype can be corrected by ap-
plying one simple rule. How to calculate the ex-
tended Levenshtein edit distance is described in
Algorithm 1.
Algorithm 1 Extended Levenshtein Edit Distance
INPUT: toks
src
, toks
dst
OUTPUT: E,P
l
src
, l
dst
? len(toks
src
), len(toks
dst
)
D[0 . . . l
src
][0 . . . l
dst
]? 0
B[0 . . . l
src
][0 . . . l
dst
]? (0, 0)
E[0 . . . l
src
][0 . . . l
dst
]? ?
for i? 1 . . . l
src
do
D[i][0]? i
B[i][0]? (i-1, 0)
E[i][0]? D
end for
for j ? 1 . . . l
dst
do
D[0][j]? j
B[0][j]? (0, j-1)
E[0][j]? A
end for
for i? 1 . . . l
src
; do
for j ? 1 . . . l
dst
do
if toks
src
[i-1] = toks
dst
[j-1] then
D[i][j]? D[i-1][j-1]
B[i][j]? (i-1, j-1)
E[i][j]? U
else
m = min(D[i-1][j-1], D[i-1][j], D[i][j-1])
if m = D[i-1][j-1] then
D[i][j]? D[i-1][j-1] + 1
B[i][j]? (i-1, j-1)
if lemma(toks
src
[i-1])
= lemma(toks
dst
[j-1]) then
E[i][j]? S
else
E[i][j]? I
end if
else if m = D[i-1][j] then
D[i][j]? D[i-1][j] + 1
B[i][j]? (i-1, j)
E[i][j]? D
else if m = D[i][j-1] then
D[i][j]? D[i][j-1] + 1
B[i][j]? (i, j-1)
E[i][j]? A
end if
end if
end for
end for
i, j ? l
src
, l
dst
while i > 0 ? j > 0 do
insert E[i][j] into head of E
insert toks
dst
[j ? 1] into head of P
(i, j)? B[i][j]
end while
return (E,P)
In this algorithm, toks
src
represents the tokens
that are annotated with one grammatical error and
toks
dst
represents the corrected tokens of toks
src
.
At first, three two dimensional matrixes D, B and
75
E are initialized. For all i and j, D[i][j] holds
the Levenshtein distance between the first i tokens
of toks
src
and first j tokens of toks
dst
. B stores
the path of the Levenshtein distance and E stores
the edit operations in this path. The original Lev-
enshtein edit distance has 4 edit operations: un-
change (U ), addition (A), deletion (D) and substi-
tution (S). We extend the ?substitution? edit into
two types of edits: inflection (I) and the original
substitution (S). If two different words have the
same lemma, the substitution operation is I, else is
S. lemma(x) returns the lemma of token x. This
algorithm returns the edit operations E and the pa-
rameters of these operations P. Here is a simple
sample illustrating this algorithm. For the golden
edit {a red apple is ? red apples are}, toks
src
is
a red apple is, toks
dst
is red apples are, the output
edits E will be {D,U , I,S}, and the parameters P
will be {-, red, apples, are}.
Then with the output of this extended Leven-
shtein distance algorithm, labels can be generated
by transforming these edit operations into readable
symbols. For those tokens without errors, we di-
rectly assign a special label ??? to them. A tricky
part of the labeling process is the problem of the
edit ?addition?,A. A new token can only be added
before or after an existing token. Thus for edit op-
eration with addition, we must find an existing to-
ken that the label can be assigned to, and this sort
of token is defined as pivot. A pivot can be a token
that is not changed in an edit operation, such as the
?apple? in edit {apple ? an apple}, or some oth-
er types of edit such as the inflection of ?look? to
?looking? in edit {look ? have been looking at}.
The names of these labels are based on BNF
syntax which is defined in Figure 1. The non-
terminal ?word? can be substituted by all words
in the vocabulary. The non-terminal ?inflection-
rules? can be substituted by terminals of inflection
rules that are used for correcting the error types of
noun number, verb form, and subject-verb agree-
ment errors. All the inflection rules are listed in
Table 1.
With the output of extended Levenshtein edits
distance algorithm, Algorithm 2 gives the process
to generate labels whose names are based on the
syntax defined in Figure 1. It takes the output E, P
of Algorithm 1 as inputs and returns the generat-
ed set of labels L. Each label in L corresponds to
one token in toks
src
in order. For our previous ex-
ample of edit {a red apple is ? red apples are},
?label? ::= ?simple-label? | ?compound-label?
?simple-label? ::= ?pivot? | ?add-before? |
?add-after?
?compound-label? ::= ?add-before? ?pivot?
| ?pivot? ?add-after?
| ?add-before? ?pivot? ?add-after?
?pivot? ::= ?unchange? | ?substitution? |
?inflection?
| ?deletion?
?add-before? ::= ?word??
| ?word???add-before?
?add-after? ::= ??word?
| ??word??add-after?
?substitution? ::= ?word?
?inflection? ::= ?inflection-rules?
?unchange? ::= ?
?deletion? ::= ?
Figure 1: BNF syntax of label
Rules Description
LEMMA change word to its lemma
NPLURAL change noun to its plural form
VSINGULAR change verb to its singular form
GERUND change verb to its gerund form
PAST change verb to its past form
PART change verb to its past partici-
ple
Table 1: Inflection rules
the L returned by Algorithm 2 is {?, ?, NPLU-
RAL, ARE} corresponding to the tokens {a, red,
apple, is} in toks
src
. Some other examples of the
generated labels are presented in Table 2.
These labels are elaborately designed that each
of them can be interpreted easily as a series of ed-
it operations. Once the labels are determined by
classifier, the correction of the grammatical errors
is conducted by applying the edit operations inter-
preted from these labels.
76
Algorithm 2 Labeling Algorithm
1: INPUT: E,P
2: OUTPUT: L
3: pivot? number of edits in E that are not A
4: L? ?
5: L?
??
6: while i < length(E) do
7: if E[i] = A then
8: L? L+ label of edit E[i] with P[i]
9: i? i + 1
10: else
11: l? L+ label of edit E[i] with P[i]
12: pivot? pivot? 1
13: if pivot = 0 then
14: i? i + 1
15: while i < length of E do
16: l? l +?+ P[i]
17: i? i + 1
18: end while
19: end if
20: push l into L
21: L?
??
22: end if
23: end while
24: L? upper case of L
25: return L
Tokens Edit Label
to
{to reveal?revealing}
?
reveal GERUND
a
{a woman?women}
?
woman NPLURAL
developing {developing world THE?
wold ?the developing world} ?
a {a? ?} ?
in {in?on} ON
apple {apple?an apple} AN?
Table 2: Examples of labeling
2.2 Label Classification
Using the approach described above, the training
corpus is converted to a sequence of words with
labels. Maximum entropy model is used as the
classifier. It allows a very rich set of features to be
used in a model and has shown good performance
in similiar tasks (Zhao et al., 2013). The features
we used are discussed in the next section.
3 Feature Selection and Generation
One key factor affecting the performance of maxi-
mum entropy classifier is the features it used. A
good feature that contains useful information to
guide classification will significantly improve the
performance of the classifier. One direct way to
involve more good features is involving more fea-
tures.
In our approach, large amounts of candidate
features are collected at first. We carefully exam-
ine the factors involved in a wide range of fea-
tures that have been or can be used to the word
label classification task. Many features that are
considered effective in various of previous work-
s (Dahlmeier et al., 2012; Rozovskaya et al.,
2012; Han et al., 2006; Rozovskaya et al., 2011;
Tetreault, Joel R and Chodorow, Martin, 2008)
are included. Besides, features that are used in
the similar spell checking tasks (Jia et al., 2013b;
Yang et al., 2012) and some novel features show-
ing effectiveness in other NLP tasks (Wang et al.,
2013; Zhang and Zhao, 2013; Xu and Zhao, 2012;
Ma and Zhao, 2012; Zhao, 2009; Zhao et al.,
2009b) are also included. However, using too
many features is time consuming. Besides, it in-
creases the probability of overfitting and may lead
to a poor solution of the maximum-likelihood pa-
rameter estimate in the ME training.
Algorithm 3 Greedy Feature Selection
1: INPUT: all feature candidates F
2: OUTPUT: selected features S
3: S = {f
0
, f
1
, . . . , f
k
}, a random subset of F
4: while do
5: C = RECRUITMORE(S)
6: if C = {} then
7: return S
8: end if
9: S
?
= SHAKEOFF(S+C)
10: if scr(M(S)) ? scr(M(S
?
)) then
11: return S
12: end if
13: S = S
?
14: end while
15: function RECRUITMORE(S)
16: C = {}, and p = scr(M(S))
17: for each f ? F ? S do
18: if p < scr(M(S + {f})) then
19: C = C + {f}
20: end if
21: end for
22: end function
23: function SHAKEOFF(S)
24: while do
25: S
?
= S
0
= S
26: for each f ? S do
27: if scr(M(S
?
)) < scr(M(S
?
? {f})) then
28: S
?
= S
?
? {f}
29: end if
30: end for
31: S = S
?
32: if S
?
= S
0
then
33: return S
?
34: end if
35: end while
36: end function
Therefore a feature selection algorithm is intro-
duced to filter out ?bad? features at first and the re-
maining features will be used to generate new fea-
tures. The feature selection algorithm has shown
77
effectiveness in (Zhao et al., 2013) and is present-
ed in Algorithm 3.
In this algorithm, M(S) represents the model
using feature set S and scr(M) represents the e-
valuation score of model M on a development da-
ta set. It repeats two main steps until no further
performance gain is achievable:
1. Include any features from the rest of F into
the current set of candidate features if the in-
clusion would lead to a performance gain.
2. Exclude any features from the current set of
candidate templates if the exclusion would
lead to no deterioration in performance.
By repeatedly adding the useful and removing
the useless features, the algorithm aims to return
a better and smaller set of features for next round.
Only 55 of the 109 candidate features remain af-
ter using this algorithm and they are presented in
Table 4. Table 3 gives an interpretation of the ab-
breviations used in Table 4. Each feature of a word
is set to that listed in feature column if the word
satisfies the condition listed in current word col-
umn, else the feature is set to ?NULL?. For ex-
ample, if the current word satisfies the condition
in the first row of Table 4 which is the first word
in the left of a NC, feature 1 of this word is set to
all words in the NC, otherwise, feature 1 is set to
?NULL?.
4 Experiment
4.1 Data Sets
The CoNLL-2014 training data is a corpus of
learner English provided by (Dahlmeier et al.,
2013). This corpus consists of 1,397 articles, 12K
sentences and 116K tokens. The official blind test
data consists of 50 articles, 245 sentences and 30K
tokens. More detailed information about this data
is described in (Ng et al., 2014; Dahlmeier et al.,
2013).
In development phase, the entire training corpus
is splited by sentence. 80% sentences are picked
up randomly and used for training and the rest
20% are used as the developing corpus. For the fi-
nal submission, the entire corpus is used for train-
ing.
Abbreviation Description
NP Noun Phrase
NC Noun Compound and is ac-
tive if second to last word in
NP is tagged as noun
VP Verb Phrase
cw Current Word
pos part-of-speech of the current
word
X.l
i
the ith word in the left of X
X.r
i
the ith word in the right of X
NP[0] the first word of NP
NP.head the head word of NP
NP.(DT or
IN or TO)
word in NP whose pos is DT
or IN or TO
VP.verb word in VPwhose pos is ver-
b
VP.NP NP in VP
dp the dependency relation gen-
erated by standford depen-
dency parser
dp.dep the dependent in the depen-
dency relation
dp.head the head in the dependency
relation
dp.rel the type of the dependency
relation
Table 3: The interpretation of the abbrevations in
Table 4
4.2 Data Labeling
The labeling algorithm described in section 2.1 is
firstly applied to the training corpus. Total 7047
labels are generated and those whose count is larg-
er than 15 is presented in Table 5. Directly ap-
plying these 7047 labels for correction receives an
M
2
score of precision=90.2%, recall=87.0% and
F 0.5=89.5%. However, the number of labels
is too large that the training process is time con-
suming and those labels appears only few times
will hurt the generalization of the trained model.
Therefore, labels with low frequency which ap-
pear less than 30 times are cut out and 109 labels
remain. The M
2
score of the system using this re-
fined labels is precision=83.9%, recall=64.0% and
F 0.5=79.0%. Note that even applying all labels,
the F 0.5 is not 100%. It is because some annota-
tions in the training corpus are not consistency.
78
current word feature
NC.l
1
NC
NP.l
1
NP
NP[0] NP.l
1
.pos
NC.l
1
NC
NC.l
1
NC.l
1
.pos
NC.l
1
and pos=DT NC
NC.l
1
and pos=VB NC
NP.l
1
and pos=VB NP
pos=VB cw
pos=DT cw
the cw.r
1
a cw.r
1
an cw.r
1
NP[0] cw
NP[0] NP.l
1
NP[0] NP.l
2
NP[0] NP.l
3
NP[0] NP.l
1
.pos
NP[0] NP.l
2
.pos
NP[0] NP.l
3
.pos
NP.l
1
NP.head
NP.l
1
NP.head.pos
NP.head NP. head
NP.head NP. head.bag
NP.head NP. head.pos
NP.head NP. head.pos.bag
NP.head NP. (JJ or CC)
NP.(DT or IN or TO) NP
NP.(DT or IN or TO) NP.head
NP.(DT or IN or TO) NP.head.pos
dp.dep dp.head
dp.head dp.dep
dp.dep dp.head.pos
dp.head dp.dep.pos
dp.dep dp.rel
dp.head dp.rel
VP.verb VP.NP
VP.verb VP.NP.head
VP.NP.head VP.verb
VP.verb VP.NP.head.pos
VP.NP.head VP.verb.pos
cw cw.l
i
, i ? {0, 1, 2, 3}
cw cw.r
i
, i ? {1, 2, 3}
cw cw.l
i
.pos, i ? {0, 1, 2, 3}
cw cw.r
i
.pos, i ? {1, 2, 3}
Table 4: Remained features after the feature selec-
tion.
Count Label
1091911 ?
31507 ?
3637 NPLURAL
2822 THE?
2600 LEMMA
948 ,?
300?900 A? PAST THE IN TO . IS OF ARE FOR
GERUND ,
50?100 AND ON AN? A VSINGULAR WAS THEIR
20?50 ELDERLY IT OF? THEY WITH TO?
WERE THIS ; ITS .? THAT ?S? AND?
THAT? HAVE? CAN AS HAVE?PART
FROM BE WOULD BY
15?20 HAVE HAS?WILL HAS AT AN THESE ?,
THEM IN? INTO #? ARE? WHICH PEO-
PLE HAS?PART ECONOMIC IS? BE? SO
COULD TO?LEMMA MANY PART MAY
LESS IT? FOR? BEING?
15?20 NOT ABOUT WILL?LEMMA SHOULD
HIS BECAUSE AGED SUCH ALSO
WHICH? HAVE?PAST WILL? WHO
WHEN MUCH
15?20 ON? ? THROUGH BE?PAST MORE
IF HELP THE?ELDERLY ?S ONE AS?
THERE THEIR? WITH? HAVE??
ECONOMY DEVELOPMENT CON-
CERNED PEOPLE? PROBLEMS BUT
MEANS THEREFORE HOWEVER BE-
ING : UP PROBLEM ?? THE?LEMMA
IN?ADDITION HOWEVER?,? AMONG
;? WHERE THUS ONLY HEALTH
HAS?PAST FUNDING EXTENT ALSO?
TECHNOLOGICAL ? OR HAD WOULD?
VERY .?THIS ITS? IMPORTANT DEVEL-
OPED ?BEEN AGE ABOUT?WHO? USE
THEY? THAN NUMBER HOWEVER?,
GOVERNMENT FURTHERMORE DURING
BUT? YOUNGER RIGHT POPULATION
PERSON? FEWER ENVIRONMENTAL-
LY WOULD?LEMMA OTHER MAY?
LIMITED HE COULD?HAVE BEEN STIL-
L SPENDING SAFETY OVER ONE??S
MAKE MADE LIFE HUMAN HAD?
FUNDS CARE ARGUED ALL ?? WHEN?
TIME THOSE SOCIETY RESEARCH
PROVIDE OLD NEEDS INCREASING DE-
VELOPING BECOME BE?? ADDITION
Table 5: Labels whose count is larger than 15.
current word feature
NC.l
1
NC, cw, cw.l
1
, cw.l
1
.pos,
cw.r
1
, cw.r
1
.pos
NP[0] NP.head, NP.l
1
, NP.l
2
,
cw, cw.l
1
, cw.l
1
.pos,
NP.head NP[0], NP.l
1
, NP.l
2
, cw,
cw.l
1
, cw.l
1
.pos,
dp.head cw, cw.l
1
, cw.l
2
dp.dep,
dp.dep.pos, dp.rel
Table 6: Examples of the new generated features.
79
4.3 Data Refinement
The training corpus is refined before used that sen-
tences which do not contain errors are filtered out.
Only 38% of the total sentences remain. With less
training corpus, it takes less time to train the ME
model. Table 7 presents the performance of sys-
tems using the unrefined training corpus and re-
fined corpus.
System Presicion Recall F 0.5
unrefined 26.99% 1.67% 6.71%
refined 11.17% 3.1% 7.34%
Table 7: Comparison of systems with differen-
t training corpus.
All sets of these systems are kept the same ex-
cept the training corpus they use. It can be seen
that the refinement also improves the performance
of the system.
4.4 Feature Selection
Figure 2 shows the results of systems with dif-
ferent feature sets. sys 10 is the system with
Figure 2: Performance of systems with different
features.
10 randomly chosen features which are used as
the initial set of features in Algorithm 3, sys 55
is the system with the refined 55 features. With
these refined features, various of new features are
generated by combining different features. This
combination is conducted empirically that features
which are considered having relations are com-
bined to generate new features. Using this method,
165 new features are generated and total 220 fea-
tures are used in sys 220. Table 6 gives a few
of examples showing the combined features. The
performance is evaluated by the precision, recal-
l and F 0.5 score of the M
2
scorer according
to (Dahlmeier and Ng, 2012). It can be seen
that sys 220 with the most number of features
achieves the best performance.
4.5 Evaluation Result
The final system we use is sys 220 with refined
training data, the performance of our system on the
developing corpus and the blind official test data is
presented in Table 8. The score is calculated using
M
2
scorer.
Data Set Precision Recall F 0.5
DEV 13.52% 6.41% 11.07%
OFFICIAL 29.83% 5.16% 15.24%
Table 8: Evaluation Results
5 Conclusion
In this paper, we describe the system of Shang-
hai Jiao Tong Univerity team in the CoNLL-2014
shared task. The grammatical error detection is re-
garded as a multi-label classification task and the
correction is conducted by applying a rule-based
approach based on these labels. A single max-
imum entropy classifier is introduced to do the
multi-label classification. Various features are in-
volved and a feature selection algorithm is used
to refine these features. Finally, large amounts of
feature templates that are generated by the combi-
nation of the refined features are used. This system
achieved precision of 29.83%, recall of 5.16% and
F 0.5 of 15.24% in the official evaluation.
References
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAA-
CL 2012), pages 568?572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 Shared Task. In
Proceedings of the Seventh Workshop on Building E-
ducational Applications Using NLP, pages 216?224,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
80
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications (BEA
2013), pages 22?31, Atlanta, Georgia, USA.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Second Workshop on Building Education-
al Applications Using NLP, pages 54?62, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
NA-RAE Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting Errors in English Article
Usage by Non-Native Speakers. Natural Language
Engineering, 12:115?129, 5.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013a.
Grammatical error correction as multiclass classi-
fication with single model. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 74?81,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013b. Graph
model for chinese spell checking. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 88?92, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Ting-hui Kao, Yu-wei Chang, Hsun-wen Chiu, Tzu-Hsi
Yen, Joanne Boisson, Jian-cheng Wu, and Jason S.
Chang. 2013. Conll-2013 shared task: Grammati-
cal error correction nthu system description. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 20?25, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Ekaterina Kochmar, ?istein Andersen, and Ted
Briscoe. 2012. HOO 2012 Error Recognition and
Correction Shared Task: Cambridge University Sub-
mission Report. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 242?250, Montr?eal, Canada, June.
Association for Computational Linguistics.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters, pages 785?796, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The conll-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association for
Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI System in the HOO 2012 Shared Task
on Error Correction. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 272?280, Montr?eal, Canada, June.
Association for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Tetreault, Joel R and Chodorow, Martin. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pages 865?872. Association for Compu-
tational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
845?850, Seattle, Washington, USA, October. Asso-
ciation for Computational Linguistics.
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hybrid
model for grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Computation-
al Natural Language Learning: Shared Task, pages
115?122, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidi-
a S. Chao, and Xiaodong Zeng. 2013. Um-checker:
A hybrid system for english grammatical error cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34?42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING 2012: Posters, pages
1341?1350, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.
Shaohua Yang, Hai Zhao, Xiaolin Wang, and Bao
liang Lu. 2012. Spell checking for chinese.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet U?gur Do?gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and
81
Stelios Piperidis, editors, Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 730?736, Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA). ACL Anthology Identifier:
L12-1423.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuza-
wa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumo-
to. 2013. Naist at 2013 conll grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 26?33, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2211?2217. AAAI Press, August.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009a.
Semantic dependency parsing of nombank and prop-
bank: An efficient integrated approach via a large-
scale feature selection. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 30?39, Singapore, August.
Association for Computational Linguistics.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009b. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 55?63,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. In-
tegrative semantic dependency parsing via efficien-
t large-scale feature selection. Journal of Artificial
Intelligence Research, 46:203?233.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 879?887, Athens, Greece,
March. Association for Computational Linguistics.
82
