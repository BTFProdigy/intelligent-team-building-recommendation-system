Machine-Assisted Rhetorical Structure Annotation
Manfred Stede and Silvan Heintze
University of Potsdam
Dept. of Linguistics
Applied Computational Linguistics
D-14415 Potsdam
Germany
stede|heintze@ling.uni-potsdam.de
Abstract
Manually annotating the rhetorical struc-
ture of texts is very labour-intensive. At the
same time, high-quality automatic analysis
is currently out of reach. We thus propose to
split the manual annotation in two phases:
the simpler marking of lexical connectives
and their relations, and the more difficult
decisions on overall tree structure. To this
end, we developed an environment of two
analysis tools and XML-based declarative
resources. Our ConAno tool allows for effi-
cient, interactive annotation of connectives,
scopes and relations. This intermediate re-
sult is exported to O?Donnell?s ?RST Tool?,
which facilitates completing the tree struc-
ture.
1 Introduction
A number of approaches tackling the difficult
problem of automatic discourse parsing have
been proposed in recent years (e.g., (Sumita
et al, 1992) (Marcu, 1997), (Schilder, 2002)).
They differ in their orientation toward sym-
bolic or statistical information, but they all ?
quite naturally ? share the assumption that
the lexical connectives or discourse markers are
the primary source of information for construct-
ing a rhetorical tree automatically. The den-
sity of discourse markers in a text depends on
its genre (e.g., commentaries tend to have more
than narratives), but in general, it is clear that
only a portion of the relations holding in a text
is lexically signalled.1 Furthermore, it is well-
known that discourse markers are often ambigu-
ous; for example, the English but can, in terms
of (Mann, Thompson, 1988), signal any of the
relations Antithesis, Contrast, and Concession.
Accordingly, automatic discourse parsing focus-
ing on connectives is bound to have its limita-
tions.
1In our corpus of newspaper commentaries (Stede,
2004), we found that 35% of the coherence relations are
signalled by a connective.
Our position is that progress in discourse
parsing relies on the one hand on a more thor-
ough understanding of the underlying issues,
and on the other hand on the availability of
human-annotated corpora, which can serve as
a resource for in-depth studies of discourse-
structural phenomena, and also for training
statistical analysis programs. Two examples
of such corpora are the RST Tree Corpus by
(Marcu et al, 1999) for English and the Pots-
dam Commentary Corpus (Stede, 2004) for
German. Producing such resources is a labour-
intensive task that requires time, trained anno-
tators, and clearly specified guidelines on what
relation to choose under which circumstances.
Nonetheless, rhetorical analysis remains to be
in part a rather subjective process (see section
2). In order to eventually arrive at more objec-
tive, comparable results, our proposal is to split
the annotation process into two parts:
1. Annotation of connectives, their scopes
(the two related textual units), and ? op-
tionally ? the signalled relation
2. Annotation of the remaining (unsignalled)
relations between larger segments
Step 1 is inspired by work done for English
in the Penn Discourse TreeBank2 (Miltsakaki
et al, 2004). In our two-step scenario, it is the
easier part of the whole task in that connectives
can be quite clearly identified, their scopes are
often (but not always, see below) transparent,
and the coherence relation is often clear. We see
the result of step 1 as a corpus resource in its
own right (it can be used for training statistical
classifiers, for instance) and at the same time
as the input for step 2, which ?fills the gaps?:
now annotators have to decide how the set of
small trees produced in step 1 is best arranged
in one complete tree, which involves assigning
2http://www.cis.upenn.edu/?pdtb/
relations to instances without any lexical sig-
nals and also making more complicated scope
judgements across larger spans of text ? the
more subjective and also more time-consuming
step.3
Our approach is as follows. To speed up the
annotation process in step 1, we have devel-
oped an XML format and a dedicated analysis
tool called ConAno, which will be introduced
in Section 4. ConAno can export the anno-
tated text in the ?rs3? format that serves as in-
put to O?Donnell?s RST Tool (O?Donnell, 1997).
His original idea was that manual annotation be
done completely with his tool; we opted however
to use it only for step 2, and will motivate the
reasons for this overall architecture in Section
5.
The net result is a modular, XML-based
annotation environment for machine-assisted
rhetorical analysis, which we see as on the one
hand less ambitious than fully-automatic dis-
course parsing and on the other hand as more
efficient than completely ?manual? analysis.
2 Approaches to rhetorical analysis
There are two different perspectives on the task
of discourse parsing: an ?ideal? one that aims
at modelling a systematic, incremental process;
and an ?empirical? one that takes the experi-
ences of human annotators into account. ?Ide-
ally?, discourse analysis proceeds incrementally
from left to right, where for each new segment,
an attachment point and a relation (or more
than one of each, cf. SDRT) are computed and
the discourse structure grows step by step. This
view is taken for instance in SDRT (Asher, Las-
carides, 2003), which places emphasis on the no-
tion of ?right frontier? (also discussed recently by
(Webber et al, 2003)).
However, when we trained two (experienced)
students to annotate the 171 newspaper com-
mentaries of the Potsdam Commentary Corpus
(Stede, 2004) and upon completion of the task
asked them about their experiences, a very dif-
ferent picture emerged. Both annotators agreed
that a strict left-to-right approach is highly im-
practical, because the intended argumentative
structure of the text often becomes clear only
in retrospect, after reflecting the possible con-
tributions of the segments to the larger scheme.
3This assessment of relative difficulty does not carry
over to PDTB, where the annotations are more complex
than in our step 1 but do not go as far as building rhetor-
ical structures.
Thus they very soon settled on a bottom-up ap-
proach: First, mark the transparent cases, in
which a connective undoubtedly signals a rela-
tion between two segments.4 Then, see how the
resulting pieces fit together into a structure that
mirrors the argument presented.
The annotators used RST Tool (O?Donnell,
1997), which worked reasonably well for the pur-
pose. However, since we also have in our group
an XML-based lexicon of German connectives
at our disposal (Berger et al, 2002), why not
use this resource to speed up the first phase of
the annotation?
3 Annotating connectives and their
scopes
In our definition of ?connective?, we largely fol-
low (Pasch et al, 2003) (a substantial catalogue
and classification of German connectives), who
require them to take two arguments that can
potentially be full clauses and that semantically
denote two-place relations between eventualities
(but they need not always be spelled out as
clauses). From the syntactic viewpoint, they are
a rather inhomogeneous group consisting of sub-
ordinating and coordinating conjunctions, some
prepositions, and a number of sententence ad-
verbials. We refer to the two related units as
an ?internal? and an ?external? one, where the
?internal? one is the unit of which the connec-
tive is actually a part. For example, in Despite
the heavy rain we had a great time, the noun
phrase the heavy rain is the internal unit, since
it forms a syntactic phrase together with the
preposition. Notice that this is a case where
the eventuality (a state of weather) is not made
explicit by a verb.
As indicated, this step of annotating connec-
tives and units is closely related to the idea
of the PDTB project, which seeks to develop
a large corpus annotated with information on
discourse structure for English texts. For this
purpose, annotators are provided with detailed
annotation guidelines, which point out various
challenges in the annotation process for explicit
as well as empty connectives and their respec-
tive arguments. They include, among others,
? words/phrases that look like connectives,
but prove not to take two propositional ar-
guments
4The clearest cases are subjunctors, which always
mark a relation between matrix clause and embedded
clause.
? words/phrases as preposed predicate com-
plements
? pre- and post-modified connectives
? co-occurring connectives
? single and multiple clauses/sentences as ar-
guments of connectives
? annotation of discontinuous connective ar-
guments
Annotators have to also make syntactic judge-
ments, which is not the case in our approach
(where syntax would be done on a different an-
notation layer, see (Stede, 2004)).
In the following, we briefly explain the most
important problematic issues with annotating
German connectives and the way we deal with
them, using our annotation scheme for Con-
Ano.
3.1 Issues with German connectives
Connective or not: Some words can be used
as connective or in other functions, such as und
(?and?), which can for example conjoin clauses
(connective) or items in a list (no connective).
Which relation: Some connectives can sig-
nal more than one relation, as the above-
mentioned but and its German counterpart aber.
Complex connectives: Connectives can be
phrasal (e.g., aus diesem Grund, ?for this rea-
son?) or even discontinuous (e.g., entweder
. . . oder, ?either . . . or?). A fortiori, some may
be used in more than one order (wenn A, dann
B / dann B, wenn A / dann, wenn A, B; ?if
. . . then . . . ?).
Multiple connectives/relations: Some
connectives can be joined to form a complex
one, which might then signal more than one
relation (e.g., combinations with und and aber,
such as aber dennoch, ?but still?).
Modified connectives: Some but not all
connectives are subject to modification (e.g.,
nur dann, wenn, ?only then, if?; besonders weil,
?especially because?).
Embedded segments: The minimal units
linked by the connective may be embedded
rather than adjacent: Wir mu?ssen, weil die Zeit
dra?ngt, uns Montag treffen (?We have to, be-
cause time is short, meet on Monday?).
3.2 A DTD and an Example
As the first step toward an annotation tool, we
defined an XML format for texts with connec-
tives and their scopes. Figure 1 shows the DTD,
and Figure 2 a short sample annotation of a
single ? yet complex ? sentence: Auch Berlin
koennte, jedenfalls dann, wenn der Bund sich
erkenntlich zeigt, um die Notlage seiner Haupt-
stadt zu lindern, davon profitieren. (?Berlin,
too, could ? at least if the federation shows
some gratitude in order to alleviate the emer-
gency of its capital ? profit from it.?) The DTD
introduces XML tags for each of the connec-
tives (<connective>), their possible modifiers
(<modifier>) and respective discourse units
(<unit>, where the type can be filled by int or
ext), as well as the entire text (<discourse>).
Henceforth, we will refer to the text unit con-
taining the connective as the internal, ?int-unit?
and to the other, external, one as ?ext-unit?. Us-
ing this DTD, it is possible to represent the
range of problematic phenomena discussed in
the previous section.
Connective or not: Only those words actu-
ally used as connectives will be marked with the
<connective> tag, while others such as the fre-
quently occurring und (?and?) or oder (?or?) will
remain unmarked, if they merely conjoin items
in a list.
Which relation: The <connective> tag in-
cludes a rel attribute for optional specification
of the rhetorical relation that holds between the
connected clauses.
Complex connectives: Using an XML
based annotation scheme, we can easily mark
phrasal connectives such as aus diesem Grund
(?for this reason?) using the <connective> tag.
In order for discontinuous connectives to be an-
notated correctly, we introduce an id attribute
that provides every connective with a distinct
reference number. This way connectives such as
entweder . . . oder, (?either . . . or?) can be repre-
sented as belonging together. (see <connective
id="4" rel="condition"> tags in Figure 2)
Multiple connectives/relations: In our
annotation scheme, complex connectives such
as aber dennoch, (?but still?) are treated as two
distinct connectives that indicate different rela-
tions holding between the same units.
Modified connectives: Connective modi-
fiers are marked with a special <modifier> tag,
which is embedded inside the <connective>
tag, as shown with jedenfalls modifying dann in
our example. Hence an additional id attribute
for this tag is not necessary.
Embedded segments: Discourse units are
marked using the <unit> tag, which also pro-
vides an id attribute. On the one hand, this
is used for assigning discourse units to their re-
spective connectives, on the other hand it pro-
vides a way of dealing with discountinuous dis-
course units, as the example shows.
<?xml version=?1.0? encoding=?UTF-8??>
<!ELEMENT modifier (#PCDATA)>
<!ELEMENT connective (#PCDATA|modifier)>
<!ATTLIST connective
id CDATA #IMPLIED
rel CDATA #IMPLIED>
<!ELEMENT unit
(#PCDATA|connective|unit)*>
<!ATTLIST unit
id CDATA #IMPLIED
type CDATA #IMPLIED>
<!ELEMENT discourse (#PCDATA|unit)*>
Figure 1: The DTD for texts-with-connectives
<?xml version="1.0"?>
<!DOCTYPE discourse SYSTEM "discourse.dtd">
<discourse>
<unit type="ext" id="4">
Auch Berlin koennte,
<connective id="4"
relation="condition">
<modifier>jedenfalls</modifier>
dann
</connective>
,
</unit>
<unit type="int" id="4">
<connective id="4"
relation="condition">
wenn
</connective>
der Bund sich erkenntlich zeigt, um
die Notlage seiner Hauptstadt zu
lindern,
</unit>
<unit type="ext" id="4">
davon profitieren.
</unit>
</discourse>
Figure 2: Sample annotation in XML-format
4 The ConAno annotation tool
A range of relatively generic linguistic annota-
tion tools are available today, but none of them
turned out suitable for our purposes: We seek a
very easy-to-use, platform-independent tool for
mouse-marking ranges of text and having them
associated with one another. Consequently, we
decided to implement our own Java-based tool,
ConAno, which is geared especially to connec-
tive/scope annotation and thus can have a very
intuitive user interface.
Just like discourse parsers do, ConAno ex-
ploits the fact that connectives are the most re-
liable source of information. Rather than at-
tempting an automatic disambiguation, how-
ever, ConAno merely makes suggestions to the
human analyst, which she might follow or dis-
card. In particular, ConAno loads a list of
(potential) connectives, and when annotation
of a text begins, highlights each candidate so
that the user can either confirm (by marking its
scope) or discard (by mouse-click) it if it not
used as a connective. Furthermore, the connec-
tive list optionally may contain associated co-
herence relations, which are then also offered to
the user for selection. This annotation phase
is thus purely data-driven: Attention is paid
only to the connectives and their specific rela-
tion candidates.
To elaborate a little, the annotation process
proceeds as follows. The text is loaded into the
annotation window, and the first potential con-
nective is automatically highlighted. Potential
preposed or postposed modifiers, if any, of the
connective are also highlighted (in a different
color). The user moves with the mouse from
one connective to the next and
? can with a mouseclick discard a highlighted
item (it is not a connective or not a modi-
fier),
? can call up a help window explaining the
syntactic behavior and the relations of this
connective,
? can call up a suggestion for the int-unit
(i.e., text portion is highlighted),
? can analogously call up a suggestion for the
ext-unit,
? can choose from a menu of the relations
associated with this connective.
A screenshot is given in Figure 4. The sugges-
tions for int-unit and ext-unit are made by Co-
nAno on the basis of the syntactic category of
Figure 3: Screenshot of ConAno
the connective; we use simple rules like ?search
up to the next comma? to find the likely int-
unit for a subjunctor, or ?search the preceding
two full-stops? to find the ext-unit for an adver-
bial (the preceding sentence). The suggestions
may be wrong; then the user discards them and
marks them with the mouse herself. The result
of this annotation phase is an XML file like the
(very short) one shown in Figure 2.
5 Overall annotation environment
A central design objective is to keep the envi-
ronment neutral with respect to the languages
of the text, the connectives to be annotated, and
the coherence relations associated with them.
Accordingly, the list of connectives is external
and read into ConAno upon startup. In our
case, we use an XSLT sheet to map our ?Dis-
course Marker Lexicon? (see below) to the input
format of ConAno. The text to be annotated
is expected in plain ASCII. When annotation is
complete, the result (or an intermediate result)
can be saved in our XML-format introduced in
section 3.2. Optionally, it can be exported to
the ?rs3? format developed by (O?Donnell, 1997)
for his RSTTool. This allows for a smooth tran-
rs3
ConAno
RSTTool
DiMLex
xslt raw text
text with connectives,
scopes (and relations)
rhetorical tree
text with full
Figure 4: Overview of annotation environment
sition to a tool for constructing complete rhetor-
ical trees. Rather than starting from scratch,
the RSTTool user can now open the file pro-
duced by ConAno, which amounts to a partial
rhetorical analysis of the text, and which the
user can now complete to a full tree.
Our Discourse Marker Lexikon ?DiMLex?
(Berger et al, 2002) assembles information on
140 German connectives, giving a range of syn-
tactic, semantic, and pragmatic features, in-
cluding the coherence relations along the lines of
(Mann, Thompson, 1988). They are encoded in
an application-neutral XML format (see Figure
5), which are mapped with XSLT sheets to vari-
ous NLP applications. Our new proposal here is
to use it also for interactive connective annota-
tion. Hence, we wrote an XSLT sheet that maps
DiMLex to a reduced list, where each connective
is associated with syntactic labels coordination,
subordination or adverb and <coh-relation>
entries for its potential relations ? see Figure
6 for DTD and 7 for an example. The, for
these purposes quite simple, syn value has been
mapped from the more complex classification in
DiMLex under kat (German for category). This
format is the input to ConAno.
As indicated above, we do not see the tran-
sition to RSTTool as a necessary step. Rather,
the intermediate result of connective/scope an-
notation is useful in its own right, as it encodes
those aspects of rhetorical structure that are in-
dependent of the chosen set of coherence rela-
tions and the conditions of assigning them.
6 Summary
With our work on German discourse connec-
tives, the structure of their argument units,
and the indicated rhetorical relations, we seek
a better understanding of underlying linguistic
issues on the one hand, and an easier way of
developing rhetorical structure-annotated cor-
pora for German texts on the other hand. For
<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl"
href="short_dictionary.xsl" ?>
<!DOCTYPE dictionary SYSTEM "dimlex.dtd">
<dictionary>
<entry id="41">
<orth phrasal="0">denn</orth>
<syn>
<kat>konj</kat>
<position>vorvorfeld</position>
<!-- . . . -->
</syn>
<semprag>
<relation>cause</relation>
<relation>explanation</relation>
<presupp>int-unit</presupp>
<!-- . . . -->
</semprag>
<example>
Das Konzert muss ausfallen,
*denn* die Saengerin ist erkrankt.
</example>
<example>
Die Blumen auf dem Balkon sind
erfroren, *denn* es hat heute
nacht Frost gegeben.
</example>
</entry>
</dictionary>
Figure 5: DiMLex extract
this purpose, we present an annotation envi-
ronment, including our ConAno tool, which
helps human annotators to mark discourse con-
nectives and their argument units by finding
possible connectives and making suggestions on
their estimated argument structure. We pointed
out several challenges in the connective annota-
tion process of German texts and introduced an
XML based annotation scheme to handle the
difficulties. For one thing, the results of this
step provide elobarate information about the
structure of German texts with respect to dis-
course connectives, but furthermore they can be
used as input to O?Donnell?s RST Tool, in or-
der to complete the annotation of the rhetorical
tree structure. The overall scenario is then one
of machine-assisted rhetorical structure anno-
tation. Since ConAno is based on an external
list of connectives (with associated syntactic la-
bels and relations), the tool is not dedicated to
one particular theory of discourse structure, let
alone to a specific set of relations. Furthermore,
it can in principle deal with texts in various lan-
guages (it just relies on string matching between
<?xml version=?1.0? encoding=?UTF-8??>
<!ELEMENT example (#PCDATA)>
<!ELEMENT coh-relation (#PCDATA)>
<!ELEMENT sem (example|coh-relation)*>
<!ELEMENT syn (sem)*>
<!ATTLIST syn
type CDATA #IMPLIED>
<!ELEMENT part (#PCDATA)>
<!ATTLIST part
type CDATA #IMPLIED>
<!ELEMENT orth (part)*>
<!ATTLIST orth
type CDATA #IMPLIED>
<!ELEMENT entry (syn|orth)*>
<!ATTLIST entry
id CDATA #IMPLIED>
<!ELEMENT conanolex (entry)*>
Figure 6: DTD for connectives in ConAno in-
put format
<entry id="116">
<orth type="cont">
<part type="single">wenn</part>
</orth>
<orth type="discont">
<part type="single">wenn</part>
<part type="single">dann</part>
</orth>
<syn type="subordination">
<sem>
<coh-relation>condition
</coh-relation>
<example>*Wenn* man auf den Knopf
drueckt, oeffnet sich die Tuer
von selbst.
</example>
<example>*Wenn* du mich fragst,
*dann* wuerde ich die Finger
davon lassen.
</example>
</sem>
</syn>
</entry>
Figure 7: Connective information in ConAno
input format
connectives in the list and in the text), but we
have so far used it only for German.
Acknowledgements
We thank the anonymous reviewers for their
constructive comments and suggestions for im-
proving the paper.
References
Asher, N. and Lascarides, A. 2003. Logics of
Conversation. Cambridge University Press.
Berger, D.; Reitter, D. and Stede, M. 2002.
XML/XSL in the Dictionary: The Case of
Discourse Markers. In: Proc. of the Coling
Workshop ?NLPXML-2002?, Tapei.
O?Donnell, M. 1997. RST-Tool: An RST Analy-
sis Tool. Proc. of the 6th European Workshop
on Natural Language Generation, Duisburg.
Mann, W. and Thompson, S. 1988. Rhetorical
Structure Theory: A Theory of Text Organi-
zation. TEXT 8(3), 243-281.
Marcu, D. 1997. The rhetorical parsing of nat-
ural language texts. Proc. of the 35th Annual
Conference of the ACL, 96-103.
Marcu, D.; Amorrortu, E. and Romera, M.
1999. Experiments in Constructing a Corpus
of Discourse Trees. In: Proc. of ACL Work-
shop ?Towards Standards and Tools for Dis-
course Tagging?, University of Maryland.
Miltsakaki, E.; Prasad, R.; Joshi, A. and Web-
ber, B. 2004. Annotating Discourse Connec-
tives and their Arguments. In: Proc. of the
HLT/NAACL Workshop ?Frontiers in Corpus
Annotation?, Boston.
Pasch, R; Brausse, U.; Breindl, E. and Wass-
ner, H. 2003. Handbuch der deutschen Kon-
nektoren. Berlin: deGruyter.
Schilder, F. 2002. Robust Discourse Parsing via
Discourse Markers, Topicality and Position.
Natural Language Engineering 8 (2/3).
Stede, M. 2004. The Potsdam Commentary
Corpus. In: Proc. of the ACL Workshop ?Dis-
course Annotation?, Barcelona.
Sumita, K.; Ono, K.; Chino, T.; Ukita, T.;
Amano, S. 1992. A discourse structure ana-
lyzer for Japanese text. Proc. of the Interna-
tional Conference on Fifth Generation Com-
puter Systems, 1133-1140.
Webber, B.; Knott, A.; Stone, M. and Joshi,
2003. A. Anaphora and Discourse Structure.
Computational Linguistics 29(4), 545-588.
  
Rhetorical Parsing with Underspecification and Forests 
Thomas Hanneforth 
 Dept. of Linguistics 
University of Potsdam 
P.O. Box 601553 
14415 Potsdam, Germany 
tom@ling.uni-
potsdam.de 
Silvan Heintze 
Dept. of Linguistics 
University of Potsdam 
P.O. Box 601553 
14415 Potsdam, Germany 
heintze@ling.uni-
potsdam.de 
Manfred Stede 
Dept. of Linguistics 
University of Potsdam 
P.O. Box 601553 
14415 Potsdam, Germany 
stede@ling.uni-
potsdam.de 
 
Abstract 
We combine a surface based approach to dis-
course parsing with an explicit rhetorical 
grammar in order to efficiently construct an 
underspecified representation of possible dis-
course structures. 
1 Introduction 
The task of rhetorical parsing, i.e., automatically de-
termining discourse structure, has been shown to be 
relevant, inter alia, for automatic summarization (e.g., 
Marcu, 2000). Not surprisingly, though, the task is very 
difficult. Previous approaches have thus emphasized the 
need for heuristic or probabilistic information in the 
process of finding the best or most likely rhetorical tree. 
As an alternative, we explore the idea of strictly 
separating ?high-confidence? information from hypo-
thetical reasoning and of working with underspecified 
trees as much as possible. We create a parse forest on 
the basis of surface cues found in the text. This forest 
can then be subject to further processing. Depending on 
the application, such further steps can either calculate 
the ?best? tree out of the forest or continue working 
with a set of structured hypotheses.  
Section 2 briefly summarizes our proposal on under-
specified rhetorical trees; section 3 introduces our 
grammar approach to text structure; section 4 compares 
this strategy to earlier work. 
2 Parse forests and underspecification  
We will illustrate the underspecification of ambiguities 
with the following example: 
?(1) Yesterday the delegates elected their new rep-
resentative by a narrow margin. Even though (2) Smith 
got only 234 votes, (3) he accepted the position. But (4) 
his predecessor was rather irritated by the results.? 
We take it that even though unambiguously marks a 
CONCESSION between the embedded clause (2, satellite) 
and the matrix clause (3, nucleus). For the purpose of 
illustration, we also assume that ?but? can only signal a 
bi-nuclear CONTRAST relation with the second nucleus 
(4); the span of the first nucleus is in this case ambigu-
ous (1-3 or 2-3). For linking (1) to the remaining mate-
rial, we suppose that either ELABORATION (with nucleus 
(1)) or SEQUENCE holds. Further relations are possible, 
which will add to the possibilities, but our points can be 
made with the situation as just described. 
Instead of enumerating all possible rhetorical trees 
for our example text, we use a parse forest representa-
tion which compactly encodes the different analysises. 
A parse forest is basically an attributed And-Or-graph 
with the properties of subtree sharing and containment 
of ambiguities. The first property means that a subtree, 
which plays different roles in some bigger structure, is 
represented only once. The second property ensures that 
two subtrees which have in common the same category 
and the same terminal yield, but which differ in the first 
step of a leftmost derivation are unified together. 
Fig. 1 shows a simplified parse forest for the exam-
ple text. 
 
Fig.1: Parse forest for the input text 
 
Subtree sharing is indicated by nodes (e.g. ?1?) 
which have several incoming edges. Containment of 
ambiguities is exemplified in fig. 1 by the upper left  
contrast node which represents a disjunctive hypothesis 
concerning the span of the relation.  
Reitter and Stede (to appear) developed an XML-
DTD scheme to represent such parse forests in XML 
notation. 
  
3 Discourse structure parsing 
In our approach, we combine a standard chunk parser 
which identifies the relevant units for discourse process-
ing with a feature-based grammar which builds larger 
rhetorical trees out of these chunks. The categories and 
features we use are summarized in table 1. 
 
Cat. Feat. 
 
Values Comment 
  RST-tree 
cat macro_seg, 
s, ip, pp, ? 
The category of the 
RST-tree: macro 
segments, phrases 
sentences etc. 
type ns,  
nn,  
term 
Type of RST-tree: 
nuc-sat, multi-
nuclear or terminal 
role nuc, sat Nucleus or satellite 
relation elaboration, 
contrast, 
cause, ? 
The relation which 
combines the 
daughters of the 
RST-tree. 
rst 
dp no_dp, 
but, al-
though,  
? 
The discourse par-
ticle triggering the 
relation, or no_dp, 
if absent. 
dp See 
above 
 Discourse particle 
chunk   Phrase or sentence 
punct   Punctuation 
Table 1: Grammar categories and features 
 
There are three groups of grammar rules: 
1. Rules combining chunks to terminal RST-trees  
2. Rules combining discourse particles and sentence 
fragments to non-primitives RST-trees 
3. Rules combining sentences or groups of sentences 
(so called macro segments) to non-primitive RST-
trees. 
 
An example for a rule in group 1 is the one which 
builds a terminal RST-tree of category mc (main clause) 
out of a discourse particle, and sentence fragment and a 
full stop (all examples are given in Prolog-style nota-
tion, with curly brackets indicating feature structures): 
(1) 
rst({cat:mc, dp:DP,  type:term}) ---> 
dp({cat:pav, dp:DP}), 
chunk({cat:ip}), 
punct({cat:fullstop}). 
 
Rules like this one are used to build terminal RST-
trees for sentences like (4) in our example text. 
The second group of rules is exemplified by a rule 
which combines two terminal RST-trees - a subordinate 
clause containing a conjunction like even though and 
another clause - to a hypotactic RST-tree: 
(2) 
rst({cat:mc, rel:concession, dp:no_dp, type:ns}) --->  
rst({cat:sc, dp:even_though, role:sat}), 
rst({cat:mc, dp:no_dp, role:nuc}). 
 
The macro segment building rules of the third group 
can be divided into two subclasses. The first class is 
constituted by rules which construct RST-trees on the 
basis of a relation that is triggered by a discourse parti-
cle. An example of this type is the possible contrast-
relation between segments 4 and 2-3 in (1), which is 
triggered by the discourse particle but. 
(3) 
rst({cat:macro_seg, rel:contrast, 
    dp:no_dp, type:ns}) ---> 
rst({cat:macro_seg, role:sat}), 
rst({cat:macro_seg, role:nuc, dp:but}). 
 
The other subclass contains rules which freely con-
struct branching RST-trees without the overt evidence 
of discourse particles. The relations which are typically 
involved here are SEQUENCE and ELABORATION. Rela-
tions which have in common the same type of nucleus-
satellite-configuration are unified into a single rule us-
ing the list-valued form of the relation-feature: 
(4) 
rst({cat:macro_seg, rel:[sequence,elaboration],  
     dp:no_dp, type:nn}) ---> 
rst({cat:macro_seg, role:nuc, dp:no_dp}), 
rst({cat:macro_seg, role:nuc, dp:no_dp}). 
 
Fig. 2 shows a parse tree which reflects one analysis 
of our example text. Note that the segments into which 
the input is broken usually smaller than sentences. 
 
Yesterday
the
delegates
elected
their
new
representative
by a
narrow
margin
chunk [cat:s]
.
punct
rst [cat:mc]
Even
though
dp [cat:kous]
Smith
got only
234
votes
chunk [cat:ip]
,
punct
rst [cat:sc]
he
accepted
the
position
chunk [cat:ip]
.
punct
rst [cat:mc]
2-7
concession
0-7
sequence
But
dp [cat:pav]
his
predecessor
was
rather
irritated
by the
results
chunk [cat:ip]
.
punct
rst [cat:mc]
0-10
contrast
 
Fig.2: Sample parse tree for the input text 
 
Rules like (4) ensure the robustness of the grammar 
as they can be used to combine partial structures with-
out any structure triggering discourse particles. 
  
Furthermore, rules of the kind shown in (4) are on 
the one hand necessary to produce all possible branch-
ing structure over a given sequence of terminal ele-
ments. On the other hand they introduce massive 
ambiguities into the grammar which causes the number 
of analyses to grow according to the Catalan numbers 
(cf. Aho and Ullman, 1972, p. 165). 
It is therefore crucial that during parsing the con-
struction of parse trees is strictly avoided because that 
would turn an otherwise polynomial parsing algorithm 
like chart parsing into an exponential one. Instead we 
incrementally build the parse forest mentioned in sec-
tion 2. This is done by assigning a unique id to each 
edge introduced into the chart and by storing the ids of 
the immediate daughters within the edge. After parsing 
the parse forest is constructed by partitioning the set of 
edges into equivalence classes. Two chart edges E1 and 
E2 are in the same equivalence class if they a) have 
identical start and end positions and b) the categories of 
E1 and E2 subsume each other. For the subsumption test 
it is necessary to ignore the role-feature, because this 
feature is an attribute of the parse forest edges and not 
of the parse forest nodes. 
Besides keeping the parsing algorithm polynomial it 
is of equal importance to keep the grammar constant 
low. For example, rule (4) which establishes a 
SEQUENCE/ELABORATION relation between two macro 
segments also connects two simple clauses (of category 
mc), a macro segment and a simple clause, or a simple 
clause and a macro segment. The standard move to 
avoid this kind of rule multiplication is to introduce an 
unary chain rule of the form  
rst({cat:macro_seg}) ---> rst({cat:mc}) 
which ensures the desired level shifting. 
Because of the inherent relational nature of RST trees 
this solution is blocked. Instead we use an inheritance 
hierarchy like that in fig. 3 and replace rule (4) with the 
following one, which is underspecified w.r.t to the cate-
gory feature. 
(5) 
rst({cat:macro_seg, rel:[sequence,elaboration],  
     dp:no_dp, type:nn}) ---> 
rst({cat:rst_tree, role:nuc, dp:no_dp}), 
rst({cat:rst_tree, role:nuc, dp:no_dp}). 
 
segment
rst_tree
mc macro_seg
non_rst_tree
pp sc  
Fig 3: Simplified inheritance hierarchy for cat  
4 Related work 
Similar to Marcu (2000) we assume discourse markers 
as indicators for rhetorical relations. 
But contrary to Marcu (1999) and also to Schilder 
(2002) we use a full-fledged discourse grammar and a 
standard parsing algorithm, which makes it, in our opin-
ion, unnecessary to propose special rhetorical tree build-
ing operations, as suggested e.g. by Marcu (1999). 
By using the chart parsing algorithm combined with 
the construction of an underspecified parse forest, it can 
easily be shown that our method is of cubic complexity. 
This is a crucial property, because it is commonly as-
sumed that the number of distinct structures that can be 
constructed over a sequence of n discourse units is ex-
ponential in n, (as it is for example implicit in the DCG 
based algorithm proposed by Schilder, 2002). 
Our system is robust in the same way as the one in 
Schilder (2002) because the grammar admits under-
specified rhetorical trees in the absence of overt dis-
course markers. 
5 Conclusion 
We have shown that a grammar based approach to rhe-
torical parsing is suitable for efficient and robust con-
struction of underspecified rhetorical structures. 
References 
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory 
of Parsing, Translating and Compiling. Volume 1. 
Prentice-Hall, Englewood Cliffs, NJ. 
Daniel Marcu. 1999. A decision-based approach to rhe-
torical parsing. The 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'99), 
pages 365-372, Maryland, June 1999.  
Daniel Marcu. 2000. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-Based Approach. Computa-
tional Linguistics, 26 (3), pages 395-448. 
David Reitter and Manfred Stede. to appear. Step by 
Step: Underspecified Markup in Incremental Rhe-
torical Analysis. To appear in: Proc. Of the 4th Inter-
national Workshop on Linguistically Interpreted 
Corpora (LINC-03). Budapest. 
Frank Schilder. 2002. Robust Discourse Parsing via 
Discourse Markers, Topicality and Position. Natural 
Language Engineering 8 (2/3). 
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 9?16,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Comparing Local and Sequential Models for
Statistical Incremental Natural Language Understanding
Silvan Heintze, Timo Baumann, David Schlangen
Department of Linguistics
University of Potsdam, Germany
firstname.lastname@uni-potsdam.de
Abstract
Incremental natural language understand-
ing is the task of assigning semantic rep-
resentations to successively larger prefixes
of utterances. We compare two types of
statistical models for this task: a) local
models, which predict a single class for
an input; and b), sequential models, which
align a sequence of classes to a sequence
of input tokens. We show that, with some
modifications, the first type of model can
be improved and made to approximate the
output of the second, even though the lat-
ter is more informative. We show on two
different data sets that both types of model
achieve comparable performance (signifi-
cantly better than a baseline), with the first
type requiring simpler training data. Re-
sults for the first type of model have been
reported in the literature; we show that for
our kind of data our more sophisticated
variant of the model performs better.
1 Introduction
Imagine being at a dinner, when your friend Bert
says ?My friend, can you pass me the salt over
there, please??. It is quite likely that you get the
idea that something is wanted of you fairly early
into the utterance, and understand what exactly it
is that is wanted even before the utterance is over.
This is possible only because you form an un-
derstanding of the meaning of the utterance even
before it is complete; an understanding which
you refine?and possibly revise?as the utterance
goes on. You understand the utterance incremen-
tally. This is something that is out of reach for
most current dialogue systems, which process ut-
terances non-incrementally, en bloc (cf. (Skantze
and Schlangen, 2009), inter alia).
Enabling incremental processing in dialogue
systems poses many challenges (Allen et al,
2001; Schlangen and Skantze, 2009); we focus
here on the sub-problem of modelling incremental
understanding?a precondition for enabling truly
interactive behaviour. More specifically, we look
at statistical methods for learning mappings be-
tween (possibly partial) utterances and meaning
representations. We distinguish between two types
of understanding, which were sketched in the first
paragraph above: a) forming a partial understand-
ing, and b) predicting a complete understanding.
Recently, some results have been published on
b), predicting utterance meanings, (Sagae et al,
2009; Schlangen et al, 2009). We investigate
here how well this predictive approach works in
two other domains, and how a simple extension of
techniques (ensembles of slot-specific classifiers
vs. one frame-specific one) can improve perfor-
mance. To our knowledge, task a), computing par-
tial meanings, has so far only been tackled with
symbolic methods (e.g., (Milward and Cooper,
1994; Aist et al, 2006; Atterer and Schlangen,
2009));1 we present here some first results on ap-
proaching it with statistical models.
Plan of the paper: First, we discuss relevant pre-
vious work. We then define the task of incremental
natural language understanding and its two vari-
ants in more detail, also looking at how models
can be evaluated. Finally, we present and discuss
the results of our experiments, and close with a
conclusion and some discussion of future work.
2 Related Work
Statistical natural language understanding is an ac-
tive research area, and many sophisticated mod-
els for this task have recently been published, be
that generative models (e.g., in (He and Young,
2005)), which learn a joint distribution over in-
1We explicitly refer to computation of incremental inter-
pretations here; there is of course a large body of work on
statistical incremental parsing (e.g., (Stolcke, 1995; Roark,
2001)).
9
(Mairesse et al, 2009) 94.50
(He and Young, 2005) 90.3
(Zettlemoyer and Collins, 2007) 95.9
(Meza et al, 2008) 91.56
Table 1: Recent published f-scores for non-
incremental statistical NLU, on the ATIS corpus
put, output and possibly hidden variables; or, more
recently, discriminative models (e.g., (Mairesse et
al., 2009)) that directly learn a mapping between
input and output. Much of this work uses the ATIS
corpus (Dahl et al, 1994) as data and hence is di-
rectly comparable. In Table 1, we list the results
achieved by this work; we will later situate our re-
sults relative to this.
That work, however, only looks at mappings be-
tween complete utterances and semantic represen-
tations, whereas we are interested in the process of
mapping semantic representations to successively
larger utterance fragments. More closely related
then is (Sagae et al, 2009; DeVault et al, 2009),
where a maximum entropy model is trained for
mapping utterance fragments to semantic frames.
(Sagae et al, 2009) make the observation that of-
ten the quality of the prediction does not increase
anymore towards the end of the utterance; that is,
the meaning of the utterance can be predicted be-
fore it is complete.
In (Schlangen et al, 2009), we presented a
model that predicts incrementally a specific as-
pect of the meaning of a certain type of utterance,
namely the intended referent of a referring expres-
sion; the similarity here is that the output is of the
same type regardless of whether the input utter-
ance is complete or not.
(DeVault et al, 2009) discuss how such ?mind
reading? can be used interactionally in a dialogue
system, e.g. for completing the user?s utterance
as an indication of the system?s grounding state.
While these are interesting uses, the approach is
somewhat limited by the fact that it is incremental
only on the input side, while the output does not
reflect how ?complete? (or not) the input is. We
will compare this kind of incremental processing
in the next section with one where the output is
incremental as well, and we will then present re-
sults from our own experiments with both kinds of
incrementality in statistical NLU.
3 Task, Evaluation, and Data Sets
3.1 The Task
We have said that the task of incremental natural
language understanding consists in the assignment
of semantic representations to progressively more
complete prefixes of utterances. This description
can be specified along several aspects, and this
yields different versions of the task, appropriate
for different uses. One question is what the as-
signed representations are, the other is what ex-
actly they are assigned to. We investigate these
questions here abstractly, before we discuss the in-
stantiations in the next sections.
Let?s start by looking at the types of representa-
tions that are typically assigned to full utterances.
A type often used in dialogue systems is the frame,
an attribute value matrix. (The attributes are here
typically called slots.) These frames are normally
typed, that is, there are restrictions on which slots
can (and must) occur together in one frame. The
frames are normally assigned to the utterance as a
whole and not to individual words.
In an incremental setting, where the input
potentially consists of an incomplete utterance,
choosing this type of representation and style of
assignment turns the task into one of prediction of
the utterance meaning. What we want our model
to deliver is a guess of what the meaning of the ut-
terance is going to be, even if we have only seen
a prefix of the utterance so far; we will call this
?whole-frame output? below.2
Another popular representation of semantics in
applied systems uses semantic tags, i.e., markers
of semantic role that are attached to individual
parts of the utterance. Such a style of assignment
is inherently ?more incremental?, as it provides a
way to assign meanings that represent only what
has indeed been said so far, and does not make as-
sumptions about what will be said. The semantic
representation of the prefix simply contains all and
only the tags assigned to the words in the prefix;
this will be called ?aligned output? below. To our
knowledge, the potential of this type of represen-
tation (and the models that create them) for incre-
mental processing has not yet been explored; we
present our first results below.
Finally, there is a hybrid form of representation
and assignment. If we allow the output frames to
?grow? as more input comes in (hence possibly vi-
olating the typing of the frames as they are ex-
pected for full utterances), we get a form of rep-
resentation with a notion of ?partial semantics? (as
2In (Schlangen and Skantze, 2009), this type of incremen-
tal processing is called ?input incremental?, as only the input
is incrementally enriched, while the output is always of the
same type (but may increase in quality).
10
only that is represented for which there is evidence
in what has already been seen), but without direct
association of parts of the representation and parts
of the utterance or utterance prefix.
3.2 Evaluation
Whole-Frame Output A straightforward met-
ric is Correctness, which can take the values 1
(output is exactly as expected) or 0 (output is not
exactly as expected). Processing a test corpus in
this way, we get one number for each utterance
prefix, and, averaging this number, one measure-
ment for the whole corpus.
This can give us a first indication of the gen-
eral quality of the model, but because it weighs
the results for prefixes of all lengths equally, it
cannot tell us much about how well the incremen-
tal processing worked. In actual applications, we
presumably do not expect the model to be correct
from the very first word on, but do expect it to get
better the longer the available utterance prefix be-
comes. To capture this, we define two more met-
rics: first occurrence (FO), as the position (relative
to the eventual length of the full utterance) where
the response was correct first; and final decision
(FD) as the position from which on the response
stayed correct (which consequently can only be
measured if indeed the response stays correct).3
The difference between FO and FD then tells us
something about the stability of hypotheses of the
model.
In some applications, we may indeed only be
able to do further processing with fully correct?
or at least correctly typed?frames; in which case
correctness and FO/FD on frames are appropriate
metrics. However, sometimes even frames that are
only partially correct can be of use, for example if
specific system reactions can be tied to individual
slots. To give us more insight about the quality of a
model in such cases, we need a metric that is finer-
grained than binary correctness. Following (Sagae
et al, 2009), we can conceptualise our task as one
of retrieval of slot/value pairs, and use precision
and recall (and, as their combination, f-score) as
metrics. As we will see, it will be informative to
plot the development of this score over the course
of processing the utterance.
For these kinds of evaluations, we need as a
gold standard only one annotation per utterance,
3These metrics of course can only be computed post-hoc,
as during processing we do not know how long the utterance
is going to be.
namely the final frame.
Aligned Output As sequence alignments have
more structure?there is a linear order between the
tags, and there is exactly one tag per input token?
correctness is a more fine-grained, and hence more
informative, metric here; we define it as the pro-
portion of tags that are correct in a sequence. We
can also use precision and recall here, looking at
each position in the sequence individually: Has
the tag been recalled (true positive), or has some-
thing else been predicted instead (false negative,
and false positive)? Lastly, we can also recon-
struct frames from the tag sequences, where se-
quences of the same tag are interpreted as seg-
menting off the slot value. (And hence, what was
several points for being right or wrong, one for
each tag, becomes one, being either the correct
slot value or not. We will discuss these differences
when we show evaluations of aligned output.)
For this type of evaluation, we need gold-
standard information of the same kind, that is, we
need aligned tag sequences. This information is
potentially more costly to create than the one fi-
nal semantic representation needed for the whole-
frame setting.
Hybrid Output As we will see below, the hy-
brid form of output (?growing? frames) is pro-
duced by ensembles of local classifiers, with one
classifier for each possible slot. How this output
can be evaluated depends on what type of informa-
tion is available. If we only have the final frame,
we can calculate f-score (in the hope that preci-
sion will be better than for the whole-frame clas-
sifier, as such a classifier ensemble can focus on
predicting slots/value pairs for which there is di-
rect evidence); if we do have sequence informa-
tion, we can convert it to growing frames and eval-
uate against that.
3.3 The Data Sets
ATIS As our first dataset, we use the ATIS air
travel information data (Dahl et al, 1994), as pre-
processed by (Meza et al, 2008) and (He and
Young, 2005). That is, we have available for each
utterance a semantic frame as in (1), and also a
tag sequence that aligns semantic concepts (same
as the slot names) and words. One feature to note
here about the ATIS representations is that the slot
values / semantic atoms are just the words in the
utterance. That is, the word itself is its own se-
mantic representation, and no additional abstrac-
11
tion is performed. In this domain, this is likely un-
problematic, as there aren?t many different ways
(that are to be expected in this domain) to refer to
a given city or a day of the week, for example.
(1) ?What flights are there arriving in Chicago after
11pm??
?
?
?
?
GOAL = FLIGHT
TOLOC.CITY NAME = Chicago
ARRIVE TIME.TIME RELATIVE = after
ARRIVE TIME.TIME = 11pm
?
?
?
?
In our experiments, we use the ATIS training
set which contains 4481 utterances, between 1
and 46 words in length (average 11.46; sd 4.34).
The vocabulary consists of 897 distinct words.
There are 3159 distinct frames, 2594 (or 58% of
all frames) of which occur only once. Which of
the 96 possible slots occur in a given frame is
distributed very unevenly; there are some very
frequent slots (like FROMLOC.CITYNAME
or DEPART DATE.DAY NAME) and
some very rare or even unique ones (e.g.,
ARRIVE DATE.TODAY RELATIVE, or
TIME ZONE).
Pentomino The second corpus we use is of ut-
terances in a domain that we have used in much
previous work (e.g., (Schlangen et al, 2009;
Atterer and Schlangen, 2009; Ferna?ndez and
Schlangen, 2007)), namely, instructions for ma-
nipulating puzzle pieces to form shapes. The par-
ticular version we use here was collected in a
Wizard-of-Oz study, where the goal was to instruct
the computer to pick up, delete, rotate or mirror
puzzle tiles on a rectangular board, and drop them
on another one. The user utterances were anno-
tated with semantic frames and also aligned with
tag sequences. We use here a frame representation
where the slot value is a part of the utterance (as
in ATIS), an example is shown in (2). (The cor-
pus is in German; the example is translated here
for presentation.) We show the full frame here,
with all possible slots; unused slots are filled with
?empty?. Note that this representation is some-
what less directly usable in this domain than for
ATIS; in a practical system, we?d need some fur-
ther module (rule-based or statistical) that maps
such partial strings to their denotations, as this
mapping is less obvious here than in the travel do-
main.
(2) ?Pick up the W-shaped piece in the upper right cor-
ner?
?
?
?
?
?
?
?
action = ?pick up?
tile = ?the W-shaped piece
in the upper right corner?
field = empty
rotpar = empty
mirpar = empty
?
?
?
?
?
?
?
The corpus contains 1563 utterances, average
length 5.42 words (sd 2.35), with a vocabulary of
222 distinct words. There are 964 distinct frames,
with 775 unique frames.
In both datasets we use transcribed utterances
and not ASR output, and hence our results present
an upper bound on real-world performance.
4 Local Models: Support Vector Machines
In this section we report the results of our exper-
iments with local classifiers, i.e. models which,
given an input, predict one out of a set of classes as
an answer. Such models are very naturally suited
to the prediction task, where the semantics of the
full utterance is treated as its class, which is to be
predicted on the basis of what possibly is only a
prefix of that utterance. We will also look at a
simple modification, however, which enables such
models to do something that is closer to the task of
computing partial meanings.
4.1 Experimental Setup
For our experiments with local models, we used
the implementations of support vector machines
provided by the WEKA toolkit (Witten and Frank,
2005); as baseline we use a simple majority class
predictor.4
We used the standard WEKA tools to convert
the utterance strings into word vectors. Training
was always done with the full utterance, but test-
ing was done on prefixes of utterances; i.e., a sen-
tence with 5 words would be one instance in train-
ing, but in a testing fold it would contribute 5 in-
stances, one with one word, one with two words,
and so on.5 Because of this special way of testing
the classifiers, and also because of the modifica-
4We tried other classifiers (C4.5, logistic regression, naive
Bayes) as well, and found comparable performance on a de-
velopment set. However, because of the high time costs
(some models needed > 40 hours for training and testing on
modern multi-CPU servers) we do not systematically com-
pare performance and instead focus on SVMs. In any case,
our interest here is not in comparing classification algorithms,
but rather in exploring approaches to the novel problem of
statistical incremental NLU.
5On a development set, we tried training on utterance pre-
fixes, but that degraded performance, presumably due to in-
crease in ambiguous training instances (same beginnings of
what ultimately are very different utterances).
12
tions described below, we had to provide our own
methods for cross-validation and evaluation. For
the larger ATIS data set, we used 10 folds in cross
validation, and for the Pentomino dataset 20 folds.
4.2 Results
To situate our results, we begin by looking at
the performance of the models that predict a full
frame, when given a full utterance; this is the
normal, ?non-incremental? statistical NLU task.6
(3)
classf. metric ATIS Pento
maj correctness 1.07 1.79
maj f-score 35.98 16.15
SVM correctness 16.21 38.77
SVM f-score 68.17 63.23
We see that the results for ATIS are considerably
lower than the state of the art in statistical NLU
(Table 1). This need not concern us too much
here, as we are mostly interested in the dynam-
ics of the incremental process, but it indicates that
there is room for improvement with more sophisti-
cated models and feature design. (We will discuss
an example of an improved model shortly.) We
also see a difference between the corpora reflected
in these results: being exactly right (good correct-
ness) seems to be harder on the ATIS corpus, while
being somewhat right (good f-score) seems to be
harder on the pento corpus; this is probably due to
the different sizes of the search space of possible
frame types (large for ATIS, small for pento).
What we are really interested in, however, is the
performance when given only a prefix of an ut-
terance, and how this develops over the course of
processing successively larger prefixes. We can
investigate this with Figure 1. First, look at the
solid lines. The black line shows the average f-
score at various prefix lengths (in 10% steps) for
the ATIS data, the grey line for the pento corpus.
We see that both lines show a relatively steady in-
cline, meaning that the f-score continues to im-
prove when more of the utterance is seen. This is
interesting to note, as both (DeVault et al, 2009)
and (Atterer et al, 2009) found that in their data,
all that is to be known can often be found some-
what before the end of the utterance. That this
does not work so well here is most likely due to
the difference in domain and the resulting utter-
ances. Utterances giving details about travel plans
6The results for ATIS are based on half of the overall
ATIS data, as cross-validating the model on all data took pro-
hibitively long, presumably due to the large number of unique
frames / classes.
2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
percentiles into utterance
f?sc
ore
l l
l
l
l
l
l
l
l l
l all utterancesshort utterancesnormal utteranceslong utterances
l
l
l
l
l
l
l
l
l l
Figure 1: F-Score by Length of Prefix
are likely to present many important details, and
some of them late into the utterance; cf. (1) above.
The data from (DeVault et al, 2009) seems to be
more conversational in nature, and, more impor-
tantly, presumable the expressible goals are less
closely related to each other and hence can be read
off of shorter prefixes.
As presented so far, the results are not very
helpful for practical applications of incremental
NLU. One thing one would like to know in a prac-
tical situation is how much the prediction of the
model can be trusted for a given partial utterance.
We would like to read this off graphs like those
in the Figure?but of course, normally we cannot
know what percentage of an utterance we have al-
ready seen! Can we trust this averaged curve if we
do not know what length the incoming utterance
will have?
To investigate this question, we have binned the
test utterances into three classes, according to their
length: ?normal?, for utterances that are of aver-
age length? half a standard deviation, and ?short?
for all that are shorter, and ?long? for all that are
longer. The f-score curves for these classes are
shown with the non-solid lines in Figure 1. We
see that for ATIS there is not much variation com-
pared to averaging over all utterances, and more-
over, that the ?normal? class very closely follows
the general curve. On the pento data, the model
seems to be comparably better for short utterances.
In a practical application, one could go with
the assumption that the incoming utterance is go-
ing to be of normal length, and use the ?normal?
13
curve for guidance; or one could devise an ad-
ditional classifier that predicts the length-class of
the incoming utterance, or more generally predicts
whether a frame can already be trusted (DeVault et
al., 2009). We leave this for future work.
As we have seen, the models that treat the se-
mantic frame simply as a class label do not fare
particularly well. This is perhaps not that surpris-
ing; as discussed above, in our corpora there aren?t
that many utterances with exact the same frame.
Perhaps it would help to break up the task, and
train individual classifiers for each slot?7 This
idea can be illustrated with (2) above. There we al-
ready included ?unused? slots in the frame; if we
now train classifiers for each slot, allowing them
to predict ?empty? in cases where a slot is unused,
we can in theory reconstruct any frame from the
ensemble of classifiers. To cover the pento data,
the ensemble is small (there are 5 frames); it is
considerably larger for ATIS, where there are so
many distinct slots.
Again we begin by looking at the performance
for full utterances (i.e., at 100% utterance length),
but this time for constructing the frame from the
reply of the classifier ensemble:
(4)
classf. metric ATIS Pento
maj correctness 0.16 0
maj f-score 33.18 20.24
SVM correctness 52.69 50.48
SVM f-score 86.79 73.15
We see that this approach leads to an impressive
improvement on the ATIS data (83.64 f-score in-
stead of 68.17), whereas the improvement on the
pento data is more modest (73.15 / 63.23).
Figure 2 shows the incremental development of
the f-scores for the reconstructed frame. We see
a similar shape in the curves; again a relatively
steady incline for ATIS and a more dramatic shape
for pento, and again some differences in behaviour
for the different length classes of utterances. How-
ever, by just looking at the reconstructed frame,
we are ignoring valuable information that the slot-
classifier approach gives us. In some applications,
we may already be able to do something useful
with partial information; e.g., in the ATIS domain,
we could look up an airport as soon as a FROM-
LOC becomes known. Hence, we?d want more
fine-grained information, not just about when we
can trust the whole frame, but rather about when
7A comparable approach is used for the non-incremental
case for example by (Mairesse et al, 2009).
2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
percentiles into utterance
f?sc
ore
l l
l
l
l
l
l
l
l
l
l all utterancesshort utterancesnormal utteranceslong utterancesl
l
l
l l
l
l
l
l l
Figure 2: F-Score by Length of Prefix; Slot Clas-
sifiers
we can trust individual predicted slot values. (And
so we move from the prediction task to the partial
representations task.)
To explore this, we look at First Occurrence and
Final Decision for some selected slots in Table 2.
For some slots, the first occurrence (FO) of the
correct value comes fairly early into the utterance
(e.g., for the name of the airline it?s at ca. 60%,
for the departure city at ca. 63%, both with rela-
tively high standard deviation, though) while oth-
ers are found the first time rather late (goal city
at 81%). This conforms well with intuitions about
how such information would be presented in an ut-
terance (?I?d like to fly on Lufthansa from Berlin
to Tokyo?).
We also see that the predictions are fairly stable:
the number of cases where the slot value stays cor-
rect until the end is almost the same as that where
it is correct at least once (FD applicable vs. FO
apl), and the average position is almost the same.
In other words, the classifiers seem to go fairly
reliably from ?empty? (no value) to the correct
value, and then seem to stay there. The overhead
of unnecessary edits (EO) is fairly low for all slots
shown in the table. (Ideally, EO is 0, meaning that
there is no change except the one from ?empty? to
correct value.) All this is good news, as it means
that a later module in a dialogue system can often
begin to work with the partial results as soon as
a slot-classifier makes a non-empty prediction. In
an actual application, how trustworthy the individ-
ual classifiers are would then be read off statistics
14
slot name avg FO stdDev apl avg FD stdDev apl avg EO stdDev apl
AIRLINE NAME 0.5914 0.2690 506 0.5909 0.2698 501 0.5180 0.5843 527
DEPART TIME.PERIOD OF DAY 0.7878 0.2506 530 0.7992 0.2476 507 0.2055 0.5558 579
FLIGHT DAYS 0.4279 0.2660 37 0.4279 0.2660 37 0.0000 0.0000 37
FROMLOC.CITY NAME 0.6345 0.1692 3633 0.6368 0.1692 3554 0.1044 0.4526 3718
ROUND TRIP 0.5366 0.2140 287 0.5366 0.2140 287 0.0104 0.1015 289
TOLOC.CITY NAME 0.8149 0.1860 3462 0.8162 0.1856 3441 0.2348 0.5723 3628
frames 0.9745 0.0811 2382 0.9765 0.0773 2361 0.7963 1.1936 4481
Table 2: FO/FD/EO for some selected slots; averaged over utterances of all lengths
like these, given a corpus from the domain.
To conclude this section, we have shown that
classifiers that predict a complete frame based on
utterance prefixes have a somewhat hard task here
(harder, it seems, than in the corpus used in (Sagae
et al, 2009), where they achieve an f-score of 87
on transcribed utterances), and the prediction re-
sults improve steadily throughout the whole utter-
ance, rather than reaching their best value before
its end. When the task is ?spread? over several
classifiers, with each one responsible for only one
slot, performance improves drastically, and also,
the results become much more ?incremental?. We
now turn to models that by design are more incre-
mental in this sense.
5 Sequential Models: Conditional
Random Fields
5.1 Experimental Setup
We use Conditional Random Fields (Lafferty et
al., 2001) as our representative of the class of se-
quential models, as implemented in CRF++.8 We
use a simple template file that creates features
based on a left context of three words.
Even though sequential models have the poten-
tial to be truly incremental (in the sense that they
could produce a new output when fed a new in-
crement, rather than needing to process the whole
prefix again), CRF++ is targeted at tagging appli-
cations, and expects full sequences. We hence test
in the same way as the SVMs from the previous
section, by computing a new tag sequence for each
prefix. Training again is done only on full utter-
ances / tag sequences.
We compare the CRF results against two base-
lines. The simplest consists of just always choos-
ing the most frequent tag, which is ?O? (for other,
marking material that does not contribute directly
to the relevant meaning of the utterance, such
as ?please? in ?I?d like to return on Monday,
please.?). The other baseline tags each word with
8http://crfpp.sourceforge.net/
2 4 6 8 10
0.5
0.6
0.7
0.8
0.9
1.0
percentiles into utterance
f?sc
ore
l l l l l l l l l l
l all utterancesshort utterancesnormal utteranceslong utterances
l l l l l l l l l
Figure 3: F-Score by Length of Prefix
ATIS Corr. Tag F-Score Frame F-Score
CRF 93.38 82.56 76.10
Maj 85.14 60.86 48.08
O 63.43 00.31 00.31
Pento Corr. Tag F-Score Frame F-Score
CRF 89.19 88.95 76.94
Maj 80.20 80.13 65.94
O 5.90 0.19 0.19
Table 3: Results of CRF models
its most frequent training data tag.
5.2 Results
We again begin by looking at the limiting case, the
results for full utterances (i.e., at the 100%mark).
Table 3 show three sets of results for each cor-
pus. Correctness looks at the proportion of tags
in a sequence that were correct. This measure is
driven up by correct recognition of the dummy
tag ?o?; as we can see, this is quite frequently
correct in ATIS, which drives up the ?always use
O?-baseline. Tag F-Score values the important
tags higher; we see here, though, that the majority
baseline (each word tagged with its most frequent
tag) is surprisingly good. It is solidly beaten for
the ATIS data, though. On the pento data, with
its much smaller tagset (5 as opposed to 95), this
baseline comes very high, but still the learner is
able to get some improvement. The last metric
evaluates reconstructed frames. It is stricter, be-
cause it offers less potential to be right (a sequence
of the same tag will be translated into one slot
value, turning several opportunities to be right into
15
only one).
The incremental dynamics looks quite different
here. Since the task is not one of prediction, we
do not expect to get better with more information;
rather, we start at an optimal point (when nothing
is said, nothing can be wrong), and hope that we
do not amass too many errors along the way. Fig-
ure 3 confirms this, showing that the classifier is
better able to keep the quality for the pento data
than for the ATIS data. Also, there is not much
variation depending on the length of the utterance.
6 Conclusions
We have shown how sequential and local statistical
models can be used for two variants of the incre-
mental NLU task: prediction, based on incomplete
information, and assignment of partial representa-
tions to partial input. We have shown that break-
ing up the prediction task by using an ensemble
of classifiers improves performance, and creates a
hybrid task that sits between prediction and incre-
mental interpretation.
While the objective quality as measured by our
metrics is quite good, what remains to be shown is
how such models can be integrated into a dialogue
system, and how what they offer can be turned into
improvements on interactivity. This is what we are
turning to next.
Acknowledgements Funded by ENP grant from DFG.
References
G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A.
Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus.
2006. Software architectures for incremental understand-
ing of human speech. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (IC-
SLP), Pittsburgh, PA, USA, September.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational systems.
In Proceedings of the conference on intelligent user inter-
faces, Santa Fe, USA, June.
Michaela Atterer and David Schlangen. 2009. RUBISC ?
a robust unification-based incremental semantic chunker.
In Proceedings of the 2nd International Workshop on Se-
mantic Representation of Spoken Language (SRSL 2009),
Athens, Greece, March.
Michaela Atterer, Timo Baumann, and David Schlangen.
2009. No sooner said than done? testing incrementality of
semantic interpretations of spontaneous speech. In Pro-
ceedings of Interspeech 2009, Brighton, UK, September.
Deborah A. Dahl, Madeleine Bates, Michael Brown, William
Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao,
Alexander Rudnicky, and Elizabeth Shriberg. 1994. Ex-
panding the scope of the atis task: the atis-3 corpus. In
Proceedings of the workshop on Human Language Tech-
nology, pages 43?48, Plainsboro, NJ, USA.
David DeVault, Kenji Sagae, and David Traum. 2009. Can
i finish? learning when to respond to incremental inter-
pretation results in interactive dialogue. In Proceedings
of the 10th Annual SIGDIAL Meeting on Discourse and
Dialogue (SIGDIAL?09), London, UK, September.
Raquel Ferna?ndez and David Schlangen. 2007. Referring
under restricted interactivity conditions. In Simon Keizer,
Harry Bunt, and Tim Paek, editors, Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue, pages
136?139, Antwerp, Belgium, September.
Yulan He and Steve Young. 2005. Semantic processing us-
ing the hidden vector state model. Computer Speech and
Language, 19(1):85?106.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. of ICML, pages 282?289.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thomson,
K. Yu, and S. Young. 2009. Spoken language understand-
ing from unaligned data using discriminative classification
models. In Proceedings of the 2009 IEEE International
Conference on Acoustics, Speech and Signal Processing,
Taipei, Taiwan, April.
Ivan Meza, Sebastian Riedel, and Oliver Lemon. 2008. Ac-
curate statistical spoken language understanding from lim-
ited development resources. In In Proceedings of ICASSP.
David Milward and Robin Cooper. 1994. Incremental in-
terpretation: Applications, theory, and relationships to dy-
namic semantics. In Proceedings of COLING 1994, pages
748?754, Kyoto, Japan, August.
Brian Roark. 2001. Robust Probabilistic Predictive Syntac-
tic Processing: Motivations, Models, and Applications.
Ph.D. thesis, Department of Cognitive and Linguistic Sci-
ences, Brown University.
Kenji Sagae, Gwen Christian, David DeVault, and David
Traum. 2009. Towards natural language understand-
ing of partial speech recognition results in dialogue sys-
tems. In Short paper proceedings of the North Ameri-
can chapter of the Association for Computational Linguis-
tics - Human Language Technologies conference (NAACL-
HLT?09), Boulder, Colorado, USA, June.
David Schlangen and Gabriel Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In Pro-
ceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics (EACL
2009), pages 710?718, Athens, Greece, March.
David Schlangen, Timo Baumann, and Michaela Atterer.
2009. Incremental reference resolution: The task, met-
rics for evaluation, and a bayesian filtering model that is
sensitive to disfluencies. In Proceedings of SIGdial 2009,
the 10th Annual SIGDIAL Meeting on Discourse and Di-
alogue, London, UK, September.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL 2009),
pages 745?753, Athens, Greece, March.
Andreas Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, USA, 2nd edition.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of EMNLP-CoNLL.
16
