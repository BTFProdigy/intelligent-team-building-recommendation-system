Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523?533,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning to Solve Arithmetic Word Problems with Verb Categorization
Mohammad Javad Hosseini
1
, Hannaneh Hajishirzi
1
, Oren Etzioni
2
, and Nate Kushman
3
1
{hosseini, hannaneh}@washington.edu,
2
OrenE@allenai.org,
3
nkushman@csail.mit.edu
1
University of Washington,
2
Allen Institute for AI,
3
Massachusetts Institute of Technology
Abstract
This paper presents a novel approach to
learning to solve simple arithmetic word
problems. Our system, ARIS, analyzes
each of the sentences in the problem state-
ment to identify the relevant variables and
their values. ARIS then maps this infor-
mation into an equation that represents
the problem, and enables its (trivial) so-
lution as shown in Figure 1. The pa-
per analyzes the arithmetic-word problems
?genre?, identifying seven categories of
verbs used in such problems. ARIS learns
to categorize verbs with 81.2% accuracy,
and is able to solve 77.7% of the problems
in a corpus of standard primary school test
questions. We report the first learning re-
sults on this task without reliance on pre-
defined templates and make our data pub-
licly available.
1
1 Introduction
Designing algorithms to automatically solve math
and science problems is a long-standing AI chal-
lenge (Feigenbaum and Feldman, 1963). For NLP,
mathematical word problems are particularly at-
tractive because the text is concise and relatively
straightforward, while the semantics reduces to
simple equations.
Arithmetic word problems begin by describing
a partial world state, followed by simple updates
or elaborations and end with a quantitative ques-
tion. For a child, the language understanding part
is trivial, but the reasoning may be challenging;
for our system, the opposite is true. ARIS needs to
1
Our data is available at https://www.cs.
washington.edu/nlp/arithmetic.
Arithmetic word Problem
Liz had 9 black kittens. She gave some of her kittens to
Joan. Joan now has 11 kittens. Liz has 5 kittens left and 3
have spots. How many kittens did Joan get?
State Transitions1	
Liz	

N: 9	

E: Kitten	

A: Black	

Liz gave some of her kittens to Joan.	

s2	
 Liz	

N: 9-L1	
E: Kitten	

A: Black	

Joan	

N:  J0+L1	
E: Kitten	

A: Black	

give	

Equation: 9? x = 5
Solution: x = 4 kittens
Figure 1: Example problem and solution.
make sense of multiple sentences, as shown in Fig-
ure 2, without a priori restrictions on the syntax or
vocabulary used to describe the problem. Figure
1 shows an example where ARIS is asked to infer
how many kittens Joan received based on facts and
constraints expressed in the text, and represented
by the state diagram and corresponding equation.
While the equation is trivial, the text could have
involved assembling toy aircraft, collecting coins,
eating cookies, or just about any activity involving
changes in the quantities of discrete objects.
This paper investigates the task of learning to
solve such problems by mapping the verbs in the
problem text into categories that describe their im-
pact on the world state. While the verbs category
is crucial (e.g., what happens if ?give? is replaced
by ?receive? in Figure 1?), some elements of the
problem are irrelevant. For instance, the fact that
three kittens have spots is immaterial to the solu-
tion. Thus, ARIS has to determine what informa-
tion is relevant to solving the problem.
To abstract from the problem text, ARIS maps
the text to a state representation which consists of
523
a set of entities, their containers, attributes, quan-
tities, and relations. A problem text is split into
fragments where each fragment corresponds to an
observation or an update of the quantity of an en-
tity in one or two containers. For example in Fig-
ure 1, the sentence ?Liz has 5 kittens left and 3
have spots? has two fragments of ?Liz has 5 kit-
tens left? and ?3 have spots?.
The verb in each sentence is associated with one
or two containers, and ARIS has to classify each
verb in a sentence into one of seven categories
that describe the impact of the verb on the con-
tainers (Table 1). ARIS learns this classifier based
on training data as described in section 4.2.
To evaluate ARIS, we compiled a corpus of
about 400 arithmetic (addition and subtraction)
word problems and utilized cross validation to
both train ARIS and evaluate its performance
over this corpus. We compare its performance
to the template-based learning method developed
independently and concurrently by Kushman et
al. (2014). We find that our approach is much
more robust to domain diversity between the train-
ing and test sets.
Our contributions are three-fold: (a) We present
ARIS, a novel, fully automated method that learns
to solve arithmetic word problems; (b) We intro-
duce a method to automatically categorize verbs
for sentences from simple, easy-to-obtain train-
ing data; our results refine verb senses in Word-
Net (Miller, 1995) for arithmetic word problems;
(c) We introduce a corpus of arithmetic word prob-
lems, and report on a series of experiments show-
ing high efficacy in solving addition and subtrac-
tion problems based on verb categorization.
2 Related Work
Understanding semantics of a natural language
text has been the focus of many researchers in nat-
ural language processing (NLP). Recent work fo-
cus on learning to align text with meaning repre-
sentations in specific, controlled domains. A few
methods (Zettlemoyer and Collins, 2005; Ge and
Mooney, 2006) use an expensive supervision in
the form of manually annotated formal representa-
tions for every sentence in the training data. More
recent work (Eisenstein et al., 2009; Kate and
Mooney, 2007; Goldwasser and Roth, 2011; Poon
and Domingos, 2009; Goldwasser et al., 2011;
Kushman and Barzilay, 2013) reduce the amount
of required supervision in mapping sentences to
meaning representations while taking advantage
of special properties of the domains. Our method,
on the other hand, requires small, easy-to-obtain
training data in the form of verb categories that
are shared among many different problem types.
Our work is also closely related to the grounded
language acquisition research (Snyder and Barzi-
lay, 2007; Branavan et al., 2009; Branavan et al.,
2012; Vogel and Jurafsky, 2010; Chen et al., 2010;
Hajishirzi et al., 2011; Chambers and Jurafsky,
2009; Liang et al., 2009; Bordes et al., 2010)
where the goal is to align a text into underlying en-
tities and events of an environment. These meth-
ods interact with an environment to obtain super-
vision from the real events and entities in the envi-
ronment. Our method, on the other hand, grounds
the problem into world state transitions by learn-
ing to predict verb categories in sentences. In addi-
tion, our method combines the representations of
individual sentences into a coherent whole to form
the equations. This is in contrast with the previous
work that study each sentence in isolation from the
other sentences.
Previous work on studying math word and logic
problems uses manually aligned meaning repre-
sentations or domain knowledge where the seman-
tics for all the words is provided (Lev, 2007; Lev
et al., 2004). Most recently, Kushman et al. (2014)
introduced an algorithm that learns to align al-
gebra problems to equations through the use of
templates. This method applies to broad range of
math problems, including multiplication, division,
and simultaneous equations, while ARIS only han-
dles arithmetic problems (addition and subtrac-
tion). However, our empirical results show that
for the problems it handles, ARIS is much more
robust to diversity in the problem types between
the training and test data.
3 Arithmetic Problem Representation
We address solving arithmetic word problems that
include addition and subtraction. A problem text
is split into fragments where each fragment is rep-
resented as a transition between two world states
in which the quantities of entities are updated or
observed (Figure 2). We refer to these fragments
as sentences. We represent the world state as a tu-
ple ?E,C,R? consisting of entities E, containers
C, and relations R among entities, containers, at-
tributes, and quantities.
Entities: An entity is a mention in the text corre-
524
N: W0-13 	
E: tree	
A: walnut	

Liz had 9 black kittens. She gave some of her kittens to Joan. Joan has now 11 kittens. Liz has 5 kitten left and 3 has spots. How many kittens did Joan get?	

Liz had 9 	
black kittens	

s0	

s1	
Liz	

N: 9	
E: Kitten	
A: Black	
 She gave some of her kittens to Joan	

s2	
 Liz	

N: 9-L1	
E: Kitten	
A: Black	

Joan	

N:  J0+L1	
E: Kitten	
A: Black	
	

Joan has now 11 kittens	

 Liz has 5 kitten left	
 And 3 has spots	

Liz	

N: 9-L1	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	

s3	
 Liz	

N: 5	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	

s4	

Liz	

N: 5	
E: Kitten	
A: Black	

Joan	

N: 11	
E: Kitten	
A: Black	
unknown	

N:3	
E: Kitten	

s5	

There are 42 walnut trees and 12 orange trees currently in the park. Park workers cut down 13 walnut trees that were damaged. How many walnut trees will be in the park when the workers are finished?	

There are 42 walnut trees and 12 orange trees currently in the park. 	

s0	

s1	
Park	

N: 42	
E: tree	
A: walnut	
 Park workers cut down 13 walnut trees that were damaged	

N: 12	
E: tree	
A: orange	

s2	
 Park	
 N: 42-13 	
E: tree	
A: walnut	

N: 12	
E: tree	
A: orange	

Workers	

Figure 2: A figure sketching different steps of our method ? a sequence of states.
sponding to an object whose quantity is observed
or is changing throughout the problem. For in-
stance, kitten and tree are entities in Fig-
ure 2. In addition, every entity has attributes that
modify the entity. For instance, black is an at-
tribute of kittens, and walnut is an attribute
of tree (more details on attributes in section 4.1).
Relations describing attributes are invariant to the
state changes. For instance kittens stay black
throughout the problem of Figure 1.
Containers: A container is a mention in the
text representing a set of entities. For instance,
Liz, Joan, park, and workers are containers
in Figure 2. Containers usually correspond to the
person possessing entities or a location contain-
ing entities. For example, in the sentence ?There
are 43 blue marbles in the basket. John found 32
marbles.?, basket and John are containers of
marbles.
Quantities: Containers include entities with their
corresponding quantities in a particular world
state. Quantities can be known numbers (e.g. 9),
unknown variables (e.g. L
1
), or numerical expres-
sions over unknown quantities and numbers (e.g.
9?L
1
). For instance, in state 2 of Figure 2, the nu-
merical expression corresponding to Liz is 9?L
1
and corresponding to Joan is J
0
+ L
1
, where J
0
is a variable representing the number of kittens
that Joan has started with.
Hereinafter, we will refer to a generic entity as
e, container as c, number as num, attribute as a.
We represent the relation between a container, an
entity, and a number in the form of a quantity ex-
Category Example
Observation There were 28 bales of hay in the barn.
Positive Joan went to 4 football games this year.
Negative John lost 3 of the violet balloons.
Positive
Transfer
Mike?s dad borrowed 7 nickels from
Mike.
Negative
Transfer
Jason placed 131 erasers in the drawer.
Construct Karen added 1/4 of a cup of walnuts to a
batch of trail mix.
Destroy The rabbits ate 4 of Dan?s potatoes.
Table 1: Examples for different verb categories in sen-
tences. Entities are underlined; containers are italic, and
verbs are bolded.
pression N(c,e). Figure 2 shows the quantity
relations in different world states.
State transitions: Sentences depict progression
of the world state (Figure 2) in the form of ob-
servations of updates of quantities. We assume
that every sentence w consists of a verb v, an en-
tity e, a quantity num (might be unknown), one
or two containers c
1
, c
2
, and attributes a. The
presence of the second container, c
2
, will be dic-
tated by the category of the verb, as we discuss
below. Sentences abstract transitions (s
t
? s
t+1
)
between states in the form of an algebraic opera-
tion of addition or subtraction. For every sentence,
we model the state transition according to the verb
category and containers in the sentence. There are
three verb categories for sentences with one con-
tainer: Observation: the quantity is initialized in
the container, Positive: the quantity is increased
in the container, and Negative: the quantity is de-
creased in the container. Moreover, there are four
categories for sentences with two containers: Pos-
525
itive transfer: the quantity is transferred from the
second container to the first one, Negative trans-
fer: the quantity is transferred from the first con-
tainer to the second one, Construct: the quantity
is increased for both containers, and Destroy: the
quantity is decreased for both containers.
Figure 2 shows how the state transitions are
determined by the verb categories. The sen-
tence ?Liz has 9 black kittens? initializes the
quantity of kittens in the container Liz
to 9. In addition, the sentence ?She gave
some of her kittens to Joan.? shows the
negative transfer of L
1
kittens from Liz to
Joan represented as N(Liz,kitten)=9-L
1
and N(Joan,kitten)=J
0
+ L
1
.
Given a math word problem, ARIS grounds the
world state into entities (e.g., kitten), contain-
ers (e.g., Liz), attributes (e.g., black), and quan-
tities (e.g., 9) (Section 4.1). In addition, ARIS
learns state transitions by classifying verb cate-
gories in sentences (Section 4.2). Finally, from the
world state and transitions, it generates an arith-
metic equation which can be solved to generate the
numeric answer to the word problem.
4 Our Method
In this section we describe how ARIS maps an
arithmetic word problem into an equation (Fig-
ure 2). ARIS consists of three main steps (Fig-
ure 3): (1) grounding the problem into entities and
containers, (2) training a model to classify verb
categories in sentences, and (3) solving the prob-
lem by updating the world states with the learned
verb categories and forming equations.
4.1 Grounding into Entities and Containers
ARIS automatically identifies entities, attributes,
containers, and quantities corresponding to every
sentence fragment (details in Figure 3 step 1). For
every problem, this module returns a sequence of
sentence fragments ?w
1
, . . . , w
T
, w
x
?where every
w
t
consists of a verb v
t
, an entity e
t
, its quantity
num
t
, its attributes a
t
, and up to two containers
c
t
1
, c
t
2
. w
x
corresponds to the question sentence
inquiring about an unknown entity. ARIS applies
the Stanford dependency parser, named entity rec-
ognizer and coreference resolution system to the
problem text (de Marneffe et al., 2006; Finkel et
al., 2005; Raghunathan et al., 2010). It uses the
predicted coreference relationships to replace pro-
nouns (including possessive pronouns) with their
coreferenent links. The named entity recognition
output is used to identify numbers and people.
Entities: Entities are references to some object
whose quantity is observed or changing through-
out the problem. So to determine the set of
entities, we define h as the set of noun types
which have a dependent number (in the depen-
dency parse) somewhere in the problem text. The
set of entities is then defined as all noun phrases
which are headed by a noun type in h. For in-
stance kitten in the first sentence of Figure 1
is an entity because it is modified by the number
9, while kitten in the second sentence of Fig-
ure 1 is an entity because kitten was modified
by a number in the first sentence. Every number
in the text is associated with one entity. Num-
bers which are dependents of a noun are associ-
ated with its entity. Bare numbers (not dependent
on a noun) are associated with the previous entity
in the text. The entity in the last sentence is identi-
fied as the question entity e
x
. Finally, ARIS splits
the problem text into T + 1 sentence fragments
?w
1
, . . . w
T
, w
x
? such that each fragment contains
a single entity and it?s containers. For simplicity
we refer to these fragments as a sentences.
Containers: Each entity is associated with one
or two container noun phrases using the algorithm
described in in Figure 3 step 1c. As we saw earlier
with numbers, arithmetic problems often include
sentences with missing information. For example
in Figure 2, the second container in the the sen-
tence ?Park workers had to cut down 13 walnut
trees that were damaged.? is not explicitly men-
tioned. To handle this missing information, we
use the circumscription assumption (McCarthy,
1980). The circumscription assumption formal-
izes the commonsense assumption that things are
as expected unless otherwise specified. In this set-
ting, we assume that the set of containers are fixed
in a problem. Thus if the container(s) for a given
entity cannot be identified they are set to the con-
tainer(s) for the previous entity with the same head
word. For example in Figure 2 we know from the
previous sentence that trees were in the park.
Therefore, we assume that the unmentioned con-
tainer is the park.
Attributes: ARIS selects attributes A as modifiers
for every entity from the dependency parser (de-
tails in Figure 3 step 1a). For example black is
an attribute of the entity kitten and is an ad-
jective modifier in the parser. These attributes are
526
1. Grounding into entities and containers: for every problem p in dataset (Section 4.1)
(a) ?e
1
, . . . , e
T
, e
x
?
p
? extract all entities and the question entity
i. Extract all numbers and noun phrases (NP).
ii. h ? all noun types which appear with a number as a dependant (in the dependency parse tree) somewhere
in the problem text.
iii. e
t
? all NPs which are headed by a noun type in h.
iv. num
t
? the dependant number of e
t
if one exists. Bare numbers (not directly dependant on any noun
phrase) are associated with the previous entity in the text. All other num
t
are set to unknown.
v. e
x
? the last identified entity.
vi. a
t
? adjective and noun modifiers of e
t
. Update implicit attributes using the previously observed attributes.
vii. v
t
? the verb with the shortest path to e
t
in the dependency parse tree.
(b) ?w
1
, . . . , w
T
, w
x
?
p
? split the problem text into fragments based on the entities and verbs
(c) ?c
t
1
, c
t
2
, . . . , c
T
1
, c
T
2
, c
x
?
p
? the list of containers for each entity
i. c
t
1
? the subject of w
t
.
If w
t
contains There is/are, c
t
1
is the first adverb of place to the verb.
ii. c
t
2
? An NP that is direct object of the verb. If not found, c
t
2
is the object of the first adverbial phrase of
the verb.
iii. Circumscription assumption: When c
t
1
or c
t
2
are not found, they are set to the previous containers.
2. Training for sentence categorization (Section 4.2)
(a) instances
1
, instances
2
? ?
(b) for every sentence w
t
? ?w
1
, . . . , w
T
, w
x
?
p
in the training set:
i. features
t
? extract features (similarity based, WordNet based, structural) (Section 4.2.1)
ii. l
t
1
, l
t
2
? determine labels for containers c
t
1
and c
t
2
based on the verb category of w
t
.
iii. append ?features
t
, l
t,1
?, ?features
t
, l
t,2
? to instances
1
, instances
2
.
(c) M
1
,M
2
? train two SVMs for instances
1
, instances
2
3. Solving: for every problem p in the test set (Section 4.3)
(a) Identifying verb categories in sentences
i. for every sentence w
t
? ?w
1
, . . . , w
T
, w
x
?
p
:
A. features
t
? extract features (similarity based, WordNet based, structural).
B. l
t
1
, l
t
2
? classify w
t
for both containers c
t
1
and c
t
2
using models M
1
,M
2
.
(b) State progression: Form ?s
0
, . . . , s
T
? (Section 4.3.1)
i. s
0
? null.
ii. for t ? ?1, . . . , T ?: s
t
? progress(s
t?1
, w
t
).
A. if e
t
= e
x
and a
t
= a
x
:
if w
t
is an observation: N
t
(c
t
1
, e
t
) = num
t
.
else: update N
t
(c
t
1
, e
t
) and N
t
(c
t
2
, e
t
) given verb categories l
t
1
, l
t
2
.
B. copy N
t?1
(c, e) to N
t
(c, e) for all other (c, e) pairs.
(c) Forming equations and solution (Section 4.3.2)
i. Mark each w
t
that matches with w
x
if:
a) c
t
1
matches with c
x
and verb categories are equal or verbs are similar.
b) c
t
2
matches with c
x
and the verbs are in opposite categories.
ii. x? the unknown quantity if w
x
matches with a sentence introducing an unknown number
iii. If the question asks about an unknown variable x or a start variable (w
x
contains ?begin? or ?start?):
For some container c, find two states s
t
(quantity expression contains x) and s
t+1
(quantity is a known
number). Then, form an equation for x: N
t
(c, e
x
) = N
t+1
(c, e
x
).
iv. else: form equation as x = N
t
(c
x
, e
x
).
v. Solve the equation and return the absolute value of x.
Figure 3: ARIS: a method for solving arithmetic word problems.
used to prune the irrelevant information in pro-
gressing world states.
Arithmetic problems usually include sentences
with no attributes for the entities. For example,
the attribute black has not been explicitly men-
tioned for the kitten in the second sentence. In
particular, ARIS updates an implicit attribute using
the previously observed attribute. For example, in
?Joan went to 4 football games this year. She went
to 9 games last year.?, ARIS assigns football as
an attribute of the game in both sentences.
4.2 Training for Verb Categories
This step involves training a model to identify verb
categories for sentences. This entails predicting
one label (increasing, decreasing) for each (verb,
container) pair in the sentence. Each possible set-
ting of these binary labels corresponds to one of
the seven verb categories discussed earlier. For ex-
ample, if c
1
is increasing and c
2
is decreasing this
is a positive transfer verb.
Our dataset includes word problems from dif-
ferent domains (more details in Section 5.2). Each
verb in our dataset is labeled with one of the 7 cat-
527
egories from Table 1.
For training, we compile a list of sentences from
all the problems in the dataset and split sentences
into training and test sets in two settings. In the
first setting no instance from the same domain
appears in the training and test sets in order to
study the robustness of our method to new prob-
lem types. In the second setting no verb is re-
peated in the training and test sets in order to study
how well our method predicts categories of unseen
verbs.
For every sentence w
t
in the problems, we build
two data instances, (w
t
, c
1
) and (w
t
, c
2
), where c
1
and c
2
are containers extracted from the sentence.
For every instance in the training data, we assign
training labels using the verb categories of the sen-
tences instead of labeling every sentence individu-
ally. The verb can be increasing or decreasing cor-
responding to every container in the sentence. For
positive (negative) and construction (destruction)
verbs, both instances are labeled positive (nega-
tive). For transfer positive (negative) verbs, the
first instance is labeled positive (negative) and the
second instance is labeled negative (positive). For
observation verbs, both instances are labeled pos-
itive. We assume that the observation verbs are
known (total of 5 verbs). Finally, we train Support
Vector Machines given the extracted features and
training labels explained above (Figure 3 step 2).
In the following, we describe the features used for
training.
4.2.1 Features
There are three sets of features: similarity based,
Wordnet-based, and structural features. The first
two sets of features focus on the verb and the third
set focuses on the dependency structure of the sen-
tence. All of our features are unlexicalized. This
allows ARIS to handle verbs in the test questions
which are completely different from those seen in
the training data.
Similarity-based Features: For every instance
(w, c), the feature vector includes similarity be-
tween the verb of the sentence w and a list of seed
verbs. The list of seed verbs is automatically se-
lected from a set V containing the 2000 most com-
mon English verbs using `
1
regularized feature se-
lection technique. We select a small set of seed
verbs to avoid dominating the other feature types
(structural and WordNet-based features).
The goal is to automatically select verbs from
V that are most discriminative for each of the 7
verb categories in Table 1. We define 7 classifi-
cation tasks: ?Is a verb a member of each cate-
gory?? Then, we select the three most represen-
tative verbs for each category. To do so, we ran-
domly select a set of 65 verbs V
l
, from all the verbs
in our dataset (118 in total) and manually anno-
tate the verb categories. For every classification
task, the feature vector X includes the similarity
scores (Equation 1) between the verb v and all the
verbs in the V . We train an `
1
regularized regres-
sion model (Park and Hastie, 2007) over the fea-
ture vector X to learn each category individually.
The number of original (similarity based) features
in X is relatively large, but `
1
regularization pro-
vides a sparse weight vector. ARIS then selects the
three most common verbs (without replacement)
among the features (verbs) with non-zero weights.
This accounts for 21 total seed verbs to be used for
the main classification task. We find that in prac-
tice using this selection technique leads to better
performance than using either all the verbs in V or
using just the 65 randomly selected verbs.
Our method computes the similarity between
two verbs v
1
and v
2
from the similarity between all
the senses (from WordNet) of these verbs (Equa-
tion 1). We compute the similarity between two
senses using linear similarity (Lin, 1998). The
similarity between two synsets sv
1
and sv
2
are pe-
nalized according to the order of each sense for the
corresponding verb. Intuitively, if a synset appears
earlier in the set of synsets of a verb, it is more
likely to be considered as the correct meaning.
Therefore, later occurrences of a synset should re-
sult in reduced similarity scores. The similarity
between two verbs v
1
and v
2
is the maximum sim-
ilarity between two synsets of the verbs:
sim(v
1
, v
2
) = max
sv:synsets(v)
lin-sim(sv
1
, sv
2
)
log(p
1
+ p
2
)
(1)
where sv
1
, sv
2
are two synsets, p
1
, p
2
are the posi-
tion of each synset match, and lin-sim is the linear
similarity. Our experiments show better perfor-
mance using linear similarity compared to other
common similarity metrics (e.g., WordNet path
similarity and Resnik similarity (Resnik, 1995)).
WordNet-based Features: We use WordNet
verb categories in the feature vector. For each
part of speech in WordNet, the synsets are or-
ganized into different categories. There are
15 categories for verbs. Some examples in-
528
clude ?verb.communication?, ?verb.possession?,
and ?verb.creation?. In addition, WordNet in-
cludes the frequency measure f
c
sv
indicating how
often the sense sv has appeared in a reference cor-
pus. For each category i, we define the feature f
i
as the ratio of the frequency of the sense sv
i
over
the total frequency of the verb i.e., f
i
= f
c
sv
i
/f
c
v
.
Structural Features: For structural features, we
use the dependency relations between the verb and
the sentence elements since they can be a good
proxy of the sentence structure. ARIS uses a bi-
nary vector including 35 dependency relations be-
tween the verb and other elements. For example,
in the sentence ?Joan picked 2 apples from the ap-
ple tree?, the dependency between (?picked? and
?tree?) and (?picked? and ?apples?) are depicted as
?prep-from? and ?dobj? relations in the dependency
parser, respectively. In addition, we include the
length of the path in the dependency parse from
the entity to the verb.
4.3 Solving the Problem
So far, ARIS grounds every problem into entities,
containers, and attributes, and learns verb cate-
gories in sentences. Solving the problem consists
of two main steps: (1) progressing states based on
verb categories in sentences and (2) forming the
equation.
4.3.1 State Progression with Verb Categories
This step (Figure 3 step 3b) involves forming
states ?s
1
, . . . , s
T
? by updating quantities in every
container using learned verb categories (Figure 3
step 3a). ARIS initializes s
0
to an empty state. It
then iteratively updates the state s
t
by progressing
the state s
t?1
given the sentence w
t
with the verb
v, entity e, number num, and containers c
1
and c
2
.
For a given sentence t, ARIS attempts to match
e
t
and c
t
to entities and categories in s
t?1
. An
entity/category is matched if has the same head
word and same set of attributes as an existing en-
tity/category. If an entity or category cannot be
matching to one in s
t?1
, then a new one is created
in s
t
.
The progress subroutine prunes the irrelevant
sentences by checking if the entity e and its at-
tributes a agree with the question entity e
x
and its
attributes a
x
in the question. For example both
game entities agree with the question entity in the
problem ?Joan went to 4 football games this year.
She went to 9 games last year. How many football
games did Joan go??. The first entity has an ex-
plicit football attribute, and the second entity
has been assigned the same attribute (Section 4.1).
Even if the question asks about games without
mentioning football, the two sentences will
match the question. Note that the second sentence
would have not been matched if there was an ex-
plicit mention of the ?basketball game? in the sec-
ond sentence.
For the matched entities, ARIS initializes or up-
dates the values of the containers c
1
, c
2
in the state
s
t
. ARIS uses the learned verb categories in sen-
tences (Section 4.2) to update the values of con-
tainers. For an observation sentence w
t
, the value
of c
1
in the state s
t
is assigned to the observed
quantity num. For other sentence types, if the
container c does not match to a container the pre-
vious state, its value is initialized with a start vari-
able C
0
. For example, the container Joan is ini-
tialized with J
0
at the state s
1
(Figure 2). Other-
wise, the values of c
1
and c
2
are updated according
to the verb category in the sentence. For instance,
if the verb category in the sentence is a positive
transfer then N
t
(c
1
, e) = N
t?1
(c
1
, e)? num and
N
t
(c
2
, e) = N
t?1
(c
2
, e) + num where N
t
(c, e)
represents the quantity of e in the container c at
state s
t
(Figure 2).
4.3.2 Forming Equations and Solution
The question entity e
x
can match either to an en-
tity in the final state, or to some unknown gener-
ated during the state progression. Concretely, the
question sentence w
x
asks about the quantity x of
the entity e
x
in a container c
x
at a particular state
s
u
or a transition after the sentence w
u
(Figure 3
step 3c).
To determine if e
x
matches to an unknown vari-
able, we define a matching subroutine between
the question sentence w
x
and every sentence w
t
to check entities, containers, and verbs (Figure 3
step 3(c)i). We consider two cases. 1) When
w
x
contains the words ?begin?, or ?start?, the un-
known variable is about the initial value of an en-
tity, and it is set to the start variable of the con-
tainer c
x
(Figure 3 step 3(c)iii). For example, in
?Bob had balloons. He gave 9 to his friends. He
now has 4 balloons. How many balloons did he
have to start with??, the unknown variable is set to
the start variable B
0
. 2) When the question verb
is not one of the defined set of observation verbs,
ARIS attempts to match e
x
with an unknown in-
troduced by one of the state transitions (Figure 3
529
step 3(c)iii). For example, the second sentence
in Figure 1 introduces an unknown variable over
kittens. The matching subroutine matches this
entity with the question entity since the question
container, i.e. Joan, matches with the second
container and verb categories are complementary.
In order to solve for the unknown variable x,
ARIS searches through consecutive states s
t
and
s
t+1
, where in s
t
, the quantity of e
x
for a container
c is an expression over x, and in s
t+1
, the quan-
tity is a known number for a container matched
to c. It then forms an equation by comparing the
quantities for containers matched between the two
states. In the previous example, the equation will
be B
0
? 9 = 4 by comparing states s
2
and s
3
,
where the numerical expression over balloons
is B
0
?9 in the state s
2
, and the quantity is a known
number in the state s
3
.
When neither of the two above cases apply,
ARIS matches e
x
to an entity in the final state,
s
T
and returns its quantity, (Figure 3 step 3(c)iv).
In the football example of the previous sec-
tion, the equation will be x = N
t
(c
x
, e
x
), where
N
t
(c
x
, e
x
) is the quantity in the final state.
Finally, the equation will be solved for the un-
known variable x and the absolute value of the un-
known variable is returned.
5 Experiments
To experimentally evaluate our method we build
a dataset of arithmetic word problems along with
their correct solutions. We test our method on the
accuracy of solving arithmetic word problems and
identifying verb categories in sentences.
5.1 Experimental Setup
Datasets: We compiled three diverse datasets
MA1, MA2, IXL (Table 2) of Arithmetic word
problems on addition and subtraction for third,
fourth, and fifth graders. These datasets have sim-
ilar problem types, but have different characteris-
tics. Problem types include combinations of ad-
ditions, subtractions, one unknown equations, and
U.S. money word problems. Problems in MA2 in-
clude more irrelevant information compared to the
other two datasets, and IXL includes more infor-
mation gaps. In total, they include 395 problems,
13,632 words, 118 verbs, and 1,483 sentences.
Tasks and Baselines: We evaluate ARIS on two
tasks: 1) solving arithmetic word problems in the
three datasets and 2) classifying verb categories in
Source #Tests Avg.# Sentences
MA1 math-aids.com 134 3.5
IXL ixl.com 140 3.36
MA2 math-aids.com 121 4.48
Table 2: Properties of the datasets.
MA1 IXL MA2 Total
3-fold Cross validation
ARIS 83.6 75.0 74.4 77.7
ARIS
2
83.9 75.4
+
69.8
+
76.5
+
KAZB 89.6 51.1 51.2 64.0
Majority 45.5 71.4 23.7 48.9
Gold sentence categorization
Gold ARIS 94.0 77.1 81.0 84.0
Table 3: Accuracy of solving arithmetic word problems in
three datasets MA1, IXL, and MA2. This table compares
our method, ARIS, ARIS
2
with the state-of-the-art KAZB. All
methods are trained on two (out of three) datasets and tested
on the other one. ARIS
2
is trained when no verb is repeated
in the training and test sets. Gold ARIS uses gold verb cat-
egories. The improvement of ARIS (boldfaced) and ARIS
2
(denoted by
+
) are significant over KAZB and the majority
baseline with p < 0.05.
sentences. We use the percentage of correct an-
swers to the problems as the evaluation metric for
the first task and accuracy as the evaluation metric
for the second task. We use Weka?s SVM (Wit-
ten et al., 1999) with default parameters for clas-
sification which is trained with verb categories in
sentences (as described in Section 4.2).
For the first task, we compare ARIS with
KAZB (Kushman et al., 2014), majority baseline,
ARIS
2
, and Gold ARIS. KAZB requires training
data in the form of equation systems and numeri-
cal answers to the problems. The majority base-
line classifies every instance as increasing. In
ARIS
2
(a variant of ARIS) the system is trained in
a way that no verb is repeated in the training and
test sets. Gold ARIS uses the ground-truth sen-
tence categories instead of predicted ones. For the
second task, we compare ARIS with a baseline that
uses WordNet verb senses.
5.2 Results
We evaluate ARIS in solving arithmetic word
problems in the three datasets and then evaluate its
ability in classifying verb categories in sentences.
5.2.1 Solving Arithmetic Problems
Table 3 shows the accuracy of ARIS in solv-
ing problems in each dataset (when trained on
the other two datasets).Table 3 shows that ARIS
530
significantly outperforms KAZB and the major-
ity baseline. As expected, ARIS shows a larger
gain on the two more complex datasets MA2 and
IXL; our method shows promising results in deal-
ing with irrelevant information (dataset MA2) and
information gaps (dataset IXL). This is because
ARIS learns to classify verb categories in sen-
tences and does not require observing similar pat-
terns/templates in the training data. Therefore,
ARIS is more robust to differences between the
training and test datasets and can generalize across
different dataset types. As discussed in the ex-
perimental setup, the datasets have mathematically
similar problems, but differ in the natural language
properties such as in the sentence length and irrel-
evant information (Table 2).
Table 3 also shows that the sentence categoriza-
tion is performed with high accuracy even if the
problem types and also the verbs are different. In
particular, there are a total of 118 verbs among
which 64 verbs belong to MA datasets and 54 are
new to IXL. To further study this, we train our
method ARIS
2
in which no verb can be repeated
in the training and test sets. ARIS
2
still signifi-
cantly outperforms KAZB. In addition, we observe
only a slight change in accuracy between ARIS
and ARIS
2
.
To further understand our method, we study the
effect of verb categorization in sentences in solv-
ing problems. Table 3 shows the results of Gold
ARIS in solving arithmetic word problems with
gold sentence categorizations. In addition, com-
paring ARIS with Gold ARIS suggests that our
method is able to reliably identify verb categories
in sentences.
We also perform an experiment where we pool
all of the problems in the three datasets and
randomly choose 3 folds for the data (instead
of putting each original dataset into it?s own
fold). We compare our method with KAZBin
this scenario. In this setting, our method?s accu-
racy is 79.5% while KAZB?s accuracy is 81.8%.
As expected, our method?s performance has not
changed significantly from the previous setting,
while KAZB?s performance significantly improves
because of the reduced diversity between the train-
ing and test sets in this scenario.
5.2.2 Sentence Categorization
Table 4 compares accuracy scores of sentence
categorization for our method with different fea-
tures, a baseline that uses WordNet verb senses,
and the majority baseline that assigns every (verb,
container) pair as increasing. Similar to ARIS
2
,
we randomly split verbs into three equal folds
and assign the corresponding sentences to each
fold. No verb is shared between training and test
sets. We then directly evaluate the accuracy of
the SVM?s verb categorization (explained in Sec-
tion 4.2). This table shows that ARIS performs
well in classifying sentence categories even with
new verbs in the test set. This suggests that our
method can generalize well to predict verb cate-
gories for unseen verbs.
Table 4 also details the performance of four
variants of our method that ablate various features
of ARIS. The table shows that similarity, contex-
tual, and WordNet features are all important to
the performance of ARIS in verb categorization,
whereas the WordNet features are less important
for solving the problems. In addition, it shows that
similarity features play more important roles. We
also performed another experiment to study the ef-
fect of the proposed feature selection method for
similarity-based features. The accuracy of ARIS
in classifying sentence categories is 69.7% when
we use all the verbs in V in the similarity feature
vector. This shows that our feature selection algo-
rithm for selecting seed verbs is important towards
categorizing verbs.
Finally, Table 4 shows that our method signif-
icantly outperforms the baseline that only uses
WordNet verb sense. An interesting observation
is that the majority baseline in fact outperforms
WordNet verb senses in verb categorization, but
is significantly worse in solving arithmetic word
problems. In addition, we evaluate the accuracy
of predicting only verb categories by assigning the
verb label according to the majority of its labels
in the sentence categories. The accuracy of verb
categories is 78.2% confirming that ARIS is able
to successfully categorize verbs.
5.2.3 Error Analysis
We analyzed all 63 errors of Gold ARIS and
present our findings in Table 5. There are five ma-
jor classes of errors. In the first category, some in-
formation is not mentioned explicitly and should
be entailed. For example, ?washing cars? is the
source of ?making money?. Despite the improve-
ments that come from ARIS, a large portion of the
errors can still be attributed to irrelevant informa-
tion. For example, ?short? is not a ?toy?. The third
category refers to errors that require knowledge
531
Categorization Solution
ARIS 81.2
+
76.5
+
No similarity features 68.8 65.4
No WordNet features 75.3 78.0
+
No structural features 75.5 72.4
+
Baseline (WordNet) 67.8 68.4
Majority Baseline 73.4 48.9
Table 4: Ablation study and baseline comparisons: this ta-
ble reports the accuracy of verb categorization in sentences
and solutions for ARIS with ablating features. It also pro-
vides comparisons to WordNet and majority baselines. The
improvement of ARIS (boldfaced) and ablations denoted by
+
are statistically significant over the baselines (with p < 0.05)
for both tasks.
Error type Example
Entailment,
Implicit
Action (26%)
Last week Tom had $74. He washed cars
over the weekend and now has $86. How
much money did he make washing cars?
Irrelevant
Information
(19%)
Tom bought a skateboard for $9.46, and
spent $9.56 on marbles. Tom also spent
$14.50 on shorts. In total, how much did
Tom spend on toys?
Set Comple-
tion (13%)
Sara?s school played 12 games this year.
They won 4 games. How many games did
they lose?
Parsing
Issues (21%)
Sally had 27 Pokemon cards. Dan gave
her 41 new Pokemon cards. How many
Pokemon cards does Sally have now?
Others (21%) In March it rained 0.81 inches. It rained
0.35 inches less in April than in March.
How much did it rain in April?
Table 5: Examples of different error categories and relative
frequencies. The cause of error is bolded.
about set completions. For example, the ?played?
games can be split into ?win? and ?lost? games.
Finally, parsing and coreference mistakes are an-
other source of errors for ARIS.
6 Discussions and Conclusion
In this paper we introduce ARIS, a method for
solving arithmetic word problems. ARIS learns
to predict verb categories in sentences using syn-
tactic and (shallow) semantic features from small,
easy-to-obtain training data. ARIS grounds the
world state into entities, sets, quantities, attributes,
and their relations and takes advantage of the cir-
cumscription assumption and successfully fills in
the information gaps. Finally, ARIS makes use
of attributes and discards irrelevant information in
the problems. Together these provide a new rep-
resentation and a learning algorithm for solving
arithmetic word problems.
This paper is one step toward building a sys-
tem that can solve any math and logic word
problem. Our empirical evaluations show that
our method outperforms a template-based learn-
ing method (developed recently by Kushman et al.
(2014)) on solving addition and subtraction prob-
lems with diversity between the training and test
sets. In particular, our method generalizes bet-
ter to data from different domains because ARIS
only relies on learning verb categories which al-
leviates the need for equation templates for arith-
metic problems. In this paper, we have focused
on addition and subtraction problems. However,
KAZB can deal with more general types of prob-
lems such as multiplication, division, and simulta-
neous equations.
We have observed a complementary behavior
between our method and that of Kushman et al.
This suggests a hybrid approach that can bene-
fit from the strengths of both methods while be-
ing applicable to more general problems while ro-
bust to the errors specific to each. In addition, we
plan to focus on incrementally collecting domain
knowledge to deal with missing information gaps.
Another possible direction is to improve parsing
and coreference resolution.
Acknowledgments
The research was supported by the Allen Institute
for AI, and grants from the NSF (IIS-1352249)
and UW-RRF (65-2775). We thank Ben Hixon
and the anonymous reviewers for helpful com-
ments and the feedback on the work.
References
Antoine Bordes, Nicolas Usunier, and Jason Weston. 2010.
Label ranking under ambiguous supervision for learning
semantic correspondences. In Proc. International Confer-
ence on Machine Learning (ICML).
SRK Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina
Barzilay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proc. of the Annual Meeting of the
Association for Computational Linguistics and the Inter-
national Joint Conference on Natural Language Process-
ing of the AFNLP (ACL-AFNLP).
SRK Branavan, Nate Kushman, Tao Lei, and Regina Barzi-
lay. 2012. Learning high-level planning from text. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
532
David Chen, Joohyun Kim, and Raymond Mooney. 2010.
Training a multilingual sportscaster: Using perceptual
context to learn language. Journal of Artificial Intelli-
gence Research, 37.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proc. Language
Resources and Evaluation Conference (LREC).
Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan
Roth. 2009. Reading to learn: Constructing features
from semantic abstracts. In Proc. Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
Edward A. Feigenbaum and Julian Feldman, editors. 1963.
Computers and Thought. McGraw Hill, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proc. of the Annual
Meeting of the Association for Computational Linguistics
(ACL).
Dan Goldwasser and Dan Roth. 2011. Learning from natural
instructions. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth.
2011. Confidence driven unsupervised semantic parsing.
In Proc. of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Hannaneh Hajishirzi, Julia Hockenmaier, Erik T. Mueller,
and Eyal Amir. 2011. Reasoning about robocup soccer
narratives. In Proc. Conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervision. In
Proc. Conference of the Association for the Advancement
of Artificial Intelligence (AAAI).
Nate Kushman and Regina Barzilay. 2013. Using seman-
tic unification to generate regular expressions from natu-
ral language. In Proceeding of the Annual Meeting of the
North American Chapter of the Association for Computa-
tional Linguistics.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina
Barzilay. 2014. Learning to automatically solve algebra
word problems. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Iddo Lev, Bill MacCartney, Christopher D. Manning, , and
Roger Levy. 2004. Solving logic puzzles: From robust
processing to precise semantics. In Workshop on Text
Meaning and Interpretation at Association for Computa-
tional Linguistics (ACL).
Iddo Lev. 2007. Packed Computation of Exact Meaning Rep-
resentations. Ph.D. thesis, CS, Stanford University.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learn-
ing semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint Con-
ference on Natural Language Processing of the AFNLP
(ACL-AFNLP).
Dekang Lin. 1998. An information-theoretic definition of
similarity. In Proc. International Conference on Machine
Learning (ICML).
John McCarthy. 1980. Circumscription?a form of non-
monotonic reasoning. Artificial Intelligence, 13.
George A Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38.
Mee Young Park and Trevor Hastie. 2007. L1-regularization
path algorithm for generalized linear models. Journal of
the Royal Statistical Society: Series B (Statistical Method-
ology), 69.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised se-
mantic parsing. In Proc. Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangara-
jan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky,
and Christopher Manning. 2010. A multi-pass sieve for
coreference resolution. In Proc. Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International joint
conference on Artificial intelligence (IJCAI).
Benjamin Snyder and Regina Barzilay. 2007. Database-text
alignment via structured multilabel classification. In Pro-
ceedings of International Joint Conference on Artificial
Intelligence (IJCAI).
Adam Vogel and Daniel Jurafsky. 2010. Learning to follow
navigational directions. In Proc. of the Annual Meeting of
the Association for Computational Linguistics (ACL).
Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall, Ge-
offrey Holmes, and Sally Jo Cunningham. 1999. Weka:
Practical machine learning tools and techniques with java
implementations.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. Confer-
ence on Uncertainty in Artificial Intelligence (UAI).
533
Proceedings of NAACL-HLT 2013, pages 826?836,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Semantic Unification to Generate
Regular Expressions from Natural Language
Nate Kushman Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{nkushman, regina}@csail.mit.edu
Abstract
We consider the problem of translating natu-
ral language text queries into regular expres-
sions which represent their meaning. The mis-
match in the level of abstraction between the
natural language representation and the regu-
lar expression representation make this a novel
and challenging problem. However, a given
regular expression can be written in many se-
mantically equivalent forms, and we exploit
this flexibility to facilitate translation by find-
ing a form which more directly corresponds to
the natural language. We evaluate our tech-
nique on a set of natural language queries
and their associated regular expressions which
we gathered from Amazon Mechanical Turk.
Our model substantially outperforms a state-
of-the-art semantic parsing baseline, yielding
a 29% absolute improvement in accuracy.1
1 Introduction
Regular expressions (regexps) have proven them-
selves to be an extremely powerful and versatile for-
malism that has made its way into everything from
spreadsheets to databases. However, despite their
usefulness and wide availability, they are still con-
sidered a dark art that even many programmers do
not fully understand (Friedl, 2006). Thus, the ability
to automatically generate regular expressions from
natural language would be useful in many contexts.
Our goal is to learn to generate regexps from nat-
ural language, using a training set of natural lan-
guage and regular expression pairs such as the one
in Figure 1. We do not assume that the data includes
an alignment between fragments of the natural lan-
guage and fragments of the regular expression. In-
1The dataset used in this work is available at
http://groups.csail.mit.edu/rbg/code/regexp/
Text Description Regular Expression
three letter word starting with ?X? \bX[A-Za-z]{2}\b
Figure 1: An example text description and its associated
regular expression.3
ducing such an alignment during learning is partic-
ularly challenging because oftentimes even humans
are unable to perform a fragment-by-fragment align-
ment.
We can think of this task as an instance of
grounded semantic parsing, similar to the work
done in the domain of database queries (Kate and
Mooney, 2006; Zettlemoyer and Collins, 2005;
Kwiatkowski et al, 2010). However, the current
success in semantic parsing relies on two impor-
tant properties of the data. First, while the past
work did not assume the alignment was given, they
did assume that finding a fine grained fragment-
by-fragment alignment was possible. Secondly,
the semantic domains considered in the past were
strongly typed. This typing provides constraints
which significantly reduce the space of possible
parses, thereby greatly reducing the ambiguity.
However, in many interesting domains these two
properties may not hold. In our domain, the align-
ment between the natural language and the regu-
lar expressions often happens at the level of the
whole phrase, making fragment-by-fragment align-
ment impossible. For example, in Figure 1 no frag-
ment of the regexp maps clearly to the phrase ?three
letter?. Instead, the regexp explicitly represents the
fact that there is only two characters after X, which is
not stated explicitly by the text description and must
be inferred. Furthermore, regular expressions have
3Our regular expression syntax supports Perl regular expres-
sion shorthand which utilizes \b to represent a break (i.e. a
space or the start or end of the line). Our regular expression
syntax also supports intersection (&) and complement(?).
826
([A-Za-z]{3})&(\b[A-Za-z]+\b)&(X.*)
(a)
three letter [A-Za-z]{3}
word \b[A-Za-z]+\b
starting with ?X? X.*
(b)
Figure 2: (a) shows a regexp which is semantically
equivalent to that in Figure 1, yet admits a fragment-by-
fragment mapping to the natural language. (b) shows this
mapping.
relatively few type constraints.
The key idea of our work is to utilize semantic
unification in the logical domain to disambiguate the
meaning of the natural language. Semantic unifi-
cation utilizes an inference engine to determine the
semantic equality of two syntactically divergent ex-
pressions. This is a departure from past work on se-
mantic parsing which has largely focused on the syn-
tactic interface between the natural language and the
logical form, and on example-based semantic equal-
ity, neither of which utilize the inference power in-
herent in many symbolic domains.
To see how we can take advantage of semantic
unification, consider the regular expression in Fig-
ure 2(a). This regular expression is semantically
equivalent to the regular expression in Figure 1. Fur-
thermore, it admits a fragment-by-fragment map-
ping as can be seen in Figure 2(b). In contrast, as
we noted earlier, the regexp in Figure 1 does not ad-
mit such a mapping. In fact, learning can be quite
difficult if our training data contains only the regexp
in Figure 1. We can, nonetheless, use the regexp in
Figure 2 as a stepping-stone for learning if we can
use semantic inference to determine the equivalence
between the two regular expressions. More gener-
ally, whenever the regexp in the training data does
not factorize in a way that facilitates a direct map-
ping to the natural language description, we must
find a regexp which does factorize and be able to
compute its equivalence to the regexp we see in the
training data. We compute this equivalence by con-
verting each regexp to a minimal deterministic finite
automaton (DFA) and leveraging the fact that mini-
mal DFAs are guaranteed to be the same for seman-
tically equivalent regexps (Hopcroft et al, 1979).
We handle the additional ambiguity stemming
from the weak typing in our domain through the use
of a more effective parsing algorithm. The state of
the art semantic parsers (Kwiatkowski et al, 2011;
Liang et al, 2011) utilize a pruned chart parsing
algorithm which fails to represent many of the top
parses and is prohibitively slow in the face of weak
typing. In contrast, we use an n-best parser which
always represents the most likely parses, and can be
made very efficient through the use of the parsing
algorithm from Jimenez and Marzal (2000).
Our approach works by inducing a combinatory
categorial grammar (CCG) (Steedman, 2001). This
grammar consists of a lexicon which pairs words
or phrases with regular expression functions. The
learning process initializes the lexicon by pairing
each sentence in the training data with the full reg-
ular expression associated with it. These lexical en-
tries are iteratively refined by considering all possi-
ble ways to split the regular expression and all pos-
sible ways to split the phrase. At each iteration we
find the n-best parses with the current lexicon, and
find the subset of these parses which are correct us-
ing DFA equivalence. We update the weights of a
log-linear model based on these parses and the cal-
culated DFA equivalence.
We evaluate our technique using a dataset of sen-
tence/regular expression pairs which we generated
using Amazon Mechanical Turk (Turk, 2013). We
find that our model generates the correct regexp
for 66% of sentences, while the state-of-the-art se-
mantic parsing technique from Kwiatkowski et al
(2010) generates correct regexps for only 37% of
sentences. The results confirm our hypothesis that
leveraging the inference capabilities of the seman-
tic domain can help disambiguate natural language
meaning.
2 Related Work
Generating Regular Expressions Past work has
looked at generating regular expressions from nat-
ural language using rule based techniques (Ranta,
1998), and also at automatically generating regular
expressions from examples (Angluin, 1987). To the
best of our knowledge, however, our work is the first
to use training data to learn to automatically gener-
ate regular expressions from natural language.
Language Grounding There is a large body of re-
search mapping natural language to some form of
meaning representation (Kate and Mooney, 2006;
Kate et al, 2005; Raymond and Mooney, 2006;
Thompson and Mooney, 2003; Wong and Mooney,
827
2006; Wong and Mooney, 2007; Zelle and Mooney,
1996; Branavan et al, 2009; Mihalcea et al, 2006;
Poon and Domingos, 2009). In some of the consid-
ered domains the issue of semantic equivalence does
not arise because of the way the data is generated.
The most directly related work in these domains, is
that by Kwiatkowski et al (2010 and 2011) which is
an extension of earlier work on CCG-based semantic
parsing by Zettlemoyer and Collins (2005). Similar
to our work, Kwiatkowski et al utilize unification to
find possible ways to decompose the logical form.
However, they perform only syntactic unification.
Syntactic unification determines equality using only
variable substitutions and does not take advantage of
the inference capabilities available in many semantic
domains. Thus, syntactic unification is unable to de-
termine the equivalence of two logical expressions
which use different lexical items, such as ?.*? and
?.*.*?. In contrast, our DFA based technique can
determine the equivalence of such expressions. It
does this by leveraging the equational inference ca-
pabilities of the regular expression domain, making
it a form of semantic unification. Thus, the contribu-
tion of our work is to show that using semantic uni-
fication to find a deeper level of equivalence helps to
disambiguate language meanings.
In many other domains of interest, determining
semantic equivalence is important to the learning
process. Previous work on such domains has fo-
cused on either heuristic or example-driven mea-
sures of semantic equivalence. For example, Artzi
and Zettlemoyer (2011) estimate semantic equiva-
lence using a heuristic loss function. Other past
work has executed the logical form on an example
world or in a situated context and then compared the
outputs. This provides a very weak form of semantic
equivalence valid only in that world/context (Clarke
et al, 2010; Liang et al, 2009; Liang et al, 2011;
Chen and Mooney, 2011; Artzi and Zettlemoyer,
2013). In contrast, our work uses an exact, theoret-
ically sound measure of semantic equivalence that
determines whether two logical representations are
equivalent in any context, i.e. on any input string.
3 Background
3.1 Finding Regexp Equivalence Using DFAs
Regular expressions can be equivalently represented
as minimal DFAs, which are guaranteed to be equal
function sig. regexp function signature regexp
cons(R,R,...) ab rep*(R) a*
and(R,R,...) [a-b]&[b-c] repminmax(I,I,R) a{3,5}
or(R,R,...) a|b repmin(I,R) a{3,}
not(R) ?(a) repexact(I,R) a{3}
Figure 3: This shows the signatures of all functions in our
lambda calculus along with their regexp syntax.
for the same regular language (Hopcroft et al,
1979). The DFA representation of a regular expres-
sion may be exponentially larger than the the orig-
inal regular expression. However, past work has
shown that most regular expressions do not exhibit
this exponential behavior (Tabakov and Vardi, 2005;
Moreira and Reis, 2012), and the conversion pro-
cess is renowned for its good performance in prac-
tice (Moreira and Reis, 2012). Hence, we compare
the equivalence of two regular expressions by con-
verting them to minimal DFAs and comparing the
DFAs. We do this using a modified version of M?ller
(2010).4
3.2 Lambda Calculus Representation
To take advantage of the inherent structure of reg-
ular expressions, we deterministically convert them
from a flat string representation into simply typed
lambda calculus expressions. The full set of func-
tions available in our lambda calculus can be seen
in Figure 3. As can be seen from the figures, our
lambda calculus is very weakly typed. It has only
two primitive types, integer (I) and regexp (R), with
most arguments being of type R.
3.3 Parsing
Our parsing model is based on a Combinatory Cate-
gorial Grammar. In CCG parsing most of the gram-
mar complexity is contained in the lexicon, ?, while
the parser itself contains only a few simple rewrite
rules called combinators.
Lexicon The lexicon, ?, consists of a set of lexical
entries that couple natural language with a lambda
calculus expression. Our lexical entries contain
words or phrases, each of which is associated with
a function from the lambda calculus we described
in ?3.2. For example:
4We set a timeout on this process to catch any cases where
the resulting DFA might be prohibitively large. We use a one
second timeout in our experiments, which results in timeouts
on less than 0.25% of the regular expressions.
828
with ?bob? after ?joe??????? ?? ??????? ??
R/R R R\R/R R
?x(.*x.*) bob ?xy.(x.*y) joe
?????????(f)
R\R
?y.joe.*y
????????????(b)
R
joe.*bob
????????????????????(f)
R
.*joe.*bob.*
Figure 4: This shows an example parse.
? after, R\R/R:?xy.(x.*y) ?
? at least, R/I/R:?xy.((x){y,}) ?
Note that the lambda expressions contain type infor-
mation indicating the number of arguments and the
type of those arguments as described in ?3.2. How-
ever, this information is augmented with a (/) or a
(\) for each argument indicating whether that argu-
ment comes from the left or the right, in sentence
order. Thus R\R/R can be read as a function which
first takes an argument of type R on the right then
takes another argument of type R on the left, and
returns an expression of type R.
Combinators Parses are built by combining lexical
entries through the use of a set of combinators. Our
parser uses only the two most basic combinators,
forward function application and backward function
application.5 These combinators work as follows:
R/R:f R:g ? R:f(g) (forward)
R:f R\R:g ? R:g(f) (backward)
The forward combinator applies a function to an ar-
gument on its right when the type of the argument
matches the type of the function?s first argument.
The backward combinator works analogously. Fig-
ure 4 shows an example parse.
4 Parsing Model
For a given lexicon, ?, and sentence, ~w, there will in
general be many valid parse trees, t ? T (~w; ?). We
assign probabilities to these parses using a standard
log-linear parsing model with parameters ?:
p(t|~w; ?,?) =
e???(t, ~w)
?
t? e
???(t?, ~w)
Our training data, however, includes only the cor-
rect regular expression, r, and not the correct parse,
5Technically, this choice of combinators makes our model
just a Categorial Grammar instead of a CCG.
t. The training objective used by the past work in
such circumstances, is to maximize the probability
of the correct regular expression by marginalizing
over all parses which generate that exact regular ex-
pression. Such an objective is limited, however, be-
cause it does not allow parses that generate seman-
tically correct regexps which are not syntactically
equivalent to r, such as those in Figure 2. The main
departure of our work is to use an objective which al-
lows such parses through the use of the DFA-EQUAL
procedure. DFA-EQUAL uses the process described
in ?3.1 to determine whether parse t evaluates to a
regexp which is semantically equivalent to r, lead-
ing to the following objective:
O =
?
i
log
?
t|DFA-EQUAL(t,ri)
p(t|~wi; ?,?) (1)
At testing time, for efficiency reasons, we calcu-
late only the top parse. Specifically, if r = eval(t)
is the regexp which results from evaluating parse t,
then we generate t? = arg maxt?T (~w) p(t|~w; ?,?),
and return r? = eval(t?).
5 Learning
Our learning algorithm starts by generating a single
lexical entry for each training sample which pairs
the full sentence, ~wi, with the associated regular ex-
pression, ri. Formally, we initialize the lexicon as
? = {?~wi, R : ri? |i = 1 . . . n}. We then run an iter-
ative process where in each iteration we update both
? and ? for each training sample. Our initial ? will
perfectly parse the training data. However it won?t
generalize at all to the test data since the lexical en-
tries contain only full sentences. Hence, in each
iteration we refine the lexicon by splitting existing
lexical entries to generate more granular lexical en-
tries which will generalize better. The candidates for
splitting are all lexical entries used by parses which
generate the correct regular expression, ri, for the
current training sample. We consider all possible
ways to factorize each lexical entry, and we add to
? a new lexical entry for each possible factorization,
as discussed in ?5.2. Finally, we update ? by per-
forming a single stochastic gradient ascent update
step for each training sample, as discussed in ?5.1.
See Algorithm 1 for details.
This learning approach follows the structure
of the previous work on CCG based seman-
tic parsers (Zettlemoyer and Collins, 2005;
829
Inputs: Training set of sentence regular expression pairs.
{?~wi, ri? |i = 1 . . . n}
Functions:
? N-BEST(~w; ?,?) n-best parse trees for ~w using the
algorithm from ?5.1
? DFA-EQUAL(t, r) calculates the equality of the regexp
from parse t and regexp r using the algorithm from ?3.1
? SPLIT-LEX(T ) splits all lexical entries used by any
parse tree in set T , using the process described in ?5.2
Initialization: ? = {?~wi, R : ri? |i = 1 . . . n}
For k = 1 . . .K, i = 1 . . . n
Update Lexicon: ?
? T = N-BEST(~wi; ?,?)
? C = {t|t ? T ? DFA-EQUAL(t, ri)}
? ? = ? ? SPLIT-LEX(C)
Update Parameters: ?
? T = N-BEST(~wi; ?,?)
? C = {t|t ? T ? DFA-EQUAL(t, ri)}
? ? = Ep(t|t?C)[?(t, ~w)]? Ep(t|t?T )[?(t, ~w)]
? ? = ? + ??
Output: The lexicon and the parameters, ??, ??
Algorithm 1: The full learning algorithm.
Kwiatkowski et al, 2010). However, our domain
has distinct properties that led to three important
departures from this past work.
First, we use the DFA based semantic unifica-
tion process described in ?3.1 to determine the set
of correct parses when performing parameter up-
dates. This is in contrast to the syntactic unification
technique, used by Kwiatkowski et al (2010), and
the example based unification used by other seman-
tic parsers, e.g. Artzi and Zettlemoyer (2011). Us-
ing semantic unification allows us to handle training
data which does not admit a fragment-by-fragment
mapping between the natural language and the reg-
ular expression, such as the example in Figure 2.
Second, our parser is based on the efficient n-best
parsing algorithm of Jimenez and Marzal (2000) in-
stead of the pruned chart parsing algorithm used
by the past work (Zettlemoyer and Collins, 2005;
Kwiatkowski et al, 2010). As we show in ?8.2, this
results in a parser which more effectively represents
the most likely parses. This allows our parser to bet-
ter handle the large number of potential parses that
exist in our domain due to the weak typing.
Third, we consider splitting lexical entries used in
any correct parse, while the past work (Zettlemoyer
and Collins, 2005; Kwiatkowski et al, 2010) con-
siders splitting only those used in the best parse. We
must utilize a less constrictive splitting policy since
our domain does not admit the feature weight ini-
tialization technique used in the domains of the past
work. We discuss this in ?5.2.1. In the remainder
of this section we discuss the process for learning ?
and for generating the lexicon, ?.
5.1 Estimating Theta
To estimate ? we will use stochastic gradient ascent,
updating the parameters based on one training exam-
ple at a time. Hence, we can differentiate the objec-
tive from equation 1 to get the gradient of parameter
?j for training example i, as follows:
?Oi
??j
=Ep(t|DFA-EQUAL(t,ri),?) [?j(t, ~wi)]? Ep(t|?) [?j(t, ~wi)]
(2)
This gives us the standard log-linear gradient, which
requires calculating expected feature counts. We de-
fine the features in our model over individual parse
productions, admitting the use of dynamic program-
ming to efficiently calculate the unconditioned ex-
pected counts. However, when we condition on gen-
erating the correct regular expression, as in the first
term in (2), the calculation no longer factorizes, ren-
dering exact algorithms computationally infeasible.
To handle this, we use an approximate gradient
calculation based on the n-best parses. Our n-best
parser uses an efficient algorithm developed orig-
inally by (Jimenez and Marzal, 2000), and subse-
quently improved by (Huang and Chiang, 2005).
This algorithm utilizes the fact that the first best
parse, t1, makes the optimal choice at each deci-
sion point, and the 2nd best parse, t2 must make the
same optimal choice at every decision point, except
for one. To execute on this intuition, the algorithm
first calculates t1 by generating an unpruned CKY-
style parse forest which includes a priority queue
of possible subparses for each constituent. The set
of possible 2nd best parses T are those that choose
the 2nd best subparse for exactly one constituent of
t1 but are otherwise identical to t1. The algorithm
chooses t2 = arg maxt?T p(t). More generally,
T is maintained as a priority queue of possible nth
best parses. At each iteration, i, the algorithm sets
ti = arg maxt?T p(t) and augments T by all parses
which both differ from ti at exactly one constituent
ci and choose the next best possible subparse for ci.
We use the n-best parses to calculate an approxi-
mate version of the gradient. Specifically, Ti is the
830
set of n-best parses for training sample i, and Ci in-
cludes all parses t in Ti such that DFA-EQUAL(t, ri).
We calculate the approximate gradient as:
? = Ep(t|t?Ci;?,?)[?(t, ~wi)]? Ep(t|t?Ti;?,?)[?(t, ~wi)]
(3)
In contrast to our n-best technique, the past
work has calculated equation (2) using a beam
search approximation of the full inside-outside algo-
rithm (Zettlemoyer and Collins, 2005; Kwiatkowski
et al, 2010; Liang et al, 2011). Specifically, since
the conditional probability of t given r does not fac-
torize, a standard chart parser would need to main-
tain the full logical form (i.e. regular expression)
for each subparse, and there may be an exponential
number of such subparses at each chart cell. Thus,
they approximate this full computation using beam
search, maintaining only the m-best logical forms at
each chart cell.
Qualitatively, our n-best approximation always
represents the most likely parses in the approxima-
tion, but the number of represented parses scales
only linearly with n. In contrast, the number of
parses represented by the beam search algorithm
of the past work can potentially scale exponentially
with the beam size,m, due to its use of dynamic pro-
gramming. However, since the beam search prunes
myopically at each chart cell, it often prunes out
the highest probability parses. In fact, we find that
the single most likely parse is pruned out almost
20% of the time. Furthermore, our results in ?8
show that the beam search?s inability to represent
the likely parses significantly impacts the overall
performance. It is also important to note that the
runtime of the n-best algorithm scales much better.
Specifically, as n increases, the n-best runtime in-
creases as O(n|~w| log(|~w||P | + n), where P is the
set of possible parse productions. In contrast, as
m is increased, the beam search runtime scales as
O(|~w|5m2), where the |~w|5 factor comes from our
use of headwords, as discussed in ?6. In practice,
we find that even with n set to 10, 000 and m set to
200, our algorithm still runs almost 20 times faster.
5.2 Lexical Entry Splitting
Each lexical entry consists of a sequence of n
words aligned to a typed regular expression func-
tion, ?w0:l, T : r?. Our splitting algorithm considers
all possible ways to split a lexical entry into two new
cons Parent Tree Child Treeb .rep*o b.rep* cons .rep*x.rep* b o bconsOriginal Tree
(a) (b) (c)
Figure 5: The tree in (a) represents the lambda expression
from the lexical entry ?with bob, R:.*bob.*?. One pos-
sible split of this lexical entry generates the parent lexical
entry ?with, R/R:?x.(.*x.*)? and the child lexical en-
try, ?bob, R:bob?, whose lambda expressions are repre-
sented by (b) and (c), respectively.
lexical entries such that they can be recombined via
function application to obtain the original lexical en-
try. This process is analogous to the syntactic unifi-
cation process done by Kwiatkowski et al (2010).
We first consider all possible ways to split the
lambda expression r. The splitting process is most
easily explained using a tree representation for r, as
shown in Figure 5(a). This tree format is simply a
convenient visual representation of a lambda calcu-
lus function, with each node representing one of the
function type constants from Figure 3. Each split,
s ? S(r), generates a child expression sc and a par-
ent expression sp such that r = sp(sc). For each
node, n, in r besides the root node, we generate a
split where sc is the subtree rooted at node n. For
such splits, sp is the lambda expression r with the
sub-expression sc replaced with a bound variable,
say x. In addition to these simple splits, we also con-
sider a set of more complicated splits at each node
whose associated function type constant can take
any number of arguments, i.e. or, and, or cons. If
C(n) are the children of node n, then we generate a
split for each possible subset, {V |V ? C(n)}. Note
that for cons nodes V must be contiguous. In ?6 we
discuss additional restrictions placed on the splitting
process to avoid generating an exponential number
of splits. For the split with subset V , the child tree,
sc, is a version of the tree rooted at node n pruned
to contain only the children in V . Additionally, the
parent tree, sp, is generated from r by replacing all
the children in V with a single bound variable, say
x. Figure 5 shows an example of such a split. We
only consider splits in which sc does not have any
bound variables, so its type, Tc, is always either R
or I . The type of sp is then type of the original ex-
pression, T augmented by an additional argument of
the child type, i.e. either T/Tc or T\Tc.
Each split s generates two pairs of lexical entries,
831
one for forward application, and one for backward
application. The set of such pairs of pairs is:
{( ?w0:j , T/Tc : sp? , ?wj:l, Tc : sc?),
(?w0:j , Tc : sc? , ?wj:l, T\Tc : sp?)|
(0 ? j ? l) ? (s ? S(r))}
5.2.1 Adding New Lexical Entries
Our model splits all lexical entries used in parses
which generate correct regular expressions, i.e.
those in Ci, and adds all of the generated lexical
entries to ?. In contrast, the previous work (Zettle-
moyer and Collins, 2005; Kwiatkowski et al, 2010)
has a very conservative process for adding new lex-
ical entries. This process relies on a good initial-
ization of the feature weights associated with a new
lexical entry. They perform this initialization using
a Giza++ alignment of the words in the training sen-
tences with the names of functions in the associated
lambda calculus expression. Such an initialization
is ineffective in our domain since it has very few
primitive functions and most of the training exam-
ples use more than half of these functions. Instead,
we add new lexical entries more aggressively, and
rely on the n-best parser to effectively ignore any
lexicon entries which do not generate high probabil-
ity parses.
6 Applying the Model
Features To allow inclusion of head words in our
features, our chart cells are indexed by start word,
end word, and head word. Thus for each parse pro-
duction we have a set of features that combine the
head word and CCG type, of the two children and
the newly generated parent. Additionally, for each
lexical entry ?~wi, R : ri? ? ?, we have four types of
features: (1) a feature for ?~wi, R : ri?, (2) a feature
for ~wi, (3) a feature for R : ri, and (4) a set of fea-
tures indicating whether ~wi contains a string literal
and whether the leaves of ri contain any exact char-
acter matches (rather than character range matches).
Initialization In addition to the sentence level ini-
tialization discussed in ?5 we also initialize the lex-
icon, ?, with two other sets of lexical entries. The
first set is all of the quoted string literals in the natu-
ral language phrases from the training set. Thus for
the phrase, ?lines with ?bob? twice? we would add
the lexical entry ? ?bob?, R:bob ?. We also add lex-
ical entries for both numeric and word representa-
tions of numbers, such as ? 1, R:1? and ? one, R:1?.
We add these last two types of lexical entries be-
cause learning them from the data is almost impos-
sible due to data sparsity. Lastly, for every individual
word in our training set vocabulary, we add an iden-
tity lexical entry whose lambda expression is just a
function which takes one argument and returns that
argument. This allows our parser to learn to skip
semantically unimportant words in the natural lan-
guage description, and ensures that it generates at
least one parse for every example in the dataset. At
test time we also add both identity lexical entries for
every word in the test set vocabulary as well as lex-
ical entries for every quoted string literal seen in the
test queries. Note that the addition of these lexical
entries requires only access to the test queries and
does not make use of the regular expressions (i.e.
labels) in the test data in any way.
Parameters We initialize the weight of all lexical
entry features except the identity features to a default
value of 1 and initialize all other features to a default
weight of 0. We regularize our log-linear model us-
ing the L2-norm and a ? value of 0.001. We use a
learning rate of ? = 1.0, set n = 10, 000 in our n-
best parser, and run each experiment with 5 random
restarts and K = 50 iterations. We report results
using the pocket alorithm technique originated by
Gallant (1990).
Constraints on Lexical Entry Splitting To prevent
the generation of an exponential number of splits,
we constrain the lexical entry splitting process as
follows:? We only consider splits at nodes which are at most
a depth of 2 from the root of the original tree.
? We limit lambda expressions to 2 arguments.
? In unordered node splits (and and or) the result-
ing child can contain at most 4 of the arguments.
These restrictions ensure the number of splits is
at most an M-degree polynomial of the regexp size.
The unification process used by Kwiatowski et al
(2010) bounded the number of splits similarly.
7 Experimental Setup
Dataset Our dataset consists of 824 natural language
and regular expression pairs gathered using Amazon
Mechanical Turk (Turk, 2013) and oDesk (oDesk,
2013).6 On Mechanical Turk we asked workers to
6This is similar to the size of the datasets used by past work.
832
generate their own original natural language queries
to capture a subset of the lines in a file (similar to
UNIX grep). In order to compare to example based
techniques we also ask the Mechanical Turk work-
ers to generate 5 positive and 5 negative examples
for each query. On oDesk we hired a set of pro-
grammers to generate regular expressions for each
of these natural language queries. We split our data
into 3 sets of 275 queries each and tested using 3-
fold cross validation. We tuned our parameters sep-
arately on each development set but ended up with
the same values in each case.
Evaluation Metrics We evaluate by comparing the
generated regular expression for each sentence with
the correct regular expression using our DFA equiv-
alence technique. As discussed in ?3.1 this met-
ric is exact, indicating whether the generated regu-
lar expression is semantically equivalent to the cor-
rect regular expression. Additionally, as discussed
in ?6, our identity lexical entries ensure we generate
a valid parse for every sentence, so we report only
accuracy instead of precision and recall.
Baselines We compared against six different base-
lines. The UBL baseline uses the published code
from Kwiatkowski et al (2010) after configuring
it to handle the lambda calculus format of our reg-
ular expressions.7 The other baselines are ablated
and/or modified versions of our model. The Beam-
Parse baselines replace the N-BEST procedure from
Algorithm 1 with the beam search algorithm used
for parsing by past CCG parsers (Zettlemoyer and
Collins, 2005; Kwiatkowski et al, 2010).8 The
StringUnify baseline replaces the DFA-EQUAL proce-
dure from Algorithm 1 with exact regular expres-
sion string equality. The HeuristicUnify baselines
strengthen this by replacing DFA-EQUAL with a smart
heuristic form of semantic unification. Our heuristic
unification procedure first flattens the regexp trees
by merging all children into the parent node if they
are both of the same type and of type or, and, or
cons. It then sorts all children of the and and or
operators. Finally, it converts both regexps back to
a flat string and compares these strings for equiva-
lence. This process should more effective than any
7This was done in consultation with the original authors.
8we set the beam size to 200, which is equivalent to the past
work. With this setting, the slow runtime of this algorithm al-
lowed us to run only two random restarts.
Model Percent Correct
UBL 36.5%
BeamParse-HeuristicUnify 9.4%
BeamParse-HeuristicUnify-TopParse 22.1%
NBestParse-StringUnify 31.1%
NBestParse-ExampleUnify 52.3%
NBestParse-HeuristicUnify 56.8%
Our Full Model 65.5%
Table 1: Accuracy of our model and the baselines.
form of syntactic unification and any simpler heuris-
tics. The ExampleUnify baseline represents the per-
formance of the example based semantic unification
techniques. It replaces DFA-EQUAL with a procedure
that evaluates the regexp on all the positive and neg-
ative examples associated with the given query and
returns true if all 10 are correctly classified. Finally,
BeamParse-HeuristicUnify-TopParse uses the same
algorithm as that for BeamParse-HeuristicUnify ex-
cept that it only generates lexical entries from the
top parse instead of all parses. This more closely
resembles the conservative lexical entry splitting al-
gorithm used by Kwiatkowski et al
8 Results
Our model outperforms all of the baselines, as
shown in Table 1. The first three baselines ?
UBL, BeamParse-HeuristicUnify, and BeamParse-
HeuristicUnify-TopParse? represent the algorithm
used by Kwiatkowski et al Our model outperforms
the best of these by over 30% in absolute terms and
180% in relative terms.
The improvement in performance of our model
over the NBestParse-StringUnify, NBestParse-
ExampleUnify and NBestParse-HeuristicUnify
baselines highlights the importance of our DFA
based semantic unification technique. Specifi-
cally, our model outperforms exact string based
unification by over 30%, example based semantic
unification by over 13% and our smart heuristic
unification procedure by 9%. These improvements
confirm that leveraging exact semantic unification
during the learning process helps to disambiguate
language meanings.
8.1 Effect of Additional Training Data
Table 2 shows the change in performance as we in-
crease the amount of training data. We see that our
model provides particularly large gains when there
833
%age of Data 15% 30% 50% 75%
NBestParse-
HeuristicUnify
12.4% 26.4% 39.0% 45.4%
Our Model 29.0% 50.3% 58.7% 65.2%
Relative Gain 2.34x 1.91x 1.51x 1.43x
Table 2: Results for varying amounts of training data.
020
4060
80100
1 2000 4000 6000 8000 10000%
Rep
rese
nted
# of Top Parses
1st Iter10th Iter
Figure 6: This graph compares the set of parses repre-
sented by the n-best algorithm used in our model to the
set of parses represented by the beam search algorithm
used by the past work. Note that our n-best algorithm
represents 100% of the top 10000 parses.
is a small amount of training data. These gains de-
crease as the amount of training data increases be-
cause the additional data allows the baseline to learn
new lexical entries for every special case. This re-
duces the need for the fine grained lexicon decom-
position which is enabled by our DFA based unifica-
tion. For example, our DFA based model will learn
separate lexical entries for ?line?, ?word?, ?starting
with?, and ?ending with?. The baseline instead will
just learn separate lexical entries for every possible
combination such as ?line starting with?, ?word end-
ing with?, etc. Our model?s ability to decompose,
however, allows it to provide equivalent accuracy to
even the best baseline with less than half the amount
of training data. Furthermore, we would expect this
gain to be even larger for domains with more com-
plex mappings and a larger number of different com-
binations.
8.2 Beam Search vs. N-Best
A critical step in the training process is calculating
the expected feature counts over all parses that gen-
erate the correct regular expression. In ?4 we dis-
cussed the trade-off between approximating this cal-
culation using the n-best parses, as our model does,
verses the beam search model used by the past work.
The effect of this trade-off can be seen clearly in Fig-
ure 6. The n-best parser always represents the n-best
parses, which is set to 10,000 in our experiments. In
contrast, on the first iteration, the beam search algo-
rithm fails to represent the top parse almost 20% of
the time and represents less than 15% of the 10,000
most likely parses. Even after 10 iterations it still
only represents 70% of the top parses and fails to
represent the top parse almost 10% of the time. This
difference in representation ability is what provides
the more than 30% difference in accuracy between
the BeamParse-HeuristicUnify version of our model
and the NBestParse-HeuristicUnify version of our
model.
9 Conclusions and Future Work
In this paper, we present a technique for learning
a probabilistic CCG which can parse a natural lan-
guage text search into the regular expression that
performs that search. The key idea behind our ap-
proach is to use a DFA based form of semantic uni-
fication to disambiguate the meaning of the natural
language descriptions. Experiments on a dataset of
natural language regular expression pairs show that
our model significantly outperforms baselines based
on a state-of-the-art model.
We performed our work on the domain of reg-
ular expressions, for which semantic unification is
tractable. In more general domains, semantic uni-
fication is undecidable. Nevertheless, we believe
our work motivates the use of semantic inference
techniques for language grounding in more general
domains, potentially through the use of some form
of approximation or by restricting those domains in
some way. For example, SAT and SMT solvers have
seen significant success in performing semantic in-
ference for program induction and hardware veri-
fication despite the computational intractability of
these problems in the general case.
10 Acknowledgments
The authors acknowledge the support of Battelle
Memorial Institute (PO#300662) and NSF (grant
IIS-0835652). We thank Luke Zettlemoyer, Tom
Kwiatkowski, Yoav Artzi, Mirella Lapata, the MIT
NLP group, and the ACL reviewers for their sugges-
tions and comments. Any opinions, findings, con-
clusions, or recommendations expressed in this pa-
per are those of the authors, and do not necessarily
reflect the views of the funding organizations.
834
References
Dana Angluin. 1987. Learning regular sets from queries
and counterexamples. Information and computation,
75(2):87?106.
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping
semantic parsers from conversations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 421?432. Association for
Computational Linguistics.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
ACL, pages 82?90.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), pages 859?865.
J. Clarke, D. Goldwasser, M.W. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
18?27. Association for Computational Linguistics.
Jeffrey Friedl. 2006. Mastering Regular Expressions.
OReilly.
Steven I Gallant. 1990. Perceptron-based learning al-
gorithms. Neural Networks, IEEE Transactions on,
1(2):179?191.
J.E. Hopcroft, R. Motwani, and J.D. Ullman. 1979. In-
troduction to automata theory, languages, and compu-
tation, volume 2. Addison-wesley Reading, MA.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 53?64. As-
sociation for Computational Linguistics.
Victor M. Jimenez and Andres Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. Advances in Pat-
tern Recognition, pages 183?192.
R.J. Kate and R.J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In ASSOCIATION FOR
COMPUTATIONAL LINGUISTICS, volume 44, page
913.
R.J. Kate, Y.W. Wong, and R.J. Mooney. 2005. Learning
to transform natural to formal languages. In Proceed-
ings of the National Conference on Artificial Intelli-
gence, volume 20, page 1062. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,
and Mark Steedman. 2010. Inducing probabilistic ccg
grammars from logical form with higher-order unifica-
tion. In Proceedings of EMNLP.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in ccg
grammar induction for semantic parsing. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1512?1523. Associa-
tion for Computational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of ACL, pages 91?99.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. Compu-
tational Linguistics, pages 1?94.
R. Mihalcea, H. Liu, and H. Lieberman. 2006. Nlp (natu-
ral language processing) for nlp (natural language pro-
gramming). Computational Linguistics and Intelligent
Text Processing, pages 319?330.
Anders M?ller. 2010. dk.brics.automaton ? finite-
state automata and regular expressions for Java.
http://www.brics.dk/automaton/.
N. Moreira and R. Reis. 2012. Implementation and ap-
plication of automata.
oDesk. 2013. http://odesk.com/.
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 1-Volume 1, pages 1?10. Association for
Computational Linguistics.
Aarne Ranta. 1998. A multilingual natural-language
interface to regular expressions. In Proceedings of
the International Workshop on Finite State Methods in
Natural Language Processing, pages 79?90. Associa-
tion for Computational Linguistics.
R.G. Raymond and J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proceedings of
the COLING/ACL on Main conference poster sessions,
pages 263?270. Association for Computational Lin-
guistics.
M. Steedman. 2001. The syntactic process. MIT press.
D. Tabakov and M. Vardi. 2005. Experimental evalua-
tion of classical automata constructions. In Logic for
Programming, Artificial Intelligence, and Reasoning,
pages 396?411. Springer.
C.A. Thompson and R.J. Mooney. 2003. Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research,
18(1):1?44.
Mechanical Turk. 2013. http://mturk.com/.
Y.W. Wong and R.J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
835
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 439?446. Association for Computational
Linguistics.
Y.W. Wong and R. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In ANNUAL MEETING-ASSOCIATION
FOR COMPUTATIONAL LINGUISTICS, volume 45,
page 960.
J.M. Zelle and R.J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence, pages 1050?1055.
L.S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars.
L.S. Zettlemoyer and M. Collins. 2007. Online learning
of relaxed ccg grammars for parsing to logical form.
In In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL-2007. Citeseer.
L.S. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to logi-
cal form. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2-Volume 2, pages 976?
984. Association for Computational Linguistics.
836
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 126?135,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning High-Level Planning from Text
S.R.K. Branavan, Nate Kushman, Tao Lei, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, nkushman, taolei, regina}@csail.mit.edu
Abstract
Comprehending action preconditions and ef-
fects is an essential step in modeling the dy-
namics of the world. In this paper, we ex-
press the semantics of precondition relations
extracted from text in terms of planning oper-
ations. The challenge of modeling this con-
nection is to ground language at the level of
relations. This type of grounding enables us to
create high-level plans based on language ab-
stractions. Our model jointly learns to predict
precondition relations from text and to per-
form high-level planning guided by those rela-
tions. We implement this idea in the reinforce-
ment learning framework using feedback au-
tomatically obtained from plan execution at-
tempts. When applied to a complex virtual
world and text describing that world, our rela-
tion extraction technique performs on par with
a supervised baseline, yielding an F-measure
of 66% compared to the baseline?s 65%. Ad-
ditionally, we show that a high-level planner
utilizing these extracted relations significantly
outperforms a strong, text unaware baseline
? successfully completing 80% of planning
tasks as compared to 69% for the baseline.1
1 Introduction
Understanding action preconditions and effects is a
basic step in modeling the dynamics of the world.
For example, having seeds is a precondition for
growing wheat. Not surprisingly, preconditions have
been extensively explored in various sub-fields of
AI. However, existing work on action models has
largely focused on tasks and techniques specific to
individual sub-fields with little or no interconnection
between them. In NLP, precondition relations have
been studied in terms of the linguistic mechanisms
1The code, data and experimental setup for this work are
available at http://groups.csail.mit.edu/rbg/code/planning
A pickaxe, which is used to harvest stone, can be
made from wood.
(a)
Low Level Actions for: wood? pickaxe? stone
step 1: move from (0,0) to (2,0)
step 2: chop tree at: (2,0)
step 3: get wood at: (2,0)
step 4: craft plank from wood
step 5: craft stick from plank
step 6: craft pickaxe from plank and stick
? ? ?
step N-1: pickup tool: pickaxe
step N: harvest stone with pickaxe at: (5,5)
(b)
Figure 1: Text description of preconditions and effects
(a), and the low-level actions connecting them (b).
that realize them, while in classical planning, these
relations are viewed as a part of world dynamics.
In this paper, we bring these two parallel views to-
gether, grounding the linguistic realization of these
relations in the semantics of planning operations.
The challenge and opportunity of this fusion
comes from the mismatch between the abstractions
of human language and the granularity of planning
primitives. Consider, for example, text describing a
virtual world such as Minecraft2 and a formal de-
scription of that world using planning primitives.
Due to the mismatch in granularity, even the simple
relations between wood, pickaxe and stone described
in the sentence in Figure 1a results in dozens of low-
level planning actions in the world, as can be seen
in Figure 1b. While the text provides a high-level
description of world dynamics, it does not provide
sufficient details for successful plan execution. On
the other hand, planning with low-level actions does
not suffer from this limitation, but is computation-
ally intractable for even moderately complex tasks.
As a consequence, in many practical domains, plan-
ning algorithms rely on manually-crafted high-level
2http://www.minecraft.net/
126
abstractions to make search tractable (Ghallab et al,
2004; Lekavy? and Na?vrat, 2007).
The central idea of our work is to express the se-
mantics of precondition relations extracted from text
in terms of planning operations. For instance, the
precondition relation between pickaxe and stone de-
scribed in the sentence in Figure 1a indicates that
plans which involve obtaining stone will likely need
to first obtain a pickaxe. The novel challenge of this
view is to model grounding at the level of relations,
in contrast to prior work which focused on object-
level grounding. We build on the intuition that the
validity of precondition relations extracted from text
can be informed by the execution of a low-level
planner.3 This feedback can enable us to learn these
relations without annotations. Moreover, we can use
the learned relations to guide a high level planner
and ultimately improve planning performance.
We implement these ideas in the reinforcement
learning framework, wherein our model jointly
learns to predict precondition relations from text and
to perform high-level planning guided by those rela-
tions. For a given planning task and a set of can-
didate relations, our model repeatedly predicts a se-
quence of subgoals where each subgoal specifies an
attribute of the world that must be made true. It
then asks the low-level planner to find a plan be-
tween each consecutive pair of subgoals in the se-
quence. The observed feedback ? whether the low-
level planner succeeded or failed at each step ? is
utilized to update the policy for both text analysis
and high-level planning.
We evaluate our algorithm in the Minecraft virtual
world, using a large collection of user-generated on-
line documents as our source of textual information.
Our results demonstrate the strength of our relation
extraction technique ? while using planning feed-
back as its only source of supervision, it achieves
a precondition relation extraction accuracy on par
with that of a supervised SVM baseline. Specifi-
cally, it yields an F-score of 66% compared to the
65% of the baseline. In addition, we show that
these extracted relations can be used to improve the
performance of a high-level planner. As baselines
3If a planner can find a plan to successfully obtain stone
after obtaining a pickaxe, then a pickaxe is likely a precondition
for stone. Conversely, if a planner obtains stone without first
obtaining a pickaxe, then it is likely not a precondition.
for this evaluation, we employ the Metric-FF plan-
ner (Hoffmann and Nebel, 2001),4 as well as a text-
unaware variant of our model. Our results show that
our text-driven high-level planner significantly out-
performs all baselines in terms of completed plan-
ning tasks ? it successfully solves 80% as compared
to 41% for the Metric-FF planner and 69% for the
text unaware variant of our model. In fact, the per-
formance of our method approaches that of an ora-
cle planner which uses manually-annotated precon-
ditions.
2 Related Work
Extracting Event Semantics from Text The task
of extracting preconditions and effects has previ-
ously been addressed in the context of lexical se-
mantics (Sil et al, 2010; Sil and Yates, 2011).
These approaches combine large-scale distributional
techniques with supervised learning to identify de-
sired semantic relations in text. Such combined ap-
proaches have also been shown to be effective for
identifying other relationships between events, such
as causality (Girju and Moldovan, 2002; Chang and
Choi, 2006; Blanco et al, 2008; Beamer and Girju,
2009; Do et al, 2011).
Similar to these methods, our algorithm capital-
izes on surface linguistic cues to learn preconditions
from text. However, our only source of supervision
is the feedback provided by the planning task which
utilizes the predictions. Additionally, we not only
identify these relations in text, but also show they
are valuable in performing an external task.
Learning Semantics via Language Grounding
Our work fits into the broad area of grounded lan-
guage acquisition, where the goal is to learn linguis-
tic analysis from a situated context (Oates, 2001;
Siskind, 2001; Yu and Ballard, 2004; Fleischman
and Roy, 2005; Mooney, 2008a; Mooney, 2008b;
Branavan et al, 2009; Liang et al, 2009; Vogel
and Jurafsky, 2010). Within this line of work, we
are most closely related to the reinforcement learn-
ing approaches that learn language by interacting
with an external environment (Branavan et al, 2009;
Branavan et al, 2010; Vogel and Jurafsky, 2010;
Branavan et al, 2011).
4The state-of-the-art baseline used in the 2008 International
Planning Competition. http://ipc.informatik.uni-freiburg.de/
127
Text (input):
A pickaxe, which is used to harvest stone, 
can be made from wood.
Precondition Relations:
pickaxe stonewood pickaxe
Plan Subgoal Sequence:
initial
state
stone
(goal)
wood
(subgoal 1)
pickaxe
(subgoal 2)
Figure 2: A high-level plan showing two subgoals in
a precondition relation. The corresponding sentence is
shown above.
The key distinction of our work is the use of
grounding to learn abstract pragmatic relations, i.e.
to learn linguistic patterns that describe relationships
between objects in the world. This supplements pre-
vious work which grounds words to objects in the
world (Branavan et al, 2009; Vogel and Jurafsky,
2010). Another important difference of our setup
is the way the textual information is utilized in the
situated context. Instead of getting step-by-step in-
structions from the text, our model uses text that de-
scribes general knowledge about the domain struc-
ture. From this text, it extracts relations between
objects in the world which hold independently of
any given task. Task-specific solutions are then con-
structed by a planner that relies on these relations to
perform effective high-level planning.
Hierarchical Planning It is widely accepted that
high-level plans that factorize a planning prob-
lem can greatly reduce the corresponding search
space (Newell et al, 1959; Bacchus and Yang,
1994). Previous work in planning has studied
the theoretical properties of valid abstractions and
proposed a number of techniques for generating
them (Jonsson and Barto, 2005; Wolfe and Barto,
2005; Mehta et al, 2008; Barry et al, 2011). In gen-
eral, these techniques use static analysis of the low-
level domain to induce effective high-level abstrac-
tions. In contrast, our focus is on learning the ab-
straction from natural language. Thus our technique
is complementary to past work, and can benefit from
human knowledge about the domain structure.
3 Problem Formulation
Our task is two-fold. First, given a text document
describing an environment, we wish to extract a set
of precondition/effect relations implied by the text.
Second, we wish to use these induced relations to
determine an action sequence for completing a given
task in the environment.
We formalize our task as illustrated in Figure 2.
As input, we are given a world defined by the tuple
?S,A, T ?, where S is the set of possible world states,
A is the set of possible actions and T is a determin-
istic state transition function. Executing action a in
state s causes a transition to a new state s? according
to T (s? | s, a). States are represented using proposi-
tional logic predicates xi ? X , where each state is
simply a set of such predicates, i.e. s ? X .
The objective of the text analysis part of our task
is to automatically extract a set of valid precondi-
tion/effect relationships from a given document d.
Given our definition of the world state, precondi-
tions and effects are merely single term predicates,
xi, in this world state. We assume that we are given
a seed mapping between a predicate xi, and the
word types in the document that reference it (see
Table 3 for examples). Thus, for each predicate
pair ?xk, xl?, we want to utilize the text to predict
whether xk is a precondition for xl; i.e., xk ? xl.
For example, from the text in Figure 2, we want to
predict that possessing a pickaxe is a precondition
for possessing stone. Note that this relation implies
the reverse as well, i.e. xl can be interpreted as the
effect of an action sequence performed on state xk.
Each planning goal g ? G is defined by a starting
state sg0, and a final goal state s
g
f . This goal state is
represented by a set of predicates which need to be
made true. In the planning part of our task our objec-
tive is to find a sequence of actions ~a that connect sg0
to sgf . Finally, we assume document d does not con-
tain step-by-step instructions for any individual task,
but instead describes general facts about the given
world that are useful for a wide variety of tasks.
4 Model
The key idea behind our model is to leverage textual
descriptions of preconditions and effects to guide the
construction of high level plans. We define a high-
level plan as a sequence of subgoals, where each
128
subgoal is represented by a single-term predicate,
xi, that needs to be set in the corresponding world
state ? e.g. have(wheat)=true. Thus the set of
possible subgoals is defined by the set of all possi-
ble single-term predicates in the domain. In contrast
to low-level plans, the transition between these sub-
goals can involve multiple low-level actions. Our al-
gorithm for textually informed high-level planning
operates in four steps:
1. Use text to predict the preconditions of each
subgoal. These predictions are for the entire
domain and are not goal specific.
2. Given a planning goal and the induced pre-
conditions, predict a subgoal sequence that
achieves the given goal.
3. Execute the predicted sequence by giving each
pair of consecutive subgoals to a low-level
planner. This planner, treated as a black-box,
computes the low-level plan actions necessary
to transition from one subgoal to the next.
4. Update the model parameters, using the low-
level planner?s success or failure as the source
of supervision.
We formally define these steps below.
Modeling Precondition Relations Given a docu-
ment d, and a set of subgoal pairs ?xi, xj?, we want
to predict whether subgoal xi is a precondition for
xj . We assume that precondition relations are gener-
ally described within single sentences. We first use
our seed grounding in a preprocessing step where
we extract all predicate pairs where both predicates
are mentioned in the same sentence. We call this set
the Candidate Relations. Note that this set will con-
tain many invalid relations since co-occurrence in a
sentence does not necessarily imply a valid precon-
dition relation.5 Thus for each sentence, ~wk, asso-
ciated with a given Candidate Relation, xi ? xj ,
our task is to predict whether the sentence indicates
the relation. We model this decision via a log linear
distribution as follows:
p(xi ? xj | ~wk, qk; ?c) ? e
?c??c(xi,xj , ~wk,qk), (1)
where ?c is the vector of model parameters. We
compute the feature function ?c using the seed
5In our dataset only 11% of Candidate Relations are valid.
Input: A document d, Set of planning tasks G,
Set of candidate precondition relations Call,
Reward function r(), Number of iterations T
Initialization:Model parameters ?x = 0 and ?c = 0.
for i = 1 ? ? ?T do
Sample valid preconditions:
C ? ?
foreach ?xi, xj? ? Call do
foreach Sentence ~wk containing xi and xj do
v ? p(xi ? xj | ~wk, qk; ?c)
if v = 1 then C = C ? ?xi, xj?
end
end
Predict subgoal sequences for each task g.
foreach g ? G do
Sample subgoal sequence ~x as follows:
for t = 1 ? ? ?n do
Sample next subgoal:
xt ? p(x | xt?1, s
g
0, s
g
f , C; ?x)
Construct low-level subtask from xt?1 to xt
Execute low-level planner on subtask
end
Update subgoal prediction model using Eqn. 2
end
Update text precondition model using Eqn. 3
end
Algorithm 1: A policy gradient algorithm for pa-
rameter estimation in our model.
grounding, the sentence ~wk, and a given dependency
parse qk of the sentence. Given these per-sentence
decisions, we predict the set of all valid precondi-
tion relations, C, in a deterministic fashion. We do
this by considering a precondition xi ? xj as valid
if it is predicted to be valid by at least one sentence.
Modeling Subgoal Sequences Given a planning
goal g, defined by initial and final goal states sg0 and
sgf , our task is to predict a sequence of subgoals ~x
which will achieve the goal. We condition this de-
cision on our predicted set of valid preconditions C,
by modeling the distribution over sequences ~x as:
p(~x | sg0, s
g
f , C; ?x) =
n?
t=1
p(xt | xt?1, s
g
0, s
g
f , C; ?x),
p(xt | xt?1, s
g
0, s
g
f , C; ?x) ? e
?x??x(xt,xt?1,s
g
0,s
g
f ,C).
Here we assume that subgoal sequences are Marko-
vian in nature and model individual subgoal predic-
tions using a log-linear model. Note that in con-
129
trast to Equation 1 where the predictions are goal-
agnostic, these predictions are goal-specific. As be-
fore, ?x is the vector of model parameters, and ?x is
the feature function. Additionally, we assume a spe-
cial stop symbol, x?, which indicates the end of the
subgoal sequence.
Parameter Update Parameter updates in our model
are done via reinforcement learning. Specifically,
once the model has predicted a subgoal sequence for
a given goal, the sequence is given to the low-level
planner for execution. The success or failure of this
execution is used to compute the reward signal r for
parameter estimation. This predict-execute-update
cycle is repeated until convergence. We assume that
our reward signal r strongly correlates with the cor-
rectness of model predictions. Therefore, during
learning, we need to find the model parameters that
maximize expected future reward (Sutton and Barto,
1998). We perform this maximization via stochastic
gradient ascent, using the standard policy gradient
algorithm (Williams, 1992; Sutton et al, 2000).
We perform two separate policy gradient updates,
one for each model component. The objective of the
text component of our model is purely to predict the
validity of preconditions. Therefore, subgoal pairs
?xk, xl?, where xl is reachable from xk, are given
positive reward. The corresponding parameter up-
date, with learning rate ?c, takes the following form:
??c ? ?c r
[
?c(xi, xj , ~wk, qk) ?
Ep(xi??xj? |?)
[
?c(xi? , xj? , ~wk, qk)
]
]
. (2)
The objective of the planning component of our
model is to predict subgoal sequences that success-
fully achieve the given planning goals. Thus we di-
rectly use plan-success as a binary reward signal,
which is applied to each subgoal decision in a se-
quence. This results in the following update:
??x ? ?x r
?
t
[
?x(xt, xt?1, s
g
0, s
g
f , C) ?
Ep(x?t|?)
[
?x(x
?
t, xt?1, s
g
0, s
g
f , C)
] ]
, (3)
where t indexes into the subgoal sequence and ?x is
the learning rate.
fish
iron
shears bucket
milkstringseeds wool
iron doorbone meal
fishing rod
wood
plank
stick
fence
Figure 3: Example of the precondition dependencies
present in the Minecraft domain.
Domain #Objects #Pred Types #Actions
Parking 49 5 4
Floortile 61 10 7
Barman 40 15 12
Minecraft 108 16 68
Table 1: A comparison of complexity between Minecraft
and some domains used in the IPC-2011 sequential satis-
ficing track. In the Minecraft domain, the number of ob-
jects, predicate types, and actions is significantly larger.
5 Applying the Model
We apply our method to Minecraft, a grid-based vir-
tual world. Each grid location represents a tile of ei-
ther land or water and may also contain resources.
Users can freely move around the world, harvest
resources and craft various tools and objects from
these resources. The dynamics of the world require
certain resources or tools as prerequisites for per-
forming a given action, as can be seen in Figure 3.
For example, a user must first craft a bucket before
they can collect milk.
Defining the Domain In order to execute a tradi-
tional planner on the Minecraft domain, we define
the domain using the Planning Domain Definition
Language (PDDL) (Fox and Long, 2003). This is the
standard task definition language used in the Inter-
national Planning Competitions (IPC).6 We define
as predicates all aspects of the game state ? for ex-
ample, the location of resources in the world, the re-
sources and objects possessed by the player, and the
player?s location. Our subgoals xi and our task goals
sgf map directly to these predicates. This results in
a domain with significantly greater complexity than
those solvable by traditional low-level planners. Ta-
ble 1 compares the complexity of our domain with
some typical planning domains used in the IPC.
6http://ipc.icaps-conference.org/
130
Low-level Planner As our low-level planner we
employ Metric-FF (Hoffmann and Nebel, 2001),
the state-of-the-art baseline used in the 2008 In-
ternational Planning Competition. Metric-FF is a
forward-chaining heuristic state space planner. Its
main heuristic is to simplify the task by ignoring op-
erator delete lists. The number of actions in the so-
lution for this simplified task is then used as the goal
distance estimate for various search strategies.
Features The two components of our model lever-
age different types of information, and as a result,
they each use distinct sets of features. The text com-
ponent features ?c are computed over sentences and
their dependency parses. The Stanford parser (de
Marneffe et al, 2006) was used to generate the de-
pendency parse information for each sentence. Ex-
amples of these features appear in Table 2. The se-
quence prediction component takes as input both the
preconditions induced by the text component as well
as the planning state and the previous subgoal. Thus
?x contains features which check whether two sub-
goals are connected via an induced precondition re-
lation, in addition to features which are simply the
Cartesian product of domain predicates.
6 Experimental Setup
Datasets As the text description of our virtual world,
we use documents from the Minecraft Wiki,7 the
most popular information source about the game.
Our manually constructed seed grounding of pred-
icates contains 74 entries, examples of which can be
seen in Table 3. We use this seed grounding to iden-
tify a set of 242 sentences that reference predicates
in the Minecraft domain. This results in a set of
694 Candidate Relations. We also manually anno-
tated the relations expressed in the text, identifying
94 of the Candidate Relations as valid. Our corpus
contains 979 unique word types and is composed of
sentences with an average length of 20 words.
We test our system on a set of 98 problems that
involve collecting resources and constructing ob-
jects in the Minecraft domain ? for example, fish-
ing, cooking and making furniture. To assess the
complexity of these tasks, we manually constructed
high-level plans for these goals and solved them us-
ing the Metric-FF planner. On average, the execu-
7http://www.minecraftwiki.net/wiki/Minecraft Wiki/
Words
Dependency Types
Dependency Type ? Direction
Word ? Dependency Type
Word ? Dependency Type ? Direction
Table 2: Example text features. A subgoal pair ?xi, xj?
is first mapped to word tokens using a small grounding
table. Words and dependencies are extracted along paths
between mapped target words. These are combined with
path directions to generate the text features.
Domain Predicate Noun Phrases
have(plank) wooden plank, wood plank
have(stone) stone, cobblestone
have(iron) iron ingot
Table 3: Examples in our seed grounding table. Each
predicate is mapped to one or more noun phrases that de-
scribe it in the text.
tion of the sequence of low-level plans takes 35 ac-
tions, with 3 actions for the shortest plan and 123
actions for the longest. The average branching fac-
tor is 9.7, leading to an average search space of more
than 1034 possible action sequences. For evaluation
purposes we manually identify a set of Gold Rela-
tions consisting of all precondition relations that are
valid in this domain, including those not discussed
in the text.
Evaluation Metrics We use our manual annotations
to evaluate the type-level accuracy of relation extrac-
tion. To evaluate our high-level planner, we use the
standard measure adopted by the IPC. This evalu-
ation measure simply assesses whether the planner
completes a task within a predefined time.
Baselines To evaluate the performance of our rela-
tion extraction, we compare against an SVM classi-
fier8 trained on the Gold Relations. We test the SVM
baseline in a leave-one-out fashion.
To evaluate the performance of our text-aware
high-level planner, we compare against five base-
lines. The first two baselines ? FF and No Text ?
do not use any textual information. The FF base-
line directly runs the Metric-FF planner on the given
task, while the No Text baseline is a variant of our
model that learns to plan in the reinforcement learn-
ing framework. It uses the same state-level features
8SVMlight (Joachims, 1999) with default parameters.
131
?Seeds  for growing  wheat  can be obtained by breaking  tall grass (false negative)
Sticks  are the only building material required to craft a  fence  or  ladder.
Figure 4: Examples of precondition relations predicted by our model from text. Check marks (3) indicate correct
predictions, while a cross (8) marks the incorrect one ? in this case, a valid relation that was predicted as invalid by
our model. Note that each pair of highlighted noun phrases in a sentence is a Candidate Relation, and pairs that are
not connected by an arrow were correctly predicted to be invalid by our model.
200100 15050
Figure 5: The performance of our model and a supervised
SVM baseline on the precondition prediction task. Also
shown is the F-Score of the full set of Candidate Rela-
tions which is used unmodified by All Text, and is given as
input to our model. Our model?s F-score, averaged over
200 trials, is shown with respect to learning iterations.
as our model, but does not have access to text.
The All Text baseline has access to the full set of
694 Candidate Relations. During learning, our full
model refines this set of relations, while in contrast
the All Text baseline always uses the full set.
The two remaining baselines constitute the upper
bound on the performance of our model. The first,
Manual Text, is a variant of our model which directly
uses the links derived from manual annotations of
preconditions in text. The second, Gold, has access
to the Gold Relations. Note that the connections
available to Manual Text are a subset of the Gold
links, because the text does not specify all relations.
Experimental Details All experimental results are
averaged over 200 independent runs for both our
model as well as the baselines. Each of these tri-
als is run for 200 learning iterations with a max-
imum subgoal sequence length of 10. To find a
low-level plan between each consecutive pair of sub-
goals, our high-level planner internally uses Metric-
FF. We give Metric-FF a one-minute timeout to find
such a low-level plan. To ensure that the comparison
Method %Plans
FF 40.8
No text 69.4
All text 75.5
Full model 80.2
Manual text 84.7
Gold connection 87.1
Table 4: Percentage of tasks solved successfully by our
model and the baselines. All performance differences be-
tween methods are statistically significant at p ? .01.
between the high-level planners and the FF baseline
is fair, the FF baseline is allowed a runtime of 2,000
minutes. This is an upper bound on the time that our
high-level planner can take over the 200 learning it-
erations, with subgoal sequences of length at most
10 and a one minute timeout. Lastly, during learning
we initialize all parameters to zero, use a fixed learn-
ing rate of 0.0001, and encourage our model to ex-
plore the state space by using the standard -greedy
exploration strategy (Sutton and Barto, 1998).
7 Results
Relation Extraction Figure 5 shows the perfor-
mance of our method on identifying preconditions
in text. We also show the performance of the super-
vised SVM baseline. As can be seen, after 200 learn-
ing iterations, our model achieves an F-Measure of
66%, equal to the supervised baseline. These results
support our hypothesis that planning feedback is a
powerful source of supervision for analyzing a given
text corpus. Figure 4 shows some examples of sen-
tences and the corresponding extracted relations.
Planning Performance As shown in Table 4 our
text-enriched planning model outperforms the text-
free baselines by more than 10%. Moreover, the
performance improvement of our model over the All
Text baseline demonstrates that the accuracy of the
132
0% 20% 40% 60% 80% 100%
No text
All text
Full model
Manual text
Gold
Easy
Hard
71%
64%
59%
48%
31% 88%
89%
91%
94%
95%
Figure 6: Percentage of problems solved by various mod-
els on Easy and Hard problem sets.
extracted text relations does indeed impact planning
performance. A similar conclusion can be reached
by comparing the performance of our model and the
Manual Text baseline.
The difference in performance of 2.35% between
Manual Text and Gold shows the importance of the
precondition information that is missing from the
text. Note that Gold itself does not complete all
tasks ? this is largely because the Markov assump-
tion made by our model does not hold for all tasks.9
Figure 6 breaks down the results based on the dif-
ficulty of the corresponding planning task. We mea-
sure problem complexity in terms of the low-level
steps needed to implement a manually constructed
high-level plan. Based on this measure, we divide
the problems into two sets. As can be seen, all of
the high-level planners solve almost all of the easy
problems. However, performance varies greatly on
the more challenging tasks, directly correlating with
planner sophistication. On these tasks our model
outperforms the No Text baseline by 28% and the
All Text baseline by 11%.
Feature Analysis Figure 7 shows the top five pos-
itive features for our model and the SVM baseline.
Both models picked up on the words that indicate
precondition relations in this domain. For instance,
the word use often occurs in sentences that describe
the resources required to make an object, such as
?bricks are items used to craft brick blocks?. In ad-
dition to lexical features, dependency information is
also given high weight by both learners. An example
9When a given task has two non-trivial preconditions, our
model will choose to satisfy one of the two first, and the Markov
assumption blinds it to the remaining precondition, preventing
it from determining that it must still satisfy the other.
path has word "craft"
path has dependency type "partmod"
path has word "equals"
path has word "use"
path has dependency type "xsubj"
path has word "use"
path has word "fill"
path has dependency type "dobj"
path has dependency type "xsubj"
path has word "craft"
Figure 7: The top five positive features on words and
dependency types learned by our model (above) and by
SVM (below) for precondition prediction.
of this is a feature that checks for the direct object
dependency type. This analysis is consistent with
prior work on event semantics which shows lexico-
syntactic features are effective cues for learning text
relations (Blanco et al, 2008; Beamer and Girju,
2009; Do et al, 2011).
8 Conclusions
In this paper, we presented a novel technique for in-
ducing precondition relations from text by ground-
ing them in the semantics of planning operations.
While using planning feedback as its only source
of supervision, our method for relation extraction
achieves a performance on par with that of a su-
pervised baseline. Furthermore, relation grounding
provides a new view on classical planning problems
which enables us to create high-level plans based on
language abstractions. We show that building high-
level plans in this manner significantly outperforms
traditional techniques in terms of task completion.
Acknowledgments
The authors acknowledge the support of the
NSF (CAREER grant IIS-0448168, grant IIS-
0835652), the DARPA Machine Reading Program
(FA8750-09-C-0172, PO#4910018860), and Batelle
(PO#300662). Thanks to Amir Globerson, Tommi
Jaakkola, Leslie Kaelbling, George Konidaris, Dy-
lan Hadfield-Menell, Stefanie Tellex, the MIT NLP
group, and the ACL reviewers for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
133
References
Fahiem Bacchus and Qiang Yang. 1994. Downward
refinement and the efficiency of hierarchical problem
solving. Artificial Intell., 71(1):43?100.
Jennifer L. Barry, Leslie Pack Kaelbling, and Toms
Lozano-Prez. 2011. DetH*: Approximate hierarchi-
cal solution of large markov decision processes. In
IJCAI?11, pages 1928?1935.
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In Pro-
ceedings of CICLing, pages 430?441.
Eduardo Blanco, Nuria Castell, and Dan Moldovan.
2008. Causal relation extraction. In Proceedings of
the LREC?08.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
ACL, pages 82?90.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay.
2010. Reading between the lines: Learning to map
high-level instructions to commands. In Proceedings
of ACL, pages 1268?1277.
S. R. K. Branavan, David Silver, and Regina Barzilay.
2011. Learning to win by reading manuals in a monte-
carlo framework. In Proceedings of ACL, pages 268?
277.
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Inf. Process. Manage., 42(3):662?678.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Q. Do, Y. Chan, and D. Roth. 2011. Minimally super-
vised event causality identification. In EMNLP, 7.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated natural language learning. In Pro-
ceedings of CoNLL, pages 104?111.
Maria Fox and Derek Long. 2003. Pddl2.1: An ex-
tension to pddl for expressing temporal planning do-
mains. Journal of Artificial Intelligence Research,
20:2003.
Malik Ghallab, Dana S. Nau, and Paolo Traverso. 2004.
Automated Planning: theory and practice. Morgan
Kaufmann.
Roxana Girju and Dan I. Moldovan. 2002. Text mining
for causal relations. In Proceedigns of FLAIRS, pages
360?364.
Jo?rg Hoffmann and Bernhard Nebel. 2001. The FF plan-
ning system: Fast plan generation through heuristic
search. JAIR, 14:253?302.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169?184. MIT Press.
Anders Jonsson and Andrew Barto. 2005. A causal
approach to hierarchical decomposition of factored
mdps. In Advances in Neural Information Processing
Systems, 13:10541060, page 22. Press.
Maria?n Lekavy? and Pavol Na?vrat. 2007. Expressivity
of strips-like and htn-like planning. Lecture Notes in
Artificial Intelligence, 4496:121?130.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of ACL, pages 91?99.
Neville Mehta, Soumya Ray, Prasad Tadepalli, and
Thomas Dietterich. 2008. Automatic discovery and
transfer of maxq hierarchies. In Proceedings of the
25th international conference on Machine learning,
ICML ?08, pages 648?655.
Raymond J. Mooney. 2008a. Learning language from its
perceptual context. In Proceedings of ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings of AAAI, pages
1598?1601.
A. Newell, J.C. Shaw, and H.A. Simon. 1959. The pro-
cesses of creative thinking. Paper P-1320. Rand Cor-
poration.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
Avirup Sil and Alexander Yates. 2011. Extract-
ing STRIPS representations of actions and events.
In Recent Advances in Natural Language Learning
(RANLP).
Avirup Sil, Fei Huang, and Alexander Yates. 2010. Ex-
tracting action and event semantics from web text. In
AAAI 2010 Fall Symposium on Commonsense Knowl-
edge (CSK).
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. Journal of Artificial Intelli-
gence Research, 15:31?90.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. The MIT Press.
Richard S. Sutton, David McAllester, Satinder Singh, and
Yishay Mansour. 2000. Policy gradient methods for
reinforcement learning with function approximation.
In Advances in NIPS, pages 1057?1063.
Adam Vogel and Daniel Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of the
ACL, pages 806?814.
Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine Learning, 8.
134
Alicia P. Wolfe and Andrew G. Barto. 2005. Identify-
ing useful subgoals in reinforcement learning by local
graph partitioning. In In Proceedings of the Twenty-
Second International Conference on Machine Learn-
ing, pages 816?823.
Chen Yu and Dana H. Ballard. 2004. On the integration
of grounding language and learning objects. In Pro-
ceedings of AAAI, pages 488?493.
135
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 271?281,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning to Automatically Solve Algebra Word Problems
Nate Kushman
?
, Yoav Artzi
?
, Luke Zettlemoyer
?
, and Regina Barzilay
?
?
Computer Science and Articial Intelligence Laboratory, Massachusetts Institute of Technology
{nkushman, regina}@csail.mit.edu
?
Computer Science & Engineering, University of Washington
{yoav, lsz}@cs.washington.edu
Abstract
We present an approach for automatically
learning to solve algebra word problems.
Our algorithm reasons across sentence
boundaries to construct and solve a sys-
tem of linear equations, while simultane-
ously recovering an alignment of the vari-
ables and numbers in these equations to
the problem text. The learning algorithm
uses varied supervision, including either
full equations or just the final answers. We
evaluate performance on a newly gathered
corpus of algebra word problems, demon-
strating that the system can correctly an-
swer almost 70% of the questions in the
dataset. This is, to our knowledge, the first
learning result for this task.
1 Introduction
Algebra word problems concisely describe a world
state and pose questions about it. The described
state can be modeled with a system of equations
whose solution specifies the questions? answers.
For example, Figure 1 shows one such problem.
The reader is asked to infer how many children and
adults were admitted to an amusement park, based
on constraints provided by ticket prices and overall
sales. This paper studies the task of learning to
automatically solve such problems given only the
natural language.
1
Solving these problems requires reasoning
across sentence boundaries to find a system of
equations that concisely models the described se-
mantic relationships. For example, in Figure 1,
the total ticket revenue computation in the second
equation summarizes facts about ticket prices and
total sales described in the second, third, and fifth
1
The code and data for this work are available
at http://groups.csail.mit.edu/rbg/code/
wordprobs/.
Word problem
An amusement park sells 2 kinds of tickets.
Tickets for children cost $1.50. Adult tickets
cost $4. On a certain day, 278 people entered
the park. On that same day the admission fees
collected totaled $792. How many children
were admitted on that day? How many adults
were admitted?
Equations
x+ y = 278
1.5x+ 4y = 792
Solution
x = 128 y = 150
Figure 1: An example algebra word problem. Our
goal is to map a given problem to a set of equations
representing its algebraic meaning, which are then
solved to get the problem?s answer.
sentences. Furthermore, the first equation models
an implicit semantic relationship, namely that the
children and adults admitted are non-intersecting
subsets of the set of people who entered the park.
Our model defines a joint log-linear distribu-
tion over full systems of equations and alignments
between these equations and the text. The space
of possible equations is defined by a set of equa-
tion templates, which we induce from the train-
ing examples, where each template has a set of
slots. Number slots are filled by numbers from
the text, and unknown slots are aligned to nouns.
For example, the system in Figure 1 is gener-
ated by filling one such template with four spe-
cific numbers (1.5, 4, 278, and 792) and align-
ing two nouns (?Tickets? in ?Tickets for children?,
and ?tickets? in ?Adult tickets?). These inferred
correspondences are used to define cross-sentence
features that provide global cues to the model.
For instance, in our running example, the string
271
pairs (?$1.50?, ?children?) and (?$4?,?adults?)
both surround the word ?cost,? suggesting an out-
put equation with a sum of two constant-variable
products.
We consider learning with two different levels
of supervision. In the first scenario, we assume ac-
cess to each problem?s numeric solution (see Fig-
ure 1) for most of the data, along with a small
set of seed examples labeled with full equations.
During learning, a solver evaluates competing hy-
potheses to drive the learning process. In the sec-
ond scenario, we are provided with a full system
of equations for each problem. In both cases, the
available labeled equations (either the seed set, or
the full set) are abstracted to provide the model?s
equation templates, while the slot filling and align-
ment decisions are latent variables whose settings
are estimated by directly optimizing the marginal
data log-likelihood.
The approach is evaluated on a new corpus of
514 algebra word problems and associated equa-
tion systems gathered from Algebra.com. Pro-
vided with full equations during training, our al-
gorithm successfully solves over 69% of the word
problems from our test set. Furthermore, we find
the algorithm can robustly handle weak supervi-
sion, achieving more than 70% of the above per-
formance when trained exclusively on answers.
2 Related Work
Our work is related to three main areas of research:
situated semantic interpretation, information ex-
traction, and automatic word problem solvers.
Situated Semantic Interpretation There is a
large body of research on learning to map nat-
ural language to formal meaning representations,
given varied forms of supervision. Reinforcement
learning can be used to learn to read instructions
and perform actions in an external world (Brana-
van et al, 2009; Branavan et al, 2010; Vogel
and Jurafsky, 2010). Other approaches have re-
lied on access to more costly annotated logical
forms (Zelle and Mooney, 1996; Thompson and
Mooney, 2003; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2005; Kwiatkowski et al,
2010). These techniques have been generalized
more recently to learn from sentences paired with
indirect feedback from a controlled application.
Examples include question answering (Clarke et
al., 2010; Cai and Yates, 2013a; Cai and Yates,
2013b; Berant et al, 2013; Kwiatkowski et al,
2013), dialog systems (Artzi and Zettlemoyer,
2011), robot instruction (Chen and Mooney, 2011;
Chen, 2012; Kim and Mooney, 2012; Matuszek et
al., 2012; Artzi and Zettlemoyer, 2013), and pro-
gram executions (Kushman and Barzilay, 2013;
Lei et al, 2013). We focus on learning from varied
supervision, including question answers and equa-
tion systems, both can be obtained reliably from
annotators with no linguistic training and only ba-
sic math knowledge.
Nearly all of the above work processed sin-
gle sentences in isolation. Techniques that con-
sider multiple sentences typically do so in a se-
rial fashion, processing each in turn with limited
cross-sentence reasoning (Branavan et al, 2009;
Zettlemoyer and Collins, 2009; Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013). We focus on
analyzing multiple sentences simultaneously, as
is necessary to generate the global semantic rep-
resentations common in domains such as algebra
word problems.
Information Extraction Our approach is related
to work on template-based information extraction,
where the goal is to identify instances of event
templates in text and extract their slot fillers. Most
work has focused on the supervised case, where
the templates are manually defined and data is la-
beled with alignment information, e.g. (Grishman
et al, 2005; Maslennikov and Chua, 2007; Ji and
Grishman, 2008; Reichart and Barzilay, 2012).
However, some recent work has studied the au-
tomatic induction of the set of possible templates
from data (Chambers and Jurafsky, 2011; Ritter et
al., 2012). In our approach, systems of equations
are relatively easy to specify, providing a type of
template structure, and the alignment of the slots
in these templates to the text is modeled primar-
ily with latent variables during learning. Addition-
ally, mapping to a semantic representation that can
be executed allows us to leverage weaker supervi-
sion during learning.
Automatic Word Problem Solvers Finally, there
has been research on automatically solving vari-
ous types of mathematical word problems. The
dominant existing approach is to hand engineer
rule-based systems to solve math problem in spe-
cific domains (Mukherjee and Garain, 2008; Lev
et al, 2004). Our focus is on learning a model
for the end-to-end task of solving word problems
given only a training corpus of questions paired
with equations or answers.
272
Derivation 1
Word
problem
An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult
tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the
admission fees collected totaled $ 792 . How many children were admitted on that
day? How many adults were admitted?
Aligned
template
u
1
1
+ u
1
2
? n
1
= 0 n
2
? u
2
1
+ n
3
? u
2
2
? n
4
= 0
Instantiated
equations
x+ y ? 278 = 0 1.5x+ 4y ? 792 = 0
Answer
x = 128
y = 150
Derivation 2
Word
problem
A motorist drove 2 hours at one speed and then for 3 hours at another speed. He
covered a distance of 252 kilometers. If he had traveled 4 hours at the first speed and
1 hour at the second speed , he would have covered 244 kilometers. Find two speeds?
Aligned
template
n
1
? u
1
1
+ n
2
? u
1
2
? n
3
= 0 n
4
? u
2
1
+ n
5
? u
2
2
? n
6
= 0
Instantiated
equations
2x+ 3y ? 252 = 0 4x+ 1y ? 244 = 0
Answer
x = 48
y = 52
Figure 2: Two complete derivations for two different word problems. Derivation 1 shows an alignment
where two instances of the same slot are aligned to the same word (e.g., u
1
1
and u
2
1
both are aligned to
?Tickets?). Derivation 2 includes an alignment where four identical nouns are each aligned to different
slot instances in the template (e.g., the first ?speed? in the problem is aligned to u
1
1
).
3 Mapping Word Problems to Equations
We define a two step process to map word prob-
lems to equations. First, a template is selected
to define the overall structure of the equation sys-
tem. Next, the template is instantiated with num-
bers and nouns from the text. During inference we
consider these two steps jointly.
Figure 2 shows both steps for two derivations.
The template dictates the form of the equations in
the system and the type of slots in each: u slots
represent unknowns and n slots are for numbers
that must be filled from the text. In Derivation 1,
the selected template has two unknown slots, u
1
and u
2
, and four number slots, n
1
to n
4
. Slots
can be shared between equations, for example, the
unknown slots u
1
and u
2
in the example appear
in both equations. A slot may have different in-
stances, for example u
1
1
and u
2
1
are the two in-
stances of u
1
in the example.
We align each slot instance to a word in the
problem. Each number slot n is aligned to a num-
ber, and each unknown slot u is aligned to a noun.
For example, Derivation 1 aligns the number 278
to n
1
, 1.50 to n
2
, 4 to n
3
, and 792 to n
4
. It also
aligns both instances of u
1
(e.g., u
1
1
and u
2
1
) to
?Tickets?, and both instances of u
2
to ?tickets?.
In contrast, in Derivation 2, instances of the same
unknown slot (e.g. u
1
1
and u
2
1
) are aligned to two
different words in the problem (different occur-
rences of the word ?speed?). This allows for a
tighter mapping between the natural language and
the system template, where the words aligned to
the first equation in the template come from the
first two sentences, and the words aligned to the
second equation come from the third.
Given an alignment, the template can then be
instantiated: each number slot n is replaced with
the aligned number, and each unknown slot u with
a variable. This output system of equations is then
automatically solved to generate the final answer.
273
3.1 Derivations
Definitions Let X be the set of all word problems.
A word problem x ? X is a sequence of k words
?w
1
, . . . w
k
?. Also, define an equation template t
to be a formulaA = B, whereA andB are expres-
sions. An expression A is one of the following:
? A number constant f .
? A number slot n.
? An unknown slot u.
? An application of a mathematical relation R
to two expressions (e.g., n
1
? u
1
).
We define a system template T to be a set of l
equation templates {t
0
, . . . , t
l
}. T is the set of
all system templates. A slot may occur more than
once in a system template, to allow variables to
be reused in different equations. We denote a spe-
cific instance i of a slot, u for example, as u
i
. For
brevity, we omit the instance index when a slot ap-
pears only once. To capture a correspondence be-
tween the text of x and a template T , we define an
alignment p to be a set of pairs (w, s), where w is
a token in x and s is a slot instance in T .
Given the above definitions, an equation e can
be constructed from a template t where each num-
ber slot n is replaced with a real number, each un-
known slot u is replaced with a variable, and each
number constant f is kept as is. We call the pro-
cess of turning a template into an equation tem-
plate instantiation. Similarly, an equation system
E is a set of l equations {e
0
, . . . , e
l
}, which can
be constructed by instantiating each of the equa-
tion templates in a system template T . Finally, an
answer a is a tuple of real numbers.
We define a derivation y from a word problem
to an answer as a tuple (T, p, a), where T is the se-
lected system template, p is an alignment between
T and x, and a is the answer generated by instan-
tiating T using x through p and solving the gener-
ated equations. Let Y be the set of all derivations.
The Space of Possible Derivations We aim to
map each word problem x to an equation system
E. The space of equation systems considered is
defined by the set of possible system templates T
and the words in the original problem x, that are
available for filling slots. In practice, we gener-
ate T from the training data, as described in Sec-
tion 4.1. Given a system template T ? T , we
create an alignment p between T and x. The set
of possible alignment pairs is constrained as fol-
An amusement park sells 2 kinds of tickets.
Tickets for children cost $ 1.50 . Adult tick-
ets cost $ 4 . On a certain day, 278 people
entered the park. On that same day the ad-
mission fees collected totaled $ 792 . How
many children were admitted on that day?
How many adults were admitted?
u
1
1
+ u
1
2
? n
1
= 0
n
2
? u
2
1
+ n
3
? u
2
2
? n
4
= 0
Figure 3: The first example problem and selected
system template from Figure 2 with all potential
aligned words marked. Nouns (boldfaced) may be
aligned to unknown slot instances u
j
i
, and num-
ber words (highlighted) may be aligned to number
slots n
i
.
lows: each number slot n ? T can be aligned to
any number in the text, a number word can only
be aligned to a single slot n, and must be aligned
to all instances of that slot. Additionally, an un-
known slot instance u ? T can only be aligned to
a noun word. A complete derivation?s alignment
pairs all slots in T with words in x.
Figure 3 illustrates the space of possible align-
ments for the first problem and system template
from Figure 2. Nouns (shown in boldface) can
be aligned to any of the unknown slot instances
in the selected template (u
1
1
, u
2
1
, u
1
2
, and u
2
2
for the
template selected). Numbers (highlighted) can be
aligned to any of the number slots (n
1
, n
2
, n
3
, and
n
4
in the template).
3.2 Probabilistic Model
Due to the ambiguity in selecting the system tem-
plate and alignment, there will be many possible
derivations y ? Y for each word problem x ? X .
We discriminate between competing analyses us-
ing a log-linear model, which has a feature func-
tion ? : X ? Y ? R
d
and a parameter vector
? ? R
d
. The probability of a derivation y given a
problem x is defined as:
p(y|x; ?) =
e
???(x,y)
?
y
?
?Y
e
???(x,y
?
)
Section 6 defines the full set of features used.
The inference problem at test time requires us
to find the most likely answer a given a problem
274
x, assuming the parameters ? are known:
f(x) = argmax
a
p(a|x; ?)
Here, the probability of the answer is marginalized
over template selection and alignment:
p(a|x; ?) =
?
y?Y
s.t. AN(y)=a
p(y|x; ?) (1)
where AN(y) extracts the answer a out of deriva-
tion y. In this way, the distribution over deriva-
tions y is modeled as a latent variable. We use a
beam search inference procedure to approximately
compute Equation 1, as described in Section 5.
4 Learning
To learn our model, we need to induce the struc-
ture of system templates in T and estimate the
model parameters ?.
4.1 Template Induction
It is possible to generate system templates T when
provided access to a set of n training examples
{(x
i
, E
i
) : i = 1, . . . , n}, where x
i
is a word
problem and E
i
is a set of equations. We general-
ize eachE to a system template T by (a) replacing
each variable with an unknown slot, and (b) re-
placing each number mentioned in the text with a
number slot. Numbers not mentioned in the prob-
lem text remain in the template as constants. This
allows us to solve problems that require numbers
that are implied by the problem semantics rather
than appearing directly in the text, such as the per-
cent problem in Figure 4.
4.2 Parameter Estimation
For parameter estimation, we assume access to
n training examples {(x
i
,V
i
) : i = 1, . . . , n},
each containing a word problem x
i
and a val-
idation function V
i
. The validation function
V : Y ? {0, 1} maps a derivation y ? Y to 1 if
it is correct, or 0 otherwise.
We can vary the validation function to learn
from different types of supervision. In Sec-
tion 8, we will use validation functions that check
whether the derivation y has either (1) the cor-
rect system of equations E, or (2) the correct an-
swer a. Also, using different types of validation
functions on different subsets of the data enables
semi-supervised learning. This approach is related
to Artzi and Zettlemoyer (2013).
Word problem
A chemist has a solution that is 18 % alco-
hol and one that is 50 % alcohol. He wants
to make 80 liters of a 30 % solution. How
many liters of the 18 % solution should he
add? How many liters of the 30 % solution
should he add?
Labeled equations
18? 0.01? x + 50? 0.01? y = 30? 0.01? 80
x + y = 80
Induced template system
n
1
? 0.01? u
1
1
+ n
2
? 0.01? u
1
2
= n
3
? 0.01? n
4
u
2
1
+ u
2
2
= n
5
Figure 4: During template induction, we automat-
ically detect the numbers in the problem (high-
lighted above) to generalize the labeled equations
to templates. Numbers not present in the text are
considered part of the induced template.
We estimate ? by maximizing the conditional
log-likelihood of the data, marginalizing over all
valid derivations:
O =
?
i
?
y?Y
s.t. V
i
(y)=1
log p(y|x
i
; ?)
We use L-BFGS (Nocedal and Wright, 2006) to
optimize the parameters. The gradient of the indi-
vidual parameter ?
j
is given by:
?O
??
j
=
?
i
E
p(y|x
i
,V
i
(y)=1;?)
[?
j
(x
i
, y)]?
E
p(y|x
i
;?)
[?
j
(x
i
, y)]
(2)
Section 5 describes how we approximate the
two terms of the gradient using beam search.
5 Inference
Computing the normalization constant for Equa-
tion 1 requires summing over all templates and all
possible ways to instantiate them. This results in
a search space exponential in the number of slots
in the largest template in T , the set of available
system templates. Therefore, we approximate this
computation using beam search. We initialize the
beam with all templates in T and iteratively align
slots from the templates in the beam to words in
the problem text. For each template, the next slot
275
to be considered is selected according to a pre-
defined canonicalized ordering for that template.
After each iteration we prune the beam to keep the
top-k partial derivations according to the model
score. When pruning the beam, we allow at most l
partial derivations for each template, to ensure that
a small number of templates don?t monopolize the
beam. We continue this process until all templates
in the beam are fully instantiated.
During learning we compute the second term in
the gradient (Equation 2) using our beam search
approximation. Depending on the available vali-
dation function V (as defined in Section 4.2), we
can also accurately prune the beam for the com-
putation of the first half of the gradient. Specifi-
cally, when assuming access to labeled equations,
we can constrain the search to consider only par-
tial hypotheses that could possibly be completed
to produce the labeled equations.
6 Model Details
Template Canonicalization There are many syn-
tactically different but semantically equivalent
ways to express a given system of equations. For
example, the phrase ?John is 3 years older than
Bill? can be written as j = b+ 3 or j ? 3 = b.
To avoid such ambiguity, we canonicalize tem-
plates into a normal form representation. We per-
form this canonicalization by obtaining the sym-
bolic solution for the unknown slots in terms of
the number slots and constants using the mathe-
matical solver Maxima (Maxima, 2014).
Slot Signature In a template like s
1
+s
2
= s
3
, the
slot s
1
is distinct from the slot s
2
, but we would
like them to share many of the features used in de-
ciding their alignment. To facilitate this, we gener-
ate signatures for each slot and slot pair. The sig-
nature for a slot indicates the system of equations
it appears in, the specific equation it is in, and the
terms of the equation it is a part of. Pairwise slot
signatures concatenate the signatures for the two
slots as well as indicating which terms are shared.
This allows, for example, n
2
and n
3
in Derivation
1 in Figure 2 to have the same signature, while the
pairs ?n
2
, u
1
? and ?n
3
, u
1
? have different ones. To
share features across templates, slot and slot-pair
signatures are generated for both the full template,
as well as for each of the constituent equations.
Features The features ?(x, y) are computed for a
derivation y and problem x and cover all deriva-
Document level
Unigrams
Bigrams
Single slot
Has the same lemma as a question object
Is a question object
Is in a question sentence
Is equal to one or two (for numbers)
Word lemma X nearby constant
Slot pair
Dep. path contains: Word
Dep. path contains: Dep. Type
Dep. path contains: Word X Dep. Type
Are the same word instance
Have the same lemma
In the same sentence
In the same phrase
Connected by a preposition
Numbers are equal
One number is larger than the other
Equivalent relationship
Solution Features
Is solution all positive
Is solution all integer
Table 1: The features divided into categories.
tion decisions, including template and alignment
selection. When required, we use standard tools
to generate part-of-speech tags, lematizations, and
dependency parses to compute features.
2
For each
number word in y we also identify the closest noun
in the dependency parse. For example, the noun
for 278 in Derivation 1, Figure 2 would be ?peo-
ple.? The features are calculated based on these
nouns, rather than the number words.
We use four types of features: document level
features, features that look at a single slot entry,
features that look at pairs of slot entries, and fea-
tures that look at the numeric solutions. Table 1
lists all the features used. Unless otherwise noted,
when computing slot and slot pair features, a sep-
arate feature is generated for each of the signature
types discussed earlier.
Document level features Oftentimes the natural
language in x will contain words or phrases which
are indicative of a certain template, but are not as-
sociated with any of the words aligned to slots in
the template. For example, the word ?chemist?
2
In our experiments these are generated using the Stan-
ford parser (de Marneffe et al, 2006)
276
might indicate a template like the one seen in Fig-
ure 4. We include features that connect each tem-
plate with the unigrams and bigrams in the word
problem. We also include an indicator feature for
each system template, providing a bias for its use.
Single Slot Features The natural language x al-
ways contains one or more questions or commands
indicating the queried quantities. For example, the
first problem in Figure 2 asks ?How many children
were admitted on that day?? The queried quanti-
ties, the number of children in this case, must be
represented by an unknown in the system of equa-
tions. We generate a set of features which look at
both the word overlap and the noun phrase overlap
between slot words and the objects of a question or
command sentence. We also compute a feature in-
dicating whether a slot is filled from a word in a
question sentence. Additionally, algebra problems
frequently use phrases such as ?2 kinds of tickets?
(e.g., Figure 2). These numbers do not typically
appear in the equations. To account for this, we
add a single feature indicating whether a number
is one or two. Lastly, many templates contain con-
stants which are identifiable from words used in
nearby slots. For example, in Figure 4 the con-
stant 0.01 is related to the use of ?%? in the text.
To capture such usage, we include a set of lexical-
ized features which concatenate the word lemma
with nearby constants in the equation. These fea-
tures do not include the slot signature.
Slot Pair Features The majority of features we
compute account for relationships between slot
words. This includes features that trigger for
various equivalence relations between the words
themselves, as well as features of the dependency
path between them. We also include features that
look at the numerical relationship of two num-
bers, where the numeric values of the unknowns
are generated by solving the system of equations.
This helps recognize that, for example, the total of
a sum is typically larger than each of the (typically
positive) summands.
Additionally, we also have a single feature look-
ing at shared relationships between pairs of slots.
For example, in Figure 2 the relationship between
?tickets for children? and ?$1.50? is ?cost?. Sim-
ilarly the relationship between ?Adult tickets? and
?$4? is also ?cost?. Since the actual nature of this
relationship is not important, this feature is not
lexicalized, instead it is only triggered for the pres-
ence of equality. We consider two cases: subject-
# problems 514
# sentences 1616
# words 19357
Vocabulary size 2352
Mean words per problem 37
Mean sentences per problem 3.1
Mean nouns per problem 13.4
# unique equation systems 28
Mean slots per system 7
Mean derivations per problem 4M
Table 2: Dataset statistics.
object relationships where the intervening verb
is equal, and noun-to-preposition object relation-
ships where the intervening preposition is equal.
Solution Features By grounding our semantics in
math, we are able to include features which look
at the final answer, a, to learn which answers are
reasonable for the algebra problems we typically
see. For example, the solution to many, but not all,
of the problems involves the size of some set of
objects which must be both positive and integer.
7 Experimental Setup
Dataset We collected a new dataset of alge-
bra word problems from Algebra.com, a crowd-
sourced tutoring website. The questions were
posted by students for members of the community
to respond with solutions. Therefore, the problems
are highly varied, and are taken from real prob-
lems given to students. We heuristically filtered
the data to get only linear algebra questions which
did not require any explicit background knowl-
edge. From these we randomly chose a set of
1024 questions. As the questions are posted to a
web forum, the posts often contained additional
comments which were not part of the word prob-
lems and the solutions are embedded in long free-
form natural language descriptions. To clean the
data we asked Amazon Mechanical Turk workers
to extract from the text: the algebra word prob-
lem itself, the solution equations, and the numeric
answer. We manually verified both the equations
and the numbers to ensure they were correct. To
ensure each problem type is seen at least a few
times in the training data, we removed the infre-
quent problem types. Specifically, we induced the
system template from each equation system, as de-
scribed in Section 4.1, and removed all problems
for which the associated system template appeared
277
less than 6 times in the dataset. This left us with
514 problems. Table 2 provides the data statistics.
Forms of Supervision We consider both semi-
supervised and supervised learning. In the semi-
supervised scenario, we assume access to the nu-
merical answers of all problems in the training cor-
pus and to a small number of problems paired with
full equation systems. To select which problems
to annotate with equations, we identified the five
most common types of questions in the data and
annotated a randomly sampled question of each
type. 5EQ+ANS uses this form of weak supervi-
sion. To show the benefit of using the weakly su-
pervised data, we also provide results for a base-
line scenario 5EQ, where the training data includes
only the five seed questions annotated with equa-
tion systems. In the fully supervised scenario
ALLEQ, we assume access to full equation sys-
tems for the entire training set.
Evaluation Protocol We run all our experiments
using 5-fold cross-validation. Since our model
generates a solution for every problem, we report
only accuracy. We report two metrics: equation
accuracy to measure how often the system gener-
ates the correct equation system, and answer accu-
racy to evaluate how often the generated numerical
answer is correct. When comparing equations, we
avoid spurious differences by canonicalizing the
equation system, as described in Section 6. To
compare answer tuples we disregard the ordering
and require each number appearing in the refer-
ence answer to appear in the generated answer.
Parameters and Solver In our experiments we set
k in our beam search algorithm (Section 5) to 200,
and l to 20. We run the L-BFGS computation for
50 iterations. We regularize our learning objec-
tive using the L
2
-norm and a ? value of 0.1. The
set of mathematical relations supported by our im-
plementation is {+,?,?, /}.Our implementation
uses the Gaussian Elimination function in the Effi-
cient Java Matrix Library (EJML) (Abeles, 2014)
to generate answers given a set of equations.
8 Results
8.1 Impact of Supervision
Table 3 summarizes the results. As expected, hav-
ing access to the full system of equations (ALLEQ)
at training time results in the best learned model,
with nearly 69% accuracy. However, training
from primarily answer annotations (5EQ+ANS)
Equation Answer
accuracy accuracy
5EQ 20.4 20.8
5EQ+ANS 45.7 46.1
ALLEQ 66.1 68.7
Table 3: Cross-validation accuracy results for var-
ious forms of supervision.
Equation Answer % of
accuracy accuracy data
? 10 43.6 50.8 25.5
11? 15 46.6 45.1 10.5
16? 20 44.2 52.0 11.3
> 20 85.7 86.1 52.7
Table 4: Performance on different template fre-
quencies for ALLEQ.
results in performance which is almost 70% of
ALLEQ, demonstrating the value of weakly super-
vised data. In contrast, 5EQ, which cannot use this
weak supervision, performs much worse.
8.2 Performance and Template Frequency
To better understand the results, we also measured
equation accuracy as a function of the frequency
of each equation template in the data set. Table 4
reports results for ALLEQ after grouping the prob-
lems into four different frequency bins. We can
see that the system correctly answers more than
85% of the question types which occur frequently
while still achieving more than 50% accuracy on
those that occur relatively infrequently. We do not
include template frequency results for 5EQ+ANS
since in this setup our system is given only the top
five most common templates. This limited set of
templates covers only those questions in the > 20
bin, or about 52% of the data. However, on this
subset 5EQ+ANS performs very well, answering
88% of them correctly, which is approximately the
same as the 86% achieved by ALLEQ. Thus while
the weak supervision is not helpful in generating
the space of possible equations, it is very helpful
in learning to generate the correct answer when
given an appropriate space of equations.
8.3 Ablation Analysis
Table 5 shows ablation results for each group of
features. The results along the diagonal show the
performance when a single group of features is
ablated, while the off-diagonal numbers show the
278
w/o w/o w/o w/o
pair document solution single
w/o pair 42.8 25.7 19.0 39.6
w/o document ? 63.8 50.4 57.6
w/o solution ? ? 63.6 62.0
w/o single ? ? ? 65.9
Table 5: Cross-validation accuracy results with
different feature groups ablated for ALLEQ. Re-
sults are for answer accuracy which is 68.7% with-
out any features ablated.
performance when two groups of features are ab-
lated together. We can see that all of the features
contribute to the overall performance, and that the
pair features are the most important followed by
the document and solution features. We also see
that the pair features can compensate for the ab-
sence of other features. For example, the perfor-
mance drops only slightly when either the docu-
ment or solution features are removed in isolation.
However, the drop is much more dramatic when
they are removed along with the pair features.
8.4 Qualitative Error Analysis
We examined our system output on one fold of
ALLEQ and identified two main classes of errors.
The first, accounting for approximately one-
quarter of the cases, includes mistakes where
more background or world knowledge might have
helped. For example, Problem 1 in Figure 5 re-
quires understanding the relation between the di-
mensions of a painting, and how this relation is
maintained when the painting is printed, and Prob-
lem 2 relies on understanding concepts of com-
merce, including cost, sale price, and profit. While
these relationships could be learned in our model
with enough data, as it does for percentage prob-
lems (e.g., Figure 4), various outside resources,
such as knowledge bases (e.g. Freebase) or distri-
butional statistics from a large text corpus, might
help us learn them with less training data.
The second category, which accounts for about
half of the errors, includes mistakes that stem from
compositional language. For example, the second
sentence in Problem 3 in Figure 5 could generate
the equation 2x?y = 5, with the phrase ?twice of
one of them? generating the expression 2x. Given
the typical shallow nesting, it?s possible to learn
templates for these cases given enough data, and in
the future it might also be possible to develop new,
cross-sentence semantic parsers to enable better
generalization from smaller datasets.
(1)
A painting is 10 inches tall and 15 inches
wide. A print of the painting is 25 inches
tall, how wide is the print in inches?
(2)
A textbook costs a bookstore 44 dollars,
and the store sells it for 55 dollars. Find
the amount of profit based on the selling
price.
(3)
The sum of two numbers is 85. The dif-
ference of twice of one of them and the
other one is 5. Find both numbers.
(4)
The difference between two numbers is
6. If you double both numbers, the sum
is 36. Find the two numbers.
Figure 5: Examples of problems our system does
not solve correctly.
9 Conclusion
We presented an approach for automatically learn-
ing to solve algebra word problems. Our algorithm
constructs systems of equations, while aligning
their variables and numbers to the problem text.
Using a newly gathered corpus we measured the
effects of various forms of weak supervision on
performance. To the best of our knowledge, we
present the first learning result for this task.
There are still many opportunities to improve
the reported results, and extend the approach to
related domains. We would like to develop tech-
niques to learn compositional models of mean-
ing for generating new equations. Furthermore,
the general representation of mathematics lends it-
self to many different domains including geome-
try, physics, and chemistry. Eventually, we hope
to extend the techniques to synthesize even more
complex structures, such as computer programs,
from natural language.
Acknowledgments
The authors acknowledge the support of Battelle
Memorial Institute (PO#300662) and NSF (grant
IIS-0835652). We thank Nicholas FitzGerald, the
MIT NLP group, the UW NLP group and the
ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
279
References
Peter Abeles. 2014. Efficient java matrix library.
https://code.google.com/p/efficient
-java-matrix-library/.
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics.
Qingqing Cai and Alexander Yates. 2013b. Seman-
tic parsing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Conference
on Lexical and Computational Semantics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of the Confer-
ence on Artificial Intelligence.
David Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of the Conference on Language Re-
sources and Evaluation.
Ralph Grishman, David Westbrook, and Adam Mey-
ers. 2005. NYUs English ACE 2005 System De-
scription. In Proceedings of the Automatic Content
Extraction Evaluation Workshop.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised pcfg induction for grounded language learning
with highly ambiguous supervision. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceeding of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Tao Lei, Fan Long, Regina Barzilay, and Martin Ri-
nard. 2013. From natural language specifications to
program input parsers. In Proceeding of the Associ-
ation for Computational Linguistics.
Iddo Lev, Bill MacCartney, Christopher Manning, and
Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proceed-
ings of the Workshop on Text Meaning and Interpre-
tation. Association for Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extrac-
tion from free text. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Maxima. 2014. Maxima, a computer algebra system.
version 5.32.1.
280
Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelli-
gence Review, 29(2).
Jorge Nocedal and Stephen Wright. 2006. Numeri-
cal optimization, series in operations research and
financial engineering. Springer, New York.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the Conference on Knowledge Dis-
covery and Data Mining.
Cynthia Thompson and Raymond Mooney. 2003.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence
Research, 18(1).
Adam Vogel and Dan Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Annual Meeting
of the North American Chapter of the Association of
Computational Linguistics. Association for Compu-
tational Linguistics.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the Conference on Ar-
tificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial gram-
mars. In Proceedings of the Conference on Uncer-
tainty in Artificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Confer-
ence of the Association for Computational Linguis-
tics and International Joint Conference on Natural
Language Processing.
281
