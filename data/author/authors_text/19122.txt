Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1124?1132, Dublin, Ireland, August 23-29 2014.
Discriminative Language Models as a Tool for
Machine Translation Error Analysis
Koichi Akabe Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
{akabe.koichi.zx8, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose a new method for effective error analysis of machine translation (MT)
systems. In previous work on error analysis of MT, error trends are often shown by frequency.
However, if we attempt to perform a more detailed analysis based on frequently erroneous word
strings, the word strings also often occur in correct translations, and analyzing these correct sen-
tences decreases the overall efficiency of error analysis. In this paper, we propose the use of
regularized discriminative language models (LMs) to allow for more focused MT error analysis.
In experiments, we demonstrate that our method is more efficient than frequency-based analysis,
and examine differences across systems, language pairs, and evaluation measures. 1
1 Introduction
Accuracy of Statistical Machine Translation (SMT) systems is continually increasing, but systems are
now more complex than ever before. As a result, not all effects of making modifications to a system are
known without actually making the modification and generating translations. Therefore, in the process
of developing an SMT system, it is common to evaluate actual translations to identify problems to make
improvements. This process is time consuming, as it is often necessary to analyze a large number of
translations to get an overall grasp of the system?s error trends. In addition, many sentences will contain
no errors, or only errors from the long tail that are not representative of the system as a whole. On the
other hand, if we are able to detect and rank important errors automatically, we will likely be able to find
representative errors of the SMT system more efficiently.
Previous work has proposed methods for automatic error analysis of MT systems based on automati-
cally separating errors into classes and sorting these classes by frequency (Vilar et al., 2006; Popovic and
Ney, 2011). These classes cover common mistakes of MT systems, e.g. conjugation, reordering, word
deletion, and insertion. This makes it possible to view overall error trends, but when the goal of analysis
is to identify errors to make some concrete improvement to the system, it is often necessary to perform a
more focused analysis, looking at actual errors made by a particular language pair or system. We show
examples of errors types that are informative, but are language- or task-specific, and not covered by pre-
vious methods in Figure 1. In this example, the type given by more standard error typologies is indicated
by ?Traditional type,? but we would prefer a more detailed analysis such as ?Fine-grained type,? would
allows us to take specific steps to fix the machine translation system (such as ensuring that Wikipedia
titles are not punctuated, or normalizing full-width characters to half-width). These fine-grained types
are difficult to conceive without actually observing the MT system output, but if we are able to group ac-
tual errors into fine-grained classes based on, for example, lexical clues, this sort of analysis will become
possible and more efficient.
Previous research on improving the efficiency of error analysis has generally focused on grouping error
types by frequency, but try to apply such frequency-based techniques to individual errors, selected errors
1Our implementation is available open-source at https://github.com/vbkaisetsu/dlm-analyzer
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1124
Src ?? ?? ??
Ref the academic exchange agreement
MT academic exchange agreement .
Traditional type Insertion error
Fine-grained type Insertion error (unneeded period)
Src ?? ? ?? ? ???? ? ???? ? ? ?? ?? ?
Ref prince kakugyoho -lrb- 1075 - 1105 -rrb- ninna-ji monzeki
MT imperial prince kakugyo -lrb-???? - 1105 -rrb- : ninna-ji temple ruins
Traditional type Replacement error or Unknown word
Fine-grained type Unknown word (number) or Half-/Full-width error
Figure 1: Example of errors in Japanese to English translation, classified into traditional, or more fine-
grained and useful classes.
1-gram 2-gram
the 61 (BOS) the 42
, 47 . (EOS) 41
and 43 , and 32
of 42 of the 27
: 42 in the 21
Table 1: Frequently occurring erroneous n-grams
are often dominated by frequently occurring linguistic phenomena that are not necessarily indicative
of translation errors. To show examples of this problem, in Table 1 we provide a list of erroneous n-
grams that were produced by an MT system (described in Section 4.1) but not contained in the respective
references. From this table, we can see that frequently occurring erroneous n-grams are simply n-grams
that frequently occur in English, and because of this we cannot discover characteristic errors of the system
for improvement just from this information.
In this paper, we propose a new method that uses regularized discriminative LMs to solve the above
problem. Discriminative LMs are LMs trained to fix common output errors of a particular system. From
the viewpoint of error analysis, if we train a discriminative LM using n-gram features and examine the
weights learned by this model, n-grams with large negative or positive weights will be indicative of pat-
terns that are over- or under-produced by the MT system. Because the weights are specifically trained to
fix errors, it is likely that these patterns will be more informative than mistakes that are simply frequently
occurring. We can also use a number of features of discriminative LMs to perform a more focused and
efficient analysis. For example, if we perform training with L1 regularization, many features will be
removed and only important patterns will remain in the model. Additionally, we can focus on specific
varieties of errors by changing the evaluation measure used for training the LMs.
In our experiments, we validate the effectiveness of error analysis based on discriminative LMs. We
perform a manual evaluation of the n-gram patterns discovered by random selection, by frequency-based
analysis, and by the proposed method. As a result, the proposed method is more effective at identifying
errors than other methods.
2 Discriminative Language Models
In this section, we first introduce the discriminative LM used in our method. As a target for our analysis,
we have input sentences F = {F
1
, . . . , F
K
}, n-best outputs ?E = { ?E
1
, . . . ,
?
E
K
} of an MT system, and
reference translations R = {R
1
, . . . , R
K
}. Discriminative LMs define feature vectors ?(E
i
) for each
candidate in ?E
k
= {E
1
, E
2
, . . . , E
I
}, and calculate inner products w ? ?(E
i
) as scores.
To train the weight vector w, we first calculate evaluation scores of all candidates using a sentence-
level evaluation measure EV such as BLEU+1 (Lin and Och, 2004) given the reference sentence R
k
.
1125
We choose the sentence with the highest evaluation EV as an oracle E?
k
. Oracles are chosen for each
n-best, and we train w so that the oracle?s score becomes higher than the other candidates.
2.1 Structured Perceptron
While there are a number of methods for training discriminative LMs, we follow Roark et al. (2007)
in using the structured perceptron as a simple and effective method for LM training. The structured
perceptron is a widely used on-line learning method that examines one training instance and updates
the weight vector using the difference between feature vectors generated from the oracle E? and the
hypothesis ?E calculated by the current model. For each iteration, w is updated using the difference
between E? and ?E. If ?E is equal to E?, the difference becomes 0, so no update is performed. This
process is run for all F sequentially, and iterated until weights converge or we reach a fixed iteration
limit N . We show the above procedure in Algorithm 1.
Algorithm 1 Structured perceptron training of the discriminative LM
for n = 1 to N do
for all ?E ? ?E do
E
?
? arg max
E?
?E
EV (E)
?
E ? arg max
E?
?E
w ? ?(E)
w ? w + ?(E
?
)? ?(
?
E)
end for
end for
2.2 Learning Sparse Discriminative LMs
While the structured perceptron is a simple and effective method for learning discriminative LMs, it also
has no bias towards reducing the number of features used in the model. However, if we add a bias towards
learning smaller models, we can keep only salient features (Tsuruoka et al., 2009).
In our work, we use L1 regularization to add this bias. L1 regularization gives a penalty to w pro-
portional to the L1 norm ?w?
1
=
?
i
|w
i
|, pushing a large number of elements in w to 0, so ineffective
features are removed from the model.
To train L1 regularized discriminative LMs, we use the forward-backward splitting (FOBOS) algo-
rithm proposed by Duchi and Singer (2009). FOBOS splits update and regularization, and lazily calcu-
lates the regularization upon using the weight to improve efficiency.
2.3 Features of Discriminative LMs
In the LM, we used the following three features:
1. System score feature ?
s
: As our goal is fixing the output of the system, we add this feature to allow
a default ordering of n-bests by score.
2. n-gram feature?
n
: We add a binary feature counting the frequency of eachn-gram in the hypothesis.
The weights of these features will be the main target of our analysis.
3. Hypothesis length feature ?
l
: If the evaluation measure has a penalty for the number of words, this
allows us to adjust it.
In this work, we do not use other features, but our method theoretically allows for addition of other
features such as POS tags or syntactic information, which could also potentially be used as a target for
analysis.
3 Discriminative LMs for Error Analysis
In this section, we describe how to incorporate information from discriminative LMs into manual error
analysis.
1126
Error types
Replacement (Context dependent)
(Context independent)
Insertion
Deletion
Reordering
Conjugation
Polarity
Unknown words
Table 2: Error categories for annotation
Src ? ??? ? ? ?? ?
Ref kyo-chan -lrb- city bus -rrb-
MT <s> kyoto chan -lrb- kyoto city bus -rrb- </s>
Rules SYMP ( x0:SYM SYMP ( NP ( NN ( ??? ) NN ( ???? ) ) x1:SYM ) )?
x0 ?kyoto? ?city? ?bus? x1
Eval Insertion error
Src ?? ?? ?? 13 ?
Ref there are 13 open patents .
MT <s> the number of public patent 13 cases </s>
Rules NP ( NP ( x0:NN x1:NN ) NN ( ???? ) )? ?number? ?of? x0 x1
NN ( ???? )? ?public?
Eval Context-dependent replacement error
Figure 2: Example of the evaluation sheet. Boxed words are chosen n-grams.
3.1 Focused Error Analysis of MT output
We first define the following general framework for focused analysis of errors in MT output. Using this,
we can find error trends of chosen n-grams:
1. Automatically choose potentially erroneous n-grams in the MT output.
2. Select one or more 1-best translations that contain each chosen n-gram.
3. Show selected translations to an annotator with the selected n-gram highlighted.
4. The annotator looks at the indicated n-gram, and marks whether or not by examining the n-gram
whether they were able to identify an error in the MT output. If the answer is ?yes,? the annotator
additionally indicates which variety of error was found according to Table 2.
A part of the actual evaluation sheet is shown in Fig. 2. The first four rows are the input, and the final
row is the annotator?s evaluation.
3.2 Selection of Target n-grams
We can think of the following three methods for choosing potentially erroneous n-grams:
Random: n-grams that are selected randomly. This corresponds to the standardmethod of error analysis,
where sentences are randomly sampled and analyzed.
1127
Sent Words
English Japanese
Train 330k 5.91M 6.09M
Dev 1166 24.3k 26.8k
Test 1160 26.7k 28.5k
Table 3: Data size of KFTT
Frequency: n-grams that are most frequently over-generated (occur in the hypothesis, but not in the
references). This corresponds to a focused version of the frequency-based automatic error analysis
methods of Vilar et al. (2006) and Popovic and Ney (2011).
LM: n-grams that have the lowest weight according to the discriminative LM. This is our proposed
method.
In particular, for discriminative LMs, n-gram features that have large positive or negative weights
indicate n-grams that are under-generated or over-generated by the system. Therefore, by examining
high-weighted or low-weighted n-grams, it is likely that we will be able to get a grasp of the system
mistakes. When performing actual evaluation, we want to analyze n-grams with 1-best translations.
Almost high-weighted n-grams are only contained in oracle translations, and not contained in 1-best
translation. Therefore, we use low-weighted n-grams for evaluation. If the discriminative LM is properly
trained, low-weighted n-grams will often correspond to actual errors.
3.3 System Comparison
When developing MT systems, it is common to not only evaluate a single system, but also compare
multiple systems, such as when comparing a new system with baselines.
To do this in the current work, we create discriminative LMs from n-bests generated by multiple
translation systems, and choose representative n-grams using the proposed method. Then we examine
the selected n-grams in context and then compare the result of this analysis.
4 Experiments
We evaluate the effectiveness of our method by performing a manual evaluation over three translation
systems, two translation directions, and two evaluation measures.
4.1 Experiment Setup
For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size
of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using
the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the
above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system
built using Moses (Koehn et al., 2007).
The f2s system is built using Nile2 for making word alignments, and syntax trees generated with
Egret3. pbmt andhiero are built usingGIZA++ (Och andNey, 2003) for word alignments. Each system
is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For
single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al.,
2010) as additional metric for training the discriminative LM.
For training discriminative LMs, our method uses the structured perceptron with 100 iterations and
FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the
range 10?6-10?2 to give the highest performance on the KFTT test data.
LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use
1-grams to 3-grams as n-gram features.
2http://code.google.com/p/nile/
3http://code.google.com/p/egret-parser/
1128
System BLEU(dev) BLEU(test)
Original LM applied Original LM applied
pbmt 0.2929 0.3521 0.2460 0.2485
hiero 0.2953 0.3859 0.2616 0.2562
f2s 0.2958 0.3887 0.2669 0.2676
Table 4:  Translation accuracy of each system, without LMs and with LMs
Method Ja? En En? Ja
Random 0.46 0.37
Frequency 0.30 0.31
LM 0.55 0.48
Table 5: Precision of top 30 n-grams that select errors in both directions
We show translation accuracies of each system before and after training in Table 4. From this table, we
can see that the LM increases the accuracy of all dev data, but it does not necessarily have a large effect
for the test data. The main reason for this is because the development set used to train the LM is relatively
small, at only 1166 sentences. However, as our goal in this paper is to perform error analysis on set of
data which we already have parallel references (in this case, the development set), the generalization
ability of the model is not necessarily fundamental to our task at hand. We directly identify the ability to
identify errors in the next section.
4.2 Evaluation of Error Identification Ability
This section evaluates the ability of our method to identify errors in MT output. As we are proposing
our method as a tool for manual analysis of MT output, it is necessary to perform manual evaluation to
ensure that our method is identifying locations that are actually erroneous according to human subjective
evaluation. To measure the accuracy of each method, we perform an evaluation as described in Section
3.1 and use the precision of selectedn-grams (the percentage of selectedn-grams for which then annotator
indicated that an error actually existed) as our evaluation measure. The annotator is an MT specialist who
is proficient in English and Japanese. The order of the evaluation sentences is shuffled so the annotator
can not determine which method was responsible for choosing each n-gram.
0 10 20 30 40 50 60 70 80 90 1000.0
0.5
1.0
# of selected n-grams
Precisi
on
FrequencyLMRandom
Figure 3: Precision of n-grams that select errors (Japanese to English)
We show the precision results for each number of selected n-grams over three methods for Japanese-
English translation in Fig. 3, and the precision of the top 30 n-grams in both directions in Table 5. From
1129
n-gram Weight Examples
-rrb- of -7.50950 Src ?? ?? ? ?? ? ? ? ? ? ? ??? ?? ? ? ? ?? ? ????
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ?
Ref his achievements were evaluated by emperor go-daigo , and he was awarded the
letter -lrb- ? -rrb- , which came from the emperor ?s real name takaharu -lrb-
?? -rrb- , so he changed the letter in his name from ??? ? to ??? ? .
MT <s> it is regarded as a valor in the fall of the bakufu , and was the first character of
takaharu , imina -lrb-???? -rrb- of emperor godaigo , and changed his name
to takauji . </s>
Eval Reordering error
<s> the first -6.55510 (Only contained in other candidates in n-bests)
senior -6.52024 Src ?? ? ? ?? ?? ? ?? ? ? ? ?? ?? ? ?? ?? ?? ? ?
?? ?
Ref kyoryukai-this organization consists of teachers of junior high , high , and other
schools who are ryukoku university graduates .
MT <s> graduates of?? association - ryukoku university , and is a organization con-
sisting of teachers such as senior . </s>
Eval Context independent replacement error
the ko clan -6.52021 Src ?? ? ? ? ? ? ?? ? ? ? ??? ? ?? ? ?? ? ? ? ? ?
? ? ?
Ref in this fighting , takewakamaru , the son of takauji ?s concubine , was killed .
MT <s> on this occasion , was killed during the confusion??? , the son of a concu-
bine of the ko clan . </s>
Eval Context dependent replacement error
foundation of -6.50773 Src ?? ?? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ?? ? ? ? ?
? ? ? ?? ? ?? ?
Ref the family name comes from the fact that the kujo family lived in kujo-den , which
was located in kyoto kujo and said to have been built by fujiwara no mototsune .
MT <s> the origin of the family name that lived in kujo dono , which was located in
kyoto kujo is said to be a foundation of fujiwara no mototsune . </s>
Eval Context dependent replacement error
Table 6: Top 5 erroneous n-grams learned by the discriminative LM and examples. Boxes on MT indi-
cates the selected n-gram, and boxes in Src and Ref indicate the corresponding words.
these results, we can see that each method is able to detect erroneous n-grams, but the proposed method
achieves a precision that outperforms other methods.
To demonstrate why this is the case, in Table 6 we show examples, in context, of potentially erroneous
n-grams chosen by our proposed method. Compared to the baseline n-grams in Table 1, we can see that
these n-grams are not limited to frequently occurring n-grams in English, and are more likely to have a
high probability of indicating actual errors.
In addition, to give a better idea of the prominence of the selected n-grams, in Table 7, we show
the mean number of locations of the KFTT test data that contain the top 100 n-grams selected by each
method. We can see that randomly selected n-grams are rarely contained in the separate test set, while
the proposed method tends to select n-grams that are more frequent than random, and thus have a better
chance of generalizing.
4.3 Effect of Evaluation Measure Choice
We can also hypothesize that by varying the evaluation measure used in training the LM, we can select
different varieties of errors for analysis. To test this, we compare analysis results obtained using one
1130
Method Ja? En En? Ja
Random 1.1 1.5
Frequency 381.0 432.6
LM 6.2 14.0
Table 7: Mean number of occurrences of selected n-grams in the test set
Type +BLEU +RIBES
Actual Error 0.55 0.41
Replacement (Context dependent) 0.36 0.30
(Context independent) 0.15 0
Insertion 0.17 0.25
Deletion 0.18 0.10
Reordering 0.14 0.27
Conjugation 0 0.08
Polarity 0 0
Unknown words 0 0
Table 8: Error statistics found when optimizing different metrics. Bold indicates the higher score.
LM optimized with BLEU and another with RIBES, which is a reordering-oriented evaluation metric.
We show a breakdown of the identified errors in Table 8. From this table, we can see that the BLEU-
optimized LM is able to detect more deletion errors than the RIBES-optimized LM. This is a natural result,
as the BLEU metric puts a heavier weight on the brevity penalty assigned to shorter translations. On the
other hand, the RIBES-optimized LM detects more reordering errors than the BLEU-optimized LM. The
RIBES metric is sensitive to reordering errors, and thus reordering errors will cause larger decreases in
RIBES. From this experiment, we can see that it is possible to focus on different error types by using
different metrics in the optimization of the LM.
4.4 Result of System Comparison
Finally, we examine whether discriminative LMs allow us to grasp characteristic errors for system com-
parison. Similarly with single system analysis, we generated the top 30 potentially erroneous n-grams
for pbmt, hiero, and f2s in two directions, and evaluated them manually. The result is listed in Table
9. From this table, we can see that pbmt and hiero count reordering errors as one of the three most
frequent types, while f2s does not, especially for English to Japanese. This is consistent with common
knowledge that syntactic information can be used to improve reordering accuracy. We can also see in-
sertion is a problem when translating into English, and conjugation is a problem when translating into
morphologically-rich Japanese. While these are only general trends, they largely match with intuition,
even after analysis of only the top 30 n-grams.
5 Conclusion
In this paper, we proposed a new method for efficiently analyzing the output of MT systems using L1
regularized discriminative LMs, and evaluate its effectiveness. As a result, weights trained by discrim-
inative LMs are more effective at identifying errors than n-grams chosen either randomly or by error
frequency. This indicates that our method allows an MT system engineer to inspect fewer sentences in
the course of identifying characteristic errors of the MT system.
The overall framework of using discriminative LMs in error analysis opens up a number of directions
for future work, and there are a number of additional points we plan to analyze in the future. For example,
while it is clear that the proposedmethod allows errors to be identifiedmore efficiently, it is still necessary
to quantify the overall benefit of having an MT expert use the result of this error analysis to improve
1131
Type Ja? En En? Ja
pbmt hiero f2s pbmt hiero f2s
Actual Error 0.58 0.60 0.55 0.81 0.64 0.48
Replacement (Context dependent) 0.41 0.33 0.36 0.10 0.17 0.52
(Context independent) 0.03 0.08 0.15 0.55 0.03 0.12
Insertion 0.26 0.22 0.17 0.06 0.13 0.15
Deletion 0.10 0.09 0.18 0.07 0.14 0.06
Reordering 0.13 0.28 0.14 0.19 0.32 0.04
Conjugation 0.07 0 0 0.04 0.20 0.12
Polarity 0 0 0 0 0.01 0
Unknown words 0 0 0 0 0 0
Table 9: Error statistics of three systems with in both directions. Bold scores are the top 3 most occuring
error types in each system.
an MT system. In addition, we plan on examining the effect of using larger training data for the LM,
incorporating different features based on POS patterns or syntactic features, and using more sophisticated
training methods.
References
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. In
Journal of Machine Learning Research, volume 10.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation
of translation quality for distant language pairs. In Proc. EMNLP, pages 944?952.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177?180.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for
machine translation. In Proc. COLING, pages 501?507.
Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.
Graham Neubig. 2013. Travatar: A forest-to-string machine translation engine based on tree transducers. In Proc.
ACL.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation
of machine translation. In Proc. ACL, pages 311?318.
Maja Popovic and Hermann Ney. 2011. Towards automatic error analysis of machine translation output. In
Computational Linguistics, pages 657?688.
Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer
Speech & Language, 21(2):373?392.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-
regularized log-linear models with cumulative penalty. In Proc. ACL, pages 477?485.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney. 2006. Error analysis of statistical machine trans-
lation output. In Proc. LREC, pages 697?702.
1132
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1706?1717, Dublin, Ireland, August 23-29 2014.
Reinforcement Learning of Cooperative Persuasive Dialogue Policies
using Framing
Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
Nara Institute of Science and Technology (NAIST), Nara, Japan
{takuya-h,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
Abstract
In this paper, we apply reinforcement learning for automatically learning cooperative persuasive
dialogue system policies using framing, the use of emotionally charged statements common in
persuasive dialogue between humans. In order to apply reinforcement learning, we describe a
method to construct user simulators and reward functions specifically tailored to persuasive dia-
logue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate
the learned policy and the effect of framing through experiments both with a user simulator and
with real users. The experimental evaluation indicates that applying reinforcement learning is
effective for construction of cooperative persuasive dialogue systems which use framing.
1 Introduction
With the basic technology supporting dialogue systems maturing, there has been more interest in recent
years about dialogue systems that move beyond the traditional task-based or chatter bot frameworks. In
particular there has been increasing interest in dialogue systems that engage in persuasion or negotiation
(Georgila and Traum, 2011; Georgila, 2013; Paruchuri et al., 2009; Heeman, 2009; Mazzotta and de
Rosis, 2006; Mazzotta et al., 2007; Nguyen et al., 2007; Guerini et al., 2003). We concern ourselves
with cooperative persuasive dialogue systems (Hiraoka et al., 2013), which try to satisfy both the user
and system goals. For these types of systems, creating a system policy that both has persuasive power
and is able to ensure that the user is satisfied is the key to the system?s success.
In recent years, reinforcement learning has gained much attention in the dialogue research community
as an approach for automatically learning optimal dialogue policies. The most popular framework for
reinforcement learning in dialogue models is based on Markov decision processes (MDP) and partially
observable Markov decision processes (POMDP). In these frameworks, the system gets a reward repre-
senting the degree of success of the dialogue. Reinforcement learning enables the system to learn a policy
maximizing the reward. Traditional reinforcement learning requires thousands of dialogues, which are
difficult to collect with real users. Therefore, a user simulator which simulates the behavior of real users
is used for generating training dialogues. Most research in reinforcement learning for dialogue system
policies has been done in slot-filling dialogue, where the system elicits information required to provide
appropriate services for the user (Levin et al., 2000; Williams and Young, 2007).
There is also ongoing research on applying reinforcement learning to persuasion and negotiation
dialogues, which are different from slot-filling dialogue (Georgila and Traum, 2011; Georgila, 2013;
Paruchuri et al., 2009; Heeman, 2009). In slot-filling dialogue, the system is required to perform the
dialogue to achieve the user goal, eliciting some information from a user to provide an appropriate ser-
vice. A reward corresponding to the achievement of the user?s goal is given to the system. In contrast,
in persuasive dialogue, the system convinces the user to take some action achieving the system goal.
Thus, in this setting, reward corresponding to the achievement of both the user?s and the system?s goal is
given to the system. The importance of each goal will vary depending on the use case of the system. For
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1706
example, a selfish system could be rewarded with an emphasis on only achievement of the system goal,
and a cooperative system could be rewarded with an emphasis on achievement of both of the goals. In
addition, negotiation dialogue could be considered as a kind of the persuasive dialogue where the user
also tries to convince the system to achieve the user?s goal.
In this paper, our research purpose is learning better policies for cooperative persuasive dialogue sys-
tems using framing. We focus on learning a policy that tries to satisfy both the user and system goals. In
particular, two elements in this work set it apart from previous works:
? We introduce framing (Irwin et al., 2013), which is known to be important for persuasion and a
key concept of this paper, as a system action. Framing uses emotionally charged words to explain
particular alternatives. In the context of research that applies reinforcement learning to persuasive
(or negotiation) dialogue, this is the first work that considers framing as a system action.
? We use a human-to-human persuasive dialogue corpus of Hiraoka et al. (2014) to train predictive
models for achievement of a human persuadee?s and a human persuader?s goals, and introduce these
models to reward calculation to enable the system to learn a policy reflecting knowledge of human
persuasion.
To achieve our research purpose, we construct a POMDP where the reward function and user simulator
are learned from a corpus of human persuasive dialogue. We define system actions based on framing and
general dialogue acts. In addition, the system dialogue state (namely, belief state) is defined for tracking
the system?s rewards. Then, we evaluate the effect of framing and learning a system policy. Experimental
evaluation is done through a user simulator and real users.
2 Reinforcement learning
Reinforcement learning is a machine learning technique for learning a system policy. The policy is a
mapping function from a dialogue state to a particular system action. In reinforcement learning, the
policy is learned by maximizing the reward function. Reinforcement learning is often applied to models
based on the framework of MDP or POMDP.
In this paper, we follow a POMDP-based approach. A POMDP is defined as a tuple
?S,A, P,R,O,Z, ?, b
0
? where S is the set of states (representing different contexts) which the system
may be in (the system?s world), A is the set of actions of the system, P : S ?A ? P (S,A) is the set of
transition probabilities between states after taking an action, R : S?A ? ? is the reward function, O is
a set of observations that the system can receive about the world, Z is a set of observation probabilities
Z : S ? A ? Z(S,A), and ? a discount factor weighting longterm rewards. At any given time step i
the world is in some unobserved state s
i
? S. Because s
i
is not known exactly, we keep a distribution
over states called a belief state b, thus b(s
i
) is the probability of being in state s
i
, with initial belief state
b
0
. When the system performs an action ?
i
? A based on b, following a policy pi : S ? A, it receives
a reward r
i
(s
i
, ?
i
) ? ? and transitions to state s
i+1
according to P (s
i+1
|s
i
, ?
i
) ? P . The system then
receives an observation o
i+1
according to P (o
i+1
|s
i+1
, ?
i
). The quality of the policy pi followed by the
agent is measured by the expected future reward also called Q-function, Q
pi
: S ?A ? ?.
In this framework, it is critical to be able to learn a good policy function. In order to do so, we use
Neural fitted Q Iteration (Riedmiller, 2005) for learning the system policy. Neural fitted Q Iteration is an
offline value-based method, and optimizes the parameters to approximate the Q-function. Neural fitted
Q Iteration repeatedly performs 1) sampling training experience using a POMDP through interaction and
2) training a Q-function approximator using training experience. Neural fitted Q Iteration uses a multi-
layered perceptron as the Q-function approximator. Thus, even if the Q-function is complex, Neural
fitted Q Iteration can approximate the Q-function better than using a linear approximation function
1
.
3 Persuasive dialogue corpus
In this section, we give a brief overview of Hiraoka et al. (2014)?s persuasive dialogue corpus between
human participants that we will use to estimate the models described in later sections.
1
In a preliminary experiment, we found that Neural fitted Q Iteration had high performance compared to using the linear
approximation of the Q-function in this domain.
1707
Table 1: The beginning of a dialogue from the cor-
pus (translated from Japanese)
Speaker Transcription GPF Tag
Cust Well, I am looking for a camera, PROPQ
do you have camera B?
Sales Yes, we have camera B. ANSWER
Sales Did you already take a look at
it somewhere? PROPQ
Cust Yes. On the Internet. ANSWER
Sales It is very nice. Don?t you think? PROPQ
Cust Yes, that?s right, yes. INFORM
Table 2: Sytem and user?s GPF tags
Inform Answer Question PropQ
SetQ Commisive Directive
Table 3: An example of positive framing
(Camera A is) able to achieve performance
of comparable single-lens cameras
and can fit in your pocket, this is a point.
3.1 Outline of persuasive dialogue corpus
As a typical example of persuasive dialogue, the corpus consists of dialogues between a salesperson
(persuader) and customer (persuadee). The salesperson attempts to convince the customer to purchase a
particular product (decision) from a number of alternatives (decision candidates). This type of dialogue
is defined as ?sales dialogue.? More concretely, the corpus assumes a situation where the customer is in
an appliance store looking for a camera, and the customer must decide which camera to purchase from 5
alternatives.
Prior to recording, the salesperson is given the description of the 5 cameras and instructed to try to
convince the customer to purchase a specific camera (the persuasive target). In this corpus, the persuasive
target is camera A, and this persuasive target is invariant over all subjects. The customer is also instructed
to select one preferred camera from the catalog of the cameras, and choose one aspect of the camera that
is particularly important in making their decision (the determinant). During recording, the customer and
the salesperson converse and refer to the information in the camera catalog as support for their dialogues.
The customer can close the dialogue whenever they want, and choose to buy a camera, not buy a camera,
or reserve their decision for a later date.
The corpus includes a role-playing dialogue with participants consisting of 3 salespeople from 30 to
40 years of age and 19 customers from 20 to 40 years of age. All salespeople have experience working
in an appliance store. The total number of dialogues is 34, and the total time is about 340 minutes. Table
1 show an example transcript of the beginning of one dialogue. Further examples are shown in Table 8
in the appendix.
3.2 Annotated dialogue acts
Each utterance is annotated with two varieties of tags, the first covering dialogue acts in general, and the
rest covering framing.
As a tag set to represent traditional dialogue acts, we use the general-purpose functions (GPF) defined
by the ISO international standard for dialogue act annotation (ISO24617-2, 2010). All annotated GPF
tags are defined to be one of the tags in this set (Table 2).
More relevant to this work is the framing annotation. Framing uses emotionally charged words to
explain particular alternatives. It has been suggested that humans generally evaluate decision candidates
by selecting based on several determinants weighted by the user?s preference, and that framing is an
effective way of increasing persuasive power. This corpus focuses on negative/positive framing (Irwin
et al., 2013; Mazzotta and de Rosis, 2006), with negative framing using negative words and positive
framing using positive words.
In the corpus, framing is defined as a tuple ?a, p, r? where a represents the target alternative, p takes
value NEG if the framing is negative, and POS if the framing is positive, and r represents whether the
framings contains a reference to the persuadees preferred determinant (for example, the performance or
price of a camera), taking the value TRUE if contained, and FALSE if not contained. The user?s preferred
determinant is annotated based on the results of a questionnaire.
Table 3 shows an example of positive framing (p=POS) about the performance of Camera A (a=A). In
this example, the customer answered that his preference is the price of camera, and this utterance does
1708
Figure 1: Dynamic Bayesian network of the user simulator. Each node represents a variable, and each
edge represents a probabilistic dependency. The system cannot observe the shaded variables.
not contain any description of price. Thus, r=NO is annotated. Further examples of positive and negative
framing are shown in Tables 9 and 10 in the appendix.
In this paper, we re-perform annotation of the framing tags and evaluate inter-annotator agreement,
which is slightly improved from Hiraoka et al. (2014). Two annotators are given the description and
examples of tags (e.g. what a positive word is), and practice with these manuscripts prior to annotation.
In corpus annotation, at first, each annotator independently chooses the framing sentences. Then, framing
tags are independently annotated to all utterances chosen by the two annotators. The inter-annotator
agreement of framing polarity is 96.9% (kappa=0.903).
4 User simulator
In this section, we describe a statistical dialogue model for the user (customer in Section 3). This model
is used to simulate the system?s conversational partner in applying reinforcement learning.
The user simulator estimates two aspects of the conversation:
1. The user?s general dialogue act.
2. Whether the preferred determinant has been conveyed to the user (conveyed preferred determinant;
CPD).
The users? general dialogue act is represented by using GPF. For example, in Table 1, PROPQ, ANSWER,
and INFORM appear as the user?s dialogue act. In our research, the user simulator chooses one GPF
described in Table 2 or None representing no response at each turn. CPD represents that the user
has been convinced that the determinant in the persuader?s framing satisfies the user?s preference. For
example, in Table 3, the ?performance? is contained in the clerk?s positive framing for camera A. If the
persuadee is convinced that the decision candidate satisfies his/her preference based on this framing,
we say that CPD has occurred (r=YES)
2
. In our research, the user simulator models CPD for each of
the 5 cameras. This information is required to calculate reward described in the following Section 5.1.
Specifically, GPF and CPD are used for calculating naturalness and persuasion success, which are part
of the reward function.
The user simulator is based on an order one Markov chain, and Figure 1 shows its dynamic Bayesian
network. The user?s GPF G
t+1
user
and CPD C
t+1
alt
at turn t + 1 are calculated by the following equations.
P (G
t+1
user
|G
t
user
, F
t
sys
, G
t
sys
, S
alt
) (1)
P (C
t+1
alt
|C
t
alt
, F
t
sys
, G
t
sys
, S
alt
) (2)
G
t
sys
represents the system GPF at time t. F
t
sys
represents the system framing at t. These two variables
correspond to system actions, and are explained in Section 5.2. G
t
user
represents the user?s GPF at t.
C
t
alt
represents the CPD at t. S
alt
represents the users?s original evaluation of the alternatives. In our
2
Note that the persuader does not necessarily know if r=YES because the persuader is not certain of the user?s preferred
determinants.
1709
research, this is the camera that the user selected as a preferred camera at the beginning of the dialogue
3
.
We use the persuasive dialogue corpus described in Section 3 for training the user simulator, considering
the customer in the corpus as the user and the salesperson in the corpus as the system. In addition, we
use logistic regression for learning Equations (1) and (2).
5 Learning cooperative persuasion policies
Now that we have introduced the user model, we describe the system?s dialogue management. In par-
ticular, we describe the reward, system action, and belief state, which are required for reinforcement
learning.
5.1 Reward
We follow Hiraoka et al. (2014) in defining a reward function according to three factors: user satisfac-
tion, system persuasion success, and naturalness. As described in Section 1, we focus on developing
cooperative persuasive dialogue systems. Therefore, the system must perform dialogue to achieve both
the system and user goals. In our research, we define three elements of the reward function as follows:
Satisfaction The user?s goal is represented by subjective user satisfaction. The reason why we use
satisfaction is that the user?s goal is not necessarily clear for the system (and system creator) in
persuasive dialogue. For example, some users may want the system to recommend appropriate
alternatives, while some users may want the system not to recommend, but only give information
upon the user?s request. As the goal is different for each user, we use abstract satisfaction as a
measure, and leave it to each user how to evaluate achievement of the goal.
Persuasive success The system goal is represented by persuasion success. Persuasion success represents
whether the persuadee finally chooses the persuasive target (in this paper, camera A) at the end of
the dialogue. Persuasion success takes the value SUCCESS when the customer decides to purchase
the persuasive target at the end of dialogue, and FAILURE otherwise.
Naturalness In addition, we use naturalness as one of the rewards. This factor is known to enhance the
learned policy performance for real users (Meguro et al., 2011).
The reward at each turn t is calculated with the following equation
4
.
r
t
= (Sat
t
user
+ PS
t
sys
+ N
t
)/3 (3)
Sat
t
user
represents a 5 level score of the user?s subjective satisfaction (1: Not satisfied?3: Neutral?
5: Satisfied) at turn t scaled into the range between 0 and 1. PS
t
sys
represents persuasion success (1:
SUCCESS?0: FAILURE) at turn t. N
t
represents bi-gram likelihood of the dialogue between system and
user at turn t as follows.
N
t
= P (F
t
sys
, G
t
sys
, G
t
user
|F
t?1
sys
, G
t?1
sys
, G
t?1
user
) (4)
In our research, Sat and PS are calculated with a predictive model constructed from the human per-
suasion dialogue corpus described in Section 3. In constructing these predictive models, the persuasion
results (i.e. persuasion success and persuadee?s satisfaction) at the end of dialogue are given as the su-
pervisory signal, and the dialogue features in Table 4 are given as the input. In the reward calculation,
the dialogue features used by the predictive model are calculated by information generated from the dia-
logue of the user simulator and the system. Table 4 shows all features used for reward calculation at each
turn
5
. Note that, for the calculating TOTAL TIME, average speaking time corresponding to speakers and
dialogue acts is added at each turn.
3
Preliminary experiments indicated that the user behaved differently depending on the first selection of the camera, thus we
introduce this variable to the user simulator.
4
We also optimized the policy in the case where the reward (Equation (3)) is given only when dialogue is closed. However,
the convergence of the learning was much longer, and the performance was relatively bad.
5
Originally, there are more dialogue features for the predictive model. However as in previous research, we choose signifi-
cant dialogue features by step-wise feature selection (Terrell and Bilge, 2012).
1710
Table 4: Features for calculating reward. These
features are also used as the system belief state.
Sat
user
Frequency of system commisive
Frequency of system question
PS
sys
Total time
C
alt
(for each 6 cameras)
S
alt
(for each 6 cameras)
N System and user current GPF
System and user previous GPF
System framing
Table 5: System framing. Pos represents positive
framing and Neg represents negative framing. A, B,
C, D, E represent camera names.
Pos A Pos B Pos C Pos D Pos E None
Neg A Neg B Neg C Neg D Neg E
Table 6: System action.
<None, ReleaseTurn> <None, CloseDialogue>
<Pos A, Inform> <Pos A, Answer>
<Neg A, Inform> <Pos B, Inform>
<Pos B, Answer> <Pos E, Inform>
<None, Inform> <None, Answer>
<None, Question> <None, Commissive>
<None, Directive>
5.2 Action
The system?s action ?F
sys
, G
sys
? is a framing/GPF pair. These pairs represent the dialogue act of the
salesperson, and are required for reward calculation (Section 5.1). There are 11 types of framing (Table
5), and 9 types of GPF which are expanded by adding RELEASETURN and CLOSEDIALOGUE to the
original GPF sets (Table 2). The number of all possible GPF/framing pairs is 99, and some pairs have not
appeared in the original corpus. Therefore, we reduce the number of actions by filtering. We construct
a unigram model of the salesperson?s dialogue acts P (F
sales
, G
sales
) from the original corpus, then
exclude pairs for which the likelihood is below 0.005
6
. As a result, the 13 pairs shown in Table 6
remained
7
. We use these pairs as the system actions.
5.3 Belief state
The current system belief state is represented by the features used for reward calculation (Table 4) and
the reward calculated at previous turn. Namely, the features for the reward calculation and calculated
reward are also used as the next input of the system policy. Note that the system cannot directly observe
C
alt
, thus the system estimates it through the dialogue by using the following equation.
P (
?
C
t+1
alt
|
?
C
t
alt
, F
t
sys
, G
t
sys
, S
alt
) (5)
where
?
C
t+1
alt
represents the estimated CPD at t + 1.
?
C
t
alt
represents the estimated CPD at t. The other
variables are the same as those in Equation (2). In contrast, we assume that the system can observe
G
user
and S
alt
. G
user
is not usually observable because traditional dialogue systems have automatic
speech recognition/Spoken language understanding errors. However, in this work, we use Wizard of Oz
in place of automatic speech recognition/Spoken language understanding (Section 6.2). Thus, we can
ignore these factors
8
.
6 Experimental evaluation
In this section, we describe the evaluation of the proposed method for learning cooperative persuasive
dialogue policies. Especially, we focus on examining how the learned policy with framing is effective
for persuasive dialogue. The evaluation is done both using a user simulator and real users.
6
We chose this threshold by trying values from 0.001 to 0.01 with incrementation of 0.001. We select the threshold that
resulted in the number of actions closest to previous work (Georgila, 2013).
7
Cameras C and D are not popular, and don?t appear frequently in the human persuasive dialogue corpus, and are therefore
excluded in filtering.
8
In addition to this reason, the G
user
is not so essential to our research (GPF is general dialogue act), and we want to focus
the CPD. This is the other reason that we assume that G
user
is observable.
1711
Figure 2: Average reward of each system. Error bars represents 95% confidence intervals. Rew repre-
sents the reward, Sat represents the user satisfaction, PS represents persuasion success, and Nat represents
naturalness.
6.1 Policy learning and evaluation using the user simulator
For evaluating the effectiveness of framing and learning the policy through the user simulator, we prepare
the following 3 policies.
Random A baseline where the action is randomly output from all possible actions.
NoFraming A baseline where the action is output based on the policy which is learned using only
GPFs. For constructing the actions, we remove actions whose framing is not None from the actions
described in Section 5.2. The policy is a greedy policy, and selects the action with the highest Q-
value.
Framing The proposed method where the action is output based on the policy learned with all actions
described in Section 5.2 including framing. The policy is also a greedy policy.
For learning the policy, we use Neural fitted Q Iteration (Section 2). For applying Neural fitted Q
Iteration, we use the Pybrain library (Schaul et al., 2010). We set the discount factor ? of learning to 0.9,
and the number of nodes in the hidden layer of the neural network for approximating the Q-function to
the sum of number of belief states and actions (i.e. Framing: 53, NoFraming: 47). The policy in learning
is the ?-greedy policy (? = 0.3). These conditions follow the default Pybrain settings. We consider 50
dialogues as one epoch, and update the parameters of the neural network at each epoch. Learning is
finished when number of epochs reaches 200 (10000 dialogues), and the policy with the highest average
reward is used for evaluation.
We evaluate the system on the basis of average reward per dialogue with the user simulator. For
calculating average reward, 1000 dialogues are performed with each policy.
Experimental results (Figure 2) indicate that 1) performance is greatly improved by learning and 2)
framing is somewhat effective for the user simulator. Learned policies (Framing, NoFraming) get a
higher reward than Random. Particularly, both of the learned policies better achieve user satisfaction than
Random. On the other hand, only Framing is able to achieve better persuasion success than Random.
This result indicates that framing is effective for persuasive success. In contrast, naturalness of Framing
is not improved from Random. One of the reasons for this is that variance of Nat is smaller than those
of the other factors, and the optimization algorithm favored the other two factors which had a higher
variance.
6.2 Real user evaluation based on Wizard of Oz
To test whether the gains shown on the user simulator will carry over to an actual dialogue scenario, we
perform an experiment with real human users. In addition to the policies described in Section 6.1, we
add the following policy.
Human An oracle where the action is output based on human selection. In this research, the first author
(who has no formal sales experience, but experience of about 1 year in analysis of camera sales
dialogue) selects the action.
1712
Figure 3: The experimental environment based on Wizard of Oz. The rectangle represents information,
and the cylinder represents a system module. The information flow (dashed line) in the experiment
through the user simulator is also shown for comparison.
Experimental evaluation is conducted, based on the Wizard of Oz framework. In the experiment, the
wizard plays the salesperson, and the evaluator plays the customer. Dialogue is performed between the
wizard and the evaluator. The wizard and evaluator are divided by a partition, and the evaluator cannot
see or detect what the wizard is doing. The evaluator selects his/her preferred camera from the catalog
before starting evaluation. Then, the evaluator starts the dialogue with the wizard who is obeying one
of the policies (Figure 3). In particular, dialogue between wizard and evaluator proceeds based on the
following steps.
1. The evaluator talks to the wizard using the mic. In this step, the evaluators can close the dialogue if
they want.
2. The wizard listens to the evaluator?s utterance, translating the utterance into the appropriate G
user
.
Then, the wizard inputs G
user
to the policy module.
3. The policy module decides action sequences (F
sys
, G
sys
) based on G
user
, then outputs the action to
the utterance database module. This module is constructed from the camera sales corpus (Section
3).
4. The utterance database module searches for similar sentences that match the history of input actions
and G
user
so far, then outputs the top 6 similar utterances to the wizard.
5. The wizard generates the system utterance (Text) using the retrieved sentences. The wizard selects
one sentence which best matches the context
9
. If the wizard determines the sentence is hard to
understand, the wizard can correct the sentence to be more natural.
6. The wizard inputs the system utterance to text-to-speech, then waits for the next evaluator utterance
(back to step 1).
Finally, the evaluator answers the following questionnaire for calculating the evaluation measures in
Section 5.1.
Satisfaction The evaluator?s subjective satisfaction defined as a 5 level score of customer satisfaction
(1: Not satisfied?3: Neutral?5: Satisfied).
Final decision The camera that the customer finally wants to buy.
We use SofTalk (cncc, 2010) as text-to-speech software.
Evaluation criteria are basically same to those of previous section (described in Section 5.1). Note
that in the previous section, Sat
user
and PS
sys
are estimated from the simulated dialogue. In contrast
to the previous section, Sat
user
and PS
sys
are calculated from the result of the real user?s questionnaire
9
Note that the wizard is not allowed to create the utterance with complete freedom, and selects an utterance from the
utterance database even when Human policy is used.
1713
Figure 4: Evaluation results for real users. Error bars represent 95% confidence intervals. Rew represents
the reward, Sat represents the user satisfaction, PS represents persuasion success, and Nat represents
naturalness.
Table 7: Part of a dialogue between Framing and an evaluator (translated from Japanese)
Speaker Transcription Fra GPF
Wiz Which pictures do you want to take? Far or near? None QUESTION
Wiz Camera B has 20x zoom, and this is good. Pos B ANSWER
Wiz How about it? RELEASET
Eva I think B sounds good. ANSWER
Wiz Yes, B is popular with zoom, Pos B INFORM
Wiz But, A has extremely good performance.
Camera A has almost the same parts as a single lens camera,
and is more reasonably priced than a single lens-camera. Pos A ANSWER
Wiz How about it? RELEASET
(described in the previous paragraph)
10
based on the definition of Sat
user
and Sat
user
in Section 6.1. The
naturalness is automatically calculated by the system, in the same manner as described in the previous
section. Finally, reward is calculated considering Sat
user
, PS
sys
and naturalness according to Equation
3.
Participants consist of 13 evaluators (3 female, 10 male) and one wizard. Evaluators perform one
dialogue with the wizard obeying each policy (a total of 4 dialogues) in random order.
Experimental results (Figure 4) indicate that framing is effective in persuasive dialogues with real
users, and that the reward of Framing is higher than NoFraming and Random, and almost equal to
Human. In addition, the score of NoFraming is almost equal to Random. This indicates that despite the
fact that it performed relatively well in the simulation experiment, NoFraming is not an effective policy
for real users. In addition, the score of NoFraming is lower than the score given by the user simulator.
In particular, persuasion success is drastically decreased. This indicates that framing is important for
persuasion.
We can see that some features in human persuasive dialogue appear in the dialogue between users
and the wizard using the Framing policy. An example of a typical dialogue of Framing is shown in
Table 7. The first feature is that the system also recommends camera B when the system does positive
framing of camera A, which is the persuasive target. This feature was found by Hiraoka et al. (2014) to
be an indicator of persuasion success in the camera sales corpus. The second feature is that the system
asks the user about the user?s profile at the first stage of the dialogue. This feature is often found when
user satisfaction is high. The second feature also appeared in the dialogue with NoFraming. However,
NoFraming does not use framing, and asks the user to make a decision (DIRECTIVE). An example
utterance from the DIRECTIVE class is ?Please, decide (which camera you want to buy) after seeing the
catalog?.
Considering the evaluation result of the previous section, we can see that Sat and PS differ between
the user simulator and the real users (p < .05). While the general trend of showing improvements for
10
Note that, though systems estimate the satisfaction and evaluator?s decision at each turn for the belief state, the human
evaluator answers the questionnaire only when the dialogue is closed.
1714
satisfaction and persuasive success is identical in Figures 2 and 4, the systems are given excessively high
Sat in simulation. In addition, systems (especially Framing) are given underestimated PS in simulation.
One of the reasons for this is that the property of dialogue features for the predictive model for reward
differs from previous research (Hiraoka et al., 2014). In this paper, dialogue features for the predictive
model are calculated at each turn. In addition, persuasion success and user satisfaction are successively
calculated at each turn. In contrast, in previous research, the predictive model was constructed with
dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive
model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is
that the simulator is not sufficiently accurate to use for reflecting real user?s behavior. Compared to
other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for
training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user
behavior. Improving the user simulator is an important challenge for future work.
7 Related work
There are a number of related works that apply reinforcement learning to persuasion and negotiation
dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user
simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between
a florist and a grocer are assumed as an example of negotiation dialogue. In addition, Georgila (2013)
also applies reinforcement learning to two-issue negotiation dialogue where participants have a party,
and decide both the date and food type. A handcrafted user simulator is used for learning the policy
of each participant. Heeman (2009) models negotiation dialogue, assuming a furniture layout task, and
Paruchuri et al. (2009) model negotiation dialogue, assuming the dialogue between a seller and buyer.
Our research differs from these in three major ways. The first is that we use framing, positive or
negative statements about the particular item, which is known to be important for persuasion (Irwin et
al., 2013). By considering framing, the system has the potential to be more persuasive. While there is
one previous example of persuasive dialogue using framing (Mazzotta et al., 2007), this system does not
use an automatically learned policy, relying on handcrafted rules. In contrast, in our research, we apply
reinforcement learning to learn the system policy automatically.
In addition, in these previous works, rewards and belief states are defined with heuristics. In contrast,
in our research, reward is defined on the basis of knowledge of human persuasive dialogue. In particular,
we calculate the reward and belief state using the predictive model of Hiraoka et al. (2014) for estimating
persuasion success and user satisfaction using dialogue features. In the real world, it is unclear what
factors are important for achieving the dialogue goal in many persuasive situations. By considering these
predictions as knowledge of human persuasion, the system can identify the important factors in human
persuasion and can track the achievement of the goal based on these.
Finally, these works do not evaluate the learned policy, or evaluate only in simulation. In contrast, we
evaluate the learned policy with real users.
8 Conclusion
We apply reinforcement learning for learning cooperative persuasive dialogue system policies using
framing. In order to apply reinforcement learning, a user simulator and reward function is constructed
based on a human persuasive dialogue corpus. Then, we evaluate the learned policy and effect of fram-
ing using a user simulator and real users. Experimental evaluation indicates that applying reinforcement
learning is effective for construction of cooperative persuasive dialogue systems that use framing.
In the future, we plan to construct a fully automatic persuasive dialogue system using framing. In this
research, automatic speech recognition, spoken language understanding and natural language generation
are performed by a human Wizard. We plan to implement these modules and evaluate system perfor-
mance. In addition, in this research, corpus collection and evaluation are done in a role-playing situation.
Therefore, we plan to evaluate the system policies in a more realistic situation. We also plan to consider
non-verbal information (Nouri et al., 2013) for estimating persuasive success and user satisfaction.
1715
References
cncc. 2010. SofTalk. http://www35.atwiki.jp/softalk/.
Kallirroi Georgila and David Traum. 2011. Reinforcement learning of argumentation dialogue policies in negoti-
ation. Proceedings of INTERSPEEECH.
Kallirroi Georgila. 2013. Reinforcement learning of two-issue negotiation dialogue policies. Proceedings of the
SIGDIAL.
Marco Guerini, Oliviero Stock, and Massimo Zancanaro. 2003. Persuasion model for intelligent interfaces.
Proceedings of the IJCAI Workshop on Computational Models of Natural Argument.
Peter A. Heeman. 2009. Representing the reinforcement learning state in a negotiation dialogue. Proceedings of
ASRU.
Takuya Hiraoka, Yuki Yamauchi, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2013.
Dialogue management for leading the conversation in persuasive dialogue systems. Proceedings of ASRU.
Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2014. Construction and
analysis of a persuasive dialogue corpus. Proceedings of IWSDS.
Levin Irwin, Sandra L. Schneider, and Gary J. Gaeth. 2013. All frames are not created equal: A typology and
critical analysis of framing effects. Organizational behavior and human decision processes 76.2.
ISO24617-2, 2010. Language resource management-Semantic annotation frame work (SemAF), Part2: Dialogue
acts. ISO.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-machine interaction
for learning dialog strategies. Proceedings of ICASSP.
Irene Mazzotta and Fiorella de Rosis. 2006. Artifices for persuading to improve eating habits. AAAI Spring
Symposium: Argumentation for Consumers of Healthcare.
Irene Mazzotta, Fiorella de Rosis, and Valeria Carofiglio. 2007. PORTIA: a user-adapted persuasion system in the
healthy-eating domain. Intelligent Systems.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Minami, and Kohji Dohsaka. 2010. Controlling listening-
oriented dialogue using partially observable Markov decision processes. Proceedings of COLING.
Toyomi Meguro, Yasuhiro Minami, Ryuichiro Higashinaka, and Kohji Dohsaka. 2011. Wizard of oz evaluation of
listening-oriented dialogue control using pomdp. Proceedings of ASRU.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and David Traum. 2012. Reinforcement learning of question-
answering dialogue policies for virtual museum guides. Proceedings of the 13th Annual Meeting of SigDial.
Hien Nguyen, Judith Masthoff, and Pete Edwards. 2007. Persuasive effects of embodied conversational agent
teams. Proceedings of HCI.
Elnaz Nouri, Sunghyun Park, Stefan Scherer, Jonathan Gratch, Peter Carnevale, Louis-Philippe Morency, and
David Traum. 2013. Prediction of strategy and outcome as negotiation unfolds by using basic verbal and
behavioral features. Proceedings of INTERSPEECH.
Praveen Paruchuri, Nilanjan Chakraborty, Roie Zivan, Katia Sycara, Miroslav Dudik, and Geoff Gordon. 2009.
POMDP based negotiation modeling. Proceedings of the first MICON.
Martin Riedmiller. 2005. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement
learning method. Machine Learning: ECML.
Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas R?uckstie?, and J?urgen
Schmidhuber. 2010. Pybrain. The Journal of Machine Learning Research.
Allison Terrell and Mutlu Bilge. 2012. A regression-based approach to modeling addressee backchannels. Pro-
ceedings of the 13th Annual Meeting of SIGDIAL.
Jason D. Williams and Steve Young. 2007. Scaling POMDPs for spoken dialog management. IEEE Transactions
on Audio, Speech, and Language Processing.
1716
Appendix
Table 8: The summary of one dialogue in the corpus (translated from Japanese)
Speaker Transcription GPF Tag
Customer Hello. INFORM
Customer I?m looking for a camera for traveling. Do you have any recommendations? PROPQ
Clerk What kind of pictures do you want to take? SETQ
Customer Well, I?m the member of a tennis club,
and want to take a picture of landscapes or tennis. ANSWER
Clerk O.K. You want the camera which can take both far and near. Don?t you? PROPQ
Clerk Well, have you used a camera before? PROPQ
Customer I have used a digital camera. But the camera was cheap and low resolution. ANSWER
Clerk I see. I see. Camera A is a high resolution camera.
A has extremely good resolution compared with other cameras.
Although this camera does not have a strong zoom,
its sensor is is almost the same as a single-lens camera. INFORM
Customer I see. INFORM
Clerk For a single lens camera,
buying only the lens can cost 100 thousand yen.
Compared to this, this camera is a bargain. INFORM
Customer Ah, I see. INFORM
Customer But, it?s a little expensive. right? PROPQ
Customer Well, I think, camera B is good at price. INFORM
Clerk Hahaha, yes, camera B is reasonably priced. ANSWER
Clerk But its performance is low compared with camera A. INFORM
Customer If I use the two cameras will I be able to tell the difference? PROPQ
Clerk Once you compare the pictures taken by these cameras,
you will understand the difference immediately.
The picture itself is very high quality.
But, camera B and E are lower resolution,
and the picture is a little bit lower quality. ANSWER
Customer Is there also difference in normal size pictures? PROPQ
Clerk Yes, whether the picture is small or large, there is a difference ANSWER
Customer Considering A has single-lens level performance, it is surely reasonable. INFORM
Clerk I think so too. INFORM
Clerk The general price of a single-lens is about 100 or 200 thousand yen.
Considering these prices, camera A is a good choice. INFORM
Customer Certainly, I?m interested in this camera. INFORM
Clerk Considering its performance, it is a bargain. INFORM
Customer I think I?ll go home, compare the pictures, and think a little more. COMMISIVE
Clerk I see. Thank you. DIRECTIVE
Table 9: Example positive framing of a salesperson?s utterance ?a
i
= B, p
i
= POS, r
i
= YES?. In this
example, the customer has indicated price as the preferred determinant.
Hahaha, yes, camera B is reasonably priced.
Table 10: Example negative framing of a salesperson?s utterance ?a
i
= B, p
i
= NEG, r
i
= NO?. In this
example, the customer has indicated price as the preferred determinant.
But, considering the long term usage, you might care about picture quality.
You might change your mind if you only buy a small camera (Camera B).
1717
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 843?853, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Inducing a Discriminative Parser to Optimize Machine
Translation Reordering
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
This paper proposes a method for learning
a discriminative parser for machine trans-
lation reordering using only aligned par-
allel text. This is done by treating the
parser?s derivation tree as a latent variable
in a model that is trained to maximize re-
ordering accuracy. We demonstrate that
efficient large-margin training is possible
by showing that two measures of reorder-
ing accuracy can be factored over the parse
tree. Using this model in the pre-ordering
framework results in significant gains in
translation accuracy over standard phrase-
based SMT and previously proposed unsu-
pervised syntax induction methods.
1 Introduction
Finding the appropriate word ordering in the
target language is one of the most difficult prob-
lems for statistical machine translation (SMT),
particularly for language pairs with widely di-
vergent syntax. As a result, there is a large
amount of previous research that handles the
problem of reordering through the use of im-
proved reordering models for phrase-based SMT
(Koehn et al2005), hierarchical phrase-based
translation (Chiang, 2007), syntax-based trans-
lation (Yamada and Knight, 2001), or pre-
ordering (Xia and McCord, 2004).
In particular, systems that use source-
language syntax allow for the handling of long-
distance reordering without large increases in
The first author is now affiliated with the Nara Institute
of Science and Technology.
decoding time. However, these require a good
syntactic parser, which is not available for many
languages. In recent work, DeNero and Uszko-
reit (2011) suggest that unsupervised grammar
induction can be used to create source-sentence
parse structure for use in translation as a part
of a pre-ordering based translation system.
In this work, we present a method for inducing
a parser for SMT by training a discriminative
model to maximize reordering accuracy while
treating the parse tree as a latent variable. As a
learning framework, we use online large-margin
methods to train the model to directly minimize
two measures of reordering accuracy. We pro-
pose a variety of features, and demonstrate that
learning can succeed when no linguistic informa-
tion (POS tags or parse structure) is available in
the source language, but also show that this lin-
guistic information can be simply incorporated
when it is available. Experiments find that the
proposed model improves both reordering and
translation accuracy, leading to average gains
of 1.2 BLEU points on English-Japanese and
Japanese-English translation without linguistic
analysis tools, or up to 1.5 BLEU points when
these tools are incorporated. In addition, we
show that our model is able to effectively max-
imize various measures of reordering accuracy,
and that the reordering measure that we choose
has a direct effect on translation results.
2 Preordering for SMT
Machine translation is defined as transforma-
tion of source sentence F = f1 . . . fJ to target
sentence E = e1 . . . eI . In this paper, we take
843
Figure 1: An example with a source sentence F re-
ordered into target order F ?, and its corresponding
target sentence E. D is one of the BTG derivations
that can produce this ordering.
the pre-ordering approach to machine transla-
tion (Xia and McCord, 2004), which performs
translation as a two step process of reordering
and translation (Figure 1). Reordering first de-
terministically transforms F into F ?, which con-
tains the same words as F but is in the order of
E. Translation then transforms F ? into E using
a method such as phrase-based SMT (Koehn et
al., 2003), which can produce accurate transla-
tions when only local reordering is required.
This general framework has been widely stud-
ied, with the majority of works relying on a
syntactic parser being available in the source
language. Reordering rules are defined over
this parse either through machine learning tech-
niques (Xia and McCord, 2004; Zhang et al
2007; Li et al2007; Genzel, 2010; Dyer and
Resnik, 2010; Khalilov and Sima?an, 2011) or
linguistically motivated manual rules (Collins et
al., 2005; Xu et al2009; Carpuat et al2010;
Isozaki et al2010b). However, as building a
parser for each source language is a resource-
intensive undertaking, there has also been some
interest in developing reordering rules without
the use of a parser (Rottmann and Vogel, 2007;
Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al2011), and we
will follow this thread of research in this paper.
In particular, two methods deserve mention
for being similar to our approach. First, DeNero
and Uszkoreit (2011) learn a reordering model
through a three-step process of bilingual gram-
mar induction, training a monolingual parser
to reproduce the induced trees, and training
a reordering model that selects a reordering
based on this parse structure. In contrast, our
method trains the model in a single step, treat-
ing the parse structure as a latent variable in
a discriminative reordering model. In addition
Tromble and Eisner (2009) and Visweswariah et
al. (2011) present models that use binary clas-
sification to decide whether each pair of words
should be placed in forward or reverse order. In
contrast, our method uses traditional context-
free-grammar models, which allows for simple
parsing and flexible parameterization, including
features such as those that utilize the existence
of a span in the phrase table. Our work is also
unique in that we show that it is possible to di-
rectly optimize several measures of reordering
accuracy, which proves important for achieving
good translations.1
3 Training a Reordering Model with
Latent Derivations
In this section, we provide a basic overview of
the proposed method for learning a reordering
model with latent derivations using online dis-
criminative learning.
3.1 Space of Reorderings
The model we present here is based on the
bracketing transduction grammar (BTG, Wu
(1997)) framework. BTGs represent a binary
tree derivation D over the source sentence F
as shown in Figure 1. Each non-terminal node
can either be a straight (str) or inverted (inv)
production, and terminals (term) span a non-
empty substring f .2
The ordering of the sentence is determined by
the tree structure and the non-terminal labels
str and inv, and can be built bottom-up. Each
subtree represents a source substring f and its
reordered counterpart f ?. For each terminal
node, no reordering occurs and f is equal to f ?.
1The semi-supervised method of Katz-Brown et al
(2011) also optimizes reordering accuracy, but requires
manually annotated parses as seed data.
2In the original BTG framework used in translation,
terminals produce a bilingual substring pair f/e, but as
we are only interested in reordering the source F , we
simplify the model by removing the target substring e.
844
For each non-terminal node spanning f with its
left child spanning f1 and its right child span-
ning f2, if the non-terminal symbol is str, the
reordered strings will be concatenated in order
as f ? = f ?1f ?2, and if the non-terminal symbol is
inv, the reordered strings will be concatenated
in inverted order as f ? = f ?2f ?1.
We define the space of all reorderings that can
be produced by the BTG as F ?, and attempt to
find the best reordering F? ? within this space.3
3.2 Reorderings with Latent
Derivations
In order to find the best reordering F? ? given only
the information in the source side sentence F , we
define a scoring function S(F ?|F ), and choose
the ordering of maximal score:
F? ? = arg max
F ?
S(F ?|F ).
As our model is based on reorderings licensed
by BTG derivations, we also assume that there
is an underlying derivation D that produced F ?.
As we can uniquely determine F ? given F and
D, we can define a scoring function S(D|F ) over
derivations, find the derivation of maximal score
D? = arg max
D
S(D|F )
and use D? to transform F into F ?.
Furthermore, we assume that the score
S(D|F ) is the weighted sum of a number of fea-
ture functions defined over D and F
S(D|F,w) =
?
i
wi?i(D,F )
where ?i is the ith feature function, and wi is
its corresponding weight in weight vector w.
Given this model, we must next consider how
to learn the weights w. As the final goal of our
model is to produce good reorderings F ?, it is
natural to attempt to learn weights that will al-
low us to produce these high-quality reorderings.
3BTGs cannot reproduce all possible reorderings, but
can handle most reorderings occurring in natural trans-
lated text (Haghighi et al2009).
Figure 2: An example of (a) the ranking function
r(fj), (b) loss according to Kendall?s ? , (c) loss ac-
cording to chunk fragmentation.
4 Evaluating Reorderings
Before we explain the learning algorithm, we
must know how to distinguish whether the F ?
produced by the model is good or bad. This
section explains how to calculate oracle reorder-
ings, and assign each F ? a loss and an accuracy
according to how well it reproduces the oracle.
4.1 Calculating Oracle Orderings
In order to calculate reordering quality, we first
define a ranking function r(fj |F,A), which indi-
cates the relative position of source word fj in
the proper target order (Figure 2 (a)). In or-
der to calculate this ranking function, we define
A = a1, . . . ,aJ , where each aj is a set of the in-
dices of the words in E to which fj is aligned.4
Given these alignments, we define an ordering
function aj1 < aj2 that indicates that the in-
dices in aj1 come before the indices in aj2 . For-
mally, we define this function as ?the first index
in aj1 is at most the first index in aj2 , similarly
for the last index, and either the first or last
index in aj1 is less than that of aj2 .?
Given this ordering, we can sort every align-
ment aj , and use its relative position in the sen-
tence to assign a rank to its word r(fj). In
4Null alignments require special treatment. To do so,
we can place unaligned brackets and quotes directly be-
fore and after the spans they surround, and attach all
other unaligned words to the word directly to the right
for head-initial languages (e.g. English), or left for head-
final languages (e.g. Japanese).
845
the case of ties, where neither aj1 < aj2 nor
aj2 < aj1 , both fj1 and fj2 are assigned the
same rank. We can now define measures of re-
ordering accuracy for F ? by how well it arranges
the words in order of ascending rank. It should
be noted that as we allow ties in rank, there
are multiple possible F ? where all words are in
strictly ascending order, which we will call ora-
cle orderings.
4.2 Kendall?s ?
The first measure of reordering accuracy that
we will consider is Kendall?s ? (Kendall, 1938),
a measure of pairwise rank correlation which
has been proposed for evaluating translation re-
ordering accuracy (Isozaki et al2010a; Birch
et al2010) and pre-ordering accuracy (Talbot
et al2011). The fundamental idea behind the
measure lies in comparisons between each pair of
elements f ?j1 and f ?j2 of the reordered sentence,
where j1 < j2. Because j1 < j2, f ?j1 comes before
f ?j2 in the reordered sentence, the ranks should
be r(f ?j1) ? r(f ?j2) in order to produce the cor-
rect ordering.
Based on this criterion, we first define a loss
Lt(F ?) that will be higher for orderings that are
further from the oracle. Specifically, we take the
sum of all pairwise orderings that do not follow
the expected order
Lt(F ?) =
J?1
?
j1=1
J
?
j2=j1+1
?(r(f ?j1) > r(f
?
j2))
where ?(?) is an indicator function that is 1 when
its condition is true, and 0 otherwise. An exam-
ple of this is given in Figure 2 (b).
To calculate an accuracy measure for ordering
F ?, we first calculate the maximum loss for the
sentence, which is equal to the total number of
non-equal rank comparisons in the sentence5
max
F ?
Lt(F ?) =
J?1
?
j1=1
J
?
j2=j1+1
?(r(f ?j1) 6= r(f
?
j2)).
(1)
5The traditional formulation of Kendall?s ? assumes
no ties in rank, and thus the maximum loss can be cal-
culated as J(J ? 1)/2.
Finally, we use this maximum loss to normalize
the actual loss to get an accuracy
At(F ?) = 1?
Lt(F ?)
max
F? ?
Lt(F? ?)
,
which will take a value between 0 (when F ? has
maximal loss), and 1 (when F ? matches one of
the oracle orderings). In Figure 2 (b), Lt(F ?) =
2 and max
F? ?
Lt(F? ?) = 8, so At(F ?) = 0.75.
4.3 Chunk Fragmentation
Another measure that has been used in eval-
uation of translation accuracy (Banerjee and
Lavie, 2005) and pre-ordering accuracy (Talbot
et al2011) is chunk fragmentation. This mea-
sure is based on the number of chunks that the
sentence needs to be broken into to reproduce
the correct ordering, with a motivation that the
number of continuous chunks is equal to the
number of times the reader will have to jump to
a different position in the reordered sentence to
read it in the target order. One way to measure
the number of continuous chunks is considering
whether each word pair f ?j and f ?j+1 is discon-
tinuous (the rank of f ?j+1 is not equal to or one
greater than f ?j)
discont(f ?j , f ?j+1) =
?(r(f ?j) 6= r(f ?j+1) ? r(f ?j) + 1 6= r(f ?j+1))
and sum over all word pairs in the sentence to
create a sentence-based loss
Lc(F ?) =
J?1
?
j=1
discont(f ?j , f ?j+1) (2)
While this is the formulation taken by previ-
ous work, we found that this under-penalizes
bad reorderings of the first and last words of
the sentence, which can contribute to the loss
only once, as opposed to other words which can
contribute to the loss twice. To account for
this, when calculating the chunk fragmentation
score, we additionally add two sentence bound-
ary words f0 and fJ+1 with ranks r(f0) = 0 and
r(fJ+1) = 1 + max
f ?j?F ?
r(f ?j) and redefine the sum-
mation in Equation (2) to consider these words
(e.g. Figure 2 (c)).
846
procedure WeightUpdate(F , A, w)
D ? parse(F,w) . Create parse forest
D? ? argmax
D?D
S(D|F,w) + L(D|F,A)
. Find the model parse
D? ? argmin
D?D
L(D|F,A)? ?S(D|F,w)
. Find the oracle parse
if L(D?|F,A) 6= L(D?|F,A) then
w ? ?(w + ?(?(D?, F )? ?(D?, F )))
. Perform weight update
end if
end procedure
Figure 3: An online update for sentence F , alignment
A, and weight vector w. ? is a very small constant,
and ? and ? are defined by the update strategy.
Similarly to Kendall?s ? , we can also define
an accuracy measure between 0 and 1 using the
maximum loss, which will be at most J + 1,
which corresponds to the total number of com-
parisons made in calculating the loss6
Ac(F ?) = 1?
Lc(F ?)
J + 1
.
In Figure 2 (c), Lc(F ?) = 3 and J + 1 = 6, so
Ac(F ?) = 0.5.
5 Learning a BTG Parser for
Reordering
Now that we have a definition of loss over re-
orderings produced by the model, we have a
clear learning objective: we would like to find
reorderings F ? with low loss. The learning algo-
rithm we use to achieve this goal is motivated
by discriminative training for machine transla-
tion systems (Liang et al2006), and extended
to use large-margin training in an online frame-
work (Watanabe et al2007).
5.1 Learning Algorithm
Learning uses the general framework of large-
margin online structured prediction (Crammer
et al2006), which makes several passes through
the data, finding a derivation with high model
score (the model parse) and a derivation with
6It should be noted that for sentences of length one or
sentences with tied ranks, the maximum loss may be less
than J +1, but for simplicity we use this approximation.
minimal loss (the oracle parse), and updating w
if these two parses diverge (Figure 3).
In order to create both of these parses effi-
ciently, we first create a parse forest encoding a
large number of derivations Di according to the
model scores. Next, we find the model parse D?i,
which is the parse in the forest Di that maxi-
mizes the sum of the model score and the loss
S(Dk|Fk,w)+L(Dk|Fk, Ak). It should be noted
that here we are considering not only the model
score, but also the derivation?s loss. This is
necessary for loss-driven large-margin training
(Crammer et al2006), and follows the basic
intuition that during training, we would like to
make it easier to select negative examples with
large loss, causing these examples to be penal-
ized more often and more heavily.
We also find an oracle parse D?i, which is se-
lected solely to minimize the loss L(Dk|Fk, Ak).
One important difference between the model we
describe here and traditional parsing models is
that the target derivation D?k is a latent variable.
Because many Dk achieve a particular reorder-
ing F ?, many reorderings F ? are able to mini-
mize the loss L(F ?k|Fk, Ak). Thus it is necessary
to choose a single oracle derivation to treat as
the target out of many equally good reorderings.
DeNero and Uszkoreit (2011) resolve this ambi-
guity with four features with empirically tuned
scores before training a monolingual parser and
reordering model. In contrast, we follow previ-
ous work on discriminative learning with latent
variables (Yu and Joachims, 2009), and break
ties within the pool of oracle derivations by se-
lecting the derivation with the largest model
score. From an implementation point of view,
this can be done by finding the derivation that
minimizes L(Dk|Fk, Ak)??S(Dk|Fk,w), where
? is a constant small enough to ensure that the
effect of the loss will always be greater than the
effect of the score.
Finally, if the model parse D?k has a loss that
is greater than that of the oracle parse D?k, we
update the weights to increase the score of the
oracle parse and decrease the score of the model
parse. Any criterion for weight updates may be
used, such as the averaged perceptron (Collins,
2002) and MIRA (Crammer et al2006), but
847
we opted to use Pegasos (Shalev-Shwartz et al
2007) as it allows for the introduction of regu-
larization and relatively stable learning.
To perform this full process, given a source
sentence Fk, alignment Ak, and model weights
w we need to be able to efficiently calculate
scores, calculate losses, and create parse forests
for derivations Dk, the details of which will be
explained in the following sections.
5.2 Scoring Derivation Trees
First, we must consider how to efficiently assign
scores S(D|F,w) to a derivation or forest during
parsing. The most standard and efficient way to
do so is to create local features that can be cal-
culated based only on the information included
in a single node d in the derivation tree. The
score of the whole tree can then be expressed as
the sum of the scores from each node:
S(D|F,w) =
?
d?D
S(d|F,w)
=
?
d?D
?
i
wi?i(d, F ).
Based on this restriction, we define a number of
features that can be used to score the parse tree.
To ease explanation, we represent each node in
the derivation as d = ?s, l, c, c + 1, r?, where s
is the node?s symbol (str, inv, or term), while
l and r are the leftmost and rightmost indices
of the span that d covers. c and c + 1 are the
rightmost index of the left child and leftmost
index of the right child for non-terminal nodes.
All features are intersected with the node la-
bel s, so each feature described below corre-
sponds to three different features (or two for
features applicable to only non-terminal nodes).
? ?lex: Identities of words in positions fl, fr,
fc, fc+1, fl?1, fr+1, flfr, and fcfc+1.
? ?class: Same as ?lex, but with words ab-
stracted to classes. We use the 50 classes
automatically generated by Och (1999)?s
method that are calculated during align-
ment in standard SMT systems.
? ?balance: For non-terminals, features indi-
cating whether the length of the left span
(c? l+1) is lesser than, equal to, or greater
than the length of the right span (r ? c).
? ?table: Features, bucketed by length, that
indicate whether ?fl . . . fr? appears as a
contiguous phrase in the SMT training
data, as well as the log frequency of the
number of times the phrase appears total
and the number of times it appears as a
contiguous phrase (DeNero and Uszkoreit,
2011). Phrase length is limited to 8, and
phrases of frequency one are removed.
? ?pos: Same as ?lex, but with words ab-
stracted to language-dependent POS tags.
? ?cfg: Features indicating the label of the
spans fl . . . fr, fl . . . fc, and fc+1 . . . fr in a
supervised parse tree, and the intersection
of the three labels. When spans do not cor-
respond to a span in the supervised parse
tree, we indicate ?no span? with the label
?X? (Zollmann and Venugopal, 2006).
Most of these features can be calculated from
only a parallel corpus, but ?pos requires a POS
tagger and ?cfg requires a full syntactic parser
in the source language. As it is preferable to
have a method that is applicable in languages
where these tools are not available, we perform
experiments both with and without the features
that require linguistic analysis tools.
5.3 Finding Losses for Derivation Trees
The above features ? and their corresponding
weights w are all that are needed to calculate
scores of derivation trees at test time. However,
during training, it is also necessary to find model
parses according to the loss-augmented scoring
function S(D|F,w)+L(D|F,A) or oracle parses
according to the loss L(D|F,A). As noted by
Taskar et al2003), this is possible if our losses
can be factored in the same way as the feature
space. In this section, we demonstrate that the
loss L(d|F,A) for the evaluation measures we
defined in Section 4 can (mostly) be factored
over nodes in a fashion similar to features.
848
5.3.1 Factoring Kendall?s ?
For Kendall?s ? , in the case of terminal nodes,
Lt(d = ?term, l, r?|F,A) can be calculated by
performing the summation in Equation (1). We
can further define this sum recursively and use
memoization for improved efficiency
Lt(d|F,A) =Lt(?term, l, r ? 1?|F,A)
+
r?1
?
j=l
?(r(fj) > r(fr)). (3)
For non-terminal nodes, we first focus on
straight non-terminals with parent node d =
?str, l, c, c+1, r?, and left and right child nodes
dl = ?sl, l, lc, lc+1, c? and dr = ?sr, c+1, rc, rc+
1, r?. First, we note that the loss for the subtree
rooted at d can be expressed as
Lt(d|F,A) =Lt(dl|F,A) + Lt(dr|F,A)
+
c
?
j1=l
r
?
j2=c+1
?(r(fj1) > r(fj2)).
In other words, the subtree?s total loss can be
factored into the loss of its left subtree, the
loss of its right subtree, and the additional loss
contributed by comparisons between the words
spanning both subtrees. In the case of inverted
terminals, we must simply reverse the compari-
son in the final sum to be ?(r(fj1) < r(fj2)).
5.3.2 Factoring Chunk Fragmentation
Chunk fragmentation loss can be factored in a
similar fashion. First, it is clear that the loss for
the terminal nodes can be calculated efficiently
in a fashion similar to Equation (3). In order to
calculate the loss for non-terminals d, we note
that the summation in Equation (2) can be di-
vided into the sum over the internal bi-grams
in the left and right subtrees, and the bi-gram
spanning the reordered trees
Lc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)
+ discont(f ?c, f ?c+1).
However, unlike Kendall?s ? , this equation re-
lies not on the ranks of fc and fc+1 in the origi-
nal sentence, but on the ranks of f ?c and f ?c+1 in
the reordered sentence. In order to keep track
of these values, it is necessary to augment each
node in the tree to be d = ?s, l, c, c + 1, r, tl, tr?
with two additional values tl and tr that indi-
cate the position of the leftmost and rightmost
words after reordering. Thus, a straight non-
terminal parent d with children dl = ?sl, l, lc, lc+
1, c, tl, tlr? and dr = ?sr, c+1, rc, rc+1, r, trl, tr?
will have loss as follows
Lc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)
+ discont(ftlr, ftrl)
with a similar calculation being possible for in-
verted non-terminals.
5.4 Parsing Derivation Trees
Finally, we must be able to create a parse forest
from which we select model and oracle parses.
As all feature functions factor over single nodes,
it is possible to find the parse tree with the high-
est score in O(J3) time using the CKY algo-
rithm. However, when keeping track of target
positions for calculation of chunk fragmentation
loss, there are a total of O(J5) nodes, an unrea-
sonable burden in terms of time and memory.
To overcome this problem, we note that this set-
ting is nearly identical to translation using syn-
chronous CFGs with an integrated bigram LM,
and thus we can employ cube-pruning to reduce
our search space (Chiang, 2007).
6 Experiments
Our experiments test the reordering and trans-
lation accuracy of translation systems using the
proposed method. As reordering metrics, we use
Kendall?s ? and chunk fragmentation (Talbot et
al., 2011) comparing the system F ? and oracle
F ? calculated with manually created alignments.
As translation metrics, we use BLEU (Papineni
et al2002), as well as RIBES (Isozaki et al
2010a), which is similar to Kendall?s ? , but eval-
uated on the target sentence E instead of the re-
ordered sentence F ?. All scores are the average
of three training runs to control for randomness
in training (Clark et al2011).
For translation, we use Moses (Koehn et al
2007) with lexicalized reordering (Koehn et al
2005) in all experiments. We test three types
849
en-ja ja-en
Chunk ? BLEU RIBES Chunk ? BLEU RIBES
orig 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36
3-step 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42
3-step+?pos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65
3-step+?cfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93
lader 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
lader+?pos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24
lader+?cfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12
Table 2: Reordering (chunk, ?) and translation (BLEU, RIBES) results for each system. Bold numbers
indicate no significant difference from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004).
sent. word (ja) word (en)
RM-train 602 14.5k 14.3k
RM-test 555 11.2k 10.4k
TM/LM 329k 6.08M 5.91M
Tune 1166 26.8k 24.3k
Test 1160 28.5k 26.7k
Table 1: The number of sentences and words for
training and testing the reordering model (RM),
translation model (TM), and language model (LM).
of pre-ordering: original order with F ? ? F
(orig), pre-orderings learned using the 3-step
process of DeNero and Uszkoreit (2011) (3-
step), and the proposed model with latent
derivations (lader).7 Except when stated oth-
erwise, lader was trained to minimize chunk
fragmentation loss with a cube pruning stack
pop limit of 50, and the regularization constant
of 10?3 (chosen through cross-validation).
We test our systems on Japanese-English and
English-Japanese translation using data from
the Kyoto Free Translation Task (Neubig, 2011).
We use the training set for training translation
and language models, the development set for
weight tuning, and the test set for testing (Table
1). We use the designated development and test
sets of manually created alignments as training
data for the reordering models, removing sen-
tences of more than 60 words.
As default features for lader and the mono-
lingual parsing and reordering models in 3-step,
we use all the features described in Section 5.2
7Available open-source: http://phontron.com/lader
except ?pos and ?cfg. In addition, we test sys-
tems with ?pos and ?cfg added. For English,
we use the Stanford parser (Klein and Manning,
2003) for both POS tagging and CFG parsing.
For Japanese, we use the KyTea tagger (Neu-
big et al2011) for POS tagging,8 and the EDA
word-based dependency parser (Flannery et al
2011) with simple manual head-rules to convert
a dependency parse to a CFG parse.
6.1 Effect of Pre-ordering
Table 2 shows reordering and translation results
for orig, 3-step, and lader. It can be seen
that the proposed lader outperforms the base-
lines in both reordering and translation.9 There
are a number of reasons why lader outper-
forms 3-step. First, the pipeline of 3-step
suffers from error propogation, with errors in
monolingual parsing and reordering resulting
in low overall accuracy.10 Second, as Section
5.1 describes, lader breaks ties between ora-
cle parses based on model score, allowing easy-
to-reproduce model parses to be chosen dur-
ing training. In fact, lader generally found
trees that followed from syntactic constituency,
while 3-step more often used terminal nodes
8In addition, following the example of Sudoh et al
(2011a)?s reordering rules, we lexicalize all particles.
9It should be noted that our results for 3-step are
significantly worse than those of DeNero and Uszkoreit
(2011). Likely reasons include a 20x difference in training
data size, the fact that we are using naturally translated
text as opposed to text translated specifically to create
word alignments, or differences in implementation.
10When using oracle parses, chunk accuracy was up to
81%, showing that parsing errors are highly detrimental.
850
en-ja ja-en
Chunk ? BLEU RIBES Chunk ? BLEU RIBES
Lc 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
Lt 70.37 79.57 22.57 69.47 72.51 78.93 18.52 66.26
Lc + Lt 72.55 80.58 22.89 70.34 74.44 79.82 19.21 66.48
Table 3: Results for systems trained to optimize chunk fragmentation (Lc) or Kendall?s ? (Lt).
that spanned constituent boundaries (as long as
the phrase frequency was high). Finally, as Sec-
tion 6.2 shows in detail, the ability of lader to
maximize reordering accuracy directly allows for
improved reordering and translation results.
It can also be seen that incorporating POS
tags or parse trees improves accuracy of both
lader and 3-step, particularly for English-
Japanese, where syntax has proven useful for
pre-ordering, and less so for Japanese-English,
where syntactic pre-ordering has been less suc-
cessful (Sudoh et al2011b).
We also tested Moses?s implementation of hi-
erarchical phrase-based SMT (Chiang, 2007),
which achieved BLEU scores of 23.21 and 19.30
for English-Japanese and Japanese-English re-
spectively, approximately matching lader in
accuracy, but with a significant decrease in de-
coding speed. Further, when pre-ordering with
lader and hierarchical phrase-based SMT were
combined, BLEU scores rose to 23.29 and 19.69,
indicating that the two techniques can be com-
bined for further accuracy improvements.
6.2 Effect of Training Loss
Table 3 shows results when one of three losses is
optimized during training: chunk fragmentation
(Lc), Kendall?s ? (Lt), or the linear interpola-
tion of the two with weights chosen so that both
losses contribute equally (Lt + Lc). In general,
training successfully maximizes the criterion it is
trained on, and Lt +Lc achieves good results on
both measures. We also find that Lc and Lc+Lt
achieve the best translation results, which is
in concert with Talbot et al2011), who find
chunk fragmentation is better correlated with
translation accuracy than Kendall?s ? . This is
an important result, as methods such as that
of Tromble and Eisner (2009) optimize pairwise
en-ja ja-en
BLEU/RIBES BLEU/RIBES
orig 21.87 68.25 18.34 65.36
man-602 23.11 69.86 19.54 66.93
auto-602 22.39 69.19 18.58 66.07
auto-10k 22.53 69.68 18.79 66.89
Table 4: Results based on data size, and whether
manual or automatic alignments are used in training.
word comparisons equivalent to Lt, which may
not be optimal for translation.
6.3 Effect of Automatic Alignments
Table 4 shows the difference between using man-
ual and automatic alignments in the training of
lader. lader is able to improve over the orig
baseline in all cases, but when equal numbers
of manual and automatic alignments are used,
the reorderer trained on manual alignments is
significantly better. However, as the number of
automatic alignments is increased, accuracy im-
proves, approaching that of the system trained
on a smaller number of manual alignments.
7 Conclusion
We presented a method for learning a discrim-
inative parser to maximize reordering accuracy
for machine translation. Future work includes
application to other language pairs, develop-
ment of more sophisticated features, investiga-
tion of probabilistic approaches to inference, and
incorporation of the learned trees directly in
tree-to-string translation.
Acknowledgments
We thank Isao Goto, Tetsuo Kiso, and anony-
mous reviewers for their helpful comments, and
Daniel Flannery for helping to run his parser.
851
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation
with improved correlation with human judgments.
In Proc. ACL Workshop.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, 24(1):15?26.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving arabic-to-english statistical ma-
chine translation by reordering post-verbal sub-
jects for alignment. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proc. ACL, pages
176?181.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
John DeNero and Jakob Uszkoreit. 2011. Induc-
ing sentence structure from parallel corpora for
reordering. In Proc. EMNLP.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT-
NAACL.
Daniel Flannery, Yusuke Miyao, Graham Neubig,
and Shinsuke Mori. 2011. Training dependency
parsers from partially annotated corpora. In Proc.
IJCNLP, pages 776?784, Chiang Mai, Thailand,
November.
Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale machine
translation. In Proc. COLING.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proc. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, and Hajime Tsukada. 2010a. Auto-
matic evaluation of translation quality for distant
language pairs. In Proc. EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,
and Kevin Duh. 2010b. Head finalization: A
simple reordering rule for sov languages. In Proc.
WMT and MetricsMATR.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proc. EMNLP, pages 183?192.
Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika, 30(1/2):81?93.
Maxim Khalilov and Khalil Sima?an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proc. IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. ACL, pages
423?430.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT, pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation eval-
uation. In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc.
EMNLP.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. ACL.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Proc.
ACL, pages 761?768.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. ACL,
pages 529?533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proc. EACL.
852
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
COLING, pages 311?318.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
pos-based distortion model. In Proc. of TMI-2007.
Shai Shalev-Shwartz, Yoram Singer, and Nathan
Srebro. 2007. Pegasos: Primal estimated sub-
gradient solver for SVM. In Proc. ICML, pages
807?814.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2011a. NTT-
UT statistical machine translation in NTCIR-9
PatentMT. In Proc. NTCIR.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. 2011b. Post-
ordering in statistical machine translation. In
Proc. MT Summit.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering. In Proc. WMT.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin Markov networks. Proc. NIPS,
16.
Roy Tromble and Jason Eisner. 2009. Learning lin-
ear ordering problems for better translation. In
Proc. EMNLP.
Karthik Visweswariah, Rajakrishnan Rajkumar,
Ankur Gandhe, Ananthakrishnan Ramanathan,
and Jiri Navratil. 2011. A word reordering
model for improved machine translation. In Proc.
EMNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proc.
EMNLP, pages 764?773.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3).
Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proc. COLING.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables.
In Proc. ICML, pages 1169?1176.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statis-
tical machine translation. In Proc. SSST.
Andreas Zollmann and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proc. WMT, pages 138?141.
853
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128?132,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Acquiring a Dictionary of Emotion-Provoking Events
Hoa Trong Vu
?,?
, Graham Neubig
?
, Sakriani Sakti
?
, Tomoki Toda
?
, Satoshi Nakamura
?
?
Graduate School of Information Science, Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
?
Vietnam National University, University of Engineering and Technology
E3 Building - 144 Xuan Thuy Street, Cau Giay, Hanoi, Vietnam
Abstract
This paper is concerned with the discov-
ery and aggregation of events that provoke
a particular emotion in the person who
experiences them, or emotion-provoking
events. We first describe the creation of a
small manually-constructed dictionary of
events through a survey of 30 subjects.
Next, we describe first attempts at auto-
matically acquiring and aggregating these
events from web data, with a baseline from
previous work and some simple extensions
using seed expansion and clustering. Fi-
nally, we propose several evaluation meas-
ures for evaluating the automatically ac-
quired events, and perform an evaluation
of the effectiveness of automatic event ex-
traction.
1 Introduction
?You look happy today, did something good hap-
pen?? This is a natural question in human dia-
logue, and most humans could think of a variety of
answers, such as ?I met my friends? or ?I passed a
test.? In this work, we concern ourselves with cre-
ating resources that answer this very question, or
more formally ?given a particular emotion, what
are the most prevalent events (or situations, con-
texts) that provoke it??
1
Information about these
emotion-provoking events is potentially useful for
emotion recognition (recognizing emotion based
on events mentioned in a dialogue), response gen-
eration (providing an answer to emotion-related
questions), and answering social-science related
questions (discovering events that affect the emo-
tion of a particular segment of the population).
1
This is in contrast to existing sentiment lexicons (Riloff
et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Ve-
likovich et al., 2010; Mohammad and Turney, 2013), which
only record the sentiment orientation of particular words
(such as ?meet? or ?friend?), which, while useful, are less dir-
ectly connected to the emotions than the events themselves.
While there is very little previous research on
this subject, one previous work of note by Tok-
uhisa et al. (2008) focused on emotion-provoking
events purely from the viewpoint of emotion re-
cognition. They used large corpus of examples
collected from the Web using manual patterns to
build a k-nearest-neighbors emotion classifier for
dialog systems and found that the classifier sig-
nificantly outperforms baseline methods. This
method provides both an inspiration and a baseline
for our work, but still lacks in that it makes no
attempt to measure the quality of the extracted
events, aggregate similar events, or rank events by
prevalence, all essential factors when attempting
to use extracted events for applications other than
simple emotion recognition.
In this paper, we describe work on creat-
ing prevalence-ranked dictionaries of emotion-
provoking events through both manual labor and
automatic information extraction. To create a
manual dictionary of events, we perform a sur-
vey asking 30 participants to describe events that
caused them to feel a particular emotion, and
manually cleaned and aggregated the results into
a ranked list. Next, we propose several methods
for extracting events automatically from large data
from the Web, which will allow us to increase the
coverage over the smaller manually created dic-
tionary. We start with Tokuhisa et al. (2008)?s pat-
terns as a baseline, and examine methods for im-
proving precision and coverage through the use of
seed expansion and clustering. Finally, we dis-
cuss evaluation measures for the proposed task,
and perform an evaluation of the automatically ex-
tracted emotion-provoking events. The acquired
events will be provided publicly upon acceptance
of the paper.
2 Manual Creation of Events
In order to create a small but clean set of gold-
standard data for each emotion, we first performed
128
Emotions Words
happiness happy, glad
sadness sad, upset
anger angry, irritated
fear afraid, scared
surprise surprised, astonished
disgust disgusted, terrible
Table 2: Seed words for each emotion.
a survey on emotion-provoking events. We did so
by asking a total of 30 subjects (a mixture of male
and female from 20-40 years of age) to write down
five events that provoke each of five emotions:
happiness, sadness, anger, fear, and surprise. As
these events created according to this survey still
have a large amount of lexical variation, we manu-
ally simplify them to their core and merge together
events that have similar meanings.
Finally, for each emotion we extract all the
events that are shared by more than one person. It
should be noted that this will not come anywhere
close to covering the entirety of human emotion,
but as each event is shared by at least two people
in a relatively small sample, any attempt to create
a comprehensive dictionary of emotion-provoking
events should at least be able to cover the pairs in
this collection. We show the most common three
events for each emotion in Table 1.
3 Automatic Extraction of Events
We also performed experiments attempting to
automatically extract and aggregate events from
Web data. As a starting point, we follow Tokuhisa
et al. (2008) in defining a single reliable pattern as
a starting point for event extraction:
I am EMOTION that EVENT
As this pattern is a relatively reliable indicator that
the event is correct, most events extracted by this
pattern will actually be emotion-provoking events.
For instance, this pattern will be matched with the
sentence ?I am happy that my mother is feeling
better?, in which my mother is feeling better cer-
tainly causes happiness.
For the EMOTION placeholder, we take into ac-
count 6 emotions - happiness, sadness, anger, fear,
disgust, and surprise - argued by Ekman (1992) to
be the most basic. We manually create a short list
of words that can be inserted into the above pattern
appropriately, as shown in Table 2.
For the EVENT placeholder, we allow any string
of words, but it is necessary to choose the scope
of the string that is referring to the emotion-
provoking event. To this end, we use a syntactic
parser and set a hard restriction that all events must
be a subtree having root tag S and containing at
least one noun phrase and one verb phrase.
Given these two restrictions, these patterns
provide us with high quality event-emotion pairs,
but the method is still lacking in two respects, lack
of coverage and lack of ability to aggregate sim-
ilar events. As both of these are essential to cre-
ating a high-quality and non-redundant dictionary
of events, we make two simple extensions to the
extraction process as follows.
3.1 Pattern Expansion
Pattern expansion, or bootstrapping algorithms are
widely used in the information extraction field
(Ravichandran and Hovy, 2002). In particular Es-
presso (Pantel and Pennacchiotti, 2006) is known
as a state-of-the-art pattern expansion algorithm
widely used in acquiring relationships between
entities. We omit the details of the algorithm
for space concerns, but note that applying the al-
gorithm to our proposed task is relatively straight-
forward, and allows us to acquire additional pat-
terns that may be matched to improve the cover-
age over the single seed pattern. We do, however,
make two changes to the algorithm. The first is
that, as we are interested in extracting events in-
stead of entities, we impose the previously men-
tioned restriction of one verb phrase and one noun
phrase over all events extracted by the patterns.
The second is that we perform normalization of
events to reduce their variability, namely removing
all function words, replacing proper nouns with
special symbol, and lemmatizing words.
3.2 Grouping events
The second improvement we perform is group-
ing the extracted events together. Grouping has a
number of potential practical advantages, as noted
frequently in previous work (Becker et al., 2011).
The first is that by grouping similar events to-
gether, we can relieve sparsity issues to some
extent by sharing statistics among the events in
a single group. The second is that aggregating
events together allows humans to browse the lists
more efficiently by reducing the number of re-
dundant entries. In preliminary experiments, we
attempted several clustering methods and even-
129
Emotions Events
happiness meeting friends going on a date getting something I want
sadness someone dies/gets sick someone insults me people leave me alone
anger someone insults me someone breaks a promise someone is too lazy
fear thinking about the future taking a test walking/driving at night
surprise seeing a friend unexpectedly someone comes to visit receiving a gift
Table 1: The top three events for each emotion.
tually settled on hierarchical agglomerative clus-
tering and the single-linkage criterion using co-
sine similarity as a distance measure (Gower and
Ross, 1969). Choosing the stopping criterion for
agglomerative clustering is somewhat subjective,
in many cases application dependent, but for the
evaluation in this work, we heuristically choose
the number of groups so the average number of
events in each group is four, and leave a further
investigation of the tuning to future work.
4 Evaluation Measures
Work on information extraction typically uses ac-
curacy and recall of the extracted information as
an evaluation measure. However, in this work, we
found that it is difficult to assign a clear-cut dis-
tinction between whether an event provokes a par-
ticular emotion or not. In addition, recall is diffi-
cult to measure, as there are essentially infinitely
many events. Thus, in this section, we propose two
new evaluation measures to measure the precision
and recall of the events that we recovered in this
task.
To evaluate the precision of the events extrac-
ted by our method, we focus on the fact that an
event might provoke multiple emotions, but usu-
ally these emotions can be ranked in prominence
or appropriateness. This is, in a way, similar to the
case of information retrieval, where there may be
many search results, but some are more appropri-
ate than others. Based on this observation, we fol-
low the information retrieval literature (Voorhees,
1999) in adapting mean reciprocal rank (MRR) as
an evaluation measure of the accuracy of our ex-
traction. In our case, one event can have multiple
emotions, so for each event that the system out-
puts, we ask an annotator to assign emotions in
descending order of prominence or appropriate-
ness, and assess MRR with respect to these ranked
emotions.
2
We also measure recall with respect to the
2
In the current work we did not allow annotators to assign
?ties? between the emotions, but this could be accommodated
in the MRR framework.
manually created dictionary described in Section
2, which gives us an idea of what percent of com-
mon emotions we were able to recover. It should
be noted that in order to measure recall, it is ne-
cessary to take a matching between the events out-
put by the system and the events in the previously
described list. While it would be ideal to do this
automatically, this is difficult due to small lexical
variations between the system output and the list.
Thus, for the current work we perform manual
matching between the system hypotheses and the
references, and hope to examine other ways of
matching in future work.
5 Experiments
In this section, we describe an experimental eval-
uation of the accuracy of automatic extraction of
emotion-provoking events.
5.1 Experimental Setup
We use Twitter
3
as a source of data, as it is it
provides a massive amount of information, and
also because users tend to write about what they
are doing as well as their thoughts, feelings and
emotions. We use a data set that contains more
than 30M English tweets posted during the course
of six weeks in June and July of 2012. To remove
noise, we perform a variety of preprocessing, re-
moving emoticons and tags, normalizing using
the scripts provided by Han and Baldwin (2011),
and Han et al. (2012). CoreNLP
4
was used to
get the information about part-of-speech, syntactic
parses, and lemmas.
We prepared four systems for comparison. As a
baseline, we use a method that only uses the ori-
ginal seed pattern mentioned in Section 3 to ac-
quire emotion-provoking events. We also evalu-
ate expansions to this method with clustering, with
pattern expansion, and with both.
We set a 10 iteration limit on the Espresso al-
gorithm and after each iteration, we add the 20
3
http://www.twitter.com
4
http://nlp.stanford.edu/software/
corenlp.shtml
130
Methods MRR Recall
Seed 46.3 (?5.0) 4.6 (?0.5)
Seed + clust 57.2 (?7.9) 8.5 (?0.9)
Espresso 49.4 (?2.8) 8.0 (?0.5)
Espresso + clust 71.7 (?2.9) 15.4 (?0.8)
Table 3: MRR and recall of extracted data (with
standard deviation for 3 annotators).
most reliable patterns to the pattern set, and in-
crease the seed set by one third of its size. These
values were set according to a manual inspection
of the results for several settings, before any eval-
uation was performed.
We examine the utility of each method accord-
ing to the evaluation measures proposed in Sec-
tion 4 over five emotions, happiness, sadness, an-
ger, fear, and surprise.
5
To measure MRR and
recall, we used the 20 most frequent events or
groups extracted by each method for these five
emotions, and thus all measures can be interpreted
as MRR@20 and recall@20. As manual annota-
tion is required to calculate both measures, we ac-
quired results for 3 annotators and report the aver-
age and standard deviation.
5.2 Experimental Results
The results are found in Table 3. From these res-
ults we can see that clustering the events causes a
significant gain on both MRR and recall, regard-
less of whether we use Espresso or not. Looking
at the results for Espresso, we see that it allows for
small boost in recall when used on its own, due
to the fact that the additional patterns help recover
more instances of each event, making the estimate
of frequency counts more robust. However, Es-
presso is more effective when used in combination
with clustering, showing that both methods are
capturing different varieties of information, both
of which are useful for the task.
In the end, the combination of pattern expansion
and clustering achieves an MRR of 71.7% and re-
call of 15.4%. While the MRR could be deemed
satisfactory, the recall is still relatively low. One
reason for this is that due to the labor-intensive
manual evaluation, it is not realistic to check many
more than the top 20 extracted events for each
emotion, making automatic evaluation metrics the
top on the agenda for future work.
5
We exclude disgust, as the seed only matched 26 times
over entire corpus, not enough for a reasonable evaluation.
Emotions MRR Recall
happiness 93.9 23.1
sadness 76.9 10.0
anger 76.5 14.0
fear 48.3 24.3
surprise 59.6 0.0
Table 4: Average MRR and recall by emotion for
the Espresso + clustering method.
However, even without considering this, we
found that the events extracted from Twitter
were somewhat biased towards common, everyday
events, or events regarding love and dating. On the
other hand, our annotators produced a wide vari-
ety of events including both everyday events, and
events that do not happen every day, but leave a
particularly strong impression when encountered.
This can be seen particularly in the accuracy and
recall results by emotion for the best system shown
in Table 4. We can see that for some emotions we
achieved recall approaching 25%, but for surprise
we didn?t manage to extract any of the emotions
created by the annotators at all, instead extracting
more mundane events such as ?surprised I?m not
fat yet? or ?surprised my mom hasn?t called me
yet.? Covering the rare, but important events is an
interesting challenge for expansions to this work.
6 Conclusion and Future Work
In this paper we described our work in creat-
ing a dictionary of emotion-provoking events, and
demonstrated results for four varieties of auto-
matic information extraction to expand this dic-
tionary. As this is the first attempt at acquiring dic-
tionaries of emotion-provoking events, there are
still many future directions that deserve further in-
vestigation. As mentioned in the experimental dis-
cussion, automatic matching for the evaluation of
event extraction, and ways to improve recall over
rarer but more impressive events are necessary.
There are also many improvements that could be
made to the extraction algorithm itself, including
more sophisticated clustering and pattern expan-
sion algorithms. Finally, it would be quite interest-
ing to use the proposed method as a tool for psy-
chological inquiry, including into the differences
between events that are extracted from Twitter and
other media, or the differences between different
demographics.
131
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identific-
ation on Twitter. In Proceedings of the Fifth Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM11).
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3-4):169?200.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Con-
ference on Language Resources and Evaluation,
pages 417?422.
John C Gower and GJS Ross. 1969. Minimum span-
ning trees and single linkage cluster analysis. Ap-
plied statistics, pages 54?64.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation diction-
ary for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432, Jeju Island, Korea,
July. Association for Computational Linguistics.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Patrick Pantel and Marco Pennacchiotti. 2006. Es-
presso: leveraging generic patterns for automatic-
ally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 113?120.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 41?47.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL
2003-Volume 4, pages 25?32. Association for Com-
putational Linguistics.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive ex-
amples extracted from the web. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08, pages
881?888.
Ro Valitutti. 2004. Wordnet-affect: an affective ex-
tension of wordnet. In In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 1083?1086.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 777?785.
Ellen M Voorhees. 1999. The trec-8 question an-
swering track report. In Proceedings of TREC,
volume 99, pages 77?82.
132
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632?641,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Unsupervised Model for Joint Phrase Alignment and Extraction
Graham Neubig1,2 Taro Watanabe2, Eiichiro Sumita2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We present an unsupervised model for joint
phrase alignment and extraction using non-
parametric Bayesian methods and inversion
transduction grammars (ITGs). The key con-
tribution is that phrases of many granulari-
ties are included directly in the model through
the use of a novel formulation that memorizes
phrases generated not only by terminal, but
also non-terminal symbols. This allows for
a completely probabilistic model that is able
to create a phrase table that achieves com-
petitive accuracy on phrase-based machine
translation tasks directly from unaligned sen-
tence pairs. Experiments on several language
pairs demonstrate that the proposed model
matches the accuracy of traditional two-step
word alignment/phrase extraction approach
while reducing the phrase table to a fraction
of the original size.
1 Introduction
The training of translation models for phrase-
based statistical machine translation (SMT) systems
(Koehn et al, 2003) takes unaligned bilingual train-
ing data as input, and outputs a scored table of
phrase pairs. This phrase table is traditionally gen-
erated by going through a pipeline of two steps, first
generating word (or minimal phrase) alignments,
then extracting a phrase table that is consistent with
these alignments.
However, as DeNero and Klein (2010) note, this
two step approach results in word alignments that
are not optimal for the final task of generating
phrase tables that are used in translation. As a so-
lution to this, they proposed a supervised discrimi-
native model that performs joint word alignment and
phrase extraction, and found that joint estimation of
word alignments and extraction sets improves both
word alignment accuracy and translation results.
In this paper, we propose the first unsuper-
vised approach to joint alignment and extraction of
phrases at multiple granularities. This is achieved
by constructing a generative model that includes
phrases at many levels of granularity, from minimal
phrases all the way up to full sentences. The model
is similar to previously proposed phrase alignment
models based on inversion transduction grammars
(ITGs) (Cherry and Lin, 2007; Zhang et al, 2008;
Blunsom et al, 2009), with one important change:
ITG symbols and phrase pairs are generated in
the opposite order. In traditional ITG models, the
branches of a biparse tree are generated from a non-
terminal distribution, and each leaf is generated by
a word or phrase pair distribution. As a result, only
minimal phrases are directly included in the model,
while larger phrases must be generated by heuris-
tic extraction methods. In the proposed model, at
each branch in the tree, we first attempt to gener-
ate a phrase pair from the phrase pair distribution,
falling back to ITG-based divide and conquer strat-
egy to generate phrase pairs that do not exist (or are
given low probability) in the phrase distribution.
We combine this model with the Bayesian non-
parametric Pitman-Yor process (Pitman and Yor,
1997; Teh, 2006), realizing ITG-based divide and
conquer through a novel formulation where the
Pitman-Yor process uses two copies of itself as a
632
base measure. As a result of this modeling strategy,
phrases of multiple granularities are generated, and
thus memorized, by the Pitman-Yor process. This
makes it possible to directly use probabilities of the
phrase model as a replacement for the phrase table
generated by heuristic extraction techniques.
Using this model, we perform machine transla-
tion experiments over four language pairs. We ob-
serve that the proposed joint phrase alignment and
extraction approach is able to meet or exceed results
attained by a combination of GIZA++ and heuristic
phrase extraction with significantly smaller phrase
table size. We also find that it achieves superior
BLEU scores over previously proposed ITG-based
phrase alignment approaches.
2 A Probabilistic Model for Phrase Table
Extraction
The problem of SMT can be defined as finding the
most probable target sentence e for the source sen-
tence f given a parallel training corpus ?E ,F?
e? = argmax
e
P (e|f , ?E ,F?).
We assume that there is a hidden set of parameters
? learned from the training data, and that e is condi-
tionally independent from the training corpus given
?. We take a Bayesian approach, integrating over all
possible values of the hidden parameters:
P (e|f , ?E ,F?) =
?
?
P (e|f , ?)P (?|?E ,F?). (1)
If ? takes the form of a scored phrase table, we
can use traditional methods for phrase-based SMT to
find P (e|f , ?) and concentrate on creating a model
for P (?|?E ,F?). We decompose this posterior prob-
ability using Bayes law into the corpus likelihood
and parameter prior probabilities
P (?|?E ,F?) ? P (?E ,F?|?)P (?).
In Section 3 we describe an existing method, and
in Section 4 we describe our proposed method for
modeling these two probabilities.
3 Flat ITG Model
There has been a significant amount of work in
many-to-many alignment techniques (Marcu and
Wong (2002), DeNero et al (2008), inter alia), and
in particular a number of recent works (Cherry and
Lin, 2007; Zhang et al, 2008; Blunsom et al, 2009)
have used the formalism of inversion transduction
grammars (ITGs) (Wu, 1997) to learn phrase align-
ments. By slightly limit reordering of words, ITGs
make it possible to exactly calculate probabilities
of phrasal alignments in polynomial time, which is
a computationally hard problem when arbitrary re-
ordering is allowed (DeNero and Klein, 2008).
The traditional flat ITG generative probabil-
ity for a particular phrase (or sentence) pair
Pflat(?e, f?; ?x, ?t) is parameterized by a phrase ta-
ble ?t and a symbol distribution ?x. We use the fol-
lowing generative story as a representative of the flat
ITG model.
1. Generate symbol x from the multinomial distri-
bution Px(x; ?x). x can take the values TERM,
REG, or INV.
2. According to the x take the following actions.
(a) If x = TERM, generate a phrase pair from
the phrase table Pt(?e, f?; ?t).
(b) If x = REG, a regular ITG rule, gener-
ate phrase pairs ?e1, f1? and ?e2, f2? from
Pflat, and concatenate them into a single
phrase pair ?e1e2, f1f2?.
(c) If x = INV, an inverted ITG rule, follows
the same process as (b), but concatenate
f1 and f2 in reverse order ?e1e2, f2f1?.
By taking the product of Pflat over every sentence
in the corpus, we are able to calculate the likelihood
P (?E ,F?|?) =
?
?e,f???E,F?
Pflat(?e, f?; ?).
We will refer to this model as FLAT.
3.1 Bayesian Modeling
While the previous formulation can be used as-is in
maximum likelihood training, this leads to a degen-
erate solution where every sentence is memorized as
a single phrase pair. Zhang et al (2008) and others
propose dealing with this problem by putting a prior
probability P (?x, ?t) on the parameters.
633
We assign ?x a Dirichlet prior1, and assign the
phrase table parameters ?t a prior using the Pitman-
Yor process (Pitman and Yor, 1997; Teh, 2006),
which is a generalization of the Dirichlet process
prior used in previous research. It is expressed as
?t ?PY (d, s, Pbase) (2)
where d is the discount parameter, s is the strength
parameter, and Pbase is the base measure. The dis-
count d is subtracted from observed counts, and
when it is given a large value (close to one), less
frequent phrase pairs will be given lower relative
probability than more common phrase pairs. The
strength s controls the overall sparseness of the dis-
tribution, and when it is given a small value the dis-
tribution will be sparse. Pbase is the prior probability
of generating a particular phrase pair, which we de-
scribe in more detail in the following section.
Non-parametric priors are well suited for mod-
eling the phrase distribution because every time a
phrase is generated by the model, it is ?memorized?
and given higher probability. Because of this, com-
mon phrase pairs are more likely to be re-used (the
rich-get-richer effect), which results in the induc-
tion of phrase tables with fewer, but more helpful
phrases. It is important to note that only phrases
generated by Pt are actually memorized and given
higher probability by the model. In FLAT, only min-
imal phrases generated after Px outputs the terminal
symbol TERM are generated from Pt, and thus only
minimal phrases are memorized by the model.
While the Dirichlet process is simply the Pitman-
Yor process with d = 0, it has been shown that the
discount parameter allows for more effective mod-
eling of the long-tailed distributions that are often
found in natural language (Teh, 2006). We con-
firmed in preliminary experiments (using the data
described in Section 7) that the Pitman-Yor process
with automatically adjusted parameters results in su-
perior alignment results, outperforming the sparse
Dirichlet process priors used in previous research2.
The average gain across all data sets was approxi-
mately 0.8 BLEU points.
1The value of ? had little effect on the results, so we arbi-
trarily set ? = 1.
2We put weak priors on s (Gamma(? = 2, ? = 1)) and
d (Beta(? = 2, ? = 2)) for the Pitman-Yor process, and set
? = 1?10 for the Dirichlet process.
3.2 Base Measure
Pbase in Equation (2) indicates the prior probability
of phrase pairs according to the model. By choosing
this probability appropriately, we can incorporate
prior knowledge of what phrases tend to be aligned
to each other. We calculate Pbase by first choosing
whether to generate an unaligned phrase pair (where
|e| = 0 or |f | = 0) according to a fixed probabil-
ity pu3, then generating from Pba for aligned phrase
pairs, or Pbu for unaligned phrase pairs.
For Pba, we adopt a base measure similar to that
used by DeNero et al (2008):
Pba(?e, f?) =M0(?e, f?)Ppois(|e|;?)Ppois(|f |;?)
M0(?e, f?) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?. As long phrases lead to spar-
sity, we set ? to a relatively small value to allow
us to bias against overly long phrases4. Pm1 is the
word-based Model 1 (Brown et al, 1993) probabil-
ity of one phrase given the other, which incorporates
word-based alignment information as prior knowl-
edge in the phrase translation probability. We take
the geometric mean5of the Model 1 probabilities in
both directions to encourage alignments that are sup-
ported by both models (Liang et al, 2006). It should
be noted that while Model 1 probabilities are used,
they are only soft constraints, compared with the
hard constraint of choosing a single word alignment
used in most previous phrase extraction approaches.
For Pbu, if g is the non-null phrase in e and f , we
calculate the probability as follows:
Pbu(?e, f?) = Puni(g)Ppois(|g|;?)/2.
Note that Pbu is divided by 2 as the probability is
considering null alignments in both directions.
4 Hierarchical ITG Model
While in FLAT only minimal phrases were memo-
rized by the model, as DeNero et al (2008) note
3We choose 10?2, 10?3, or 10?10 based on which value
gave the best accuracy on the development set.
4We tune ? to 1, 0.1, or 0.01 based on which value gives the
best performance on the development set.
5The probabilities of the geometric mean do not add to one,
but we found empirically that even when left unnormalized, this
provided much better results than the using the arithmetic mean,
which is more theoretically correct.
634
and we confirm in the experiments in Section 7, us-
ing only minimal phrases leads to inferior transla-
tion results for phrase-based SMT. Because of this,
previous research has combined FLAT with heuris-
tic phrase extraction, which exhaustively combines
all adjacent phrases permitted by the word align-
ments (Och et al, 1999). We propose an alterna-
tive, fully statistical approach that directly models
phrases at multiple granularities, which we will refer
to as HIER. By doing so, we are able to do away with
heuristic phrase extraction, creating a fully proba-
bilistic model for phrase probabilities that still yields
competitive results.
Similarly to FLAT, HIER assigns a probability
Phier(?e, f?; ?x, ?t) to phrase pairs, and is parame-
terized by a phrase table ?t and a symbol distribu-
tion ?x. The main difference from the generative
story of the traditional ITG model is that symbols
and phrase pairs are generated in the opposite order.
While FLAT first generates branches of the derivation
tree using Px, then generates leaves using the phrase
distribution Pt, HIER first attempts to generate the
full sentence as a single phrase from Pt, then falls
back to ITG-style derivations to cope with sparsity.
We allow for this within the Bayesian ITG context
by defining a new base measure Pdac (?divide-and-
conquer?) to replace Pbase in Equation (2), resulting
in the following distribution for ?t.
?t ? PY (d, s, Pdac) (3)
Pdac essentially breaks the generation of a sin-
gle longer phrase into two generations of shorter
phrases, allowing even phrase pairs for which
c(?e, f?) = 0 to be given some probability. The
generative process of Pdac, similar to that of Pflat
from the previous section, is as follows:
1. Generate symbol x from Px(x; ?x). x can take
the values BASE, REG, or INV.
2. According to x take the following actions.
(a) If x = BASE, generate a new phrase pair
directly from Pbase of Section 3.2.
(b) If x = REG, generate ?e1, f1? and ?e2, f2?
from Phier, and concatenate them into a
single phrase pair ?e1e2, f1f2?.
Figure 1: A word alignment (a), and its derivations ac-
cording to FLAT (b), and HIER (c). Solid and dotted lines
indicate minimal and non-minimal pairs respectively, and
phrases are written under their corresponding instance of
Pt. The pair hate/cou?te is generated from Pbase.
(c) If x = INV, follow the same process as
(b), but concatenate f1 and f2 in reverse
order ?e1e2, f2f1?.
A comparison of derivation trees for FLAT and
HIER is shown in Figure 1. As previously de-
scribed, FLAT first generates from the symbol dis-
tribution Px, then from the phrase distribution Pt,
while HIER generates directly from Pt, which falls
back to divide-and-conquer based on Px when nec-
essary. It can be seen that while Pt in FLAT only gen-
erates minimal phrases, Pt in HIER generates (and
thus memorizes) phrases at all levels of granularity.
4.1 Length-based Parameter Tuning
There are still two problems with HIER, one theo-
retical, and one practical. Theoretically, HIER con-
tains itself as its base measure, and stochastic pro-
cess models that include themselves as base mea-
sures are deficient, as noted in Cohen et al (2010).
Practically, while the Pitman-Yor process in HIER
shares the parameters s and d over all phrase pairs in
the model, long phrase pairs are much more sparse
635
Figure 2: Learned discount values by phrase pair length.
than short phrase pairs, and thus it is desirable to
appropriately adjust the parameters of Equation (2)
according to phrase pair length.
In order to solve these problems, we reformulate
the model so that each phrase length l = |f |+|e| has
its own phrase parameters ?t,l and symbol parame-
ters ?x,l, which are given separate priors:
?t,l ? PY (s, d, Pdac,l)
?x,l ? Dirichlet(?)
We will call this model HLEN.
The generative story is largely similar to HIER
with a few minor changes. When we generate a sen-
tence, we first choose its length l according to a uni-
form distribution over all possible sentence lengths
l ? Uniform(1, L),
where L is the size |e| + |f | of the longest sentence
in the corpus. We then generate a phrase pair from
the probability Pt,l(?e, f?) for length l. The base
measure for HLEN is identical to that of HIER, with
one minor change: when we fall back to two shorter
phrases, we choose the length of the left phrase from
ll ? Uniform(1, l ? 1), set the length of the right
phrase to lr = l?ll, and generate the smaller phrases
from Pt,ll and Pt,lr respectively.
It can be seen that phrases at each length are gen-
erated from different distributions, and thus the pa-
rameters for the Pitman-Yor process will be differ-
ent for each distribution. Further, as ll and lr must
be smaller than l, Pt,l no longer contains itself as a
base measure, and is thus not deficient.
An example of the actual discount values learned
in one of the experiments described in Section 7
is shown in Figure 2. It can be seen that, as ex-
pected, the discounts for short phrases are lower than
those of long phrases. In particular, phrase pairs of
length up to six (for example, |e| = 3, |f | = 3) are
given discounts of nearly zero while larger phrases
are more heavily discounted. We conjecture that this
is related to the observation by Koehn et al (2003)
that using phrases where max(|e|, |f |) ? 3 cause
significant improvements in BLEU score, while us-
ing larger phrases results in diminishing returns.
4.2 Implementation
Previous research has used a variety of sampling
methods to learn Bayesian phrase based alignment
models (DeNero et al, 2008; Blunsom et al, 2009;
Blunsom and Cohn, 2010). All of these techniques
are applicable to the proposed model, but we choose
to apply the sentence-based blocked sampling of
Blunsom and Cohn (2010), which has desirable con-
vergence properties compared to sampling single
alignments. As exhaustive sampling is too slow for
practical purpose, we adopt the beam search algo-
rithm of Saers et al (2009), and use a probability
beam, trimming spans where the probability is at
least 1010 times smaller than that of the best hypoth-
esis in the bucket.
One important implementation detail that is dif-
ferent from previous models is the management of
phrase counts. As a phrase pair ta may have been
generated from two smaller component phrases tb
and tc, when a sample containing ta is removed from
the distribution, it may also be necessary to decre-
ment the counts of tb and tc as well. The Chinese
Restaurant Process representation of Pt (Teh, 2006)
lends itself to a natural and easily implementable so-
lution to this problem. For each table representing a
phrase pair ta, we maintain not only the number of
customers sitting at the table, but also the identities
of phrases tb and tc that were originally used when
generating the table. When the count of the table
ta is reduced to zero and the table is removed, the
counts of tb and tc are also decremented.
5 Phrase Extraction
In this section, we describe both traditional heuris-
tic phrase extraction, and the proposed model-based
extraction method.
636
Figure 3: The phrase, block, and word alignments used
in heuristic phrase extraction.
5.1 Heuristic Phrase Extraction
The traditional method for heuristic phrase extrac-
tion from word alignments exhaustively enumerates
all phrases up to a certain length consistent with the
alignment (Och et al, 1999). Five features are used
in the phrase table: the conditional phrase proba-
bilities in both directions estimated using maximum
likelihood Pml(f |e) and Pml(e|f), lexical weight-
ing probabilities (Koehn et al, 2003), and a fixed
penalty for each phrase. We will call this heuristic
extraction from word alignments HEUR-W. These
word alignments can be acquired through the stan-
dard GIZA++ training regimen.
We use the combination of our ITG-based align-
ment with traditional heuristic phrase extraction as
a second baseline. An example of these alignments
is shown in Figure 3. In model HEUR-P, minimal
phrases generated from Pt are treated as aligned, and
we perform phrase extraction on these alignments.
However, as the proposed models tend to align rel-
atively large phrases, we also use two other tech-
niques to create smaller alignment chunks that pre-
vent sparsity. We perform regular sampling of the
trees, but if we reach a minimal phrase generated
from Pt, we continue traveling down the tree un-
til we reach either a one-to-many alignment, which
we will call HEUR-B as it creates alignments simi-
lar to the block ITG, or an at-most-one alignment,
which we will call HEUR-W as it generates word
alignments. It should be noted that forcing align-
ments smaller than the model suggests is only used
for generating alignments for use in heuristic extrac-
tion, and does not affect the training process.
5.2 Model-Based Phrase Extraction
We also propose a method for phrase table ex-
traction that directly utilizes the phrase probabil-
ities Pt(?e, f?). Similarly to the heuristic phrase
tables, we use conditional probabilities Pt(f |e)
and Pt(e|f), lexical weighting probabilities, and a
phrase penalty. Here, instead of using maximum
likelihood, we calculate conditional probabilities di-
rectly from Pt probabilities:
Pt(f |e) = Pt(?e, f?)/
?
{f? :c(?e,f??)?1}
Pt(?e, f??)
Pt(e|f) = Pt(?e, f?)/
?
{e?:c(?e?,f?)?1}
Pt(?e?, f?).
To limit phrase table size, we include only phrase
pairs that are aligned at least once in the sample.
We also include two more features: the phrase
pair joint probability Pt(?e, f?), and the average
posterior probability of each span that generated
?e, f? as computed by the inside-outside algorithm
during training. We use the span probability as it
gives a hint about the reliability of the phrase pair. It
will be high for common phrase pairs that are gen-
erated directly from the model, and also for phrases
that, while not directly included in the model, are
composed of two high probability child phrases.
It should be noted that while for FLAT and HIER Pt
can be used directly, as HLEN learns separate models
for each length, we must combine these probabilities
into a single value. We do this by setting
Pt(?e, f?) = Pt,l(?e, f?)c(l)/
L
?
l?=1
c(l?)
for every phrase pair, where l = |e|+ |f | and c(l) is
the number of phrases of length l in the sample.
We call this model-based extraction method MOD.
5.3 Sample Combination
As has been noted in previous works, (Koehn et al,
2003; DeNero et al, 2006) exhaustive phrase extrac-
tion tends to out-perform approaches that use syn-
tax or generative models to limit phrase boundaries.
DeNero et al (2006) state that this is because gen-
erative models choose only a single phrase segmen-
tation, and thus throw away many good phrase pairs
that are in conflict with this segmentation.
Luckily, in the Bayesian framework it is simple to
overcome this problem by combining phrase tables
637
from multiple samples. This is equivalent to approx-
imating the integral over various parameter configu-
rations in Equation (1). In MOD, we do this by taking
the average of the joint probability and span prob-
ability features, and re-calculating the conditional
probabilities from the averaged joint probabilities.
6 Related Work
In addition to the previously mentioned phrase
alignment techniques, there has also been a signif-
icant body of work on phrase extraction (Moore and
Quirk (2007), Johnson et al (2007a), inter alia).
DeNero and Klein (2010) presented the first work
on joint phrase alignment and extraction at multiple
levels. While they take a supervised approach based
on discriminative methods, we present a fully unsu-
pervised generative model.
A generative probabilistic model where longer
units are built through the binary combination of
shorter units was proposed by deMarcken (1996) for
monolingual word segmentation using the minimum
description length (MDL) framework. Our work dif-
fers in that it uses Bayesian techniques instead of
MDL, and works on two languages, not one.
Adaptor grammars, models in which non-
terminals memorize subtrees that lie below them,
have been used for word segmentation or other
monolingual tasks (Johnson et al, 2007b). The pro-
posed method could be thought of as synchronous
adaptor grammars over two languages. However,
adaptor grammars have generally been used to spec-
ify only two or a few levels as in the FLAT model in
this paper, as opposed to recursive models such as
HIER or many-leveled models such as HLEN. One
exception is the variational inference method for
adaptor grammars presented by Cohen et al (2010)
that is applicable to recursive grammars such as
HIER. We plan to examine variational inference for
the proposed models in future work.
7 Experimental Evaluation
We evaluate the proposed method on translation
tasks from four languages, French, German, Span-
ish, and Japanese, into English.
de-en es-en fr-en ja-en
TM (en) 1.80M 1.62M 1.35M 2.38M
TM (other) 1.85M 1.82M 1.56M 2.78M
LM (en) 52.7M 52.7M 52.7M 44.7M
Tune (en ) 49.8k 49.8k 49.8k 68.9k
Tune (other) 47.2k 52.6k 55.4k 80.4k
Test (en) 65.6k 65.6k 65.6k 40.4k
Test (other) 62.7k 68.1k 72.6k 48.7k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
7.1 Experimental Setup
The data for French, German, and Spanish are from
the 2010 Workshop on Statistical Machine Transla-
tion (Callison-Burch et al, 2010). We use the news
commentary corpus for training the TM, and the
news commentary and Europarl corpora for training
the LM. For Japanese, we use data from the NTCIR
patent translation task (Fujii et al, 2008). We use
the first 100k sentences of the parallel corpus for the
TM, and the whole parallel corpus for the LM. De-
tails of both corpora can be found in Table 1. Cor-
pora are tokenized, lower-cased, and sentences of
over 40 words on either side are removed for TM
training. For both tasks, we perform weight tuning
and testing on specified development and test sets.
We compare the accuracy of our proposed method
of joint phrase alignment and extraction using the
FLAT, HIER and HLEN models, with a baseline of
using word alignments from GIZA++ and heuris-
tic phrase extraction. Decoding is performed using
Moses (Koehn and others, 2007) using the phrase
tables learned by each method under consideration,
as well as standard bidirectional lexical reordering
probabilities (Koehn et al, 2005). Maximum phrase
length is limited to 7 in all models, and for the LM
we use an interpolated Kneser-Ney 5-gram model.
For GIZA++, we use the standard training reg-
imen up to Model 4, and combine alignments
with grow-diag-final-and. For the proposed
models, we train for 100 iterations, and use the final
sample acquired at the end of the training process for
our experiments using a single sample6. In addition,
6For most models, while likelihood continued to increase
gradually for all 100 iterations, BLEU score gains plateaued af-
ter 5-10 iterations, likely due to the strong prior information
638
de-en es-en fr-en ja-en
Align Extract # Samp. BLEU Size BLEU Size BLEU Size BLEU Size
GIZA++ HEUR-W 1 16.62 4.91M 22.00 4.30M 21.35 4.01M 23.20 4.22M
FLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7k
HIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723k
HLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820k
HIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21M
HLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70M
Table 2: BLEU score and phrase table size by alignment method, extraction method, and samples combined. Bold
numbers are not significantly different from the best result according to the sign test (p < 0.05) (Collins et al, 2005).
we also try averaging the phrase tables from the last
ten samples as described in Section 5.3.
7.2 Experimental Results
The results for these experiments can be found in Ta-
ble 2. From these results we can see that when using
a single sample, the combination of using HIER and
model probabilities achieves results approximately
equal to GIZA++ and heuristic phrase extraction.
This is the first reported result in which an unsu-
pervised phrase alignment model has built a phrase
table directly from model probabilities and achieved
results that compare to heuristic phrase extraction. It
can also be seen that the phrase table created by the
proposed method is approximately 5 times smaller
than that obtained by the traditional pipeline.
In addition, HIER significantly outperforms FLAT
when using the model probabilities. This confirms
that phrase tables containing only minimal phrases
are not able to achieve results that compete with
phrase tables that use multiple granularities.
Somewhat surprisingly, HLEN consistently
slightly underperforms HIER. This indicates
potential gains to be provided by length-based
parameter tuning were outweighed by losses due
to the increased complexity of the model. In
particular, we believe the necessity to combine
probabilities from multiple Pt,l models into a single
phrase table may have resulted in a distortion of the
phrase probabilities. In addition, the assumption
that phrase lengths are generated from a uniform
distribution is likely too strong, and further gains
provided by Pbase. As iterations took 1.3 hours on a single
processor, good translation results can be achieved in approxi-
mately 13 hours, which could further reduced using distributed
sampling (Newman et al, 2009; Blunsom et al, 2009).
FLAT HIER
MOD 17.97 117k 21.50 751k
HEUR-W 21.52 5.65M 21.68 5.39M
HEUR-B 21.45 4.93M 21.41 2.61M
HEUR-P 21.56 4.88M 21.47 1.62M
Table 3: Translation results and phrase table size for var-
ious phrase extraction techniques (French-English).
could likely be achieved by more accurate modeling
of phrase lengths. We leave further adjustments to
the HLEN model to future work.
It can also be seen that combining phrase tables
from multiple samples improved the BLEU score
for HLEN, but not for HIER. This suggests that for
HIER, most of the useful phrase pairs discovered by
the model are included in every iteration, and the in-
creased recall obtained by combining multiple sam-
ples does not consistently outweigh the increased
confusion caused by the larger phrase table.
We also evaluated the effectiveness of model-
based phrase extraction compared to heuristic phrase
extraction. Using the alignments from HIER, we cre-
ated phrase tables using model probabilities (MOD),
and heuristic extraction on words (HEUR-W), blocks
(HEUR-B), and minimal phrases (HEUR-P) as de-
scribed in Section 5. The results of these ex-
periments are shown in Table 3. It can be seen
that model-based phrase extraction using HIER out-
performs or insignificantly underperforms heuris-
tic phrase extraction over all experimental settings,
while keeping the phrase table to a fraction of the
size of most heuristic extraction methods.
Finally, we varied the size of the parallel corpus
for the Japanese-English task from 50k to 400k sen-
639
Figure 4: The effect of corpus size on the accuracy (a) and
phrase table size (b) for each method (Japanese-English).
tences and measured the effect of corpus size on
translation accuracy. From the results in Figure 4
(a), it can be seen that at all corpus sizes, the re-
sults from all three methods are comparable, with
insignificant differences between GIZA++ and HIER
at all levels, and HLEN lagging slightly behind HIER.
Figure 4 (b) shows the size of the phrase table in-
duced by each method over the various corpus sizes.
It can be seen that the tables created by GIZA++ are
significantly larger at all corpus sizes, with the dif-
ference being particularly pronounced at larger cor-
pus sizes.
8 Conclusion
In this paper, we presented a novel approach to joint
phrase alignment and extraction through a hierar-
chical model using non-parametric Bayesian meth-
ods and inversion transduction grammars. Machine
translation systems using phrase tables learned di-
rectly by the proposed model were able to achieve
accuracy competitive with the traditional pipeline of
word alignment and heuristic phrase extraction, the
first such result for an unsupervised model.
For future work, we plan to refine HLEN to use
a more appropriate model of phrase length than
the uniform distribution, particularly by attempting
to bias against phrase pairs where one of the two
phrases is much longer than the other. In addition,
we will test probabilities learned using the proposed
model with an ITG-based decoder. We will also ex-
amine the applicability of the proposed model in the
context of hierarchical phrases (Chiang, 2007), or
in alignment using syntactic structure (Galley et al,
2006). It is also worth examining the plausibility
of variational inference as proposed by Cohen et al
(2010) in the alignment context.
Acknowledgments
This work was performed while the first author
was supported by the JSPS Research Fellowship for
Young Scientists.
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proceed-
ings of the Human Language Technology: The 11th
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association for Computa-
tional Linguistics, pages 782?790.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint 5th Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL Workshop on Syntax and
Structure in Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars. In
Proceedings of the Human Language Technology: The
640
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 564?572.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 531?540.
Carl de Marcken. 1996. Unsupervised Language Acqui-
sition. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 25?28.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1453?1463.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of the 1st Workshop
on Statistical Machine Translation, pages 31?38.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 314?323.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of the 7th NTCIR Workshop Meeting on Evaluation of
Information Access Technologies, pages 389?400.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics, pages 961?968.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007a. Improving translation quality
by discarding most of the phrasetable. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
Advances in Neural Information Processing Systems,
19:641.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference (HLT-
NAACL), pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 104?111.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. pages 133?139.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of
the 2nd Workshop on Statistical Machine Translation,
pages 112?119.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 4th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 20?28.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the The 11th International Workshop
on Parsing Technologies.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 97?105.
641
Pointwise Prediction for Robust, Adaptable
Japanese Morphological Analysis
Graham Neubig, Yosuke Nakata, Shinsuke Mori
Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
Abstract
We present a pointwise approach to Japanese
morphological analysis (MA) that ignores
structure information during learning and tag-
ging. Despite the lack of structure, it is able to
outperform the current state-of-the-art struc-
tured approach for Japanese MA, and achieves
accuracy similar to that of structured predic-
tors using the same feature set. We also
find that the method is both robust to out-
of-domain data, and can be easily adapted
through the use of a combination of partial an-
notation and active learning.
1 Introduction
Japanese morphological analysis (MA) takes an un-
segmented string of Japanese text as input, and out-
puts a string of morphemes annotated with parts of
speech (POSs). As MA is the first step in Japanese
NLP, its accuracy directly affects the accuracy of
NLP systems as a whole. In addition, with the prolif-
eration of text in various domains, there is increasing
need for methods that are both robust and adaptable
to out-of-domain data (Escudero et al, 2000).
Previous approaches have used structured predic-
tors such as hidden Markov models (HMMs) or con-
ditional random fields (CRFs), which consider the
interactions between neighboring words and parts
of speech (Nagata, 1994; Asahara and Matsumoto,
2000; Kudo et al, 2004). However, while struc-
ture does provide valuable information, Liang et al
(2008) have shown that gains provided by struc-
tured prediction can be largely recovered by using a
richer feature set. This approach has also been called
?pointwise? prediction, as it makes a single indepen-
dent decision at each point (Neubig andMori, 2010).
While Liang et al (2008) focus on the speed ben-
efits of pointwise prediction, we demonstrate that it
also allows for more robust and adaptable MA. We
find experimental evidence that pointwise MA can
exceed the accuracy of a state-of-the-art structured
approach (Kudo et al, 2004) on in-domain data, and
is significantly more robust to out-of-domain data.
We also show that pointwise MA can be adapted
to new domains with minimal effort through the
combination of active learning and partial annota-
tion (Tsuboi et al, 2008), where only informative
parts of a particular sentence are annotated. In a
realistic domain adaptation scenario, we find that a
combination of pointwise prediction, partial annota-
tion, and active learning allows for easy adaptation.
2 Japanese Morphological Analysis
Japanese MA takes an unsegmented string of char-
acters xI1 as input, segments it into morphemes wJ1 ,
and annotates each morpheme with a part of speech
tJ1 . This can be formulated as a two-step process of
first segmenting words, then estimating POSs (Ng
and Low, 2004), or as a single joint process of find-
ing a morpheme/POS string from unsegmented text
(Kudo et al, 2004; Nakagawa, 2004; Kruengkrai et
al., 2009). In this section we describe an existing
joint sequence-based method for Japanese MA, as
well as our proposed two-step pointwise method.
2.1 Joint Sequence-Based MA
Japanese MA has traditionally used sequence based
models, finding a maximal POS sequence for en-
Figure 1: Joint MA (a) performs maximization over the
entire sequence, while two-step MA (b) maximizes the 4
boundary and 4 POS tags independently.
Type Feature Strings
Unigram tj , tjwj , c(wj), tjc(wj)
Bigram tj?1tj , tj?1tjwj?1,
tj?1tjwj , tj?1tjwj?1wj
Table 1: Features for the joint model using tags t and
words w. c(?) is a mapping function onto character types
(kanji, katakana, etc.).
tire sentences as in Figure 1 (a). The CRF-based
method presented by Kudo et al (2004) is gener-
ally accepted as the state-of-the-art in this paradigm.
CRFs are trained over segmentation lattices, which
allows for the handling of variable length sequences
that occur due to multiple segmentations. The model
is able to take into account arbitrary features, as well
as the context between neighboring tags.
We follow Kudo et al (2004) in defining our fea-
ture set, as summarized in Table 11. Lexical features
were trained for the top 5000 most frequent words in
the corpus. It should be noted that these are word-
based features, and information about transitions be-
tween POS tags is included. When creating training
data, the use of word-based features indicates that
word boundaries must be annotated, while the use
of POS transition information further indicates that
all of these words must be annotated with POSs.
1More fine-grained POS tags have provided small boosts in
accuracy in previous research (Kudo et al, 2004), but these in-
crease the annotation burden, which is contrary to our goal.
Type Feature Strings
Character xl, xr, xl?1xl, xlxr,
n-gram xrxr+1, xl?1xlxr, xlxrxr+1
Char. Type c(xl), c(xr)
n-gram c(xl?1xl), c(xlxr), c(xrxr+1)
c(xl?2xl?1xl), c(xl?1xlxr)
c(xlxrxr+1), c(xrxr+1xr+2)
WS Only ls, rs, is
POS Only wj , c(wj), djk
Table 2: Features for the two-step model. xl and xr indi-
cate the characters to the left and right of the word bound-
ary or word wj in question. ls, rs, and is represent the
left, right, and inside dictionary features, while djk indi-
cates that tag k exists in the dictionary for word j.
2.2 2-Step Pointwise MA
In our research, we take a two-step approach, first
segmenting character sequence xI1 into the word se-
quencewJ1 with the highest probability, then tagging
each word with parts of speech tJ1 . This approach is
shown in Figure 1 (b).
We follow Sassano (2002) in formulating word
segmentation as a binary classification problem, es-
timating boundary tags bI?11 . Tag bi = 1 indi-
cates that a word boundary exists between charac-
ters xi and xi+1, while bi = 0 indicates that a word
boundary does not exist. POS estimation can also
be formulated as a multi-class classification prob-
lem, where we choose one tag tj for each word wj .
These two classification problems can be solved by
tools in the standard machine learning toolbox such
as logistic regression (LR), support vector machines
(SVMs), or conditional random fields (CRFs).
We use information about the surrounding charac-
ters (character and character-type n-grams), as well
as the presence or absence of words in the dictio-
nary as features (Table 2). Specifically dictionary
features for word segmentation ls and rs are active
if a string of length s included in the dictionary is
present directly to the left or right of the present
word boundary, and is is active if the present word
boundary is included in a dictionary word of length
s. Dictionary feature djk for POS estimation indi-
cates whether the current word wj occurs as a dic-
tionary entry with tag tk.
Previous work using this two-stage approach has
used sequence-based prediction methods, such as
maximum entropy Markov models (MEMMs) or
CRFs (Ng and Low, 2004; Peng et al, 2004). How-
ever, as Liang et al (2008) note, and we confirm,
sequence-based predictors are often not necessary
when an appropriately rich feature set is used. One
important difference between our formulation and
that of Liang et al (2008) and all other previous
methods is that we rely only on features that are di-
rectly calculable from the surface string, without us-
ing estimated information such as word boundaries
or neighboring POS tags2. This allows for training
from sentences that are partially annotated as de-
scribed in the following section.
3 Domain Adaptation for Morphological
Analysis
NLP is now being used in domains such as medi-
cal text and legal documents, and it is necessary that
MA be easily adaptable to these areas. In a domain
adaptation situation, we have at our disposal both
annotated general domain data, and unannotated tar-
get domain data. We would like to annotate the
target domain data efficiently to achieve a maximal
gain in accuracy for a minimal amount of work.
Active learning has been used as a way to pick
data that is useful to annotate in this scenario for
several applications (Chan and Ng, 2007; Rai et
al., 2010) so we adopt an active-learning-based ap-
proach here. When adapting sequence-based predic-
tion methods, most active learning approaches have
focused on picking full sentences that are valuable to
annotate (Ringger et al, 2007; Settles and Craven,
2008). However, even within sentences, there are
generally a few points of interest surrounded by
large segments that are well covered by already an-
notated data.
Partial annotation provides a solution to this prob-
lem (Tsuboi et al, 2008; Sassano and Kurohashi,
2010). In partial annotation, data that will not con-
tribute to the improvement of the classifier is left
untagged. For example, if there is a single difficult
word in a long sentence, only the word boundaries
and POS of the difficult word will be tagged. ?Dif-
2Dictionary features are active if the string exists, regardless
of whether it is treated as a single word in wJ1 , and thus can be
calculated without the word segmentation result.
Type Train Test
General 782k 87.5k
Target 153k 17.3k
Table 3: General and target domain corpus sizes in words.
ficult? words can be selected using active learning
approaches, choosing words with the lowest classi-
fier accuracy to annotate. In addition, corpora that
are tagged with word boundaries but not POS tags
are often available; this is another type of partial an-
notation.
When using sequence-based prediction, learning
on partially annotated data is not straightforward,
as the data that must be used to train context-based
transition probabilities may be left unannotated. In
contrast, in the pointwise prediction framework,
training using this data is both simple and efficient;
unannotated points are simply ignored. A method
for learning CRFs from partially annotated data has
been presented by Tsuboi et al (2008). However,
when using partial annotation, CRFs? already slow
training time becomes slower still, as they must be
trained over every sequence that has at least one an-
notated point. Training time is important in an active
learning situation, as an annotator must wait while
the model is being re-trained.
4 Experiments
In order to test the effectiveness of pointwise MA,
we did an experiment measuring accuracy both on
in-domain data, and in a domain-adaptation situa-
tion. We used the Balanced Corpus of Contempo-
rary Written Japanese (BCCWJ) (Maekawa, 2008),
specifying the whitepaper, news, and books sections
as our general domain corpus, and the web text sec-
tion as our target domain corpus (Table 3).
As a representative of joint sequence-based MA
described in 2.1, we used MeCab (Kudo, 2006), an
open source implementation of Kudo et al (2004)?s
CRF-based method (we will call this JOINT). For the
pointwise two-step method, we trained logistic re-
gression models with the LIBLINEAR toolkit (Fan
et al, 2008) using the features described in Section
2.2 (2-LR). In addition, we trained a CRF-based
model with the CRFSuite toolkit (Okazaki, 2007)
using the same features and set-up (for both word
Train Test JOINT 2-CRF 2-LR
GEN GEN 97.31% 98.08% 98.03%
GEN TAR 94.57% 95.39% 95.13%
GEN+TAR TAR 96.45% 96.91% 96.82%
Table 4: Word/POS F-measure for each method when
trained and tested on general (GEN) or target (TAR) do-
main corpora.
segmentation and POS tagging) to examine the con-
tribution of context information (2-CRF).
To create the dictionary, we added all of the words
in the corpus, but left out a small portion of single-
tons to prevent overfitting on the training data3. As
an evaluation measure, we follow Nagata (1994) and
Kudo et al (2004) and use Word/POS tag pair F-
measure, so that both word boundaries and POS tags
must be correct for a word to be considered correct.
4.1 Analysis Results
In our first experiment we compared the accuracy of
the three methods on both the in-domain and out-
of-domain test sets (Table 4). It can be seen that
2-LR outperforms JOINT, and achieves similar but
slightly inferior results to 2-CRF. The reason for
accuracy gains over JOINT lies largely in the fact
that while JOINT is more reliant on the dictionary,
and thus tends to mis-segment unknown words, the
two-step methods are significantly more robust. The
small difference between 2-LR and 2-CRF indicates
that given a significantly rich feature set, context-
based features provide little advantage, although the
advantage is larger on out-of-domain data. In addi-
tion, training of 2-LR is significantly faster than 2-
CRF. 2-LR took 16m44s to train, while 2-CRF took
51m19s to train on a 3.33GHz Intel Xeon CPU.
4.2 Domain Adaptation
Our second experiment focused on the domain
adaptability of each method. Using the target do-
main training corpus as a pool of unannotated data,
we performed active learning-based domain adapta-
tion using two techniques.
? Sentence-based annotation (SENT), where sen-
tences with the lowest total POS and word
3For JOINT we removed singletons randomly until coverage
was 99.99%, and for 2-LR and 2-CRF coverage was set to 99%,
which gave the best results on held-out data.
Figure 2: Domain adaptation results for three approaches
and two annotation methods.
boundary probabilities were annotated first.
? Word-based partial annotation (PART), where
the word or word boundary with the smallest
probability margin between the first and second
candidates was chosen. This can only be used
with the pointwise 2-LR approach4 .
For both methods, 100 words (or for SENT until
the end of the sentence in which the 100th word
is reached) are annotated, then the classifier is re-
trained and new probability scores are generated.
Each set of 100 words is a single iteration, and 100
iterations were performed for each method.
From the results in Figure 2, it can be seen that
the combination of PART and 2-LR allows for sig-
nificantly faster adaptation than other approaches,
achieving accuracy gains in 15 iterations that are
achieved in 100 iterations with SENT, and surpassing
2-CRF after 15 iterations. Finally, it can be seen that
JOINT improves at a pace similar to PART, likely due
to the fact that its pre-adaptation accuracy is lower
than the other methods. It can be seen from Table 4
that even after adaptation with the full corpus, it will
still lag behind the two-step methods.
5 Conclusion
This paper proposed a pointwise approach to
Japanese morphological analysis. It showed that de-
spite the lack of structure, it was able to achieve re-
4In order to prevent wasteful annotation, each unique word
was only annotated once per iteration.
sults that meet or exceed structured prediction meth-
ods. We also demonstrated that it is both robust and
adaptable to out-of-domain text through the use of
partial annotation and active learning. Future work
in this area will include examination of performance
on other tasks and languages.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 21?27.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau.
2000. An empirical study of the domain dependence
of supervised word sense disambiguation systems. In
Proceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the 47th Annual Meeting of
the Association for Computational Linguistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 230?237.
Taku Kudo. 2006. MeCab: yet another
part-of-speech and morphological analyzer.
http://mecab.sourceforge.net.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th International Conference
on Machine Learning, pages 592?599.
Kikuo Maekawa. 2008. Balanced corpus of contempo-
rary written Japanese. In Proceedings of the 6th Work-
shop on Asian Language Resources, pages 101?102.
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A? N-
best search algorithm. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics,
pages 201?207.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th International
Conference on Computational Linguistics.
Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: one-at-a-time or all-at-once? word-
based or character-based. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th International Conference on Computational Lin-
guistics.
Piyush Rai, Avishek Saha, Hal Daume? III, and Suresh
Venkatasubramanian. 2010. Domain Adaptation
meets Active Learning. In Workshop on Active Learn-
ing for Natural Language Processing (ALNLP-10).
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop,
pages 101?108.
Manabu. Sassano and Sadao Kurohashi. 2010. Us-
ing smaller constituents rather than sentences in ac-
tive learning for Japanese dependency parsing. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 356?365.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 505?512.
Burr Settles and Mark Craven. 2008. An analysis of
active learning strategies for sequence labeling tasks.
In Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1070?1079.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke
Mori, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations. In
Proceedings of the 22th International Conference on
Computational Linguistics, pages 897?904.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Machine Translation without Words through Substring Alignment
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
In this paper, we demonstrate that accu-
rate machine translation is possible without
the concept of ?words,? treating MT as a
problem of transformation between character
strings. We achieve this result by applying
phrasal inversion transduction grammar align-
ment techniques to character strings to train
a character-based translation model, and us-
ing this in the phrase-based MT framework.
We also propose a look-ahead parsing algo-
rithm and substring-informed prior probabil-
ities to achieve more effective and efficient
alignment. In an evaluation, we demonstrate
that character-based translation can achieve
results that compare to word-based systems
while effectively translating unknown and un-
common words over several language pairs.
1 Introduction
Traditionally, the task of statistical machine trans-
lation (SMT) is defined as translating a source sen-
tence fJ1 = {f1, . . . , fJ} to a target sentence eI1 =
{e1, . . ., eI}, where each element of fJ1 and eI1 is
assumed to be a word in the source and target lan-
guages. However, the definition of a ?word? is of-
ten problematic. The most obvious example of this
lies in languages that do not separate words with
white space such as Chinese, Japanese, or Thai, in
which the choice of a segmentation standard has
a large effect on translation accuracy (Chang et
al., 2008). Even for languages with explicit word
The first author is now affiliated with the Nara Institute of Sci-
ence and Technology.
boundaries, all machine translation systems perform
at least some precursory form of tokenization, split-
ting punctuation and words to prevent the sparsity
that would occur if punctuated and non-punctuated
words were treated as different entities. Sparsity
also manifests itself in other forms, including the
large vocabularies produced by morphological pro-
ductivity, word compounding, numbers, and proper
names. A myriad of methods have been proposed
to handle each of these phenomena individually,
including morphological analysis, stemming, com-
pound breaking, number regularization, optimizing
word segmentation, and transliteration, which we
outline in more detail in Section 2.
These difficulties occur because we are translat-
ing sequences of words as our basic unit. On the
other hand, Vilar et al (2007) examine the possibil-
ity of instead treating each sentence as sequences of
characters to be translated. This method is attrac-
tive, as it is theoretically able to handle all sparsity
phenomena in a single unified framework, but has
only been shown feasible between similar language
pairs such as Spanish-Catalan (Vilar et al, 2007),
Swedish-Norwegian (Tiedemann, 2009), and Thai-
Lao (Sornlertlamvanich et al, 2008), which have
a strong co-occurrence between single characters.
As Vilar et al (2007) state and we confirm, accu-
rate translations cannot be achieved when applying
traditional translation techniques to character-based
translation for less similar language pairs.
In this paper, we propose improvements to the
alignment process tailored to character-based ma-
chine translation, and demonstrate that it is, in fact,
possible to achieve translation accuracies that ap-
165
proach those of traditional word-based systems us-
ing only character strings. We draw upon recent
advances in many-to-many alignment, which allows
for the automatic choice of the length of units to
be aligned. As these units may be at the charac-
ter, subword, word, or multi-word phrase level, we
conjecture that this will allow for better character
alignments than one-to-many alignment techniques,
and will allow for better translation of uncommon
words than traditional word-based models by break-
ing down words into their component parts.
We also propose two improvements to the many-
to-many alignment method of Neubig et al (2011).
One barrier to applying many-to-many alignment
models to character strings is training cost. In the
inversion transduction grammar (ITG) framework
(Wu, 1997), which is widely used in many-to-many
alignment, search is cumbersome for longer sen-
tences, a problem that is further exacerbated when
using characters instead of words as the basic unit.
As a step towards overcoming this difficulty, we in-
crease the efficiency of the beam-search technique of
Saers et al (2009) by augmenting it with look-ahead
probabilities in the spirit of A* search. Secondly,
we describe a method to seed the search process us-
ing counts of all substring pairs in the corpus to bias
the phrase alignment model. We do this by defining
prior probabilities based on these substring counts
within the Bayesian phrasal ITG framework.
An evaluation on four language pairs with differ-
ing morphological properties shows that for distant
language pairs, character-based SMT can achieve
translation accuracy comparable to word-based sys-
tems. In addition, we perform ablation studies,
showing that these results were not possible with-
out the proposed enhancements to the model. Fi-
nally, we perform a qualitative analysis, which finds
that character-based translation can handle unseg-
mented text, conjugation, and proper names in a uni-
fied framework with no additional processing.
2 Related Work on Data Sparsity in SMT
As traditional SMT systems treat all words as single
tokens without considering their internal structure,
major problems of data sparsity occur for less fre-
quent tokens. In fact, it has been shown that there
is a direct negative correlation between vocabulary
size (and thus sparsity) of a language and transla-
tion accuracy (Koehn, 2005). Sparsity causes trou-
ble for alignment models, both in the form of incor-
rectly aligned uncommon words, and in the form of
garbage collection, where uncommon words in one
language are incorrectly aligned to large segments
of the sentence in the other language (Och and Ney,
2003). Unknown words are also a problem during
the translation process, and the default approach is
to map them as-is into the target sentence.
This is a major problem in agglutinative lan-
guages such as Finnish or compounding languages
such as German. Previous works have attempted to
handle morphology, decompounding and regulariza-
tion through lemmatization, morphological analysis,
or unsupervised techniques (Nie?en and Ney, 2000;
Brown, 2002; Lee, 2004; Goldwater and McClosky,
2005; Talbot and Osborne, 2006; Mermer and Ak?n,
2010; Macherey et al, 2011). It has also been noted
that it is more difficult to translate into morpho-
logically rich languages, and methods for modeling
target-side morphology have attracted interest in re-
cent years (Bojar, 2007; Subotin, 2011).
Another source of data sparsity that occurs in all
languages is proper names, which have been handled
by using cognates or transliteration to improve trans-
lation (Knight and Graehl, 1998; Kondrak et al,
2003; Finch and Sumita, 2007), and more sophisti-
cated methods for named entity translation that com-
bine translation and transliteration have also been
proposed (Al-Onaizan and Knight, 2002).
Choosing word units is also essential for creat-
ing good translation results for languages that do
not explicitly mark word boundaries, such as Chi-
nese, Japanese, and Thai. A number of works have
dealt with this word segmentation problem in trans-
lation, mainly focusing on Chinese-to-English trans-
lation (Bai et al, 2008; Chang et al, 2008; Zhang et
al., 2008b; Chung and Gildea, 2009; Nguyen et al,
2010), although these works generally assume that a
word segmentation exists in one language (English)
and attempt to optimize the word segmentation in
the other language (Chinese).
We have enumerated these related works to
demonstrate the myriad of data sparsity problems
and proposed solutions. Character-based transla-
tion has the potential to handle all of the phenom-
ena in the previously mentioned research in a single
166
unified framework, requiring no language specific
tools such as morphological analyzers or word seg-
menters. However, while the approach is attractive
conceptually, previous research has only been shown
effective for closely related language pairs (Vilar et
al., 2007; Tiedemann, 2009; Sornlertlamvanich et
al., 2008). In this work, we propose effective align-
ment techniques that allow character-based transla-
tion to achieve accurate translation results for both
close and distant language pairs.
3 Alignment Methods
SMT systems are generally constructed from a par-
allel corpus consisting of target language sentences
E and source language sentences F . The first step
of training is to find alignments A for the words in
each sentence pair.
We represent our target and source sentences as
eI1 and fJ1 . ei and fj represent single elements of
the target and source sentences respectively. These
may be words in word-based alignment models or
single characters in character-based alignment mod-
els.1 We define our alignment as aK1 , where each
element is a span ak = ?s, t, u, v? indicating that the
target string es, . . . , et and source string fu, . . . , fv
are aligned to each-other.
3.1 One-to-Many Alignment
The most well-known and widely-used models for
bitext alignment are for one-to-many alignment, in-
cluding the IBM models (Brown et al, 1993) and
HMM alignment model (Vogel et al, 1996). These
models are by nature directional, attempting to find
the alignments that maximize the conditional prob-
ability of the target sentence P (eI1|fJ1 ,aK1 ). For
computational reasons, the IBM models are re-
stricted to aligning each word on the target side to
a single word on the source side. In the formal-
ism presented above, this means that each ei must
be included in at most one span, and for each span
u = v. Traditionally, these models are run in both
directions and combined using heuristics to create
many-to-many alignments (Koehn et al, 2003).
However, in order for one-to-many alignment
methods to be effective, each fj must contain
1Some previous work has also performed alignment using
morphological analyzers to normalize or split the sentence into
morpheme streams (Corston-Oliver and Gamon, 2004).
enough information to allow for effective alignment
with its corresponding elements in eI1. While this is
often the case in word-based models, for character-
based models this assumption breaks down, as there
is often no clear correspondence between characters.
3.2 Many-to-Many Alignment
On the other hand, in recent years, there have been
advances in many-to-many alignment techniques
that are able to align multi-element chunks on both
sides of the translation (Marcu and Wong, 2002;
DeNero et al, 2008; Blunsom et al, 2009; Neu-
big et al, 2011). Many-to-many methods can be ex-
pected to achieve superior results on character-based
alignment, as the aligner can use information about
substrings, which may correspond to letters, mor-
phemes, words, or short phrases.
Here, we focus on the model presented by Neu-
big et al (2011), which uses Bayesian inference in
the phrasal inversion transduction grammar (ITG,
Wu (1997)) framework. ITGs are a variety of syn-
chronous context free grammar (SCFG) that allows
for many-to-many alignment to be achieved in poly-
nomial time through the process of biparsing, which
we explain more in the following section. Phrasal
ITGs are ITGs that allow for non-terminals that can
emit phrase pairs with multiple elements on both
the source and target sides. It should be noted
that there are other many-to-many alignment meth-
ods that have been used for simultaneously discov-
ering morphological boundaries over multiple lan-
guages (Snyder and Barzilay, 2008; Naradowsky
and Toutanova, 2011), but these have generally been
applied to single words or short phrases, and it is not
immediately clear that they will scale to aligning full
sentences.
4 Look-Ahead Biparsing
In this work, we experiment with the alignment
method of Neubig et al (2011), which can achieve
competitive accuracy with a much smaller phrase ta-
ble than traditional methods. This is important in
the character-based translation context, as we would
like to use phrases that contain large numbers of
characters without creating a phrase table so large
that it cannot be used in actual decoding. In this
framework, training is performed using sentence-
167
Figure 1: (a) A chart with inside probabilities in boxes
and forward/backward probabilities marking the sur-
rounding arrows. (b) Spans with corresponding look-
aheads added, and the minimum probability underlined.
Lightly and darkly shaded spans will be trimmed when
the beam is log(P ) ? ?3 and log(P ) ? ?6 respectively.
wise block sampling, acquiring a sample for each
sentence by first performing bottom-up biparsing to
create a chart of probabilities, then performing top-
down sampling of a new tree based on the probabil-
ities in this chart.
An example of a chart used in this parsing can
be found in Figure 1 (a). Within each cell of the
chart spanning ets and fvu is an ?inside? probabil-
ity I(as,t,u,v). This probability is the combination
of the generative probability of each phrase pair
Pt(ets,fvu) as well as the sum the probabilities over
all shorter spans in straight and inverted order2
I(as,t,u,v) = Pt(ets, fvu)
+
?
s?S?t
?
u?U?v
Px(str)I(as,S,u,U )I(aS,t,U,v)
+
?
s?S?t
?
u?U?v
Px(inv)I(as,S,U,v)I(aS,t,u,U )
where Px(str) and Px(inv) are the probability of
straight and inverted ITG productions.
While the exact calculation of these probabilities
can be performed in O(n6) time, where n is the
2Pt can be specified according to Bayesian statistics as de-
scribed by Neubig et al (2011).
length of the sentence, this is impractical for all but
the shortest sentences. Thus it is necessary to use
methods to reduce the search space such as beam-
search based chart parsing (Saers et al, 2009) or
slice sampling (Blunsom and Cohn, 2010).3
In this section we propose the use of a look-ahead
probability to increase the efficiency of this chart
parsing. Taking the example of Saers et al (2009),
spans are pushed onto a different queue based on
their size, and queues are processed in ascending or-
der of size. Agendas can further be trimmed based
on a histogram beam (Saers et al, 2009) or probabil-
ity beam (Neubig et al, 2011) compared to the best
hypothesis a?. In other words, we have a queue dis-
cipline based on the inside probability, and all spans
ak where I(ak) < cI(a?) are pruned. c is a constant
describing the width of the beam, and a smaller con-
stant probability will indicate a wider beam.
This method is insensitive to the existence of
competing hypotheses when performing pruning.
Figure 1 (a) provides an example of why it is unwise
to ignore competing hypotheses during beam prun-
ing. Particularly, the alignment ?les/1960s? com-
petes with the high-probability alignment ?les/the,?
so intuitively should be a good candidate for prun-
ing. However its probability is only slightly higher
than ?anne?es/1960s,? which has no competing hy-
potheses and thus should not be trimmed.
In order to take into account competing hypothe-
ses, we can use for our queue discipline not only the
inside probability I(ak), but also the outside proba-
bility O(ak), the probability of generating all spans
other than ak, as in A* search for CFGs (Klein and
Manning, 2003), and tic-tac-toe pruning for word-
based ITGs (Zhang and Gildea, 2005). As the cal-
culation of the actual outside probability O(ak) is
just as expensive as parsing itself, it is necessary to
approximate this with heuristic function O? that can
be calculated efficiently.
Here we propose a heuristic function that is de-
signed specifically for phrasal ITGs and is com-
putable with worst-case complexity of n2, compared
with the n3 amortized time of the tic-tac-toe pruning
3Applying beam-search before sampling will sample from
an improper distribution, although Metropolis-in-Gibbs sam-
pling (Johnson et al, 2007) can be used to compensate. How-
ever, we found that this had no significant effect on results, so
we omit the Metropolis-in-Gibbs step for experiments.
168
algorithm described by (Zhang et al, 2008a). Dur-
ing the calculation of the phrase generation proba-
bilities Pt, we save the best inside probability I? for
each monolingual span.
I?e (s, t) = max
{a?=?s?,t?,u?,v??;s?=s,t?=t}
Pt(a?)
I?f (u, v) = max
{a?=?s?,t?,u?,v??;u?=u,v?=v}
Pt(a?)
For each language independently, we calculate for-
ward probabilities ? and backward probabilities ?.
For example, ?e(s) is the maximum probability of
the span (0, s) of e that can be created by concate-
nating together consecutive values of I?e :
?e(s) = max
{S1,...,Sx}
I?e (0, S1)I?e (S1, S2) . . . I?e (Sx, s).
Backwards probabilities and probabilities over f can
be defined similarly. These probabilities are calcu-
lated for e and f independently, and can be calcu-
lated in n2 time by processing each ? in ascending
order, and each ? in descending order in a fashion
similar to that of the forward-backward algorithm.
Finally, for any span, we define the outside heuristic
as the minimum of the two independent look-ahead
probabilities over each language
O?(as,t,u,v) = min(?e(s) ? ?e(t), ?f (u) ? ?f (v)).
Looking again at Figure 1 (b), it can be seen
that the relative probability difference between the
highest probability span ?les/the? and the spans
?anne?es/1960s? and ?60/1960s? decreases, allowing
for tighter beam pruning without losing these good
hypotheses. In contrast, the relative probability of
?les/1960s? remains low as it is in conflict with a
high-probability alignment, allowing it to be dis-
carded.
5 Substring Prior Probabilities
While the Bayesian phrasal ITG framework uses
the previously mentioned phrase distribution Pt dur-
ing search, it also allows for definition of a phrase
pair prior probability Pprior(ets,fvu), which can ef-
ficiently seed the search process with a bias towards
phrase pairs that satisfy certain properties. In this
section, we overview an existing method used to cal-
culate these prior probabilities, and also propose a
new way to calculate priors based on substring co-
occurrence statistics.
5.1 Word-based Priors
Previous research on many-to-many translation has
used IBM model 1 probabilities to bias phrasal
alignments so that phrases whose member words are
good translations are also aligned. As a representa-
tive of this existing method, we adopt a base mea-
sure similar to that used by DeNero et al (2008):
Pm1(e,f) =M0(e,f)Ppois(|e|;?)Ppois(|f |;?)
M0(e,f) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?, which we set to 0.01. Pm1 is the
word-based (or character-based) Model 1 probabil-
ity, which can be efficiently calculated using the dy-
namic programming algorithm described by Brown
et al (1993). However, for reasons previously stated
in Section 3, these methods are less satisfactory
when performing character-based alignment, as the
amount of information contained in a character does
not allow for proper alignment.
5.2 Substring Co-occurrence Priors
Instead, we propose a method for using raw sub-
string co-occurrence statistics to bias alignments to-
wards substrings that often co-occur in the entire
training corpus. This is similar to the method of
Cromieres (2006), but instead of using these co-
occurrence statistics as a heuristic alignment crite-
rion, we incorporate them as a prior probability in
a statistical model that can take into account mutual
exclusivity of overlapping substrings in a sentence.
We define this prior probability using three counts
over substrings c(e), c(f), and c(e,f). c(e) and
c(f) count the total number of sentences in which
the substrings e and f occur respectively. c(e,f) is
a count of the total number of sentences in which the
substring e occurs on the target side, and f occurs
on the source side. We perform the calculation of
these statistics using enhanced suffix arrays, a data
structure that can efficiently calculate all substrings
in a corpus (Abouelhoda et al, 2004).4
While suffix arrays allow for efficient calculation
of these statistics, storing all co-occurrence counts
c(e,f) is an unrealistic memory burden for larger
4Using the open-source implementation esaxx http://
code.google.com/p/esaxx/
169
corpora. In order to reduce the amount of mem-
ory used, we discount every count by a constant d,
which we set to 5. This has a dual effect of reducing
the amount of memory needed to hold co-occurrence
counts by removing values for which c(e,f) < d, as
well as preventing over-fitting of the training data. In
addition, we heuristically prune values for which the
conditional probabilities P (e|f) or P (f |e) are less
than some fixed value, which we set to 0.1 for the
reported experiments.
To determine how to combine c(e), c(f), and
c(e,f) into prior probabilities, we performed pre-
liminary experiments testing methods proposed by
previous research including plain co-occurrence
counts, the Dice coefficient, and ?-squared statistics
(Cromieres, 2006), as well as a newmethod of defin-
ing substring pair probabilities to be proportional to
bidirectional conditional probabilities
Pcooc(e,f) = Pcooc(e|f)Pcooc(f |e)/Z
=
(
c(e,f) ? d
c(f) ? d
)(
c(e,f) ? d
c(e) ? d
)
/Z
for all substring pairs where c(e,f) > d and where
Z is a normalization term equal to
Z =
?
{e,f ;c(e,f)>d}
Pcooc(e|f)Pcooc(f |e).
The experiments showed that the bidirectional con-
ditional probability method gave significantly better
results than all other methods, so we adopt this for
the remainder of our experiments.
It should be noted that as we are using discount-
ing, many substring pairs will be given zero proba-
bility according to Pcooc. As the prior is only sup-
posed to bias the model towards good solutions and
not explicitly rule out any possibilities, we linearly
interpolate the co-occurrence probability with the
one-to-many Model 1 probability, which will give
at least some probability mass to all substring pairs
Pprior(e,f) = ?Pcooc(e,f) + (1 ? ?)Pm1(e,f).
We put a Dirichlet prior (? = 1) on the interpolation
coefficient ? and learn it during training.
6 Experiments
In order to test the effectiveness of character-based
translation, we performed experiments over a variety
of language pairs and experimental settings.
de-en fi-en fr-en ja-en
TM (en) 2.80M 3.10M 2.77M 2.13M
TM (other) 2.56M 2.23M 3.05M 2.34M
LM (en) 16.0M 15.5M 13.8M 11.5M
LM (other) 15.3M 11.3M 15.6M 11.9M
Tune (en) 58.7k 58.7k 58.7k 30.8k
Tune (other) 55.1k 42.0k 67.3k 34.4k
Test (en) 58.0k 58.0k 58.0k 26.6k
Test (other) 54.3k 41.4k 66.2k 28.5k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
6.1 Experimental Setup
We use a combination of four languages with En-
glish, using freely available data. We selected
French-English, German-English, Finnish-English
data from EuroParl (Koehn, 2005), with develop-
ment and test sets designated for the 2005 ACL
shared task on machine translation.5 We also did
experiments with Japanese-English Wikipedia arti-
cles from the Kyoto Free Translation Task (Neu-
big, 2011) using the designated training and tuning
sets, and reporting results on the test set. These lan-
guages were chosen as they have a variety of inter-
esting characteristics. French has some inflection,
but among the test languages has the strongest one-
to-one correspondence with English, and is gener-
ally considered easy to translate. German has many
compound words, which must be broken apart to
translate properly into English. Finnish is an ag-
glutinative language with extremely rich morphol-
ogy, resulting in long words and the largest vocab-
ulary of the languages in EuroParl. Japanese does
not have any clear word boundaries, and uses logo-
graphic characters, which contain more information
than phonetic characters.
With regards to data preparation, the EuroParl
data was pre-tokenized, so we simply used the to-
kenized data as-is for the training and evaluation of
all models. For word-based translation in the Kyoto
task, training was performed using the provided tok-
enization scripts. For character-based translation, no
tokenization was performed, using the original text
for both training and decoding. For both tasks, we
selected as training data all sentences for which both
5http://statmt.org/wpt05/mt-shared-task
170
de-en fi-en fr-en ja-en
GIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70
ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89
GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34
ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58
en-de en-fi en-fr en-ja
GIZA-word 17.94 / 62.71 / 37.88 13.22 / 58.50 / 27.03 32.19 / 69.20 / 52.39 20.79 / 27.01 / 38.41
ITG-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34
GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67
ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71
Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal
ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ-
ence from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004).
source and target were 100 characters or less,6 the
total size of which is shown in Table 1. In character-
based translation, white spaces between words were
treated as any other character and not given any spe-
cial treatment. Evaluation was performed on tok-
enized and lower-cased data.
For alignment, we use the GIZA++ implementa-
tion of one-to-many alignment7 and the pialign im-
plementation of the phrasal ITG models8 modified
with the proposed improvements. For GIZA++, we
used the default settings for word-based alignment,
but used the HMM model for character-based align-
ment to allow for alignment of longer sentences.
For pialign, default settings were used except for
character-based ITG alignment, which used a prob-
ability beam of 10?4 instead 10?10.9 For decoding,
we use the Moses decoder,10 using the default set-
tings except for the stack size, which we set to 1000
instead of 200. Minimum error rate training was per-
formed to maximize word-based BLEU score for all
systems.11 For language models, word-based trans-
lation uses a word 5-gram model, and character-
based translation uses a character 12-gram model,
both smoothed using interpolated Kneser-Ney.
6100 characters is an average of 18.8 English words
7http://code.google.com/p/giza-pp/
8http://phontron.com/pialign/
9Improvement by using a beam larger than 10?4 was
marginal, especially with co-occurrence prior probabilities.
10http://statmt.org/moses/
11We chose this set-up to minimize the effect of tuning crite-
rion on our experiments, although it does indicate that we must
have access to tokenized data for the development set.
6.2 Quantitative Evaluation
Table 2 presents a quantitative analysis of the trans-
lation results for each of the proposed methods. As
previous research has shown that it is more diffi-
cult to translate into morphologically rich languages
than into English (Koehn, 2005), we perform exper-
iments translating in both directions for all language
pairs. We evaluate translation quality using BLEU
score (Papineni et al, 2002), both on the word and
character level (with n = 4), as well as METEOR
(Denkowski and Lavie, 2011) on the word level.
It can be seen that character-based translation
with all of the proposed alignment improvements
greatly exceeds character-based translation using
one-to-many alignment, confirming that substring-
based information is necessary for accurate align-
ments. When compared with word-based trans-
lation, character-based translation achieves better,
comparable, or inferior results on character-based
BLEU, comparable or inferior results on METEOR,
and inferior results on word-based BLEU. The dif-
ferences between the evaluation metrics are due to
the fact that character-based translation often gets
words mostly correct other than one or two letters.
These are given partial credit by character-based
BLEU (and to a lesser extent METEOR), but marked
entirely wrong by word-based BLEU.
Interestingly, for translation into English,
character-based translation achieves higher ac-
curacy compared to word-based translation on
Japanese and Finnish input, followed by German,
171
fi-en ja-en
ITG-word 2.851 2.085
ITG-char 2.826 2.154
Table 3: Human evaluation scores (0-5 scale).
Ref: directive on equality
Source Unk. Word: tasa-arvodirektiivi
(13/26) Char: equality directive
Ref: yoshiwara-juku station
Target Unk. Word: yoshiwara no eki
(5/26) Char: yoshiwara-juku station
Ref: world health organisation
Uncommon Word: world health
(5/26) Char: world health organisation
Table 4: The major gains of character-based translation,
unknown, hyphenated, and uncommon words.
and finally French. This confirms that character-
based translation is performing well on languages
that have long words or ambiguous boundaries, and
less well on language pairs with relatively strong
one-to-one correspondence between words.
6.3 Qualitative Evaluation
In addition, we performed a subjective evaluation of
Japanese-English and Finnish-English translations.
Two raters evaluated 100 sentences each, assigning
a score of 0-5 based on how well the translation con-
veys the information contained in the reference. We
focus on shorter sentences of 8-16 English words to
ease rating and interpretation. Table 3 shows that
the results are comparable, with no significant dif-
ference in average scores for either language pair.
Table 4 shows a breakdown of the sentences for
which character-based translation received a score
of at 2+ points more than word-based. It can be seen
that character-based translation is properly handling
sparsity phenomena. On the other hand, word-based
translation was generally stronger with reordering
and lexical choice of more common words.
6.4 Effect of Alignment Method
In this section, we compare the translation accura-
cies for character-based translation using the phrasal
ITG model with and without the proposed improve-
ments of substring co-occurrence priors and look-
ahead parsing as described in Sections 4 and 5.2.
fi-en en-fi ja-en en-ja
ITG +cooc +look 28.94 25.31 24.58 35.71
ITG +cooc -look 28.51 24.24 24.32 35.74
ITG -cooc +look 28.65 24.49 24.36 35.05
ITG -cooc -look 27.45 23.30 23.57 34.50
Table 5: METEOR scores for alignment with and without
look-ahead and co-occurrence priors.
Figure 5 shows METEOR scores12 for experi-
ments translating Japanese and Finnish. It can be
seen that the co-occurrence prior gives gains in all
cases, indicating that substring statistics are effec-
tively seeding the ITG aligner. The introduced look-
ahead probabilities improve accuracy significantly
when substring co-occurrence counts are not used,
and slightly when co-occurrence counts are used.
More importantly, they allow for more aggressive
beam pruning, increasing sampling speed from 1.3
sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6
sent/s for Japanese.
7 Conclusion and Future Directions
This paper demonstrated that character-based trans-
lation can act as a unified framework for handling
difficult problems in translation: morphology, com-
pound words, transliteration, and segmentation.
One future challenge includes scaling training up
to longer sentences, which can likely be achieved
through methods such as the heuristic span prun-
ing of Haghighi et al (2009) or sentence splitting
of Vilar et al (2007). Monolingual data could also
be used to improve estimates of our substring-based
prior. In addition, error analysis showed that word-
based translation performed better than character-
based translation on reordering and lexical choice,
indicating that improved decoding (or pre-ordering)
and language modeling tailored to character-based
translation will likely greatly improve accuracy. Fi-
nally, we plan to explore the middle ground between
word-based and character based translation, allow-
ing for the flexibility of character-based translation,
while using word boundary information to increase
efficiency and accuracy.
12Similar results were found for character and word-based
BLEU, but are omitted for lack of space.
172
References
Mohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohle-
busch. 2004. Replacing suffix trees with enhanced
suffix arrays. Journal of Discrete Algorithms, 2(1).
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. ACL.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proc. IJCNLP.
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proc.
HLT-NAACL, pages 238?241.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proc. ACL.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proc. WMT.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proc. TMI.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
WMT.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004. Nor-
malizing German and English inflectional morphology
to improve statistical word alignment. Machine Trans-
lation: From Real Users to Research.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proc. COL-
ING/ACL 2006 Student Research Workshop.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Proc.
WMT.
Andrew Finch and Eiichiro Sumita. 2007. Phrase-based
machine transliteration. In Proc. TCAST.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proc. EMNLP.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proc. ACL.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proc. NAACL.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: fast exact Viterbi parse selection. In Proc. HLT.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. HLT,
pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proc. HLT.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. HLT.
Klaus Macherey, Andrew Dai, David Talbot, Ashok
Popat, and Franz Och. 2011. Language-independent
compound splitting with morphological operations. In
Proc. ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsu-
pervised search for the optimal segmentation for sta-
tistical machine translation. In Proc. ACL Student Re-
search Workshop.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proc. ACL.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL, pages 632?641, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation task.
http://www.phontron.com/kftt.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. COLING.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Proc. COL-
ING.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. COLING.
173
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proc. IWPT, pages 29?32.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. Proc. ACL.
Virach Sornlertlamvanich, Chumpol Mokarat, and Hi-
toshi Isahara. 2008. Thai-lao machine translation
based on phoneme transfer. In Proc. 14th Annual
Meeting of the Association for Natural Language Pro-
cessing.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
David Talbot and Miles Osborne. 2006. Modelling lexi-
cal redundancy for machine translation. In Proc. ACL.
Jo?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proc. 13th Annual
Conference of the European Association for Machine
Translation.
David Vilar, Jan-T. Peter, and Hermann Ney. 2007. Can
we translate letters. In Proc. WMT.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proc. ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of
non-compositional phrases with synchronous parsing.
Proc. ACL.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proc. WMT.
174
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678?683,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptation Data Selection using Neural Language Models:
Experiments in Machine Translation
Kevin Duh, Graham Neubig
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Japan
kevinduh@is.naist.jp
neubig@is.naist.jp
Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Labs.
NTT Corporation
2-4 Hikaridai, Seika, Kyoto, Japan
sudoh.katsuhito@lab.ntt.co.jp
tsukada.hajime@lab.ntt.co.jp
Abstract
Data selection is an effective approach
to domain adaptation in statistical ma-
chine translation. The idea is to use lan-
guage models trained on small in-domain
text to select similar sentences from large
general-domain corpora, which are then
incorporated into the training data. Sub-
stantial gains have been demonstrated in
previous works, which employ standard n-
gram language models. Here, we explore
the use of neural language models for data
selection. We hypothesize that the con-
tinuous vector representation of words in
neural language models makes them more
effective than n-grams for modeling un-
known word contexts, which are prevalent
in general-domain text. In a comprehen-
sive evaluation of 4 language pairs (En-
glish to German, French, Russian, Span-
ish), we found that neural language mod-
els are indeed viable tools for data se-
lection: while the improvements are var-
ied (i.e. 0.1 to 1.7 gains in BLEU), they
are fast to train on small in-domain data
and can sometimes substantially outper-
form conventional n-grams.
1 Introduction
A perennial challenge in building Statistical Ma-
chine Translation (SMT) systems is the dearth
of high-quality bitext in the domain of interest.
An effective and practical solution is adaptation
data selection: the idea is to use language models
(LMs) trained on in-domain text to select similar
sentences from large general-domain corpora. The
selected sentences are then incorporated into the
SMT training data. Analyses have shown that this
augmented data can lead to better statistical esti-
mation or word coverage (Duh et al, 2010; Had-
dow and Koehn, 2012).
Although previous works in data selection (Ax-
elrod et al, 2011; Koehn and Haddow, 2012; Ya-
suda et al, 2008) have shown substantial gains, we
suspect that the commonly-used n-gram LMs may
be sub-optimal. The small size of the in-domain
text implies that a large percentage of general-
domain sentences will contain words not observed
in the LM training data. In fact, as many as 60% of
general-domain sentences contain at least one un-
known word in our experiments. Although the LM
probabilities of these sentences could still be com-
puted by resorting to back-off and other smoothing
techniques, a natural question remains: will alter-
native, more robust LMs do better?
We hypothesize that the neural language model
(Bengio et al, 2003) is a viable alternative, since
its continuous vector representation of words is
well-suited for modeling sentences with frequent
unknown words, providing smooth probability es-
timates of unseen but similar contexts. Neu-
ral LMs have achieved positive results in speech
recognition and SMT reranking (Schwenk et al,
2012; Mikolov et al, 2011a). To the best of our
knowledge, this paper is the first work that exam-
ines neural LMs for adaptation data selection.
2 Data Selection Method
We employ the data selection method of (Ax-
elrod et al, 2011), which builds upon (Moore
and Lewis, 2010). The intuition is to select
general-domain sentences that are similar to in-
domain text, while being dis-similar to the average
general-domain text.
To do so, one defines the score of an general-
domain sentence pair (e, f) as:
[INE(e)?GENE(e)] + [INF (f)?GENF (f)]
(1)
where INE(e) is the length-normalized cross-
entropy of e on the English in-domain LM.
GENE(e) is the length-normalized cross-entropy
678
Figure 1: Recurrent neural LM.
of e on the English general-domain LM, which
is built from a sub-sample of the general-domain
text. Similarly, INF (f) and GENF (f) are the
cross-entropies of f on Foreign-side LM. Finally,
sentence pairs are ranked according to Eq. 1 and
those with scores lower than some empirically-
chosen threshold are added to the bitext for trans-
lation model training.
2.1 Neural Language Models
The four LMs used to compute Eq. 1 have con-
ventionally been n-grams. N-grams of the form
p(w(t)|w(t ? 1), w(t ? 2), . . .) predict words by
using multinomial distributions conditioned on the
context (w(t?1), w(t?2), . . .). But when the con-
text is rare or contains unknown words, n-grams
are forced to back-off to lower-order models, e.g.
p(w(t)|w(t ? 1)). These backoffs are unfortu-
nately very frequent in adaptation data selection.
Neural LMs, in contrast, model word probabili-
ties using continuous vector representations. Fig-
ure 1 shows a type of neural LMs called recurrent
neural networks (Mikolov et al, 2011b).1 Rather
than representing context as an identity (n-gram
hit-or-miss) function on [w(t ? 1), w(t ? 2), . . .],
neural LMs summarize the context by a hidden
state vector s(t). This is a continuous vector of
dimension |S| whose elements are predicted by
the previous word w(t ? 1) and previous state
s(t ? 1). This is robust to rare contexts because
continuous representations enable sharing of sta-
tistical strength between similar contexts. Bengio
(2009) shows that such representations are better
than multinomials in alleviating sparsity issues.
1Another major type of neural LMs are the so-called
feed-forward networks (Bengio et al, 2003; Schwenk, 2007;
Nakamura et al, 1990). Both types of neural LMs have seen
many improvements recently, in terms of computational scal-
ability (Le et al, 2011) and modeling power (Arisoy et al,
2012; Wu et al, 2012; Alexandrescu and Kirchhoff, 2006).
We focus on recurrent networks here since there are fewer
hyper-parameters and its ability to model infinite context us-
ing recursion is theoretically attractive. But we note that feed-
forward networks are just as viable.
Now, given state vector s(t), we can predict the
probability of the current word. Figure 1 is ex-
pressed formally in the following equations:
w(t) = [w0(t), . . . , wk(t), . . . w|W |(t)] (2)
wk(t) = g
?
?
|S|?
j=0
sj(t)Vkj
?
? (3)
sj(t)=f
?
?
|W |?
i=0
wi(t? 1)Uji +
|S|?
i?=0
si?(t? 1)Aji?
?
?
(4)
Here, w(t) is viewed as a vector of dimension
|W | (vocabulary size) where each element wk(t)
represents the probability of the k-th vocabulary
item at sentence position t. The function g(zk) =
ezk/?k ezk is a softmax function that ensures the
neural LM outputs are proper probabilities, and
f(z) = 1/(1 + e?z) is a sigmoid activation that
induces the non-linearity critical to the neural net-
work?s expressive power. The matrices V , U , and
A are trained by maximizing likelihood on train-
ing data using a ?backpropagation-through-time?
method.2 Intuitively, U and A compress the con-
text (|S| < |W |) such that contexts predictive of
the same word w(t) are close together.
Since proper modeling of unknown contexts is
important in our problem, training text for both n-
gram and neural LM is pre-processed by convert-
ing all low-frequency words in the training data
(frequency=1 in our case) to a special ?unknown?
token. This is used only in Eq. 1 for selecting
general-domain sentences; these words retain their
surface forms in the SMT train pipeline.
3 Experiment Setup
We experimented with four language pairs in the
WIT3 corpus (Cettolo et al, 2012), with English
(en) as source and German (de), Spanish (es),
French (fr), Russian (ru) as target. This is the
in-domain corpus, and consists of TED Talk tran-
scripts covering topics in technology, entertain-
ment, and design. As general-domain corpora,
we collected bitext from the WMT2013 campaign,
including CommonCrawl and NewsCommentary
for all 4 languages, Europarl for de/es/fr, UN for
es/fr, Gigaword for fr, and Yandex for ru. The in-
domain data is divided into a training set (for SMT
2The recurrent states are unrolled for several time-steps,
then stochastic gradient descent is applied.
679
en-de en-es en-fr en-ru
In-domain Training Set
#sentence 129k 140k 139k 117k
#token (en) 2.5M 2.7M 2.7M 2.3M
#vocab (en) 26k 27k 27k 25k
#vocab (f) 42k 39k 34k 58k
General-domain Bitext
#sentence 4.4M 14.7M 38.9M 2.0M
#token (en) 113M 385M 1012M 51M
%unknown 60% 58% 64% 65%
Table 1: Data statistics. ?%unknown?=fraction of
general-domain sentences with unknown words.
pipeline and neural LM training), a tuning set (for
MERT), a validation set (for choosing the optimal
threshold in data selection), and finally a testset of
1616 sentences.3 Table 1 lists data statistics.
For each language pair, we built a baseline in-
data SMT system trained only on in-domain data,
and an alldata system using combined in-domain
and general-domain data.4 We then built 3 systems
from augmented data selected by different LMs:
? ngram: Data selection by 4-gram LMs with
Kneser-Ney smoothing (Axelrod et al, 2011)
? neuralnet: Data selection by Recurrent neu-
ral LM, with the RNNLM Toolkit.5
? combine: Data selection by interpolated LM
using n-gram & neuralnet (equal weight).
All systems are built using standard settings in
the Moses toolkit (GIZA++ alignment, grow-diag-
final-and, lexical reordering models, and SRILM).
Note that standard n-grams are used as LMs for
SMT; neural LMs are only used for data selection.
Multiple SMT systems are trained by thresholding
on {10k,50k,100k,500k,1M} general-domain sen-
tence subsets, and we empirically determine the
single system for testing based on results on a sep-
arate validation set (in practice, 500k was chosen
for fr and 1M for es, de, ru.).
3The original data are provided by http://wit3.fbk.eu and
http://www.statmt.org/wmt13/. Our domain adaptation sce-
nario is similar to the IWSLT2012 campaign but we used our
own random train/test splits, since we wanted to ensure the
testset for all languages had identical source sentences for
comparison purposes. For replicability, our software is avail-
able at http://cl.naist.jp/?kevinduh/a/acl2013.
4More advanced phrase table adaptation methods are pos-
sible. but our interest is in comparing data selection methods.
The conclusions should transfer to advanced methods such as
(Foster et al, 2010; Niehues and Waibel, 2012).
5http://www.fit.vutbr.cz/?imikolov/rnnlm/
4 Results
4.1 LM Perplexity and Training Time
First, we measured perplexity to check the gen-
eralization ability of our neural LMs as language
models. Recall that we train four LMs to com-
pute each of the components of Eq. 1. In Table 2,
we compared each of the four versions of ngram,
neuralnet, and combine LMs on in-domain test
sets or general-domain held-out sets. It re-affirms
previous positive results (Mikolov et al, 2011a),
with neuralnet outperforming ngram by 20-30%
perplexity across all tasks. Also, combine slightly
improves the perplexity of neuralnet.
Task ngram neuralnet combine
In-Domain Test Set
en-de de 157 110 (29%) 110 (29%)
en-de en 102 81 (20%) 78 (24%)
en-es es 129 102 (20%) 98 (24%)
en-es en 101 80 (21%) 77 (24%)
en-fr fr 90 67 (25%) 65 (27%)
en-fr en 102 80 (21%) 77 (24%)
en-ru ru 208 167 (19%) 155 (26%)
en-ru en 103 83 (19%) 79 (23%)
General-Domain Held-out Set
en-de de 234 174 (25%) 161 (31%)
en-de en 218 168 (23%) 155 (29%)
en-es es 62 43 (31%) 43 (31%)
en-es en 84 61 (27%) 59 (30%)
en-fr fr 64 43 (33%) 43 (33%)
en-fr en 95 67 (30%) 65 (32%)
en-ru ru 242 199 (18%) 176 (27%)
en-ru en 191 153 (20%) 142 (26%)
Table 2: Perplexity of various LMs. Number in
parenthesis is percentage improvement vs. ngram.
Second, we show that the usual concern of neu-
ral LM training time is not so critical for the in-
domain data sizes used domain adaptation. The
complexity of training Figure 1 is dominated by
computing Eq. 3 and scales as O(|W | ? |S|) in
the number of tokens. Since |W | can be large, one
practical trick is to cluster the vocabulary so that
the output dimension is reduced. Table 3 shows
the training times on a 3.3GHz XeonE5 CPU by
varying these two main hyper-parameters (|S| and
cluster size). Note that the setting |S| = 200 and
cluster size of 100 already gives good perplexity
in reasonable training time. All neural LMs in this
paper use this setting, without additional tuning.
680
|S| Cluster Time Perplexity
200 100 198m 110
100 |W | 12915m 110
200 400 208m 113
100 100 52m 118
100 400 71m 120
Table 3: Training time (in minutes) for various
neural LM architectures (Task: en-de de).
4.2 End-to-end SMT Evaluation
Table 4 shows translation results in terms of BLEU
(Papineni et al, 2002), RIBES (Isozaki et al,
2010), and TER (Snover et al, 2006). We observe
that all three data selection methods essentially
outperform alldata and indata for all language
pairs, and neuralnet tend to be the best in all met-
rics. E.g., BLEU improvements over ngram are
in the range of 0.4 for en-de, 0.5 for en-es, 0.1
for en-fr, and 1.7 for en-ru. Although not all im-
provements are large in absolute terms, many are
statistically significant (95% confidence).
We therefore believe that neural LMs are gen-
erally worthwhile to try for data selection, as it
rarely underperform n-grams. The open question
is: what can explain the significant improvements
in, for example Russian, Spanish, German, but the
lack thereof in French? One conjecture is that
neural LMs succeeded in lowering testset out-of-
vocabulary (OOV) rate, but we found that OOV
reduction is similar across all selection methods.
The improvements appear to be due to better
probability estimates of the translation/reordering
models. We performed a diagnostic by decoding
the testset using LMs trained on the same test-
set, while varying the translation/reordering ta-
bles with those of ngram and neuralnet; this is a
kind of pseudo forced-decoding that can inform us
about which table has better coverage. We found
that across all language pairs, BLEU differences of
translations under this diagnostic become insignif-
icant, implying that the raw probability value is
the differentiating factor between ngram and neu-
ralnet. Manual inspection of en-de revealed that
many improvements come from lexical choice in
morphological variants (?meinen Sohn? vs. ?mein
Sohn?), segmentation changes (?baking soda? ?
?Backpulver? vs. ?baken Soda?), and handling of
unaligned words at phrase boundaries.
Finally, we measured the intersection between
the sentence set selected by ngram vs neural-
Task System BLEU RIBES TER
en-de indata 20.8 80.1 59.0
alldata 21.5 80.1 59.1
ngram 21.5 80.3 58.9
neuralnet 21.9+ 80.5+ 58.4
combine 21.5 80.2 58.8
en-es indata 30.4 83.5 48.7
alldata 31.2 83.2 49.9
ngram 32.0 83.7 48.4
neuralnet 32.5+ 83.7 48.3+
combine 32.5+ 83.8 48.3+
en-fr indata 31.4 83.9 51.2
alldata 31.5 83.5 51.4
ngram 32.7 83.7 50.4
neuralnet 32.8 84.2+ 50.3
combine 32.5 84.0 50.5
en-ru indata 14.8 72.5 69.5
alldata 23.4 75.0 62.3
ngram 24.0 75.7 61.4
neuralnet 25.7+ 76.1 60.0+
combine 23.7 75.9 61.9?
Table 4: End-to-end Translation Results. The best
results are bold-faced. We also compare neural
LMs to ngram using pairwise bootstrap (Koehn,
2004): ?+? means statistically significant im-
provement and ??? means significant degradation.
net. They share 60-75% of the augmented train-
ing data. This high overlap means that ngram
and neuralnet are actually not drastically different
systems, and neuralnet with its slightly better se-
lections represent an incremental improvement.6
5 Conclusions
We perform an evaluation of neural LMs for
adaptation data selection, based on the hypothe-
sis that their continuous vector representations are
effective at comparing general-domain sentences,
which contain frequent unknown words. Com-
pared to conventional n-grams, we observed end-
to-end translation improvements from 0.1 to 1.7
BLEU. Since neural LMs are fast to train in the
small in-domain data setting and achieve equal or
incrementally better results, we conclude that they
are an worthwhile option to include in the arsenal
of adaptation data selection techniques.
6This is corroborated by another analysis: taking the
union of sentences found by ngram and neuralnet gives sim-
ilar BLEU scores as neuralnet.
681
Acknowledgments
We thank Amittai Axelrod for discussions about
data selection implementation details, and an
anonymous reviewer for suggesting the union idea
for results analysis. K. D. would like to credit Spy-
ros Matsoukas (personal communication, 2010)
for the trick of using LM-based pseudo forced-
decoding for error analysis.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Pa-
pers, NAACL-Short ?06, pages 1?4, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28, Montre?al, Canada,
June. Association for Computational Linguistics.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage models. JMLR.
Yoshua Bengio. 2009. Learning Deep Architectures
for AI, volume Foundations and Trends in Machine
Learning. NOW Publishers.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261?268, Trento, Italy,
May.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation for
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT) - Technical Papers Track.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on smt systems. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422?432, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 944?952, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In WMT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural net-
work language model. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
training large scale neural network language model.
In ASRU.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model.
In Proceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
english texts. In Proceedings of the 13th conference
on Computational linguistics - Volume 3, COLING
?90, pages 213?218, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jan Niehues and Alex Waibel. 2012. Detailed analysis
of different strategies for phrase table adaptation in
SMT. In AMTA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
682
HLT, pages 11?19, Montre?al, Canada, June. Associ-
ation for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518,
July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Youzheng Wu, Xugang Lu, Hitoshi Yamamoto,
Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.
2012. Factored language model based on recurrent
neural network. In Proceedings of COLING 2012,
pages 2835?2850, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In ICJNLP.
683
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 91?96,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Travatar: A Forest-to-String Machine Translation Engine
based on Tree Transducers
Graham Neubig
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
neubig@is.naist.jp
Abstract
In this paper we describe Travatar, a
forest-to-string machine translation (MT)
engine based on tree transducers. It pro-
vides an open-source C++ implementation
for the entire forest-to-string MT pipeline,
including rule extraction, tuning, decod-
ing, and evaluation. There are a number
of options for model training, and tuning
includes advanced options such as hyper-
graph MERT, and training of sparse fea-
tures through online learning. The train-
ing pipeline is modeled after that of the
popular Moses decoder, so users famil-
iar with Moses should be able to get
started quickly. We perform a valida-
tion experiment of the decoder on English-
Japanese machine translation, and find that
it is possible to achieve greater accuracy
than translation using phrase-based and
hierarchical-phrase-based translation. As
auxiliary results, we also compare differ-
ent syntactic parsers and alignment tech-
niques that we tested in the process of de-
veloping the decoder.
Travatar is available under the LGPL at
http://phontron.com/travatar
1 Introduction
One of the recent trends in statistical machine
translation (SMT) is the popularity of models that
use syntactic information to help solve problems
of long-distance reordering between the source
and target language text. These techniques can
be broadly divided into pre-ordering techniques,
which first parse and reorder the source sentence
into the target order before translating (Xia and
McCord, 2004; Isozaki et al, 2010b), and tree-
based decoding techniques, which take a tree or
forest as input and choose the reordering and
translation jointly (Yamada and Knight, 2001; Liu
et al, 2006; Mi et al, 2008). While pre-ordering is
not able to consider both translation and reorder-
ing in a joint model, it is useful in that it is done
before the actual translation process, so it can be
performed with a conventional translation pipeline
using a standard phrase-based decoder such as
Moses (Koehn et al, 2007). For tree-to-string sys-
tems, on the other hand, it is necessary to have
available or create a decoder that is equipped with
this functionality, which becomes a bottleneck in
the research and development process.
In this demo paper, we describe Travatar, an
open-source tree-to-string or forest-to-string trans-
lation system that can be used as a tool for transla-
tion using source-side syntax, and as a platform
for research into syntax-based translation meth-
ods. In particular, compared to other decoders
which mainly implement syntax-based translation
in the synchronous context-free grammar (SCFG)
framework (Chiang, 2007), Travatar is built upon
the tree transducer framework (Graehl and Knight,
2004), a richer formalism that can help capture
important distinctions between parse trees, as we
show in Section 2. Travatar includes a fully docu-
mented training and testing regimen that was mod-
eled around that of Moses, making it possible for
users familiar with Moses to get started with Tra-
vatar quickly. The framework of the software is
also designed to be extensible, so the toolkit is ap-
plicable for other tree-to-string transduction tasks.
In the evaluation of the decoder on English-
Japanese machine translation, we perform a com-
parison to Moses?s phrase-based, hierarchical-
phrase-based, and SCFG-based tree-to-string
91
Figure 1: Tree-to-string translation rules for
SCFGs and tree transducers.
translation. Based on the results, we find that tree-
to-string, and particularly forest-to-string, transla-
tion using Travatar provides competitive or supe-
rior accuracy to all of these techniques. As aux-
iliary results, we also compare different syntactic
parsers and alignment techniques that we tested in
the process of developing the decoder.
2 Tree-to-String Translation
2.1 Overview
Tree-to-string translation uses syntactic informa-
tion to improve translation by first parsing the
source sentence, then using this source-side parse
tree to decide the translation and reordering of the
input. This method has several advantages, includ-
ing efficiency of decoding, relatively easy han-
dling of global reordering, and an intuitive repre-
sentation of de-lexicalized rules that express gen-
eral differences in order between the source and
target languages. Within tree-to-string translation
there are two major methodologies, synchronous
context-free grammars (Chiang, 2007), and tree
transducers (Graehl and Knight, 2004).
An example of tree-to-string translation rules
supported by SCFGs and tree transducers is shown
in Figure 1. In this example, the first rule is a
simple multi-word noun phrase, the second exam-
ple is an example of a delexicalized rule express-
ing translation from English SVO word order to
Japanese SOV word order. The third and fourth
examples are translations of a verb, noun phrase,
and prepositional phrase, where the third rule has
the preposition attatched to the verb, and the fourth
has the preposition attached to the noun.
For the SCFGs, it can be seen that on the source
side of the rule, there are placeholders correspond-
ing to syntactic phrases, and on the target side of
the rule there corresponding placeholders that do
not have a syntactic label. On the other hand in the
example of the translation rules using tree trans-
ducers, it can be seen that similar rules can be ex-
pressed, but the source rules are richer than simple
SCFG rules, also including the internal structure
of the parse tree. This internal structure is im-
portant for achieving translation results faithful to
the input parse. In particular, the third and fourth
rules show an intuitive example in which this in-
ternal structure can be important for translation.
Here the full tree structures demonstrate important
differences in the attachment of the prepositional
phrase to the verb or noun. While this is one of
the most difficult and important problems in syn-
tactic parsing, the source side in the SCFG is iden-
tical, losing the ability to distinguish between the
very information that parsers are designed to dis-
ambiguate.
In traditional tree-to-string translation methods,
the translator uses a single one-best parse tree out-
put by a syntactic parser, but parse errors have the
potential to degrade the quality of translation. An
important advance in tree-to-string translation that
helps ameliorate this difficulity is forest-to-string
translation, which represents a large number of
potential parses as a packed forest, allowing the
translator to choose between these parses during
the process of translation (Mi et al, 2008).
2.2 The State of Open Source Software
There are a number of open-source software pack-
ages that support tree-to-string translation in the
SCFG framework. For example, Moses (Koehn et
al., 2007) and NiuTrans (Xiao et al, 2012) sup-
port the annotation of source-side syntactic labels,
and taking parse trees (or in the case of NiuTrans,
forests) as input.
There are also a few other decoders that sup-
port other varieties of using source-side syntax
to help improve translation or global reorder-
ing. For example, the cdec decoder (Dyer et al,
2010) supports the context-free-reordering/finite-
state-translation framework described by Dyer and
Resnik (2010). The Akamon decoder (Wu et
al., 2012) supports translation using head-driven
92
phrase structure grammars as described by Wu et
al. (2010).
However, to our knowledge, while there is a
general-purpose tool for tree automata in general
(May and Knight, 2006), there is no open-source
toolkit implementing the SMT pipeline in the tree
transducer framework, despite it being a target of
active research (Graehl and Knight, 2004; Liu et
al., 2006; Huang et al, 2006; Mi et al, 2008).
3 The Travatar Machine Translation
Toolkit
In this section, we describe the overall framework
of the Travatar decoder, following the order of the
training pipeline.
3.1 Data Preprocessing
This consists of parsing the source side sentence
and tokenizing the target side sentences. Travatar
can decode input in the bracketed format of the
Penn Treebank, or also in forest format. There is
documentation and scripts for using Travatar with
several parsers for English, Chinese, and Japanese
included with the toolkit.
3.2 Training
Once the data has been pre-processed, a tree-
to-string model can be trained with the training
pipeline included in the toolkit. Like the train-
ing pipeline for Moses, there is a single script that
performs alignment, rule extraction, scoring, and
parameter initialization. Language model training
can be performed using a separate toolkit, and in-
structions are provided in the documentation.
For word alignment, the Travatar training
pipeline is integrated with GIZA++ (Och and Ney,
2003) by default, but can also use alignments from
any other aligner.
Rule extraction is performed using the GHKM
algorithm (Galley et al, 2006) and its extension to
rule extraction from forests (Mi and Huang, 2008).
There are also a number of options implemented,
including rule composition, attachment of null-
aligned target words at either the highest point in
the tree, or at every possible position, and left and
right binarization (Galley et al, 2006; Wang et al,
2007).
Rule scoring uses a standard set of forward
and backward conditional probabilities, lexical-
ized translation probabilities, phrase frequency,
and word and phrase counts. Rule scores are
stored as sparse vectors by default, which allows
for scoring using an arbitrarily large number of
feature functions.
3.3 Decoding
Given a translation model Travatar is able to de-
code parsed input sentences to generate transla-
tions. The decoding itself is performed using the
bottom-up forest-to-string decoding algorithm of
Mi et al (2008). Beam-search implemented us-
ing cube pruning (Chiang, 2007) is used to adjust
the trade-off between search speed and translation
accuracy.
The source side of the translation model is
stored using a space-efficient trie data structure
(Yata, 2012) implemented using the marisa-trie
toolkit.1 Rule lookup is performed using left-to-
right depth-first search, which can be implemented
as prefix lookup in the trie for efficient search.
The language model storage uses the implemen-
tation in KenLM (Heafield, 2011), and particu-
larly the implementation that maintains left and
right language model states for syntax-based MT
(Heafield et al, 2011).
3.4 Tuning and Evaluation
For tuning the parameters of the model, Travatar
natively supports minimum error rate training
(MERT) (Och, 2003) and is extension to hyper-
graphs (Kumar et al, 2009). This tuning can
be performed for evaluation measures including
BLEU (Papineni et al, 2002) and RIBES (Isozaki
et al, 2010a), with an easily extendable interface
that makes it simple to support other measures.
There is also a preliminary implementation of
online learning methods such as the structured per-
ceptron algorithm (Collins, 2002), and regularized
structured SVMs trained using FOBOS (Duchi
and Singer, 2009). There are plans to implement
more algorithms such as MIRA or AROW (Chi-
ang, 2012) in the near future.
The Travatar toolkit also provides an evaluation
program that can calculate the scores of transla-
tion output according to various evaluation mea-
sures, and calculate the significance of differ-
ences between systems using bootstrap resampling
(Koehn, 2004).
1http://marisa-trie.googlecode.com
93
4 Experiments
4.1 Experimental Setup
In our experiments, we validated the performance
of the translation toolkit on English-Japanese
translation of Wikipedia articles, as specified by
the Kyoto Free Translation Task (KFTT) (Neubig,
2011). Training used the 405k sentences of train-
ing data of length under 60, tuning was performed
on the development set, and testing was performed
on the test set using the BLEU and RIBES mea-
sures. As baseline systems we use the Moses2 im-
plementation of phrase-based (MOSES-PBMT), hi-
erarchical phrase-based (MOSES-HIER), and tree-
to-string translation (MOSES-T2S). The phrase-
based and hierarchical phrase-based models were
trained with the default settings according to tuto-
rials on each web site.
For all systems, we use a 5-gram Kneser-Ney
smoothed language model. Alignment for each
system was performed using either GIZA++3 or
Nile4 with main results reported for the aligner
that achieved the best accuracy on the dev set, and
a further comparison shown in the auxiliary exper-
iments in Section 4.3. Tuning was performed with
minimum error rate training to maximize BLEU
over 200-best lists. Tokenization was performed
with the Stanford tokenizer for English, and the
KyTea word segmenter (Neubig et al, 2011) for
Japanese.
For all tree-to-string systems we use Egret5 as
an English parser, as we found it to achieve high
accuracy, and it allows for the simple output of
forests. Rule extraction was performed using one-
best trees, which were right-binarized, and lower-
cased post-parsing. For Travatar, composed rules
of up to size 4 and a maximum of 2 non-terminals
and 7 terminals for each rule were used. Null-
aligned words were only attached to the top node,
and no count normalization was performed, in
contrast to Moses, which performs count normal-
ization and exhaustive null word attachment. De-
coding was performed over either one-best trees
(TRAV-T2S), or over forests including all edges in-
cluded in the parser 200-best list (TRAV-F2S), and
a pop limit of 1000 hypotheses was used for cube
2http://statmt.org/moses/
3http://code.google.com/p/giza-pp/
4http://code.google.com/p/nile/ As Nile is
a supervised aligner, we trained it on the alignments provided
with the KFTT.
5http://code.google.com/p/
egret-parser/
BLEU RIBES Rules Sent/s.
MOSES-PBMT 22.27 68.37 10.1M 5.69
MOSES-HIER 22.04 70.29 34.2M 1.36
MOSES-T2S 23.81 72.01 52.3M 1.71
TRAV-T2S 23.15 72.32 9.57M 3.29
TRAV-F2S 23.97 73.27 9.57M 1.11
Table 1: Translation results (BLEU, RIBES), rule
table size, and speed in sentences per second for
each system. Bold numbers indicate a statistically
significant difference over all other systems (boot-
strap resampling with p > 0.05) (Koehn, 2004).
pruning.
4.2 System Comparison
The comparison between the systems is shown in
Table 1. From these results we can see that the
systems utilizing source-side syntax significantly
outperform the PBMT and Hiero, validating the
usefulness of source side syntax on the English-to-
Japanese task. Comparing the two tree-to-string
sytems, we can see that TRAV-T2S has slightly
higher RIBES and slightly lower BLEU than
MOSES-T2S. One reason for the slightly higher
BLEU of MOSES-T2S is because Moses?s rule ex-
traction algorithm is more liberal in its attachment
of null-aligned words, resulting in a much larger
rule table (52.3M rules vs. 9.57M rules) and mem-
ory footprint. In this setting, TRAV-T2S is approx-
imately two times faster than MOSES-T2S. When
using forest based decoding in TRAV-F2S, we see
significant gains in accuracy over TRAV-T2S, with
BLEU slightly and RIBES greatly exceeding that
of MOSES-T2S.
4.3 Effect of Alignment/Parsing
In addition, as auxiliary results, we present a com-
parison of Travatar?s tree-to-string and forest-to-
string systems using different alignment methods
and syntactic parsers to examine the results on
translation (Table 2).
For parsers, we compared Egret with the Stan-
ford parser.6 While we do not have labeled data
to calculate parse accuracies with, Egret is a clone
of the Berkeley parser, which has been reported to
achieve higher accuracy than the Stanford parser
on several domains (Kummerfeld et al, 2012).
From the translation results, we can see that STAN-
6http://nlp.stanford.edu/software/
lex-parser.shtml
94
GIZA++ Nile
BLEU RIBES BLEU RIBES
PBMT 22.28 68.37 22.37 68.43
HIER 22.05 70.29 21.77 69.31
STAN-T2S 21.47 70.94 22.44 72.02
EGRET-T2S 22.82 71.90 23.15 72.32
EGRET-F2S 23.35 71.77 23.97 73.27
Table 2: Translation results (BLEU, RIBES), for
several translation models (PBMT, Hiero, T2S,
F2S), aligners (GIZA++, Nile), and parsers (Stan-
ford, Egret).
T2S significantly underperforms EGRET-T2S, con-
firming that the effectiveness of the parser plays a
large effect on the translation accuracy.
Next, we compared the unsupervised aligner
GIZA++, with the supervised aligner Nile, which
uses syntactic information to improve alignment
accuracy (Riesa and Marcu, 2010). We held out
10% of the hand aligned data provided with the
KFTT, and found that GIZA++ achieves 58.32%
alignment F-measure, while Nile achieves 64.22%
F-measure. With respect to translation accuracy,
we found that for translation that does not use syn-
tactic information, improvements in alignment do
not necessarily increase translation accuracy, as
has been noted by Ganchev et al (2008). How-
ever, for all tree-to-string systems, the improved
alignments result in significant improvements in
accuracy, showing that alignments are, in fact, im-
portant in our syntax-driven translation setup.
5 Conclusion and Future Directions
In this paper, we introduced Travatar, an open-
source toolkit for forest-to-string translation using
tree transducers. We hope this decoder will be
useful to the research community as a test-bed for
forest-to-string systems. The software is already
sufficiently mature to be used as is, as evidenced
by the competitive, if not superior, results in our
English-Japanese evaluation.
We have a number of plans for future devel-
opment. First, we plan to support advanced rule
extraction techniques, such as fuller support for
count regularization and forest-based rule extrac-
tion (Mi and Huang, 2008), and using the EM
algorithm to choose attachments for null-aligned
words (Galley et al, 2006) or the direction of rule
binarization (Wang et al, 2007). We also plan
to incorporate advances in decoding to improve
search speed (Huang and Mi, 2010). In addition,
there is a preliminary implementation of the abil-
ity to introduce target-side syntactic information,
either through hard constraints as in tree-to-tree
translation systems (Graehl and Knight, 2004), or
through soft constraints, as in syntax-augmented
machine translation (Zollmann and Venugopal,
2006). Finally, we will provide better support of
parallelization through the entire pipeline to in-
crease the efficiency of training and decoding.
Acknowledgements: We thank Kevin Duh and an
anonymous reviewer for helpful comments. Part
of this work was supported by JSPS KAKENHI
Grant Number 25730136.
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, pages 1159?1187.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899?
2934.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT-
NAACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, pages 961?968.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
Proc. ACL.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT, pages 105?112.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
95
model state for syntactic machine translation. In
Proc. IWSLT.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proc. WMT, pages 187?
197.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proc. EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proc. WMT
and MetricsMATR.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. ACL,
pages 163?171.
Jonathan K Kummerfeld, David Hall, James R Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: an empirical investigation of er-
ror types in parser output. In Proc. EMNLP, pages
1048?1059.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL.
Jonathan May and Kevin Knight. 2006. Tiburon:
A weighted tree automata toolkit. In Implementa-
tion and Application of Automata, pages 102?113.
Springer.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. EMNLP, pages 206?
214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL, pages 192?199.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. ACL,
pages 529?533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318, Philadelphia, USA.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proc. ACL, pages
157?166.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proc. EMNLP, pages
746?754.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proc. ACL, pages 325?334.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2012. Akamon: An open source toolkit for
tree/forest-based statistical machine translation. In
Proceedings of the ACL 2012 System Demonstra-
tions, pages 127?132.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. Niutrans: An open source toolkit for phrase-
based and syntax-based machine translation. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 19?24.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL.
Susumu Yata. 2012. Dictionary compression using
nested prefix/Patricia tries (in Japanese). In Proc.
17th NLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. WMT.
96
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 143?149,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On the Elements of an Accurate
Tree-to-String Machine Translation System
Graham Neubig, Kevin Duh
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
{neubig,kevinduh}@is.naist.jp
Abstract
While tree-to-string (T2S) translation the-
oretically holds promise for efficient, ac-
curate translation, in previous reports T2S
systems have often proven inferior to other
machine translation (MT) methods such as
phrase-based or hierarchical phrase-based
MT. In this paper, we attempt to clarify
the reason for this performance gap by
investigating a number of peripheral ele-
ments that affect the accuracy of T2S sys-
tems, including parsing, alignment, and
search. Based on detailed experiments
on the English-Japanese and Japanese-
English pairs, we show how a basic T2S
system that performs on par with phrase-
based systems can be improved by 2.6-4.6
BLEU, greatly exceeding existing state-
of-the-art methods. These results indi-
cate that T2S systems indeed hold much
promise, but the above-mentioned ele-
ments must be taken seriously in construc-
tion of these systems.
1 Introduction
In recent years, syntactic parsing is being viewed
as an ever-more important element of statistical
machine translation (SMT) systems, particularly
for translation between languages with large dif-
ferences in word order. There are many ways of
incorporating syntax into MT systems, including
the use of string-to-tree translation (S2T) to ensure
the syntactic well-formedness of the output (Gal-
ley et al, 2006; Shen et al, 2008), tree-to-string
(T2S) using source-side parsing as a hint during
the translation process (Liu et al, 2006), or pre-
or post-ordering to help compensate for reorder-
ing problems experienced by non-syntactic meth-
ods such as phrase-based MT (PBMT) (Collins et
al., 2005; Sudoh et al, 2011). Among these, T2S
translation has a number of attractive theoretical
properties, such as joint consideration of global re-
ordering and lexical choice while maintaining rel-
atively fast decoding times.
However, building an accurate T2S system is
not trivial. On one hand, there have been multiple
reports (mainly from groups with a long history
of building T2S systems) stating that systems us-
ing source-side syntax greatly out-perform phrase-
based systems (Mi et al, 2008; Liu et al, 2011;
Zhang et al, 2011; Tamura et al, 2013). On the
other hand, there have been also been multiple re-
ports noting the exact opposite result that source-
side syntax systems perform worse than Hiero,
S2T, PBMT, or PBMT with pre-ordering (Ambati
and Lavie, 2008; Xie et al, 2011; Kaljahi et al,
2012). In this paper, we argue that this is due to the
fact that T2S systems have the potential to achieve
high accuracy, but are also less robust, with a num-
ber of peripheral elements having a large effect on
translation accuracy.
Our motivation in writing this paper is to pro-
vide a first step in examining and codifying the
more important elements that make it possible to
construct a highly accurate T2S MT system. To do
so, we perform an empirical study of the effect of
parsing accuracy, packed forest input, alignment
accuracy, and search. The reason why we choose
these elements is that past work that has reported
low accuracy for T2S systems has often neglected
to consider one or all of these elements.
As a result of our tests on English-Japanese (en-
ja) and Japanese-English (ja-en) machine transla-
tion, we find that a T2S system not considering
these elements performs only slightly better than a
standard PBMT system. However, after account-
ing for all these elements we see large increases of
accuracy, with the final system greatly exceeding
not only standard PBMT, but also state-of-the-art
methods based on syntactic pre- or post-ordering.
143
2 Experimental Setup
2.1 Systems Compared
In our experiments, we use a translation model
based on T2S tree transducers (Graehl and Knight,
2004), constructed using the Travatar toolkit (Neu-
big, 2013). Rules are extracted using the GHKM
algorithm (Galley et al, 2006), and rules with
up to 5 composed minimal rules, up to 2 non-
terminals, and up to 10 terminals are used.
We also prepare 3 baselines not based on T2S
to provide a comparison with other systems in the
literature. The first two baselines are standard sys-
tems using PBMT or Hiero trained using Moses
(Koehn et al, 2007). We use default settings, ex-
cept for setting the reordering limit or maximum
chart span to the best-performing value of 24. As
our last baselines, we use two methods based on
syntactic pre- or post-ordering, which are state-of-
the-art methods for the language pairs. Specifi-
cally, for en-ja translation we use the head finaliza-
tion pre-ordering method of (Isozaki et al, 2010b),
and for ja-en translation, we use the syntactic post-
ordering method of (Goto et al, 2012). For all
systems, T2S or otherwise, the language model is
a Kneser-Ney 5-gram, and tuning is performed to
maximize BLEU score using minimum error rate
training (Och, 2003).
2.2 Data and Evaluation
We perform all of our experiments on en-ja
and ja-en translation over data from the NTCIR
PatentMT task (Goto et al, 2011), the most stan-
dard benchmark task for these language pairs. We
use the training data from NTCIR 7/8, a total of
approximately 3.0M sentences, and perform tun-
ing on the NTCIR 7 dry run, testing on the NTCIR
7 formal run data. As evaluation measures, we use
the standard BLEU (Papineni et al, 2002) as well
as RIBES (Isozaki et al, 2010a), a reordering-
based metric that has been shown to have high
correlation with human evaluations on the NTCIR
data. We measure significance of results using
bootstrap resampling at p < 0.05 (Koehn, 2004).
In tables, bold numbers indicate the best system
and all systems that were not significantly differ-
ent from the best system.
2.3 Motivational Experiment
Before going into a detailed analysis, we first
present results that stress the importance of the el-
ements described in the introduction. To do so,
en-ja ja-en
System BLEU RIBES BLEU RIBES
PBMT 35.84 72.89 30.49 69.80
Hiero 34.45 72.94 29.41 69.51
Pre/Post 36.69 77.05 29.42 73.85
T2S-all 36.23 76.60 31.15 72.87
T2S+all 40.84 80.15 33.70 75.94
Table 1: Overall results for five systems.
we compare the 3 non-T2S baselines with two
T2S systems that vary the settings of the parser,
alignment, and search, as described in the follow-
ing Sections 3, 4, and 5. The first system ?T2S-
all? is a system that uses the worst settings
1
for
each of these elements, while the second system
?T2S+all? uses the best settings.
2
The results for
the systems are shown in Table 1.
The most striking result is that T2S+all signif-
icantly exceeds all of the baselines, even includ-
ing the pre/post-ordering baselines, which provide
state-of-the-art results on this task. The gains are
particularly striking on en-ja, with a gain of over 4
BLEU points over the closest system, but still sig-
nificant on the ja-en task, where the use of source-
side syntax has proven less effective in previous
work (Sudoh et al, 2011). The next thing to notice
is that if we had instead used T2S-all, our conclu-
sion would have been much different. This system
is able to achieve respectable accuracy compared
to PBMT or Hiero, but does not exceed the more
competitive pre/post-ordering systems.
3
With this
result in hand, we will investigate the contribution
of each of these elements in detail in the following
sections. In the remainder of the paper settings
follow T2S+all except when otherwise noted.
3 Parsing
3.1 Parsing Overview
As T2S translation uses parse trees both in train-
ing and testing of the system, an accurate syntactic
parser is required. In order to test the extent that
parsing accuracy affects translation, we use two
1
Stanford/Eda, GIZA++, pop-limit 5000 cube pruning.
2
Egret forests, Nile, pop-limit 5000 hypergraph search.
3
We have also observed similar trends on other genres and
language pairs. For example, in a Japanese-Chinese/English
medical conversation task (Neubig et al, 2013), forests,
alignment, and search resulted in BLEU increases of en-ja
24.55?30.81, ja-en 19.28?22.46, zh-ja 15.22?20.67, ja-zh
30.88?33.89.
144
different syntactic parsers and examine the trans-
lation accuracy realized by each parser.
For English, the two most widely referenced
parsers are the Stanford Parser and Berkeley
Parser. In this work, we compare the Stanford
Parser?s CFG model, with the Berkeley Parser?s
latent variable model. In previous reports, it has
been noted (Kummerfeld et al, 2012) that the la-
tent variable model of the Berkeley parser tends to
have the higher accuracy of the two, so if the accu-
racy of a system using this model is higher then it
is likely that parsing accuracy is important for T2S
translation. Instead of the Berkeley Parser itself,
we use a clone Egret,
4
which achieves nearly iden-
tical accuracy, and is able to output packed forests
for use in MT, as mentioned below. Trees are
right-binarized, with the exception of phrase-final
punctuation, which is split off before any other el-
ement in the phrase.
For Japanese, our first method uses the MST-
based pointwise dependency parser of Flannery et
al. (2011), as implemented in the Eda toolkit.
5
In order to convert dependencies into phrase-
structure trees typically used in T2S translation,
we use the head rules implemented in the Travatar
toolkit. In addition, we also train a latent variable
CFG using the Berkeley Parser and use Egret for
parsing. Both models are trained on the Japanese
Word Dependency Treebank (Mori et al, 2014).
In addition, Mi et al (2008) have proposed a
method for forest-to-string (F2S) translation us-
ing packed forests to encode many possible sen-
tence interpretations. By doing so, it is possible to
resolve some of the ambiguity in syntactic inter-
pretation at translation time, potentially increasing
translation accuracy. However, the great majority
of recent works on T2S translation do not consider
multiple syntactic parses (e.g. Liu et al (2011),
Zhang et al (2011)), and thus it is important to
confirm the potential gains that could be acquired
by taking ambiguity into account.
3.2 Effect of Parsing and Forest Input
In Table 2 we show the results for Stanford/Eda
with 1-best tree input vs. Egret with trees or
forests as input. Forests are those containing all
edges in the 100-best parses.
First looking at the difference between the two
parsers, we can see that the T2S system using
4
http://code.google.com/p/egret-parser
5
http://plata.ar.media.kyoto-u.ac.jp/tool/EDA
en-ja ja-en
System BLEU RIBES BLEU RIBES
Stan/Eda 38.95 78.47 32.56 73.03
Egret-T 39.26 79.26 32.97 74.94
Egret-F 40.84 80.15 33.70 75.94
Table 2: Results for Stanford/Eda, Egret with tree
input, and Egret with forest input.
1 0 100
Forest n-best Cutoff
0.0
0.2
0.4
0.6
0.8
1.0
B
LE
U 39
40
41
42
1.75
2.03
2.58
3.06 3.21 3.70
3.73
4.61en-ja
ja-en
1 10 100
32
33
34
35
1.21 1.32
1.39 1.50
1.61 1.74 2.05
2.07
Figure 1: BLEU scores using various levels of for-
est pruning. Numbers in the graph indicate decod-
ing time in seconds/sentence.
Egret achieves greater accuracy than that using the
other two parsers. This improvement is particu-
larly obvious in RIBES, indicating that an increase
in parsing accuracy has a larger effect on global
reordering than on lexical choice. When going
from T2S to F2S translation using Egret, we see
another large gain in accuracy, although this time
with the gain in BLEU being more prominent. We
believe this is related to the observation of Zhang
and Chiang (2012) that F2S translation is not nec-
essarily helping fixing parsing errors, but instead
giving the translation system the freedom to ignore
the parse somewhat, allowing for less syntactically
motivated but more fluent translations.
As passing some degree of syntactic ambigu-
ity on to the decoder through F2S translation has
proven useful, a next natural question is how much
of this ambiguity we need to preserve in our forest.
The pruning criterion that we use for the forest is
based on including all edges that appear in one or
more of the n-best parses, so we perform transla-
tion setting n to 1 (trees), 3, 6, 12, 25, 50, 100, and
200. Figure 1 shows results for these settings with
regards to translation accuracy and speed. Over-
all, we can see that every time we double the size
of the forest we get an approximately linear in-
145
crease in BLEU at the cost of an increase in decod-
ing time. Interestingly, the increases in BLEU did
not show any sign of saturating even when setting
the n-best cutoff to 200, although larger cutoffs re-
sulted in exceedingly large translation forests that
required large amounts of memory.
4 Alignment
4.1 Alignment Overview
The second element that we investigate is align-
ment accuracy. It has been noted in many previ-
ous works that significant gains in alignment accu-
racy do not make a significant difference in trans-
lation results (Ayan and Dorr, 2006; Ganchev et
al., 2008). However, none of these works have ex-
plicitly investigated the effect on T2S translation,
so it is not clear whether these results carry over to
our current situation.
As our baseline aligner, we use the GIZA++ im-
plementation of the IBM models (Och and Ney,
2003) with the default options. To test the effect
of improved alignment accuracy, we use the dis-
criminative alignment method of Riesa and Marcu
(2010) as implemented in the Nile toolkit.
6
This
method has the ability to use source- and target-
side syntactic information, and has been shown to
improve the accuracy of S2T translation.
We trained Nile and tested both methods on
the Japanese-English alignments provided with
the Kyoto Free Translation Task (Neubig, 2011)
(430k parallel sentences, 1074 manually aligned
training sentences, and 120 manually aligned test
sentences).
7
As creating manual alignment data is
costly, we also created two training sets that con-
sisted of 1/4 and 1/16 of the total data to test if
we can achieve an effect with smaller amounts of
manually annotated data. The details of data size
and alignment accuracy are shown in Table 3.
4.2 Effect of Alignment on Translation
In Table 4, we show results when we vary the
aligner between GIZA++ and Nile. For reference,
we also demonstrate results when using the same
alignments for PBMT and Hiero.
From this, we can see that while for PBMT and
Hiero systems the results are mixed, as has been
noted in previous work (Fraser and Marcu, 2007),
6
http://code.google.com/p/nile
7
This data is from Wikipedia articles about Kyoto City,
and is an entirely different genre than our MT test data. It is
likely that creating aligned data that matches the MT genre
would provide larger gains in MT accuracy.
Name Sent. Prec. Rec. F-meas
GIZA++ 0 60.46 55.48 57.86
Nile/16 68 70.21 60.81 65.17
Nile/4 269 72.85 62.70 67.40
Nile 1074 72.73 63.97 68.07
Table 3: Alignment accuracy (%) by method and
number of manually annotated training sentences.
en-ja ja-en
System BLEU RIBES BLEU RIBES
PBMT-G 35.84 72.89 30.49 69.80
PBMT-N 36.05 71.84 30.77 69.75
Hiero-G 34.45 72.94 29.41 69.51
Hiero-N 33.90 72.63 28.90 69.83
T2S-G 39.57 78.94 32.62 75.19
T2S-N/16 40.79 80.05 32.82 74.89
T2S-N/4 40.97 80.32 33.35 75.46
T2S-N 40.84 80.15 33.70 75.94
Table 4: Results varying the aligner (GIZA++ vs.
Nile), including results for Nile when using 1/4 or
1/16 of the annotated training data.
Figure 2: Probabilities for SVO?SOV rules.
improving the alignment accuracy gives signifi-
cant gains for T2S translation. The reason for this
difference is two-fold. The first is that in rule
extraction in syntax-based translation (Galley et
al., 2006), a single mistaken alignment crossing
phrase boundaries results not only in a bad rule be-
ing extracted, but also prevents the extraction of a
number of good rules. This is reflected in the size
of the rule table; the en-ja system built using Nile
contains 92.8M rules, while the GIZA++ system
contains only 83.3M rules, a 11.2% drop.
The second reason why alignment is important
is that while one of the merits of T2S models is
their ability to perform global re-ordering, it is dif-
ficult to learn good reorderings from bad align-
ments. We show an example of this in Figure 2.
When translating SVO English to SOV Japanese,
we expect rules containing a verb and a following
noun phrase (VO) to have a high probability of be-
ing reversed (to OV), possibly with the addition of
146
the Japanese direct object particle ?wo.? From the
figure, we can see that the probabilities learned by
Nile match this intuition, while the probabilities
learned by GIZA heavily favor no reordering.
Finally, looking at the amount of data needed to
train the model, we can see that a relatively small
amount of manually annotated data proves suffi-
cient for large gains in alignment accuracy, with
even 68 sentences showing a 7.31 point gain in F-
measure over GIZA++. This is because Nile?s fea-
ture set uses generalizable POS/syntactic informa-
tion and also because mis-alignments of common
function words (e.g. a/the) will be covered even
by small sets of training data. Looking at the MT
results, we can see that even the smaller data sets
allow for gains in accuracy, although the gains are
more prominent for en-ja.
5 Search
5.1 Search Overview
Finally, we examine the effect that the choice of
search algorithm has on the accuracy of transla-
tion. The most standard search algorithm for T2S
translation is bottom-up beam search using cube
pruning (CP, Chiang (2007)). However, there are
a number of other search algorithms that have
been proposed for tree-based translation in gen-
eral (Huang and Chiang, 2007) or T2S systems
in particular (Huang and Mi, 2010; Feng et al,
2012). In this work, we compare CP and the hy-
pergraph search (HS) method of Heafield et al
(2013), which is also a bottom-up pruning algo-
rithm but performs more efficient search by group-
ing together similar language model states.
5.2 Effect of Search
Figure 3 shows BLEU and decoding speed results
using HS or CP on T2S and F2S translation, us-
ing a variety of pop limits. From this, we can see
that HS out-performs CP for both F2S and T2S,
especially with smaller pop limits. Comparing the
graphs for F2S and T2S translation, it is notable
that the shapes of the graphs for the two meth-
ods are strikingly similar. This result is somewhat
surprising, as the overall search space of F2S is
larger and it would be natural for the characteris-
tics of the search algorithm to vary between these
two settings. Finally, comparing ja-en and en-ja,
search is simpler for the former, a result of the fact
that the Japanese sentences contain more words,
and thus more LM evaluations per sentence.
100 0 10000
Pop Limit
0.0
0.2
0.4
0.6
0.8
1.0
B
LE
U
 (
F2
S)
37
38
39
40
41
42
0.33
0.42
0.72 1.07 1.81
3.73 5.60 9.59
0.33
0.43
0.66 1.04
1.77 4.43
9.60 17.40
en-ja HS
en-ja CP
100 1000 10000
30
31
32
33
34
35
0.22
0.30 0.41 0.58 0.91 2.05 3.54 6.44
0.24
0.32
0.43 0.71
1.01 2.29 4.73 9.18
ja-en HS
ja-en CP
100 0 10000
Pop Limit
0.0
0.2
0.4
0.6
0.8
1.0
B
LE
U
 (
T2
S)
36
37
38
39
40
41
0.08
0.12
0.25 0.43
0.76 1.75 2.96 4.80
0.08
0.09
0.27
0.38 0.71
1.75 4.34 8.73
en-ja HS
en-ja CP
100 1000 10000
29
30
31
32
33
34
0.10 0.14 0.25 0.37 0.57 1.21 2.22 3.97
0.10
0.13
0.24 0.44 0.64
1.60 3.83 5.74
ja-en HS
ja-en CP
Figure 3: Hypergraph search (HS) and cube
pruning (CP) results for F2S and T2S. Numbers
above and below the lines indicate time in sec-
onds/sentence for HS and CP respectively.
6 Conclusion
In this paper, we discussed the importance of three
peripheral elements that contribute greatly to the
accuracy of T2S machine translation: parsing,
alignment, and search. Put together, a T2S sys-
tem that uses the more effective settings for these
three elements greatly outperforms a system that
uses more standard settings, as well as the current
state-of-the-art on English-Japanese and Japanese-
English translation tasks.
Based on these results we draw three conclu-
sions. The first is that given the very competitive
results presented here, T2S systems do seem to
have the potential to achieve high accuracy, even
when compared to strong baselines incorporating
syntactic reordering into a phrase-based system.
The second is that when going forward with re-
search on T2S translation, one should first be sure
to account for these three elements to ensure a
sturdy foundation for any further improvements.
Finally, considering the fact that parsing and align-
ment for each of these languages is far from per-
fect, further research investment in these fields
may very well have the potential to provide ad-
ditional gains in accuracy in the T2S framework.
Acknowledgments: This work was supported
by JSPS KAKENHI Grant Number 25730136.
147
References
Vamshi Ambati and Alon Lavie. 2008. Improving syn-
tax driven translation models by re-structuring diver-
gent and non-isomorphic parse tree structures. In
Proc. AMTA, pages 235?244.
Necip Ayan and Bonnie Dorr. 2006. Going beyond
AER: an extensive analysis of word alignments and
their impact on MT. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.
2012. Left-to-right tree-to-string decoding with pre-
diction. In Proc. EMNLP, pages 1191?1200.
Daniel Flannery, Yusuke Miyao, Graham Neubig, and
Shinsuke Mori. 2011. Training dependency parsers
from partially annotated corpora. In Proc. IJCNLP,
pages 776?784.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, pages 961?968.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
Proc. ACL.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR, volume 9, pages 559?578.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proc. ACL, pages
311?316.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT, pages 105?112.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words to
speed k?best extraction from hypergraphs. In Proc.
NAACL, pages 958?968.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. ACL, pages 144?151.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proc. EMNLP, pages 273?283.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proc. WMT
and MetricsMATR.
Rasoul Samad Zadeh Kaljahi, Raphael Rubino, Johann
Roturier, and Jennifer Foster. 2012. A detailed
analysis of phrase-based and syntax-based machine
translation: The search for systematic differences.
In Proc. AMTA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Jonathan K Kummerfeld, David Hall, James R Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: an empirical investigation of er-
ror types in parser output. In Proc. EMNLP, pages
1048?1059.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL.
Yang Liu, Qun Liu, and Yajuan Lu?. 2011. Adjoin-
ing tree-to-string translation. In Proc. ACL, pages
1278?1287.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL, pages 192?199.
Shinsuke Mori, Hideki Ogura, and Tetsuro Sasada.
2014. A Japanese word dependency corpus. In
Proc. LREC.
Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi
Nakamura, Yuji Matsumoto, Ryosuke Isotani, and
Yukichi Ikeda. 2013. Towards high-reliability
speech translation in the medical domain. In Proc.
MedNLP, pages 22?29.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. In Proc. ACL Demo Track, pages 91?96.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
148
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proc. ACL, pages
157?166.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL, pages 577?585.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering
in statistical machine translation. In Proc. MT Sum-
mit.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Proc. ACL, pages 841?
851.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proc. EMNLP, pages 216?226.
Hui Zhang and David Chiang. 2012. An exploration
of forest-to-string translation: Does translation help
or hurt parsing? In Proc. ACL, pages 317?321.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized forest to string translation. In Proc.
ACL, pages 835?845.
149
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551?556,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Optimizing Segmentation Strategies for Simultaneous Speech Translation
Yusuke Oda Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
Takayama, Ikoma, Nara 630-0192, Japan
{oda.yusuke.on9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose new algorithms
for learning segmentation strategies for si-
multaneous speech translation. In contrast
to previously proposed heuristic methods,
our method finds a segmentation that di-
rectly maximizes the performance of the
machine translation system. We describe
two methods based on greedy search and
dynamic programming that search for the
optimal segmentation strategy. An experi-
mental evaluation finds that our algorithm
is able to segment the input two to three
times more frequently than conventional
methods in terms of number of words,
while maintaining the same score of auto-
matic evaluation.
1
1 Introduction
The performance of speech translation systems
has greatly improved in the past several years,
and these systems are starting to find wide use in
a number of applications. Simultaneous speech
translation, which translates speech from the
source language into the target language in real
time, is one example of such an application. When
translating dialogue, the length of each utterance
will usually be short, so the system can simply
start the translation process when it detects the end
of an utterance. However, in the case of lectures,
for example, there is often no obvious boundary
between utterances. Thus, translation systems re-
quire a method of deciding the timing at which
to start the translation process. Using estimated
ends of sentences as the timing with which to start
translation, in the same way as a normal text trans-
lation, is a straightforward solution to this problem
(Matusov et al, 2006). However, this approach
1
The implementation is available at
http://odaemon.com/docs/codes/greedyseg.html.
impairs the simultaneity of translation because the
system needs to wait too long until the appearance
of a estimated sentence boundary. For this reason,
segmentation strategies, which separate the input
at appropriate positions other than end of the sen-
tence, have been studied.
A number of segmentation strategies for simul-
taneous speech translation have been proposed in
recent years. F?ugen et al (2007) and Bangalore et
al. (2012) propose using prosodic pauses in speech
recognition to denote segmentation boundaries,
but this method strongly depends on characteris-
tics of the speech, such as the speed of speaking.
There is also research on methods that depend on
linguistic or non-linguistic heuristics over recog-
nized text (Rangarajan Sridhar et al, 2013), and it
was found that a method that predicts the location
of commas or periods achieves the highest perfor-
mance. Methods have also been proposed using
the phrase table (Yarmohammadi et al, 2013) or
the right probability (RP) of phrases (Fujita et al,
2013), which indicates whether a phrase reorder-
ing occurs or not.
However, each of the previously mentioned
methods decides the segmentation on the basis
of heuristics, so the impact of each segmenta-
tion strategy on translation performance is not di-
rectly considered. In addition, the mean number
of words in the translation unit, which strongly af-
fects the delay of translation, cannot be directly
controlled by these methods.
2
In this paper, we propose new segmentation al-
gorithms that directly optimize translation perfor-
mance given the mean number of words in the
translation unit. Our approaches find appropri-
ate segmentation boundaries incrementally using
greedy search and dynamic programming. Each
boundary is selected to explicitly maximize trans-
2
The method using RP can decide relative frequency of
segmentation by changing a parameter, but guessing the
length of a translation unit from this parameter is not trivial.
551
lation accuracy as measured by BLEU or another
evaluation measure.
We evaluate our methods on a speech transla-
tion task, and we confirm that our approaches can
achieve translation units two to three times as fine-
grained as other methods, while maintaining the
same accuracy.
2 Optimization Framework
Our methods use the outputs of an existing ma-
chine translation system to learn a segmentation
strategy. We define F = {f
j
: 1 ? j ? N},
E = {e
j
: 1 ? j ? N} as a parallel corpus
of source and target language sentences used to
train the segmentation strategy. N represents the
number of sentences in the corpus. In this work,
we consider sub-sentential segmentation, where
the input is already separated into sentences, and
we want to further segment these sentences into
shorter units. In an actual speech translation sys-
tem, these sentence boundaries can be estimated
automatically using a method like the period es-
timation mentioned in Rangarajan Sridhar et al
(2013). We also assume the machine translation
system is defined by a function MT (f) that takes
a string of source words f as an argument and re-
turns the translation result
?
e.
3
We will introduce individual methods in the fol-
lowing sections, but all follow the general frame-
work shown below:
1. Decide the mean number of words ? and the
machine translation evaluation measure EV
as parameters of algorithm. We can use an
automatic evaluation measure such as BLEU
(Papineni et al, 2002) as EV . Then, we cal-
culate the number of sub-sentential segmen-
tation boundaries K that we will need to in-
sert into F to achieve an average segment
length ?:
K := max
(
0,
?
?
f?F |f |
?
?
?N
)
. (1)
2. Define S as a set of positions in F in which
we will insert segmentation boundaries. For
example, if we will segment the first sentence
after the third word and the third sentence af-
ter the fifth word, then S = {?1, 3? , ?3, 5?}.
3
In this work, we do not use the history of the language
model mentioned in Bangalore et al (2012). Considering this
information improves the MT performance and we plan to
include this in our approach in future work.
Figure 1: Concatenated translation MT (f ,S).
Based on this representation, choose K seg-
mentation boundaries in F to make the set
S
?
that maximizes an evaluation function ?
as below:
S
?
:= arg max
S?{S
?
:|S
?
|=K}
?(S;F , E , EV,MT ).
(2)
In this work, we define ? as the sum of the
evaluation measure for each parallel sentence
pair ?f
j
,e
j
?:
?(S) :=
N
?
j=1
EV (MT (f
j
,S), e
j
), (3)
where MT (f ,S) represents the concatena-
tion of all partial translations {MT (f
(n)
)}
given the segments S as shown in Figure 1.
Equation (3) indicates that we assume all
parallel sentences to be independent of each
other, and the evaluation measure is calcu-
lated for each sentence separately. This lo-
cality assumption eases efficient implementa-
tion of our algorithm, and can be realized us-
ing a sentence-level evaluation measure such
as BLEU+1 (Lin and Och, 2004).
3. Make a segmentation model M
S
?
by treating
the obtained segmentation boundaries S
?
as
positive labels, all other positions as negative
labels, and training a classifier to distinguish
between them. This classifier is used to de-
tect segmentation boundaries at test time.
Steps 1. and 3. of the above procedure are triv-
ial. In contrast, choosing a good segmentation ac-
cording to Equation (2) is difficult and the focus
of the rest of this paper. In order to exactly solve
Equation (2), we must perform brute-force search
over all possible segmentations unless we make
some assumptions about the relation between the
? yielded by different segmentations. However,
the number of possible segmentations is exponen-
tially large, so brute-force search is obviously in-
tractable. In the following sections, we propose 2
552
I ate lunch but she left
Segments already selected at the k-th iteration
? = 0.5 ? = 0.8
(k+1)-th segment
? = 0.7
Figure 2: Example of greedy search.
Algorithm 1 Greedy segmentation search
S
?
? ?
for k = 1 to K do
S
?
? S
?
?
{
arg max
s

?S
?
?(S
?
? {s})
}
end for
return S
?
methods that approximately search for a solution
to Equation (2).
2.1 Greedy Search
Our first approximation is a greedy algorithm that
selects segmentation boundaries one-by-one. In
this method, k already-selected boundaries are left
unchanged when deciding the (k+1)-th boundary.
We find the unselected boundary that maximizes ?
and add it to S:
S
k+1
= S
k
?
{
arg max
s

?S
k
?(S
k
? {s})
}
. (4)
Figure 2 shows an example of this process for a
single sentence, and Algorithm 1 shows the algo-
rithm for calculating K boundaries.
2.2 Greedy Search with Feature Grouping
and Dynamic Programming
The method described in the previous section
finds segments that achieve high translation per-
formance for the training data. However, because
the translation system MT and evaluation mea-
sureEV are both complex, the evaluation function
? includes a certain amount of noise. As a result,
the greedy algorithm that uses only ? may find a
segmentation that achieves high translation perfor-
mance in the training data by chance. However,
these segmentations will not generalize, reducing
the performance for other data.
We can assume that this problem can be solved
by selecting more consistent segmentations of the
training data. To achieve this, we introduce a con-
straint that all positions that have similar charac-
teristics must be selected at the same time. Specif-
ically, we first group all positions in the source
I ate lunch but she left
PRP VBD NN CC PRP VBD
I ate an apple and an orange
PRP VBD DT NN CC DT NN
WORD:
 POS:
WORD:
 POS:
Group
PRP+VBD
Group
NN+CC
Group
DT+NN
Figure 3: Grouping segments by POS bigrams.
sentences using features of the position, and intro-
duce a constraint that all positions with identical
features must be selected at the same time. Figure
3 shows an example of how this grouping works
when we use the POS bigram surrounding each
potential boundary as our feature set.
By introducing this constraint, we can expect
that features which have good performance over-
all will be selected, while features that have rela-
tively bad performance will not be selected even if
good performance is obtained when segmenting at
a specific location. In addition, because all posi-
tions can be classified as either segmented or not
by evaluating whether the corresponding feature is
in the learned feature set or not, it is not necessary
to train an additional classifier for the segmenta-
tion model when using this algorithm. In other
words, this constraint conducts a kind of feature
selection for greedy search.
In contrast to Algorithm 1, which only selected
one segmentation boundary at once, in our new
setting there are multiple positions selected at one
time. Thus, we need to update our search algo-
rithm to handle this setting. To do so, we use
dynamic programming (DP) together with greedy
search. Algorithm 2 shows ourGreedy+DP search
algorithm. Here, c(?;F) represents the number
of appearances of ? in the set of source sentences
F , and S(F ,?) represents the set of segments de-
fined by both F and the set of features ?.
The outer loop of the algorithm, like Greedy,
iterates over all S of size 1 to K. The inner loop
examines all features that appear exactly j times
in F , and measures the effect of adding them to
the best segmentation with (k ? j) boundaries.
2.3 Regularization by Feature Count
Even after we apply grouping by features, it
is likely that noise will still remain in the less
frequently-seen features. To avoid this problem,
we introduce regularization into the Greedy+DP
algorithm, with the evaluation function ? rewrit-
553
Algorithm 2 Greedy+DP segmentation search
?
0
? ?
for k = 1 to K do
for j = 0 to k ? 1 do
?
?
? {? : c(?;F) = k ? j ? ?
Segmentation for Efficient Supervised Language Annotation with an
Explicit Cost-Utility Tradeoff
Matthias Sperber1, Mirjam Simantzik2, Graham Neubig3, Satoshi Nakamura3, Alex Waibel1
1Karlsruhe Institute of Technology, Institute for Anthropomatics, Germany
2Mobile Technologies GmbH, Germany
3Nara Institute of Science and Technology, AHC Laboratory, Japan
matthias.sperber@kit.edu, mirjam.simantzik@jibbigo.com, neubig@is.naist.jp
s-nakamura@is.naist.jp, waibel@kit.edu
Abstract
In this paper, we study the problem of manu-
ally correcting automatic annotations of natu-
ral language in as efficient a manner as pos-
sible. We introduce a method for automati-
cally segmenting a corpus into chunks such
that many uncertain labels are grouped into
the same chunk, while human supervision
can be omitted altogether for other segments.
A tradeoff must be found for segment sizes.
Choosing short segments allows us to reduce
the number of highly confident labels that are
supervised by the annotator, which is useful
because these labels are often already correct
and supervising correct labels is a waste of
effort. In contrast, long segments reduce the
cognitive effort due to context switches. Our
method helps find the segmentation that opti-
mizes supervision efficiency by defining user
models to predict the cost and utility of su-
pervising each segment and solving a con-
strained optimization problem balancing these
contradictory objectives. A user study demon-
strates noticeable gains over pre-segmented,
confidence-ordered baselines on two natural
language processing tasks: speech transcrip-
tion and word segmentation.
1 Introduction
Many natural language processing (NLP) tasks re-
quire human supervision to be useful in practice,
be it to collect suitable training material or to meet
some desired output quality. Given the high cost of
human intervention, how to minimize the supervi-
sion effort is an important research problem. Previ-
ous works in areas such as active learning, post edit-
(a) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
(b) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
(c) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
Figure 1: Three automatic transcripts of the sentence ?It
was a bright cold day in April, and the clocks were strik-
ing thirteen?, with recognition errors in parentheses. The
underlined parts are to be corrected by a human for (a)
sentences, (b) words, or (c) the proposed segmentation.
ing, and interactive pattern recognition have inves-
tigated this question with notable success (Settles,
2008; Specia, 2011; Gonza?lez-Rubio et al., 2010).
The most common framework for efficient anno-
tation in the NLP context consists of training an NLP
system on a small amount of baseline data, and then
running the system on unannotated data to estimate
confidence scores of the system?s predictions (Set-
tles, 2008). Sentences with the lowest confidence
are then used as the data to be annotated (Figure 1
(a)). However, it has been noted that when the NLP
system in question already has relatively high accu-
racy, annotating entire sentences can be wasteful, as
most words will already be correct (Tomanek and
Hahn, 2009; Neubig et al., 2011). In these cases, it
is possible to achieve much higher benefit per anno-
tated word by annotating sub-sentential units (Fig-
ure 1 (b)).
However, as Settles et al. (2008) point out, sim-
ply maximizing the benefit per annotated instance
is not enough, as the real supervision effort varies
169
Transactions of the Association for Computational Linguistics, 2 (2014) 169?180. Action Editor: Eric Fosler-Lussier.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
1 3 5 7 9 11 13 15 17 19
0
2
4
6
Segment length
A
vg
. t
im
e 
/ i
ns
ta
nc
e 
[s
ec
]
 
 
Transcription task
Word segmentation task
Figure 2: Average annotation time per instance, plotted
over different segment lengths. For both tasks, the effort
clearly increases for short segments.
greatly across instances. This is particularly impor-
tant in the context of choosing segments to annotate,
as human annotators heavily rely on semantics and
context information to process language, and intu-
itively, a consecutive sequence of words can be su-
pervised faster and more accurately than the same
number of words spread out over several locations in
a text. This intuition can also be seen in our empiri-
cal data in Figure 2, which shows that for the speech
transcription and word segmentation tasks described
later in Section 5, short segments had a longer anno-
tation time per word. Based on this fact, we argue
it would be desirable to present the annotator with
a segmentation of the data into easily supervisable
chunks that are both large enough to reduce the num-
ber of context switches, and small enough to prevent
unnecessary annotation (Figure 1 (c)).
In this paper, we introduce a new strategy for nat-
ural language supervision tasks that attempts to op-
timize supervision efficiency by choosing an appro-
priate segmentation. It relies on a user model that,
given a specific segment, predicts the cost and the
utility of supervising that segment. Given this user
model, the goal is to find a segmentation that mini-
mizes the total predicted cost while maximizing the
utility. We balance these two criteria by defining a
constrained optimization problem in which one cri-
terion is the optimization objective, while the other
criterion is used as a constraint. Doing so allows
specifying practical optimization goals such as ?re-
move as many errors as possible given a limited time
budget,? or ?annotate data to obtain some required
classifier accuracy in as little time as possible.?
Solving this optimization task is computationally
difficult, an NP-hard problem. Nevertheless, we
demonstrate that by making realistic assumptions
about the segment length, an optimal solution can
be found using an integer linear programming for-
mulation for mid-sized corpora, as are common for
supervised annotation tasks. For larger corpora, we
provide simple heuristics to obtain an approximate
solution in a reasonable amount of time.
Experiments over two example scenarios demon-
strate the usefulness of our method: Post editing
for speech transcription, and active learning for
Japanese word segmentation. Our model predicts
noticeable efficiency gains, which are confirmed in
experiments with human annotators.
2 Problem Definition
The goal of our method is to find a segmentation
over a corpus of word tokens wN1 that optimizes
supervision efficiency according to some predictive
user model. The user model is denoted as a set of
functions ul,k(wba) that evaluate any possible sub-
sequence wba of tokens in the corpus according to
criteria l2L, and supervision modes k2K.
Let us illustrate this with an example. Sperber et
al. (2013) defined a framework for speech transcrip-
tion in which an initial, erroneous transcript is cre-
ated using automatic speech recognition (ASR), and
an annotator corrects the transcript either by correct-
ing the words by keyboard, by respeaking the con-
tent, or by leaving the words as is. In this case,
we could define K={TYPE, RESPEAK, SKIP}, each
constant representing one of these three supervision
modes. Our method will automatically determine
the appropriate supervision mode for each segment.
The user model in this example might evaluate ev-
ery segment according to two criteria L, a cost crite-
rion (in terms of supervision time) and a utility cri-
terion (in terms of number of removed errors), when
using each mode. Intuitively, respeaking should be
assigned both lower cost (because speaking is faster
than typing), but also lower utility than typing on a
keyboard (because respeaking recognition errors can
occur). The SKIP mode denotes the special, unsuper-
vised mode that always returns 0 cost and 0 utility.
Other possible supervision modes include mul-
tiple input modalities (Suhm et al., 2001), several
human annotators with different expertise and cost
170
(Donmez and Carbonell, 2008), and correction vs.
translation from scratch in machine translation (Spe-
cia, 2011). Similarly, cost could instead be ex-
pressed in monetary terms, or the utility function
could predict the improvement of a classifier when
the resulting annotation is not intended for direct hu-
man consumption, but as training data for a classifier
in an active learning framework.
3 Optimization Framework
Given this setting, we are interested in simulta-
neously finding optimal locations and supervision
modes for all segments, according to the given cri-
teria. Each resulting segment will be assigned ex-
actly one of these supervision modes. We de-
note a segmentation of the N tokens of corpus wN1
into M?N segments by specifying segment bound-
ary markers sM+11 =(s1=1, s2, . . . , sM+1=N+1).
Setting a boundary marker si=a means that we
put a segment boundary before the a-th word to-
ken (or the end-of-corpus marker for a=N+1).
Thus our corpus is segmented into token sequences
[(wsj , . . . , wsj+1 1)]Mj=1. The supervision modes
assigned to each segment are denoted by mj . We
favor those segmentations that minimize the cumu-
lative valuePMj=1[ul,mj (wsj+1sj )] for each criterion l.
For any criterion where larger values are intuitively
better, we flip the sign before defining ul,mj (wsj+1sj )
to maintain consistency (e.g. negative number of er-
rors removed).
3.1 Multiple Criteria Optimization
In the case of a single criterion (|L|=1), we obtain
a simple, single-objective unconstrained linear opti-
mization problem, efficiently solvable via dynamic
programming (Terzi and Tsaparas, 2006). However,
in practice one usually encounters several compet-
ing criteria, such as cost and utility, and here we
will focus on this more realistic setting. We balance
competing criteria by using one as an optimization
objective, and the others as constraints.1 Let crite-
1This approach is known as the bounded objective function
method in multi-objective optimization literature (Marler and
Arora, 2004). The very popular weighted sum method merges
criteria into a single efficiency measure, but is problematic in
our case because the number of supervised tokens is unspec-
ified. Unless the weights are carefully chosen, the algorithm
might find, e.g., the completely unsupervised or completely su-
(at)% (what?s)% a% bright% ?%
[RESPEAK:1.5/2]/
[SKIP:0/0]/
1/ cold%2/ 3/ 4/ 5/ 6/
[TYPE:2/5]/[TYPE:1/4]/
[TYPE:1/4]/
[RESPEAK:0/3]/[SKIP:0/0]/
Figure 3: Excerpt of a segmentation graph for an ex-
ample transcription task similar to Figure 1 (some edges
are omitted for readability). Edges are labeled with their
mode, predicted number of errors that can be removed,
and necessary supervision time. A segmentation scheme
might prefer solid edges over dashed ones in this exam-
ple.
rion l0 be the optimization objective criterion, and
let Cl denote the constraining constants for the cri-
teria l 2 L l0 = L \ {l0}. We state the optimization
problem:
min
M ;sM+11 ;mM1
MX
j=1
?
ul0,mj
 
wsj+1sj
 ?
s.t.
MX
j=1
?
ul,mj
 
wsj+1sj
 ?
? Cl (8l 2 L l0)
This constrained optimization problem is difficult
to solve. In fact, the NP-hard multiple-choice knap-
sack problem (Pisinger, 1994) corresponds to a spe-
cial case of our problem in which the number of seg-
ments is equal to the number of tokens, implying
that our more general problem is NP-hard as well.
In order to overcome this problem, we refor-
mulate search for the optimal segmentation as a
resource-constrained shortest path problem in a di-
rected, acyclic multigraph. While still not efficiently
solvable in theory, this problem is well studied in
domains such as vehicle routing and crew schedul-
ing (Irnich and Desaulniers, 2005), and it is known
that in many practical situations the problem can
be solved reasonably efficiently using integer linear
programming relaxations (Toth and Vigo, 2001).
In our formalism, the set of nodes V represents
the spaces between neighboring tokens, at which the
algorithm may insert segment boundaries. A node
with index i represents a segment break before the
i-th token, and thus the sequence of the indices in
a path directly corresponds to sM+11 . Edges E de-
note the grouping of tokens between the respective
pervised segmentation to be most ?efficient.?
171
nodes into one segment. Edges are always directed
from left to right, and labeled with a supervision
mode. In addition, each edge between nodes i and j
is assigned ul,k(wj 1i ), the corresponding predicted
value for each criterion l 2 L and supervision mode
k 2 K, indicating that the supervision mode of the
j-th segment in a path directly corresponds to mj .
Figure 3 shows an example of what the result-
ing graph may look like. Our original optimization
problem is now equivalent to finding the shortest
path between the first and last nodes according to
criterion l0, while obeying the given resource con-
straints. According to a widely used formulation for
the resource constrained shortest path problem, we
can defineEij as the set of competing edges between
i and j, and express this optimization problem with
the following integer linear program (ILP):
min
x
X
i,j2V
X
k2Eij
xijkul0,k(sj 1i ) (1)
s.t.
X
i,j2V
X
k2Eij
xijkul,k(sj 1i ) ? Cl
(8l 2 L l0)
(2)
X
i2V
k2Eij
xijk =
X
i2V
k2Eij
xjik
(8j 2 V \{1, n})
(3)
X
j2V
k2E1j
x1jk = 1 (4)
X
i2V
k2Ein
xink = 1 (5)
xijk 2 {0, 1} (8xijk 2 x) (6)
The variables x={xijk|i, j 2 V , k 2 Eij} denote
the activation of the k?th edge between nodes i and
j. The shortest path according to the minimization
objective (1), that still meets the resource constraints
for the specified criteria (2), is to be computed. The
degree constraints (3,4,5) specify that all but the first
and last nodes must have as many incoming as out-
going edges, while the first node must have exactly
one outgoing, and the last node exactly one incom-
ing edge. Finally, the integrality condition (6) forces
all edges to be either fully activated or fully deacti-
vated. The outlined problem formulation can solved
directly by using off-the-shelf ILP solvers, here we
employ GUROBI (Gurobi Optimization, 2012).
3.2 Heuristics for Approximation
In general, edges are inserted for every supervision
mode between every combination of two nodes. The
search space can be constrained by removing some
of these edges to increase efficiency. In this study,
we only consider edges spanning at most 20 tokens.
For cases in which larger corpora are to be anno-
tated, or when the acceptable delay for delivering re-
sults is small, a suitable segmentation can be found
approximately. The easiest way would be to parti-
tion the corpus, e.g. according to its individual doc-
uments, divide the budget constraints evenly across
all partitions, and then segment each partition inde-
pendently. More sophisticated methods might ap-
proximate the Pareto front for each partition, and
distribute the budgets in an intelligent way.
4 User Modeling
While the proposed framework is able to optimize
the segmentation with respect to each criterion, it
also rests upon the assumption that we can provide
user models ul,k(wj 1i ) that accurately evaluate ev-
ery segment according to the specified criteria and
supervision modes. In this section, we discuss our
strategies for estimating three conceivable criteria:
annotation cost, correction of errors, and improve-
ment of a classifier.
4.1 Annotation Cost Modeling
Modeling cost requires solving a regression prob-
lem from features of a candidate segment to annota-
tion cost, for example in terms of supervision time.
Appropriate input features depend on the task, but
should include notions of complexity (e.g. a confi-
dence measure) and length of the segment, as both
are expected to strongly influence supervision time.
We propose using Gaussian process (GP) regres-
sion for cost prediction, a start-of-the-art nonpara-
metric Bayesian regression technique (Rasmussen
and Williams, 2006)2. As reported on a similar
task by Cohn and Specia (2013), and confirmed by
our preliminary experiments, GP regression signifi-
cantly outperforms popular techniques such as sup-
2Code available at http://www.gaussianprocess.org/gpml/
172
port vector regression and least-squares linear re-
gression. We also follow their settings for GP, em-
ploying GP regression with a squared exponential
kernel with automatic relevance determination. De-
pending on the number of users and amount of train-
ing data available for each user, models may be
trained separately for each user (as we do here), or
in a combined fashion via multi-task learning as pro-
posed by Cohn and Specia (2013).
It is also crucial for the predictions to be reliable
throughout the whole relevant space of segments.
If the cost of certain types of segments is system-
atically underpredicted, the segmentation algorithm
might be misled to prefer these, possibly a large
number of times.3 An effective trick to prevent such
underpredictions is to predict the log time instead of
the actual time. In this way, errors in the critical low
end are penalized more strongly, and the time can
never become negative.
4.2 Error Correction Modeling
As one utility measure, we can use the number of
errors corrected, a useful measure for post editing
tasks over automatically produced annotations. In
order to measure how many errors can be removed
by supervising a particular segment, we must es-
timate both how many errors are in the automatic
annotation, and how reliably a human can remove
these for a given supervision mode.
Most machine learning techniques can estimate
confidence scores in the form of posterior probabil-
ities. To estimate the number of errors, we can sum
over one minus the posterior for all tokens, which
estimates the Hamming distance from the reference
annotation. This measure is appropriate for tasks in
which the number of tokens is fixed in advance (e.g.
a part-of-speech estimation task), and a reasonable
approximation for tasks in which the number of to-
kens is not known in advance (e.g. speech transcrip-
tion, cf. Section 5.1.1).
Predicting the particular tokens at which a human
will make a mistake is known to be a difficult task
(Olson and Olson, 1990), but a simplifying constant
3For instance, consider a model that predicts well for seg-
ments of medium size or longer, but underpredicts the supervi-
sion time of single-token segments. This may lead the segmen-
tation algorithm to put every token into its own segment, which
is clearly undesirable.
human error rate can still be useful. For example,
in the task from Section 2, we may suspect a certain
number of errors in a transcript segment, and predict,
say, 95% of those errors to be removed via typing,
but only 85% via respeaking.
4.3 Classifier Improvement Modeling
Another reasonable utility measure is accuracy of a
classifier trained on the data we choose to annotate
in an active learning framework. Confidence scores
have been found useful for ranking particular tokens
with regards to how much they will improve a clas-
sifier (Settles, 2008). Here, we may similarly score
segment utility as the sum of its token confidences,
although care must be taken to normalize and cali-
brate the token confidences to be linearly compara-
ble before doing so. While the resulting utility score
has no interpretation in absolute terms, it can still be
used as an optimization objective (cf. Section 5.2.1).
5 Experiments
In this section, we present experimental results ex-
amining the effectiveness of the proposed method
over two tasks: speech transcription and Japanese
word segmentation.4
5.1 Speech Transcription Experiments
Accurate speech transcripts are a much-demanded
NLP product, useful by themselves, as training ma-
terial for ASR, or as input for follow-up tasks like
speech translation. With recognition accuracies
plateauing, manually correcting (post editing) auto-
matic speech transcripts has become popular. Com-
mon approaches are to identify words (Sanchez-
Cortina et al., 2012) or (sub-)sentences (Sperber et
al., 2013) of low confidence, and have a human edi-
tor correct these.
5.1.1 Experimental Setup
We conduct a user study in which participants
post-edited speech transcripts, given a fixed goal
word error rate. The transcription setup was such
that the transcriber could see the ASR transcript of
parts before and after the segment that he was edit-
ing, providing context if needed. When imprecise
time alignment resulted in segment breaks that were
4Software and experimental data can be downloaded from
http://www.msperber.com/research/tacl-segmentation/
173
slightly ?off,? as happened occasionally, that context
helped guess what was said. The segment itself was
transcribed from scratch, as opposed to editing the
ASR transcript; besides being arguably more effi-
cient when the ASR transcript contains many mis-
takes (Nanjo et al., 2006; Akita et al., 2009), prelim-
inary experiments also showed that supervision time
is far easier to predict this way. Figure 4 illustrates
what the setup looked like.
We used a self-developed transcription tool to
conduct experiments. It presents our computed seg-
ments one by one, allows convenient input and play-
back via keyboard shortcuts, and logs user interac-
tions with their time stamps. A selection of TED
talks5 (English talks on technology, entertainment,
and design) served as experimental data. While
some of these talks contain jargon such as medi-
cal terms, they are presented by skilled speakers,
making them comparably easy to understand. Initial
transcripts were created using the Janus recognition
toolkit (Soltau et al., 2001) with a standard, TED-
optimized setup. We used confusion networks for
decoding and obtaining confidence scores.
For reasons of simplicity, and better compara-
bility to our baseline, we restricted our experiment
to two supervision modes: TYPE and SKIP. We
conducted experiments with 3 participants, 1 with
several years of experience in transcription, 2 with
none. Each participant received an explanation on
the transcription guidelines, and a short hands-on
training to learn to use our tool. Next, they tran-
scribed a balanced selection of 200 segments of
varying length and quality in random order. This
data was used to train the user models.
Finally, each participant transcribed another 2
TED talks, with word error rate (WER) 19.96%
(predicted: 22.33%). We set a target (predicted)
WER of 15% as our optimization constraint,6 and
minimize the predicted supervision time as our ob-
jective function. Both TED talks were transcribed
once using the baseline strategy, and once using the
proposed strategy. The order of both strategies was
reversed between talks, to minimize learning bias
due to transcribing each talk twice.
The baseline strategy was adopted according to
5www.ted.com
6Depending on the level of accuracy required by our final
application, this target may be set lower or higher.
Sperber et al. (2013): We segmented the talk into
natural, subsentential units, using Matusov et al.
(2006)?s segmenter, which we tuned to reproduce
the TED subtitle segmentation, producing a mean
segment length of 8.6 words. Segments were added
in order of increasing average word confidence, until
the user model predicted a WER<15%. The second
segmentation strategy was the proposed method,
similarly with a resource constraint of WER<15%.
Supervision time was predicted via GP regres-
sion (cf. Section 4.1), using segment length, au-
dio duration, and mean confidence as input features.
The output variable was assumed subject to addi-
tive Gaussian noise with zero mean, a variance of
5 seconds was chosen empirically to minimize the
mean squared error. Utility prediction (cf. Section
4.2) was based on posterior scores obtained from
the confusion networks. We found it important to
calibrate them, as the posteriors were overconfident
especially in the upper range. To do so, we automat-
ically transcribed a development set of TED data,
grouped the recognized words into buckets accord-
ing to their posteriors, and determined the average
number of errors per word in each bucket from an
alignment with the reference transcript. The map-
ping from average posterior to average number of
errors was estimated via GP regression. The result
was summed over all tokens, and multiplied by a
constant human confidence, separately determined
for each participant.7
5.1.2 Simulation Results
To convey a better understanding of the poten-
tial gains afforded by our method, we first present a
simulated experiment. We assume a transcriber who
makes no mistakes, and needs exactly the amount of
time predicted by a user model trained on the data of
a randomly selected participant. We compare three
scenarios: A baseline simulation, in which the base-
line segments are transcribed in ascending order of
confidence; a simulation using the proposed method,
in which we change the WER constraint in small in-
crements; finally, an oracle simulation, which uses
7More elaborate methods for WER estimation exist, such as
by Ogawa et al. (2013), but if our method achieves improve-
ments using simple Hamming distance, incorporating more so-
phisticated measures will likely achieve similar, or even better
accuracy.
174
(3) SKIP: ?nineteen forty six until today you see the green?
(4) TYPE: <annotator types: ?is the traditional?>
(5) SKIP: ?Interstate conflict?
(6) TYPE: <annotator types: ?the ones we used to?>
(7) SKIP: . . .
Figure 4: Result of our segmentation method (excerpt).
TYPE segments are displayed empty and should be tran-
scribed from scratch. For SKIP segments, the ASR tran-
script is displayed to provide context. When annotating a
segment, the corresponding audio is played back.
0 10 20 30 40 50 60
0
5
10
15
20
25
Post editing time [min]
R
es
ul
tin
g 
W
ER
 [%
]
 
 
Baseline
Proposed
Oracle
Figure 5: Simulation of post editing on example TED
talk. The proposed method reduces the WER consider-
ably faster than the baseline at first, later both converge.
The much superior oracle simulation indicates room for
further improvement.
the proposed method, but uses a utility model that
knows the actual number of errors in each segment.
For each supervised segment, we simply replace the
ASR output with the reference, and measure the re-
sulting WER.
Figure 5 shows the simulation on an example
TED talk, based on an initial transcript with 21.9%
WER. The proposed method is able to reduce the
WER faster than the baseline, up to a certain point
where they converge. The oracle simulation is even
faster, indicating room for improvement through
better confidence scores.
5.1.3 User Study Results
Table 1 shows the results of the user study. First,
we note that the WER estimation by our utility
model was off by about 2.5%: While the predicted
improvement in WER was from 22.33% to 15.0%,
the actual improvement was from 19.96% to about
12.5%. The actual resulting WER was consistent
Participant Baseline ProposedWER Time WER Time
P1 12.26 44:05 12.18 33:01
P2 12.75 36:19 12.77 29:54
P3 12.70 52:42 12.50 37:57
AVG 12.57 44:22 12.48 33:37
Table 1: Transcription task results. For each user, the
resultingWER [%] after supervision is shown, along with
the time [min] they needed. The unsupervised WER was
19.96%.
across all users, and we observe strong, consistent
reductions in supervision time for all participants.
Prediction of the necessary supervision time was ac-
curate: Averaged over participants, 45:41 minutes
were predicted for the baseline, 44:22 minutes mea-
sured. For the proposed method, 32:11 minutes were
predicted, 33:37 minutes measured. On average,
participants removed 6.68 errors per minute using
the baseline, and 8.93 errors per minute using the
proposed method, a speed-up of 25.2%.
Note that predicted and measured values are not
strictly comparable: In the experiments, to provide
a fair comparison participants transcribed the same
talks twice (once using baseline, once the proposed
method, in alternating order), resulting in a notice-
able learning effect. The user model, on the other
hand, is trained to predict the case in which a tran-
scriber conducts only one transcription pass.
As an interesting finding, without being informed
about the order of baseline and proposed method,
participants reported that transcribing according to
the proposed segmentation seemed harder, as they
found the baseline segmentation more linguistically
reasonable. However, this perceived increase in dif-
ficulty did not show in efficiency numbers.
5.2 Japanese Word Segmentation Experiments
Word segmentation is the first step in NLP for lan-
guages that are commonly written without word
boundaries, such as Japanese and Chinese. We ap-
ply our method to a task in which we domain-adapt a
word segmentation classifier via active learning. In
this experiment, participants annotated whether or
not a word boundary occurred at certain positions in
a Japanese sentence. The tokens to be grouped into
segments are positions between adjacent characters.
175
5.2.1 Experimental Setup
Neubig et al. (2011) have proposed a pointwise
method for Japanese word segmentation that can be
trained using partially annotated sentences, which
makes it attractive in combination with active learn-
ing, as well as our segmentation method. The
authors released their method as a software pack-
age ?KyTea? that we employed in this user study.
We used KyTea?s active learning domain adaptation
toolkit8 as a baseline.
For data, we used the Balanced Corpus of Con-
temporary Written Japanese (BCCWJ), created by
Maekawa (2008), with the internet Q&A subcor-
pus as in-domain data, and the whitepaper subcor-
pus as background data, a domain adaptation sce-
nario. Sentences were drawn from the in-domain
corpus, and the manually annotated data was then
used to train KyTea, along with the pre-annotated
background data. The goal (objective function) was
to improve KyTea?s classification accuracy on an in-
domain test set, given a constrained time budget of
30 minutes. There were again 2 supervision modes:
ANNOTATE and SKIP. Note that this is essentially a
batch active learning setup with only one iteration.
We conducted experiments with one expert with
several years of experience with Japanese word seg-
mentation annotation, and three non-expert native
speakers with no prior experience. Japanese word
segmentation is not a trivial task, so we provided
non-experts with training, including explanation of
the segmentation standard, a supervised test with
immediate feedback and explanations, and hands-on
training to get used to the annotation software.
Supervision time was predicted via GP regression
(cf. Section 4.1), using the segment length and mean
confidence as input features. As before, the output
variable was assumed subject to additive Gaussian
noise with zero mean and 5 seconds variance. To ob-
tain training data for these models, each participant
annotated about 500 example instances, drawn from
the adaptation corpus, grouped into segments and
balanced regarding segment length and difficulty.
For utility modeling (cf. Section 4.3), we first nor-
malized KyTea?s confidence scores, which are given
in terms of SVM margin, using a sigmoid function
(Platt, 1999). The normalization parameter was se-
8http://www.phontron.com/kytea/active.html
lected so that the mean confidence on a development
set corresponded to the actual classifier accuracy.
We derive our measure of classifier improvement for
correcting a segment by summing over one minus
the calibrated confidence for each of its tokens. To
analyze how well this measure describes the actual
training utility, we trained KyTea using the back-
ground data plus disjoint groups of 100 in-domain
instances with similar probabilities and measured
the achieved reduction of prediction errors. The cor-
relation between each group?s mean utility and the
achieved error reduction was 0.87. Note that we ig-
nore the decaying returns usually observed as more
data is added to the training set. Also, we did not
attempt to model user errors. Employing a con-
stant base error rate, as in the transcription scenario,
would change segment utilities only by a constant
factor, without changing the resulting segmentation.
After creating the user models, we conducted the
main experiment, in which each participant anno-
tated data that was selected from a pool of 1000
in-domain sentences using two strategies. The first,
baseline strategy was as proposed by Neubig et al.
(2011). Queries are those instances with the low-
est confidence scores. Each query is then extended
to the left and right, until a word boundary is pre-
dicted. This strategy follows similar reasoning as
was the premise to this paper: To decide whether or
not a position in a text corresponds to a word bound-
ary, the annotator has to acquire surrounding context
information. This context acquisition is relatively
time consuming, so he might as well label the sur-
rounding instances with little additional effort. The
second strategy was our proposed, more principled
approach. Queries of both methods were shuffled
to minimize bias due to learning effects. Finally, we
trained KyTea using the results of both methods, and
compared the achieved classifier improvement and
supervision times.
5.2.2 User Study Results
Table 2 summarizes the results of our experi-
ment. It shows that the annotations by each partic-
ipant resulted in a better classifier for the proposed
method than the baseline, but also took up consider-
ably more time, a less clear improvement than for
the transcription task. In fact, the total error for
time predictions was as high as 12.5% on average,
176
Participant Baseline ProposedTime Acc. Time Acc.
Expert 25:50 96.17 32:45 96.55
NonExp1 22:05 95.79 26:44 95.98
NonExp2 23:37 96.15 31:28 96.21
NonExp3 25:23 96.38 33:36 96.45
Table 2: Word segmentation task results, for our ex-
pert and 3 non-expert participants. For each participant,
the resulting classifier accuracy [%] after supervision is
shown, along with the time [min] they needed. The unsu-
pervised accuracy was 95.14%.
where the baseline method tended take less time than
predicted, the proposed method more time. This is
in contrast to a much lower total error (within 1%)
when cross-validating our user model training data.
This is likely due to the fact that the data for train-
ing the user model was selected in a balanced man-
ner, as opposed to selecting difficult examples, as
our method is prone to do. Thus, we may expect
much better predictions when selecting user model
training data that is more similar to the test case.
Plotting classifier accuracy over annotation time
draws a clearer picture. Let us first analyze the re-
sults for the expert annotator. Figure 6 (E.1) shows
that the proposed method resulted in consistently
better results, indicating that time predictions were
still effective. Note that this comparison may put the
proposed method at a slight disadvantage by com-
paring intermediate results despite optimizing glob-
ally.
For the non-experts, the improvement over the
baseline is less consistent, as can be seen in Fig-
ure 6 (N.1) for one representative. According to
our analysis, this can be explained by two factors:
(1) The non-experts? annotation error (6.5% on av-
erage) was much higher than the expert?s (2.7%),
resulting in a somewhat irregular classifier learn-
ing curve. (2) The variance in annotation time
per segment was consistently higher for the non-
experts than the expert, indicated by an average
per-segment prediction error of 71% vs. 58% rela-
tive to the mean actual value, respectively. Infor-
mally speaking, non-experts made more mistakes,
and were more strongly influenced by the difficulty
of a particular segment (which was higher on av-
erage with the proposed method, as indicated by a
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
 
 
Prop.
Basel
N.1E.1 N.2E.2 N.3E.3 N.4E.4
Annotation time [min.]
Classifier Accu
racy
.
Figure 6: Classifier improvement over time, depicted for
the expert (E) and a non-expert (N). The graphs show
numbers based on (1) actual annotations and user mod-
els as in Sections 4.1 and 4.3, (2) error-free annotations,
(3) measured times replaced by predicted times, and (4)
both reference annotations and replaced time predictions.
lower average confidence).9
In Figures 6 (2-4) we present a simulation experi-
ment in which we first pretend as if annotators made
no mistakes, then as if they needed exactly as much
time as predicted for each segment, and then both.
This cheating experiment works in favor of the pro-
posed method, especially for the non-expert. We
may conclude that our segmentation approach is ef-
fective for the word segmentation task, but requires
more accurate time predictions. Better user models
will certainly help, although for the presented sce-
nario our method may be most useful for an expert
annotator.
9Note that the non-expert in the figure annotated much faster
than the expert, which explains the comparable classification
result despite making more annotation errors. This is in contrast
to the other non-experts, who were slower.
177
5.3 Computational Efficiency
Since our segmentation algorithm does not guar-
antee polynomial runtime, computational efficiency
was a concern, but did not turn out problematic.
On a consumer laptop, the solver produced seg-
mentations within a few seconds for a single docu-
ment containing several thousand tokens, and within
hours for corpora consisting of several dozen doc-
uments. Runtime increased roughly quadratically
with respect to the number of segmented tokens. We
feel that this is acceptable, considering that the time
needed for human supervision will likely dominate
the computation time, and reasonable approxima-
tions can be made as noted in Section 3.2.
6 Relation to Prior Work
Efficient supervision strategies have been studied
across a variety of NLP-related research areas, and
received increasing attention in recent years. Ex-
amples include post editing for speech recogni-
tion (Sanchez-Cortina et al., 2012), interactive ma-
chine translation (Gonza?lez-Rubio et al., 2010), ac-
tive learning for machine translation (Haffari et al.,
2009; Gonza?lez-Rubio et al., 2011) and many other
NLP tasks (Olsson, 2009), to name but a few studies.
It has also been recognized by the active learn-
ing community that correcting the most useful parts
first is often not optimal in terms of efficiency, since
these parts tend to be the most difficult to manually
annotate (Settles et al., 2008). The authors advocate
the use of a user model to predict the supervision ef-
fort, and select the instances with best ?bang-for-the-
buck.? This prediction of supervision effort was suc-
cessful, and was further refined in other NLP-related
studies (Tomanek et al., 2010; Specia, 2011; Cohn
and Specia, 2013). Our approach to user modeling
using GP regression is inspired by the latter.
Most studies on user models consider only super-
vision effort, while neglecting the accuracy of hu-
man annotations. The view on humans as a perfect
oracle has been criticized (Donmez and Carbonell,
2008), since human errors are common and can
negatively affect supervision utility. Research on
human-computer-interaction has identified the mod-
eling of human errors as very difficult (Olson and
Olson, 1990), depending on factors such as user ex-
perience, cognitive load, user interface design, and
fatigue. Nevertheless, even the simple error model
used in our post editing task was effective.
The active learning community has addressed the
problem of balancing utility and cost in some more
detail. The previously reported ?bang-for-the-buck?
approach is a very simple, greedy approach to com-
bine both into one measure. A more theoretically
founded scalar optimization objective is the net ben-
efit (utility minus costs) as proposed by Vijaya-
narasimhan and Grauman (2009), but unfortunately
is restricted to applications where both can be ex-
pressed in terms of the same monetary unit. Vijaya-
narasimhan et al. (2010) and Donmez and Carbonell
(2008) use a more practical approach that specifies a
constrained optimization problem by allowing only
a limited time budget for supervision. Our approach
is a generalization thereof and allows either specify-
ing an upper bound on the predicted cost, or a lower
bound on the predicted utility.
The main novelty of our presented approach is
the explicit modeling and selection of segments of
various sizes, such that annotation efficiency is opti-
mized according to the specified constraints. While
some works (Sassano and Kurohashi, 2010; Neubig
et al., 2011) have proposed using subsentential seg-
ments, we are not aware of any previous work that
explicitly optimizes that segmentation.
7 Conclusion
We presented a method that can effectively choose
a segmentation of a language corpus that optimizes
supervision efficiency, considering not only the ac-
tual usefulness of each segment, but also the anno-
tation cost. We reported noticeable improvements
over strong baselines in two user studies. Future user
experiments with more participants would be desir-
able to verify our observations, and allow further
analysis of different factors such as annotator ex-
pertise. Also, future research may improve the user
modeling, which will be beneficial for our method.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n 287658 Bridges Across the Language
Divide (EU-BRIDGE).
178
References
Yuya Akita, Masato Mimura, and Tatsuya Kawahara.
2009. Automatic Transcription System for Meetings
of the Japanese National Congress. In Interspeech,
pages 84?87, Brighton, UK.
Trevor Cohn and Lucia Specia. 2013. Modelling Anno-
tator Bias with Multi-task Gaussian Processes: An Ap-
plication to Machine Translation Quality Estimation.
In Association for Computational Linguistics Confer-
ence (ACL), Sofia, Bulgaria.
Pinar Donmez and Jaime Carbonell. 2008. Proactive
Learning : Cost-Sensitive Active Learning with Mul-
tiple Imperfect Oracles. In Conference on Information
and Knowledge Management (CIKM), pages 619?628,
Napa Valley, CA, USA.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2010. Balancing User Effort and
Translation Error in Interactive Machine Translation
Via Confidence Measures. In Association for Compu-
tational Linguistics Conference (ACL), Short Papers
Track, pages 173?177, Uppsala, Sweden.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2011. An active learning scenario
for interactive machine translation. In International
Conference on Multimodal Interfaces (ICMI), pages
197?200, Alicante, Spain.
Gurobi Optimization. 2012. Gurobi Optimizer Refer-
ence Manual.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active Learning for Statistical Phrase-based
Machine Translation. In North American Chapter
of the Association for Computational Linguistics -
Human Language Technologies Conference (NAACL-
HLT), pages 415?423, Boulder, CO, USA.
Stefan Irnich and Guy Desaulniers. 2005. Shortest Path
Problems with Resource Constraints. In Column Gen-
eration, pages 33?65. Springer US.
Kikuo Maekawa. 2008. Balanced Corpus of Contem-
porary Written Japanese. In International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 101?102, Hyderabad, India.
R. Timothy Marler and Jasbir S. Arora. 2004. Survey
of multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369?395, April.
EvgenyMatusov, ArneMauser, and Hermann Ney. 2006.
Automatic Sentence Segmentation and Punctuation
Prediction for Spoken Language Translation. In Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 158?165, Kyoto, Japan.
Hiroaki Nanjo, Yuya Akita, and Tatsuya Kawahara.
2006. Computer Assisted Speech Transcription Sys-
tem for Efficient Speech Archive. In Western Pacific
Acoustics Conference (WESPAC), Seoul, Korea.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise Prediction for Robust , Adapt-
able Japanese Morphological Analysis. In Associa-
tion for Computational Linguistics: Human Language
Technologies Conference (ACL-HLT), pages 529?533,
Portland, OR, USA.
Atsunori Ogawa, Takaaki Hori, and Atsushi Naka-
mura. 2013. Discriminative Recognition Rate Esti-
mation For N-Best List and Its Application To N-Best
Rescoring. In International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pages 6832?
6836, Vancouver, Canada.
Judith Reitman Olson and Gary Olson. 1990. The
Growth of Cognitive Modeling in Human-Computer
Interaction Since GOMS. Human-Computer Interac-
tion, 5(2):221?265, June.
Fredrik Olsson. 2009. A literature survey of active ma-
chine learning in the context of natural language pro-
cessing. Technical report, SICS Sweden.
David Pisinger. 1994. A Minimal Algorithm for the
Multiple-Choice Knapsack Problem. European Jour-
nal of Operational Research, 83(2):394?410.
John C. Platt. 1999. Probabilistic Outputs for Sup-
port Vector Machines and Comparisons to Regularized
Likelihood Methods. In Advances in Large Margin
Classifiers, pages 61?74. MIT Press.
Carl E. Rasmussen and Christopher K.I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT
Press, Cambridge, MA, USA.
Isaias Sanchez-Cortina, Nicolas Serrano, Alberto San-
chis, and Alfons Juan. 2012. A prototype for Inter-
active Speech Transcription Balancing Error and Su-
pervision Effort. In International Conference on Intel-
ligent User Interfaces (IUI), pages 325?326, Lisbon,
Portugal.
Manabu Sassano and Sadao Kurohashi. 2010. Using
Smaller Constituents Rather Than Sentences in Ac-
tive Learning for Japanese Dependency Parsing. In
Association for Computational Linguistics Conference
(ACL), pages 356?365, Uppsala, Sweden.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active Learning with Real Annotation Costs. In
Neural Information Processing Systems Conference
(NIPS) - Workshop on Cost-Sensitive Learning, Lake
Tahoe, NV, United States.
Burr Settles. 2008. An Analysis of Active Learning
Strategies for Sequence Labeling Tasks. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1070?1079, Honolulu, USA.
Hagen Soltau, Florian Metze, Christian Fu?gen, and Alex
Waibel. 2001. A One-Pass Decoder Based on Poly-
morphic Linguistic Context Assignment. In Auto-
matic Speech Recognition and Understanding Work-
179
shop (ASRU), pages 214?217, Madonna di Campiglio,
Italy.
Lucia Specia. 2011. Exploiting Objective Annota-
tions for Measuring Translation Post-editing Effort. In
Conference of the European Association for Machine
Translation (EAMT), pages 73?80, Nice, France.
Matthias Sperber, Graham Neubig, Christian Fu?gen,
Satoshi Nakamura, and Alex Waibel. 2013. Efficient
Speech Transcription Through Respeaking. In Inter-
speech, pages 1087?1091, Lyon, France.
Bernhard Suhm, Brad Myers, and Alex Waibel. 2001.
Multimodal error correction for speech user inter-
faces. Transactions on Computer-Human Interaction,
8(1):60?98.
Evimaria Terzi and Panayiotis Tsaparas. 2006. Efficient
algorithms for sequence segmentation. In SIAM Con-
ference on Data Mining (SDM), Bethesda, MD, USA.
Katrin Tomanek and Udo Hahn. 2009. Semi-Supervised
Active Learning for Sequence Labeling. In Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), pages 1039?1047, Singapore.
Katrin Tomanek, Udo Hahn, and Steffen Lohmann.
2010. A Cognitive Cost Model of Annotations Based
on Eye-Tracking Data. In Association for Compu-
tational Linguistics Conference (ACL), pages 1158?
1167, Uppsala, Sweden.
Paolo Toth and Daniele Vigo. 2001. The Vehicle Routing
Problem. Society for Industrial & Applied Mathemat-
ics (SIAM), Philadelphia.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2009. Whats It Going to Cost You?: Predicting Ef-
fort vs. Informativeness for Multi-Label Image Anno-
tations. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 2262?2269, Miami
Beach, FL, USA.
Sudheendra Vijayanarasimhan, Prateek Jain, and Kristen
Grauman. 2010. Far-sighted active learning on a bud-
get for image and video recognition. In Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 3035?3042, San Francisco, CA, USA, June.
180
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 88?96,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Linguistic and Acoustic Features for Automatic Identification of Autism
Spectrum Disorders in Children?s Narrative
Hiroki Tanaka, Sakriani Sakti, Graham Neubig, Tomoki Toda, Satoshi Nakamura
Graduate School of Information Science, Nara Institute of Science and Technology
{hiroki-tan, ssakti, neubig, tomoki, s-nakamura}@is.naist.jp
Abstract
Autism spectrum disorders are develop-
mental disorders characterised as deficits
in social and communication skills, and
they affect both verbal and non-verbal
communication. Previous works measured
differences in children with and without
autism spectrum disorders in terms of
linguistic and acoustic features, although
they do not mention automatic identifi-
cation using integration of these features.
In this paper, we perform an exploratory
study of several language and speech fea-
tures of both single utterances and full nar-
ratives. We find that there are charac-
teristic differences between children with
autism spectrum disorders and typical de-
velopment with respect to word categories,
prosody, and voice quality, and that these
differences can be used in automatic clas-
sifiers. We also examine the differences
between American and Japanese children
and find significant differences with re-
gards to pauses before new turns and lin-
guistic cues.
1 Introduction
Autism spectrum disorders (ASD) are develop-
mental disorders, first described by Kanner and
Asperger in 1943 and 1944 respectively (Kanner,
1943; Asperger, 1944). The American Psychi-
atric Association defines the two characteristics of
ASD as: 1) persistent deficits in social communi-
cation and social interaction across multiple con-
texts, and 2) restricted, repetitive patterns of be-
havior, interests, or activities (American Psychi-
atric Association, 2013). In particular, the former
deficits in social communication are viewed as the
most central characteristic of ASD. Thus, quanti-
fying the degree of social communication skills is
a necessary component of understanding the na-
ture of ASD, creating systems for automatic ASD
screening, and early intervention methods such as
social skills training and applied behaviour analy-
sis (Wallace et al., 1980; Lovaas et al., 1973).
There are a number of studies finding differ-
ences between people with ASD and people with
typical development (TD). In terms of deficits in
social communication, there have been reports de-
scribing atypical usage of gestures (Ashley and
Inge-Marie, 2010), frequency of eye-contact and
laughter (Geraldine et al., 1990), prosody (Mc-
Cann and Peppe, 2003; Rhea et al., 2005), voice
quality (Asgari et al., 2013), delay responses
(Heeman et al., 2010), and unexpected words
(Rouhizadeh et al., 2013). In this paper, we par-
ticularly focus on the cues of ASD that appear in
children?s language and speech
In the case of language, Newton et al. (2009)
analyze blogs of people with ASD and TD, and
found that people with ASD have larger variation
of usage of words describing social processes, al-
though there are no significant differences in other
word categories. In the case of speech, people with
ASD tend to have prosody that differs from that
of their peers (Kanner, 1943), although McCann
and Peppe (2003) note that prosody in ASD is an
under-researched area and that where research has
been undertaken, findings often conflict. Since
then, there have been various studies analyzing
and modeling prosody in people with ASD (Daniel
et al., 2012; Kiss et al., 2013; Santen et al., 2013;
Van et al., 2010). For example, Kiss et al. (2012)
find several significant differences in the pitch
characteristics of ASD, and report that automatic
classification utilizing these features achieves ac-
curacy well above chance level. To our knowl-
edge, there is no previous work integrating both
language and speech features to identify differ-
ences between people with ASD and TD. How-
ever, it has been noted that differences in person-
88
ality traits including introversion/extroversion can
be identified using these features (Mairesse et al.,
2007).
In this paper, we perform a comprehensive anal-
ysis of language and speech features mentioned in
previous works, as well as novel features specific
to this work. In addition, while previous works an-
alyzed differences between people with ASD and
TD, we additionally investigate whether it is possi-
ble to automatically distinguish between children
with ASD or TD using both language and speech
features and a number of classification methods.
We focus on narratives, where the children serving
as our subjects tell a memorable story to their par-
ent (Davis et al., 2004). Here, the use of narrative
allows us to consider not only single-sentence fea-
tures, but also features considering interaction as-
pects between the child and parent such as pauses
before new turns and overall narrative-specific fea-
tures such as words per minute and usage of un-
expected words. Given this setting, we perform
a pilot study examining differences between chil-
dren with ASD and TD, the possibilities of auto-
matic classification between ASD and TD, and the
differences between American and Japanese chil-
dren.
2 Data Description
As a target for our analysis, we first collected a
data set of interactions between Japanese children
and their parents. In collecting the data, we fol-
lowed the procedure used in the creation of the
USC Rachel corpus (Mower et al., 2011). The data
consists of four sessions: doh (free play), jenga (a
game), narrative, and natural conversation. The
first child-parent interaction is free play with the
parent. The child and parent are given play doh,
Mr. Potato Head, and blocks. The second child-
parent interaction is a jenga game. Jenga is a game
in which the participants must remove blocks, one
at a time, from a tower. The game ends when the
tower falls. The third child-parent interaction is a
narrative task. The child and parent are asked to
explain stories in which they experienced a mem-
orable emotion. The final child-parent interaction
is a natural conversation without a task. These
child-parent interactions are recorded and will en-
able comparison of the child?s interaction style and
communication with their parent. Each session
continues for 10 minutes. During interaction, a pin
microphone and video camera record the speech
and video of the child and the parent.
In this paper, we use narrative data of four chil-
dren with ASD (male: 3, female: 1) and two
children with TD (male: 1, female: 1) as an ex-
ploratory study. The intelligence quotient (IQ) for
all subjects is above 70, which is often used as
a threshold for diagnosis of intellectual disabil-
ity. Each subject?s age and diagnosis as ASD/TD
is provided in Table 1. In the narrative session,
each child and parent speaks ?a memorable story?
for 5 minutes in turn, and the listener responds to
the speaker?s story by asking questions. After 5
minutes, the experimenter provides directions to
change the turn.
Table 1: Subjects? age and diagnosis
Subject A1 A2 A3 A4 T1 T2
Age 10 10 10 13 10 12
Diagnosis ASD ASD ASD ASD TD TD
In this paper, we analyze the child-speaking turn
of the narrative session in which the parent re-
sponds to the child?s utterances. All utterances are
transcribed based on USC Rachel corpus manual
(Mower et al., 2011) to facilitate comparison with
this existing corpus. In the transcription manual, if
the speaker pauses for more than one second, the
speech is transcribed as separate utterances. In this
paper, we examine two segment levels, the first
treating each speech segment independently, and
the second handling a whole narrative as the tar-
get. When handling each segment independently,
we use a total of 116 utterances for both children
with ASD and TD.
3 Single Utterance Level
In this section, we describe language and speech
features and analysis of these characteristics to-
wards automatic classification of utterances based
on whether they were spoken by children with
ASD or TD. We hypothesize that based on the fea-
tures extracted from the speech signal we are ca-
pable to classify children with ASD and TD on a
speech segment level, as well as on narrative level
after temporally combining all the segment-based
decisions.
3.1 Feature Extraction
We extract language and speech features based
on those proposed by (Mairesse et al., 2007) and
89
(Hanson, 1995). Extracted features are summa-
rized in Table 2. We also add one feature not cov-
ered in previous work counting the number of oc-
currences of laughter.
Table 2: Description of language and speech fea-
tures.
Language Features
Words per sentence (WPS)
General descriptor Words with more than 6 letters
Occurrences of laughter
Sentence structure
Percentage of pronouns, conjunctions,
negations, quantifiers, numbers
Psychological proc.
Percentage of words describing social,
affect, cognitive, perceptual,
and biological
Personal concerns
Percentage of words describing work,
achievement, leisure, and home
Paralinguistic
Percentage of assent,
disfluencies, and fillers
Speech Features
Pitch Statistics of sd and cov
Intensity Statistics of sd and cov
Speech rate Words per voiced second
Amplitude of a3
Voice quality Difference of the h1 and the h2
Difference of the h1 and the a3
3.1.1 Language Features
We use the linguistic inquiry and word count
(LIWC) (Pennebaker et al., 2007), which is a tool
to categorize words, to extract language features.
Because a Japanese version of LIWC is not avail-
able and there is no existing similar resource for
Japanese, we implement the following procedures
to automatically establish correspondences be-
tween LIWC categories and transcribed Japanese
utterances. First, we use Mecab
1
for part-of-
speech tagging in Japanese utterances, translate
each word into English using the WWWJDIC
2
dictionary, and finally determine the LIWC cate-
gory corresponding to the English word. Among
the language features described in Table 2, we
calculate sentence structures, psychological pro-
cesses, and personal concerns using LIWC, and
other features using Mecab. Here, we do not
consider language-dependent features and subcat-
egories of LIWC.
1
https://code.google.com/p/mecab/
2
http://www.edrdg.org/cgi-bin/wwwjdic/wwwjdic?1C
3.1.2 Speech Features
For speech feature extraction, we use the Snack
sound toolkit
3
. Here, we consider fundamental
frequency, power, and voice quality, which are ef-
fective features according to previous works (Mc-
Cann and Peppe, 2003; Hanson, 1995). We do
not extract mean values of fundamental frequency
and power because those features are strongly re-
lated to individuality. Thus, we extract statistics
of standard deviation (fsd, psd) and coefficient of
variation (fcov, pcov) for fundamental frequency
and power. We calculate speech rate, which is a
feature dividing the number of words by the num-
ber of voiced seconds. Voice quality is also com-
puted using: the amplitude of the third formant
(a3), the difference between the first harmonic and
the second harmonic (h1h2), and the difference
between the first harmonic and the third formant
(h1a3) (Hanson, 1995).
3.1.3 Projection Normalization
For normalization, we simply project all feature
values to a range of [0, 1], where 0 corresponds
to the smallest observed value and 1 to the largest
observed value across all utterances. For utterance
i, we define the value of the jth feature as v
ij
and
define p
ij
=
v
ij
?min
j
max
j
?min
j
, where p
ij
is the feature
value after normalisation.
3.2 Characteristics of Language and Speech
Features
In this section, we report the result of a t-test, prin-
cipal component analysis, factor analysis, and de-
cision tree using the normalised features. We use
R
4
for statistical analysis.
Table 3 shows whether utterances of children
with ASD or TD have a greater mean on the cor-
responding feature. The results indicate that the
children with ASD more frequently use words
with more than 6 letters (e.g. complicated words),
assent (e.g. ?uh-huh,? or ?un? in Japanese), and
fillers (e.g. ?umm,? or ?eh? in Japanese) signif-
icantly more than the children with TD. In con-
trast, the children with TD more frequently use the
words words categorized as social (e.g. friend), af-
fect (e.g. enjoy), and cognitive (e.g. understand)
significantly more than the children with ASD. In
addition, there are differences in terms of funda-
mental frequency variations and voice quality (e.g.
3
http://www.speech.kth.se/snack/
4
http://www.r-project.org
90
Table 3: Difference of mean values between ASD and TD based on language and speech features from
children?s utterances. Each table cell notes which of the two classes has the greater mean on the corre-
sponding feature (*: p < 0.01, **: p < 0.005).
WPS 6 let. laughter adverb pronoun conjunctions negations quantifiers numbers social
- ASD* - - - - - - - TD**
affect cognitive perceptual biological relativity work achievement leisure home assent
TD** TD* - - - - - - - ASD**
nonfluent fillers fsd fcov psd pcov speech rate a3 h1h2 h1a3
- ASD* TD** TD* - - - - - ASD**
h1a3). In particular, we observe that the children
with ASD tend to use monotonous intonation as
reported in (Kanner, 1943). We do not confirm a
significant differences in other features.
Next, we use principal component analysis and
factor analysis to find features that have a large
contribution based on large variance values. As
a result of principal component analysis, features
about fundamental frequency, power, and h1a3
have large variance in the first component, and the
feature counting perceptual words also has large
value in the second component. To analyze a dif-
ferent aspect of principal component analysis with
rotated axes, we use factor analysis with the vari-
max rotation method. Figure 1 shows the result of
factor analysis indicating that features regarding
fundamental frequency and power have large vari-
ance. In addition, other features such as speech
rate, a3, and h1a3 also have large variance. Here,
we can see that for features such as statistics of
fundamental frequency (fsd and fcov) and power
(psd and pcov), the correlation coefficient between
these features are over 80% (p < 0.01). For cor-
related features, we use only standard deviation in
the following sections.
We also analyze important features to distin-
guish between children with ASD and TD by us-
ing a decision tree. Figure 2 shows the result of a
decision tree with 10 leaves indicating that speech
features fill almost all of the leaves (e.g. fsd is a
most useful feature to distinguish between ASD
and TD). In terms of the language features, we
confirm that WPS and perceptual words are im-
portant for classification.
3.3 Classification
In this section, we examine the possibility of au-
tomatic identification of whether an utterance be-
longs to a speaker with ASD or TD. Based on
the previous analysis, we prepare the following
Figure 1: Factor analysis with varimax rotation
method. First and second factors are indicated.
feature sets: 1) language features (Language), 2)
speech features (Speech), 3) all features (All), 4)
important features according to the t-test, princi-
pal component analysis, factor analysis, and de-
cision tree (Selected), 5) important features ac-
cording to the t-test that are not highly correlated
(T-Uncor). The feature set of T-Uncor is as fol-
lows: 6 let., social, affect, cognitive, fillers, as-
sent, fed, and h1a3. We also show the chance
rate, which is a baseline of 50% because the num-
ber of utterances in each group is the same, and
measure accuracy with 10-fold cross-validation
and leave-one-speaker-out cross-validation using
naive Bayes (NB) and support vector machines
with a linear kernel (SVM). In the case of leave-
one-speaker-out cross-validation, we use T-Uncor
because the number of utterances without one
speaker is too small to train using high dimen-
sional feature sets.
Table 4 shows the result indicating that accu-
91
racies with almost all feature sets and classifiers
are over 65%. The SVM with Selected achieves
the best performance for the task of 10-fold cross-
validation, and The SVM with T-Uncor achieves
66.7% for the task of leave-one-speaker-out. The
accuracy for the task of leave-one-speaker-out on
each speaker A1 to T2 is as follows: 78%, 60%,
53%, 51%, 82%, and 78%.
Table 4: Accuracy using Naive Bayes and SVM
classifiers. The p-value of the t-test is measured
compared to baseline (chance rate) (?: p < 0.1, *:
p < 0.01)
Feature set Accuracy [%]
Baseline NB SVM
Language 62.2? 70.3*
Speech 57.6 67.6*
All 50.0 65.0? 68.8*
Selected 67.4* 71.9*
T-Uncor 67.8? 68.1?
Per-Speaker 50.0 65.5? 66.7?
4 Narrative Level
In this section, we focus on the features of en-
tire narratives, which allows us to examine other
features of child-parent interaction for a better un-
derstanding of ASD and classification in children
with ASD and TD. Each following subsection de-
scribes the procedure of feature extraction and
analysis of characteristics at the narrative level.
We consider pauses before new turns and unex-
pected words, which are mentioned in previous
works, as well as words per minute.
4.1 Pauses Before New Turns
Heeman et al., (2010) reported that children with
ASD tend to delay responses to their parent more
than children with TD in natural conversation. In
this paper, we examine whether a similar result is
found in interactive narrative. We denote values
of pauses before new turns as time between the
end of the parent?s utterance and the start of the
child?s utterance. We do not consider overlap of
utterances. We test goodness of fit of pauses to a
gamma and an exponential distribution based on
(Theodora et al., 2013), because the later is a spe-
cial case of gamma with a unity shape parameter,
using the Kolmogorov-Smirnov test.
Figure 3 shows a fitting of pauses to gamma
or exponential distributions, and we select a bet-
2 4 6 8 10
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
Pauses before new turns (sec)
E
x
p
o
n
e
n
t
i
a
l
/
G
a
m
m
a
 
p
r
o
b
a
b
i
l
i
t
y
 
v
a
l
u
e
s
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
TD
ASD
Figure 3: Gamma/Exponential pause distributions
with parameters computed using Maximum Like-
lihood Estimation (MLE) for children with ASD
and TD.
ter fitted distribution. All subjects significantly fit
(p > 0.6). As shown in Figure 3, we confirm that
children with ASD tend to delay responses to their
parent compared with children with TD. To reflect
this information in our following experiments in
automatic identification of ASD in narrative, we
extract the expectation value of the exponential
distribution
Heeman et al., (2010) also reported the rela-
tionship of the parent?s previous utterance?s type
(question or non-question) and the child?s pauses.
We examine the relationship between the parent?s
previous question?s type and pauses before new
turns. For each of the children?s utterances, we
label the parent?s utterance that directly precedes
as either ?open question,? ?closed question,? or
?non-question?, and we calculate pause latency.
Closed-questions are those which can be answered
by a simple ?yes? or ?no,? while open-questions
are those which require more thought and more
than a simple one-word answer. As shown in Table
5, children with ASD tend to delay responses to
their parent to a greater extent than children with
TD. We found no difference between open and
closed questions, although a difference between
questions and non-questions is observed. These
results are consistent with those of previous work
(Heeman et al., 2010) in terms of differences be-
tween questions and non-questions.
92
|fsd < 0.366375
fcov < 0.308899
WPS < 0.0543478
fcov < 0.204553
psd < 0.306304
pcov < 0.46429
fsd < 0.513756
perceptual < 0.07
pcov < 0.230634
a
t
t
a a
t
a
t
a
a
Figure 2: Decision tree with 10 leaves (a: ASD, t: TD).
Table 5: Relationship of pauses before new turns
and parents? question types. The mean value and
standard deviation are shown.
Question type TD ASD
Closed-question 0.47 (0.46) 1.61 (1.87)
Open-question 0.43 (0.34) 1.76 (1.51)
Non-question 0.95 (1.18) 2.60 (3.64)
4.2 Words Per Minute
We analyze words per minute (WPM) in children
with ASD and TD to clarify the relationship be-
tween ASD and frequency of speech. We use a
total of 5 minutes of data in each narrative, and
thus the total number of words are divided by 5 to
calculate WPM. Table 6 shows the result. The data
in this table indicates that some children with ASD
have a significantly lower speaking rate than oth-
ers with TD, but it is not necessarily the case that
ASD will result in a low speaking rate such as the
case of Asperger?s syndrome (Asperger, 1944).
4.3 Unexpected Words
Characteristics of ASD include deficits in social
communication, and these deficits affect inappro-
Table 6: Mean value of words per minute.
Subj. Averaged WPM
A1 18.25
A2 86.75
A3 23.75
A4 115.5
T1 99.25
T2 103.5
priate usage of words (Rouhizadeh et al., 2013).
We evaluate these unexpected words using two
measures, term frequency-inverse document fre-
quency (TF-IDF) and log odds ratio. We use
the following formulation to calculate TF-IDF for
each child?s narrative i and each word in that nar-
rative j, where c
ij
is the count of word j in narra-
tive i. f
j
is the number of narratives from the full
data of child narratives containing that word j, and
D is the total number of narratives (Rouhizadeh et
al., 2013).
tf ? idf
ij
= (1 + log c
ij
) log
D
f
j
The log odds ratio, another measure used in in-
93
formation retrieval and extraction tasks, is the ratio
between the odds of a particular word, j, appear-
ing in a child?s narrative, i. Letting the probabil-
ity of a word appearing in a narrative be p
1
and
the probability of that word appearing in all other
narratives be p
2
, we can express the odds ratio as
follows:
odds ratio =
odds(p
1
)
odds(p
2
)
=
p
1
/(1? p
1
)
p
2
/(1? p
2
)
A large TF-IDF and log odds score indicates
that the word j is very specific to the narrative
i, which in turn suggests that the word might be
unexpected or inappropriate. In addition, because
the overall amount of data included in the narra-
tives is too small to robustly analyze these statis-
tics for all words, we also check for the presence
of each word in Japanese WordNet
5
and deter-
mine that if it exists in WordNet it is likely a com-
mon (expected) word. Table 7 shows the result
of TF-IDF, log odds ratio, and their summation,
and we confirm that there is no difference between
children with ASD and TD. This result is differ-
ent from that of previous work (Rouhizadeh et al.,
2013). The children in that study were all telling
the same story, and one possible explanation for
this is due to the fact that in this work we do
not use language-constricted data such as narrative
retelling, and thus differences due to individuality
are more prevalent.
Table 7: TF-IDF, log odds ratio, and their summa-
tion.
Subj. TF-IDF Log-odds T+L
A1 0.50 1.01 1.52
A2 0.58 0.49 1.08
A3 0.66 1.23 1.89
A4 0.66 0.31 0.96
T1 0.74 0.49 1.23
T2 0.62 0.44 1.06
4.4 Classification
In this section, we examine the possibility of auto-
matic classification of whether an interactive nar-
rative belongs to children with ASD or TD. Be-
cause of the total number of subjects is small (n=4
for ASD, n=2 for TD), we perform classification
5
http://www.omomimi.com/wnjpn/
with a K-NN classifier with K=1 nearest neigh-
bour. As features, we compute the features men-
tioned in Section 3.1, and use the average over all
utterances as the features for the entire narrative.
Finally, we use pauses before new turns (expecta-
tion value of the exponential distribution), WPM,
TF-IDF, log odds ratio, 6 let., social, affect, cogni-
tive, assent, fillers, fsd, h1a3, and calculate accu-
racy with leave-one-speaker-out cross-validation.
As a result, we achieved an accuracy of 100%
in classification between ASD and TD on the full-
narrative level, which shows that these features
are effective to some extent to distinguish children
with ASD and TD. However, with only a total of 6
children, our sample size is somewhat small, and
thus experiments with a larger data set will be nec-
essary to draw more firm conclusions.
5 Data Comparison
As all our preceding experiments have been per-
formed on data for Japanese child-parent pairs, it
is also of interest to compare these results with
data of children and parents from other cultures.
In particular, we refer to the USC Rachel corpus
(Mower et al., 2011) (the subjects are nine chil-
dren with ASD) for comparison. Using the USC
Rachel corpus, there is a report mentioning the re-
lationship of parent?s and child?s linguistic infor-
mation and pauses before new turns (Theodora et
al., 2013). In this paper, we follow this work us-
ing Japanese data. The USC Rachel corpus in-
cludes a session of child-parent interaction, and
the same transcription standard is used. We ex-
tract pauses before new turns, and short and long
pauses are differentiated based on the 70th per-
centile of latency values for each child individu-
ally. We investigate the relationship between the
parent and child?s language information based on
features used in Section 3.1, and short and long
pauses.
Table 8 and 9 show significantly greater mean
values performed using bootstrap significance
testing on the means of the two pause types. By
observing the values in the table, we can see
that the trends are similar for both American and
Japanese children. However, in terms of WPS,
there is a difference. The American ASD chil-
dren have greater means for WPS in the case of
long pauses, while Japanese children have greater
means for WPS in the case of short pauses. We
analyze these differences in detail.
94
Table 8: In the case of USC Rachel corpus, boot-
strap on difference of means between short (S) and
long (L) pauses based on linguistic features from
child?s and parent?s utterances (?: p < 0.1, *: p <
0.01). Each table cell notes which of the two types
of pauses has greater mean on the corresponding
feature.
Subj.
Child Parent
WPS conj. affect nonflu. adverb cogn. percept.
S1 L* L* S* - L* L* L*
S2 L* L* S? L* L* L* L*
S3 L* L? - S? L* L* L*
S4 - - - L* L* L* L*
S5 L? - - - L* L* L*
S6 L* - S* - L* L* -
S7 L? - S? - L? - -
S8 L* - - - L* L* L*
S9 - - - S? L* L* L*
Table 9: Bootstrap for pause differences in the
Japanese corpus.
Subj.
Child Parent
WPS conj. affect nonflu. adverb cogn. percept.
A1 S* - - - S* L* -
A2 S? - S* - L* L* L*
A3 S? - - - L* L* L*
A4 S* - - - - - -
In the Japanese corpus, we observe that WPS is
larger in the case of short pauses. As we noticed
that the child often utters only a single word for
responses that follow a long pause, we analyzed
the content of these single word utterances. As
shown in Figure 4, for example, A1 tends to use
a word related to assent when latency is long, and
A4 tends to use a word related to filler, assent or
others when latency is long. Though there are in-
dividual differences, we confirm that the Japanese
children with ASD examined in this study tend
to delay their responses before uttering one word.
These characteristics may be related to the parent?s
question types and the child?s cognitive process,
and thus we need to examine these possibilities in
detail.
6 Conclusion
In this work, we focused on differentiation of chil-
dren with ASD and TD in terms of social com-
munication, particularly focusing on language and
speech features. Using narrative data, we exam-
ined several features on both the single utterance
A1 A2 A3 A4
Others
Laugh
Filler
Assent
Subject
P
e
r
c
e
n
t
a
g
e
 
o
f
 
o
n
e
?
w
o
r
d
 
r
e
s
p
o
n
c
e
s
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
Figure 4: The language category of one-word re-
sponses in the case of a long pause.
level and the narrative level. We examined fea-
tures mentioned in a number of previous works, as
well as a few novel features. We confirmed about
70% accuracy in an evaluation over single utter-
ances, and some narrative features also proved to
have a correlation with ASD.
For future directions, we plan to perform larger
scale experiments to examine the potential of these
features for automated ASD screening. Given the
results of this, we plan to move to applications in-
cluding the development of dialogue systems for
automatic ASD screening and social skills train-
ing.
Acknowledgments
We would like to thank the participants, children
and their parents, in this study. We also thank
Dr. Hidemi Iwasaka for his advice and support as
clinician in pediatrics. A part of this study was
conducted in Signal Analysis and Interpretation
Laboratory (SAIL), University of Southern Cali-
fornia. This study is supported by JSPS KAKEN
24240032.
References
American Psychiatric Association. 2013. The Diag-
nostic and Statistical Manual of Mental Disorders:
DSM 5.
Asgari, Meysam, Alireza Bayestehtashk, and Izhak
Shafran. 2013. Robust and Accurate Featuers for
Detecting and Diagnosing Autism Spectrum Disor-
ders. Proceedings of Interspeech, 191?194.
95
Asperger, H.. 1944. Die ,,Autistischen Psychopathen?
im Kindesalter. European Archives of Psychiatry
and Clinical Neuroscience, 117: 76?136.
Bone, D., Black, M. P., Lee, C. C., Williams, M.
E., Levitt, P., Lee, S., and Narayanan, S.. 2012.
Spontaneous-Speech Acoustic-Prosodic Features of
Children with Autism and the Interacting Psycholo-
gist. Proceedings of Interspeech.
Chaspari, T., Gibson, D. B., Lee, C.-C., and Narayanan,
S. S. 2013. Using physiology and language cues for
modeling verbal response latencies of children with
ASD. Proceedings of ICASSP, 3702?3706.
Davis, Megan, Kerstin Dautenhahn, CL Nehaniv, and
SD Powell. 2004. Towards an Interactive Sys-
tem Facilitating Therapeutic Narrative Elicitation in
Autism. Proceedings of NILE.
Dawson, Geraldine, Deborah Hill, Art Spencer, Larry
Galpert, and Linda Watson.. 1990. Affective ex-
changes between young autistic children and their
mothers. Journal of Abnormal Child Psychology,
18: 335?345.
de Marchena, A. and Inge-Marie E.. 2010. Conversa-
tional gestures in autism spectrum disorders: asyn-
chrony but not decreased frequency. Autism Re-
search, 3: 311?322.
Hanson M. H.. 1995. Glottal characteristics of female
speakers. Harvard University, Ph.D. dissertation.
Heeman, P. A., Lunsford, R., Selfridge, E., Black, L.,
and Van Santen, J.. 2010. Autism and interactional
aspects of dialogue. Proceedings of SIGDIAL, 249?
252.
Kanner, L.. 1943. Autistic disturbances of affective
contact. Nervous Child, 2: 217?250.
Kiss, G. and van Santen, J. P. H.. 2013. Estimating
Speaker-Specific Intonation Patterns Using the Lin-
ear Alignment Model. Proceedings of Interspeech
354?358.
Kiss, G., van Santen, J. P. H., Prud?hommeaux, E. T.,
and Black, L. M.. 2012. Quantitative Analysis of
Pitch in Speech of Children with Neurodevelopmen-
tal Disorders. Proceedings of Interspeech.
Lovaas, O Ivar, Robert Koegel, James Q Simmons, and
Judith Stevens Long. 1973. Some generalisation
and follow-up measures on autistic children in be-
haviour therapy. Journal of Applied Behavior Anal-
ysis, 6: 131?166.
Mairesse, Francois, Marilyn A Walker, Matthias R
Mehl, and Roger K Moore. 2007. Using Linguis-
tic cues for the automatic recognition of personality
in conversation and text. Journal of Artificial Intel-
ligence Research, 30: 457?500.
McCann, J. and Sue, P.. 2003. Prosody in autism
spectrum disorders: a critical review. International
Journal of Language & Communication Disorders,
38(4): 325?350.
Mower, E., Black, M. P., Flores, E., Williams, M., and
Narayanan, S.. 2011. Rachel: Design of an emo-
tionally targeted interactive agent for children with
autism. Proceedings of IEEE ICME, 1?6.
Newton, A. T., Kramer, A. D. I., and McIntosh, D. N..
2009. Autism online: a comparison of word usage
in bloggers with and without autism spectrum disor-
ders. Proceedings of SIGCHI, 463?466.
Paul, Rhea, Amy Augustyn, Ami Klin, and Fred R
Volkmar. 2005. Perception and production of
prosody by speakers with autism spectrum disor-
ders. Journal of Autism and Developmental Disor-
ders, 35: 205?220.
Pennebaker, James W, Martha E Francis, and Roger J
Booth. 2005. Linguistic inquiry and word count:
LIWC [Computer software] Austin, TX: liwc. net.
Rouhizadeh Masoud, Prud?hommeaux Emily, Roark
Brian, and van Santen Jan. 2013. Distributional se-
mantic models for the evaluation of disordered lan-
guage. Proceedings of NAACL-HLT, 709?714.
Santen, Jan PH, Richard W Sproat, and Alison Pres-
manes Hill. 2013. Quantifying repetitive speech
in autism spectrum disorders and language impair-
ment. Autism Research, 6: 372?383.
Sharda, Megha, T Padma Subhadra, Sanchita Sahay,
Chetan Nagaraja, Latika Singh, Ramesh Mishra,
Amit Sen, Nidhi Singhal, Donna Erickson, and Nan-
dini C Singh. 2010. Sounds of melody?Pitch pat-
terns of speech in autism. Neuroscience letters, 478:
42?45.
Van Santen, Jan PH, Emily T Prud?hommeaux, Lois
M Black, and Margaret Mitchell. 2010. Compu-
tational prosodic markers for autism. Autism, 14:
215?236.
Wallace, Charles J, Connie J Nelson, Robert Paul
Liberman, Robert A Aitchison, David Lukoff, John
P Elder, and Chris Ferris. 1980. A review and cri-
tique of social skills training with schizophrenic pa-
tients. Schizophrenia Bulletin, 6:42?63.
96
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34?42,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Rule-based Syntactic Preprocessing
for Syntax-based Machine Translation
Yuto Hatakoshi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
Nara Institute of Science and Technology
Graduate School of Information Science
Takayama, Ikoma, Nara 630-0192, Japan
{hatakoshi.yuto.hq8,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
Abstract
Several preprocessing techniques using
syntactic information and linguistically
motivated rules have been proposed to im-
prove the quality of phrase-based machine
translation (PBMT) output. On the other
hand, there has been little work on similar
techniques in the context of other trans-
lation formalisms such as syntax-based
SMT. In this paper, we examine whether
the sort of rule-based syntactic preprocess-
ing approaches that have proved beneficial
for PBMT can contribute to syntax-based
SMT. Specifically, we tailor a highly suc-
cessful preprocessing method for English-
Japanese PBMT to syntax-based SMT,
and find that while the gains achievable are
smaller than those for PBMT, significant
improvements in accuracy can be realized.
1 Introduction
In the widely-studied framework of phrase-based
machine translation (PBMT) (Koehn et al., 2003),
translation probabilities between phrases consist-
ing of multiple words are calculated, and trans-
lated phrases are rearranged by the reordering
model in the appropriate target language order.
While PBMT provides a light-weight framework
to learn translation models and achieves high
translation quality in many language pairs, it does
not directly incorporate morphological or syntac-
tic information. Thus, many preprocessing meth-
ods for PBMT using these types of information
have been proposed. Methods include preprocess-
ing to obtain accurate word alignments by the divi-
sion of the prefix of verbs (Nie?en and Ney, 2000),
preprocessing to reduce the errors in verb conju-
gation and noun case agreement (Avramidis and
Koehn, 2008), and many others. The effectiveness
of the syntactic preprocessing for PBMT has been
supported by these and various related works.
In particular, much attention has been paid to
preordering (Xia and McCord, 2004; Collins et
al., 2005), a class of preprocessing methods for
PBMT. PBMT has well-known problems with lan-
guage pairs that have very different word order,
due to the fact that the reordering model has dif-
ficulty estimating the probability of long distance
reorderings. Therefore, preordering methods at-
tempt to improve the translation quality of PBMT
by rearranging source language sentences into an
order closer to that of the target language. It?s of-
ten the case that preordering methods are based
on rule-based approaches, and these methods have
achieved great success in ameliorating the word
ordering problems faced by PBMT (Collins et al.,
2005; Xu et al., 2009; Isozaki et al., 2010b).
One particularly successful example of rule-
based syntactic preprocessing is Head Finalization
(Isozaki et al., 2010b), a method of syntactic pre-
processing for English to Japanese translation that
has significantly improved translation quality of
English-Japanese PBMT using simple rules based
on the syntactic structure of the two languages.
The most central part of the method, as indicated
by its name, is a reordering rule that moves the
English head word to the end of the corresponding
syntactic constituents to match the head-final syn-
tactic structure of Japanese sentences. Head Final-
ization also contains some additional preprocess-
ing steps such as determiner elimination, parti-
cle insertion and singularization to generate a sen-
tence that is closer to Japanese grammatical struc-
ture.
In addition to PBMT, there has also recently
been interest in syntax-based SMT (Yamada and
Knight, 2001; Liu et al., 2006), which translates
using syntactic information. However, few at-
tempts have been made at syntactic preprocessing
for syntax-based SMT, as the syntactic informa-
tion given by the parser is already incorporated
directly in the translation model. Notable excep-
34
tions include methods to perform tree transforma-
tions improving correspondence between the sen-
tence structure and word alignment (Burkett and
Klein, 2012), methods for binarizing parse trees to
match word alignments (Zhang et al., 2006), and
methods for adjusting label sets to be more ap-
propriate for syntax-based SMT (Hanneman and
Lavie, 2011; Tamura et al., 2013). It should be
noted that these methods of syntactic preprocess-
ing for syntax-based SMT are all based on auto-
matically learned rules, and there has been little in-
vestigation of the manually-created linguistically-
motivated rules that have proved useful in prepro-
cessing for PBMT.
In this paper, we examine whether rule-based
syntactic preprocessing methods designed for
PBMT can contribute anything to syntax-based
machine translation. Specifically, we examine
whether the reordering and lexical processing of
Head Finalization contributes to the improvement
of syntax-based machine translation as it did for
PBMT. Additionally, we examine whether it is
possible to incorporate the intuitions behind the
Head Finalization reordering rules as soft con-
straints by incorporating them as a decoder fea-
ture. As a result of our experiments, we demon-
strate that rule-based lexical processing can con-
tribute to improvement of translation quality of
syntax-based machine translation.
2 Head Finalization
Head Finalization is a syntactic preprocessing
method for English to Japanese PBMT, reducing
grammatical errors through reordering and lexi-
cal processing. Isozaki et al. (2010b) have re-
ported that translation quality of English-Japanese
PBMT is significantly improved using a transla-
tion model learned by English sentences prepro-
cessed by Head Finalization and Japanese sen-
tences. In fact, this method achieved the highest
results in the large scale NTCIR 2011 evaluation
(Sudoh et al., 2011), the first time a statistical ma-
chine translation (SMT) surpassed rule-based sys-
tems for this very difficult language pair, demon-
strating the utility of these simple syntactic trans-
formations from the point of view of PBMT.
2.1 Reordering
The reordering process of Head Finalization uses
a simple rule based on the features of Japanese
grammar. To convert English sentence into
John hit a ball
John hita ball
NN VBD DT NN
NP
VP
NP
S
VBD
VP
NP
S
DT NN
NP
NN
Original English
Head Final English
Add Japanese Particles
John hita ballva0 va2
Singularize, 
Eliminate Determiners
John hita ballva0 va2
Reordering
Figure 1: Head Finalization
Japanese word order, the English sentence is first
parsed using a syntactic parser, and then head
words are moved to the end of the corresponding
syntactic constituents in each non-terminal node
of the English syntax tree. This helps replicate
the ordering of words in Japanese grammar, where
syntactic head words come after non-head (depen-
dent) words.
Figure 1 shows an example of the application
of Head Finalization to an English sentence. The
head node of the English syntax tree is connected
to the parent node by a bold line. When this node
is the first child node, we move it behind the de-
pendent node in order to convert the English sen-
tence into head final order. In this case, moving
the head node VBD of black node VP to the end of
this node, we can obtain the sentence ?John a ball
hit? which is in a word order similar to Japanese.
2.2 Lexical Processing
In addition to reordering, Head Finalization con-
ducts the following three steps that do not affect
word order. These steps do not change the word
35
ordering, but still result in an improvement of
translation quality, and it can be assumed that the
effect of this variety of syntactic preprocessing is
not only applicable to PBMT but also other trans-
lation methods that do not share PBMT?s problems
of reordering such as syntax-based SMT. The three
steps included are as follows:
1. Pseudo-particle insertion
2. Determiner (?a?, ?an?, ?the?) elimination
3. Singularization
The motivation for the first step is that in con-
trast to English, which has relatively rigid word
order and marks grammatical cases of many noun
phrases according to their position relative to the
verb, Japanese marks the topic, subject, and object
using case marking particles. As Japanese parti-
cles are not found in English, Head Finalization
inserts ?pseudo-particles? to prevent a mistransla-
tion or lack of particles in the translation process.
In the pseudo-particle insertion process (1), we in-
sert the following three types of pseudo-particles
equivalent to Japanese case markers ?wa? (topic),
?ga? (subject) or ?wo? (object).
? va0: Subject particle of the main verb
? va1: Subject particle of other verbs
? va2: Object particle of any verb
In the example of Figure 1, we insert the topic par-
ticle va0 behind of ?John?, which is a subject of a
verb ?hit? and object particle va2 at the back of
object ?ball.?
Another source of divergence between the two
languages stems from the fact that Japanese does
not contain determiners or makes distinctions be-
tween singular and plural by inflection of nouns.
Thus, to generate a sentence that is closer to
Japanese, Head Finalization eliminates determin-
ers (2) and singularizes plural nouns (3) in addi-
tion to the pseudo-particle insertion.
In Figure 1, we can see that applying these
three processes to the source English sentence re-
sults in the sentence ?John va0 (wa) ball va2 (wo)
hit? which closely resembles the structure of the
Japanese translation ?jon wa bo-ru wo utta.?
3 Syntax-based Statistical Machine
Translation
Syntax-based SMT is a method for statistical
translation using syntactic information of the sen-
tence (Yamada and Knight, 2001; Liu et al., 2006).
By using translation patterns following the struc-
ture of linguistic syntax trees, syntax-based trans-
lations often makes it possible to achieve more
grammatical translations and reorderings com-
pared with PBMT. In this section, we describe
tree-to-string (T2S) machine translation based on
synchronous tree substitution grammars (STSG)
(Graehl et al., 2008), the variety of syntax-based
SMT that we use in our experiments.
T2S captures the syntactic relationship between
two languages by using the syntactic structure of
parsing results of the source sentence. Each trans-
lation pattern is expressed as a source sentence
subtree using rules including variables. The fol-
lowing example of a translation pattern include
two noun phrases NP
0
and NP
1
, which are trans-
lated and inserted into the target placeholders X
0
and X
1
respectively. The decoder generates the
translated sentence in consideration of the proba-
bility of translation pattern itself and translations
of the subtrees of NP
0
and NP
1
.
S((NP
0
) (VP(VBD hit) (NP
1
)))
? X
0
wa X
1
wo utta
T2S has several advantages over PBMT. First,
because the space of translation candidates is re-
duced using the source sentence subtree, it is often
possible to generate translations that are more ac-
curate, particularly with regards to long-distance
reordering, as long as the source parse is correct.
Second, the time to generate translation results is
also reduced because the search space is smaller
than PBMT. On the other hand, because T2S gen-
erates translation results using the result of auto-
matic parsing, translation quality highly depends
on the accuracy of the parser.
4 Applying Syntactic Preprocessing to
Syntax-based Machine Translation
In this section, we describe our proposed method
to apply Head Finalization to T2S translation.
Specifically, we examine two methods for incor-
porating the Head Finalization rules into syntax-
based SMT: through applying them as preprocess-
ing step to the trees used in T2S translation, and
36
through adding reordering information as a feature
of the translation patterns.
4.1 Syntactic Preprocessing for T2S
We applied the two types of processing shown in
Table 1 as preprocessing for T2S. This is similar
to preprocessing for PBMTwith the exception that
preprocessing for PBMT results in a transformed
string, and preprocessing for T2S results in a trans-
formed tree. In the following sections, we elabo-
rate on methods for applying these preprocessing
steps to T2S and some effects expected therefrom.
Table 1: Syntactic preprocessing applied to T2S
Preprocessing Description
Reordering Reordering based on Japanese
typical head-final grammatical
structure
Lexical Processing Pseudo-particle insertion, deter-
miner elimination, singulariza-
tion
4.1.1 Reordering for T2S
In the case of PBMT, reordering is used to change
the source sentence word order to be closer to
that of the target, reducing the burden on the rel-
atively weak PBMT reordering models. On the
other hand, because translation patterns of T2S
are expressed by using source sentence subtrees,
the effect of reordering problems are relatively
small, and the majority of reordering rules spec-
ified by hand can be automatically learned in a
well-trained T2S model. Therefore, preordering
is not expected to cause large gains, unlike in the
case of PBMT.
However, it can also be thought that preordering
can still have a positive influence on the translation
model training process, particularly by increasing
alignment accuracy. For example, training meth-
ods for word alignment such as the IBM or HMM
models (Och and Ney, 2003) are affected by word
order, and word alignment may be improved by
moving word order closer between the two lan-
guages. As alignment accuracy plays a important
role in T2S translation (Neubig and Duh, 2014), it
is reasonable to hypothesize that reordering may
also have a positive effect on T2S. In terms of the
actual incorporation with the T2S system, we sim-
ply follow the process in Figure 1, but output the
reordered tree instead of only the reordered termi-
nal nodes as is done for PBMT.
John hit a ball
NN VBD DT NN
NP
VP
NP
S
Original English
NN VBD NN VA
NP
VP
NP
S
VA
John hit ball va2va0
Lexical Processing
Figure 2: A method of applying Lexical Process-
ing
4.1.2 Lexical Processing for T2S
In comparison to reordering, Lexical Processing
may be expected to have a larger effect on T2S,
as it will both have the potential to increase align-
ment accuracy, and remove the burden of learning
rules to perform simple systematic changes that
can be written by hand. Figure 2 shows an ex-
ample of the application of Lexical Processing to
transform not strings, but trees.
In the pseudo-particle insertion component,
three pseudo particles ?va0,? ?va1,? and ?va2? (as
shown in Section 2.2) are added in the source En-
glish syntax tree as terminal nodes with the non-
terminal node ?VA?. As illustrated in Figure 2, par-
ticles are inserted as children at the end of the cor-
responding NP node. For example, in the figure
the topic particle ?va0? is inserted after ?John,?
subject of the verb ?hit,? and the object particle
?va2? is inserted at the end of the NP for ?ball,?
the object.
In the determiner elimination process, terminal
nodes ?a,? ?an,? and ?the? are eliminated along
with non-terminal node DT. Determiner ?a? and
its corresponding non-terminal DT are eliminated
in the Figure 2 example.
Singularization, like in the processing for
PBMT, simply changes plural noun terminals to
their base form.
4.2 Reordering Information as Soft
Constraints
As described in section 4.1.1, T2S work well on
language pairs that have very different word order,
but is sensitive to alignment accuracy. On the other
hand, we know that in most cases Japanese word
order tends to be head final, and thus any rules that
do not obey head final order may be the result of
bad alignments. On the other hand, there are some
cases where head final word order is not applica-
ble (such as sentences that contain the determiner
37
?no,? or situations where non-literal translations
are necessary) and a hard constraint to obey head-
final word order could be detrimental.
In order to incorporate this intuition, we add
a feature (HF-feature) to translation patterns that
conform to the reordering rules of Head Final-
ization. This gives the decoder ability to discern
translation patterns that follow the canonical re-
ordering patterns in English-Japanese translation,
and has the potential to improve translation quality
in the T2S translation model.
We use the log-linear approach (Och, 2003) to
add the Head Finalization feature (HF-feature). As
in the standard log-linear model, a source sen-
tence f is translated into a target language sen-
tence e, by searching for the sentence maximizing
the score:
?
e = arg max
e
w
T
? h(f ,e). (1)
where h(f , e) is a feature function vector. w is
a weight vector that scales the contribution from
each feature. Each feature can take any real value
which is useful to improve translation quality, such
as the log of the n-gram language model proba-
bility to represent fluency, or lexical/phrase trans-
lation probability to capture the word or phrase-
wise correspondence. Thus, if we can incorporate
the information about reordering expressed by the
Head Finalization reordering rule as a features in
this model, we can learn weights to inform the de-
coder that it should generally follow this canonical
ordering.
Figure 3 shows a procedure of Head Finaliza-
tion feature (HF-feature) addition. To add the
HF-feature to translation patterns, we examine
the translation rules, along with the alignments
between target and source terminals and non-
terminals. First, we apply the Reordering to the
source side of the translation pattern subtree ac-
cording to the canonical head-final reordering rule.
Second, we examine whether the word order of the
reordered translation pattern matches with that of
the target translation pattern for which the word
alignment is non-crossing, indicating that the tar-
get string is also in head-final word order. Finally,
we set a binary feature (h
HF
(f , e) = 1) if the tar-
get word order obeys the head final order. This
feature is only applied to translation patterns for
which the number of target side words is greater
than or equal to two.
VP
VBD NP
hit x0:NP
x0 wo
Source side of
translation pattern
Target side of
translation pattern
VP
NP VBD
hitx0:NP
1. Apply Reordering to 
source translation pattern
2. Add HF-feature
if word alignment is 
non-crossing
utta
Word alignment 
x0 woTarget side of
translation pattern
utta
Reordered
translation pattern
Figure 3: Procedure of HF-feature addition
Table 2: The details of NTCIR7
Dataset Lang Words Sentences
Average
length
train
En 99.0M 3.08M 32.13
Ja 117M 3.08M 37.99
dev
En 28.6k 0.82k 34.83
Ja 33.5k 0.82k 40.77
test
En 44.3k 1.38k 32.11
Ja 52.4k 1.38k 37.99
5 Experiment
In our experiment, we examined how much each
of the preprocessing steps (Reordering, Lexical
Processing) contribute to improve the translation
quality of PBMT and T2S. We also examined the
improvement in translation quality of T2S by the
introduction of the Head Finalization feature.
5.1 Experimental Environment
For our English to Japanese translation experi-
ments, we used NTCIR7 PATENT-MT?s Patent
corpus (Fujii et al., 2008). Table 2 shows the
details of training data (train), development data
(dev), and test data (test).
As the PBMT and T2S engines, we used the
Moses (Koehn et al., 2007) and Travatar (Neubig,
2013) translation toolkits with the default settings.
38
Enju (Miyao and Tsujii, 2002) is used to parse En-
glish sentences and KyTea (Neubig et al., 2011) is
used as a Japanese tokenizer. We generated word
alignments using GIZA++ (Och and Ney, 2003)
and trained a Kneser-Ney smoothed 5-gram LM
using SRILM (Stolcke et al., 2011). Minimum
Error Rate Training (MERT) (Och, 2003) is used
for tuning to optimize BLEU. MERT is replicated
three times to provide performance stability on test
set evaluation (Clark et al., 2011).
We used BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010a) as evaluation mea-
sures of translation quality. RIBES is an eval-
uation method that focuses on word reordering
information, and is known to have high correla-
tion with human judgement for language pairs that
have very different word order such as English-
Japanese.
5.2 Result
Table 3 shows translation quality for each com-
bination of HF-feature, Reordering, and Lexical
Processing. Scores in boldface indicate no sig-
nificant difference in comparison with the con-
dition that has highest translation quality using
the bootstrap resampling method (Koehn, 2004)
(p < 0.05).
For PBMT, we can see that reordering plays an
extremely important role, with the highest BLEU
and RIBES scores being achieved when using Re-
ordering preprocessing (line 3, 4). Lexical Pro-
cessing also provided a slight performance gain
for PBMT.When we applied Lexical Processing to
PBMT, BLEU and RIBES scores were improved
(line 1 vs 2), although this gain was not significant
when Reordering was performed as well.
Overall T2S without any preprocessing
achieved better translation quality than all con-
ditions of PBMT (line 1 of T2S vs line 1-4 of
PBMT). In addition, BLEU and RIBES score of
T2S were clearly improved by Lexical Processing
(line 2, 4, 6, 8 vs line 1, 3, 5, 7), and these scores
are the highest of all conditions. On the other
hand, Reordering and HF-Feature addition had no
positive effect, and actually tended to slightly hurt
translation accuracy.
5.3 Analysis of Preprocessing
With regards to PBMT, as previous works on
preordering have already indicated, BLEU and
RIBES scores were significantly improved by Re-
ordering. In addition, Lexical Processing also con-
Table 5: Optimized weight of HF-feature in each
condition
HF-feature Reordering
Word Weight of
Processing HF-feature
+ - - -0.00707078
+ - + 0.00524676
+ + - 0.156724
+ + + -0.121326
tributed to improve translation quality of PBMT
slightly. We also investigated the influence
that each element of Lexical Processing (pseudo-
particle insertion, determiner elimination, singu-
larization) had on translation quality, and found
that the gains were mainly provided by particle
insertion, with little effect from determiner elim-
ination or singularization.
Although Reordering was effective for PBMT,
it did not provide any benefit for T2S. This in-
dicates that T2S can already conduct long dis-
tance word reordering relatively correctly, and
word alignment quality was not improved as much
as expected by closing the gap in word order be-
tween the two languages. This was verified by a
subjective evaluation of the data, finding very few
major reordering issues in the sentences translated
by T2S.
On the other hand, Lexical Processing func-
tioned effectively for not only PBMT but also T2S.
When added to the baseline, lexical processing on
its own resulted in a gain of 0.57 BLEU, and 0.99
RIBES points, a significant improvement, with
similar gains being seen in other settings as well.
Table 4 demonstrates a typical example of the
improvement of the translation result due to Lex-
ical Processing. It can be seen that translation
performance of particles (indicated by underlined
words) was improved. The underlined particle is
in the direct object position of the verb that corre-
sponds to ?comprises? in English, and thus should
be given the object particle ?? wo? as in the refer-
ence and the system using Lexical Processing. On
the other hand, in the baseline system the genitive
?? to? is generated instead due to misaligned par-
ticles being inserted in an incorrect position in the
translation rules.
5.4 Analysis of Feature Addition
Our experimental results indicated that translation
quality is not improved by HF-feature addition
(line 1-4 vs line 5-8). We conjecture that the rea-
son why HF-feature did not contribute to an im-
39
Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold
indicates results that are not statistically significantly different from the best result (39.60 BLEU in line
4 and 79.47 RIBES in line 2).
ID
PBMT T2S
HF-feature Reordering Lexical Processing BLEU RIBES BLEU RIBES
1 - - - 32.11 69.06 38.94 78.48
2 - - + 33.16 70.19 39.51 79.47
3 - + - 37.62 77.56 38.44 78.48
4 - + + 37.77 77.71 39.60 79.26
5 + - - ? ? 38.74 78.33
6 + - + ? ? 39.29 79.23
7 + + - ? ? 38.48 78.44
8 + + + ? ? 39.38 79.21
Table 4: Improvement of translation results due to Lexical Processing
Source another connector 96 , which is matable with this cable connector 90 , comprises a plurality of
male contacts 98 aligned in a row in an electrically insulative housing 97 as shown in the figure .
Reference ????????????????????????????????????
?????????????????????????????????
- Lexical Processing ???????????????????????????????????
???????????????????????????????????
???
+ Lexical Processing ???????????????????????????????????
???????????????????????????????????
??
provement in translation quality is that the reorder-
ing quality achieved by T2S translation was al-
ready sufficiently high, and the initial feature led
to confusion in MERT optimization.
Table 5 shows the optimized weight of the HF
feature in each condition. From this table, we can
see that in two of the conditions positive weights
are learned, and in two of the conditions negative
weights are learned. This indicates that there is no
consistent pattern of learning weights that corre-
spond to our intuition that head-final rules should
receive higher preference.
It is possible that other optimization methods,
or a more sophisticated way of inserting these fea-
tures into the translation rules could help alleviate
these problems.
6 Conclusion
In this paper, we analyzed the effect of applying
syntactic preprocessing methods to syntax-based
SMT. Additionally, we have adapted reordering
rules as a decoder feature. The results showed
that lexical processing, specifically insertion of
pseudo-particles, contributed to improving trans-
lation quality, and it was effective as preprocessing
for T2S.
It should be noted that this paper, while demon-
strating that the simple rule-based syntactic pro-
cessing methods that have been useful for PBMT
can also contribute to T2S in English-Japanese
translation, more work is required to ensure that
this will generalize to other settings. A next step in
our inquiry is the generalization of these results to
other proposed preprocessing techniques and other
language pairs. In addition, we would like to try
two ways described below. First, it is likely that
other tree transformations, for example changing
the internal structure of the tree by moving chil-
dren to different nodes, would help in cases where
it is common to translate into highly divergent syn-
tactic structures between the source and target lan-
guages. Second, we plan to investigate other ways
of incorporating the preprocessing rules as a soft
constraints, such as using n-best lists or forests to
enode many possible sentence interpretations.
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statisti-
cal machine translation. In Annual Meeting of the
40
Association for Computational Linguistics (ACL),
pages 763?770.
David Burkett and Dan Klein. 2012. Transforming
trees to improve syntactic convergence. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 863?872.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer
instability. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 176?181.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 531?
540.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2008. Overview of the
patent translation task at the NTCIR-7 workshop. In
Proceedings of the 7th NTCIR Workshop Meeting,
pages 389?400.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, pages 391?427.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Workshop on Syntax and Structure in
Statistical Translation, pages 98?106.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 609?
616.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research, pages 292?
297.
Graham Neubig and Kevin Duh. 2014. On the ele-
ments of an accurate tree-to-string machine transla-
tion system. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143?
149.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 529?533.
Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. Annual Meeting of the Association for Compu-
tational Linguistics (ACL), page 91.
Sonja Nie?en and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
Proceedings of the 18th conference on Computa-
tional linguistics-Volume 2, pages 1081?1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and out-
look. In IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU), page 5.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki,
and Jun ?ichi Tsujii. 2011. NTT-UT statistical ma-
chine translation in NTCIR-9 PatentMT. In Pro-
ceedings of NTCIR, pages 585?592.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 841?851.
41
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In International Conference on Computa-
tional Linguistics (COLING), page 508.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 523?530.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In North American Chapter of the
Association for Computational Linguistics, pages
256?263.
42
