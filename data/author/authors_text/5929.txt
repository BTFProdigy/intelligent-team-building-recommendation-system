Mixing Weak Learners in Semantic Parsing
Rodney D. Nielsen
Dept of Computer Science
University of Colorado, UCB-430
Boulder, CO 80309-0430
USA
Rodney.Nielsen@Colorado.edu
Sameer Pradhan
Center for Spoken Language Research
University of Colorado
Boulder, CO 80303
USA
Sameer.Pradhan@Colorado.edu
Abstract
We apply a novel variant of Random Forests
(Breiman, 2001) to the shallow semantic parsing
problem and show extremely promising results.
The final system has a semantic role classification
accuracy of 88.3% using PropBank gold-standard
parses. These results are better than all others
published except those of the Support Vector Ma-
chine (SVM) approach implemented by Pradhan
et al (2003) and Random Forests have numerous
advantages over SVMs including simplicity, faster
training and classification, easier multi-class classi-
fication, and easier problem-specific customization.
We also present new features which result in a 1.1%
gain in classification accuracy and describe a tech-
nique that results in a 97% reduction in the feature
space with no significant degradation in accuracy.
1 Introduction
Shallow semantic parsing is the process of finding
sentence constituents that play a semantic role rela-
tive to a target predicate and then labeling those con-
stituents according to their respective roles. Speci-
fying an event?s agent, patient, location, time of oc-
currence, etc, can be useful for NLP tasks such as
information extraction (c.f., Surdeanu et al, 2003),
dialog understanding, question answering, text sum-
marization, and machine translation. Example 1 de-
picts a semantic parse.
(1) [Agent She] [P bought] [Patient the vase]
[Locative in Egypt]
We expand on previous semantic parsing work
(Gildea and Jurafsky, 2002; Pradhan et al, 2003;
Surdeanu et al, 2003) by presenting a novel algo-
rithm worthy of further exploration, describing a
technique to drastically reduce feature space size,
and presenting statistically significant new features.
The accuracy of the final system is 88.3% on the
classification task using the PropBank (Kingsbury
et al, 2002) corpus. This is just 0.6% off the best
accuracy reported in the literature.
The classification algorithm used here is a vari-
ant of Random Forests (RFs) (Breiman, 2001).
This was motivated by Breiman?s empirical stud-
ies of numerous datasets showing that RFs often
have lower generalize error than AdaBoost (Fre-
und and Schapire, 1997), are less sensitive to noise
in the training data, and learn well from weak in-
puts, while taking much less time to train. RFs
are also simpler to understand and implement than
SVMs, leading to, among other things, easier in-
terpretation of feature importance and interactions
(c.f., Breiman, 2004), easier multi-class classifica-
tion (requiring only a single training session versus
one for each class), and easier problem-specific cus-
tomization (e.g., by introducing prior knowledge).
The algorithm described here is considerably differ-
ent from those in (Breiman, 2001). It was signifi-
cantly revised to better handle high dimensional cat-
egorical inputs and as a result provides much better
accuracy on the shallow semantic parsing problem.
The experiments reported here focus on the clas-
sification task ? given a parsed constituent known
to play a semantic role relative to a given predicate,
decide which role is the appropriate one to assign
to that constituent. Gold-standard sentence parses
for test and training are taken from the PropBank
dataset. We report results on two feature sets from
the literature and a new feature set described here.
In section 2, we describe the data used in the ex-
periments. Section 3 details the classification algo-
rithm. Section 4 presents the experimental results
and describes each experiment?s feature set. Sec-
tion 5 provides a discussion and thoughts on future
work.
2 The Data
The classifiers were trained on data derived from
the PropBank corpus (Kingsbury et al, 2002). The
same observations and features are used as de-
scribed by (Pradhan et al, 2003). They acquired
the original data from the July 15, 2002 release
of PropBank, which the University of Pennsylva-
nia created by manually labeling the constituents
S
NP VP
She bought
NP PP
the vase in Egypt
Arg0 Predicate Arg1 ArgM-Loc
Figure 1: Syntactic parse of the sentence in (2)
of the Penn TreeBank gold-standard parses (Marcus
et al, 1994). Predicate usages (at present, strictly
verbs) are hand annotated with 22 possible seman-
tic roles plus the null role to indicate grammatical
constituents that are not arguments of the predicate.
The argument labels can have different meanings
depending on their target predicate, but the anno-
tation method attempted to assign consistent mean-
ings to labels, especially when associated with sim-
ilar verbs. There are seven core roles or arguments,
labeled ARG0-5 and ARG9. ARG0 usually corre-
sponds to the semantic agent and ARG1 to the entity
most affected by the action. In addition to the core
arguments, there are 15 adjunctive arguments, such
as ARGM-LOC which identifies locatives. Thus our
previous example, ?She bought the vase in Egypt?,
would be parsed as shown in example 2. Figure
1 shows the associated syntactic parse without the
parts of speech.
(2) [Arg0 She] [P bought] [Arg1 the vase]
[ArgM-Loc in Egypt]
Development tuning is based on PropBank sec-
tion 00 and final results are reported for section 23.
We trained and tested on the same subset of obser-
vations as did Pradhan et al (2003). They indicated
that a small number of sentences (less than 1%)
were discarded due to manual tagging errors in the
original PropBank labeling process, (e.g., an empty
role tag). This one percent reduction applies to all
sections of the corpus (training, development and
test). They removed an additional 2% of the train-
ing data due to issues involving the named entity
tagger splitting corpus tokens into multiple words.
However, where these issues occurred in tagging the
section 23 test sentences, they were manually cor-
rected. The size of the dataset is shown in Table 1.
3 The Algorithm
3.1 Random Forests
Breiman (2001) defines a random forest as ?a clas-
sifier consisting of a collection of tree structured
classifiers {h(x,?k), k=1, ...} where the {?k} are
independently identically distributed random [train-
ing] vectors and each tree casts a unit vote for
Section # sent # words # preds # args
training 28 651 50 129
development 1.2 28 2.2 5.7
test 1.5 33 2.7 7.0
Table 1: Number of sentences, words, marked pred-
icates, and labeled arguments in thousands
the most popular class at input x.? Thus Bagging
(Breiman, 1996) is a form of Random Forest, where
each tree is grown based on the selection, with re-
placement, of N random training examples, where
N is the number of total examples in the training
set.
Breiman (2001) describes two new subclasses of
Random Forests, Forest-RI and Forest-RC. In each,
he combines Bagging, using the CART methodol-
ogy to create trees, with random feature selection
(Amit and Geman, 1997) at each node in the tree.
That is, at each node he selects a different random
subset of the input features and considers only these
in establishing the decision at that node.
The big idea behind Random Forests is that by in-
jecting randomness into the individual trees via ran-
dom feature selection, the correlation between their
classification results is minimized. A lower correla-
tion combined with reasonably good classification
accuracy for individual trees leads to a much higher
accuracy for the composite forest. In fact, Breiman
shows that a theoretical upper bound can be estab-
lished for the generalization error in terms of the
strength of the forest, s, and the mean value of the
classification correlation from individual trees, ??.
The strength, s, is the expected margin over the in-
put space, where the margin of an ensemble classi-
fier is defined as the difference between the fraction
of the ensemble members that vote for the correct
class versus the fraction voting for the most popular
alternative class. See (Breiman, 2001) for a detailed
description of s and ?? and how they are calculated.
The upper bound on the generalization error is given
by the following equation:
E? ? ??(1? s
2)
s2 (1)
Breiman found that Forest-RI and Forest-RC
compare favorably to AdaBoost in general, are far
less sensitive to noise in the training data, and can
learn well using weak inputs.
3.2 Feature Issues
Before describing the variant of Random Forests we
use here, it is helpful to discuss a couple of impor-
tant issues related to the input features. In the exper-
iments here, the true input features to the algorithm
are all categorical. Breiman?s approach to handling
categorical inputs is as follows. He modifies their
selection probability such that they are V -1 times as
likely as a numeric input to be selected for evalu-
ation at each node, where V is the number of val-
ues the categorical feature can take. Then when a
categorical input is selected he randomly chooses a
subset of the category values and converts the input
into a binary-valued feature whose value is one if
the training observation?s corresponding input value
is in the chosen subset and zero otherwise.
In many machine learning approaches, a categor-
ical feature having V different values would be con-
verted to V (or V -1) separate binary-valued features
(e.g., this is the case with SVMs). Here, we process
them as categorical features, but conceptually think
of them as separate binary-valued features. In an
attempt to minimize confusion, we will refer to the
categorical input features simply as inputs or as in-
put features, the equivalent set of binary-valued fea-
tures as the binary-valued features, and the features
that are randomly composed in the tree building pro-
cess (via random category value subset selection) as
composed features.
3.3 Algorithm Description
Take any tree building algorithm (e.g., C5.0 (Quin-
lan, 2002)) and modify it such that instead of exam-
ining all of the input features at each node, it con-
siders only a random subset of those features. Con-
struct a large number of trees using all of the train-
ing data (we build 128 trees in each experiment). Fi-
nally, allow the trees to individually cast unit votes
for each test observation. The majority vote deter-
mines the classification and ties are broken in favor
of the class that occurs most frequently in the train-
ing set.
Our implementation is the most similar to Forest-
RI, but has several differences, some significant.
These differences involve not using Bagging, the
use of a single forest rather than two competing
forests, the assumed size of V?i (the number of rele-
vant values for input i), the probability of selecting
individual inputs, how composed features are cre-
ated, and the underlying tree building algorithm. We
delineate each of these differences in the following
paragraphs.
Forest-RI combines random feature selection
with Bagging. Surprisingly, we found that, in our
experiments, the use of Bagging was actually hurt-
ing the classification accuracy of the forests and so
we removed this feature from the algorithm. This
means that we use all training observations to con-
struct each tree in the forest. This is somewhat
counter-intuitive given that it should increase cor-
relation in the outputs of the trees. However, the
strength of the forest is based in part on the accu-
racy of its trees, which will increase when utilizing
more training data. We also hypothesize that, given
the feature sets here, the correlation isn?t affected
significantly by the removal of Bagging. The rea-
son for this is the massive number of binary-valued
features in the problem (577,710 in just the baseline
feature set). Given this fact, using random feature
selection alone might result in substantially uncor-
related trees. As seen in equation 1 and shown em-
pirically in (Breiman, 2001), the lack of correlation
produced by random feature selection directly im-
proves the error bound.
Forest-RI involves growing two forests and se-
lecting the one most likely to provide the best re-
sults. These two forests are constructed using dif-
ferent values for F , the number of random features
evaluated at each node. The choice of which forest
is more likely to provide the best results is based on
estimates using the observations not included in the
training data (the out-of-bag observations). Since
we did not use Bagging, all of our observations are
used in the training of each tree and we could not
take this approach. Additionally, it is not clear that
this provided better results in (Breiman, 2001) and
preliminary experiments (not reported here) suggest
that it might be more effective to simply find a good
value for F .
To create composed features, we randomly select
a number of the input?s category values, C, given by
the following equation:
C = 1, V? ? 4
C = ?1.5 + log2 V? ?, V? > 4
(2)
where V? is the number of category values still po-
tentially relevant. Random category value selec-
tion is consistent with Breiman?s work, as noted in
section 3.2. This random selection method should
act to further reduce the correlation between trees
and Breiman notes that it gets around the problem
caused by categorical inputs with large numbers of
values. However, he leaves the number of values
chosen unspecified. There is also no indication of
what to do as the categorical input becomes more
sparse near the leaves of the tree (e.g., if the algo-
rithm sends every constituent whose head word is in
a set ? down the right branch of the node, what ef-
fect does this have on future random value selection
in each branch). This is the role of V? in the above
equation.
A value is potentially relevant if it is not known
to have been effectively removed by a previous de-
cision. The decision at a given node typically sends
all of the observations whose input is in the se-
lected category value subset down one branch, and
the remaining observations are sent down the other
(boolean compositions would result in exceptions).
The list of relevant category values for a given in-
put is immediately updated when the decision has
obvious consequences (e.g., the values in ? are re-
moved from the list of relevant values used by the
left branch in the previous example and the list for
the right branch is set to ?). However, a decision
based on one input can also affect the remaining rel-
evant category values of other inputs (e.g., suppose
that at the node in our previous example, all prepo-
sitional phrase (PP) constituents had the head word
with and with was a member of ?, then the phrase
type PP would no longer be relevant to decisions
in the left branch, since all associated observations
were sent down the right branch). Rather than up-
date all of these lists at each node (a computation-
ally expensive proposition), we only determine the
unique category values when there are fewer than
1000 observations left on the path, or the number of
observations has been cut to less than half what it
was the last time unique values were determined. In
early experimentation, this reduced the accuracy by
about 0.4% relative to calculating the remaining cat-
egory values after each decision. So when speed is
not important, one should take the former approach.
Breiman indicates that, when several of the in-
puts are categorical, in order to increase strength
enough to obtain a good accuracy rate the number
of inputs evaluated at each node must be increased
to two-three times ?1 + log2 M? (where M is the
number of inputs). It is not clear whether the input
selection process is with or without replacement.
Some of the inputs in the semantic parsing prob-
lem have five orders of magnitude more category
values than others. Given this issue, if the selec-
tion is without replacement, it leads to evaluating
features composed from each of our seven baseline
inputs (figure 2) at each node. This would likely
increase correlation, since those inputs with a very
small number of category values will almost always
be the most informative near the root of the tree and
would be consistently used for the upper most deci-
sions in the tree. On the other hand, if selection is
with replacement, then using the Forest-RI method
for calculating the input selection probability will
result in those inputs with few category values al-
most never being chosen. For example, the baseline
feature set has 577710 equivalent binary-valued fea-
tures by the Forest-RI definition, including two true
binary inputs. The probability of one of these two
inputs not being chosen in a given random draw ac-
cording to the Forest-RI method is 577709/577710
(see section 3.2 above). With M=7 inputs, generat-
ing 3?1 + log2 M? = 9 random composed features
results in these two binary inputs having a selection
probability of 1? (577709/577710)9, or 0.000016.
Our compromise is first to use C and V? from
equation 2 to calculate a baseline number of com-
posable features for each input i. This quantity is
the total number of potentially relevant category val-
ues divided by the number used to create a com-
posed feature:
fi =
V?i
Ci
(3)
Second, given the large number of composable fea-
tures fi, we also evaluate a larger number, F , of
random features at each node in the tree:
F = max(?
?
f?,min(f, ?1.5 + 3 log2(f)?)) (4)
where f is the sum of fi over all inputs. Finally,
selection and feature composition is done with re-
placement. The final feature selection process has at
least two significant effects we find positive. First,
the number of composable features reflects the fact
that several category values are considered simul-
taneously, effectively splitting on Ci binary-valued
features. This has the effect of reducing the selec-
tion probability of many-valued inputs and increas-
ing the probability of selecting inputs with fewer
category values. Using the baseline feature set as
an example, the probability of evaluating one of the
binary-valued inputs at the root of the tree increases
from 0.000016 to 0.0058. Second, as category val-
ues are used they are periodically removed from the
set under consideration, reducing the correspond-
ing size of Vi, and the input selection probabilities
are then adjusted accordingly. This has the effect
of continuously raising the selection probability for
those inputs that have not yet been utilized.
Finally, we use ID3 to grow trees rather than
CART, which is the tree algorithm Forest-RI uses.
We don?t believe this should have any significant
effect on the final results. The choice was purely
based on already having an implementation of ID3.
From a set of possible split decisions, ID3 chooses
the decision which leads to the minimum weighted
average entropy among the training observations as-
signed to each branch, as determined by class labels
(Quinlan, 1986; Mitchell, 1997).
These algorithm enhancements are appropriate
for any task with high dimensional categorical in-
puts, which includes many NLP applications.
PREDICATE: the lemma of the predicate whose
arguments are to be classified ? the infinitive form
of marked verbs in the corpus
CONSTITUENT PHRASE TYPE: the syntactic type
assigned to the constituent/argument being classi-
fied
HEAD WORD (HW): the head word of the target
constituent
PARSE TREE PATH (PATH): the sequence of parse
tree constituent labels from the argument to its
predicate
POSITION: a binary value indicating whether the
target argument precedes or follows its predicate
VOICE: a binary value indicating whether the
predicate was used in an active or passive phrase
SUB-CATEGORIZATION: the parse tree expansion
of the predicate?s grandparent constituent
Figure 2: Baseline feature set of experiment 1, see
(Gildea and Jurafsky, 2002) for details
4 The Experiments
Four experiments are reported: the first uses the
baseline features of Gildea and Jurafsky (2002); the
second is composed of features proposed by Prad-
han et al (2003) and Surdeanu et al (2003); the
third experiment evaluates a new feature set; and the
final experiment addresses a method of reducing the
feature space. The experiments all focus strictly on
the classification task ? given a syntactic constituent
known to be an argument of a given predicate, de-
cide which argument role is the appropriate one to
assign to the constituent.
4.1 Experiment 1: Baseline Feature Set
The first experiment compares the random for-
est classifier to three other classifiers, a statisti-
cal Bayesian approach with backoff (Gildea and
Palmer, 2002), a decision tree classifier (Surdeanu
et al, 2003), and a Support Vector Machine (SVM)
(Pradhan et al, 2003). The baseline feature set uti-
lized in this experiment is described in Figure 2 (see
(Gildea and Jurafsky, 2002) for details).
Surdeanu et al omit the
SUB-CATEGORIZATION feature, but add a
binary-valued feature that indicates the governing
category of noun-phrase argument constituents.
This feature takes on the value S or VP depending
on which constituent type (sentence or verb phase
respectively) eventually dominates the argument in
the parse tree. This generally indicates grammatical
subjects versus objects, respectively. They also
used the predicate with its case and morphology
intact, in addition to using its lemma. Surdeanu
et al indicate that, due to memory limitations on
Classifier Accuracy
Bayesian (Gildea and Palmer, 2002) 82.8
Decision Tree (Surdeanu et al, 2003) 78.8
SVM (Pradhan et al, 2003) 87.1
First Tree 78.3
Random Forest 84.6
Table 2: Results of baseline feature set experiment
their hardware, they trained on only 75 KB of the
PropBank argument constituents ? about 60% of
the annotated data.
Table 2 shows the results of experiment 1, com-
paring the classifier accuracies as trained on the
baseline feature set. Using a difference of two pro-
portions test as described in (Dietterich, 1998), the
accuracy differences are all statistically significant
at p=0.01. The Random Forest approach outper-
forms the Bayesian method and the Decision Tree
method. However, it does not perform as well as the
SVM classifier. Interestingly, the classification ac-
curacy of the first tree in the Random Forest, given
in row four, is almost as high as that of the C5 deci-
sion trees (Quinlan, 2002) of Surdeanu et al
4.2 Experiment 2: Extended Feature Set
The second experiment compares the random for-
est classifier to the boosted decision tree and the
SVM using all of the features reported by Pradhan
et al The additional features used in this experi-
ment are listed in Figure 3 (see sources for further
details). In addition to the extra features noted in the
previous experiment, Surdeanu et al report on four
more features, not included here (content word part
of speech (CW PoS)1, CW named entity class, and
two phrasal verb collocation features).
Table 3 shows the results of experiment 2, com-
paring the classifier accuracies using the full feature
sets reported in each source. Surdeanu et al also ap-
plied boosting in this experiment and chose the out-
come of the boosting iteration that performed best.
Using the difference of two proportions test, the ac-
curacy differences are all statistically significant at
p=0.01. The Random Forest approach outperforms
the Boosted Decision Tree method by 3.5%, but
trails the SVM classifier by 2.3%. In analyzing the
performance on individual argument classes using
McNemar?s test, Random Forest performs signifi-
cantly better on ARG0 (p=0.001) then the SVM, and
the SVM has significantly better results on ARG1
(p=0.001). The large number of degrees of freedom
1We also tested the CW PoS, but it did not improve the de-
velopment results and was omitted.
NAMED ENTITIES: seven binary-valued fea-
tures indicating whether specific named enti-
ties (PERSON, ORGANIZATION, DATE, TIME,
MONEY, LOCATION, and PERCENT) occurred
anywhere in the target constituent (Surdeanu et al,
2003)
HW POS: the grammatical part of speech of the
target constituent?s head word (Surdeanu et al,
2003)
CONTENT WORD (CW): ?lexicalized feature that
selects an informative word from the constituent,
different from the head word?(Surdeanu et al,
2003)
VERB CLUSTER: a generalization of the verb
predicate by clustering verbs into 64 classes
(Pradhan et al, 2003)
HALF PATH: the sequence of parse tree con-
stituent labels from the argument to the lowest
common ancestor of the predicate (Pradhan et al,
2003)
Figure 3: Additional features in experiment 2
Classifier Accuracy
Boosted Decision Tree (Surdeanu et al,
2003)
83.7
Random Forest (trained with CW) 87.2
SVM (Pradhan et al, 2003) 88.9
Random Forest (trained without CW) 86.6
Table 3: Results of experiment 2
prevent significance at p=0.1 for any other argu-
ments, but the SVM appears to perform much better
on ARG2 and ARG3.
4.3 Experiment 3: New Features
We evaluated several new features and report on the
most significant here, as described in figure 4.2 The
results are reported in table 4. The accuracy im-
provements relative to the results from experiment
2 are all statistically significant at p=0.001 (McNe-
mar?s test is used for all significance tests in this sec-
tion). Comparing the SVM results in experiment 2
to the best results here shows statistical significance
2Due to space, we cannot report all experiments; contact the
first author for more information. The other features we eval-
uated involved: the phrase type of the parent constituent, the
list of phrase types encompassing the sentence fragment be-
tween the target predicate and constituent, the prefix and suffix
of the cw and hw, animacy, high frequency words preceding
and following the predicate, and the morphological form of the
predicate. All of these improved accuracy on the development
set (some with statistical significance at p=0.01), but we sus-
pect the development baseline was at a low point, since these
features largely did not improve performance when combined
with CW Base and GP.
GOVERNING PREPOSITION (GP): if the con-
stituent?s parent is a PP, this is the associated
preposition (e.g., in ?made of [Arg2 gallium ar-
senide]?, this feature is ?of?, since the Arg2-NP is
governed by an ?of?-based PP)
CW BASE: starting with the CW, convert it to its
singular form, remove any prefix, and convert dig-
its to ?n? (e.g., this results in the following CW ?
CW Base mappings: accidents ? accident, non-
binding ? binding, repayments ? payment, and
1012 ? nnnn)
Figure 4: Features in experiment 3
Feature Set Accuracy
Extended (see figures 2 & 3) 86.6
Extended + CW BASE 87.4
Extended + GOVERNING PREPOSITION 87.4
Extended + CW BASE & GP 88.3
Table 4: Results of experiment 2
only at p=0.1.
In analyzing the effect on individual argument
classes, seven have high ?2 values (ARG2-4,
ARGM-DIS (discourse), ARGM-LOC (locative),
ARGM-MNR (manner), and ARGM-TMP (temporal)),
but given the large number of degrees of free-
dom, only ARGM-TMP is significant (p=0.05). Ex-
ample section-00 sentence fragments including the
target predicate (P) and ARG2 role whose classi-
fication was corrected by the GP feature include
?[P banned] to [everyday visitors]?, ?[P consid-
ered] as [an additional risk for the investor]?, and
?[P made] of [gallium arsenide]?. Comparing the
SVM results to the best results here, the Ran-
dom Forest performs significantly better on Arg0
(p=0.001), and the SVM is significantly better on
Arg1 (p=0.001). Again the degrees of freedom pre-
vent significance at p=0.1, but the Random Forest
outperforms the SVM with a fairly high ?2 value on
ARG4, ARGM-DIS, ARGM-LOC, and ARGM-TMP.
4.4 Experiment 4: Dimensionality Reduction
We originally assumed we would be using binary-
valued features with sparse matrices, much like in
the SVM approach. Since many of the features have
a very large number of values (e.g., the PATH fea-
ture has over 540k values), we sought ways to re-
duce the number of equivalent binary-valued fea-
tures. This section reports on one of these meth-
ods, which should be of interest to others in resource
constrained environments.
In this experiment, we preprocess the baseline in-
puts described in Figure 2 to reduce their number
of category values. Specifically, for each original
category value, vi ? V , we determine whether it
occurs in observations associated with one or more
than one semantic role label, R. If it is associated
with more than one R, vi is left as is. When vi maps
to only a single Rj , we replace vi with an arbitrary
value, vk /? V , which is the same for all such v oc-
curring strictly in association with Rj . The PATH
input starts with 540732 original feature values and
has only 1904 values after this process, while HEAD
WORD is reduced from 33977 values to 13208 and
PHRASE TYPE is reduced from 62 to 44 values.
The process has no effect on the other baseline input
features. The total reduction in equivalent binary-
valued features is 97%. We also test the effect of
disregarding feature values during training if they
only occur once in the training data. This has a
more modest effect, reducing PATH to 156788 val-
ues and HEAD WORD to 29482 values, with no other
reductions. The total reduction in equivalent binary-
valued features is 67%.
Training on the baseline feature set, the net effect
of these two procedures was less than a 0.3% loss
of accuracy on the development set. The McNemar
test indicates this is not significant at p=0.1. In the
end, our implementation used categorical features,
rather than binary-valued features (e.g., rather than
use 577710 binary-valued features to represent the
baseline inputs, we use 7 features which might take
on a large number of values ? PATH has 540732 val-
ues). In this case, the method does not result in as
significant a reduction in the memory requirements.
While we did not use this feature reduction in any
of the experiments reported previously, we see it as
being very beneficial to others whose implementa-
tion may be more resource constrained, particularly
those using a binary-valued feature representation.
The method also reduced training time by 17%
and should lead to much larger reductions for im-
plementations using binary-valued features. For ex-
ample, the worst case training time for SVMs is
quadratic in the number of features and this method
reduced the dimensionality to 3% of its original
size. Therefore, the method has the theoretical
potential to reduce training time by up to 100(1-
0.032) = 99.91%. While it is unlikely to ap-
proach this in practice, it should provide signifi-
cant savings. This may be especially helpful during
model selection or feature evaluation, after which,
one could revert to the full dimensionality for fi-
nal training to improve classification accuracy. The
slight decrement in accuracy may also be overcome
by the ability to handle larger datasets.
5 Discussion and Future Research
The version of Random Forests described here out-
performs the Bayesian algorithm (Gildea and Juraf-
sky, 2002; Gildea and Palmer, 2002) by 1.8% on the
same feature set and outperforms the boosted deci-
sion tree classifier (Surdeanu et al, 2003) by 3.5%
on the extended feature set with 5 fewer features.
The SVM classifier (Pradhan et al, 2003) was 2.3%
better training on the same data, but only 0.6% bet-
ter than our best results.
The Random Forest (RF) approach has advan-
tages that might make it a better choice than an
SVM in certain circumstances. Conceptually, it
is simpler to understand and can be implemented
more easily. This also makes it easier to modify
the algorithm to evaluate new techniques. RFs al-
low one to more easily implement multi-class clas-
sifiers. The RFs here were implemented as a single
classifier, rather than as the 22 one-against-all clas-
sifiers required by the SVM approach. Since RFs
are not overly sensitive to noise in the training data
(Breiman, 2001), it might be the case that they will
narrow the performance gap when training is based
on automatically parsed sentences. Further research
is required in this area. Additionally, RFs have an
advantage in training time. It takes about 40% of
the SVM time (8 versus 20 hours) to train on the
extended feature set for the classification task and
we expect this time to be cut by up to a factor of 10
in porting from MatLab to C. Classification time is
generally faster for RFs as well, which is important
for real-time tasks.
In a class-by-class comparison, using the same
features, the RF performed significantly better than
the SVM on Arg0 roles, the same or slightly better
on 12 of the other 21 arguments, and slightly bet-
ter overall on the 14 adjunctive arguments (77.8%
versus 77.3% accuracy on 1882 observations). Re-
viewing performance on data not seen during train-
ing, both algorithms degraded to about 94% of their
accuracy on seen data.
The RF algorithm should be evaluated on the
identification task and on the combined identifica-
tion and classification task. This will provide addi-
tional comparative evidence to contrast it with the
SVM approach. Further research is also required to
determine how RFs generalize to new genres.
Another area for future research involves the es-
timation of class probabilities. MOB-ESP, a variant
of Random Forests which outputs class probability
estimates, has been shown to produce very good re-
sults (Nielsen, 2004). Preliminary experiments sug-
gest that using these probability estimates in con-
junction with an SVM classifier might be more ef-
fective than estimating probabilities based on the
example?s distance from the decision surface as in
(Platt, 2000). Class probabilities are useful for sev-
eral semantic parsing and more general NLP tasks,
such as selective use of labeled examples during
training (c.f., Pradhan et al, 2003) and N-best list
processing.
6 Conclusion
The results documented in these experiments are
very promising and mandate further research. The
final classification accuracy of the Random For-
est was 88.3%, just 0.6% behind the SVM results
(Pradhan et al, 2003) and 4.6% higher than the next
best results (Surdeanu et al, 2003) ? results that
were based on a number of additional features.
We defined several modifications to the RF algo-
rithm that increased accuracy. These improvements
are important for any application with high dimen-
sional categorical inputs, which includes many NLP
tasks. We introduced new features which provided
a 1.1% improvement in accuracy over the best re-
sults using features from the literature. We also in-
troduced a technique to reduce the dimensionality
of the feature space, resulting in a reduction to just
3% of the original feature space size. This could
be an important enabler for handling larger datasets
and improving the efficiency of feature and model
selection.
Acknowledgements
We thank Dan Jurafsky for miscellaneous support
and for valuable feedback on a draft of this paper.
Thanks also go to the anonymous reviewers whose
feedback improved the paper.
References
Yali Amit and Donald Geman. 1997. Shape Quan-
tization and Recognition with Randomized Trees.
Neural Computation, 9:1545?1588.
Leo Breiman. 2001. Random Forests. Journal of
Machine Learning, 45(1):5?32.
Leo Breiman. 2004. Random Forests. http://stat-
www.berkeley.edu/users/breiman/RandomForests/
Leo Breiman. 1996. Bagging Predictors. Machine
Learning, 26(2):123?140.
Thomas G. Dietterich. 1998. Approximate statis-
tical tests for comparing supervised classifica-
tion learning algorithms. Neural Computation,
10(7):1895?1924.
Y. Freund and R. E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and
an application to boosting. Journal of Computer
and Systems Sciences, 55(1):119?139.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic Labeling of Semantic Roles. Computa-
tional Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Parsing for Predicate Argument Recog-
nition. Proceedings of ACL-02.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn
Treebank. Proceedings of the HLT-02.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating
predicate argument structure.
Tom M. Mitchell. 1997. Machine Learning.
McGraw-Hill, Boston, MA.
Rodney D. Nielsen. 2004. MOB-ESP and other Im-
provements in Probability Estimation. Proceed-
ings of the 20th Conference on Uncertainty in Ar-
tificial Intelligence.
John Platt. 2000. Probabilities for Support Vector
Machines. In A. Smola, P. Bartlett, B. Scolkopf,
and D. Schuurmans (Eds), Advances in Large
Margin Classifiers. MIT Press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, Daniel Jurafsky.
2003. Shallow Semantic Parsing using Support
Vector Machines. University of Colorado Tech-
nical Report: TR-CSLR-2003-03.
J. R. Quinlan. 1986. Induction of decision trees.
Machine Learning, 1:81?106.
J. R. Quinlan. 2002. Data Mining Tools See5 and
C5.0. http://www.rulequest.com/see5-info.html.
Mihai Surdeanu, Sanda Harabagiu, John Williams
and Paul Aarseth. 2003. Using Predicate-
Argument Structures for Information Extraction.
Proceedings of ACL-03.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 28?35,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Corpus of Fine-Grained Entailment Relations
Rodney D. Nielsen and Wayne Ward
Center for Spoken Language Research
Institute of Cognitive Science
Department of Computer Science
University of Colorado, Boulder
Rodney.Nielsen, Wayne.Ward@Colorado.edu
Abstract
This paper describes on-going efforts to an-
notate  a  corpus  of  almost  16000  answer 
pairs with an estimated 69000 fine-grained 
entailment relationships.  We illustrate the 
need for  more detailed classification than 
currently  exists  and  describe  our  corpus 
and annotation scheme.  We discuss early 
statistical  analysis showing substantial  in-
ter-annotator  agreement  even  at  the  fine-
grained level.  The corpus described here, 
which is  the only one providing such de-
tailed annotations,  will  be made available 
as a public resource later this year (2007). 
This is expected to enable application de-
velopment that is currently not practical.
1 Introduction
Determining whether the propositions in one text 
fragment are entailed by those in another fragment 
is important to numerous NLP applications. Con-
sider an intelligent tutoring system (ITS), where it 
is  critical  for  the  tutor  to  assess  which  specific 
facets of  the desired or reference answer are en-
tailed by the student?s answer. Truly effective in-
teraction and pedagogy is only possible if the auto-
mated tutor can assess this entailment at a relative-
ly fine level of detail (c.f. Jordan et al, 2004).
The PASCAL Recognizing Textual Entailment 
(RTE) challenge (Dagan et al, 2005) has brought 
the issue of textual entailment before a broad com-
munity of researchers in a task independent fash-
ion. This task requires systems to make simple yes-
no judgments as to whether a human reading a text 
t of  one  or  more  full  sentences  would  typically 
consider a second, hypothesis, text  h (usually one 
full sentence) to most likely be true.  This paper 
discusses some of the extensions necessary to this 
scheme in order to satisfy the requirements of an 
ITS and provides a preliminary report on our ef-
forts  to  produce  an  annotated  corpus  applying 
some of  these  additions  to  children?s  answers  to 
science questions.  
We first  provide a  brief  overview of the RTE 
challenge  task  and  a  synopsis  of  answer  assess-
ment  technology  within  existing  ITSs  and  large 
scale  assessment  applications.   We  then  detail 
some of the types of changes required in order to 
facilitate more effective pedagogy.  We provide a 
report on our work in this direction and describe a 
corpus we are annotating with fine-grained entail-
ment information.  Finally, we discuss future direc-
tion and the relevance of this annotation scheme to 
other applications such as question answering.
2 Prior Work
2.1 RTE Challenge Task
Example 1 shows a typical  t-h pair from the RTE 
challenge.  The task is to determine whether typi-
cally a reader would say that  h is most likely true 
having read t.  The system output is a simple yes or 
no decision about this entailment ? in this example, 
the decision is no ? and that is similarly the extent 
to which training data is annotated.  There is no in-
dication of whether some facets of, the potentially 
quite long, h are addressed (as they are in this case) 
in  t or conversely, which facets are not discussed 
or are explicitly contradicted.
(1) <t>At an international disas-ter conference in Kobe, Japan, the 
28
U.N. humanitarian chief said the United Nations should take the lead in creating a tsunami early-warning system in the Indian Ocean.</t><h>Nations affected by the Asian tsunami disaster have agreed the UN should begin work on an early warning system in the Indian Ocean.</h>
However, in the third RTE challenge, there is an 
optional pilot task1 that begins to address some of 
these issues.  Specifically, they have extended the 
task  by  including  an  unknown label,  where  h is 
neither entailed nor contradicted, and have request-
ed justification for decisions.  The form that these 
justifications  will  take  has  been  left  up  to  the 
groups  participating,  but  could  conceivably  pro-
vide some of the information about which specific 
facets of the hypothesis are entailed, contradicted 
and unaddressed.
2.2 Existing Answer Assessment Technology
Effective ITSs exist in the laboratory producing 
learning  gains  in  high-school,  college,  and  adult 
subjects through text-based dialog interaction (e.g., 
Graesser et al, 2001; Koedinger et al, 1997; Peters 
et al, 2004, VanLehn et al, 2005).  However, most 
ITSs today provide only a shallow assessment of 
the learner?s comprehension (e.g., a correct versus 
incorrect decision).  Many ITS researchers are stri-
ving  to  provide  more  refined  learner  feedback 
(Aleven et al, 2001; Graesser et al, 2001; Jordan 
et al, 2004; Peters et al, 2004; Roll et al, 2005; 
Ros? et al, 2003).  However, they are developing 
very  domain-dependent  approaches,  requiring  a 
significant investment in hand-crafted logic repre-
sentations,  parsers,  knowledge-based  ontologies, 
and  or  dialog  control  mechanisms.   Simply  put, 
these domain-dependent techniques will not scale 
to the task of developing general purpose ITSs and 
will  never enable the long-term goal of effective 
unconstrained interaction with learners or the peda-
gogy that requires it.
There is also a small, but growing, body of re-
search in the area of scoring free-text responses to 
short answer questions (e.g., Callear et al,  2001; 
Leacock,  2004;  Mitchell  et  al.,  2003;  Pullman, 
2005; Sukkarieh, 2005).  Shaw (2004) and Whit-
tington (1999) provide reviews of some of these 
approaches.  Most of the systems that have been 
implemented and tested are based on Information 
1 http://nlp.stanford.edu/RTE3-pilot/
Extraction  (IE)  techniques  (Cowie  &  Lehnert, 
1996).  They hand-craft a large number of pattern 
rules, directed at detecting the propositions in com-
mon  correct  and  incorrect  answers.   In  general, 
short-answer  free-text  response  scoring  systems 
are designed for large scale assessment tasks, such 
as those associated with the tests administered by 
ETS.  Therefore,  they are  not  designed with the 
goal  of  accommodating  dynamically  generated, 
previously unseen questions.  Similarly, these sys-
tems do not provide feedback regarding the specif-
ic aspects of answers that are correct or incorrect; 
they merely provide a raw score for each question. 
As with the  related work directed specifically  at 
ITSs, these approaches all require in the range of 
100-500 example student answers for each planned 
test question to assist in the creation of IE patterns 
or to train a machine learning algorithm used with-
in some component of their solution.
3 The Necessity of Finer-grained Analysis
Imagine that you are an elementary school science 
tutor and that rather than having access to the stu-
dent?s full response to your questions, you are sim-
ply  given  the  information  that  their  answer  was 
correct or incorrect,  a yes or no entailment deci-
sion.  Assuming the student?s answer was not cor-
rect, what question do you ask next?  What follow 
up question or action is most likely to lead to better 
understanding on the part  of  the child?  Clearly, 
this is a far from ideal scenario, but it is roughly 
the situation within which many ITSs exist today.
In order to optimize learning gains in the tutor-
ing environment, there are myriad issues the tutor 
must  understand  regarding  the  semantics  of  the 
student?s  response.   Here,  we  focus  strictly  on 
drawing inferences regarding the student?s under-
standing  of  the  low-level  concepts  and  relation-
ships or facets of the reference answer.  I use the 
word facet throughout this paper to generically re-
fer to some part of  a text?s meaning.   The most 
common  type  of  answer  facet  discussed  is  the 
meaning  associated  with  a  pair  of  related  words 
and the relation that connects them.
Rather than have a single yes or no entailment 
decision for the reference answer as a whole, (i.e., 
does the student understand the reference answer 
in its entirety or is there some unspecified part of it 
that  we  are  unsure  whether  the  student  under-
stands),  we  instead  break  the  reference  answer 
29
down into what we consider to be its lowest level 
compositional  facets.   This  roughly  translates  to 
the set of triples composed of labeled dependencies 
in  a  dependency  parse  of  the  reference  answer.2 
The following illustrates  how a  simple  reference 
answer (2) is decomposed into the answer facets 
(2a-d)  derived  from  its  dependency  parse  and 
(2a?-d?) provide a gloss of each facet?s meaning. 
As can be seen in 2b and 2c, the dependencies are 
augmented by thematic roles (Kipper et al, 2000) 
(e.g.,  Agent,  Theme,  Cause,  Instrument?)  pro-
duced  by  a  semantic  role  labeling  system  (c.f., 
Gildea and Jurafsky,  2002).   The facets  also  in-
clude  those  semantic  role  relations  that  are  not 
derivable from a typical dependency tree.  For ex-
ample, in the sentence ?As it freezes the water will  
expand and crack the glass?, water is not a modifi-
er of crack in the dependency tree, but it does play 
the role of Agent in a shallow semantic parse.
(2) A long string produces a low pitch.
(2a) NMod(string, long)
(2b) Agent(produces, string)
(2c) Product(produces, pitch)
(2d) NMod(pitch, low)
(2a?) There is a long string.
(2b?) The string is producing some-
thing.
(2c?) A pitch is being produced.
(2d?) The pitch is low.
Breaking the reference answer down into low-
level  facets  provides  the  tutor?s  dialog  manager 
with a much finer-grained assessment of the stu-
dent?s response, but a simple yes or no entailment 
at  the  facet  level  still  lacks semantic  expressive-
ness with regard to the relation between the studen-
t?s answer and the facet in question.  Did the stu-
dent contradict the facet?  Did they express a relat-
ed  concept  that  indicates  a  misconception?   Did 
they leave the facet unaddressed?  Can you assume 
that they understand the facet even though they did 
not express it, since it was part of the information 
given in the question?  It is clear that, in addition to 
2 The goal of most English dependency parsers is to pro-
duce a single projective tree structure for each sentence, 
where each node represents a word in the sentence, each 
link represents a functional category relation, usually la-
beled, between a governor (head) and a subordinate 
(modifier), and each node has a single governor (c.f., 
Nivre and Scholz, 2004).
breaking  the  reference  answer  into  fine-grained 
facets, it is also necessary to break the annotation 
into finer levels in order to specify more clearly the 
relationship between the student?s answer and the 
reference answer aspect.
There  are  many  other  issues  that  the  system 
must know to achieve near optimal tutoring, some 
of which are mentioned later in the discussion sec-
tion, but these two ? breaking the reference answer 
into fine-grained facets and utilizing more expres-
sive annotation labels ? are the emphasis of this ef-
fort.
4 Current Annotation Efforts
This section describes our current efforts in anno-
tating  a  corpus  of  answers  to  science  questions 
from elementary school students. 
4.1 Corpus
Lacking data from a real tutoring situation, we ac-
quired data gathered from 3rd-6th grade students in 
schools utilizing the Full  Option Science System 
(FOSS).  Assessment is a major FOSS research fo-
cus,  of  which  the  Assessing  Science  Knowledge 
project  is  a  key component.3  The FOSS project 
has developed sixteen science teaching and learn-
ing modules targeted at  grades 3-6,  as shown in 
Table 1.   The ASK project created assessments for 
each of these modules, including multiple choice, 
fill  in  the  blank,  free  response,  and  somewhat 
lengthy  experimental  design  questions.   We  re-
viewed these questions and selected about 290 free 
response questions that were in line with the objec-
tives  of  this  research project,  specifically  we se-
lected questions whose expected responses ranged 
in length from moderately short verb phrases to a 
few sentences, that could be assessed objectively, 
and that were not too open ended.  Table 2 shows a 
3 ?FOSS is a research-based science program for grades 
K?8 developed at the Lawrence Hall of Science, Uni-
versity of California at Berkeley with support from the 
National Science Foundation and published by Delta 
Education.  FOSS is also an ongoing research project 
dedicated to improving the learning and teaching of sci-
ence.?
Assessing Science Knowledge (ASK) is ?designed to 
define, field test, and validate effective assessment tools 
and techniques to be used by grade 3?6 classroom 
teachers to assess, guide, and confirm student learning 
in science.?
http://www.lawrencehallofscience.org/foss/
30
Grade Life Science Physical Science and 
Technology
Earth and Space 
Science
Scientific Reasoning 
and Technology
3-4 HB: Human Body
ST: Structure of Life 
ME: Magnetism & Electricity
PS: Physics of Sound 
WA: Water
EM: Earth Materials 
II: Ideas & Inventions
MS: Measurement 
5-6 FN: Food & Nutrition
EV: Environments
LP: Levers & Pulleys
MX: Mixtures & Solutions
SE: Solar Energy
LF: Landforms
MD: Models & Designs
VB: Variables
1Table 1 FOSS / ASK Learning and Assessment Modules by Area and Grade
HB Q: Dancers need to be able to point their feet. The tibialis is the major muscle on the front of the leg 
and the gastrocnemius is the major muscle on the back of the leg. Describe how the muscles in the 
front and back of the leg work together to make the dancer?s foot point.
R: The muscle in the back of the leg (the gastrocnemius) contracts and the muscle in the front of the 
leg (the tibialis) relaxes to make the foot point.
A: The back muscle and the front muscle stretch to help each other pull up the foot.
ST Q: Why is it important to have more than one shelter in a crayfish habitat with several crayfish?
R: Crayfish are territorial and will protect their territory. The shelters give them places to hide from 
other crayfish. [Crayfish prefer the dark and the shelters provide darkness.]
A: So all the crayfish have room to hide and so they do not fight over them.
ME Q: Lee has an object he wants to test to see if it is an insulator or a conductor. He is going to use the 
circuit you see in the picture. Explain how he can use the circuit to test the object.
R: He should put one of the loose wires on one part of the object and the other loose wire on another 
part of the object (and see if it completes the circuit).
A: You can touch one wire on one end and the other on the other side to see if it will run or not.
PS Q: Kate said: ?An object has to move to produce sound.?  Do you agree with her?   Why or why not?
R: Agree. Vibrations are movements and vibrations produce sound.
A: I agree with Kate because if you talk in a tube it produce sound in a long tone.  And it vibrations 
and make sound.
WA Q: Anna spilled half of her cup of water on the kitchen floor. The other half was still in the cup. When 
she came back hours later, all of the water on the floor had evaporated but most of the water in the 
cup was still there. (Anna knew that no one had wiped up the water on the floor.)  Explain to Anna 
why the water on the floor had all evaporated but most of the water in the cup had not.
R: The water on the floor had a much larger surface area than the water in the cup.
A: Well Anna, in science, I learned that when water is in a more open are, then water evaporates faster. 
So, since tile and floor don't have any boundaries or wall covering the outside, the water on the 
floor evaporated faster, but since the water in the cup has boundaries, the water in the cup didn't 
evaporate as fast.
EM Q: You can tell if a rock contains calcite by putting it into a cold acid (like vinegar). 
Describe what you would observe if you did the acid test on a rock that contains this substance.
R: Many tiny bubbles will rise from the calcite when it comes into contact with cold acid.
A: You would observe if it was fizzing because calcite has a strong reaction to vinegar.
Table 2 Sample Qs from FOSS-ASK with their reference (R) and an example student answer (A).
few questions that are representative of those se-
lected for inclusion in the corpus, along with their 
reference answers and an example student answer 
for  each.   Questions  without  at  least  one  verb 
phrase were rejected because they were assumed to 
be  more  trivial  and  less  interesting  from the  re-
search  perspective.   Examples  of  such  questions 
along with their reference answers and an example 
student response include: Q:  Besides air, what (if  
anything)  can  sound  travel  through? Reference 
Answer: Sound can also travel through liquids and 
solids.  (Also  other  gases.) Student  Answer:  A 
screen door.  Q: Name a property of the sound of a  
fire engine?s siren. Reference Answer:  The sound 
is very loud. OR The sound changes in pitch. Stu-
dent Answer: Annoying.  An example of a free re-
sponse item that was dropped because it was too 
open ended is: Design an investigation to find out  
a plant?s range of tolerance for number of hours of  
sunlight per day. You can use drawings to help ex-
plain your design.
We generated a corpus from a random sample of 
the kids? handwritten responses to these questions. 
The only special transcription instructions were to 
31
fix spelling errors (since these would be irrelevant 
in a spoken dialog environment), but not grammat-
ical errors (which would still be relevant), and to 
skip blank answers and non-answers similar in na-
ture to I don?t know (since these are not particular-
ly interesting from the research perspective).
Three modules were designated as  the test  set 
(Environments, Human Body, and Water) and the 
remaining 13 modules  will  be  used for  develop-
ment and training of  classification systems.   We 
judged the three test set modules to be representa-
tive of the entire corpus in terms of difficulty and 
appropriateness for the types of questions that met 
our research interests.  We transcribed the respons-
es of approximately 40 randomly selected students 
for each question in the training set and 100 ran-
domly selected students for  each question in  the 
test set.  In order to maximize the diversity of lan-
guage and knowledge represented by the training 
and test datasets, random selection of students was 
performed at the question level  rather than using 
the same students? answers for all of the questions 
in a given module.  However, in total there were 
only about 200 children that participated in any in-
dividual  science  module  assessment,  so  there  is 
still  moderate  overlap  in  the  students  from  one 
question to another within a given module.  On the 
other hand, each assessment module was given to a 
different group of kids, so there is no overlap in 
students  between modules.   There  are  almost  60 
questions and 5700 student answers in the test set, 
comprising approximately 20% of all of the ques-
tions utilized and 36% of the total number of tran-
scribed student responses.  In total, including test 
and training datasets, there are nearly 16000 stu-
dent responses.
4.2 Annotation
The answer assessment annotation described in this 
paper is intended to be a step toward specifying the 
detailed semantic understanding of a student?s an-
swer that is required for an ITS to interact effec-
tively with a learner.  With that goal in mind, anno-
tators were asked to consider and annotate accord-
ing to what they would want to know about the stu-
dent?s answer if they were the tutor (but a tutor that 
for some reason could not understand the unstruc-
tured text of the student?s answer).  The key excep-
tion here is that we are only annotating a student?s 
answer in terms of whether or not it accurately and 
completely  addresses  the  facets  of  the  reference 
(desired or correct) answer.  So, if the student also 
discusses concepts not addressed in the reference 
answer, we will not annotate those points regard-
less of their quality or accuracy.
Each reference answer in the corpus is decom-
posed into its constituent facets.  Then each student 
answer is annotated relative to the facets in the cor-
responding reference answer.  As described earlier, 
the reference answer facets are roughly extracted 
from the relations in a syntactic dependency parse 
(c.f.,  Nivre and Scholz,  2004) and a shallow se-
mantic parse (Gildea and Jurafsky, 2002).  These 
are modified slightly to either eliminate most func-
tion words or incorporate them into the relation la-
bels (c.f., Lin and Pantel, 2001).  Example 3 illus-
trates the decomposition of one of the reference an-
swers  into  its  constituent  parts  along  with  their 
glosses.
(3) The string is tighter, so the 
pitch is higher.
(3a)  Is(string, tighter)
(3a?) The string is tighter.
(3b)  Is(pitch, higher)
(3b?) The pitch is higher.
(3c)  Cause(3b, 3a)
(3c?) 3b is caused by 3a
The annotation  tool  lists  the  reference  answer 
facets that students are expected to address.  Both a 
formal  relational  representation  and  an  English-
like gloss of the facet are displayed in a table, one 
row per facet.  The annotator?s job is to label each 
of those facets to indicate the extent to which the 
student addressed it.  We settled on the eight anno-
tation  labels  noted  in  Table  3.   Descriptions  of 
where each annotation label applies and some of 
the most common annotation issues were detailed 
with  several  examples  in  the  guidelines  and  are 
only very briefly summarized in the remainder of 
this subsection.
Example 4 shows a student answer correspond-
ing to  the  reference answer  in  example  3,  along 
with its initial annotation in 4a-c and its final anno-
tation in 4a?-c?.  It is assumed that the student un-
derstands that the pitch is higher (facet 4b), since 
this  is  given in the question (? Write a note to  
David to tell him why the pitch gets higher rather  
than lower) and similarly it is assumed that the stu-
dent will be explaining what has the causal effect 
of producing this higher pitch (facet 4c).  There-
fore, these facets are initialized to Assumed by the 
32
system.  Since the student does not contradict the 
fact that the string is tighter (the string can be both 
longer and tighter),  we do not label this facet  as 
Contradicted.  If the student?s response did not 
mention anything about either the string or tight-
ness,  we  would  annotate  facet  4a  as  Unad-
dressed.   However,  the  student  did  discuss  a 
property of the string, the string is long, producing 
the  facet  Is(string, long).   This  parallels  the 
reference answer facet Is(string, tighter) with 
the exception of a different argument to the Is re-
lation, resulting in the annotation Diff-Arg.  This 
indicates to the tutor that the student expressed a 
related concept, but one which neither implies that 
they understand the reference answer facet nor that 
they explicitly hold a contradictory belief.  Often, 
this indicates that the student has a misconception. 
For example, when asked about an effect on pitch, 
many students say things like the pitch gets louder, 
rather than higher or lower, which implies a mis-
conception involving their understanding of pitch 
and volume.  In this case, the Diff-Arg label can 
help focus the tutor on correcting this misconcep-
tion.  Facet 4c expressing the causal relation be-
tween 4a and 4b is labeled  Expressed, since the 
student did express a causal relation between the 
concepts aligned with 4a and 4c.  The tutor then 
knows that the student was on track in regard to at-
tempting to express the desired causal relation and 
the tutor need only deal with the fact that the cause 
given was incorrect.  
Table 3 Facet Annotation Labels
(4) David this is why because you 
don't listen to your teacher. If the 
string is long, the pitch will be 
high.
(4a) Is(string, tighter), ---
(4b) Is(pitch, higher), Assumed
(4c) Cause(4b, 4a), Assumed
(4a?) Is(string, tighter), Diff-Arg
(4b?) Is(pitch, higher), Expressed
(4c?) Cause(4b, 4a), Expressed
The  Self-Contra annotation is used in cases 
like the response in example 5, where the student 
simultaneously expresses the contradictory notions 
that the string is tighter and that there is less ten-
sion.
(5) The string is tighter, so there is 
less tension so the pitch gets higher.
(5a) Is(string, tighter), Self-Contra
There is no compelling reason from the perspec-
tive of the automated tutoring system to differenti-
ate  between  Expressed and  Inferred facets, 
since in either case the tutor can assume that the 
student understands the concepts involved.  How-
ever,  from  the  systems  development  perspective 
there are three primary reasons for differentiating 
between these facets and similarly between facets 
that are contradicted by inference versus more ex-
plicit expression.  The first reason is that most sta-
tistical  machine  learning  systems  today  cannot 
hope to detect very many pragmatic inferences and 
including these in the training data is likely to con-
fuse the algorithm resulting in worse performance. 
Having separate labels allows one to remove the 
more  difficult  inferences  from the  training  data, 
thus eliminating this problem.  The second ratio-
nale is that systems hoping to handle both types of 
inference might more easily learn to discriminate 
between these opposing classifications if the class-
es are distinguished (for algorithms where this is 
not the case, the classes can easily be combined au-
tomatically).  Similarly, this allows the possibility 
of training separate classifiers to handle the differ-
ent forms of inference.  The third reason for sepa-
rate labels is  that it  facilitates system evaluation, 
including  the  comparison  of  various  techniques 
and the effect of individual features.
Example 6 illustrates an example  of  a student 
answer with the label Inferred.  In this case, the 
decision  requires  pragmatic  inferences,  applying 
the Gricean maxims of Relation, be relevant ? why 
Expressed: Any facet directly expressed or inferred 
by simple reasoning
Inferred: Facets inferred by pragmatics or nontrivial 
logical reasoning
Contra-Expr: Facets directly contradicted by nega-
tion, antonymous expressions and their paraphrases
Contra-Infr:  Facets  contradicted  by  pragmatics  or 
complex reasoning
Self-Contra:  Facets  that  are  both contradicted and 
implied (self contradictions)
Diff-Arg: The core relation is expressed, but it has a 
different modifier or argument
Assumed:  The  system assigns  this  label,  which  is 
changed if any of the above labels apply
Unaddressed: Facets that are not addressed at all by 
the student?s answer
33
would the student  mention vibrations  if  they did 
not  know they were  a  form of  movement  ?  and 
Quantity, do not make your contribution more in-
formative than is required (Grice, 1975).
(6) Q: Kate said: ?An object has to move to produce sound.? Do you agree with her? Why or why not?Ref Ans: ?Agree. Vibrations are move-ments and vibrations produce sound.?Student Answer: Yes because it has to vibrate to make sounds.
(6b) Is(vibration, movement), Inferred
Annotators are primarily students of Education 
and Linguistics and require moderate training on 
the annotation task.  The annotated reference an-
swers are stored in a stand-off markup in xml files, 
including an annotated element for each reference 
answer facet.
4.3 Inter-Annotator Agreement Results
The results reported here are preliminary, based on 
the first two annotators, and must be viewed under 
the light that we have not yet completed annotator 
training.   We  report  results  under  three  label 
groupings: (1)  All-Labels,  where all  labels are 
left  separate,  (2)  Tutor-Labels,  where  Ex-
pressed,  Inferred and  Assumed are combined 
as are  Contra-Expr and  Contra-Infr,  and (3) 
Yes-No, which is a two-way division, Expressed, 
Inferred and Assumed versus all other labels.  
Agreement  on  Tutor-Labels indicates  the 
benefit to the tutor, since it is relatively unimpor-
tant to differentiate between the types of inference 
required  in  determining  that  the  student  under-
stands a reference answer facet (or has contradict-
ed it).  We evaluated mid-training inter-annotator 
agreement  on  a  random selection  of  15  answers 
from each of 14 Physics of Sound questions, total-
ing 210 answers  and 915 total  facet  annotations. 
Mid-training agreement on the  Tutor-Labels is 
87.4%, with a Kappa statistic of 0.717 correspond-
ing with substantial agreement (Cohen, 1960).  In-
ter-annotator  agreement  at  mid-training  is  81.1% 
on All-Labels and 90.1% on the binary Yes-No 
decision.  These also have Kappa statistics in the 
range of substantial agreement.  
The distribution of the 915 annotations is shown 
in Table 4.  It is somewhat surprising that this sci-
ence module had so few contradictions, just 2.7% 
of all annotations, particularly given that many of 
the questions seem more likely to draw contradic-
tions than unaddressed facets (e.g., many ask about 
the effect on pitch and volume, typically eliciting 
one of two possible responses).  An analysis of the 
inter-annotator confusion matrix indicates that the 
most probable disagreement is between Inferred 
and  Unaddressed.   The second most likely dis-
agreement is  between  Assumed and  Expressed. 
In discussing disagreements, the annotators almost 
always  agree  quickly,  reinforcing  our  belief  that 
we will increase agreement significantly with addi-
tional training.
Label Count % Count %
Expressed 348 38.0
Inferred 51 5.6
Assumed 258 28.2
657 71.8
Contra-Expr 21 2.3
Contra-Infr 4 0.4 25 2.7
Self-Contra 1 0.1 1 0.1
Diff-Arg 33 3.6 33 3.6
Unaddressed 199 21.7 199 21.7
Table 4 Distribution of classifications (915 facets)
5 Discussion and Future Work
The goal of our fine-grained classification is to en-
able  more  effective  tutoring  dialog  management. 
The  additional  labels  facilitate  understanding  the 
type  of  mismatch  between  the  reference  answer 
and the student?s answer.  Breaking the reference 
answer down into low-level facets enables the tutor 
to provide feedback relevant specifically to the ap-
propriate  facet  of  the  reference  answer.   In  the 
question answering domain, this facet-based classi-
fication would allow systems to accumulate entail-
ing  evidence  from  a  variety  of  corroborating 
sources and incorporate answer details that might 
not be found in any single sentence.  In other appli-
cations outside  of  the tutoring domain,  this  fine-
grained classification can also facilitate more di-
rected user feedback.  For example, both the addi-
tional classifications and the break down of facets 
can be used to justify system decisions, which is 
the stated goal of the pilot task at the third RTE 
challenge.  
The corpus described in this paper, which will 
be released later this year (2007), represents a sub-
stantial contribution to the entailment community, 
including an estimated 69000 facet entailment an-
notations.  By contrast, three years of RTE chal-
lenge  data  comprise  fewer  than  4600 entailment 
34
annotations.   More  importantly,  this  is  the  only 
corpus that provides entailment information at the 
fine-grained  level  described  in  this  paper.   This 
will enable application development that was not 
practical previously.
Future work includes training machine learning 
algorithms to perform the classifications described 
in this paper.  We also plan to annotate other as-
pects of the students? understanding that are not di-
rect  inferences  of  reference  answer  knowledge. 
Consider example (4), in addition to the issues al-
ready annotated, the student  contradicts  a law of 
physics  that  they  have  surely  encountered  else-
where  in  the  text,  specifically that  longer strings 
produce lower, not higher, pitches.  Under the cur-
rent annotation scheme this is not annotated, since 
it does not pertain directly to the reference answer 
which has to do with the effect of string tension. 
In other annotation plans, it would be very useful 
for training learning algorithms if we provide an 
indication of which student answer facets played a 
role in making the inferences classified.
Initial  inter-annotator  agreement  results  look 
promising, obtaining substantial agreement accord-
ing to the Kappa statistic.  We will continue to re-
fine our annotation guidelines and provide further 
training in order to push the agreement higher on 
all classifications.  
Acknowledgement
We would like to thank Martha Palmer for valu-
able advice on this annotation effort.
References
Aleven V, Popescu O, & Koedinger K. (2001) A tutorial 
dialogue system with knowledge-based understanding 
and classification of student explanations.  IJCAI WS 
knowledge & reasoning in practical dialogue systems
Callear, D., Jerrams-Smith, J., and Soh, V. (2001). CAA 
of short non-MCQ answers. In 5th Intl CAA.
Cohen J. (1960). A coefficient of agreement for nominal 
scales. Educational & Psych Measurement. 20:37-46.
Cowie,  J.,  Lehnert,  W.G.  (1996).  Information  Extrac-
tion. In Communications of the ACM, 39(1), 80-91.
Dagan,  Ido,  Glickman,  Oren,  and Magnini,  Bernardo. 
(2005).  The  PASCAL  Recognizing  Textual  Entail-
ment Challenge.  In 1st RTE Challenge Workshop.
Gildea, D. & Jurafsky, D. (2002). Automatic labeling of 
semantic roles. Computational Linguistics, 28:3, 245?288.
Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person, 
N.K., Louwerse, M., and Olde, B. (2001). AutoTutor: 
An  Intelligent  Tutor  and  Conversational  Tutoring 
Scaffold. In 10th ICAI in Education, 47-49.
Grice,  H.  Paul.  (1975).  Logic  and  conversation.  In  P 
Cole and J Morgan, editors,  Syntax and Semantics,  
Vol 3, Speech Acts, 43?58. Academic Press.
Jordan, P. W., Makatchev, M., & VanLehn, K. (2004). 
Combining  competing  language  understanding  ap-
proaches in an intelligent tutoring system. In 7th ITS.
Kipper, K, Dang, H, & Palmer, M. (2000). Class-Based 
Construction of a Verb Lexicon. AAAI 17th NCAI
Koedinger,  K.R.,  Anderson,  J.R.,  Hadley,  W.H.  & 
Mark,  M.A.  (1997).   Intelligent  tutoring  goes  to 
school in the big city.  Intl Jrnl of AI in Ed, 8, 30-43.
Leacock,  Claudia.  (2004).  Scoring free-response auto-
matically: A case study of a large-scale Assessment. 
Examens, 1(3).
Lin, Dekang and Pantel, Patrick. (2001). Discovery of 
inference rules for Question Answering. In  Natural 
Language Engineering, 7(4):343-360.
Mitchell,  T.  Aldridge, N.,  and Broomhead, P.  (2003). 
Computerized marking of  short-answer  free-text  re-
sponses. In 29th IAEA.
Nivre, J. and Scholz, M. (2004). Deterministic Depen-
dency Parsing of English Text. In Proc COLING.
Peters,  S,  Bratt,  E.O.,  Clark,  B.,  Pon-Barry,  H.  and 
Schultz, K. (2004). Intelligent  Systems for Training 
Damage Control Assistants. In Proc. of ITSE.
Pulman S.G. & Sukkarieh J.Z. (2005). Automatic Short 
Answer Marking. ACL WS Bldg Ed Apps using NLP.
Roll, I, Baker, R, Aleven, V, McLaren, B, & Koedinger, 
K. (2005).  Modeling Students? Metacognitive Errors 
in Two Intelligent Tutoring Systems.  In UM 379?388
Ros?, P. Roque, A., Bhembe, D. & VanLehn, K. (2003). 
A hybrid text classification approach for analysis of 
student essays. In Bldg Ed Apps using NLP
Shaw, Stuart. (2004). Automated writing assessment: a 
review of four conceptual models. In Research Notes,  
Cambridge ESOL.  Downloaded Aug 10, 2005 from 
http://www.cambridgeesol.org/rs_notes/rs_nts17.pdf
Sukkarieh, J. & Pulman, S. (2005). Information extrac-
tion and machine learning: Auto-marking short free 
text responses to science questions. In Proc of AIED.
VanLehn,  K.,  Lynch,  C.,  Schulze,  K.  Shapiro,  J.  A., 
Shelby, R., Taylor, L., Treacy, D., Weinstein, A., & 
Wintersgill,  M.  (2005).  The Andes physics tutoring 
system: Five years of evaluations.  In 12th ICAI in Ed
Whittington,  D.,  Hunt,  H.  (1999).  Approaches  to  the 
Computerised  Assessment  of  Free-Text  Responses. 
Third ICAA.
35
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 241?244,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting a Representation from Text for Semantic Analysis  Rodney D. Nielsen1,2, Wayne Ward1,2, James H. Martin1, and Martha Palmer1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin, Martha.Palmer@Colorado.edu       Abstract We present a novel fine-grained semantic rep-resentation of text and an approach to con-structing it. This representation is largely extractable by today?s technologies and facili-tates more detailed semantic analysis. We dis-cuss the requirements driving the representation, suggest how it might be of value in the automated tutoring domain, and provide evidence of its validity. 1 Introduction This paper presents a new semantic representation intended to allow more detailed assessment of stu-dent responses to questions from an intelligent tu-toring system (ITS). Assessment within current ITSs generally provides little more than an indica-tion that the student?s response expressed the target knowledge or it did not. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extraction frames, parsers, logic representations, or knowledge-based ontologies (c.f., Jordan et al, 2004). This is also true of re-search in the area of scoring constructed response questions (e.g., Leacock, 2004). The goal of the representation described here is to facilitate domain-independent assessment of student responses to questions in the context of a known reference answer and to perform this as-sessment at a level of detail that will enable more effective ITS dialog. We have two key criteria for this representation: 1) it must be at a level that fa-cilitates detailed assessment of the learner?s under-standing, indicating exactly where and in what manner the answer did not meet expectations and 
2) the representation and assessment should be learnable by an automated system ? they should not require the handcrafting of domain-specific representations of any kind.  Rather than have a single expressed versus un-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or completely fail to address it?) Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet. The emphasis of this paper is on this fine-grained facet-based representation ? considerations in defining it, the process of extract-ing it, and the benefit of using it. 2 Representing the Target Knowledge We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately short verb phrases to several sentences, cover all 16 diverse Full Option Science System teaching and learning modules spanning life science, physi-cal science, earth and space science, scientific rea-soning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 
241
2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These ref-erence answers were manually decomposed into fine-grained facets, roughly extracted from the re-lations in a syntactic dependency parse and a shal-low semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by deter-mining the dependency parse following the style of MaltParser (Nivre et al, 2006). This dependency parse was then modified in several ways. The ra-tionale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to gen-erate features for the assessment classification task. These types of modifications to the parser output address known limitations of current statistical parser outputs, and are reminiscent of the modifi-cations advocated by Briscoe and Carroll for more effective parser evaluation, (Briscoe, et. al, 2002). Example 1 illustrates the reference answer facets derived from the final dependencies in Figure 1, along with their glosses.  Figure 1. Reference answer representation revisions (1) The brass ring would not stick to the nail because the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d. Various linguistic theories take a different stance on what term should be the governor in a 
number of phrase types, particularly noun phrases. In this regard, the manual parses here varied from the style of MaltParser by raising lexical items to governor status when they contextually carried more significant semantics. In our example, the verb stick is made the governor of would, whose modifiers are reattached to stick. Similarly, the noun phrases the pattern of pigments and the bunch of leaves typically result in identical dependency parses. However, the word pattern is considered the governor of pigments; whereas, conversely the word leaves is treated as the governor of bunch because it carries more semantics. Then, terms that were not crucial to the student answer, frequently auxiliary verbs, were removed (e.g., the modal would and determiners in our example). Next, we incorporate prepositions into the de-pendency type labels following (Lin and Pantel, 2001). This results in the two dependencies vmod(stick, to) and pmod(to, nail), each of which carries little semantic value over its key lexical item, stick and nail, being combined into the sin-gle, more expressive dependency vmod_to(stick, nail), ultimately vmod is replaced with destination, as described below. Likewise, the dependencies connected by because are consolidated and be-cause is integrated into the new dependency type.  Next, copulas and a few similar verbs are also incorporated into the dependency types. The verb?s predicate is reattached to its subject, which be-comes the governor, and the dependency is labeled with the verb?s root. In our example, the two se-mantically impoverished dependencies sub(is, ring) and prd(is, iron) are combined to form the more meaningful dependency be(ring, iron). Then terms of negation are similarly incorporated into the dependency types. Finally, wherever a shallow semantic parse would identify a predicate argument structure, we used the thematic role labels in VerbNet (Kipper et al, 2000) between the predicate and the argu-ment?s headword, rather than the MaltParser de-pendency tags. This also involved adding new structural dependencies that a typical dependency parser would not generate. For example, in the sen-tence As it freezes the water will expand and crack the glass, typically the dependency between crack and its subject water is not generated since it would lead to a non-projective tree, but it does play the role of Agent in a semantic parse. In a small number of instances, these labels were also at-
242
tached to noun modifiers, most notably the Loca-tion label. For example, given the reference answer fragment The water on the floor had a much larger surface area, one of the facets extracted was Loca-tion_on(water, floor). We refer to facets that express relations between higher-level propositions as inter-propositional facets. An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron. In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in example 1). Reference answer facets that are assumed to be understood by the learner a pri-ori, (e.g., because they are part of the question), are also annotated to indicate this.  There were a total of 2878 reference answer fac-ets, resulting in a mean of 10 facets per answer (median 8). Facets that were assumed to be under-stood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%. The results of automated annotation of stu-dent answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional.  A total of 36 different facet relation types were utilized. The majority, 21, are VerbNet thematic roles. Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets). The seven highest frequency relations are NMod, Theme, Cause, Be, Patient, AMod, and Location, which together account for 70% of the reference answer facet relations 2.2 Student Answer Annotation For each student answer, we annotated each reference answer facet to indicate whether and how 
the student addressed that facet. We settled on the five annotation categories in Table 1. These labels and the annotation process are detailed in (Nielsen et al, 2008b).  Understood: Reference answer facets directly ex-pressed or whose understanding is inferred Contradiction: Reference answer facets contradicted by negation, antonymous expressions, pragmatics, etc. Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels 3 Automated Classification As partial validation of this knowledge representa-tion, we present results of an automatic assessment of our student answers. We start with the hand generated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on training data and use it to classify unseen test examples, assigning a Table 1 label for each reference answer facet. We used a variety of linguistic features that as-sess the facets? similarity via lexical entailment probabilities following (Glickman et al, 2005), part of speech tags and lexical stem matches. They include information extracted from modified de-pendency parses such as relevant relation types and path edit distances. Revised dependency parses are used to align the terms and facet-level information for feature extraction. Remaining details can be found in (Nielsen et al, 2008a) and are not central to the semantic representation focus of this paper. Current classification accuracy, assigning a Table 1 label to each reference answer facet to indicate the student?s expressed understanding, is 79% within domain (assessing unseen answers to ques-tions associated with the training data) and 69% out of domain (assessing answers to questions re-garding entirely different science subjects). These results are 26% and 15% over the majority class baselines, respectively, and 21% and 6% over lexi-
243
cal entailment baselines based on Glickman et al (2005). 4 Discussion and Future Work Analyzing the results of reference facet extraction, there are many interesting open linguistic issues in this area. This includes the need for a more sophisticated treatment of adjectives, conjunctions, plurals and quantifiers, all of which are known to be beyond the abilities of state of the art parsers. Analyzing the dependency parses of 51 of the student answers, about 24% had errors that could easily lead to problems in assessment. Over half of these errors resulted from inopportune sentence segmentation due to run-on student sentences con-joined by and (e.g., the parse of a shorter string makes a higher pitch and a longer string makes a lower pitch, errantly conjoined a higher pitch and a longer string as the subject of makes a lower pitch, leaving a shorter string makes without an object). We are working on approaches to mitigate this problem.  In the long term, when the ITS generates its own questions and reference answers, the system will have to construct its own reference answer facets. The automatic construction of reference answer facets must deal with all of the issues described in this paper and is a significant area of future research. Other key areas of future research involve integrating the representation described here into an ITS and evaluating its impact. 5 Conclusion We presented a novel fine-grained semantic repre-sentation and evaluated it in the context of auto-mated tutoring. A significant contribution of this representation is that it will facilitate more precise tutor feedback, targeted to the specific facet of the reference answer and pertaining to the specific level of understanding expressed by the student. This representation could also be useful in areas such as question answering or document summari-zation, where a series of entailed facets could be composed to form a full answer or summary. The representation?s validity is partially demon-strated in the ability of annotators to reliably anno-tate inferences at this facet level, achieving substantial agreement (86%, Kappa=0.72) and by promising results in automatic assessment of stu-
dent answers at this facet level (up to 26% over baseline), particularly given that, in addition to the manual reference answer facet representation, an automatically extracted approximation of the rep-resentation was a key factor in the features utilized by the classifier.  The domain independent approach described here enables systems that can easily scale up to new content and learning environments, avoiding the need for lesson planners or technologists to create extensive new rules or classifiers for each new question the system must handle. This is an obligatory first step to the long-term goal of creat-ing ITSs that can truly engage children in natural unrestricted dialog, such as is required to perform high quality student directed Socratic tutoring. Acknowledgments This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Briscoe, E., Carroll, J., Graham, J., and Copestake, A. 2002. Relational evaluation schemes. In Proc. of the Beyond PARSEVAL Workshop at LREC. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics. Glickman, O, Dagan, I, and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Proc RTE. Jordan, P, Makatchev, M, VanLehn, K. 2004. Combin-ing competing language understanding approaches in an intelligent tutoring system. In Proc ITS. Kipper, K, Dang, H, and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. In Proc. AAAI. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), UC Berkeley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens. Lin, D & Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natl. Lang. Engineering. Nielsen, R, Ward, W, and Martin, JH. 2008a. Learning to Assess Low-level Conceptual Understanding. In Proc. FLAIRS. Nielsen, R, Ward, W, Martin, JH and Palmer, P. 2008b. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nivre, J, Hall, J, Nilsson, J, Eryigit, G and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Pars-ing with Support Vector Machines. In Proc. CoNLL. Palmer, M, Gildea, D, & Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. 
244
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Classification Errors in a Domain-Independent Assessment System   Rodney D. Nielsen1,2, Wayne Ward1,2 and James H. Martin1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin@Colorado.edu       Abstract We present a domain-independent technique for assessing learners? constructed responses.  The system exceeds the accuracy of the ma-jority class baseline by 15.4% and a lexical baseline by 5.9%.  The emphasis of this paper is to provide an error analysis of performance, describing the types of errors committed, their frequency, and some issues in their resolution.   1 Introduction Assessment within state of the art Intelligent Tu-toring Systems (ITSs) generally provides little more than an indication that the student?s response expressed the target knowledge or it did not. There is no indication of exactly what facets of the con-cept a student contradicted or failed to express. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extrac-tion frames, parsers, logic representations, or knowledge-based ontologies (c.f., Graesser et al, 2001; Jordan et al, 2004; Peters et al, 2004; Roll et al, 2005; VanLehn et al, 2005). This is also true of research in the area of scoring constructed re-sponse questions (e.g., Callear et al, 2001; Lea-cock, 2004; Mitchell et al, 2002; Pulman and Sukkarieh, 2005). The present paper analyzes the errors of a system that was designed to address these limitations.   Rather than have a single expressed versus not-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately 
its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or just fail to address it?).  Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet.   In this paper, we present an error analysis of our system, detailing the most frequent types of errors encountered in our implementation of a domain-independent ITS assessment component and dis-cuss plans for correcting or mitigating some of the errors.  The system expects constructed responses of a phrase to a few sentences, but does not rely on technology developed specifically for the domain or subject matter being tutored ? without changes, it should handle history as easily as science.  We first briefly describe the corpus used, the knowl-edge representation, and the annotation.  In section 3, we describe our assessment system.  Then we present the error analysis and discussion. 2 Assessing Student Answers 2.1 Corpus We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately 
10
short verb phrases to several sentences, cover all 16 diverse teaching and learning modules, span-ning life science, physical science, earth and space science, scientific reasoning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 2.2 Knowledge Representation The ASK assessments included a reference an-swer for each of their constructed response ques-tions.  We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frame-works, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). These fac-ets are the basis for assessing learner answers. See (Nielsen et al, 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets.   Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses.  These facets represent the fine-grained knowledge the student is expected to address in their response.  (1) The brass ring would not stick to the nail be-cause the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d.  Figure 1. Reference answer representation revisions 
Typical facets, as in (1a), are derived directly from a dependency parse, in this case retaining its dependency type label, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(stick, to) and PMod(to, nail) in the case of (1c). When the head of the dependency is a verb, as in (1b,c), we use Thematic Roles from VerbNet (Kipper et al, 2000) and adjuncts from PropBank (Palmer et al, 2005) to label the facet relation.  Some copulas and simi-lar verbs were themselves used as facet relations, as in (1d).  Dependencies involving determiners and many modals, such as would, in ex. 1, are dis-carded and negations, such as not, are incorporated into the associated facets. We refer to facets that express relations between higher-level propositions as inter-propositional facets.  An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron.  In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in ex. 1).  Reference answer facets that are assumed to be understood by the learner a priori, (generally because they are part of the information given in the question), are also annotated to indi-cate this. There were a total of 2878 reference answer fac-ets, with a mean of 10 facets per reference answer (median of 8).  Facets that were assumed to be un-derstood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%.  The experiments in automated annotation of student answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional. 2.3 Annotating Student Understanding After defining the reference answer facets, we annotated each student answer to indicate whether and how they addressed each reference answer facet. We settled on the annotation labels in Table 1. For a given student answer, one label is assigned for each facet in the associated reference answer.  These labels and the annotation process are de-tailed in (Nielsen et al, 2008a).  
11
Assumed: Reference answer facets that are assumed to be understood a priori based on the question Expressed: Any reference answer facet directly ex-pressed or inferred by simple reasoning Inferred: Reference answer facets whose understanding is inferred by pragmatics or nontrivial logical reasoning Contra-Expr: Reference answer facets directly contra-dicted by negation, antonymous expressions, and their paraphrases Contra-Infr: Reference answer facets contradicted by pragmatics or complex reasoning Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels Example 2 shows a fragment of a question and associated reference answer broken down into its constituent facets with an indication of whether the facet is assumed to be understood a priori.  A cor-responding student answer is shown in (3) along with its final annotation in 2a?-c?.  It is assumed that the student understands that the pitch is higher (facet 2b), since this is given in the question and similarly it is assumed that the student will be ex-plaining what has the causal effect of producing this higher pitch (facet 2c).  Therefore, unless the student explicitly addresses these facets they are labeled Assumed.  The student phrase the string is long is aligned with reference answer facet 2a, since they are both expressing a property of the string, but since the phrase neither contradicts nor indicates an understanding of the facet, the facet is labeled Diff-Arg, 2a?.  The causal facet 2c? is la-beled Expressed, since the student expresses a causal relation and the cause and effect are each properly aligned.  In this way, the automated tutor will know the student is on track in attempting to address the cause and it can focus on remediating the student?s understanding of that cause. (2) Question: ... Write a note to David to tell him why the pitch gets higher rather than lower. Reference Answer: The string is tighter, so the pitch is higher... (2a) Be(string, tighter), --- (2b) Be(pitch, higher), Assumed (2c) Cause(2b, 2a), Assumed 
(3) David this is why because you don't listen to your teacher. If the string is long, the pitch will be high. (2a?) Be(string, tighter), Diff-Arg (2b?) Be(pitch, higher), Expressed (2c?) Cause(2b?, 2a?), Expressed A tutor will treat the labels Expressed, Inferred and Assumed all as Understood by the student and similarly Contra-Expr and Contra-Infr are com-bined as Contradicted.  These labels are kept sepa-rate in the annotation to facilitate training different systems to detect these different inference relation-ships, as well as to allow evaluation at that level.  The consolidated set of labels, comprised of Un-derstood, Contradicted, Self-Contra, Diff-Arg and Unaddressed, are referred to as the Tutor Labels. 3 Automated Classification A high level description of the assessment proce-dure is as follows. We start with the hand gener-ated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on our training data and use it to classify un-seen test examples, assigning a Tutor Label (described in the preceding paragraph), for each reference answer facet.  3.1 Preprocessing and Representation Many of the features utilized by the machine learn-ing algorithm here are based on document co-occurrence counts.  We use three publicly available corpora (English Gigaword, The Reuters corpus, and Tipster) totaling 7.4M articles and 2.6B terms.  These corpora are all drawn from the news do-main, making them less than ideal sources for as-sessing student?s answers to science questions. We utilized these corpora to generate term relatedness statistics primarily because they comprised a read-ily available large body of text.  They were in-dexed and searched using Lucene, a publicly available Information Retrieval tool.   Before extracting features, we automatically generate dependency parses of the reference an-swers and student answers using MaltParser (Nivre 
12
et al, 2006).  These parses are then automatically modified in a way similar to the manual revisions made when extracting the reference answer facets, as sketched in section 2.2.  We reattach auxiliary verbs and their modifiers to the associated regular verbs.  We incorporate prepositions and copulas into the dependency relation labels, and similarly append negation terms onto the associated depend-ency relations.  These modifications, all made automatically, increase the likelihood that terms carrying significant semantic content are joined by dependencies that are utilized in feature extraction.  In the present work, we did not make use of a the-matic role labeler.   3.2 Machine Learning Features & Approach We investigated a variety of linguistic features and chose to utilize the features summarized in Table 2, informed by training set cross validation results.  The features assess the facets? lexical similarity via lexical entailment probabilities following (Glick-man et al, 2005), part of speech (POS) tags, and lexical stem matches.  They include syntactic in-formation extracted from the modified dependency parses such as relevant relation types and path edit distances.  Remaining features include information about polarity among other things.  The revised dependency parses described earlier are used in aligning the terms and facet-level information for feature extraction, as indicated in the feature de-scriptions.  The data was split into a training set and three test sets.  The first test set, Unseen Modules, con-sists of all the data from three of the 16 science modules, providing a domain-independent test set.  The second, Unseen Questions, consists of all the student answers associated with 22 randomly se-lected questions from the 233 questions in the re-maining 13 modules, providing a question-independent test set.  The third test set, Unseen Answers, was created by randomly assigning all of the facets from approximately 6% of the remaining learner answers to a test set with the remainder comprising the training set. In the present work, we utilize only the facets that were not assumed to be understood a priori. This selection resulted in a total of 54,967 training examples, 30,514 examples in the Unseen Modules test set, 6,699 in the Un-seen Questions test set and 3,159 examples in the Unseen Answers test set. 
Lexical Features Gov/Mod_MLE: The lexical entailment probabilities (LEPs) for the reference answer facet governor (Gov; e.g., string in 2a) and modifier (Mod; e.g., tighter in 2a) following (Glickman et al, 2005; c.f., Turney, 2001). The LEP of a reference answer word w is defined as: (1) ,  where v is a word in the student answer, nv is the # of docs (see section 3.1) containing v, and nw,v is the # of docs where w & v cooccur. {Ex. 2a: the LEPs for string?string and tension? tighter, respectively}? Gov/Mod_Match: True if the Gov (Mod) stem has an exact match in learner answer. {Ex. 2a: True for Gov: string, and (False for Mod: no stem match for tighter)}? Subordinate_MLEs: The lexical entailment probabili-ties for the primary constituent facets? Govs and Mods when the facet represents a relation between higher-level propositions (see inter-propositional facet defini-tion in section 2.2). {Ex. 2c: the LEPs for pitch?pitch, up?higher, string?string, and tension?tighter}? Syntactic Features Gov/Mod_POS: POS tags for the facet?s Gov and (Mod). {Ex. 2a: NN for string and (JJR for tighter)}? Facet/AlignedDep_Reltn: The labels of the facet and aligned learner answer dependency ? alignments were based on co-occurrence MLEs as with words, (i.e., they estimate the likelihood of seeing the reference answer dependency in a document given it contains the learner answer dependency ? replace words with dependencies in equation 1 above). {Ex. 2a: Be is the facet label and Have is the aligned student answer dependency}? Dep_Path_Edit_Dist: The edit distance between the dependency path connecting the facet?s Gov and Mod (not necessarily a single step due to parser errors) and the path connecting the aligned terms in the learner an-swer. Paths include the dependency relations generated in our modified parse with their attached prepositions, negations, etc, the direction of each dependency, and the POS tags of the terms on the path. The calculation ap-plies heuristics to judge the similarity of each part of the path (e.g., dropping a subject had a much higher cost than dropping an adjective).  Alignment for this feature was made based on which set of terms in an N-best list (N=5 in the present experiments) for the Gov and Mod resulted in the smallest edit distance.  The N-best list was generated based on the lexical entailment values (see Gov/Mod_MLE). {Ex. 2b: Distance(up:VMod> went:V<pitch:Subject, pitch:Be>higher)}? Other Features Consistent_Negation: True if the facet and aligned student dependency path had the same number of nega-tions. {Ex. 2a: True: neither one have a negation}? RA_CW_cnt: The number of content words (non-function words) in the reference answer. {Ex. 2: 5 = count(string, tighter, so, pitch & higher)}? ? Examples within {} braces are based on reference answer Ex. 2 and the learner answer:  The pitch went up because the string has more tension Table 2. Machine Learning Features 
13
We evaluated several machine learning algo-rithms (rules, trees, boosting, ensembles and an svm) and C4.5 (Quinlan, 1993) achieved the best results in cross validation on the training data.  Therefore, we used it to obtain all of the results presented here.  A number of classifiers performed comparably and Random Forests outperformed C4.5 with a previous feature set and subset of data.  A thorough analysis of the impact of the classifier chosen has not been completed at this time. 3.3 System Results Given a student answer, we generate a separate Tutor Label (described at the end of section 2.3) for each associated reference answer facet to indi-cate the level of understanding expressed in the student?s answer (similar to giving multiple marks on a test).  Table 3 shows the classifier?s Tutor La-bel accuracy over all reference answer facets in cross validation on the training set as well as on each of our test sets.  The columns first show two simpler baselines, the accuracy of a classifier that always chooses the most frequent class in the train-ing set ? Unaddressed, and the accuracy based on a lexical decision that chooses Understood if both the governing term and the modifier are present in the learner?s answer and outputs Unaddressed oth-erwise, (we also tried placing a threshold on the product of the governor and modifier lexical en-tailment probabilities following Glickman et al (2005), who achieved the best results in the first RTE challenge, but this gave virtually the same results as the word matching baseline).  The col-umn labeled Table 2 Features presents the results of our classifier. (Reduced Training is described in the Discussion section, which follows.)  Majority Label Lexical Baseline Table 2 Features Reduced Training Training Set CV 54.6 59.7 77.1  Unseen Answers 51.1 56.1 75.5  Unseen Questions 58.4 63.4 61.7 66.5 Unseen Modules 53.4 62.9 61.4 68.8 Table 3. Classifier Accuracy 4 Discussion and Error Analysis 4.1 Results Discussion The accuracy achieved, assessing learner answers within this new representation framework, repre-
sent an improvement of 24.4%, 3.3%, and 8.0% over the majority class baseline for Unseen An-swers, Questions, and Modules respectively.  Ac-curacy on Unseen Answers is also 19.4% better than the lexical baseline. However, this simple baseline outperformed the classifier on the other two test sets.  It seemed probable that the decision tree over fit the data due to bias in the data itself; specifically, since many of the students? answers are very similar, there are likely to be large clusters of identical feature-class pairings, which could re-sult in classifier decisions that do not generalize as well to other questions or domains.  This bias is not problematic when the test data is very similar to the training data, as is the case for our Unseen Answers test set, but would negatively affect per-formance on less similar data, such as our Unseen Questions and Modules.   To test this hypothesis, we reduced the size of our training set to about 8,000 randomly selected examples, which would result in fewer of these dense clusters, and retrained the classifier.  The result for Unseen Questions, shown in the Reduced Training column, was an improvement of 4.8%.  Given this promising improvement, we attempted to find the optimal training set size through cross-validation on the training data.  Specifically, we iterated over the science modules holding one module out, training on the other 12 and testing on the held out module. We analyzed the learning curve varying the number of randomly selected examples per facet.  We found the optimal accu-racy for training set cross-validation by averaging the results over all the modules and then trained a classifier on that number of random examples per facet in the training set and tested on the Unseen Modules test set.  The result was an increase in accuracy of 7.4% over training on the full training set.  In future work, we will investigate other more principled techniques to avoid this type of over-fitting, which we believe is somewhat atypical. 4.2 Error Analysis In order to focus future work on the areas most likely to benefit the system, an error analysis was performed based on the results of 13-fold cross-validation on the training data (one fold per science module). In other words, 13 C4.5 decision tree classifiers were built, one for each science module in the training set; each classifier was trained, 
14
utilizing the feature set shown in Table 2, on all of the data from 12 science modules and then tested on the data in the remaining, held-out module. This effectively simulates the Unseen Modules test condition. To our knowledge, no prior work has analyzed the assessment errors of such a domain-independent ITS. Several randomly selected examples were analyzed to look for patterns in the types of errors the system makes.  However, only specific categories of data were considered.  Specifically, only the subsets of errors that were most likely to lead to short-term system improvements were considered.  This included only examples where all of the annotators agreed on the annotation, since if the annotation was difficult for humans, it would probably be harder to construct features that would allow the machine learning algorithm to correct its error.  Second, only Expressed and Unaddressed facets were considered, since Inferred facets represent the more challenging judgments, typically based on pragmatic inferences.  Contradictions were excluded since there was almost no attempt to handle these in the present system.  Third, only facets that were not inter-propositional were considered, since the inter-propositional facets are more complicated to process and only represent 12% of the non-Assumed data. We discuss Expressed facets in the next section of the paper and Unaddressed in the following section. 4.3 Errors in Expressed Facets Without examining each example relative to the decision tree that classified it, it is not possible to know exactly what caused the errors.  The analysis here simply indicates what factors are involved in inferring whether the reference answer facets were understood and what relationships exist between the student answer and the reference answer facet.  We analyzed 100 random examples of errors where annotators considered the facet Expressed and the system labeled it Unaddressed, but the analysis only considered one example for any given reference answer facet.  Out of these 100 examples, only one looked as if it was probably incorrectly annotated.  We group the potential error factors seen in the data, listed in order of frequency, according to issues associated with paraphrases, logical inference, pragmatics, and 
preprocessing errors.  In the following paragraphs, these groups are broken down for a more fine-grained analysis.  In over half of the errors considered, there were two or more of these fine-grained factors involved. Paraphrase issues, taken broadly, are subdivided into three main categories: coreference resolution, lexical substitution, syntactic alternation and phrase-based paraphrases. Our results in this area are in line with (Bar-Haim et al, 2005), who considered which inference factors are involved in proving textual entailment. Three coreference resolution factors combined are involved in nearly 30% of the errors.  Students use on average 1.1 pronouns per answer and, more importantly, the pronouns tend to refer to key entities or concepts in the question and reference answer.  A pronoun was used in 15 of the errors (3 personal pronouns ? she, 11 uses of it, and 1 use of one).  It might be possible to correct many of these errors by simply aligning the pronouns to essentially all possible nouns in the reference answer and then choosing the single alignment that gives the learner the most credit. In 6 errors, the student referred to a concept by another term (e.g., substituting stuff for pieces). In another 6 errors, the student used one of the terms in a noun phrase from either the question or reference answer to refer to a concept where the reference answer facet included the other term as its modifier or vice versa. For example, one reference answer was looking for NMod(particles, clay) and Be(particles, light) and the student said Because clay is the lightest, which should have resulted in an Understood classification for the second facet (one could argue that there is an important distinction between the answers, but requiring elementary school students to answer at this level of specificity could result in an overwhelming number of interactions to clarify understanding). As a group, the simple lexical substitution categories (synonymy, hypernymy, hyponymy, meronymy, derivational changes, and other lexical paraphrases) appear more often in errors than any of the other factors with around 35 occurrences.  Roughly half of these relationships should be detectable using broad coverage lexical resources.  For example, substituting tiny for small, CO2 for gas, put for place, pen for ink and push for carry (WordNet entailment).  However, many of these lexical paraphrases are not necessarily associated 
15
in lexical resources such as WordNet.  For example, in the substitution of put the pennies for distribute the pennies, these terms are only connected at the top of the WordNet hierarchy at the Synset (move, displace).  Similarly, WordNet appears not to have any connection at all between have and contain. VerbNet alo does not show a relation between either pair of words. Concept definitions account for an additional 14 issues that could potentially be addressed by lexical resources such as WordNet. Vanderwende et al (2005) found that 34% of the Recognizing Textual Entailment Challenge test data could be handled by recognizing simple syntactic variations.  However, while syntactic variation is certainly common in the kids? data, it did not appear to be the primary factor in any of the system errors.  Most of the remaining paraphrase errors were classified as involving phrase-based paraphrases.  Examples here include ...it will heat up faster versus it got hotter faster and in the middle versus halfway between.  Six related errors essentially involved negation of an antonym, (e.g., substituting not a lot for little and no one has the same fingerprint for everyone has a different print).  Paraphrase recognition is an area that we intend to invest significant time in future research (c.f., Lin and Pantel, 2001; Dolan et al, 2004).  This research should also reduce the error rate on lexical paraphrases. The next most common issues after paraphrases were deep or logical reasoning and then pragmatics.  These two factors were involved in nearly 40% of the errors.  Examples of logical inference include recognizing that two cups have the same amount of water given the following student response, no, cup 1 would be a plastic cup 25 ml water and cup 2 paper cup 25 ml and 10 g sugar, and that two sounds must be very different in the case that ?it is easy to discriminate? Examples of pragmatic issues include recognizing that saying Because the vibrations implies that a rubber band is vibrating given the question context, and that the earth in the response ?the fulcrum is too close to the earth should be considered to be the load referred to in its reference answer. It is interesting that these are all examples that three annotators unanimously considered to be Expressed versus Inferred facets.  Finally, the remaining errors were largely the result of preprocessing issues.  At least two errors 
would be eliminated by simple data normalization (3?three and g?grams). Semantic role labeling has the potential to provide the classifier with information that would clearly indicate the relationships between the student and the reference answer, but there was only one error in which this came to mind as an important factor and it was not due to the role labels themselves, but because MaltParser labels only a single head. Specifically, in the sentence She could sit by the clothes and check every hour if one is dry or not, the pronoun She is attached as the subject of could sit, but check is left without a subject.   In previous work, analyzing the dependency parses of fifty one of the student answers, many had what were believed to be minor errors, 31% had significant errors, and 24% had errors that looked like they could easily lead to problems for the answer assessment classifier. Over half of the more serious dependency parse errors resulted from inopportune sentence segmentation due to run-on student sentences conjoined by and. To overcome these issues, the text could be parsed once using the original sentence segmentation and then again with alternative segmentations under conditions to be determined by further dependency parser error analysis.  One partial approach could be to split sentences when two noun phrases are conjoined and they occur between two verbs, as is the case in the preceding example, where the alternative segmentation results in correct parses. Then the system could choose the parse that is most consistent with the reference answer. While we believe improving the parser output will result in higher accuracy by the assessment classifier, there was little evidence to support this in the small number of parses examined in the assessment error analysis.  We only checked the parses when the dependency path features looked wrong and it was somewhat surprising that the classifier made an error (for example, when there were simple lexical substitutions involving very similar words) ? this was the case for only about 10-15 examples. Only two of these classification errors were associated with parser errors. However, better parses should lead to more reliable (less noisy) features, which in turn will allow the machine learning algorithm to more easily recognize which features are the most predictive. It should be emphasized that over half of the errors in Expressed facets involved more than one 
16
of the fine-grained factors discussed here. For example, to recognize the child understands a tree is blocking the sunlight based on the answer There is a shadow there because the sun is behind it and light cannot go through solid objects. Note, I think that question was kind of dumb, requires resolving it to the tree and the solid object mentioned to the tree, and then recognizing that light cannot go through [the tree] entails the tree blocks the light. 4.4 Errors in Unaddressed Facets Unlike the errors in Expressed facets, a number of the examples here appeared to be questionable annotations. For example, given the student answer fragment You could take a couple of cardboard houses and? 1 with thick glazed insulation?, all three annotators suggested they could not infer the student meant the insulation should be installed in one of the houses. Given the student answer Because the darker the color the faster it will heat up, the annotators did not infer that the student believed the sheeting chosen was the darkest color.  One of the biggest sources of errors in Unaddressed facets is the result of ignoring the context of words. For example, consider the question When you make an electromagnet, why does the core have to be iron or steel? and its reference answer Iron is the only common metal that can become a temporary magnet. Steel is made from iron. Then, given the student answer It has to be iron or steel because it has to pick up the washers, the system classified the facet Material_from(made, iron) as Understood based on the text has to be iron, but ignores the context, specifically, that this should be associated with the production of steel, Product(made, steel). Similarly, the student answer You could wrap the insulated wire to the iron nail and attach the battery and switch leads to the classification of Understood for a facet indicating to touch the nail to a permanent magnet to turn it into a temporary magnet, but wrapping the wire to the nail should have been aligned to a different method of making a temporary magnet. Many of the errors in Unaddressed facets appear to be the result of antonyms having very similar statistical co-occurrence patterns. Examples of errors here include confusing closer with greater distance and absorbs energy with reflects energy. 
However, both of these also may be annotation errors that should have been labeled Contra-Expr. The biggest source of error is simply classifying a number of facets as Understood if there is partial lexical similarity and perhaps syntactic similarity as in the case of accepting the balls are different in place of different girls. However, there are also a few cases where it is unclear why the decision was made, as in an example where the system apparently trusted that the student understood a complicated electrical circuit based on the student answer we learned it in class. The processes and the more informative features described in the preceding section describing errors in Expressed facets should allow the learning algorithm to focus on less noisy features and avoid many of the errors described in this section. However, additional features will need to be added to ensure appropriate lexical and phrasal alignment, which should also provide a significant benefit here. Future plans include training an alignment classifier separate from the assessment classifier.  5 Conclusion To our knowledge, this is the first work to success-fully assess constructed-response answers from elementary school students.  We achieved promis-ing results, 24.4% and 15.4% over the majority class baselines for Unseen Answers and Unseen Modules, respectively.  The annotated corpus asso-ciated with this work will be made available as a public resource for other researches working on educational assessment applications or other tex-tual entailment applications. The focus of this paper was to provide an error analysis of the domain-independent (Unseen Mod-ules) assessment condition.  We discussed the common types of issues involved in errors and their frequency when assessing young students? understanding of the fine-grained facets of refer-ence answers.  This domain-independent assess-ment will facilitate quicker adaptation of tutoring systems (or general test assessment systems) to new topics, avoiding the need for a significant ef-fort in hand-crafting new system components.   It is also a necessary prerequisite to enabling unre-stricted dialogue in tutoring systems.  
17
Acknowledgements We would like to thank the anonymous reviewers, whose comments improved the final paper.  This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Bar-Haim, R., Szpektor, I. and Glickman, O. 2005. Definition and Analysis of Intermediate Entailment Levels. In Proc. Workshop on Empirical Modeling of Semantic Equivalence and Entailment. Callear, D., Jerrams-Smith, J., and Soh, V. 2001. CAA of short non-MCQ answers. In Proc. of the 5th Inter-national CAA conference, Loughborough. Dolan, W.B., Quirk, C, and Brockett, C. 2004. Unsu-pervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. Pro-ceedings of COLING 2004, Geneva, Switzerland. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:3, 245?288. Glickman, O. and Dagan, WE., and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Pro-ceedings of the PASCAL Recognizing Textual En-tailment Challenge Workshop. Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person, N.K., Louwerse, M., Olde, B., and the Tutoring Re-search Group. 2001. AutoTutor: An Intelligent Tutor and Conversational Tutoring Scaffold. In Proceed-ings for the 10th International Conference of Artifi-cial Intelligence in Education San Antonio, TX, 47-49. Jordan, P.W., Makatchev, M., and VanLehn, K. 2004. Combining competing language understanding ap-proaches in an intelligent tutoring system. In J. C. Lester, R. M. Vicari, and F. Paraguacu, Eds.), 7th Conference on Intelligent Tutoring Systems, 346-357. Springer-Verlag Berlin Heidelberg. Kipper, K., Dang, H.T., and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. AAAI Seven-teenth National Conference on Artificial Intelligence, Austin, TX. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), University of California at Ber-keley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens, 1(3). Lin, D. and Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natural Language Engineering, 7(4):343-360. Mitchell, T., Russell, T., Broomhead, P. and Aldridge, N. 2002. Towards Robust Computerized Marking of Free-Text Responses. In Proc. of 6th International 
Computer Aided Assessment Conference, Loughbor-ough.  Nielsen, R., Ward, W., Martin, J. and Palmer, M. 2008a. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nielsen, R., Ward, W., Martin, J. and Palmer, M. 2008b. Extracting a Representation from Text for Semantic Analysis. In Proc. ACL-HLT. Nivre, J. and Scholz, M. 2004. Deterministic Depend-ency Parsing of English Text. In Proceedings of COLING, Geneva, Switzerland, August 23-27. Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines. In Proceed-ings of the Tenth Conference on Computational Natural Language Learning (CoNLL). Palmer, M., Gildea, D., and Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. Peters, S., Bratt, E.O., Clark, B., Pon-Barry, H., and Schultz, K. 2004. Intelligent Systems for Training Damage Control Assistants. In Proc. of Inter-service/Industry Training, Simulation, and Education Conference. Pulman, S.G. and Sukkarieh, J.Z. 2005. Automatic Short Answer Marking. In Proc. of the 2nd Workshop on Building Educational Applications Using NLP, ACL. Quinlan, J.R. 1993. C4.5: Programs for Machine Learn-ing. Morgan Kaufmann. Roll, WE., Baker, R.S., Aleven, V., McLaren, B.M., and Koedinger, K.R. 2005. Modeling Students? Metacog-nitive Errors in Two Intelligent Tutoring Systems. In L. Ardissono, P. Brna, and A. Mitrovic (Eds.), User Modeling, 379?388. Turney, P.D. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), 491?502. Vanderwende, L., Coughlin, D. and Dolan, WB. 2005. What Syntax can Contribute in the Entailment Task. In Proc. of the PASCAL Workshop for Recognizing Textual Entailment. VanLehn, K., Lynch, C., Schulze, K. Shapiro, J. A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A., and Wintersgill, M. 2005. The Andes physics tutoring system: Five years of evaluations. In G. McCalla and C. K. Looi (Eds.), Proceedings of the 12th Interna-tional Conference on Artificial Intelligence in Educa-tion. Amsterdam: IOS Press.  
18
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200?210,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Towards Effective Tutorial Feedback for Explanation Questions:
A Dataset and Baselines
Myroslava O. Dzikovska? and Rodney D. Nielsen? and Chris Brew?
?School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, UK
?Computational Language & Education Research Center
University of Colorado at Boulder, Boulder, CO 80309-0594, USA
?Educational Testing Service, Princeton, NJ 08451, USA
m.dzikovska@ed.ac.uk, rodney.nielsen@colorado.edu, cbrew@ets.org
Abstract
We propose a new shared task on grading stu-
dent answers with the goal of enabling well-
targeted and flexible feedback in a tutorial di-
alogue setting. We provide an annotated cor-
pus designed for the purpose, a precise speci-
fication for a prediction task and an associated
evaluation methodology. The task is feasible
but non-trivial, which is demonstrated by cre-
ating and comparing three alternative baseline
systems. We believe that this corpus will be
of interest to the researchers working in tex-
tual entailment and will stimulate new devel-
opments both in natural language processing
in tutorial dialogue systems and textual entail-
ment, contradiction detection and other tech-
niques of interest for a variety of computa-
tional linguistics tasks.
1 Introduction
In human-human tutoring, it is an effective strategy
to ask students to explain instructional material in
their own words. Self-explanation (Chi et al, 1994)
and contentful talk focused on the domain are cor-
related with better learning outcomes (Litman et al,
2009; Chi et al, 1994). There has therefore been
much interest in developing automated tutorial dia-
logue systems that ask students open-ended expla-
nation questions (Graesser et al, 1999; Aleven et
al., 2001; Jordan et al, 2006; VanLehn et al, 2007;
Nielsen et al, 2009; Dzikovska et al, 2010a). In
order to do this well, it is not enough to simply
ask the initiating question, because students need
the experience of engaging in meaningful dialogue
about the instructional content. Thus, systems must
respond appropriately to student explanations, and
must provide detailed, flexible and appropriate feed-
back (Aleven et al, 2002; Jordan et al, 2004).
In simple domains, we can adopt a knowledge en-
gineering approach and build a domain model and a
diagnoser, together with a natural language parser to
produce detailed semantic representations of student
input (Glass, 2000; Aleven et al, 2002; Pon-Barry
et al, 2004; Callaway et al, 2006; Dzikovska et al,
2010a). The advantage of this approach is that it
allows for flexible adaptation of feedback to a va-
riety of factors such as student performance. For
example, it is easy for the system to know if the
student made the same error before, and adjust its
feedback to reflect it. Moreover, this approach al-
lows for easy addition of new exercises : as long as
an exercise relies on the concepts covered by the do-
main model, the system can apply standard instruc-
tional strategies to each new question automatically.
However, this approach is significantly limited by
the requirement that the domain be small enough to
allow comprehensive knowledge engineering, and it
is very labor-intensive even for small domains.
Alternatively, we can adopt a data-driven ap-
proach, asking human tutors to anticipate in ad-
vance a range of possible correct and incorrect an-
swers, and associating each answer with an appro-
priate remediation (Graesser et al, 1999; Jordan et
al., 2004; VanLehn et al, 2007). The advantage
of this approach is that it allows more complex and
interesting domains and provides a good framework
for eliciting the necessary information from the hu-
man experts. A weakness of this approach, which
200
also arises in content-scoring applications such as
ETS?s c-rater (Leacock and Chodorow, 2003), is that
human experts find it extremely difficult to predict
with any certainty what the full range of student re-
sponses will be. This leads to a lack of adaptivity
and generality ? if the system designers have failed
to predict the full range of possibilities, students will
often receive the default feedback. It is frustrating
and confusing for students to repeatedly receive the
same feedback, regardless of their past performance
or dialogue context (Jordan, 2004).
Our goal is to address the weaknesses of the data-
driven approach by creating a framework for sup-
porting more flexible and systematic feedback. Our
approach identifies general classes of error, such as
omissions, incorrect statements and off-topic state-
ments, then aims to develop general remediation
strategies for each error type. This has the potential
to free system designers from the need to pre-author
separate remediations for each individual question.
A precondition for the success of this approach is
that the system be able to identify error types based
on the student response and the model answers.
A contribution of this paper is to provide a new
dataset that will enable researchers to develop clas-
sifiers specifically for this purpose. The hope is that
with an appropriate dataset the data-driven approach
will be flexible and responsive enough to maintain
student engagement. We provide a corpus that is la-
beled for a set of five student response types, develop
a precise definition of the corresponding supervised
classification task, and report results for a variety of
simple baseline classifiers. This will provide a ba-
sis for the development, comparison and evaluation
of alternative approaches to the error classification
task. We believe that the natural language capabil-
ities needed for this task will be directly applicable
to a far wider range of tasks in educational assess-
ment, information extraction and computational se-
mantics. This dataset is publicly available and will
be used in a community-wide shared task.
2 Corpus
The data set we developed draws on two established
sources ? a data set collected and annotated during
an evaluation of the BEETLE II tutorial dialogue sys-
tem (Dzikovska et al, 2010a) (henceforth, BEETLE
corpus) and a set of student answers to questions
from 16 science modules in the Assessing Science
Knowledge (ASK) assessment inventory (Lawrence
Hall of Science, 2006) (henceforth, the Science En-
tailments Bank or SCIENTSBANK).
In both corpora, each question was associated
with one or more reference answers provided by
the experts. Student answers were evaluated against
these reference answers and, using corpus-specific
annotation schemes, assigned labels for correct-
ness. In order to reconcile the two different schemes
and to cast the task in terms of standard supervised
machine classification at the sentence level, we de-
rived a new set of annotations, using the annotation
scheme shown in Figure 1.
Our label set has some similarity to the RTE5 3-
way task (Bentivogli et al, 2009), which used ?en-
tailment?, ?contradiction? and ?unknown? labels.
The additional distinctions in our labels reflect typi-
cal distinctions made by tutorial dialogue systems.
They match our human tutors? intuitions about
the general error types observed in student answers
and corresponding teaching tactics. For example,
a likely response to ?partially correct incomplete?
would be to tell the student that what they said so far
was correct but it had some gaps, and to encourage
them to fill in those gaps. In contrast, the response
to ?contradictory? would emphasize that there is a
mistake and the student needs to change their an-
swer rather than just expand it. Finally, the response
to ?irrelevant? may encourage the student to address
relevant concepts. The ?non domain? content could
be an indicator that the student is frustrated or con-
fused, and may require special attention.
The annotations in the source corpora make some
more fine-grained distinctions based on the needs of
the corresponding systems. In principle, it is possi-
ble to have answers that have both correct and con-
tradictory parts, and acknowledge correct parts be-
fore pointing out mistakes. There are also distinct
classes of ?non domain? utterances, e.g., social and
metacognitive statements, to which an ITS may want
to react differently (described in Section 2.1). How-
ever, these situations were rare in our corpora, and
we decided to use a single class for all contradictory
answers and a single non-domain class. This may be
expanded in the future as more data becomes avail-
able for new versions of this challenge task.
201
Label Definition
non domain does not contain domain content, e.g., a help request or ?I don?t know?
correct the student answer is correct
partially correct incomplete the answer does not contradict the reference answer and includes some
correct nuggets, but parts are missing
contradictory an answer that contradicts some part of the reference answer
irrelevant contains domain content, but does not answer the question
Figure 1: The set of answer labels used in our task
We further discuss the relationship with the task
of recognizing textual entailment in Section 5. In
the rest of this section, we describe our corpora and
discuss how we obtained these labels from the raw
data available in our datasets.
2.1 BEETLE II data
The BEETLE corpus consists of the interactions be-
tween students and the BEETLE II tutorial dialogue
system (Dzikovska et al, 2010b). The BEETLE II
system is an intelligent tutoring system that teaches
students with no knowledge of high-school physics
concepts in basic electricity and electronics. In the
first system evaluation, students spend 3-5 hours go-
ing through prepared reading material, building and
observing circuits in the simulator and interacting
with a dialogue-based tutor. The interaction was
by keyboard, with the computer tutor asking ques-
tions, receiving replies and providing feedback via a
text-based chat interface. The data from 73 under-
graduate volunteer participants at southeastern US
university were recorded and annotated to form the
BEETLE human-computer dialogue corpus.
The BEETLE II lesson material contains two types
of questions. Factual questions require them to name
a set of objects or a simple property, e.g., ?Which
components in circuit 1 are in a closed path?? or
?Are bulbs A and B wired in series or in parallel?.
Explanation and definition questions require longer
answers that consist of 1-2 sentences, e.g., ?Why
was bulb A on when switch Z was open?? (expected
answer ?Because it was still in a closed path with the
battery?) or ?What is voltage?? (expected answer
?Voltage is the difference in states between two ter-
minals?). From the full BEETLE evaluation corpus,
we automatically extracted only the students? an-
swers to explanation and definition questions, since
reacting to them appropriately requires processing
more complex input than factual questions.
The extracted answers were filtered to remove du-
plicates. In the BEETLE II lesson material there
are a number of similar questions and the tutor ef-
fectively had a template answer such as ?Terminal
X is connected to the negative/positive battery ter-
minal?. A number of students picked up on this
and used the same pattern in their responses (Stein-
hauser et al, 2011). This resulted in a number of an-
swers to certain questions that came from different
speakers but which were exact copies of each other.
We removed such answers from the data set, since
they were likely to be in both the training and test
set, thus inflating our results. Note that only exact
matches were removed: for example, answers that
were nearly identical but contained spelling errors
were retained, since they would need to be handled
in a practical system.
Student utterances were manually labeled using a
simplified version of the DEMAND coding scheme
(Campbell et al, 2009) shown in Figure 2. The utter-
ances were first classified as related to domain con-
tent, student?s metacognitive state, or social inter-
action. Utterances addressing domain content were
further classified with respect to their correctness as
described in the table. The Kappa value for this
annotation effort was ? = 0.69.
This annotation maps straightforwardly into our
set of labels. The social and metacognitive state-
ments are mapped to the ?non domain? label;
?pc some error?, ?pc? and ?incorrect? are mapped
to the ?contradictory? label; and the other classes
have a one-to-one correspondence with our task la-
bels.
2.2 SCIENTSBANK data
The SCIENTSBANK corpus (Nielsen et al, 2008)
consists of student responses to science assessment
202
Category Subcategory Description
Metacognitive positive
negative
content-free expressions describing student knowledge, e.g., ?I don?t
know?
Social positive
negative
neutral
expressions describing student?s attitudes towards themselves and
the computer (mostly negative in this data, e.g., ?You are stupid?)
Content the utterance addresses domain content.
correct the student answer is fully correct
pc some missing the student said something correct, but incomplete
incorrect the student?s answer is completely incorrect
pc some error the student?s answer contains correct parts, but some errors as well
pc the answer contains a mixture of correct, incorrect and missing parts
irrelevant the answer may be correct or incorrect, but it is not answering the
question.
Figure 2: Annotation scheme used in the BEETLE corpus
questions. Specifically, around 16k answers were
collected spanning 16 distinct science subject ar-
eas within physical sciences, life sciences, earth
sciences, space sciences, scientific reasoning and
technology. The tests were part of the Berke-
ley Lawrence Hall of Science Assessing Science
Knowledge (ASK) standardized assessments cover-
ing material from their Full Option Science System
(FOSS) (Lawrence Hall of Science, 2011). The an-
swers came from students in grades 3-6 in schools
across North America.
The tests included a variety of questions includ-
ing ?fill in the blank? and multiple choice, but the
SCIENTSBANK corpus only used a subset that re-
quired students to explain their beliefs about top-
ics, typically in one to two sentences. We reviewed
the questions and a sample of the responses and
decided to filter the following types of questions
from the corpus, because they did not mesh with
our goals. First, we removed questions whose ex-
pected answer was more than two full sentences
(typically multi-step procedures), which were be-
yond the scope of our task. Second, we removed
questions where the expected answer was ill-defined
or very open-ended. Finally, the most frequent rea-
son for removing questions was an extreme imbal-
ance in the answer classifications (e.g., for many
questions, almost all of the answers were labeled
?partially correct incomplete?). Specifically, we re-
moved questions where more than 80% of the an-
swers had the same label and questions with fewer
than three correct answers, since these questions
were unlikely to be useful in differentiating between
the quality of assessment systems.
The SCIENTSBANK corpus was developed for the
purpose of assessing student responses at a very fine-
grained level. The reference answers were broken
down into several facets, which consisted roughly
of two key terms and the relation connecting them.
Nielsen et al annotated student responses to indicate
for each reference answer facet whether the response
1) implied the student understood the facet, 2) im-
plied they held a contradictory belief, 3) included a
related, non-contradicting facet, or 4) left the facet
unaddressed. Reported agreement was 86.2% with
a kappa statistic (Cohen, 1960) of 0.728, which is in
the range of substantial agreement.1
Because our task focuses on answer classifica-
tion rather than facet classification, we developed a
set of rules indicating which combinations of facets
constituted a correct answer. We were then able
to compute an answer label from the gold-standard
facet annotations, as follows. First, if any facet
was annotated as contradictory, the answer was also
labeled ?contradictory?. Second, if all of the ex-
pected facets for any valid answer were annotated
as being understood, the answer was labeled ?cor-
1These statistics were actually based on five labels, but we
chose to combine the fifth, a self-contradiction, with other con-
tradictions for the purposes of our task.
203
rect?. Third, the remaining answers that included
some but not all of the expected facets were la-
beled ?partially correct incomplete?. Fourth, if an
answer matched none of the expected facets, and
had not been previously labeled as ?contradictory? it
was given the label ?irrelevant?. Finally, all ?irrele-
vant? answers were reviewed manually to determine
whether they should be relabeled as ?non domain?.
However, since Nielsen et al had already removed
most of the responses that originally fell into this
category, we only found 24 ?non domain? answers.
3 Baselines
We established three baselines for our data set ? a
straightforward majority class baseline, an existing
system baseline (BEETLE II system performance,
which we report only for the BEETLE portion of the
dataset), and the performance of a simple classifier
based on lexical similarity, which we report in order
to offer a substantial example of applying the same
classifier to both portions of the dataset.
3.1 BEETLE II system baseline
The interpretation component of the BEETLE II
system uses a syntactic parser and a set of hand-
authored rules to extract the domain-specific se-
mantic representations of student utterances from
the text. These representations were then matched
against the semantic representations of expected cor-
rect answers supplied by tutors. The resulting sys-
tem output was automatically mapped into our target
labels as discussed in (Dzikovska et al, 2012).
3.2 Lexical similarity baseline
To provide a higher baseline that is compara-
ble across both subsets of the data, we built
a simple decision tree classifier using the Weka
3.6.2 implementation of C4.5 pruned decision trees
(weka.classifiers.trees.J48 class), with default pa-
rameters. As features, we used lexical similar-
ity scores computed by the Text::Similarity
package with default parameters2. The code com-
putes four similarity metrics ? the raw number of
overlapping words, F1 score, Lesk score and cosine
score. We compared the learner response to the ex-
pected answer(s) and the question, resulting in eight
2http://search.cpan.org/dist/Text-Similarity/
total features (the four values indicated above for
the comparison with the question and the highest of
each value from the comparisons with each possible
expected answer).
This baseline is based on the lexical overlap base-
line used in RTE tasks (Bentivogli et al, 2009).
However, we measured overlap with the question
text in addition to the overlap with the expected
answers. Students often repeat parts of the ques-
tion in their answer and this needs to be taken
into account to differentiate, for example, ?par-
tially correct incomplete? and ?correct? answers.
4 Results
4.1 Experimental Setup
We held back part of the data set for use as standard
test data in the future challenge tasks. For BEETLE,
this consisted of all student answers to 9 out of 56
explanation questions asked by the system, plus ap-
proximately 15% of the student answers to the re-
maining 47 questions, sampling so that the distribu-
tion of labels in test data was similar to the training
data. For SCIENTSBANK, we used a previous train-
test split (Nielsen et al, 2009). For both data sets,
the data was split so that in the future we can test
how well the different systems generalize: i.e., how
well they perform on answers to questions for which
they have some sample student answers vs. how
well they perform on answers to questions that were
not in the training data (e.g., newly created questions
in a deployed system). We discuss this in more detail
in Section 5.
In this paper, we report baseline performance on
the training set to demonstrate that the task is suf-
ficiently challenging to be interesting and that sys-
tems can be compared using our evaluation met-
rics. We preserve the true test data for use in the
planned large-scale system comparisons in a com-
munity shared task.
For the lexical similarity baseline, we use 10-fold
cross-validation.3 For the BEETLE II system base-
line, the language understanding module was de-
3We did not take the student id into account explicitly during
cross-validation. While there is some risk that the classifiers
will learn features specific to the student, we concluded (based
on our understanding of data collection specifics for both data
sets) that there is little enough overlap in cross-validation on the
training data that this should not have a big effect on the results.
204
veloped based on eight transcripts, each taken from
the interaction of a different student with an earlier
version of the system. These sessions were com-
pleted prior to the beginning of the experiment dur-
ing which the BEETLE corpus was collected, and are
not included in the corpus presented here. Thus, the
dataset used in the paper constitutes unseen data for
the BEETLE II system.
We process the two corpora separately because
the additional system baseline is available for bee-
tle, and because the corpora may be different enough
that it will be helpful for shared task participants to
devise processing strategies that are sensitive to the
provenance of the data.
4.2 Evaluation Metrics
Table 1 shows the distribution of codes in the anno-
tated data. The distribution is unbalanced, and there-
fore in our evaluation results we report per-class pre-
cision, recall and F1 scores, plus the averaged scores
using two different ways to average over per-class
evaluation scores, micro- and macro- averaging.
For a set of classes C, each represented with Nc
instances in the test set, the macro-averaged recall is
defined as
Rmacro =
1
|C|
?
cC
R(c)
and the micro-averaged recall as
Rmicro =
?
cC
1
Nc
R(c)
Micro- and macro-averaged precision and F1 are de-
fined similarly.
Micro-averaging takes class sizes into account, so
a system that performs well on the most common
classes will have a high micro-average score. This is
the most commonly used classifier evaluation met-
ric. Note that, in particular, overall classification
accuracy (defined as the number of correctly clas-
sified instances out of all instances) is mathemat-
ically equivalent to micro-averaged recall (Abuda-
wood and Flach, 2011). However, macro-averaging
better reflects performance on small classes, and is
commonly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)). We report both val-
ues in our results.
BEETLE SCIENTSBANK
Label Count Freq. Count Freq.
correct 1157 0.42 2095 0.40
partially correct
incomplete
626 0.23 1431 0.27
contradictory 656 0.24 526 0.10
irrelevant 86 0.03 1175 0.22
non domain 204 0.07 24 0.005
total 2729 5251
Table 1: Distribution of annotated labels in the data
In addition, we report the system scores on the bi-
nary decision of whether or not the corrective feed-
back should be issued (denoted ?corrective feed-
back? in the results table). It assumes that a tutoring
system using a classifier will give corrective feed-
back if the classifiers returns any label other than
?correct?. Thus, every instance classified as ?par-
tially correct incomplete?, ?contradictory?, ?irrele-
vant? or ?non domain? is counted as true positive
if the hand-annotated label also belongs to this set
(even if the classifier disagrees with the annotation);
and as false positive if the hand-annotated label is
?correct?. This reflects the idea that students are
likely to be frustrated if the system gives corrective
feedback when their answer is in fact a fully accurate
paraphrase of a correct answer.
4.3 BEETLE baseline performance
The detailed evaluation results for all baselines are
presented in Table 2.
The majority class baseline is to assign ?correct?
to every test instance. It achieves 42% overall ac-
curacy. However, this is obviously at the expense
of serious errors; for example, such a system would
tell the students that they are correct if they are say-
ing something contradictory. This is reflected in a
much lower macro-averaged F1 score.
The BEETLE II system performs only slightly bet-
ter than the baseline on the overall accuracy (0.44
vs. 0.42 micro-averaged recall). However, the
macro-averaged F1 score of the BEETLE II system
is substantially higher (0.46 vs. 0.12). The micro-
averaged results show a similar pattern, although the
majority-class baseline performs slightly better than
in the macro-averaged case, as expected.
Comparing the BEETLE II parser to our lexical
205
similarity baseline, BEETLE II has lower overall ac-
curacy, but performs similarly on micro- and macro-
averaged scores. BEETLE II precision is higher than
that of the classifier in all cases except for the binary
decision as to whether corrective feedback should
be issued. This is not unexpected given how the sys-
tem was designed ? since misunderstandings caused
dialogue breakdown in pilot tests, the parser was
built to prefer rejecting utterances as uninterpretable
rather than assigning them an incorrect class, lead-
ing to a considerably lower recall. Around 31% of
utterances could not be interpreted.
Our recent analysis shows that both incorrect
interpretations (in particular, confusions between
?partially correct incomplete? and ?contradictory?)
and rejections have significant negative effects on
learning gain (Dzikovska et al, 2012). Classifiers
can be tuned to reject examples where classification
confidence falls below a given threshold, resulting
in precision-recall trade-offs. Our baseline classifier
classified all answer instances; exploring the possi-
bilities for rejecting some low-confidence answers is
planned for future work.
4.4 SCIENTSBANK baseline performance
The accuracy of the majority class baseline (which
assumes all answers are ?correct?) is 40% for SCI-
ENTSBANK, about the same as it was for BEE-
TLE. The evaluation results, based on 10-fold cross-
validation, for our simple lexical similarity classi-
fier are presented in Table 3. The lexical similar-
ity based classifier outperforms the majority class
baseline by 0.18 and 3% on the macro-averaged
F1-measure and accuracy, respectively. The F1-
measure for the two-way classification detecting an-
swers which need corrective feedback is 0.66.
The scores on SCIENTSBANK are noticeably
lower than those for BEETLE. The SCIENTSBANK
includes questions from 12 distinct science subject
areas, rather than a single area as in BEETLE. This
decision tree classifier learns a function from the
eight text similarity features to the desired answer
label. Because the features do not mention particular
words, the model can be applied to items other than
the ones on which it was trained, and even to items
from different subject areas. However, the correct
weighting of the textual similarity features depends
on the extent and nature of the expected textual over-
Predictn correct pc inc contra irrlvnt nondom
correct 1213 553 209 392 2
pc inc 432 497 128 241 2
contra 115 109 58 74 3
irrlvnt 335 272 131 468 17
nondom 0 0 0 0 0
Figure 4: Confusion matrix for lexical classifier on SCI-
ENTSBANK. Predictions in rows, gold labels in columns
lap, which does vary from subject-area to subject-
area. We suspect that the differences between sub-
ject areas made it hard for the decision-tree classi-
fier to find a single, globally appropriate strategy.
Nielsen (2009) reported the best results for classify-
ing facets when training separate question-specific
or even facet-specific classifiers. Although separate
training for each item reduces the amount of relevant
training data for each classifier, it allows each clas-
sifier to learn the specifics of how that item works.
A comparison using this style of training would be a
reasonable next step,
5 Discussion and Future Work
The results presented satisfy two critical require-
ments for a challenge task. First, we have shown that
it is feasible to develop a system that performs sig-
nificantly better than the majority class baseline. On
the macro-averaged F1-measure, our lexical clas-
sifier outperformed the majority-class baseline by
0.33 (on BEETLE) and 0.18 (on SCIENTSBANK)
and by 13% and 3% on accuracy. Second, we have
also shown, as is desired for a challenge task, that
the task is not trivial. With a system specifically
designed to parse the BEETLE corpus answers, the
macro-averaged F1-measure was just 0.46 and on
the binary decision regarding whether the response
needed corrective feedback, it achieved just 0.63.
One contribution of this work was to define a gen-
eral classification scheme for student responses that
allows more specific learner feedback. Another key
contribution was to unify two, previously incom-
patible, large student response corpora under this
common annotation scheme. The resultant corpus
will enable researchers to train learning algorithms
to classify student responses. These classifications
can then be used by a dialogue manager to generate
targeted learner feedback. The corpus is available
206
Classifier: majority lexical similarity BEETLE II
Predicted label prec. recall F1 prec. recall F1 prec. recall F1
correct 0.42 1.00 0.60 0.68 0.75 0.72 0.93 0.53 0.68
partially correct incomplete 0.00 0.00 0.00 0.41 0.38 0.39 0.43 0.53 0.47
contradictory 0.00 0.00 0.00 0.39 0.34 0.36 0.58 0.23 0.33
irrelevant 0.00 0.00 0.00 0.05 0.02 0.03 0.23 0.17 0.20
non domain 0.00 0.00 0.00 0.66 0.82 0.73 0.92 0.46 0.61
macroaverage 0.09 0.20 0.12 0.44 0.46 0.45 0.62 0.39 0.46
microaverage 0.18 0.42 0.25 0.53 0.55 0.54 0.71 0.44 0.53
corrective feedback 0.00 0.00 0.00 0.80 0.74 0.77 0.73 0.56 0.63
Table 2: Evaluation results for BEETLE corpus
Classifier: lexical similarity BEETLE II
Predicted label corrct pc inc contra irrlvnt nondom corrct pc inc contra irrlvnt nondom
correct 870 187 199 20 2 617 20 23 0 3
part corr incmp 138 239 178 24 11 249 332 146 29 20
contradictory 139 153 221 33 22 68 38 149 3 0
irrelevant 3 20 12 2 1 4 22 23 15 1
non domain 7 27 46 7 168 3 3 1 1 94
uninterpretable n/a n/a n/a n/a n/a 216 211 314 38 86
Figure 3: Confusion matrix for BEETLE corpus. Predictions in rows, gold labels in columns
Classifier: baseline lexical similarity
Predicted label prec. recall F1 prec. recall F1
correct 0.40 1.00 0.57 0.51 0.58 0.54
partially correct incomplete 0.00 0.00 0.00 0.38 0.35 0.36
contradictory 0.00 0.00 0.00 0.16 0.11 0.13
irrelevant 0.00 0.00 0.00 0.38 0.40 0.39
non domain 0.00 0.00 0.00 0.00 0.00 0.00
macroaverage 0.08 0.20 0.11 0.29 0.29 0.29
microaverage 0.16 0.40 0.23 0.41 0.43 0.42
corrective feedback 0.00 0.00 0.00 0.69 0.63 0.66
Table 3: Evaluation results for SCIENTSBANK baselines
207
for general research purposes and forms the basis
of SEMEVAL-2013 shared task ?Textual entailment
and paraphrasing for student input assessment?.4
A third contribution of this work was to provide
basic evaluation benchmark metrics and the corre-
sponding evaluation scripts (downloadable from the
site above) for other researchers, including shared
task participants. This will facilitate the comparison
and, hence, the progress, of research.
The work reported here is based on approximately
8000 student responses to questions covering 12 dis-
tinct science subjects and coming from a wide range
of student ages. These responses comprise the train-
ing data for our task. The vast majority of prior
work, including BEETLE II, which was included as
a benchmark here, has been designed to provide ITS
feedback for relatively small, well-defined domains.
The corpus presented in this paper is intended to en-
courage research into more generalizable, domain-
independent techniques. Following Nielsen (2009),
from whom the SCIENTSBANK corpus was adapted,
our shared task evaluation corpus will be composed
of three types of data: additional student responses
for all of the questions in the training data (Un-
seen Answers), student responses to questions that
were not seen in the training data, but that are from
the same subject areas (Unseen Questions), and re-
sponses to questions from three entirely different
subject areas (Unseen Domains), though in this case
the questions are still from the same general domain
? science. Unseen Answers is the typical scenario
for the vast majority of prior work ? training and
testing on responses to the same questions. Unseen
Questions and Unseen Domains allow researchers to
evaluate how well their systems generalize to near
and far domains, respectively.
The primary target application for this work is in-
telligent tutoring systems, where the classification of
responses is intended to facilitate specific pedagogic
feedback. Beneath the surface, the baseline systems
reported here are more similar to grading systems
that use the approach of (Leacock and Chodorow,
2003), which uses classifier technology to detect ex-
pressions of facet-like concepts, then converts the
result to a numerical score, than to grading systems
like (Mohler et al, 2011), which directly produces a
4See http://www.cs.york.ac.uk/semeval-2013/task4/
numerical score, using support vector regression and
similar techniques. Either approach is reasonable,
but we think that feedback is the more challeng-
ing test of a system?s ultimate abilities, and there-
fore a better candidate for the shared task. The cor-
pora from those systems, alongside with new cor-
pora currently being collected in BEETLE and SCI-
ENTSBANK domains, can serve as sources of data
for future tasks extensions.
Future systems developed for this task can benefit
from the large amount of existing work on recog-
nizing textual entailment (Giampiccolo et al, 2007;
Giampiccolo et al, 2008; Bentivogli et al, 2009)
and on detecting contradiction (Ritter et al, 2008;
De Marneffe et al, 2008). However, there are sub-
stantial challenges in applying the RTE tools directly
to this data set. Our set of labels is more fine-grained
than RTE labels to reflect the needs of intelligent tu-
toring systems (see Section 2). In addition, the top-
performing systems in RTE5 3-way task, as well as
contradiction detection methods, rely on NLP tools
such as dependency parsers and semantic role la-
belers; these do not perform well on specialized
terminology and language constructs coming from
(typed) dialogue context. We chose to use lexical
similarity as a baseline specifically because a simi-
lar measure was used as a standard baseline in RTE
tasks, and we expect that adapting the more complex
RTE approaches for purposes of this task will result
in both improved results on our data set and new de-
velopments in computational linguistics algorithms
used for RTE and related tasks.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation. The research
reported here was supported by the US ONR award
N000141010085 and by the Institute of Education
Sciences, U.S. Department of Education, through
Grant R305A110811 to Boulder Language Tech-
nologies Inc. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education.
208
References
Tarek Abudawood and Peter Flach. 2011. Learn-
ing multi-class theories in ilp. In The 20th Interna-
tional Conference on Inductive Logic Programming
(ILP?10). Springer, June.
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
Vincent Aleven, Octav Popescu, and Koedinger
Koedinger. 2002. Pilot-testing a tutorial dialogue
system that supports self-explanation. Lecture Notes
in Computer Science, 2363:344?354.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Charles Callaway, Myroslava Dzikovska, Colin Mathe-
son, Johanna Moore, and Claus Zinn. 2006. Using
dialogue to learn math in the LeActiveMath project.
In Proceedings of the ECAI Workshop on Language-
Enhanced Educational Technology, pages 1?8, Au-
gust.
Gwendolyn C. Campbell, Natalie B. Steinhauser, My-
roslava O. Dzikovska, Johanna D. Moore, Charles B.
Callaway, and Elaine Farrow. 2009. The DeMAND
coding scheme: A ?common language? for represent-
ing and analyzing student discourse. In Proceedings
of 14th International Conference on Artificial Intelli-
gence in Education (AIED), poster session, Brighton,
UK, July.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):3746.
M.C. De Marneffe, A.N. Rafferty, and C.D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL-08: HLT, pages 1039?1047.
Myroslava Dzikovska, Diana Bental, Johanna D. Moore,
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Elaine Farrow, and Charles B. Callaway. 2010a. In-
telligent tutoring with natural language support in the
Beetle II system. In Sustaining TEL: From Innovation
to Learning and Practice - 5th European Conference
on Technology Enhanced Learning, (EC-TEL 2010),
Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
2010) demo session, Uppsala, Sweden, July.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
computational Linguistics, Avignon, France, April.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9, Prague, June.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pro-
ceedings of the AAAI Fall Symposium on Building Di-
alogue Systems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, K. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language un-
derstanding approaches in an intelligent tutoring sys-
tem. In James C. Lester, Rosa Maria Vicari, and
Fa?bio Paraguac?u, editors, Intelligent Tutoring Systems,
volume 3220 of Lecture Notes in Computer Science,
pages 346?357. Springer.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Pamela W. Jordan. 2004. Using student explanations
as models for adapting tutorial dialogue. In Valerie
Barr and Zdravko Markov, editors, FLAIRS Confer-
ence. AAAI Press.
Lawrence Hall of Science. 2006. Assessing Science
Knowledge (ask). University of California at Berke-
ley, NSF-0242510.
Lawrence Hall of Science. 2011. Full option science
system.
209
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
David D. Lewis. 1991. Evaluating text categorization. In
Proceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 312?318, Stroudsburg, PA,
USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proceedings of 14th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Brighton, UK, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. The Journal of Natural Language Engineer-
ing, 15:479?501.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proceedings of ITS-
2004, pages 390?400.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction?no, it?s not: a case study
using functional relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 11?20.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
210
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 321?326,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Linguistic Considerations in Automatic Question Generation
Karen Mazidi
HiLT Lab
University of North Texas
Denton TX 76207, USA
KarenMazidi@my.unt.edu
Rodney D. Nielsen
HiLT Lab
University of North Texas
Denton TX 76207, USA
Rodney.Nielsen@unt.edu
Abstract
As students read expository text, compre-
hension is improved by pausing to answer
questions that reinforce the material. We
describe an automatic question generator
that uses semantic pattern recognition to
create questions of varying depth and type
for self-study or tutoring. Throughout, we
explore how linguistic considerations in-
form system design. In the described sys-
tem, semantic role labels of source sen-
tences are used in a domain-independent
manner to generate both questions and an-
swers related to the source sentence. Eval-
uation results show a 44% reduction in the
error rate relative to the best prior systems,
averaging over all metrics, and up to 61%
reduction in the error rate on grammatical-
ity judgments.
1 Introduction
Studies of student learning show that answering
questions increases depth of student learning, fa-
cilitates transfer learning, and improves students?
retention of material (McDaniel et al, 2007; Car-
penter, 2012; Roediger and Pyc, 2012). The aim
of this work is to automatically generate questions
for such pedagogical purposes.
2 Related Work
Approaches to automatic question generation from
text span nearly four decades. The vast ma-
jority of systems generate questions by select-
ing one sentence at a time, extracting portions
of the source sentence, then applying transfor-
mation rules or patterns in order to construct a
question. A well-known early work is Wolfe?s
AUTOQUEST (Wolfe, 1976), a syntactic pattern
matching system. A recent approach from Heil-
man and Smith (2009, 2010) uses syntactic pars-
ing and transformation rules to generate questions.
Syntactic, sentence-level approaches outnumber
other approaches as seen in the Question Gen-
eration Shared Task Evaluation Challenge 2010
(Boyer and Piwek, 2010) which received only one
paragraph-level, semantic entry. Argawal, Shah
and Mannem (2011) continue the paragraph-level
approach using discourse cues to find appropriate
text segments upon which to construct questions
at a deeper conceptual level. The uniqueness of
their work lies in their use of discourse cues to
extract semantic content for question generation.
They generate questions of types: why, when, give
an example, and yes/no.
In contrast to the above systems, other ap-
proaches have an intermediate step of transform-
ing input into some sort of semantic represen-
tation. Examples of this intermediate step can
be found in Yao and Zhang (2010) which uses
Minimal Recursive Semantics, and in Olney et
al. (2012) which uses concept maps. These ap-
proaches can potentially ask deeper questions due
to their focus on semantics. A novel question gen-
erator by Curto et al (2012) leverages lexico-
syntactic patterns gleaned from the web with seed
question-answer pairs.
Another recent approach is Lindberg et al
(2013), which used semantic role labeling to iden-
tify patterns in the source text from which ques-
tions can be generated. This work most closely
parallels our own with a few exceptions: our sys-
tem only asks questions that can be answered
from the source text, our approach is domain-
independent, and the patterns also identify the an-
swer to the question.
3 Approach
The system consists of a straightforward pipeline.
First, the source text is divided into sentences
which are processed by SENNA
1
software, de-
1
http://ml.nec-labs.com/senna/
321
scribed in (Collobert et al, 2011). SENNA pro-
vides the tokenizing, pos tagging, syntactic con-
stituency parsing and semantic role labeling used
in the system. SENNA produces separate seman-
tic role labels for each predicate in the sentence.
For each predicate and its associated semantic ar-
guments, a matcher function is called which will
return a list of patterns that match the source sen-
tence?s predicate-argument structure. Then ques-
tions are generated and stored by question type in
a question hash table.
Generation patterns specify the text, verb forms
and semantic arguments from the source sentence
to form the question. Additionally, patterns indi-
cate the semantic arguments that provide the an-
swer to the question, required fields, and filter con-
dition fields. As these patterns are matched, they
will be rejected as candidates for generation for a
particular sentence if the required arguments are
absent or if filter conditions are present. For ex-
ample, a filter for personal pronouns will prevent
a question being generated with an argument that
starts with a personal pronoun. From: It means
that the universe is expanding, we do not want to
generate a vague question such as: What does it
mean? Coreference resolution, which could help
avoid vague question generation, is discussed in
Section 5. Table 1 shows selected required and fil-
ter fields, Section 3.3 gives examples of their use.
Patterns specify whether verbs should be in-
cluded in their lexical form or as they appear in the
source text. Either form will include subsequent
particles such as: The lungs take in air. The most
common use of the verb as it appears in the sen-
tence is with the verb be, as in: What were fused
into helium nuclei? This pattern takes the copu-
lar be as it appears in the source text. However,
most patterns use the lexical form of the main verb
along with the appropriate form of the auxiliary do
(do, does, did), for the subject-auxiliary inversion
required in forming interrogatives.
3.1 Pattern Authoring
The system at the time of this evaluation had 42
patterns. SENNA uses the 2005 PropBank cod-
ing scheme and we followed the documentation in
(Babko-Malaya, 2005) for the patterns. The most
commonly used semantic roles are A0, A1 and A2,
as well as the ArgM modifiers.
2
2
Within PropBank, the precise roles of A0 - A6 vary by
predicate.
Field Meaning
Ax Sentence must contain an Ax
!Ax Sentence must not contain an Ax
AxPER Ax must refer to a person
AxGER Ax must contain a gerund
AxNN Ax must contain nouns
!AxIN Ax cannot start with a preposition
!AxPRP Ax cannot start with per. pronoun
V=verb Verb must be a form of verb
!be Verb cannot be a form of be
negation Sentence cannot contain negation
Table 1: Selected required and filter fields (Ax is a
semantic argument such as A0 or ArgM)
3.2 Software Tools and Source Text
The system was created using SENNA and
Python. Importing NLTK within Python provides
a simple interface to WordNet from which we de-
termine the lexical form of verbs. SENNA pro-
vided all the necessary processing of the data,
quickly, accurately and in one run.
In order to generate questions, passages were
selected from science textbooks downloaded from
www.ck12.org. Textbooks were chosen rather
than hand-crafted source material so that a more
realistic assessment of performance could be
achieved. For the experiments in this paper, we
selected three passages from the subjects of bi-
ology, chemistry, and earth science, filtering out
references to equations and figures. The passages
average around 60 sentences each, and represent
chapter sections. The average grade level is ap-
proximately grade 10 as indicated by the on-line
readability scorer read-able.com.
3.3 Examples
Table 2 provides examples of generated questions.
The pattern that generated Question 1 requires ar-
gument A1 (underlined in Table 2) and a causation
ArgM (italicized). The pattern also filters out sen-
tences with A0 or A2. The patterns are designed
to match only the arguments used as part of the
question or the answer, in order to prevent over
generation of questions. The system inserted the
correct forms of release and do, and ignored the
phrase As this occurs since it is not part of the se-
mantic argument.
The pattern that generated Question 2 requires
A0, A1 and a verb whose lexical form is mean
(V=mean in Table 1). In this pattern, A1 (itali-
322
Question 1: Why did potential energy release?
Answer: because the new bonds have lower potential energy than the original bonds
Source: As this occurs, potential energy is released because the new bonds have lower potential
energy than the original bonds.
Question 2: What does an increased surface area to volume ratio indicate?
Answer: increased exposure to the environment
Source: An increased surface area to volume ratio means increased exposure to the environment.
Question 3: What is another term for electrically neutral particles?
Answer: neutrons
Source: The nucleus contains positively charged particles called protons and
electrically neutral particles called neutrons.
Question 4: What happens if you continue to move atoms closer and closer together?
Answer: eventually the two nuclei will begin to repel each other
Source: If you continue to move atoms closer and closer together, eventually the two nuclei will
begin to repel each other.
Table 2: Selected generated questions with source sentences
cized) forms the answer and A0 (underlined) be-
comes part of the question along with the appro-
priate form of do. This pattern supplies the word
indicate instead of the source text?s mean which
broadens the question context.
Question 3 is from the source sentence?s 3rd
predicate-argument set because this matched the
pattern requirements: A1, A2, V=call. The answer
is the text from the A2 argument. The ability to
generate questions from any predicate-argument
set means that sentence simplification is not re-
quired as a preprocessing step, and that the sen-
tence can match multiple patterns. For example,
this sentence could also match patterns to gener-
ate questions such as: What are positively charged
particles called? or Describe the nucleus.
Question 4 requires A1 and an ArgM that in-
cludes the discourse cue if. The ArgM (under-
lined) becomes part of the question, while the rest
of the source sentence forms the answer. This pat-
tern also requires that ArgM contain nouns (AxNN
from Table 1), which helps filter vague questions.
4 Results
This paper focuses on evaluating generated ques-
tions primarily in terms of their linguistic quality,
as did Heilman and Smith (2010a). In a related
work (Mazidi and Nielsen, 2014) we evaluated
the quality of the questions and answers from a
pedagogical perspective, and our approach outper-
formed comparable systems in both linguistic and
pedagogical evaluations. However, the task here
is to explore the linguistic quality of generated
questions. The annotators are university students
who are science majors and native speakers of En-
glish. Annotators were given instructions to read a
paragraph, then the questions based on that para-
graph. Two annotators evaluated each set of ques-
tions using Likert-scale ratings from 1 to 5, where
5 is the best rating, for grammaticality, clarity, and
naturalness. The average inter-annotator agree-
ment, allowing a difference of one between the
annotators? ratings was 88% and Pearson?s r=0.47
was statistically significant (p<0.001), suggesting
a high correlation and agreement between annota-
tors. The two annotator ratings were averaged for
all the evaluations reported here.
We present results on three linguistic evalua-
tions: (1) evaluation of our generated questions,
(2) comparison of our generated questions with
those from Heilman and Smith?s question gener-
ator, and (3) comparison of our generated ques-
tions with those from Lindberg, Popowich, Nesbit
and Winne. We compared our system to the H&S
and LPN&W systems because they produce ques-
tions that are the most similar to ours, and for the
same purpose: reading comprehension reinforce-
ment. The Heilman and Smith system is available
online;
3
Lindberg graciously shared his code with
us.
4.1 Evaluation of our Generated Questions
This evaluation was conducted with one file
(Chemistry: Bonds) which had 59 sentences, from
which the system generated 142 questions. The
3
http://www.ark.cs.cmu.edu/mheilman/questions/
323
purpose of this evaluation was to determine if any
patterns consistently produce poor questions. The
average linguistics score per pattern in this evalu-
ation was 5.0 to 4.18. We were also interested to
know if first predicates make better questions than
later ones. The average score by predicate position
is shown in Table 3. Note that the Rating column
gives the average of the grammaticality, clarity and
naturalness scores.
Predicate Questions Rating
First 58 4.7
Second 35 4.7
Third 23 4.5
Higher 26 4.6
Table 3: Predicate depth and question quality
Based on this sample of questions there is
no significant difference in linguistic scores for
questions generated at various predicate positions.
Some question generation systems simplify com-
plex sentences in initial stages of their system. In
our approach this is unnecessary, and simplifying
could miss many valid questions.
4.2 Comparison with Heilman and Smith
This task utilized a file (Biology: the body) with
56 source sentences from which our system gener-
ated 102 questions. The Heilman and Smith sys-
tem, as they describe it, takes an over-generate and
rank approach. We only took questions that scored
a 2.0 or better with their ranking system,
4
which
resulted in less than 27% of their top questions.
In all, 84 of their questions were evaluated. The
questions again were presented with accompany-
ing paragraphs of the source text. Questions from
the two systems were randomly intermingled. An-
notators gave 1 - 5 scores for each category of
grammaticality, clarity and naturalness.
As seen in Table 4, our results represent a 44%
reduction in the error rate relative to Heilman and
Smith on the average rating over all metrics, and
as high as 61% reduction in the error rate on gram-
maticality judgments. The error reduction calcu-
lation is shown below. Note that rating
?
is the
maximum rating of 5.0.
rating
system2
? rating
system1
rating
?
? rating
system1
? 100.0 (1)
4
In our experiments, their rankings ranged from very
small negative numbers to 3.0.
System Gram Clarity Natural Avg
H&S 4.38 4.13 3.94 4.15
M&N 4.76 4.26 4.53 4.52
Err. Red. 61% 15% 56% 44%
Table 4: Comparison with Heilman and Smith
System Gram Clarity Natural Avg
LPN&W 4.57 4.56 4.55 4.57
M&N 4.80 4.69 4.78 4.76
Err. Red. 54% 30% 51% 44%
Table 5: Comparison with Lindberg et al
4.3 Comparison with Lindberg et al
For a comparison with the Lindberg, Popowich,
Nesbit and Winne system we used a file (Earth
science: weather fronts) that seemed most sim-
ilar to the text files for which their system was
designed. The file has 93 sentences and our sys-
tem generated 184 questions; the LPN&W sys-
tem generated roughly 4 times as many questions.
From each system, 100 questions were randomly
selected, making sure that the LPN&W questions
did not include questions generated from domain-
specific templates such as: Summarize the influ-
ence of the maximum amount on the environment.
The phrases Summarize the influence of and on
the environment are part of a domain-specific tem-
plate. The comparison results are shown in Table
5. Interestingly, our system again achieved a 44%
reduction in the error rate when averaging over all
metrics, just as it did in the Heilman and Smith
comparison.
5 Linguistic Challenges
Natural language generation faces many linguistic
challenges. Here we briefly describe three chal-
lenges: negation detection, coreference resolution,
and verb forms.
5.1 Negation Detection
Negation detection is a complicated task because
negation can occur at the word, phrase or clause
level, and because there are subtle shades of nega-
tion between definite positive and negative polar-
ities (Blanco and Moldovan, 2011). For our pur-
poses we focused on negation as identified by the
NEG label in SENNA which identified not in verb
phrases. We have left for future work the task of
324
identifying other negative indicators, which occa-
sionally does lead to poor question/answer quality
as in the following:
Source sentence: In Darwin?s time and to-
day, many people incorrectly believe that evolu-
tion means humans come from monkeys.
Question: What does evolution mean?
Answer: that humans come from monkeys
The negation in the word incorrectly is not iden-
tified.
5.2 Coreference Resolution
Currently, our system does not use any type of
coreference resolution. Experiments with existing
coreference software performed well only for per-
sonal pronouns, which occur infrequently in most
expository text. Not having coreference resolution
leads to vague questions, some of which can be
filtered as discussed previously. However, further
work on filters is needed to avoid questions such
as:
Source sentence: Air cools when it comes into
contact with a cold surface or when it rises.
Question: What happens when it comes into
contact with a cold surface or when it rises?
Heilman and Smith chose to filter out ques-
tions with personal pronouns, possessive pronouns
and noun phrases composed simply of determiners
such as those. Lindberg et al used the emPronoun
system from Charniak and Elsner, which only han-
dles personal pronouns. Since current state-of-the-
art systems do not deal well with relative and pos-
sessive pronouns, this will continue to be a limi-
tation of natural language generation systems for
the time being.
5.3 Verb Forms
Since our focus is on expository text, system pat-
terns deal primarily with the present and simple
past tenses. Some patterns look for modals and so
can handle future tense:
Source sentence: If you continue to move
atoms closer and closer together, eventually the
two nuclei will begin to repel each other.
Question: Discuss what the two nuclei will re-
pel.
Light verbs pose complications in NLG because
they are highly idiosyncratic and subject to syn-
tactic variability (Sag et al, 2002). Light verbs
can either carry semantic meaning (take your pass-
port) or can be bleached of semantic content when
combined with other words as in: make a deci-
sion, have a drink, take a walk. Common English
verbs that can be light verbs include give, have,
make, take. Handling these constructions as well
as other multi-word expressions may require both
rule-based and statistical approaches. The catena-
tive construction also potentially adds complexity
(Huddleston and Pullum, 2005), as shown in this
example: As the universe expanded, it became less
dense and began to cool. Care must be taken not
to generate questions based on one predicate in the
catenative construction.
We are also hindered at times by the perfor-
mance of the part of speech tagging and parsing
software. The most common error observed was
confusion between the noun and verb roles of a
word. For example in: Plant roots and bacterial
decay use carbon dioxide in the process of respira-
tion, the word use was classified as NN, leaving no
predicate and no semantic role labels in this sen-
tence.
6 Conclusions
Roediger and Pyc (2012) advocate assisting stu-
dents in building a strong knowledge base be-
cause creative discoveries are unlikely to occur
when students do not have a sound set of facts
and principles at their command. To that end, au-
tomatic question generation systems can facilitate
the learning process by alternating passages of text
with questions that reinforce the material learned.
We have demonstrated a semantic approach to
automatic question generation that outperforms
similar systems. We evaluated our system on
text extracted from open domain STEM textbooks
rather than hand-crafted text, showing the robust-
ness of our approach. Our system achieved a 44%
reduction in the error rate relative to both the Heil-
man and Smith, and the Lindberg et al system on
the average over all metrics. The results shows are
statistically significant (p<0.001). Our question
generator can be used for self-study or tutoring,
or by teachers to generate questions for classroom
discussion or assessment. Finally, we addressed
linguistic challenges to question generation.
Acknowledgments
This research was supported by the Institute of
Education Sciences, U.S. Dept. of Ed., Grant
R305A120808 to UNT. The opinions expressed
are those of the authors.
325
References
Agarwal, M., Shah, R., and Mannem, P. 2011. Auto-
matic question generation using discourse cues. In
Proceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications, As-
sociation for Computational Linguistics.
Babko-Malaya, O. 2005. Propbank annotation guide-
lines. URL: http://verbs.colorado.edu
Blanco, E., and Moldovan, D. 2011. Some issues on
detecting negation from text. In FLAIRS Confer-
ence.
Boyer, K. E., and Piwek, P., editors. 2010. In Proceed-
ings of QG2010: The Third Workshop on Question
Generation. Pittsburgh: questiongeneration.org
Carpenter, S. 2012. Testing enhances the transfer of
learning. In Current directions in psychological sci-
ence, 21(5), 279-283.
Charniak, E., and Elsner, M. 2009. EM works for pro-
noun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., & Kuksa, P. 2011. Natural lan-
guage processing (almost) from scratch. The Jour-
nal of Machine Learning Research, 12, 2493-2537.
Curto, S., Mendes, A., and Coheur, L. 2012. Ques-
tion generation based on lexico-syntactic patterns
learned from the web. Dialogue & Discourse, 3(2),
147-175.
Heilman, M., and Smith, N. 2009. Question gener-
ation via overgenerating transformations and rank-
ing. Technical Report CMU-LTI-09-013, Language
Technologies Institute, Carnegie-Mellon University.
Heilman, M., and Smith, N. 2010a. Good ques-
tion! statistical ranking for question generation. In
Proceedings of NAACL/HLT 2010. Association for
Computational Linguistics.
Heilman, M., and Smith, N. 2010b. Rating computer-
generated questions with Mechanical Turk. In Pro-
ceedings of the NAACL-HLT Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk. Association for Computational Linguis-
tics.
Huddleston, R. and Pullum, G. 2005. A Student?s In-
troduction to English Grammar, Cambridge Univer-
sity Press.
Lindberg, D., Popowich, F., Nesbit, J., and Winne, P.
2013. Generating natural language questions to sup-
port learning on-line. In Proceedings of the 14th Eu-
ropean Workshop on Natural Language Generation,
(2013): 105-114.
Mannem, P., Prasad, R. and Joshi, A. 2010. Question
generation from paragraphs at UPenn: QGSTEC
system description. In Proceedings of QG2010: The
Third Workshop on Question Generation.
Mazidi, K. and Nielsen, R.D. 2014. Pedagogical eval-
uation of automatically generated questions. In In-
telligent Tutoring Systems. LNCS 8474, Springer In-
ternational Publishing Switzerland.
McDaniel, M. A., Anderson, J. L., Derbish, M. H., and
Morrisette, N. 2007. Testing the testing effect in the
classroom. European Journal of Cognitive Psychol-
ogy, 19(4-5), 494-513.
Olney, A., Graesser, A., and Person, N. 2012. Ques-
tion generation from concept maps. Dialogue &
Discourse, 3(2), 75-99.
Roediger III, H. L., and Pyc, M. 2012. Inexpensive
techniques to improve education: Applying cog-
nitive psychology to enhance educational practice.
Journal of Applied Research in Memory and Cogni-
tion, 1.4: 242-248.
Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. 2002. Multiword expressions: A pain
in the neck for NLP. In Computational Linguistics
and Intelligent Text Processing, (pp. 1-15). Springer
Berlin Heidelberg.
Sternberg, R. J., & Grigorenko, E. L. 2003. Teach-
ing for successful intelligence: Principles, proce-
dures, and practices. Journal for the Education of
the Gifted, 27, 207-228.
Wolfe, J. 1976. Automatic question generation from
text-an aid to independent study. In Proceedings of
ACM SIGCSE-SIGCUE.
Yao, X., and Zhang, Y. 2010. Question generation
with minimal recursion semantics. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration.
326
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 1?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Towards a Domain Independent Semantics: Enhancing Semantic Representation with Construction Grammar   Jena D. Hwang1,2 Rodney D. Nielsen1 Martha Palmer1,2   1Ctr. for Computational Language and Education Research University of Colorado at Boulder Boulder, CO 80302 2Department of Linguistics University of Colorado at Boulder Boulder, CO 80302 {hwangd,rodney.nielsen,martha.palmer}@colorado.edu     Abstract 
In Construction Grammar, structurally patterned units called constructions are assigned meaning in the same way that words are ?  via convention rather than composition. That is, rather than piecing semantics together from individual lexical items, Construction Grammar proposes that semantics can be assigned at the construction level. In this paper, we investigate whether a classifier can be taught to identify these constructions and consider the hypothesis that identifying construction types can improve the semantic interpretation of previously unseen predicate uses. Our results show that not only can the constructions be automatically identified with high accuracy, but the classifier also performs just as well with out-of-vocabulary predicates.  1 Introduction The root of many challenges in natural language processing applications is the fact that humans can convey a single piece of information in numerous and creative ways. Syntactic variations (e.g. I gave him my book. vs. I gave my book to him.), the use of synonyms (e.g. She bought a used car. vs. She purchased a pre-owned automobile.) and numerous other variations can complicate the semantic analysis and the automatic understanding of a text.  Consider the following sentence.  (1) They hissed him out of the university  While (1) is clearly understandable for humans, to automatically discern the meaning of hissed in this 
instance would take more than learning that the verb hiss is defined as ?make a sharp hissing sound? (WordNet 3.0). Knowing that hiss can also mean ?a show of contempt? is helpful. However, it would also require the understanding that the sentence describes a causative event if we are to interpret this sentence as meaning something like ?They caused him to leave the university by means of hissing or contempt?. The problem of novel words, expressions and usages are especially significant because discriminative learning methods used for automatic text classification do not perform as well when tested on text with a feature distribution that is different from what was seen in the training data. This is recognized to be a critical issue in domain adaptation (Ben-David et. al, 2006). Whether we seek to account for words or usages that are infrequent in the training data or to adapt a trained classifier to a new domain of text that includes new vocabulary or new forms of expressions, success in overcoming these challenges partly lies in the successful identification and the use of features that generalize over linguistic variation.  In this paper we borrow from the theories presented by Construction Grammar (CxG) to explore the development of general features that may help account for the linguistic variability and creativity we see in the data.  Specifically, we investigate whether a classifier can be taught to identify constructions as described by CxG and gauge their value in interpreting novel words. The development of approaches to effectively capture such novel semantics will enhance applications requiring richer representations of language understanding such as machine 
1
 translation, information retrieval, and text summarization. Consider, for instance, the following machine translation into Spanish by the Google translate (http://translate.google.com/):   They hissed him out of the university. ? Silbaban fuera de la universidad. Tr. They were whistling outside the university.1  The translation has absolutely no implication that a group of people did something to cause another person to leave the university. However, when the verb is changed to a verb that is seen to frequently appear in a caused motion interpretation (e.g. throw), the results are correct:  They threw him out of the university. ? Lo sacaron de la universidad. Tr. They took him out of the university.  Thus, if we could facilitate a caused motion interpretation by bootstrapping semantics from constructions (e.g. ?X ___ Y out of Z? implies caused motion), we could enable accurate translations that otherwise would not be possible. 2 Current Approaches In natural language processing (NLP), the issue of semantic analysis in the presence of lexical and syntactic variability is often perceived as the purview of either word sense disambiguation (WSD) or semantic role labeling (SRL) or both. In the case of WSD, the above issue is often tackled through the use of large corpora tagged with sense information to train a classifier to recognize the different shades of meaning of a semantically ambiguous word (Ng and Lee, 2006; Agirre and Edmonds, 2006).  In the case of SRL, the goal is to identify each of the arguments of the predicate and label them according to their semantic relationship to the predicate (Gildea and Jurafsky, 2002).   There are several corpora available for training WSD classifiers such as WordNet?s SemCor (Miller 1995; Fellbaum 1998) and the GALE OntoNotes data (Hovy et. al., 2006). However, most, if not all, of these corpora include only a small fraction of all English predicates. Since WSD systems train separate classifiers for each                                                 1 We have hand translated the Google translation back to English for comparison. 
predicate, if a particular predicate does not exist in the sparse training data, a system cannot create an accurate semantic interpretation. Even if the predicate is present, the appropriate sense might not be. In such a case, the WSD will again be unable to contribute to a correct overall semantic interpretation. This is the case in example (1), where even the extremely fine-grained sense distinctions provided by WordNet do not include a sense of hiss that is consistent with the caused motion interpretation rendered in the example. Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but rather are defined in terms of semantic structures called frames evoked by the verb. That is, one or more verbs can be associated with a single semantic frame. Currently FN has over 2000 distinct Frame Elements.  The lexical resource VerbNet (Kipper-Schuler, 2005) details semantic classes of verbs, where a class is composed of verbs that have similar syntactic realizations, following work by Levin (1993). Verbs are grouped by their syntactic realization or frames, and each frame is associated with a meaning. For example, the verbs loan and rent are grouped together in class 13.1 with roughly a ?give? meaning, and the verbs deposit and situate are grouped into 9.1 with roughly a ?put? meaning.  Although differing in the nature of their tasks, WSD and SRL systems both treat lexical items as the source of meaning in a clause. In WSD, for every sense we need a new entry in our dictionary to be able to interpret the sentence. With SRL, we                                                 2 PropBank labels Arg0 and Arg1, for the most part, correspond to Dowty?s Prototypical Agent and Prototypical Patient, respectively, providing important generalizations. 
2
 need the semantic role labels that describe the predicate argument relationships in order to extract the meaning.  In either case, we are still left with the same issue ? if the meaning lies in the lexical items, how do we interpret unseen words and novel lexical usages? As shown in the CoNLL-2005 shared task (Carreras and Marquez, 2005), system performance numbers drop significantly when a classifier, trained on the Wall Street Journal (WSJ) corpus, is tested on the Brown corpus. This is largely due to the ?highly ambiguous and unseen predicates (i.e. predicates that do not have training examples)? (Giuglea and Moschitti, 2006). 3 Construction Grammar This issue of scalability and generalizability across genres could possibly be improved by linking semantics more directly with syntax, as theorized by Construction Grammar (CxG) (Fillmore et. al., 1988; Golderg, 1995; Kay, 2002; Michaelis, 2004; Goldberg, 2006). This theory suggests that the meaning of a sentence arises not only from the lexical items but also from the patterned structures or constructions they sit in. The meaning of a given phrase, a sentence, or an utterance, then, arises from the combination of lexical items and the syntactic structure in which they are found, including any patterned structural configurations (e.g. patterns of idiomatic expressions such as ?The Xer, the Yer? ? The bigger, the better) or recurring structural elements (e.g. function words such as determiners, particles, conjunctions, and prepositions). That is, instead of focusing solely on the semantic label of words, as is done in SRL and in many traditional theories in Linguistics, CxG brings more into focus the interplay of lexical items and syntactic forms or structural patterns as the source of meaning.  3.1 Application of Construction Grammar Thus, rather than just assigning labels at the level of lexical items and predicate arguments as a way of piecing together the meaning of a sentence, we follow the central premise of CxG. Specifically, that semantics can be and should be interpreted at the level of the larger structural configuration.  Consider the following three sentences, each having the same syntactic structure, each taken 
from different genres of writing available on the web.  Blogger arrested - blog him out of jail! [Blog] Someone mind controlled me off the cliff. [Gaming] He clocked the first pitch into center field. [Baseball]  Each of these sentences makes use of words, especially the verb, in ways particular to their genre. Even if we are unfamiliar with the specific jargon used, as a human we can infer the general meaning intended by each of the three sentences: a person X causes an entity Y to move in the path specified by the prepositional phrase (e.g. third sentence: ?A player causes something to land in the center field.?).   In a similar way, if we can assign a meaning of caused motion at the sentence level and an automatic learner can be trained to accurately identify the construction, then even when presented with an unseen word, a useful semantic analysis is still possible. 3.2 Caused-Motion Construction For this effort, we focused on the caused-motion construction, which can be defined as having the coarse-grained syntactic structure of Subject Noun Phrase followed by a verb that takes both a Noun Phrase Object and a Prepositional Phrase: (NP-SBJ (V NP PP)); and the semantic meaning ?the agent, NP-SBJ, directly causes the patient, NP, to move along the path specified by the PP? (Goldberg 1995). This construction is exemplified by the following sentences from (Goldberg 1995):  (2) Frank sneezed the tissue off the table. (3) Mary urged Bill into the house. (4) Fred stuffed the papers in the envelope. (5) Sally threw a ball to him.  However, not all syntactic structures of the form (NP-SBJ (V NP PP)) belong to the caused-motion construction. Consider the following sentences.  (6) I considered Ben as one of my brothers. (7) Jen took the highway into Pennsylvania. (8) We saw the bird in the shopping mall. (9) Mary kicked the ball to my relief.  In (6) and (9), the PPs do not specify a location, a direction or a path. In (8), the PP is a location; 
3
 however, the PP indicates the location in which the ?seeing? event happened, not a path along which ?we? caused ?the bird? to move.  Though the PP in (7) expresses a path, it is not a path in which Jen causes ?the highway? to move. 3.3 Goals As an initial step in determining the usefulness of construction grammar for interpreting semantics in computational linguistics, we present the results of our study aimed at ascertaining if a classifier can be taught to identify caused-motion constructions. We also report on our investigations into which features were most useful in the classification of caused-motion constructions.  4 Data & Experiments The data for this study was pulled from the WSJ part of Penn Treebank II (Marcus et al, 1994). From this corpus, all sentences with the syntactic form (NP-SBJ (V NP PP)) were selected. The selection allowed for intervening adverbial phrases (e.g. ?Sally threw a ball quickly to him?) and additional prepositional phrases (e.g. ?Sally threw a ball to him on Tuesday? or ?Sally threw a ball in anger into the scorer?s table?). A total of 14.7k instances3 were identified in this manner. To reduce the size of the corpus to be labeled to a target of 1800 instances, we removed, firstly, instances containing traces as parsed by the TreeBank. These included passive usages (e.g. ?Coffee was shipped from Colombia by Gracie?) and instances with traces in the object NP or PP including questions and relative clauses (e.g. ?What did Gracie ship from Colombia??). In construction grammar, however, traces do not exist, since grammar is a set of patterns of varying degrees of complexity. Thus CxG would characterize passives, questions structures, and relative clauses as having their own respective phrasal constructions, which combine with the caused-motion construction. In order to ensure sufficient training data with the standard form of the caused-motion construction as defined in Goldberg 1995 and 2006 (see Section 3.2), we                                                 3 We use the term instances over sentences since a sentence can have more than one instance. For example, the sentence ?I gave the ball to Bill, and he kicked it to the wall.? is composed of 2 instances. 
chose to remove these usages.  Secondly, we removed the instances of sentences that can be deterministically categorized as non-caused motion constructions: instances containing ADV, EXT, PRD, VOC, or TMP type object NPs (e.g.?Cindy drove five hours from Dallas?, ?You listen, boy, to what I say!?). Because we can automatically identify this category, keeping these examples in our data would have resulted in even higher performance. We also considered the possibility of reducing the size by removing certain classes of verbs such as verbs of communication (e.g. reply, bark), psychological state (e.g. amuse, admire), or existence (e.g. be, exist). While it is reasonable to say that these verb types are highly unlikely to appear in a caused-motion construction, if we were to remove sets of verbs based on their likely behavior, we would also be excluding interesting usages such as ?The stand-up comedian amused me into a state of total enjoyment.? or ?The leader barked a command into a radio.? After filtering these sentences, 8700 remained. From the remaining instances, we selected 1800 instances at random for the experiments presented. 4.1 Labels and Classifier The 1800 instances were hand-labeled with one of the following two labels:   - Caused-Motion (CM)  - Non Caused-Motion (NON-CM)  The CM label included both literal usages (e.g. ?Well-wishers stuck little ANC flags in their hair.?) and non-literal usages (e.g. ?Producers shepherded ?Flashdance? through several scripts.?) of caused-motion. After the annotation, the corpus was randomly divided into two sets: 75% for training data and 25% for testing data. The distribution of the labels in the test data is 33.3% CM and 66.7% NON-CM. The distribution in the training set is 31.8% CM and 68.2% NON-CM. For our experiments, we used a Support Vector Machine (SVM) classifier with a linear kernel. In particular we made use of the LIBSVM (Chang and Lin, 2001) as training and testing software. 
4
 4.2 Baseline Features The baseline consisted of a single conceptual feature - the lemmatized, case-normalized verb. We chose the verb as a baseline feature because it is generally accepted to be the core lexical item in a sentence, which governs the syntactic structure and semantic constituents around it. This is especially evidenced in the Penn Treebank where NP nodes are assigned with syntactic labels according to the position in the tree relative to the verb (e.g. Subject). In VerbNet and PropBank, the semantic labels are assigned to the constituents around the verb, each according to its semantic relationship with the verb.   This verb feature was encoded as 478 binary features (one for each unique verb in the dataset), where the feature value corresponding to the instance?s verb was 1 and all others were 0. 4.3 Additional Features In the present experiments, we utilize gold-standard values for two of the PP features for a proof of feasibility. Future work will evaluate the effect of automatically extracting these features. In addition to the baseline verb feature (feature 1), our full feature set consisted of 8 additional types for a total of 334 features. Examples used in the feature descriptions are pulled from our data.  PP features:  2. Preposition (76 features) The preposition heading the prepositional phrase (e.g. ?Producers shepherded ?Flashdance? [[through]P several scripts]PP.?) was encoded as 76 binary features, one per preposition type in the training data. For instances with multiple PPs, preposition features were extracted from each of the PPs. 3. Function Tag on PP (11 features) Penn Treebank encodes grammatical, adverbial, and other related information on the PP?s POS tag (e.g. ?PP-LOC?). The function tag on the prepositional phrase was encoded as 10 binary features plus an extra feature for PPs without function tags. Again, for instances with multiple PPs, each corresponding function tag feature was set to 1. 4. Complement Category to P (19 features) Normally a PP node consists of a P and a NP. 
However, there are some cases where the complement of the P can be of a different syntactic category (e.g. ?So, view permanent insurance [[for]P [what it is]SBAR]PP.?). Thus, the phrasal category tags (e.g. NP, SBAR) of the preposition?s sister nodes were encoded as 19 binary features. For instances with multiple PPs, all sister nodes of the prepositions were collected.  VerbNet features: The following features were automatically extracted from VerbNet classes with frames matching the target syntactic structure, namely ?NP V NP PP?.  5. VerbNet Classes (123 features) The verbs in the data were associated with one or more of the above VerbNet classes according to their membership. The VerbNet classes were then encoded as 122 binary features with one additional feature for verbs that were not found to be members of any of these classes. If a verb belongs to multiple matching classes, each corresponding feature was set. 6. VerbNet PP Type (27 features) VerbNet frames associate the PP with a description (e.g. ?NP V NP PP.location?). The types were encoded as 26 binary features, plus an extra feature for PPs without a description. The features represented the union of all PP types (i.e. if a VerbNet class included multiple PPs, each of the corresponding features was assigned a value of 1). If a verb was associated with multiple VerbNet classes, the features were set according to the union over both the corresponding classes and their set of PP types.  Named Entity features: These features were automatically annotated using BBN?s IdentiFinder (Bikel, 1999). The feature counts for the subject NP and object NP differ strictly due to what entities were represented in the data. For example, the entity type ?DISEASE? was found in an object NP position but not in a subject NP. 7. NEs for Subject NP (23 features) The union of all named entities under the NP-SBJ node was encoded as 23 binary features.  8. NEs for Object NP (27 features) The union of all named entities under the object NP node was encoded as 27 binary features.  9. NEs for PP?s Object (28 features) The union 
5
 of all named entities under the NP under the PP node was encoded as 28 binary features. 5 Results For the baseline system, the model was built from the training data using a linear kernel and a cost parameter of C=1 (LIBSVM default value). When using the full feature set, the model was also built from the training data using a linear kernel, but the cost parameter was C=0.5, the best value from 10-fold cross validation on the training data.  In Table 1, we report the precision (P), recall (R), F1 score, and accuracy (A) for identifying caused-motion constructions4.  Features P% R% F A% Baseline* Set 78.0 52.0 0.624 79.1 Full Set 87.2 86.0 0.866 91.1 Table 1: System Performance (*verb feature baseline) The results show that the addition of the features presented in section 4.3 resulted in a significant increase in both precision and recall, which in turn boosted the F score from 0.624 to 0.857, an increase of 0.233.  6 Feature Performance In order to determine the usefulness of the individual features in the classification of caused-motion, we evaluated the features in two ways. In one (Table 2), we compared the performance of each of the features to a majority class baseline (i.e. 66.7% accuracy). A useful feature was expected to show an increase over this baseline with statistical significance. Significance of each feature?s performance was evaluated via a chi-squared test (p<0.05).  Our results show that the features 3, 1, 2 and 5 performed significantly better over the majority class baseline. The features 4, 7 and 8 were unable to distinguish between the caused-motion constructions and the non caused-motion usages.                                                 4 As we can see in Table 1, the accuracy is higher than precision or recall. This is because precision and recall are calculated with regard to identifying caused-motion constructions, whereas accuracy is based on identifying both caused-motion and non-caused motion constructions. Since it?s easier to get better performance on the majority class (NON-CM), the overall accuracy is higher.  
Their precision values could not be calculated due to the fact that these features resulted in zero positive (CM) classification.  In a second study, we evaluated the performance of the system when each feature was removed individually from the full set of features (Table 3). The removal of a useful feature was expected to show a statistically significant drop in performance compared to that of the full feature set.  Significance in this performance degradation when compared against the full set of features was evaluated via chi-squared test (p<0.05). Here, features 3, 8 and 1, when removed, showed a statistically significant performance drop. The rest of the features were not shown to have a statistically significant effect on the performance. Our results show that the preposition feature is the single most predictive feature and the feature that has the most significant effect in the full feature set. These results are encouraging: unlike the purely lexical features like the named entity features (6, 7, and 8) that are dependent on the particular expression used in the sentence, 
Table 2:  Effect of each feature on the performance in classification of the caused-motion construction, in the order of decreasing F-score. Features that performed statistically higher than the majority class baseline are marked with an * in the last column.  
# Removed Feature P% R% F A%  3 Preposition 76.9 73.3 0.751 83.8 * 8 NEs for Object NP 84.6 80.7 0.826 88.7 * 1 Verb 85.9 81.3 0.836 89.3 * 2 Function Tag on PP 85.2 84.7 0.849 90.0  9 NEs for PP?s Object 87.5 84.0 0.857 90.7  7 NEs for Subject NP 87.0 84.7 0.858 90.7  5 VerbNet Classes 86.0 86.0 0.860 90.7  4 Comp. Cat. of P 86.7 86.7 0.867 91.1  6 VerbNet PP Type 87.8 86.0 0.869 91.3  Table 3: System performance when the specified feature is removed from the full set of features, in the order of increasing F-score. Significant performance degradation, when compared against the full feature set performance (Table 1) was labeled with an * in the last column. 
# Included Feature P% R% F A%  3 Preposition 82.4 65.3 0.729 83.8 * 1 Verb  78.0 52.0 0.624 79.1 * 2 Function Tag on PP 82.6 38.0 0.521 76.7 * 5 VerbNet Classes 73.5 33.3 0.459 73.8 * 6 VerbNet PP Type 59.6 33.3 0.427 70.2  9 NEs for PP?s Object 71.4 6.7 0.122 68.0  4 Comp. Cat. of P   0.0  66.7  7 NEs for Subject NP  0.0  66.7  8 NEs for Object NP  0.0  66.7  
6
 prepositions are function words. Like syntactic elements, these function words also contribute to the patterned structures of a construction as discussed in Section 3. Furthermore, unlike the semantics of features that are dependent on content words that are subject to lexical variability, prepositions are limited in their lexical variability, which make them good general features that scale well across different semantic domains. In addition to the preposition feature, the verb feature was found to affect performance at a statistically significant level in both cases. Based on the numerous studies in the past that have shown the usefulness of the verb as a feature, this is not an unexpected result. Interestingly, our results seem to indicate interactions between features. This can be seen in two different instances. First, while feature 8 (NEs for Object NP) alone was not found to be a predictive feature, when removed, it resulted in a statistically significant drop in performance compared to that of the full feature set. The opposite effect can be seen with the VerbNet Classes feature. While it showed a statistically significant boost in performance when introduced into the system by itself, when dropped from the full feature set, the drop in the system performance was not found to be significant. This seems to indicate that NEs for Object NP and the VerbNet Classes features have strong interactions with one or more of the other features. We will continue investigating these interactions in future work. 7 Out-of-Vocabulary Verbs Additionally, we separately examined the performance on the test set verbs that were not seen in the training data (i.e. out-of-vocabulary/OOV items).  Just over a fifth of the instances (92 out of 450 constructions) in the test data had unseen verbs, with a total of 83 unique verb types. The results show that there was no decrease in the accuracy or F-score. In fact, there was a chance increase, not statistically significant, in a two-sample t-test (t=1.13; p>0.2).  We carried out the same feature studies for the OOV verbs, as detailed in section 6 (Tables 4 and 5). The performance in both of the studies reflected the results seen in Tables 2 and 3, with one expected exception. The verb feature was, of course, found to be of no value to the predictor. 
What is interesting here is that the verb feature did perform at a significant level for the full test data. By this observation, it would be expected that the overall performance on the OOV verbs would be negatively affected since there is no available verb information. However, this was not the case. 8 Discussion and Conclusion  The results presented show that a classifier can be trained to automatically identify the semantics of constructions; at least for the caused-motion construction, and that it can do this with high accuracy. Furthermore, we have determined that the preposition feature is the most useful feature when identifying caused-motion constructions. Moreover, in considering our results in light of the performance of the SRL systems (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005), where unseen predicates result in significant performance degradation, we found in contrast that using CxG to inform semantics resulted in equally high performance on the out-of-vocabulary predicates. This serves as evidence that semantic 
Table 4: Effect of each feature on the performance in classification of the caused-motion construction with OOV verbs, in the order of decreasing F-score. The precision values could not be calculated for the performance of the features 1,4,7, and 8 due to the fact that these features resulted in zero positive classifications. 
# Removed Feature P% R% F A% 3 Preposition 63 76 0.69 90 2 Function Tag on PP 83 80 0.82 82 6 VerbNet PP Type 84 84 0.84 67 5 VerbNet Classes 84 84 0.84 73 9 NEs for PP?s Object 84 84 0.84 74 1 Verb  0  73 4 Comp. Cat. of P  0  73 7 NEs for Subject NP  0  73 8 NEs for Object NP  0  73 
# Removed Feature P% R% F A% 3 Preposition 63 76 0.69 82 8 NEs for Object NP 83 80 0.82 90 2 Function Tag on PP 84 84 0.84 91 5 VerbNet Classes 84 84 0.84 91 7 NEs for Subject NP 84 84 0.84 91 1 Verb 88 88 0.88 93 4 Comp. Cat. of P 88 88 0.88 93 6 VerbNet PP Type 92 88 0.90 95 9 NEs for PP?s Object 92 88 0.90 95 Table 5: System performance when the specified feature is removed from the full set of features in the classification of constructions with OOV items, in the order of increasing F-score. 
7
 analysis of novel lexical combinations and unseen verbs can be improved by enriching semantics with a construction-level analysis. 9 Future Work There are several directions to go from here. First, in this paper we have kept our study within the scope of caused-motion constructions. We intend to introduce more types of constructions and include more syntactic variation in our data.  We will also add more annotated instances. Secondly, we examine the impact of the introduction of additional features, such as a bag-of-words feature. In particular, we will include semantic features based on FrameNet to the VerbNet semantic features we are already using.  This will be more feasible once the SemLink semantic role labeler for FrameNet becomes available (Palmer, 2009). Finally, we plan to include a more detailed analysis of the feature interactions, and examine the benefit that a construction grammar perspective might add to our semantic analysis. Acknowledgements We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No. HR0011-06-C-0022, subcontract from BBN, Inc. We are also grateful to Laura Michaelis for helpful discussions and comments. References  Agirre, Eneko and Philip Edmonds. 2006. Introduction. In Word Sense Disambiguation: Algorithms and Applications, Agirre and Edmonds (eds.), Springer. Ben-David, Shai, Blitzer, John, Crammer, Koby  Pereira, Fernando. 2006. 'Analysis of representations for domain adaptation', in NIPS. Bikel, D., Schwartz, R., Weischedel, R.  1999.  An algorithm that learns what?s in a name.  Machine Learning: Special Issue on NL Learning, 34, 1-3. Carreras, Xavier and Lluis Marquez. 2005. Introduction to the CoNLL- 2005 shared task: Semantic role labeling. Procs of CoNLL- 2005.  Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm Gildea, Daniel and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28:3, 245-288. 
Fillmore, Charles J., Christopher R. Johnson and Miriam R.L. Petruck (2003) Background to Framenet, International Journal of Lexicography, Vol 16.3: 235-250. Fillmore, Charles, Paul Kay and Catherine O'Connor (1988). Regularity and Idiomaticity in Grammatical Constructions: The Case of let alne. Language 64: 501-38. Giuglea, Ana-Maria and Alessandro Moschitti. 2006. Shallow semantic parsing based on FrameNet, Verb-Net and PropBank. In Proceedings of the 17th European Conference on Artificial Intelligence, Riva del Garda, Italy. Goldberg, Adele E. 2006. Constructions at work. The nature of generalization in language. Oxford: Oxford University Press Goldberg, Adele. E. 1995. Constructions: A construction grammar approach to argument structure. Chicago: University of Chicago Press. Hovy, Edward H., Mitch Marcus, Martha Palmer, Sameer Pradhan, Lance Ramshaw, and Ralph M. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of the Human Language Technology / North American Association of Computational Linguistics conference (HLT-NAACL 2006). pp. 57-60, New York, NY. Kay, Paul. 2002. English Subjectless Tag Sentences. Language 78: 453-81. Kipper-Schuler, Karin. 2005. VerbNet: A broad coverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania. Levin, Beth. 1993. English Verb Classes and Alternations: A Preliminary Investigation, University of Chicago Press, Chicago, IL. Michaelis, Laura A. (2004). Type Shifting in Construction Grammar: An Integrated Approach to Aspectual Coercion. Cognitive Linguistics 15: 1-67. Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California, 40?47.  Marcus, Mitchell P, Santorini, Beatrice, Marcinkiewicz, Mary A. (1994) "Building a large annotated corpus of English: the Penn Treebank" Computational Linguistics 19: 313-330. Palmer, Martha. "Semlink: Linking PropBank, VerbNet and FrameNet." Proceedings of the Generative Lexicon Conference. Sept. 2009, Pisa, Italy: GenLex-09, 2009. Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71?106. 
8
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 64?72,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
To Annotate More Accurately or to Annotate More
Dmitriy Dligach
Department of Computer Science
University of Colorado at Boulder
Dmitriy.Dligach@colorado.edu
Rodney D. Nielsen
The Center for Computational Language
and Education Research
University of Colorado at Boulder
Rodney.Nielsen@colorado.edu
Martha Palmer
Department of Linguistics
Department of Computer Science
University of Colorado at Boulder
Martha.Palmer@colorado.edu
Abstract
The common accepted wisdom is that
blind double annotation followed by adju-
dication of disagreements is necessary to
create training and test corpora that result
in the best possible performance. We pro-
vide evidence that this is unlikely to be the
case. Rather, the greatest value for your
annotation dollar lies in single annotating
more data.
1 Introduction
In recent years, supervised learning has become
the dominant paradigm in Natural Language Pro-
cessing (NLP), thus making the creation of hand-
annotated corpora a critically important task. A
corpus where each instance is annotated by a sin-
gle tagger unavoidably contains errors. To im-
prove the quality of the data, an annotation project
may choose to annotate each instance twice and
adjudicate the disagreements, thus producing the
(largely) error-free gold standard. For example,
OntoNotes (Hovy et al, 2006), a large-scale an-
notation project, chose this option.
However, given a virtually unlimited supply of
unlabeled data and limited funding ? a typical set
of constraints in NLP ? an annotation project must
always face the realization that for the cost of dou-
ble annotation, more than twice as much data can
be single annotated. The philosophy behind this
alternative says that modern machine learning al-
gorithms can still generalize well in the presence
of noise, especially when given larger amounts of
training data.
Currently, the commonly accepted wisdom
sides with the view that says that blind double
annotation followed by adjudication of disagree-
ments is necessary to create annotated corpora that
leads to the best possible performance. We pro-
vide empirical evidence that this is unlikely to be
the case. Rather, the greatest value for your an-
notation dollar lies in single annotating more data.
There may, however, be other considerations that
still argue in favor of double annotation.
In this paper, we also consider the arguments of
Beigman and Klebanov (2009), who suggest that
data should be multiply annotated and then filtered
to discard all of the examples where the annota-
tors do not have perfect agreement. We provide
evidence that single annotating more data for the
same cost is likely to result in better system per-
formance.
This paper proceeds as follows: first, we out-
line our evaluation framework in Section 2. Next,
we compare the single annotation and adjudica-
tion scenarios in Section 3. Then, we compare
the annotation scenario of Beigman and Klebanov
(2009) with the single annotation scenario in Sec-
tion 4. After that, we discuss the results and future
work in section 5. Finally, we draw the conclusion
in Section 6.
2 Evaluation
2.1 Data
For evaluation we utilize the word sense data an-
notated by the OntoNotes project. The OntoNotes
data was chosen because it utilizes full double-
blind annotation by human annotators and the dis-
agreements are adjudicated by a third (more expe-
64
rienced) annotator. This allows us to
? Evaluate single annotation results by using
the labels assigned by the first tagger
? Evaluate double annotation results by using
the labels assigned by the second tagger
? Evaluate adjudication results by using the la-
bels assigned by the the adjudicator to the in-
stances where the two annotators disagreed
? Measure the performance under various sce-
narios against the double annotated and adju-
dicated gold standard data
We selected the 215 most frequent verbs in the
OntoNotes data. To make the size of the dataset
more manageable, we randomly selected 500 ex-
amples of each of the 15 most frequent verbs. For
the remaining 200 verbs, we utilized all the an-
notated examples. The resulting dataset contained
66,228 instances of the 215 most frequent verbs.
Table 1 shows various important characteristics of
this dataset averaged across the 215 verbs.
Inter-tagger agreement 86%
Annotator1-gold standard agreement 93%
Share of the most frequent sense 70%
Number of classes (senses) per verb 4.74
Table 1: Data used in evaluation at a glance
2.2 Cost of Annotation
Because for this set of experiments we care pri-
marily about the cost effectiveness of the annota-
tion dollars, we need to know how much it costs
to blind annotate instances and how much it costs
to adjudicate disagreements in instances. There is
an upfront cost associated with any annotation ef-
fort to organize the project, design an annotation
scheme, set up the environment, create annotation
guidelines, hire and train the annotators, etc. We
will assume, for the sake of this paper, that this
cost is fixed and is the same regardless of whether
the data is single annotated or the data is double
annotated and disagreements adjudicated.
In this paper, we focus on a scenario where there
is essentially no difference in cost to collect ad-
ditional data to be annotated, as is often the case
(e.g., there is virtually no additional cost to down-
load 2.5 versus 1.0 million words of text from the
web). However, this is not always the case (e.g.,
collecting speech can be costly).
To calculate a cost per annotated instance for
blind annotation, we take the total expenses asso-
ciated with the annotators in this group less train-
ing costs and any costs not directly associated with
annotation and divide by the total number of blind
instance annotations. This value, $0.0833, is the
per instance cost used for single annotation. We
calculated the cost for adjudicating instances sim-
ilarly, based on the expenses associated with the
adjudication group. The adjudication cost is an ad-
ditional $0.1000 per instance adjudicated. The per
instance cost for double blind, adjudicated data is
then computed as double the cost for single an-
notation plus the per instance cost of adjudication
multiplied by the percent of disagreement, 14%,
which is $0.1805.
We leave an analysis of the extent to which the
up front costs are truly fixed and whether they can
be altered to result in more value for the dollar to
future work.
2.3 Automatic Word Sense Disambiguation
For the experiments we conduct in this study, we
needed a word sense disambiguation (WSD) sys-
tem. Our WSD system is modeled after the state-
of-the-art verb WSD system described in (Dligach
and Palmer, 2008). We will briefly outline it here.
We view WSD as a supervised learning prob-
lem. Each instance of the target verb is represented
as a vector of binary features that indicate the pres-
ence (or absence) of the corresponding features in
the neighborhood of the target verb. We utilize
all of the linguistic features that were shown to be
useful for disambiguating verb senses in (Chen et
al., 2007).
To extract the lexical features we POS-tag
the sentence containing the target verb and the
two surrounding sentences using MXPost soft-
ware (Ratnaparkhi, 1998). All open class words
(nouns, verbs, adjectives, and adverbs) in these
sentences are included in our feature set. In addi-
tion to that, we use as features two words on each
side of the target verb as well as their POS tags.
To extract the syntactic features we parse the
sentence containing the target verb with Bikel?s
constituency parser and utilize a set of rules to
identify the features in Table 2.
Our semantic features represent the semantic
classes of the target verb?s syntactic arguments
65
Feature Explanation
Subject and object - Presence of subject and
object
- Head word of subject
and object NPs
- POS tag of the head
word of subject and
object NPs
Voice - Passive or Active
PP adjunct - Presence of PP adjunct
- Preposition word
- Head word of the
preposition?s NP
argument
Subordinate clause - Presence of subordinate
clause
Path - Parse tree path from
target verb to neighboring
words
- Parse tree path from
target verb to subject and
object
- Parse tree path from
target verb to subordinate
clause
Subcat frame - Phrase structure rule
expanding the target
verb?s parent node in
parse tree
Table 2: Syntactic features
such as subject and object. The semantic classes
are approximated as
? WordNet (Fellbaum, 1998) hypernyms
? NE tags derived from the output of Identi-
Finder (Bikel et al, 1999)
? Dynamic dependency neighbors (Dligach
and Palmer, 2008), which are extracted in an
unsupervised way from a dependency-parsed
corpus
Our WSD system uses the Libsvm software
package (Chang and Lin, 2001) for classification.
We accepted the default options (C = 1 and lin-
ear kernel) when training our classifiers. As is the
case with most WSD systems, we train a separate
model per verb.
3 Experiment One
The results of experiment one show that in these
circumstances, better performance is achieved by
single annotating more data than by deploing re-
sources towards ensuring that the data is annotated
more accurately through an adjudication process.
3.1 Experimental Design
We conduct a number of experiments to compare
the effect of single annotated versus adjudicated
data on the accuracy of a state of the art WSD sys-
tem. Since OntoNotes does not have a specified
test set, for each word, we used repeated random
partitioning of the data with 10 trials and 10% into
the test set and the remaining 90% comprising the
training set.
We then train an SVM classifier on varying frac-
tions of the data, based on the number of examples
that could be annotated per dollar. Specifically,
in increments of $1.00, we calculate the number
of examples that can be single annotated and the
number that can be double blind annotated and ad-
judicated with that amount of money.
The number of examples computed for single
annotation is selected at random from the train-
ing data. Then the adjudicated examples are se-
lected at random from this subset. Selecting from
the same subset of data approaches pair statisti-
cal testing and results in a more accurate statistical
comparison of the models produced.
Classifiers are trained on this data using the la-
bels from the first round of annotation as the single
annotation labels and the final adjudicated labels
for the smaller subset. This procedure is repeated
ten times and the average results are reported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
3.2 Results
Figure 1 shows a plot of the accuracy of the clas-
sifiers relative to the annotation investment for a
typical verb, to call. As can be seen, the accu-
racy is always higher when training on the larger
amount of single annotated data than when train-
ing on the amount of adjudicated data that had the
equivalent cost of annotation.
Figures 2 and 3 present results averaged over
all 215 verbs in the dataset. First, figure 2 shows
the average accuracy over all verbs by amount in-
vested. These accuracy curves are not smooth be-
66
Figure 1: Performance of single annotated vs. ad-
judicated data by amount invested for to call
cause the verbs all have a different number of total
instances. At various annotation cost values, all of
the instances of one or more verbs will have been
annotated. Hence, the accuracy values might jump
or drop by a larger amount than seen elsewhere in
the graph.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. We only
display the dollar investments of up to $60 due to
the fact that only five verbs have more than $60?s
worth of instances in the training set.
Figure 2: Average performance of single anno-
tated vs. adjudicated data by amount invested
The average difference in accuracy for Figure 2
across all amounts of investment is 1.64%.
Figure 3 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at a
given percent of total investment was interpolated
for each verb using linear interpolation and then
averaged over all of the verbs.
Figure 3: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 3
across each percent of investment is 2.10%.
Figure 4 presents essentially the same informa-
tion as Figure 2, but as a reduction in error rate for
single annotation relative to full adjudication.
Figure 4: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 2
The relative reduction in error rate averaged
over all investment amounts in Figure 2 is 7.77%.
Figure 5 presents the information in Figure 3
as a reduction in error rate for single annotation
relative to full adjudication.
The average relative reduction in error rate over
the fractions of total investment in Figure 5 is
9.32%.
67
Figure 5: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 3
3.3 Discussion
First, it is worth noting that, when the amount of
annotated data is the same for both scenarios, ad-
judicated data leads to slightly better performance
than single annotated data. For example, consider
Figure 3. The accuracy at 100% of the total invest-
ment for the double annotation and adjudication
scenario is 81.13%. The same number of exam-
ples can be single annotated for 0.0833 / 0.1805 =
0.4615 of this dollar investment (using the costs
from Section 2.2). The system trained on that
amount of single annotated data shows a lower ac-
curacy, 80.21%. Thus, in this case, the adjudica-
tion scenario brings about a performance improve-
ment of about 1%.
However, the main thesis of this paper is that in-
stead of double annotating and adjudicating, it is
often better to single annotate more data because
it is a more cost-effective way to achieve a higher
performance. The results of our experiments sup-
port this thesis. At every dollar amount invested,
our supervised WSD system performs better when
trained on single annotated data comparing to dou-
ble annotated and adjudicated data.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 81.13% (Figure 3).
When trained on single annotated data, the system
attains the same accuracy much earlier, at approxi-
mately 60% of the total investment. When trained
on the entire available single annotated data, the
system reaches an accuracy of 82.99%, nearly a
10% relative reduction in error rate over the same
system trained on the adjudicated data obtained for
the same cost.
Averaged over the 215 verbs, the single anno-
tation scenario outperformed adjudication at every
dollar amount investigated.
4 Experiment Two
In this experiment, we consider the arguments of
Beigman and Klebanov (2009). They suggest that
data should be at least double annotated and then
filtered to discard all of the examples where there
were any annotator disagreements.
The main points of their argument are as fol-
lows. They first consider the data to be dividable
into two types, easy (to annotate) cases and hard
cases. Then they correctly note that some anno-
tators could have a systematic bias (i.e., could fa-
vor one label over others in certain types of hard
cases), which would in turn bias the learning of
the classifier. They show that it is theoretically
possible that a band of misclassified hard cases
running parallel to the true separating hyperplane
could mistakenly shift the decision boundary past
up to
?
N easy cases.
We suggest that it is extremely unlikely that a
consequential number of easy cases would exist
nearer to the class boundary than the hard cases.
The hard cases are in fact generally considered to
define the separating hyperplane.
In this experiment, our goal is to determine how
the accuracy of classifiers trained on data labeled
according to Beigman and Klebanov?s discard dis-
agreements strategy compares empirically to the
accuracy resulting from single annotated data. As
in the previous experiment, this analysis is per-
formed relative to the investment in the annotation
effort.
4.1 Experimental Design
We follow essentially the same experimental de-
sign described in section 3.1, using the same state
of the art verb WSD system. We conduct a num-
ber of experiments to compare the effect of single
annotated versus double annotated data. We uti-
lized the same training and test sets as the previous
experiment and similarly trained an SVM on frac-
tions of the data representing increments of $1.00
investments.
As before, the number of examples designated
68
for single annotation is selected at random from
the training data and half of that subset is selected
as the training set for the double annotated data.
Again, selecting from the same subset of data re-
sults in a more accurate statistical comparison of
the models produced.
Classifiers for each annotation scenario are
trained on the labels from the first round of an-
notation, but examples where the second annota-
tor disagreed are thrown out of the double anno-
tated data. This results in slightly less than half as
much data in the double annotation scenario based
on the disagreement rate. Again, the procedure is
repeated ten times and the average results are re-
ported.
For a given verb, each classifier created
throughout this process is tested on the same dou-
ble annotated and adjudicated held-out test set.
4.2 Results
Figure 6 shows a plot of the accuracy of the classi-
fiers relative to the annotation investment for a typ-
ical verb, to call. As can be seen, the accuracy for
a specific investment performing single annotation
is always higher than it is for the same investment
in double annotated data.
Figure 6: Performance of single annotated vs.
double annotated data with disagreements dis-
carded by amount invested for to call
Figures 7 and 8 present results averaged over
all 215 verbs in the dataset. First, figure 7 shows
the average accuracy over all verbs by amount
invested. Again, these accuracy curves are not
smooth because the verbs all have a different num-
ber of total instances. Hence, the accuracy val-
ues might jump or drop by a larger amount at the
points where a given verb is no longer included in
the average.
Toward the higher dollar amounts the curve is
dominated by fewer and fewer verbs. As before,
we only display the results for investments of up
to $60.
The average difference in accuracy for Figure 7
across all amounts of investment is 2.32%.
Figure 8 presents the average accuracy relative
to the percent of the total cost to single annotate
all of the instances for a verb. The accuracy at
a given percent of total investment was interpo-
lated for each verb and then averaged over all of
the verbs.
Figure 7: Average performance of single anno-
tated vs. double annotated data with disagree-
ments discarded by amount invested
Figure 8: Average performance of single anno-
tated vs. adjudicated data by fraction of total in-
vestment
The average difference in accuracy for Figure 8
across all amounts of investment is 2.51%.
69
Figures 9 and 10 present this information as a
reduction in error rate for single annotation rela-
tive to full adjudication.
Figure 9: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 7
The relative reduction in error rate averaged
over all investment amounts in Figure 9 is 10.88%.
Figure 10: Reduction in error rate from adjudica-
tion to single annotation scenario based on results
in Figure 8
The average relative reduction in error rate over
the fractions of total investment in Figure 10 is
10.97%.
4.3 Discussion
At every amount of investment, our supervised
WSD system performs better when trained on sin-
gle annotated data comparing to double annotated
data with discarded cases of disagreements.
The maximum annotation investment amount
for each verb is the cost of single annotating all
of its instances. When the system is trained on
the amount of double annotated data possible at
this investment, its accuracy is 80.78% (Figure 8).
When trained on single annotated data, the system
reaches the same accuracy much earlier, at approx-
imately 52% of the total investment. When trained
on the entire available single annotated data, the
system attains an accuracy of 82.99%, an 11.5%
relative reduction in error rate compared to the
same system trained on the double annotated data
obtained for the same cost.
The average accuracy of the single annotation
scenario outperforms the double annotated with
disagreements discarded scenario at every dollar
amount investigated.
While this empirical investigation only looked
at verb WSD, it was performed using 215 distinct
verb type datasets. These verbs each have con-
textual features that are essentially unique to that
verb type and consequently, 215 distinct classi-
fiers, one per verb type, are trained. Hence, these
could loosely be considered 215 distinct annota-
tion and classification tasks.
The fact that for the 215 classification tasks the
single annotation scenario on average performed
better than the discard disagreements scenario of
Beigman and Klebanov (2009) strongly suggests
that, while it is theoretically possible for annota-
tion bias to, in turn, bias a classifier?s learning, it
is more likely that you will achieve better results
by training on the single annotated data.
It is still an open issue whether it is generally
best to adjudicate disagreements in the test set or
to throw them out as suggested by (Beigman Kle-
banov and Beigman, 2009).
5 Discussion and Future Work
We investigated 215 WSD classification tasks,
comparing performance under three annotation
scenarios each with the equivalent annotation cost,
single annotation, double annotation with dis-
agreements adjudicated, and double annotation
with disagreements discarded. Averaging over the
215 classification tasks, the system trained on sin-
gle annotated data achieved 10.0% and 11.5% rel-
ative reduction in error rates compared to training
on the equivalent investment in adjudicated and
disagreements discarded data, respectively. While
we believe these results will generalize to other an-
notation tasks, this is still an open question to be
determined by future work.
70
There are probably similar issues in what were
considered fixed costs for the purposes of this pa-
per. For example, it may be possible to train fewer
annotators, and invest the savings into annotating
more data. Perhaps more appropriately, it may be
feasible to simply cut back on the amount of train-
ing provided per annotator and instead annotate
more data.
On the other hand, when the unlabeled data
is not freely obtainable, double annotation may
be more suitable as a route to improving system
performance. There may also be factors other
than cost-effectiveness which make double anno-
tation desirable. Many projects point to their ITA
rates and corresponding kappa values as a mea-
sure of annotation quality, and of the reliability of
the annotators (Artstein and Poesio, 2008). The
OntoNotes project used ITA rates as a way of eval-
uating the clarity of the sense inventory that was
being developed in parallel with the annotation.
Lexical entries that resulted in low ITA rates were
revised, usually improving the ITA rate. Calculat-
ing these rates requires double-blind annotation.
Annotators who consistently produced ITA rates
lower than average were also removed from the
project. Therefore, caution is advised in determin-
ing when to dispense with double annotation in fa-
vor of more cost effective single annotation.
Double annotation can also be used to shed light
on other research questions that, for example, re-
quire knowing which instances are ?hard.? That
knowledge may help with designing additional,
richer annotation layers or with cognitive science
investigations into human representations of lan-
guage.
Our results suggest that systems would likely
benefit more from the larger training datasets that
single annotation makes possible than from the
less noisy datasets resulting from adjudication.
Regardless of whether single or double annota-
tion with adjudication is used, there will always be
noise. Hence, we see the further investigation of
algorithms that generalize despite the presence of
noise to be critical to the future of computational
linguistics. Humans are able to learn in the pres-
ence of noise, and our systems must follow suit.
6 Conclusion
Double annotated data contains less noise than
single annotated data and thus improves the per-
formance of supervised machine learning systems
that are trained on a specific amount of data. How-
ever, double annotation is expensive and the alter-
native of single annotating more data instead is on
the table for many annotation projects.
In this paper we compared the performance of
a supervised machine learning system trained on
double annotated data versus single annotated data
obtainable for the same cost. Our results clearly
demonstrate that single annotating more data can
be a more cost-effective way to improve the sys-
tem performance in the many cases where the un-
labeled data is freely available and there are no
other considerations that necessitate double anno-
tation.
7 Acknowledgements
We gratefully acknowledge the support of the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In ACL-IJCNLP
?09: Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 280?287, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
put. Linguist., 35(4):495?503.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3):211?231.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
71
J. Chen and M. Palmer. 2005. Towards robust high
performance word sense disambiguation of english
verbs using rich linguistic features. pages 933?944.
Springer.
Jinying Chen, Dmitriy Dligach, and Martha Palmer.
2007. Towards large-scale high-performance en-
glish verb sense disambiguation by using linguisti-
cally motivated features. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 378?388, Washington, DC, USA.
IEEE Computer Society.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
HLT ?08: Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies, pages 29?32, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In NAACL ?06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57?60, Morristown, NJ, USA. Association for
Computational Linguistics.
A. Ratnaparkhi. 1998. Maximum entropy models for
natural language ambiguity resolution. Ph.D. the-
sis, University of Pennsylvania.
72
Proceedings of the 8th International Natural Language Generation Conference, pages 103?107,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Framework for Health Behavior Change using Companionable Robots
Bandita Sarma
University of North Texas
banditasarma@my.unt.edu
Amitava Das
University of North Texas
Amitava.Das@unt.edu
Rodney D. Nielsen
University of North Texas
Rodney.Nielsen@unt.edu
Abstract
In this paper, we describe a dialogue
system framework for a companionable
robot, which aims to guide patients to-
wards health behavior changes via natu-
ral language analysis and generation. The
framework involves three broad stages,
rapport building and health topic identifi-
cation, assess patient?s opinion of change,
and designing plan and closing session.
The framework uses concepts from psy-
chology, computational linguistics, and
machine learning and builds on them. One
of the goals of the framework is to ensure
that the Companionbot builds and main-
tains rapport with patients.
1 Introduction
Human beings engage in many activities or be-
haviors that can aggravate existing health prob-
lems or lead to new ones. Abandoning such be-
haviors with the help of motivational interview-
ing or counseling sessions is called health behav-
ior change. Providing counseling sessions that re-
sult in behavior change is a difficult task even for
expert practitioners, and hence poses a great chal-
lenge for automated dialogue systems. The pro-
cess demands constant monitoring and an in-depth
understanding of the patient1. A wrong move on
the counselor?s part could undo what might oth-
erwise be a successful attempt to bring about the
targeted health behavior change. This could re-
quire the counselor to return to the first stage of
the process and regain the patient?s trust.
In this paper, we describe a framework for a
companionable robot, which can counsel its hu-
man companion and assist them in making health-
ier choices for a better life. In our previous work
1The terms patient and user are used interchangeably
throughout the paper to refer to the human using the Com-
panionbot.
(Nielsen et al., 2010), we described an overall ar-
chitecture for a companion robot capable of proac-
tive dialogue with patients. This paper focuses
on a natural language processing framework for
health behavior change dialogue. Bickmore and
Sidner (2006) outline a plan-based framework,
COLLAGEN, based on the transtheoretical model
and motivational interviewing to generate health
behavior change counseling dialogue for physical
activity promotion. COLLAGEN conducts a ses-
sion in four steps: greeting exchange, discussion
of previous day?s exercise, discussion of plans for
next day, and finally, farewell exchange. In addi-
tion to having similar steps, our framework also
discusses in detail the natural language process-
ing modules that are involved in judging the user?s
mindset at each step and guiding him/her towards
making a decision on changing health behavior. In
their follow up work (Bickmore et al., 2009), they
discuss several issues such as minimizing repet-
itiveness in the behavioral, linguistic and visual
aspect of the agent, establishing a therapeutic al-
liance between the user and the agent for a suc-
cessful dialogue, maintaining continuity over mul-
tiple sessions, and the challenge of open-ended
question generation. In addition to these issues,
there might be verbal resistance from the patient
to the suggestions by the Companionbot.
Use of telemedicine is becoming a common
practice in providing remote clinical health care. It
involves the use of various technologies like tele-
phone, Facsimile, e-mail, video meetings, etc. to
provide medical services. However, telemedicine
is not flexible and adaptive, or when it is, it re-
quires a human in the loop. It might also require
long wait times on the patient side to receive a
response from a health expert. Using compan-
ionable robots to provide guidance for health be-
havior change can provide greater flexibility com-
pared to standard telemedicine practices. Bajwa
(2010) described a virtual medical expert system
103
that leverages natural language processing tech-
niques to provide medical help to user queries
from a knowledge base using pattern matching.
In case the query does not have a match in the
knowledge base, it is directed to an expert. The
present work is along similar lines in terms of pro-
viding medical advice but in case of an unknown
health condition, the Companionbot provides in-
formation through Web search. In addition to this,
our framework adds the capability of generating
small talk, which will help the user overcome inhi-
bitions that might arise in talking to a robot instead
of a human. The medical advice provided by the
Companionbot will be in the form of suggestions
rather than instructions. This is intended to make
users reflect on their own choices comfortably in-
stead of receiving instructions from the Compan-
ionbot?s advice. The Nursebot project (Roy et al.,
2000) discussed five different functions to assist
the elderly through personal robots. One of the
functions is to provide virtual telemedicine based
facilities. Another robot called Paro (Kidd et al.,
2006) was developed to cater to the social needs of
elderly in nursing homes and was capable of gen-
erating a small set of vocal utterances in addition
to limited voice recognition and body movement.
Our framework, when implemented successfully,
will be capable of engaging the user in a complete
conversation, both casual and therapeutic.
2 Framework
The proposed dialogue system framework con-
sists of three broad stages. The first stage aims
to build rapport with the patient and identify the
health topic to be discussed. The second stage
involves identifying the issues and challenges the
patient perceives associated with enacting relevant
health behavior changes and motivating the patient
to make the most appropriate change(s). The final
stage summarizes the overall plans and goals, and
encourages the patient to follow through. The en-
tire process from building rapport with the patient
through motivating changes in health-related be-
havior may span several sessions, and of course, is
likely to be repeated for other behaviors.
2.1 Build rapport & identify health topic
In order to initiate a counseling session it is essen-
tial to build and maintain rapport with the patient.
This helps the patient feel more comfortable with
the situation, which facilitates more open commu-
nication. Reasonable rapport needs to be estab-
lished in the initial stages when the Companionbot
is first introduced to the patient. However, since
the Companionbot is meant to be present con-
stantly with its human companion, rapport build-
ing and maintenance is expected to be an on-
going process. Throughout both the casual and
health behavior change dialogue, the Companion-
bot will identify the patient?s interpersonal rela-
tions, health conditions and beliefs, likes and dis-
likes, habits, hobbies, and routines. These will be
stored in a user model, which the language gen-
eration engine will exploit to engage the user in
dialogue that is guided by, and infused with, per-
sonal context. A language understanding compo-
nent will constantly assess the user?s engagement
level in the conversation. If the user seems to
be disinterested at any point, a Typical Day strat-
egy (Mason and Butler, 2010) is used to deal with
the situation where the Companionbot will ask the
user what a typical day for them is like.
When the system has achieved an adequate level
of rapport, the next step is to identify a health
topic of concern to the patient, so that there can
be a focused discussion geared towards health be-
havior change. The present project will start with
a small list of conditions and the behaviors that,
when altered, can bring about an improvement in
the condition. For example, heart disease includes
diet and exercise, among others, as the associ-
ated behaviors. These conditions will be identi-
fied primarily using named-entity recognition and
keyword spotting. If the Companionbot identi-
fies heart disease as the topic, then the discussion
could focus on food habits or exercise related is-
sues.
2.2 Assess patient?s opinion of change
Once a health concern is identified, the next step
is to determine how important the patient thinks
it is to change the associated behaviors and how
confident they are about enacting those changes.
This is an important stage because not all people
have the same mindset regarding behavior change.
Some might understand the importance of it but
are not confident about achieving it while others
might not consider it important at all. In order to
understand the user?s mindset, Mason and Butler
(2010) suggest asking the user to assign cardinal
values to quantify these opinions. The values may
be on a scale of 0 to 10, where 0 is the lowest and
104
Figure 1: Block diagram for assessing patient?s
opinion of change
10 is the highest.
If there is a large difference between the user
ratings of importance and confidence, the Com-
panionbot will discuss the lower-ranked factor first
(Mason and Butler, 2010). If the scores are ap-
proximately equal (e.g., the patient gives both im-
portance and confidence a medium rating), then
the Companionbot?s dialogue will focus on help-
ing the user understand the importance of the be-
havior change (Mason and Butler, 2010). Low val-
ues for both importance and confidence scores in-
dicate that the user is not ready for these health
behavior changes (Mason and Butler, 2010), and
the Companionbot should move on to a different
health topic or behavior. If both the scores are
high, the Companionbot can move on to the next
stage, summarizing the discussion and motivating
the behavior changes. Figure 1 shows the block
diagram representation for this module.
2.3 Design plan & close the session
The Companionbot moves toward concluding the
conversation by asking an open-ended question re-
garding how the user feels about the health be-
havior changes that they have been discussing.
A user?s attitude can be categorized into one of
three categories, ready for change, not ready for
change, or ambivalent. If the patient is ready for
change, the Companionbot will provide sugges-
tions on how to bring about the change in the be-
Figure 2: Block diagram for designing plan and
closing the session
havior in previous step by leveraging knowledge
from the user model and the conversation history.
There may be patients who belong to the second
category and are not ready for the health behav-
ior change. We have already discussed ways on
how to tackle such a situation in Subsection 2.2.
If the patient is ambivalent about changing a be-
havior, the Companionbot will close by providing
information to help the patient reflect on the pros
and cons of the health behavior change until it is
appropriate to bring it up again in a future ses-
sion. A knowledge base will be maintained about
the behaviors associated with common and criti-
cal health conditions. Information about a health
condition, which is outside the domain of cur-
rent knowledge base, will be retrieved using Web
search. Figure 2 shows the block diagram repre-
sentation of this stage.
If a session exceeds a pre-defined time, deemed
to be the limit of most patients? ability to stay ad-
equately focused on health behavior change, or
if the system recognizes that the patient is los-
ing their focus, the Companionbot will check-in
with the patient, and if appropriate, will bring the
session to a close following strategies that parallel
those described in the preceding paragraph.
3 Challenges
Automatic generation of dialogue becomes a par-
ticularly challenging task when its purpose is to
105
guide people through sensitive or personal issues
like health behavior change. Some patients may
not like to be told what is good or bad for them.
In such a case, the patient might begin resisting
suggestions for change (Mason and Butler, 2010).
This places the entire counseling session in a pre-
carious position and any wrong move on the Com-
panionbot?s part could push the patient to a higher
level of resistance. To mitigate this scenario, the
Companionbot will include patient resistance de-
tection in the framework. If mild resistance is de-
tected, the discourse is automatically directed to-
wards bringing the user back on track. Whereas if
there is high resistance, the Companionbot moves
on to a different topic In case the user continues re-
sisting then the Companionbot will close the ses-
sion.
For successful implementation of therapeutic
dialogue systems, it is essential to ensure that they
do not sound monotonous. This is possible only
if the responses are generated dynamically and
hardcoding is limited. During rapport building
and user modeling, questions will be generated by
the Companionbot from various sources like the
Internet, medical forms, information provided by
physicians, family members, etc. At other times,
responses will be constructed using both syntactic
and semantic information from the user utterances.
Since multiple sessions might be held with the
user to discuss a specific behavior, it is neces-
sary to maintain continuity between the sessions
(Bickmore et al., 2009). Bickmore and Sidner
(2006) advocate dedicating a part of the dialogue
to reviewing prior discussions, associated actions,
and patient plans, as well as discussing what the
patient has done since the last session to follow
though on their plans. The Companionbot main-
tains a detailed user model including logs of the
previous sessions, which will be used to review
prior discussions, plans and actions and to guide
ongoing motivational interviews.
Another challenge is choosing appropriate eval-
uation measures to determine the system?s use-
fulness in bringing about the desired change in
the patient. The efficacy of the system will be
judged by monitoring the users behavior regularly.
Any noticeable changes, such as weight gain or
loss and increased or decreased smoking, will be
tracked. How frequently a patient interacts with
the Companionbot is an implicit qualitative mea-
sure of how much they appreciate it. We also plan
to use questionnaires to elicit user ratings of the
system for its acceptability and quality on a Lick-
ert scale (Lickert, 1932).
4 Conclusion
In this paper we proposed a novel framework
for automatic health behavior change counsel-
ing. Successful implementation of this frame-
work would mean that the Companionbot could be
used to guide patients towards bringing changes in
their behavior for a healthier life. This can reduce
the long wait period in conventional telemedicine
practices from the time the patients contact the
remote heatlh care provider to the instance they
receive the instruction (Bajwa, 2010). Since the
Companionbot will be capable of small talk aimed
at connecting with the user on an emotional level,
we hypothesize it will be perceived as being much
more natural than existing conversational robots.
References
Cory D. Kidd, Will Taggart and Sherry Turkle. 2006.
A Sociable Robot to Encourage Social Interaction
among the Elderly. IEEE International Conference
on Robotics and Automation, 3972?3976.
Imran S. Bajwa. 2010. Virtual Telemedicine Using
Natural Language Processing. International Jour-
nal of Information Technology and Web Engineer-
ing, 5(1):43?55.
Nicholas Roy, Gregory Baltus, Dieter Fox, Francine
Gemperle, Jennifer Goetz, Tad Hirsch, Dimitris
Margaritis, Michael Montemerlo, Joelle Pineau,
Jamieson Schulte and Sebastian Thrun. 2000. To-
wards Personal Service Robots for the Elderly.
Workshop on Interactive Robots and Entertainment.
Pip Mason and Christopher C. Butler. 2010. Health
Behavior Change. Elsevier Health Sciences.
Rensis Likert. 1932. A Technique for the Measurement
of Attitudes. Archives of Psychology, 140:1?55.
Rodney D. Nielsen, Richard Voyles, Daniel Bolanos,
Mohammad H. Mahoor, Wilson D. Pace, Katie A.
Siek and Wayne H. Ward. 2010. A Platform for
Human-Robot Dialog Systems Research. In Pro-
ceedings of AAAI Fall Symposium, Dialog with
Robots, 161?162.
Timothy W. Bickmore and Candace L. Sidner.
2006. Towards Plan-based Health Behavior Change
Counseling Systems. In proceedings of AAAI
Spring Symposium on Argumentation for Con-
sumers of Healthcare.
106
Timothy Bickmore, Daniel Mauer, Francisco Crespo
and Thomas Brown. 2008. Negotiating Task In-
terruptions with Virtual Agents for Health Behav-
ior Change. In Proceedings of the 7th International
Joint Conference on Autonomous Agents and Mul-
tiagent Systems, 1241?1244.
Timothy Bickmore, Daniel Schulman and Candace
Sidner. 2009. Issues in Designing Agents for Long
Term Behavior Change. CHI?09 Workshop on En-
gagement by Design.
107
