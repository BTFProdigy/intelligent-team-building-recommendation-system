Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540?549,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
What Substitutes Tell Us ?
Analysis of an ?All-Words? Lexical Substitution Corpus
Gerhard Kremer
Institute for Computational Linguistics
University of Heidelberg, Germany
kremer@cl.uni-heidelberg.de
Katrin Erk
Dept. of Linguistics
University of Texas, Austin, U.S.A.
katrin.erk@utexas.edu
Sebastian Pad?
Institute for Natural Language Processing
University of Stuttgart, Germany
pado@ims.uni-stuttgart.de
Stefan Thater
Dept. of Computational Linguistics
Saarland University, Saarbr?cken, Germany
stth@coli.uni-sb.de
Abstract
We present the first large-scale English ?all-
words lexical substitution? corpus. The
size of the corpus provides a rich resource
for investigations into word meaning. We
investigate the nature of lexical substitute
sets, comparing them to WordNet synsets.
We find them to be consistent with, but
more fine-grained than, synsets. We also
identify significant differences to results
for paraphrase ranking in context reported
for the SEMEVAL lexical substitution data.
This highlights the influence of corpus con-
struction approaches on evaluation results.
1 Introduction
Many, if not most, words have multiple meanings;
for example, the word ?bank? has a financial and
a geographical sense. One common approach to
deal with this lexical ambiguity is supervised word
sense disambiguation, or WSD (McCarthy, 2008;
Navigli, 2009), which frames the task as a lemma-
level classification problem, to be solved by train-
ing classifiers on samples of lemma instances that
are labelled with their correct senses.
This approach has its problems, however. First,
it assumes a complete and consistent set of labels.
WordNet, used in the majority of studies, does
cover several 10,000 lemmas, but has been criti-
cised for both its coverage and granularity. Second,
WSD requires annotation for each sense and lemma,
leading to an ?annotation bottleneck?. A number
of technical solutions have been suggested regard-
ing the second problem (Ando and Zhang, 2005;
Navigli and Ponzetto, 2012), but not for the first.
In 2009, McCarthy and Navigli address both
problems by proposing a fundamentally different
approach, called Lexical Substitution (McCarthy
and Navigli, 2009) which avoids capturing a word?s
meaning by a single label. Instead, annotators are
asked to list, for each instance of a word, one or
more alternative words or phrases to be substituted
for the target in this particular context. This setup
provides a number of benefits over WSD. It al-
lows characterising word meaning without using
an ontology and can be obtained easily from native
speakers through crowdsourcing. Work on mod-
elling Lexical Substitution data has also assumed a
different focus from WSD. It tends to see the predic-
tion of substitutes along the lines of compositional
lexical semantics, concentrating on explaining how
word meaning is modulated in context (Mitchell
and Lapata, 2010).
There are, however, important shortcomings of
the work in the Lexical Substitution paradigm. All
existing datasets (McCarthy and Navigli, 2009;
Sinha and Mihalcea, 2014; Biemann, 2013; Mc-
Carthy et al., 2013) are either comparatively small,
are ?lexical sample? datasets, or both. ?Lexical
sample? datasets consist of sample sentences for
each target word drawn from large corpora, with
just one target word substituted in each sentence. In
WSD, ?lexical sample? datasets contrast with ?all-
words? annotation, in which all content words in a
text are annotated for sense (Palmer et al., 2001).
540
In this paper, we present the first large ?all-
words? Lexical Substitution dataset for English. It
provides substitutions for more than 30,000 words
of running text from two domains of MASC (Ide et
al., 2008; Ide et al., 2010), a subset of the Ameri-
can National Corpus (http://www.anc.org)
that is freely available and has (partial) manual
annotation. The main advantage of the all-words
setting is that it provides a realistic frequency distri-
bution of target words and their senses. We use this
to empirically investigate (a) the nature of lexical
substitution and (b) the nature of the corpus, seen
through the lens of word meaning in context.
2 Related Work
2.1 Lexical Substitution: Data
The original ?English Lexical Substitution? dataset
(McCarthy and Navigli, 2009) comprises 200 target
content words (balanced numbers of nouns, verbs,
adjectives and adverbs). Targets were explicitly se-
lected to exhibit interesting ambiguities. For each
target, 10 sentences were chosen (mostly at ran-
dom, but in part by hand) from the English Internet
Corpus (Sharoff, 2006) and presented to 5 anno-
tators to collect substitutes. Its total size is 2,000
target instances. Sinha and Mihalcea (2014) pro-
duced a small pilot dataset (500 target instances) for
all-words substitution, asking three annotators to
substitute all content words in presented sentences.
Biemann (2013) first investigated the use of
crowdsourcing, developing a three-task bootstrap-
ping design to control for noise. His study covers
over 50,000 instances, but these correspond only to
397 targets, all of which are high-frequency nouns.
Biemann clusters the resulting substitutes into word
senses. McCarthy et al. (2013) applied lexical sub-
stitution in a cross-lingual setting, annotating 130
of the original McCarthy and Navigli targets with
Spanish substitutions (i. e., translations).
2.2 Lexical Substitution: Models
The LexSub task at SEMEVAL 2007 (McCarthy
and Navigli, 2009) required systems to both de-
termine substitution candidates and choose con-
textual substitutions in each case. Erk and Pad?
(2008) treated the gold substitution candidates as
given and focused on the context-specific ranking
of those candidates. In this form, the task has been
addressed through three types of (mostly unsuper-
vised) approaches. The first group computes a sin-
gle type representation and modifies it according
to sentence context (Erk and Pad?, 2008; Thater et
al., 2010; Thater et al., 2011; Van de Cruys et al.,
2011). The second group of approaches clusters
instance representations (Reisinger and Mooney,
2010; Dinu and Lapata, 2010; Erk and Pad?, 2010;
O?S?aghdha and Korhonen, 2011). The third op-
tion is to use a language model (Moon and Erk,
2013). Recently, supervised models have emerged
(Biemann 2013; Szarvas et al., 2013a,b).
3 COINCO ? The MASC All-Words
Lexical Substitution Corpus
1
Compared to, e. g., WSD, there still is little gold-
annotated data for lexical substitution. With the
exception of the dataset created by Biemann (2013),
all existing lexical substitution datasets are fairly
small, covering at most several thousand instances
and few targets which are manually selected. We
aim to fill this gap, providing a dataset that mirrors
the actual corpus distribution of targets in sentence
context and is sufficiently large to enable a detailed,
lexically specific analysis of substitution patterns.
3.1 Source Corpus Choice
For annotation, we chose a subset of the ?Manually
Annotated Sub-Corpus? MASC (Ide et al., 2008;
Ide et al., 2010) which is ?equally distributed across
19 genres, with manually produced or validated
annotations for several layers of linguistic phenom-
ena?, created with the purpose of being ?free of
usage and redistribution restrictions?. We chose
this corpus because (a) our analyses can profit from
the preexisting annotations and (b) we can release
our annotations as part of MASC.
Since we could not annotate the complete MASC,
we selected (complete) text documents from two
prominent genres: news (18,942 tokens) and fiction
(16,605 tokens). These two genres are both rele-
vant for NLP and provide long, coherent documents
that are appropriate for all-words annotation. We
used the MASC part-of-speech annotation to iden-
tify all content words (verbs, nouns, adjectives, and
adverbs), which resulted in a total of over 15,000
targets for annotation. This method differs from
Navigli and McCarthy?s (2009) in two crucial re-
spects: we annotate all instances of each target, and
include all targets regardless of frequency or level
of lexical ambiguity. We believe that our corpus is
considerably more representative of running text.
1
Available as XML-formatted corpus ?Concepts in Con-
text? (COINCO) from http://goo.gl/5C0jBH. Also
scheduled for release as part of MASC.
541
3.2 Crowdsourcing
We used the Amazon Mechanical Turk (AMT) plat-
form to obtain substitutes by crowdsourcing. Inter-
annotator variability and quality issues due to non-
expert annotators are well-known difficulties (see,
e. g., Fossati et al. (2013)). Our design choices
were shaped by ?best practices in AMT?, including
Mason and Suri (2012) and Biemann (2013).
Defining HITs. An AMT task consists of Human
Intelligence Tasks (HITs), each of which is sup-
posed to represent a minimal, self-contained task.
In our case, potential HITs were annotations of
(all target words in) one sentence, or just one tar-
get word. The two main advantages of annotating
a complete sentence at a time are (a) less over-
head, because the sentence has only to be read
once; (b) higher reliability, since all words within a
sentence will be annotated by the same person.
Unfortunately, presenting individual sentences
as HITs also means that all sentences pay the same
amount irrespective of their length. Since long sen-
tences require more effort, they are likely to receive
less attention. We therefore decided to generally
present two random target words per HIT, and one
word in the case of ?leftover? singleton targets.
In the HITs, AMT workers (?turkers?) saw the
highlighted target word in context. Since one sen-
tence was often insufficient to understand the target
fully, we also showed the preceding and the follow-
ing sentence. The task description asked turkers to
provide (preferably single-word) substitutes for the
target that ?would not change the meaning?. They
were explicitly allowed to use a ?more general term?
in case a substitute was hard to find (e. g., dog for
the target dachshund, cf. basic level effects: Rosch
et al. (1976)). Turkers were encouraged to produce
as many replacements as possible (up to 5). If they
could not find a substitute, they had to check one of
the following radio buttons: ?proper name?, ?part
of a fixed expression?, ?no replacement possible?,
?other problem (with description)?.
Improving Reliability. Another major problem
is reliability. Ideally, the complete dataset should
be annotated by the same group of annotators, but
turkers tend to work only on a few HITs before
switching to other AMT jobs. Following an idea
of Biemann and Nygaard (2010), we introduced a
two-tier system of jobs aimed at boosting turker
loyalty. A tier of ?open tasks? served to identify
reliable turkers by manually checking their given
substitutes for plausibility. Such turkers were then
invited to the second, ?closed task? tier, with a
higher payment. In both tiers, bonus payments
were offered to those completing full HIT sets.
For each target, we asked 6 turkers to provide
substitutions. In total, 847 turkers participated suc-
cessfully. In the open tasks, 839 turkers submitted
12,158 HITs (an average of 14.5 HITs). In the
closed tasks, 25 turkers submitted 42,827 HITs (an
average of 1,713 HITs), indicating the substantial
success of our turker retention scheme.
Cost. In the open task, each HIT was paid for
with $ 0.03, in the closed task the wage was $ 0.05
per HIT. The bonus payment for completing a HIT
set amounted to $ 2 ($ 1) in the open (closed) tasks.
The average cost for annotations was $ 0.22 for one
target word instance and $ 0.02 for one substitute.
The total cost with fees was ~$ 3,400.
3.3 COINCO: Corpus and Paraset Statistics
We POS-tagged and lemmatised targets and substi-
tutes in sentence context with TreeTagger (Schmid,
1994). We manually lemmatised unknown words.
Our annotated dataset comprises a total of 167,336
responses by turkers for 15,629 target instances in
2,474 sentences (7,117 nouns, 4,617 verbs, 2,470
adjectives, and 1,425 adverbs). As outlined above,
targets are roughly balanced across the two gen-
res (news: 8,030 instances in 984 sentences; fic-
tion: 7,599 instances in 1,490 sentences). There are
3,874 unique target lemmas; 1,963 of these occur
more than once. On this subset, there is a mean of
6.99 instances per target lemma. To our knowledge,
our corpus is the largest lexical substitution dataset
in terms of lemma coverage.
Each target instance is associated with a paraset
(i. e., the set of substitutions or paraphrases pro-
duced for a target in its context) with an average
size of 10.71. Turkers produced an average of
1.68 substitutions per target instance.
2
Despite
our instructions to provide single-word substitutes,
11,337 substitutions contain more than one word.
3.4 Inter-Annotator Agreement
McCarthy and Navigli (2009) introduced two inter-
annotator agreement (IAA) measures for their
dataset. The first one is pairwise agreement (PA),
2
Note that a small portion of the corpus was annotated by
more than 6 annotators.
542
dataset # targets PA mode-% PA
m
MN09 1,703 27.7 73.9 50.7
SM13 550 15.5 N/A N/A
COINCO (complete) 15,400 19.3 70.9 44.7
COINCO (subset) 2,828 24.6 76.4 50.9
Table 1: Pairwise turker agreement (mode-%: per-
centage of target instances with a mode)
measuring the overlap of produced substitutions:
PA =
?
t?T
?
?s
t
,s
?
t
? ?C
t
|s
t
? s
?
t
|
|s
t
? s
?
t
|
?
1
|C
t
| ? |T |
where t is a target in our target set T , s
t
is the
paraset provided by one turker for t, and C
t
is the
set comprising all pairs of turker-specific parasets
for t. Only targets with non-empty parasets (i. e.,
not marked by turkers as a problematic target) from
at least two turkers are included. The second one
is mode agreement (PA
m
), the agreement of an-
notators? parasets with the mode (the unique most
frequent substitute) for all targets where one exists:
PA
m
=
?
t?T
m
?
s
t
?S
t
[m ? s
t
] ?
1
|s
t
| ? |T
m
|
where T
m
is the set of all targets with some mode
m and S
t
is the set of all parasets for target t. The
Iverson bracket notation [m ? s
t
] denotes 1 if
mode m is included in s
t
(otherwise 0).
Table 1 compares our dataset to the results by
McCarthy and Navigli (2009, MN09) and Sinha
and Mihalcea (2014, SM13). The scores for
our complete dataset (row 3) are lower than Mc-
Carthy and Navigli?s both for PA (?8 %) and PA
m
(?6 %), but higher than Sinha and Mihalcea?s, who
also note the apparent drop in agreement.
3
We believe that this is a result of differences in
the setup rather than an indicator of low quality:
Note that PA will tend to decrease both in the face
of more annotators and of more substitutes. Both
of these factors are present in our setup. To test this
interpretation, we extracted a subset of our data that
is comparable to McCarthy and Navigli?s regard-
ing these factors. It comprises all target instances
where (a) exactly 6 turkers gave responses (9,521
targets), and (b) every turker produced between one
and three substitutes (5,734 targets). The results for
this subset (row 4) are much more similar to those
of McCarthy and Navigli: the pairwise agreement
3
Please see McCarthy and Navigli (2009) for a possible
explanation of the generally low IAA numbers in this field.
relation all verb noun adj adv
syn 9.4 12.5 7.7 8.0 10.4
direct-hyper 6.6 9.3 7.6 N/A N/A
direct-hypo 7.5 11.6 8.0 N/A N/A
trans-hyper 3.2 2.8 4.7 N/A N/A
trans-hypo 3.0 3.7 3.8 N/A N/A
wn-other 68.9 60.7 66.5 88.5 85.4
not-in-wn 2.1 0.9 2.2 3.4 4.2
Table 2: Target?substitute relations in percentages,
overall (all) and by POS. Note: WordNet contains
no hypo-/hypernyms for adjectives and adverbs.
differs only by 3 %, and the mode agreement is
almost identical. We take these figures as indica-
tion that crowdsourcing can serve as a sufficiently
reliable way to create substitution data; note that
Sinha and Mihalcea?s annotation was carried out
?traditionally? by three annotators.
Investigating IAA numbers by target POS and by
genre, we found only small differences (? 2.6 %)
among the various subsets, and no patterns.
4 Characterising Lexical Substitutions
This section examines the collected lexical substi-
tutions, both quantitatively and qualitatively. We
explore three questions: (a) What lexical relations
hold between targets and their substitutes? (b) Do
parasets resemble word senses? (c) How similar
are the parasets that correspond to the same word
sense of a target? These questions have not been
addressed before, and we would argue that they
could not be addressed before, because previous
corpora were either too small or were sampled in a
way that was not conducive to this analysis.
We use WordNet (Fellbaum, 1998), release 3.1,
as a source for both lexical relations and word
senses. WordNet is the de facto standard in NLP
and is used for both WSD and broader investiga-
tions of word meaning (Navigli and Ponzetto, 2012;
Erk and McCarthy, 2009). Multi-word substitutes
are excluded from all analyses.
4
4.1 Relating Targets and Substitutes
We first look at the most canonical lexical relations
between a target and its substitutes. Table 2 lists the
percentage of substitutes that are synonyms (syn),
direct/transitive (direct-/trans-) hypernyms (hyper)
4
All automatic lexical substitution approaches, including
Section 5, omit multi-word expressions. Also, they can be
expected to have WordNet coverage and normalisation issues,
which would constitute a source of noise for this analysis.
543
sentence substitutes
Now, how can I help the elegantly mannered friend of
my Nepthys and his surprising young charge ?
dependent, person, task, lass, prot?g?, effort, companion
The distinctive whuffle of pleasure rippled through the
betas on the bridge, and Rakal let loose a small growl,
as if to caution his charges against false hope.
dependent, command, accusation, private, companion, follower,
subordinate, prisoner, teammate, ward, junior, underling, enemy,
group, crew, squad, troop, team, kid
Table 3: Context effects below the sense level: target noun ?charge? (wn-other shown in italics)
and hyponyms (hypo) of the target. If a substitute
had multiple relations to the target, the shortest path
from any of its senses to any sense of the target
was chosen. The table also lists the percentage of
substitutes that are elsewhere in WordNet but not
related to the target (wn-other) and substitutes that
are not covered by WordNet (not-in-wn).
We make three main observations. First, Word-
Net shows very high coverage throughout ? there
are very few not-in-wn substitutes. Second, the per-
centages of synonyms, hypernyms and hyponyms
are relatively similar (even though the annotation
guidelines encouraged the annotation of hyponyms
over hypernyms), but relatively small. Finally, and
most surprisingly, the vast majority of substitutes
across all parts of speech are wn-other.
A full analysis of wn-other is beyond the cur-
rent paper. But a manual analysis of wn-other
substitutes for 10 lemmas
5
showed that most of
them were context-specific substitutes that can dif-
fer even when the sense of the target is the same.
This is illustrated in Table 3, which features two
occurrences of the noun ?charge? in the sense of
?person committed to your care?. But because of
the sentence context, the first occurrence got sub-
stitutes like ?prot?g??, while the second one was
paraphrased by words like ?underling?. We also
see evidence of annotator error (e. g., ?command?
and ?accusation? in the second sentence).
6
Dis-
counting such instances still leaves a prominent
role for correct wn-other cases.
But are these indeed contextual modulation ef-
fects below the sense level, or are parasets funda-
mentally different from word senses? We perform
two quantitative analyses to explore this question.
4.2 Comparing Parasets to Synsets
To what extent do parasets follow the boundaries
of WordNet senses? To address this question, we
5
We used the nouns business, charge, place, way and the
verbs call, feel, keep, leave, show, stand.
6
A manual analysis of the same 10 lemmas showed only
38 out of 1,398 (0.027) of the substitutes to be erroneous.
paraset?sense mapping class verb noun adj adv
mappable 90.3 73.5 33.0 49.6
uniquely mappable 63.1 57.5 24.3 41.3
Table 4: Ratios of (uniquely) mappable parasets
establish a mapping between parasets and synsets.
Since gold standard word senses in MASC are lim-
ited to high-frequency lemmas and cover only a
small part of our data, we create a heuristic map-
ping that assigns each paraset to that synset of its
target with which it has the largest intersection. We
use extended WordNet synsets that include direct
hypo- and hypernyms to achieve better matches
with parasets. We call a paraset uniquely mappable
if it has a unique best WordNet match, and map-
pable if one or more best matches exist. Table 4
shows that most parasets are mappable for nouns
and verbs, but not for adjectives or adverbs.
We now focus on mappable parasets for nouns
and verbs. To ensure that this does not lead to a
confounding bias, we performed a small manual
study on the 10 noun and verb targets mentioned
above (247 parasets). We found 25 non-mappable
parasets, which were due to several roughly equally
important reasons: gaps in WordNet, multi-word
expressions, metaphor, problems of sense granular-
ity, and annotator error. We also found 66 parasets
with multiple best matches. The two dominant
sources were target occurrences that evoked more
than one sense and WordNet synset pairs with very
close meanings. We conclude that excluding non-
mappable parasets does not invalidate our analysis.
To test whether parasets tend to map to a single
synset, we use a cluster purity test that compares
a set of clusters C to a set of gold standard classes
C
?
. Purity measures the accuracy of each cluster
with respect to its best matching gold class:
purity(C,C
?
) =
1
N
K
?
k=1
max
k
?
|C
k
? C
?
k
?
|
where N is the total number of data points, K is the
544
measure verbs nouns
cluster purity (%) 75.1 81.2
common core size within sense 1.84 2.21
common core size across senses 0.39 0.41
paraset size 6.89 6.29
Table 5: Comparing uniquely mappable parasets to
senses: overlap with best WordNet match as cluster
purity (top), and intersection size of parasets with
and without the same WordNet match (bottom)
number of clusters, and C
?
k
?
is the gold class that
has the largest overlap with cluster C
k
. In our case,
C is the set of mappable parasets
7
, C
?
the set of
extended WordNet synsets, and we only consider
substitutes that occur in one of the target?s extended
synsets (these are the data points). This makes the
current analysis complementary to the relational
analysis in Table 2.
8
The result, listed in the first row of Table 5,
shows that parasets for both verbs and nouns have
a high purity, that is, substitutes tend to focus on a
single sense. This can be interpreted as saying that
annotators tend to agree on the general sense of a
target. Roughly 20?25 % of substitutes, however,
tend to stem from a synset of the target that is not
the best WordNet match. This result comes with
the caveat that it only applies to substitutes that
are synonyms or direct hypo- and hypernyms of
the target. So in the next section, we perform an
analysis that also includes wn-other substitutes.
4.3 Similarity Between Same-Sense Parasets
We now use the WordNet mappings from the pre-
vious section to ask how (dis-)similar parasets are
that represent the same word sense. We also try to
identify the major sources for dissimilarity.
We quantify paraset similarity as the common
core, that is, the intersection of all parasets for
the same target that map onto the same extended
WordNet synset. Surprisingly, the common core
is mostly non-empty (in 85.6 % of all cases), and
contains on average around two elements, as the
second row in Table 5 shows. For this analysis, we
only use uniquely mappable parasets. In relation
to the average paraset size (see row 4), this means
that one quarter to one third of the substitutes are
7
For non-uniquely mappable parasets, the purity is the
same for all best-matching synsets.
8
Including wn-other substitutes would obscure whether
low purity means substitutes from a mixture of senses (which
we are currently interested in) or simply a large number of
wn-other substitutes (which we have explored above).
set elements
synset \ core feel, perceive, comprehend
synset ? core sense
core \ synset notice
non-core substitutes detect, recall, perceive, experi-
ence, note, realize, discern
Table 6: Target feel.v.03: synset and common core
shared among all instances of the same target?sense
combination. In contrast, the common core for
all parasets of targets that map onto two or more
synsets contains only around 0.4 substitutes (see
row 3) ? that is, it is empty more often than not.
At the same time, if about one quarter to one
third of the substitutes are shared, this means that
there are more non-shared than shared substitutes
even for same-sense parasets. Some of these cases
result from small samples: Even 6 annotators can-
not always exhaust all possible substitutes. For
example, the phrase ?I?m starting to see more busi-
ness transactions? occurs twice in the corpus. The
two parasets for ?business? share the same best
WordNet sense match, but they have only 3 shared
and 7 non-shared substitutes. This is even though
the substitutes are all valid and apply to both in-
stances. Other cases are instances of the context
sensitivity of the Lexical Substitution task as dis-
cussed above. Table 6 illustrates on an example
how the common core of a target sense relates to
the corresponding synset; note the many context-
specific substitutes outside the common core.
5 Ranking Paraphrases
While there are several studies on modelling lexi-
cal substitutes, almost all reported results use Mc-
Carthy and Navigli?s SEMEVAL 2007 dataset. We
now compare the results of three recent computa-
tional models on COINCO (our work) and on the
SEMEVAL 2007 dataset to highlight similarities
and differences between the two datasets.
Models. We consider the paraphrase ranking
models of Erk and Pad? (2008, EP08), Thater et
al. (2010, TFP10) and Thater et al. (2011, TFP11).
These models have been analysed by Dinu et al.
(2012) as instances of the same general framework
and have been shown to deliver state-of-the-art per-
formance on the SEMEVAL 2007 dataset, with best
results for Thater et al. (2011).
The three models share the idea to represent the
meaning of a target word in a specific context by
545
corpus syntactically structured syntactically filtered bag of words random
TFP11 TFP10 EP08 TFP11/EP08 TFP10 TFP11/EP08 TFP10
COINCO
context 47.8 46.0 47.4 47.4 41.9 46.2 40.8
33.0
baseline 46.2 44.6 46.2 45.8 38.8 44.7 37.5
SEMEVAL 2007
context 52.5 48.6 49.4 50.1 44.7 48.0 42.6
30.0
baseline 43.7 42.7 43.7 44.4 38.0 42.7 35.8
COINCO Subset
context 40.3 37.7 39.0 39.2 34.1 37.7 32.5
23.7
baseline 36.7 35.7 36.7 36.4 30.6 35.4 28.0
Table 7: Corpus comparison in terms of paraphrase ranking quality (GAP percentage). SEMEVAL results
from Thater et al. (2011). ?Context?: full models, ?baseline?: uncontextualised target-substitute similarity.
modifying the target?s basic meaning vector with
information from the vectors of the words in the
target?s direct syntactic context. For instance, the
vector of ?coach? in the phrase ?the coach derailed?
is obtained by modifying the basic vector represen-
tation of ?coach? through the vector of ?derail?, so
that the resulting contextualised vector reflects the
train car sense of ?coach?.
We replicate the setup of Thater et al. (2011)
to make our numbers directly comparable. We
consider three versions of each model: (a) syntacti-
cally structured models use vectors which record
co-occurrences based on dependency triples, ex-
plicitly recording syntactic role information within
the vectors; (b) syntactically filtered models also
use dependency-based co-occurrence information,
but the syntactic role is not explicitly represented in
the vector representations; (c) bag-of-words mod-
els use a window of ? 5 words. All co-occur-
rence counts are extracted from the English Giga-
word corpus (http://catalog.ldc.upenn.
edu/LDC2003T05), analysed with Stanford de-
pendencies (de Marneffe et al., 2006).
We apply the models to our dataset as follows:
We first collect all substitutes for all occurrences of
a target word in the corpus. The task of our models
for each target instance is then to rank the candi-
dates so that the actual substitutes are ranked higher
than the rest. We rank candidates according to the
cosine similarity between the contextualised vec-
tor of the target and the vectors of the candidates.
Like most previous approaches, we compare the
resulting ranked list with the gold standard annota-
tion (the paraset of the target instance), using gen-
eralised average precision (Kishida, 2005, GAP),
and using substitution frequency as weights. GAP
scores range between 0 and 1; a score of 1 indicates
a perfect ranking in which all correct substitutes
precede all incorrect ones, and correct high-weight
substitutes precede low-weight substitutes.
Results. The upper part of Table 7 shows results
for our COINCO corpus and the previous stan-
dard dataset, SEMEVAL 2007. ?Context? refers to
the full models, and ?baseline? to global, context-
unaware ranking based on the semantic similarity
between target and substitute. Baselines are model-
specific since they re-use the models? vector repre-
sentations. Note that EP08 and TFP11 are identical
unless syntactically structured vectors are used, and
their baselines are identical.
The behaviour of the baselines on the two cor-
pora is quite similar: random baselines have GAPs
around 0.3, and uncontextualised baselines have
GAPs between 0.35 and 0.46. The order of the
models is also highly parallel: the syntactically
structured TFP11 is the best model, followed by
its syntactically filtered version and syntactically
structured EP08. All differences between these
models are significant (p< 0.01) for both corpora,
as computed with bootstrap resampling (Efron and
Tibshirani, 1993). That is, the model ranking on
SEMEVAL is replicated on COINCO.
There are also substantial differences between
the two corpora, though. Most notably, all models
perform substantially worse on COINCO. This
is true in absolute terms (we observe a loss of 2?
5 % GAP) but even more dramatic expressed as the
gain over the uninformed baselines (almost 9 % for
TFP11 on SEMEVAL but only 1.2 % on COINCO).
All differences between COINCO and SEMEVAL
are again significant (p< 0.01).
We see three major possible reasons for these
differences: variations in (a) the annotation setup
(crowdsourcing, multiple substitutes); (b) the sense
distribution; (c) frequency and POS distributions
between the two corpora. We focus on (c) since it
can be manipulated most easily. SEMEVAL con-
tains exactly 10 instances for all targets, while CO-
INCO reflects the Zipf distribution of ?natural? cor-
pora, with many targets occurring only once. Such
546
corpora are easier to model in terms of absolute
performance, because the paraphrase lists for rare
targets contain less false positives for each instance.
For hapax legomena, the set of substitution candi-
dates is identical to the gold standard, and the only
way to receive a GAP score lower than 1 for such
targets is to rank low-weight substitutes ahead of
high-weight substitutes. Not surprisingly, the mean
GAP score of the syntactically structured TFP11
for hapax legomena is 0.863. At the same time,
such corpora make it harder for full models to out-
perform uncontextualised baselines; the best model
(TFP11) only outperforms the baseline by 1.6 %.
To neutralise this structural bias, we created
?SEMEVAL-like? subsets of COINCO (collectively
referred to as the COINCO Subset) by extracting
all COINCO targets with at least 10 instances (141
nouns, 101 verbs, 50 adjectives, 36 adverbs) and
building 5 random samples by drawing 10 instances
for each target. These samples match SEMEVAL in
the frequency distribution of its targets. To account
for the unequal distribution of POS in the samples,
we compute GAP scores for each POS separately
and calculate these GAP scores? average.
The results for the various models on the CO-
INCO Subset in the bottom part of Table 7 show
that the differences between COINCO and SE-
MEVAL are not primarily due to the differences
in target frequencies and POS distribution ? the
COINCO Subset is actually more different to SE-
MEVAL than the complete COINCO. Strikingly,
the COINCO Subset is very difficult, with a ran-
dom baseline of 24 % and model performances be-
low 37 % (baselines) and up to 40 % (full models),
which indicates that the set of substitutes in CO-
INCO is more varied than in SEMEVAL as an effect
of the annotation setup. Encouragingly, the margin
between full models and baselines is larger than on
the complete COINCO and generally amounts to
2?4 % (3.6 % for TFP11). That is, the full models
are more useful on the COINCO corpus than they
appeared at first glance; however, their effect still
remains much smaller than on SEMEVAL.
6 Conclusion
This paper describes COINCO, the first large-scale
?all-words? lexical substitution corpus for English.
It was constructed through crowdsourcing on the
basis of MASC, a corpus of American English.
The corpus has two major advantages over previ-
ous lexical substitution corpora. First, it covers con-
tiguous documents rather than selected instances.
We believe that analyses on our corpus generalise
better to the application domain of lexical substitu-
tion models, namely random unseen text. In fact,
we find substantial differences between the perfor-
mances of paraphrase ranking models for COINCO
and the original SEMEVAL 2007 LexSub dataset:
the margin of informed methods over the baselines
are much smaller, even when controlling for target
frequencies and POS distribution. We attribute this
divergence at least in part to the partially manual se-
lection strategy of SEMEVAL 2007 (cf. Section 2.1)
which favours a more uniform distribution across
senses, while our whole-document annotation faces
the ?natural? distribution skewed towards predom-
inant senses. This favours the non-contextualised
baseline models, consistent with our observations.
At the very least, our findings demonstrate the sen-
sitivity of evaluation results on corpus properties.
The second benefit of our corpus is that its size
enables more detailed analyses of lexical substi-
tution data than previously possible. We are able
to investigate the nature of the paraset, i. e., the
set of lexical substitutes given for one target in-
stance, finding that lexical substitution sets corre-
spond fairly well to WordNet sense distinctions
(parasets for the same synset show high similarity,
while those for different senses do not). In addition,
however, we observe a striking degree of context-
dependent variation below the sense level: the ma-
jority of lexical substitutions picks up fine-grained,
situation-specific meaning components that do not
qualify as sense distinctions in WordNet.
Avenues for future work include a more detailed
analysis of the substitution data to uncover genre-
and domain-specific patterns and the development
of lexical substitution models that take advantage
of the all-words substitutes for global optimisation.
Acknowledgements
We are grateful to Jan Pawellek for implementing
the AMT task, extracting MASC data, and preparing
HITs. Furthermore, we thank Georgiana Dinu for
her support with the word meaning models.
References
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multiple
tasks and unlabeled data. Journal of Machine Learn-
ing Research, 6:1817?1853.
547
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing WordNet. In Proceedings of the 5th Global
WordNet conference, Mumbai, India.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Language Resources and Evaluation, 47(1):97?122.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454, Genoa, Italy.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP, pages 1162?1172, Cambridge, MA.
Georgiana Dinu, Stefan Thater, and S?ren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL, pages 611?615,
Montr?al, Canada.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of EMNLP, pages
440?449, Singapore.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897?906, Honolulu,
HI.
Katrin Erk and Sebastian Pad?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97, Uppsala, Sweden.
Christiane Fellbaum, editor. 1998. WordNet: An
electronic lexical database. MIT Press, Cambridge,
MA.
Marco Fossati, Claudio Giuliano, and Sara Tonelli.
2013. Outsourcing FrameNet to the crowd. In Pro-
ceedings of ACL, pages 742?747, Sofia, Bulgaria.
Nancy Ide, Collin F. Baker, Christiane Fellbaum,
Charles Fillmore, and Rebecca Passonneau. 2008.
MASC: The manually annotated sub-corpus of
American English. In Proceedings of LREC, pages
2455?2461, Marrakech, Morocco.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of ACL, pages 68?73, Upp-
sala, Sweden.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evalua-
tion indicator for information retrieval experiments.
Technical Report NII-2005-014E, Japanese National
Institute of Informatics.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on Amazon?s Mechanical Turk.
Behavior Research Methods, 44(1):1?23.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Ravi Sinha, and Rada Mihalcea.
2013. The cross-lingual lexical substitution task.
Language Resources and Evaluation, 47(3):607?
638.
Diana McCarthy. 2008. Word sense disambiguation.
In Linguistics and Language Compass. Blackwell.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Taesun Moon and Katrin Erk. 2013. An inference-
based model of word meaning in context as a para-
phrase distribution. ACM Transactions on Intelli-
gent Systems and Technology, 4(3).
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:1?69.
Diarmuid O?S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of EMNLP, pages 1047?1057,
Edinburgh, UK.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Pro-
ceedings of the SENSEVAL-2 workshop, pages 21?
24, Toulouse, France.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceeding of NAACL, pages 109?117, Los
Angeles, CA.
Eleanor Rosch, Carolyn B. Mervis, Wayne D. Gray,
David M. Johnson, and Penny Boyes-Braem. 1976.
Basic objects in natural categories. Cognitive Psy-
chology, 8(3):382?439.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
NEMLAP, pages 44?49, Manchester, UK.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal
of Corpus Linguistics, 11(4):435?462.
Ravi Sinha and Rada Mihalcea. 2014. Explorations
in lexical sample and all-words lexical substitution.
Natural Language Engineering, 20(1):99?129.
548
Gy?rgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013a. Supervised all-words lexical substitution
using delexicalized features. In Proceedings of
NAACL-HLT, pages 1131?1141, Atlanta, GA.
Gy?rgy Szarvas, R?bert Busa-Fekete, and Eyke H?ller-
meier. 2013b. Learning to rank lexical substitutions.
In Proceedings of EMLNP, pages 1926?1932, Seat-
tle, WA.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948?957, Uppsala, Sweden.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of IJCNLP, pages
1134?1143, Chiang Mai, Thailand.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of EMNLP, pages
1012?1022, Edinburgh, Scotland.
549
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 94?101
Manchester, August 2008
Cognitively Salient Relations
for Multilingual Lexicography
Gerhard Kremer
CIMeC
University of Trento
gerhard.kremer@unitn.it
Andrea Abel
EURAC
Bolzano
aabel@eurac.edu
Marco Baroni
CIMeC
University of Trento
marco.baroni@unitn.it
Abstract
Providing sets of semantically related
words in the lexical entries of an electronic
dictionary should help language learners
quickly understand the meaning of the tar-
get words. Relational information might
also improve memorisation, by allowing
the generation of structured vocabulary
study lists. However, an open issue is
which semantic relations are cognitively
most salient, and should therefore be used
for dictionary construction. In this paper,
we present a concept description elicita-
tion experiment conducted with German
and Italian speakers. The analysis of the
experimental data suggests that there is a
small set of concept-class?dependent rela-
tion types that are stable across languages
and robust enough to allow discrimination
across broad concept domains. Our further
research will focus on harvesting instantia-
tions of these classes from corpora.
1 Introduction
In electronic dictionaries, lexical entries can be
enriched with hyperlinks to semantically related
words. In particular, we focus here on those re-
lated words that can be seen as systematic prop-
erties of the target entry, i. e., the basic concepts
that would be used to define the entry in relation to
its superordinate category and coordinate concepts.
So, for example, for animals the most salient rela-
tions would be notions such as ?parts? and ?typical
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
behaviour?. For a horse, salient properties will in-
clude the mane and hooves as parts, and neighing
as behaviour.
Sets of relevant and salient properties allow the
user to collocate a word within its so-called ?word
field? and to distinguish it more clearly from neigh-
bour concepts, since the meaning of a word is
not defined in isolation, but in contrast to related
words in its word field (Geckeler, 2002). More-
over, knowing the typical relations of concepts in
different domains might help pedagogical lexicog-
raphy to produce structured networks where, from
each word, the learner can naturally access entries
for other words that represent properties which are
salient and distinctive for the target concept class
(parts of animals, functions of tools, etc.). We
envisage a natural application of this in the au-
tomated creation of structured vocabulary study
lists. Finally, this knowledge might be used as
a basis to populate lexical networks by building
models of concepts in terms of ?relation sketches?
based on salient typed properties (when an animal
is added to our lexicon, we know that we will have
to search a corpus to extract its parts, behaviour,
etc., whereas for a tool the function would be the
most important property to mine).
This paper provides a first step in the direction of
dictionaries enriched with cognitively salient prop-
erty descriptions by eliciting concept descriptions
from subjects speaking different languages, and
analysing the general patterns emerging from these
data.
It is worth distinguishing our approach to enrich-
ing connections in a lexical resource from the one
based on free association, such as has been recently
pursued, e. g., within the WordNet project (Boyd-
Graber et al, 2006). While we do not dispute the
usefulness of free associates, they are irrelevant to
94
our purposes, since we want to generate system-
atic, structured descriptions of concepts, in terms
of the relation types that are most salient for their
semantic fields. Knowing that the word Holland
is ?evoked? by the word tulip might be useful for
other reasons, but it does not allow us to harvest
systematic properties of flowers in order to popu-
late their relation sketch: we rather want to find
out that tulips, being flowers, will have colour as
a salient property type. As a location property of
tulips, we would prefer something like garden in-
stead of the name of a country or individual asso-
ciations. To minimise free association, we asked
participants in our experiments to produce concept
descriptions in terms of characteristic properties
of the target concepts (although we are not aware
of systematic studies comparing free associates to
concept description tasks, the latter methodology
is fairly standard in cognitive science: see sec-
tion 2.2 below).
To our knowledge, this sort of approach has
not been proposed in lexicography, yet. Cognitive
scientists focus on ?concepts?, glossing over the
fact that what subjects will produce are (strings
of) words, and as such they will be, at least to
a certain extent, language-dependent. For lexico-
graphic applications, this aspect cannot, of course,
be ignored, in particular if the goal is to produce
lexical entries for language learners (so that both
their first and their second languages should be
taken into account).
We face this issue directly in the elicitation ex-
periment we present here, in which salient rela-
tions for a set of 50 concepts from 10 different
categories are collected from comparable groups
of German and Italian speakers. In particular, we
collected data from high school students in South
Tyrol, a region situated in Northern Italy, inhabited
by both German and Italian speakers. Both Ger-
man and Italian schools exist, where the respective
non-native language is taught. It is important to
stress that the two communities are relatively sep-
arated, and most speakers are not from bilingual
families or bilingual social environments: They
study the other language as an intensively taught
L2 in school. Thus, we move in an ideal sce-
nario to test possible language-driven differences
in property descriptions, among speakers that have
a very similar cultural background.
South Tyrol also provides the concrete applica-
tive goal of our project. In public administration
and service, employees need to master both lan-
guages up to a certain standardised level (they have
to pass a ?bilingual? proficiency exam). Therefore,
there is a big need for language learning materi-
als. The practical outcome of our research will be
an extension of ELDIT1, an electronic learner?s dic-
tionary for German and Italian (Abel and Weber,
2000).
2 Related Work
Lexicographic projects providing semantic rela-
tions and experimental research on property gen-
eration are the basis for our research.
2.1 Dictionaries
In most paper-based general and learners? dictio-
naries only some information about synonyms and
sometimes antonyms is presented. Newer dictio-
naries, such as the ?Longman Language Activa-
tor? (Summers, 1999), are providing lists of related
words. While these will be useful to learners, infor-
mation about the kind of semantic relation is usu-
ally missing.
Semantic relations are often available in elec-
tronic resources, most famously in WordNet (Fell-
baum, 1998) and related projects like Kirrkirr
(Jansz et al, 1999), ALEXIA (Chanier and Selva,
1998), or as described in Fontenelle (1997). How-
ever, these resources tend to include few relation
types (hypernymy, meronymy, antonymy, etc.).
The salience of the relations chosen is not veri-
fied experimentally, and the same set of relation
types is used for all words that share the same part-
of-speech. Our results below, as well as work by
Vinson et al (2008), indicate that different concept
classes should, instead, be characterised by differ-
ent relation types (e. g., function is very salient for
tools, but not at all for animals).
2.2 Work in Cognitive Sciences
Several projects addressed the collection of prop-
erty generation data to provide the community
with feature norms to be used in different psy-
cholinguistic experiments and other analyses: Gar-
rard et al (2001) instructed subjects to complete
phrases (?concept is/has/can. . . ?), thus restricting
the set of producible feature types. McRae et
al. (2005) instructed their subjects to list concept
properties without such restrictions, but providing
them with some examples. Vinson et al (2008)
1URL http://www.eurac.edu/eldit
95
gave similar instructions, but explicitly asked sub-
jects not to freely associate.
However, these norms have been collected for
the English language. It remains to be explored
if concept representations in general and seman-
tic relations for our specific investigations have the
same properties across languages.
3 Data Collection
After choosing the concept classes and appropri-
ate concepts for the production experiment, con-
cept descriptions were collected from participants.
These were transcribed, normalised, and annotated
with semantic relation types.
3.1 Stimuli
The stimuli for the experiment consisted of 50 con-
crete concepts from 10 different classes (i. e., 5
concepts for each of the classes): mammal (dog,
horse, rabbit, bear, monkey), bird (seagull, spar-
row, woodpecker, owl, goose), fruit (apple, orange,
pear, pineapple, cherry), vegetable (corn, onion,
spinach, peas, potato), body part (eye, finger, head,
leg, hand), clothing (chemise, jacket, sweater,
shoes, socks), manipulable tool (comb, broom,
sword, paintbrush, tongs), vehicle (bus, ship, air-
plane, train, truck), furniture (table, bed, chair,
closet, armchair), and building (garage, bridge,
skyscraper, church, tower). They were mainly
taken from Garrard et al (2001) and McRae et
al. (2005). The concepts were chosen so that they
had unambiguous, reasonably monosemic lexical
realizations in both target languages.
The words representing these concepts were
translated into the two target languages, German
and Italian. A statistical analysis (using Tukey?s
honestly significant difference test as implemented
in the R toolkit2) of word length distributions
(within and across categories) showed no signif-
icant differences in either language. There were
instead significant differences in the frequency of
target words, as collected from the German, Italian
and English WaCky corpora3. In particular, words
of the class body part had significantly larger fre-
quencies across languages than the words of the
other classes (not surprisingly, the words eye, head
and hand appear much more often in corpora than
the other words in the stimuli list).
2URL http://www.r-project.org/
3URL http://wacky.sslmit.unibo.it/
3.2 Experimental Procedure
The participants in the concept description exper-
iment were students attending the last 3 years of
a German or Italian high school and reported to
be native speakers of the respective languages. 73
German and 69 Italian students participated in the
experiment, with ages ranging between 15 and 19.
The average age was 16.7 (standard deviation 0.92)
for Germans and 16.8 (s.d. 0.70) for Italians.
The experiment was conducted group-wise in
schools. Each participant was provided with a ran-
dom set of 25 concepts, each presented on a sep-
arate sheet of paper. To have an equal number of
participants describing each concept, for each ran-
domly matched subject pair the whole set of con-
cepts was randomised and divided into 2 subsets.
Each subject saw the target stimuli in his/her sub-
set in a different random order (due to technical
problems, the split was not always different across
subject pairs).
Short instructions were provided orally before
the experiment, and repeated in written format on
the front cover of the questionnaire booklet dis-
tributed to each subject. To make the concept de-
scription task more natural, we suggested that par-
ticipants should imagine a group of alien visitors,
to each of which a particular word for a concrete
object was unknown and thus had to be described.
Participants should assume that each alien visitor
knew all other words of the language apart from
the unknown (target) word.
Participants were asked to enter a descriptive
phrase per line (not necessarily a whole sentence)
and to try and write at least 4 phrases per word.
They were given a maximum of one minute per
concept, and they were not allowed to go back to
the previous pages.
Before the real experiment, subjects were pre-
sented an example concept (not in the target list)
and were encouraged to describe it while asking
clarifications about the task.
All subjects returned the questionnaire so that
for a concept we obtained, on average, descriptions
by 36.48 German subjects (s.d. 1.24) and 34.34
Italian subjects (s.d. 1.72).
3.3 Transcription and Normalisation
The collected data were digitally transcribed and
responses were manually checked to make sure
that phrases denoting different properties had been
properly split. We tried to systematically apply the
96
criterion that, if at least one participant produced
2 properties on separate lines, then the properties
would always be split in the rest of the data set.
However, this approach was not always equally
applicable in both languages. For example, Trans-
portmittel (German) and mezzo di trasporto (Ital-
ian) both are compounds used as hypernyms for
what English speakers would probably rather clas-
sify as vehicles. In contrast to Transportmittel,
mezzo di trasporto is splittable as mezzo, that can
also be used on its own to refer to a kind of vehi-
cle (and is defined more specifically by adding the
fact that it is used for transportation). The German
compound word also refers to the function of trans-
portation, but -mittel has a rather general meaning,
and would not be used alone to refer to a vehicle.
Hence, Transportmittel was kept as a whole and
the Italian quasi-equivalent was split, possibly cre-
ating a bias between the two data sets (if the Italian
string is split into mezzo and trasporto, these will
be later classified as hypernym and functional fea-
tures, respectively; if the German word is not split,
it will only receive one of these type labels). More
in general, note that in German compounds are
written as single orthographic words, whereas in
Italian the equivalent concepts are often expressed
by several words. This could also create further
bias in the data annotation and hence in the analy-
sis.
Data were then normalised and transcribed into
English, before annotating the type of semantic re-
lation. Normalisation was done in accordance with
McRae et al (2005), using their feature norms as
guidelines, and it included leaving habitual words
like ?normally,?, ?often?, ?most? etc. out, as they
just express the typicality of the concept descrip-
tion, which is the implicit task.
3.4 Mapping to Relation Types
Normalised and translated phrases were sub-
sequently labelled for relation types following
McRae et al?s criteria and using a subset of the se-
mantic relation types described in Wu and Barsa-
lou (2004): see section 4.1 below for the list of
relations used in the current analysis.
Trying to adapt the annotation style to that of
McRae et al, we encountered some dubious cases.
For example, in the McRae et al?s norms, carni-
vore is classified as a hypernym, but eats meat as
a behaviour, whereas they seem to us to convey es-
sentially the same information. In this case, we
decided to map both to eats meat (behaviour).
Among other surprising choices, the normalised
phrase used for cargo is seen by McRae et al as
a function, but used by passengers is classified as
denoting the participants in a situation. In this case,
we followed their policy.
While we tried to be consistent in relation la-
belling within and across languages, it is likely
that our own normalisation and type mapping also
include a number of inconsistencies, and our re-
sults must be interpreted by keeping this important
caveat in mind.
The average number of normalised phrases ob-
tained for a concept presented is 5.24 (s.d. 1.82) for
the German participants and 4.96 (s.d. 1.86) for the
Italian participants; in total, for a concept in our set,
the following number of phrases was obtained on
average: 191.28 (German, s.d. 25.96) and 170.42
(Italian, s.d. 25.49).
4 Results
The distribution of property types is analysed both
class-independently and within each class (sepa-
rately for German and Italian), and an unsuper-
vised clustering analysis based on property types
is conducted.
4.1 Distributional Analysis
We first look at the issue of how comparable the
German and Italian data are, starting with a check
of the overlap at the level of specific properties.
There are 226 concept?property pairs that were
produced by at least 10 German subjects; 260 pairs
were produced by at least 10 Italians. Among these
common pairs, 156 (i. e., 69% of the total Ger-
man pairs, and 60% of the Italian pairs) are shared
across the 2 languages. This suggests that the two
sets are quite similar, since the overlap of specific
pairs is strongly affected by small differences in
normalisation (e. g., has a fur, has fur and is hairy
count as completely different properties).
Of greater interest to us is to check to what
extent property types vary across languages and
across concept classes. In order to focus on the
main patterns emerging from the data, we limit our
analysis to the 6 most common property types in
the whole data set (that are also the top 6 types in
the two languages separately), accounting for 69%
of the overall responses. These types are:
? category (Wu/Barsalou code: ch;
?pear is a fruit?)
97
? (external) part (WB code: ece;
?dog has 4 legs?)
? (external) quality (WB code: ese;
?apple is green?)
? behaviour (WB code: eb;
?dog barks?)
? function (WB code: sf ;
?broom is for sweeping?)
? location (WB code: sl;
?skyscraper is found in cities?)
Figure 1 compares the distribution of property
types in the two languages via a mosaic plot
(Meyer et al, 2006), where rectangles have areas
proportional to observed frequencies in the corre-
sponding cells. The overall distribution is very
similar. The only significant differences pertain to
category and location types: Both differences are
significant at the level p < 0.0001, according to a
Pearson residual test (Zeileis et al, 2005).
For the difference in location, no clear pattern
emerges from a qualitative analysis of German and
Italian location properties. Regarding the differ-
ence in (superordinate) categories, we find, inter-
estingly, a small set of more or less abstract hy-
pernyms that are frequently produced by Italians,
but never by Germans: construction (72), object
(36), structure (16). In the these cases, the Ital-
ian translations have subtle shades of meaning that
make them more likely to be used than their Ger-
man counterparts. For example, the Italian word
oggetto (?object?) is used somewhat more con-
cretely than the extremely abstract German word
Objekt (or English ?object?, for that matter) ? in
Italian, the word might carry more of an ?arti-
fact, man-made item? meaning. At the same time,
oggetto is less colloquial than German Sache, and
thus more amenable to be entered in a written def-
inition. In addition, among others, the category ve-
hicle was more frequent in the Italian than in the
German data set (for which one reason could be the
difference between the German and Italian equiva-
lents, which was discussed in section 3.3). Differ-
ences of this sort remind us that property elicita-
tion is first and foremost a verbal task, and as such
it is constrained by language-specific usages. It is
left to future research to test to what extent linguis-
tic constraints also affect deeper conceptual repre-
sentations (would Italians be faster than Germans
type
lang
uag
e
Italian
German
cate
gory part quali
ty
beha
viour funct
ion
locat
ion
Figure 1: Cross-language distribution of property
types
at recognising superordinate properties of concepts
when they are expressed non-verbally?).
Despite the differences we just discussed, the
main trend emerging from figure 1 is one of es-
sential agreement between the two languages, and
indicates that, with some caveats, salient property
types may be cross-linguistically robust. We, thus,
turn to the issue of how such types are distributed
across concepts of different classes. This question
is visually answered by the association plots in fig-
ure 2 on the following page.
Each plot illustrates, through rectangle heights,
how much each cell deviates from the value ex-
pected given the overall contingency tables (in
our case, the reference contingency tables are the
language-specific distributions of figure 1). The
sign of the deviation is coded by direction with re-
spect to the baseline. For example, the first row
of the left plot tells us, among other things, that
in German behaviour properties are strongly over-
represented in mammals, whereas function proper-
ties are under-represented within this class. Like in
figure 1, shades of grey cue degrees of significance
of the deviation (Meyer et al, 2003).
The first observation we can make about figure 2
is how, for both languages, a large proportion of
cells show a significant departure from the overall
distribution. This confirms what has already been
observed and reported in the literature on English
norms ? see, in particular, Vinson et. al. (2008):
98
German
type
clas
s
building
furniture
vehicle
tool
clothing
body
vegetable
fruit
bird
mammal
category part qualitybehaviourfunction location
Italian
type
clas
s
building
furniture
vehicle
tool
clothing
body
vegetable
fruit
bird
mammal
category part qualitybehaviourfunction location
Figure 2: Distribution of property types across classes
property types are highly distinctive characteristics
of concept classes.
The class-specific distributions are extremely
similar in German and Italian. There is no sin-
gle case in which the same cell is deviating sig-
nificantly but in opposite directions in the two lan-
guages; and the most common pattern by far is the
one in which the two languages show the same de-
viation profile across cells, often with very simi-
lar effect sizes (compare, e. g., the behaviour and
function columns). These results suggest that prop-
erty types are not much affected by linguistic fac-
tors, an intrinsically interesting finding that also
supports our idea of structuring relation-based nav-
igation in a multi-lingual dictionary using concept-
class?specific property types.
The type patterns associated with specific con-
cept classes are not particularly surprising, and
they have been already observed in previous stud-
ies (Vinson and Vigliocco, 2008; Baroni and Lenci,
2008). In particular, living things (animals and
plants) are characterised by paucity of functional
features, that instead characterise all man-made
concepts. Within the living things, animals are
characterised by typical behaviours (they bark, fly,
etc.) and, to a lesser extent, parts (they have legs,
wings, etc.), whereas plants are characterised by
a wealth of qualities (they are sweet, yellow, etc.)
Differences are less pronounced within man-made
objects, but we can observe parts as typical of
tool and furniture descriptions. Finally, location is
a more typical definitional characteristic of build-
ings (for clothing, nothing stands out, if not, per-
haps, the pronounced lack of association with typ-
ical locations). Body parts, interestingly, have a
type profile that is very similar to the one of (ma-
nipulable) tools ? manipulable objects are, after all,
extensions of our bodies.
4.2 Clustering by Property Types
The distributional analysis presented in the previ-
ous section confirmed our main hypotheses ? that
property types are salient properties of concepts
that differ from a concept class to the other, but are
robust across languages. However, we did not take
skewing effects associated to specific concepts into
account (e. g., it could be that, say, the property
profile we observe for body parts in figure 2 is
really a deceiving average of completely oppo-
site patterns associated to, say, heads and hands).
Moreover, our analysis already assumed a division
into classes ? but the type patterns, e. g., of mam-
mals and birds are very similar, suggesting that a
higher-level ?animal? class would be more appro-
priate when structuring concepts in terms of type
profiles. We tackled both issues in an unsupervised
clustering analysis of our 50 target concepts based
on their property types. If the postulated classes
are not internally coherent, they will not form co-
herent clusters. If some classes should be merged,
they will cluster together.
Concepts were represented as 6-dimensional
vectors, with each dimension corresponding to one
99
of the 6 common types discussed above, and the
value on a dimension given by the number of times
that concept triggered a response of the relevant
type. We used the CLUTO toolkit4, selecting the
rbr method and setting all other clustering param-
eters to their default values. We explored partitions
into 2 to 10 clusters, manually evaluating the out-
put of each solution.
Both in Italian and in German, the best results
were obtained with a 3-way partition, neatly cor-
responding to the division into animals (mammals
and birds), plants (vegetables and fruits) and ob-
jects plus body parts (that, as we observed above,
have a distribution of types very similar to the one
of tools). The 2-way solution resulted in merging
two of the classes animals and plants both in Ger-
man and in Italian. The 4-way solution led to an
arbitrary partition among objects and body parts
(and not, as one could have expected, in separat-
ing objects from body parts). Similarly, the 5-
to 10-way solutions involve increasingly granular
but still arbitrary partitions within the objects/body
parts class. However, one notable aspect is that in
most cases almost all concepts of mammals and
birds, and vegetables and fruits are clustered to-
gether (both in German and Italian), expressing
their strong similarity in terms of property types
as compared to the other classes as defined here.
Looking at the 3-way solution in more detail,
in Italian, the concept horse is in the same clus-
ter with objects and body parts (as opposed to Ger-
man, where the solution is perfect). The misclassi-
fication results mainly from the fact that for horse
a lot of functional properties were obtained (which
is a feature of objects), but none of them for the
other animals in the Italian data. In German, some
functional properties were assigned to both horse
and dog, which might explain why it was not mis-
classified there.
To conclude, the type profiles associated with
animals, vegetables and objects/body parts have
enough internal coherence that they robustly iden-
tify these macro-classes in both languages. Inter-
estingly, a 3-way distinction of this sort ? exclud-
ing body parts ? is seen as fundamental on the ba-
sis of neuro-cognitive data by Caramazza and Shel-
ton (1998). On the other hand, we did not find
evidence that more granular distinctions could be
made based on the few (6) and very general types
4URL http://glaros.dtc.umn.edu/gkhome/
cluto/cluto/overview
we used. We plan to explore the distribution across
the remaining types in the future (preliminary clus-
tering experiments show that much more nuanced
discriminations, even among all 10 categories, can
be made if we use all types). However, for our ap-
plied purposes, it is sensible to focus on relatively
coarse but well-defined classes, and on just a few
common relation types (alternatively, we plan to
combine types into superordinate ones, e. g. exter-
nal and internal quality). This should simplify both
the automatic harvesting of corpus-based proper-
ties of the target types and the structuring of the
dictionary relational interface.
Finally, the peculiar object-like behaviour of
body parts on the one hand, and the special na-
ture of horse, on the other, should remind us of
how concept classification is not a trivial task, once
we try to go beyond the most obvious categories
typically studied by cognitive scientists ? animals,
plants, manipulable tools. In a lexicographic per-
spective, this problem cannot be avoided, and, in-
deed, the proposed approach should scale in diffi-
cultiese to even trickier domains, such as those of
actions or emotions.
5 Conclusion
This research is part of a project that aims to inves-
tigate the cognitive salience of semantic relations
for (pedagogical) lexicographic purposes. The re-
sulting most salient relations are to be used for re-
vising and adding to the word field entries of a mul-
tilingual electronic dictionary in a language learn-
ing environment.
We presented a multi-lingual concept descrip-
tion experiment. Participants produced differ-
ent semantic relation type patterns across concept
classes. Moreover, these patterns were robust
across the two native languages studied in the ex-
periment ? even though a closer look at the data
suggested that linguistic constraints might affect
(verbalisations of) conceptual representations (and
thus, to a certain extent, which properties are pro-
duced). This is a promising result to be used for au-
tomatically harvesting semantically related words
for a given lexical entry of a concept class.
However, the granularity of concept classes has
to be defined. In addition, to yield a larger number
of usable data for the analysis, a re-mapping of the
rare semantic relation types occurring in the actual
data set should be conducted. Moreover, the stim-
uli set will have to be expanded to include, e. g., ab-
100
stract concepts ? although we hope to mine some
abstract concept classes on the basis of the proper-
ties of our concept set (colours, for example, could
be characterised by the concrete objects of which
they are typical).
To complement the production experiment re-
sults, we aim to conduct an experiment which in-
vestigates the perceptual salience of the produced
semantic relations (and possibly additional ones),
in order to detect inconsistencies between genera-
tion and retrieval of salient properties. If, as we
hope, we will find that essentially the same proper-
ties are salient for each class across languages and
both in production and perception, we will then
have a pretty strong argument to suggest that these
are the relations one should focus on when popu-
lating multi-lingual dictionaries.
Of course, the ultimate test of our approach will
come from empirical evidence of the usefulness of
our relation links to the language learner. This is,
however, beyond the scope of the current project.
References
[Abel and Weber2000] Abel, Andrea and Vanessa We-
ber. 2000. ELDIT?A Prototype of an Innovative
Dictionary. In Heid, Ulrich, Stefan Evert, Egbert
Lehmann, and Christian Rohrer, editors, EURALEX
Proceedings, volume 2, pages 807?818, Stuttgart.
[Baroni and Lenci2008] Baroni, Marco and Alessandro
Lenci. 2008. Concepts and Properties in Word
Spaces. Italian Journal of Linguistics. To appear.
[Boyd-Graber et al2006] Boyd-Graber, Jordan, Chris-
taine Fellbaum, Daniel Osherson, and Robert
Schapire. 2006. Adding Dense, Weighted Connec-
tions to WordNet. In Proceedings of the Thirds Inter-
national WordNet Conference. Masaryk University
Brno.
[Caramazza and Shelton1998] Caramazza, Alfonso and
Jennifer R. Shelton. 1998. Domain?Specific Knowl-
edge Systems in the Brain: The Animate?Inanimate
Distinction. Journal of Cognitive Neuroscience,
10:1?34.
[Chanier and Selva1998] Chanier, Thierry and Thierry
Selva. 1998. The ALEXIA system: The Use of Vi-
sual Representations to Enhance Vocabulary Learn-
ing. In Computer Assisted Language Learning, vol-
ume 11, pages 489?522.
[Fellbaum1998] Fellbaum, Christiane, editor. 1998.
WordNet: An Electronic Lexical Database. Lan-
guage, Speech, and Communication. MIT Press,
Cambridge, MA.
[Fontenelle1997] Fontenelle, Thierry. 1997. Using a
Bilingual Dictionary to Create Semantic Networks.
International Journal of Lexicography, 10(4):275?
303.
[Garrard et al2001] Garrard, Peter, Matthew A. Lam-
bon Ralph, John R. Hodges, and Karalyn Patterson.
2001. Prototypicality, Distinctiveness, and Intercor-
relation: Analyses of the Semantic Attributes of Liv-
ing and Nonliving Concepts. Cognitive Neuropsy-
chology, 18(2):125?174.
[Geckeler2002] Geckeler, Horst. 2002. Anfa?nge und
Ausbau des Wortfeldgedankens. In Cruse, D. Alan,
Franz Hundsnurscher, Michael Job, and Peter Rolf
Lutzeier, editors, Lexikologie. Ein internationales
Handbuch zur Natur und Struktur von Wo?rtern und
Wortscha?tzen, volume 21 of Handbu?cher zur Sprach-
und Kommunikationswissenschaft, pages 713?728.
de Gruyter, Berlin ? New York.
[Jansz et al1999] Jansz, Kevin, Christopher Manning,
and Nitin Indurkha. 1999. Kirrkirr: Interactive Visu-
alisation and Multimedia From a Structured Warlpiri
Dictionary. In Proceedings of the 5th Australian
World Wide Web Conference (AusWeb?99), pages
302?316.
[McRae et al2005] McRae, Ken, George S. Cree,
Mark S. Seidenberg, and Chris McNorgan. 2005.
Semantic Feature Production Norms for a Large
Set of Living and Nonliving Things. Be-
haviour Research Methods, Instruments & Comput-
ers, 37(4):547?559.
[Meyer et al2003] Meyer, David, Achim Zeileis, and
Kurt Hornik. 2003. Visualizing Independence Using
Extended Association Plots. In Proceedings of DSC
2003. Online at URL http://www.ci.tuwien.
ac.at/Conferences/DSC-2003/.
[Meyer et al2006] Meyer, David, Achim Zeileis, and
Kurt Hornik. 2006. The Strucplot Framework: Vi-
sualizing Multi?Way Contingency Tables With vcd.
Journal of Statistical Software, 17(3):1?48.
[Summers1999] Summers, Della, editor. 1999. Long-
man Language Activator. The World?s First Produc-
tion Dictionary. Longman, Harlow.
[Vinson and Vigliocco2008] Vinson, David P. and
Gabriella Vigliocco. 2008. Semantic Feature Pro-
duction Norms for a Large Set of Objects and Events.
Behaviour Research Methods, 40(1):183?190.
[Wu and Barsalou2004] Wu, Ling?ling and
Lawrence W. Barsalou. 2004. Grounding Con-
cepts in Perceptual Simulation: I. Evidence From
Property Generation. Unpublished manuscript.
[Zeileis et al2005] Zeileis, Achim, David Meyer, and
Kurt Hornik. 2005. Residual?Based Shadings for
Visualizing (Conditional) Independence. Technical
Report 20, Department of Statistics and Mathemat-
ics, Wirtschaftsuniversita?t, Vienna.
101
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 54?62,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Predicting Cognitively Salient Modifiers
of the Constitutive Parts of Concepts
Gerhard Kremer and Marco Baroni
CIMeC, University of Trento, Italy
(gerhard.kremer|marco.baroni)@unitn.it
Abstract
When subjects describe concepts in terms
of their characteristic properties, they often
produce composite properties, e. g., rabbits
are said to have long ears, not just ears. We
present a set of simple methods to extract
the modifiers of composite properties (in
particular: parts) from corpora. We achieve
our best performance by combining evi-
dence about the association between the
modifier and the part both within the con-
text of the target concept and independently
of it. We show that this performance is rel-
atively stable across languages (Italian and
German) and for production vs. perception
of properties.
1 Introduction
Subject-generated concept descriptions in terms of
properties of different kinds (category: rabbits are
mammals, parts: they have long ears, behaviour:
they jump, . . . ) are widely used in cognitive sci-
ence as proxies to feature-based representations of
concepts in the mind (Garrard et al, 2001; McRae
et al, 2005; Vinson and Vigliocco, 2008). These
feature norms (as collections of subject-elicited
properties are called in the relevant literature) are
used in simulations of cognitive tasks and experi-
mental design. Moreover, vector spaces that have
subject-generated properties as dimensions have
been shown to be a good complement or alternative
to traditional semantic models based on corpus col-
locates (Andrews et al, 2009; Baroni et al, 2010).
Since the concept?property pairs in feature
norms resemble the tuples that relation extraction
algorithms extract from corpora (Hearst, 1992; Pan-
tel and Pennacchiotti, 2006), recent research has
attempted to extract feature-norm-like concept de-
scriptions from corpora (Almuhareb, 2006; Baroni
et al, 2010; Shaoul and Westbury, 2008). From
a practical point of view, the success of this en-
terprise would mean being able to produce much
larger norms without the need to resort to expensive
and time-consuming elicitation experiments, lead-
ing to wider cognitive simulations and possibly bet-
ter vector space models of semantics. From a the-
oretical point of view, a corpus-based system that
produces human-like concept descriptions might
provide cues of how humans themselves come up
with such descriptions.
However, the corpus-based models proposed for
this task up to this point overlook the fact that sub-
jects very often produce composite properties: Sub-
jects state that rabbits have long ears, not just ears;
cars have four wheels; a calf is a baby cow, etc.
Composite properties are not multi-word expres-
sions in the usual sense. There is nothing special
or idiomatic about long ears. It is just that we
find it to be a remarkable fact about rabbits, worth
stating in their description, that their ears are long.
In the norms described in section 3, around one
third of the part descriptions are composite. Note
that while our focus is on feature norms, a similar
point about the importance of composite properties
could be made for other knowledge repositories of
importance to computational linguistics, such as
WordNet (Fellbaum, 1998) and ConceptNet (Liu
and Singh, 2004), approximately 68,000 (36%) of
the entries and 1,300 (32%) of the part entries being
composites, respectively.
In this paper, we tackle the problem of gener-
ating composite properties from corpus data by
simplifying it in various ways. First, we focus
on part properties only, because they are com-
monly encountered in feature norms, and because
they are are commonly composite (cf. section 3).
Second, we assume that an early step in the pro-
cess of property extraction has already generated
a list of simple parts, perhaps using an existing
whole?part relation extraction algorithm (Girju et
al., 2006). Finally, we focus on composite parts
54
with an adjective?noun structure ? together with
numeral?noun cases, these constitute the near to-
tality of composite parts in the norms described
in section 3. Having thus delimited the scope of
our exploration, we will adopt the following ter-
minology: concept for the target nominal concept
(rabbit), part for the (nominal) part property (ear)
and modifier for the adjective that makes the part
composite (long).
We present simple methods that, given a list of
concept?part pairs and a POS-tagged and lemma-
tised corpus, rank and extract candidate modifiers
for the parts when predicated of the concepts. We
exploit the co-occurrence patterns of the part with
the modifier both near the concept and in other con-
texts (both kinds of co-occurrences turn out to be
helpful). We first test our methods on German fea-
ture norms, and then we show that they generalise
well by applying them to similar data in Italian, and
to the same set of German concept?part pairs when
evaluated by asking new subjects to rate the top
ranked modifiers generated by the ranking meth-
ods. This also leads to a more general discussion
of differences between modifiers produced by sub-
jects in the elicitation experiment and those that are
rated acceptable in perception, and the significance
of this for corpus-based property generation.
The paper is structured as follows. After shortly
reviewing some related work in section 2, in sec-
tion 3, we describe our feature norms focusing in
particular on composite properties. In section 4,
we describe our methods to harvest modifiers from
a corpus and report the extraction experiments,
whereas section 5 concludes by discussing direc-
tions for further work.
2 Related Work
We are not aware of other attempts to extract
concept-dependent modifiers of properties. We
review instead related work in feature norm col-
lection and prediction, and mention some rele-
vant literature on the extraction of significant co-
occurrences from corpora.
Feature-based concept description norms have
been collected in psychology for decades. Among
the more recent publicly available norms of this
sort, there are those collected by Garrard et al
(2001), Vinson and Vigliocco (2008) and McRae
et al (2005). The latter was the main methodologi-
cal inspiration for the bilingual norms we rely on
(see section 3 below). The norms of McRae and
colleagues include descriptions of 541 concrete
concepts corresponding to English nouns. The 725
subjects that rated these concepts had to list their
features on a paper questionnaire. The produced
features were then normalised and classified into
categories such as part and function by the exper-
imenters. The published norms include, among
other kinds of information, the frequency of pro-
duction of each feature for a concept by the sub-
jects.
Almuhareb (2006) was the first to attempt to
reproduce subject-generated features with text min-
ing techniques. He computed precision and re-
call measures of various pattern-based feature ex-
traction methods using Vinson and Vigliocco?s
norms for 35 concepts as a gold standard. The
best precision was around 16% at about 11% re-
call; maximum recall was around 60% with less
than 2% precision, confirming how difficult the
task is. Importantly for our purposes, Almuhareb
removed the modifier from composite features be-
fore running the experiments (1 wheel converted
to wheel), thus eschewing the main characteris-
tic of subject-generated concept descriptions that
we tackle here. Shaoul and Westbury (2008) and
Baroni et al (2010) used corpus-based semantic
space models to predict the top 10 features of 44
concepts from the McRae norms. The best model
(Baroni et al?s Strudel) guesses on average 24% of
the human-produced features, again confirming the
difficulty of the task. And, again, the test set was
pre-processed to remove modifiers of composite
features, thus sidestepping the problem we want
to deal with. It is worth remarking that, by remov-
ing modifiers, previous authors are making the task
easier in terms of feature extraction procedure (be-
cause the algorithms only need to look for single
words), but they also create artificial ?salient? fea-
tures that, once the modifier has been stripped of,
are not that salient anymore (what distinguishes a
monocycle from a tricycle is that one has 1 wheel,
the other 3, not simply having wheels). It is con-
ceivable that a method to assign sensible modifiers
to features might actually improve the overall qual-
ity of feature extraction algorithms.
Following a very long tradition in computational
linguistics (Church and Hanks, 1990), we use co-
occurrence statistics for words in certain contexts
to hypothesise a meaningful connection between
the words. In this respect, what we propose is not
different from common methods to extract and rank
55
collocations, multi-word expressions or semanti-
cally related terms (Evert, 2008). From a technical
point of view, the innovative aspect of our task is
that we do not just look for co-occurrences between
two items, but for co-occurrences in the context of
a third element, i. e., we are interested in modifier?
part pairs that are related when predicated of a
certain concept. The method we apply to the ex-
traction of modifier?part pairs when they co-occur
with the target concept in a large window is similar
to the idea of looking for partially untethered con-
textual patterns proposed by Garera and Yarowsky
(2009), that extract name?pattern?property tuples
where the pattern and the property must be adja-
cent, but the target name is only required to occur
in the same sentence.
3 Composite Parts in Feature Norms
Our empirical starting point are the feature norms
collected in parallel from 73 German and 69 Ital-
ian subjects by Kremer et al (2008), following a
methodology similar to that of McRae et al (2005).
The norms pertain to 50 concrete concepts from 10
classes such as mammals (e. g., dog), manipulable
tools (e. g., comb), etc. The concept?part pairs in
these norms served on the one hand as input to our
algorithm ? on the other hand, its output (the set of
selected modifiers from the corpus) could be evalu-
ated against those modifiers that were produced by
the subjects. Furthermore, the bilingual nature of
the norms allows us to tune our algorithm on one
language (German), and evaluate its performance
on the other (Italian), to assess its cross-lingual
generalisation capability.
To confirm that speakers actually frequently pro-
duce properties composed of part and modifier, ob-
serve that in the German data (10,010 descriptive
phrases in total), of the 1,667 parts produced, 625
(more than one third) were composite parts, and
404 were composed of an adjective and a noun, the
target of this research work. Looking at the distinct
parts that were elicited, 92 were always produced
with a modifier, 280 only without modifier, and 122
both with and without modifier. That is, for about
43% of the parts at least some speakers used a com-
posite expression of adjective and noun. This high
proportion motivates our work and is not surpris-
ing, given that, for describing a specific concept,
one will tend to come up with whatever makes this
concept special and distinguishes it from other con-
cepts ? which (considering parts) sometimes is the
part itself (elephant: trunk) and sometimes some-
thing special about the shape, colour, size, or other
attributes of the part (elephant: big ears).
The data set for modifier extraction and subse-
quent method evaluation comprises all the concept?
modifier?part triples (e. g., onion: brown peel) pro-
duced by at least one subject, taken from the Ger-
man and the Italian norms. The German (Italian)
speakers described 41 (30) different concepts by
using at least one out of 80 (45) different parts in
combination with one out of 62 (50) different mod-
ifiers, totalling to 229 (127) differently combined
triples.
4 Experiments
This section describes the approach we explored for
ranking and extracting modifiers of composite parts
and evaluates the performance of 6 different extrac-
tion methods in terms of the production norms.
Acceptance rate data from a follow-up judgement
experiment complete the evaluation.
4.1 Ranked Modifier Lists
Based on the idea that the co-occurrence of words
in a text corpus reflects to some extent how strong
these words are associated in speakers? minds
(Spence and Owens, 1990), our extraction approach
works on the lemmatised and POS-tagged German
WaCky1 web corpus of about 1.2 billion tokens.
Modifier?Part Frequencies
Using the CQP2 tool, corpus frequencies were col-
lected for all co-occurrences of adjectives with
those part nouns that were produced in the exper-
iment described above. A possible gap of up to
3 tokens between the pair of adjective and noun
allowed to extract also adjectives that are not di-
rectly adjacent to the nouns in the corpus (but in a
sequence of adjectives, for example). For each part
noun, the 5 most frequent adjective modifiers from
the ranked modifier?part list were selected under
the assumption that the preferred usage of these
modifiers with the specific part indicates the most
common attributes which that part typically has.
1See the WaCky project at http://wacky.sslmit.
unibo.it
2Corpus Query Processor (part of the IMS Open Corpus
Workbench, see http://cwb.sourceforge.net)
56
Log-Likelihood Values of Frequencies
An attempt to improve the performance of the first
method is to calculate3 the log-likelihood associ-
ation value for each modifier?part pair instead of
keeping the raw co-occurrence frequency, and se-
lect the 5 highest ranked modifiers for each part
from this list. Log-likelihood weighting should
account for typical modifiers which have a low fre-
quency but do generally not occur often in the cor-
pus, and with not many other parts ? their log-likeli-
hood value will be higher, and so will be their rank
(e. g., two-sided blade in contrast to long blade).
Modifier?Part Frequencies in Concept Context
However, both of these methods do not necessarily
yield generally atypical modifiers that are however
typical of a part when it is attributed to a specific
concept. For example, birds? beaks are typically
brown, orange or yellow, but aiming to extract mod-
ifiers for a crow?s beak, black would be one of the
desired modifiers ? which does not appear at a high
frequency rank as a generic beak modifier. The
methods described so far did not take the concept
into account when generating the modifier?part
pairs, i. e., for all concepts with a specific part the
same set of modifiers would be extracted.
To address this issue, a second frequency rank
list was prepared in the same manner ? with the
only difference that the part noun had to appear
within the context of the concept noun. That way,
also modifiers for specific concepts? parts that devi-
ate from the most typical part modifiers appear at a
high rank. However, these data are sparser, which
is why we used a wide context of 40 sentences (20
sentences before and after the part) within which
the concept had to occur (i. e., a paragraph-like con-
text size in which the topic, presumably, comprises
the concept). We refer to ranked lists of modifier?
part pairs that do not take the target concept into
account as contextless lists, and to lists within the
span of a context as in-context lists.
Due to the already mentioned data sparseness
problem, not all modifiers used for a part noun in
the production norms could be extracted with the
latter method, as some of the obvious modifiers for
specific parts are just not written about. For these,
there is a higher chance that they appear, if at all, in
the contextless rank list. For example, thin bristles
does not appear in the context of broom. In the in-
3Using the UCS toolkit, described at http://www.
collocations.de/software.html#UCS
contextless concept context
rank freq modifier freq modifier
1 507 thick 16 thick
2 209 dense 14 white
3 204 soft 11 small
4 185 black 11 soft
5 175 long 9 dense
Table 1: Top 5 modifiers from frequency rank lists
for part fur and concept bear
context list, 33% of the 229 triples extracted from
the German norms were not found (in the context-
less list, only 9% modifier?part pairs are missing).
Additionally, particular concepts, parts, or concept?
part pairs (within the 40 sentence span) might be
missing from the corpus, as well. From the Ger-
man norms collection, all concepts appeared in the
corpus, but one part (a noun?noun compound), and
6 concept?part pairs (rare, colloquial part nouns)
were missing. In the evaluation to follow, all the
modifiers pertaining to these missing data from the
corpus will be counted as positives not found by
the algorithm.
The example excerpt in table 1 shows modifiers
that were selected for bear and fur, using the two
frequency rank lists described above. Although in
this example most of the modifiers (thick, dense,
soft) are found in both lists, two arguably reason-
able modifiers are just in the contextless set (black,
long), and one only in the in-context set (white).
A disadvantage of selecting modifiers from the in-
context rank list is that many modifiers have the
same low frequency, but they should nevertheless
have differing ranks. In such cases, we assigned
ranks according to alphabetic order of modifiers.
Summed Log-Rescaled Frequencies
Next, to improve performance and profit from both
information sources the above methods provide,
the in-context and contextless rank lists were com-
bined. In one variant, the scaled frequencies for
the concept?modifier?part triples appearing in both
lists were added. Scaling was necessary because
the frequencies in the contextless list are in general
much higher than in the in-context list. Further-
more, to account for the fact that at high ranks
the difference in frequency between subsequent
ranks is much higher than at lower ranks, scaling
was done by using the logarithmic values of the fre-
57
quencies: For each concept?modifier?part triple, its
logarithmic frequency value was divided by the log-
arithmic value of the maximum corpus frequency
of all parts in the corpus (in the contextless list)
or of all concept?part pairs co-occurring within 40
sentences (in the case of the in-context list).
Productwise Combination of Frequencies
As an alternative back-off approach, the raw fre-
quencies were combined productwise into a new
list (for those modifier?part pairs missing in the in-
context list, the frequency of the pair in the context-
less list was taken alone, instead of multiplying it
by zero; i. e., the in-context term was max(freq, 1)).
This achieves a sort of ?intersective? effect, where
modifiers that are both commonly attributed to the
part and predicated of it in the context of the tar-
get concept are boosted up in the list, according to
the intuition that a good modifier should be both
plausible for the part in general, and typical for the
concept at hand.
Cosine-Based Re-Ranking
An attempt to further improve performance is based
on the idea that parts are described by some spe-
cific types of attributes. For example, a leaf would
be characterised by its shape or consistency (e. g.,
long, stiff ), whereas for fur rather colour should be
considered (e. g., white, brown). If we are able to
cluster modifiers for their attribute type and find
out which attribute types are in particular important
for a specific part, those could get a preference in
the rank list and be moved towards the top. To
approach this in a simple way, a re-ranking method
is used which is supposed to cluster and choose the
right cluster of modifiers implicitly: The modifiers
in the (productwise-) combined list were tested for
their similarity by looking if they co-occur with
the same relative frequency with the same set of
nouns. In case of high similarity (in this respect)
of a modifier to a single other modifier, or if the
modifier was similar to a lot of modifiers, it should
be re-ranked to a higher position. In more detail,
a vector was created for each modifier, denoting
its co-occurrence frequencies with each noun in
the corpus within a window of 4 tokens (on the
left side of the noun). Random indexing helped to
reduce the vector dimensionality from 27,345 to
3,000 elements (Sahlgren, 2005). These vectors
served for calculating the cosine distances between
modifiers. Then, for each of the top 200 modifiers
in the combined frequency rank list (covering 84%
of the triples from the German norms), the cosine
distance was calculated to each of the top 100 mod-
ifiers in the contextless rank list. A constant of 1
was added to each of the computed cosines, thus
obtaining a quantity between 1 and 2. The original
combined frequency value was multiplied by this
quantity (thus leaving it unchanged when the orig-
inal cosine was 0, increasing it otherwise). From
the re-ranked list resulting from this operation, we
selected, again, the top 5 modifiers of each concept?
part pair. For example, suppose that black is among
the modifiers of a crow?s beak in the combined list.
We compute the cosine similarity of black with the
top 100 modifiers of beak (in any context), and,
for each of these cosines, we multiply the original
combined value of black by cosine+1. Since the
colour is a common attribute of beaks, the presence
of modifiers like yellow and brown, high on the con-
textless beak list, helps re-ranking black high in the
crow-specific beak list. We hope that this method
helps out concept-specific values (e. g., black for
crow) of attributes that are in general typical of a
part (colour for beak).
4.2 Performance on Composite Parts From
the Production Norms
The feature norms data represented the gold stan-
dard for the evaluation of all sets of modifiers cho-
sen by each of the described methods for the given
concept?part pairs. Note that, even if a modifier?
part pair was produced only once in the fea-
ture production norms, the corresponding concept?
modifier?part triple was included in the gold stan-
dard ? which contains 41 different concepts, 80
different parts, and 62 different modifiers, totalling
to 229 concept?modifier?part triples. As in the Ger-
man corpus there are 154,935 adjective?part-noun
pairs, the random baseline (random guessing) for
finding these 229 pairs is approaching 0 (similarly
for Italian and the judgement dataset).
Figure 1 displays the performance of the meth-
ods on German in the form of a recall?precision
graph. For each rank (1?5), overall recall and inter-
polated precision values are given for all modifier?
part pairs up to this rank ? note that precision at
1% recall is overrated as it is based on an arbitrary
fraction of rank 1 pairs. As expected, extracting
modifiers of parts within a concept context (the in-
context list) achieves low recall. In contrast, modi-
fiers that were extracted by querying the corpus for
parts without considering the concept context have
58
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Method Performance
recall
prec
ision
parts in concept context (freq)contextless (freq)combination productwise (freqs)cosine re?ranking (freq product)parts in context (log?likelihood)combination by sum (log rescaled freqs)baseline: averaged random guessing
Figure 1: Evaluation on German norms
a higher recall. But this method has a lower preci-
sion in general. The performance for the method
combining frequencies productwise and for the one
that re-ranks this combined list via cosine-based
smoothing are substantially better. Not only the pre-
cision is much higher at all recall levels, but also
their maximum recall values are higher than those
of the contextless lists, i. e., it was worth combin-
ing the complementing information in the two lists.
However, the performance of the cosine-based re-
ranked list compared to the productwise-combined
list is not considerably higher, as we might have
hoped. The remaining two alternative methods per-
formed much worse: the one using log-likelihood
values as ranking criterion had in general a low pre-
cision and a low recall, and the method combining
the in-context and the contextless rank list by sum-
ming up the rescaled logarithmic frequency values
performs as bad as the contextless rank list. Never-
theless, note that all methods perform distinctively
well above the baseline.
Qualitatively analysing the data collected with
the described methods did not give definite clues
about why some performed not as good as expected.
As a comprehensible example, the modifier short
for legs is at rank 5 in the contextless list, but be-
cause of the frequent co-occurrence with monkey it
rises to rank 2 in the productwise combination of
these lists, and even to rank 1 in the cosine-based
re-ranked list. An understandable bad performing
example is the modifier yellow for the eyes of an
owl: Although it appears in the in-context list at
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Method Performance
recall
prec
ision
parts in concept context (freq)contextless (freq)combination productwise (freqs)cosine re?ranking (freq product)
Figure 2: Evaluation on Italian norms
rank 2, it is a quite infrequent modifier for eyes
in general (i. e., low in the contextless list), and
thus it is not contained in the top 5 modifiers in
the productwise combined rank list. On the other
hand, it is not perfectly clear to us why, e. g., flat for
the roof of a skyscraper, which is at rank 5 in the
contextless list and at rank 6 in the combined list,
is lowered to rank 9 in the cosine-based re-ranked
list (in the in-context list, it does not appear at all).
For all methods, collected modifiers include such
of undesired attributes not describing the part, but
other, rather situational aspects, e. g., own, left, new,
protecting, and famous. Furthermore, we observed
that some modifiers are reasonable for the respec-
tive concept?part pair, but they are counted as false
because they did not occur in the production experi-
ment (that we took as the evaluation basis), e. g., for
the blade of a sword, not only large is acceptable,
but also long and wide, essentially making the same
assertion about the size of the blade. This issue is
addressed further below by creating a new evalua-
tion standard based on plausibility judgements.
To evaluate the cross-lingual performance of
the extraction approach, the Italian norms were
explored similarly to the German norms for com-
posite parts. The gold standard here comprised
127 triples (from combinations of 30 different con-
cepts, 45 parts, and 50 different modifiers). The
same methods described above were used to ex-
tract modifiers from the Italian WaCky web corpus
(more than 1.5 billion tokens), with one difference
regarding the query for adjectives near nouns: As
59
in the Italian language adjectives in a noun phrase
can be used both before and after the noun (with
differences in their meaning), and given that most
of them were produced after the noun, we collected
all adjectives occurring up to 2 words from the left
of the noun and up to 4 words to the right.
Figure 2 shows the performance curves of the
methods for the Italian data. In this evaluation, the
method using log-likelihood values and the method
combining lists via addition of logarithmic rescaled
frequencies are omitted as their performance was
not promising at all in the German data, and they
are conceptually similar to the contextless and
productwise-combination approaches, respectively.
Like in German, the in-context method yields a
low recall, in contrast to the method not consid-
ering the presence of concepts in context. Again,
cosine-based re-ranking performs very similarly to
the method using the productwise-combined list.
For the performance on the Italian data, their differ-
ence from the simple frequency rank lists is not as
large as it is for the German data, but it is clearly
visible, especially at higher recall values.
Summarising, our comparison of various corpus-
based ranking methods to the feature production
norms, both in German and Italian, suggests that
composite parts produced by subjects are best
mined in corpora by making use of both general in-
formation about typical modifiers of the parts (the
contextless rank) and more specific information
about modifiers that co-occur with the part near the
target concept. Moreover, it is better to combine
the two information sources productwise, which
suggests an intersecting effect (the most likely mod-
ifiers are both well-attested out of context and seen
near the target concept). For both languages, there
is no strong evidence that re-ranking by cosine sim-
ilarity (a method that should favour modifiers that
are values of common attributes of a part) is im-
proving on the plain combination method (although
re-ranking is not hurting, either).
By looking at the overall performance, the re-
sults are somewhat underwhelming, with precision
around 20% at around 30% recall for the best mod-
els in both languages. A natural question at this
point is whether the modifiers ranked at the top
by the best methods and treated as false positives
because they are not in the norms are nevertheless
sensible modifiers for the parts, or whether they are
truly noise. In order to explore this issue we turn
now to our next experiment.
4.3 Performance Evaluation Based on
Plausibility Judgements
The purpose of this judgement experiment was to
see which concept?modifier?part triples the ma-
jority of participants would rate as acceptable. It
allows us to investigate two topics: (i) the compari-
son of what people produce and what they perceive
as being a prominent modifier for a concept?part
pair (our algorithm might actually provide good
candidates which were just not produced, as we just
said) and (ii) a re-evaluation of the cosine-based
re-ranking method (it could be in fact better than
we thought because we only evaluated what was
produced, but did not have a definite plausibility
rating of the candidates missing in the norms).
The tested set contained the triples yielded by
our two best performing methods (productwise
combination and cosine-based re-ranking), which
were applied to the German feature norms (692
triples, comprising 41 concepts and 71 parts). From
this set, a set of triples was chosen randomly for
each of the 46 participants (recruited by e-mail
among acquaintances of the first author). The
triples were presented to participants embedded
into a natural-sounding sentence of the form ?The
[part] of a [concept] is [modifier]?. Each partic-
ipant rated 333 sentences, presented on separate
lines of a text file (this set of sentences presented
comprised additional triples which were intended
for other purposes ? for the current evaluation, we
used a subset of 110 of these from each partici-
pant, on the average). Participants were instructed
to read the sentences as general statements about
a concept?s part and mark them by typing a let-
ter (?w? for wonderful and ?d? for dubious ? to
facilitate one-handed typing and easy memorisa-
tion) at the beginning of the line, if they thought it
plausible/unlikely that someone used the sentence
to explain an aspect of the relevant part. In total,
5,525 judgements were collected; each sentence in
the set was judged on the average by 8 persons.
The performance evaluation is based on the ac-
ceptance rate of the participants: Modifiers ac-
cepted by at least 75% of the raters are consid-
ered plausible. Figure 3 shows the recall?precision
graph for the methods tested on the concept?part
pairs from the German norms. From the 692 triples
judged, around 13% were accepted by the majority
of speakers. The precision rate is comparable with
the evaluation on the basis of the modifiers pro-
duced by participants (highest recall is 1, of course,
60
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Method Performance
recall
prec
ision
combination productwise (freqs)cosine re?ranking (freq product)
Figure 3: Evaluation on judgements (German)
because all modifiers to be judged were exclusively
from the data set selected by our methods).
Again, the performance of the cosine-based re-
ranking method is similar to the performance of the
productwise-combination method. For a more ex-
act evaluation of the difference between these two,
a last test was conducted: Instead of measuring the
performance in the form of counts of modifiers that
were accepted by the majority of participants, we
used the acceptance rates of all modifiers: The ac-
ceptance rates of all judged triples were summed up
if they contained the same concept?part pair. This
means that each concept?part pair received a score
reflecting the overall acceptance of the set of modi-
fiers for that pair (e. g., for bear: fur, all acceptance
rates for bear: brown fur, bear: soft fur, . . . were
summed up). Then, the score of each concept?part
pair in the productwise-combined list was com-
pared against the score of the same pair for the
cosine-based re-ranking method, using a pairwise
t-test (this procedure is sound because the modifiers
per pair are the same for the two methods). The
test showed a significant difference (p = 0.008), but
in favour of the productwise-combination method
(score means were slightly higher). That is, cosine-
based re-ranking in the current form brings no ad-
vantage over the simpler productwise combination
of the frequency lists.
Finally, turning to the qualitative comparison of
production and perception, there was a relatively
small overlap of triples (46) contrasting with modi-
fiers only produced but not accepted (53), and mod-
ifiers accepted but not produced (42). Intuitively,
we would have expected that what was produced
will be also accepted by the majority of people.
Possibly, some participants in the judgement ex-
periment found a few of the triples produced ques-
tionable (goose: long beak) ? such triples were in
our gold standard because we deliberately did not
want to exclude composite parts even if produced
by only one speaker ? whereas participants produc-
ing parts for given concepts probably just did not
think of specific parts or modifiers (e. g., aeroplane:
small windows and bear: dense fur). The important
fact regarding this difference is, however, that our
method captures both kinds of modifiers.
5 Discussion
We presented several corpus-based methods that
provide a set of adjective modifiers for each con-
crete concept?part pair, to be compared to those
modifiers that are salient to human subjects. The
general approach was to generate ranked lists, and
select the 5 candidates at the top of the ranks.
The best of our methods works on the simple
(productwise-) combination of frequency informa-
tion of co-occurring adjective?noun pairs with and
without considering a wide ?concept context? in
which the part noun has to occur. This method per-
formed better than the one based on co-occurrence
frequency not in concept context (generic modi-
fiers, not appropriate for every concept) and the
one based on co-occurrence frequencies in concept
context, only (low recall because of sparse data).
We evaluated the methods on feature production
norms and on plausibility judgements of generated
concept?modifier?part triples to compare produc-
tion and perception of modifiers. The performance
was similar in precision ? although the qualitative
analysis showed that modifiers produced and modi-
fiers perceived did not have a large overlap. This
means our algorithm is capable of collecting both
with the same performance.
After tuning the algorithm on German norms, we
evaluated its generalisation capability to a different
language (Italian). Performance was similar. Less
satisfying at first glance is the precision value of
just around 20% at the maximum recall level (how-
ever, when compared to the baseline of below 1%
precision, this is an essentially better value) ? as
well as the fact that our implementation of the intu-
itive idea to re-rank modifiers that are similar (and
should instantiate the same attribute) did not have
61
a performance advantage. This is subject to further
work. Moreover, using a machine-learning method
(building a binary classifier) could be tried. An-
other idea was to crawl the web and select concept-
specific text passages to build a specialised corpus.
Possibly, we could draw then from a richer infor-
mation source. A rough attempt to do this did not
seem to yield promising results.
So far, we included only adjectives as permis-
sible modifiers. A future extension could be also
aiming for numerals (e. g., four wheels). Then, for
the simulation of human-like behaviour we imag-
ine as part of the possible future work to enable
the algorithm to decide if a part noun should be
paired with a modifier, at all ? or if the part itself is
sufficient to describe a concept (big ears vs. trunk).
Regarding the evaluation, a more exact perfor-
mance measure would probably be achieved by
either having more participants producing concept
descriptions and then only selecting those modi-
fiers for the gold standard that were produced by a
majority ? or letting participants in a judgement ex-
periment also judge modifiers that were produced,
to filter out the unlikely ones.
A next step in the project will be extracting
salient parts for concepts (which we assumed to
have done already for the purpose of this paper),
possibly by integrating the information we already
collected by extracting modifiers. In the end, we
would like to come up with an adaptable method
that extracts not only parts but also other types
of relations (e. g., category, behaviour, function,
etc.), which have been already addressed in re-
lated works, though. The issue we presented in
this paper, however, is new and, we think, worth
exploring.
References
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463?498.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Kenneth Church and Peter Hanks. 1990. Word associ-
ation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Stefan Evert. 2008. Corpora and collocations. In
A. Lu?deling and M. Kyto?, editors, Corpus Linguis-
tics: An International Handbook, pages 1212?1248.
Mouton de Gruyter, Berlin, Germany.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Nikesh Garera and David Yarowsky. 2009. Structural,
transitive and latent models for biographic fact ex-
traction. In Proceedings of EACL, pages 300?308,
Athens, Greece.
Peter Garrard, Matthew Lambon Ralph, John Hodges,
and Karalyn Patterson. 2001. Prototypicality, dis-
tinctiveness, and intercorrelation: Analyses of the
semantic attributes of living and nonliving concepts.
Cognitive Neuropsychology, 18(2):25?174.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539?545, Nantes, France.
Gerhard Kremer, Andrea Abel, and Marco Baroni.
2008. Cognitively salient relations for multilingual
lexicography. In Proceedings of the COGALEX
Workshop at COLING08, pages 94?101.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
tical commonsense reasoning toolkit. BT Technol-
ogy Journal, pages 211?226.
Ken McRae, George Cree, Mark Seidenberg, and Chris
McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of COLING-ACL, pages 113?120, Sydney, Aus-
tralia.
Magnus Sahlgren. 2005. An introduction to random
indexing. http://www.sics.se/?mange/
papers/RI_intro.pdf.
Cyrus Shaoul and Chris Westbury. 2008. Performance
of HAL-like word space models on semantic cluster-
ing. In Proceedings of the ESSLLI Workshop on Dis-
tributional Lexical Semantics, pages 42?46, Ham-
burg, Germany.
Donald Spence and Kimberly Owens. 1990. Lexical
co-occurrence and association strength. Journal of
Psycholinguistic Research, 19(5):317?330.
David Vinson and Gabriella Vigliocco. 2008. Seman-
tic feature production norms for a large set of objects
and events. Behavior Research Methods, 40(1):183?
190.
62
