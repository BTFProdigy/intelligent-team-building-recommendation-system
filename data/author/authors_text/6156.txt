Corpus-Based Syntactic Error Detection Using Syntactic Patterns 
Koldo Gojenola, Maite Oronoz 
Informatika Fakultatea, 649 P. K., Euskal Herriko Unibertsitatea, 
20080 Donostia (Euskal Herria) 
jipgogak@si.ehu.es, jiboranm@si.ehu.es 
Abstract 
This paper presents a parsing system for the 
detection of syntactic errors. It combines a 
robust partial parser which obtains the main 
sentence components and a finite-state 
parser used for the description of syntactic 
error patterns. The system has been tested on 
a corpus of real texts, containing both 
correct and incorrect sentences, with 
promising results. 
Introduction 
The problem of syntactic error detection and 
correction has been addressed since the early 
years of natural language processing. Different 
techniques have been proposed for the treatment 
of the significant portion of errors (typographic, 
phonetic, cognitive and grammatical) that result 
m valid words (Weischedel and Sondheimer 
1983; Heidorn et al 1982). However, although 
most currently used word-processors actually 
provide a grammar checking module, little work 
has been done on the evaluation of results. 
There are several reasons for this: 
? Incomplete coverage. Some of the best 
parsers at the moment can analyze only a 
subset of the sentences in real texts. 
Compared to syntactic valid structures, the set 
of syntactically incorrect sentences can be 
considered almost infinite. When a sentence 
cannot be parsed it is difficult to determine 
whether it corresponds to a syntactic error or 
to an uncovered syntactic onstruction. In the 
literature, syntactic errors have been defined 
mostly with respect to their corresponding 
correct constructions. The use of unrestricted 
corpora confronts us with the problem of 
flagging a correct structure as erroneous 
(false alarms). These facts widen the scope of 
the problem, as not only incorrect structures 
but also correct ones must be taken into 
account. 
On the other hand, robust parsing systems 
(e.g., statistical ones) are often unable to 
distinguish ungrammatical structures from 
correct ones. 
24 
? The need for big corpora. Each kind of 
syntactic error occurs with very low 
frequency and, therefore, big corpora are 
needed for testing. Even if such corpora were 
available, the task of recognizing error 
instances for evaluation is a hard task, as there 
are no syntactically annotated treebanks with 
error marks for the purposes of evaluation 
and testing. Thus, to obtain naturally 
occurring test data, hundreds of texts must be 
automatically and manually examined and 
marked. 
The aim of the present work is to examine the 
feasibility of corpus-based syntactic error 
detection, with methods that are sensitive enough 
to obtain high correction rates and 
discriminating enough to maintain low false 
alarm rates. The system will be applied to 
Basque, an agglutinative language with relative 
free order among sentence components. Its 
recent standardization makes it necessary to 
develop a syntactic hecking tool. 
The remainder of this paper is organized as 
follows. After commenting on the literature on 
syntactic error detection in section 2, section 3 
presents a description of the linguistic resources 
we have used. Section 4 describes the error types 
we have treated, while section 5 gives the 
evaluation results. 
1 Background 
Kukich (1992) surveys the state of the art in 
syntactic error detection. She estimates that a 
proportion of all the errors varying between 
25% and over 50%, depending on the 
application, are valid words. Atwell and Elliott 
(1987) made a manual study concluding that 
55% of them are local syntactic errors 
(detectable by an examination of the local 
syntactic context), 18% are due to global 
syntactic errors (involving long-distance 
syntactic dependencies, which need a full parse 
of the sentence), and 27% are semantic errors. 
Regarding their treatment, different approaches 
have been proposed: 
? The relaxation of syntactic constraints 
(Douglas and Dale 1992). This grammar- 
based method allows the analysis of sentences 
\[ . . . . . . . . . . . . . . . . . . . . .  
, Sentence I 
I I 
I Morphological 
analysis and disambiguation 
II 
i 
Chart-parser 
, chart (automaton) I 
J I 
Finite-state parser 
, No Error / Error Type(s) ,J 
I . . . . . . . . . . . . . . . . . . . . .  | 
Figure 1. Overview of the system. 
that do not fulfill some of the constraints of 
the language by identifying a rule that might 
have been violated, determining whether its 
relaxation might lead to a successful parse. Its 
main disadvantage is the need of a full- 
coverage grammar, a problem not solved at 
the moment, except for restricted 
environments (Menzel and SchrOder 1999). 
? Error patterns (Kukich 1992; Golding and 
Schabes 1996; Mangu and Brill 1997), in the 
form of statistical information, hand-coded 
rules or automatically earned ones. 
? Charts have been used in grammar-based 
systems as a source of information; they can 
be resorted to if no complete analysis is 
found, so as to detect a syntactic error 
(Mellish 1989; Min-Wilson 1998). 
2 L ingu is t i c  resources  
We have used a parsing system (Aldezabal et al 
1999, 2000) divided in three main modules (see 
figure 1): 
? Morphological analysis and disambiguation. 
A robust morphological analyzer (Alegria et 
al. 1996) obtains for each word its 
segmentation(s) into component morphemes. 
After that, morphological disambiguation 
(Ezeiza et al 1998) is applied, reducing the 
high word-level ambiguity from 2.65 to 1.19 
interpretations. 
? Unification-based chart-parsing. After 
morphological nalysis and disambiguation, a 
PATR-II unification grammar is applied 
bottom-up to each sentence, giving a chart as 
a result. The grammar is partial but it gives a 
complete coverage of the main sentence 
elements, uch as noun phrases, prepositional 
phrases, sentential complements and simple 
sentences. The result is a shallow parser 
(Abney 1997) that can be used for 
subsequent processing (see figure 2). In this 
figure, dashed lines are used to indicate 
lexical elements (lemmas and morphemes), 
while plain lines define syntactic onstituents. 
Bold circles represent word-boundaries, and 
plain ones delimit morpheme-boundaries. 
The figure has been simplified, as each arc is 
actually represented by its morphological nd 
syntactic information, in the form of a 
sequence of feature-value pairs. 
? Finite-state parsing. A tool is needed that will 
allow the definition of complex linguistic 
error patterns over the chart. For that reason, 
we view the chart as an automaton to which 
finite-state constraints can be applied 
encoded in the form of automata and 
transducers (we use the Xerox Finite State 
Tool, XFST, (Karttunen et al 1997)). Finite- 
state rules provide a modular, declarative and 
flexible workbench to deal with the resulting 
chart. Among the finite-state operators used, 
we apply composition, intersection and union 
of regular expressions and relations. 
PP (in the nice house at the mountain) 
~modi f ie r  (at the mountain) 
~ -  . _ - -  ~ S (I have seen (it)) 
~ PP (in the nice house) kk ~ e s e e n ~  
mend i~o-~'O- -k~O " 0 et ~e"~)~ ~" 0 " 
Figure 2. State of the chart after the analysis ofMendiko etxepolitean ikusi dut nik ('I have seen (it) 
in the nice house at the mountain'). 
25  
Durangon, 1999ko martxoaren 7an 
In Durango, 1999, March the 7th 
(Durango, (1999 (March, (7, 
inessive, genitive) genitive, inessive 
sing) sing) sing) 
Example 1. Format of a valid date expression. 
The full system provides a robust basis, 
necessary for any treatment based on corpora. 
In the case of error detection, a solid base is 
indispensable. 
3 Er ror  detect ion  
As a test, we chose the case of date expressions 
due to several reasons: 
? It was relatively easy to obtain test data 
compared to other kinds of errors. Although 
the data must be obtained mostly manually, 
date expressions contain several cues (month 
names, year numbers) that help in the process 
of finding semiautomatically test sentences. 
In any case, manual marking is needed for all 
the retrieved sentences. 
? The context of application is wide, that is, 
date expressions contain morphologically 
and syntactically rich enough phenomena 
where several types of errors can be found. 
These can be viewed as representative of the 
set of local syntactic errors so that the same 
procedure can be used when dealing with 
other kinds of errors. Example 1 shows one 
of the formats of a date expression. 
Basque being an agglutinative language, most of 
the elements appearing in date expressions (year 
numbers, months and days) must be inflected, 
attaching to them the corresponding number 
and case morphemes. Moreover, each different 
date format requires that the elements involved 
appear in fixed combinations. This is a common 
source of errors, not detectable by a spelling- 
checker, as each isolated word-form is correct. 
For evaluation, we collected 267 essays written 
by students (with a high proportion of errors) 
and texts from newspapers and magazines, 
totaling more than 500,000 words. From them 
we chose 658 sentences, including correct dates, 
incorrect dates, and also structures 'similar' to 
dates (those sentences containing months and 
years, which could be mistaken for a date), in 
order to test false positives (see table 1). As a 
result of the selection procedure, the proportion 
of errors is higher than in normal texts. We 
divided our data into two groups. One of them 
was used for development, leaving the second 
one for the final test. The proportion of correct 
dates is higher in the case of test data with 
respect o those in the development corpus, so 
that the effect of false positives will be evaluated 
with more accuracy. 
Number of sentences 
Correct dates 
i 
\[Structures 'similar' to dates 
Incorrect dates 
Incorrect dates with 1 error 
Deve lopment  
411 
corpus 
247 
65 39 
255 171 
91 37 
Test  corpus 
43 % 47 6 % 16 
Incorrect dates with 2 errors 42 % 46 27 % 73 
Incorrect dates with 3 errors 6 % 7 4 % 11 
Table 1. Test data. 
i 
Error t~,pe 
1. The year number cannot be inflected using a hyphen 
2. The month lmartxoak) must appear in lowercase 
3. The optional locative preceding dates (Frantzia) 
must be followed by a comma 
4. The day number after a month in genitive case 
(martxoaren) must have a case mark 
5. The day number after a month in absolutive case 
(ekainak) cannot have a case mark 
6. The month (martxoan) must be inflected in genitive 
or absolutive case 
Example  
I 
Donostian, 1995-eko martxoaren 14an 
1997ko martxoak 14 
Frantzia 1997ko irailaren 8an 
Donostian, 19995eko martxoaren 22 
1998.eko ekainak 14ean argitaratua 
Donostian, 1995.eko martxoan 28an 
Combination of errors I2, 3 and 4) karrera bukatu nuenean 1997ko Ekainaren 30an 
Table 2. Most frequent error types in dates. 
26  
define NP_Mon th_Absolu t ive or_Ergat ive 
define PP Year_Genitive 
define Error_Type 5 
define Mark_Error Type__5 
NP_Month_Abs olut ive_or_Ergat ive Inflected_Number; 
\[ Error__Type_5 \] @-> BEGINERRORTYPE5 "... " ENDERRORTYPE5 
I I Optional_place_Name Optional_Cor~na PP_Year_Genit ive _ 
Example 2. Regular expressions for an error pattern. 
After examining different instances of errors, we 
chose the six most frequent error types (see table 
2). In a first phase, one or more patterns were 
defined for each error type. However, we soon 
realized that this approach failed because quite 
often two or three errors might appear in the 
same expression. This phenomenon asked for a 
kind of 'gradual relaxation' approach, which 
had to consider that several mistakes could co- 
occur. Instead of treating each error 
independently, we had to design error patterns 
bearing in mind not only the correct expression, 
but its erroneous versions as well. For example, 
the last sentence in table 2 contains three 
different errors, so that the error pattern for the 
second error should consider the possibility of 
also containing errors 3 and 4. This relaxation 
on what could be considered a correct date had 
the risk of increasing the number of false 
positives. As the number of interactions among 
errors grows exponentially with the number of  
errors (there are potentially 2 6 combinations of 
the six error types), we based our error patterns 
on the combinations actually found in the 
corpus, so that in practice that number can be 
considerably reduced (we did not find any 
expression containing more than three errors in 
the corpus). 
The error pattern for the fifth kind of error (see 
example 21 ) is defined in two steps. First, the 
syntactic pattern of the error is defined (an NP 
consisting of a month in ergative or absolutive 
case followed by an inflected number), and 
named Error_Type5. Second, a transducer 
(Mark_Error_Type_5) is defined which 
surrounds the incorrect pattem (represented by 
Number of sentences 
Undetected date errors 
Detected date errors 
False alarms 
"... ") by two error tags (BEGINERRORTYPE5 
and ENDERRORTYPE5). To further restrict the 
application of the rule, left and right contexts for 
the error can be defined (in a notation 
reminiscent of two-level morphology), mostly to 
assure that the rule is only applied to dates, thus 
preventing the possibility of obtaining false 
positives. 
Concerning the definition of error patterns, 
equal care must be taken for correct and 
incorrect dates. In a first phase, we devised rules 
for the errors but, after testing them on correct 
dates from the development corpus, we had to 
extend the rules so as to eliminate false positives. 
As a result, more than 60 morphosyntactic 
patterns (each corresponding to a finite-state 
automata or transducer) were needed for the 
definition of the six basic error patterns. They 
range from small local constraints (45 automata 
with less than 100 states) to the most complex 
patterns (a transducer with 10,000 states and 
475,000 arcs). 
4 Eva luat ion  
Table 3 shows the results? As the development 
corpus could be inspected uring the refinement 
of the parser, the results in the second and third 
columns can be understood as an upper limit of  
the parser in its current state, with 100% 
precision (no false alarms) and 91% recall. 
The system obtains 84% recall over the corpus 
of previously unseen 247 sentences? 31 errors 
out of 37 are detected giving the exact cause of  
the error (in cases with multiple errors almost all 
of them were found)? 
Development corpus 
411 
7 9% 
84 91% 
0 
Table 3. Evaluation results. 
Test corpus 
'247 
6 16% 
31 84% 
5 
i For more information on XFST regular expressions, 
see (Karttunen et al 1997)? 
27 
Example 
atxiloketa 1998ko urtarriletik irailaren 16ra ... 
the imprisonment from January 1998 till the 16th of 
September 
Donostian 1960ko Urtarrilaren jaioa 
born in Donostia in the January of 1960 
etorriko da 1997ko irailaren 26ko 1 : 15etan 
it will come the 26 of Septernber 1997 at 1:15 
atzotik 1999ko abenduaren 31arte 
,from ~esterday until the 31st of December 
Primakovek 1998ko irailaren 1 in hartu zuen ... 
Primakov took it on the 11th o\[ September 1998 
Cause of the error 
Structure similar to a date incorrectly interpreted as a 
date and flagged as erroneous. 
Incorrect Basque construction that is interpreted asa 
date. 
The system takes the hour number (1:15) as the day of 
the month. 
The grammar does not cover the arte (until) particle, so 
a correct date is flagged as ungrammatical. 
The unknown word Primakov is interpreted as a 
locative. 
Table 4. False alarms. 
Regarding precision, there are 5 false alarms, 
that is, correct dates or sentences similar to dates 
flagged as erroneous. If these false positives are 
divided by the number of sentences (247) of the 
test corpus, we can estimate the false alarm rate 
to be 2.02% over the number of dates in real 
texts. Table 4 examines ome of the false alarms, 
two of them due to expressions imilar to dates 
that are mistaken for dates, other two relate to 
constructions not taken into account in the 
design of the partial grammar, and the last one is 
due to insufficient lexical coverage. 
Although the results are promising, more corpus 
data will be needed in order to maximize 
precision. 
Conc lus ions  
This work presents the application of a parsing 
system to syntactic error detection. The reported 
experiment has as its main features: 
? It is corpus-based. If a system is to be useful, 
it must be tested on real examples of both 
correct and incorrect sentences. Although this 
may seem evident, it has not been the case for 
most of the previous work on syntactic errors. 
This implies the existence of big corpora and, 
for most of the errors, manual annotation. 
? The most successful methods for error 
detection, i.e., relaxation of syntactic 
constraints and error patterns over a chart, 
have been combined with good results. On 
the other hand, the relaxation is not applied 
dynamically at parsing time, but it has been 
manually coded. This implies a considerable 
amount of work, as we had to consider the 
formats for valid sentences as well as for all 
their incorrect variants. 
? A partial robust parsing architecture provides 
a powerful way to consider simultaneously 
information at the morphemic and syntactic 
levels. The unification grammar is necessary 
to treat aspects like complex agreement and 
word order variations, currently unsolvable 
using finite-state networks. It constructs all 
the possible syntactic components. On the 
other hand, regular expressions in the form 
of automata nd transducers are suitable for 
the definition of complex error patterns 
based on linguistic units. 
We are currently exploring new extensions to the 
system: 
? Adding new kinds of errors. Our system, as 
well as any system dealing with syntactic 
errors, suffers the problem of scaling up, as 
the addition of new types of errors will 
suppose an increment in the number of error 
patterns that involves a considerable amount 
of work in the process of  hand-coding the 
rules. The possible interaction among rules 
for different error types must be studied, 
although we expect that the rule sets will be 
mostly independent. Another interesting 
aspect is the reusability of the linguistic 
patterns: in the process of treating errors in 
dates some patterns describe general 
linguistic facts that can be reused, while 
others pertain to idiosyncratic facts of dates. 
We plan to extend the system to other 
qualitatively different ypes of errors, such as 
those involving agreement between the main 
components of the sentence, which is very 
rich in Basque, errors due to incorrect use of  
subcategorization and errors in post- 
positions. Although the number of potential 
syntactic errors is huge, we think that the 
treatment of the most frequent kinds of error 
with high recall and precision can result in 
useful grammar-checking tools. 
? Automatic acquisition of error detecting 
patterns. Although manual examination 
seems unavoidable we think that, with a 
corpus of errors big enough, machine 
learning techniques could be applied to the 
28 
problem of writing error patterns (Golding 
and Roth 1996; Mangu and Brill 1997). This 
solution would be even more useful in the 
case of combinations of different errors. In 
any case, it must be examined whether 
automatic methods reach the high precision 
and reliability obtained by hand-coded rules. 
? Using either hand-coded rules or 
automatically learned ones, both methods 
have still the problem .of obtaining and 
marking big test corpora, a process that will 
have to be made mostly manually (except for 
some limited cases like word confusion 
(Golding and Roth 1996)). This is one of the 
major bottlenecks. 
Acknowledgements 
This research is supported by the Basque 
Government, the University of the Basque 
Country and the Interministerial Commission for 
Science and Technology (CICYT). Thanks to 
Gorka Elordieta for his help writing the final 
version of the paper. 
References 
Agirre E., Gojenola K., Sarasola K., Voutilainen A. 
(1998) Towards a Single Proposal in Spelling 
Correction. COLING-ACL'98, Montreal. 
Abney S. (1997) Part-of-Speech Tagging and Partial 
Parsing. In Corpus-Based Methods in Language and 
Speech Processing, Kluwer, Dordrecht, 1997. 
Aldezabal I., Gojenola K., Oronoz M. (1999) 
Combining Chart-Parsing and Finite State Parsing. 
Proceedings of the Student Session of the European 
Summer School in Logic, Language and 
Computation (ESSLLI'99), Utrecht. 
Aldezabal I., Gojenola K., Sarasola K. (2000) A 
Bootstrapping Approach to Parser Development. 
Sixth International Workshop on Parsing 
Technologies, Trento. 
Alegria I., Artola X., Sarasola K., Urkia. M. (1996) 
Automatic morphological nalysis of Basque. Literary 
& Linguistic Computing, Vol. 11. 
Atwell E., Elliott S. (1987) Dealing with Ill-Formed 
English Text. In The Computational Analysis of 
English: a Corpus-Based Approach, De. Longman. 
Douglas, S., Dale R. 1992. Towards Robust PATR. 
COL1NG'92, Nantes. 
Ezeiza N., Alegria I., Arriola J.M., Urizar R., Aduriz I. 
(1998) Combining Stochastic and Rule-Based 
Methods for Disambiguation in Agglutinative 
Languages. COLING-ACL-98, Montreal. 
Golding A. and Schabes. Y. (1996) Combining trigram- 
based and feature-based methods for context-sensitive 
spelling correction. In Proc. of the 34th ACL 
Meeting, Santa Cruz, CA. 
Golding A., Roth. D. (1996) A Winnow-based 
Approach to Spelling Correction. Proceedings of the 
13th International Conference on Machine Learning, 
ICML'96. 
Heidom G. E., Jensen K., Miller L. A., Byrd R. J., 
Chodorow M. S. (1982) The EPISTLE text-critiquing 
system. IBM Systems Journal, Vol. 21, No. 3. 
Karttunen L., Chanod J-P., Grefenstette G., Schiller A. 
(1997) Regular Expressions For Language 
Engineering. Journal of Natural Language 
Engineering. 
Kukich K. (1992) Techniques for automatically 
correcting words in text. In ACM Computing 
Surveys, Vol. 24, N. 4, December, pp. 377-439. 
Mangu L., Brill E. (1997) Automatic Rule Acquisition 
for Spelling Correction. Proceedings of the 14th 
International Conference on Machine Learning, 
ICML'97. 
Mellish C. (1989) Some Chart-Based Techniques for 
Parsing Ill-Formed Input. EACL' 89. 
Menzel W., Schr6der I. (1999) Error Diagnosis for 
Language Learning Systems. RECALL, special 
edition, May 1999. 
Min K., Wilson W. (1998) Integrated Control of Chart 
Items for Error Repair. COLING-ACL'98, Montreal. 
Roche E., Schabes Y. (1997) Finite-State Language 
Processing. MIT Press. 
Weischedel R.M., Sondheimer N.K. (1983) Meta-rules 
as a Basis for Processing Ill-Formed Input. American 
Journal of Computational Linguistics, 9. 
29 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 31?34
Manchester, August 2008
Detecting Erroneous Uses of Complex Postpositions in an 
Agglutinative Language 
Arantza D?az de Ilarraza Koldo Gojenola Maite Oronoz  
IXA NLP group. University of the Basque Country 
jipdisaa@si.ehu.es koldo.gojenola@ehu.es maite.oronoz@ehu.es  
 
Abstract 
This work presents the development of a 
system that detects incorrect uses of com-
plex postpositions in Basque, an aggluti-
native language. Error detection in com-
plex postpositions is interesting because: 
1) the context of detection is limited to a 
few words; 2) it implies the interaction of 
multiple levels of linguistic processing 
(morphology, syntax and semantics). So, 
the system must deal with problems rang-
ing from tokenization and ambiguity to 
syntactic agreement and examination of 
local contexts. The evaluation was per-
formed in order to test both incorrect uses 
of postpositions and also false alarms.1 
1 Structure of complex postpositions  
Basque postpositions play a role similar to 
English prepositions, with the difference that 
they appear at the end of noun phrases or 
postpositional phrases. They are defined as 
?forms that represent grammatical relations 
among phrases appearing in a sentence? 
(Euskaltzaindia, 1994). There are two main types 
of postpositions in Basque: (1) a suffix appended 
to a lemma and, (2) a suffix followed by a lemma 
(main element) that can also be inflected. 
(1) etxe-tik 
house-(from the)  
from the house 
 (2) etxe-aren gain-etik  
house-(of the)  top-(from the) 
from the top of the house 
The last type of elements has been termed as 
complex postposition. We will use this term to 
name the whole sequence of two words involved, 
and not just to refer to the second element. Com-
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 
Unported license (http://creativecommons.org/ 
licenses/by-nc-sa/3.0/). Some rights reserved. 
plex postpositions can be described as: 
(3) lemma1 + (suffix1 + lemma2 + suffix2) 
In these constructions, the second lemma is fixed 
for each postposition, while the first lemma al-
lows for much more variation, ranging from 
every noun to some specific semantic classes. 
The above description (3) is intended to stress  
(with parentheses) the fact that the combination 
of both suffixes with the second lemma acts as a 
complex case-suffix that is ?appended? to the 
first lemma. Both suffixes present different com-
binations of number and case, which can agree in 
several ways, depending on the lemma, case or 
contextual factors. Table 1 shows the different 
variants of two complex postpositions, derived 
from the lemmas bitarte and aurre. For example, 
the lemma bitarte is polysemous (?means, by 
means of, instrument, while (temporal), be-
tween?). Multiple factors affect the correctness 
of a postposition, including morphological and 
syntactic constraints. We also discovered a num-
ber of relevant contextual factors, which are not 
explicitly accounted for in standard grammars. 
2 The corpus 
The detection of erroneous uses of complex 
postpositions needs first a corpus that can serve 
for both development and evaluation of the sys-
tem. To obtain such a corpus is a labor-intensive 
task, to which it must be added the examination 
and markup of incorrect examples. The use of a 
big ?correct? corpus will allow us to test our sys-
tem negatively, thoroughly testing the system?s 
behavior in respect to false alarms. We used an 
automatic system for detecting complex postpo-
sitions in order to get development and test data. 
There are two text types: Newspaper corpora 
(henceforth NC, 8,207,919 word-forms) that is 
subject to an edition process and style guides, 
and Learner corpora (LC, 994,658 word-forms), 
which come from texts written by learners of 
Basque and University students. These texts are 
more ?susceptible? of containing errors. 
31
We decided to study those types of postpositions 
that appear most frequently in texts, those con-
taining the following lemmas as their second ele-
ment: arte, aurre, bitarte, buruz, and zehar2. We 
selected these postpositions given that they are 
well documented in grammar books, with de-
tailed descriptions of their correct and incorrect 
uses (e.g. see Table 1 for bitarte), and also that 
they are very frequent in both types of texts. 
Each kind of syntactic error occurs with very 
low frequency and, therefore, big corpora are 
needed for evaluation and testing3. Even if such 
corpora are available, to obtain naturally occur-
ring test data, hundreds of texts should be manu-
ally examined and marked. As a result, we de-
cided to only manually mark errors in Learners? 
Corpora (LC), because NC, an order of magni-
tude bigger than LC, is presumed to contain less 
errors. This implies that we will be able to meas-
ure precision4 in both corpora, while recall5 will 
only be evaluated in LC. Table 2 shows the 
number of sentences used for development (60% 
of each corpus) and test (40%). We treated LC 
and NC separately, as they presumably differ in 
the number of errors. 
3 Linguistic Processing Tools 
The corpus was automatically analyzed by means 
of several linguistic processors: a morphosyntac-
tic analyzer (Aduriz et al, 2000), EUSTAGGER, 
the lemmatizer/tagger for Basque, and the Con-
straint Grammar parser (CG, Tapanainen, 1996) 
for morphological disambiguation.  
                                                 
2
 As each lemma has several meanings depending on each 
variant, we will not give their translation equivalence. 
3
 We made an estimate of more than 1% of elements in 
general corpora being complex postpositions. 
4
 Number of errors correctly identified by the system / total 
number of elements identified as erroneous. 
5
 Number of errors correctly identified by the system / total 
number of real errors. 
Added to these, we also used other resources: 
? Grammar books which describe errors in 
postpositions (Zubiri & Zubiri, 1995). 
? Place names. Two of the selected postposi-
tions (arte, aurre) are used in expressions 
that denote temporal and spatial coordinates, 
but their variants impose different restric-
tions and agreement (case, number). In order 
to recognize common nouns that refer to a 
spatial context, we made use of a new lexical 
resource: electronic versions of dictionaries 
(Sarasola, 2007; Elhuyar, 2000). 168 and 242 
words were automatically acquired from 
each dictionary. To this, we added proper 
names corresponding to places. 
? Animate/inanimate distinction. Regarding 
postpositions formed with aurre, Zubiri et al 
(1995) point out that ?typically the previous 
word takes the genitive case, although it can 
also be used without a case mark with inani-
mate nouns?. For this reason, we used a dic-
tionary enriched with semantic features, such 
as animate/inanimate, time or instrument. 
We selected 1,642 animate words. We also 
added person names and pronouns. 
4 Rule design 
The system will assign an error-tag to those 
word-forms that show the presence of an incor-
rect use of a postposition. We use the CG formal-
ism (Tapanainen, 1996) for this task. CG allows 
 
NC LC  
Dev Test Dev Test 
arte 7769 5179 1209 806 
aurre 8129 5420 1157 771 
bitarte 3846 2564 772 514 
buruz 5435 3623 560 373 
zehar 1500 1000 186 126 
Total 26679 17786 3884 2590 
Errors   60 29 
Table 2. Number of sentences in development 
and test sets, including the errors in LC.  
lemma2 suffix1 suffix 2 Examples 
-en (genitive) -z (instrumental) etxearen bitartez  (by means of the house) 
-ra (alative) -n (inessive, sg.) etxera bitartean  (while going to the house) 
-a (absolutive, sg.) -n (inessive, sg.) ordubata bitartean (around one o?clock) 
-? (no case) -n (inessive, sg.) meza bitartean (while attending mass) 
-en (genitive) -n (inessive, sg.) mendeen bitartean (between those centuries) 
-? (no case) -? /ko (no case/genitive) Lau hektarea bitarte  (in a range of four hectares) 
-ak (absolutive, pl.) 
-? /ko (no case/genitive) seiak bitarte (around six o?clock) 
bitarte 
(noun) 
-ra (alative) 
-? /ko (no case/genitive) etxera bitarte (in the way home) 
-? /-en (no 
case/genitive) 
-n/-ra/-tik/-ko (inessive/ ala-
tive/ ablative/ genitive) 
eliza aurrean (in front of the church) aurre 
(noun) 
-tik (ablative) -ra (alative) hemendik aurrera (from here onwards) 
Table 1. Complex postpositions for bitarte and aurre. 
32
the definition of complex contextual rules in or-
der to detect error patterns by means of mapping 
rules and a notation akin to regular expressions. 
Fig. 1 shows a general overview of the system. 
Syntactic constraints are encoded by means of 
CG rules using morphosyntactic categories (part 
of speech, case, number, ?). Semantic restric-
tions are enforced by lists of words belonging to 
a semantic group. All of the five postpositions 
have clear requirements about the combinations 
of case and number in the surrounding context.  
Overall, the CG grammar contains 30 rules for 
the set of 5 postpositions. We found that 
although the study of authoritative grammatical 
descriptions was exhaustive, the grammarians? 
descriptions of correct and incorrect uses refer 
mainly to morphology and syntax. Nevertheless, 
we discovered empirically that most of the rules 
needed to be extended with several classes of 
semantic restrictions. Among others, distinctions 
were needed for animate nouns, place names, or 
several classes of time expressions, depending on 
each different variant of each postposition.  
5 Evaluation 
The rules were applied both to the (presumably) 
correct newspapers texts (NC) and to the learn-
ers? texts (LC). The actual errors in LC were 
marked in advance but not in NC, which means 
that recall can only be evaluated in LC. Table 3 
shows the main results including all the selected 
five postpositions. The LC corpus contains 60 
and 29 error instances in development and test 
corpus, respectively. If we concentrate on preci-
sion, Table 4 shows the overall precision results 
for the total of errors detected in the test corpora. 
When we consider the whole set of postpositions 
precision is 50.5%, giving 42 false alarms out of 
85 detected elements. We performed an analysis 
of false alarms which showed several causes: 
? Morphological ambiguity (43% of alarms). 
? Semantic ambiguity (28%). We included sets 
of context words to identify the correct 
senses, but it still causes many false alarms. 
? Syntactic ambiguity (22%). The false alarms 
are mostly concerned with coordination. 
? Tokenization errors (7%). 
As most of the false alarms came from postpo-
sitions formed with arte, the most ambiguous 
one, we counted the errors when dealing only 
with the other four postpositions, giving a better 
precision (70.4%, second row in Table 4), al-
though detecting less true errors. If the system 
only deals with three postpositions (third row in 
Table 4), then precision reaches 78.3%. Johan-
nessen et al (2002) note that the acceptable num-
ber of false alarms in a grammar checker should 
not exceed 30%, that is, at least 70% of all 
alarms had to report true errors. Our experiments 
show that our system performs within that limit, 
albeit restricting its application to the most ?prof-
itable? postpositions. Although the number of 
rules varies widely (from 15 rules for arte to 2 
rules in the case of zehar) their effectiveness 
greatly depends on the complexity and ambiguity 
of the contextual factors. For that reason, arte 
presents the worst precision results even when it 
contains by far the biggest set of detection rules. 
On the other hand, zehar, with 2 rules, presents 
the best precision, due to its limited ambiguity. 
So, to deal with the full set postpositions (several 
works estimate more than 150), it will be more 
profitable to make a preliminary study on ambi-
guities and variants for each postposition. 
6 Related work 
Kukich (1992) surveys the state of the art in syn-
tactic error detection. She estimates that a pro-
portion of all the errors varying between 25% 
and over 50% are valid words. Atwell and Elliott 
Postpositions Precision 
arte, aurre, bitarte, buruz, zehar 50.5% (43/85)  
aurre, bitarte, buruz, zehar 70.4% (31/44) 
bitarte, buruz, zehar 78.3% (29/37) 
Table 4. Precision for the test sets (NC + LC). 
 NC LC 
 Dev Test Dev Test 
Sentences 26679 17786 3884 2590 
Errors - - 60 29 
Undetected - - 10 10 
Detected 30 24 50 19 
False alarms 45 33 2 9 
Recall - - 83% 65% 
Precision 40% 42% 96% 67% 
Table 3. Evaluation results. 
Sentences 
Morphological  
analysis 
Constraint Grammar 
parser 
No Error / Error Type 
Figure 1. General architecture. 
Error detection  
grammar 
Place 
nouns 
Animate 
nouns 
?
33
(1987) concluded that 55% of them are local syn-
tactic errors (detectable by an examination of the 
local syntactic context), 18% are due to global 
syntactic errors (which need a full parse of the 
sentence), and 27% are semantic errors. Regard-
ing their treatment, there have been proposals 
ranging from error patterns (Kukich 1992; Gold-
ing and Schabes 1996), in the form of hand-
coded rules or automatically learned ones, to sys-
tems that integrate syntactic analysis. 
(Chodorow et al, 2007) present a system for 
detecting errors in English prepositions using 
machine learning. Although both English prepo-
sitions and Basque postpositions have in some 
part relation with semantic features, Basque 
postpositions are, in our opinion, qualitatively 
more complex, as they are distributed across two 
words, and they also show different kinds of syn-
tactic agreement in case and number, together 
with a high number of variants. This is the main 
reason why we chose a knowledge-based method. 
7 Conclusions 
We have presented a system for the detection of 
errors in complex postpositions in Basque. Al-
though at first glance it could seem that postposi-
tions imply the examination of two consecutive 
words, a posterior analysis showed that they of-
fer rich and varied contexts of application, re-
quiring the inspection of several context words, 
albeit not enough to need a full syntactic or se-
mantic analysis of sentences. The system uses a 
varied set of linguistic resources, ranging from 
morphological analysis to specialized lexical re-
sources. As the detection of these errors implies a 
detailed and expert linguistic knowledge, the sys-
tem uses a purely knowledge-based approach. 
A considerable effort has been invested in the 
compilation of a corpus that provides a testbed 
for the system, which should be representative 
enough as to predict the behaviour of the system 
in an environment of a grammar checker. For 
that reason, we have tried to put a real emphasis 
on avoiding false alarms, that is, treating also lots 
of correct instances. The results show that good 
precision can be obtained. Regarding recall, our 
experiments do not allow to make an estimation, 
as the NC test corpora is too big to perform a 
detailed examination. However, the LC corpora 
can give us an upper bound of 65% (see Table 3). 
This work also shows that the use of purely 
morphosyntactic information is not enough for 
the detection of errors in postpositions. For that 
reason we were forced to also include several 
types of semantic features into the system. On 
the other hand, the process of automatic error 
detection has also helped us to explore new sets 
of semantic distinctions. So, the process of error 
detection has helped us to organize concepts into 
sets of semantically related elements, and can 
serve to make explicit types of knowledge that 
can be used to enrich other linguistic resources. 
We can conclude saying that descriptive lin-
guistics could benefit from error diagnosis and 
detection, as this could help to deeply understand 
the linguistic descriptions of postpositions, which 
are done at the moment mainly by means of 
morphosyntactic information, insufficient to give 
an account of the involved phenomena.  
Acknowledgements 
This research is supported by the University of 
the Basque Country (GIU05/52) and the Basque 
Government (ANHITZ project, IE06-185). 
References 
Aduriz I., Agirre E., Aldezabal I., Alegria I., Arregi 
X., Arriola J., Artola X., Gojenola K., Sarasola 
K.  2000. A Word-grammar based morphological 
analyzer for agglutinative languages. COLING-00. 
Atwell E., Elliott S. (1987) Dealing with Ill-Formed 
English Text. In The Computational Analysis of 
English: a Corpus-Based Approach. Longman. 
Chodorow M., Tetreault J. and Han N. 2007. Detec-
tion of Grammatical Errors Involving Prepositions. 
4th ACL-SIGSEM Workshop on Prepositions. 
D?az de Ilarraza A., Gojenola K., Oronoz M.  2008. 
Detecting Erroneous Uses of Complex Postposi-
tions in an Agglutinative Language. Internal report 
(extended version). (https://ixa.si.ehu.es/Ixa/Argitalpenak) 
Elhuyar. 2000. Modern Basque Dictionary. Elkar.  
Euskaltzaindia. 1994. Basque Grammar: First Steps 
(in Basque). Euskaltzaindia. 
Golding A. and Schabes. Y. (1996) Combining tri-
gram-based and feature-based methods for context-
sensitive spelling correction. ACL 1996. 
Johannessen J.B., Hagen K., and Lane P. 2002. The 
performance of a grammar checker with deviant 
language input. Proceedings of COLING, Taiwan. 
Kukich K. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys. 
Tapanainen P. 1996. The Constraint Grammar parser 
CG-2. Publications of the Univ. of Helsinki, 27. 
Sarasola, Ibon. 2007. Basque Dictionary (in Basque). 
Donostia : Elkar, L.G. ISBN 978-84-9783-258-8. 
Zubiri I. and  Zubiri E. 1995. E. Euskal Gramatika 
Osoa (in Basque). Didaktiker, Bilbo. 
34
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 361?365,
Dublin, Ireland, August 23-24, 2014.
IxaMed: Applying Freeling and a Perceptron Sequential Tagger at the
Shared Task on Analyzing Clinical Texts
Koldo Gojenola, Maite Oronoz, Alicia P
?
erez, Arantza Casillas
IXA Taldea (UPV-EHU)
maite.oronoz@ehu.es
http://ixa.si.ehu.es
Abstract
This paper presents the results of the Ix-
aMed team at the SemEval-2014 Shared
Task 7 on Analyzing Clinical Texts.
We have developed three different sys-
tems based on: a) exact match, b) a
general-purpose morphosyntactic analyzer
enriched with the SNOMED CT termi-
nology content, and c) a perceptron se-
quential tagger based on a Global Linear
Model. The three individual systems re-
sult in similar f-score while they vary in
their precision and recall. We have also
tried direct combinations of the individual
systems, obtaining considerable improve-
ments in performance.
1 Introduction
This paper presents the results of the IxaMed team.
The task is focused on the identification (Task A)
and normalization (Task B) of diseases and disor-
ders in clinical reports.
We have developed three different systems
based on: a) exact match, b) a general-
purpose morphosyntactic analyzer enriched with
the SNOMED CT terminology content, and c) a
perceptron sequential tagger based on a Global
Linear Model. The first system can be seen as
a baseline that can be compared with other ap-
proaches, while the other two represent two alter-
native approaches based on knowledge organized
in dictionaries/ontologies and machine learning,
respectively. We also tried direct combinations of
the individual systems, obtaining considerable im-
provements in performance.
These approaches are representative of different
solutions that have been proposed in the literature
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
(Pradhan et al., 2013), which can be broadly clas-
sified in the following types:
? Knowledge-based. This approach makes use
of large-scale dictionaries and ontologies,
that are sometimes integrated in general tools
adapted to the clinical domain, as MetaMap
(Aronson and Lang, 2010) and cTAKES (Xia
et al., 2013).
? Rule-based. For example, in (Wang and
Akella, 2013) the authors show the use
of a rule-based approach on the output of
MetaMap.
? Statistical techniques. These systems take a
training set as input and apply different vari-
ants of machine learning, such as sequen-
tial taggers based on hidden Markov mod-
els (HMMs) or conditional random fields
(CRFs) (Zuccon et al., 2013; Bodnari et al.,
2013; Gung, 2013; Hervas et al., 2013; Lea-
man et al., 2013).
? Combinations. These approaches try to take
the advantages of different system types, us-
ing methods such as voting or metaclassi-
fiers (Liu et al., 2013).
In the rest of the paper, we will first introduce
the different systems that we have developed in
section 2, presenting the main results in section 3,
and ending with the main conclusions.
2 System Description
The task of detecting diseases and their corre-
sponding concept unique identifiers (CUI) has
been faced using three methods that are described
in the following subsections.
2.1 Exact Match
The system based on Exact Match (EM) simply
obtained a list of terms and their corresponding
361
CUI identifier from the training set and marked
any appearance of those terms in the evaluation
set. This simple method was improved with some
additional extensions:
? Improving precision. In order to reduce the
number of false positives (FP), we applied
first the EM system to the training set it-
self. This process helped to measure FPs,
for example, blood gave 184 FPs and 2 true
positives (TPs). For the sake of not hurting
the recall, we allowed the system to detect
only those terms where TP > FP , that is,
?blood? would not be classified as disorder.
? Treatment of discontinuous terms. For
these terms, our system performed a soft-
matching comparison allowing a limited vari-
ation for the text comprised between the
term elements (for example ?right atrium is
mildly/moderately dilated?). These patterns
were tuned manually.
2.2 Adapting Freeling to the Medical Domain
Freeling is an open-source multilingual language
processing library providing a wide range of ana-
lyzers for several languages (Padr?o et al., 2010),
Spanish and English among others. We had al-
ready adapted Freeling to the medical domain in
Spanish (Oronoz et al., 2013), so we used our pre-
vious experience to adapt the English version to
the same domain. For the sake of clarity, we will
refer to this system as FreeMed henceforth.
The linguistic resources (lexica, grammars,. . . )
in Freeling can be modified, so we took advantage
of this flexibility extending two standard Freel-
ing dictionaries: a basic dictionary of terms con-
sisting of a unique word, and a multiword-term
dictionary. Both of them were enriched with a
dictionary of medical abbreviations
1
and with the
Systematized Nomenclature of Medicine Clinical
Terms (SNOMED CT) version dated 31st of July
of 2013. In addition to the changes in the lexica,
we added regular expressions in the tokenizer to
recognize medical terms as ?Alzheimer?s disease?
as a unique term.
In our approach, the system distinguishes be-
tween morphology and syntax on one side and
semantics on the other side. First, on the mor-
phosyntactic processing, our system only catego-
rizes word-forms using their basic part-of-speech
1
http://www.jdmd.com/abbreviations-glossary.asp
(POS) categories. Next, the semantic distinctions
are applied (the identification of the term as sub-
stance, disorder, procedure,. . . ). Following this
approach, whenever the specific term on the new
domain (biomedicine in this case) was already in
Freeling?s standard dictionaries, the specific en-
tries will not be added to the lexicon. Instead,
medical meanings are added in a later semantic
tagging stage. For example: the widely used term
?fever?, as common noun, was not added to the
lexicon but its semantic class is given in a sec-
ond stage. Only very specific terms not appear-
ing in the lexica as, for instance, ?diskospondyli-
tis? were inserted. This solution helps to avoid
an explosion of ambiguity in the morphosyntactic
analysis and, besides, it enables a clear separation
between morphosyntax and semantics.
In figure 1 the results of both levels of anal-
ysis, morphosyntactic and semantic, are shown.
The linguistic and medical information of medical
texts is stored in the Kyoto Annotation Format or
KAF (Bosma et al., 2009) that is based in the eX-
tended Markup Language (XML). In this example
the term aneurysm is analyzed as NN (meaning
noun) and it is semantically categorized as mor-
phological abnormality and disorder.
SNOMED CT is part of the Metathesaurus,
one of the elements of the Unified Medical Lan-
guage System (UMLS). We used the Metathe-
saurus vocabulary database to extract the map-
ping between SNOMED CT?s concept identifiers
and their corresponding UMLS?s concept unique
identifier (CUI). All the medical terms appearing
in SNOMED CT and analyzed with FreeMed are
tagged with both identifiers. For instance, the term
aneurysm in figure 1 has the 85659009 SNOMED
CT identifier when the term is classified in the
morphological abnormality content hierarchy and
the 432119003 identifier as disorder. Both are
linked to the same concept identifier, C0002940,
in UMLS. This mapping has been used for Task
B, whenever the CUI is the same in all the analy-
sis of the same term.
All the terms from all the 19 content hierarchies
of SNOMED CT were tagged with semantic infor-
mation in the provided texts.
The training corpus was linguistically analyzed
and its format was changed from XML to the for-
mat specified at the shared task. After a manual
inspection of the results and the Gold Standard,
some selection of terms was performed:
362
<term tid=?t241? lemma=?aneurysm? pos=?NN?>
<extRefs>
<extRef resource=?SCT 20130731? reference=?85659009?
reftype=?morphologic abnormality? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/ >
</extRef>
<extRef resource=?SCT 20130731? reference=?432119003?
reftype=?disorder? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/>
</extRef>
</extRefs>
</term>
Figure 1: Analysis with augmented information.
? Selection and combination of semantic
classes. All the terms from the disor-
der semantic class (for example ?Hypothy-
roidism?) and from the finding class (for in-
stance ?headache?) are chosen, as well as
some tag combinations (see figure 1). After
analyzing the train corpus we decided to join
into a unique term a body structure immedi-
ately followed by a disorder/finding. In this
way, we identify terms as ?MCA aneurysm?
that are composed of the MCA abbreviation
(meaning ?middle cerebral artery?) and the
inmediately following ?aneurysm? disorder.
? Filtering. Not all the terms from the men-
tioned SNOMED CT hierarchies are identi-
fied as disorders in the Gold Standard. Some
terms are discarded following these criteria:
i) findings describing personal situations (e.g.
?alcoholic?), ii) findings describing current
situations (e.g. ?awake?), iii) findings with
words indicating a negation or normal situ-
ation (e.g. ?stable blood pressure?) and iv)
too general terms (e.g. ?problems?).
The medical terms indicating disorders that are
linked to more than one CUI identifier, were
tagged as CUI-less. That is, we did not perform
any CUI disambiguation.
In subsequent iterations and after analyzing our
misses, new terms and term variations (Hina et
al., 2013) are added to the lexica in Freeling with
the restriction that, at least, one synonym should
appear in SNOMED CT. Thus, equivalent forms
were created for all the terms indicating a cancer,
a tumor, a syndrome, or a specific disease. For in-
stance, variants for the term ?cancer of colon? and
with the same SNOMED CT concept identifier
(number 363406005) are created with the forms
?colon cancer?, ?cancer of the colon? and ?can-
cer in colon?. Some abbreviation variations found
in the Gold Standard are added in the lexica too,
following the same criteria.
2.3 Perceptron Sequential Tagger
This system uses a Global Linear Model (GLM),
a sequential tagger using the perceptron algorithm
(Collins, 2002), that relies on Viterbi decoding of
training examples combined with simple additive
updates. The algorithm is competitive to other op-
tions such as maximum-entropy taggers or CRFs.
The original textual files are firstly processed by
FreeMed, and then the tagger uses all the available
information to assign tags to the text. Each token
contains information about the word form, lemma,
part of speech, and SNOMED CT category.
Our GLM system only deals with Task A, and
it will not tackle the problem of concept normal-
ization, due to time constraints. In this respect, for
Task B the GLM system will simply return the first
SNOMED CT category given by FreeMed. This
does not mean that GLM and FreeMed will give
the same result for Task B, as the GLM system
first categorizes each element as a disease, and it
gives a CUI only when that element is identified.
2.4 Combinations
The previous subsections presented three differ-
ent approaches to the problem that obtain com-
parable scores (see table 1). In the area of auto-
matic tagging, there are several works that com-
bine disparate systems, usually getting good re-
sults. For this reason, we tried the simplest ap-
proach of merging the outputs of the three individ-
ual systems into a single file.
3 Results
Table 1 presents the results of the individual and
combined systems on the development set. Look-
ing at the individual systems on Task A, we can see
that all of them obtain a similar f-score, although
there are important differences in terms of preci-
sion and recall. Contrary to our initial intuition,
the FreeMed system, based on dictionaries and on-
tologies, gives the best precision and the lowest re-
call. In principle, having SNOMED CT as a base,
we could expect that the coverage would be more
complete (attaining the highest recall). However,
the results show that there is a gap between the
writing of the standard SNOMED CT terms and
the terms written by doctors in their notes. On the
other hand, the sequential tagger gives the best re-
363
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
INDIVIDUAL SYSTEMS
Exact Match (EM) 0.804 0.505 0.620 0.958 0.604 0.740 0.479 0.948
FreeMed 0.822 0.501 0.622 0.947 0.578 0.718 0.240 0.479
GLM 0.715 0.570 0.634 0.908 0.735 0.813 0.298 0.522
COMBINATIONS
FreeMed + EM 0.766 0.652 0.704 0.936 0.754 0.835 0.556 0.855
FreeMed + GLM 0.689 0.668 0.678 0.903 0.790 0.843 0.345 0.518
EM + GLM 0.680 0.679 0.679 0.907 0.819 0.861 0.398 0.598
FreeMed + EM + GLM 0.659 0.724 0.690 0.899 0.845 0.871 0.421 0.584
Table 1: Results of the different systems on the development set.
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
FreeMed + EM 0.729 0.701 0.715 0.885 0.808 0.845 0.604 0.862
FreeMed + EM + GLM 0.681 0.786 0.730 0.872 0.890 0.881 0.439 0.558
Best system 0.843 0.786 0.813 0.936 0.866 0.900 0.741 0.873
Table 2: Results on the test set.
call. Since the tagger uses both contextual words
and prefixes and suffixes as features for learning,
this method has proven helpful for the recognition
of terms that do not appear in the training data (see
the difference with the EM approach).
Looking at the different combinations in table 1,
we see that two approaches work best, either com-
bining FreeMed and EM, or combining the three
individual systems. The inclusion of GLM results
in the best coverage, but at the expense of preci-
sion. On the other hand, combining FreeMed and
EM gives a better precision but lower coverage.
As pointed out by Collins (2002), the results of
the perceptron tagger are competitive with respect
to other statistical approaches such as CRFs (Zuc-
con et al., 2013; Bodnari et al., 2013; Gung, 2013;
Hervas et al., 2013; Leaman et al., 2013).
Regarding Task B, we can see that the EM sys-
tem is by far the most accurate, while FreeMed
is well below its a priori potential. The reason of
this low result is mainly due to the high ambiguity
found on the output of the SNOMED CT tagger, as
many terms are associated with more than one CUI
and, consequently, are left untagged. This problem
deserves future work on automatic semantic dis-
ambiguation. On the combinations, FreeMed and
EM together give the best result. However, as we
told before, the GLM system was only trained for
Task A, so it is not surprising to see that its results
deteriorate the accuracy in Task B.
We chose these best two combinations for the
evaluation on the test set (using training and de-
velopment for experimentation or training), which
are presented in table 2. Here we can see that re-
sults on the development also hold on the test set.
Given the unsophisticated approach to combine
the systems, we can figure out more elaborated so-
lutions, such as majority or weighted voting, or
even more, the definition of a machine learning
classifier to select the best system for every pro-
posed term. These ideas are left for future work.
4 Conclusions
We have presented the IxaMed approach, com-
posed of three systems that are based on exact
match, linguistic and knowledge repositories, and
a statistical tagger, respectively. The results of in-
dividual systems are comparable, with differences
in precision and recall. We also tested a sim-
ple combination of the systems, which proved to
give significant improvements over each individ-
ual system. The results are competitive, although
still far from the winning system.
For future work, we plan to further improve the
individual systems. Besides, we hope that the ex-
perimentation with new combination approaches
will offer room for improvement.
Acknowledgements
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
364
References
Alan R Aronson and Francois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association (JAMIA), 17:229?236.
Andreea Bodnari, Louise Deleger, Thomas Lavergne,
Aurelie Neveol, and Pierre Zweigenbaum. 2013.
A Supervised Named-Entity Extraction System for
Medical Text. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. KAF: a
Generic Semantic Annotation Format. In Proceed-
ings of the 5th International Conference on Gener-
ative Approaches to the Lexicon GL, pages 17?19,
Septembre.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing, pages 1?8. Asso-
ciation for Computational Linguistics, July.
James Gung. 2013. Using Relations for Identification
and Normalization of Disorders: Team CLEAR in
the ShARe/CLEF 2013 eHealth Evaluation Lab. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Lucia Hervas, Victor Martinez, Irene Sanchez, and Al-
berto Diaz. 2013. UCM at CLEF eHealth 2013
Shared Task1. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Saman Hina, Eric Atwell, and Owen Johnson. 2013.
SnoMedTagger: A semantic tagger for medical nar-
ratives. In Conference on Intelligent Text Processing
and Computational Linguistics (CICLING).
Robert Leaman, Ritu Khare, and Zhiyong Lu. 2013.
NCBI at 2013 ShARe/CLEF eHealth Shared Task:
Disorder Normalization in Clinical Notes with
Dnorm. In Online Working Notes of the CLEF 2013
Evaluation Labs and Workshop, September.
Hongfang Liu, Kavishwar Wagholikar, Siddhartha Jon-
nalagadda, and Sunghwan Sohn. 2013. Integrated
cTAKES for Concept Mention Detection and Nor-
malization. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic Annotation of
Medical Records in Spanish with Disease, Drug
and Substance Names. In Lecture Notes in Com-
puter Science, 8259. Progress in Pattern Recogni-
tion, ImageAnalysis, ComputerVision, and Applica-
tions 18th Iberoamerican Congress, CIARP 2013,
Havana, Cuba, November 20-23.
Lluis Padr?o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010. Semantic Services in Freeling 2.1:
WordNet and UKB. In Global Wordnet Conference,
Mumbai, India.
Sameer Pradhan, Noemie Elhadad, Brett R. South,
David Martinez, Lee Christensen, Amy Vogel,
Hanna Suominen, Wendy W. Chapman, and Guer-
gana Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Online Working Notes
of the CLEF 2013 Evaluation Labs and Workshop,
September.
Chunye Wang and Ramakrishna Akella. 2013. UCSCs
System for CLEF eHealth 2013 Task 1. In Online
Working Notes of the CLEF 2013 Evaluation Labs
and Workshop, September.
Yunqing Xia, Xiaoshi Zhong, Peng Liu, Cheng Tan,
Sen Na, Qinan Hu, and Yaohai Huang. 2013. Com-
bining MetaMap and cTAKES in Disorder Recogni-
tion: THCIB at CLEF eHealth Lab 2013 Task 1. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Guido Zuccon, Alexander Holloway, Bevan Koop-
man, and Anthony Nguyen. 2013. Identify Disor-
ders in Health Records using Conditional Random
Fields and Metamap AEHRC at ShARe/CLEF 2013
eHealth Evaluation Lab Task 1. In Online Working
Notes of the CLEF 2013 Evaluation Labs and Work-
shop, September.
365
Proceedings of BioNLP Shared Task 2011 Workshop, pages 138?142,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Using Kybots for Extracting Events in Biomedical Texts
Arantza Casillas (*)
arantza.casillas@ehu.es
Arantza D??az de Ilarraza (?)
a.diazdeilarraza@ehu.es
Koldo Gojenola (?)
koldo.gojenola@ehu.es
Maite Oronoz (?)
maite.oronoz@ehu.es
German Rigau (?)
german.rigau@ehu.es
IXA Taldea UPV/EHU
(*) Department of Electricity and Electronics
(?) Department of Computer Languages and Systems
Abstract
In this paper we describe a rule-based sys-
tem developed for the BioNLP 2011 GENIA
event detection task. The system applies Ky-
bots (Knowledge Yielding Robots) on anno-
tated texts to extract bio-events involving pro-
teins or genes. The main goal of this work is to
verify the usefulness and portability of the Ky-
bot technology to the domain of biomedicine.
1 Introduction
The aim of the BioNLP?11 Genia Shared Task (Kim
et al, 2011b) concerns the detection of molecular
biology events in biomedical texts using NLP tools
and methods. It requires the identification of events
together with their gene or protein arguments. Nine
event types are considered: localization, binding,
gene expression, transcription, protein catabolism,
phosphorylation, regulation, positive regulation and
negative regulation.
When identifying the events related to the given
proteins, it is mandatory to detect also the event
triggers, together with its associated event-type, and
recognize their primary arguments. There are ?sim-
ple? events, concerning an event together with its
arguments (Theme, Site, ...) and also ?complex?
events, or events that have other events as secundary
arguments. Our system did not participate in the op-
tional tasks of recognizing negation and speculation.
The training dataset contained 909 texts together
with a development dataset of 259 texts. 347 texts
were used for testing the system.
The main objective of the present work was to ver-
ify the applicability of a new Information Extraction
(IE) technology developed in the KYOTO project1
(Vossen et al, 2008), to a new specific domain. The
KYOTO system comprises a general and extensible
multilingual architecture for the extraction of con-
ceptual and factual knowledge from texts, which has
already been applied to the environmental domain.
Currently, our system follows a rule-based ap-
proach (i.e. (Kim et al, 2009), (Kim et al, 2011a),
(Cohen et al, 2011) or (Vlachos, 2009)), using a set
of manually developed rules.
2 System Description
Our system proceeds in two phases. Firstly, text doc-
uments are tokenized and structured using an XML
layered structure called KYOTO Annotation Format
(KAF) (Bosma et al, 2009). Secondly, a set of Ky-
bots (Knowledge Yielding Robots) are applied to de-
tect the biological events of interest occurring in the
KAF documents. Kybots form a collection of gen-
eral morpho-syntactic and semantic patterns on se-
quences of KAF terms. These patterns are defined
in a declarative format using Kybot profiles.
2.1 KAF
Firstly, basic linguistic processors apply segmenta-
tion and tokenization to the text. Additionally, the
offset positions of the proteins given by the task or-
ganizers are also considered. The output of this ba-
sic processing is stored in KAF, where words, terms,
syntactic and semantic information can be stored in
separate layers with references across them.
Currently, our system only considers a minimal
amount of linguistic information. We are only using
1http://www.kyoto-project.eu/
138
the word form and term layers. Figure 1 shows an
example of a KAF document where proteins have
been annotated using a special POS tag (PRT). Note
that our approach did not use any external resource
apart of the basic linguistic processing.
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<KAF xml:lang="en">
<text>
...
<wf wid="w210" sent="10">phosphorylation</wf>
<wf wid="w211" sent="10">of</wf>
<wf wid="w212" sent="10">I</wf>
<wf wid="w213" sent="10">kappaB</wf>
<wf wid="w214" sent="10">alpha<...
</text>
<term tid="t210" type="open" lemma="phosphorylation"
start="1195" end="1210" pos="W">
<span><target id="w210"/></span>
</term>
<term tid="t211" type="open" lemma="of"
start="1211" end="1213" pos="W">
<span><target id="w211"/></span>
</term>
<term tid="T5" type="open" lemma="I kappaB alpha"
start="1214" end="1228" pos="PRT">
<span><target id="w212"/></span>
<target id="w213"/>
<target id="w214"/></span>
</term>...
</terms>
</KAF>
Figure 1: Example of a document in KAF format.
2.2 Kybots
Kybots (Knowledge Yielding Robots) are abstract
patterns that detect actual concept instances and re-
lations in KAF. The extraction of factual knowledge
by the mining module is done by processing these
abstract patterns on the KAF documents. These pat-
terns are defined in a declarative format using Kybot
profiles, which describe general morpho-syntactic
and semantic conditions on sequences of terms. Ky-
bot profiles are compiled to XQueries to efficiently
scan over KAF documents uploaded into an XML
database. These patterns extract and rank the rele-
vant information from each match.
Kybot profiles are described using XML syn-
tax and each one consists of three main declarative
parts:
? Variables: In this part, the entities and its prop-
erties are defined
? Relations: This part specifies the positional re-
lations among the previously defined variables
? Events: describes the output to be produced for
every matching
Variables (see the Kybot section variables in fig-
ure 2) describe the term variables used by the Kybot.
They have been designed with the aim of being flex-
ible enough to deal with many different information
associated with the KAF terms including semantic
and ontological statements.
Relations (see the Kybot section relations in fig-
ure 2) define the sequence of variables the Kybot
is looking for. For example, in the Kybot in fig-
ure 2, the variable named Phosphorylation
is the main pivot, the variable Of must follow
Phosphorylation (immediate is true indi-
cating that it must be the next term in the sequence),
and a variable representing a Proteinmust follow
Of. Proteins and genes are identified with the PRT
tag.
Events (expressions marked as events in figure 2)
describes the output template of the Kybot. For ev-
ery matched pattern, the kybot produces a new event
filling the template structure with the selected pieces
of information. For example, the Kybot in figure 2
selects some features of the event represented with
the variable called Phosphorylation: its term-
identification (@tid), its lemma, part of speech and
offset. The expression also describes that the vari-
able Protein plays the role of being the ?Theme?
of the event. The output obtained when aplying the
Kybot in figure 2 is shown in figure 3. Comparing
the examples in table 1 and in figure 3 we observe
that all the features needed for generating the files
for describing the results are also produced by the
Kybot.
<doc shortname="PMID-9032271.kaf">
<event eid="e1" target="t210" kybot="phosphorylation of P"
type="Phosphorylation"
lemma="phosphorylation" start="1195" end="1210" />
<role target="T5" rtype="Theme"
lemma="I kappaB alpha" start="1214" end="1228" />
</doc>
Figure 3: Output obtained after the application of the Ky-
bot in figure 2.
3 GENIA Event Extraction Task and
Results
We developed a set of basic auxiliary pro-
grams to extract event patterns from the train-
ing corpus. These programs obtain the struc-
139
<?xml version="1.0" encoding="utf-8"?>
<!-- Sentence: phosphorylation of Protein
Event1: phosphorylation
Role: Theme Protein -->
<Kybot id="bionlp">
<variables>
<var name="Phosphorylation" type="term" lemma="phosphorylat*>
<var name="Of" type="term" lemma="of"/>
<var name="Protein" type="term" pos="PRT"/>
</variables>
<relations>
<root span="Phosphorylation"/>
<rel span="Of" pivot="Phosphorylation" direction="following" immediate="true"/>
<rel span="Protein" pivot="Of" direction="following" immediate="true"/>
</relations>
<events>
<event eid="" target="$Phosphorylation/@tid" kybot="phosphorylation of P"
type="Phosphorylation" lemma="$Phosphorylation/@lemma"
pos="$Phosphorylation/@pos" start="$Phosphorylation/@start" end="$Phosphorylation/@end"/>
<role target="$Protein/@tid" rtype="Theme" lemma="$Protein/@lemma" start="$Protein/@start"
end="$Protein/@end"/>
</events>
</Kybot>
Figure 2: Example of a Kybot for the pattern Event of Protein.
.a1 file
T5 Protein 1214 1228 I kappaB alpha
.a2 file
T20 Phosphorylation 1195 1210 phosphorylation
E7 Phosphorylation:T20 Theme:T5
Table 1: Results in the format required in the GENIA
shared task.
ture of the events, their associated trigger words
and their frequency. For example, in the
training corpus, a pattern of the type Event
of Protein appears 35 times, where the
Event is further described as phosporylation,
phosphorylated.... Taking the most fre-
quently occurring patterns in the training data into
account, we manually developed the set of Kybots
used to extract the events from the development and
test corpora. For example, in this way we wrote the
Kybot in figure 2 that fulfils the conditions of the
pattern of interest.
The two phases mentioned in section 2, corre-
sponding to the generation of the KAF documents
and the application of Kybots, have different input
files depending on the type of event we want to
detect: simple or complex events. When extract-
ing simple events (see figure 4), we used the in-
put text and the files containing protein annotations
(?.a1? files in the task) to generate the KAF docu-
ments. These KAF documents and Kybots for sim-
ple events are provided to the mining module. In
the case of complex events (events that have other
KAF generator
.txt .a1
.kaf
Kybot processor
Kybots
(Simple)
.a2
Figure 4: Application of Kybots. Simple events.
events as arguments), the identifiers of the detected
simple events are added to the KAF document in the
first phase. A new set of Kybots describing complex
events and KAF (now with annotations of the simple
events) are used to obtain the final result (see figure
5).
For the evaluation, we also developed some pro-
grams for adapting the output of the Kybots (see fig-
ure 3) to the required format (see table 1).
We used the development corpus to improve the
Kybot performance. We developed 65 Kybots for
detecting simple events. Table 2 shows the number
of Kybots for each event type. Complex events rela-
tive to regulation (also including negative and posi-
tive regulations) were detected using a set of 24 Ky-
bots.
The evaluation of the task was based on the output
140
KAF generator
.a2 .kaf
.kaf
(with simple events)
Kybot processor
Kybots
(Complex)
.a2
Figure 5: Application of Kybots. Complex events.
Event Class Simple Kyb. Complex Kyb.
Transcription 10
Protein Catabolism 5
Binding 5
Regulation 3
Negative Regulation 5 4
Positive Regulation 3 17
Localization 7
Phosphorylation 18
Gene Exrpesion 12
Total 65 24
Table 2: Number of Kybots generated for each event.
of the system when applied to the test dataset of 347
previously unseen texts. Table 3 shows in the Gold
column the number of instances for each event-type
in the test corpus. R, P and F-score columns stand
for the recall, precision and f-score the system ob-
tained for each type of event. As a consequence of
the characteristics of our system, precision is primed
over recall. For example, the system obtains 95%
and 97% precision on Phosphorylation an Localiza-
tion events, respectively, although its recall is con-
siderably lower (41% and 19%).
4 Conclusions and Future work
This work presents the first results of the applica-
tion of the KYOTO text mining system for extracting
events when ported to the biomedical domain. The
KYOTO technology and data formats have shown to
be flexible enough to be easily adapted to a new task
and domain. Although the results are far from satis-
factory, we must take into account the limited effort
we dedicated to adapting the system and designing
the kybots, which can be roughly estimated in two
Event Class Gold R P F-score
Localization 191 19.90 97.44 33.04
Binding 491 5.30 50.00 9.58
Gene Expression 1002 54.19 42.22 47.47
Transcription 174 13.22 62.16 21.80
Protein catabolism 15 26.67 44.44 33.33
Phosphorylation 185 41.62 95.06 57.89
Non-reg total 2058 34.55 47.27 39.92
Regulation 385 7.53 9.63 8.45
Positive regulation 1443 6.38 62.16 11.57
Negative regulation 571 3.15 26.87 5.64
Regulatory total 2399 5.79 26.94 9.54
All total 4457 19.07 42.08 26.25
Table 3: Performance analysis on the test dataset.
person/months.
After the final evaluation, our system obtained the
thirteenth position out of 15 participating systems
in the main task (processing PubMed abstracts and
full paper articles), obtaining 19.07%, 42.08% and
26.25 recall, precision an f-score, respectively, far
from the best competing system (49.41%, 64.75%
and 56.04%). Although they are far from satisfac-
tory, we must take into account the limited time we
dedicated to adapting the system and designing the
kybots. Apart from that, due to time restrictions,
our system did not make use of the ample set of
resources available, such as named entities, corefer-
ence resolution or syntactic parsing of the sentences.
On the other hand, the system, based on manually
developed rules, obtains reasonable accuracy in the
task of processing full paper articles, obtaining 45%
precision and 21% recall, compared to 59% and 47%
for the best system, which means that the rule-based
approach performs more robustly when dealing with
long texts (5 full texts correspond to approximately
150 abstracts). As we have said before, our main
objective was to evaluate the capabilities of the KY-
OTO technology without adding any additional in-
formation. The use of more linguistic information
will probably facilitate our work and will benefit the
system results. In the near future we will study the
application of machine learning techniques for the
automatic generation of Kybots from the training
data. We also plan to include additional linguistic
and semantic processing in the event extraction pro-
cess to exploit the current semantic and ontological
capabilities of the KYOTO technology.
141
Acknowledgments
This research was supported by the the KYOTO
project (STREP European Community ICT-2007-
211423) and the Basque Government (IT344-10).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini and Carlo Aliprandi. KAF: a generic semantic
annotation format Proceedings of the 5th International
Conference on Generative Approaches to the Lexicon
GL 2009 Pisa, Italy, September 17-19, 2009
Kevin Bretonnel Cohen, Karin Verspoor, Helen L. John-
son, Chris Roeder, Philip V. Ogren, Willian A. Baum-
gartner, Elizabeth White, Hannah Tipney, and Lawer-
ence Hunter. High-precision biological event extrac-
tion: Effects of system and data. Computational Intel-
ligence, to appear, 2011.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. Overview of
BioNLP?09 Shared Task on Event Extraction. Pro-
ceedings of the BioNLP 2009 Workshop. Association
for Computational Linguistics. Boulder, Colorado, pp.
89?96., 2011
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. Proceedings of the
BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Andreas Vlachos. Two Strong Baselines for the BioNLP
2009 Event Extraction Task. Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing. Association for Computational Linguistics Upp-
sala, Sweden, pp. 1?9., 2010
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Chris-
tiane Fellbaum, Shu-kai Hsieh, Chu-Ren Huang, Hi-
toshi Isahara, Kyoko Kanzaki, Andrea Marchetti,
Monica Monachini, Federico Neri, Remo Raffaelli,
German Rigau, Maurizio Tescon, Joop VanGent. KY-
OTO: A System for Mining, Structuring, and Dis-
tributing Knowledge Across Languages and Cultures.
Proceedings of LREC 2008. Marrakech, Morocco,
2008.
142
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 60?64,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
First approaches on Spanish medical record classification using
Diagnostic Term to class transduction
A. Casillas(1), A. D??az de Ilarraza(2), K. Gojenola(2), M. Oronoz(2), A. Pe?rez(2)
(1) Dep. Electricity and Electronics
(2) Dep. Computer Languages and Systems
University of the Basque Country (UPV/EHU)
arantza.casillas@ehu.es
Abstract
This paper presents an application of finite-
state transducers to the domain of medicine.
The objective is to assign disease codes to
each Diagnostic Term in the medical records
generated by the Basque Health Hospital Sys-
tem. As a starting point, a set of manually
coded medical records were collected in order
to code new medical records on the basis of
this set of positive samples. Since the texts
are written in natural language by doctors, the
same Diagnostic Term might show alternative
forms. Hence, trying to code a new medical
record by exact matching the samples in the
set is not always feasible due to sparsity of
data. In an attempt to increase the coverage
of the data, our work centered on applying a
set of finite-state transducers that helped the
matching process between the positive sam-
ples and a set of new entries. That is, these
transducers allowed not only exact matching
but also approximate matching. While there
are related works in languages such as En-
glish, this work presents the first results on au-
tomatic assignment of disease codes to medi-
cal records written in Spanish.
1 Introduction
During the last years an exponential increase in
the number of electronic documents in the medi-
cal domain has occurred. The automatic process-
ing of these documents allows to retrieve informa-
tion, helping the health professionals in their work.
There are different sort of valuable data that help to
exploit medical information. Our framework lays
on the classification of Medical Records (MRs) ac-
cording to a standard. In our context, the MRs pro-
duced in a hospital have to be classified with re-
spect to the World Health Organization?s 9th Revi-
sion of the International Classification of Diseases1
(ICD-9). ICD-9 is designed for the classification of
morbidity and mortality information and for the in-
dexing of hospital records by disease and procedure.
The already classified MRs are stored in a database
that serves for further classification purposes. Each
MR consists of two pieces of information:
Diagnostic Terms (DTs): one or more terms that
describe the diseases corresponding to the MR.
Body-text: a description of the patient?s details,
antecedents, symptoms, adverse effects, meth-
ods of administration of medicines etc.
Even though the DTs are within a limited domain,
their description is not subject to a standard. Doc-
tors express the DTs in natural language with their
own style and different degrees of precision. Usu-
ally, a given concept might be expressed by alterna-
tive DTs with variations due to modifiers, abbrevia-
tions, acronyms, dates, names, misspellings or style.
This is a typical problem that arises in natural lan-
guage processing due to the fact that doctors focus
on the patients and not so much on the writing of the
MR. On account of this, there is ample variability in
the presentation of the DTs. Consequently, it is not
a straightforward task to get the corresponding ICD-
codes. That is, the task is by far more complex than
a standard dictionary lookup.
1http://www.cdc.gov/nchs/icd/icd9.htm
60
The Basque Health Hospital System is concerned
with the automatization of this ICD-code assign-
ment task. So far, the hospital processes the daily
produced documents in the following sequence:
1. Automatic: exact match of the DTs in a set of
manually coded samples.
2. Semi-automatic: through semantic match,
ranking the DTs by means of machine-learning
techniques. This stage requires that experts se-
lect amongst the ranked choices.
3. Manual: the documents that were not matched
in the previous two stages are examined by pro-
fessional coders assigning the codes manually.
The goal of this paper is to bypass the variability
associated to natural language descriptions in an at-
tempt to maximize the proportion of automatically
assigned codes, as the Hospital System aims to ex-
pand the use of the automatic codification of MRs
to more hospitals. According to experts, even an in-
crease of 1% in exact match would represent a sig-
nificant improvement allowing to gain time and re-
sources.
Related work can be found in the literature. For
instance, Pestian et al (2007) reported on a shared
task involving the assignment of ICD-codes to radi-
ology reports written in English from a reduced set
of 45 codes. In general it implied the examination of
the full MR (including body-text). In our case, the
number of ICD-codes is above 1,000, although we
restrict ourselves to exact and approximate match
over the diagnoses.
Farkas and Szarvas (2008) used machine learning
for the automatic assignment of ICD-9 codes. Their
results showed that hand-crafted systems could be
reproduced by replacing several laborious steps in
their construction with machine learning models.
Tsuruoka et al (2008) presented a system that
tried to normalize different variants of the terms con-
tained in a medical dictionary, automatically getting
normalizing rules for genes, proteins, chemicals and
diseases in English.
The contribution of this work is: i) to collect
manually coded MRs in Spanish; ii) to approximate
transduction with finite-state (FS) models for auto-
matic MR coding and, iii) to assess the performance
of the proposed FS transduction approaches.
2 Approximate transduction
As it was previously mentioned, there are variations
regarding the DT descriptions due to style, miss-
spells, etc. Table 1 shows several pairs of DT and
ICD-codes within the collected samples that illus-
trate some of those variations.
DT ICD
1 Adenocarcinoma de prostata 185
2 Adenocarcinomas pro?stata. 185
3 Ca. prostata 185
4 CA?NCER DE PROSTATA 185
5 adenocarcinoma de pulmon estadio IV 1629
6 CA pulmo?n estadio 4 1629
7 ADENOCARCINOMA PANCREAS 1579
Table 1: Examples of DTs and their ICD-codes.
There are differences in the use of uppercase/lower
case; omissions of accents; use of both standard and
non-standard abbreviations (e.g. ca. for both ca?ncer
and adenocarcinoma); punctuation marks (inciden-
tal use of full-stop as commas, etc.); omission of
prepositions (see rows 1 and 2); equivalence be-
tween Roman and Arabic numerals (rows 5 and 6).
Due to these variations, our problem can be defined
as an approximate lookup in a dictionary.
2.1 Finite-state models
Foma toolkit was used to build the FS machines and
code the evaluation sets. Foma (Hulden, 2009) is
a freely available2 toolkit that allows to both build
and parse FS automata and transducers. Foma of-
fers a versatile layout that supports imports/exports
from/to other tools such as: Xerox XFST (Beesley
and Karttunen, 2003), AT&T (Mehryar Mohri
and Riley, 2003), OpenFST (Riley et al, 2009).
There are, as well, outstanding alternatives such as
HFST (Linde?n et al, 2010). Refer to (Yli-Jyra? et al,
2006) for a thorough inventory on FS resources.
The FS models in Figure 1 perform the conver-
sions necessary to carry out a soft match between
the dictionary entries and their variants.
? First, we define the transducer Accents that
takes into account the correspondences be-
tween standard letters and their versions using
accent text marks.
2http://code.google.com/p/foma
61
define Accents [a:a?|e:e?|i:??|o:o?|u:u?|...];
define Case [a:A|b:B|c:C|d:D|e:E|f:F|...];
define Spaces [..] (->) " " || [.#. | "."] , .#.;
define Punctuation ["."|"-"|" "]:["."|"-"|" "];
define Plurals [..] -> ([s|es]) || [.#. | "." | " "];
define PluralsI [s|es] (->) "" || [.#. | "." | ","| " "];
define Preps [..] (->) [de |del |con |por ] || " " ;
define Disease [enf|enf.|enfermedad]:[enf|enf.|enfermedad];
define AltCa [tumor|ca|ca.|carcinoma|adenocarcinoma|ca?ncer];
define TagNormCa AltCa:AltCa;
define AltIzq [izquierdo|izquierda|izq|izq.|izqda|izqda.|
izqdo|izqdo.|izda|izda.|izdo|izdo.];
define TagNormIzq AltIzq:AltIzq;
Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to
bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs.
? The expression Case matches uppercase and
lowercase versions of the DTs.
? There is a set of transducers (Spaces,
Punctuation, Plurals and PluralsI)
that deal with the addition or deletion of spaces
and separators (as full-stop, comma, and hy-
phen) between words or at the end of the DT.
? Prepositions. Many DTs can be differen-
tiated by the use or absence of prepositions, al-
though they correspond to the same ICD-code.
For that reason, we designed a transducer that
inserts or deletes the prepositions from a re-
duced set that were identified by inspection of
the training set. In this way, expressions as
?Adenocarcinoma prostata? and ?Adenocarci-
noma de prostata? can be mapped to each other.
? Tag Normalization of synonyms, vari-
ants and abbreviations. The examination of the
DTs in the training set revealed that there were
several terms used indistinctly, including syn-
onyms and different kinds of variants (mascu-
line and feminine) and abbreviations. For ex-
ample, the words adenocarcinoma, adenoca.,
carcinoma, ca, ca. and cancer serve to name
the same disease. There are also multiple vari-
ants of left/right, indicating the location of an
illness, that do not affect the assignment of the
ICD-code (e.g. izquierdo, izq., izda.).
Finally, all the FS transducers were composed
into a single machine that served to overcome all the
sources of distortion together.
3 Experimental results
To begin with, coded MRs produced in the hospi-
tal throughout 12 months were collected summing
up a total of 8,020 MRs as described in Table 2.
Note that there are ambiguities in our data-set since
there are 3,313 different DTs that have resulted in
3,407 (DT, ICD-code) different pairs (as shown in
Table 2). That is, the same DT was not always as-
signed the same ICD-code.
DT ICD-code
entries 8,020
different entries 3,407
different forms 3,313 1,011
Table 2: The data-set of (DT, ICD-code) pairs.
Next, the data-set was shuffled and divided into 3
disjoint sets for training, development and test pur-
poses as shown in Table 3.
train dev test
entries 6,020 1,000 1,000
different entries 2,825 734 728
Table 3: The data-set shuffled and divided into 3 sets
Using the set of mappings derived from the train-
ing set we performed the experiments on the devel-
opment set. After several rounds of tuning the sys-
tem, the resulting system was applied to the test set.
62
PERCENTAGE OF UNCLASSIFIED DTs
TRAIN EVAL-SET exact-match + case-ins. + punct. + plurals +preps. + tag-norm.
train dev 30.6 27.0 25.2 24.4 23.9 23.2
train test 29.8 26.7 25.1 24.8 24.3 23.2
train+dev test 27.7 24.5 23.0 22.9 22.5 21.4
Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the
classified entries were correctly classified, yielding, as a result, a precision of 100%.
Given a DT, the goal is to find its corresponding
ICD-code despite the variations. Different FS ap-
proaches (described in Section 2.1) were proposed
to bypass particular sources of noise in the DT. Their
performance was assessed by means of the percent-
age of unclassified DTs, as summarized in Table 4.
Note that the lower the number of unclassified DTs
the better the performance. In each of the three rows
of Table 4 the results of different experimental se-
tups are shown: in the first two rows the training set
was used to build the models and either the devel-
opment or the test set was evaluated in their turn;
in the third row, both the training and the devel-
opment sets were used to build the model and the
test set was evaluated. The impact of adding pro-
gressively the FS machines built to tackle particular
sources of noise is shown by columns. Thus, the re-
sults of the last column represent the performance
of the transducer allowing exact-match search to-
gether with case-insensitive search, bypassing punc-
tuation marks, allowing plurals, bypassing preposi-
tions and allowing tag-normalization. The compo-
sition of each transducer outperforms the previous
result, yielding an improvement on the test of 6 ab-
solute points over the exact-match baseline, from
27.7% to 21.4%. As it can be derived from the
first column of Table 4 the test set contributed to the
training+development set with %27.7 of new DTs.
Overall, the FSMs progressively improved the re-
sults for the three series of experiments carried out
in more than 6%. As a result, less and less DTs are
left unclassified. In other words, the FS machines
tackling different sources of errors contribute to as-
sign ICD-codes to previously unassigned DTs.
A manual inspection over the results associated
to the evaluation of the development set (focus on
the first row of Table 4) showed that all the DTs
were correctly classified according to the training
data. Overall, the resulting transducer was unable
to classify 232 DTs out of 1,000 (see last column
in first row). Among the unclassified DTs, 10 out
of 232 were due to misspellings: e.g. cic atriz
(instead of cicatriz), desprendimineot (instead of
desprendimiento). In fact, spelling correction re-
ported improvements in related tasks (Patrick et al,
2010). The remaining DTs showed wider variations
in their forms, as unexpected degree of specificity
(e.g. named entities), spurious dates or numbers.
4 Conclusions
Medical records in Spanish were collected yielding
a data set of 8,020 DT and ICD-code pairs. While
there are a number of references dealing with En-
glish medical records, there are few for Spanish.
The goal of this work was to build a system that
given a DT it would find its corresponding ICD-
code as in a standard key-value dictionary. Yet, the
DTs are far from being standard since they contain
a number of variations. We proposed the use of sev-
eral FS models to bypass different variants and al-
low to provide ICD-codes even when the exact DT
was not found. Each source of variations was tack-
led with a specific transducer based on handwritten
rules. The composition of each machine improved
the performance of the system gradually, leading to
an improvement up to 6% in accuracy, from 27.7%
unclassified DTs with the exact-match baseline to
21.4% with the tag-normalization transducer.
Future work will focus on the unclassified DTs.
Together with FS models, other strategies shall be
explored. Machine-learning strategies in the field of
information retrieval might help to make the most of
the piece of information that was here discarded (i.e.
the body-text). All in all, regardless of the approach,
the command in this MR classification context is to
get an accuracy of 100%, possibly through the inter-
active inference framework (Toselli et al, 2011).
63
Acknowledgments
Authors would like to thank the Hospital Galdakao-
Usansolo for their contributions and support, in par-
ticular to Javier Yetano, responsible of the Clinical
Documentation Service.
This research was supported by the Department of
Industry of the Basque Government (IT344-10, S-
PE11UN114, GIC10/158 IT375-10), the University
of the Basque Country (GIU09/19) and the Span-
ish Ministry of Science and Innovation (MICINN,
TIN2010- 20218).
References
[Beesley and Karttunen2003] Kenneth R. Beesley and
Lauri Karttunen. 2003. Finite State Morphology.
CSLI Publications,.
[Farkas and Szarvas2008] Richa?rd Farkas and Gyo?rgy
Szarvas. 2008. Automatic construction of rule-based
ICD-9-CM coding systems. BMC Bioinformatics., 9
(Suppl 3): S10.
[Hulden2009] Mans Hulden. 2009. Foma: a Finite-State
Compiler and Library. In EACL (Demos), pages 29?
32. The Association for Computer Linguistics.
[Linde?n et al2010] Krister Linde?n, Miikka Silfverberg,
and Tommi Pirinen. 2010. HFST tools for morphol-
ogy ? an efficient open-source package for construc-
tion of morphological analyzers.
[Mehryar Mohri and Riley2003] Fernando C. N. Pereira
Mehryar Mohri and Michael D. Riley. 2003. AT&T
FSM LibraryTM ? Finite-State Machine Library.
www.research.att.com/sw/tools/fsm.
[Patrick et al2010] Jon Patrick, Mojtaba Sabbagh, Suvir
Jain, and Haifeng Zheng. 2010. Spelling correction
in clinical notes with emphasis on first suggestion ac-
curacy. In 2nd Workshop on Building and Evaluating
Resources for Biomedical Text Mining (BioTxtM2010)
LREC. ELRA.
[Pestian et al2007] John P. Pestian, Chris Brew, Pawel
Matykiewicz, D. J Hovermale, Neil Johnson, K. Bre-
tonnel Cohen, and Wlodzislaw Duch. 2007. A shared
task involving multi-label classification of clinical free
text. In Biological, translational, and clinical lan-
guage processing, pages 97?104, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Riley et al2009] Michael Riley, Cyril Allauzen, and
Martin Jansche. 2009. OpenFST: An open-source,
weighted finite-state transducer library and its applica-
tions to speech and language. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
[Toselli et al2011] Alejandro H. Toselli, Enrique Vi-
dal, and Francisco Casacuberta. 2011. Multi-
modal Interactive Pattern Recognition and Applica-
tions. Springer.
[Tsuruoka et al2008] Yoshimasa Tsuruoka, John Mc-
Naught, and Sophia Ananiadou. 2008. Normalizing
biomedical terms by minimizing ambiguity and vari-
ability. BMC Bioinformatics, 9(Suppl 3):S2.
[Yli-Jyra? et al2006] A. Yli-Jyra?, K. Koskenniemi, and
K.. Linde?n. 2006. Common infrastructure for
finite-state based methods and linguistic descriptions.
In Proceedings of International Workshop Towards
a Research Infrastructure for Language Resources.,
Genoa, May.
64
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 38?45,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Translating SNOMED CT Terminology into a Minor Language
Olatz Perez-de-Vi
?
naspre and Maite Oronoz
IXA NLP Group
University of the Basque Country UPV/EHU
Donostia
{olatz.perezdevinaspre, maite.oronoz}@ehu.es
Abstract
This paper presents the first attempt to
semi-automatically translate SNOMED
CT (Systematized Nomenclature of
Medicine ? Clinical Terms) terminology
content to Basque, a less resourced lan-
guage. Thus, it would be possible to build
a new clinical healthcare terminology for
Basque. We have designed the translation
algorithm and the first two phases of the
algorithm that feed the SNOMED CT?s
Terminology content, have been imple-
mented (it is composed of four phases).
The goal of the translation is twofold: the
enforcement of the use of Basque in the
bio-sanitary area and the access to a rich
multilingual resource in our language.
1 Introduction
SNOMED Clinical Terms (SNOMED CT)
(IHTSDO, 2014) is considered the most com-
prehensive, multilingual clinical healthcare
terminology in the world. The use of a standard
clinical terminology improves the quality and
health care by enabling consistent representation
of meaning in an electronic health record
1
.
Osakidetza, the Basque Sanitary System ought
to provide its service in the two co-official lan-
guages of the Basque Autonomous Community,
in Spanish and in Basque. However, and being
Basque a minority language in front of the power-
ful Spanish language, the use of Basque in the doc-
umentation services (for example in the Electronic
Medical Records (EMR)) of Osakidetza, is almost
zero. One of our goals in this work is to offer a
medical terminology in Basque to the bio-medical
personnel to try to enforce the use of Basque in
the bio-sanitary area and in this way protect the
1
http://www.ihtsdo.org/snomed-ct/whysnomedct/
snomedfeatures/
linguistic rights of patients and doctors. Another
objective in this work is to be able to access multi-
lingual medical resources in Basque language. To
try to reach the mentioned objectives, we want to
semi-automatically translate the terminology con-
tent of SNOMED CT focusing in some of its main
hierarchies.
To achieve our translation goal, we have defined
an algorithm that is based on Natural Language
Processing (NLP) techniques and that is composed
of four phases. In this paper we show the systems
and results obtained when developing the first two
phases of the algorithm that, in this case, trans-
lates English terms into Basque. The first phase
of the algorithm is based on the use of multilin-
gual lexical resources, while the second one uses
a finite-state approach to obtain Basque equivalent
terms using medical affixes and also transcription
rules.
In this paper we will leave aside explanations
about i) the translation application, ii) the knowl-
edge management and iii) the knowledge repre-
sentation, and we will focus on term generation.
The application framework that manages the terms
has been already developed and it is in use. The
knowledge representation schema has been de-
signed and implemented and it is also being used
(Perez-de-Vin?aspre and Oronoz, 2013).
In the rest of the paper after motivating the work
and connecting it to other SNOMED CT transla-
tions (sections 2 and 3), the algorithm and the ma-
terial that are needed to implement the first two
phases of the translation-algorithm are described
(section 4). After that, results are shown and dis-
cussed (sections 5 and 6). Finally, some conclu-
sions and future work are listed in the last section
(section 7).
2 Background and significance
?Basque is the ancestral language of the Basque
people, who inhabit the Basque Country, a region
38
spanning an area in northeastern Spain and south-
western France. It is spoken by 27% of Basques in
all territories (714,136 out of 2,648,998). Of these,
663,035 live in the Spanish part of the Basque
country (Basque Country and Navarre) and the re-
maining 51,100 live in the French part (Pyre?ne?es-
Atlantiques)
2
?. Basque is a minority language in
its standardization process and persists between
two powerful languages, Spanish and French. Al-
though today Basque holds co-official language
status in the Basque Autonomous Community,
during centuries Basque was not an official lan-
guage; it was out of educational systems, out of
media, and out of industrial environments. Due to
this features, the use of the Basque Language in
the bio-sanitary system is low. One of the reasons
for translating SNOMED CT is to try to increase
the use of the Basque language in this area.
SNOMED CT is a multilingual resource as its
concepts are linked to terms in different languages
by means of a concept identifier. Thus, terms in
our language will be linked to terms in all the lan-
guages in which SNOMED CT is released. Be-
sides, as SNOMED CT is part of the Metathe-
saurus of UMLS (Unified Medical Language Sys-
tem (Bodenreider, 2004)), Basque speakers will
have the possibility of accessing other lexical med-
ical resources (RxNorm, MeSH) containing the
concepts of SNOMED CT.
SNOMED CT has been already translated to
other languages using different techniques. These
translations were done either manually (this is the
case of the Danish language (Petersen, 2011)),
combining automatic translation with manual
work (in Chinese, for example (Zhu et al., 2012)),
or using exclusively an automatic translation help-
ing system (that is the case of French (Abdoune et
al., 2011)). In the design of the translation task,
we have followed the guidelines for the transla-
tion of SNOMED CT (H?y, 2010) published by
the IHTSDO as it is recommended.
3 SNOMED CT
SNOMED CT provides the core terminology for
electronic health records and contains more than
296,000 active concepts with their descriptions or-
ganized into hierarchies. (Humphreys et al., 1997)
shows that SNOMED CT has an acceptable cov-
erage of the terminology needed to record patient
2
http://en.wikipedia.org/wiki/Basque language (January
23, 2014)
conditions. Concepts are defined by means of de-
scription logic axioms and are used also to group
terms with the same meaning. Those descriptions
are more generally considered as terms.
There are three types of descriptions in
SNOMEDCT: Fully Specified Names (FSN), Pre-
ferred Terms (PT) and Synonyms. Fully Speci-
fied Names are the descriptions used to identify
the concepts and they usually have a semantic tag
in parenthesis that indicates its semantic type and,
consequently, its hierarchy. Regarding what we
sometimes refer to as ?terms? we can distinguish
between PTs and Synonyms.
There are 19 hierarchies to organize the con-
tent of SNOMED CT (plus 1 hierarchy for meta-
data). The concepts of SNOMED CT are grouped
into hierarchies as Clinical finding/disorder, Or-
ganism, and so on. For translation purposes it is
important to deeply analyze these hierarchies as
some of them need to translate all the terms while
others as Organism only admit the translation of
the synonyms (the preferred term should be the
taxonomic one). The guidelines for the transla-
tion of the hierarchies are given in (H?y, 2010).
We want to remark that only the terms classified
as PTs and synonyms in SNOMED CT have been
taken into consideration for the translation pur-
poses, as the structure (relationships, for example)
is the ontological core of SNOMED CT.
Considering the lexical resources available in
the bio-sanitary domain for Basque and the
SNOMED CT language versions released, two
source languages can be used for our translation
task: English and Spanish. Basque is classified as
a language isolate, and in consequence it is not re-
lated to English or Spanish and its linguistic char-
acteristics are far away from both of them. For that
reason, no English nor Spanish offers any advan-
tage as translation source. Thus, we deeply ana-
lyzed both of them to choose the best option. Our
starting point was the Release Format 2 (RF2),
Snapshot distributions and the versions dated the
31-07-2012 for English and the 30-10-2012 for
Spanish. It must be taken into consideration that
the Spanish version of SNOMED CT is a manual
translation of the English version.
To choose the source version of SNOMED CT
that will be translated, we analyzed aspects as i)
general numbers of FSNs, PTs and Synonyms, ii)
length of the terms in each language and, ii) the
lack of elements in each version. These data help
39
us to come to a decision:
1. The number of active concepts in both lan-
guages is the same (296,433) as the Spanish
version uses the English concept file. Nev-
ertheless, the number of terms in Spanish is
significantly smaller. In Spanish 15,715 con-
cepts lack of PTs and Synonyms.
2. Regarding the length of the PTs and syn-
onyms, we counted the terms containing one
token, two tokens, three tokens, four tokens
and those with more than four tokens. In the
English version the 6.76% of the terms has
one token, the 23.28% two and the 20.70%
three tokens. That is, quite simple terms com-
pose the half of the synonyms in the lexicon.
In the Spanish version, nevertheless, only the
33.79% of the synonyms has three tokens or
less, and there are 66.21% synonyms with
four tokens or more.
Considering these data, we can conclude that i)
the English version is more complete and consis-
tent than the Spanish one, and that ii) the terms
in the English version are shorter in length and, in
consequence, simpler to translate than the ones in
the Spanish version. Thus, we decided to use the
English version of SNOMEDCT as the translation
source as starting point.
We fix the priority between hierarchies for the
translation taking into account the number of
terms in each hierarchy. The most populated hi-
erarchies are Clinical finding/disorder (139,643
concepts) and Procedure (75,078 concepts). The
next most populated hierarchies are Organism
(35,870 concepts) and Body Structure (26,960).
The translation guidelines indicate that the PTs
of the organisms should not be translated. For
this reason and being conscious of our limita-
tion to translate this huge terminology, we decided
to prioritize the translation of the Clinical find-
ing/disorder, the Procedure and the Body Struc-
ture hierarchies.
4 Translation Algorithm
We have defined a general algorithm that tries to
achieve the translation with an incremental ap-
proach. Although the design is general and the al-
gorithm could be used for any language pair, some
linguistic resources for the source and objective
languages are necessary. In our implementation,
the algorithm takes a term in English as input and
obtains one or more equivalent terms in Basque.
The mapping of SNOMED CT with ICD-10
works at concept level. Thus, before executing the
implementation of the algorithm the mapping be-
tween them should be done (see section 5).
The algorithm is composed of four main phases.
The first two phases are already developed and re-
sults regarding quantities are given in section 5.
The last two phases will be undertaken in the very
near future.
We want to remark that all the processes fin-
ish in the step numbered as 4 in the algorithm
(see Figure 1). The Basque equivalents with their
original English terms, and relative information
(for instance, the SNOMED CT concept identi-
fier) are stored in an XML document that follows
the TermBase eXchange (TBX) (Melby, 2012) in-
ternational standard (ISO 30042) as exposed in
(Perez-de-Vin?aspre and Oronoz, 2013). All the
lexical resources are stored in another simpler
TBX document called ItzulDB (see number 1 in
Figure 1). This document is initialized with all
the lexical resources available, such as specialized
dictionaries and it is enriched with the new trans-
lation pairs generated that overcome a confidence
threshold with the intention of using them to trans-
late new terms. In this way we achieve feedback.
Let us describe the main phases:
1. Lexical knowledge. In this phase of the al-
gorithm (see numbers 1-2-4 in Figure 1),
some specialized dictionaries and the En-
glish, Spanish and Basque versions of the In-
ternational Statistical Classification of Dis-
eases and Related Health in its 10th ver-
sion (ICD-10) are used. ItzulDB is initial-
ized with all the translation pairs (English-
Basque) extracted from different dictionaries
of the bio-medical domain and the pairs ex-
tracted from the ICD-10. For example the in-
put term ?abortus? will be stored with all its
Basque equivalents ?abortu?, ?abortatze?
and ?hilaurtze?. This XML database is en-
riched with the new elements that are gener-
ated when the algorithm is applied (number 4
in Figure 1). Figure 2 shows an example of
some translations obtained using ItzulDB.
2. Morphosemantics. When a simple term (term
with a unique token) is not found in ItzulDB
(number 3 in Figure 1) it is analyzed at word-
level, and some generation-rules are used to
40
Figure 1: Schema of the Algorithm.
Input term: Deoxyribonuclic acid
Steps in Figure 1 number: 1,2,4
Translation: Azido desoxirribonukleiko,
ADN, DNA
Figure 2: Terms obtained from ItzulDB.
create the translation. We apply medical suf-
fix and prefix equivalences and morphotactic
rules, as well as some transcription rules, for
this purpose. This is the case in Figure 3.
Input term: Photodermatitis
Steps in Figure 1 number: 3,5,7,6,4
Applied rules:
Identified parts: photo+dermat+itis
Translated parts: foto+dermat+itis
Translation: Fotodermatitis
Figure 3: Terms obtained using generation-rules.
3. Shallow Syntax. In the case that the input
term does not appear in ItzulDB and it can
not be generated by word-level rules (number
8 in the algorithm), chunk-level generation
rules are used. Our hypothesis is that some
chunks of the term will appear in ItzulDB
with their translation. The application should
generate the entire term using the translated
components (see example in Figure 4).
Input term: Deoxyribonucleic acid sample
Steps in Figure 1 number: 8, 9, 10, 6, 4
Chunks in ItzulDB:
1st chunk: Deoxyribonucleic acid
Basque: azido desoxirribonukleiko,
ADN, DNA
2nd chunk: sample
Basque: lagin
Translation: Azido desoxirribonukleikoaren
lagin, ADN lagin, DNA lagin
Figure 4: Terms obtained using chunk-level gen-
eration rules.
4. Machine Translation. In the last phase, our
aim is to use a rule-based automatic trans-
lation system called Matxin (Mayor et al.,
2011) that we want to adapt to the medical
domain. Figure 5 shows an attempt of trans-
lation with the non adapted translator. For ex-
ample, Matxin translates ?colon? as the punc-
tuation mark (?bi puntu? or ?:?) because it
lacks the anatomical meaning.
Input term: Partial excision of oesophagus
and interposition of colon
Steps in Figure 1 number: 12, 4
Translation: Esofagoaren zati baten exci-
siona eta interpositiona bi puntua
Figure 5: Terms obtained using Matxin.
The IHTSDO organization releases a semi-
automatic mapping between SNOMED CT and
the ICD-10. By identifying the sense of a con-
cept in SNOMED CT, the best semantic space in
the ICD-10 for this concept is searched obtaining
linked codes. In this way we can obtain the corre-
sponding Basque term for some of the SNOMED
CT concepts through ICD-10. Considering that
the structures of SNOMED CT and the ICD-10
are quite different, and that the mapping some-
times has ?mapping conditions?, the use of this
41
resource has been complex, but fruitful for very
specialised terms. Although as we said this map-
ping is the unique source for obtaining very spe-
cialised terms, it should be used carefully as the
objectives of SNOMED CT and ICD-10 are dif-
ferent. ICD-10 has classification purposes while
SNOMED CT has representation purposes.
A brief description of the first two phases of the
algorithm is done in the next subsections (subsec-
tions 4.1 and 4.2):
4.1 Phase 1: Lexical Resources
The multilingual specialized dictionaries with En-
glish and Basque equivalences that have been used
to enrich ItzulDB in the first phase of the algorithm
are:
? ZT Dictionary
3
: This is a dictionary about
science and technology that contains areas
as medicine, biochemistry, biology. . . It con-
tains 13,764 English-Basque equivalences.
? Nursing Dictionary
4
: It has 5,393 entries in
the English-Basque chapter.
? Glossary of Anatomy: It contains anatomi-
cal terminology (2,578 useful entries) used
by University experts in their lectures.
? ICD-10
5
: This classification of diseases was
translated into Basque in 1996. It is also
available in English and in Spanish. The
mapping between the different language edi-
tions conforming a little dictionary, allowed
us to obtain 7,061 equivalences between En-
glish and Basque.
? EuskalTerm
6
: This terminology bank con-
tains 75,860 entries from wich 26,597 term
equivalences are labeled as from the biomed-
ical domain.
? Elhuyar Dictionary
7
: This English-Basque
dictionary, is a general dictionary that con-
tains 39,164 equivalences from English to
Basque.
All these quite different dictionaries have been
preprocessed in order to initialize ItzulDB. Elhu-
yar Dictionary is a general dictionary that has
3
http://zthiztegia.elhuyar.org
4
http://www.ehu.es/euskalosasuna/Erizaintza2.pdf
5
http://www.ehu.es/PAT/Glosarios/GNS10.txt
6
http://www.euskadi.net/euskalterm
7
http://hiztegiak.elhuyar.org/en
both not domains pairs but also contains some spe-
cialized terminology. This general dictionary will
help i) in the translation of not domain terms and
ii) also in the translation of the chunks in Phase
3, and thus, on the generation of new terms in
Basque.
4.2 Phase 2: Finite State Transducers and
Biomedical Affixes
A first approach to this work is presented in
(Perez-de-Vin?aspre et al., 2013). In that work, fi-
nite state transducers described in Foma (Hulden,
2009) are used to automatically identify the affixes
in English Medical terms and by means of affix
translation pairs, to generate the equivalent terms
in Basque. We observed that the behavior of the
roots in this type of words is similar to prefixes, so,
we will not make distinction between them and we
will name them prefixes. A list of 826 prefixes and
143 suffixes with medical meanings was manually
translated. An evaluation of the system was per-
formed in a Gold Standard of 885 English-Basque
pairs. The Gold Standard was composed of the
simple terms that were previously translated in the
first phase of the algorithm. A precision of 93%
and a recall of 41% were obtained.
In that occasion, only SNOMED CT terms for
which all the prefixes and suffixes were identified
were translated. For example, terms with the pref-
fix ?phat? were not translated as this affix does
not appear in the prefixes and suffixes list. For
instance, the ?hypophosphatemia? term was not
translated even though the ?hypo?, ?phos? and
?emia? affixes were identified.
We have improved this work by increasing the
number of affixes and implementing transcription
rules from English/Latin/Greek to Basque.
Figure 6 will help us to get a wider view of
the work exposed. The input term ?symphys-
iolysis? is split into the possible affix combi-
nation in the first step (?sym+physio+lysis? or
?sym+physi+o+lysis?). Then, those affixes are
translated by means of its equivalents in Basque
(?sim+fisio+lisi? or ?sim+fisi+o+lisi?). And fi-
nally, by means of morphotactic rules, the well-
formed Basque term is composed (in both cases
?sinfisiolisi? is generated).
5 Results
Considering the huge size of the descriptions in
SNOMED CT and to make the translation pro-
42
Table 1: Results of the translation.
Disorder Finding Body Structure Procedure
#Synonyms #Matches #Synonyms #Matches #Synonyms #Matches #Synonyms #Matches
ICD-10 mapping 11,227 - 1,878 - 0 - 0 -
In dictionaries 4,804 3,488 1,836 915 5,896 2,992 778 473
ZT Dictionary 1,104 883 367 311 1,812 1,212 293 253
Nursing Dictionary 437 350 340 245 978 725 199 157
Glossary of Anatomy 3 3 10 8 1,982 1,431 2 2
ICD-10 2,434 2,308 216 195 410 370 5 4
EuskalTerm 906 596 442 306 2,346 1,423 202 155
Elhuyar 299 135 956 300 1,090 367 270 91
Morphosemantics 2,620 2,184 705 578 970 779 1,551 1,362
Total 17,627 5,672 4,419 1,493 6,866 3,771 2,329 1,835
Input term: symphysiolysis
Identified affixes: sym+physio+lysis,
sym+physi+o+lysis
Translation of the affixes: sim+fisio+lisi,
sim+fisi+o+lisi
Morphotactics output term: sinfisiolisi
Figure 6: Term translated by means of affix equiv-
alences.
cess easy to handle, we have divided it into hier-
archies. The Clinical finding/disorder hierarchy is
specially populated so we have split it consider-
ing its semantic tags: disorders and findings. In
addition, the terms from the Procedure and Body
Structure hierarchies have been evaluated too.
Before showing the results, we want to remark
some aspects of the evaluation:
? Phase 1: the evaluation has been performed
in terms of quantity, not of quality of the
equivalent terms obtained. As the used re-
sources are dictionaries manually generated
by lexicographers and domain experts, the
quality of the Basque terms is assumed. In
any case, and due to the fact that Basque is in
its standardization process, the orthographic
correctness of the descriptions (see section 6)
will be manually checked in the near future.
? Phase 2: the quality of the generated terms
could be measured extrapolating the results
in the evaluation of the baseline system de-
scribed in subsection 4.2. That is, 93% pre-
cision and 41% recall. The quantity results
are shown considering the improvements de-
scribed in the same subsection.
Table 1 shows the results for the mentioned hi-
erarchies and semantic tags when the translation is
performed using both methods: dictionary match-
ing and morphosemantics. Remind that in a pre-
vious phase a concept level mapping is completed
between SNOMED CT and ICD-10. The first row
in Table 1 labeled as ?ICD-10 mapping? shows
that it is relevant only for the Clinical disorders
and findings hierarchy, being the disorder seman-
tic tag the most benefited one with 11,228 equiv-
alences. The remainder of the results is given at
term level.
We made a distinction between the number of
obtained Basque terms (1st column, labeled as
?#Synonyms?) and the number of English terms
translated (2nd column, labeled as ?#Matches?).
Let us see the difference between those two
columns looking at the numbers in Table 1. For ex-
ample, in the disorder semantic tag there are 3,488
matches (3,488 original English terms translated),
but the number of obtained Basque terms is 4,804
(adding the number of equivalents of all the dic-
tionaries). The reason is that the same input term
may have synonyms or even the same equivalent
term given by different dictionaries. For example,
for the term ?allopathy?, the same term ?alopatia?
is obtained in the ZT and Nursing dictionaries (this
equivalence will be counted in both ZT and Nurs-
ing dictionaries rows).
Table 2 shows the number of tokens in the origi-
nal English terms. This table refers not to the con-
cepts, but to the terms in the source SNOMED CT
in English. The first row shows the number of En-
glish terms to which we obtained a Basque equiv-
alent or synonym, the second one the total of En-
glish terms and finally, the last row the percentage
of translated terms.
Table 3 gives the overall numbers of the trans-
lated concepts, in order to take a wide view of the
process done.
Let us see the highlights of the results for each
43
Table 2: Results of the translation regarding the number of tokens of the original term.
1 token 2 tokens 3 tokens 4 tokens > 4 tokens Total
Translated Terms 3,315 1,114 538 279 426 5,672
Disorder Terms in total 4,066 22,023 24,036 20,005 37,316 107,446
Percentage 81.53% 5.06% 2.24% 1.40% 1.14% 5.27%
Translated Terms 1,222 158 39 20 54 1,493
Finding Terms in total 1,830 8,837 10,980 9,814 19,106 50,567
Percentage 66.78% 1.79% 0.36% 0.20% 0.28% 2.95%
Translated Terms 1,942 1,416 334 66 13 3,771
Body Structure Terms in total 2,692 11,519 12,575 10,903 21,631 59,320
Percentage 72.14% 12.29% 2.66% 0.61% 0.06% 6.36%
Translated Terms 1,741 80 11 2 1 1,835
Procedure Terms in total 1,982 9,966 15,848 16,578 37,695 82,069
Percentage 87.84% 0.80% 0.07% 0.01% 0.003% 2.24%
Table 3: Overall results.
Disorder Finding Body Structure Procedure
Translated Concepts 14,125 2,777 3,231 1,502
Concepts in total 65,386 33,204 31,105 82,069
Percentage 21.60% 8.36% 10.39% 1.83%
hierarchy or semantic tag:
? 21.60% of the disorders has been translated
(see Table 3). This can be considered a very
good result. The ICD-10 mapping produces
the majority of the translations as it could be
expected in this hierarchy (11,227 synonyms
obtained). In Table 2 the strength of the mor-
phosemantics phase is evident as the 81.53%
of the simple terms is translated.
? The finding semantic tag is the most bal-
anced, as no one of the algorithm phase?s
contribution outlines. The translation of the
8.36% of the concepts is achieved.
? Regarding the results of the Body Structure
hierarchy, Table 1 shows that the Glossary of
Anatomy only contributes in this area. The
10.39% of the concepts get a Basque equiva-
lent.
? In the translation of the Procedure hierarchy
the dictionaries do not help much as shown
in Table 1. In contrast, the mophosemantics
contribution allows to translate the 87.84% of
the simple terms (see Table 2).
6 Discussion
Some general dictionaries as the ZT dictionary
usually contribute in the translation of most of the
terms, while more specialized dictionaries only
provide translations in the terms related to their
domain. For example, both dictionaries, the ZT
dictionary and the Nursing dictionary, obtained the
Basque terms ?mikrozefalia? for ?microcephaly?
and ?metatartso? for ?metatarsus?. The ICD-10
mapping contributed mainly in the translation of
the disorders, and the Glossary of Anatomy in the
translation of terms from the Body Structure hi-
erarchy. Sometimes more than an equivalent in
Basque is obtained in the translation. For exam-
ple, for the term ?leprosy? we got the equivalents
?legen beltz?, ?legen? and ?legenar?. Some prob-
lems were detected in the Basque terms regarding
the standard orthography (the ICD-10 was trans-
lated in 1996 and the spelling rules have changed
since then) and the form of the word (some obtain
the word in finite forms, i.e. ?abdomena? for ?ab-
domen? and other in non finite form, ?abdomen?).
To which the terms generated by finite-state
transducers concern, we detected many new af-
fixes from the SNOMED CT terms that do not ap-
pear in our lexicon. Even most of those affixes
will be correctly transcripted by our transducers,
experts insist on enriching the lexicon with new
pairs.
7 Conclusions
We have designed a translation algorithm for the
multilingual terminology content of SNOMEDCT
and we have implemented the first two phases. On
the one hand, lexical resources feed our database,
and on the other hand, Basque equivalents are gen-
erated using transducers and medical and biologi-
44
cal affixes.
Dictionaries provide Basque equivalents of any
term length (i.e. unique and multitoken terms)
while transducers get as input unique token terms.
In both translation methods results for the most
populated hierarchies are shown even though they
are applied for all the hierarchies in SNOMEDCT.
When using lexical resources, results are promis-
ing and the contribution of the ICD-10 mapping
is remarkable. We obtained the equivalents in
Basque of 21.60% of the disorders.
In any case, as we said before, our objective in
the future is that specialist in medical terminol-
ogy can check the quality of the obtained terms
and correct them with the help of a domain cor-
pus in Basque. A platform is being developed for
this purpose. After the evaluation, and only if it
reaches high quality results, our aim is to contact
SNOMEDCT providers to offer them the result of
our work, that at the moment only pertains to the
research area.
Regarding the developed systems evaluation,
the system used in the first phase extracts English-
Basque pairs from dictionaries, so being quite a
simple system, does not need of a deep evalua-
tion. A first evaluation of the system that generates
terms using medical affixes has been presented.
At present, we are evaluating the improvements of
this second system with promising results.
In a near future, we want to implement the re-
mainder of the phases in the algorithm: the use of
syntax rules for term generation, and the adapta-
tion of the machine translation tool. The promis-
ing results in this first approximation encourage us
in the way to semi-automatically generate a ver-
sion in Basque of SNOMED CT.
Acknowledgments
The authors would like to thank Mikel Lersundi
for his help. This work was partially sup-
ported by the European Commission (325099),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Basque Gov-
ernment (IT344-10 and IE12-333). Olatz Perez-
de-Vin?aspre?s work is funded by a PhD grant from
the Basque Governement (BFI-2011-389).
References
Hocine Abdoune, Tayeb Merabti, Ste?fan J. Darmoni,
and Michel Joubert. 2011. Assisting the Translation
of the CORE Subset of SNOMED CT Into French.
In Anne Moen, Stig Kj?r Andersen, Jos Aarts, and
Petter Hurlen, editors, Studies in Health Technology
and Informatics, volume 169, pages 819?823.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): Integrating Biomedi-
cal Terminology. Nucleic acids research, 32(suppl
1):D267?D270.
Asta H?y. 2010. Guidelines for Translation of
SNOMED CT. Technical Report version 2.0, In-
ternational Health Terminology Standards Develop-
ment Organization IHTSDO.
M. Hulden. 2009. Foma: a Finite-State Compiler and
Library. In Proceedings of EACL 2009, pages 29?
32, Stroudsburg, PA, USA.
Betsy L Humphreys, Alexa T McCray, and May L
Cheh. 1997. Evaluating the coverage of controlled
health data terminologies: report on the results of
the NLM/AHCPR large scale vocabulary test. Jour-
nal of the American Medical Informatics Associa-
tion, 4(6):484?500.
International Health Terminology Standards Develop-
ment Organisation IHTSDO. 2014. SNOMED CT
Starter Guide. February 2014. Technical report, In-
ternational Health Terminology Standards Develop-
ment Organisation.
Aingeru Mayor, In?aki Alegria, Arantza Diaz de Ilar-
raza, Gorka Labaka,Mikel Lersundi, and Kepa Sara-
sola. 2011. Matxin, an Open-source Rule-based
Machine Translation System for Basque. Machine
Translation, 25:53?82. 10.1007/s10590-011-9092-
y.
Alan K. Melby. 2012. Terminology in the Age of Mul-
tilingual Corpora. The Journal of Specialised Trans-
lation, 18:7?29, July.
Olatz Perez-de-Vin?aspre and Maite Oronoz. 2013. An
XML Based TBX Framework to Represent Multi-
lingual SNOMED CT for Translation. In Advances
in Artificial Intelligence and Its Applications, pages
419?429. Springer.
Olatz Perez-de-Vin?aspre, Maite Oronoz, Manex Agir-
rezabal, and Mikel Lersundi. 2013. A Finite-State
Approach to Translate SNOMED CT Terms into
Basque Using Medical Prefixes and Suffixes. Fi-
nite State Methods and Natural Language Process-
ing, page 99.
Palle G. Petersen. 2011. How to Manage the Transla-
tion of a Terminology. Presentation at the IHTSDO
October 2011 Conference and Showcase, October.
Yanhui Zhu, Huiting Pan, Lei Zhou, Wei Zhao, Ana
Chen, Ulrich Andersen, Shuxiang Pan, Lixin Tian,
and Jianbo Lei. 2012. Translation and Localization
of SNOMED CT in China: A pilot study. Artificial
Intelligence in Medicine, 54(2):147?149.
45
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 85?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Adverse Drug Event prediction combining
shallow analysis and machine learning
Sara Santiso
Alicia P
?
erez
Koldo Gojenola
IXA Taldea (UPV-EHU)
Arantza Casillas
Maite Oronoz
IXA Taldea (UPV-EHU)
http://ixa.si.ehu.es
Abstract
The aim of this work is to infer a model
able to extract cause-effect relations be-
tween drugs and diseases. A two-level
system is proposed. The first level car-
ries out a shallow analysis of Electronic
Health Records (EHRs) in order to iden-
tify medical concepts such as drug brand-
names, substances, diseases, etc. Next,
all the combination pairs formed by a
concept from the group of drugs (drug
and substances) and the group of diseases
(diseases and symptoms) are characterised
through a set of 57 features. A supervised
classifier inferred on those features is in
charge of deciding whether that pair rep-
resents a cause-effect type of event.
One of the challenges of this work is the
fact that the system explores the entire
document. The contributions of this pa-
per stand on the use of real EHRs to dis-
cover adverse drug reaction events even in
different sentences. Besides, the work fo-
cuses on Spanish language.
1 Introduction
This work deals with semantic data mining within
the clinical domain. The aim is to automatically
highlight the Adverse Drug Reactions (ADRs) in
EHRs in order to alleviate the work-load to sev-
eral services within a hospital (pharmacy service,
documentation service,. . . ) that have to read these
reports. Event detection was thoroughly tackled in
the Natural Language Processing for Clinical Data
2010 Challenge. Since then, cause-effect event ex-
traction has emerged as a field of interest in the
Biomedical domain (Bj?orne et al., 2010; Mihaila
et al., 2013). The motivation is, above all, practi-
cal. Electronic Health Records (EHRs) are studied
by several services in the hospital, not only by the
doctor in charge of the patient but also by the phar-
macy and documentation services, amongst oth-
ers. There are some attempts in the literature that
aim to make the reading of the reports in English
easier and less time-consuming by means of an au-
tomatic annotation toolkit (Rink et al., 2011; Bot-
sis et al., 2011; Toldo et al., 2012). This work is
a first approach on automatic learning of relations
between drugs causing diseases in Spanish EHRs.
This work presents a system that entails two
stages in cascade: 1) the first one carries out the
annotation of drugs or substances (from now on-
wards both of them shall be referred to as DRUG)
and diseases or symptoms (referred to as DIS-
EASE); 2) the second one determines whether a
given (DRUG, DISEASE) pair of concepts repre-
sents a cause-effect reaction. Note that we are in-
terested in highlighting events involving (DRUG,
DISEASE) pairs where the drug caused an adverse
reaction or a disease. By contrast, often, (DRUG,
DISEASE) pairs would entail a drug prescribed to
combat a disease, but these correspond to a differ-
ent kind of events (indeed, diametrically opposed).
Besides, (DRUG, DISEASE) pairs might represent
other sort of events or they might even be unre-
lated at all. Finally, the system should present the
ADRs marked in a friendly front-end. To this end,
the aim is to represent the text in the framework
provided by Brat (Stenetorp et al., 2012). Figure 1
shows an example, represented in Brat, of some
cause-effect events manually tagged by experts.
There are related works in this field aiming at
a variety of biomedical event extraction, such as
binary protein-protein interaction (Wong, 2001),
biomolecular event extraction (Kim et al., 2011),
and drug-drug interaction extraction (Segura-
Bedmar et al., 2013). We are focusing on a variety
of interaction extraction: drugs causing diseases.
There are previous works in the literature that try
to warn whether a document contains or not this
type of events. There are more recent works that
85
Figure 1: Some cause-effect events manually annotated in the Brat framework.
cope with event extraction within the same sen-
tence, that is, intra-sentence events. By contrast, in
this work we have realised that around 26% of the
events occur between concepts that are in differ-
ent sentences. Moreover, some of them are at very
long distance. Hence, our method aims at provid-
ing all the (DRUG, DISEASE) concepts within the
document that represent a cause-effect relation.
We cope with real discharge EHRs written by
around 400 different doctors. These records are
not written in a template, that is, the EHRs do not
follow a pre-determined structure, and this, by it-
self entails a challenge. The EHRs we are dealing
with are written in a free structure using natural
language, non-standard abbreviations etc. More-
over, we tackle Spanish language, for which little
work has been carried out. In addition, we do not
only aim at single concept-words but also at con-
cepts based on multi-word terms.
2 System overview
The system, as depicted in Figure 2 entails two
stages.
EHR
Stage 1:
ANNOTATING
CONCEPTS
Stage 2:
EXTRACTING
EVENTS
MARKED 
EHR
Figure 2: The ADR event extraction system.
In the first stage, relevant pairs of concepts have
to be identified within an EHR. Concept annota-
tion is accomplished by means of a shallow anal-
yser system (described in section 2.1). Once the
analyser has detected (DRUG, DISEASE) pairs in
a document, all the pairs will be examined by
an inferred supervised classifier (described in sec-
tion 2.2).
2.1 Annotating concepts by shallow analysis
The first stage of the system has to detect and an-
notate two types of semantic concepts: drugs and
diseases. Each concept, as requested by the phar-
macy service, should gather several sub-concepts
stated as follows:
1. DRUG concept:
(a) Generic names for pharmaceutical
drugs: e.g. corticoids;
(b) Brand-names for pharmaceutical drugs:
e.g. Aspirin;
(c) Active ingredients: e.g. vancomycin;
(d) Substances: e.g. dust, rubber;
2. DISEASE concept:
(a) Diseases
(b) Signs
(c) Symptoms
These concepts were identified by means of a
general purpose analyser available for Spanish,
called FreeLing (Padr?o et al., 2010), that had been
enhanced with medical ontologies and dictionar-
ies, such as SNOMED-CT, BotPLUS, ICD-9-CM,
etc. (Oronoz et al., 2013). This toolkit is able
to identify multi-word context-terms, lemmas and
also POS tags. An example of the morphological,
semantic and syntactic analysis, provided by this
parser is given in Figure 3. In the figure two pieces
of information can be distinguished: for exam-
ple, given the word ?secundarios? (meaning sec-
ondaries) 1) the POS tag provided is AQOM corre-
sponding to Qualificative Adjective Ordinal Mas-
culine Singular; and 2) the provided lemma is ?se-
cundario? (secondary). Besides, in a third layer,
the semantic tag is given, that is, the tag ?ENFER-
MEDAD? (meaning disease) involves the multi-
word concept ?HTP severa? (severe pulmonary
hypertension).
86
Figure 3: Lemmas, POS-tags and semantic tags are identified by the clinic domain analyser (diseases in
yellow and drugs or substances in violet).
2.2 Extracting adverse drug reaction events
using inferred classifiers
The goal of the second stage is to determine if a
given (DRUG, DISEASE) pair represents an ADR
event or not. On account of this, we resorted to
supervised classification models. These models
can be automatically inferred from a set of doc-
uments in which the target concepts had been pre-
viously annotated. Hence, first of all, a set of an-
notated data representative for the task is required.
To this end, our starting point is a manually anno-
tated corpus (presented in section 2.2.1). Besides,
in order to automatically learn the classifier, the
(DRUG, DISEASE) pairs have to be described in an
operative way, that is, in terms of a finite-set of
features (see section 2.2.2). The supervised clas-
sification model selected was a type of ensemble
classifier: Random Forests (for further details turn
to section 2.2.3).
2.2.1 Producing an annotated set
A supervised classifier was inferred from an-
notated real EHRs. The annotation was carried
out by doctors from the same hospital that pro-
duced the EHRs. Given the text with the con-
cepts marked on the first stage (turn to section 2.1)
and represented within the framework provided by
Brat
1
, around 4 doctors from the same hospital an-
notated the events. This annotated set would work
as a source of data to get instances that would
serve to train supervised classification models, as
the one referred in section 2.2.
2.2.2 Operational description of events
As it is well-known, the success of the techniques
based on Machine Learning relies upon the fea-
tures used to describe the instances. Hence, we se-
lected the following features that eventually have
1
Brat is the framework a priori selected as the output
front-end shown in Figure 1
proven useful to capture the semantic relations be-
tween ADRs. The features can be organised in the
following sets:
? Concept-words and context-words: to be
precise, we make use of entire terms
including both single-words and multi-
words.
? DRUG concept-word together with
left and right context words (a con-
text up to 3, yielding, thus, 7 fea-
tures).
? DISEASE concept-word together
with left and right context words (7
features).
? Concept-lemmas and context-lemmas
for both drug and disease (14 features
overall)
? Concept-POS and context-POS for both
drug and disease (14 features)
? Negation and speculation: these are
binary valued features to determine
whether the concept words or their con-
text was either negated or speculated (2
features).
? Presence/absence of other drugs in the
context of the target drug and disease (12
features)
? Distance: the number of characters from
the DRUG concept to the DISEASE con-
cept (1 feature).
2.2.3 Inferring a supervised classifier
Given the operational description of a set of
(DRUG, DISEASE) pairs, this stage has to deter-
87
mine if there exists an ADR event (that is, a cause-
effect relation) or not. To do so, we resorted
to Random Forests (RFs), a variety of ensemble
models. RFs combine a number of decision trees
being each tree built on the basis of the C4.5 algo-
rithm (Quinlan, 1993) but with a distinctive char-
acteristic: some randomness is introduced in the
order in which the nodes are generated. Particu-
larly, each time a node is generated in the tree, in-
stead of chosing the attribute that maximizes the
Information Gain, the attribute is randomly se-
lected amongst the k best options. We made use
of the implementation of this algorithm available
in Weka-6.9 (Hall et al., 2009). Ensemble models
were proved useful on drug-drug interaction ex-
traction tasks (Thomas et al., 2011).
3 Experimental results
We count on data consisting of discharge sum-
maries from Galdakao-Usansolo Hospital. The
records are semi-structured in the sense that there
are two main fields: the first one for personal data
of the patient (age, dates relating to admittance)
that were not provided by the hospital for privacy
issues; and the second one, our target, a single
field that contains the antecedents, treatment, clin-
ical analysis, etc. This second field is an unstruc-
tured section (some hospitals rely upon templates
that divide this field into several subfields, provid-
ing it with further structure). The discharge notes
describe a chronological development of the pa-
tient?s condition, the undergone treatments, and
also the clinical tests that were carried out.
Given the entire set of manually annotated doc-
uments, 34% were randomly selected without re-
placement to produce the evaluation set. The re-
sulting partition is presented in Table 1 (where the
train and evaluation sets are referred to as Train
and Eval respectivelly).
Documents Concepts Relations
Train 144 6,105 4,675
Eval 50 2,206 1,598
Table 1: Quantitative description of the data.
All together, there are 194 EHRs manually
tagged with more than 8,000 concepts (entailing
diseases, symptoms, drugs, substances and proce-
dures). From these EHRs all the (DRUG,DISEASE)
pairs are taken into account as event candidates,
and these are referred to as relations in Table 1.
The system was assessed using per-class aver-
aged precision, recall and f1-measure as presented
in Table 2.
Precision Recall F1-measure
0.932 0.849 0.883
Table 2: Experimental results.
Semantic knowledge and contextual features
have proven very relevant to detect cause-effect re-
lations. Particularly, those used to detect the con-
cepts and also negation or speculation of the con-
text in which the concept appear.
A manual inspection was carried out on both the
false positives and false negative predictions and
the following conclusions were drawn:
? The majority of false positives were caused
by i) pairs of concepts at a very long distance;
ii) pairs where one of the elements is related
to past-events undergone while the other el-
ement is in the current treatment prescribed
(e.g. the disease is in the antecedents and the
drug in the current diagnostics).
? The vast majority of false negatives were
due to concepts in the same sentence where
the context-words are irrelevant (e.g. filler
words, determiners, etc.).
4 Concluding Remarks and Future Work
This work presents a system that first identifies rel-
evant pairs of concepts in EHRs by means of a
shallow analysis and next examines all the pairs
by an inferred supervised classifier to determine if
a given pair represents a cause-effect event. A rel-
evant contribution of this work is that we extract
events occurring between concepts that are in dif-
ferent sentences. In addition, this is one of the first
works on medical event extraction for Spanish.
Our aim for future work is to determine whether
the (DRUG, DISEASE) pair represents either a rela-
tion where 1) the drug is to overcome the disease;
2) the drug causes the disease; 3) there is no rela-
tionship between the drug and the disease.
The aim of context features is to capture charac-
teristics of the text surrounding the relevant con-
cepts that trigger a relation. More features could
also be explored such as trigger words, regular pat-
terns, n-grams, etc.
88
Acknowledgments
The authors would like to thank the Pharmacy
and Pharmacovigilance services of Galdakao-
Usansolo Hospital.
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
References
Jari Bj?orne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Complex event ex-
traction at pubmed scale. Bioinformatics [ISMB],
26(12):382?390.
Taxiarchis Botsis, Michael D. Nguyen, Emily Jane
Woo, Marianthi Markatou, and Robert Ball. 2011.
Text mining for the vaccine adverse event reporting
system: medical text classification using informative
feature selection. JAMIA, 18(5):631?638.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1):10?18.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In
Proceedings of the BioNLP Shared Task 2011
Workshop, pages 1?6. Association for Computa-
tional Linguistics.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic annotation of
medical records in Spanish with disease, drug and
substance names. In Lecture Notes in Computer
Science, volume 8259, pages 536?547. Springer-
Verlag.
Lluis Padr?o, S. Reese, Eneko Agirre, and Aitor Soroa.
2010. Semantic Services in Freeling 2.1: WordNet
and UKB. In Global Wordnet Conference, Mumbai,
India.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Bryan Rink, Sanda Harabagiu, and Kirk Roberts.
2011. Automatic extraction of relations between
medical concepts in clinical texts. JAMIA, 18:594?
600.
Isabel Segura-Bedmar, P Mart??nez, and Mar?a Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts (ddiex-
traction 2013). Proceedings of Semeval, pages 341?
350.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: A web-based tool for nlp-
assisted text annotation. In In Proceedings of the
Demonstrations Session at EACL 2012.
Philippe Thomas, Mariana Neves, Ill?es Solt,
Domonkos Tikk, and Ulf Leser. 2011. Relation
extraction for drug-drug interactions using ensem-
ble learning. 1st Challenge task on Drug-Drug
Interaction Extraction (DDIExtraction 2011), pages
11?18.
Luca Toldo, Sanmitra Bhattacharya, and Harsha Gu-
rulingappa. 2012. Automated identification of ad-
verse events from case reports using machine learn-
ing. In Workshop on Computational Methods in
Pharmacovigilance.
Limsoon Wong. 2001. A protein interaction extraction
system. In Pacific Symposium on Biocomputing,
volume 6, pages 520?531. Citeseer.
89
