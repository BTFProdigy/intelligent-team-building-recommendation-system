Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 42?50,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Determining Compositionality of Word Expressions
Using Word Space Models
Lubom??r Krc?ma?r?, Karel Jez?ek
University of West Bohemia
Faculty of Applied Sciences
Department of Computer Science and Engineering
Pilsen, Czech Republic
{lkrcmar,jezek ka}@kiv.zcu.cz
Pavel Pecina
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Prague, Czech Republic
pecina@ufal.mff.cuni.cz
Abstract
This research focuses on determining seman-
tic compositionality of word expressions us-
ing word space models (WSMs). We discuss
previous works employing WSMs and present
differences in the proposed approaches which
include types of WSMs, corpora, preprocess-
ing techniques, methods for determining com-
positionality, and evaluation testbeds.
We also present results of our own approach
for determining the semantic compositionality
based on comparing distributional vectors of
expressions and their components. The vec-
tors were obtained by Latent Semantic Analy-
sis (LSA) applied to the ukWaC corpus. Our
results outperform those of all the participants
in the Distributional Semantics and Composi-
tionality (DISCO) 2011 shared task.
1 Introduction
A word expression is semantically compositional
if its meaning can be understood from the literal
meaning of its components. Therefore, semanti-
cally compositional expressions involve e.g. ?small
island? or ?hot water?; on the other hand, seman-
tically non-compositional expressions are e.g. ?red
tape? or ?kick the bucket?.
The notion of compositionality is closely related
to idiomacy ? the higher the compositionality the
lower the idiomacy and vice versa (Sag et al, 2002;
Baldwin and Kim, 2010).
Non-compositional expressions are often referred
to as Multiword Expressions (MWEs). Baldwin and
Kim (2010) differentiate the following sub-types of
compositionality: lexical, syntactic, semantic, prag-
matic, and statistical. This paper is concerned with
semantic compositionality.
Compositionality as a feature of word expressions
is not discrete. Instead, expressions populate a con-
tinuum between two extremes: idioms and free word
combinations (McCarthy et al, 2003; Bannard et al,
2003; Katz, 2006; Fazly, 2007; Baldwin and Kim,
2010; Biemann and Giesbrecht, 2011). Typical ex-
amples of expressions between the two extremes are
?zebra crossing? or ?blind alley?.
Our research in compositionality is motivated
by the hypothesis that a special treatment of se-
mantically non-compositional expressions can im-
prove results in various Natural Language Process-
ing (NPL) tasks, as shown for example by Acosta et
al. (2011), who utilized MWEs in Information Re-
trieval (IR). Besides that, there are other NLP ap-
plications that can benefit from knowing the degree
of compositionality of expressions such as machine
translation (Carpuat and Diab, 2010), lexicography
(Church and Hanks, 1990), word sense disambigua-
tion (Finlayson and Kulkarni, 2011), part-of-speech
(POS) tagging and parsing (Seretan, 2008) as listed
in Ramisch (2012).
The main goal of this paper is to present an anal-
ysis of previous approaches using WSMs for de-
termining the semantic compositionality of expres-
sions. The analysis can be found in Section 2. A
special attention is paid to the evaluation of the pro-
posed models that is described in Section 3. Section
4 presents our first intuitive experimental setup and
results of LSA applied to the DISCO 2011 task. Sec-
tion 5 concludes the paper.
42
2 Semantic Compositionality of Word
Expressions Determined by WSMs
Several recent works, including Lin (1999), Schone
and Jurafsky (2001), Baldwin et al (2003), Mc-
Carthy et al (2003), Katz (2006), Johannsen et al
(2011), Reddy et al (2011a), and Krc?ma?r? et al
(2012), show the ability of methods based on WSMs
to capture the degree of semantic compositionality
of word expressions. We analyse the proposed meth-
ods and discuss their differences. As further de-
scribed in detail and summarized in Table 1, the ap-
proaches differ in the type of WSMs, corpora, pre-
processing techniques, methods for determining the
compositionality, datasets for evaluation, and meth-
ods of evaluation itself.
Our understanding of WSM is in agreement with
Sahlgren (2006): ?The word space model is a com-
putational model of word meaning that utilizes the
distributional patterns of words collected over large
text data to represent semantic similarity between
words in terms of spatial proximity?. For more
information on WSMs, see e.g. Turney and Pan-
tel (2010), Jurgens and Stevens (2010), or Sahlgren
(2006).
WSMs and their parameters WSMs can be built
by different algorithms including LSA (Landauer
and Dumais, 1997), Hyperspace Analogue to Lan-
guage (HAL) (Lund and Burgess, 1996), Random
Indexing (RI) (Sahlgren, 2005), and Correlated Oc-
currence Analogue to Lexical Semantics (COALS)
(Rohde et al, 2005). Every algorithm has its own
specifics and can be configured in different ways.
The configuration usually involves e.g. the choice
of context size, weighting functions, or normaliz-
ing functions. While Schone and Jurafsky (2001),
Baldwin et al (2003), and Katz (2006) addopted
LSA-based approaches, Johannsen et al (2011) and
Krc?ma?r? et al (2012) employ COALS; the others use
their own specific WSMs.
Corpora and text preprocessing Using differ-
ent corpora and their preprocessing naturally leads
to different WSMs. The preprocessing can differ
e.g. in the choice of used word forms or in re-
moval/retaining of low-frequency words. For exam-
ple, while Lin (1999) employs a 125-million-word
newspaper corpus, Schone and Jurafsky (2001) use
a 6.7-million-word subset of the TREC databases,
Baldwin et al (2003) base their experiments on
90 million words from the British National Corpus
(Burnard, 2000). Krc?ma?r? et al (2012), Johannsen et
al. (2011), and Reddy et al (2011a) use the ukWaC
corpus, consisting of 1.9 billion words from web
texts (Baroni et al, 2009). As for preprocessing,
Lin (1999) extracts triples with dependency relation-
ships, Baldwin et al (2003), Reddy et al (2011a),
and Krc?ma?r? et al (2012) concatenate word lemmas
with their POS categories. Johannsen et al (2011)
use word lemmas and remove low-frequency words
while Reddy et al (2011a), for example, keep only
frequent content words.
Methods We have identified three basic methods
for determining semantic compositionality:
1) The substitutability-based methods exploit
the fact that replacing components of non-
compositional expressions by words which are
similar leads to anti-collocations (Pearce, 2002).
Then, frequency or mutual information of such
expressions (anti-collocations) is compared with
the frequency or mutual information of the original
expressions. For example, consider expected occur-
rence counts of ?hot dog? and its anti-collocations
such as ?warm dog? or ?hot terrier?.
2) The component-based methods, utilized for ex-
ample by Baldwin et al (2003) or Johannsen et al
(2011), compare the distributional characteristics of
expressions and their components. The context vec-
tors expected to be different from each other are
e.g. the vector representing the expression ?hot dog?
and the vector representing the word ?dog?.
3) The compositionality-based methods compare
two vectors of each analysed expression: the true
co-occurrence vector of an expression and the vec-
tor obtained from vectors corresponding to the com-
ponents of the expression using a compositional-
ity function (Reddy et al, 2011a). The most com-
mon compositionality functions are vector addition
or pointwise vector multiplication (Mitchell and La-
pata, 2008). For example, the vectors for ?hot dog?
and ?hot???dog? are supposed to be different.
Evaluation datasets There is still no consensus
on how to evaluate models determining semantic
compositionality. However, by examining the dis-
cussed papers, we have observed an increasing ten-
43
Paper Corpora WSMs Methods Data (types) Evaluation
Lin (1999) 125m, triples own SY NVAA c. dicts., P/R
Schone+Jurafsky(2001) 6.7m TREC LSA SY, CY all types WN, P/Rc
Baldwin et al (2003) BNC+POS LSA CT NN, VP WN, PC
McCarthy et al (2003) BNC+GR own CTn PV MA, WN, dicts., S
Katz (2006) GNC LSA CY PNV MA, P/R, Fm
Krc?ma?r? et al (2012) ukWaC+POS COALS SY AN, VO, SV MA, CR, APD, CL
Johannsen et al (2011) ukWaC COALS SY, CT AN, VO, SV MA, CR, APD, CL
Reddy et al (2011a) ukWaC+POS own CT, CY NN MA, S, R2
Table 1: Overview of experiments applying WSMs to determine semantic compositionality of word expressions. BNC
- British National Corpus, GR - grammatical relations, GNC - German newspaper corpus, TREC - TREC corpus;
SY - substitutability-based methods, CT - component-based methods, CTn - component-based methods comparing
WSM neighbors of expressions and their components, CY - compositionality-based methods; NVAP c. - noun, verb,
adjective, adverb combinations, NN - noun-noun, VP - verb-particles, AN - adjective-noun, VO - verb-object, SV -
subject-verb, PV - phrasal-verb, PNV - preposition-noun-verb; dicts. - dictionaries of idioms, WN - Wordnet, MA
- use of manually annotated data, S - Spearman correlation, PC - Pearson correlation, CR - Spearman and Kendall
correlations, APD - average point difference, CL - classification, P/R - Precision/Recall, P/Rc - Precision/Recall
curves, Fm - F measure, R2 - goodness.
dency to exploit manually annotated data from a
specific corpus, ranging from semantically composi-
tional to non-compositional expressions (McCarthy
et al, 2003; Katz, 2006; Johannsen et al, 2011;
Reddy et al, 2011a; Krc?ma?r? et al, 2012).
This approach, as opposed to the methods
based on dictionaries of MWEs (idioms) or Word-
net (Miller, 1995), has the following advantages:
Firstly, the classification of a manually annotated
data is not binary but finer-grained, enabling the
evaluation to be more detailed. Secondly, the low-
coverage problem of dictionaries, which originates
for example due to the facts that new MWEs still
arise or are domain specific, is avoided.1 For exam-
ple, Lin (1999), Schone and Jurafsky (2001), Bald-
win et al (2003) used Wordnet or other dictionary-
type resources.
3 Evaluation Methods
This section discusses evaluation methods includ-
ing average point difference (APD), Spearman and
Kendall correlations, and precision of classifica-
tion (PoC) suggested by Biemann and Giesbrecht
(2011); Precision/nBest, Recall/nBest and Preci-
sion/Recall curves proposed by Evert (2005); and
1The consequence of using a low-coverage dictionary can
cause underestimation of the used method since the dictionary
does not have to contain MWEs correctly found by that method.
Average Precision used by Pecina (2009). Our eval-
uation is based on the English part of the manu-
ally annotated datasets DISCO 2011 (Biemann and
Giesbrecht, 2011), further referred to as DISCO-En-
Gold.
Disco-En-Gold consists of 349 expressions di-
vided into training (TrainD), validation (ValD), and
test data (TestD) manually assigned scores from 0
to 100, indicating the level of compositionality (the
lower the score the lower the compositionality and
vice versa). The expressions are of the following
types: adjective-noun (AN), verb-object (VO), and
subject-verb (SV). Based on the numerical scores,
the expressions are also classified into three disjoint
classes (coarse scores): low, medium, and high com-
positional.2 A sample of the Disco-En-Gold data is
presented in Table 2.
Comparison of evaluation methods The purpose
of the DISCO workshop was to find the best meth-
ods for determining semantic compositionality. The
participants were asked to create systems capable of
assigning the numerical values closest to the ones
assigned by the annotators (Gold values). The pro-
posed APD evaluation measure is calculated as the
mean difference between the particular systems? val-
2Several expressions with the numerical scores close to the
specified thresholds were not classified into any class.
44
Type Expression Ns Cs
EN ADJ NN blue chip 11 low
EN V OBJ buck trend 14 low
EN ADJ NN open source 49 medium
EN V OBJ take advantage 57 medium
EN ADJ NN red squirrel 90 high
EN V SUBJ student learn 98 high
Table 2: A sample of manually annotated expressions
from Disco-En-Gold with their numerical scores (Ns) and
coarse scores (Cs).
ues and the Gold values assigned to the same expres-
sions. PoC is defined as the ratio of correct coarse
predictions to the number of all the predictions.
Following Krc?ma?r? et al (2012), we argue that
for the purpose of comparison of the methods, the
values assigned to a set of expressions by a certain
model are not as important as is the ranking of the
expressions (which is not sensitive to the original
distribution of compositionality values). Similarly
as Evert (2005), Pecina (2009), and Krc?ma?r? et al
(2012) we adopt evaluation based on ranking (al-
though the measures such as PoC or APD might pro-
vide useful information too).
Evaluation based on ranking can be realized
by measuring ranked correlations (Spearman and
Kendall) or Precision/Recall scores and curves com-
monly used e.g. in IR (Manning et al, 2008). In
IR, Precision is defined as the ratio of found rele-
vant documents to all the retrieved documents with
regards to a user?s query. Recall is defined as the ra-
tio of found relevant documents to all the relevant
documents in a test set to the user?s query. The
Precision/Recall curve is a curve depicting the de-
pendency of Precision upon Recall. Analogously,
the scheme can be used for evaluation of the meth-
ods finding semantically non-compositional expres-
sions. However, estimation of Recall is not possible
without knowledge of the correct class3 for every ex-
pression in a corpus. To bypass this, Evert (2005)
calculates Recall with respect to the set of annotated
data divided into non-compositional and composi-
tional classes. The Precision/nBest, Recall/nBest,
and Precision/Recall curves for the LSA experiment
3A semantically non-compositional expression or a seman-
tically compositional expressions
described in the following section are depicted in
Figures 1 and 2.
Evert?s (2005) curves allow us to visually com-
pare the results of the methods in more detail. To
facilitate comparison of several methods, we also
suggest using average precision (AP) adopted from
Pecina (2009), which reduces information provided
by a single Precision/Recall curve to one value. AP
is defined as a mean Precision at all the values of
Recall different from zero.
4 LSA experiment
LSA is WSM based on the Singular Value De-
composition (SVD) factorization (Deerwester et al,
1990) applied to the co-occurrence matrix. In the
matrix, the numbers of word occurrences in speci-
fied contexts4 are stored. The row vectors of the ma-
trix capture the word meanings.5 The idea of using
SVD is to project vectors corresponding to the words
into a lower-dimensional space and thus bring the
vectors of words with similar meaning near to each
other.
We built LSA WSM and applied the component-
based method to Disco-En-Gold. We used our
own modification of the LSA algorithm originally
implemented in the S-Space package (Jurgens and
Stevens, 2010). The modification lies in treating ex-
pressions and handling stopwords. Specifically, we
added vectors for the examined expressions to WSM
in such a way that the original vectors for words
were preserved. This differentiates our approach
e.g. from Baldwin et al (2003) or Johannsen et al
(2011) who label the expressions ahead of time and
build WSMs treating them as single words. Treat-
ing the expressions as the single words affects the
WSM vectors of their constituents. As an example,
consider the replacement of occurrences of ?short
distance? by e.g. the EXP#123 label. This affects
the WSM vectors of ?short? and ?distance? since
the numbers of their occurrences and the numbers
of contexts they occur in drops. Consequently, this
also affects the methods for determining the compo-
sitionality which are based upon using the vectors of
4The commonly used contexts for words are documents or
the preceding and following words in a specified window.
5WSMs exploit Harris? distributional hypothesis (Harris,
1954), which states that semantically similar words tend to ap-
pear in similar contexts.
45
expressions? constituents.
As for treating stopwords, we mapped the trigram
expressions containing the determiners ?the?, ?a?,
or ?an? as the middle word to the corresponding bi-
gram expressions without the determiners. The intu-
ition is to extract more precise co-occurrence vectors
for the VO expressions often containing some inter-
vening determiner. As an example, compare the oc-
currences of ?reinvent wheel? and ?reinvent (deter-
miner) wheel? in the ukWaC corpus which are 27
and 623, respectively, or the occurrences of ?cross
bridge? and ?cross (determiner) bridge? being 50
and 1050, respectively.6
We built LSA WSM from the whole ukWaC
POS-tagged corpus for all the word lemmas con-
catenated with their POS tags excluding stopwords.
We treated the following strings as stopwords: the
lemmas with frequency below 50 (omitting low-
frequency words), the strings containing two adja-
cent non-letter characters (omitting strings such as
web addresses and sequences of e.g. star symbols),
and lemmas with a different POS tag from noun,
proper noun, adjective, verb, and adverb (omitting
closed-class words). As contexts, the entire docu-
ments were used.
The co-occurrence matrix for words was normal-
ized by applying the log-entropy transformation and
reduced to 300 dimensions. Using these settings,
Landauer and Dumais (1997) obtained the best re-
sults. Finally, the co-occurrence vectors of expres-
sions were expressed in the lower-dimensional space
of words in a manner analogous to how a user?s
query is being expressed in lower-dimensional space
of documents in IR (Berry et al, 1995). The Disco-
En-Gold expressions were sorted in ascending order
by the average cosine similarity between the vec-
tors corresponding to the expressions and the vectors
corresponding to their components.
Evaluation We have not tried to find the optimal
parameter settings for the LSA-based model yet.
Therefore, we present the results on the concate-
nation of TrainD with ValD giving us TrainValD
and on TestD. The expressions ?leading edge? and
?broken link? were removed from TestD because
they occur in the ukWaC corpus assigned with the
6More precisely, the occurrences were calculated from the
POS-tagged parallels of the expressions.
required POS tags less than 50 times. APs with
the Spearman and Kendall correlations between the
compositionality values assigned by the LSA-based
model and the Gold values are depicted in Table 3.
The Spearman correlations of the LSA model ap-
plied to the whole TrainValD and TestD are highly
significant with p-values < 0.001. For the AP evalu-
ation, the expressions with numerical values less or
equal to 50 were classified as non-compositional7,
giving us the ratio of non-compositional expressions
in TrainValD and TestD equal to 0.26 and 0.20, re-
spectively. The Precision/nBest and Recall/nBest
graphs corresponding to the LSA-based model ap-
plied to TestD are depicted in Figure 1. The Preci-
sion/Recall graphs corresponding to the LSA-based
model applied to TrainD and TestD are depicted in
Figure 2.
For comparison, the graphs in Figures 1 and 2
also show the curves corresponding to the evaluation
of Pointwise Mutual Information (PMI).8 The co-
occurrence statistics of the expressions in Disco-En-
Gold was extracted from the window of size three,
sliding through the whole lemmatized ukWaC cor-
pus.
Discussion As suggested in Section 3, we com-
pare the results of the methods using Spearman and
Kendall correlations, AP, and Everts? curves. We
present the results of the LSA and PMI models
alongside the results of the best performing models
participating in the DISCO task. Namely, Table 3
presents the correlation values of our models, the
best performing WSM-based model (Reddy et al,
2011b), the best performing model based upon as-
sociation measures (Chakraborty et al, 2011), and
random baseline models.
The poor results achieved by employing PMI are
similar to the results of random baselines and in ac-
cordance with those of participants of the DISCO
workshop (Chakraborty et al, 2011). We hypoth-
esize that the PMI-based model incorrectly assigns
low values of semantic compositionality (high val-
7Choice of this value can affect the results. The value of 50
was chosen since it is the middle value between the manually
assigned scores ranging from 0 to 100.
8PMI is an association measure used to determine the
strength of association between two or more words based
on their occurrences and co-occurrences in a corpus (Pecina,
2009).
46
Model Dataset ?-All ?-AN ?-VO ?-SV ? -All ? -AN ? -VO ? -SV AP-All
LSA TrainValD 0.47 0.54 0.36 0.57 0.32 0.38 0.24 0.44 0.61
PMI TrainValD 0.02 -0.25 0.29 0.14 0.01 -0.18 0.20 0.10 0.28
baseline TrainValD 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.26
LSA TestD 0.50 0.50 0.56 0.41 0.35 0.36 0.39 0.30 0.53
Reddy-WSM TestD 0.35 - - - 0.24 - - - -
StatMix TestD 0.33 - - - 0.23 - - - -
PMI TestD -0.08 -0.07 0.13 -0.08 -0.06 -0.04 0.08 -0.07 0.21
baseline TestD 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20
Table 3: The values of AP, Spearman (?) and Kendall (? ) correlations between the LSA-based and PMI-based model
respectively and the Gold data with regards to the expression type. Every zero value in the table corresponds to the
theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the
ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator.
Reddy-WSM stands for the best performing WSM in the DISCO task (Reddy et al, 2011b). StatMix stands for the best
performing system based upon association measures (Chakraborty et al, 2011). Only ?-All and ? -All are available for
the models explored by Reddy et al (2011b) and Chakraborty et al (2011).
ues of PMI) to frequently occurring fixed expres-
sions. For example, we observed that the calculated
values of PMI for ?international airport? and ?reli-
gious belief? were high.
To the contrary, our results achieved by employ-
ing the LSA model are statistically significant and
better than those of all the participants of the DISCO
workshop. However, the data set is probably not
large enough to provide statistically reliable com-
parison of the methods and it is not clear how re-
liable the dataset itself is (the interannotator agree-
ment was not analyzed) and therefore we can not
make any hard conclusions.
5 Conclusion
We analysed the previous works applying WSMs
for determining the semantic compositionality of ex-
pressions. We discussed and summarized the major-
ity of techniques presented in the papers. Our anal-
ysis reveals a large diversity of approaches which
leads to incomparable results (Table 1). Since it has
been shown that WSMs can serve as good predic-
tors of semantic compositionality, we aim to create
a comparative study of the approaches.
Our analysis implies to evaluate the proposed ap-
proaches using human annotated data and evalua-
tion techniques based on ranking. Namely, we sug-
gest using Spearman and Kendall correlations, Pre-
cision/nBest, Recall/nBest, Precision/Recall curves,
and AP.
Using the suggested evaluation techniques, we
present the results of our first experiments exploit-
ing LSA (Figures 1, 2 and Table 3). The results of
the LSA-based model, compared with random base-
lines, PMI-based model, and all the WSM-based and
statistical-based models proposed by the participants
of the DISCO task, are very promising.
Acknowledgments
We thank to V??t Suchomel for providing the
ukWaC corpus and the anonymous reviewers for
their helpful comments and suggestions. The re-
search is supported by Advanced Computing and
Information Systems (grant no. SGS-2013-029)
and by the Czech Science Foundation (grant no.
P103/12/G084). Also, the access to the CERIT-SC
computing facilities provided under the programme
Center CERIT Scientific Cloud, part of the Opera-
tional Program Research and Development for Inno-
vations, reg. no. CZ. 1.05/3.2.00/08.0144 is highly
appreciated.
References
Otavio Costa Acosta, Aline Villavicencio, and Viviane P.
Moreira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval. In
Proceedings of the Workshop on Multiword Expres-
sions: from Parsing and Generation to the Real World,
MWE ?11, pages 101?109, Stroudsburg, PA, USA.
47
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, FL. ISBN 978-1420085921.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. Proceedings
of the ACL 2003 workshop on Multiword expressions
analysis acquisition and treatment, pages 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, acquisition
and treatment, volume 18 of MWE ?03, pages 65?72,
Stroudsburg, PA, USA.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Journal of Language Resources And
Evaluation, 43(3):209?226.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O?Brien. 1995. Using linear algebra for intelligent
information retrieval. SIAM Rev., 37(4):573?595.
Chris Biemann and Eugenie Giesbrecht. 2011. Distri-
butional semantics and compositionality 2011: shared
task description and results. In Proceedings of the
Workshop on Distributional Semantics and Composi-
tionality, DiSCo ?11, pages 21?28.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in statis-
tical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 242?245, Strouds-
burg, PA, USA.
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal,
Tanik Saikh, and Sivaju Bandyopadhyay. 2011.
Shared task system description: Measuring the com-
positionality of bigrams using statistical methodolo-
gies. In Proceedings of the Workshop on Distribu-
tional Semantics and Compositionality, pages 38?42,
Portland, Oregon, USA.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22?29.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Afsaneh Fazly. 2007. Automatic Acquisition of Lexical
Knowledge about Multiword Predicates. Ph.D. thesis,
University of Toronto.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Generation
to the Real World, MWE ?11, pages 20?24, Strouds-
burg, PA, USA.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Anders Johannsen, Hector Martinez Alonso, Christian
Rish?j, and Anders S?gaard. 2011. Shared task sys-
tem description: frustratingly hard compositionality
prediction. In Proceedings of the Workshop on Distri-
butional Semantics and Compositionality, DiSCo ?11,
pages 29?32, Stroudsburg, PA, USA.
David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ?10, pages 30?35, Stroudsburg,
PA, USA.
Graham Katz. 2006. Automatic identification of
non-compositional multi-word expressions using la-
tent semantic analysis. In In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Proper-
ties, pages 12?19.
Lubom??r Krc?ma?r?, Karel Jez?ek, and Massimo Poesio.
2012. Detection of semantic compositionality using
semantic spaces. Lecture Notes in Computer Science,
7499 LNAI:353?361.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
pages 317?324, Stroudsburg, PA, USA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28(2):203?
208.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
48
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop on
Multiword expressions analysis acquisition and treat-
ment, volume 18 of MWE ?03, pages 73?80.
George A. Miller. 1995. WordNet: A lexical database
for English. Communications of the ACM, 38:39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, LREC.
Pavel Pecina. 2009. Lexical Association Measures: Col-
location Extraction, volume 4 of Studies in Compu-
tational and Theoretical Linguistics. U?FAL, Praha,
Czechia.
Carlos Ramisch. 2012. A generic framework for multi-
word expressions treatment: from acquisition to appli-
cations. In Proceedings of ACL 2012 Student Research
Workshop, ACL ?12, pages 61?66, Stroudsburg, PA,
USA.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011a. An empirical study on compositionality in
compound nouns. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 210?218, Chiang Mai, Thailand.
Siva Reddy, Diana McCarthy, Suresh Manandhar, and
Spandana Gella. 2011b. Exemplar-based word-space
model for compositionality detection: Shared task sys-
tem description. In Proceedings of the Workshop on
Distributional Semantics and Compositionality, pages
54?60, Portland, Oregon, USA.
Douglas L. Rohde, Laura M. Gonnerman, and David C.
Plaut. 2005. An improved model of semantic sim-
ilarity based on lexical co-occurrence. Unpublished
manuscript.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceedings
of the Third International Conference on Computa-
tional Linguistics and Intelligent Text Processing, CI-
CLing ?02, pages 1?15, London, UK. Springer-Verlag.
Magnus Sahlgren. 2005. An introduction to random in-
dexing. In Methods and Applications of Semantic In-
dexing Workshop at the 7th International Conference
on Terminology and Knowledge Engineering, Leipzig,
Germany.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, pages 100?108.
Violeta Seretan. 2008. Collocation extraction based on
syntactic parsing. Ph.D. thesis, University of Geneva.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188.
49
baseline PMI LSA
0 2 5 5 0 7 5 100 125 150 175
nBest
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Pre
cisio
n
baseline PMI LSA
0 2 5 5 0 7 5 100 125 150 175
nBest
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Rec
all
Figure 1: Smoothed graphs depicting the dependency of Precision (left) and Recall (right) upon the nBest selected
non-compositional candidates from the ordered list of expressions in TestD created by the LSA and PMI-based models.
baseline PMI LSA
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Recall
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Pre
cisio
n
baseline PMI LSA
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Recall
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Pre
cisio
n
Figure 2: Smoothed graphs depicting the dependency of Precision upon Recall using the LSA and PMI-based models
ordering the expressions in TrainValD (left) and TestD (right) according to their non-compositionality.
50
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 64?73,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Determining Compositionality of Word Expressions Using
Various Word Space Models and Measures
Lubom??r Krc?ma?r?1,2
1University of West Bohemia,
Faculty of Applied Sciences,
NTIS ? New Technologies
for the Information Society,
Pilsen, Czech Republic
lkrcmar@kiv.zcu.cz
Karel Jez?ek2
2University of West Bohemia,
Faculty of Applied Sciences,
Department of Computer
Science and Engineering,
Pilsen, Czech Republic
jezek ka@kiv.zcu.cz
Pavel Pecina3
3Charles University in Prague,
Faculty of Mathematics and
Physics, Institute of Formal
and Applied Linguistics,
Prague, Czech Republic
pecina@ufal.mff.cuni.cz
Abstract
This paper presents a comparative study
of 5 different types of Word Space Mod-
els (WSMs) combined with 4 different
compositionality measures applied to the
task of automatically determining seman-
tic compositionality of word expressions.
Many combinations of WSMs and mea-
sures have never been applied to the task
before.
The study follows Biemann and Gies-
brecht (2011) who attempted to find a list
of expressions for which the composition-
ality assumption ? the meaning of an ex-
pression is determined by the meaning of
its constituents and their combination ?
does not hold. Our results are very promis-
ing and can be appreciated by those inter-
ested in WSMs, compositionality, and/or
relevant evaluation methods.
1 Introduction
Our understanding of WSM is in agreement with
Sahlgren (2006): ?The word space model is a
computational model of word meaning that uti-
lizes the distributional patterns of words collected
over large text data to represent semantic similar-
ity between words in terms of spatial proximity?.
There are many types of WSMs built by different
algorithms. WSMs are based on the Harris distri-
butional hypothesis (Harris, 1954), which assumes
that words are similar to the extent to which they
share similar linguistic contexts. WSM can be
viewed as a set of words associated with vectors
representing contexts in which the words occur.
Then, similar vectors imply (semantic) similarity
of the words and vice versa. Consequently, WSMs
provide a means to find words semantically simi-
lar to a given word. This capability of WSMs is
exploited by many Natural Language Processing
(NLP) applications as listed e.g. by Turney and
Pantel (2010).
This study follows Biemann and Giesbrecht
(2011), who attempted to find a list of non-
compositional expressions whose meaning is not
fully determined by the meaning of its con-
stituents and their combination. The task turned
out to be frustratingly hard (Johannsen et al,
2011). Biemann?s idea and motivation is that non-
compositional expressions could be treated as sin-
gle units in many NLP applications such as In-
formation Retrieval (Acosta et al, 2011) or Ma-
chine Translation (Carpuat and Diab, 2010). We
extend this motivation by stating that WSMs could
also benefit from a set of non-compositional ex-
pressions. Specifically, WSMs could treat se-
mantically non-compositional expressions as sin-
gle units. As an example, consider ?kick the
bucket?, ?hot dog?, or ?zebra crossing?. Treat-
ing such expressions as single units might improve
the quality of WSMs since the neighboring words
of these expressions should not be related to their
constituents (?kick?, ?bucket?, ?dog? or ?zebra?),
but instead to the whole expressions.
Recent works, including that of Lin (1999),
Baldwin et al (2003), Biemann and Giesbrecht
(2011), Johannsen et al (2011), Reddy et al
(2011a), Krc?ma?r? et al (2012), and Krc?ma?r? et al
(2013), show the applicability of WSMs in deter-
mining the compositionality of word expressions.
The proposed methods exploit various types of
WSMs combined with various measures for de-
termining the compositionality applied to various
datasets. First, this leads to non-directly compa-
rable results and second, many combinations of
64
WSMs and measures have never before been ap-
plied to the task. The main contribution and nov-
elty of our study lies in systematic research of
several basic and also advanced WSMs combined
with all the so far, to the best of our knowledge,
proposed WSM-based measures for determining
the semantic compositionality.
The explored WSMs, described in more detail
in Section 2, include the Vector Space Model,
Latent Semantic Analysis, Hyperspace Analogue
to Language, Correlated Occurrence Analogue to
Lexical Semantics, and Random Indexing. The
measures, including substitutability, endocentric-
ity, compositionality, and neighbors-in-common-
based, are described in detail in Section 3. Sec-
tion 4 describes our experiments performed on
the manually annotated datasets ? Distributional
Semantics and Compositionality dataset (DISCO)
and the dataset built by Reddy et al (2011a). Sec-
tion 5 summarizes the results and Section 6 con-
cludes the paper.
2 Word Space Models
The simplest and oldest types of WSMs1 are the
Vector Space Model (VSM) and Hyperspace Ana-
logue to Language (HAL). More recent and ad-
vanced models include Latent Semantic Analy-
sis (LSA), which is based on VSM, and Corre-
lated Occurrence Analogue to Lexical Semantics
(COALS), which originates from HAL. Random
Indexing (RI) is WSM joining the principles of
LSA and HAL. Many other WSMs have been pro-
posed too. Their description is outside the scope
of this paper and can be found e.g. in Turney and
Pantel (2010) or Jurgens and Stevens (2010).
VSM is based on the assumption that similar (re-
lated) words tend to occur in the same documents.2
VSM stores occurrence counts of all word types
in documents a given corpus in a co-occurrence
matrix C. The row vectors of the matrix corre-
spond to the word types and the columns to the
documents in the corpus. The numbers of occur-
rences cij in C are usually weighted by the prod-
uct of the local and global weighting functions
(Nakov et al, 2001). The local function weights
cij by the same mathematical function; typically
none (further denoted as no), log(cij + 1) (de-
1WSMs are also referred to as distributional models of
semantics, vector space models, or semantic spaces.
2VSM was originally developed for the SMART informa-
tion retrieval system (Salton, 1971).
noted as log) or
?
cij (denoted as sqrt). The
purpose of local weighting is to lower the im-
portance of highly occurring words in the docu-
ment. The global function weights every value
in row i of C by the same value calculated for
row i. Typically: none (denoted as No), In-
verse Document Frequency (denoted as Idf ) or
a function referred to as Entropy (Ent). Idf
is calculated as 1 + log(ndocs/df(i)) and Ent
as 1 + {
?
j p(i, j) log p(i, j)}/ log ndocs, where
ndocs is the number of documents in the corpora,
df(i) is the number of documents containing word
type i, and p(i, j) is the probability of occurrence
of word type i in document j.
LSA builds on VSM and was introduced by
Landauer and Dumais (1997). The LSA algo-
rithm works with the same co-occurrence matrix
C which can be weighted in the same manner as
in VSM. The matrix is than transformed by Sin-
gular Value Decomposition (SVD) (Deerwester et
al., 1990) into C. The purpose of SVD is to
project the row vectors and column vectors of C
into a lower-dimensional space and thus bring the
vectors of word types and vectors of documents,
respectively, with similar meanings near to each
other.3 The output number of dimensions is a pa-
rameter of SVD and typically ranges from 200 to
1000 (Landauer and Dumais, 1997; Rohde et al,
2005).
HAL was first explored by Lund and Burgess
(1996). It differs from VSM and LSA in that it
only exploits neighboring words as contexts for
word types. HAL processes the corpus by moving
a sliding double-sided window with a size rang-
ing from 1 to 5 around the word type in focus
and accumulating the weighted co-occurrences of
the preceding and following words into a matrix.
Typically, the linear weighting function is used
to ensure that the occurrences of words which
are closer to the word type in focus are more
significant. The dimensions of the resulting co-
occurrence matrix are of size |V | and 2|V |, where
V denotes the vocabulary consisting of all the
word types occurring in the processed corpora. Fi-
nally, the HAL co-occurrence matrix can be re-
duced by retaining the most informative columns
only. The columns with the highest values of en-
tropy (?
?
j pj log pj , where pj denotes the prob-
3In this way, LSA is able to capture higher-order co-
occurrences.
65
ability of a word in the investigated column j) can
be considered as the most informative. The alter-
natives and their description can be found e.g. in
Song et al (2004).
COALS was introduced by Rohde et al (2005).
Compared to HAL, COALS also processes a cor-
pus by using a sliding window and linear weight-
ing, but differs in several aspects: the window size
of COALS is 4 and this value is fixed; COALS
does not distinguish between the preceding and
following words and treats them equally; applying
COALS supposes that all but the most frequent m
columns reflecting the most common open-class
words are discarded; COALS transforms weighted
counts in the co-occurrence matrix in a special
way (all the word pair correlations are calculated,
negative values are set to 0, and non-negative ones
are square rooted ? corr); and optionally, Singu-
lar Value Decomposition (Deerwester et al, 1990)
can be applied to the COALS co-occurrence ma-
trix.
RI is described in Sahlgren (2005) and can be
viewed as a mixture of HAL and LSA. First, RI
assigns random vectors to each word type in the
corpus. The random vectors, referred to as index
vectors, are very sparse, typically with a length
of thousands, and contain only several (e.g. 7)
non-zero values from the {-1,1} set. Second, RI
processes the corpus by exploiting a sliding win-
dow like HAL and COALS. However, RI does not
accumulate the weighted co-occurrence counts of
neighboring words to the vector of the word type
in focus. Instead, RI accumulates the index vec-
tors of the co-occurring words. For accounting the
word order, the permutation variant of RI was also
developed (Sahlgren et al, 2008). This variant
permutes the index vectors of neighboring words
of the word type in focus according to the word
order.
3 Compositionality Measures
We experimented with four basically different
compositionality measures (further referred to as
Measures) (Krc?ma?r? et al, 2013). Each Measure
employs a function to measure similarity of WSM
vectors. We experimented with the following
ones: cosine (cos), Euclidian (inverse to Euclid-
ian distance) (euc), and Pearson correlation (cor).
The mathematical formulas are presented below.
cos(a,b) =
?n
i=1 aibi??n
i=1(ai)
2
?n
i=1(bi)
2
euc(a,b) =
1
1 +
??n
i=1 (ai ? bi)
2
cor(a,b) =
?n
i=1 (ai ? a?)(bi ? b?)??n
i=1(ai ? a?)
2
?n
i=1(bi ? b?)
2
where a? =
?n
i=1 ai
n
, b? =
?n
i=1 bi
n
SU The substitutability-based Measure is based
on the fact that the replacement of non-
compositional expressions? constituents by the
words similar to them leads to anti-collocations
(Pearce, 2002). The compositionality of expres-
sions is calculated as the ratio between the num-
ber of occurrences of the expression in a corpora
and the sum of occurrences of its alternatives ?
possibly anti-collocations. In a similar way, we
can compare pointwise mutual information scores
(Lin, 1999). As an example, consider the possible
occurrences of ?hot dog? and ?warm dog? in the
corpora.
Formally, adopted from Krc?ma?r? et al (2012),
we calculate the compositionality score csu for an
examined expression as follows:
csu =
?H
i=1W ?a
h
i ,m? ?
?M
j=1W ?h, a
m
j ?
W ?h,m?
,
where ?h,m? denotes the number of corpora oc-
currences of the examined expression consisting
of a head and a modifying word, ahi and a
m
j denote
i-th and j-th most similar word4 in a certain WSM
to the head and modifying word of the expression,
respectively. W stands for a weighting function;
following Krc?ma?r? et al (2012), we experimented
with no (no) and logarithm (log) weighting. The
? symbol stands for one of the two operators: ad-
dition (plus) and multiplication (mult).
EN The endocentricity-based Measure, also re-
ferred to as component or constituent-based, com-
pares the WSM vectors of the examined expres-
sions and their constituents. The vectors expected
to be different from each other are e.g. the vector
representing the expression ?hot dog? and the vec-
tor representing the word ?dog?. Formally, the
4When exploiting POS tags, we constrained the similar
words to be of the same POS category in our experiments.
66
compositionality score cen can be calculated as
follows:
cen = f(xh, xm) ,
where xh and xm denote the similarity (sim) or
inverse rank distance (?dist) between the exam-
ined expression and its head and modifying con-
stituent, respectively, with regards to a certain
WSM. Function f stands for a combination of its
parameters: 0.5xh + 0.5xm (avg), 0xh + 1xm
(mOnly), 1xh + 0xm (hOnly), min(xh, xm) (min),
and max(xh, xm) (max).
CO The compositionality-based Measure com-
pares the true co-occurrence vector of the exam-
ined expression and the vector obtained from the
vectors corresponding to the constituents of the
expression using some compositionality function
(Reddy et al, 2011a). Commonly used compo-
sitionality functions are vector addition (?) and
pointwise vector multiplication (?) (Mitchell and
Lapata, 2008). The vectors expected to be dif-
ferent from each other are e.g. ?hot dog? and
?hot???dog?. Formally,
cco = s(ve, vh ? vm) ,
where ve, vh, and vm stand for vectors of an ex-
amined expression, its head and modifying con-
stituents, respectively. ? stands for a vector opera-
tion.
NE The neighbors-in-common-based Measure
is based on overlap of the most similar words to
the examined expression and to its constituents
(McCarthy et al, 2003). As an example, consider
that ?hot dog? is similar to ?food? or ?chips? and
?dog? is similar to ?cat? or ?bark?. On the other
hand, the list of neighbors of a semantically com-
positional expression such as ?black dog? is sup-
posed to overlap with at least one of the lists of
neighbors of both the expression constituents. For-
mally,
cne = o
h
N + o
m
N ,
where ohN and o
m
N stand for the number of same
words occurring in the list of the most similar
words to the examined expression and to its head
and modifying constituent, respectively.
4 Experiments
We evaluated the ability of various combinations
of WSMs and Measures to rank expressions as the
human annotators had done ahead of time.
Datasets We experimented with the DISCO
(Biemann and Giesbrecht, 2011) and Reddy
(Reddy et al, 2011a) human annotated datasets,
built for the task of automatic determining of se-
mantic compositionality. The DISCO and Reddy
datasets consist of manually scored expressions
of adjective-noun (AN), verb-object (VO), and
subject-verb (SV) types and the noun-noun (NN)
type, respectively. The DISCO dataset consists
of 349 expressions divided into training, valida-
tion, and test data (TestD); the Reddy dataset con-
sists of one set containing 90 expressions. Since
the DISCO validation data are of low size (35),
we concatenated them with the training data (Tr-
ValD). To TrValD and TestD we added the Reddy
dataset, which we had divided stratifically ahead
of time. Numbers of expressions of all the differ-
ent types are summarized in Table 1.
dataset AN-VO-SV AN VO SV NN
TrValD 175 68 68 39 45
TestD 174 77 62 35 45
Table 1: Numbers of expressions of all the differ-
ent types from the DISCO and Reddy datasets.
WSM construction Since the DISCO and
Reddy data were extracted from the ukWaC cor-
pus (Baroni et al, 2009), we also build our WSMs
from the same corpus. We use our own modifica-
tion of the S-Space package (Jurgens and Stevens,
2010). The modification lies in treating multiword
expressions and handling stopwords. Specifically,
we extended the package with the capability of
building WSM vectors for the examined expres-
sions in such a way that the WSM vectors previ-
ously built for words are preserved. This differen-
tiates our approach e.g. from Baldwin et al (2003),
who label the expressions in the corpus ahead of
time and treat them as single words.5 As for treat-
ing stopwords, we map trigrams containing deter-
miners as the middle word into bigrams without
the determiners. The intuition is to extract better
co-occurrence statistics for VO expressions often
containing an intervening determiner. As an ex-
ample, compare the occurrences of ?reinvent (de-
5Since many single word occurrences disappear, the
WSM vectors for words change. The more expressions are
treated as single words, the more WSM changes. Conse-
quently, we believe that this approach cannot be used for
building a list of all expressions occurring in an examined
corpus ordered by their compositionality score.
67
terminer) wheel? and ?reinvent wheel? in ukWaC
being 623 and 27, respectively.
We experimented with lemmas (noT) or with
lemmas concatenated with their part of speech
(POS) tags (yesT). We labeled the following
strings in ukWaC as stopwords: low-frequency
words (lemmas with frequency< 50), strings con-
taining two adjacent non-letter characters (thus
omitting sequences of various symbols), and
closed-class words.
For our experiments, we built WSMs using var-
ious parameters examined in previous works (see
Section 2) and parameters which are implied from
our own experience with WSMs. Figure 1 sum-
marizes all the parameters we used for building
WSMs.
Measure settings We examined various Mea-
sure settings (see Section 3), summarized in Ta-
ble 2. For all the vector comparisons, we used the
cos similarity. Only for HAL we also examined
euc and for COALS cor, since these are the rec-
ommended similarity functions for these particu-
lar WSMs (Lund and Burgess, 1996; Rohde et al,
2005).
Met. par. possible values
all sim. cos, euc if HAL, cor if COALS
SU H 0,1,...,20,30,...,100
SU M 0,1,...,20,30,...,100
SU W no, log
SU ? plus, mult
EN x sim, ?dist
EN f avg, mOnly, hOnly, min, max
CO ? ?, ?
NE N 10,20,...,50,100,200,...,500,1000
Table 2: All the parameters of Measures for de-
termining semantic compositionality described in
Section 3 used in our experiments.
Experimental setup Following Biemann and
Giesbrecht (2011), Reddy et al (2011a), Krc?ma?r?
et al (2012), and Krc?ma?r? et al (2013), we use
the Spearman correlation (?) for the evaluation of
all the combinations of WSMs and Measures (Se-
tups). Since the distribution of scores assigned to
Reddy?s NN dataset might not have corresponded
to the distribution of DISCO scores, we decided
not to map them to the same scale. Thus, we do not
create a single list consisting of all the examined
expressions. Instead, we order our Setups accord-
ing to the weighted average of Spearman corre-
lations calculated across all the expression types.
The weights are directly proportional to the fre-
quencies of the particular expression types. Thus,
the Setup score (wAvg) is calculated as follows:
wAvg =
|AN |?AN + |V O|?V O + |SV |?SV + |NN |?NN
|AN | + |V O| + |SV | + |NN |
.
Having the evaluation testbed, we tried to find
the optimal parameter settings for all WSMs com-
bined with all Measures with the help of TrValD.
Then, we applied the found Setups to TestD.
Notes Because several expressions or their con-
stituents concatenated with their POS tags did not
occur sufficiently often (for expressions: ? 0,
for constituents: ? 50) in ukWaC, we removed
them from the experiments; we removed ?number
crunching?, ?pecking order?, and ?sacred cow?
from TrValD and ?leading edge?, ?broken link?,
?spinning jenny?, and ?sitting duck? from TestD.
5 Results
The Setups achieving the highest wAvg when ap-
plied to TrValD are depicted in Table 3. The same
Setups and their results when applied to TestD are
depicted in Table 4. The values of Spearman cor-
relations in TestD confirm many of the observa-
tions from TrValD6:
Almost all the combinations of WSMs and
Measures achieve correlation values which are sta-
tistically significant. This is best illustrated by the
?(AN ?V O?SV ) column in Table 4, where a
lot of correlation values are statistically (p<0.05)
or highly statistically (p<0.001) significant, with
regards to the number of expressions (172).
The results suggest that for every expression
type, the task of determining compositionality is
of varying difficulty. While determining the com-
positionality of the NN expression type seems to
be the simplest (the highest correlations observed),
determining the compositionality of the SV ex-
pression type seems to be hard since the majority
of values in the ?SV column are not statistically
significant; taking into account the number of SV
expressions in TestD ? 35, the statistically signifi-
cant value of ? at the p<0.05 level is 0.34.
The correlation values differ with regards to the
expression type. Certain WSMs combined with
6A test of statistical difference between two values of the
Spearman correlation is adopted from Papoulis (1990).
68
Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon
denotes OR. All the examined combinations of parameters are implied from reading the diagram from
left to right.
certain Measures, although achieving high corre-
lations upon certain expression types, fail to cor-
relate with the rest of the expression types. Com-
pare e.g. the correlation values of VSM and LSA
combined with the SU Measure upon the AN and
SV types with the correlation values upon the VO
and NN types.
The results, as expected, illustrate that employ-
ing more advanced alternatives of basic WSMs is
more appropriate. Specifically, LSA outperforms
VSM and COALS outperforms HAL in 21 and 23
correlation values out of 24, respectively. Con-
cerning RI, the values of correlations seem to be
close to the values of VSM and HAL.
An interesting observation showing the appro-
priateness of using wAvg(of?) as a good evalu-
ation score is supported by a comparison of the
wAvg(of?) and ?(AN?V O?SV ) columns. The
columns suggest that some Setups might only be
able to order the expressions of the same type and
might not be able to order the expressions of dif-
ferent types among each other. As an example,
compare the value of ? = 0.42 in wAvg(of?)
with ? = 0.28 in ?(AN?V O?SV ) in the row cor-
responding to COALS combined with SU. Con-
sider also that all the values of correlations are
higher or equal to the value in ?(AN?V O?SV ).
As for the parameters learned from applying
all the combinations of differently set WSM algo-
rithms and Measures to TrValD, their diversity is
well illustrated in Tables 5 and 6. Due to this diver-
sity, we cannot recommend any particular settings
except for one. All our SU Measures benefit from
weighting numbers of expression occurrences by
logarithm.
The correlation values in TestD are slightly
lower ? probably due to overfitting ? than the
ones observed in TrValD. HAL combined with the
Measures using euc similarity was not as success-
ful as when combined with cos.7
For comparison, the results of Reddy et al
(2011b) and Chakraborty et al (2011) as the
results of the best performing Setups based on
WSMs and association measures, respectively, ap-
plied to the DISCO data, are presented (Biemann
and Giesbrecht, 2011). The correlation values of
our Setups based on LSA and COALS, respec-
tively, are mostly higher. However, the improve-
ments are not statistically significant. Also, the re-
cent results achieved by Krc?ma?r? et al (2012) em-
ploying COALS and Krc?ma?r? et al (2013) employ-
7However, using HAL combined with euc, we observed
significant negative correlations which deserve further explo-
ration.
69
ing LSA are depicted.
Discussion As described above, we observed
different values of correlations for different ex-
pression types. This motivates us to think about
other classes of expressions different from types;
Measures could be e.g. varyingly successful with
regards to different occurrence frequency classes
of expressions (Evert, 2005). However, with such
small datasets, as shown e.g. by the fact that the
majority of our results are statistically indistin-
guishable, we cannot carry out any deeper in-
vestigations. A large dataset would provide a
more reliable comparison. Ideally, this would
consist of all the candidate expressions occurring
in some smaller corpus. Also, we would pre-
fer the annotated dataset not to be biased towards
non-compositional expressions and to be provided
with an inner-annotator agreement (Pecina, 2008);
which is unfortunately not the case of the DISCO
dataset.
6 Conclusion
Our study suggests that different WSMs combined
with different Measures perform reasonably well
in the task of determining the semantic composi-
tionality of word expressions of different types.
Especially, LSA and COALS perform well in
our experiments since their results are better than
those of their basic variants (VSM and HAL, re-
spectively) and, although not statistically signifi-
cantly, they outperform the best results of the pre-
viously proposed approaches (Table 4).
Importantly, our results demonstrate (Section 5)
that the datasets used for the experiments are small
for: first, a statistical learning of optimal parame-
ters of both WSM algorithms and Measures; sec-
ond, a thorough (different types) and reliable (sta-
tistically significant) comparison of our and the
previously proposed approaches.
Therefore, we plan to build a larger manually-
annotated dataset. Finally, we plan to extract
a list of semantically non-compositional expres-
sions from a given corpus and experiment with us-
ing it in NLP applications.
Acknowledgments
We thank to V??t Suchomel for providing the
ukWaC corpus and the anonymous reviewers
for their helpful comments and suggestions.
This work was supported by the European
Regional Development Fund (ERDF), project
NTIS ? New Technologies for the Informa-
tion Society, European Centre of Excellence,
CZ.1.05/1.1.00/02.0090; by Advanced Comput-
ing and Information Systems (grant no. SGS-
2013-029); and by the Czech Science Foun-
dation (grant no. P103/12/G084). Also, the
access to the CERIT-SC computing facilities
provided under the programme Center CERIT
Scientific Cloud, part of the Operational Pro-
gram Research and Development for Innovations,
reg. no. CZ.1.05/3.2.00/08.0144 is highly appreci-
ated.
References
Otavio Costa Acosta, Aline Villavicencio, and Vi-
viane P. Moreira. 2011. Identification and treatment
of multiword expressions applied to information re-
trieval. In Proceedings of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World, MWE ?11, pages 101?109, Strouds-
burg, PA, USA.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. Proceed-
ings of the ACL 2003 workshop on Multiword ex-
pressions analysis acquisition and treatment, pages
89?96.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web:
a collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources And Evaluation, 43(3):209?226.
Chris Biemann and Eugenie Giesbrecht. 2011. Dis-
tributional semantics and compositionality 2011:
shared task description and results. In Proceedings
of the Workshop on Distributional Semantics and
Compositionality, DiSCo ?11, pages 21?28.
Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 242?
245, Stroudsburg, PA, USA.
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal,
Tanik Saikh, and Sivaju Bandyopadhyay. 2011.
Shared task system description: Measuring the com-
positionality of bigrams using statistical methodolo-
gies. In Proceedings of the Workshop on Distribu-
tional Semantics and Compositionality, pages 38?
42, Portland, Oregon, USA.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
70
WSM Measure wAvg(of ?) ?AN-VO-SV ?AN ?VO ?SV ?NN
VSM1 SU1 0.31 0.11 -0.03 0.36 0.31 0.75
VSM2 EN1 0.36 0.32 0.41 0.30 0.10 0.61
VSM3 CO1 0.40 0.34 0.40 0.26 0.39 0.64
VSM1 NE1 0.34 0.26 0.20 0.48 0.07 0.60
LSA1 SU2 0.34 0.19 -0.05 0.46 0.42 0.71
LSA2 EN2 0.56 0.53 0.54 0.51 0.59 0.65
LSA3 CO1 0.55 0.53 0.49 0.56 0.63 0.58
LSA2 NE2 0.50 0.45 0.46 0.37 0.64 0.62
HAL1 SU3 0.45 0.36 0.28 0.50 0.40 0.67
HAL2 EN3 0.36 0.35 0.47 0.28 0.27 0.38
HAL3 CO1 0.23 0.15 0.28 0.12 -0.01 0.54
HAL4 NE3 0.27 0.25 0.31 0.21 0.17 0.39
COALS1 SU4 0.48 0.41 0.28 0.56 0.49 0.68
COALS2 EN2 0.58 0.54 0.6 0.63 0.37 0.68
COALS2 CO1 0.59 0.54 0.6 0.64 0.37 0.70
COALS2 NE4 0.58 0.56 0.61 0.58 0.46 0.67
RI1 SU5 0.52 0.44 0.45 0.51 0.52 0.68
RI2 EN3 0.45 0.44 0.41 0.57 0.33 0.45
RI3 CO1 0.21 0.13 0.13 0.16 0.11 0.54
RI2 NE5 0.43 0.43 0.43 0.53 0.21 0.49
Table 3: The Spearman correlations ? of the best performing (wAvg) combinations of particular WSMs
and Measures from all the tested Setups applied to TrValD. The highest correlation values in the particular
columns and the correlation values which are not statistically different from them (p < 0.05) are in bold
(yet we do not know how to calculate the stat. significance for the wAvg(of ?) column). The parameters
of WSMs and Measures corresponding to the indexes are depicted in Tables 5 and 6, respectively.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert. 2005. The statistics of word cooccur-
rences : word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Anders Johannsen, Hector Martinez Alonso, Christian
Rish?j, and Anders S?gaard. 2011. Shared task sys-
tem description: frustratingly hard compositionality
prediction. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, DiSCo
?11, pages 29?32, Stroudsburg, PA, USA.
David Jurgens and Keith Stevens. 2010. The s-
space package: an open source package for word
space models. In Proceedings of the ACL 2010 Sys-
tem Demonstrations, ACLDemos ?10, pages 30?35,
Stroudsburg, PA, USA.
Lubom??r Krc?ma?r?, Karel Jez?ek, and Massimo Poesio.
2012. Detection of semantic compositionality using
semantic spaces. Lecture Notes in Computer Sci-
ence, 7499 LNAI:353?361.
Lubom??r Krc?ma?r?, Karel Jez?ek, and Pavel Pecina. 2013.
Determining compositionality of word expressions
using word space models. In Proceedings of the 9th
Workshop on Multiword Expressions, pages 42?50,
Atlanta, Georgia, USA.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1999. Automatic identification of
non-compositional phrases. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
ACL ?99, pages 317?324, Stroudsburg, PA, USA.
Kevin Lund and Curt Burgess. 1996. Produc-
ing high-dimensional semantic spaces from lexi-
cal co-occurrence. Behavior Research Methods,
28(2):203?208.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions analysis acqui-
sition and treatment, volume 18 of MWE ?03, pages
73?80.
71
WSM Measure wAvg(of ?) ?AN-VO-SV ?AN ?VO ?SV ?NN
VSM1 SU1 0.28 0.03 0.01 0.51 0.04 0.62
VSM2 EN1 0.26 0.19 0.08 0.29 0.04 0.69
VSM3 CO1 0.32 0.26 0.24 0.23 0.25 0.65
VSM1 NE1 0.32 0.19 0.36 0.25 -0.13 0.73
LSA1 SU2 0.31 0.06 0.05 0.50 0.20 0.59
LSA2 EN2 0.50 0.40 0.39 0.55 0.32 0.78
LSA3 CO1 0.48 0.36 0.29 0.60 0.42 0.69
LSA2 NE2 0.44 0.33 0.34 0.40 0.44 0.67
HAL1 SU3 0.29 0.16 0.09 0.32 0.34 0.56
HAL2 EN3 0.36 0.28 0.33 0.35 0.26 0.53
HAL3 CO1 0.24 0.22 0.25 0.16 0.15 0.42
HAL4 NE3 0.21 0.14 0.02 0.33 0.06 0.47
COALS1 SU4 0.42 0.28 0.28 0.54 0.30 0.59
COALS2 EN2 0.49 0.44 0.52 0.51 0.07 0.72
COALS2 CO1 0.47 0.40 0.47 0.51 0.07 0.74
COALS2 NE4 0.52 0.48 0.55 0.50 0.21 0.74
RI1 SU5 0.30 0.14 0.14 0.29 0.12 0.72
RI2 EN3 0.44 0.34 0.37 0.54 0.20 0.63
RI3 CO1 0.23 0.23 0.29 0.17 0.17 0.26
RI2 NE5 0.31 0.26 0.26 0.42 0.04 0.44
Reddy-WSM - 0.35 - - - -
StatMix - 0.33 - - - -
Krcmar-COALS - 0.42 0.42 0.69 0.24 -
Krcmar-LSA - 0.50 0.50 0.56 0.41 -
Table 4: The Spearman correlations ? of the best performing (wAvg) combinations of particular WSMs
and Measures trained in TranValD applied to TestD. The highest correlation values in the particular
columns and the correlation values which are not statistically different from them (p < 0.05) are in bold
(yet we do not know how to calculate the stat. significance for the wAvg(of ?) column). Reddy-WSM and
StatMix stand for the best performing system based on WSMs and association measures, respectively,
applied to the DISCO task (Biemann and Giesbrecht, 2011). Krcmar-COALS and Krcmar-LSA stand for
the best published results achieved upon the dataset presented in Krc?ma?r? et al (2012) and Krc?ma?r? et al
(2013), respectively. The parameters of WSMs and Measures corresponding to the indexes are depicted
in Tables 5 and 6, respectively.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Preslav Nakov, Antonia Popova, and Plamen Ma-
teev. 2001. Weight functions impact on lsa per-
formance. In Proceedings of the EuroConference
Recent Advances in Natural Language Processing
(RANLP?01), pages 187?193.
Athanasios Papoulis. 1990. Probability & statistics.
Prentice Hall.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings
of the Third International Conference on Language
Resources and Evaluation, LREC.
Pavel Pecina. 2008. Reference data for Czech collo-
cation extraction. In Proceedings of the LREC 2008
Workshop Towards a Shared Task for Multiword Ex-
pressions, pages 11?14, Marrakech, Morocco. Euro-
pean Language Resources Association.
Siva Reddy, Diana McCarthy, and Suresh Manand-
har. 2011a. An empirical study on composition-
ality in compound nouns. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 210?218, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Siva Reddy, Diana McCarthy, Suresh Manandhar, and
Spandana Gella. 2011b. Exemplar-based word-
space model for compositionality detection: Shared
task system description. In Proceedings of the Work-
shop on Distributional Semantics and Composition-
ality, pages 54?60, Portland, Oregon, USA.
72
WSM parameters
VSM tags trans.
VSM1 noT noNo
VSM2 yesT noNo
VSM3 yesT noIdf
LSA tags trans. dim.
LSA1 noT logEnt 900
LSA2 yesT noNo 300
LSA3 noT noIdf 300
HAL tags win s. ret. c.
HAL1 noT 5 20000
HAL2 yesT 5 20000
HAL3 noT 2 10000
HAL4 yesT 5 all
COALS tags ret. c.
COALS1 noT 7000
COALS2 yesT 7000
RI tags win. s. vec. s. perm.
RI1 noT 2 4000 no
RI2 noT 4 4000 no
RI3 noT 2 4000 yes
Table 5: Parameters of WSMs (Section 2) which,
combined with particular Measures, achieved the
highest average correlation in TrValD.
Douglas L. Rohde, Laura M. Gonnerman, and David C.
Plaut. 2005. An improved model of semantic sim-
ilarity based on lexical co-occurrence. Unpublished
manuscript.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In V. Sloutsky, B. Love, and K. Mcrae,
editors, Proceedings of the 30th Annual Conference
of the Cognitive Science Society, pages 1300?1305.
Cognitive Science Society, Austin, TX.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Semantic
Indexing Workshop at the 7th International Confer-
ence on Terminology and Knowledge Engineering,
Leipzig, Germany.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Gerard Salton. 1971. The SMART Retrieval Sys-
tem; Experiments in Automatic Document Process-
ing. Prentice-Hall, Inc., Upper Saddle River, NJ,
USA.
Dawei Song, Peter Bruza, and Richard Cole. 2004.
Concept learning and information inferencing on a
Measure parameters
SU sim. ? W H M
SU1 cos plus log 30 3
SU2 cos plus log 100 5
SU3 cos mult log 12 2
SU4 cos mult log 80 4
SU5 cos mult log 4 3
EN sim. func. x
EN1 cos min sim
EN2 cos avg sim
EN3 cos min ?dist
CO sim. ?
CO1 cos ?
NE sim. O
NE1 cos 1000
NE2 cos 500
NE3 cos 50
NE4 cor 500
NE5 cos 20
Table 6: Parameters of Measures (Section 3)
which, combined with particular WSMs, achieved
the highest average correlation in TrValD.
highdimensional semantic space. In ACM SIGIR
2004 Workshop on Mathematical/Formal Methods
in Information Retrieval.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141?188.
73
