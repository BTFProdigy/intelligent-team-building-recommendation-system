Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 141?146,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Parsing TAG with Abstract Categorial Grammar
Sylvain Salvati
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
salvati@nii.ac.jp
Abstract
This paper presents informally an Earley
algorithm for TAG which behaves as the
algorithm given by (Schabes and Joshi,
1988). This algorithm is a specialization
to TAG of a more general algorithm ded-
icated to second order ACGs. As second
order ACGs allows to encode Linear Con-
text Free Rewriting Systems (LCFRS) (de
Groote and Pogodalla, 2004), the presen-
tation of this algorithm gives a rough pre-
sentation of the formal tools which can
be used to design efficient algorithms for
LCFRS. Furthermore, as these tools allow
to parse linear ?-terms, they can be used
as a basis for developping algorithms for
generation.
1 Introduction
The algorithm we present is a specialization to
TAGs of a more general one dedicated to second
order Abstract Categorial Grammars (ACGs) (de
Groote, 2001). Our aim is to give here an informal
presentation of tools that can be used to design ef-
ficient parsing algorithms for formalisms more ex-
pressive than TAG. Therefore, we only give a rep-
resentation of TAGs with linear ?-terms together
with simple derivation rules; we do not give in
complete details the technical relation with ACGs.
For some more information about ACGs and their
relation to TAGs, one may read (de Groote, 2001)
and (de Groote, 2002).
The advantage of using ACG is that they are
defined with very few primitives, but can encode
many formalisms. Thus they are well suited to
study from a general perspective a full class of for-
malisms. In particular, a special class of ACGs
(second order ACGs) embeds LCFRS (de Groote
and Pogodalla, 2004), i.e. mildly context sensi-
tive languages. Therefore, the study of second
order ACGs leads to insights on mildly context
sensitive languages. Having a general framework
to describe parsing algorithms for mildly context
sensitive languages may give some help to trans-
fer some interesting parsing technique from one
formalism to another. It can be, for example, a
good mean to obtain prefix-valid algorithms, LC
algorithms, LR algorithms. . . for the full class of
mildly context sensitive languages.
The class of languages described by second or-
der ACGs is wider than mildly context sensitive
languages. They can encode tree languages, and
more generally languages of linear ?-terms. As
Montague style semantics (Montague, 1974) is
based on ?-calculus, being able to parse linear ?-
term is a first step towards generation algorithms
seen as parsing algorithm. Furthermore, since this
parsing algorithm is a generalization of algorithms
a` la Earley for CFGs and TAGs, the more general
algorithm that can be used for generation (when
semantic formulae are linear) can be considered
as efficient.
The paper is organized as follows: section two
gives basic defintions and tools concerning the lin-
ear ?-calculus. Section three explains how the in-
dices usually used by parsers are represented for
the linear ?-calculus. Section four gives a rough
explaination of the encoding of TAGs within a
compiled representation of second order ACGs.
Section five explains the parsing algorithm and we
conclude with section six.
2 The linear ?-calculus
We begin by giving a brief definition of linear
types and linear ?-terms together with some stan-
141
dard notations. We assume that the reader is famil-
iar with the usual notions related to ?-calculus (?-
conversion, free variables, capture-avoiding sub-
stitutions. . . ); for more details about ?-calculus,
one may consult (Barendregt, 1984).
Definition 1 The set of linear types, T , is the
smallest set containing {?} and such that if ?, ? ?
T then (?( ?) ? T .
Given a type (?1 ( (? ? ? (?n ( ?) ? ? ? )), we
write it (?1, . . . , ?n)( ?.
Definition 2 Given a infinite enumerable set of
variables, X , and an alphabet ?, we define the
set of linear ?-terms of type ? ? T , ??, as the
smallest set satisfying the following properties:
1. x ? X ? x? ? ??
2. t ? ?? ? x? ? FV (t) ? ?x?.t ? ??(?
3. a ? ? ? a ? ??(?
4. t1 ? ??(??t2 ? ???FV (t1)?FV (t2) ?
(t1t2) ? ??
In general, we write ?x1 . . . xn.t for
?x1. . . . ?xn.t and we write t0t1 . . . tn for
(. . . (t0t1) . . . tn). Strings are represented by
closed linear ?-terms of type str = ? ( ?.
Given a string abcde, it is represented by the
following linear ?-term: ?y?.a(b(c(d(e y?))));
/w/ represents the set of terms which are
?-convertible to the ?-term representing the
string w. Concatenation is represented by
+ = ?xstr1 xstr2 y?.xstr1 (xstr2 y?), and (+w1)w2
will be written w1 + w2. The concatenation
is moreover associative, we may thus write
w1 + ? ? ? + wn.
For the description of our algorithm, we rely on
contexts:
Definition 3 A context is a ?-term with a hole.
Contexts are defined by the following grammar:
C = [] | ?C | C? | ?V.C
The insertion of a term within a context is done
the obvious way. One has nevertheless to remark
that when a term t is inserted in a context C[], the
context C[] can bind variables free in t. For exam-
ple, if C[] = ?x.[] and t = x then C[t] = ?x.x
and x which was free in t is not free anymore in
C[t].
3 Indices as syntactic descriptions
Usually the items of Earley algorithms use indices
to represent positions in the input string. The algo-
rithm we describe is a particular instance of a more
general one which parses linear ?-terms rather
than strings. In that case, one cannot describe in a
simple way positions by means of indices. Instead
of indices, positions in a term t will be represented
with zippers ((Huet, 1997)), i.e. a pair (C[], v) of
a context and a term such that C[v] = t. Figure 1
explicits the correspondence between indices and
zippers via an example.
The items of Earley algorithms for TAGs use
pairs of indices to describe portions of the input
string. In our algorithm, this role is played by lin-
ear types built upon zippers; the parsing process
can be seen as a type-checking process in a par-
ticular type system. We will not present this sys-
tem here, but we will give a flavor of the mean-
ing of those types called syntactic descriptions
(Salvati, 2006). In order to represent the portion
of a string between the indices i and j, we use
the zippers (Ci[], vi) and (Cj [], vj) which respec-
tively represent the position i and j in the string.
The portion of string is represented by the syntac-
tic description (Cj [], vj) ( (Ci[], vi); this syn-
tactic description can be used to type functions
which take vj as argument and return vi as a re-
sult. For example, given the syntactic description:
(?x.a(b(c[])), d(e x)) ( (?x.a[], b(c(d(e x)))),
it represents the set of functions that result in
terms that are ?-convertible to b(c(d(e x))) when
they take d(e x) as an argument; this set is
exactly /bc/. Our algorithm uses representa-
tions of string contexts with syntactic descrip-
tions such as d = ((C1[], v1) ( (C2[], v2)) (
(C3[], v3)( (C4[], v4) (in the following we write
((C1[], v1)( (C2[], v2), (C3[], v3))( (C4[], v4)
for such syntactic descriptions). Assume that
(C1[], v1) ( (C2[], v2) represents /bc/ and that
(C3[], v3) ( (C4[], v4) represents /abcde/, then
d describes the terms which give a result in
/abcde/ when they are applied to an element
of /bc/. Thus, d describes the set of terms ?-
convertible to ?fy.a(f(d(e y))), the set of terms
representing the string context a[ ]de.
Some of the syntactic descriptions we use may
contain variables denoting non-specified syntactic
descriptions that may be instanciated during pars-
ing. In particular, the syntactic description vari-
able F will always be used as a non-specified syn-
142
0 (?x.[], a(b(c(d(e x))))) 1 (?x.a[], b(c(d(e x))))
2 (?x.a(b[]), c(d(e x))) 3 (?x.a(b(c[])), d(e x))
4 (?x.a(b(c(d[]), e x) 5 (?x.a(b(c(d(e[])))), x)
?a?b?c?d?e?
Figure 1: Correspondence indices/zippers for the string abcde
tactic description representing strings (i.e. F may
only be substituted by a syntactic description of
the form (C1[], v1) ( (C2[], v2)), such syntac-
tic descriptions will represent the foot of an auxil-
iary tree. We will also use Y to represent a non-
specifed point in the input sentence (i.e. Y may
only be substituted by syntactic descriptions of
the form (C[], v)), such syntactic descriptions will
represent the end of an elementary tree.
As syntactic desccriptions are types for the lin-
ear ?-calculus, we introduce the notion of typing
context for syntactic descriptions.
Definition 4 A typing context ? (context for
short), is a set of pairs of the form x : d where x
is a variable and d is a syntactic description such
that x : d ? ? and x : e ? ? iff d = e.
If x : d ? ?, then we say that x is declared with
type d in ?.
Typing contexts ? must not be confused with
contexts C[]. If a typing context ? is the set
{x1 : d1; . . . ;xn : dn} then we will write if by
x1 : d1, . . . , xn : dn. In the present paper, typing
contexts may declare at most two variables.
4 Representing TAG with second order
ACGs
We cannot give here a detailed definition of second
order ACGs here. We therefore directly explain
how to transform TAGs into lexical entries repre-
senting a second order ACG that can be directly
used by the algorithm.
We represent a TAG G by a set of lexical en-
tries LG. Lexical entries are triples (?, t, ?) where
? is a typing context, t is a linear ?-term and ?
is either Na, Ns or Na.1 if N is a non-terminal
of the considered TAG. Without loss of general-
ity, we consider that the adjunction at an interior
node of an elementary tree is either mandatory
or forbidden1 . We adopt the convention of rep-
1We do not treat here the case of optional adjunction, but
our method can be straightforwardly extended to cope with
it, following ideas from (de Groote, 2002). It only modifies
the way we encode a TAG with a set of lexical entries, the
algorithm remains unchanged.
resenting adjunction nodes labeled with N by the
variable xstr(strNa , the substitution nodes labeled
with N ? by the variable xstrNs , the foot node of
an auxiliary tree labeled with N? by the variable
f strNa.1 and the variable y? will represent the end
of strings. When necessary, in order to respect
the linearity constraints of the ?-terms, indices are
used to distinguish those variables. This conven-
tion being settled, the type annotation on variables
is not necessary anymore, thus we will write xNa ,
xNs , fNa.1 and y. To translate the TAG, we use
the function ? defined by figure 2. Given an initial
tree T whose root is labeled by N and t the normal
form of ?(T ), ( , t,Ns)2 is the lexical entry asso-
ciated to T ; if T is an auxiliary tree whose root
is labeled by N and t is the normal form of ?(T )
then ( , ?fNa.1.t,Na)2 is the lexical entry associ-
ated to T . A TAG G is represented by LG the
smallest set verifying:
1. if T is an elementary tree of G then the lexi-
cal entry associated to T is in LG.
2. if ( , t, ?) ? LG, with ? equals to Na or Ns,
and t = C[xNat1t2] then (?, t1, Na.1) ? LG
where ? = fMa.1 : F if fMa.1 ? FV (t1)
otherwise ? is the empty typing context.
Given a term t such that x? ? FV (t), and
(?, t?, ?) ? LG, then we say that t is rewritten
as t[x? := t?], t ? t[x? := t?]. Furthermore if x?
is the leftmost variable we write t ?l t[x? := t?].
It is easy to check that if t ?? t? with FV (t?) = ?,
then t ??l t?. A string w is generated by a LG
whenever xSs
?? t and t ? /w/ (S being the start
symbol of G). Straightforwardly, the set of strings
generated by LG is exactly the language of G.
5 The algorithm
As we want to emphasize the fact that the algo-
rithm we propose borrows much to type checking,
we use sequents in the items the algorithm manip-
ulates. Sequents are objects of the form ? ` t : d
2In that case the typing context is empty.
143
??
?
?
?
N
T1 Tn. . .
?
?
?
?
?? ?y.xNa(?(T1) + ? ? ? + ?(Tn))y xNa and y are fresh
?
?
?
?
?
NNA
T1 Tn. . .
?
?
?
?
?? ?(T1) + ? ? ? + ?(Tn)
?(N?) ?? ?y.xNa(?y.fN.1y)y
?(N?NA) ?? ?y.fN.1y
?(N ?) ?? ?y.xNsy
?(a) ?? ?y.ay
?() ?? ?y.y
Figure 2: Translating TAG into ACG: definition of ?
where ? is a typing context, t is a linear ?-term,
and d is a syntactic description.
The algorithm uses two kinds of items; either
items of the form (?; ? ` t : d;L) (where L is
a list of sequents, the subgoals, here L contains
either zero or one element) or items of the form
[Na.1; ?; t; (C1[], v1) ( (C2[], v2)]. All the pos-
sible instances of the items are given by figure 3.
The algorithm is a recognizer but can easily be ex-
tended into a parser3. It fills iteratively a chart until
a fixed-point is reached. Elements are added to the
chart by means of inference rules given by figure
4, in a deductive parsing fashion (Shieber et al,
1995). Inference rules contain two parts: the first
part is a set of premises which state conditions on
elements that are already in the chart. The second
part gives the new element to add to the chart if
it is not already present. For the more general al-
gorithm, the rules are not much more numerous as
they can be abstracted into more general schemes.
An item of the form (?; ?1 ` t1 : d; ?2 ` t2 :
(C1[], v1)) verifies:
1. (??1, t1, ?) ? LG where ??1 = fNa.1 : F if
?1 = fNa.1 : e or ??1 = ?1 otherwise.
2. there is a context C[] such that t1 = C[t2] and
if d is of the form (d1, . . . ,dn)( (C2[], v2)
(n must be 1, or 2) then C[y] ??l t? so that t?
is described by (C1[], v1)( (C2[], v2).
3. if ?1 = fNa.1 : (C3[], v3) ( (C4[], v4)
or if d = ((C3[], v3) ( (C4[], v4), Y ) (
3Actually, if it is extended into a parser, it will ouput the
shared forest of the derivation trees; (de Groote, 2002) ex-
plains how to obtain the derived trees from the derivation
trees in the framework of ACGs
(C2[], v2) and t1 = ?fNa.1y.v then fNa.1 ?l
t?? and t?? is described by (C3[], v3) (
(C4[], v4)
An item of the form (?; ? ` t : d; ) verifies:
1. (??, t, ?) ? LG where ?? = fNa.1 : F if
? = fNa.1 : e or ?? = ? otherwise
2. d does not contain non-specified syntactic
descriptions4 .
3. t ??l t? and t? is described by d (d may either
represent a string context or a string).
4. if ? = fNa.1 : (C3[], v3) ( (C4[], v4) or if
d = ((C3[], v3) ( (C4[], v4), (C1[], v1)) (
(C2[], v2) and t1 = ?fNa.1y.t? then fMa.1
??l
t?? and t?? is described by (C3[], v3) (
(C4[], v4)
Finally an item of the form
[Na.1; ?; t; (C1[], v1) ( (C2[], v2)] im-
plies the existence of t?, (C3[], v3) and
(C4[], v4) such that (Na;` t? : ((C3[], v3) (
(C4[], v4), (C1[], v1)) ( (C2[], v2); ) and
(Na.1; ? ` t : (C3[], v3) ( (C4[], v4)); ) are in
the chart.
An input ?y.C[y] is recognized iff when the
fixed-point is reached, the chart contains an item
of the form (Ss; ` t : (?y.C[], y) (
(?y.[], C[y]); ) (where S is the start symbol of the
TAG G.
4There is no occurence of F or Y in d.
144
General items
(Na ; ` ?fNa.1y.t1 : (F, Y )( (C1[], v1) ; fNa.1 : F, y : Y ` t2 : (C2[], v2))
(Na ; ` ?fNa.1y.t : ((C1[], v1)( (C2[], v2), Y )( (C3[], v3) ; y : Y ` t2 : (C4[], v4))
(Na ; ` ?fNa.1y.t : ((C1[], v1)( (C2[], v2), (C3[], v3))( (C4[], v4) ; )
(? ; ` ?y.t1 : Y ( (C1[], v1) ; y : Y ` t2 : (C2[], v2))
(? ; ` ?y.t : (C1[], v1)( (C2[], v2) ; )
(Na.1 ; fMa.1 : F ` ?y.t : Y ( (C[], v) ; fMa.1 : F, y : Y ` t2 : (C2[], v2)
(Na.1 ; fMa.1 : (C1[], v1)( (C2[], v2) ` ?y.t : Y ( (C3[], v3) ; y : Y ` t2 : (C4[], v4))
(Na.1 ; fMa.1 : (C1[], v1)( (C2[], v2) ` ?y.t : (C3[], v3)( (C4[], v4) ; )
Wrapped subtrees
[Na.1 ; ; t ; (C1[], v1)( (C2[], v2)]
[Na.1 ; fMa.1 : (C1[], v1)( (C2[], v2) ; t ; (C3[], v3)( (C4[], v4)]
Figure 3: Possible items
6 Conclusion and perspective
In this paper, we have illustrated the use for TAGs
of general and abstract tools, syntactic descrip-
tions, which can be used to parse linear ?-terms.
Even though ACGs are very general in their def-
inition, the algorithm we describe shows that this
generality is not a source of unefficiency. Indeed,
this algorithm, a special instance of a general one
which can parse any second order ACG and it be-
haves exactly the same way as the algorithm given
by (Schabes and Joshi, 1988) so that it parses a
second order ACG encoding a TAG in O(n6).
The technique used enables to see generation as
parsing. In the framework of second order ACG,
the logical formulae on which generation is per-
formed are bound to be obtained from semantic re-
cipies coded with linear ?-terms and are therefore
not really adapted to Montague semantics. Nev-
ertheless, syntactic descriptions can be extended
with intersection types (Dezani-Ciancaglini et al,
2005) in order to cope with simply typed ?-
calculus. With this extension, it seems possible
to extend the algorithm for second order ACGs
so that it can deal with simply typed ?-terms and
without loosing its efficiency in the linear case.
References
Henk P. Barendregt. 1984. The Lambda Calculus: Its
Syntax and Semantics, volume 103. Studies in Logic
and the Foundations of Mathematics, North-Holland
Amsterdam. revised edition.
Philippe de Groote and Sylvain Pogodalla. 2004. On
the expressive power of abstract categorial gram-
mars: Representing context-free formalisms. Jour-
nal of Logic, Language and Information, 13(4):421?
438.
Philippe de Groote. 2001. Towards abstract categorial
grammars. In Association for Computational Lin-
guistic, editor, Proceedings 39th Annual Meeting
and 10th Conference of the European Chapter,
pages 148?155. Morgan Kaufmann Publishers.
Philippe de Groote. 2002. Tree-adjoining grammars
as abstract categorial grammars. TAG+6, Proceed-
ings of the sixth International Workshop on Tree Ad-
joining Grammars and Related Frameworks, pages
145?150.
Mariangiola Dezani-Ciancaglini, Furio Honsell, and
Yoko Motohama. 2005. Compositional Characteri-
zation of ?-terms using Intersection Types. Theoret.
Comput. Sci., 340(3):459?495.
Ge?rard Huet. 1997. The zipper. Journal of Functional
Programming, 7(5):549?554.
Richard Montague. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New Haven, CT.
Sylvain Salvati. 2006. Syntactic descriptions: a type
system for solving matching equations in the linear
?-calculus. In to be published in the proceedings
of the 17th International Conference on Rewriting
Techniques and Applications.
Yves Schabes and Aravind K. Joshi. 1988. An earley-
type parsing algorithm for tree adjoining grammars.
In Proceedings of the 26th annual meeting on Asso-
ciation for Computational Linguistics, pages 258?
269, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1?2):3?36, July?August. Also available as cmp-
lg/9404008.
145
The initializer
(?y.t, Ss) ? LG
(Ss; ` ?y.t : Y ( (?y.[], u); y : Y ` t : (?y.[], u))
The scanner
(?; ?1 ` t1 : d; ?2 ` at2 : (C[], av))
(?; ?1 ` t1 : d; ?2 ` t2 : (C[a[]], v))
(?; ? ` t : d; y : Y ` y : (C[], v)) ? = [Y := (C[], v)]
(?; ? ` t : d.?; )
The predictor
(?; ?1 ` t1 : d; ?2 ` xNat2t3 : (C[], v)) ( , ?fNa.1y.t,Na) ? LG
(Na; ` ?fNa.1y.t : (F, Y )( (C[], v); fNa.1 : F, y : Y ` t : (C[], v))
(?; ?1 ` t1 : d; ?2 ` xNst2 : (C[], v)) ( , ?y.t,Ns) ? LG
(Ns; ` ?y.t : Y ( (C[], v); y : Y ` t : (C[], v))
(?; ?1 ` t1 : d; ?2 ` fNa.1t2 : (C2[], v2))
(?3, ?y.t3, Na.1) ? LG
(Na.1; ?3 ` ?y.t3 : Y ( (C2[], v2); ?3, y : Y ` t3 : (C2[], v2))
The completer
(Na; ` t1 : ((C1[], v1)( (C2[], v2), (C3[], v3))( (C4[], v4); )
(Na.1; ?2; t2 : (C1[], v1)( (C2[], v2); )
[Na.1; ?2; t2; (C3[], v3)( (C4[], v4)]
(?; ?1 ` t1 : d; y : Y,??2 ` xNat2t3 : (C1[], v1))
[Na.1; ?2; t2; (C2[], v2)( (C1[], v1)]
if ?2 = fMa.1 : f then ? = [F := f ] else ? = Id
(?; ?1.? ` t1 : d.?; ?2 ` t3 : (C2[], v2))
(?; ?1 ` t1 : d; fNa.1 : F, y : Y ` fNa.1t2 : (C1[], v1))
(Na.1; ?2 ` t2 : (C2[], v2)( (C1[], v1); )
? = [F := (C2[], v2)( (C1[], v1)]
(?; ?1.? ` t1 : d.?; y : Y ` t2 : (C2[], v2))
(?; ?1 ` t1 : d; ?2 ` xNst2 : (C1[], v1))
(Ns; ` t2 : (C2[], v2)( (C1[], v1); )
(?; ?1 ` t1 : d; ?2 ` t2 : (C2[], v2))
Figure 4: The rules of the algorithm
146
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 666?674,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
MIX Is Not a Tree-Adjoining Language
Makoto Kanazawa
National Institute of Informatics
2?1?2 Hitotsubashi, Chiyoda-ku
Tokyo, 101?8430, Japan
kanazawa@nii.ac.jp
Sylvain Salvati
INRIA Bordeaux Sud-Ouest, LaBRI
351, Cours de la Libe?ration
F-33405 Talence Cedex, France
sylvain.salvati@labri.fr
Abstract
The language MIX consists of all strings over
the three-letter alphabet {a, b, c} that contain
an equal number of occurrences of each letter.
We prove Joshi?s (1985) conjecture that MIX
is not a tree-adjoining language.
1 Introduction
The language
MIX = {w ? {a, b, c}? | |w|a = |w|b = |w|c }
has attracted considerable attention in computational
linguistics.1 This language was used by Bach (1981)
in an exercise to show that the permutation closure
of a context-free language is not necessarily context-
free.2 MIX may be considered a prototypical exam-
ple of free word order language, but, as remarked by
Bach (1981), it seems that no human language ?has
such complete freedom for order?, because ?typi-
cally, certain constituents act as ?boundary domains?
for scrambling?. Joshi (1985) refers to MIX as rep-
resenting ?an extreme case of the degree of free
word order permitted in a language?, which is ?lin-
guistically not relevant?. Gazdar (1988) adopts a
similar position regarding the relation between MIX
1If w is a string and d is a symbol, we write |w|d to mean the
number of occurrences of d in w. We will use the notation |w| to
denote the length of w, i.e., the total number of occurrences of
symbols in w.
2According to Gazdar (1988), ?MIX was originally de-
scribed by Emmon Bach and was so-dubbed by students in
the 1983 Hampshire College Summer Studies in Mathematics?.
According to Bach (1988), the name MIX was ?the happy in-
vention of Bill Marsh?.
and natural languages, noting that ?it seems rather
unlikely that any natural language will turn out to
have a MIX-like characteristic?.
It therefore seems natural to assume that lan-
guages such as MIX should be excluded from any
class of formal languages that purports to be a tight
formal characterization of the possible natural lan-
guages. It was in this spirit that Joshi et al (1991)
suggested that MIX should not be in the class of so-
called mildly context-sensitive languages:
?[mildly context-sensitive grammars] cap-
ture only certain kinds of dependencies,
e.g., nested dependencies and certain lim-
ited kinds of cross-serial dependencies
(for example, in the subordinate clause
constructions in Dutch or some variations
of them, but perhaps not in the so-called
MIX (or Bach) language) . . . .?
Mild context-sensitivity is an informally defined no-
tion first introduced by Joshi (1985); it consists of
the three conditions of limited cross-serial depen-
dencies, constant growth, and polynomial parsing.
The first condition is only vaguely formulated, but
the other two conditions are clearly satisfied by tree-
adjoining grammars. The suggestion of Joshi et al
(1991) was that MIX should be regarded as a vio-
lation of the condition of limited cross-serial depen-
dencies.
Joshi (1985) conjectured rather strongly that MIX
is not a tree-adjoining language: ?TAGs cannot gen-
erate this language, although for TAGs the proof is
not in hand yet?. An even stronger conjecture was
made by Marsh (1985), namely, that MIX is not an
666
indexed language.3 (It is known that the indexed
languages properly include the tree-adjoining lan-
guages.) Joshi et al (1991), however, expressed a
more pessimistic view about the conjecture:
?It is not known whether TAG . . . can
generate MIX. This has turned out to be
a very difficult problem. In fact, it is
not even known whether an IG [(indexed
grammar)] can generate MIX.?
This open question has become all the more press-
ing after a recent result by Salvati (2011). This re-
sult says that MIX is in the class of multiple context-
free languages (Seki et al, 1991), or equivalently,
languages of linear context-free rewriting systems
(Vijay-Shanker et al, 1987; Weir, 1988), which has
been customarily regarded as a formal counterpart
of the informal notion of a mildly context-sensitive
language.4 It means that either we have to aban-
don the identification of multiple context-free lan-
guages with mildly context-sensitive languages, or
we should revise our conception of limited cross-
serial dependencies and stop regarding MIX-like
languages as violations of this condition. Surely, the
resolution of Joshi?s (1985) conjecture should cru-
cially affect the choice between these two alterna-
tives.
In this paper, we prove that MIX is not a tree-
adjoining language. Our proof is cast in terms of the
formalism of head grammar (Pollard, 1984; Roach,
1987), which is known to be equivalent to TAG
(Vijay-Shanker and Weir, 1994). The key to our
proof is the notion of an n-decomposition of a string
over {a, b, c}, which is similar to the notion of a
derivation in head grammars, but independent of any
particular grammar. The parameter n indicates how
unbalanced the occurrence counts of the three let-
ters can be at any point in a decomposition. We first
3The relation of MIX with indexed languages is also of in-
terest in combinatorial group theory. Gilman (2005) remarks
that ?it does not . . . seem to be known whether or not the
word problem of Z ? Z is indexed?, alluding to the language
O2 = {w ? {a, a?, b, b?}? | |w|a = |w|a?, |w|b = |w|b? }. Since O2 and
MIX are rationally equivalent, O2 is indexed if and only if MIX
is indexed (Salvati, 2011).
4Joshi et al (1991) presented linear context-free rewriting
systems as mildly context-sensitive grammars. Groenink (1997)
wrote ?The class of mildly context-sensitive languages seems to
be most adequately approached by LCFRS.?
show that if MIX is generated by some head gram-
mar, then there is an n such that every string in MIX
has an n-decomposition. We then prove that if every
string in MIX has an n-decomposition, then every
string in MIX must have a 2-decomposition. Finally,
we exhibit a particular string in MIX that has no 2-
decomposition. The length of this string is 87, and
the fact that it has no 2-decomposition was first ver-
ified by a computer program accompanying this pa-
per. We include here a rigorous, mathematical proof
of this fact not relying on the computer verification.
2 Head Grammars
A head grammar is a quadruple G = (N,?, P, S),
where N is a finite set of nonterminals, ? is a fi-
nite set of terminal symbols (alphabet), S is a distin-
guished element of N, and P is a finite set of rules.
Each nonterminal is interpreted as a binary predicate
on strings in ??. There are four types of rules:
A(x1x2y1, y2)? B(x1, x2),C(y1, y2)
A(x1, x2y1y2)? B(x1, x2),C(y1, y2)
A(x1y1, y2x2)? B(x1, x2),C(y1, y2)
A(w1,w2)?
Here, A, B,C ? N, x1, x2, y1, y2 are variables, and
w1,w2 ? ? ? {?}.5 Rules of the first three types are
binary rules and rules of the last type are terminat-
ing rules. This definition of a head grammar actu-
ally corresponds to a normal form for head gram-
mars that appears in section 3.3 of Vijay-Shanker
and Weir?s (1994) paper.6
The rules of head grammars are interpreted as im-
plications from right to left, where variables can be
instantiated to any terminal strings. Each binary
5We use ? to denote the empty string.
6This normal form is also mentioned in chapter 5, section 4
of Kracht?s (2003) book. The notation we use to express rules
of head grammars is borrowed from elementary formal sys-
tems (Smullyan, 1961; Arikawa et al, 1992), also known as
literal movement grammars (Groenink, 1997; Kracht, 2003),
which are logic programs over strings. In Vijay-Shanker and
Weir?s (1994) notation, the four rules are expressed as follows:
A? C2,2(B,C)
A? C1,2(B,C)
A? W(B,C)
A? C1,1(w1 ? w2)
667
rule involves an operation that combines two pairs
of strings to form a new pair. The operation in-
volved in the third rule is known as wrapping; the
operations involved in the first two rules we call left
concatenation and right concatenation, respectively.
If G = (N,?, P, S) is a head grammar, A ? N, and
w1,w2 ? ??, then we say that a fact A(w1,w2) is
derivable and write `G A(w1,w2), if A(w1,w2) can
be inferred using the rules in P. More formally, we
have `G A(w1,w2) if one of the following conditions
holds:
? A(w1,w2)? is a terminating rule in P.
? `G B(u1, u2), `G C(v1, v2), and there is a bi-
nary rule A(?1, ?2) ? B(x1, x2),C(y1, y2) in
P such that (w1,w2) is the result of substitut-
ing u1, u2, v1, v2 for x1, x2, y1, y2, respectively,
in (?1, ?2).
The language of G is
L(G) = {w1w2 | `G S(w1,w2) }.
Example 1. Let G = (N,?, P, S), where N =
{S, A, A?,C,D, E, F}, ? = {a, a?, #}, and P consists of
the following rules:
S(x1y1, y2x2)? D(x1, x2),C(y1, y2)
C(?, #)?
D(?, ?)?
D(x1y1, y2x2)? F(x1, x2),D(y1, y2)
F(x1y1, y2x2)? A(x1, x2), E(y1, y2)
A(a, a) ?
E(x1y1, y2x2)? D(x1, x2), A?(y1, y2)
A?(a?, a?)?
We have L(G) = {w#wR | w ? D
{a,a?} }, where D{a,a?}
is the Dyck language over {a, a?} and wR is the re-
versal of w. All binary rules of this grammar are
wrapping rules.
If `G A(w1,w2), a derivation tree for A(w1,w2) is
a finite binary tree whose nodes are labeled by facts
that are derived during the derivation of A(w1,w2).
A derivation tree for A(w1,w2) represents a ?proof?
of `G A(w1,w2), and is formally defined as follows:
? If A(w1,w2)? is a terminating rule, then a tree
with a single node labeled by A(w1,w2) is a
derivation tree for A(w1,w2).
S(aaa?a?aa?, #a?aa?a?aa)
D(aaa?a?aa?, a?aa?a?aa)
F(aaa?a?, a?a?aa)
A(a, a) E(aa?a?, a?a?a)
D(aa?, a?a)
F(aa?, a?a)
A(a, a) E(a?, a?)
D(?, ?) A?(a?, a?)
D(?, ?)
A?(a?, a?)
D(aa?, a?a)
F(aa?, a?a)
A(a, a) E(a?, a?)
D(?, ?) A?(a?, a?)
D(?, ?)
C(?, #)
Figure 1: An example of a derivation tree of a head gram-
mar.
? If `G A(w1,w2) is derived from `G B(u1, u2)
and `G C(v1, v2) by some binary rule, then a
binary tree whose root is labeled by A(w1,w2)
and whose immediate left (right) subtree is a
derivation tree for B(u1, u2) (for C(v1, v2), re-
spectively) is a derivation tree for A(w1,w2).
If w ? L(G), a derivation tree for w is a derivation
tree for some S(w1,w2) such that w1w2 = w.
Example 1 (continued). Figure 1 shows a derivation
tree for aaa?a?aa?#a?aa?a?aa.
The following lemma should be intuitively clear
from the definition of a derivation tree:
Lemma 1. Let G = (N,?, P, S) be a head grammar
and A be a nonterminal in N. Suppose that w ?
L(G) has a derivation tree in which a fact A(v1, v2)
appears as a label of a node. Then there are strings
z0, z1, z2 with the following properties:
(i) w = z0v1z1v2z2, and
(ii) `G A(u1, u2) implies z0u1z1u2z2 ? L(G).
Proof. We can prove by straightforward induction
on the height of derivation trees that whenever
A(v1, v2) appears on a node in a derivation tree for
B(w1,w2), then there exist z0, z1, z2, z3 that satisfy
one of the following conditions:
(a) w1 = z0v1z1v2z2, w2 = z3, and `G A(u1, u2)
implies `G B(z0u1z1u2z2, z3).
(b) w1 = z0, w2 = z1v1z2v2z3, and `G A(u1, u2)
implies `G B(z0, z1u1z2u2z3).
668
(c) w1 = z0v1z1, w2 = z2v2z3, and `G A(u1, u2)
implies `G B(z0u1z1, z2u2z3).
We omit the details. 
We call a nonterminal A of a head grammarG use-
less if A does not appear in any derivation trees for
strings in L(G). Clearly, useless nonterminals can be
eliminated from any head grammar without affecting
the language of the grammar.
3 Decompositions of Strings in MIX
Henceforth, ? = {a, b, c}. Let Z denote the set of in-
tegers. Define functions ?1, ?2 : ?? ? Z, ? : ?? ?
Z ? Z by
?1(w) = |w|a ? |w|c,
?2(w) = |w|b ? |w|c,
?(w) = (?1(w), ?2(w)).
Clearly, we have ?(a) = (1, 0), ?(b) = (0, 1), ?(c) =
(?1,?1), and
w ? MIX iff ?(w) = (0, 0).
Note that for all strings w1,w2 ? ??, ?(w1w2) =
?(w1)+?(w2). In other words, ? is a homomorphism
from the free monoid ?? to Z ? Z with addition as
the monoid operation and (0, 0) as identity.
Lemma 2. Suppose that G = (N,?, P, S) is a head
grammar without useless nonterminals such that
L(G) ? MIX. There exists a function ?G : N ? Z ?
Z such that `G A(u1, u2) implies ?(u1u2) = ?G(A).
Proof. Since G has no useless nonterminals, for
each nonterminal A of G, there is a derivation tree
for some string in L(G) in which A appears in a node
label. By Lemma 1, there are strings z0, z1, z2 such
that `G A(u1, u2) implies z0u1z1u2z2 ? L(G). Since
L(G) ? MIX, we have ?(z0u1z1u2z2) = (0, 0), and
hence
?(u1u2) = ??(z0z1z2). 
A decomposition of w ? ?? is a finite binary tree
satisfying the following conditions:
? the root is labeled by some (w1,w2) such that
w = w1w2,
? each internal node whose left and right children
are labeled by (u1, u2) and (v1, v2), respectively,
is labeled by one of (u1u2v1, v2), (u1, u2v1v2),
(u1v1, v2u2).
? each leaf node is labeled by some (s1, s2) such
that s1s2 ? {b, c}? ? {a, c}? ? {a, b}?.
Thus, the label of an internal node in a decomposi-
tion is obtained from the labels of its children by left
concatenation, right concatenation, or wrapping. It
is easy to see that ifG is a head grammar over the al-
phabet ?, any derivation for w ? L(G) induces a de-
composition ofw. (Just strip off nonterminals.) Note
that unlike with derivation trees, we have placed no
bound on the length of a string that may appear on
a leaf node of a decomposition. This will be conve-
nient in some of the proofs below.
When p and q are integers, we write [p, q] for the
set { r ? Z | p ? r ? q }. We call a decomposition of
w an n-decomposition if each of its nodes is labeled
by some (v1, v2) such that ?(v1v2) ? [?n, n]?[?n, n].
Lemma 3. If MIX = L(G) for some head grammar
G = (?,N, P, S), then there exists an n such that each
w ? MIX has an n-decomposition.
Proof. We may suppose without loss of generality
that G has no useless nonterminal. Since MIX =
L(G), there is a function ?G satisfying the condition
of Lemma 2. Since the set N of nonterminals of G
is finite, there is an n such that ?G(A) ? [?n, n] ?
[?n, n] for all A ? N. Then it is clear that a derivation
tree for w ? L(G) induces an n-decomposition of
w. 
If w = d1 . . . dm ? ?m, then for 0 ? i ? j ? m,
we write w[i, j] to refer to the substring di+1 . . . dj
of w. (As a special case, we have w[i, i] = ?.) The
following is a key lemma in our proof:
Lemma 4. If each w ? MIX has an n-
decomposition, then each w ? MIX has a 2-
decomposition.
Proof. Assume that each w ? MIX has an n-
decomposition. Define a homomorphism ?n : ?? ?
?
? by
?n(a) = an,
?n(b) = b
n
,
?n(c) = c
n
.
669
Clearly, ?n is an injection, and we have ?(?n(v)) =
n ? ?(v) for all v ? ??.
Let w ? MIX with |w| = m. Then w? = ?n(w) ?
MIX and |w?| = mn. By assumption, w? has an n-
decomposition D. We assign a 4-tuple (i, j, k, l) of
natural numbers to each node of D in such a way
that (w?[i, j],w?[k, l]) equals the label of the node.
This is done recursively in an obvious way, start-
ing from the root. If the root is labeled by (w1,w2),
then it is assigned (0, |w1|, |w1|, |w1w2|). If a node is
assigned a tuple (i, j, k, l) and has two children la-
beled by (u1, u2) and (v1, v2), respectively, then the
4-tuples assigned to the children are determined ac-
cording to how (u1, u2) and (v1, v2) are combined at
the parent node:
u1 u2 v1 v2
i j k l
i + |u1| i + |u1u2|
u1 u2 v1 v2
i j k l
k + |u2| k + |u2v1|
u1 v1 v2 u2
i j k l
i + |u1| k + |v2|
Now define a function f : [0,mn] ? { kn | 0 ?
k ? m } by
f (i) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
i if n divides i,
n ? bi/nc if n does not divide i and
w?[i ? 1, i] ? {a, b},
n ? di/ne if n does not divide i and
w?[i ? 1, i] = c.
Clearly, f is weakly increasing in the sense that i ? j
implies f (i) ? f ( j). LetD? be the result of replacing
the label of each node inD by
(w?[ f (i), f ( j)],w?[ f (k), f (l)]),
where (i, j, k, l) is the 4-tuple of natural numbers as-
signed to that node by the above procedure. It is easy
to see that D? is another decomposition of w?. Note
that since each of f (i), f ( j), f (k), f (l) is an integral
multiple of n, we always have
(w?[ f (i), f ( j)],w?[ f (k), f (l)]) = (?n(u), ?n(v))
for some substrings u, v of w. This implies that for
h = 1, 2,
?h(w?[ f (i), f ( j)]w?[ f (k), f (l)])
is an integral multiple of n.
Claim. D? is a 2n-decomposition.
We have to show that every node label (v1, v2) in D?
satisfies ?(v1v2) ? [?2n, 2n] ? [?2n, 2n]. For h =
1, 2, define ?h : [0,mn] ? [0,mn]? Z as follows:
?h(i, j) =
?
?
?
?
?
?
?
?h(w?[i, j]) if i ? j,
??h(w?[ j, i]) otherwise.
Then it is easy to see that for all i, j, i?, j? ? [0,mn],
?h(i
?
, j?) = ?h(i
?
, i) + ?h(i, j) + ?h( j, j
?).
Inspecting the definition of the function f , we can
check that
?h( f (i), i) ? [0, n ? 1]
always holds. Suppose that (i, j, k, l) is assigned
to a node in D. By assumption, we have
?h(w?[i, j]w?[k, l]) ? [?n, n], and
?h(w
?[ f (i), f ( j)]w?[ f (k), f (l)])
= ?h(w
?[ f (i), f ( j)]) + ?h(w
?[ f (k), f (l)])
= ?h( f (i), f ( j)) + ?h( f (k), f (l))
= ?h( f (i), i) + ?h(i, j) + ?h( j, f ( j))
+ ?h( f (k), k) + ?h(k, l) + ?h(l, f (l))
= ?h( f (i), i) + ?h(w?[i, j]) + ?h( j, f ( j))
+ ?h( f (k), k) + ?h(w
?[k, l]) + ?h(l, f (l))
= ?h(w
?[i, j]w?[k, l]) + ?h( f (i), i) + ?h( f (k), k)
+ ?h( j, f ( j)) + ?h(l, f (l))
? { p + q1 + q2 + r1 + r2 | p ? [?n, n],
q1, q2 ? [0, n ? 1], r1, r2 ? [?n + 1, 0] }
= [?3n + 2, 3n ? 2].
Since ?h(w?[ f (i), f ( j)]w?[ f (k), f (l)]) must be an in-
tegral multiple of n, it follows that
?h(w
?[ f (i), f ( j)]w?[ f (k), f (l)]) ? {?2n,?n, 0, n, 2n}.
This establishes the claim.
670
We have shown that each node ofD? is labeled by
a pair of strings of the form (?n(u), ?n(v)) such that
?(?n(u)?n(v)) ?
{?2n,?n, 0, n, 2n} ? {?2n,?n, 0, n, 2n}.
Now it is easy to see that inverting the homomor-
phism ?n at each node of D?
(?n(u), ?n(v)) 7? (u, v)
gives a 2-decomposition of w. 
4 A String in MIX That Has No
2-Decomposition
By Lemmas 3 and 4, in order to prove that there is no
head grammar for MIX, it suffices to exhibit a string
in MIX that has no 2-decomposition. The following
is such a string:
z = a5b14a19c29b15a5.
In this section, we prove that the string z has no 2-
decomposition.7
It helps to visualize strings in MIX as closed
curves in a plane. If w is a string in MIX, by plotting
the coordinates of ?(v) for each prefix v of w, we can
represent w by a closed curve C together with a map
t : [0, |w|] ? C. The representation of the string z is
given in Figure 2.
Let us call a string w ? {a, b, c}? such that ?(w) ?
[?2, 2] ? [?2, 2] long if w contains all three letters,
and short otherwise. (If ?(w) < [?2, 2] ? [?2, 2],
then w is neither short nor long.) It is easy to see
that a short string w always satisfies
|w|a ? 4, |w|b ? 4, |w|c ? 2.
The maximal length of a short string is 6. (For ex-
ample, a4c2 and b4c2 are short strings of length 6.)
We also call a pair of strings (v1, v2) long (or short)
if v1v2 is long (or short, respectively).
According to the definition of an n-
decomposition, a leaf node in a 2-decomposition
7This fact was first verified by the computer program ac-
companying this paper. The program, written in C, imple-
ments a generic, memoized top-down recognizer for the lan-
guage {w ? MIX | w has a 2-decomposition }, and does not rely
on any special properties of the string z.
0 5
19 38
67
82
87 a5
b14
a19
c29
b15
a5
Figure 2: Graphical representation of the string z =
a5b14a19c29b15a5. Note that every point (i, j) on the di-
agonal segment has i > 7 or j < ?2.
must be labeled by a short pair of strings. We call
a 2-decomposition normal if the label of every
internal node is long. Clearly, any 2-decomposition
can be turned into a normal 2-decomposition by
deleting all nodes that are descendants of nodes
with short labels.
One important property of the string z is the fol-
lowing:
Lemma 5. If z = x1vx2 and ?(v) ? [?2, 2]? [?2, 2],
then either v or x1x2 is short.
Proof. This is easy to see from the graphical rep-
resentation in Figure 2. If a substring v of z has
?(v) ? [?2, 2] ? [?2, 2], then the subcurve corre-
sponding to v must have initial and final coordi-
nates whose difference lies in [?2, 2] ? [?2, 2]. If
v contains all three letters, then it must contain as
a substring at least one of ba19c, ac29b, and cb15a.
The only way to satisfy both these conditions is to
have the subcurve corresponding to v start and end
very close to the origin, so that x1x2 is short. (Note
that the distance between the coordinate (5, 0) corre-
sponding to position 5 of z and the diagonal segment
corresponding to the substring c29 is large enough
that it is impossible for v to start at position 5 and
end in the middle of c29 without violating the condi-
tion ?(v) ? [?2, 2] ? [?2, 2].) 
Lemma 5 leads to the following observation. Let
us call a decomposition of a string concatenation-
free if each of its non-leaf labels is the wrapping of
the labels of the children.
671
Lemma 6. If z has a 2-decomposition, then z has a
normal, concatenation-free 2-decomposition.
Proof. Let D be a 2-decomposition of z. Without
loss of generality, we may assume that D is nor-
mal. Suppose that D contains a node ? whose la-
bel is the left or right concatenation of the labels
of its children, (u1, u2) and (v1, v2). We only con-
sider the case of left concatenation since the case
of right concatenation is entirely analogous; so we
suppose that the node ? is labeled by (u1u2v1, v2).
It follows that z = x1u1u2x2 for some x1, x2, and
by Lemma 5, either u1u2 or x1x2 is short. If u1u2
is short, then the left child of ? is a leaf because
D is normal. We can replace its label by (u1u2, ?);
the label (u1u2v1, v2) of ? will now be the wrapping
(as well as left concatenation) of the two child la-
bels, (u1u2, ?) and (v1, v2). If x1x2 is short, then we
can combine by wrapping a single node labeled by
(x1, x2) with the subtree ofD rooted at the left child
of ?, to obtain a new 2-decomposition of z. In ei-
ther case, the result is a normal 2-decomposition of
z with fewer instances of concatenation. Repeat-
ing this procedure, we eventually obtain a normal,
concatenation-free 2-decomposition of z. 
Another useful property of the string z is the fol-
lowing:
Lemma 7. Suppose that the following conditions
hold:
(i) z = x1u1v1yv2u2x2,
(ii) x1yx2 is a short string, and
(iii) both ?(u1u2) and ?(v1v2) are in [?2, 2] ?
[?2, 2].
Then either (u1, u2) or (v1, v2) is short.
Proof. Suppose (u1, u2) and (v1, v2) are both long.
Since (u1, u2) and (v1, v2) must both contain c, either
u1 ends in c and v1 starts in c, or else v2 ends in c
and u2 starts in c.
Case 1. u1 ends in c and v1 starts in c. Since
(v1, v2) must contain at least one occurrence of a,
the string v1yv2 must contain cb15a as a substring.
a5b14 a19 c29 b15 a5
v1yv2
Since x1yx2 is short, we have |y|b ? 4. It follows that
|v1v2|b ? 11. But v1yv2 is a substring of c28b15a5,
so |v1v2|a ? 5. This clearly contradicts ?(v1v2) ?
[?2, 2] ? [?2, 2].
Case 2. v2 ends in c and u2 starts in c. In this
case, cb15a5 is a suffix of u2x2. Since x1yx2 is short,
|x2|a ? 4. This means that cb15a is a substring of u2
and hence |u2|b = 15.
a5b14 a19 c29 b15 a5
u2 x2v1yv2u1
On the other hand, since (v1, v2) must contain at least
one occurrence of b, the string v1yv2 must contain
ba19c as a substring. This implies that |u1u2|a ? 10.
But since |u2|b = 15, we have |u1u2|b ? 15. This
clearly contradicts ?(u1u2) ? [?2, 2] ? [?2, 2]. 
We now assume that z has a normal,
concatenation-free 2-decomposition D and de-
rive a contradiction. We do this by following
a certain path in D. Starting from the root, we
descend in D, always choosing a non-leaf child, as
long as there is one. We show that this path will
never terminate.
The i-th node on the path will be denoted by
?i, counting the root as the 0-th node. The la-
bel of ?i will be denoted by (wi,1,wi,2). With each
i, we associate three strings xi,1, yi, xi,2 such that
xi,1wi,1yiwi,2xi,2 = z, analogously to Lemma 1. Since
?(wi,1wi,2) ? [?2, 2] ? [?2, 2] and ?(z) = (0, 0), we
will always have ?(xi,1yixi,2) ? [?2, 2] ? [?2, 2].
Initially, (w0,1,w0,2) is the label of the root ?0 and
x0,1 = y0 = x0,2 = ?. If ?i is not a leaf node, let
(ui,1, ui,2) and (vi,1, vi,2) be the labels of the left and
right children of ?i, respectively. If the left child
is not a leaf node, we let ?i+1 be the left child,
in which case we have (wi+1,1,wi+1,2) = (ui,1, ui,2),
xi+1,1 = xi,1, xi+1,2 = xi,2, and yi+1 = vi,1yvi,2. Oth-
erwise, ?i+1 will be the right child of ?i, and we
have (wi+1,1,wi+1,2) = (vi,1, vi,2), xi+1,1 = xi,1ui,1,
xi+1,2 = ui,2xi,2, and yi+1 = yi.
The path ?0, ?1, ?2, . . . is naturally divided into
two parts. The initial part of the path consists of
nodes where xi,1yixi,2 is short. Note that x0,1y0x0,2 =
? is short. As long as xi,1yixi,2 is short, (wi,1,wi,2)
must be long and ?i has two children labeled
by (ui,1, ui,2) and (vi,1, vi,2). By Lemma 7, either
(ui,1, ui,2) or (vi,1, vi,2) must be short. Since the length
672
of z is 87 and the length of a short string is at most 6,
exactly one of (ui,1, ui,2) and (vi,1, vi,2) must be long.
We must eventually enter the second part of
the path, where xi,1yixi,2 is no longer short. Let
?m be the first node belonging to this part of the
path. Note that at ?m, we have ?(xm,1ymxm,2) =
?(xm?1,1ym?1xm?1,2) + ?(v) for some short string v.
(Namely, v = um?1,1um?1,2 or v = vm?1,1vm?1,2.)
Lemma 8. If u and v are short strings and ?(uv) ?
[?2, 2]? [?2, 2], then |uv|d ? 4 for each d ? {a, b, c}.
Proof. Since u and v are short, we have |u|a ?
4, |u|b ? 4, |u|c ? 2 and |v|a ? 4, |v|b ? 4, |v|c ? 2. It
immediately follows that |uv|c ? 4. We distinguish
two cases.
Case 1. |uv|c ? 2. Since ?(uv) ? [?2, 2] ? [?2, 2],
we must have |uv|a ? 4 and |uv|b ? 4.
Case 2. |uv|c ? 3. Since |u|c ? 2 and |v|c ? 2,
we must have |u|c ? 1 and |v|c ? 1. Also, ?(uv) ?
[?2, 2] ? [?2, 2] implies that |uv|a ? 1 and |uv|b ? 1.
Since u and v are short, it follows that one of the
following two conditions must hold:
(i) |u|a ? 1, |u|b = 0 and |v|a = 0, |v|b ? 1.
(ii) |u|a = 0, |u|b ? 1 and |v|a ? 1, |v|b = 0.
In the former case, |uv|a = |u|a ? 4 and |uv|b = |v|b ?
4. In the latter case, |uv|a = |v|a ? 4 and |uv|b =
|u|b ? 4. 
By Lemma 8, the number of occurrences of each
letter in xm,1ymxm,2 is in [1, 4]. This can only be if
xm,1xm,2 = a
j
,
ym = c
kbl,
for some j, k, l ? [1, 4]. This means that the string z
must have been split into two strings (w0,1,w0,2) at
the root of D somewhere in the vicinity of position
67 (see Figure 2).
It immediately follows that for all i ? m, wi,1 is
a substring of a5b14a19c28 and wi,2 is a substring of
b14a5. We show by induction that for all i ? m, the
following condition holds:
(?) ba19c17 is a substring of wi,1.
The condition (?) clearly holds for i = m. Now as-
sume (?). Then (wi,1,wi,2) is long, and ?i has left and
right children, labeled by (ui,1, ui,2) and (vi,1, vi,2), re-
spectively, such that wi,1 = ui,1vi,1 and wi,2 = vi,2ui,2.
We consider two cases.
Case 1. ui,1 contains c. Then ba19c is a substring
of ui,1. Since ui,2 is a substring of b14a5, it cannot
contain any occurrences of c. Since ?1(ui,1ui,2) ?
[?2, 2], it follows that ui,1 must contain at least 17
occurrences of c; hence ba19c17 is a substring of ui,1.
Since (ui,1, ui,2) is long, (wi+1,1,wi+1,2) = (ui,1, ui,2).
Therefore, the condition (?) holds with i+ 1 in place
of i.
Case 2. ui,1 does not contain c. Then (ui,1, ui,2) is
short and (wi+1,1,wi+1,2) = (vi,1, vi,2). Note that vi,1
must contain at least 17 occurrences of c, but vi,2 is
a substring of b14a5 and hence cannot contain more
than 14 occurrences of b. Since ?2(vi,1vi,2) ? [?2, 2],
it follows that vi,1 must contain at least one occur-
rence of b. Therefore, ba19c17 must be a substring
of vi,1 = wi+1,1, which shows that (?) holds with i+1
in place of i.
We have proved that (?) holds for all i ? m. It fol-
lows that for all i, ?i has two children and hence ?i+1
is defined. This means that the path ?0, ?1, ?2, . . .
is infinite, contradicting the assumption that D is a
2-decomposition of z.
We have proved the following:
Lemma 9. There is a string in MIX that has no 2-
decomposition.
Theorem 10. There is no head grammar G such that
L(G) = MIX.
Proof. Immediate from Lemmas 3, 4, and 9. 
References
Setsuo Arikawa, Takeshi Shinohara, and Akihiro Ya-
mamoto. 1992. Learning elementary formal systems.
Theoretical Computer Science, 95(1):97?113.
Emmon Bach. 1981. Discontinuous constituents in gen-
eralized categorial grammars. In Victoria Burke and
James Pustejovsky, editors, Proceedings of the 11th
Annual Meeting of the North East Linguistic Society,
pages 1?12.
Emmon Bach. 1988. Categorial grammars as theories
of language. In Richard T. Oehrle, Emmon Bach, and
Deirdre Wheeler, editors, Categorial Grammars and
Natural Language Structures, pages 17?34. D. Reidel,
Dordrecht.
673
Gerald Gazdar. 1988. Applicability of indexed gram-
mars to natural languages. In U. Reyle and C. Rohrer,
editors,Natural Language Parsing and Linguistic The-
ories, pages 69?94. D. Reidel Publishing Company,
Dordrecht.
Robert Gilman. 2005. Formal languages and their ap-
plication to combinatorial group theory. In Alexan-
dre V. Borovik, editor, Groups, Languages, Algo-
rithms, number 378 in Contemporary Mathematics,
pages 1?36. American Mathematical Society, Provi-
dence, RI.
Annius V. Groenink. 1997. Mild context-sensitivity and
tuple-based generalizations of context-free grammar.
Linguistics and Philosophy, 20:607?636.
Aravind K. Joshi, Vijay K. Shanker, and David J. Weir.
1991. The converence of mildly context-sensitive
grammar formalisms. In Peter Sells, Stuart M.
Shieber, and ThomasWasow, editors, Foundational Is-
sues in Natural Language Processing, pages 31?81.
The MIT Press, Cambridge, MA.
Aravind K. Joshi. 1985. Tree-adjoining grammars: How
much context sensitivity is required to provide reason-
able structural descriptions? In David Dowty, Lauri
Karttunen, and Arnold M. Zwicky, editors, Natural
Language Parsing, pages 206?250. Cambridge Uni-
versity Press, Cambridge.
Markus Kracht. 2003. The Mathematics of Language,
volume 63 of Studies in Generative Grammar. Mou-
ton de Gruyter, Berlin.
William Marsh. 1985. Some conjectures on indexed
languages. Paper presented to the Association for
Symbolic Logic Meeting, Stanford University, July
15?19. Abstract appears in Journal of Symbolic
Logic 51(3):849 (1986).
Carl J. Pollard. 1984. Generalized Phrase Structure
Grammars, Head Grammars, and Natural Language.
Ph.D. thesis, Department of Linguistics, Stanford Uni-
versity.
Kelly Roach. 1987. Formal properties of head gram-
mars. In Alexis Manaster-Ramer, editor, Mathematics
of Language, pages 293?347. John Benjamins, Ams-
terdam.
Sylvain Salvati. 2011. MIX is a 2-MCFL and the word
problem in Z2 is captured by the IO and the OI hierar-
chies. Technical report, INRIA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context free gram-
mars. Theoretical Computer Science, 88(2):191?229.
Raymond M. Smullyan. 1961. Theory of Formal Sys-
tems. Princeton University Press, Princeton, NJ.
K. Vijay-Shanker and D. J. Weir. 1994. The equivalence
of four extensions of context-free grammars. Mathe-
matical Systems Theory, 27:511?546.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics, pages 104?111.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadephia, PA.
674
