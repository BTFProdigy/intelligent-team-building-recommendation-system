Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 29?36,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Agreement and Disputes in Dialogue
Alex Lascarides
School of Informatics,
University of Edinburgh
alex@inf.ed.ac.uk
Nicholas Asher
IRIT
Universite? Paul Sabatier, Toulouse
asher@irit.fr
Abstract
In this paper we define agreement in terms
of shared public commitments, and implicit
agreement is conditioned on the semantics of
the relational speech acts (e.g., Narration, Ex-
planation) that each agent performs. We pro-
vide a consistent interpretation of disputes,
and updating a logical form with the current
utterance always involves extending it and not
revising it, even if the current utterance denies
earlier content.
1 Introduction
A semantic theory of dialogue should account for
what content dialogue agents agree on. This in-
cludes implicit agreement:
(1) a. A: The room went dark.
b. A: Max turned out the light.
c. B: And John drew the blinds.
Intuitively, A and B agree that the room went dark,
that Max turned out the light, and that the latter is
at least part of the reason why the former occurred.
Thus, implicatures can be agreed upon (that (1b)
is part of the cause of (1a) goes beyond composi-
tional semantics), and agreement can be implicated
(B does not repeat (1a) and (1b) nor utter OK to in-
dicate his agreement with A).
In principle, the Grounding Acts Model (GAM,
Traum (1994), Traum and Allen (1994)) supports
implicit agreement. But it demands an acceptance
act for agreement to occur, and its current rules don?t
predict such an act from (1c). Segmented Discourse
Representation Theory (SDRT, Asher and Lascarides
(2003)) errs in the opposite direction. It stipulates
that lack of disagreement implicates agreement, and
so in (1) too much is agreed upon; e.g., (1c). Thus,
SDRT needs modification to deal with (1), just as
GAM needs supplementation.
Agreement can occur even in the context of cor-
rections or disputes. In (2), A asserts (2a) and B its
negation, but a consistent interpretation of (2) over-
all is a pre-requisite to explaining how A and B end
up agreeing on (2b).
(2) a. A: It?s raining.
b. B: No it?s not.
c. A: OK.
Since a correction negates content in the discourse
context, an obvious strategy for maintaining consis-
tency would be to revise the semantic representation
of the context when updating it with a correction.
But we want to avoid revision, both at the level of
model theory and at the level of composing logi-
cal form. This is for two reasons. Firstly, revision
means that there is in principle no general way of
stating what information is preserved from the pre-
vious discourse state to the current one. But if we
construct logical form in a monotonic way?in our
case, this means that the discourse structure for a
conversation at turn n is an elementary substructure
of the discourse structure at turn n + 1?then stan-
dard preservation results from model theory apply.
Secondly, monotonicity guarantees that interpreta-
tion algorithms can proceed incrementally, combin-
ing information from various sources in a nonde-
structive way (Alshawi and Crouch, 1992).
To our knowledge, there is currently no dynamic
semantics for dialogue that yields adequate interpre-
tations of corrections and implicit agreement. We
will address this gap here. In Section 2, we re-
29
view two existing approaches to motivate our ba-
sic strategy, which we then describe in Section 3.
We will refine SDRT so that it tracks each dialogue
participant?s public commitments. Further, while
identifying a speech act involves default reasoning,
constructing logical form will be monotonic, in the
sense that the logical form of an updated discourse
always extends that of its discourse context, rather
than revising it.
2 Motivation
We will say that a proposition p is grounded just
in case p is agreed by the dialogue agents to be
true. This follows Clark?s terminology, in particu-
lar the concept of grounding a joint action at level 4
(Clark, 1996, p388). Clark?s work focusses almost
entirely on grounding at the so-called ?lower? lev-
els; how agents ground an understanding of what
was said, for instance. By contrast, in order to fo-
cus on grounding at the higher level, we will assume
a highly idealised scenario where dialogue agents
understand each other perfectly, resolving ambigu-
ities in the same way. One of Clark?s main claims is
that grounding at all levels occurs only when there
is positive evidence for it, and we aim to explore in
a logically precise manner exactly what amount of
positive evidence suffices for grounding a proposi-
tion. In future work, we intend to demonstrate that
our definition of grounding can model grounding at
the lower levels too; this will involve extending the
framework to represent misunderstandings.
GAM links the speech acts performed with its ef-
fects, including effects on grounding (Traum, 1994).
Each conversational participant builds a conversa-
tional information state (or CIS). Update effects of
particular speech acts (and their preconditions) are
specified in terms of changes to (and conditions on)
the CIS. For example, Figure 1 is the update rule for
the speech act e where B assertsK to A. It updates
the common ground (G) to include an event e? that
B intends A to believe K and a conditional event
e?? that should A accept the assertion, then A would
be socially committed to B to believeK (shown via
the attitude SCCOE). The update rules form a hier-
archy, so that more specific acts inherit effects from
more general ones. The speech act in Figure 1 in-
herits that B is SCCOE-ed to A to K, for instance.
Decision trees then predict which speech acts have
been performed.
While it is possible in principle for GAM to in-
clude rules that accurately predict (1c)?s illocution-
ary effects, the rules that are actually provided only
recognise (1c) as an assertion. Consequently, its ef-
fects are under-generated: B is socially committed
to (1c), but not to (1a), (1b) or a causal relation be-
tween them. GAM needs to be supplemented with
rules for inferring that B was also implicitly accept-
ing parts of A?s contribution.
Such acceptances, we argue, should be condi-
tioned on relational speech acts. (1c) continues
(1b) as a narrative, and the narrative so formed ex-
plains (1a). These are relational speech acts (Asher
and Lascarides, 2003): they are speech acts because
continuing a narrative or explaining something are
things that people do with utterances; and they are
relational because the successful performance of the
speech act Explanation, say, is logically dependent
on the content of the utterance (or sequence of ut-
terances) that is being explained (in this case, (1a)).
Thus even though the compositional semantics of
(1c) does not entail (1b) or (1a), its illocutionary
contribution does entail them?or, perhaps more ac-
curately, entails that B is publicly committed to
them. Similarly, through using (1b) as an Explana-
tion of (1a), A is publicly committed to (1a), (1b)
and a causal relationship between them. Thus, what
is grounded amounts to the shared semantic entail-
ments of the rhetorical relations?or speech acts?
that both A and B performed. This explains why
positive evidence for grounding is necessary (Clark,
1996): both agents must perform a speech act with
appropriate semantic consequences for a proposition
to become grounded. An implicit acceptance (or ac-
knowledgement in SDRT terms) is then logically de-
pendent on the formal semantic interpretations of the
relational speech acts performed. For instance, B?s
commitments to (1a) and (1b) stem from Narration
and Explanation acts he performed in uttering (1c).
Since GAM incorporates relational speech acts,
the general principles that we propose here could
extend it. However, we have chosen to use SDRT
because it defines logical form more abstractly, al-
lowing us to exploit its model theory to determine
grounded propositions. In contrast to GAM, we will
not explicitly represent what?s grounded (and what?s
not) in logical form. Doing so would force us to in-
30
Name: Assert
Condition on update: G :: [e : Assert(B,A,K)]
Update G+= [e?]e? : Try(B, ?s?.s? : Bel(A,K)),
[e??]e?? : Accept(A, e) ? [s|s : SCCOE(A,B,K)]
Figure 1: The update rule for assertion
corporate revision should grounded content get dis-
puted, as can happen in a dynamic setting, where
facts and beliefs change as the agents engage in di-
alogue. We will make grounding a property of the
interpretation of a logical form, and not part of its
form.
SDRT offers a formal semantics of relational
speech acts (Asher and Lascarides, 2003). Further-
more, in contrast to theories of discourse interpreta-
tion that equate interpreting a discourse with its ef-
fects on the agents? beliefs (e.g., Hobbs et al (1993),
Grosz and Sidner (1990)), SDRT separates the glue
logic (i.e., the logic for constructing a logical form
of what was said) from the logic for interpreting
the logical form (i.e., reasoning about whether what
was said is true, or should be believed). This en-
ables SDRT to maintain a decidable procedure for
computing logical form, even though identifying the
speech acts performed inherently involves common-
sense reasoning, and hence consistency tests. Asher
and Lascarides (2003, p78) argue that it must be de-
cidable to explain why, as Lewis (1969) claims, peo-
ple by and large have a common understanding of
what was said.
SDRT?s current representation of (1) is (1?), where
pi1, pi2 and pi3 label the contents of the clauses (1a?
c) respectively, and pi0 and pi label the content of the
dialogue segments that are created by the rhetorical
connections:
(1?) pi0 : Explanation(pi1, pi)
pi : Narration(pi2, pi3)
In words, (1?) implies that the room went dark, and
this was caused by a combination of Max switching
off the light followed by John drawing the blinds.
In the absence of speech acts of denial such as Cor-
rection, SDRT stipulates that all content is grounded
(Asher and Lascarides, 2003, p363). This leads di-
rectly to the wrong predictions for (1).
Unlike GAM, SDRT fails to track the different
commitments of individual speakers. Simply la-
belling each speech act with its speaker doesn?t suf-
fice, as dialogue (3) shows.1
(3) pi1. A: John went to Harrods.
pi2. B: He bought a suit.
pi3. A: He then entered the food halls.
pi4. B: He looked for foie gras.
Intuitively, A?s utterance pi3 publicly commits
him not only to Narration(pi2, pi3), but also to
Narration(pi1, pi2) (for this latter speech act entails,
while the former does not, that John bought the suit
at Harrods). And yet B was the speaker who per-
formed the speech act Narration(pi1, pi2), for it is
B who uttered pi2. Accordingly, we abandon repre-
senting dialogue with a single SDRS, and replace it
with a tuple of SDRSs?one SDRS per discourse par-
ticipant per turn, representing all his commitments
up to and including that turn. We define grounding
a proposition p in terms of joint entailments from
those commitments, and hence grounding becomes
a semantic property of the logical form. This solves
SDRT?s over-generation problems with grounding.
For instance in (1), A?s public commitments are to
Explanation(pi1, pi2). B, on the other hand, is com-
mitted to the content expressed by (1?). The shared
public commitments then accurately reflect what A
and B agree on. We also avoid the under-generation
problems of GAM; grounding need not arise from
an acceptance but instead from so-called veridical
rhetorical relations (e.g., Explanation and Narra-
tion) and the logical relationships among their mean-
ings.
Grounded content is not marked as such in logical
form. This makes monotonic construction of logical
form feasible, even when grounded propositions get
disputed. A further part of our strategy for eschew-
ing revision is to assume that the SDRSs for each turn
represent all of A?s and B?s current commitments,
1For simplicity, we use a contructed example here, although
Sacks (1992) attests many similar, naturally occurring dialogues
where the agents build a narrative together.
31
from the beginning of the dialogue to the end of that
turn. The alternative, where prior but ongoing com-
mitments from turn i? 1 are not shown in the repre-
sentation of turn i, and accordingly the input context
for interpreting turn i is the output one from inter-
preting turn i? 1, would condemn us to incorporat-
ing revision into the model theory. This is because
A may commit in turn i to something that is incon-
sistent with his commitments in turn i? 1 (e.g., A?s
utterance (2c)), and without revision the output con-
text from turn i would then be ?. We want to avoid
revision while maintaining consistency. Represent-
ing all current commitments in each turn avoids re-
vision in the model theory, because one can com-
pute the current commitments of A and B by dy-
namically interpreting their SDRSs for just the last
turn. One can detect how A?s commitments have
changed during the dialogue, but only by comparing
the SDRSs for the relevant turns.2
We will model disputes by adding non-truth pre-
serving operators over relevant segments in the log-
ical form. This avoids the need for downdating and
revision in both the construction and the interpreta-
tion of logical form.
3 Individuating Commitments
The logical form for a dialogue turn proposed in
Section 2 generalises to dialogues with more than
two agents in the obvious way: the logical form of a
dialogue turn is a set {Sa : a ? D}, where Sa is an
SDRS and D is the set of dialogue agents. The log-
ical form of the dialogue overall will be the logical
forms of each of its turns (and all dialogue agents
build all the SDRSs in the logical form, not just the
SDRSs representing their own commitments). We
assume an extremely simple notion of turns, where
turn boundaries occur whenever the speaker changes
(even if this happens mid-clause), and we ignore for
now cases where agents speak simultaneously.
This new logical form for dialogue requires a new
dynamic interpretation. The context Cd of evalua-
tion for interpreting a dialogue turn is a set of dy-
namic contexts for interpreting SDRSs?one for each
2Pre?vot et al (2006) represent dialogue in terms of commit-
ment slates. Their idea inspired our work, but the details differ
considerably, particularly on monotonic construction.
agent a ? D:
Cd = {?C
i
a, C
o
a? : a ? D}
Thus Cia and C
o
a are world assignment pairs, given
the definitions from Asher and Lascarides (2003).
For instance, (4) defines the dynamic interpreta-
tion of veridical relations (e.g. Narration, Explana-
tion), where meaning postulates then stipulate the
illocutionary effects ?R(?,?)?e.g., for Narration
they stipulate the spatio-temporal progression of the
events (we gloss the content that?s labelled pi asKpi,
and m in [.]m stands for monologue). Equation (5)
defines the dynamic interpretation of Correction.
(4) (w, f)[R(?, ?)]m(w
?, g) iff
(w, f)[K? ?K? ? ?R(?,?)]m(w
?, g)
(5) (w, f)[Correction(?, ?)]m(w
?, g) iff
(w, f)[(?K?) ?K? ? ?Corr(?,?)]m(w
?, g)
The context change potential (CCP) of a dialogue
turn T = {Sa : a ? D} is the product of the CCPs
of the individual SDRSs:
Cd[T ]dC
?
d iff C
?
d = {?C
i
a, C
o
a? ? [Sa]m :
?Cia, C
o
a? ? Cd, a ? D}
Accordingly, dialogue entailments can be defined in
terms of the entailment relation |=m for SDRSs af-
forded by [.]m:
T |=d ? iff ?a ? D,Sa |=m ?
This makes |=d the shared entailment of each agent?s
public commitments. And we assume that content ?
is grounded or agreed upon by a dialogue turn T iff
T |=d ?. Finally, given that the SDRSs for a dialogue
turn reflect all an agent?s current commitments, the
interpretation of the dialogue overall is the CCP of
its last turn.
The logical form of (3) is shown in Table 1 (we
have omitted the logical forms of the clauses, la-
belled pi1 to pi4). The semantics of the SDRSs for
the last turn correctly predict the following proposi-
tion to be grounded (for it is entailed by them): John
went to Harrods, followed by buying a suit (at Har-
rods), followed by his entering the food halls.
There is a sharing of labels across the SDRSs in
Table 1. This general feature reflects the reality
that one speaker may perform a relational speech act
whose first argument is part of someone else?s turn,
32
Turn A?s SDRS B?s SDRS
1 pi1 ?
2 pi1 pi2B : Narration(pi1, pi2)
3 pi3A : Narration(pi1, pi2) ? Narration(pi2, pi3) pi2B : Narration(pi1, pi2)
4 pi3A : Narration(pi1, pi2) ? Narration(pi2, pi3) pi4B : Narration(pi1, pi2) ? Narration(pi2, pi3)?
Narration(pi3, pi4)
Table 1: The logical form of dialogue (3).
or part of his own previous turns. Sharing labels cap-
tures the intuition that an agent?s speech acts can re-
veal his commitments (or lack of them) to contextual
content, even if this is linguistically implicit.
Including prior but ongoing commitments in the
SDRS for the current turn has consequences for the
general architecture of the theory: we must stipu-
late what commitments persist across turns when
constructing the SDRSs. Consider the fourth turn
of dialogue (3). Intuitively, uttering pi4 commits
B to the illocutionary content of Narration(pi3, pi4).
But in addition, he is also committed at this point
to Narration(pi1, pi2)?Narration(pi2, pi3), as shown.
Those commitments persist from prior turns; they
are even transferred from one speaker to another.
However, we will shortly examine other examples,
involving corrections and even explicit acknowl-
edgements (or an acceptance in Traum?s (1994) ter-
minology), where the commitments do not persist.
To handle the data, we must make the ?commitment
persistence? principle sensitive to distinct relational
speech acts, and it must support a monotonic con-
struction of logical form.
To motivate our persistence principle, consider
how A and B get to the commitments shown in
Table 1. A?s SDRS for the first turn is pi1 : Kpi1 ,
where Kpi1 stands for the representation of John
went to Harrods. Since B hasn?t said anything yet,
his SDRS for the first turn is ?. SDRT?s glue logic
uses default axioms to predict the relation that con-
nects pi2 to pi1 (Asher and Lascarides, 2003); here,
these defaults should yield that B is committed to
pi2B : Narration(pi1, pi2) (we adopt the convention
that the root label of the speaker d?s SDRS for turn j
is named pijd). A?s SDRS for the second turn is the
same as the first turn: he hasn?t spoken since, and so
his commitments are unchanged.
In the third turn, the glue logic should predict that
A?s utterance pi3 forms a narrative with pi2. But sim-
ply adding this to A?s prior SDRS isn?t sufficient.
First, the result is not a well-formed SDRS, because it
won?t contain a single root label. Secondly, it misses
an important interplay between discourse structure
and grounding: adding only Narration(pi2, pi3) to
A?s existing commitment to Kpi1 makes A commit-
ted to the compositional semantics of pi2, but not
to its illocutionary contribution conveyed by B (e.g.
that John bought the suit at Harrods). And yet intu-
itively, uttering pi3 implicates that this (linguistically
implicit) content is agreed on.
Dialogues (1) and (3) feature discourse relations
that occur in monologue as well. Several agents can
use these to build up a narrative together, as noted by
Sacks (1992). Sacks? observations affirm that such
discourse relations can be used to perform ?implicit?
acknowledgements, and what?s more they suggest
that the implicit acknowledgement is not only of
the prior contribution?s compositional semantics but
also its illocutionary effects. These observations
lead us to add the following Persistence princi-
ple to the glue logic, together with axioms that iden-
tify undenied commitments (UC (?) stands for the
undenied commitments of the utterance or segment
?):
? Persistence:
? : R(?, ?) ? ? : UC (?)
Different glue-logic axioms will then identify the
undenied commitments for different speech acts.
The present case concerns simple left veridical (slv)
relations?those that do not explicitly endorse or
criticise any previous commitments. Note ? > ?
means ?If ? then normally ??, and T (d, j, pi) means
that label pi is a part of agent d?s SDRS for turn j:
? Undenied Commitments:
(? : R(?, ?) ? T (d1, j, ?) ? slv(R)?
?? : R?(?, ?) ? T (d2, j ? 1, ??)) >
(? : UC(?) ? ? : R?(?, ?))
33
Undenied Commitments states that if d1 com-
mits to R(?, ?) where R is simple left veridical and
d2 is already committed to R?(?, ?), then normally
the undenied commitments of ? include R?(?, ?).
Examples of simple left veridical relations include
Narration and Explanation but not Acknowledge-
ment (since this explicitly endorses prior content) or
Correction (since this denies prior content).
Persistence and Undenied
Commitments predict that A?s SDRS for the third
turn of (3) includes pi3A : Narration(pi1, pi2). This is
because default rules yield pi3A : Narration(pi2, pi3),
and Narration(pi1, pi2) is in B?s SDRS.
Persistence and Undenied Commitments
likewise predict that Narration(pi1, pi2) and
Narration(pi2, pi3) are a part of B?s SDRS for the
fourth turn, as shown in Table 1.
Undenied Commitments is defeasible. This
is because if the illocutionary contribution of A?s
(left-veridical) speech act R(?, ?) conflicts with
some proposition p that B conveyed by uttering
?, then clearly A?s speech act should not be con-
strued as an implicit acknowledgement of p. This
affects the analysis of (1), whose logical form is
Table 2. B?s SDRS after the second turn does
not include Explanation(pi1, pi2), even though his
utterance pi3 attaches with the veridical relation
Narration to pi2, and A?s SDRS for turn 1 in-
cludes Explanation(pi1, pi2). Persistence ap-
plies to this example (for label pi2) and the an-
tecedent to Undenied Commitments is sat-
isfied, but Explanation(pi1, pi2) is not an unde-
nied commitment of pi2 because its (nonmono-
tonic) semantic consequences conflict with those of
Explanation(pi1, pi), a speech act that the glue logic
must identify as one that B intended to perform (or,
in other words, publicly commit to) as a byproduct
of uttering pi3. Explanation(pi1, pi2) conflicts with
Explanation(pi1, pi) because the former nonmono-
tonically entails, via a scalar implicature, that Max
turning out the light was the sole cause of the room
going dark, while the latter (monotonically) entails
it was a strict part of it. This example illustrates how
the default logic rendered by > must be specified in
terms of the consistency in what follows nonmono-
tonically, rather than what follows monotonically.
Undenied Commitments does not apply
for the veridical relation Acknowledgement; i.e.,
utterances of the form OK, I agree, repeat-
ing prior content, and the like. In words,
Acknowledgement(pi1, pi2) entails Kpi1 , Kpi2 and
that Kpi2 implies Kpi1 ; to use the GAM term, it is an
act of explicit acceptance. Dialogue (6) illustrates
why Acknowledgement behaves differently from the
simple left veridical relations like Narration:
(6) pi1. B: John is not a good speaker
pi2. B: because he?s hard to understand.
pi3. A: I agree he?s hard to understand.
The compositional semantics of pi3 makes A
explicit about what in B?s turn he acknowl-
edges: A must be committed to (at least)
Acknowledgement(pi2, pi3). What is outside the
scope of the acknowledgement?namely, B?s pu-
tative explanation for why John is not a good
speaker?is not denied in (6). It would be consistent
to add Explanation(pi1, pi2) toA?s commitments, but
it?s simply not warranted. Dialogue (6) shows that
when the explicit endorsement conveys sufficiently
specific content, it appears to carry a scalar impli-
cature that this precise content is endorsed, and no
more.
Another reason for excluding explicit acknowl-
edgements from the set of simple left veridical rela-
tions is that such speech acts come with their own
grounding requirements. Acknowledgements can
have scope over implicatures as well as composi-
tional semantic contents, since the first argument
to an Acknowledgement relation can be a label of
an arbitrarily complex SDRS. So by acknowledg-
ing pij , we do not thereby acknowledge the impli-
catures of pij itself; had we wished to do so, we
would have included them within the scope of the
acknowledgement. That is, we would infer the re-
lation Acknowledgement(pi?j , pii), where pi
?
j has se-
mantic scope over pij , making pij and the rhetori-
cal relations it engages in part of what is (explic-
itly) endorsed. It is because the discourse function
of an acknowledgement is precisely to say what one
agent commits to from another agent?s turn?i.e.,
what are the undenied commitments in this case?
that Persistence applies redundantly.
Explicit acknowledgements have been studied
by Traum and Hinkelman (1992), among others.
Here, we will ignore interpretations of an utter-
ance pi2 (e.g., OK) as an acknowledgement thatKpi1
34
Turn A?s SDRS B?s SDRS
1 pi1A : Explanation(pi1, pi2) ?
2 pi1A : Explanation(pi1, pi2) pi2B : Explanation(pi1, pi)
pi : Narration(pi2, pi3)
Table 2: The logical form of (1).
was said (represented in SDRT with the so-called
metatalk relation Acknowledgement*(pi1, pi2)), in-
stead focussing entirely on an interpretation of pi2
using Acknowledgement (i.e., a commitment toKpi1 ,
which in turn entails a commitment that Kpi1 was
said). But even so there is ambiguity, because lin-
guistic form does not always fully determine what
the acknowledgement has scope over. Let?s assume
that A?s utterance pi3 in (7) is an acknowledgement
of content and not just of understanding that content:
(7) pi1. B: John is not a good speaker
pi2. B: because he is hard to understand.
pi3. A: OK.
Acknowledgement(pi2, pi3) entails Kpi2 . Making pi2
the only label that?s acknowledged leads to an inter-
pretation where the proposition that pi2 explains pi1
is not acknowledged. This ?narrow scope? attach-
ment permits A to continue by challenging the ex-
planatory link, e.g., by uttering but that?s not why
he?s not a good speaker. Another interpretation
of (7) is that A commits to all of B?s commit-
ments, including the implicatures: this is expressed
by adding Acknowledgement(pi1B, pi3) to A?s SDRS,
where pi1B : Explanation(pi1, pi2). Indeed, if OK
is all that A says, then one defaults to this wide-
scope interpretation. Even if A follows OK with
He IS hard to understand with high pitch accents
and a falling boundary tone, the preferred interpre-
tation contrasts with (6), to be one where OK is an
Acknowledgement of pi1B , and He?s hard to under-
stand is an explanation of that acknowledgement act
(marked with the metatalk relation Explanation* in
SDRT). It is straightforward to add glue-logic ax-
ioms for constructing logical form that reflect these
principles for identifying the first argument of Ac-
knowledgement.
In dialogue (2), A commits to the negation of
his prior commitment. As before, constructing B?s
SDRS for the second turn involves using the glue
logic to identify how pi2 connects to pi1. So long
as their semantic incompatibility is transferred, in
shallow form, to the glue logic, then the general
principle that the necessary semantic consequences
of a speech act are normally sufficient for inferring
that it was performed will apply, yielding pi2B :
Correction(pi1, pi2) (see Table 3). The cue phrase
OK is then used by the glue logic to infer pi3A :
Acknowledgement(pi2, pi3). This resolves the under-
specified content OK to Kpi2 ; and thus as before the
glue logic also yields pi3A : Correction(pi1, pi3), as
shown. It?s not raining is entailed by the SDRSs
for turn 3. The interpretation of each turn is con-
sistent (i.e., the output state is non-empty), although
the SDRSs for turn 2 are mutually inconsistent (A?s
SDRS entails that it?s raining andB?s entails it?s not).
Finally, the content associated with each label does
not change from one turn to the next, making the
construction of logical form monotonic.
Clark (1996) doesn?t make precise exactly what
counts as sufficient positive evidence for grounding.
Similarly, Traum and Allen (1994) don?t provide
rules for inferring when a speaker has performed
an implicit acceptance. Our framework makes
the quantity of positive evidence that?s needed for
grounding propositions logically precise, in terms
of the relational speech acts that both speakers
perform, and the logical relationships between the
semantics of those speech acts. Persistence
and Undenied Commitments capture a gen-
eral class of examples involving implicit agreement.
Sufficient positive evidence for grounding a propo-
sition through explicit endorsements and challenges
rests on the formal semantic interpretation of the rel-
evant speech acts?namely Acknowledgement and
Correction?and the rules by which one determines
the first argument of these relations.
4 Conclusion
We have presented a novel treatment of agreements
and disputes in which the construction of logi-
cal form is monotonic in the subsumptive sense
35
Turn A?s SDRS B?s SDRS
1 pi1 : Kpi1 ?
2 pi1 : Kpi1 pi2B : Correction(pi1, pi2)
3 pi3A : Correction(pi1, pi3) ? Acknowledgement(pi2, pi3) pi2B : Correction(pi1, pi2)
Table 3: The logical form of dialogue (2).
(Shieber, 1986); the semantic representation of the
discourse context is an elementary substructure of
the representation of the dialogue updated with the
current utterance, even if the current utterance de-
nies earlier content. However, the logical form re-
mains a product of complex default reasoning, since
identifying the speech acts that were performed in-
volves commonsense reasoning with the linguistic
and non-linguistic context.
The relationship between the grounded proposi-
tions and the interpretation of the dialogue is entirely
transparent and is defined in terms of the model the-
ory of the logical forms. It provides a logical basis
for exploring Clark?s (1996) notion of positive evi-
dence for grounding. A crucial ingredient in our ac-
count was the use of relational speech acts, and the
logical relationships among their semantics.
We believe our definition of grounding as
shared commitment is capable of modelling Clark?s
more central concern?grounding the understand-
ing of what was said. The left-veridical rela-
tions that are the hallmark of grounding at level
4 entail grounding at the lower levels thanks to
the semantics of DSDRSs. Moreover, SDRT?s
metatalk relations?such as Explanation*(?, ?) and
Acknowledgement*(?, ?)?commit an agent to the
fact that K? was said without committing him K?.
Thus shared commitments that follow from a repre-
sentation of the dialogue can ground acts at lower
levels without grounding (or denying) acts at level
4. A full model of grounding at lower levels, how-
ever, requires us to extend the framework to handle
misunderstandings.
This paper presents just some first steps towards a
dynamic theory of grounding. For instance, we have
not yet modelled the impact of questions and imper-
atives on public commitments and grounding. We
have started to explore links between public com-
mitments and other attitudes, such as beliefs, prefer-
ences, and intentions (Asher and Lascarides, 2008),
but this also remains a matter of ongoing research.
References
H. Alshawi and R. Crouch. Monotonic semantic in-
terpretation. In Proceedings of ACL, pages 32?39,
1992.
N. Asher and A. Lascarides. Logics of Conversation.
CUP, 2003.
N. Asher and A. Lascarides. Commitments, beliefs
and intentions in dialogue. In Proceedings of Lon-
dial, 2008.
H. H. Clark. Using Language. CUP, 1996.
B. Grosz and C. Sidner. Plans for discourse. In
J. Morgan P. R. Cohen andM. Pollack, editors, In-
tentions in Communication, pages 365?388. MIT
Press, 1990.
J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin.
Interpretation as abduction. Artificial Intelligence,
63(1?2):69?142, 1993.
D. Lewis. Convention: A Philosophical Study. Har-
vard University Press, 1969.
L. Pre?vot, N. Maudet, and P. Muller. Conversational
game-board and discourse structure. In Proceed-
ings of Constraints in Discourse, Ireland, 2006.
H. Sacks. Lectures on Conversation. Blackwells,
1992.
S. Shieber. An Introduction to Unification-based Ap-
proaches to Grammar. CSLI Publications, 1986.
D. Traum. A Computational Theory of Grounding
in Natural Language Conversation. PhD thesis,
University of Rochester, 1994.
D. Traum and J. Allen. Discourse obligations in di-
alogue processing. In Proceedings of ACL, pages
1?8, 1994.
D. Traum and E. Hinkelman. Conversation acts in
task-oriented spoken dialogue. Computational In-
telligence, 8(3):575?599, 1992.
36
Coling 2008: Companion volume ? Posters and Demonstrations, pages 7?10
Manchester, August 2008
Distilling Opinion in Discourse: A Preliminary Study
Nicholas Asher and Farah Benamara
IRIT-CNRS Toulouse,
France
{asher, benamara}@irit.fr
Yvette Yannick Mathieu
LLF-CNRS Paris,
France
yannick.mathieu@linguist.jussieu.fr
Abstract
In this paper, we describe a preliminary
study for a discourse based opinion cate-
gorization and propose a new annotation
schema for a deep contextual opinion anal-
ysis using discourse relations.
1 Introduction
Computational approaches to sentiment analysis
eschew a general theory of emotions and focus
on extracting the affective content of a text from
the detection of expressions of sentiment. These
expressions are assigned scalar values, represent-
ing a positive, a negative or neutral sentiment to-
wards some topic. Using information retrieval, text
mining and computational linguistic techniques to-
gether with a set of dedicated linguistic resources,
one can calculate opinions exploiting the detected
?bag of sentiment words?. Recently, new meth-
ods aim to assign fine-grained affect labels based
on various psychological theories?e.g., the MPQA
project (Wiebe et al, 2005) based on literary the-
ory and linguistics and work by (Read et al, 2007)
based on the Appraisal framework (Martin and
White, 2005).
We think there is still room for improvement in
this field. To get an accurate appraisal of opin-
ion in texts, NLP systems have to go beyond pos-
itive/negative classification and to identify a wide
range of opinion expressions, as well as how they
are discursively related in the text. In this paper,
we describe a preliminary study for a discourse
based opinion categorization. We propose a new
annotation schema for a fine-grained contextual
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
opinion analysis using discourse relations. This
analysis is based on a lexical semantic analysis of
a wide class of expressions coupled together with
an analysis of how clauses involving these expres-
sions are related to each other within a discourse.
The aim of this paper is to establish the feasibil-
ity and stability of our annotation scheme at the
subsentential level and propose a way to use this
scheme to calculate the overall opinion expressed
in a text on a given topic.
2 A lexical semantic analysis of opinion
expressions
We categorize opinion expressions using a typol-
ogy of four top-level categories (see table 1): RE-
PORTING expressions, which provide an evalu-
ation of the degree of commitment of both the
holder and the subject of the reporting verb, JUDG-
MENT expressions, which express normative eval-
uations of objects and actions, ADVISE expres-
sions, which express an opinion on a course of ac-
tion for the reader, and SENTIMENT expressions,
which express feelings (for a more detailed de-
scription of our categories see (Asher et al 2008)).
Our approach to categorize opinions uses the
lexical semantic research of (Wierzbicka, 1987),
(Levin, 1993) and (Mathieu, 2004). From these
classifications, we selected opinion verb classes
and verbs which take opinion expressions within
their scope and which reflect the holder?s com-
mitment on the opinion expressed. We removed
some verb classes, modified others and merged re-
lated classes into new ones. Subjective verbs were
split into these new categories which were then ex-
tended by adding nouns and adjectives.
Our classification is the same for French and En-
glish. It differs from psychologically based classi-
fications like Martin?s Appraisal system : in ours
7
Groups SubGroups Examples
Reporting
a) Inform inform, notify, explain
b) Assert assert, claim, insist
c) Tell say, announce, report
d) Remark comment, observe, remark
e) Think think, reckon, consider
f) Guess presume, suspect, wonder
Judgment
g) Blame blame, criticize, condemn
h) Praise praise, agree, approve
i) Appreciation good, shameful, brilliant
Advise
j) Recommend advise, argue for
k) Suggest suggest, propose
l) Hope wish, hope
Sentiment
m) Anger/CalmDown irritation, anger
n) Astonishment astound, daze, impress
o) Love, fascinate fascinate, captivate
p) Hate, disappoint demoralize, disgust
q) Fear fear, frighten, alarm
r) Offense hurt, chock
s) Sadness/Joy happy, sad
t) Bore/entertain bore, distraction
u) Touch disarm, move, touch
Table 1: Top-Level opinion categories.
the contents of the JUDGMENT and SENTIMENT
categories are quite different, and more detailed
for SENTIMENT descriptions with 14 sub-classes.
Ours is also broader: the REPORTING and the AD-
VISE categories do not appear as such in the Ap-
praisal system. In addition, we choose not to build
our discourse based opinion categorization on the
top of MPQA (Wiebe et al 2005) for two reasons.
First, we suggest a more detailed analysis of pri-
vate states by defining additional sets of opinion
classes such as HOPES and RECOMMENDATIONS.
We think that refined categories are needed to build
a more nuanced appraisal of opinion expressions
in discourse. Second, text anchors which corre-
spond to opinion in MPQA are not well defined
since each annotator is free to identify expression
boundaries. This is problematic if we want to in-
tegrate rhetorical structure into opinion identifica-
tion task. MPQA often groups discourse indica-
tors (but, because, etc.) with opinion expressions
leading to no guarantee that the text anchors will
correspond to a well formed discourse unit.
3 Towards a Discursive Representation of
Opinion Expressions
Rhetorical structure is an important element in un-
derstanding opinions conveyed by a text. The fol-
lowing simple examples drawn from our French
corpus show that discourse relations affect the
strength of a given sentiment. S1 : [I agree with
you]
a
even if I was shocked and S2 : Buy the DVD,
[you will not regret it]
b
. Opinions in S1 and S2
are positive but the contrast introduced by even in
S
1
decreases the strength of the opinion expressed
in (a) whereas the explanation provided by (b) in
S2 increases the strength of the recommendation.
Using the discourse theory SDRT (Asher and Las-
carides, 2003) as our formal framework, our four
opinion categories are used to label opinion ex-
pressions within a discourse segment. For exam-
ple, there are three opinion segments in the sen-
tence S3: [[It?s poignant]
d
, [sad]
e
]
g
and at the
same time [horrible]
f
We use five types of rhetorical relations: CON-
TRAST, CORRECTION, SUPPORT, RESULT and
CONTINUATION (For a more detailed description
see (Asher et al 2008)). Within a discourse seg-
ment, negations were treated as reversing the po-
larities of the opinion expressions within their
scope. Conditionals are hard to interpret because
they affect the opinion expressed within the conse-
quent of a conditional in different ways. For exam-
ple, conditionals,expressions of ADVISE can block
the advice or reverse it. Thus if you want to waste
you money, buy this movie will be annotated as a
recommendation not to buy it. On the other hand,
conditionals can also strengthen the recommenda-
tion as in if you want to have good time, go and
see this movie. We have left the treatment of con-
ditionals as well as disjunctions for future work.
3.1 Shallow Semantic Representation
In order to represent and evaluate the overall
opinion of a document, we characterize discourse
segments using a shallow semantic representa-
tion using a feature structure (FS) as described
in (Asher et al 2008). Figure 1 shows the dis-
cursive representation of the review movie S4:
[This film is amazing.]
a
. [[One leaves not com-
pletely convinced]
b.1
, but [one is overcome]
b.2
].
[[It?s poignant]
c.1
, [sad]
c.2
] and at the same time
[horrible]
c.3
].[Buy it]
d
. [You won?t regret it]
e
.
Figure 1: Discursive representation of S4.
Once we have constructed the discursive repre-
sentation of a text, we have to combine the dif-
ferent FS in order to get a general representation
8
that goes beyond standard positive/negative repre-
sentation of opinion texts. In this section, we first
explain the combination process of FS. We then
show how an opinion text can be summarized us-
ing a graphical representation.
The combination of low-level FS is performed
in two steps: (1) combine the structures related
by coordinating relations (such as CONTRAST and
CONTINUATION). In figure 1, this allows to build
from the segments b.1 and b.2 a new FS ; (2) com-
bine the strutures related via subordinating rela-
tions (such as SUPPORT and RESULT) in a bottom
up way. In figure 1, the FS of the segment a is com-
bined with the structure deduced from step 1. Dur-
ing this process, a set of dedicated rules is used.
The procedure is formalized as follows. Let a, b be
two segments related by the rhetorical relation R
such as: R(a, b). Let S
a
, S
b
be the FS associated
respectively to a and b i.e S
a
: [category : [group
a
:
subgroup
a
],modality : [polarity : p
a
, strength : s
a
] ? ? ?]
and S
b
: [category : [group
b
: subgroup
b
], modality :
[polarity : p
b
, strength : s
b
] ? ? ?] and let S : [category :
[group],modality : [polarity : p, strength : s] ? ? ?] be
the FS deduced from the combination of S
a
and
S
b
. Some of our rules are:
CONTINUATIONS strengthen the polarity of the
common opinion. One of the rule used is: if
(group
a
= group
b
) and (subgroup
a
6= subgroup
b
)) then
if ((p
a
= neutral) and (p
b
6= neutral)) then group =
group
a
and p = p
b
and s = max(s
a
, s
b
), as in moving
and sad news.
For CONTRAST, let OW
i
be the set of opinion
words that belongs to a segment S
i
. We have for
OW
a
= ? and OW
b
6= ? : group = group
b
, p = p
b
and
s = s
b
+ 1, as in I don?t know a lot on Edith Piaf?s
life but I was enthraled by this movie.
Finally, an opinion text is represented by a graph
G = (?,?) such as:
? ? = H ? T is the set of nodes where :
H = {ho
i
/ho
i
is an opinion holder} and T =
{to
i
: value/to
i
is a topic and value is a FS}, such as :
value = [Polarity : p, Strength : s,Advice : a], where:
p = {positive, negative, neutral} and s, a = {0, 1, 2}.
? ? = ?
H
? ?
T
? ?
H?T
where: ?
H
=
{(h
i
, h
j
)/h
i
, h
j
? H} means that two top-
ics are related via an ELABORATION relation.
This holds generally between a topic and a
subtopic, such as a movie and a scenario ; ?
T
=
{(t
i
, t
j
, type)/t
i
, t
j
? T and type = support/contrast}
means that two holders are related via a CON-
TRAST (holders h
i
and h
j
have a contrasted opin-
ion on the same topic) or a SUPPORT relation
(holders share the same point of view) ; and
?
H?T
= {(h
i
, t
j
, type)/h
i
? H and t
j
? T and type =
attribution/commitment} means that an opinion to-
wards a topic t
j
is attributed or committed to a
holder h
i
. For example, in John said that the film
was horrible, the opinion is only attributed to John
because verbs from the TELL group do not con-
vey anything about the author view. However, in
John infomed the commitee that the situation was
horrible, the writer takes the information to be es-
tablished. The figure 2 below shows the general
representation of the movie review S4.
Figure 2: General representation of S4.
4 Annotating Opinion Segments:
Experiments and Preliminary Results
We have analyzed the distribution of our categories
in three different types of digital corpora, each
with a distinctive style and audience : movie re-
views, Letters to the Editor and news reports in
English and in French. We randomly selected 150
articles for French corpora (around 50 articles for
each genre). Two native French speakers anno-
tated respectively around 546 and 589 segments.
To check the cross linguistic feasability of gener-
alisations made about the French data, we also an-
notated opinion categories for English. We have
annotated around 30 articles from movie reviews
and letters. For news reports, the annotation in En-
glish was considerably helped by using texts from
the MUC 6 corpus (186 articles), which were an-
notated independently with discourse structure by
three annotators in the University of Texas?s DIS-
COR project (NSF grant, IIS-0535154); the anno-
tation for our opinion expressions involved a col-
lapsing of structures proposed in DISCOR.
The annotation methodology is described in
(Asher et al 2008). For each corpus, annotators
first begin to annotate elementary discourse seg-
ments, define its shallow representation and finally,
connect the identified segments using the set of
rhetorical relations we have identified. A segment
is annotated only if it explicitly contains an opin-
ion word that belong to our lexicon or if it bears a
rhetorical relation to an opinion segment.
9
The average distribution of opinion expressions
in our corpus across our categories for each lan-
guage is shown in table 2. The annotation of movie
reviews was very easy. The opinion expressions
are mainly adjectives and nouns. We found an av-
erage of 5 segments per review. Opinion words in
Letters to the Editor are adjectives and nouns but
also verbs. We found an average of 4 segments per
letter. Finally, opinions in news documents involve
principally reported speech. As we only annotated
segments that clearly expressed opinions or were
related via one of our rhetorical relations to a seg-
ment expressing an opinion, our annotations typ-
ically only covered a fraction of the whole docu-
ment. This corpus was the hardest to annotate and
generally contained lots of embedded structure in-
troduced by REPORTING type verbs.
To compute the inter-annotator agreements
(IAA) we did not take into account the opinion
holder and the topic as well as the polarity and the
strength because we chose to focus, at a first step,
only on agreements on opinion categorization, seg-
ment idendification and rhetorical structure detec-
tion. We computed the agreements only on the
French corpus. The French annotators performed
a two step annotation where an intermediate anal-
ysis of agreement and disagreement between the
two annotators was carried out. This analysis al-
lowed each annotator to understand the reason of
some annotation choices. Using the Kappa mea-
sure, the IAA on opinion categorization is 95% for
movie reviews, 86% for Letters to the Editors and
73% for news documents.
Annotators had good agreement concerning
what the basic segments were (82%), which shows
that the discourse approach in sentiment analysis
is easier compared to the lexical task where an-
notators have low agreements on the identification
of opinion tokens. The principal sources of dis-
agreement in the annotation process came from
annotators putting opinion expressions in different
categories (mainly between PRAISE/BLAME group
and APPRECIATION group, such as shame) and the
choice of rhetorical relations. Nevertheless, by us-
ing explicit discourse connectors, we were able
to get relatively high agreement on the choice of
rhetorical relations. We also remained quite un-
sure how to distinguish between the reporting of
neutral opinions and the reporting of facts. The
main extension of this work are to (1) deepen our
opinion typology, specifically to include modals
Groups Movie (%) Letters (%) News (%)
French English French English French English
Reporting 2.67 2.12 14.80 13.34 43.91 42.85
a 0 0 0.71 1.33 4.02 4.76
b 0.53 0 0 4 5.83 0
c 0 0 1.79 0 4.51 35.71
d 0.88 0 2.17 0 11.82 0
e 1.33 0 10.12 6.67 5.89 1.34
f 0 2.12 0 1.34 11.77 0
Judgment 60.53 40.52 52.50 73.34 39.23 33.34
g 0.54 0 6.32 26.66 13.69 16.67
h 2.45 2.12 7.54 20 1.81 4.76
i 54.49 38.29 33.48 26.87 23.72 11.90
Advise 6.92 10.63 10.05 13.34 7.27 9.52
j 6.26 8.51 0.70 5.33 1.37 0
k 0.66 2.12 3.94 1.33 3.61 0
l 0 0 5.38 6.67 2.28 9.52
Sentiment 27.30 34.04 33.08 2.67 11.35 16.67
m 0.54 0 3.23 0 0,90 0
n 2.23 6.38 3.96 2.66 0,90 7.14
o 7.38 4.25 3.74 0 1,87 9.52
p 4.97 2.12 5.03 0 2,72 0
q 2.23 0 5.03 0 1,86 0
r 0.89 0 7.17 0 2,28 0
s 3.79 4.25 2.87 0 0.88 0
t 1.33 14.9 0 0 0 0
u 4.46 2.12 2.15 2.12 0 0
Table 2: Average distribution of our categories.
and moods like the subjunctive, and to (2) provide
a deep semantic representation that associates for
each category of opinion a lambda term involving
the proferred content and a lambda term for the
presuppositional content of the expression, if it has
one. In terms of automatization, we plan to exploit
a syntactic parser to get the argument structure of
verbs and then a discourse segmenter like that de-
veloped in the DISCOR project, followed by the
detection of discourse relations using cue words.
References
Asher N. and Benamara F. and Mathieu Y.Y. 2008. Catego-
rizing Opinions in Discourse. ECAI08.
Asher N. and Lascarides A. 2003. Logics of Conversation.
Cambridge University Press.
Levin B. 1993. English Verb Classes and Alterna-tions: A
Preliminary Investigation. University of Chicago Press
Martin J.R and White P.R.R. 2005. Language of Evaluation:
Appraisal in English. Palgrave Macmillan.
Mathieu Y. Y. 2004. A Computational Semantic Lexicon
of French Verbs of Emotion. In Shanahan, G., Qu, Y.,
Wiebe, J. (eds.): Computing Attitude and Affect in Text.
Dordrecht.
Read J., Hope D. and Carroll J. 2007. Annotating Expres-
sions of Appraisal in English. The Linguistic Annotation
Workshop, ACL 2007.
Wiebe J., Wilson T. and Cardie C. 2005. Annotating Expres-
sions of Opinions and Emotions in Language. Language
Resources and Evaluation 1(2).
Wierzbicka A. 1987. Speech Act Verbs. Sydney: Academic
Press.
10
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1?9,
Beijing, August 2010
Testing SDRT?s Right Frontier
Stergos D. Afantenos and Nicholas Asher
Institut de recherche en informatique de Toulouse (IRIT),
CNRS, Universit? Paul Sabatier
{stergos.afantenos, nicholas.asher}@irit.fr
Abstract
The Right Frontier Constraint (RFC), as a
constraint on the attachment of new con-
stituents to an existing discourse struc-
ture, has important implications for the in-
terpretation of anaphoric elements in dis-
course and for Machine Learning (ML) ap-
proaches to learning discourse structures.
In this paper we provide strong empirical
support for SDRT?s version of RFC. The
analysis of about 100 doubly annotated
documents by five different naive annota-
tors shows that SDRT?s RFC is respected
about 95% of the time. The qualitative
analysis of presumed violations that we
have performed shows that they are either
click-errors or structural misconceptions.
1 Introduction
A cognitively plausible way to view the construc-
tion of a discourse structure for a text is an incre-
mental one. Interpreters integrate discourse con-
stituent n into the antecedently constructed dis-
course structure D for constituents 1 to n ? 1 by
linking n to some constituent in D with a dis-
course relation. SDRT?s Right Frontier Constraint
(RFC) (Asher, 1993; Asher and Lascarides, 2003)
says that a new constituent n cannot attach to an
arbitrary node in D. Instead it must attach to ei-
ther the last node entered into the graph or one of
the nodes that dominate this last node. Assuming
that the last node is usually found on the right of
the structure, this means that the nodes available
for attachment occur on the right frontier (RF) of
the discourse graph or SDRS.
Researchers working in different theoretical
paradigms have adopted some form of this con-
straint. Polanyi (1985; 1988) originally pro-
posed the RFC as a constraint on antecedents to
anaphoric pronouns. SDRT generalizes this to a
condition on all anaphoric elements. As the at-
tachment of new information to a contextually
given discourse graph in SDRT involves the reso-
lution of an anaphoric dependency, RFC furnishes
a constraint on the attachment problem. (Webber,
1988; Mann and Thompson, 1987; 1988) have
also adopted versions of this constraint. But there
are important differences. While SDRT and RST
both take RFC as a constraint on all discourse at-
tachments (in DLTAG, in contrast, anaphoric dis-
course particles are not limited to finding an an-
tecedent on the RF), SDRT?s notion of RF is sub-
stantially different from that of RST?s or Polanyi?s,
because SDRT?s notion of a RF depends on a 2-
dimensional discourse graph built from coordinat-
ing and subordinating discourse relations. Defin-
ing RFC with respect to SDRT?s 2-dimensional
graphs allows the RF to contain discourse con-
stituents that do not include the last constituent
entered into the graph (in contrast to RST). SDRT
also allows for multiple attachments of a con-
stituent to the RFC.
SDRT?s RFC has important implications for the
interpretation of various types of anaphoric ele-
ments: tense (Lascarides and Asher, 1993), ellip-
sis (Hardt et al, 2001; Hardt and Romero, 2004;
Asher, 2007), as well as pronouns referring to in-
dividuals and abstract entities (Asher, 1993; Asher
and Lascarides, 2003). The RFC, we believe, will
also benefit ML approaches to learning discourse
structures, as a constraint limiting the search space
for possible discourse attachments. Despite its
importance, SDRT?s RFC has never been empiri-
cally validated, however. We present evidence in
this paper providing strong empirical support for
SDRT?s version of the constraint. We have cho-
sen to study SDRT?s notion of a RF, because of
SDRT?s greater expressive power over RST (Dan-
los, 2008), the greater generality of SDRT?s defi-
1
nition of RFC, and because of SDRT?s greater the-
oretical reliance on the constraint for making se-
mantic predictions. SDRT also makes theoretically
clear why the RFC should apply to discourse re-
lation attachment, since it treats discourse struc-
ture construction as a dynamic process in which
all discourse relations are essentially anaphors.
The analysis of about 100 doubly annotated docu-
ments by five different naive annotators shows that
this constraint, as defined in SDRT, is respected
about 95% of the time. The qualitative analysis of
the presumed violations that we have performed
shows that they are either click-errors or structural
misconceptions by the annotators.
Below, we give a formal definition of SDRT?s
RFC; section 3 explains our annotation procedure.
Details of the statistical analysis we have per-
formed are given in section 4, and a qualitative
analysis is provided in section 5. Finally, sec-
tion 6 presents the implications of the empirical
study for ML techniques for the extraction of dis-
course structures while sections 7 and 8 present
the related work and conclusions.
2 The Right Frontier Constraint in SDRT
In SDRT, a discourse structure or SDRS (Seg-
mented Discourse Representation Structure) is a
tuple < A,F , LAST >, where A is the set of
labels representing the discourse constituents of
the structure, LAST ? A the last introduced label
and F a function which assigns each member of
A a well-formed formula of the SDRS language
(defined (Asher and Lascarides, 2003, p 138)).
SDRSs correspond to ? expressions with a contin-
uation style semantics. SDRT distinguishes coor-
dinating and subordinating discourse relations us-
ing a variety of linguistic tests (Asher and Vieu,
2005),1 and isolates structural relations (Parallel
and Contrast) based on their semantics.
The RF is the set of available attachment points
1The subordinating relations of SDRT are currently: Elab-
oration (a relation defined in terms of the main eventualities
of the related constituents), Entity-Elaboration (E-Elab(a,b)
iff b says more about an entity mentioned in a that is not the
main eventuality of a) Comment, Flashback (the reverse of
Narration), Background, Goal (intentional explanation), Ex-
planation, and Attribution. The coordinating relations are:
Narration, Contrast, Result, Parallel, Continuation, Alterna-
tion, and Conditional, all defined in Asher and Lascarides
(2003).
to which a new utterance can be attached. What
this set includes depends on the discourse relation
used to make the attachment. Here is the defini-
tion from (Asher and Lascarides, 2003, p 148).
Suppose that a constituent ? is to be attached to a
constituent in the SDRS with a discourse relation
other than Parallel or Contrast. Then the avail-
able attachment points for ? are:
1. The label ? = LAST;
2. Any label ? such that:
(a) i-outscopes(?, ?) (i.e. R(?, ?) or
R(?, ?) is a conjunct in F(?) for
some R and some ?); or
(b) R(?, ?) is a conjunct in F(?) for
some label ?, where R is a subordi-
nating discourse relation.
We gloss this as ? < ?.
3. Transitive Closure:
Any label ? that dominates ? through a
sequence of labels ?1, ?2, . . . ?n such that
? < ?1 < ?2 < . . . ?n < ?
We can represent an SDRS as a graph G, whose
nodes are the labels of the SDRSs constituents and
whose typed arcs represent the relations between
them. The nodes available for attachment of a new
element ? in G are the last introduced node LAST
and any other node dominating LAST, where the
notion of domination should be understood as the
transitive closure over the arrows given by sub-
ordinating relations or those holding between a
complex segment and its parts. Subordinating re-
lations like Elaboration extend the vertical dimen-
sion of the graph, whereas coordinating relations
like Narration expand the structure horizontally.
The graph of every SDRS has a unique top label
for the whole structure or formula; however, there
may be multiple < paths defined within a given
SDRS, allowing for multiple parents, in the ter-
minology of (Wolf and Gibson, 2006). Further-
more, SDRT allows for multiple arcs between con-
stituents and attachments to multiple constituents
on the RFC, making for a very rich structure.
SDRT?s RFC is restricted to non-structural rela-
tions, because structural relations postulate a par-
tial isomorphism from the discourse structure of
the second constituent to the discourse structure
of the first, which provides its own attachment
possibilities for subconstituents of the two related
structures (Asher, 1993). Sometimes such paral-
lelism or contrast, also known as discourse subor-
dination (Asher, 1993), can be enforced in a long
2
distance way by repeating the same wording in the
two constituents.
RFC has the name it does because the segments
that belong on this set (the ?s in the above def-
inition) are typically nodes on a discourse graph
which are geometrically placed at the RF of the
graph. Consider the following example embel-
lished from Asher and Lascarides (2003):
(1) (?1) John had a great evening last night. (?2) He first
had a great meal at Michel Sarran. (?3) He ate
profiterolles de foie gras, (?4) which is a specialty of
the chef. (?5) He had the lobster, (?6) which he had
been dreaming about for weeks. (?7) He then went
out to a several swank bars.
The graph of the SDRS for 1 looks like this:
(2) ?1
Elaboration
??
?2
ElaborationNarration
?7
???
?3
E-elabNarration
?5
Background
?4 ?6
where ?? and ??? represent complex segments.
Given that the last introduced utterance is repre-
sented by the node ?7, the set of nodes that are
on the RF are ?7 (LAST), ?? (the complex segment
that includes ?7) and ?1 (connected via a subordi-
nating relation to ??). All those nodes are geomet-
rically placed at the RF of the graph.
SDRT?s notion of a RF is more general than
RST?s or DLTAG?s. First, SDRSs can have com-
plex constituents with multiple elements linked
by coordinate relations that serve as arguments
to other relations, thus permitting instances of
shared structure that are difficult to capture in a
pure tree notation (Lee et al, 2008). In addi-
tion, in RST the RF picks out the adjacent con-
stituents, LAST and complex segments including
LAST. Contrary to RST, SDRT, as it uses 2-
dimensional graphs, predicts that an available at-
tachment point for ?7 is the non local and non ad-
jacent ?2, which is distinct from the complex con-
stituent consisting of ?2 to ?6.2 This difference
is crucial to the interpretation of the Narration:
2The 2-dimensionality of SDRSs also allows us to rep-
Narration claims a sequence of two events; mak-
ing the complex constituent (essentially a sub-
SDRS) an argument of Narration, as RST does,
makes it difficult to recover such an interpreta-
tion. Danlos?s (2008) interpretation of the Nu-
clearity Principle provides an interpretation of the
Narration([2-4],5) that is equivalent to the SDRS
graph above.3 But even an optional Nuclearlity
Principle interpretation won?t help with discourse
structures like (2) where the backgrounding ma-
terial in ?4 and the commentary in ?6 do not and
cannot figure as part of the Elaboration for seman-
tic reasons. In our corpus described below, over
20% of the attachments were non adjacent; i.e. the
attachment point for the new material did not in-
clude LAST.
A further difference between SDRT and other
theories is that, as SDRT?s RFC is applied re-
cursively over complex segments within a given
SDRS, many more attachment points are available
in SDRT. E.g., consider the SDRS for this example,
adapted from (Wolf and Gibson, 2006):
(3) (?1) Mary wanted garlic and thyme. (?2) She also
needed basil. (?3) The recipe called for them. (?4)
The basil would be hard to come by this time of year.
? Explanation
?1 Parallel ?2E-elab
?3
?4
Because ? is the complex segment consisting
of ?1 and ?2, attachment to ? with a subordinat-
ing discourse relation permits attachment ??s open
constituents as well.4
3 Annotated Corpus
Our corpus comes from the discourse structure an-
notation project ANNODIS5 which represents an
on going effort to build a discourse graph bank
for French texts with the two-fold goal of test-
ing various theoretical proposals about discourse
resent many examples with Elaboration that involve cross-
ing dependencies in Wolf and Gibson?s (2006) representation
without violation of the RFC.
3Baldridge et al (2007), however, show that the Nuclear-
ity Principle does not always hold.
4This part of the RFC was not used in (Asher and Las-
carides, 2003).
5http://w3.erss.univ-tlse2.fr/annodis
3
structure and providing a seed corpus for learning
discourse structures using ML techniques. ANN-
ODIS?s annotation manual provides detailed in-
structions about the segmentation of a text into
Elementary Discourse Units (EDUs). EDUs corre-
spond often to clauses but are also introduced by
frame adverbials,6 appositive elements, correla-
tive constructions ([the more you work,] [the more
you earn]), interjections and discourse markers
within coordinated VPs [John denied the charges]
[but then later admitted his guilt]. Appositive ele-
ments often introduce embedded EDUs; e.g., [Jim
Powers, [President of the University of Texas at
Austin], resigned today.], which makes our seg-
mentation more fine-grained than Wolf and Gib-
son?s (2006) or annotation schemes for RST or the
PDTB.
The manual also details the meaning of dis-
course relations but says nothing about the struc-
tural postulates of SDRT. For example, there is no
mention of the RFC in the manual and very little
about hierarchical structure. Subjects were told
to put whatever discourse relations from our list
above between constituents they felt were appro-
priate. They were also told that they could group
constituents together whenever they felt that as a
whole they jointly formed the term of a discourse
relation. We purposely avoided making the man-
ual too restrictive, because one of our goals was
to examine how well SDRT predicts the discourse
structure of subjects who have little knowledge of
discourse theories.
In total 5 subjects with little to no knowledge
of discourse theories that use RFC participated
in the annotation campaign. Three were under-
graduate linguistics students and two were grad-
uate linguistics students studying different areas.
The 3 undergraduates benefitted from a completed
and revised annotation manual. The two gradu-
ate students did their annotations while the anno-
tation manual was undergoing revisions. All in
all, our annotators doubly annotated about 100
French newspaper texts and Wikipedia articles.
Subjects first segmented each text into EDUs, and
then they were paired off and compared their seg-
6Frame adverbials are sentence initial adverbial phrases
that can either be temporal, spatial or ?topical" (in Chem-
istry).
mentations, resolving conflicts on their own or via
a supervisor. The annotation of the discourse re-
lations was performed by each subject working
in isolation. ANNODIS provided a new state of
the art tool, GLOZZ, for discourse annotation for
the three undergraduates. With GLOZZ annotators
could isolate sections of text corresponding to sev-
eral EDUs, and insert relations between selected
constituents using the mouse. Though it did por-
tray relations selected as lines between parts of the
text, GLOZZ did not provide a discourse graph or
SDRS as part of its graphical interface. The rep-
resentation often yielded a dense number of lines
between segments that annotators and evaluators
found hard to read. The inadequate interline spac-
ing in GLOZZ also contributed to certain number
of click errors that we detail below in the paper.
The statistics on the number of documents, EDUs
and relations provided by each annotator are in ta-
ble 1.
annotator # Docs # EDUs # Relations
undergrad 1 27 1342 1216
undergrad 2 31 1378 1302
undergrad 3 31 1376 1173
grad 1 47 1387 1390
grad 2 48 1314 1321
Table 1: Statistics on documents, EDUs and Rela-
tions.
4 Experiments and Results
Using ANNODIS?s annotated corpus, we checked
for all EDUs ?, whether ? was attached to a con-
stituent in the SDRS built from the previous EDUs
in a way that violated the RFC. Given a discourse
as a series of EDUs ?1, ?2, . . . , ?n, we constructed
for each ?i the corresponding sub-graph and cal-
culated the set of nodes on the RF of this sub-
graph. We then checked whether the EDU ?i+1
was attached to a node that was found in this set.
We also checked whether any newly created com-
plex segment was attached to a node on the RF of
this sub-graph.
4.1 Calculating the Nodes at the RF
To calculate the nodes on the RF, we slightly ex-
tended the annotated graphs, in order to add im-
4
plied relations left out by the annotators.7
Disconnected Graphs While checking the RFC
for the attachment of a node n, the SDRS graph
at this point might consist of 2 or more disjoint
subgraphs which get connected together at a later
point. Because we did not want to decide which
way these graphs should be connected, we defined
a right frontier for each one using its own LAST.
We then calculated the RF for each one of them
and set the set of available nodes to be those in
the union of the RFs of the disjoint subgraphs. If
the subgraphs were not connected at the end of
the incremental process in a way that conformed
to RFC, we counted this as a violation. Annotators
did not always provide us with a connected graph.
Postponed Decisions SDRT allows for the at-
tachment not only of EDUs but also of subgraphs
to an available node in the contextually given
SDRS. For instance, in the following example, the
intended meaning is given by the graph in which
the Contrast is between the first label and the com-
plex constituent composed of the disjunction of ?2
and ?3.
(?1) Bill doesn?t like sports. (?2) But Sam does.
(?3) Or John does.
?1 Contrast ?
?
?2 Altern. ?3
Naive annotators attached subgraphs instead of
EDUs to the RF with some regularity (around 2%).
This means that an EDU ?i+1 could be attached to
a node that was not present in the subgraph pro-
duced by ?1, . . . , ?i. There were two main rea-
sons for this: (1) ?i+1 came from a syntactically
fronted clause, a parenthetical or apposition in a
sentence whose main clause produced ?i+2 and
?i+1 was attached to ?i+2; (2) ?i+1 was attached
to a complex segment [. . . , ?i+1, . . . , ?i+k, . . .]
which was not yet introduced in the subgraph.
Since the nodes to which ?i+1 is attached in
such cases are not present in the graph, by def-
inition they are not in the RF and they could be
counted as violations. Nonetheless, if the nodes
7In similar work on TimeML annotations, Setzer et al
(2003; Muller and Raymonet (2005) add implied relations to
annotated, temporal graphs.
which connect nodes like ?i+1 eventually link up
to the incrementally built SDRS in the right way,
?i+1 might eventually end up linked to something
on the RF. For this reason, we postponed the de-
cision on nodes like ?i+1 until the nodes to which
they are attached were explicitly introduced in the
SDRS.
The Coherence of Complex Segments In an
SDRS, several EDUs may combine to form a com-
plex segment ? that serves as a term for a dis-
course relation R. The interpretation of the SDRS
implies that all of ??s constituents contribute to
the rhetorical function specified by R. This im-
plies that the coordinating relation Continuation
holds between the EDUs inside ?, unless there is
some other relation between them that is incom-
patible with Continuation (like a subordinating
relation). Continuations are often used in SDRT
(Asher, 1993; Asher and Lascarides, 2003). Dur-
ing the annotation procedure, our subjects did not
always explicitly link the EDUs within a complex
segment. In order to enforce the coherence of
those complex segments we added Continuation
relations between the constituents of a complex
segment unless there was already another path be-
tween those constituents.
Expanding Continuations Consider the fol-
lowing discourse:
(4) [John, [who owns a chain of restaurants]?2 , [and is a
director of a local charity organization,]?3 wanted to
sell his yacht.]?1 [He couldn?t afford it anymore.]?4
Annotators sometimes produced the following
SDRT graph for the first three EDUs of this dis-
course:
(5) ?1
E-Elab
?2 Continuation ?3
In this case the only open node is ?3 due to
the coordinating relation Continuation. Nonethe-
less, ?4 should be attached to ?1, without vi-
olating the RFC. Indeed, SDRT?s definition of
the Continuation relation enforces that if we have
R(?1, ?2) and Continuation(?2, ?3) then we ac-
tually have the complex segment [?2, ?3] with
R(?1, [?2, ?3]). So there is in fact a missing com-
plex segment in (5). The proper SDRS graph of (4)
is:
5
(6) ?1
E-Elab
?
?2 Continuation ?3
which makes ?1 an available attachment site for
?4. Such implied constituents have been added to
the SDRS graphs.
Factoring Related to the operation of Ex-
pansion, SDRT?s definition of Continuation and
various subordinating relations also requires
that if we have R(a, [?1, ?2, . . . , ?n]) where
[?1, ?2, . . . , ?n] is a complex segment with
?1, . . . ?n linked by Continuation and R is Elabo-
ration, Entity-Elaboration, Frame, Attribution, or
Commentary, then we also have R(a, ?i) for each
i. We added these relations when they were miss-
ing.
4.2 Results
With the operations just described, we added sev-
eral inferred relations to the graph. We then cal-
culated statistics concerning the percentage of at-
tachments for which the RFC is respected using
the following formula:
RFCEDU =
# EDUs attached to the RF
# EDUs in total
As we explained, an EDU can be attached to an
SDRT graph directly by itself or indirectly as part
of a bigger complex segment. In order to calcu-
late the nominator we determine first whether an
EDU directly attaches to the graph?s RF, and if that
fails we determine whether it is part of a larger
complex segment which is attached to the graph?s
RF. The results obtained are shown in the first two
columns of table 2. The RFC is respected by at
least some attachment decision 95% of the time?
i.e., 95% of the EDUs get attached to another node
that is found on the RF. The breakdown across our
annotators is given in table 2.
SDRT allows for multiple attachments of an
EDU to various nodes in an SDRS; e.g. while an
EDU may be attached via one relation to a node
on the RF, it may be attached to another node off
the RF. To take account of all the attachments for a
given EDU, we need another way of measuring the
percentage of attachments that respects the RFC.
So we counted the ways each EDU is related to a
node in the SDRS for the previous text and then
divided the number of attachment decisions that
respect the RFC by the total number of attachment
decisions?i.e. :
RFCr =
# RF attachment decisions
# Total attachment decisions
.
annotator RFCEDU RFCr
undergrad 1 98.57% 91.28%
undergrad 2 98.12% 94.39%
undergrad 3 91.93% 89.17%
grad 1 94.38% 86.54%
grad 2 92.68% 83.57%
Mean for all annotators 95.24% 88.91%
Mean for 3 undergrad 96.17% 91.71%
Table 2: The % with which each annotator has re-
spected SDRT?s RFC using the EDU and attachment
decision measures.
The third column of table 2 shows that having
a stable annotation manual and GLOZZ improved
the results across our two annotator populations,
even though the annotation manual did not say
anything about RFC or about the structure of the
discourse graphs. Moreover, the distribution of vi-
olations of the RFC follows a power law and only
4.56% of the documents contained more than 5 vi-
olations. This is strong evidence that there is little
propagation of violations.
5 Analysis of Presumed Violations
Although 95% of EDUs attach to nodes on the
RF of an SDRT graph, 5% of EDUs don?t. SDRT
experts performed a qualitative analysis of some
of these presumed violations. In many cases, the
experts judged that the presumed violations were
due to click-errors: sometimes the annotators sim-
ply clicked on something that did not translate into
a segment. Sometimes, the experts judged that the
annotators picked the wrong segment to attach a
new segment or the wrong type of relation during
the construction of the SDRT graph. For example,
in the graph that follows the relation between seg-
ments 74 and 75 is not a Comment but an Entity-
Elaboration.
6
As expected, there were also ?structural? er-
rors, arising from a lack or a misuse of complex
segments. Here is a typical example (translated
from the original French):
[Around her,]_74 [we should mention Joseph
Racaille]_75 [responsible for the magnificent ar-
rangements,]_76 [Christophe Dupouy]_77 [reg-
ular associate of Jean-Louis Murat responsi-
ble for mixing,]_78 [without forgetting her two
guardian angels:]_79 [her agent Olivier Gluz-
man]_80 [who signed after a love at first
sight,]_81 [and her husband Mokhtar]_82 [who
has taken care of the family]_83
Here is the annotated structure up to EDU 78:
74
Comment
75
E-elab Cont
77
E-elab
76 78 (LAST)
Note that the attachment of 77 to 75 is non-local
and non-adjacent. The annotator then attaches
EDU 79 to 75 which is blocked from the RF due to
the Continuation coordinating relation. By not
having created a complex segment due the enu-
meration that includes EDUs 75 to 78, the annota-
tor had no option but to violate the RF. Here is the
proper SDRT graph for segments 74 to 79 (where
the attachment of 79 to 74 is also both non-local
and non-adjacent):
74
Elab
Elab
? 79
75
E-elabContinuation
77
E-elab
76 78
In this case, before the introduction of EDU 79,
EDU 78 is LAST and by consequence 77, ? and 74
are on the RF. Attaching 79 to 74 is thus legiti-
mate.
We also found more interesting examples of
right frontier violations. One annotator produced
a graph for a story which is about the attacks of
9/11/2001 and is too long to quote here. A sim-
plified graph of the first part of the story is shown
below. EDU 4 elaborates on the main event of the
story but it is not on the RF for 19. However, 19
is the first recurrence of the complex definite de-
scription le 11 septembre 2001 since the title and
the term?s definition in EDU 4.
4
E-elab
Continuation
7 Result [11-13] Result [14-16]Comment
19
This reuse of the full definite description could be
considered a case of SDRT?s discourse subordina-
tion.
6 RFC and distances of attachment
Our empirical study vindicates SDRT?s RFC, but
it also has computational implications. Using the
RFC dramatically diminishes the number of at-
tachment possibilities and thus greatly reduces the
search space for any incremental discourse pars-
ing algorithm.8 The mean of nodes that are open
on the RF at any given moment on our ANNODIS
data is 16.43% of all the nodes in the graph.
Our data also allowed us to calculate the dis-
tance of attachment sites from LAST, which could
be an important constraint on machine learning
algorithms for constructing discourse structures.
Given a pair of constituents (?i, ?j) distance is
calculated either textually (the number of inter-
vening EDUs between ?i and ?j) or topologically
(the length the shortest path between ?i and ?j).
Topological distance, however, does not take into
account the fact that a textually further segment is
cognitively less salient. Moreover, this measure
can give the same distance to nodes that are textu-
ally far away between them due to long distance
pop-ups (Asher and Lascarides, 2003). A purely
textual distance, on the other hand, gives the same
distance to an EDU ?i and a complex segment
[?1, . . . , ?i] even if ?1 and ?i are textually dis-
tant (since both have the same span end). We used
a measure combining both. The distance scheme
that we used assigns to each EDU its textual dis-
tance from LAST in the graph under consideration,
while a complex segment of rank 1 gets a distance
which is computed from the highest distance of
their constituent EDUs plus 1. For a constituent ?
of rank n we have:
Dist = Max{dist(x) : x in ?}+ n
8An analogous approach for search space reduction is fol-
lowed by duVerle and Prendinger (2009) who use the ?Prin-
ciple of Sequentiality? (Marcu, 2000), though they do not say
how much the search space is reduced.
7
The distribution of attachment follows a power
law with 40% of attachments performed non-
locally, that is on segments of distance 2 or more
(figure 1). This implies that the distance between
candidate attachment sites that are on the RF is an
important feature for an ML algorithm. It is impor-
tant to note at this point that following the baseline
approach of always attaching on the LAST misses
40% of attachments. We also have 20.38% of the
non-local, non-adjacent attachments in our anno-
tations. So an RST parser using Marcu?s (2000)
adjacency constraint as do duVerle and Prendinger
(2009) would miss these.
0
10
20
30
40
50
60
0 2 4 6 8 10 12 14 16 18 20
P
e
r
c
e
n
t
ag
e
Attachment distance
3
3333333333333333333
Figure 1: Distribution of attachment distance
7 Related Work
Several studies have shown that the RFC may be
violated as an anaphoric constraint when there
are other clues, content or linguistic features, that
determine the antecedent. (Poesio and di Euge-
nio, 2001; Holler and Irmen, 2007; Asher, 2008;
Pr?vot and Vieu, 2008), for example, show that
anaphors such as definite descriptions and com-
plex demonstratives, which often provide enough
content on their own to isolate their antecedents,
or pronouns in languages like German which must
obey gender agreement, might remain felicitous
although the discourse relations between them and
their antecedents might violate the RFC. Usually
there are few linguistic clues that help find the
appropriate antecedent to a discourse relation, in
contrast to the anaphoric expressions mentioned
above. Exceptions involve stylistic devices like
direct quotation that license discourse subordina-
tion. Thus, SDRT predicts that RFC violations for
discourse attachments should be much more rare
than those for the resolution of anaphors that pro-
vide linguistic clues about their antecedents.
As regards other empirical validation of var-
ious versions of the RFC for the attachment of
discourse constituents, Wolf and Gibson (2006)
show an RST-like RFC is not supported in their
corpus GraphBank. Our study concurs in that
some 20% of the attachments in our corpus can-
not be formulated in RST.9 On the other hand,
we note that because of the 2 dimensional nature
of SDRT graphs and because of the caveats intro-
duced by structural relations and discourse sub-
ordination, the counterexamples from GraphBank
against, say, RST representations do not carry over
straightforwardly to SDRSs. In fact, once these
factors are taken into account, the RFC violations
in our corpus and in GraphBank are roughly about
the same.
8 Conclusions
We have shown that SDRT?s RFC has strong empir-
ical support: the attachments of our 3 completely
naive annotators fully comply with RFC 91.7% of
the time and partially comply with it 96% of the
time. As a constraint on discourse parsing SDRT?s
RFC, we have argued, is both empirically and
computationally motivated. We have also shown
that non-local attachments occur about 40% of the
time, which implies that attaching directly on the
LAST will not yield good results. Further, many of
the non local attachments do not respect RST?s ad-
jacency constraint. We need SDRT?s RFC to get the
right attachment points for our corpus. We believe
that empirical studies of the kind we have given
here are essential to finding robust and useful fea-
tures that will vastly improve discourse parsers.
9One other study we are aware of is Sassen and K?hn-
lein (2005), who show that in chat conversations, the RFC
does not always hold unconditionally. Since this genre of
discourse is not always coherent, it is expected that the RFC
will not always hold here.
8
References
Asher, N. and A. Lascarides. 2003. Logics of Con-
versation. Studies in Natural Language Processing.
Cambridge University Press, Cambridge, UK.
Asher, N. and L. Vieu. 2005. Subordinating and co-
ordinating discourse relations. Lingua, 115(4):591?
610.
Asher, N. 1993. Reference to Abstract Objects in Dis-
course. Kluwer Academic Publishers.
Asher, N. 2007. A large view of semantic content.
Pragmatics and Cognition, 15(1):17?39.
Asher, N. 2008. Troubles on the right frontier.
In Benz, A. and P. K?hnlein, editors, Constraints
in Discourse, Pragmatics and Beyond New Series,
chapter 2, pages 29?52. John Benjamins Publishing
Company.
Baldridge, J., N. Asher, and J. Hunter. 2007. An-
notation for and robust parsing of discourse struc-
ture on unrestricted texts. Zeitschrift fur Sprachwis-
senschaft, 26:213?239.
Danlos, L. 2008. Strong generative capacity of rst,
sdrt and discourse dependency dags. In Benz, A.
and P. K?hnlein, editors, Constraints in Discourse,
Pragmatics and Beyond New Series, pages 69?95.
John Benjamins Publishing Company.
duVerle, D. and H. Prendinger. 2009. A novel dis-
course parser based on support vector machine clas-
sification. In Proceedings of ACL, pages 665?673,
Suntec, Singapore, August.
Hardt, D. and M. Romero. 2004. Ellipsis and
the structure of discourse. Journal of Semantics,
21:375?414, November.
Hardt, D., N. Asher, and J. Busquets. 2001. Discourse
parallelism, scope and ellipsis. Journal of Seman-
tics, 18:1?16.
Holler, A. and L. Irmen. 2007. Empirically assessing
effects of the right frontier constraint. In Anaphora:
Analysis, Algorithms and Applications, pages 15?
27. Springer, Berlin/Heidelberg.
Lascarides, A. and N. Asher. 1993. Temporal interpre-
tation, discourse relations and commonsense entail-
ment. Linguistics and Philosophy, 16(5):437?493.
Lee, A., R. Prasad, A. Joshi, and B. Webber. 2008.
Departures from tree structures in discourse: Shared
arguments in the penn discourse treebank. In Con-
straints in Discourse (CID ?08), pages 61?68.
Mann, W. and S. Thompson. 1987. Rhetorical struc-
ture theory: A framework for the analysis of texts.
Technical Report ISI/RS-87-185, Information Sci-
ences Institute, Marina del Rey, California.
Mann, W. and S. Thompson. 1988. Rhetorical struc-
ture theory: Towards a functional theory of text or-
ganization. Text, 8(3):243?281.
Marcu, D. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press.
Muller, P. and A. Raymonet. 2005. Using inference
for evaluating models of temporal discourse. In
12th International Symposium on Temporal Repre-
sentation and Reasoning, pages 11?19. IEEE Com-
puter Society Press.
Poesio, M. and B. di Eugenio. 2001. Discourse struc-
ture and anaphoric accessibility. In Proc. of the
ESSLLI Workshop on Discourse Structure and In-
formation Structure, August.
Polanyi, L. 1985. A theory of discourse structure and
discourse coherence. In Kroeber, P. D., W. H. Eil-
fort, and K. L. Peterson, editors, Papers from the
General Session at the 21st Regional Meeting of the
Chicago Linguistics Society.
Polanyi, L. 1988. A formal model of the structure of
discourse. Journal of Pragmatics, 12:601?638.
Pr?vot, L. and L. Vieu. 2008. The moving right fron-
tier. In Benz, A. and P. K?hnlein, editors, Con-
straints in Discourse, Pragmatics and Beyond New
Series, chapter 3, pages 53?66. John Benjamins
Publishing Company.
Sassen, C. and P. K?hnlein. 2005. The right fron-
tier constraint as conditional. In Computational
Linguistics and Intelligent Text Processing, Lecture
Notes in Computer Science (LNCS), pages 222?
225.
Setzer, A., R. Gaizauskas, and M. Hepple. 2003.
Using semantic inferences for temporal annotation
comparison. In Proceedings of the Fourth Interna-
tional Workshop on Inference in Computational Se-
mantics (ICoS-4).
Webber, B. 1988. Title discourse deixis and discourse
processing. Technical Report MS-CIS-88-75, Uni-
versity of Pennsylvania, Department of Computer
and Information Science, September.
Wolf, F. and E. Gibson. 2006. Coherence in Natural
Language: Data Stuctures and Applications. The
MIT Press.
9
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2184?2194, Dublin, Ireland, August 23-29 2014.
Unsupervised extraction of semantic relations using discourse cues
Juliette Conrath Stergos Afantenos Nicholas Asher Philippe Muller
IRIT, Universit? Toulouse & CNRS, Univ. Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse
{firstname.lastname@irit.fr}
Abstract
This paper presents a knowledge base containing triples involving pairs of verbs associated with
semantic or discourse relations. The relations in these triples are marked by discourse connectors
between two adjacent instances of the verbs in the triple in the large French corpus, frWaC.
We detail several measures that evaluate the relevance of the triples and the strength of their
association. We use manual annotations to evaluate our method, and also study the coverage of
our resource with respect to the discourse annotated corpus Annodis. Our positive results show
the potential impact of our resource for discourse analysis tasks as well as other semantically
oriented tasks like temporal and causal information extraction.
1 Introduction
Relational lexical resources, which describe semantic relations between lexical items, have tradition-
ally focused on relations like synonymy or similarity in thesauri, perhaps including some hierarchical
semantic relations like hyperonymy or hyponomy or part-whole relations as in the resource Wordnet (Fel-
baum, 1998). Some distributional thesauri contain more varied relations, see e.g. (Grefenstette, 1994),
however these relations are not typed. The lexical semantics given by FrameNet (Baker et al., 1998) does
include causal and temporal relations, as does Verbocean (Chklovski and Pantel, 2004), but coverage is
limited and empirical validation of these resources is partial and still largely remains to be done.
Lexical relations, in particular between verbs, are nevertheless crucial for understanding natural lan-
guage and for many information processing tasks. They are needed for textual inference, in which one
has to infer certain relations between eventualities (Hashimoto et al., 2009; Tremper and Frank, 2013),
for information extraction tasks, like finding temporal relations between eventualities mentioned in a text
(UzZaman et al., 2013), for automatic summarization (Liu et al., 2007), and for discourse parsing in the
absence of explicit discourse markers (Sporleder and Lascarides, 2008).
In this paper we report on our efforts to extract semantic relations essential to the analysis of discourse
and its interpretation, in which links are made between units of text or rather their semantic representa-
tions as in (1) in virtue of semantic information about the two main verbs of those clauses.
(1) The candidate demonstrated his expertise during the interview. The committee was completely
convinced.
We follow similar work on the extraction of causal, temporal, entailment and presuppositional relations
from corpora (Do et al., 2011; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Tremper and Frank,
2013), though our goals and validation methods are different. While one of our goals is to use this
information to improve performance in predicting discourse relations between clauses, we believe that
such a lexical resource will have other uses in other tasks in which semantic information is needed.
Discourse analysis is a difficult task. Rhetorical relations are frequently implicit and require for their
identification inference using diverse sources of lexical and compositional semantic information. In the
Penn Discourse Treebank corpus for example, 52% of the discourse relations are unmarked (Prasad et
This work has been supported by the French agency Agence Nationale de la Recherche (ANR-12-CORD-0004).
It is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added
by the organizers. License details : http://creativecommons.org/licenses/by/4.0/.
2184
al., 2008). Accordingly, annotation with discourse structure is a slow and error prone task, and relatively
little annotated data is currently available ; and so machine learning approaches have had limited suc-
cess in this area. Our approach addresses this problem, using non annotated data with features that can
be automatically detected to find typical contexts (pairs of discourse units) in which various discourse
relations occur. We suppose with (Sporleder and Lascarides, 2008; Braud and Denis, 2013) that such
contexts display regular lexical associations, in particular with verbs in those discourse units. An ex-
plicit, manually compiled list of all possible associations between two verbs and the semantic relations
they suggest is infeasible, so we present here an automatic method for compiling such a list, inspired by
the Verbocean project (Chklovski and Pantel, 2004).
Our hypothesis, supported by existing corpora, is that adjacent clauses are often arguments of discourse
relations. When these clauses contain certain adverbs or other discourse connectors, we can recover
automatically one or more discourse relations that we associate with the main verbs of those clauses. We
extract triples consisting of the two verbs and a semantic relation from a large corpus with the aim of
inferring that such a pair of verbs can suggest the semantic relation even in the absence of an explicit
discourse marker. We thus also suppose, with (Sporleder and Lascarides, 2008; Braud and Denis, 2013),
that such discourse markers are at least partially redundant ; inferring a discourse relation between two
clauses relies not only the marker but on the two verbs in the related clauses as well. All of our work has
been done on French data.
Our paper is organized as follows. We describe first the knowledge base of verb semantic relation
triples that we have constructed (section 2) ; we then present our methods for isolating verb pairs impli-
cating discourse or temporal information (section 3). A third section describes our methods of evaluation
(section 4) and a fourth discusses related work (section 5).
2 Exploring relations between verbs in a corpus
We built a knowledge base (V
2
R)
1
using the frWaC corpus(Baroni et al., 2009). frWaC contains about
1.6 billion words and was collected on the Web on the .fr domain. We first parsed the documents in our
corpus using BONSAI
2
, which first produced a morpho-syntactic labeling using MElt (Denis and Sagot,
2012) and then a syntactic analysis in the form of dependency trees via a French version of the MaltParser
(Nivre et al., 2007).
Our goal is to find pairs of verbs linked by a relation explicitly marked by a discourse connector
in the corpus, as an indication of a regular semantic relation between the two verbs. The relations we
have considered are common to most theories of discourse analysis, and they can be grouped into four
classes (Prasad et al., 2008) : causal (contingency) relations, temporal relations, comparison relations
(mainly contrast type relations), and expansion relations (e.g. elaboration or continuation).
To find explicitly marked relations, we used a lexicon of discourse connectors for French, the man-
ually constructed LEXCONN resource (Roze et al., 2012)
3
. LEXCONN includes 358 connectors and
gives their syntactic category as well as associated discourse relations inspired from (Asher and Las-
carides, 2003). Some connectors are ambiguous in that they are associated with several relations. We
used only the unambiguous connectors (263 in all) in LEXCONN, as a first step. We regrouped the
LEXCONN relations into classes
4
: explanation relations (parce que/because) and result (ainsi/thus)
form the causal class ; temporal relations (puis, apr?s que/then,after that) form the narration group. We
also considered other relations like contrast (mais/but), continuation (et, encore/and,again), background
(alors que/while), temporal location (quand, pendant que/when), detachment (de toutes fa?ons/anyway),
elaboration (en particulier/in particular), alternation (ou/or), commentary (au fait/by the way), rephras-
ing (du moins/at least), and evidence (effectivement/indeed).
We searched our syntactically parsed corpus for connectors. When a connector is found and its syn-
tactic category verified, if it is close enough to the root of the sentence (at most one dependency link
from the root), we look for an inter-sentential link. The first verb of our pair corresponds in this case
1. Available as an SQLite database at https://dl.dropboxusercontent.com/u/78938139/v2r_db
2. http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html or (Candito et al., 2010)
3. Freely available at : https://gforge.inria.fr/frs/download.php/31052/lexconn.tar.gz.
4. We illustrate each relation with examples of potentially ambiguous markers.
2185
to the last verb of the previous sentence in the case of connectors for narration, or to its main verb for
all the other relations. We search for the second verb in the pair within a window of two dependency
links after the connector. If the connector is not close enough to the root of the sentence, we look for
a intra-sentential link. In this case, we look for the two verbs of the pair in the same sentence within a
forward and backward window of two dependency links.
If two verbs are found, we examine their local context to better characterize their usage and to improve
our results. If one of the verbs is a modal or support verb, we look for the verb dependent on the modal
or support verb and use that as the verb in our pair (if it exists), while keeping the presence of the
support verb in memory. Unlike support verbs, we use the presence of a negation or a reflexive particle
in the local context to distinguish verbs with different meanings ; e.g., comprendre/understand vs. ne pas
comprendre/not understand, agir/act vs. s?agir/concern are all distinct entries. To get at different verb
senses, we search for idiomatic usage of prepositions using the Dicovalence resource (Van Den Eynde
and Mertens, 2010), which contains valency frames for more than 3700 simple French verbs. We also
use the Lefff resource (Sagot, 2010) to find idiomatic verbal locutions. We also encode other information
that do not lead to distinct lexical entries : tense, and voice.
Once we have obtained a list of verb pairs associated with
Relation Distribution
contrast 50,104%
cause 33,108%
continuation 8,243%
narration 6,362%
background 1,853%
temporal localisation 0.177%
detachement 0.149%
elaboration 0.002%
alternation 0.002%
TABLE 1 ? Distribution of relations in
V
2
R ;commentary, reformulation and
evidence occur with negligible fre-
quency.
a connector, we aggregate this data to get a list of triple types
(verb1, verb2, relation). Given that we have used only unam-
biguous connectors (so classified by LEXCONN), the associ-
ation of a relation with a connector is immediate. We asso-
ciate to each triple type the number of intra-sentential, inter-
sentential and total number of occurrences. The other features
mentioned above are stored in a separate table.
Our method has isolated more than 1 million distinct types
of triples for V
2
R and 2 million occurrences, of which 95% are
intra-sentential
5
. Among these triples, 6.2% have 5 or more
occurrences.
Table 1 summarizes the distribution of triples by relation
in V
2
R. Note that triples with contrast and causal relations
comprise the majority. This does not mean that these are the
most frequent relations in the corpus but only that they are the
most frequently marked by the connectors we considered. This
makes for a very different distribution than that of the French manually annotated discourse corpus Ann-
odis (Afantenos et al., 2012).
3 Measuring the association of a pair of verbs with a relation
In the last section we presented our extraction method. We now present the measures we have used to
rank verb pairs with respect to the strength of their association with a particular discourse relation. We
adapted versions of standard lexical association measures like PMI (pointwise mutual information) and
their variants, as well as some measures specific to the association of a causal relation between items (Do
et al., 2011). We also experimented with a new measure specifically designed for our knowledge base.
Measures of lexical association used in research on co-occurrences in distributional semantics pick
out significant associations, taking into account the frequency of the related items. We examined over
10 measures ; we discuss the ones with the best results (see section 4). One simple measure, PMI, and
its variants, normalized, local (Evert, 2005), discounted (Lin and Pantel, 2002), which are designed
to reduce biases in the original measure, work well. The idea behind PMI is to estimate whether the
probability of the co-occurrence of two items is greater than the a priori probability of the two items
appearing independently. In distributional semantics, the measure is also used to estimate the significance
of two items co-occurring with a particular grammatical dependency relation like the subject or object
relation between an NP and a verb. This use of PMI measures over triples in distributional semantics
fits perfectly with our task of measuring the significance of triples consisting of a pair of verbs and
5. The low proportion of inter-sentential occurrences comes from our conservative scheme for finding these occurrences,
which uses only those connectors at the beginning of the second sentence. Other schemes are possible but would, we fear,
introduce too much noise into the data.
2186
a particular semantic or discourse relation ; our PMI measures estimate whether the co-occurrence of
two items with a particular discourse relation is higher than the a priori probability of the three items
occurring independently. Our measures consider co-occurrences of two lexical items in a certain relation
denoted by an explicit discourse marker. PMI and normalized PMI are defined as :
PMI = log(
P (V
1
, V
2
, R)
P (V
1
)? P (V
2
)? P (R)
)
PMI _normalized =
PMI
?2 log(P (V
1
, V
2
, R))
Indeed, when we have a complete co-occurrence of the three items, we have : P (V
1
) = P (V
2
) =
P (R) = P (V
1
, V
2
, R), and PMI = ?2 log(P (V
1
, V
2
, R)). The values of normalized PMI lie between
?1 and 1, approaching ?1 when the items never appear together, taking the value 0 in the case of
independence, and the value 1 when they always appear together. We also considered a weighted PMI
measure (Lin and Pantel, 2002) that corrects the bias of PMI for rare triples.
A specificity measure (Mirroshandel et al., 2013), originally used to measure the precision of subcat-
egorization frames, also performed well :
specificity =
1
3
? (
P (V
1
, V
2
, R)
?
i
P (V
1
, V
i
, R)
+
P (V
1
, V
2
, R)
?
i
P (V
i
, V
2
, R)
+
P (V
1
, V
2
, R)
?
i
P (V
1
, V
2
, R
i
)
)
A version of Do et al. (2011)?s measure for triples involving causal relations did not fare so well on
other types of relation. The definition of the measure can be found in (Do et al., 2011).
6
Finally, we investigated a measure that evaluates the contribution of each element in the triple to the
significance measure (this measure is similar to specificity).
W
combined
(V
1
, V
2
, R) =
1
3
(w
V
1
+ w
V
2
+ w
R
)
with : w
V
1
=
P (V
1
,V
2
,R)
max
i
(P (V
i
,V
2
,R))
, w
V
2
=
P (V
1
,V
2
,R)
max
i
(P (V
1
,V
i
,R))
, and w
R
=
P (V
1
,V
2
,R)
max
i
(P (V
1
,V
2
,R
i
))
.
4 Evaluating extracted relations
We evaluated V
2
R in several ways ; we provided : (i) an intrinsic evaluation of the relations between
verbs (section 4.1) and (ii) an extrinsic evaluation where we evaluated the coverage of the resource on a
discourse annotated corpus and its potential to help in predicting discourse relations in contexts with no
explicit marking (section 4.2).
4.1 Intrinsic evaluation
Our intrinsic evaluation first evaluates the feasibility of assigning an ?inherent? semantic link to a verb
pair, independently of any linguistic context. For example, is it possible to judge that there is a typical
causality link between push and fall, in scenarios where they share some arguments (subject, object, ...),
these scenarios being left to the annotator?s imagination (section 4.1.1). In a second stage, we selected
several verb pairs linked with different relations in V
2
R, and 40 contexts in which these verbs occur
together in the original corpus, to judge the semantic link in context (section 4.1.2).
In both cases we restricted the study to three relation groups : causal, contrastive, and narrative. These
are the most often marked relations and correspond to different types of links with a meaningful semantic
aspect (as opposed to the ?continuation? relation for instance, which is often marked too).
4.1.1 Out of context evaluation
For out of context judgments, we adopted the following protocol : one of the authors chose for each
relation 100 verbs with equivalent proportions of good and bad normalized PMI scores. Then the other
6. We simplified their measure by ignoring IDF (inverse document frequency) and the distance between the verbs, as neither
measure applies to our task.
2187
three authors judged the validity of associating each of the 300 pairs with the corresponding relation,
without any knowledge of the source of these pairs.
We measured the inter-annotator agreements with Cohen?s Kappa (Carletta, 1996), which resulted in :
0.17 for cause, 0.42 for narration and 0.56 for contrast as mean values. If a 0.6 kappa serves a measure for
a feasible semantic judgment task, out of context judgments appear very difficult, with only contrastive
pairs as a relative exception. We decided to only consider judgments about contrast, after an adjudication
phase, and we evaluated the measures presented in section 3 to see if they could discriminate between
the two verb groups, those judged positively or negatively according to human annotations. A Mann-
Whitney U statistical test showed all of our measures to be discriminative, with the exception of raw
co-occurrence counts for which p>0.05.
4.1.2 In context evaluation
We also judged associations in context.
Verb pair translation association
/human
Cause
inviter/souhaiter invite/wish 12.8%
promettre/?lire promise/elect 25.6%
aimer/trouver like/find 38.5%
b?n?ficier/cr?er benefit/create 51.3%
aider/gagner help/win 53.8%
Contrast
proposer/refuser propose/refuse 59.0%
augmenter/diminuer increase/decrease 64.1%
tenter/?chouer try/fail 64.1%
gagner/perdre win/lose 71.8%
autoriser/interdire authorize/forbid 74.4%
Narration
parler/r?fl?chir speak/think 42.5%
acheter/essayer buy/try 70.0%
atteindre/traverser reach/cross 77.5%
commencer/finir begin/end 80.0%
envoyer/transmettre send/transmit 82.5%
TABLE 2 ? For each relation, the list of verb pairs manu-
ally evaluated in context (and an approximate translation),
and the association percentage resulting from the adjudi-
cated human annotation.
This task was easier and also gave more
fine-grained results, because with it we can
quantify the degree of association, and the
typicality of the link, as a proportion of con-
texts where the two verbs appear together
in a given semantic relation. We can then
observe if this proportion is correlated with
the association measures we already pre-
sented. Nevertheless, this is a costly way of
evaluating a verb pair, as we require a num-
ber of judgments on each pair. It is also not
easy to sample the possible pairs with dif-
ferent values to be able to observe signif-
icant correlations, because we cannot pre-
dict in advance how they will be judged by
the annotators.
We selected 40 contexts for each of the
15 pairs of verbs we chose, 5 for each of the
target relation (cause, narration, contrast).
Selected pairs range over different values of
normalized PMI, again chosen by one of the
authors independently of the others, who
annotated the 600 contexts. Prior to adjudi-
cation, raw agreement was 78% on average,
for an average kappa of 0.46 (and a max-
imum of 0.49). These values seem moder-
ately good, as the task is also rather diffi-
cult.
Table 2 shows the results after adjudication : for each pair, the proportion of contexts in which the
considered relation is judged to appear.
We computed two correlation values between the association ratio in contexts manually annotated
and each association measure considered : one based on all annotated contexts, and one on the subset
of contexts devoid of explicit markers of a semantic relation (implicit contexts). The latter is important
to quantify the actual impact of the method, since explicit marking is already used as the basis of verb
association in the same corpus. Implicit contexts, however, never appeared in the computation of the verb
pair associations.
2188
normalized
PMI
specificity W_combined
discounted
PMI
PMI
local
PMI
U_do
raw fre-
quency
Global
correlation
0.749 0.747 0.720 0.716 0.709 0.434 0.376 0.170
Correlation
for implicit
instances
0.806 0.760 0.738 0.761 0.756 0.553 0.499 0.242
TABLE 3 ? Pearson correlation for the 15 pairs considered and measures from section 3, in decreasing
order.
Table 3 shows that mutual information measures are well correlated with human annotations, and
that our W_combined seems useful too. We also observed results on each relation separately, although
one should be careful drawing conclusions from these results since the correlations are then computed
on 5 points only. These results (not shown here) show a lot of variation between relations. The U_do
measure, designed for causal relations, does indeed produce good results for these relations, but does not
generalize well to our other chosen relations.
Also, local PMI seems to work very well on narration and causal relations. This needs to be confirmed
with more verb pairs.
We conclude that the best three measures are : normalized PMI, specificity, and W_combined. The last
two assign their maximal value to several pairs, so we used them in a lexicographical ordering to sort all
associated pairs, using normalized PMI to break ties.
Verb pair Translation Relation
abandonner / mener abandon / lead background
ne pas s?arr?ter / rouler not stop / drive narration
donner satisfaction sur / r??lire give satisfaction concerning / re-elect continuation
emporter / ne pas cesser take away / not stop summary
emprunter / assurer borrow / insure cause
ne pas manquer / prolonger not miss / prolong detachment
ratifier / trembler ratify / tremble background
avoir honte / faire piti? be ashamed / cause pity cause
avoir droit / cotiser pour be entitled / contribute to temploc
ne pas repr?senter / st?r?otyper not represent / stereotype temploc
TABLE 4 ? Ten best triples in the database.
Table 4 shows the best triples with our lexicographical ranking.
4.2 Extrinsic evaluation
In order to evaluate the performance of our resource relative to its main intended application?
predicting rhetorical relations in text, we intend to use our association measures as additional features
to an inductive prediction model. Whether this evaluation produces results depends on the proportion
of cases in which this information could help and on the coverage of our resource with respect to these
cases. We used the Annodis corpus (Afantenos et al., 2012), a set of French texts annotated with rhetori-
cal relations, for our study.
To improve existing models, a significant number of the predictions to be made must involve a verb
pair for which we have information in the resource. A first indication of its usefulness is also that the
verb pair appears most frequently with the relation group to which the annotation belongs, for instance
the fact that two verbs are related with a causal relation whenever we want to predict an explanation. This
is interesting only in the absence of an explicit marking of the target relation, i.e for implicit relations.
2189
Beyond that, it should be interesting to use all the available information about other semantic relations
too : for instance a potential causal link between two events could indicate the relevance of a temporal link
for the prediction of a relation. We relied again on the Lexconn marker database. As an approximation
we considered that a relation between two discourse units is explicit when a Lexconn marker is present
in any of the two segments, and one of the potential senses of the marker is the annotated relation.
This may overestimate the number of explicit instances but ensures that all implicit instances are indeed
implicit (assuming a good enough coverage of the marker resource). The Annodis corpus lists rhetorical
relations between elementary discourse units (EDUs), typically clauses, and complex discourse units
(sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a main verb of
a complex unit is difficult to answer. This is a relatively small corpus, as it includes about 2000 instances
of relations between elementary discourse units.
Table 5 present results for coverage, for the main relations in the annotated corpus. Note that only a
small part of the set of relations between EDUs is considered when we restrict instances to both EDUs
with verbs (about 20% of the whole). It turns out that a lot of EDUs in Annodis are short segments
(incises, detached segments, ...).
global narration cause contrast elab. cont. BG other
Annodis pairs 427 73 67 41 96 92 24 16
Annodis pairs ? V
2
R 68.9 71.2 70.8 78.0 68.3 61.9 74.1 62.5
Annodis triples ? V
2
R 26.5 34.2 50.0 70.7 0.0 20.6 11.1 0.0
Implicit Annodis pairs 83.4 71.2 79.2 36.6 99.0 94.8 88.9 100.0
Implicit Annodis pairs ? V
2
R
(any relation)
56.9 52.1 54.2 31.7 67.3 58.8 66.7 62.5
Implicit Annodis triples ?
V
2
R (with correct relation)
17.7 24.7 40.3 31.7 0.0 19.6 11.1 0.0
TABLE 5 ? Coverage of verb pairs in V
2
R with respect to EDU pairs in the Annodis corpus containing
two verbs. Except for the first line, all numbers are percentages. Pair = verb pairs in the EDUs linked
by a rhetorical relation R, Triple=verb pair associated with a relation R in V
2
R, BG = Background,
cont.=continuation, elab.=elaboration.
Our table includes : the proportion of verb pairs found in Annodis EDUs that appear in V
2
R, the
proportion of triples from Annodis that appear in V
2
R (with the correct relation), and the restriction
of these proportions to implicit contexts in Annodis. Except for a few exceptions due to lemmatisation
errors, all verbs in Annodis are in V
2
R in at least one pair, and we can see that the pairs in V
2
R cover
most of the pairs appearing in Annodis (almost 70% globally and between 60 and 80% depending on the
relation), and a little less of implicit cases (around 55% on average). We note that a high proportion of the
implicit cases contains verb pairs that have been collected in a marked context, even for rarely marked
relations like elaboration or continuation?contexts with these relations are the majority in Annodis.
Furthermore more than half of these contexts are associated with the right relation in V
2
R. Thus the
hypothesis of the partial redundancy of connectors appears useful when isolating verbal associations
relevant for discourse from a large corpus. We also looked at semantic neighbors of the verbs in V
2
R but
this did not increase coverage significantly.
A good test of the predictive power of the semantic information we gathered is also to include the
association measures as additional features to a predictive model, to improve classically low results
on implicit discourse relations. The only available discursive corpus in French, Annodis, is small, and
as shown above only about 400 instances have a verb in both related EDUs. We trained and tested
a maximum entropy model with and without the association measures as features, on top of features
presented in Muller et al. (2012), who trained a relation model on the same corpus. We did a 10-fold
cross-validation on the 400 instance subset as evaluation, and did not find a significant difference between
the two set-ups (F1 score was in the range .40?.42, similar to the cited paper), which is unsurprising
2190
given the size of the subset. We plan to evaluate our method relative to discourse parsing by building an
English resource like V
2
R ; we will then be able to use the much larger PDTB corpus (10 times as large
as Annodis) as a source of implicit discourse relations. This should prove a much more telling evaluation
of the usefulness of association measures in predicting implicit discourse relations.
5 Related work
There are two different groups of related work. The first group aims to alleviate the lack of annotated
data for discourse parsing by using a weakly supervised approach, exploiting the presence of discourse
connectors in a large non-annotated corpus. Each pair of elementary discourse units is automatically
annotated with the discourse relation triggered by the presence of the connector (connectors are often
filtered for non-discursive uses). Those connectors are afterwards eliminated from the corpus so that the
model trained on this dataset will not be informed by the presence of those connectors. The pioneering
article in this group is Marcu and Echihabi (2002). Such learning methods with such ?artificial data?
obtain low scores, barely above chance as shown in Sporleder and Lascarides (2008). Braud and Denis
(2013) observe that the performance of a classifier for the prediction of implicit relations is much lower
when using ?artificial? data than on ?natural? data (implicit relations annotated by a human being). They
propose a method which exploits these two different kinds of datasets together in various mixtures and
on the level of the prediction algorithm, obtaining thus a significant improvement on the Annodis corpus.
Our approach is different and complementary ; we isolate the semantic relations between pairs of verbs.
We can use that as a feature on discourse units for discourse parsing but it has other uses as well.
A second group aims at identifying discourse relations (implicit or not) by focusing on the use of fine-
grained lexical relations as another feature during the training phase. Most of this work focuses mainly
on the use of lexical relations between two verbs. Chklovski and Pantel (2004), for example, rely on
specific patterns constructed manually for each semantic relation between (similarity, strength, antonymy,
enablement and temporal happens-before). They use the web as a corpus in order to estimate the PMI
between a pattern and a pair of verbs (a precise measurement cannot be achieved over the web since the
probability of a pattern is not precisely known over all the web). A threshold on the value of the PMI
(manually fixed) permits thus to determine the pairs of verbs that are related to the relation denoted by the
pattern. In the same spirit, Kozareva (2012) is using a weakly supervised approach for the extraction of
pairs of verbs that are potentially implied in a cause-effect relation. Her method consists in using patterns
applied to the web in order to extract pairs and generate new seeds. Do et al. (2011) focus on causal
relations and take into account not only verbs but also event denoting nouns. According to this paper,
an event is denoted by a predicate with a specific number of arguments and thus the association of the
events is the sum of the association between predicates, between predicates and arguments and between
arguments. Their association measures are based on PMI and are quite complex. Our results show that
their measures do not generalize well to association with all discourse relations. Using Gigaword as a
corpus and a reimplementation of Lin et al. (2014) they have extracted discourse relations. An inductive
logic programming approach is finally used exploiting the interaction between causal pairs and discourse
relations in order to extract causal links. Those papers focus on specific relations with the exception of
Chklovski and Pantel (2004) who do not present a systematic evaluation of their results. An important
difference of our approach is also to consider predicates and their negation as separate entries.
Finally, we mention the approaches which while focusing on the learning of discourse structures,
nonetheless enrich their systems with lexical information. Feng and Hirst (2012) have used HILDA (Her-
nault et al., 2010) adding more features. A specific family of features represents lexical similarity based
on the hierarchical distance in VERBNET and WORDNET. In a similar fashion, Wellner et al. (2006) fo-
cus on intra-sentential discourse relations adding lexical information on the features based on measures
proposed by Lin (1998) calculated on the British National Corpus. Those approaches use thus only infor-
mation on lexical similarity without semantically typing this link. The impact of this information seems
limited. As far as evaluation is concerned, our method is similar to that followed in Tremper and Frank
(2013) for implication relations combining in and out of context evaluation for verbal associations. Their
inter-annotator agreement is similar to ours (0.42-0.44 of Kappa) with very different choices : the anno-
2191
tators were supposed to discriminate verbal links between the different possible sub-cases. The pairs of
verbs were identified by the system of Lin and Pantel. These authors also present a classification model
among the different types of relationships, assuming that two verbs are semantically related.
6 Conclusions
We have presented a knowledge base of triples involving pairs of verbs associated with semantic or
discourse relations. We extracted these triples from the large French corpus, frWaC, using discourse con-
nectors as markers of relations between two adjacent clauses containing verbs. We investigated several
measures to give the strength of association of a pair of verbs with a relation. We used manual annotations
to evaluate our method and select the best measures, and we also studied the coverage of our resource on
the discourse annotated corpus Annodis. Our positive results show our resource has the potential to help
discourse analysis as well as other semantically oriented tasks.
2192
References
Stergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, Cecile Fabre, Mai Ho-Dac, Anne Le Draoulec,
Philippe Muller, Marie-Paul Pery-Woodley, Laurent Prevot, Josette Rebeyrolles, Ludovic Tanguy, Marianne
Vergez-Couret, and Laure Vieu. 2012. An empirical resource for discovering cognitive principles of discourse
organisation : the ANNODIS corpus. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U?gur
Do?gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight
International Conference on Language Resources and Evaluation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Studies in Natural Language Processing.
Cambridge University Press, Cambridge, UK.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of
the COLING-ACL, Montreal, Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web : a collection
of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3) :209?
226.
Chlo? Braud and Pascal Denis. 2013. Identification automatique des relations discursives "implicites" ? partir
de donn?es annot?es et de corpus bruts. In TALN - 20?me conf?rence du Traitement Automatique du Langage
Naturel 2013, volume 1, pages 104?117, Sables d?Olonne, France, June.
Marie Candito, Beno?t Crabb?, and Pascal Denis. 2010. Statistical french dependency parsing : Treebank conver-
sion and first results. In LREC.
Jean Carletta. 1996. Assessing agreement on classification tasks : the kappa statistic. Computational linguistics,
22(2) :249?254.
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proceedings
of ACL-08 : HLT, pages 789?797, Columbus, Ohio, June. Association for Computational Linguistics, Morris-
town, NJ, USA.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean : Mining the web for fine-grained semantic verb relations.
In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 33?40, Barcelona, Spain, July.
Association for Computational Linguistics.
P. Denis and B. Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging. Lan-
guage Resources and Evaluation, (46) :721?736.
Quang Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In Proceed-
ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303, Edinburgh,
Scotland, UK., July. Association for Computational Linguistics.
Stefan Evert. 2005. The statistics of word cooccurrences. Ph.D. thesis, Stuttgart University.
C. Felbaum. 1998. Wordnet, an Electronic Lexical Database for English. Cambridge : MIT Press.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1 : Long Papers),
pages 60?68, Jeju Island, Korea, July. Association for Computational Linguistics.
G. Grefenstette. 1994. Explorations in automatic thesaurus discovery. Springer.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama. 2009.
Large-scale verb entailment acquisition from the Web. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1172?1181, Singapore, August. Association for Computational
Linguistics.
Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. HILDA : A Discourse Parser
Using Support Vector Machine Classification. Dialogue and Discourse, 1(3) :1?33.
Zornitsa Kozareva. 2012. Cause-effect relation learning. In Workshop Proceedings of TextGraphs-7 : Graph-
based Methods for Natural Language Processing, pages 39?43, Jeju, Republic of Korea, July. Association for
Computational Linguistics.
Dekang Lin and Patrick Pantel. 2002. Concept discovery from text. In Proceedings of Coling 2002, pages 1?7.
Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2) :151?184.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th ACL and 17th
COLING joint conference, volume 2, pages 768?774, Montreal.
Maofu Liu, Wenjie Li, Mingli Wu, and Qin Lu. 2007. Extractive summarization based on event term clustering. In
Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions, pages 185?188, Prague, Czech Republic, June. Association for
Computational Linguistics.
2193
Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations.
In Proceedings of ACL, pages 368?375.
Seyed Abolghasem Mirroshandel, Alexis Nasr, and Beno?t Sagot. 2013. Enforcing subcategorization constraints in
a parser using sub-parses recombining. In Proceedings of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics : Human Language Technologies, pages 239?247, Atlanta,
Georgia, June. Association for Computational Linguistics.
Philippe Muller, Stergos Afantenos, Pascal Denis, and Nicholas Asher. 2012. Constrained decoding for text-
level discourse parsing. In Proceedings of COLING 2012, pages 1883?1900, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?lsen Eryigit, Sandra K?bler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser : A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13(2) :95?135.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie L. Webber.
2008. The Penn Discourse TreeBank 2.0. In Proceedings of LREC 2008.
Charlotte Roze, Laurence Danlos, and Philippe Muller. 2012. Lexconn : A french lexicon of discourse connectives.
Discours, (10).
Beno?t Sagot. 2010. The lefff, a freely available and large-coverage morphological and syntactic lexicon for
french. In 7th international conference on Language Resources and Evaluation (LREC 2010).
Caroline Sporleder and Alex Lascarides. 2008. Using Automatically Labelled Examples to Classify Rhetorical
Relations : An Assessment. Natural Language Engineering, 14(3) :369?416, July.
Galina Tremper and Anette Frank. 2013. A discriminative analysis of fine-grained semantic relations including
presupposition : Annotation and classification. Dialogue & Discourse, 4(2) :282?322.
Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013.
Semeval-2013 task 1 : Tempeval-3 : Evaluating time expressions, events, and temporal relations. In Second
Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2 : Proceedings of the Seventh
International Workshop on Semantic Evaluation (SemEval 2013), pages 1?9, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
K. Van Den Eynde and P. Mertens, 2010. Le dictionnaire de valence : Dicovalence. Leuven : Universit? de
Leuven. [http ://bach. arts. kuleuven. be/dicovalence/].
Ben Wellner, James Pustejovsky, Catherine Havasi, Anna Rumshisky, and Roser Saur?. 2006. Classification of
discourse coherence relations : an exploratory study using multiple knowledge sources. In Proceedings of
the 7th SIGdial Workshop on Discourse and Dialogue, SigDIAL ?06, pages 117?125, Stroudsburg, PA, USA.
Association for Computational Linguistics.
2194
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 357?368,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Grounding Strategic Conversation:
Using negotiation dialogues to predict trades in a win-lose game
Ana??s Cadilhac
IRIT
Univ. Toulouse, France
cadilhac@irit.fr
Nicholas Asher
IRIT, CNRS
Toulouse, France
asher@irit.fr
Farah Benamara
IRIT
Univ. Toulouse, France
benamara@irit.fr
Alex Lascarides
School of Informatics
Univ. Edinburgh, UK
alex@inf.ed.ac.uk
Abstract
This paper describes a method that predicts
which trades players execute during a win-
lose game. Our method uses data collected
from chat negotiations of the game The Set-
tlers of Catan and exploits the conversation
to construct dynamically a partial model of
each player?s preferences. This in turn yields
equilibrium trading moves via principles from
game theory. We compare our method against
four baselines and show that tracking how
preferences evolve through the dialogue and
reasoning about equilibrium moves are both
crucial to success.
1 Introduction
Rational agents act so as to maximise their expected
utilities?an optimal trade off between what they
prefer and what they believe they can achieve (Sav-
age, 1954). Solving a game problem involves find-
ing equilibrium strategies: an optimal action for
each player that maximises his expected utility, as-
suming that the other players perform their speci-
fied action (Shoham and Leyton-Brown, 2009). Cal-
culating equilibria thus requires knowledge of the
other players? preferences but almost all bargaining
games occur under the handicap of imperfect infor-
mation about this (Osborne and Rubinstein, 1994).
Players therefore try to extract their opponents? pref-
erences from what they say, likewise revealing their
own preferences in their own utterances. These
elicited preferences guide an agent?s decisions, like
choosing to make such and such a bargain with such
and such a person. Tracking preferences through
dialogue is thus crucial for analyzing the agents?
strategic reasoning in real game scenarios.
In this paper, we design a model that maps what
people say in a win-lose game into a prediction of
exactly which players, if any, trade with each other,
and exactly what resources they exchange. We use
both statistics and logic: we use a corpus of nego-
tiation dialogues to learn classifiers that map each
utterance to its speech act and to other acts perti-
nent to bargaining; and we develop a symbolic al-
gorithm that, from the classifiers? output, dynami-
cally constructs a model of each player?s preferences
as the conversation proceeds (for instance, the pref-
erence to receive a certain resource, or to accept a
certain trade). This preference model uses CP-nets
(Boutilier et al, 2004), a representation of prefer-
ences for which algorithms for computing equilib-
rium strategies exist. We adapt those algorithms to
predict the trades executed in the game.
The algorithm for construcing CP-nets uses only
the output of our classifiers, which in turn rely en-
tirely on shallow features in the raw text and robust
parsers. Together they provide an end to end model,
from raw text to a prediction of which trade, if any,
occurred. We evaluate the various components of
this (pipeline) algorithm separately, as well as the
end to end model.
Our study exploits a corpus of negotiation dia-
logues from an online version of the win lose game
The Settlers of Catan. Sections 2 and 3 describe
the corpus and its annotation. Section 4 introduces
our method for constructing the agents? preferences
from the dialogues. We use this in Section 5 to pre-
dict whether a trade is executed as a result of the
357
players? negotiations, and if so we predict who took
part in the trade, and what they exchanged. Our
method shows promising results, beating baselines
that don?t adequately track or reason about prefer-
ences. We compare our model to related work in
Section 6 and point to future work in Section 7.
2 The game
The Settlers of Catan (www.catan.com) is a win-
lose game that involves negotiations over restricted
resources. Each player (three or more) acquires re-
sources (of 5 types: ore, wood, wheat, clay, sheep),
which they use in different combinations to build
roads, settlements and cities, which in turn earns
them points towards winning. The first player to
10 points wins. Players acquire resources in sev-
eral ways, in particular through agreed trades with
other players. Some methods (e.g., robbing) are hid-
den from view, so players lack complete information
about their opponents? resources.
Our corpus contains conversations of humans
playing an online version of Settlers (Afantenos et
al., 2012). Players must converse in a chat inter-
face to carry out trades. Each game contains several
dozen self-contained bargaining dialogues. Our ex-
periments use 10 Settlers games, consisting of more
than 2000 individual dialogue turns (see Section 3).
Table 1 is a sample dialogue from the corpus. The
sentences in the corpus have a relatively simple syn-
tax, though many also exhibit long distance depen-
dencies. However, these conversations are pragmat-
ically complex. They exhibit complex anaphoric de-
pendences (e.g., utterance ID 4 in Table 1). Other
pragmatic inferences, which are dependent on rea-
soning about intentions, speech acts and discourse
structure, are also ubiquitous. For example, the
question Have you got any ore? implies an offer for
the speaker to receive ore in exchange for something
from someone unspecified, and its response I?ve got
wheat not only implies a willingness to exchange
wheat for something, but as a response to the ques-
tion it also implies a refusal to give any ore.
More generally, a dialogue turn in our corpus can
express an offer, a counteroffer, an acceptance or re-
jection of an offer, or a commentary on the above
or on moves in the game. All except the last pro-
vide clues about preferences: e.g., which players
a speaker wants to execute a trade with; or what
resources to exchange. For instance, the utterance
Anybody have any sheep for wheat? conveys sev-
eral preferences. First, it conveys the speaker?s pref-
erence to trade with someone unspecified. Other
informative but underspecified preferences include:
the speaker?s preference to acquire some sheep over
alternatives; and in a context where she receives
sheep, a preference to give away some of her wheat
over the alternatives. Crucially, it does not convey
a preference to give away wheat in a context where
she receives nothing or something other than sheep.
In line with a non-cooperative bargaining game,
the preferences and offers that a speaker reveals are
less specific than an executable trade requires, where
the trading partners and the type of resources offered
and received must all be defined. Such general dia-
logue moves are essentially information seeking?
evidence that humans playing Settlers have imper-
fect information about their opponents? preferences.
In fact, many offers to trade result in no trade be-
ing agreed to and executed. While observed negoti-
ation failure would be puzzling in a bargaining game
with perfect information (Osborne and Rubinstein,
1994), it occurs relatively frequently in Settlers.
3 Annotation
We have a multi-layered dialogue annotation
scheme that includes: (1) a pre-annotation that seg-
ments the dialogue into turns which are further seg-
mented into Elementary Discourse Units (EDUs)
with the author of each turn automatically given;
(2) a characterization of each EDU in terms of ba-
sic speech acts (assertion, question, request) as well
as dialogue acts that are specific to bargaining (of-
fers, counteroffers, etc.); and (3) associated infor-
mation about the givable and/or receivable resources
that EDUs express.
Two annotators received training on 77 dialogues,
totaling 699 EDUs. They then both annotated the
remaining dialogues independently (2741 EDUs and
511 dialogues in total). Kappas for inter-annotator
agreement are given below.
3.1 Dialogue act annotation (Kappa=0.79)
Each turn logs what a player enters in the chat win-
dow and also aspects of the game state at the time:
358
ID Dialogue Act Text Speaker Addressee Resource
1 Offer i need clay, any1 have? Rainbow All Receivable (clay, ?)
2 Refusal Nope, sorry inca Rainbow
3 Refusal Not at the moment, unfortunately. ariachiba Rainbow
4 Refusal need mine sorry Kittles Rainbow Not givable (Anaphoric, ?)
Anaphora Link:(mine , clay )
5 Offer no one has ore to giv? Rainbow All Receivable (ore, ?)
6 Accept oh yeah me Kittles Rainbow
7 Counteroffer ore for wheat again? Kittles Rainbow Givable (ore, ?) Receivable (wheat, ?)
8 Accept ya Rainbow Kittles
9 Accept ok Kittles Rainbow
Table 1: Example of an annotated negotiation dialogue.
his resources, the state of the game board and a time
stamp. The pre-annotation divides each turn into
EDUs. The annotators then have to specify the dia-
logue act of each EDU: Offer, Counteroffer, Accept
or Refusal (of an offer addressed to the emitter), and
Other. Other labels units that either comment on
strategic moves in the game or are not directly perti-
nent to bargaining. Annotators also specify the ad-
dressee of the EDU and its surface type: Question,
Request or Assertion.
3.2 Resource type annotation (Kappa=0.80)
Annotators also specify for each EDU and its dia-
logue act an associated feature structure, which cap-
tures (partial) information that the EDU expresses
about the type and quantity of resources that are of
the following four attributes: Givable, Not Givable,
Receivable or Not Receivable. These attributes can
take Boolean combinations of resources as values
via two operators AND and OR, that respectively
stand for conjunction (the agent expresses two pref-
erences and he prefers to achieve one of them if he
cannot have both, such as I need clay and wood) and
disjunction (free choice) of preferences (e.g., I can
give you clay or wood). We allow attributes to have
unknown values: the annotation tool inserts a ? in
these cases. We also insist that the annotators re-
solve anaphoric dependencies when specifying val-
ues to attributes, as shown in EDU (4) in Table 1.
4 Dialogue act and resource prediction
Predicting the executed trades from the dialogues
starts with three sub-tasks: automatically identify-
ing each EDU?s dialogue act; detecting the EDU?s
resources; and specifying the attributes of those re-
sources (i.e., Givable, Receivable, etc.).
4.1 Identifying dialogue acts
As is well established, one EDU?s dialogue act
depends on previous dialogue acts (Stolcke et al,
2000). In our corpus, Accept or Reject frequently
follow Offer and Counteroffer. Since labeling is se-
quential, we use Conditional Random Fields (CRFs)
to learn dialogue acts. CRFs have been shown to
yield better results in dialogue act classification on
online chat than HMM-SVN and Naive Bayes (Kim
et al, 2012).
We use three types of features: lexical, syntactic
and semantic. And we exploit them as unigrams and
bigrams: unigrams associate the value of the feature
with the current output class (level 0); bigrams take
account of the value of the feature associated with
a combination of the current output class and pre-
vious output class (level -1). 6 features were used
exclusively as unigrams: the EDU?s position in the
dialogue, its first and last words, its subject lemma,
a boolean feature to indicate if the current speaker is
the one that initiates the dialogue and the position of
the speaker?s first turn in the dialogue.
We have 15 unigram and bigram features (at lev-
els 0 and -1), as well as templates that combine
feature values for the two levels. These include
14 boolean features that indicate if the EDU con-
tains: bargaining verbs (e.g. trade, offer), refer-
ences to another player (e.g. you), resource tokens
as encoded in a task dedicated lexicon (e.g. wheat,
clay), quantifiers (e.g. one, none), anaphoric pro-
nouns, occurrences of ?for? prepositional phrases
(e.g. wheat for clay), acceptance words (e.g. OK),
negation words, emoticons, opinion words (from
(Benamara et al, 2011)), words of politeness, ex-
clamation marks, questions, and finally whether the
EDU?s speaker has talked previously in the dialogue.
359
The last feature gives the EDU speaker lemma. In
addition, 3 unigram and bigram booleans indicate
whether the current EDU contains the most frequent
tokens, couple of tokens and syntactic patterns in our
corpus. Finally, we use 2 composed bigram features
that encode whether the EDU contains an accep-
tance or refusal word, given that the previous EDU
is a question.
To assign sequential tags of dialogue acts within
a negotiation dialogue, we use the CRF++ tool
(crfpp.googlecode.com). Our data consists of
2741 EDUs in 511 dialogues. Each EDU is asso-
ciated with a dialogue act resulting in 410 Offer,
197 Counteroffer, 179 Accept, 398 Refusal and 1557
Other. We use 10-fold cross-validation to evalu-
ate our model, computing precision, recall and F-
score for each class and global accuracy from the
total number of true positives, false positives, false
negatives and true negatives obtained by summing
over all fold decisions. The results (in percent) are
given in Table 2 (MaF is the average of F-scores
of all the classes). Our model significantly out-
performs the frequency-based baseline (MaF=14.5;
Accuracy=56.8), with the best F-score achieved for
Other. The least good results are for the two least
frequent classes in our data. In addition to the fre-
quency problem, the lower score for Counteroffer is
mainly due to the model confusing it with Offer. Er-
rors in the Accept class were often due to misspelling
or to chat style conversation; e.g., kk, yup.
Dialogue act Precision Recall F-score
Other 87.4 93.1 90.1
Offer 80.0 81.0 80.5
Counterof. 64.8 53.3 58.5
Accept 65.1 53.1 58.5
Refusal 81.7 73.9 77.6
Macro-averaged F-score (MaF) 73.0
Accuracy 83.0
Table 2: Results for dialogue act classification.
4.2 Finding resource text spans
Since the resource vocabulary in The Settlers of
Catan is a closed set composed of words denoting
specific resources (e.g., clay, wood) and their syn-
onyms (brick), we use a simple rule to detect them:
a noun phrase (NP) is a resource text span if and
only if it contains a lemma from our resource lexi-
con. A closed set resource vocabulary is common to
many different types of negotiation dialogues. We
used the Stanford parser (Klein and Manning, 2003)
to obtain the NPs: there are 4361 NPs, where (by the
gold standard annotations) 21% are resources and
79% are not. We obtain an F-score of 96.9% and ac-
curacy of 97.9%, clearly beating both the frequency
and random baselines for this task.
4.3 Recognizing the type of resources
Recall that each resource within an EDU can be the
value of four types of attributes: Givable, Receiv-
able, Not Givable or Not Receivable (cf. Section
3.2). We predict these attributes using CRFs with the
following features. 8 features are used as unigram at
the current and the previous EDU level: the speaker,
the EDU?s subject, the dialogue act, and (if present)
the lemma of a bargaining verb, and 4 boolean fea-
tures indicate if the EDU contains an opinion word,
a reference to another speaker, if the resource comes
after a ?for? and if it contains a refusal word. These
features also serve as bigrams at the current EDU
level. Additionally, we have a set of unigram and
bigram boolean features that indicate if the current
EDU contains the most frequent verbs in the corpus.
And finally, we use a feature that encodes the com-
bination subject/bargaining verb in the current EDU.
We used CRF++ to implement our classifier. Our
corpus data consists of 1077 Resources, split into
510 Receivable, 432 Givable, 116 Not Givable and
19 Not Receivable. We use again 10-fold cross-
validation to evaluate our model and compute the
results by summing over all fold decisions. We
present them (in percent) in Table 3. They beat
the frequency-based baseline (MaF=16.1; Accu-
racy=47.4), although performance on the Not Re-
ceivable class is poor probably due to its low fre-
quency in the data.
Ambiguities make this task challenging. For in-
stance, anyone wheat for clay? can mean that the
speaker wants to receive wheat and give clay or the
opposite, and resolving which meaning is intended
involves reasoning not only with the previous and/or
the following EDU, but also sometimes EDUs with
long distance attachments, which are not supported
by our classifier and require a full discourse parser.
360
Res. type Precision Recall F-score
Receivable 66.8 71.4 69.0
Givable 62.6 59.7 61.1
Not Giv. 88.1 89.7 88.9
Not Rec. 0 0 0
Macro-averaged F-score (MaF) 54.8
Accuracy 67.4
Table 3: Results for resource type classification.
5 Predicting Players? Strategic Actions
We aim to capture the evolution of commitments to
certain preferences as the dialogue proceeds so as
to predict the agents? bargaining behavior. In other
words, we wish to predict which of the 61 possi-
ble trade actions is executed at the end of each dia-
logue. The possible trades vary over which partner
the player whose turn it is trades with (3 options in a
4 player game), the resources exchanged (assuming
each partner gives one type of resource and receives
another type yields 5?4 = 20 possibilities), or there
is no trade; i.e., (3 ? 20) + 1 = 61 possible actions
in the hypothesis space (we predict the types of re-
sources that are exchanged, but not their quantity).
We predict the executed action by identifying the
equilibrium trade entailed by the model of the play-
ers? preferences, which in turn we construct dynam-
ically from the output of the classifiers in Section 4.
We use the attributes of resources in the EDUs (Giv-
able, etc.) to identify the preference that a speaker
conveys in the EDU, and we use the dialogue acts
(Offer, Accept, etc.) to update a model of the pref-
erences expressed so far in the dialogue with this
new preference (see Section 5.2). Our model of
preferences consists of a set of partial CP-nets, one
for each player (see Section 5.1 for details). The
resulting CP-nets are then used to infer the exe-
cuted trading action (if any) automatically, via well-
understood principles from game theory for identi-
fying rational behavior (Bonzon, 2007).
5.1 CP-Nets
Following Cadilhac et al (2011), we use CP-nets
(Boutilier et al, 2004) to model preferences and
their dependencies. CP-nets are compatible with the
kind of partial information about preferences that ut-
terances reveal, and inference with CP-nets is com-
putationally efficient.
Just as Bayesian nets are a graphical model that
exploits probabilistic conditional independence to
provide a compact representation of a joint probabil-
ity distribution (Pearl, 1988), CP-nets are a graphi-
cal model that exploits conditional preferential in-
dependence to provide a compact representation of
the preference order over all outcomes. The CP-
net structures the decision maker?s preferences un-
der a ceteris paribus assumption: outcomes are com-
pared, other things being equal.
More formally, let V be a finite set of variables
whose combination of values determine all out-
comes O. Then a preference relation  over O is
a reflexive and transitive binary relation with strict
preference  defined as: o  o? and o? 6 o. Indif-
ference, written o ? o?, means o  o? and o?  o.
Definition 1 defines conditional preference indepen-
dence and Definition 2 defines CP-nets: the graphi-
cal component G of a CP-net specifies for each vari-
able X ? V its parent variables Pa(X) that affect
the agent?s preferences over the values of X , such
thatX is conditionally preferentially independent of
V \ ({X} ? Pa(X)) given Pa(X).
Definition 1 Let V be a set of variables, each vari-
able Xi with a domain D(Xi). Let {X,Y, Z} be
a partition of V . X is conditionally preferentially
independent of Y givenZ if and only if ?z ? D(Z),
?x1, x2 ? D(X) and ?y1, y2 ? D(Y ), x1y1z 
x2y1z iff x1y2z  x2y2z.
Definition 2 NV = ?G, T ? is a CP-net on variables
V , where G is a directed graph over V , and T is a
set of Conditional Preference Tables (CPTs). That
is, T = {CPT(Xj): Xj ? V }, where CPT(Xj)
specifies for each combination p of values of the par-
ent variables Pa(Xj) either p : xj  xj , p : xj 
xj or p : xj ? xj where the ?? symbol sets the vari-
able to false.
We discuss below how a CP-net predicts rational
action, but first we describe how CP-nets are con-
structed from the dialogues. In the Settlers cor-
pus, preferences involve a quadruplet (o, a, <r,q>)
where: o is the preference owner, a is the ad-
dressee, r is the resource and q is its quantity. So
each variable in the CP-nets we construct is such a
quadruplet, and for each variable the possibles val-
ues are Givable (Giv), Not Givable (Giv), Receiv-
361
able (Rcv) and Not Receivable (Rcv).
For example, the utterance Anyone want to give
me a wheat for a clay? expresses two prefer-
ences: one for receiving wheat, represented by the
variable Pw = (A,All,<wheat,1>); and given this
preference, another for giving clay, represented by
Pc = (A,All,<clay,1>) (where A is the name of the
speaker). The corresponding CP-Net is Figure 1.
Pw
Pc
CPT(Pw) = Rcv  Rcv
CPT(Pc) = Rcv Pw : Giv  Giv
Figure 1: An example CP-net
5.2 Modeling players? preferences
As stated above, we first automatically acquire a CP-
net from each EDU by using the EDU?s dialogue act
and the attributes (Givable, etc.) of its resources.
We then apply the rules presented in (Cadilhac et al,
2011) to dynamically construct a preference model
of the dialogue overall: this uses an equivalence
between their coherence relations and our dialogue
acts. Our CP-nets reasoning model handles uncer-
tain information and noise because it use as input
only the outputs of the statistical models described
in Section 4, and these prior models handle uncer-
tain information and noise. The symbolic rules for
constructing CP-nets have complete coverage over
any possible combination of classes that are output
by the statistical models, and so they are robust. We
give our rules below where pii stands for EDU ID i.
Offers. Because an Offer may specify or refine an
existing preference or offer, we must model how the
preferences expressed in an EDU that?s an Offer up-
dates the prior declared preferences. So, while our
annotations treat Offer as a property of EDUs, we
treat them here as binary relations: Offer(pi1, pi2),
where the second term, pi2, is the actual EDU whose
dialogue act is Offer and pi1 is the set of EDUs oc-
curring between pi2 and the last EDU uttered by
the same speaker. Offers then have a similar effect
on the CP-net as the coherence relation Elaboration
presented in (Cadilhac et al, 2011). That is, to auto-
matically update the CP-net constructed so far with
a current EDU that?s an Offer, the two step rule for
Offer(pi1, pi2) is:
1. to update the speaker?s CP-net according to the
preferences expressed in pi1, and
2. if pi2 expresses preferences, to enrich the CP-
net with these new preferences so that each
variable in pi2 depends on each variable in pi1.
Counteroffers. They specify or modify the terms
of a previous Offer or Counteroffer. Their purpose
is to give new information to refine the negotiation.
Like Offers they must also receive a contextually de-
pendent interpretation. The rule is quite similar to
that for Offer; however, Counteroffer can modify or
correct elements in a previously introduced offer. So
for Counteroffer(pi1, pi2), the rule is :
1. to partially update the speaker?s CP-net accord-
ing to the preferences expressed in pi1 which do
not have the same resource type (Givable, Re-
ceivable) than the ones in pi2.
2. same as step 2 Offer rule.
Accepts and Refusals. As they are answers to
Offers and Counteroffers, they behave like question
answer pairs (QAPs) presented in (Cadilhac et al,
2011). Because we are not doing full discourse pars-
ing, we once again approximate its effects by mak-
ing Accepts and Refusals respond to the set of EDUs
between the current EDU and the speaker?s last turn.
Accepts are positive responses to Offers or Coun-
teroffers and are de facto similar to QAP(pi1, pi2)
where pi2 is Yes. Thus, the rule is, as for Offer, to
update and enrich the CP-net.
Refusals are instead negative responses and be-
have like QAP(pi1, pi2) where pi2 is No. For
Refusal(pi1, pi2), there is no update of the prefer-
ences expressed in pi1. Instead, we enrich the CP-net
with the Non Givable and Non Receivable informa-
tion obtained from the negation of the preferences
expressed in the previous Offer or Counteroffer. We
then enrich the CP-net based on any new preferences
expressed in pi2. If there is a conflict between the
value of a variable to be updated and the current
value in the CP-net, we apply the Correction rule:
all occurrences of the old value are replaced by the
new value in pi2.
Other. This category pertains to content that does
not directly relate to trading in the game, and so we
choose to ignore resources expressed in the EDUs
with this dialogue act.
At the end of the negotiation dialogue, to predict
exactly what trade is executed (if any), the method
362
checks if there are complete and reciprocal prefer-
ences expressed in the CP-nets that respectively rep-
resent the declared preferences of two agents A and
B. This is done in two steps. First, we use the logic
of CP-nets to determine each agent?s best outcome
bestOA and bestOB from their respective CP-nets
(we?ll discuss how shortly). Secondly, we compare
these best outcomes: if they correspond to the same
trade, we predict that this trade was executed; if
not, we predict no trade is executed. Specifically,
bestOA (resp. bestOB) corresponds to a prefer-
ence for receiving a resource r1 from an agent B
(or from all the agents indifferently) and for giving
a resource r2 to this (or these) agent(s). We predict
that A gives B r2 and B gives A r1 if and only if:
bestOA = Rcv(A, B, r1) ? Giv(A, B, r2) and
bestOB = Rcv(B, A, r2) ?Giv(B, A, r1).
The first step?computing each agent?s best out-
come from his CP-net?can be found in linear time
using the forward sweep algorithm (Boutilier et al,
2004): sweep through the CP-net?s graph from top to
bottom, instantiating each variable with its preferred
value, given the values that are (already) assigned to
its parents. This algorithm is sound with respect to
the semantics of CP-nets.
Example. We apply this method for constructing
CP-nets and determining the executed trade to the
negotiation dialogue presented in Table 1.
pi1 The EDU is an Offer, so Rainbow?s CP-net is
updated according to pi1?s content.
CPT(R,All,<clay,?>) = Rcv  Rcv
pi2 It?s a Refusal, so we update inca?s CP-net with
the negation of the preferences expressed in Rain-
bow?s offer.
CPT(I,R,<clay,?>) = Giv  Giv
pi3 Idem for ariachiba.
CPT(A,R,<clay,?>) = Giv  Giv
pi4 Idem for Kittles where the preferences ex-
pressed in this EDU are redundant with the negation
of the preferences in Rainbow?s offer.
CPT(K,R,<clay,?>) = Giv  Giv
pi5 It?s an Offer, so Rainbow?s CP-net is first up-
dated according to previous EDUs (pi2 to pi4 until his
last speaking), then according to the content of pi5.
CPT(R,All,<clay,?>) = Rcv  Rcv (inactive)
CPT(R,I,<clay,?>) = Rcv  Rcv
CPT(R,A,<clay,?>) = Rcv  Rcv
CPT(R,K,<clay,?>) = Rcv  Rcv
CPT(R,All,<ore,?>) = Rcv(R,I,<clay,?>) ? Rcv(R,A,
<clay,?>) ? Rcv(R,K,<clay,?>): Rcv  Rcv
The introduction of the preference to receive ore
conflicts with the prior one for receiving clay. So
the method adds to the associated CPT the label ?in-
active? to indicate that this is older and should be
ignored if the preference about ore is satisfied.
pi6 The EDU is an Accept, so Kittles?s CP-net is
updated according to previous EDUs (only pi5).1
CPT(K,R,<ore,?>) = Giv(K,R,<clay,?>): Giv  Giv
pi7 The EDU is a Counteroffer. Since she is the
last speaker, her CP-net gets updated only according
to the content of the current EDU, to obtain:
CPT(K,R,<ore,?>) = Giv(K,R,<clay,?>): Giv  Giv
CPT(K,R,<wheat,?>) = Giv(K,R,<clay,?>) ?
Giv(K,R, <ore,?>) : Rcv  Rcv
pi8 The EDU is an Accept, so Rainbow?s CP-net
is updated according to previous EDUs (pi6 and pi7):
CPT(R,K,<ore,?>) = Rcv(R,I,<clay,?>) ? Rcv(R,A,
<clay,?>) ? Rcv(R,K,<clay,?>) : Rcv  Rcv
CPT(R,K,<wheat,?>) =Rcv(R,I,<clay,?>) ?Rcv(R,A,
<clay,?>) ? Rcv(R,K,<clay,?>) ? Rcv(R,K,<ore,?>) :
Giv  Giv
pi9 It?s an Accept with nothing new to update.
At the end of the dialogue, these agents? CP-nets
(correctly) predict that Kittles gave ore to Rainbow
in exchange for wheat.
5.3 Evaluation and results
We compare our model against four baselines. Since
none of these baselines support reasoning about
equilibrium moves, they all rely on the presence of
an Accept act to predict there was a trade, and its
absence to predict there wasn?t. The baselines dif-
fer, however, in how they identify the trading part-
ners and resources in an executed trade. The first
baseline predicts a trade according to the first Of-
fer and the last person to Accept, and if the Offer
doesn?t specify one of the resources then it is chosen
randomly (similar random choices complete all par-
tial predictions in all the models we consider here):
e.g., for Table 1 this would predict that Kittles gave
clay to Rainbow (which is incorrect) in exchange for
1Due to lack of space, in the following CP-nets, we do not
copy the inactive CPTs and CPTs about Not Givable or Not Re-
ceivable resources.
363
something that?s chosen randomly (which will prob-
ably be incorrect). The second baseline uses the
last Offer and the last person to Accept: e.g., for
Table 1 this predicts that Kittles gave ore to Rain-
bow (correct) for something random (probably in-
correct). The third baseline uses the last Offer or
Counteroffer, whichever is latest, and the last per-
son to Accept: e.g., for Table 1 this correctly pre-
dicts that Kittles gave ore to Rainbow in exchange
for wheat. And the fourth baseline, uses default
unification between the prior Offers or Counterof-
fers and the current one to resolve any of the cur-
rent offer?s elided parts and to replace specific val-
ues in prior offers with conflicting specific values in
the current offer (Ehlen and Johnston, 2013). One
then takes the executed trade to be the result of this
unification process at the point where the last Accept
occurs. This makes the same predictions as the third
baseline for Table 1, but outperforms it in the corpus
example (1) by predicting the correct and complete
trade (i.e., Rainbow gave Kittles sheep for wheat,
rather than for something random):
(1) Rainbow: i need clay ore or wheat
Kittles: i got wheat
Rainbow: i cn giv sheep
Kittles: ok
We performed the evaluation on the data pre-
sented in Sections 3 and 4: 254 dialogues in total
since we ignore dialogues that contain only Others.
90 of these dialogues end with a trade being exe-
cuted and 2 of them end with 2 trades. A random
baseline would give 1.6% accuracy (given the 61
possible trading actions) and a frequency baseline
(always choose no trade) gives 64.1% accuracy.
Table 4 presents the accuracy figures for all the
models when calculated from the gold standard la-
bels rather than the classifiers? predicted labels from
Section 4, so that we can compare the models in
isolation of the classifiers? errors. McNemar?s test
shows that our model significantly outperforms all
the baselines (p < 0.05). A predicted trade counts as
correct only if it specifies the right participants and
the correct type of resources offered and received
(we ignore their quantity). True Positives (TP) are
thus examples where the model correctly predicts
not only that a trade happened, but also the correct
partners and resources; Wrong Positives (WP), on
the other hand, constitute a correct prediction that
there was a trade but errors on the partners and/or
resources involved (so WPs undermine accuracy).
True Negatives (TN) are examples where the model
correctly predicts there was no trade (so TPs and
TNs contribute to accuracy). False Positives (FP)
and False Negatives (FN) are respectively incorrect
predictions that there was a trade, or that there was
no trade.
While Table 4 does not reflect this, the first three
baselines tend to predict incomplete information
about the trade even when what they do predict is
correct: that is, they predict the correct addressee
and the owner but resort to random choice for a re-
source that?s missing from the Offer or Counterof-
fer that predicts which trade occurred. For the first
baseline 34 examples are like this; for the second
and third baselines it?s 32. In contrast, this prob-
lem occurs only once with the fourth baseline, and
all the trades predicted by our method are complete,
making random choice unnecessary. Moreover, the
first three baselines often make incorrect predictions
about the addressee or resources exchanged because
in contrast to our model and the fourth baseline, they
don?t track how potential trades evolve through a se-
quence of offers and counteroffers.
Even though the fourth baseline, which uses de-
fault unification to track the content of the current
offer, is smart and gives good results, it has statis-
tically significant lower accuracy than our model.
One major problem with the fourth baseline is that,
in contrast to our model, it does not track each
player?s attitude towards the current offer. Instead,
like all our baselines, it relies on the presence of an
Accept act to predict that there?s a trade.2 But sev-
eral corpus examples are like (2), in which a trade
is executed but there?s no Accept act, thus yielding a
False Negative (FN) for all four baselines:
(2) Joel: anyone have sheep or wheat
Cardlinger: neither :(
Joel: will give clay or ore
Euan: not just now
Jon: got a wheat for a clay
(Joel gives clay to Jon and receives wheat)
2We tried a baseline that doesn?t rely on the presence of an
Accept act, but rather predicts a trade whenever default unifica-
tion yields a complete offer. It performed worse than the fourth
baseline.
364
So overall, our analysis shows that using CP-nets
significantly outperforms all baselines that don?t
model how preferences evolve in the dialogue, and
error analysis yields evidence that our model outper-
forms the fourth baseline because our model sup-
ports reasoning about player preferences, rational
behavior and equilibrium strategies.
1st baseline: first Offer/last Accept
TP FP FN TN WP Accuracy
24 14 30 150 38 68.0
2nd baseline: last Offer/last Accept
TP FP FN TN WP Accuracy
29 6 32 158 31 73.0
3rd baseline: last (Counter)Offer/last Accept
TP FP FN TN WP Accuracy
39 4 23 160 30 77.7
4th baseline: default unification
TP FP FN TN WP Accuracy
64 4 23 160 5 87.5
Our method
TP FP FN TN WP Accuracy
75 4 15 160 2 91.8
Table 4: Results for trade prediction. TP, FP, FN, TN
and WP are the True and False Positives, False and True
Negatives and Wrong Positives.
Table 5 presents the results for the end to end
evaluation, where trade predictions are made from
the classifiers? output from Section 4 rather than the
gold standard labels. As expected, performance de-
creases due to the classifiers? errors, mainly on the
type of resources (Givable, etc.). But our method
still significantly outperforms all the baselines with
an accuracy of 73.4% when the baselines obtain val-
ues between 60.9% and 68.4%.
4th baseline: default unification
TP FP FN TN WP Accuracy
23 12 37 152 32 68.4
Our method
TP FP FN TN WP Accuracy
34 10 43 154 15 73.4
Table 5: Results for the end to end trade prediction.
6 Related Work
6.1 Dialogue act modeling
Most work on dialogue act modeling focuses on spo-
ken dialogue (Stolcke et al, 2000; Ferna?ndez et al,
2005; Keizer et al, 2002). But live chats introduce
specific complications (Kim et al, 2012): ill-formed
data, abbreviations and acronyms, emotional indi-
cators and entanglement (especially for multi-party
chat). Among related work in this emerging field,
Joty et al (2011) use unsupervised learning to model
dialogue acts in Twitter, Ivanovic (2008) and Kim et
al. (2010) analyze one-to-one online chat in a cus-
tomer service domain, and Wu et al (2002) and Kim
et al (2012) predict dialogue acts in a multi-party
setting. We used a similar classifier to predict dia-
logue acts as the one reported in (Kim et al, 2012)
and evaluation yields similar results.
This paper proposes an approach to dialogue act
identification in online chat that aims to predict
strategic actions like bargaining. Compared to (Sid-
ner, 1994) and DAMSL (Core and Allen, 1997), our
domain level annotation is much more detailed: we
not only predict moves like Accept but also features
like the Givable and Receivable resources. Our gen-
eral speech act typology of EDUs lacks intentional
descriptions of speech acts, however. This reflects
a conscious choice to specify the semantics of each
act purely by the public commitments made to offer
or to receive goods.
6.2 Preference extraction
While preference extraction from non-linguistic ac-
tions is well studied (Chen and Pu, 2004; Fu?rnkranz
and Hu?llermeier, 2011), their extraction from spon-
taneous conversation has received little attention. To
our knowledge, the only existing work is (Asher
et al, 2010; Cadilhac et al, 2011; Cadilhac et al,
2012) which we build on. Cadilhac et al (2011)
compute CP-nets from coherence relations, found in
the annotation of the Verbmobil corpus (Baldridge
and Lascarides, 2005). Here we adapt their algo-
rithm from coherence relations to unary dialogue
acts. Further, while they assume that preferences are
given, here we apply versions of the NLP techniques
from Cadilhac et al(2012) to estimate the prefer-
ences of EDUs automatically. And we go further
than any of these works by using the elicited pref-
365
erences to infer the domain-level actions that result
from information exchanged in the conversation.
In this respect, our work relates to models for
grounding language, where semantic parsing tech-
niques are used to automatically map linguistic in-
structions to domain-level actions (Artzi and Zettle-
moyer, 2013; Kim and Mooney, 2013). Our do-
main of application is more challenging, however:
to our knowledge, this is the first attempt to map
non-cooperative dialogues into predictions about
domain-level actions. We can tackle these strategic
scenarios because we exploit a logic of preferences
as part of our model, yielding inferences about ratio-
nal action even when agents? preferences conflict.
Compared to previous work, our task is new. Our
aim is not to predict what dialogue act to perform
next, but what non verbal action should be per-
formed, mapping dialogue acts to non verbal ac-
tions. The difference between our work and other
work on grounding is that we are grounding non-
cooperative dialogue rather than instructions in a co-
operative setting. There is no prior work of which
we?re aware that maps a non-cooperative dialogue
into a prediction about which joint non-verbal ac-
tion the agents will do as a result of what they?ve
learned about their opponent through conversation.
Furthermore, both the CP-net and the fourth base-
line, whose accuracy is quite high (making it a hard
baseline to beat), use the dialogue history as they in-
crementally build up the preference model.
6.3 Predicting strategic actions
Modeling player behavior in real-time strategy
games is a growing research area in AI. These mod-
els can be used to identify common strategic states,
discover new strategies as they emerge or predict
an opponents future actions and so help players to
optimize their choices. For example, Schadd et
al. (2007) develop a hierarchical opponent model in
the game Spring, Dereszynski et al (2011) reason
about strategic behavior in StarCraft using hidden
Markov models and Amato and Shani (2010) use re-
inforcement learning to acquire a policy for switch-
ing among high-level strategies in Civilization IV.
In comparison, we propose a novel approach for
predicting strategic action based on the symbolically
formalized preferences that each agent commits to in
spontaneous conversation. Our approach thus deals
with imperfect information by exploiting the agents?
declared preferences. By predicting what bargain (if
any) will take place, we are able to verify the cor-
rectness of our preference descriptions. Our task is
a subtask of learning a strategy over an entire game
space, but our approach yields good predictive re-
sults on relatively little data?an advantage of ex-
ploiting CP-nets and the symbolic rules that guide
their evolution from observable evidence.
7 Conclusion
We have proposed a linguistic approach to strategy
prediction in spontaneous conversation, exploiting
dialogue acts to build a partial model of the agents?
declared preferences. Our method tracks how pref-
erences evolve during the dialogue, which we use to
infer their bargaining behavior, i.e. what resources,
if any, are exchanged, and by whom.
We based our study on a corpus collected using an
online version of The Settlers of Catan. Negotiations
in this game mirror complex real life negotiations
and provide a fruitful arena to study strategic con-
versation. Evaluation shows that our approach pro-
vides more accurate and complete information about
trades than baselines that don?t track how an offer
evolves through the dialogue, and we also argued
that game-theoretic reasoning about rational behav-
ior has advantages over relying on the presence or
absence of an Accept act to make predictions.
Our approach, however, does not exploit dis-
course structure, which is needed to properly handle
long distance dependencies of offers on prior mate-
rial. We will exploit this in future work to improve
our results. We also plan to investigate other aspects
of strategic reasoning on a larger dataset.
We have proposed a method that relies on a typol-
ogy of dialogue acts that is domain sensitive. How-
ever, in other work we have shown how to adapt
our algorithms to several domains (Cadilhac et al,
2012). In future work, we plan to link our prefer-
ence extraction algorithms to an automatically ac-
quired discourse structure for a given text. This will
provide a domain independent means for extracting
preferences from dialogue.
Acknowledgments
This work is supported by ERC grant 269427 STAC.
366
References
Stergos Afantenos, Nicholas Asher, Farah Benamara,
Ana??s Cadilhac, Ce?dric De?gremont, Pascal Denis,
Markus Guhe, Simon Keizer, Alex Lascarides, Oliver
Lemon, Philippe Muller, Soumya Paul, Verena Rieser,
and Laure Vieu. 2012. Developing a corpus of strate-
gic conversation in the settlers of catan. In Pro-
ceedings of the 1st Workshop on Games and NLP
(GAMNLP-12).
Christopher Amato and Guy Shani. 2010. High-
level reinforcement learning in strategy games. In
Proceedings of the 9th International Conference on
Autonomous Agents and Multiagent Systems (AA-
MAS?10), pages 75?82.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1:49?62.
Nicholas Asher, Elise Bonzon, and Alex Lascarides.
2010. Extracting and modelling preferences from dia-
logue. In IPMU, pages 542?553.
Jason Baldridge and Alex Lascarides. 2005. Annotating
discourse structures for robust semantic interpretation.
In Proceedings of the 6th IWCS.
Farah Benamara, Baptiste Chardon, Yannick Mathieu,
and Vladimir Popescu. 2011. Towards context-based
subjectivity analysis. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1180?1188, Chiang Mai, Thailand.
Elise Bonzon. 2007. Mode?lisation des interactions en-
tre agents rationnels : les jeux boole?ens. PhD thesis,
Universite? Paul Sabatier, Toulouse.
Craig Boutilier, Craig Brafman, Carmel Domshlak, Hol-
ger H. Hoos, and David Poole. 2004. Cp-nets: A tool
for representing and reasoning with conditional ceteris
paribus preference statements. Journal of Artificial In-
telligence Research, 21:135?191.
Ana??s Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2011. Commitments to preferences
in dialogue. In Proceedings of SIGDIAL, pages 204?
215. ACL.
Ana??s Cadilhac, Nicholas Asher, Farah Benamara,
Vladimir Popescu, and Mohamadou Seck. 2012. Pref-
erence extraction from negotiation dialogues. In Eu-
ropean Conference on Artificial Intelligence (ECAI),
pages 211?216. IOS Press.
Li Chen and Pearl Pu. 2004. Survey of preference elici-
tation methods. Technical report.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In Work-
ing Notes of the AAAI Fall Symposium on Communica-
tive Action in Humans and Machines.
Ethan W. Dereszynski, Jesse Hostetler, Alan Fern,
Thomas G. Dietterich, Thao-Trang Hoang, and Mark
Udarbe. 2011. Learning probabilistic behavior mod-
els in real-time strategy games. In AIIDE.
Patrick Ehlen and Michael Johnston. 2013. A multi-
modal dialogue interface for mobile local search. In
IUI Companion, pages 63?64.
Raquel Ferna?ndez, Jonathan Ginzburg, and Shalom Lap-
pin. 2005. Using machine learning for non-sentential
utterance classification. In Proceedings of the 6th SIG-
dial Workshop on Discourse and Dialogue, pages 77?
86.
Johannes Fu?rnkranz and Eyke Hu?llermeier, editors.
2011. Preference Learning. Springer.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts. In
Masters thesis, The University of Melbourne.
Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in asyn-
chronous conversations. In Proceedings of the 22nd
International Joint Conference on Artificial Intelli-
gence, pages 1807?1813.
Simon Keizer, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with bayesian net-
works for dutch dialogues. In Proceedings of the 3rd
SIGdial Workshop on Discourse and Dialogue, pages
88?94. Association for Computational Linguistics.
Joohyun Kim and Raymond J. Mooney. 2013. Adapting
discriminative reranking to grounded language learn-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (ACL-
2013), Sofia, Bulgaria.
Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin.
2010. Classifying dialogue acts in 1-to-1 live chats.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 862?
871.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats. In 26th Pacific Asia Conference on Lan-
guage,Information and Computation, pages 463?472.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423?430.
Martin Osborne and Ariel Rubinstein. 1994. A Course in
Game Theory. MIT Press.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kauffmann.
Leonard Savage. 1954. The Foundations of Statistics.
John Wiley.
367
Frederik Schadd, Sander Bakkes, and Pieter Spronck.
2007. Opponent modeling in real-time strategy games.
In Games and Simulation GAMEON, pages 61?68.
Yoav Shoham and Kevin Leyton-Brown. 2009. Multia-
gent Systems: Algorithmic, Game-Theoretic and Logi-
cal Foundations. Cambridge University Press.
Candace Sidner. 1994. An artificial discourse language
for collaborative negotiation. In AAAI, volume 1,
pages 814?819. MIT Press, Cambridge.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol V. Ess-dykema, and Marie
Meteer. 2000. Dialogue act modeling for automatic
tagging and recognition of conversational speech. In
Computational Linguistics, pages 26:339?373.
Tianhao Wu, Faisal M. Khan, Todd A. Fisher, Lori A.
Shuler, and William M. Pottenger. 2002. Posting
act tagging using transformation-based learning. In
Foundations of Data Mining and knowledge Discov-
ery, pages 319?331. Springer.
368
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 105?113,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Annotating Preferences in Negotiation Dialogues
Ana??s Cadilhac, Nicholas Asher and Farah Benamara
IRIT, CNRS and University of Toulouse
118, route de Narbonne
31062 Toulouse, France
{cadilhac, asher, benamara}@irit.fr
Abstract
Modeling user preferences is crucial in many
real-life problems, ranging from individual
and collective decision-making to strategic in-
teractions between agents and game theory.
Since agents do not come with their prefer-
ences transparently given in advance, we have
only two means to determine what they are if
we wish to exploit them in reasoning: we can
infer them from what an agent says or from
his nonlinguistic actions. In this paper, we an-
alyze how to infer preferences from dialogue
moves in actual conversations that involve bar-
gaining or negotiation. To this end, we pro-
pose a new annotation scheme to study how
preferences are linguistically expressed in two
different corpus genres. This paper describes
the annotation methodology and details the
inter-annotator agreement study on each cor-
pus genre. Our results show that preferences
can be easily annotated by humans.
1 Introduction
Modeling user preferences is crucial in many real-
life problems, ranging from individual and collec-
tive decision-making (Arora and Allenby, 1999)
to strategic interactions between agents (Brainov,
2000) and game theory (Hausman, 2000). A web-
based recommender system can, for example, help
a user to identify (among an optimal ranking) the
product item that best fits his preferences (Burke,
2000). Modeling preferences can also help to find
some compromise or consensus between two or
more agents having different goals during a nego-
tiation (Meyer and Foo, 2004).
Working with preferences involves three subtasks
(Brafman and Domshlak, 2009): preference acquisi-
tion, which extracts preferences from users, prefer-
ence modeling where a model of users? preferences
is built using a preference representation language
and preference reasoning which aims at computing
the set of optimal outcomes. We focus in this paper
on the first task.
Handling preferences is not easy. First, specifying
an ordering over acceptable outcomes is not trivial
especially when multiple aspects of an outcome mat-
ter. For instance, choosing a new camera to buy may
depend on several criteria (e.g. battery life, weight,
etc.), hence, ordering even two outcomes (cameras)
can be cognitively difficult because of the need to
consider trade-offs and dependencies between the
criteria. Second, users often lack complete infor-
mation about preferences initially. They build a
partial description of agents? preferences that typi-
cally changes over time. Indeed, users often learn
about the domain, each others? preferences and even
their own preferences during a decision-making pro-
cess. Since agents don?t come with their preferences
transparently given in advance, we have only two
means to determine what they are if we wish to ex-
ploit them in reasoning: we can infer them from
what an agent says or from his nonlinguistic actions.
In this paper, we analyze how to infer preferences
from dialogue moves in actual conversations that in-
volve bargaining or negotiation.
Within the Artificial Intelligence community,
preference acquisition from nonlinguistic actions
has been performed using a variety of specific
tasks, including preference learning (Fu?rnkranz and
105
Hu?llermeier, 2011) and preference elicitation meth-
ods (Chen and Pu, 2004) (such as query learning
(Blum et al, 2004), collaborative filtering (Su and
Khoshgoftaar, 2009) and qualitative graphical rep-
resentation of preferences (Boutilier et al, 1997)).
However, these tasks don?t occur in actual conver-
sations about negotiation. We are interested in how
agents learn about preferences from actual conver-
sational turns in real dialogue (Edwards and Barron,
1994), using NLP techniques.
To this end, we propose a new annotation scheme
to study how preferences are linguistically expressed
in dialogues. The annotation study is performed
on two different corpus genres: the Verbmobil cor-
pus (Wahlster, 2000) and a booking corpus, built
by ourselves. This paper describes the annotation
methodology and details the inter-annotator agree-
ment study on each corpus genre. Our results show
that preferences can be easily annotated by humans.
2 Background
2.1 What are preferences?
A preference is commonly understood as an order-
ing by an agent over outcomes, which are under-
stood as actions that the agent can perform or goal
states that are the direct result of an action of the
agent. For instance, an agent?s preferences may be
defined over actions like buy a new car or by its end
result like have a new car. The outcomes over which
a preference is defined will depend on the domain or
task.
Among these outcomes, some are acceptable for
the agent, i.e. the agent is ready to act in such a
way as to realize them, and some outcomes are not.
Among the acceptable outcomes, the agent will typ-
ically prefer some to others. Our aim is not to de-
termine the most preferred outcome of an agent but
follows rather the evolution of their commitments to
certain preferences as the dialogue proceeds. To give
an example, if an agent proposes to meet on a certain
day X and at a certain time Y, we learn that among
the agent?s acceptable outcomes is a meeting on X
at Y, even if this is not his most preferred outcome.
We are interested in an ordinal definition of prefer-
ences, which consists in imposing a ranking over all
(relevant) possible outcomes and not a cardinal defi-
nition which is based on numerical values that allow
comparisons.
More formally, let ? be a set of possible
outcomes. A preference relation, written , is a
reflexive and transitive binary relation over elements
of ?. The preference orderings are not necessarily
complete, since some candidates may not be com-
parable by a given agent. Given the two outcomes
o1 and o2, o1  o2 means that outcome o1 is equally
or more preferred to the decision maker than o2.
Strict preference o1  o2 holds iff o1  o2 and not
o2  o1. The associated indifference relation is
o1 ? o2 if o1  o2 and o2  o1.
2.2 Preferences vs. opinions
It is important to distinguish preferences from opin-
ions. While opinions are defined as a point of view, a
belief, a sentiment or a judgment that an agent may
have about an object or a person, preferences, as
we have defined them, involve an ordering on be-
half of an agent and thus are relational and com-
parative. Hence, opinions concern absolute judg-
ments towards objects or persons (positive, negative
or neutral), while preferences concern relative judg-
ments towards actions (preferring them or not over
others). The following examples illustrate this:
(a) The movie is not bad.
(b) The scenario of the first season is better than the
second one.
(c) I would like to go to the cinema. Let?s go and see
Madagascar 2.
(a) expresses a direct positive opinion towards the
movie but we do not know if this movie is the most
preferred. (b) expresses a comparative opinion be-
tween two movies with respect to their shared fea-
tures (scenarios) (Ganapathibhotla and Liu, 2008).
If actions involving these movies (e.g. seeing them)
are clear in the context, such a comparative opin-
ion will imply a preference, ordering the first season
scenario over the second. Finally, (c) expresses two
preferences, one depending on the other. The first
is that the speaker prefers to go to the cinema over
other alternative actions; the second is, given that
preference, that he wants to see Madagascar 2 over
other possible movies.
Reasoning about preferences is also distinct from
reasoning about opinions. An agent?s preferences
106
determine an order over outcomes that predicts how
the agent, if he is rational, will act. This is not true
for opinions. Opinions have at best an indirect link
to action: I may hate what I?m doing, but do it any-
way because I prefer that outcome to any of the al-
ternatives.
3 Data
Our data come from two corpora: one already-
existing, Verbmobil (CV ), and one that we cre-
ated, Booking (CB).
The first corpus is composed of 35 dialogues ran-
domly chosen from the existing corpus Verbmobil
(Wahlster, 2000), where two agents discuss on when
and where to set up a meeting. Here is a typical frag-
ment:
pi1 A: Shall we meet sometime in the next week?
pi2 A: What days are good for you?
pi3 B: I have some free time on almost every day
except Fridays.
pi4 B: Fridays are bad.
pi5 B: In fact, I?m busy on Thursday too.
pi6 A: Next week I am out of town Tuesday, Wednes-
day and Thursday.
pi7 A: So perhaps Monday?
The second corpus was built from various En-
glish language learning resources, available on the
Web (e.g., www.bbc.co.uk/worldservice/
learningenglish). It contains 21 randomly se-
lected dialogues, in which one agent (the customer)
calls a service to book a room, a flight, a taxi, etc.
Here is a typical fragment:
pi1 A: Northwind Airways, good morning. May I
help you?
pi2 B: Yes, do you have any flights to Sydney next
Tuesday?
pi3 A: Yes, there?s a flight at 16:45 and one at 18:00.
pi4 A: Economy, business class or first class ticket?
pi5 B: Economy, please.
Our approach to preference acquisition exploits
discourse structure and aims to study the impact
of discourse for extracting and reasoning on prefer-
ences. Cadilhac et al (2011) show how to compute
automatically preference representations for a whole
stretch of dialogue from the preference representa-
tions for elementary discourse units. Our annota-
tion here concentrates on the commitments to pref-
erences expressed in elementary discourse units or
EDUs. We analyze how the outcomes and the depen-
dencies between them are linguistically expressed
by performing, on each corpus, a two-level anno-
tation. First, we perform a segmentation of the di-
alogue into EDUs. Second, we annotate preferences
expressed by the EDUs.
The examples above show the effects of segmen-
tation. Each EDU is associated with a label pii.
For Verbmobil, we rely on the already avail-
able discourse annotation of Baldridge and Las-
carides (2005). For Booking, the segmentation
was made by consensus.
We detail, in the next section, our preference an-
notation scheme.
4 Preference annotation scheme
To analyze how preferences are linguistically ex-
pressed in each EDU, we must: (1) identify the set
? of outcomes, on which the agent?s preferences
are expressed, and (2) identify the dependencies be-
tween the elements of ? by using a set of specific
operators, i.e. identifying the agent?s preferences on
the stated outcomes. Consider the segment ?Let?s
meet Thursday or Friday?. We have ? = {meet
Thursday, meet Friday} where outcomes are linked
by a disjunction that means the agent is ready to act
for one of these outcomes, preferring them equally.
Within an EDU, preferences can be expressed in
different ways. They can be atomic preference state-
ments or complex preference statements.
4.1 Atomic preferences
Atomic preference statements are of the form ?I pre-
fer X?, ?Let?s X?, or ?We need X?, where X de-
scribes an outcome. X may be a definite noun phrase
(?Monday?, ?next week?, ?almost every day?), a
prepositional phrase (?at my office?) or a verb
phrase (?to meet?). They can be expressed within
comparatives and/or superlatives (?a cheaper room?
or ?the cheapest flight?).
Preferences can also be expressed in an indirect
way using questions. Although not all questions
entail that their author commits to a preference, in
many cases they do. That is, if A asks ?can we meet
next week?? he implicates a preference for meeting.
For negative and wh-interrogatives, the implication
107
is even stronger. Expressions of sentiment or polite-
ness can also be used to indirectly introduce prefer-
ences. In Booking, the segment ?economy please?
indicates the agent?s preference to be in an economy
class.
EDUs can also express preferences via free-choice
modalities; ?I am free on Thursday? or ?I can meet
on Thursday? tells us that Thursday is a possible day
to meet, it is an acceptable outcome.
A negative preference expresses an unacceptable
outcome, i.e. what the agent does not prefer. Neg-
ative preference can be expressed explicitly with
negation words (?I don?t want to meet on Friday?)
or inferred from the context (?I am busy on Mon-
day?).
While the logical form of an atomic preference
statement is something of the form Pref(X), we
abbreviate this in the annotation language, using just
the outcome expression X to denote that the agent
prefers X to the alternatives, i.e. X  X . If X is
an unacceptable outcome, we use the non-boolean
operator not to denote that the agent prefers not X to
other alternatives, i.e. X  X . In our Verbmobil
annotation, X is typically an NP denoting a time or
place; X as an outcome is thus shorthand for meet
on X or meet at X . For Booking, X is short for
reserve or book X .
4.2 Complex preferences
Preference statements can also be complex, express-
ing dependencies between outcomes. Borrowing
from the language of conditional preference net-
works or CP-nets (Boutilier et al, 2004), we rec-
ognize that some preferences may depend on an-
other action. For instance, given that I have cho-
sen to eat fish, I will prefer to have white wine
over red wine?something which we express as
eat fish : drink white wine  drink red wine.
Among the possible combinations, we find con-
junctions, disjunctions and conditionals. We exam-
ine these conjunctive, disjunctive and conditional
operations over outcomes and suppose a language
with non-boolean operators &,5 and 7? taking out-
come expressions as arguments.
With conjunctions of preferences, as in ?Could
I have a breakfast and a vegetarian meal?? or in
?Mondays and Fridays are not good??, the agent ex-
presses two preferences (respectively over the ac-
ceptable outcomes breakfast and vegetarian meal
and the non acceptable outcomes not Mondays and
not Fridays) that he wants to satisfy and he prefers
to have one of them if he can not have both. Hence
o1 & o2 means o1  o1 and o2  o2.
The semantics of a disjunctive preference is a free
choice one. For example in ?either Monday or Tues-
day is fine for me? or in ?I am free Monday and
Tuesday?, the agent states that either Monday or
Tuesday is an acceptable outcome and he is indif-
ferent between the choice of the outcomes. Hence
o1 5 o2 means o2 : o1 ? o1, o2 : o1  o1 and
o1 : o2 ? o2, o1 : o2  o2.
Finally, some EDUs express conditional among
preferences. For example, in the sentence ?What
about Monday, in the afternoon??, there are two
preferences: one for the day Monday, and, given the
Monday preference, one for the time afternoon (of
Monday), at least for one syntactic reading of the
utterance. Hence o1 7? o2 means o1  o1 and
o1 : o2  o2.
For each EDU, annotators identify how outcomes
are expressed and then indicate if the outcomes are
acceptable, or not, using the operator not and how
the preferences on these outcomes are linked using
the operators &,5 and 7?.
4.3 Example
We give below an example of how some EDUs are
annotated. <o> i indicates that o is the outcome
number i in the EDU, the symbol // is used to sepa-
rate the two annotation levels and brackets indicate
how outcomes are attached.
pi1 : <Tuesday the sixteenth> 1 I got class<from nine
to twelve> 2? // 1 7? not 2
pi2 : What about <Friday afternoon> 1, <at two
thirty> 2 or <three> 3, // 1 7? (25 3)
pi3 : <The room with balcony> 1 should be equipped
<with a queen size bed> 2, <the other one> 3
<with twin beds> 4, please. // (1 7? 2) & (3 7?
4)
In pi1, the annotation tells us that we have two out-
comes and that the agent prefers outcome 1 over any
other alternatives and given that, he does not pre-
fer outcome 2. In pi2, the annotation tells us that
the agent prefers to have one of outcome 2 and out-
come 3 satisfied given that he prefers outcome 1. In
this example, the free choice between outcome 2 and
108
outcome 3 is lexicalized by the coordinating con-
junction ?or?. On the contrary, pi3 is a more complex
example where there is no discursive marker to find
that the preference operator between the couples of
outcomes 1 and 2 on one hand, and 3 and 4 on the
other hand, is the conjunctive operator &.
5 Inter-annotator agreements
Our two corpora (Verbmobil and Booking)
were annotated by two annotators using the pre-
viously described annotation scheme. We per-
formed an intermediate analysis of agreement and
disagreement between the two annotators on two
Verbmobil dialogues. Annotators were thus
trained only for Verbmobil. The aim is to study to
what extent our annotation scheme is genre depen-
dent. The training allowed each annotator to under-
stand the reason of some annotation choices. After
this step, the dialogues of our corpora have been an-
notated separately, discarding those two dialogues.
Table 1 presents some statistics about the annotated
data in the gold standard.
CV CB
No. of dialogues 35 21
No. of outcomes 1081 275
No. of EDUs with outcomes 776 182
% with 1 outcome 71% 70%
% with 2 outcomes 22% 19%
% with 3 or more outcomes 8% 11%
No. of unacceptable outcomes (not) 266 9
No. of conjunctions (&) 56 31
No. of disjunctions (5) 75 29
No. of conditionals (7?) 184 37
Table 1: Statistics for the two corpora.
We compute four inter-annotator agreements: on
outcome identification, on outcome acceptance, on
outcome attachment and finally on operator identifi-
cation. Table 2 summarizes our results.
5.1 Agreements on outcome identification
Two inter-annotator agreements were computed us-
ing Cohen?s Kappa. One based on an exact matching
between two outcome annotations (i.e. their corre-
sponding text spans), and the other based on a le-
CV CB
Outcome identification (Kappa) exact : 0.66
lenient : 0.85
Outcome acceptance (Kappa) 0.90 0.95
Outcome attachment (F-measure) 93% 82%
Operator identification (Kappa) 0.93 0.75
Table 2: Inter-annotator agreements for the two corpora.
nient match between annotations (i.e. there is an
overlap between their text spans as in ?2p.m? and
?around 2p.m?). This approach is similar to the one
used by Wiebe, Wilson and Cardie (2005) to com-
pute agreement when annotating opinions in news
corpora. We obtained an exact agreement of 0.66
and a lenient agreement of 0.85 for both corpus gen-
res.
We made the gold standard after discussing cases
of disagreement. We observed four cases. The first
one concerns redundant preferences which we de-
cided not to keep in the gold standard. In such cases,
the second EDU pi2 does not introduce a new prefer-
ence, neither does it correct the preferences stated in
pi1; rather, the agent just wants to insist by repeat-
ing already stated preferences, as in the following
example:
pi1 A: Thursday, Friday, and Saturday I am out.
pi2 A: So those days are all out for me,
The second case of disagreement comes from
anaphora which are often used to introduce new, to
make more precise or to accept preferences. Hence,
we decided to annotate them in the gold standard.
Here is an example:
pi1 A: One p.m. on the seventeenth?
pi2 B: That sounds fantastic.
The third case of disagreement concerns prefer-
ence explanation. We chose not to annotate these
expressions in the gold standard because they are
used to explain already stated preferences. In the
following example, one judge annotated ?from nine
to twelve? to be expressions of preferences while the
other did not :
pi1 A: Monday is really not good,
pi2 A: I have got class from nine to twelve.
109
Finally, the last case of disagreement comes from
preferences that are not directly related to the action
of fixing a date to meet but to other actions, such as
having lunch, choosing a place to meet, etc. Even
though those preferences were often missed by an-
notators, we decided to keep them, when relevant.
5.2 Agreements on outcome acceptance
The aim here is to compute the agreement on the not
operator, that is if an outcome is acceptable, as in
?<Mondays> 1 are good // 1?, or unacceptable, as
in ?<Mondays> 1 are not good // not 1?. We get a
Cohen?s Kappa of 0.9 for Verbmobil and 0.95 for
Booking. The main case of disagreement concerns
anaphoric negations that are inferred from the con-
text, as in pi2 below where annotators sometimes fail
to consider ?in the morning? as unacceptable out-
comes:
pi1 A: Tuesday is kind of out,
pi2 A: Same reason in the morning
Same case of disagreement in this example where
?Monday? is an unacceptable outcome:
pi1 well, I am, busy <in the afternoon of the twenty
sixth> 1, // not 1
pi2 that is <Monday> 1 // not 1
5.3 Agreements on outcome attachment
Since this task involves structure building, we com-
pute the agreement using the F-score measure. The
agreement was computed on the previously built
gold standard once annotators discussed cases of
outcome identification disagreements. We compare
how each outcome is attached to the others within
the same EDU. This agreement concerns EDUs
that contain at least three outcomes, that is 8% of
EDUs from Verbmobil and 11% of EDUs from
Booking. When comparing annotations for the ex-
ample pi1 below, there is three errors, one for out-
come 2, one for 3 and one for 4.
pi1 <for the next week> 1 the only days I have
open are <Monday> 2 or <Tuesday> 3 <in the
morning> 4.
? Annotation 1 : 1 7? (25 (3 7? 4))
? Annotation 2 : 1 7? ((25 3) 7? 4)
We obtain an agreement of 93% for Verbmobil
and 82% for Booking.
5.4 Agreements on outcome dependencies
Finally, we compute the agreements for each couple
of outcomes on which annotators agreed about how
they are attached.
In Verbmobil, the most frequently used binary
operator is 7?. Because the main purpose of the
agents in this corpus is to schedule an appointment,
the preferences expressed by the agents are mainly
focused on concepts of time and there are many con-
ditional preferences since it is common that prefer-
ences on specific concepts depend on more broad
temporal concepts. For example, preferences on
hours are generally conditional on preferences on
days. In Booking, there are almost as many & as
7? because independent and dependent preferences
are more balanced in this corpus. The agents dis-
cuss preferences about various criteria that are in-
dependent. For example, to book a hotel, the agent
express his preferences towards the size of the bed
(single or double), the quality of the room (smoker
or nonsmoker), the presence of certain conveniences
(TV, bathtub), the possibility to have breakfast in
his room, etc. Within an EDU, such preferences are
often expressed in different sentences (compared to
Verbmobil where segments? lengths are smaller)
which lead annotators to link those preferences with
the operator &. Conditionals between preferences
hold when decision criteria are dependent. For ex-
ample, the preference for having a vegetarian meal
is conditional on the preference for having lunch.
There also are conditionals between temporal con-
cepts, for example, to choose the time of a flight.
Table 3 shows the Kappa for each operator on
each corpus genre. The Cohen?s Kappa, averaged
over all the operators, is 0.93 for Verbmobil and
0.75 for Booking. We observe two main cases of
disagreement: between 5 and &, and between &
and 7?. These cases are more frequent for Booking
mainly because annotators were not trained on this
corpus. This is why the Kappa was lower than for
Verbmobil. We discuss below the main two cases
of disagreement.
Confusion between 5 and &. The same lin-
guistic realizations do not always lead to the same
operator. For instance, in ?<Monday> 1 and
<Wednesday> 2 are good? we have 15 2 whereas
in ?<Monday> 1 and <Wednesday> 2 are not
110
CV CB
& 0.90 0.66
5 0.97 0.89
7? 0.92 0.71
Table 3: Agreements on binary operators.
good? or in ?I would like a <single room> 1 and
a <taxi> 2? we have respectively not 1 & not 2
and 1 & 2.
The coordinating conjunction ?or? is a strong pre-
dictor for recognizing a disjunction of preferences,
at least when the ?or? is clearly outside of the scope
of a negation1, as in the examples below (in pi1, the
negation is part of the wh-question, and not boolean
over the preference):
pi1 Why don?t we <meet, either Thursday the first> 1,
or <Thursday the eighth> 2 // 15 2
pi2 Would you like <a single> 1 or <a double> 2? //
15 2
The coordinating conjunction ?and? is also a
strong indication, especially when it is used to link
two acceptable outcomes that are both of a single
type (e.g., day of the week, time of day, place,
type of room, etc.) between which an agent wants
to choose a single realization. For example, in
Verbmobil, agents want to fix a single appoint-
ment so if there is a conjunction ?and? between two
temporal concepts of the same level, it is a disjunc-
tion of preference (see pi3 below). It is also the case
in Booking when an agent wants to book a single
plane flight (see pi4).
pi3 <Monday> 1 and <Tuesday> 2 are good for me
// 15 2
pi4 You could <travel at 10am.> 1, <noon> 2 and
<2pm> 3 // 15 (25 3)
The acceptability modality distributes across
the conjoined NPs to deliver something like
3(meet Monday) ? 3(meet Tuesday) in modal
logic (clearly acceptability is an existential
rather than universal modality), and as is
known from studies of free choice modality
1When there is a propositional negation over the disjunction
as in ?I don?t want sheep or wheat?, which occurs frequently
in a corpus in preparation, we no longer have a disjunction of
preferences.
(Schulz, 2007), such a conjunction translates to
3(meet Monday ? meet Tuesday), which ex-
presses our free choice disjunction of preferences,
o1 5 o2.
On the other hand, when the conjunction ?and?
links two outcomes referring to a single concept
that are not acceptable, it gives a conjunction of
preferences, as in pi5. Once again thinking in
terms of modality is helpful. The ?not accept-
able? modality distributes across the conjunction,
this gives something like 2?o1 ? 2?o2 (where ?
is truth conditional negation) which is equivalent to
2(?o1 ? ?o2), i.e. not o1 & not o2 and not equiv-
alent to 2(?o1 ? ?o2), i.e. not o1 5 not o2.
The connector ?and? also involves a conjunction
of preferences when it links two independent out-
comes that the agent wants to satisfy simultaneously.
For example, in pi6, the agent wants to book two ho-
tel rooms, and so the outcomes are independent. In
pi7, the agent expresses his preferences on two differ-
ent features he wants for the hotel room he is book-
ing.
pi5 <Thursday the thirtieth> 1, and <Wednesday the
twenty ninth> 2 are, booked up // not 1 & not 2
pi6 Can I have one room< with balcony> 1 and <one
without balcony> 2? // 1 & 2
pi7 <Queen> 1 and <nonsmoking> 2 // 1 & 2
Confusion between & and 7?. In this case, dis-
agreements are mainly due to the difficulty for an-
notators to decide if preferences are dependent, or
not. For example, in ?I have a meeting <starting
at three> 1, but I could meet <at one o?clock> 2?,
one annotator put not 1 7? 2 meaning that the
agent is ready to meet at one o?clock because he
can not meet at three, while the other annotated
not 1 & 2 meaning that the agent is ready to meet
at one o?clock independently of what it will do at
three.
Some connectors introduce contrast between the
preferences expressed in a segment as ?but?,
?although? and ?unless?. In the annotation, we can
model it thanks to the operator 7?. When it is used
between two conflicting values, it represents a cor-
rection. Thus, the annotation o1 7? not o1 means we
need to replace in our model of preferences o1  o1
by o1  o1. And vice versa for not o1 7? o1.
pi8 I have class <on Monday> 1, but, <any time, after
one or two> 2 I am free. // not 1 7? (1 7? 2)
111
pi9 <Friday> 1 is a little full, although there is some
possibility, <before lunch> 2 // not 1 7? (1 7? 2)
pi10 we?re full <on the 22nd> 1, unless you want <a
smoking room> 2 // not 1 7? (1 7? 2)
However, it is important to note that the coordi-
nating conjunction ?but? does not always introduce
contrast, as in the example below, where it intro-
duces a conjunction of preferences.
pi11 I am busy <on Monday> 1, but <Tuesday
afternoon> 2, sounds good // not 1 & 2
The subordinating conjunctions ?if?, ?because?
and ?so? are indications for detecting conditional
preferences. The preferences in the main clause de-
pend on the preferences in the subordinate clause
(if-clause, because-clause, so-clause), as in the ex-
amples below.
pi12 so if we are going to be able to meet <that, last
week in January> 1, it is going have to be <the,
twenty fifth> 2 // 1 7? 2
pi13 <the twenty eighth> 1 I am free, <all day> 2, if
you want to go for <a Sunday meeting> 3 // 3 7?
(2 7? 1)
pi14 it is going to have to be <Wednesday the third> 1
because, I am busy <Tuesday> 2 // not 2 7? 1
pi15 I have a meeting <from eleven to one> 1, so
we could, meet <in the morning from nine to
eleven> 2, or,<in the afternoon after one> 3 // not
1 7? (25 3)
Whether or not there are some discursive markers
between two outcomes, to find the appropriate oper-
ator, we need to answer some questions : does the
agent want to satisfy the two outcomes at the same
time ? Are the preferences on the outcomes depen-
dent or independent ?
We have shown in this section that it is difficult to
answer the second question and there is quite some
ambiguity between the operators & et 7?. This am-
biguity can be explained by the fact that both opera-
tors model the same optimal preference. Indeed, we
saw in section 4.2 that for two outcomes o1 and o2
linked by a conjunction of preferences (o1 & o2), we
have o1  o1 and o2  o2. For two outcomes o1 and
o2 where o2 is linked to o1 by a conditional prefer-
ence (o1 7? o2), we have o1  o1 and o1 : o2  o2.
In both cases, the best possible world for the agent
is the one where o1 and o2 are both satisfied at the
same time.
6 Conclusion and Future Work
In this paper, we proposed a linguistic approach
to preference aquisition that aims to infer prefer-
ences from dialogue moves in actual conversations
that involve bargaining or negotiation. We stud-
ied how preferences are linguistically expressed in
elementary discourse units on two different cor-
pus genres: one already available, the Verbmobil
corpus and the Booking corpus purposely built
for this project. Annotators were trained only for
Verbmobil. The aim is to study to what extent
our annotation scheme is genre dependent.
Our preference annotation scheme requires two
steps: identify the set of acceptable and non accept-
able outcomes on which the agents preferences are
expressed, and then identify the dependencies be-
tween these outcomes by using a set of specific non-
boolean operators expressing conjunctions, disjunc-
tions and conditionals. The inter-annotator agree-
ment study shows good results on each corpus genre
for outcome identification, outcome acceptance and
outcome attachment. The results for outcome de-
pendencies are also good but they are better for
Verbmobil. The difficulties concern the confu-
sion between disjunctions and conjunctions mainly
because the same linguistic realizations do not al-
ways lead to the same operator. In addition, anno-
tators often fail to decide if the preferences on the
outcomes are dependent or independent.
This work shows that preference acquisition from
linguistic actions is feasible for humans. The next
step is to automate the process of preference extrac-
tion using NLP methods. We plan to do it using an
hybrid approach combining both machine learning
techniques (for outcome extraction and outcome ac-
ceptance) and rule-based approaches (for outcome
attachment and outcome dependencies).
References
Neeraj Arora and Greg M. Allenby. 1999. Measur-
ing the influence of individual preference structures
in group decision making. Journal of Marketing Re-
search, 36:476?487.
Jason Baldridge and Alex Lascarides. 2005. Annotating
discourse structures for robust semantic interpretation.
In Proceedings of the 6th IWCS.
Avrim Blum, Jeffrey Jackson, Tuomas Sandholm, and
112
Martin Zinkevich. 2004. Preference elicitation and
query learning. Journal of Machine Learning Re-
search, 5:649?667.
Craig Boutilier, Ronen Brafman, Chris Geib, and David
Poole. 1997. A constraint-based approach to prefer-
ence elicitation and decision making. In AAAI Spring
Symposium on Qualitative Decision Theory, pages 19?
28.
Craig Boutilier, Craig Brafman, Carmel Domshlak, Hol-
ger H. Hoos, and David Poole. 2004. Cp-nets: A tool
for representing and reasoning with conditional ceteris
paribus preference statements. Journal of Artificial In-
telligence Research, 21:135?191.
Ronen I. Brafman and Carmel Domshlak. 2009. Prefer-
ence handling - an introductory tutorial. AI Magazine,
30(1):58?86.
Sviatoslav Brainov. 2000. The role and the impact of
preferences on multiagent interaction. In Proceedings
of ATAL, pages 349?363. Springer-Verlag.
Robin Burke. 2000. Knowledge-based recommender
systems. In Encyclopedia of Library and Information
Science, volume 69, pages 180?200. Marcel Dekker.
Ana??s Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2011. Commitments to preferences
in dialogue. In Proceedings of SIGDIAL, pages 204?
215. ACL.
Li Chen and Pearl Pu. 2004. Survey of preference elici-
tation methods. Technical report.
Ward Edwards and F. Hutton Barron. 1994. Smarts
and smarter: Improved simple methods for multiat-
tribute utility measurement. Organizational Behavior
and Human Decision Processes, 60(3):306?325.
Johannes Fu?rnkranz and Eyke Hu?llermeier, editors.
2011. Preference Learning. Springer.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 241?248,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel M. Hausman. 2000. Revealed preference, be-
lief, and game theory. Economics and Philosophy,
16(01):99?115.
Thomas Meyer and Norman Foo. 2004. Logical founda-
tions of negotiation: Strategies and preferences. In In
Proceedings of the Ninth International Conference on
Principles of Knowledge Representation and Reason-
ing (KR04, pages 311?318.
Katrin Schulz. 2007. Minimal Models in Semantics and
Pragmatics: Free Choice, Exhaustivity, and Condi-
tionals. PhD thesis, ILLC.
Xiaoyuan Su and Taghi M. Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances in
Artificial Intelligence, 2009:1?20.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
113
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 204?215,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Commitments to Preferences in Dialogue
Anais Cadilhac*, Nicholas Asher*, Farah Benamara*, Alex Lascarides**
*IRIT, University of Toulouse, **School of Informatics, University of Edinburgh
Abstract
We propose a method for modelling how dialogue
moves influence and are influenced by the agents?
preferences. We extract constraints on preferences
and dependencies among them, even when they are
expressed indirectly, by exploiting discourse struc-
ture. Our method relies on a study of 20 dia-
logues chosen at random from the Verbmobil cor-
pus. We then test the algorithms predictions against
the judgements of naive annotators on 3 random un-
seen dialogues. The average annotator-algorithm
agreement and the average inter-annotator agree-
ment show that our method is reliable.
1 Introduction
Dialogues are structured by various moves that the
participants make?e.g., answering questions, asking
follow-up questions, elaborating prior claims, and so
on. Such moves come with commitments to certain at-
titudes such as intentions and preferences. While map-
ping utterances to their underlying intentions is well
studied through the application of plan recognition tech-
niques (e.g., Grosz and Sidner (1990), Allen and Litman
(1987)), game-theoretic models of rationality generally
suggest that intentions result from a deliberation to find
the optimal tradeoff between one?s preferences and one?s
beliefs about possible outcomes (Rasmusen, 2007). So
mapping dialogue moves to preferences is an important
task: for instance, they are vital in decisions on how to
re-plan and repair should the agents? current plan fail, for
they inform the agents about the relative importance of
their various goals. Classical game theory, however, de-
mands a complete and cardinal representation of prefer-
ences for the optimal intention to be defined. This is not
realistic for modelling dialogue because agents often lack
complete information about preferences prior to talking:
they learn about the domain, each other?s preferences and
even their own preferences through dialogue exchange.
For instance, utterance (1) implies that the speaker wants
to go to the mall given that he wants to eat, but we do not
know his preferences over ?go to the mall? if he does not
want to eat.
(1) I want to go to the mall to eat something.
Existing formal models of dialogue content either do not
formalise a link between utterances and preferences (e.g.,
Ginzburg (to appear)), or they encode such links in a
typed feature structure, where desire is represented as a
feature that takes conjunctions of values as arguments
(e.g., Poesio and Traum (1998)), making the language
too restricted to express dependencies among preferences
of the kind we just described. Existing implemented
dialogue systems likewise typically represent goals as
simple combinations of values on certain information
?slots? (e.g., He and Young (2005), Lemon and Pietquin
(2007)); thus (1) yields a conjunction of preferences, to
go to the mall and to eat something. But such a system
could lead to suboptimal dialogue moves?e.g., to help
the speaker go to the mall even if he has already received
food.
What?s required, then, is a method for extracting par-
tial information about preferences and the dependencies
among them that are expressed in dialogue, perhaps indi-
rectly, and a method for exploiting that partial informa-
tion to identify the next optimal action. This paper pro-
poses a method for achieving these tasks by exploiting
discourse structure.
We exploited the corpus of Baldridge and Lascarides
(2005a), who annotated 100 randomly chosen sponta-
neous face-to-face dialogues from the Verbmobil cor-
pus (Wahlster, 2000) with their discourse structure ac-
cording to Segmented Discourse Representation Theory
(SDRT, Asher and Lascarides (2003))?these structures
represent the types of (relational) speech acts that the
agents perform. Here?s a typical fragment:
(2) a. A: Shall we meet sometime in the next
week?
b. A: What days are good for you?
c. B: Well, I have some free time on almost
every day except Fridays.
204
d. B: In fact, I?m busy on Thursday too.
e. A: So perhaps Monday?
Across the corpus, more than 30% of the discourse units
are either questions or assertions that help to elaborate a
plan to achieve the preferences revealed by a prior part
of the dialogue?these are marked respectively with the
discourse relations Q-Elab and Plan-Elab in SDRT, and
utterances (2b) and (2e) and the segments (2c) and (2d)
invoke these relations (see Section 2). Moreover, 10% of
the moves revise or correct prior preferences (like (2d)).
We will model the interaction between dialogue con-
tent and preferences in two steps. The first maps ut-
terances and their rhetorical connections into a partial
description of the agents? preferences. The mapping is
compositional and monotonic over the dialogue?s logi-
cal form (i.e., the description of preferences for an ex-
tended segment is defined in terms of and always sub-
sumes those for its subsegments): it exploits recursion
over discourse structure. The descriptions partially de-
scribe ceteris paribus preference nets or CP-nets with
Boolean variables (Boutilier et al, 2004). We chose CP-
nets over alternative logics of preferences, because they
provide a compact, computationally efficient, qualitative
and relational representation of preferences and their de-
pendencies, making them compatible with the kind of
partial information about preferences that utterances re-
veal. Our mapping from the logical form of dialogue
to partial descriptions of Boolean CP-nets proceeds in a
purely linguistic or domain independent way (e.g., it ig-
nores information such as Monday and Tuesday cannot
co-refer) and will therefore apply to dialogue generally
and not just Verbmobil.
In a second stage, we ?compress? and refine our descrip-
tion making use of constraints proper to CP-nets (e.g.,
that preference is transitive) and constraints provided by
the domain?in this case constraints about times and
places, as well as constraints from deep semantics. This
second step reduces the complexity of inferring which
CP-net(s) satisfy the partial description and allows us to
identify the minimal CP-net that satisfies the domain-
dependent description of preferences. We can thus ex-
ploit dependencies between dialogue moves and mental
states in a compact, efficient and intuitive way.
We start by motivating and describing the semantic repre-
sentation of dialogue from which our CP-net descriptions
and then our CP-nets will be constructed.
2 The Logical Form of Dialogue
Our starting point for representing dialogue con-
tent is SDRT. Like Hobbs et al (1993) and
Mann and Thompson (1987), it structures discourse
into units that are linked together with rhetorical re-
lations such as Explanation, Question Answer Pair
(QAP), Q-Elab, Plan-Elab, and so on. Logical forms
in SDRT consist of Segmented Discourse Representation
Structures (SDRSs). As defined in Asher and Lascarides
(2003), an SDRS is a set of labels representing discourse
units, and a mapping from each label to an SDRS-formula
representing its content?these formulas are based on
those for representing clauses or elementary discourse
units (EDUs) plus rhetorical relation symbols between
labels. Lascarides and Asher (2009) argue that to make
accurate predictions about acceptance and denial, both
of which can be implicated rather than linguistically
explicit, the logical form of dialogue should track each
agent?s commitments to content, including rhetorical
connections. They represent a dialogue turn (where turn
boundaries occur whenever the speaker changes) as a
set of SDRSs?one for each agent representing all his
current commitments, from the beginning of the dialogue
to the end of that turn. The representation of the dialogue
overall?a Dialogue SDRS or DSDRS?is that of each of
its turns. Each agent constructs the SDRSs for all other
agents as well as his own. For instance, (2) is assigned
the DSDRS in Table 1, with the content of the EDUs
omitted for reasons of space (see Lascarides and Asher
(2009) for details). We adopt a convention of indexing
the root label of the nth turn, spoken by agent d, as
nd; and pi : ? means that ? describes pi?s content (we?ll
sometimes also write ?pi to identify this description).
We now return to our example (2). Intuitively, (2a) com-
mits A to a preference for meeting next week but it does
so indirectly: the preference is not asserted, or equiva-
lently entailed at the level of content from the semantics
of Q-Elab(a,b). Accordingly, responding with "I do too"
(meaning "I want to meet next week too") is correctly pre-
dicted to be highly anomalous. A?s SDRS for turn 1 in Ta-
ble 1 commits him to the questions (2a) and (2b) because
Q-Elab is veridical: i.e. Q-Elab(a,b) entails the dynamic
conjunction ?a ??b. Since intuitively (2a) commits A to
the implicature that he prefers next week, our algorithm
for eliciting preferences from dialogue must ascribe this
preference to A on the basis of his move Q-Elab(a,b).
Furthermore,Q-Elab(a,b) entails that any answer to (2b)
must elaborate a plan to achieve the preference revealed
by (2a); this makes ?b paraphrasable as ?What days next
week are good for you??, which does not add new prefer-
ences.
B?s contribution in the second turn attaches to (2b) with
QAP and also Plan-Elab?he answers with a non-empty
extension for what days. Lascarides and Asher (2009) ar-
gue that this means that B is also committed to the illo-
cutionary contribution of (2b), as shown in Table 1 by
the addition of Q-Elab(a,b) to B?s SDRS. This addition
commits B also to the preference of meeting next week,
with his answer making the preferencemore precise: (2c)
reveals that B prefers any day except Friday; by linking
(2d) with Plan-Correction he retracts the preference for
Thursday. This compels A to revise his inferences about
205
Turn A?s SDRS B?s SDRS
1 pi1A : Q-Elab(a,b) /0
2 pi1A : Q-Elab(a,b) pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)
pi : Plan-Correction(c,d)
3 pi3A : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)? pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)
Plan-Elab(pi,e) pi : Plan-Correction(c,d)
Table 1: The DSDRS for Dialogue (2).
B?s preference for meeting on Thursday. A?s Plan-Elab
move (2e) in the third turn reveals another preference for
Monday. This may not match his preferred day when the
dialogue started: perhaps that was Friday. He may con-
tinue to prefer that day. But engaging in dialogue can
compel agents to revise their commitments to preferences
as they learn about the domain and each other.
The above discussion of (2) exhibits how different types
of rhetorical relations between utterances rather than
Searle-like speech acts like question, construed as a prop-
erty of an utterance, are useful for encoding how pref-
erences evolve in a dialogue and how they relate to
one another. While the Grounding Acts dialogue model
(Poesio and Traum, 1998) and the Question Under Dis-
cussion (QUD) model (Ginzburg, to appear) both have
many attractive features, they do not encode as fine-
grained a taxonomy of types of speech acts and their se-
mantic effects as SDRT: in SDRT each rhetorical relation
is a different kind of (relational) speech act, so that, for
instance, the speech act of questioning is divided into the
distinct types Q-Elab, Plan-Correction, and others. For
the QUD model to encode such relations would require
implicit questions of all sorts of different types to be in-
cluded in the taxonomy, in which case the result may be
equivalent to the SDRT taxonomy of dialogue moves. We
have not explored this eventual equivalence here.
3 CP-nets and CP-net descriptions
A preference is standardly understood as an ordering by
an agent over outcomes; at the very least it entails a com-
parison between one entity and another (outcomes being
one sort of entity among others). As indicated in the in-
troduction, we are interested in an ordinal definition of
preferences, which consists in imposing an ordering over
all (relevant) possible outcomes. Among these outcomes,
some are acceptable for the agent, in the sense that the
agent is ready to act in such a way as to realize them;
and some outcomes are not acceptable. Amongst the ac-
ceptable outcomes, the agent will typically prefer some
to others. Our method does not try to determine the most
preferred outcome of an agent but follows rather the evo-
lution of their commitments to certain preferences as the
dialogue proceeds. To give an example, if an agent pro-
poses to meet on a certain day X and at a certain time Y,
we infer that among the agent?s acceptable outcomes is a
meeting on X at Y, even if this is not his most preferred
outcome (see earlier discussion of (2e)).
A CP-net (Boutilier et al, 2004) offers a compact rep-
resentation of preferences. It is a graphical model that
exploits conditional preferential independence so as to
structure the decision maker?s preferences under a ceteris
paribus assumption.
Although CP-nets generally consider variables with a fi-
nite range of values, to define the mapping from dialogue
turns to descriptions of CP-nets in a domain indepen-
dent and compositional way, we use Boolean proposi-
tional variables: each variable describes an action that an
agent can choose to perform, or not. We will then refine
the CP-net description by using domain-specific informa-
tion, transforming CP-nets with binary valued variables
to CP-nets with multiple valued variables. This reduces
the complexity of the evaluation of the CP-net by a large
factor.
More formally, let V be a finite set of propositional vari-
ables and LV the description language built from V via
Boolean connectives and the constants ? (true) and ?
(false). Formulas of LV are denoted by ?,?, etc. 2V is the
set of interpretations for V , and as usual for M ? 2V and
x?V , M gives the value true to x if x?M and false other-
wise. Where X ?V , let 2X be the set of X-interpretations.
X-interpretations are denoted by listing all variables of
X , with a ? symbol when the variable is set to false: e.g.,
where X = {a,b,d}, the X-interpretation M = {a,d} is
expressed as abd.
A preference relation  is a reflexive and transitive bi-
nary relation on 2V with strict preference ? defined in
the usual way (i.e., M  M? but M? 6 M). Note that
preference orderings are not necessarily complete, since
some candidates may not be comparable by a given agent.
An agent is said to be indifferent between two options
M,M? ? 2V , written M ?M?, if M M? and M? M.
As we stated earlier, CP-nets exploit conditional pref-
erential independence to compute a preferential ranking
over outcomes:
Definition 1 Let V be a set of propositional variables
and {X ,Y,Z} a partition of V . X is conditionally pref-
erentially independent of Y given Z if and only if ?z ?
2Z , ?x1,x2 ? 2X and ?y1,y2 ? 2Y we have: x1y1z 
206
x2y1z iff x1y2z x2y2z.
For each variable X , the agent specifies a set of parent
variables Pa(X) that can affect his preferences over the
values of X . Formally, X is conditionally preferentially
independent of V \ ({X}?Pa(X)). This is then used to
create the CP-net.
Definition 2 Let V be a set of propositional variables.
N = ?G ,T ? is a CP-net on V , where G is a directed
graph over V , and T is a set of Conditional Preference
Tables (CPTs) with indifference. That is, T = {CPT(X j):
X j ? V}, where CPT(X j) specifies for each instantiation
p ? 2Pa(X j) either x j ?p x j, x j ?p x j or x j ?p x j.
The following simple example illustrates these defini-
tions. Suppose our agent prefers to go from Paris to
Hong Kong by day rather than overnight. If he takes an
overnight trip, he prefers a non stop flight, but if he goes
by day, he prefers a flight with a stop. Figure 1 shows the
associated CP-net. The variable T stands for the prefer-
ence over the period of travel. Its values are Td for a day
trip and Tn for a night one. The variable St stands for the
preference over stops. Its values are S for a trip with stops
and S without.
T
St
CPT(T) = Td ? Tn
CPT(St) = Td : S ? S
Tn : S? S
Figure 1: Travel CP-net
With CP-nets defined, we proceed to a description lan-
guage for them. The description language formula w ?
y(CPT ) describes a CP-net where a CPT contains an en-
try of the form w ?p y for some possibly empty list of
parent variables p. A CP-net description is a set of such
formulas. The CP-net N |= x1, . . .xn : w? y(CPT ) iff the
CP-net N ?s CPT T contains an entry w?~u y?also writ-
ten~u :w? y?where x1, . . .xn figure in~u. Satisfaction of a
description formula by a CP-net yields a notion of logical
consequence between a CP-net descriptionD N and a de-
scription formula in the obvious way. Dialogue turns also
sometimes inform us that certain variables enter into pref-
erence statements. We?ll express the fact that the vari-
ables x1, . . . ,xn are associated with discourse constituent
pi by the formula x1, . . . ,xn(P(pi)), where P(pi) refers to
the partial description of the preferences expressed by the
discourse unit pi (see Section 4).
The description language allows us to impose constraints
on the CP-nets that agents commit to without specifying
the CP-net completely, as is required for utterances like
(1). In section 6, we describe how to construct a min-
imal CP-net from a satisfiable CP-net description. One
can then use the forward sweep procedure for outcome
optimisation (Boutilier et al, 2004). This is a proce-
dure of linear complexity, which consists in instantiating
variables following an order compatible with the graph,
choosing for each variable (one of) its preferred values
given the value of the parents.
4 From EDUs to Preferences
EDUs are described in SDRT using essentially Boolean
formulas over labels (Asher and Lascarides, 2003); thus
?(pi)??(pi) means that ? and ? describe aspects of pi?s
content. Not(pi1,pi)? ?(pi1) means that the logical form
of the EDU pi is of the form ?pi1 and that pi1 is described
by ?; so pi has the content ??. Our task is to map such
descriptions of content into descriptions of preferences.
Our preference descriptions will use Boolean connectives
and operators over preference entries (e.g., of the form
x ? y): namely, &,?, 7?, and a modal operator ?. The
rules below explain the semantics of preference opera-
tors (they are in effect defined in terms of the semantics
of buletic attitudes and Boolean connectives) and how
to recursively calculate preference descriptions from the
EDU?s logical structure.
Simple EDUs can provide atomic preference statements
(e.g., I want X or We need X). This means that with this
EDU the speaker commits to a preference for X . X will
typically involve a Boolean variable and a preference en-
try for its CPT. P(pi) is the label of the preference descrip-
tion associated with discourse unit pi. Hence for a sim-
ple EDU pi, we have X(P(pi)) as its description. Simple
EDUs also sometimes express preferences in an indirect
way (see (2a)).
More generally, P recursively exploits the logical struc-
ture of an EDU?s logical form to produce an EDU pref-
erence representation (EDUPR). For instance, since the
logical form of the EDU I want fish and wine features
conjunction, likewise so does its preference description:
?&?(P(pi)) means that among the preferences included
in pi, the agent prefers to have both ? and ? and prefers ei-
ther one if he can?t have both.1 We also have disjunctions
(let?s meet Thursday or Friday), and negations (I don?t
want to meet on Friday), whose preferences we?ll express
respectively as Thurs?Fri(P(pi)) and ?Fri(P(pi)).
Some EDUs express commitments to dependencies
among preferences. For example, in the sentence What
about Monday, in the afternoon?, there are two prefer-
ences: one for the day Monday, and, given the Monday
preference, one for the time afternoon (of Monday), at
least on one syntactic disambiguation. We represent this
dependency as Mon 7? Aft(P(pi)). Note that 7? is not
expressible with just Boolean operators. Finally, EDUs
can express commitment to preferences via free choice
1The full set of rules also includes a stronger conjunction ???(P(pi))
(the agent prefers both ? and ?, but is indifferent if he can?t have both).
207
modalities; I am free on Thursday, or?Thurs(P(pi)), tells
us that Thursday is a possible day to meet. ?? says that ?
is an acceptable outcome (as described earlier, this means
the agent is ready to act so as to realize an outcome that
entails ?). Thus, ??(pi) entails ?(pi), and ?-embedded
preferences obey reduction axioms permitting ? to be
eliminated when combined with other preference oper-
ators. But a ? preference statement does affect a prefer-
ence description when is is conjoined in Boolean fashion
with another ? preference statement in an EDU or com-
bined via a discourse relation like Continuation. This is
because ? is a free choice modality and obeys the equiv-
alence (3) below, which in turn yields a disjunctive pref-
erence ???(P(pi)) from what appeared to be a conjunc-
tion.2
(3) (??(P(pi))???(P(pi)))??(???)(P(pi))
The variables introduced by a discourse segment pi are
integrated into the CP-net description D N via the oper-
ation Commit(pi,D N ). The following seven rules cover
the different possible logical structures for the EDU pref-
erence representation. In the following, X ,Y,Z,W denote
propositional variables and ?, ? propositional formulas
from EDUPR. Var(?) are the variables in ?, and ?X
the preference relation describing CPT (X). Sat(?) (or
non-Sat(?)) is a conjunction of literals from Var(?) that
satisfy (or do not satisfy) ?. Sat(?)?X is the formula that
results from removing the conjunct with X from Sat(?).
1. Where X(P(pi)) (X is a variable of P(pi), e.g., I want
X), Commit(pi,D N ) adds the description D N |=
X ? X(CPT (X)).3
2. Where ?&?(P(pi)), Commit(pi,D N ) adds descrip-
tions as follows:
? For each X ?Var(?), addVar(?) to Pa(X) and
modifyCPT (X) as follows:
If Sati(?), Sat j(?) ? X (resp. X), then Sati(?),
Sat j(?)?X : X ? X (resp. X ? X), for all sat-
isfiers i and j.
? Similarly for each Y ?Var(?).
If ? and ? are literals X and Y we get: D N |= Y ?
Y (CPT (Y )) and D N |= X ? X(CPT (X)). Graph-
ically, this yields the following preference relation
(where one way arrows denote preference, two way
2We provide here the reduction axioms over preference descriptions
1. ?(?&?)(P(pi))? (?&?)(P(pi))
2. ?(? 7? ?)(P(pi))? (? 7? ?)(P(pi))
3. ?(???)? (?? ?)(P(pi))
4. ???(P(pi))???(P(pi))
3Given our description language semantics, this means that any
CP-net which satisfies the description D N contains a preference ta-
ble CPT (X) with an entry X ? X with at least one instantiation of the
variables in Pa(X).
arrows denote indifference or equal preference, and
no arrow means the options are incomparable):
XY
XY XY
XY
3. Where ?? ?(P(pi)) (the agent prefers to have at
least one of ? and ? satisfied). If ? and ? are X
and Y , we get:
? Var(X) ? Pa(Var(Y )) and D N |= X : Y ?
Y (CPT (Y )), D N |= X : Y ? Y (CPT (Y )).
? Var(Y ) ? Pa(Var(X)) and D N |= Y : X ?
X(CPT (X)), D N |= Y : X ? X(CPT (X)).
This corresponds to the following preference rela-
tion:
XY
XY XY XY
As before, the use of indifference allows us to find
the best outcomes (XY , XY and XY ) easily.
4. Where ? 7? ?(P(pi)) (the agent prefers that ? is sat-
isfied and if so that ? is also satisfied. If ? is not
satisfied, it is not possible to define preferences on
?). If ? and ? are X and Y , we get:
? D N |= X ? X(CPT (X))
? Var(X) ? Pa(Var(Y)) and
D N |= X : Y ? Y (CPT (Y )).
Note that this description is also produced by
Elab(pii,pi j) below where X(P(pii)) and Y (P(pi j))
(see rule 8). Thus the implication symbol 7? is a
"shortcut" in that it represents elaborations whose
arguments are in the same EDU.
5. Where ??(P(pi)) (the agent prefers a free choice of
?). Given the behaviour of?, this reduces to treating
?(P(pi)).
6. Where ??(P(pi)). We can apply rules 1-5 by con-
verting ?? into conjunctive normal form.
7. Where ?(P(pi))??(P(pi)), with ? and ? nonmodal,
we simply apply the rule for ? and that for ?.
5 From Discourse Structure to Preferences
We must now define how the agents? preferences, repre-
sented as a partial description of a CP-net, are built com-
positionally from the discourse structure over EDUs. The
constraints are different for different discourse relations,
reflecting the fact that the semantics of connections be-
tween segments influences how their preferences relate
to one another.
We will add rules for defining Commit over la-
bels pi whose content ?pi express rhetorical relations
R(pii,pi j)?indeed, we overload the notation and write
Commit(R(pii,pi j),D N ). Since Commit applies com-
positionally, starting with the EDUs and working up
208
the discourse structure towards the unique root la-
bel of the SDRS, we can assume in our definition of
Commit(R(pii,pi j),D N ) that the EDUPRs are already de-
fined. We give rules for all the relations in the Verbmobil
corpus, though we will be very brief with those that are
less prevalent. A complete example using our rules is in
appendix A.
IExplanation, Elab, Plan-Elab, Q-elab
IExplanation(pii, pi j): i.e., pi j?s preferences explain pii?s
(e.g., see (1), where P(pii) would be going to the mall
and P(pi j) is eating something). With Elab(pii, pi j) a
preference in pii is elaborated on or developed in pi j,
as in: I want wine. I want white wine. That is, a
preference for white wine depends on a preference for
wine. Plan-Elab(pii,pi j) means that pi j describes a plan
for achieving the preferences expressed by pii, and with
Q-Elab we have a similar dependence between prefer-
ences, but the second constituent is a question (so often
in practice this means preference commitments from pii
transfer from one agent to another).
Plan-Elab(pi j,pii), Elab(pi j,pii) and IExplanation(pii,pi j)
all follow the same two-step rule, and so from the point
of view of preference updates they are equivalent:
8. i Firstly, preference description D N is up-
dated according to P(pi j) by applying
Commit(pi j,D N ), if pi j expresses a new
preference. If not go to step (ii).
ii. Secondly, description D N is modified so that
each variable in P(pii) depends on each vari-
able in P(pi j): i.e., ?X ? Var(P(pii)), ?Y ?
Var(P(pi j)), Y ? Pa(X). Then, D N is enriched
according to P(pii), if pii expresses a preference.
If it does not, then end.
We now give some details concerning step (ii) above. To
this end, let ? denote a formula with SDRS description
predicates, ?? its corresponding boolean (preference) for-
mula and ?? its negation. Then for ?=Y , we define ??=Y
and ?? = Y ; for ? = Y 7? Z we define ?? = Y ? Z and
?? = Y ? Z; and for ? = Y ? Z and ? = Y&Z, we have
?? = Y ?Z and ?? = Y ?Z.
a. X(P(pii)) and ?(P(pi j)). The agent explains his pref-
erences on X by ?. So, if no preferences on X are
already defined, ? is a reason to prefer X . That is,
D N |= ??: X ? X(CPT (X)). However, it is not pos-
sible to define preferences on X if ? is false. If, on
the other hand, preferences on X are already defined,
the agent prefers X if ? is satisfied, and does not
modify his preferences otherwise?i.e.,?X ,??= X ?
X , ?X ,??=?X .4
4If we have ?X such that Z: X ? X , Z: X ? X , ?X ,?? represents
preferences defined by Z??? and Z???, whereas ?X ,?? represents pref-
erences defined by Z??? and Z???.
For ? = Y , if ?X is not already defined, we obtain
the following preference relation (no information on
the preference for X if Y is false makes XY and XY
incomparable):
XY
XYXY
XY
b. X?Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences X?Z by ?: he wants to satisfy X or Z
if ? is satisfied.
First, we set Var(Z) ? Pa(Var(X)), Var(X) ?
Pa(Var(Z)). If ?X is not already defined, we have:
D N |= ?? ? Z: X ? X(CPT (X)), D N |= ?? ? Z:
X ? X(CPT (X)).
Otherwise, ?X ,??,Z= X ? X , ?X ,??,Z= X ? X ,
?X ,??,Z= ?X ,??,Z= ?X .
CPT (Z) is defined asCPT (X) by inverting X and Z.
For ? =Y , if ?X and?Z are not already defined, we
obtain the following preference relation (again, the
lack of preference information on X and Z when Y
is false yields incomparability among states whereY
is false):
XYZXYZ
XYZ
XYZ
XYZ
XYZ
XYZXYZ
c. X&Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences on X&Z by ?.
? If ?X is not already defined, we have: D N |=
?? : X ? X(CPT (X)).
Otherwise, ?X ,??= X ? X , ?X ,??= ?X ,
? CPT (Z) is defined as CPT (X) by replacing X
by Z.
d. X 7? Z(P(pii)) and ?(P(pi j)). The agent explains his
preferences on X 7? Z by ?: he wants to satisfy X
and after Z if ? is satisfied.
If ?X is not already defined, we have D N |= ?? :
X ? X(CPT (X)) and we setVar(X)? Pa(Var(Z)).5
If ?Z is not yet defined, we have : D N |= ?? ?X :
Z ? Z(CPT (Z)), D N |= ???X : Z ? Z(CPT (Z)).
Else, ?Z,(???X)= Z ? Z, ?Z,(???X)= Z ? Z,
?Z,(???X)= ?Z,(???X)=?Z.
e. ?(P(pii)) and ?(P(pi j)). We can apply rules 8 by
decomposing ?.
5Otherwise, there is no need to modify ?X . This is what we call a
?partial elaboration?. Variables that were evoked since preferences on
X were introduced are parents of Z but not of X . For example, if an
agent commits to a preference for Monday then Afternoon, and later in
the discourse he commits to 2oclock, then Afternoon is 2oclock?s parent
but not Monday?s.
209
f. ?(?)(P(pii)) and ?(?)(P(pi j)). We treat this like a
free choice EDU (see rule 5).
g. ?(?)(P(pii)) and ?(P(pi j)), where ? is non modal.
We treat this like ?(P(pii)) and ?(P(pi j)) (see rule
8.e)
Let?s briefly look at how the rule changes for
Q-elabA(pi1,pi2) (where the subscript A identifies the
speaker of pi2):
9. Q-ElabA(pi1,pi2) implies that we update A?s CP-
net description D N by applying the rule for
Elab(pi1,pi2), where if pi2 expresses no preferences
on their own, we simply make the P(pi2) description
equal to the P(pi1) description. Thus A?s CP-net de-
scription is updated with the preferences expressed
by utterance pi1, regardless of who said pi1.
QAPAnswers to questions affect preferences in complex
ways:
10. The first case concerns yes/no questions and there
are two cases, depending on whether B replies yes
or no:
Yes QAPB(pi1,pi2) where pi2 is yes. B?s pref-
erence descriptions are updated by apply-
ing Commit(ElabB(pi1,pi2),D N ) (and so B?s
preference description include preferences ex-
pressed by pi1 and pi2).
No QAPB(pi1,pi2) where pi2 is no. If P(pi1)
and P(pi2) are consistent, then B?s pref-
erence descriptions are updated by ap-
plying CommitB(ElabB(pi1,pi2),D N );
otherwise, they are updated by applying
Commit(Correction(pi1,pi2),D N ) (see rule
13).
11. When pi1 is a wh-question and QAPB(pi1,pi2), B?s
preferences over variables in pi1 and pi2 are ex-
actly the same as the ones defined for a yes/no
question where the answer is yes. Variables in pi2
will refine preferences over variables in pi1. So,
B?s preference descriptions are updated by applying
CommitB(ElabB(pi1,pi2),D N ).
In previous rules, it is relatively clear how to update the
preference commitments. However, in some cases it?s not
clear what the answer in a QAP targets: in Could we meet
the 25 in the morning? No, I can?t., we do not know if
No is about the 25 and the morning, or only about the
morning. So, we define the following rule for managing
cases where the target is unknown :
12. If we know the target, we can change the description
of the CP-net. Otherwise, we wait to learn more.
Correction and Plan-Correction allow a speaker to rec-
tify a prior commitment to preferences. Self-corrections
also occur in the corpus: I could do it on the 27th. No I
can not make it on the 27th, sorry I have a seminar. Cor-
rection and Plan-Correction can have several effects on
the preferences. For instance, they can correct preference
entries. That is, given Correction(pi1,pi2), some variables
in P(pi1) are replaced by variables in P(pi2) (in the self-
correction example, every occurrence of 27 in P(pi1) is
replaced with 27 and vice versa). We have a set of rules
of the form X ?{Y1, . . . ,Ym}, which means that the vari-
able X ? Var(P(pi1)) is replaced by the set of variables
{Y1, . . . ,Ym} ? Var(P(pi2)). We assume that X can?t de-
pend on {Y1, . . . ,Ym} before the Correction is performed.
Then replacement proceeds as follows:
13. If Pa(X) = /0, we add the description D N |= Yk ?
Y k(CPT (Yk)) for all k ? {1, . . . ,m} and remove X ?
X(CPT (X)) (or X ? X(CPT (X))). Otherwise, we
replace every description ofCPT (X) with an equiv-
alent statement using Yk (to describe CPT (Yk)), for
all k ? {1, . . .m}.
The specific target of the correction behaves similarly to
the target of a QAP. In some cases we don?t know the
target, in which case we apply rule 12.
Plan-Correction can also lead to the modification of an
agent?s own plan because of other agent?s proposals. In
this case it corrects the list of parent variables on which
a preference depends. We call that list of variables the
operative variables. Once the operative variables are
changed, Plan-Correction can elaborate a plan if some
new preferences are expressed. For example, all agents
have agreed to meet next week, so in their CP-net descrip-
tion, there is the entry Week1?Week1. Then discussion
shows that their availabilities are not compatible and one
of them says "okay, that week is not going to work.". That
does not mean the agent prefersWeek1 toWeek1 because
both agreed on Week1 as preferable. Rather, Week1 has
been removed as an operative variable in the following
discourse segments. This leads us to the following rule:
14. For Plan-Correction(pi1,pi2) which corrects
the list of parent variables, the operative vari-
able list becomes the intersection of all Pa(X)
where X ? Var(P(pi1)). We can now apply
Commit(Plan-Elab(pi1,pi2),D N ), if P(pi2) contains
some new preferences ?. If the CPT affected by a
rule has no entry for the current operative variable
list O , then O : ? has to be added to D N .
Continuation, Contrast and Q-Cont pattern with the
rule for Elab. Alternation patterns with rule 8.b.6 Expla-
nation, Explanation*,Result, Qclar (clarification ques-
tion), Commentary, Summary and Acknowledgment
6The rule for Alternative questions like Do you want fish or chicken?
is a special case yielding ???(P(pi)), but we don?t offer details here.
210
either do nothing or have the same effect on preference
elicitation as Elab. Sometimes, adding these preferences
via the Elab rule may yield an unsatisfiable CP-net de-
scription, because an implicit correction is involved. If an
evaluation of the CP-net (see next section) is performed
after a processing of one of these rules shows that the
CP-net description is not satisfiable, then we apply the
rule 13, associated with Correction.
6 From Descriptions to Models
Each dialogue turn adds constraints monotonically to the
descriptions of the CP-nets to which the dialogue partic-
ipants commit. We have interpreted each new declared
variable in our rules as independent, which allows us to
give a domain independent description of preference elic-
itation. However, when it comes to evaluating a CP-net
description for satisfiability, we need to take into account
various axioms about preference (irreflexivity and transi-
tivity), and axioms for the domain of conversation: in our
case, temporal designations (Wednesdays are not Tues-
days and so on). This typically adds dependencies among
the variables in the description. In the case of the Verb-
mobil domain, since the variable Monday means essen-
tially "to meet on Monday", Monday implies Meet , and
this must be reflected via a dependency in the CP-net: we
must view the variable Meet as filling a hidden slot in
the variable Monday in the preference description, Meet :
Mon?Mon. This likewise allows us to fill in the negative
clauses of the CP-net description: we can now infer that
Meet : Mon ? Mon. These axioms also predict certain
preference descriptions to be unsatisfiable. For instance,
if we have Mon ?Mon, our axioms imply Mon ? Tues,
Mon ?Wed, etc. At this point we can calculate, ceteris
paribus, inconsistencies on afternoons and mornings of
particular days.
Domain knowledge also allows us to collapse Boolean
valued variables that all denote, say, days or times of the
day into multiple valued variables. So for instance, our
domain independent algorithm from dialogue moves to
preference descriptions might yield:
(4) Meet?31.01?30.01?02.02: am? am
Domain knowledge collapses all Boolean variables for
distinct days into one variable with values for days to get:
(5) Meet?02.02: am? pm
This leads to a sizeable reduction in the set of variables
that are used in the CP-net.
We can test any CP-net description for satisfiability by
turning the description formulas into CP-net entries. Our
description automatically produces a directed graph over
the parent variables. We have to check that the ? state-
ments form an irreflexive and transitive relation and that
each variable introduced into the CP-net has a preference
entry consistent given these constraints. If the description
does not yield a preference entry for a given variable X ,
we will add the indifference formula X ? X as the entry.
If our CP-net description meets these requirements, this
procedure yields a minimal CP-net. Testing for satisfia-
bility is useful in eliciting preferences from several dis-
course moves like Explanation, Qclar or Result, since in
the case of unsatisfiability, we will exploit the Correction
rule 13 with these moves.
7 Evaluation of the proposed method
We evaluate our method by testing it against the judg-
ments of three annotators on three randomly chosen un-
seen test dialogues from the Verbmobil corpus. The
test corpus contains 75 EDUs and the proportion of dis-
course relations is the same as in the corpus overall. The
three annotators were naive in the sense that they were
not familiar with preference representations and prefer-
ence reasoning strategies. For each dialogue segment,
we checked if the judges had the same intuitions that we
did on: (i) how commitments to preferences are extracted
from EDUs, and (ii) how preferences evolve through dia-
logue exchange.
The judges were given a manual with all the instructions
and definitions needed to make the annotations. For ex-
ample, the manual defined preference to be "a notion of
comparison between one thing at least one other". The
manual also instructs annotators to label each EDU with
the following four bits of information: (1) preferences
(if any) expressed in the EDU; (2) dependencies between
preferences expressed in the EDU; (3) dependencies be-
tween preferences in the current EDU and previous ones;
and (4) preference evolution (namely, the appearance of
a new factor that affects preferred outcomes, update to
preferences over values for an existing factor, and so on).
For each of these four components, example dialogues
were given for each type of decision they would need to
make, and instructions were given on the format in which
to code their judgements. Appendix A shows an example
of an annotated dialogue.
Table 2 presents results of the evaluation of (i). For each
EDU, we asked the annotator to list the preferences ex-
pressed in the EDU and we compared the preferences ex-
tracted by each judge with those extracted by our algo-
rithm. The triple (a, b, c) respectively indicates the pro-
portion of common preferences (two preference sets ?i
and ? j are common if (?i = ? j) or (?x ? ?i,y ? ? j ,x?
y)?for example, the preference MeetBefore2?MeetAt2
implies MeetAt2 ? MeetAt2), the proportion of prefer-
ences that one judge extracts and the other judge or our al-
gorithm misses and the proportion of preferences missed
by one judge and extracted by the other judge or by our
algorithm. The average annotator-algorithm agreement
(AAA) is 75.6% and the average inter-annotator agree-
211
Our algorithm J1 J2 J3 % of EDUs that commit to preferences
Our algorithm (83, 4, 13) (91, 0, 9) (91, 0, 9) 76%
J1 (83, 13, 4) (85, 7, 8) (91, 4, 5) 80%
J2 (91, 9, 0) (85, 8, 7) (92, 4, 4) 86%
J3 (91, 9, 0) (91, 5, 4) (92, 4, 4) 84%
Table 2: Evaluating how preferences are extracted from EDUs.
Our algorithm J1 J2 J3
Our algorithm (85, 71) (96, 100) (93, 86)
J1 (85, 71) (89, 71) (91, 86)
J2 (96, 100) (89, 71) (98, 86)
J3 (93, 86) (91, 86) (98, 86)
Table 3: Evaluating how preferences evolve through dialogue.
ment (IAA) is 77.9%; this shows that our method for ex-
tracting preferences from EDUs is reliable.
The evaluation (ii) proceeds as follows. For each EDU, we
ask the judge if the segment introduces new preferences
or if it updates, corrects or deletes preferences commited
in previous turns. As in (i), judges have to justify their
choices. Table 3 presents the preliminary results where
the couple (a,b) indicates respectively the proportion of
common elaborations (preference updates or new prefer-
ences) and the proportion of common corrections. Since
elaboration is also applied in case of other discourse re-
lations (e.g., Q-Elab), the measure a evaluates the rules
8, 9, 10 (yes) and 11. Similarly, the measure b evalu-
ates the rules 10 (no), 13 and 14. We obtain AAA=91%
IAA=92.7% for elaboration and AAA=85.7% IAA=81%
for correction.
8 Conclusion
We have proposed a compositional method for elicit-
ing preferences from dialogue consisting of a domain-
independent algorithm for constructing a partial CP-net
description of preferences, followed by a domain-specific
method for identifying the minimal CP-net satisfying the
partial description and domain constraints. The method
supports qualitative and partial information about prefer-
ences, with CP-nets benefiting from linear algorithms for
computing the optimal outcome from a set of preferences
and their dependencies. The need to compute intentions
from partially defined preferences is crucial in dialogue,
since preferences are acquired and change through dia-
logue exchange.
Our work partially confirms that CP-nets have a certain
naturalness, as the map from dialogue moves to prefer-
ences using the CP-net formalism is relatively intuitive.
The next step is to implement our method. This depends
on extracting discourse structure from text, which, though
difficult, is becoming increasingly tractable for simple
domains (Baldridge and Lascarides, 2005b). We plan to
extract CP-net descriptions from EDUs and to evaluate
these descriptions using "multi-valued variables" auto-
matically. We will then evaluate our method on a large
number of dialogues.
Our work here is also and more generally a first step to-
wards modelling the complex interaction between what
agents say, what their preferences are, and what they take
the preferences of other dialogue agents to be. It leads
to a conception of dialogue that?s more general than one
based purely on Gricean cooperative principles (Grice,
1975). On a purely Gricean approach, conversation is
cooperative in at least two ways: a basic level concern-
ing the conventions that govern linguistic meaning (ba-
sic cooperativity); and a level concerning shared attitudes
towards what is said, including shared intentions (con-
tent cooperativity). While basic cooperation is needed
for communication to work at all, content cooperativ-
ity involves strongly cooperative axioms like Coopera-
tivity (interlocutors normally adopt the speaker?s inten-
tions) (Allen and Litman, 1987, Grosz and Sidner, 1990,
Lochbaum, 1998). Our approach allows for divergent
preferences and divergent intentions, i.e. conversations
that aren?t based on content cooperativity. This will al-
low us to exploit information about conflicting agents?
preferences and game-theoretic techniques that are inher-
ent in the logics of CP-nets for computing optimal moves
(Bonzon, 2007). And in contrast to Franke et al (2009),
who analyse conversations where content cooperativity
doesn?t hold using a game-theoretic framework, our ap-
proach allows for partial and qualitative representations
of preferences rather than demanding complete and quan-
titative representations of them.
212
References
J. Allen and D. Litman. A plan recognition model for
subdialogues in conversations. Cognitive Science, 11
(2):163?200, 1987.
N. Asher and A. Lascarides. Logics of Conversation.
Cambridge University Press, 2003.
J. Baldridge and A. Lascarides. Annotating discourse
structures for robust semantic interpretation. In Pro-
ceedings of the Sixth International Workshop on Com-
putational Semantics (IWCS), Tilburg, The Nether-
lands, 2005a.
J. Baldridge and A. Lascarides. Probabilistic head-driven
parsing for discourse structure. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 96?103, 2005b.
E. Bonzon. Mod?lisation des Interactions entre Agents
Rationnels: les Jeux Bool?ens. PhD thesis, Universit?
Paul Sabatier, Toulouse, 2007.
C. Boutilier, R.I. Brafman, C. Domshlak, H.H. Hoos, and
David Poole. Cp-nets: A tool for representing and
reasoning with conditional ceteris paribus preference
statements. Journal of Artificial Intelligence Research,
21:135?191, 2004.
M. Franke, T. de Jager, and R. van Rooij. Relevance in
cooperation and conflict. Journal of Logic and Lan-
guage, 2009.
J. Ginzburg. The Interactive Stance: Meaning for Con-
versation. CSLI Publications, to appear.
H. P. Grice. Logic and conversation. In P. Cole and
J. L. Morgan, editors, Syntax and Semantics Volume
3: Speech Acts, pages 41?58. Academic Press, 1975.
B. Grosz and C. Sidner. Plans for discourse. In J. Mor-
gan P. R. Cohen and M. Pollack, editors, Intentions in
Communication, pages 365?388. MIT Press, 1990.
Y. He and S. Young. Spoken language understsanding
using the hidden vector state model. Speech Commu-
nication, 48(3-4):262?275, 2005.
J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. Inter-
pretation as abduction. Artificial Intelligence, 63(1?2):
69?142, 1993.
A. Lascarides and N. Asher. Agreement, disputes and
commitment in dialogue. Journal of Semantics, 26(2):
109?158, 2009.
O. Lemon and O. Pietquin. Machine learning for spoken
dialogue systems. In Interspeech, 2007.
K. E. Lochbaum. A collaborative planning model of in-
tentional structure. Computational Linguistics, 24(4):
525?572, 1998.
W. C. Mann and S. A. Thompson. Rhetorical structure
theory: A framework for the analysis of texts. Interna-
tional Pragmatics Association Papers in Pragmatics,
1:79?105, 1987.
M. Poesio and D. Traum. Towards an axiomatisation of
dialogue acts. In J. Hulstijn and A. Nijholt, editors,
Proceedings of the Twente Workshop on the Formal Se-
mantics and Pragmatics of Dialogue. 1998.
E. Rasmusen. Games and Information: An Introduction
to Game Theory. Blackwell Publishing, 2007.
W. Wahlster, editor. Verbmobil: Foundations of Speech-
to-Speech Translation. Springer, 2000.
213
Appendix A : Treatment of an example
We illustrate in this section how our rules work on an
example. Since this dialogue was also evaluated by our
judges (cf section 7), we give where relevant some details
on those annotations. The example is as follows:
(6) pi1. A: so, I guess we should have another meet-
ing
pi2. A: how long do you think it should be for.
pi3. B: well, I think we have quite a bit to talk
about.
pi4. B: maybe, two hours?
pi5. B: how does that sound.
pi6. A: deadly,
pi7. A: but, let us do it anyways.
pi8. B: okay, do you have any time next week?
pi9. B: I have got, afternoons on Tuesday and
Thursday.
pi10. A: I am out of Tuesday Wednesday Thurs-
day,
pi11. A: so, how about Monday or Friday
Table 4 is the DSDRS associated with (6).
Relation(pii, [pi j ? pik]) indicates that a rhetorical re-
lation holds between the segment pii and a segment
consisting of pi j, pi j+1, . . . , pik
pi1 provides an atomic preference. We apply the rule
1 and so CommitA(pi1,D N A) adds the description
D N A |=M ?M(CPT (M)) where M means Meet.
pi2 We have Q-Elab(pi1, pi2). A continues to commit to
M in pi2 and no new preferences are introduced by
pi2. We apply rule 9, which makes the P(pi2) de-
scription the same as P(pi1)?s.
pi3 is linked to pi2 with QAP. B accepts A?s preference
and we apply the rule 11 since pi2 is a wh-question.
Thus CommitB(ElabB(pi2,pi3),D N B) adds the de-
scription D N B |= M ?M(CPT (M)). It is interest-
ing to note that some judges consider that agent?s
utterance in pi3 indicates a preference towards "talk-
ing a long time" while other judges consider, as our
method predicts, that this segment does not convey
any preference.
pi4 is linked to pi3 by Q-Elab. B commits to a new
preference. We apply rule 9, rule 8 and then rule
8.a. The preference on the hour is now dependent
on the preference on meeting; i.e., D N B |= M :
2h ? 2h(CPT (2h)), where the variable 2h means
two hours.
pi5 is related to pi4 with the Q-Cont relation. We
then follow the same rule as the continued relation,
namely Q-Elab. We apply rule 9 which does not
change the CP-net description of B because pi5 does
not convey any preference.
pi6 is related to pi5 with QAP relation. In this case, it?s
not clear what is the QAP target and so we apply
rule 12: we wait to learn more and we do not change
B?s CP-net description.
All the Judges indicated that segments pi5 and pi6
are ambiguous and therefore hesitated to say if they
commit to preferences. For example in pi6, do we
have a preference for meeting more than 2 hours
or less than 2 hours? This indecision is compatible
with the predictions of rule 12.
pi7 A accepts B?s preference. We apply rule 9 and then
rule 8 to obtain:
D N A |=M ?M(CPT (M)),
D N A |=M : 2h? 2h(CPT (2h)).
pi8 is linked to pi7 by Q-Elab. B introduces a new pref-
erence for meeting next week.
We apply rule 9 and then 8 to obtain:
D N B |=M ?M(CPT (M)),
D N B |=M : 2h? 2h(CPT (2h)),
D N B |=M?2h :NW ?NW (CPT (NW ))where the
variable NW means next week.
pi9 is linked to pi8 by Plan-Elab. pi9 expresses com-
mitments to preference that already involve a
CP-net description. B introduces three prefer-
ences: one for meeting on Tuesday, the other
for meeting on Thursday and given the conjunc-
tion of preferences Tues ? Thurs, one for time
afternoon (of Tuesday and Thursday). That is,
((?(Tues)??(Thurs)) 7? Aft)(P(pi9)). We apply
the equivalence (3) and obtain :
(?(Tues?Thurs)? Aft)(P(pi9)).
Then, we apply rules 8.g, 8.b and 8.d. The CP-net
description of B is thus updated as follows:
D N B |= M ? 2h ? NW ? Tues : Thurs ?
Thurs(CPT (Thurs)),
D N B |= M ? 2h ? NW ? Tues : Thurs ?
Thurs(CPT (Thurs)),
D N B |= M ? 2h ? NW ? Thurs : Tues ?
Tues(CPT (Tues)),
D N B |= M ? 2h ? NW ? Thurs : Tues ?
Tues(CPT (Tues)),
D N B |= M ? 2h ? NW ? (Thurs ? Tues) : Aft ?
Aft(CPT (Aft)).
Most judges express here a preference ranking over
outcomes. For instance, if B elaborates by adding
the preference "I have got Monday morning too"
(as it is in the test corpus), some consider the rank-
ing "(Tuesday or Thursday afternoons) ? (Monday
214
Turn A?s SDRS B?s SDRS
1 pi1A : Q-Elab(pi1,pi2) /0
2 pi1A:is the same as in turn 1 pi2B : Q-Elab(pi1, [pi2?pi5])?QAP(pi2, [pi3?pi5])?
Q-Elab(pi3,pi)
pi : Q-Cont(pi4,pi5)
3 pi3A : Q-Elab(pi1, [pi2?pi7])?QAP(pi2, [pi3?pi7])? pi2B: is the same as in turn 2
Q-Elab(pi3, [pi4,pi7])?QAP(pi,pi?)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
4 pi3A: is the same as in turn 3 pi4B : Q-Elab(pi1, [pi2?pi9])?QAP(pi2, [pi3?pi9])?
Q-Elab(pi3, [pi4?pi9])?QAP(pi, [pi6?pi9])?
Q-Elab(pi?,pi??)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
pi?? : Plan-Elab(pi8,pi9)
5 pi5A : Q-Elab(pi1, [pi2?pi11])?QAP(pi2, [pi3?pi11])? pi4B: is the same as in turn 4
Q-Elab(pi3, [pi4?pi11])?QAP(pi, [pi6?pi11])?
Q-Elab(pi?, [pi8?pi11])?QAP(pi??,pi???)
pi : Q-Cont(pi4,pi5),pi? : Contrast(pi6,pi7)
pi?? : Plan-Elab(pi8,pi9),pi??? : Q-Elab(pi10,pi11)
Table 4: The DSDRS for Dialogue (6).
morning)? (other days)", while others consider the
ranking "(Tuesday or Thursday afternoon) or (Mon-
day morning)? (other days)". We did not treat such
preference ranking.
pi10 is related to pi9 by QAP where A answers no to B?s
question asked in pi8. We apply rule 10 (no). Since
Tues&Weds&Thurs(P(pi10)) is not consistent with
((?(Tues) ??(Thurs)) 7? Aft)(P(pi9)), we apply
CommitA(Correction(pi9,pi10),D N A), which adds
the preference Weds to A?s description and then
the rule 13 where Tues and Thurs are respectively
replaced by Tues and Thurs :
D N A |=M?2h?NW : Tues? Tues(CPT (Tues)),
D N A |= M ? 2h ? NW : Thurs ?
Thurs(CPT (Thurs)),
D N A |= M ? 2h ? NW : Weds ?
Weds(CPT (Weds)).
pi11 Finally, this segment is linked to pi10 with Q-Elab
where Mond?Fri(P(pi11)). We apply rules 9 and
8.b and update A?s CP-net description as follows:
D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :
Mond ?Mond(CPT (Mond)),
D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :
Mond ?Mond(CPT (Mond)),
D N A |= M ? 2h ? NW ? Tues ? Thurs ?Weds ?
Mond : Fri? Fri(CPT (Fri)),
D N A |= M ? 2h ? NW ? Tues ? Thurs ?Weds ?
Mond : Fri? Fri(CPT (Fri)).
The evaluation of this dialogue also reveals to what extent
naive annotators reason with binary (Monday preferred
to not Monday) or multi-valued variables (Monday pre-
ferred to Tuesday). Most judges use multi-valued vari-
ables to express the preference extracted from an EDU,
and the way in which our method exploits domain knowl-
edge to yield the minimal CP-net satisfying the descrip-
tion reflects this. In addition, some judges use a small
set of variables (for example the variable time of meeting
that groups together the notion of week, day, hours, etc.)
while others use a distinct variable for each preference.
Finally, we also noticed that judges do not describe the
same preference dependencies. For example, in:
(7) We could have lunch together and then have the
meeting from one to three?
some consider that the preference on having lunch is in-
dependent from the preference on the meeting (in this
case, they consider that the preference on the period one
to three is independent from the preference on meeting)
while others consider that the two preferences are depen-
dent.
215
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 10?18, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
How do Negation and Modality Impact on Opinions?
Farah Benamara1 Baptiste Chardon1,2 Yannick Mathieu3 Vladimir Popescu1 Nicholas Asher1
1 IRIT, Univ. Toulouse, France
{benamara,popescu,asher}@irit.fr
2 Synapse De?veloppement, Toulouse, France
baptiste.chardon@synapse-fr.com
3 LLF-CNRS, Paris, France
yannick.mathieu@linguist.jussieu.fr
Abstract
In this paper, we propose to study the effects
of negation and modality on opinion expres-
sions. Based on linguistic experiments in-
formed by native speakers, we distill these ef-
fects according to the type of modality and
negation. We show that each type has a spe-
cific effect on the opinion expression in its
scope: both on the polarity and the strength for
negation, and on the strength and/or the degree
of certainty for modality. The empirical re-
sults reported in this paper provide a basis for
future opinion analysis systems that have to
compute the sentiment orientation at the sen-
tence or at the clause level. The methodology
we used for deriving this basis was applied
for French but it can be easily instantiated for
other languages like English.
1 Introduction
Negation and modality are complex linguistic phe-
nomena widely studied in philosophy, logic and lin-
guistics. From an NLP perspective, their analy-
sis has recently become a new research area. In
fact, they can be beneficial to several NLP appli-
cations needing deep language understanding, such
as sentiment analysis, textual entailment, dialogue
systems and question answering. Handling negation
and modality in NLP applications roughly involves
two sub-tasks: (i) identifying these expressions and
their scope and (ii) analyzing their effect on mean-
ing and how this effect can help to improve text un-
derstanding. In this paper, we deal with the second
task focusing on fine-grained sentiment analysis of
French opinion texts.
Negation and modality function as operators mod-
ifying the meaning of the phrases in their scope.
Negation can be used to deny or reject statements.
It is grammatically expressed via a variety of forms:
using prefixes (?un-?, ?il-?), suffixes (?-less?), nega-
tor words, such as ?not? and negative polarity items
(NPIs), which are words or idioms that appear in
negative sentences, but not in their affirmative coun-
terparts, or in questions, but not in assertions, for
example ?any?, ?anything?, ?ever?. Negation can
also be expressed using some nouns or verbs where
negation is part of their lexical semantics (as ?abate?
and ?eliminate?), or expressed implicitly without us-
ing any negative words, as in ?this restaurant was
below my expectations?. Modality can be used to
express possibility, necessity, permission, obligation
or desire. It is grammatically expressed via adver-
bial phrases (?maybe?, ?certainly?), conditional ver-
bal moods and some verbs (?must?, ?can?, ?may?).
Adjectives and nouns can also express modality (e.g.
?a probable cause?).
Negation and modality can aggregate in a va-
riety of ways: (1) multiple negatives, e.g, ?This
restaurant never fails to disappoint on flavor?. In
some languages, double negatives cancel the effect
of negation, while in negative-concord languages
like French, double negations usually intensify the
effect of negation. (2) cumulative modalities, as in
?You definitely must see this movie? and (3) both
negation and modality, as in ?you should not go see
this movie?.
Several reports have shown that negations and
modalities are sentiment-relevant (Wiegand et al,
2010). Kennedy and Inkpen (2006) point out that
10
negations are more sentiment-relevant than dimin-
ishers. Wilson et al (2009) show that modalities as
well as negations are good cues for opinion identifi-
cation. Given that the sentiment-relevance of nega-
tions and modalities is an established fact, this paper
aims to go further by exploring how this relevance is
distilled according to the semantics of each operator.
To this end, we first study several taxonomies
along with their associated categories of both modal-
ity and negation given by the linguistic literature.
Among these categories, we decide to choose the
categories of (Godard, to appear) for negations. For
modalities, we rely on the categories of (Larreya,
2004) and (Portner, 2009). We thus distinguish
three types of negation: negative operators, negative
quantifiers and lexical negations and three types of
modality: buletic, epistemic and deontic. We show
that each type has a specific effect on the opinion
expression in its scope: both on the polarity and
the strength for negation, and on the strength and/or
the degree of certainty for modality. These effects
are structured as a set of hypotheses that we empiri-
cally validated via several linguistic experiments in-
formed by native speakers. This evaluation method-
ology has already been used in sentiment analysis.
Greene and Resnik (2009) chose psycholinguistic
methods for assessing the connection between sen-
tence structure and implicit sentiment. Taboada et
al. (2011) used Mechanical Turk to check subjective
dictionaries for consistency.
The empirical results reported in this paper pro-
vide a basis for future opinion analysis systems that
have to compute the sentiment orientation at the sen-
tence or at the clause level. The methodology we
used for deriving this basis was applied for French
but it can be easily instantiated for other languages
like English. In this paper, all examples are in
French along with their direct translation in English.
Note however that there are substantial semantic dif-
ferences between the two languages.
2 Related Work
2.1 Negation in Sentiment Analysis
Research efforts using negation in sentiment anal-
ysis can be grouped according to three main crite-
ria: the effect of negation on opinion expressions,
the types of negation used and the method employed
to update the prior polarity of opinion expressions.
According to the first criterion, most approaches
treat negation as polarity reversal (Polanyi and Za-
enen, 2006; Wilson et al, 2005; Moilanen and Pul-
man, 2007; Choi and Cardie, 2008). However, nega-
tion cannot be reduced to reversing polarity. For ex-
ample, if we assume that the score of the adjective
?excellent? is +3, then the opinion score in ?this stu-
dent is not excellent? cannot be -3. It rather means
that the student is not good enough. Hence, dealing
with negation requires to go beyond polarity rever-
sal. Liu and Seneff (2009) propose a linear additive
model that treats negations as modifying adverbs. In
the same way, in (Taboada et al, 2011), the negation
of an opinion expression shifts the value of its score
to the opposite polarity by a fixed amount. Thus a +2
adjective is negated to a -2, but the negation of a very
negative adjective is only slightly positive. Based
on (Taboada et al, 2011)?s shift model, Yessenalina
and Cardie (2011) propose to represent each word
as a matrix and combine words using iterated ma-
trix multiplication, which allows for modeling both
additive (for negations) and multiplicative (for in-
tensifiers) semantic effects. In our framework, we
assume, as in (Liu and Seneff, 2009) and (Taboada
et al, 2011), that negation affects both the polarity
and the strength of an opinion expression. However,
unlike other studies, we distill that effect depending
on the type of the negation.
Two main types of negation were studied in
the literature: negators such as ?not? and content
word negators such as ?eliminate? (Choi and Cardie,
2008). Wilson et al (2009) also consider negators
and in addition distinguish between positive polarity
shifters and negative polarity shifters since they only
reverse a particular polarity type. Few studies take
into account other types of negation. Among them,
Taboada et al (2011) treat NPIs (as well as modali-
ties) as ?irrealis blockers? by ignoring the semantic
orientation of the word in their scope. For example,
the opinion word ?good? will just be ignored in ?any
good movie in this theater?. We think that ignoring
NPIs is not suitable and a more accurate analysis is
needed. In addition, to our knowledge, no studies
have investigated the effect of multiple negatives on
opinions.
Finally, methods dealing with negation can be
classified into three categories (Wiegand et al,
11
2010). In the shallow approach, negation is embed-
ded into a bag-of-words model which is then used
by supervised machine-learning algorithms for po-
larity classification (Pang et al2002; Ng et al 2006).
This method, rather simple, seems linguistically in-
accurate and increases the feature space with more
sparse features. The second approach concerns a
local contextual analysis of valence shifter terms
where negation modifies the prior scores of those
terms (Taboada et al, 2011; Wilson et al, 2009).
The last approach uses semantic composition where
the polarities of words within the sentence are aggre-
gated (Moilanen and Pulman, 2007). In this paper,
we provide a way of treating negation and modality
in a semantic composition framework.
2.2 Modality in Sentiment Analysis
In sentiment analysis, the presence of modalities can
be used as a feature in a machine learning setting
for sentence-level opinion classification. Among the
few research efforts in this direction, Wilson et al
(2009) use a list of modal words. In (Kobayakawa
et al, 2009), modalities are defined in a flat taxon-
omy: request, recommendation, desire, will, judg-
ment, etc. According to the reported results, the gain
brought by the modalities seems difficult to assess.
However, to our knowledge, no work has investi-
gated how modality impacts on opinions.
In NLP, modality is less addressed than other lin-
guistic operators, such as negations. Most of the
computational studies involving modality are fo-
cused on: (i) building annotated resources in terms
of factuality information and (ii) uncertainty mod-
eling and hedge detection in texts. Among anno-
tated resources, we cite the FactBank corpus (Saur??
and Pustejovsky, 2009) and the BioScope corpus
(Vincze et al, 2008). In the second research strand,
the efforts go from detecting uncertainty in texts
(Rubin, 2010), to finding hedges and their scopes
in specialized corpora (Vincze et al, 2008; Gan-
ter and Strube, 2009; Zhao et al, 2010). However,
there is only partial overlapping between hedges and
modal constructions. Hedges are linguistic means
whereby the authors show that they cannot back
their opinions with facts. Thus, hedges include
certain modal constructions (especially epistemic),
along with other markers such as indirect speech,
e.g., ?According to certain researchers,...?. On the
other hand, there are modal constructions which are
not hedges, e.g. when expressing a factual possibil-
ity, without uncertainty on behalf of the speaker, e.g.
may in ?These insects may play a part in the repro-
duction of plants as well?.
3 Dealing with Negation
Negation has been well studied in linguistics (Horn,
1989; Swart, 2010; Giannakidou, 2011). For
French, we cite (Muller, 1991; Moeschler, 1992;
Corblin and Tovena, 2003) and (Godard, to ap-
pear)?s work as part of the ?Grande Grammaire
du franc?ais? project (Abeille? and Godard, 2010).
Our treatment of negation is based on the lexical-
syntactic classification of (Godard, to appear) that
distinguishes three types of negation in French:
? Negative operators, denoted by NEG: they
are the adverbs ?pas? (?not?), ?plus? (?no
more?), ?non? (?no one?), the preposition
?sans? (?without?) and the conjunction ?ni?
(?neither?). These operators always appear
alone in the sentence and they cannot be com-
bined with each other.
? Negative quantifiers, denoted by NEG quant,
express both a negation and a quantifica-
tion. They are, for example, the nouns
and pronouns ?aucun? (?none?), ?nul? (?no?),
?personne? (?nobody?), ?rien? (?nothing?)
and the adverbs ?jamais? (?never?) and
?aucunement?/?nullement? (?in no way?).
Neg quant have three main properties: (i)
they can occur in positive sentences (that is not
negated), particularly in interrogatives, when
they are employed as indefinite or when they
appear after the relative pronoun ?que? (?that?)
(ii) in negative contexts, they are always associ-
ated to the adverb ?ne? (?not?) and (iii) they can
be combined with each other as well as with
negative operators. Here are some examples
of this type of negation extracted form our cor-
pus: ?on ne s?ennuie jamais? (?you will never
be bored?), ?je ne recommande cette se?rie a`
personne? (?I do not recommend this movie to
anyone?).
? Lexical negations denoted by NEG lex which
are implicit negative words, such as ?manque
12
de? (?lack of?), ?absence de?(?absence of?),
?carence? (?deficiency?), ?manquer de? (?to
lack?), ? de?nue? de? (?deprived of?). NEG lex
can be combined with each other as well as
with the two previous types of negation.
This classification does not cover words such as
few or only, since we consider them as weak inten-
sifiers (strength diminishers) rather than negations.
For each opinion expression exp, OP(exp)
indicates that the expression exp is in the
scope of the negation OP ? NEG, NEG quant,
NEG lex. Multiple negations are denoted by
OP i(OP j((exp))). In French, there are at
most three negative words in a multiple negative.
However, this case is relatively rare in opinion texts;
this is why, we only deal with two negatives. Usu-
ally, multiple negatives preserve polarity, except for
those composed of NEG lex and NEG quant or
NEG which cancel the effect of NEG lex. For ex-
ample, in ?manque de gou?t? (?lack of taste?), i.e
NEG lex(taste), the polarity is negative, while
in ?il ne manque pas de gou?t? (roughly, ?no lack of
taste?), i.e. NEG(NEG lex(taste)), the opin-
ion is positive. This property was also observed
in (Rowan et al, 2006). Thus, multiple negatives
preserving negation concern the following combina-
tions:
NEG quant(NEG quant(exp)),
NEG quant(NEG(exp)),
NEG(NEG quant(exp)).
We analyse the frequency of our negation cate-
gories in a corpus of French opinion texts. We use a
manually built subjective lexicon (Benamara et al,
2011) that contains 95 modalities and 21 negations.
An analysis of a corpus of 26132 French movie re-
views (about 863 TV series) extracted from the al-
locine? web site1 shows that around 26 % of reviews
contain NPIs and/or multiple negations.
3.1 Hypotheses
The effects of each negation type are based on the
following hypotheses:
N1.a The negation always reverses the polarity
of an opinion expression, that is a positive opinion
expression becomes negative when in the scope of
1http://www.allocine.fr
a negation. For example, ?exceptionnel? (?excep-
tional?) and ?pas exceptionnel? (?not exceptional?).
N1.b The strength of an opinion expression in the
scope of a negation, is not greater than of the opin-
ion expression alone. For example, for the adjec-
tive ?exceptionnel? (?exceptional?), the strength of
its negation, that is ?pas exceptionnel? (?not excep-
tional?), is lower.
N2. The strength of an expression when in the
scope of a NEG quant is greater than when in the
scope of a NEG. For instance: ?jamais exceptionnel?
(?never exceptional?) is stronger than ?pas excep-
tionnel? (?not exceptional?).
N3. NEG lex has the same effect as NEG, as for
lack of taste and no taste.
N4. The strength of an expression when in the
scope of multiple negatives is greater than when in
the scope of each negation alone. For example, ?plus
jamais bon? (?no longer ever good?) is stronger than
?plus bon? (?no longer good?).
3.2 The experimental setup
The previous hypotheses have been empirically val-
idated by volunteer subjects through two protocols:
Protocol 1 for N1.a and N1.b, and Protocol 2 for N2
to N4 2.
Both protocols are based on a set of questions that
we built so that: (1) they reflect the most frequent
linguistic structures found in our corpus, and (2)
they do not contain words or expressions on which
people have prior opinions for/against. In addition,
the number of questions within each protocol was
designed so that we ensure a trade-off between the
amount of data needed for proving our hypotheses
and the quality of the data, subjects have to remain
focused in order to avoid errors due to tiredness.
Protocol 1. A set of six questions are shown to
subjects. In each question, an opinionated sentence
is presented, along with its negation using negative
operators, as in ?This student is brilliant? and ?This
student is not brilliant?. The strengths of the opin-
ions vary from one question to another on a dis-
crete scale. Several types of scales have been used
in sentiment analysis research, going from continu-
ous scales (Benamara et al, 2007) to discrete ones
2They are respectively available at:
http://goo.gl/CQzKy and http://goo.gl/YnZPS.
13
Figure 1: Empirical validation of N1 to N4.
(Taboada et al, 2011). Since our negation hypothe-
ses have to be evaluated against human subjects, the
chosen length of the scale has to ensure a trade-off
between a fine-grained categorisation of subjective
words and the reliability of this categorisation with
respect to human judgments. We thus use in our
framework a discrete 7-point scale, going from ?3
(which corresponds to ?extremely negative? opin-
ions) to +3 (for ?extremely positive? ones) to quan-
tify the strength of an opinion expression. Note that
0 corresponds to cases where in the absence of any
context, the opinion expression can be neither posi-
tive nor negative. A set of 81 native French speakers
were asked to indicate the strength of each sentence
in a question on the same 7-point scale.
Protocol 2. Eight questions are shown. Each
question contains a pair of sentences: one contain-
ing a negative operator, the other having either a
negative quantifier or a lexical negation, or multi-
ple negatives, as in ?This student is not brilliant?
and ?This student is never brilliant?. Subjects are
asked to compare the strengths of the sentences in
each pair. A set of 96 native French speakers partic-
ipated in this study.
3.3 Results
The results of these assessments are shown in Fig-
ure 1, as the average agreement and disagreement
between the subjects? answers and our hypotheses.
The results show that all four hypotheses are vali-
dated. For N1.a, we obtain an average agreement of
90.7 % when excluding the answers corresponding
to the strength 0 (20.37 % of all answers). We note
that for opinion strengths from ?1 to +2 (that is,
?mildly negative? to ?very positive? opinions), N1.a
is 100 % verified. The same trend is observed for?2
(?very negative?) and +3 opinion strengths (87.8 %
and 93 % agreement, respectively). However, for
?extremely negative? opinions, e.g., ?l?acteur est
nullisime? (?the actor is worthless?), we observe that
only 48.8 % of subjects reverse its polarity. The re-
sults for N1.b are shown in Table 1. The rows cor-
respond to opinion strenghts given by subjects for
sentences without negation and the columns, and the
subjects? answers to the same sentences, this time
negated. In this table, we discarded the row for
the subjects? answers to the 0-strength original sen-
tences (without negation) because the number of in-
stances was very low.
+3 +2 +1 0 -1 -2 -3
+3 0 0 4.7 32.9 58.9 3.5 0
+2 0 0 0 4.9 82.0 13.1 0
+1 0 0 0 0 84.3 14.5 1.2
-1 0 0 62.5 37.5 0 0 0
-2 0 1.2 51.9 39.5 7.4 0 0
-3 0 1.4 26.4 43.0 23.6 5.6 0
Table 1: Results (in percents) for N1.b
We observe that the hypothesis N1.b is verified
for all configurations of strengths. In addition, a
non-negligible percentage of the subjects assign a 0
strength to the negation of all negative opinion ex-
pressions. This is particularly salient for extremely
negative expressions. The same goes for extremely
positive expressions.
N2 is verified at 67 %. This might me because the
gap between the strength of NEG quant (exp)
and NEG(exp) is rather small.
N3 is verified at 43 %. This low result reflects the
fact that, as expected, for ?lack of? (i.e., ?manque
de?, very frequent in French movie reviews) N3 is
not validated: 81 % of the subjects consider the
opinion in the scope of this lexical negation to be
less negative than the opinion in the scope of the
negative operator ?not?. This disparity in the results
show that a thorougher study has to be undertaken in
order to better distill the effect of lexical negations
on opinion expressions.
Finally, N4 is verified at almost 64 %. The
disagreement comes from the question testing the
effect of the NEG quant (NEG quant) com-
bination. We think this might come from the
14
fact that NEG quant already boosts the strength
of an opinion expression, hence adding more
NEG quant does not necessarily yield an even
stronger opinion expression.
4 Dealing with Modality
Drawing partly on (Portner, 2009) and on (Larreya,
2004) for French, we have chosen to split modality
in three categories:
? buletic, denoted by Mod B ? it indicates the
speaker?s desires/wishes. This type of modality
is expressed via a closed set of verbs denoting
hope e.g. ?I wish he were kind?.
? epistemic, denoted by Mod E ? it indicates the
speaker?s belief in the propositional content he
asserts. It is expressed via doubt, possibil-
ity or necessity adverbs, such as ?peut-e?tre?
(?perhaps?), ?de?cide?ment? (?definitely?), ?cer-
tainement? (?certainly?), etc., and via the verbs
?devoir? (?have to?), ?falloir? (?need to/must?)
and ?pouvoir? (?may/can?), e.g. ?The movie
might be good?,
? deontic, denoted by Mod D ? it indicates a
possibility or an obligation (with their con-
trapositives, impossibility and permission, re-
spectively). It is only expressed via the same
modal verbs as for epistemic modality, but with
a deontic reading, e.g., ?You must go see the
movie?.
Note that this classification takes into account
neither evidential usage of modality nor epistemic
modalities expressed in conditional verb moods
since these usages are less frequent in our corpus.
Just like for negations, we project these categories
on our corpus of French movie reviews and we ob-
serve that 53 % of the reviews contain at least one
modal construction. In addition, the most frequent
modals in those reviews are in decreasing order of
occurrence: the epistemic and deontic verbs ?de-
voir? and ?pouvoir?, buletic modal verbs and epis-
temic adverbs.
Unlike for negations, for the moment we do not
take into account cumulative effects of modalities on
an opinion expression, like in: ?You definitely must
see the movie!? as well as combination of negations
and modalities.
We consider that each modal expression has a se-
mantic effect on opinions. Unlike negation, this ef-
fect is not on both the polarity and the strength of
opinions, but only on their strength ? for instance,
the strength of the recommendation ?You must go
see the movie, it?s a blast? is greater than for ?Go
see the movie, it?s a blast?, and certainty degree ?
for instance, ?This movie is definitely good? has a
greater certainty than ?This movie is good?. In our
framework, the strength is discretized on a three-
level scale, going from 1 (minimal strength) to 3
(maximal strength). The certainty degree also has
three possible values, in line with standard literature
(Lyons, 1977; Saur?? and Pustejovsky, 2009): pos-
sible, probable and certain. However, we consider
that, in an opinion analysis context, the frontier be-
tween the first two values is rather vague, hence we
conflate them into a value that we denote by uncer-
tain. We thus obtain two certainty degrees, from
which we built a three-level scale, by inserting be-
tween these values a ?default? certainty degree for
all expressions which are neither a modal nor in the
scope of a modal.
4.1 Hypotheses
We will now specify the semantic effect of each
modality type, on the strength and/or certainty de-
gree of the opinion expressions. These effects are
structured as a set of six hypotheses:
M1. Mod B alters the certainty degree of opinion
expressions in its scope. Thus, the certainty degree
of an opinion expression in the scope of a Mod B
is weaker than the certainty degree of the opinion
expression itself. e.g. in ?I hope this movie is funny?
there is less certainty than in ?This movie is funny?.
M2.1 Mod E alters the certainty degree of opinion
expressions in its scope. For adverbial Mod E, this
degree is altered according to the certainty of the re-
spective adverb: if the latter is uncertain, then the
certainty of the opinion in the scope of the adverb is
reduced; otherwise, the certainty is augmented. For
instance, ?Le film est probablement bon? (?Proba-
bly the film is good?) is less certain than ?Le film est
bon? (?The film is good?), which is, in turn, less cer-
tain than ?Le film est de?cide?ment bon? (?The film is
definitely good?).
15
M2.2 The certainty of opinion expressions when
in the scope of a verbal Mod E is always lower than
when alone. It varies according to the certainty of
the respective verb, from pouvoir ? lowest certainty,
to devoir and falloir ? greater certainty. For instance,
the certainty of ?Le film peut e?tre bon? (?the film
might be good?) is lower than of ?Le film doit e?tre
bon? (?the film must be good?), which, in turn, is
lower than of ?Le film est bon? (?the film is good?).
M2.3 The certainty degrees of opinion expres-
sions in the scope of epistemic devoir and falloir are
the same.
M3.1 Mod D alters the strength of opinion expres-
sions in its scope. Hence, strength varies according
to the verb: pouvoir reduces the strength of the opin-
ion, whereas devoir and falloir boost it.
M3.2 The strengths of opinion expressions in the
scope of deontic devoir and falloir are the same.
4.2 The experimental setup
We empirically validated the previous hypotheses
through the same methodology as for negation. We
designed three protocols, Protocol 1 for M1, Proto-
col 2 for M2.1 to M2.3, and Protocol 3 for M3.1 and
M3.2.
Protocol 1. In this protocol, five questions are
proposed. In one of them, the subject is presented
an opinionated sentence without modality. In each
of the other questions, we present a subjective sen-
tence with buletic modality. For each question, we
then ask the subject to specify whether the author of
the sentence has an established opinion (positive or
negative), e.g., ?I saw this movie yesterday. I hope
it will be a blockbuster.?, or ?The movie is interest-
ing.?, or hasn?t an established opinion yet ?I hope
this movie is interesting?. 78 native French speakers
participated in this protocol.
Protocol 2. Eight questions are proposed to sub-
jects. In each question we present an opinionated
sentence. The first one is a sentence without modal-
ity, e.g. ?The movie is good?. Each of the other
sentences contains an epistemic modality of differ-
ent certainty degree, either ?uncertain? or ?certain?.
111 native French speakers were asked whether the
modal sentence was less, more or as certain as the
sentence without modality.
Protocol 3. Four questions are presented. In each
question we show a pair of opinionated sentences:
Figure 2: Empirical validation of M1 to M3.2.
one sentence without modality, and another one with
a deontic modality, as in ?Go see this movie, it
is good? and ?You should go see this movie, it is
good?. We ask subjects compare the strengths of the
sentences in each pair. A set of 78 native French
speakers participated in this study.
4.3 Results
We show the results of these assessments in Figure
2. M1 is validated at 86.5 %. More specifically,
when the phrase in the scope of the buletic modality
denotes an event, all subjects consider it to vehic-
ulate an opinion. This, in French at least, usually
corresponds to an implicit opinion3. Moreover, ac-
cording to all subjects, buletic modality cancels the
opinion in its scope, when the phrase expressing the
latter denotes a state. Therefore, subjective words
do not make sentences like ?I hope her husband is
kind? opinionated.
M2.1 is validated at around 72 % for both cer-
tainty degrees (?certain? and ?uncertain?), which
shows that, in addition to polarity and strength, cer-
tainty is a relevant feature of an opinion expres-
sion. Concerning M2.2, almost 79 % of the subjects
validated that a phrase when outscoped by ?pou-
voir? is less certain than when outscoped by ?de-
voir?, whereas only 23 % of them consider that ?de-
voir? lowers the certainty degree of the phrase in its
scope. M2.3 is validated at around 57 % overall
since for ?devoir? (?have to?) and ?falloir? (?need
to?/?must?) subjects considered them as having the
3Implicit opinions, also called opinionated sentences (Liu,
2010), are sentences that express positive or negative opinions
and do not contain any explicit subjective clues. Here are some
examples: ?The movie is not bad, although some persons left
the auditorium? or ?Laborious and copy/paste of the first part?.
16
same effect.
M3.1 is validated to a lesser extent: 54 %. 62.5 %
of the subjects agreed with the hypothesis that deon-
tic ?pouvoir? (?may?/?can?) reduces the strength of
the opinion in its scope. This might be explained by
the ambiguity between deontic and epistemic read-
ings of these three verbs. The strengths of ?devoir?
and ?falloir? are deemed identical (M3.2) at 60 %.
The rest of 40 % are evenly split between ?devoir?
being stronger than ?falloir? and vice versa.
5 Conclusion
In this paper, we showed that the effects of modality
and negation on opinion expressions in their scope
depend on the type of these operators. Based on a
set of protocols, we empirically validated that nega-
tion affects both polarity and strength, and that neg-
ative quantifiers and multiple negations boost the
strength of the negation. We also empirically vali-
date that modality affects the strength, in case of de-
ontic modality, and the certainty degree for buletic
and epistemic modality. Our approach is novel in
two ways:
? Our treatment of negation goes beyond the ap-
proaches of (Wilson et al, 2009)(Taboada et
al., 2011) and (Liu and Seneff, 2009) since we
propose a specific treatment for negative polar-
ity items and for multiple negatives. In addi-
tion, our results for negative operators confirm,
as in (Taboada et al, 2011) and (Liu and Seneff,
2009), that the strength of an opinion expres-
sion in the scope of a negation, is not greater
than of the opinion expression alone.
? For modality, to our knowledge, our approach
is the first study dealing with the semantics of
modality for sentiment analysis.
The empirical results reported in this paper pro-
vide a basis for future opinion analysis systems that
have to compute the sentiment orientation at the sen-
tence or at the clause level.
In further work, we plan to study the effect of
cumulative modalities, as in ?you definitely must
see this movie?, and of co-occurring negation and
modality, as in ? you should not go see this movie?,
on opinion expressions. We also plan to evaluate
to what extent our empirical results extrapolate to
other languages. Finally, we will plug our results
to a computational model in order to determine the
contextual polarity of opinion expressions at the sen-
tence or clause level.
Acknowledgement
This work was supported by a DGA-RAPID project
under grant number 0102906143. We also thank all
the volunteers for participating in the experiments.
References
Anne Abeille? and Danie`le Godard. 2010. The grande
grammaire du franc?ais project. In Proceedings of
LREC?10.
Farah Benamara, Carmine Cesarano, Antonio Picariello,
Diego Reforgiato, and V. S. Subrahmanian. 2007.
Sentiment analysis: Adjectives and adverbs are better
than adjectives alone. In Proceedings of ICWSM.
Farah Benamara, Baptiste Chardon, Yannick Mathieu,
and Vladimir Popescu. 2011. Towards context-based
subjectivity analysis. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1180?1188.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
EMNLP?08, pages 793?801.
Francis Corblin and Lucia Tovena. 2003. L?expression
de la ne?gation dans les langues romanes. In D. Go-
dard., editor, Les langues romanes : proble`mes de la
phrase simple. Paris: CNRS Editions.
V. Ganter and M. Strube. 2009. Finding hedges by chas-
ing weasels: Hedge detection using wikipedia tags and
shallow linguistic features. In Proceedings of ACL-
IJCNLP?09, pages 173?176.
Anastasia Giannakidou. 2011. Positive polarity items
and negative polarity items: variation, licensing, and
compositionality. Semantics: An International Hand-
book of Natural Language Meaning.
Danie`le Godard. to appear. Les ne?gateurs. In La Grande
Grammaire du franc?ais, chapter 10.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL?09, pages 503?511.
Laurence Horn. 1989. A natural history of negation.
University of Chicago Press.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie and product reviews using con-
textual valence shifters. Computational Intelligence,
22(2):110?125.
17
Takeshi S. Kobayakawa, Tadashi Kumano, Hideki
Tanaka, Naoaki Okazaki, Jin-Dong Kim, and Jun ichi
Tsujii. 2009. Opinion classification with tree kernel
svm using linguistic modality analysis. In Proceed-
ings of CIKM?09, pages 1791?1794.
Paul Larreya. 2004. L?expression de la modalite? en
franc?ais et en anglais (domaine verbal). Revue belge
de philologie et d?histoire, 82(3):733?762.
Jingjing Liu and Stephanie Seneff. 2009. Review senti-
ment scoring via a parse-and-paraphrase paradigm. In
Proceedings of EMNLP?09, pages 161?169.
Bing Liu. 2010. Sentiment analysis and subjectivity. In
Nitin Indurkhya and Fred J. Damerau, editors, Hand-
book of Natural Language Processing, Second Edi-
tion. CRC Press, Taylor and Francis Group, Boca Ra-
ton, FL.
J. Lyons. 1977. Semantics. vol. 2. Cambridge University
Press.
Jacques Moeschler. 1992. The pragmatic aspects of lin-
guistic negation: Speech act, argumentation and prag-
matic inference. Argumentation, 6(1):51?76.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of RANLP?07, pages
378?382.
Claude Muller. 1991. La ne?gation en franc?ais. Syntaxe,
se?mantique et e?le?ments de comparaison avec les autres
langues romanes:. Droz, Gene`ve.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifters. In Computing Attitude and Affect in
Text: Theory and Applications, The Information Re-
trieval Series, pages 1?10. Springer-Verlag.
Paul Portner. 2009. Modality, volume 1. Oxford Univer-
sity Press, USA.
Nairn Rowan, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Victoria Rubin. 2010. Epistemic modality: From uncer-
tainty to certainty in the context of information seeking
as interactions with texts. Information Processing and
Management, 46(5):533?540.
Roser Saur?? and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Henriette De Swart. 2010. Expression and interpreta-
tion of negation. An OT typology. Studies in Natural
Language and Linguistic Theory, 77.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37:267?307.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The bioscope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
bioinformatics, 9(Suppl 11):S9.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT?05, pages
347?354.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3):399?433.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of EMNLP?11, pages 172?182.
Q. Zhao, C. Sun, B. Liu, and Y. Cheng. 2010. Learning
to detect hedges and their scope using crf. In Proceed-
ings of CoNLL?10, pages 100?105.
18
Proceedings of the SIGDIAL 2013 Conference, pages 2?11,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Expressivity and comparison of models of discourse structure
Antoine Venant1 Nicholas Asher2 Philippe Muller1 Pascal Denis3 Stergos Afantenos1
(1) IRIT, Toulouse University, France, (2) IRIT, CNRS, France (3) Mostrare, INRIA, France ?
Abstract
Several discourse annotated corpora now ex-
ist for NLP. But they use different, not eas-
ily comparable annotation schemes: are the
structures these schemes describe incompati-
ble, incomparable, or do they share interpre-
tations? In this paper, we relate three types
of discourse annotation used in corpora or dis-
course parsing: (i) RST, (ii) SDRT, and (iii)
dependency tree structures. We offer a com-
mon language in which their structures can be
defined and furnished a range of interpreta-
tions. We define translations between RST and
DT preserving these interpretations, and intro-
duce a similarity measure for discourse repre-
sentations in these frameworks. This will en-
able researchers to exploit different types of
discourse annotated data for automated tasks.
1 Introduction
Computer scientists and linguists now largely agree
that representing discourse structure as a hierarchical
relational structure over discourse units linked by dis-
course relations is appropriate to account for a variety
of interpretative tasks. There is also some agreement
over the taxonomy of discourse relations ?almost all
current theories include expressions that refer to rela-
tions like Elaboration, Explanation, Result, Narration,
Contrast, Attribution. Sanders, Spooren, and Noord-
man 1992; Bateman and Rondhuis 1997 discuss corre-
spondences between different taxonomies.
Different theories, however, assume different sets of
constraints that govern these representations; some ad-
vocate trees: RST Mann and Thompson 1987, DLTAG
Webber et al 1999; others, graphs of different sorts:
SDRT Asher and Lascarides 2003, Graphbank Wolf
and Gibson 2005. Consider:
(1) [?he was a very aggressive firefighter.]C1 [he
loved the work he was in,?]C2 [said acting fire
chief Lary Garcia.]C3 . [?He couldn?t be bested
in terms of his willingness and his ability to do
something to help you survive.?]C4 (from Egg
and Redeker 2010)
Using RST, Egg and Redeker 2010 provide the tree an-
notated with nuclearity features for this example (given
by the linear encoding in (s1)), while SDRT provides
?This research was supported by ERC grant 269427.
a different kind of structure (s2). Dependency trees
(DTs), similar to syntactic dependency trees and used
in Muller et al 2012 for automated parsing, give yet an-
other representation (s3). Elab stands for elaboration,
Attr for attribution, and Cont for continuation.
Elab1(Attr(Elab2(C1N ,C2S )N ,C3S )N ,C4S ) (s1)
Attr(pi,C3) ? pi :Elab(C1, pi1) ? pi1 :Cont(C2,C4) (s2)
Elab1(C1,C2) ? Attr(C1,C3) ? Elab(C1,C4) (s3)
Several corpora now exist annotated with such struc-
tures: RSTTB Carlson, Marcu, and Okurowski 2002,
Discor Baldridge, Asher, and Hunter 2007, Graph-
Bank1. But how exactly do these annotations compare?
In the illustrative example chosen and for the relation
types they agree on (Elaboration and Attribution), dif-
ferent annotation models and theoretical frameworks
invoke different numbers of instances of these relations
and assign the instances different arguments or differ-
ent scopes, at least on the surface. In this paper we de-
velop a method of comparing the scopes of relations in
different types of structures by developing a notion of
interpretation shared between different structures. This
interpretation specifies the set of possible scopes of re-
lations compatible with a given structure. This theoret-
ical work is important for furthering empirical research
on discourse. Discourse annotations are expensive. It
behooves researchers to use as much data as they can,
annotated in several formalisms, while pursuing pre-
diction or evaluation in their chosen theory. This paper
provides a theoretical basis to do this.
What a given structure expresses exactly is often not
clear; some discourse theories are not completely for-
malized or lack a worked out semantics. Neverthe-
less, in all of them rhetorical relations have semantic
consequences bearing on tasks like text summarization,
textual entailment, anaphora resolution, as well as the
temporal, spatial and thematic organization of a text
Hobbs, Stickel, and Martin 1993; Kehler 2002; Asher
1993; Lascarides and Asher 1993; Hobbs, Stickel, and
Martin 1993; Hitzeman, Moens, and Grover 1995, inter
alia. Theories like SDRT or Polanyi et al 2004 adopt a
conception of discourse structure as logical form. Dis-
course structures are like logical formulae and relations
1The Penn Discourse Treebank Prasad et al 2008 could
also be considered as a corpus with partial dependency struc-
tures.
2
function like logical operators on the meaning of their
arguments. Hence their exact scope has great semantic
impact on the phenomena we have mentioned, in ex-
actly the way the relative scope of quantifiers make a
great semantic difference in first order logic. By con-
centrating on exact meaning representations, however,
the syntax-semantics interface becomes quite complex:
as happens with quantifiers at the intra sentential level,
discourse relations might semantically require a scope
that is, at least a priori, not determined by syntactic
considerations alone and violates surface order (see s2).
Other theories like Polanyi?s Linguistic Discourse
Model (LDM) of Polanyi 1985; Polanyi and Scha 1984,
and DLTAG Webber et al 1999 explicitly adopt a
syntactic point of view, and RST with strongly con-
strained (tree-shaped) structures is subject to parsing
approaches duVerle and Prendinger 2009; Sagae 2009;
Subba and Di Eugenio 2009 that adhere to the syntac-
tic approach in adopting decoding strategies of syntac-
tic parsing. In such theories, discourse structure repre-
sentations, subject to syntactic constraints (e.g. domi-
nance of spans of text one over another) respect surface
order but do not always and unproblematically yield a
semantic interpretation that fits intuitions. According
to Marcu 1996, an RST tree is not by itself sufficient to
generate desired predictions; he employs the nuclearity
principle, NP, as an additional interpretation principle
on scopes of relations.
We focus on two theories: RST, which offers the
model for the annotations of the RST treebank Carl-
son, Marcu, and Okurowski 2002 and the Potsdam
commentary corpus Stede 2004, and on SDRT, which
counts several small corpora annotated with semantic
scopes, Discor Baldridge, Asher, and Hunter 2007 and
Annodis Afantenos et al 2012. We describe these the-
ories in section 2. We will also compare these two the-
ories to dependency tree representations of discourse
Muller et al 2012. Section 3 introduces a language for
describing semantics scopes of relations that is power-
ful enough to: i) compare the expressiveness (in terms
of what different scopes can be expressed) of the dif-
ferent formalisms considered; ii) give a formal target
language that will provide comparable interpretations
of the different structures at stake. Section 4 discusses
Marcu?s nuclearity principle and proposes an alterna-
tive way to interpret an RST tree as a set of different
possible scopes expressed in our language. Section 5
provides intertranslability results between the different
formalisms. Section 6 defines a measure of similarity
over discourse structures in different formalisms.
2 Discourse formalisms
These formalisms we introduce here all require the in-
put text to be segmented into elementary units (EDUs).
The definition of what an EDU is varies slightly with
the formalism, but roughly corresponds to the clause
level in RST, SDRT and other theories. We assume a
segmentation common to the different formalisms and
use examples with a non controversial and intuitive
segmentation.
Rhetorical Structure Theory (RST), the theory un-
derlying the RST-Treebank is the most used corpus for
discourse parsing, cf. duVerle and Prendinger 2009,
Subba and Di Eugenio 2009, inter alia.
In its Mann and Thompson 1987 formulation, RST
builds a descriptive tree for the discourse by the recur-
sive application of schemata in a bottom-up procedure.
Each schema application ideally reflects the most plau-
sible relation the writer intended between two contigu-
ous spans of text, as well as hierarchical information
about the arguments of the relation, distinguishing be-
tween nuclei as essential arguments of a relation and
satellites as more contingent parts. The set of RS Trees
is inductively defined as follows:
1- An EDU is a RS Tree.
2- if R is a nucleus-statellite relation symbol, s1 and
s2 are both RS Trees with contiguous spans (the left-
most leaf in s2 is textually located right after the right-
most one in s1), and ?a1, a2? ? {?N, S ?; ?S ,N?} then
R(t1 a1, t2 a2) is an RS Tree.
3- if R is a multinuclear relation symbol and
?s1, . . . , sn? are n RS Trees with contiguous spans then
R(s1 N, . . . , sn N) is an RS Tree.
Following Mann and Thompson 1987 a complete RS
tree makes explicit the content the author intended to
communicate. RS Trees are graphically represented
Marcu 1996 with intermediate nodes labelled with re-
lation names, leaves with symbols referring to EDUs,
and edges with nucleus/satellite distinctions.
Segmented Discourse Representation Theory
(SDRT), our second case-study theory, inherits a
framework from dynamic semantics and enriches
it with rhetorical relations. The set of SDRSs is
inductively defined as follows:
Assume a set of rhetorical relations R, distinguished
between coordinating and subordinating relations.
- Any EDU is an SDRS.
- Any Complex Discourse Unit (CDU) is a SDRS.
- a CDU is an acyclic labelled graph (A, E) where
every node is a discourse unit (DU) or SDRS and each
labelled edge is a discourse relation such that:
(i) every node is connected to some other node;
(ii) no two nodes are linked by subordinating and co-
ordinating relations,
(iii) given EDUs a1, . . . , an+1 in their textual order
that yield a CDU (A, E) = G, each EDU a j+1 j < n is
linked either: (a) to nodes on the right frontier of the
CDU G? a subgraph of G constructed from a1, . . . , a j;
or (b) to one or more nodes in G? = (A?,G?), a subgraph
of G, which linked to one or more nodes on the right
frontier of the graph G?, and where G? is constructed
from a subset of a j+2, . . . an.
The right frontier of a graph G consists of the nodes
a that are not the left arguments to any coordinating
relation and for which if any node b is linked to some
node dominating a, then there is a path of subordinating
3
relations from b to a.
A Segmented Discourse Representation Structure
(SDRS), is assigned a recursively computed meaning
in terms of context-change potential (relation between
pairs of ? world, assignation function ?) in the tradi-
tion of dynamic semantics. The semantics of a complex
constituent is compositionally defined from the seman-
tics of rhetorical relations and the interpretation of its
subconstituents. In the base case of an EDU, the se-
mantics is given in dynamic semantics.
We also consider dependency trees (DTs). Muller
et al 2012 derive DTs from the SDRSs of the ANN-
ODIS corpus to get a reduced search space, simplify-
ing automated discourse parsing. A DT is an SDRS
in which there are no CDUs and there is a unique arc
between any two nodes. Muller et al 2012 provide
a procedure from SDRSs to DTs, which we slightly
modify to respect the Frontier Contraint that they use.
? works in a bottom-up fashion replacing every CDU
X that is an argument of a rhetorical relation in ? by
their top-most immediate sub-constituent which do not
appear on the right of any relation in X, or distributing
the top relation when necessary to preserve projectivity.
To give a simple example: ?(R([R?(a, [R??(b, c)])], d)) =
?(R([R?(a, b) ? R??(b, c)], d)) = R(a, d) ? R?(a, b) ?
R??(b, c). (1) provides a more complicated example we
discuss in Section 6).
3 Describing the scope of relations
We provide here a language expressive and general
enough to express the structures of the 3 theories. All
our case-study theories involve structures described by
a list of rhetorical relations and their arguments. Two
things may vary: first, the nature of the arguments.
SDRT for instance, introduces complex constituents
as arguments of relations (e.g.
{
pi : Rsubord(b, c)
Rsubord(a, pi) ),
which finds a counterpart within RS Trees, where a
relation may directly appear as argument of another
(R(aN ,R(bN , cS )S )) but not within dependency trees.
Second, the set of constraints that restrict the possi-
ble lists of such relations can vary across theories (e.g.
right frontier, or requirement for a tree structure).
To deal with the first point above, we remark that
it suffices to list, for each instance of a discourse rela-
tion, the set of elementary constituents that belong to its
left and right scope in order to express the three kinds
of structures. We do this in a way that an isomorphic
structure can always be recovered. Models of our com-
mon language will be a list of relation instances and el-
ementary constituents, together with a set of predicates
stating what is in the scope of what. As for the second
point, we axiomatize each constraint in our common
language, thereby describing each of the 3 types of dis-
course structures as a theory in our language.
Our language contains only binary relations. Among
discourse formalisms, only RST makes serious (and
empirical) use of n?ary discourse relations. Neverthe-
less, such RST structures are expressible in our frame-
work, if we assume certain semantic equivalences.
RST allows for two cases of non-binary trees: (i) nu-
cleus with n satellites, each one linked to the nucleus
by some relation Rn. Such a structure is semantically
equivalent to the conjunction of n-binary relations Rn
between the nucleus and the nth satellite, which is ex-
pressible in our framework. (ii) RST also allows for n-
ary multinuclear relations such as List and Sequence. In
our understanding, multinuclear relations R(a1, . . . an),
essentially serve a purpose of expressiveness, and such
an n-ary tree is an equivalent to the split non-tree
shaped structure R(a1, a2) ? R(a2, a3) . . .R(a(n?1), an).
This seems clear for the Sequence relation, which
states that a1 . . . an are in temporal sequence and can
be equivalently formulated as ?each ai precedes ai+1?.
This might appear less obvious for the List relation.
The semantics (as it appears on the RST website http:
//www.sfu.ca/rst/) of this relation requires the ai to
be ?comparable?, and as far as this is a transitive prop-
erty, we can split the relation into a set of binary ones.
Formally, our scope language Lscopes is a fragment of
that of monadic second order logic with two sorts of in-
dividuals: relation instances (i), and elementary consti-
tuants (l). Below, we assume R is the set of all relation
names (elaboration, narration, justification, . . . ).
Definition 1 (Scoping language). Let S be the set {i, l}.
The set of primitive, disjoint types of Lscopes consists of
i, l and t (type of formulae). For each of the types in
S , we have a countable set of variable symbols Vi (Vl).
Two additional countable sets of variable symbols V?i,t?
and V?l,t? range over sets of individuals. These four sets
of variable symbols are pairwise disjoint.
The alphabet of our language is constituted by Vi, Vs,
a set of predicates, equality, connector and quantifier
symbols. The set of predicate symbols is as follows:
1) For each relation symbol r in R, LR is a unary
predicate of type ?i, t??i.e., LR : ?i, t? .
2) unary predicates, sub, coord and sub?1 : ?i, t?.
3) binary predicates ?l and ?r : ?i, l, t?.
4) two equality relations, =s : ?s, s, t? for s ? {i, l}.
Logical connectors, and quantifiers are as usual.
The sets of terms ?i,?l and ?t are recursively defined:
1. Vi ? ?i, Varl ? ?l. 2. For v ? Vs,t, v : ?s, t?. 3. For
each symbol ? of type ?u1, . . . , un? in the alphabet, for
all (t1, . . . , tn?1) ? ?u1?? ? ???un?1, ?[t1, . . . , tn?1] ? ?un .
?t is the set of well formed formulae of the scope lan-
guage.
The predicates ?l and ?r take a relation instance r of
type i and a elementary constituent x of type l as argu-
ments. Intuitively, they mean that x has to be included
in the left (for ?l) or right (for ?r) scope of r. For each
relation symbol R such as justification or elaboration,
the predicate LR takes a relation instance r has argu-
ment and states that r is an instance of the rhetorical re-
lation R. Predicates sub, coord and sub?1 apply to a re-
lation instance r, respectively specifying that r?s left ar-
gument hierarchically dominate its right argument, that
4
both are of equal hierarchical importance, or that the
left one is subordinate to the right one.
Definition 2 (Scope structure and Interpretation).
A scope structure is an Lscopes-structure M =
?Di,Dl, |.|M?. Di and Dl are disjoint sets of individu-
als for the sorts i and l respectively, and |.|M assigns to
each predicate symbol P of type ?u1, . . . , un, t? a func-
tion |.|P : Du1?? ? ??Dun 7? {0, 1}. Variables of type ?i, t?
are assigned subsets of Di and similarly for variables of
type ?l, t?, The predicates =i and =s are interpreted as
equality over Di and Dl respectively.
The interpretation ~?Mv of a formula ? ? ?S is the
standard interpretation of a monadic second order for-
mula w.r.t to a model and a valuation (interpretation of
first order quantifiers and connectors is as usual, quan-
tification over sets is over all sets of individuals). Va-
lidity |= also follows the standard definition.
These scope structures offer a common framework
for different discourse formalisms. Given one of the
three formalisms, we say that two structures S 1 and S 2
are equivalent iff there is an encoding from one struc-
ture into a scoped structure or set of scoped structures
and a decoding back from the scoped structure or set of
scoped structures into S 2
Fact 1. One can define two algorithms I and E such
that:
? from a given structure s which is a RS Tree, a
SDRS or a DT, I computes a scope structure I(s).
? given such a computed structure, E allow to re-
trieve the original structure s (E(I(s)) = s).
RST Encoding and Decoding To flesh out I and E
for RST, we need to define dominance. Set lArgs(r) =
{e ? Dl | (r, e) ? |?l|M}; rArgs(r) is defined analogously
(where ?r replaces ?l). The left and right dominance
relations vl and vr are defined as follows: r vl r? iff
(Args(r) ? lArgs(r?)).
- r vl r? ? ?z : l((z ?l r)? z ?r r))? z ?l r?) with r vr r?
defined analogously.
Dominance v is: v=vl ? vr.
- lArgs(r, X)??z : l(z ?l r) ? z ? X), with rArgs(r, X)
similar and
-Args(r, X)? ?z : l((z ?l r) ? z ?r r))? z ? X).
The NS, NN and NS schemes of RST will be re-
spectively encoded by the predicates sub, coord and
sub?1. We proceed recursively. If t is an EDU e, re-
turn Mt = ?Di = ?,Dl = {e}, ? where  is the inter-
pretation that assigns the empty set to each predicate
symbol. If the root of t is a binary node instantiating
a relation R(t1a1 , t2a2 ), let Tr ? {sub, coord, sub?1} bethe predicate that encodes the schema a1a2, let Mt1 =
?D1i ,D1l , |.|1? and Mt2 = ?D2i ,D2l , |.|2?. The algorithm re-turns Mt = ?D1i ? D2i ? {r},D1l ? D2l , |.|Mt ? where r is a?fresh? relation instance variable not in D1i or D2i , and
|.|Mt is updated in the appropriate fashion to reflect the
left and right arguments of r. Finally, if the root of t is
an n-ary node, split it into a sequence of binary relation
R1(t1, t2),R2(t2, t3), . . . , proceed to recursively compute
the scope-structures Mi for each of the relations using
2 (take care to introduce a ?fresh? relation instance in-
dividual for each relation of the sequence), then return
the union of the models Mi.
RST Decoding Given a finite scope structure M =
?Di,Dl, |.|M?, for each relation instance r compute the
left arguments of r and its right arguments. We then
identify L(r), the unique relation symbol R such that
r ? |LR|M. If that fails, the algorithm fails. Similarly
retrieve the right nuclearity schema from the adequate
predicate that applies to r. Then compute the domi-
nance relations for r. If the input structure M = I(t)
for some RS Tree t then there is at least one maximal
relation instance for the dominance relation. If t the
root node of t is a binary relation, there is exactly one
maximal element in the dominance relation. If there
is none, then we return fail. If there is exactly one,
recursively compute the two RS Trees obtained from
the models computed from the left and right arguments
and descendants of r. If there is more than one, the root
node of the encoded RS Tree was a n-ary relation and
one has to reconstruct the n-ary node if that is possi-
ble; if not the algorithm fails (but that means the input
structure was not obtained from a valid RS Tree).
SDRT Encoding and Decoding: This is similar to
the RST encoding and decoding; for the encoding al-
gorithm, we proceed recursively top down. A SDRS
s is a complex constituent that contents a graph g =
?V, E? whose edges are relations holding between sub-
constituents, simple or complex as well. First come
up with an encoding of the set E of all edges that
hold between two sub-constituents of s, i.e. a struc-
ture M = ?Di = Ei,Dl = V, {LR}, ?l, ?r?, where, for
each edge e ? Ei, LR encodes its relation type, and
?l1 and ?r1 consists of all the pairs (x, e) of left and
right nodes x of the edges e ? E. Finally, for each
complex immediate sub-constituent of s in Dl, update
M as follows: for c such a subconsituent, recursively
compute its encoding Mc, then add everything of Mc
to M, finally remove c from M but add instead for
each relation r scoping over c to the right (left), all
the pairs {(r, x) | x is a constituent in Mc}. The decod-
ing works again similarly to the one for RST, top-down
once again: one recursively retrieves immediate con-
tent of the current complex constituent at each level
then moves to inner constituents.
DT: Dependency trees are syntactically a special case
of SDRSs; there is only one CDU whose domain is
only EDUs.
The scope language allows us to axiomatize three
classes of scope structures corresponding to RS Trees,
SDRSs and DTs. Not every scope structure will yield
a RS Tree when fed to the RST decoding algorithm,
only those obtainable from encoding an RS tree. As not
all scope structures obey these axioms, our language is
5
strictly more expressive than any of these discourse for-
malisms.
As an example of an axiom, the following formula
expresses that a relation cannot have both left and right
scope over the same elementary constituent:
Strong Irreflexivity:
?r : i?x : l?(x ?l r ? x ?r r)) (A0)
Strong irreflexivity entails irreflexivity; a given relation
instance cannot have the same (complete) left and right
scopes. All discourse theories validate A0.
In the Appendix, we define left and right strong dom-
inance relations vl(r) as well as n-ary RS trees and
CDUs of SDRT. We exploit these facts in the Appendix
to express axioms (A1-A9) that axiomatize the struc-
tures corresponding to RST, SDRT and DTs. Axiom
A1 says that every discourse unit is linked via some dis-
course relation instance. Axiom A2 insures that all our
relation instances have the right number of arguments;
Axioms A3 and A4 ensure acyclicity and no crossing
dependencies. A5a and A5b restrict structures to a tree-
like dominance relation with a maximal dominating el-
ement, while A6 defines the Right Frontier constraint
for SDRT, and A7 fixes the domain for SDRT con-
straints on CDUs. A8 ensures that no coordinating and
subordinating relations have the same left and right ar-
guments, while A9 provide the restrictions needed to
define the set of DTs. We use the encoding and decod-
ing maps to show:
Fact 2.
1. The theory TRS T ={A0, A1, A2, A3, A4, A5a, A5b, A8}
characterizes RST structures in the sense that:
- E applied to any structure M such that M |= TRS T
yield an RST Tree.
- for any RST Tree t, I(t) |= TRS T .
2. The theory TS DRT ={A0, A1, A2, A3, A6, A7, A8}
similarly characterizes SDRSs.
3. The theory TDT =TS DRT ? {A9a, A9b} similarly
characterizes Dependency Trees structures.
4 Different Interpretations of Scope
The previous section defined the set of scope structures
as well as the means to import, and then retrieve, RS
trees, DTs, or SDRs into, and from, this set. Some of
these scope structures export both into RST and SDRT,
yielding a 1 ? 1 correspondence between a subset of
SDRT and RST structures. But what does this corre-
spondence actually tell us about these two structures?
In mathematics, the existence of an isomorphism relies
on a bijection that preserves structure. Our correspon-
dence preserves the immediate interpretation of the se-
mantic scopes of relations.
Immediate Interpretation Consider a scope struc-
tureM (validating A0, A1, A2). The predicates lArgs(r)
and rArgs(r) are the sets of all units in the left or right
scope of a relation instance r. Whether r, labelled by
relation name R holds of two discourse units or not
in M, depends on the semantic content of its left and
right arguments, recursively described by lArgs(r) and
all relations r? such that r? @l r, and rArgs(r) and all
relations r? such that r? @r r. Algorithm I computes
what we call the immediate interpretation of an input
structure. Intuitively, in this interpretation the semantic
scope of relations is directly read from the structures
themselves; a node R(t1, t2) in a RS Tree expresses that
R holds between contents expressed by the whole sub-
structures t1 and t2. Similarly, for SDRT and DTs, im-
mediate interpretation of an edge pi1 ?R pi2 is that R
holds between the whole content of pi1 and pi2.
While this immediate interpretation is standard in
SDRT, it is not in RST. Consider again (1) from the
introduction or:
(2) [In 1988, Kidder eked out a $ 46 mil-
lion profit,]31 [mainly because of severe cost
cutting.]32 [Its 1,400-member brokerage oper-
ation reported an estimated $ 5 million loss last
year,]33 [although Kidder expects to turn a profit
this year]34 (RST Treebank, wsj 0604).
(3) [Suzanne Sequin passed away Saturday at the
communal hospital of Bar-le-Duc,]3 [where she
had been admitted a month ago.]4 [. . . ] [Her fu-
neral will be held today at 10h30 at the church
of Saint-Etienne of Bar-le-Duc.]5 (annodis cor-
pus).
These examples involve what are called long distance
attachments. (2) involves a relation of contrast, or com-
parison between 31 and 33, but which does not involve
the contribution of 32 (the costs cutting of 1988). (3)
displays something comparable. A causal relation like
result, or at least a temporal narration holds between
3 and 5, but it should not scope over 4 if one does
not wish to make Sequin?s admission to the hospital
a month ago a consequence of her death last Saturday.
Finally in (1) C4 elaborates on C1, but not on the fact
that C1 is attributed to chief Garcia, so the correspond-
ing elaboration relation should not scope over C3.
It is impossible however, to account for long distance
attachment using the immediate interpretation of RST
trees. (2), for instance, also involves an explanation
relation between 31 and 32, which should include none
of 33 or 34 in its scope. Since 31 is in the scope of both
the explanation and the contrast relation, Axiom A5a of
the previous section entails than an RST tree involving
the two relations has to make one of the two relations
dominates the other.
Marcu?s Nuclearity Principle (NP) Marcu 1996 pro-
vides an alternative to the immediate interpretation and
captures some long distance attachments Danlos 2008;
Egg and Redeker 2010. According to the NP, a rela-
6
tion between two spans of text, expressed at a node of
a RS Tree should hold between the most salient parts
of these spans. Most salient part is recursively defined:
the most salient part of an elementary constituent is it-
self, for a multinuclear relation R(t1N , . . . , tkN) its most
salient part is the union of the most salient parts of the
ti2. Following Egg and Redeker 2010, the NP, or weak
NP is a constraint on which RST trees may correctly
characterize an input text; it is not a mechanism for
computing scopes. Given their analysis of (1) given in
the introduction, NP entails that Elab1 holds between
C1 and C4, accounting for the long distance attach-
ment, and that Attribution holds between C1 and C4
which meets intuition in this case. There is however no
requirement that Attribution do not hold between the
wider span [C1,C2] and C3, as there is no requirement
that Elab1 does not hold between [C1,C2,C3] and C4.
In order to accurately account for (1), the former must
be true and the latter false.
However, this interpretation of NP together with an
RST tree does not determine the semantic scope of all
relations. Danlos 2008 reformulates NP as a Mixed
Nuclearity Principle (MNP) that outputs determinate
scopes for a given structure. The MNP requires for a
given node, that the most salient parts of his daughters
furnish the exact semantic scope for the relation at that
node. The MNP transforms an RST tree t into a scope
structureMt, which validates A0 ? A3 but also A6.3, A7
and A8. HenceM could be exported back to SDRT and
the MNP would yield a translation from RST-trees to
SDRSs.
But when applied to the RST Treebank, the MNP
yields wrong, or at least incomplete, semantic scopes
for intuitively correct RS Trees. The mixed principle
applied to the tree of s1 gives the Attribution scope
over C1 only, but not C2, which is incorrect. Focus-
ing on the attribution relation which is the second most
frequent in the RST Treebank, we find out that, regard-
less of whether we assign Attribution?s arguments S
and N or N and S, this principle makes wrong predic-
tions 86% of the time in a random sampling of 50 cases
in which we have attributions with multi-clause second
argument spans. Consider the following example from
the RST Treebank:
(4) [Interprovincial Pipe Line Co. said]1 [it will de-
lay a proposed two-step, 830 million Canadian-
dollar [(US$705.6 million)]3 expansion of its
system]2 [because Canada?s output of crude oil
is shrinking.]4
Applied to the annotated RS Tree for this example (fig-
2Except for Sequence which only retains the most salient
part of tk
3That A6 is valid in the resulting model is not immediate.
Assume a multinuclear (coordinating) relation instance r has
scope over xn and xn+k later in the textual order. Then it is
impossible to attach with r? a later found constituent xn+k+l to
xn alone, for it would require that xn+1 escapes the scope of r?
from the MNP which it will not do by multinuclearity of r.
attribution
1
S
reason
restatement
2
N
3SN
5S
N
Figure 1: Annotated RST Tree for example (4).
ure 1), the MNP yields an incorrect scope of the attribu-
tion relation over 2 only, regardless of whether the at-
tribution is annotated N-S or S -N. The idea behind the
weak NP provides a better fit with intuitions. The prin-
ciple gives minimal semantic requirements for scoping
relations; everything beyond those requirements is left
underspecified. We formalize this as the relaxed Nu-
clearity Principle (RNP), which does not compute one
structure where each relation is given its exact scope,
but a set of such structures.
The target structures are not trees any more, but we
want them to still reflect the dominance information
present in the RS Tree. We therefore define a notion
of weak dominance over structures of the scoping lan-
guage: for two sets of constituents, X  Y iff X ? Y or
there is a subordinating relation whose left argument is
X and right one Y . Weak dominance is given by tran-
sitive closure ? of . For two relations, r ?l r? iff theleft argument of r weakly dominates both arguments
of r?. ?r is symmetrically defined. Finally, structures
computed by the RNP have to validate the weakened
version of A5: if two relations scope over the same el-
ementary constituent one has to weakly dominates the
other. Let AW5 denote this axiom.
Definition 3 (Relaxed Nuclearity Principle). One can
assign to an RS Tree t a formula of the scoping lan-
guage ?t = ?x??r??t ? ?t such that:
1? ?t is a formula specifying that all individuals
quantified in x? and r? are pairwise distinct, and that there
is no other individuals that the ones just mentioned. ?t
also specifies for each intermediate node n that the cor-
responding relation instance rn is labelled with the ad-
equate relation symbol R and relation type (subordinat-
ing if N-S . . . ).
2? ?t encodes the nuclearity principle applied to t:
for all intermediate nodes ni and n j in t such that nl is
the left (resp. right) daughter of ni, ?t specifies that ni
must scope to the left (resp. right) over the nucleus of
n j.
The interpretation ~t is defined as the set of struc-
turesM that validate ?t and A0, A1, A2, A3, AW5 (they allhave |t| individuals, as fixed by ?t). Moreover, it can
be shown that each model of this set validates TS DRT ;
so we have a interpretation of an RS-Tree into a set of
SDRSs.
5 Intertranslability between RST/DTs
DTs are a restriction of SDRSs to structures without
complex constituents. So the ? function of section 2
7
can transform distinct SDRSs transform into the same
DT with a consequent loss of information.
a?R1 pi
pi : b?R2 c | a?R1 b?R2 c |
pi?R2 b
pi : a?R1 b (1)
Each of the SDRSs above yields the same DT after sim-
plification, namely the second one a?R1 b?R2 c.
The natural interpretation of a DT g describes the
set of fully scoped SDRS structures that are compat-
ible with these minimal requirements, i.e that would
yield g by simplification. To get this set, every edge
r(x, y) in g, r, must be assigned left scope among the
descendants of x in g (and right scope among those of
y); this is a consequence of i) x and y being heads of the
left and right arguments of r and ii) the SDRSs that are
compatible with g do not admit relations with a right
argument in one constituent and a left one outside of it.
Definition 4. Assume that we map each node4 x of g
into a unique variable vx ? Vl and each edge e into a
unique variable symbol re ? Vi. Define x? and r? in an
analogous way as in definition 3.
For a given dependency tree g, we compute a for-
mula ?g = ?x??r? ?g ? ?g such that
? ?g is defined analogously as in definition 3, defin-
ing the set of relation instances and EDUs.
? ?g is the formula stating the minimal scopes for
each relation instance: for all edge in e = R(x, y)
in g, ?g entails i) re has vx in its left scope and
vy in its right scope and ii) let Des(x) be the set
of variable symbols for all the descendants of x in
g, ?g entails that if re has left scope over some vz
then vz is in Des(x) (symmetrically for y and right
scope).
The interpretation ~g of a DT is: {M | M |=
?g, A0-A3, A6, A7}. The DT a ?R1 b ?R2 c for in-
stance, is interpreted as a set of three structures iso-
morphic to the ones in (1) above.
We now relate DTs to RS Trees interpreted with the
RNP. To this aim, we focus on a restricted class of DTs,
those who involve i) coordinating chains of 3 edus or
more only if they involve a single coordinating relation:
x1 ?R1 x2 ?R2 ? ? ? ?Rn?1 xn may appear only for n >
2 if all the Ri are the same coordinating relation, and
ii) subordinating nests of 3 edus or more only if they
involve a single subordinating relation:
x
y1
R1
. . . yn
Rn
is allowed for n > 1 only if all Ri
are labelled with the same subor-
dinating relation.
This restricted class of DTs corresponds exactly with
the set of RS-Trees interpreted with the RNP, provided
that we restrict the interpretation of a DT in the fol-
lowing way: a principle called Continuing Discourse
Pattern, CDP Asher and Lascarides 2003 must apply,
4Recall that unlike RS Trees, DTs have EDUs as nodes
and relations as edges.
who states that whenever a sequence of coordinating
relation Ric originates as a node which appear to be
also in the right scope of a subordinating relation Rs,
Rs must totally include all the Ric in its right scope. A
second principle is required, who states that whenever
two subordinating relations R0s and R?s originate at the
same node in the DT, and the right argument of R?s is
located after the right argument of Rs, any structure in
the interpretation of the DT must verify R?s l Rs. The
translation needs these requirements to work, because:
i) with the NP a relation scoping over a multinuclear
one must includes all the nucleus in RST, and ii)a node
in a RS Tree cannot scope over something that is not its
descendant). Let CDP+ denote these requirements.
Using the restricted interpretation of a DT g;
~gCDP = {M | M |= A0-A3, A6, A7,CDP+}, we trans-
form an RS Tree t into a dependency graph G(t) such
that ~t = ~G(t)CDP:
Definition 5 (RS Trees to dependency graphs). The
translation G takes a RS Tree t as input and outputs
a pair ?G, n?, where G = ?Nodes, Edges? is the corre-
sponding dependency graph, and n an attachment point
used along the recursive definition of G.
? If t is an EDU x then (G)(t) = ?({x}, {}), x?.
? If t = R(t1N , t2S ) then let ?G1, n1? = G(t1) and
?G2, n2? = G(t2).
G(t) = ?(G1 ?G2 ? {Rsubord(n1, n2))}; n1?
? If t = R(t1S , t2N) then G(t) = G(R(t2N , t1S ))
? If t = R(t1N , . . . , tkN) (multinuclear), let ?Gi, ni? =
G(ti), let G be the result of adding a chain
n1 ?Rcoord ? ? ? ?Rcoord nk to the union of the Gi,
G(t) = ?G; n1?
? If t is a nuclear satellite relation with several satel-
lites R(t1S , . . . t jN , . . . tkS ), compute the Gi has inthe previous case, then add to the union of the Gi
the nest of k ? 1 subordinating relations R linking
n j to each of the ni, i , j.
Recall RS Tree (s1). Applying G to this tree yields
the dependency tree (s3): Elab1(C1,C2)?Attr(C1,C3)?
Elab2(C1,C4). ~s3 supports any reading of (s1) pro-
vided by RNP, but also an additional one where Attr
scopes over [C1,C2,C4]. This is however forbidden
by CDP+ for C4 is after C3 in the textual order but
Elab(C1,C4) l Attr(C1,C3).
6 Similarities and distances
The framework we have presented yields a notion of
similarity that applies to structures of different for-
malisms. To motivate our idea, recall example (1);
the structure in (s3) in which Attribution just scopes
over C1 differs from the intuitively correct interpreta-
tion only in that Attribution should also scope over C2
8
as in (s2), while a structure that does this but in which
C3 is in the scope of the Elaboration relation is intu-
itively further away from the correct interpretation.
Our similarity measure Sim over structures M1 and
M2 assumes a common set of elementary constituents
and a correspondence between relation types in the
structures. We measure similarity in terms of the
scopes given to the relations. The intuition, is that given
a map f from elements of relation instances inM1 re-
lation instances in M2, we achieve a similarity score
by counting for each relation instance r the number of
EDUs that are both in the left scope of one element of
r and in f (r), then divide this number by the total num-
ber of diffrents constituents in the left scope of r1 and
r2, and do the same for right scopes as well. The global
similarity is given by the correspondence which yields
the best score.
Given a relation r1 ? M1 and a relation r2?M2, let
?(r1, r2) =
{ 1 if r1 and r2 have the same label
0 otherwise . De-
fine Cl(r1, r2) = |{x : l | M1 |= x ?l r1 ?M2 |= x ?l r2}|,
the number of constituents over which r1 and r2 scope
and Dl(r1, r2) = |{x : l |M1 |= x ?l r1?M2 |= x ?l r2}|.
Define Cr and Dr analogously and assume thatM1 has
less relation instances thanM2. Let Inj(D1i ,D2i ) be theset of injections of relations instances of M1 to those
ofM2.
S im(M1,M2) = 12Max(|M1|, |M2|)?
Max
f?Inj(D1i ,D2i )
?
r:i
?(r, f (r)) ? ( Cl(r, f (r))Dl(r, f (r)) +
Cr(r, f (r))
Dr(r, f (r)) )
If M2 has more relation instances, Invert arguments
and use the definition above. If they have same number
of instances, both directions coincide.
d(M1,M2)=1 ? S im(M1,M2)
For a discourse structureM, S im(M,M) = 1; Sim
ranges between 1 and 0. d is a Jaccard-like met-
ric obeying symmetry, d(x, x) = 0 d(x, y) , 0 for
x , y, and the triangle equality. One can further define
the maximal or average similarity between any pair of
structures of two sets S 1 and S 2. This gives an idea
of the similarity between two underspecified interpre-
tations, such as the ones provided by RNP of section 4.
For example, the maximal similarity between (s2) in-
terpreted as itself (immediate interpretation) and a pos-
sible scope structure for the DT (s3), interpreted with
the underspecified ~ of section 5, is 7/12. It is pro-
vided by the interpretation of (s3) where Attr is given
left scope over C1,C2,C4, Elab1 holds between C1 and
C2, and the second Elab fails to match the continua-
tion of (s3). sim(~s2, ~?(s2) = 7/12 also, because
? must distribute [2, 4] in s2 to avoid crossing depen-
dencies; so ~?(s2)  ~s3. The maximal similarity
between the RS tree in (s1) with RNP (or equivalently,
(3) with ~CDP+) and (s2) is 19/36, achieved when both
C1 and C2 are left argument of Attr (though not C4).
With MNP, the similarity is 17/36.
Given our results in sections 4 and 5, we have:
Fact 3. (i) For any DT g without a > 3 length flat se-
quence and interpreted using CDP+, there an RS tree
t interpreted with RNP such that S im(g, t) = 1. (ii)
For any RS tree with RNP there is a DT g such that
S im(t, g) = 1.
To prove (i) construct a model using Definition 4 and
then use RST decoding. To prove (ii) construct a model
given Definition 3 and use DT encoding. Our similarity
measure provides general results for SDRSs and DTTs
(and a fortiori SDRSs and RS trees) (See Appendix).
7 Related Work
Our work shares a motivation with Blackburn, Gardent,
and Meyer-Viol 1993: Blackburn, Gardent, and Meyer-
Viol 1993 provides a modal logic framework for for-
malizing syntactic structures; we have used MSO and
our scope language to formalize discourse structures.
While many concepts of discourse structure admit of
a modal formalization, the fact that discourse relations
can have scope over multiple elementary nodes either
in their first or second argument makes an MSO treat-
ment more natural. Danlos 2008 compares RST, SDRT
and Directed Acyclic Graphs (DAGs) in terms of their
strong generative capacity in a study of structures and
examples involving 3 EDUS. We do not consider gen-
erative capacity, but we have given a generic and gen-
eral axiomatization of RST, SDRT and DT in a formal
interpreted language. We can translate any structure of
these theories into this language, independent of their
linguistic realization. We agree with Danlos that the
NP does not yield an accurate semantic representation
of some discourses. We agree with Egg and Redeker
2010 that the NP is rather a constraint on structures, and
we formalize this with the relaxed principle and show
how it furnishes a translation from RS trees to sets of
scoped structures. Danlos?s interesting correspondence
between restricted sets of RST trees, SDRSs and DAGs
assumes an already fixed scope-interpretation for each
kind of structure: SDRSs and DAGs are naturally in-
terpreted as themselves, and RS Trees are interpreted
with the mixed NP Our formalism allows us both to
describe the structures themselves and various ways of
computing alternate scopes for relations.
With regard to the discussion in Egg and Redeker
2008; Wolf and Gibson 2005 of tree vs. graph struc-
tures, we show exactly how tree based structures
like RST with or without the NP compare to graph
based formalisms like SDRT. We have not investigated
Graphbank here, but the scope language can axioma-
tize Graphbank (with A0-A3, A8).
8 Conclusions
We have investigated how to determine the semantic
scopes of discourse relations in various formalisms by
9
developing a canonical formalism that encodes scopes
of relations regardless of particular assumptions about
discourse structure. This provides a lingua franca for
comparing discourse formalisms and a way to measure
similarity between structures, which can help to com-
pare different annotations of a same text.
References
Afantenos, S. et al (2012). ?An empirical resource for
discovering cognitive principles of discourse organ-
isation: the ANNODIS corpus?. In: Proceedings of
LREC 2012. ELRA.
Asher, N. and A. Lascarides (2003). Logics of Conver-
sation. Cambridge University Press.
Asher, N. (1993). Reference to Abstract Objects in Dis-
course. Studies in Linguistics and Philosophy 50.
Dordrecht: Kluwer.
Baldridge, J., N. Asher, and J. Hunter (2007). ?Anno-
tation for and Robust Parsing of Discourse Structure
on Unrestricted Texts?. In: Zeitschrift fr Sprachwis-
senschaft 26, pp. 213?239.
Bateman, J. and K. J. Rondhuis (1997). ?Coherence re-
lations : Towards a general specification?. In: Dis-
course Processes 24.1, pp. 3?49.
Blackburn, P., C. Gardent, and W. Meyer-Viol (1993).
?Talking about Trees?. In: EACL 6, pp. 21?29.
Carlson, L., D. Marcu, and M. E. Okurowski (2002).
RST Discourse Treebank. Linguistic Data Consor-
tium, Philadelphia.
Danlos, L. (2008). ?Strong generative capacity of RST,
SDRT and discourse dependency DAGSs?. English.
In: Constraints in Discourse. Ed. by A. Benz and P.
Kuhnlein. Benjamins, pp. 69?95.
duVerle, D. and H. Prendinger (2009). ?A Novel Dis-
course Parser Based on Support Vector Machine
Classification?. In: Proceedings of ACL-IJCNLP
2009. ACL, pp. 665?673.
Egg, M. and G. Redeker (2008). ?Underspecified dis-
course representation?. In: PRAGMATICS AND BE-
YOND NEW SERIES 172, p. 117.
? (2010). ?How Complex is Discourse Structure?? In:
Proceedings of LREC?10. Ed. by N. Calzolari et al
ELRA.
Hitzeman, J., M. Moens, and C. Grover (1995). ?Algo-
rithms for Analyzing the Temporal Structure of Dis-
course?. In: Proceedings of the 7th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics, pp. 253?260.
Hobbs, J. R., M. Stickel, and P. Martin (1993). ?In-
terpretation as Abduction?. In: Artificial Intelligence
63, pp. 69?142.
Kehler, A. (2002). Coherence, Reference and the The-
ory of Grammar. CSLI Publications.
Lascarides, A. and N. Asher (1993). ?Temporal In-
terpretation, Discourse Relations and Commonsense
Entailment?. In: Linguistics and Philosophy 16,
pp. 437?493.
Mann, W. C. and S. A. Thompson (1987). ?Rhetorical
Structure Theory: A Framework for the Analysis of
Texts?. In: International Pragmatics Association Pa-
pers in Pragmatics 1, pp. 79?105.
Marcu, D. (1996). ?Building up rhetorical structure
trees?. In: Proceedings of the thirteenth national
conference on Artificial intelligence - Volume 2.
AAAI?96. Portland, Oregon: AAAI Press, pp. 1069?
1074. isbn: 0-262-51091-X.
Muller, P. et al (2012). ?Constrained decoding for
text-level discourse parsing?. Anglais. In: COLING
- 24th International Conference on Computational
Linguistics. Mumbai, Inde.
Polanyi, L. (1985). ?A Theory of Discourse Structure
and Discourse Coherence?. In: Papers from the Gen-
eral Session at the 21st Regional Meeting of the
Chicago Linguistics Society. Ed. by P. D. K. W. H.
Eilfort and K. L. Peterson.
Polanyi, L. and R. Scha (1984). ?A Syntactic Ap-
proach to Discourse Semantics?. In: Proceedings of
the 10th International Conference on Computational
Linguistics (COLING84). Stanford, pp. 413?419.
Polanyi, L. et al (2004). ?A Rule Based Approach
to Discourse Parsing?. In: Proceedings of the 5th
SIGDIAL Workshop in Discourse and Dialogue,
pp. 108?117.
Prasad, R. et al (2008). ?The penn discourse tree-
bank 2.0?. In: Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008), p. 2961.
Sagae, K. (2009). ?Analysis of Discourse Structure
with Syntactic Dependencies and Data-Driven Shift-
Reduce Parsing?. In: Proceedings of IWPT?09. ACL,
pp. 81?84.
Sanders, T., W. Spooren, and L. Noordman (1992).
?Toward a taxonomy of coherence relations?. In:
Discourse processes 15.1, pp. 1?35.
Stede, M. (2004). ?The Potsdam Commentary Cor-
pus?. In: ACL 2004 Workshop on Discourse Annota-
tion. Ed. by B. Webber and D. K. Byron. Barcelona,
Spain: Association for Computational Linguistics,
pp. 96?102.
Subba, R. and B. Di Eugenio (2009). ?An effective Dis-
course Parser that uses Rich Linguistic Information?.
In: Proceedings of HLT-NAACL. ACL, pp. 566?574.
Webber, B. et al (1999). ?Discourse Relations: A
Structural and Presuppositional Account Using Lex-
icalised TAG?. In: Proceedings of the 37th ACL
Conference. College Park, Maryland, USA: Associ-
ation for Computational Linguistics, pp. 41?48. doi:
10.3115/1034678.1034695.
Wolf, F. and E. Gibson (2005). ?Representing Dis-
course Coherence: A Corpus Based Study?. In:
Computational Linguistics 31.2, pp. 249?287.
Appendix
In what follows, let @ denotes the irreflexive part of
v We assume that we have access to the textual order
10
of EDUs as a function f : EDUs ? N with an associ-
ated strict linear ordering < over EDUs. We also ap-
peal to the notion of a chain over EDUs {x1, x2, . . . xn}
with a set of relation instances r1, . . . , rn} all of which
are instances of an n-ary relation type, of the form
x1 ?r1 x2 ?r2 . . . ?rn xn which can be defined in
MSO. To handle RST relations with multiple satellites,
we define a nest: Nest(X,R) iff all r ? R have the same
left argument in X but take different right arguments in
X. Finally, we define CDUs:
cdu(X,R)? ?rArgs(r, X)?
?r? (?x x ?r r? ? x ? X)? r? ? R
Axiomatization
?x : l ?r : i (x ?l r) ? (x ?r r)
(A1:Weak Connectedness)
?r?x, y(x ?r r) ? y ?l r))
(A2 :Properness of the relation)
?X : (l, t)(X , 0? ?y?X ?n?y ?l n
(A3 :Acyclicity or Well Foundedness)
No crossing dependencies using the textual order < of
EDUs:
?x, y, z,w((x < y < z < w) ?
?m, n?(x ?l n ? z ?r n
? y ?l m ? w ?r m)) (A4)
Tree Structures. Define scopes(r, x) := x ?l r ? x ?r r.
?r, r? ((?(?X,R r, r? ? R ? chain(X,R) ? nest(X,R))
? (?x scopes(r, x) ? scopes(r?, x)))
? (r v r? ? r? v r))
(A5a)
?R : (i, t)?!r : i ?r? ? R r? v r (A5b)
Right Frontier:
?n, xn, xn+1?r ((xn+1 ?r r)? (xn ?l r) ? (?xn ?l r
? ?X,R(chain(X,R) ? ?r?(r? ? R? sub(r?))
? ?y ? X?z?k ?m, j ? R (scopes( j, y) ? acc(z, y)
? scopes(m, xn) ? z ?l k ? k ? ?xn+1)))) (A6)
(The definition of SDRS accessibility acc is easy)
CDUs or EDUs and no overlapping CDUs:
?!x : l ? ?X,R cdu(X,R)?
?X,Y,R,R? (cdu(X,R) ? cdu(Y,R?)?
(R ? R? , 0? (R ? R? ? R? ? R))
(A7)
The same arguments cannot be linked by subordinating
and coordinating relations. The formal axiom is evi-
dent.
Finally, two axioms for restricting SDRSs to depen-
dency trees:
?r?x, y((x ?l r) ? y ?l r))
? (x ?r r) ? y ?r r)))? x = y
(A9a : NoCDUs.)
?r?r??X,Y(lArgs(r, X) ? rArgs(r,Y)
? lArgs(r?, X) ? rArgs(r?,Y))
? r = r?
(A9b :unique arc)
We note that as a consequence of A5a and A5b we have
no danglers or contiguous spans:
?x, y, n (x ?l n ? y ?l n ? x , y)
? ??m?z (x ?l m ? z ?r m
? ?(z ?l n ? z ?r n))
We also note that A5a and A5b entail A7, A8 and A9b,
though not vice-versa.
Fact 4. Where ? is any SDRS and ? : S DRS ? DT as
in section 2, set R1 = {r : i : |{x : M? |= x ?l r)}| > 1},
R2 = {r : i : |{x : M? |= x ?r r)}| > 1}, and
R{x,y} = {r|?r? : i(x ?l r? ? y ?r r? ? r? , r}. Assume the
immediate interpretation of ? and ?(?):
S im(?, ?(?))=
2|I| ? |(R1 ? R2) ??x,y?D2l X{x,y}|
2|I|
+
1
2|I| {?r?R1
1
|x : M? |= x ?l r)}|
+?r?R2
1
|x : M? |= x ?r r)}| }
Explanation: We suppose that I is the number of re-
lation instances in the SDRS. ? removes CDUs in an
SDRS and attaches all incoming arcs to the CDUs to
the head of the CDU. It also removes multiple arcs
into any given node. So for any node m such that
|{r : m ?r r}| = a > 1, then the information contained
in the a ? 1 arcs will be lost. In addition ? will restrict
that one incoming arc that in the SDRS has in its scope
all the elements in the CDU to just the head. So the
scope information concerning all the other elements in
the CDU will be lost.
11
