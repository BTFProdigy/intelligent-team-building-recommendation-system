CUCWeb: a Catalan corpus built from the Web
G. Boleda1 S. Bott1 R. Meza2 C. Castillo2 T. Badia1 V. Lo?pez2
1Grup de Lingu???stica Computacional
2Ca?tedra Telefo?nica de Produccio?n Multimedia
Fundacio? Barcelona Media
Universitat Pompeu Fabra
Barcelona, Spain
{gemma.boleda,stefan.bott,rodrigo.meza}@upf.edu
{carlos.castillo,toni.badia,vicente.lopez}@upf.edu
Abstract
This paper presents CUCWeb, a 166 mil-
lion word corpus for Catalan built by
crawling the Web. The corpus has been
annotated with NLP tools and made avail-
able to language users through a flexible
web interface. The developed architecture
is quite general, so that it can be used to
create corpora for other languages.
1 Introduction
CUCWeb is the outcome of the common interest
of two groups, a Computational Linguistics group
and a Computer Science group interested on Web
studies. It fits into a larger project, The Span-
ish Web Project, aimed at empirically studying the
properties of the Spanish Web (Baeza-Yates et al,
2005). The project set up an architecture to re-
trieve a portion of the Web roughly correspond-
ing to the Web in Spain, in order to study its for-
mal properties (analysing its link distribution as a
graph) and its characteristics in terms of pages,
sites, and domains (size, kind of software used,
language, among other aspects).
One of the by-products of the project is a 166
million word corpus for Catalan.1 The biggest
annotated Catalan corpus before CUCWeb is the
CTILC corpus (Rafel, 1994), consisting of about
50 million words.
In recent years, the Web has been increasingly
used as a source of linguistic data (Kilgarriff and
Grefenstette, 2003). The most straightforward ap-
proach to using the Web as corpus is to gather data
online (Grefenstette, 1998), or estimate counts
1Catalan is a relatively minor language. There are
currently about 10.8 million Catalan speakers, similar
to Serbian (12), Greek (10.2), or Swedish (9.3). See
http://www.upc.es/slt/alatac/cat/dades/catala-04.html
(Keller and Lapata, 2003) using available search
engines. This approach has a number of draw-
backs, e.g. the data one looks for has to be known
beforehand, and the queries have to consist of lex-
ical material. In other words, it is not possible
to perform structural searches or proper language
modeling.
Current technology makes it feasible and rela-
tively cheap to crawl and store terabytes of data.
In addition, crawling the data and processing it
off-line provides more potential for its exploita-
tion, as well as more control over the data se-
lection and pruning processes. However, this ap-
proach is more challenging from a technological
viewpoint. 2 For a comprehensive discussion of
the pros and cons of the different approaches to
using Web data for linguistic purposes, see e.g.
Thelwall (2005) and Lu?deling et al (To appear).
We chose the second approach because of the ad-
vantages discussed in this section, and because it
allowed us to make the data available for a large
number of non-specialised users, through a web
interface to the corpus. We built a general-purpose
corpus by crawling the Spanish Web, processing
and filtering them with language-intensive tools,
filtering duplicates and ranking them according to
popularity.
The paper has the following structure: Sec-
tion 2 details the process that lead to the consti-
tution of the corpus, Section 3 explores some of
the exploitation possibilities that are foreseen for
CUCWeb, and Section 4 discusses the current ar-
chitecture. Finally, Section 5 contains some con-
clusions and future work.
2The WaCky project (http://wacky.sslmit.unibo.it/) aims
at overcoming this challenge, by developing ?a set of tools
(and interfaces to existing tools) that will allow a linguist to
crawl a section of the web, process the data, index them and
search them?.
19
2 Corpus Constitution
2.1 Data collection
Our goal was to crawl the portion of the Web re-
lated to Spain. Initially, we crawled the set of
pages with the suffix .es. However, this domain
is not very popular, because it is more expensive
than other domains (e.g. the cost of a .com do-
main is about 15% of that of an .es domain), and
because its use is restricted to company names or
registered trade marks.3 In a second phase a dif-
ferent heuristic was used, and we considered that
a Web site was in Spain if either its IP address was
assigned to a network located in Spanish land, or if
the Web site?s suffix was .es. We found that only
16% of the domains with pages in Spain were un-
der .es.
The final collection of the data was carried
out in September and October 2004, using a
commercial piece of software by Akwan (da Silva
et al, 1999). 4 The actual collection was started
by the crawler using as a seed the list of URLs in a
Spanish search engine ?which was a commercial
search engine back in 2000? under the name of
Buscopio. That list covered the major part of the
existing Web in Spain at that time. 5. New URLs
were extracted from the downloaded pages, and
the process continued recursively while the pages
were in Spain ?see above. The crawler down-
loaded all pages, except those that had an identical
URL (http://www.web.es/main/ and
http://www.web.es/main/index.html
were considered different URLs). We retrieved
over 16 million Web pages (corresponding to
over 300,000 web sites and 118,000 domains),
and processed them to extract links and text. The
uncompressed text of the pages amounts to 46
GB, and the metadata generated during the crawl
to 3 GB.
In an initial collection process, a number of dif-
ficulties in the characterisation of the Web of Spain
were identified, which lead to redundancy in the
contents of the collection:
Parameters to a program inside URL addresses.
This makes it impossible to adequately sep-
3In the case of Catalan, additionally, there is a political
and cultural opposition to the .es domain.
4We used a PC with two Intel-4 processors running at 3
GHz and with 1.6 GB of RAM under Red-Hat Linux. For
the information storage we used a RAID of disks with 1.8 TB
of total capacity, although the space used by the collection is
about 50 GB.
5http://www.buscopio.net
arate static and dynamic pages, and may
lead to repeatedly crawl pages with the same
content.
Mirrors (geographically distributed copies of the
same contents to ensure network efficiency).
Normally, these replicas are entire collections
with a large volume, so that there are many
sites with the same contents, and these are
usually large sites. The replicated informa-
tion is estimated between 20% and 40% of
the total Web contents ((Baeza-Yates et al,
2005)).
Spam on the Web (actions oriented to deceive
search engines and to give to some pages a
higher ranking than they deserve in search re-
sults). Recognizing spam pages is an active
research area, and it is estimated that over 8%
of what is indexed by search engines is spam
(Fetterly et al, 2004). One of the strategies
that induces redundancy is to automatically
generate pages to improve the score they ob-
tain in link-based rankings algorithms.
DNS wildcarding (domain name spamming).
Some link analysis ranking functions assign
less importance to links between pages in
the same Web site. Unfortunately, this has
motivated spammers to use several different
Web sites for the same contents, usually
through configuring DNS servers to assign
hundreds or thousands of site names to
the same IP address. Spain?s Web seems
to be quite populated with domain name
spammers: 24 out of the 30 domains with the
highest number of Web sites are configured
with DNS wildcarding (Baeza-Yates et al,
2005).
Most of the spam pages were under the .com
top-level domain. We manually checked the do-
mains with the largest number of sites and pages to
ban a list of them, mostly sites containing pornog-
raphy or collections of links without information
content. This is not a perfect solution against
spam, but generates significant savings in terms
of bandwidth and storage, and allows us to spend
more resources in content-rich Web sites. We also
restricted the crawler to download a maximum of
400 pages per site, except for the Web sites within
.es, that had no pre-established limit.
20
Documents (%) Words (%)
Language classifier 491,850 100 375,469,518 100
Dictionary filter 277,577 56.5 222,363,299 59
Duplicate detector 204,238 41.5 166,040,067 44
Table 1: Size of the Catalan corpus
2.2 Data processing
The processing of the data to obtain the Catalan
corpus consisted of the following steps: language
classification, linguistic filtering and processing,
duplicate filtering and corpus indexing. This sec-
tion details each of these aspects.
We built a language classifier with the Naive
Bayes classifier of the Bow system (Mccallum,
1996). The system was trained with corpora cor-
responding to the 4 official languages in Spain
(Spanish, Catalan, Galician and Basque), as well
as to the other 6 most frequent languages in
the Web (Anonymous, 2000): English, German,
French, Italian, Portuguese, and Dutch.
38% of the collection could not be reliably clas-
sified, mostly because of the presence of pages
without enough text, for instance, pages contain-
ing only images or only lists of proper nouns.
Within the classified pages, Catalan was the third
most used language (8% of the collection). As
expected, most of the collection was in Spanish
(52%), but English had a large part (31%). The
contents in Galician and Basque only comprise
about 2% of the pages.
We wanted to use the Catalan portion as a cor-
pus for NLP and linguistic studies. We were not
interested in full coverage of Web data, but in
quality. Therefore, we filtered it using a compu-
tational dictionary and some heuristics in order to
exclude documents with little linguistic relevance
(e.g. address lists) or with a lot of noise (program-
ming code, multilingual documents). In addition,
we performed a simple duplicate filter: web pages
with a very similar content (determined by a hash
of the processed text) were considered duplicates.
The sizes of the corpus (in documents and
words6) after each of the processes are depicted in
Table 1. Note that the two filtering processes dis-
card almost 60% of the original documents. The
final corpus consists of 166 million words from
204 thousand documents.
Its distribution in terms of top-level domains is
shown in Table 2, and the 10 biggest sites in Ta-
6Word counts do not include punctuation marks.
ble 3. Note that the .es domain covers almost
half of the pages and com a quarter, but .org and
.net alo have a quite large share of the pages.
As for the biggest sites, they give an idea of the
content of CUCWeb: they mainly correspond to
university and institutional sites. A similar dis-
tribution can be observed for the 50 biggest sites,
which will determine the kind of language found
in CUCWeb.
Documents (%)
es 89,541 44.6
com 49,146 24.5
org 35,528 17.7
net 18,819 9.4
info 5,005 2.5
edu 688 0.3
others 2,042 1.4
Table 2: Domain distribution in CUCWeb
The corpus was further processed with CatCG
( `Alex Alsina et al, 2002), a POS-tagger and shal-
low parser for Catalan built with the Connexor
Constraint Grammar formalism and tools.7 CatCG
provides part of speech, morphological features
(gender, number, tense, etc.) and syntactic infor-
mation. The syntactic information is a functional
tag (e.g. subject, object, main verb) annotated at
word level.
Since we wanted the corpus not only to be an
in-house resource for NLP purposes, but also to
be accessible to a large number of users. To that
end, we indexed it using the IMS Corpus Work-
bench tools8 and we built a web interface to it (see
Section 3.1). The CWB includes facilities for in-
dexing and searching corpora, as well as a special
module for web interfaces. However, the size of
the corpus is above the advisable limit for these
tools. 9 Therefore, we divided it into 4 subcorpora
7http://www.connexor.com/
8http://www.ims.uni-stuttgart.de/projekte/CorpusWorkbench/
9According to Stefan Evert ?personal communication?, if
a corpus has to be split into several parts, a good rule of thumb
is to split it in 100M word parts. In his words ?depending on
various factors such as language, complexity of annotations
21
Site Description Documents
upc.es University 1574
gencat.es Institution 1372
publicacions.bcn.es Institution 1282
uab.es University 1190
revista.consumer.es Company 1132
upf.es University 1076
nil.fut.es Distribution lists 1045
conc.es Insitution 1033
uib.es University 977
ajtarragona.es Institution 956
Table 3: 10 biggest sites in CUCWeb
and indexed each of them separately. The search
engine for the corpus is the CQP (Corpus Query
Processor, one of the modules of the CWB).
Since CQP provides sequential access to doc-
uments we ordered the corpus documents by
PageRank so that they are retrieved according to
their popularity on the Internet.
3 Corpus Exploitation
CUCWeb is being exploited in two ways: on the
one hand, data can be accessed through a web
interface (Section 3.1). On the other hand, the
annotated data can be exploited by theoretical or
computational linguists, lexicographers, transla-
tors, etc. (Section 3.2).
3.1 Corpus interface
Despite the wide use of corpora in NLP, few in-
terfaces have been built, and still fewer are flex-
ible enough to be of interest to linguistic re-
searchers. As for Web data, some initiatives ex-
ist (WebCorp 10, the Linguist?s Search Engine 11,
KWiCFinder 12), but they are meta-interfaces to
search engines. For Catalan, there is a web inter-
face for the CTILC corpus13, but it only allows for
one word searches, of which a maximum of 50 hits
are viewed. It is not possible either to download
search results.
From the beginning of the project our aim was
to create a corpus which could be useful for both
the NLP community and for a more general au-
dience with an interest in the Catalan language.
and how much RAM you have, a larger or smaller size may
give better overall performance.?.
10http://www.webcorp.org.uk/
11http://lse.umiacs.umd.edu
12http://miniappolis.com/KWiCFinder
13http://pdl.iec.es
This includes linguists, lexicographers and lan-
guage teachers.
We expected the latter kind of user not to be fa-
miliar with corpus searching strategies and corpus
interfaces, at least not to a large extent. Therefore,
we aimed at creating a user-friendly web interface
which should be useful for both non-trained and
experienced users.14 Further on, we wanted the
interface to support not only example searches but
also statistical information, such as co-occurrence
frequency, of use in lexicographical work and po-
tentially also in language teaching or learning.
There are two web interfaces to the corpus:
an example search interface and a statistics inter-
face. Furthermore, since the flexibility and expres-
siveness of the searches potentially conflicts with
user-friendliness, we decided to divide the exam-
ple search interface into two modalities: a simple
search mode and an expert search mode.
The simple mode allows for searches of words,
lemmata or word strings. The search can be re-
stricted to specific parts of speech or syntactic
functions. For instance, a user can search for
an ambiguous word like Catalan ?la? (masculine
noun, or feminine determiner or personal pro-
noun) and restrict the search to pronouns. Or look
for word ?traduccions? (?translations?) function-
ing as subject. The advantage of the simple mode
is that an untrained person can use the corpus al-
most without the need to read instructions. If new
users find it useful to use CUCWeb, we expect that
the motivation to learn how to create advanced cor-
pus queries will arise.
The expert mode is somewhat more complex
but very flexible. A string of up to 5 word units
can be searched, where each unit may be a word
14http://www.catedratelefonica.upf.es/cucweb
22
form, lemma, part of speech, syntactic function or
combination of any of those. If a part of speech
is specified, further morphological information is
displayed, which can also be queried.
Each word unit can be marked as optional or
repeated, which corresponds to the Boolean op-
erators of repetition and optionality. Within each
word unit each information field may be negated,
allowing for exclusions in searches, e.g. requiring
a unit not to be a noun or not corresponding to a
certain lemma. This use of operators gives the ex-
pert mode an expressiveness close to regular gram-
mars, and exploits almost all querying functional-
ities of CQP ?the search engine.
In both modes, the user can retrieve up to 1000
examples, which can be viewed online or down-
loaded as a text file, and with different context
sizes. In addition, a link to a cache copy of the
document and to its original location is provided.
As for the statistics interface, it searches for
frequency information regarding the query of the
user. The frequency can be related to any of the
4 annotation levels (word, lemma, POS, function).
For example, it is possible to search for a given
verb lemma and get the frequencies of each verb
form, or to look for adjectives modifying the word
dona (?woman?) and obtain the list of lemmata
with their associated frequency. The results are
offered as a table with absolute and relative fre-
quency, and they can be viewed online or retrieved
as a CSV file. In addition, each of the results has
an associated link to the actual examples in the
corpus.
The interface is technically quite complex, and
the corpus quite large. There are still aspects to
be solved both in the implementation and the doc-
umentation of the interface. Even restricting the
searches to 1000 hits, efficiency remains often a
problem in the example search mode, and more
so in the statistics interface. Two partial solutions
have been adopted so far: first, to divide the cor-
pus into 4 subcorpora, as explained in Section 2.2,
so that parallel searches can be performed and thus
the search engine is not as often overloaded. Sec-
ond, to limit the amount of memory and time for a
given query. In the statistics interface, a status bar
shows the progress of the query in percentage and
the time left.
The interface does not offer the full range of
CWB/CQP functionalities, mainly because it was
not demanded by our ?known? users (most of them
linguists and translators from the Department of
Translation and Philology at Universitat Pompeu
Fabra). However it is planned to increasingly add
new features and functionalities. Up to now we did
not detect any incompatibility between splitting
the corpora and the implementation of CWB/CQP
deployment or querying functionalities.
3.2 Whole dataset
The annotated corpus can be used as a source of
data for NLP purposes. A previous version of the
CUCWeb corpus ?obtained with the methodology
described in this paper, but crawling only the .es
domain, consisting of 180 million words? has al-
ready been exploited in a lexical acquisition task,
aimed at classifying Catalan verbs into syntactic
classes (Mayol et al, 2006).
Cluster analysis was applied to a 200 verb set,
modeled in terms of 10 linguistically defined fea-
tures. The data for the clustering were first ex-
tracted from a fragment of CTILC (14 million
word). Using the manual tagging of the corpus, an
average 0.84 f-score was obtained. Using CatCG,
the performance decreased only 2 points (0.82 f-
score).
In a subsequent experiment, the data were ex-
tracted from the CUCWeb corpus. Given that it
is 12 times larger than the traditional corpus, the
question was whether ?more data is better data?
(Church and Mercer, 1993, 18-19). Banko and
Brill (2001) present a case study on confusion set
disambiguation that supports this slogan. Surpris-
ingly enough, results using CUCWeb were sig-
nificantly worse than those using the traditional
corpus, even with automatic linguistic processing:
CUCWeb lead to an average 0.71 f-score, so an 11
point difference resulted. These results somewhat
question the quality of the CUCWeb corpus, par-
ticularly so as the authors attribute the difference
to noise in the CUCWeb and difficulties in linguis-
tic processing (see Section 4). However, 0.71 is
still well beyond the 0.33 f-score baseline, so that
our analysis is that CUCWeb can be successfully
used in lexical acquisition tasks. Improvement in
both filtering and linguistic processing is still a
must, though.
4 Discussion of the architecture
The initial motivation for the CUCWeb project
was to obtain a large annotated corpus for Catalan.
However, we set up an architecture that enables
23
Figure 1: Architecture for building Web corpora
the construction of web corpora in general, pro-
vided the language-dependent modules are avail-
able. Figure 1 shows the current architecture for
CUCWeb.
The language-dependent modules are the lan-
guage classifier (our classifier now covers 10 lan-
guages, as explained in Section 2.2) and the lin-
guistic processing tools. In addition, the web inter-
face has to be adapted for each new tagset, piece
of information and linguistic level. For instance,
the interface currently does not support searches
for chunks or phrases.
Most of the problems we have encountered in
processing Web documents are not new (Baroni
and Ueyama, To appear), but they are much more
frequent in that kind of documents than in standard
running text.15 We now review the main problems
we came across:
Textual layout In general, they are problems
that arise due to the layout of Web documents,
which is very different to that of standard text. Pre-
processing tools have to be adapted to deal with
these elements. These include headers or footers
(Last modified...), copyright statements or frame
elements, the so-called boilerplates. Currently,
due to the fact that we process the text extracted by
the crawler, no boilerplate detection is performed,
which increases the amount of noise in the cor-
pus. Moreover, the pre-processing module does
not even handle e-mail addresses or phone num-
bers (they are not frequently found in the kind of
15By ?standard text?, we mean edited pieces of text, such
as newspapers, novels, encyclopedia, or technical manuals.
24
text it was designed to process); as a result, for
example, one of the most frequent determiners in
the corpus is 93, the phone prefix for Barcelona.
Another problem for the pre-processing module,
again due to the fact that we process the text ex-
tracted from the HTML markup, is that most of the
structural information is lost and many segmenta-
tion errors occur, errors that carry over to subse-
quent modules.
Spelling mistakes Most of the texts published
on the Web are only edited once, by their au-
thor, and are neither reviewed nor corrected, as is
usually the case in traditional textual collections
(Baeza-Yates et al, 2005). It could be argued
that this makes the language on the Web closer
to the ?actual language?, or at least representative
of other varieties in contrast to traditional corpora.
However, this feature makes Web documents diffi-
cult to process for NLP purposes, due to the large
quantity of spelling mistakes of all kinds. The
HTML support itself causes some of the difficul-
ties that are not exactly spelling mistakes: A par-
ticularly frequent kind of problem we have found
is that the first letter of a word gets segmented
from the rest of the word, mainly due to formatting
effects. Automatic spelling correction is a more
necessary module in the case of Web data.
Multilinguality Multilinguality is also not a
new issue (there are indeed multilingual books or
journals), but is one that becomes much more ev-
ident when handling Web documents. Our cur-
rent approach, given that we are not interested in
full coverage, but in quality, is to discard multi-
lingual documents (through the language classifier
and the linguistic filter). This causes two prob-
lems. On the one hand, potentially useful texts
are lost, if they are inserted in multilingual doc-
uments (note that the linguistic filter reduces the
initial collection to almost a half; see Table 1). On
the other hand, many multilingual documents re-
main in the corpus, because the amount of text
in another language does not reach the specified
threshold. Due to the sociological context of Cata-
lan, Spanish-Catalan documents are particularly
frequent, and this can cause trouble in e.g. lexical
acquisition tasks, because both are Romance lan-
guages and some word forms coincide. Currently,
both the language classifier and the dictionary fil-
ter are document-based, not sentence-based. A
better approach would be to do sentence-based
language classification. However, this would in-
crease the complexity of corpus construction and
management: If we want to maintain the notion
of document, pieces in other languages have to be
marked but not removed. Ideally, they should also
be tagged and subsequently made searchable.
Duplicates Finally, a problem which is indeed
particular to the Web is redundancy. Despite all
efforts in avoiding duplicates during the crawl-
ing and in detecting them in the collection (see
Section 2), there is still quite a lot of dupli-
cates or near-duplicates in the corpus. This is a
problem both for NLP purposes and for corpus
querying. More sophisticated algorithms, as in
Broder (2000), are needed to improve duplicate
detection.
5 Conclusions and future work
We have presented CUCWeb, a project aimed at
obtaining a large Catalan corpus from the Web and
making it available for all language users. As an
existing resource, it is possible to enhance it and
modify it, with e.g. better filters, better duplicate
detectors, or better NLP tools. Having an actual
corpus stored and annotated also makes it possible
to explore it, be it through the web interface or as
a dataset.
The first CUCWeb version (from data gathering
to linguistic processing and web interface imple-
mentation) was developed in only 6 months, with
partial dedication of a a team of 6 people. Since
then, many improvements have taken place, and
many more remain as a challenge, but it confirms
that creating a 166 million word annotated corpus,
given the current technological state of the art, is a
relatively easy and cheap issue.
Resources such as CUCWeb facilitate the tech-
nological development of non-major languages
and quantitative linguistic research, particularly so
if flexible web interfaces are implemented. In ad-
dition, they make it possible for NLP and Web
studies to converge, opening new fields of research
(e.g. sociolinguistic studies of the Web).
We have argued that the developed architecture
allows for the creation of Web corpora in general.
In fact, in the near future we plan to build a Span-
ish Web corpus and integrate it into the same web
interface, using the data already gathered. The
Spanish corpus, however, will be much larger than
the Catalan one (a conservative estimate is 600
25
million words), so that new challenges in process-
ing and searching it will arise.
We have also reviewed some of the challenges
that Web data pose to existing NLP tools, and ar-
gued that most are not new (textual layout, mis-
spellings, multilinguality), but more frequent on
the Web. To address some of them, we plan to de-
velop a more sophisticated pre-processing module
and a sentence-based language classifier and filter.
A more general challenge of Web corpora is the
control over its contents. Unlike traditional cor-
pora, where the origin of each text is clear and
deliberate, in CUCWeb the strategy is to gather
as much text as possible, provided it meets some
quality heuristics. The notion of balance is not
present anymore, although this needs not be a
drawback (Web corpora are at least representa-
tive of the language on the Web). However, what
is arguably a drawback is the black box effect
of the corpus, because the impact of text genre,
topic, and so on cannot be taken into account.
It would require a text classification procedure to
know what the collected corpus contains, and this
is again a meeting point for Web studies and NLP.
Acknowledgements
Mar??a Eugenia Fuenmayor and Paulo Golgher managed the
Web crawler during the downloading process. The language
classifier was developed by Ba?rbara Poblete. The corpora
used to train the language detection module were kindly
provided by Universita?t Gesamthochschule, Paderborn (Ger-
man), by the Institut d?Estudis Catalans, Barcelona (Catalan),
by the TALP group, Universitat Polite`cnica de Catalunya
(Spanish), by the IXA Group, Euskal Herriko Unibertsitatea
(Basque), by the Centre de Traitement Automatique du Lan-
gage de l?UCL, Leuven (French, Dutch and Portuguese),
by the Seminario de Lingu???stica Informa?tica, Universidade
de Vigo (Galician) and by the Istituto di Linguistica Com-
putazionale, Pisa (Italian). We thank Mart?? Quixal for his
revision of a previous version of this paper and three anony-
mous reviewers for useful criticism.
This project has been partially funded by Ca?tedra
Telefo?nica de Produccio?n Multimedia.
References
`Alex Alsina, Toni Badia, Gemma Boleda, Stefan Bott,
`Angel Gil, Mart?? Quixal, and Oriol Valent??n. 2002.
CATCG: a general purpose parsing tool applied. In
Proceedings of Third International Conference on
Language Resources and Evaluation, Las Palmas,
Spain.
Anonymous. 2000. 1.6 billion served: the Web ac-
cording to Google. Wired, 8(12):18?19.
Ricardo Baeza-Yates, Carlos Castillo, and Vicente
Lo?pez. 2005. Characteristics of the Web of Spain.
Cybermetrics, 9(1).
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Association for Computational Linguistics,
pages 26?33.
Marco Baroni and Motoko Ueyama. To appear. Build-
ing general- and special-purpose corpora by web
crawling. In Proceedings of the NIJL International
Workshop on Language Corpora.
Andrei Z. Broder. 2000. Identifying and filtering
near-duplicate documents. In Combinatorial Pat-
tern Matching, 11th Annual Symposium, pages 1?
10, Montreal, Canada.
Kenneth W. Church and Robert L. Mercer. 1993. In-
troduction to the special issue on computational lin-
guistics using large corpora. Computational Lin-
guistics, 19(1):1?24.
Altigran da Silva, Eveline Veloso, Paulo Golgher, Al-
berto Laender, and Nivio Ziviani. 1999. Cobweb -
a crawler for the brazilian web. In String Processing
and Information Retrieval (SPIRE), pages 184?191,
Cancun, Mexico. IEEE CS Press.
Dennis Fetterly, Mark Manasse, and Marc Najork.
2004. Spam, damn spam, and statistics: Using sta-
tistical analysis to locate spam web pages. In Sev-
enth workshop on the Web and databases (WebDB),
Paris, France.
Gregory Grefenstette. 1998. The World Wide Web
as a resource for example-based machine translation
tasks. In ASLIB Conference on Translating and the
Computer, volume 21, London, England.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29:459?484.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the Web as corpus.
Computational Linguistics, 29(3):333?347.
Anke Lu?deling, Stefan Evert, and Marco Baroni. To
appear. Using web data for linguistic purposes. In
Marianne Hundt, Caroline Biewer, and Nadja Nes-
selhauf, editors, Corpus Linguistics and the Web.
Rodopi, Amsterdam.
Laia Mayol, Gemma Boleda, and Toni Badia. 2006.
Automatic acquisition of syntactic verb classes with
basic resources. Submitted.
Andrew K. Mccallum. 1996. Bow: A
toolkit for statistical language modeling,
text retrieval, classification and clustering.
<http://www.cs.cmu.edu/?mccallum/bow/>.
Joaquim Rafel. 1994. Un corpus general de refere`ncia
de la llengua catalana. Caplletra, 17:219?250.
Mike Thelwall. 2005. Creating and using web cor-
pora. International Journal of Corpus Linguistics,
10(4):517?541.
26
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 79?84,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Second-Order Joint Eisner Model for Syntactic and Semantic Dependency
Parsing
Xavier Llu??s Stefan Bott Llu??s Ma`rquez
TALP Research Center ? Software Department (LSI)
Technical University of Catalonia (UPC)
{xlluis,sbott,lluism}@lsi.upc.edu
Abstract
We present a system developed for the
CoNLL-2009 Shared Task (Hajic? et al, 2009).
We extend the Carreras (2007) parser to
jointly annotate syntactic and semantic depen-
dencies. This state-of-the-art parser factor-
izes the built tree in second-order factors. We
include semantic dependencies in the factors
and extend their score function to combine
syntactic and semantic scores. The parser is
coupled with an on-line averaged perceptron
(Collins, 2002) as the learning method. Our
averaged results for all seven languages are
71.49 macro F1, 79.11 LAS and 63.06 seman-
tic F1.
1 Introduction
Systems that jointly annotate syntactic and semantic
dependencies were introduced in the past CoNLL-
2008 Shared Task (Surdeanu et al, 2008). These
systems showed promising results and proved the
feasibility of a joint syntactic and semantic pars-
ing (Henderson et al, 2008; Llu??s and Ma`rquez,
2008).
The Eisner (1996) algorithm and its variants are
commonly used in data-driven dependency pars-
ing. Improvements of this algorithm presented
by McDonald et al (2006) and Carreras (2007)
achieved state-of-the-art performance for English in
the CoNLL-2007 Shared Task (Nivre et al, 2007).
Johansson and Nugues (2008) presented a sys-
tem based on the Carreras? extension of the Eis-
ner algorithm that ranked first in the past CoNLL-
2008 Shared Task. We decided to extend the Car-
reras (2007) parser to jointly annotate syntactic and
semantic dependencies.
The present year Shared Task has the incentive
of being multilingual with each language presenting
their own particularities. An interesting particularity
is the direct correspondence between syntactic and
semantic dependencies provided in Catalan, Spanish
and Chinese. We believe that these correspondences
can be captured by a joint system. We specially look
at the syntactic-semantic alignment of the Catalan
and Spanish datasets.
Our system is an extension of the Llu??s and
Ma`rquez (2008) CoNLL-2008 Shared Task system.
We introduce these two following novelties:
? An extension of the second-order Car-
reras (2007) algorithm to annotate semantic
dependencies.
? A combined syntactic-semantic scoring for
Catalan and Spanish to exploit the syntactic-
semantic mappings.
The following section outlines the system archi-
tecture. The next sections present in more detail the
system novelties.
2 Architecture
The architecture consists on four main components:
1) Preprocessing and feature extraction. 2) Syntactic
preparsing. 3) Joint syntactic-semantic parsing. 4)
Predicate classification.
The preprocessing and feature extraction is in-
tended to ease and improve the performance of
the parser precomputing a binary representation of
79
each sentence features. These features are borrowed
from existing and widely-known systems (Xue and
Palmer, 2004; McDonald et al, 2005; Carreras et al,
2006; Surdeanu et al, 2007).
The following step is a syntactic pre-parse. It
is only required to pre-compute additional features
(e.g., syntactic path, syntactic frame) from the syn-
tax. These new features will be used for the semantic
role component of the following joint parser.
The joint parser is the core of the system. This
single algorithm computes the complete parse that
optimizes a score according to a function that de-
pends on both syntax and semantics. Some of the
required features that could be unavailable or expen-
sive to compute at that time are provided by the pre-
vious syntactic pre-parse.
The predicate sense classification is performed as
the last step. Therefore no features representing the
predicate sense are employed during the training.
The predicates are labeled with the most frequent
sense extracted from the training corpus.
No further postprocessing is applied.
3 Second-order Eisner model
The Carreras? extension of the Eisner inference al-
gorithm is an expensive O(n4) parser. The number
of assignable labels for each dependency is a hidden
multiplying constant in this asymptotic cost.
We begin describing a first-order dependency
parser. It receives a sentence x and outputs a de-
pendency tree y. A dependency, or first-order factor,
is defined as f1 = ?h,m, l?. Where h is the head
token, m the modifier and l the syntactic label. The
score for this factor f1 is computed as:
score1(f1, x,w) = ?(h,m, x) ?w(l)
Where w(l) is the weight vector for the syntactic la-
bel l and ? a feature extraction function.
The parser outputs the best tree y? from the set
T (x) of all projective dependency trees.
y?(x) = argmax
y?T (x)
?
f1?y
score(f1, x,w)
The second-order extension decomposes the de-
pendency tree in factors that include some children
of the head and modifier. A second-order factor is:
f2 = ?h,m, l, ch, cmo, cmi?
where ch is the daughter of h closest to m within
the tokens [h, . . . ,m]; cmo is the outermost daugh-
ter of m outside [h, . . . ,m]; and cmi is the furthest
daughter of m inside [h, . . . ,m].
The score for these new factors is computed by
score2(f2, x,w) = ?(h,m, x) ?w(l) +
?(h,m, ch, x) ?w(l)ch +
?(h,m, cmi, x) ?w(l)cmi +
?(h,m, cmo, x) ?w(l)cmo
The parser builds the best-scoring projective tree
factorized in second-order factors. The score of the
tree is also defined as the sum of the score of its
factors.
3.1 Joint second-order model
We proceeded in an analogous way in which the
Llu??s and Ma`rquez (2008) extended the first-order
parser. That previous work extended a first-order
model by including semantic labels in first-order de-
pendencies.
Now we define a second-order joint factor as:
f2syn-sem =?
h,m, l, ch, cmo, cmi, lsemp1 , . . . , lsempq
?
Note that we only added a set of semantic labels
lsemp1 , . . . , lsempq to the second-order factor. Each
one of these semantic labels represent, if any, one
semantic relation between the argument m and the
predicate pi. There are q predicates in the sentence,
labeled p1, . . . , pq.
The corresponding joint score to a given joint fac-
tor is computed by adding a semantic score to the
previously defined score2 second-order score func-
tion:
score2syn-sem(f2syn-sem, x,w) =
score2(f2, x,w) +
?
pi
scoresem(h,m, pi, lsempi , x,w)
q
where,
scoresem(h,m, pi, lsem, x,w) =
?sem(h,m, pi, x) ?w(lsem)
80
We normalize the semantic score by the number
of predicates q. The semantic score is computed as a
score betweenm and each sentence predicate pi. No
second-order relations are considered in these score
functions. The search of the best ch, cmo and cmi is
independent of the semantic components of the fac-
tor. The computational cost of the algorithm is in-
creased by one semantic score function call for every
m, h, and pi combination. The asymptotic cost of
this operation is O(q ? n2) and it is sequentially per-
formed among other O(n2) operations in the main
loop of the algorithm.
Algorithm 1 Extension of the Carreras (2007) algo-
rithm
C[s][t][d][m]? 0, ?s, t, d,m
O[s][t][d][l]? 0,?s, t, d, l
for k = 1, . . . , n do
for s = 0, . . . , n? k do
t? s+ k
?l O[s][t][?][l] = maxr,cmi,ch
C[s][r][?][cmi] + C[r + 1][t][?][ch]
+score(t, s, l)+scorecmi(t, s, cmi, l)+
scorech(t, s, l, ch)+?
pi maxlsemscoresem(t, s, pi, lsem)/q
?l O[s][t][?][l] = maxr,cmi,ch
C[s][r][?][ch] + C[r + 1][t][?][cmi]+
score(s, t, l)+scorecmi(s, t, cmi, l)+
scorech(s, t, l, ch)+?
pi maxlsemscoresem(t, s, pi, lsem)/q
?m C[s][t][?][m] = maxl,cmo
C[s][m][?][cmo] +O[m][t][?][l]+
scorecmo(s,m, l, cmo)
?m C[s][t][?][m] = maxl,cmo
O[s][m][?][l] + C[m][t][?][cmo]+
scorecmo(m, t, l, cmo)
end for
end for
Our implementation slightly differs from the orig-
inal Carreras algorithm description. The main dif-
ference is that no specific features are extracted for
the second-order factors. This allows us to reuse the
feature extraction mechanism of a first-order parser.
Algorithm 1 shows the Carreras? extension of the
Eisner algorithm including our proposed joint se-
mantic scoring.
The tokens s and t represent the start and end
tokens of the current substring, also called span.
The direction d ? {?,?} defines whether t or
s is the head of the last dependency built inside
the span. The score functions scorech,scorecmi and
scorecmo are the linear functions that build up the
previously defined second-order global score, e.g.,
scorech= ?(h,m, ch, x)?w(l)ch . The two tablesC and
O maintain the dynamic programming structures.
Note that the first steps of the inner loop are ap-
plied for all l, the syntactic label, but the semantic
score function does not depend on l. Therefore the
best semantic label can be chosen independently.
For simplicity, we omitted the weight vectors re-
quired in each score function and the backpointers
tables to save the local decisions. We also omit-
ted the definition of the domain of some variables.
Moreover, the filter of the set of assignable labels
is not shown. A basic filter regards the POS of the
head and modifier to filter out the set of possible ar-
guments for each predicate. Another filter extract
the set of allowed arguments for each predicate from
the frames files. These last filters were applied to the
English, German and Chinese.
3.2 Catalan and Spanish joint model
The Catalan and Spanish datasets (Taule? et al, 2008)
present two interesting properties. The first prop-
erty, as previously said, is a direct correspondence
between syntactic and semantic labels. The second
interesting property is that all semantic dependen-
cies exactly overlap with the syntactic tree. Thus
the semantic dependency between a predicate and
an argument always has a matching syntactic depen-
dency between a head and a modifier. The Chinese
data also contains direct syntactic-semantic map-
pings. But due to the Shared Task time constraints
we did not implemented a specific parsing method
for this language.
The complete overlap between syntax and seman-
tics can simplify the definition of a second-order
joint factor. In this case, a second-order factor will
only have, if any, one semantic dependency. We only
allow at most one semantic relation lsem between
the head token h and the modifier m. Note that h
must be a sentence predicate and m its argument if
81
lsem is not null. We extend the second-order fac-
tors with a single and possibly null semantic label,
i.e., f2syn-sem = ?h,m, l, ch, cmo, cmi, lsem?. This
slightly simplifies the scoring function:
score2syn-sem(f2syn-sem, x,w) =
score2(f2, x,w) +
? ? scoresem(h,m, x,w)
where ? is an adjustable parameter of the model and,
scoresem(h,m, x,w) = ?sem(h,m, x) ?w(lsem)
The next property that we are intended to exploit
is the syntactic-semantic mappings. These map-
pings define the allowed combinations of syntactic
and semantic labels. The label combinations can
only be exploited when there is semantic depen-
dency between the head h and the modifier m of a
factor. An argument identification classifier deter-
mines the presence of a semantic relation, given h
is a predicate. In these cases we only generate fac-
tors that are compliant with the mappings. If a syn-
tactic label has many corresponding semantic labels
we will score all of them and select the combination
with the highest score.
The computational cost is not significantly in-
creased as there is a bounded number of syntactic
and semantic combinations to score. In addition, the
only one-argument-per-factor constraint reduces the
complexity of the algorithm with respect to the pre-
vious joint extension.
We found some inconsistencies in the frames files
provided by the organizers containing the correspon-
dences between syntax and semantics. For this rea-
son we extracted them directly from the corpus. The
extracted mappings discard the 7.9% of the cor-
rect combinations in the Catalan development cor-
pus that represent a 1.7% of its correct syntactic de-
pendencies. The discarded semantic labels are the
5.14% for Spanish representing the 1.3% of the syn-
tactic dependencies.
4 Results and discussion
Table 1 shows the official results for all seven lan-
guages, including out-of-domain data labeled as
ood. The high computational cost of the second-
order models prevented us from carefully tuning the
system parameters. After the shared task evaluation
deadline, some bug were corrected, improving the
system performance. The last results are shown in
parenthesis.
The combined filters for Catalan and Spanish hurt
the parsing due to the discarded correct labels but
we believe that this effect is compensated by an im-
proved precision in the cases where the correct la-
bels are not discarded. For example, in Spanish
these filters improved the syntactic LAS from 85.34
to 86.77 on the development corpus using the gold
syntactic tree as the pre-parse tree.
Figure 1 shows the learning curve for the English
and Czech language. The results are computed in
the development corpus. The semantic score is com-
puted using gold syntax and gold predicate sense
classification. We restricted the learning curve to
the first epoch. Although the this first epoch is very
close to the best score, some languages showed im-
provements until the fourth epoch. In the figure we
can see better syntactic results for the joint system
with respect to the syntactic-only parser. We should
not consider this improvement completely realistic
as the semantic component of the joint system uses
gold features (i.e., a gold pre-parse). Nonetheless,
it points that a highly accurate semantic component
could improve the syntax.
Table 2 shows the training time for a second-order
syntactic and joint configurations of the parser. Note
that the time per instance is an average and some
sentences could require a significantly higher time.
Recall that our parser is O(n4) dependant on the
sentence length. We discarded large sentences dur-
ing training for efficiency reasons. We discarded
sentences with more than 70 words for all languages
except for Catalan and Spanish where the thresh-
old was set to 100 words in the syntactic parser.
This larger number of sentences is aimed to im-
prove the syntactic performance of these languages.
The shorter sentences used in the joint parsing and
the pruning of the previously described filters re-
duced the training time for Catalan and Spanish. The
amount of main memory consumed by the system is
0.5?1GB. The machine used to perform the compu-
tations is an AMD64 Athlon 5000+.
82
avg cat chi cze eng ger jap spa
macro F1 71.49 (74.90) 56.64 (73.21) 66.18 (70.91) 75.95 81.69 72.31 81.76 65.91 (68.46)
syn LAS 79.11 (82.22) 64.21(84.20) 70.53 (70.90) 75.00 87.48 81.94 91.55 83.09 (84.48)
semantic F1 63.06 (67.41) 46.79 (61.68) 59.72 (70.88) 76.90 75.86 62.66 71.60 47.88 (52.30)
ood macro F1 71.92 - - 74.56 73.91 67.30 - -
ood syn LAS 75.09 - - 72.11 80.92 72.25 - -
ood sem F1 68.74 - - 77.01 66.88 62.34 - -
Table 1: Overall results. In parenthesis post-evaluation results.
cat chi cze eng ger jap spa
syntax only (s/sentence) 18.39 8.07 3.18 2.56 1.30 1.07 15.31
joint system (s/sentence) 10.91 9.49 3.99 3.13 2.36 1.25 12.29
Table 2: Parsing time per sentence.
 
70
 
72
 
74
 
76
 
78
 
80
 
82
 
84
 
86
 
88
 
90
 
92  10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90
 
100
semanic f1, LAS
% of c
orpus
syn cz
syn cz
 joint
sem 
cz joint syn eng
syn en
g joint
sem 
eng join
t
Figure 1: Learning curves for the syntactic-only and joint
parsers in Czech and English.
5 Conclusion
We have shown that a joint syntactic-semantic
parsing can be based on the state-of-the-art Car-
reras (2007) parser at an expense of a reasonable
cost. Our second-order parser still does not repro-
duce the state-of-the art results presented by similar
systems (Nivre et al, 2007). Although we achieved
mild results we believe that a competitive system
based in our model can be built. Further tuning is
required and a complete set of new second-order fea-
tures should be implemented to improve our parser.
The multilingual condition of the task allows us to
evaluate our approach in seven different languages.
A detailed language-dependent evaluation can give
us some insights about the strengths and weaknesses
of our approach across different languages. Unfor-
tunately we believe that this objective was possibly
not accomplished due to the time constraints.
The Catalan and Spanish datasets presented in-
teresting properties that could be exploited. The
mapping between syntax and semantics should be
specially useful for a joint system. In addition
the semantic dependencies for these languages are
aligned with the projective syntactic dependencies,
i.e., the predicate-argument pairs exactly match syn-
tactic dependencies. This is a useful property to si-
multaneously build joint dependencies.
6 Future and ongoing work
Our syntactic and semantic parsers, as many others,
is not exempt of bugs. Furthermore, very few tuning
and experimentation was done during the develop-
ment of our parser due to the Shared Task time con-
straints. We believe that we still did not have enough
data to fully evaluate our approach. Further exper-
imentation is required to asses the improvement of
a joint architecture vs. a pipeline architecture. Also
a careful analysis of the system across the different
languages is to be performed.
Acknowledgments
We thank the corpus providers (Taule? et al, 2008;
Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006; Kawahara et al,
2002) for their effort in the annotation and conver-
sion of the seven languages datasets.
83
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras, Mihai Surdeanu, and Llu??s Ma`rquez.
2006. Projective dependency parsing with perceptron.
In Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL-2006).
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
11th Conference on Computational Natural Language
Learning (CoNLL-2007).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING-96).
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of the 12th Conference on
Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with propbank and nombank. In Proceedings of the
12th Conference on Computational Natural Language
Learning (CoNLL-2008), Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s and Llu??s Ma`rquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL-2008), Manch-
ester, UK.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In 11th Conference of the European Chapter of
the Association for Computational Linguistics (EACL-
2006).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
2005).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In Proceedings of the
Empirical Methods in Natural Language Processing
(EMNLP-2004).
84
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 182?192,
Dublin, Ireland, August 23-24 2014.
Syntactic Transfer Patterns of German Particle Verbs and their Impact on
Lexical Semantics
Stefan Bott Sabine Schulte im Walde
Institut f?ur Maschinelle Sprachverabeitung
Universit?at Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{stefan.bott,schulte}@ims.uni-stuttgart.de
Abstract
German particle verbs, like anblicken (to
gaze at) combine a base verb (blicken)
with a particle (an) to form a special
kind of Multi Word Expression. Parti-
cle verbs may share the semantics of the
base verb and the particle to a variable de-
gree. However, while syntactic subcate-
gorization frames tend to be good predic-
tor for the semantics of verbs in general
(verbs that are similar in meaning also tend
to have similar subcategorization frames
and selectional preferences), there are reg-
ular changes in subcategorization frames
by particle verbs with regard to the corre-
sponding base verbs. This paper demon-
strates that the syntactic behavior of par-
ticle verbs and base verbs together (mod-
eling regular changes in subcategorization
frames by particle verbs and correspond-
ing base verbs) and applying clustering
techniques allows us to distinguish parti-
cle verb meaning and shows the tight con-
nection between transfer patterns and the
semantic classes of particle verbs.
1 Introduction
In German, particle verbs (PVs), like anblicken in
(1), are a highly productive class. PVs present
challenges for a both theoretical analysis and their
computational treatment. One of the central prob-
lems is the prediction of their meaning from their
constituent parts: the base verb (BV, e.g. blicken
in (1)) and the particle (e.g. an). Many PVs de-
rive their meaning from the corresponding BVs ?
with a varying degree of transparency. It is often
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
not clear, however, how to interpret the semantics
of the particles and their contribution to the mean-
ing of the PVs. Since particles never occur iso-
lated, without the context of the verb, it is difficult
to assign them a lexical semantic entry on their
own. Even more, German particles are a notori-
ously ambiguous word class.
(1) Das
The
Kind
child
blickt
gazes
seine
his-acc
Mutter
mother
an.
PRT.
The child looks at his mother.
One way to approximate the meaning of parti-
cles is to group together the particle verbs which
share the same particle into semantic groups (such
as anblicken, anstarren, anschauen ?to stare/look
at?), such that both the meaning of the PV and the
meaning of the BV is similar in each group. This
allows us to make inferences like ?taking a BV
from semantic group ? and particle ?, we will de-
rive a PV from semantic group ??. Such groups
can be established and they represent productive
paradigms. Springorum et al. (2013) have shown
in a generation experiment setup that subjects are
able to associate a meaning to artificially created,
previously unattested PVs and to construct exam-
ple sentences for them.
1
Different subjects also
agree to a large degree on the meaning they at-
tribute to the newly formed lexical items.
But this approach also rises a series of ques-
tions, especially concerning the way in which such
groups can be distinguished, both from a theoreti-
cal and a corpus-based perspective. For example,
which kinds of linguistic features allow us to dis-
criminate such semantic classes? In this paper we
investigate the influence of syntax, which repre-
sents one of the possible feature sources. Syn-
1
For example for the neologism anlauschen, referring to a
partitive meaning of the particle, senentence like the follow-
ing could be found: Er hatte an der Wand angelauscht und
wusste Bescheid. (?He had listened at the wall and knew
everything.?)
182
tactic subcategorization frames tend to be good
predictors for the semantics of verbs in general:
verbs that are similar in meaning also tend to have
similar subcategorization frames and selectional
preferences (Schulte im Walde, 2000; Merlo and
Stevenson, 2001; Korhonen et al., 2003; Schulte
im Walde, 2006a; Joanis et al., 2008). But, as
we will show below, PV-BV pairs tend to have a
special behavior with respect to their subcatego-
rization, even if their meanings are closely related.
Because we are interested in pairs of PVs and their
BVs, we thus have to look at pairs of subcatego-
rization preferences, and rely on the concept of
syntactic transfer. We use syntactic transfer as
a technical term here, which we define as regular
changes in subcategorization frames by PVs and
corresponding BVs, e.g., the incorporation or ad-
dition of complements of PVs in comparison to
their BVs (Stiebels, 1996; L?udeling, 2001; Fleis-
cher and Barz, 2012a). We claim that the syntac-
tic behavior of PVs and BVs together allows us to
distinguish semantic classes.
A better understanding of the nature of the con-
nection between syntactic transfer patterns and se-
mantic classes may be beneficial for both theoret-
ical and computational linguistics. On the theo-
retical side we can hope to find new arguments to
guide and justify lexical semantic classifications.
We may also shed light on what particles actu-
ally mean, a topic which is not trivial by itself.
In computational semantics, a better understand-
ing of syntactic transfer patterns can potentially
contribute to a better treatment of PVs in meaning-
related areas, such as machine translation and in-
formation retrieval.
In sum, this paper makes the following contri-
butions:
? We show that the meaning of verb particles
can be modeled as classes of pairs of PVs and
their corresponding BVs, where both PVs
and BVs in each class are closely related in
meaning. In addition, the PV-BV pairs in
each class undergo the same syntactic trans-
fers, i.e. the selectional preferences of PV-
BV pairs within each class tend to be very
similar, even if the subcategorization pref-
erences may be different between PVs and
BVs.
? We show that automatic clustering can repli-
cate a gold standard classification of PV-BV
pairs to a large degree when clustering only
relies on syntax and the gold standard reflects
semantic regularities.
The rest of this paper is organized as follows: In
section 2 we describe the task and our goals. Here
we also define the term syntactic transfer pattern,
which is central to our discussion. Section 3 is
dedicated to related work relevant for our study.
In section 4 we describe the experimental setup,
while sections 5 and 6 present the experiment re-
sults and discuss them.
2 Goal and Motivation
The work we describe here centers around the con-
cept of semantic classes and syntactic transfer pat-
terns. As concerning the semantic side, the PVs
which share the same particle may be grouped into
different classes according to their meaning. For
example, among the PVs incorporating the parti-
cle an we find a group of verbs whose meanings
center around the concept of ?to look at some-
one/something in manner X?, ?to attach something
somewhere in manner X?, ?to make an unpleasant
sound towards someone in a manner X? and ?to
start an action X on something which starts con-
suming it?, as exemplified in (2) a-d.
(2) a. A
A
blickt/schaut/starrt/stiert/
looks/stares/gazes
B
B
an.
PRT.
A looks/stares/gazes at B.
b. A
A
klebt/heftet/schraubt/nagelt
glues/affixes/screws
B
B
an
at/onto
C
C
an.
PRT.
A glues/affixes/screws B onto C.
c. A
A
br?ullt/faucht/bellt/meckert
roars/hisses/bleats
B
B
an.
PRT.
A brawls/hisses/scolds at B.
d. A
A
schneidet/bricht/rei?t
cuts/breaks/tears
B
B
an.
PRT.
A cuts/breaks/tears the first
slice/piece of B.
Such semantic classes are not easy to define and
they are also difficult to induce automatically. Al-
though there is general agreement in the theo-
retical literature that such semantic classes for
PVs exist (cf. Lechler and Ro?deutscher (2009),
Kliche (2011) and Springorum (2011)) the agree-
ment on the number and nature of such classes is
not very high. For example, Springorum (2011)
(who develops her analysis within Discourse Rep-
183
resentation Theory (Kamp and Reyle, 1993)) dis-
tinguishes between 11 classes of PVs with the par-
ticle an, while Fleischer and Barz (2012b) only
distinguish 3 major de-verbal classes, based on
their aktionsart, which can be divided into some
9 minor classes.
2
It should be noted that all the
PVs and BVs in (2) a-d are not only quite homo-
geneous in their semantics; they also form coher-
ent syntactic classes. The PVs and BVs of these
examples are quite similar in the way they typi-
cally select their syntactic complements. For ex-
ample, the BVs of (2-a) typically take a PP argu-
ment that expresses the direction of gaze using a
prepositional phrases with one of the prepositions
auf, zu, nach or in subcategorizing a dative noun
phrase. The corresponding PVs, however, typi-
cally express this semantic role by an accusative
object. The type of change from the typical frame
of a BV to the typical frame of a PV is an example
of what we mean by a syntactic transfer pattern.
So, while similar syntactic behavior of two
verbs in general may indicate that the verbs are
also semantically similar, this is typically not the
case for PV-BV pairs. Compare (1) to (3), which
are nearly synonymous but (3) uses the BV blicken
instead of the PV anblicken in (1). We can only in-
duce the similarity of the PV and the BV if we take
the syntactic transfer into consideration.
(3) a. Das
The
Kind
child
blickt
looks
zu
at
seiner
his-dat
Mutter.
mother.
b. Das
The
Kind
child
stiert/starrt/schaut
stares/stares/looks
zu
at
seiner
his-dat
Mutter.
Mother.
Looking at the class to which this PV belongs, all
the variants of (3-b) are semantically very similar
to (3-a). This also corresponds to a syntactic sim-
ilarity: all the verbs of this group share the same
preferred syntactic subcategorization frames. The
dominant frame of theses verbs is ?NPnom+PP-
dat? (the head preposition of the PP may vary, but
within well-defined limits). But this is not the case
for the PV anblicken in (1). (1) is nearly synony-
mous to (3-a), but the PV in this example has a to-
tally different frame, namely the simple transitive
?NPnom+NP-acc?. It may not come as a surprise
that all of the verbs in (3-b) have PV counterparts
(anstieren, anstarren, etc.), which all behave syn-
2
The subdivision is, however not fully spelled out and
only implicit in their description.
tactically like anblicken.
In sum, we part from the hypothesis that there
is a tight connection between transfer patterns and
the semantic classes of PVs. There is only one
more point to make: the classes shown in (2),
could actually be seen as reflecting different mean-
ings of the particle an itself.
3 Related Work
Particle verbs have been studied from the theo-
retical perspective and, to a more limited extend,
from the aspect of the computational predictabil-
ity of the degree of semantic compositionality (the
transparency of their meaning with respect to the
meaning of the base verb and the particle) and the
semantic classifiabilty of PVs.
For English, there is work on the automatic
extraction of PVs from corpora (Baldwin and
Villavicencio, 2002; Baldwin, 2005; Villavicen-
cio, 2005) and the determination of composition-
ality (McCarthy et al., 2003; Baldwin et al., 2003;
Bannard, 2005).
To the best of our knowledge Aldinger (2004)
is the first work that studies German PVs from a
corpus based perspective, with an emphasis on the
syntactic behavior and syntactic change. Schulte
im Walde (2004), Schulte im Walde (2005) and
Schulte im Walde (2006b) present preliminary dis-
tributional studies to explore salient features at the
syntax-semantics interface that determine the se-
mantic nearest neighbours of German PVs. Re-
lying on the insights of those studies, Schulte
im Walde (2006b) and Hartmann (2008) describe
experiments which model the subcategorization
transfer of German PVs with respect to their BVs
in order to strengthen PV-BV distributional simi-
larity. The main goal for them is to use transfer in-
formation in order to predict the degree of seman-
tic compositionality of PVs. K?uhner and Schulte
im Walde (2010) use clustering to determine the
degree of compositionality of German PVs, via
common PV-BV cluster membership. They are,
again, mainly interested in the assessment of com-
positionality, which is done on the basis of lexi-
cal information. They use syntactic information,
but only as a filter and for lexical heads as cooc-
currence features in order to limit the selected ar-
gument slots to certain syntactic functions. They
conclude that the best results can be obtained with
information stemming from direct objects and PP-
objects. The incorporation of syntactic informa-
184
tion in the form of dependency arc labels (concate-
nated with the head nouns) does not yield satisfac-
tory results, putting the syntactic transfer problem
in evidence, again. They conclude that an incor-
poration of syntactic transfer information between
BVs and PVs could possibly improve the results.
Based on a theoretical study (Springorum,
2011), which explains particle meanings in terms
of Discourse Representation Theory (Kamp and
Reyle, 1993), Springorum et al. (2012) show that
four classes of PVs with the particle an can be
classified automatically. They take a supervised
approach using decision trees. The use of decision
trees also allows them to manually inspect and an-
alyze the decisions made by the classifier. As pre-
dictive features they use the head nouns of objects,
generalized classes of these nouns and PP types.
The approach we take here is not fully compa-
rable to any of the former approaches, since we
try to derive a semantic classification BV-PP pairs
in an unsupervised manner and we only use syn-
tactic features, stemming from corpus instances of
both the BVs and the PVs. In other words, we do
not attempt to classify PVs, but we try to classify
syntactic transfers and, by doing so, we identify
syntactic transfer patterns which we hypothesize
to have a close relation to semantic PV classes and
the semantics of the particles.
4 Experimental Setup
4.1 Gold Standard Classification
For testing our hypothesis, we created a gold stan-
dard of 32 PVs, including 14 with the particle an
and 18 with the particle auf. We concentrated on
two particles here in order to have a small and con-
trolled test bed which allows us to study the syn-
tactic transfers.
We based the creation of the gold standard on
the classification by Fleischer and Barz (2012b),
but we further distinguished the classes based
on the meanings of the BVs. For example, we
grouped all the BVs with the meaning of ?looking
in a manner X? or ?tying X to Y in a manner Z?.
From these classes we selected those which had
a clear subcategorization pattern for both the BVs
and the PVs. We discarded such PVs where ei-
ther the PV itself or its underlying BV was clearly
ambiguous. The full gold standard can be seen in
table 2. The table also lists the expected dominant
subcategorization frames for the BVs and PVs of
each category.
While the gold standard was based on theo-
retic considerations, we expected it to correlate
with human intuitions. To test this, we presented
the gold standard verbs to 6 human raters. These
raters were all German native speakers with work-
ing practice in various areas of linguistics or lan-
guage didactics. The raters were not directly asked
to group PVs into categories. Instead the PVs were
presented in pairs
3
and raters had to make a deci-
sion on whether or not the pairs belong to the same
semantic category (even if they could not think
of a name or description of that category). No
pre-defined categories were given, nor were raters
asked to provide a name or description for these
categories. The annotators were asked to take the
similarity of the BVs and the similarity of the PVs
into consideration for their judgements. In order
to avoid possible bias, the verbs were presented
without given context. What is important here is
that we did not ask them to take any syntactic cri-
terion into consideration, the criterion we used for
the initial compilation of the gold standard.
The inter-annotator agreement was substantial
with a Fleiss? Kappa score of 0.68 (Fleiss, 1971).
4
As a measure of agreement between raters and the
previously created gold standard, we performed
pair-wise calculations between the ratings of each
annotator and the gold standard. For the compar-
ison, the gold standard was transformed into PV
pairs and the value true was assigned if the two
verbs of a pairs belonged to the same category, and
false otherwise. We calculated the Kappa scores
for each annotator and took the average of the
agreement scores. Table 1 resumes the compari-
son. Values are given for the parts of the gold stan-
dard corresponding to PVs with an and auf sepa-
rately and also for the gold standard as a whole.
It can clearly be seen that humans agreement
with the gold standard is as high as the agree-
ment among different annotators. This shows that
the gold standard used here is a valid represen-
tation of human language intuition. Most impor-
tantly, the annotators did not use syntactic criteria
3
All possible PV combinations were generated, but the
PVs with an were kept separate from those with auf in order
to avoid an unnecessary explosion of the number of pairs to
be rated.
4
One of the 6 raters showed less agreement with the other
raters. If we eliminate this rater from the calculation of agree-
ment, we achieve an even higher Kappa score of 0.76 and also
agreement scores with the gold standard improved. Two of
the annotators even achieved Kappa scores of over 0.80 when
compared to the gold standard.
185
and still validated a gold standard whose creation
was explicitly based on syntactic subcategoriza-
tion frames. In other words: there is an apparent
tight interrelation between syntax and semantics
for PVs, at least in the sense that semantic dis-
tinctions can be used to predict different syntactic
behaviour. The inverse case - predicting semantic
classes from syntactic information - will be dis-
cussed below.
4.2 Corpus Data
We used a lemmatized and tagged version of the
SdeWaC corpus (Faa? and Eckart, 2013), a web
corpus of 880 million words. For linguistic pre-
processing we used the MATE parser (Bohnet,
2010), which allowed us to extract syntactic sub-
categorization frames.
4.3 Feature Selection
For each PV-BV pair we extracted two parallel sets
of features, one pertaining to the BV and one for
the PV. This allows us to model the syntactic trans-
fer. For example, we expected that an ideal trans-
fer from a group of transitive BVs to a group of
intransitive PVs should be reflected in high values
for the features BV:transitive and PV:intransitive
5
and, in turn, low values for BV:intransitive and
PV:transitive.
We had two ways of selecting the feature types:
manually and automatically. For the manual fea-
ture selection we extracted only those features
from the parsed frames which we already used in
the creation of the gold standard and which are
listed in table 2. This resulted in a small feature
set of 30 features (15 features for PVs and BVs,
respectively). For the automatic feature selection
we simply used the n most frequent frames which
could be observed in the corpus for the set of verbs
of the gold standard.
From the syntactic dependency representation
provided by the parser, we excluded subjects and
modifiers (except for PP-modifiers) in the repre-
sentation of subcat frames. We did not use infor-
mation on subjects, because in German all verbs
have subjects, which may be implicit in the case
of subordinate clauses. We found that for this
reason that with the representation of subjects in
the extracted features no relevant information was
5
Note that transitive and transitive are only convenient
abreviations for the labels NPnom and NPnom+NPacc, which
are used in table2.
gained, but some distortion was introduced. Mod-
ifiers in the MATE parser represent information
which is too general to be good predictors. Based
on theoretical considerations on the best lexico-
graphic representation of verbs, we included PP-
modifiers, however, because quantitative informa-
tion on PP-adjuncts has proven successful next to
that of PP-arguments (Schulte im Walde, 2006a;
Joanis et al., 2008), and in addition the parser of-
ten distinguishes poorly between PP-modifiers and
PP-arguments.
In order to create an idealized artificial upper
bound, we also created a set of idealized ?lexico-
graphic? descriptions in the form of manually in-
stantiated feature vectors and feature values, us-
ing the manually selected feature configuration
we just described (and ultimately based on the
gold standard description represented by table 2).
These idealized vectors were also used for clus-
tering experiments in order to estimate an upper
bound.
4.4 Clustering Methods
For the clustering experiments we used two dif-
ferent clustering algorithms: K-means and La-
tent Semantic Classes (LSC). K-means is a stan-
dard flat, hard-clustering algorithm; we used the
Weka implementation (Witten and Frank, 2005).
LSC (Rooth, 1998; Rooth et al., 1999) is a
two-dimensional soft-clustering algorithm which
learns three probability distributions: one for the
clusters, and one for the output probabilities of
each element and for each feature type with regard
to a cluster. The latter two (elements and features)
correspond to the two dimensions of the cluster-
ing. In our case the elements are the PV-BV pairs,
and the features are normalized counts of the sub-
categorization frames.
4.5 Evaluation
Our feature vectors are a combination of the fea-
ture vector for the BV and the feature vector for
the PV of each PV-BV pair. Since the length of
each vector depends on the base frequency of each
verb we need to apply a feature normalization: we
simply reduce each feature to its unit vector of
length 1. Because the frequency ratio between BV
and PV may vary strongly, we need to normalize
PV vectors and BV vectors separately before they
can be combined.
The vector combination for each PV-BV pair is
done by simply adding the dimensions (and not the
186
an auf an+auf
Inter-annotator agreement 0.79 0.64 0.70
Average agreement between 0.73 0.74 0.73
annotators and gold standard
Table 1: Inter-annotator agreement and comparison of the gold standard to the ratings of 6 human anno-
tators (Fleiss? Kappa Scores).
Particle Typical frames Typical frames Semantic Verbs in Class
for the BV for the PV Class
an
NPnom
+NPacc
+PP-an
NPnom
+NPacc
+PP-an
locative/
relational
tying
an|binden to tie at
an|ketten to chain at
NPnom
+PP-zu/in/
nach/auf
NPnom
+NPacc
locative/
relational
gaze
an|blicken to glance at
an|gucken to look at
an|starren to stare at
NPnom
+NPacc
+PP-mit
NPnom
+NPacc
+PP-mit
ingressive
consump-
tion
an|brechen start to break
an|rei?en start to tear
an|schneiden start to cut
NPnom
NPnom
+NPacc
locative/
relational
sound
an|br?ullen to roar at
an|fauchen to hiss at
an|meckern to bleat at
NPnom
+NPacc
+PP-an
NPnom
+NPacc
locative/
relational
fixation
an|heften to stick at
an|kleben to glue at
an|schrauben to screw at
auf
NPnom NPnom
locative
blaze-
bubble
auf|brodeln to bubble up
auf|flammen to light up
auf|lodern to blaze up
auf|spudeln to bubble up
NPnom
+PP-zu/in/
nach/auf
NPnom
locative
gaze
auf|blicken to glance up
auf|schauen to look up
auf|sehen to look up
NPnom
+NPacc
NPnom
+NPacc
locative/
dimensional
instigate
auf|hetzen to instigate
auf|scheuchen to rouse
NPnom
+NPacc
+PP-auf
NPnom
+NPacc
locative/
relational
fixation
auf|heften to staple on
auf|kleben to glue on
auf|pressen to press on
NPnom NPnom
ingressive
sound
auf|br?ullen suddenly roar
auf|heulen suddenly howl
auf|klingen suddenly sound
auf|kreischen suddenly scream
auf|schluchzen suddenly sob
auf|st?ohnen suddenly moan
Table 2: The gold standard classes for the experiments, with subcategorization patterns.
187
an auf an+auf
Purity RI ARI Purity RI ARI Purity RI ARI
Human 0.93 0.92 0.92
ratings
K-means
idealized features 0.83 0.91 0.70 0.88 0.92 0.72 0.93 0.97 8.2
(manually set)
selected features 0.67 0.82 0.29 0.75 0.87 0.52 0.46 0.88 0.32
(extracted)
20 feat 0.58 0.74 0.18 0.69 0.69 0.40 0.43 0.88 0.14
50 feat 0.67 0.80 0.20 0.75 0.83 0.38 0.43 0.90 0.19
100 feat 0.67 0.79 0.18 0.75 0.83 0.40 0.49 0.90 0.21
200 feat 0.58 0.74 0.13 0.81 0.86 0.52 0.43 0.88 0.18
LSC selected features 0.63 0.78 0.22 0.80 0.85 0.55 0.85 0.92 0.59
(extracted)
Cutoff: 0.1
Table 3: Comparison of the results from different clustering methods and feature configurations.
dimension extensions) of the two vectors. In this
way, each subcategorization frame is represented
separately for the BV and the PV. For example,
the vectors for the intransitive frame will be repre-
sented as BV:intransitive and PV:intransitive.
We evaluated the clusterings in terms of Pu-
rity (Manning et al., 2008), Rand Index and Ad-
justed Rand Index (Rand, 1971; Hubert and Ara-
bie, 1985). Purity is a measure with values be-
tween 0 and 1 which captures the purity of indi-
vidual clusters in terms of the ratio between the
number of elements of the majority class in each
cluster and the total of elements in the cluster. A
perfect clustering will have a purity of 1. What Pu-
rity does not capture is the amount of clusters over
which each target class is distributed. That means
that also non-perfect clusters may achieve a Purity
of 1 if there are more clusters than target classes.
As long as the number of clusters is constant, how-
ever, purity is a good and intuitive approximation
to clustering evaluation.
The Rand Index (RI) looks at pairs of ele-
ments and assesses whether they have been cor-
rectly placed in the same cluster (which is correct
if they pertain to the same target class) or in dif-
ferent clusters (correct if they belong to different
target classes). RI is sensitive to the number of
non-empty clusters and can capture both the qual-
ity of individual clusters and the amount to which
elements of target categories have been grouped
together. RI looks as pair-wise decisions, which
makes it also applicable to the human ratings de-
scribed in section 4.1. The Adjusted Rand Index
(ARI) is a version of RI which is corrected for
chance. While RI has values between 0 and 1, ARI
can have negative values; 1 still represents a per-
fect clustering.
The Adjusted Rand Index (ARI) is a version of
RI which is corrected for chance. While RI has
values between 0 and 1, ARI can have negative
values; 1 still represents a perfect clustering.
We evaluated the clustering of the verbs with the
particles an and auf separately from each other,
since we have to expect that there is a different set
of semantic classes for each verb particle. We also
ran the same experiments for the gold standard as
a whole (an+auf ), in order to test if we could find
some tendencies across clusters.
We set the number of clusters equal to the num-
ber of target categories from the gold standard.
This gave us 5 clusters for both the an-set and the
auf -set and 10 clusters for the classification of the
whole gold standard.
Note that LSC is a soft clustering algorithm. For
the evaluation of LSC clusters with respect to pu-
rity and RI and ARI, a conversion to hard clus-
tering must be done. We did this conversion by
simply applying a cutoff value for the output prob-
abilities for cluster membership. We tried out var-
ious cut-off levels and found that for the sets of an
and auf PVs the value of 0.1 gave a good trade-off
between coverage (the total number of elements
retained in all clusters) and ARI (cf also Table 4
below). This value is also the one used in K?uhner
and Schulte im Walde (2010).
188
5 Results
The comparison of the results from different meth-
ods can be seen in table 3. The strongest automati-
cally obtained results are printed in bold face. The
human rating scores are given in the first row and
allow for a direct comparison between automatic
clustering and human decisions.
6
The second row
shows the artificial upper bound represented by the
manually set feature vectors as lexicographic en-
tries. Note that this is an artificial upper bound
and not an experimental result, even if obtained
by clustering.
The third row corresponds to the evaluation re-
sults for the manually selected corpus-based fea-
ture configuration used within K-means. They are
to be compared with the following rows concern-
ing the results based on automatically selected n
most frequent features. The last row shows the
results obtained with the LSC soft clustering al-
gorithm, applying a cutoff of 0.1 output probabil-
ity for cluster membership, again for the manu-
ally selected feature configuration. This result is
not fully comparable to the rows above, which are
obained with K-means or human ratings. Since
LSC is a soft clustering algorithm, there is a trade-
off between coverage and accuracy which depends
on the cutoff point selected for the conversion into
hard clusters.
Note that the Purity values are comparable
among each other since the number of clusters was
held constant. We always chose a number of clus-
ters equal to the number of target categories (5 cat-
egories for an, 5 for auf and 10 for an+auf ).
Table 4 shows the results for LSC clustering
in more detail. The soft clusterings have to be
converted to hard clusterings. Because of this
the cut-off point within the conversion becomes
an important parameter. We chose here cut-off
points which correspond to the output probabil-
ity of cluster-elements (e.g. PV-BV pairs) with
regard to each cluster. The table shows a clear ten-
dency towards better ARI scores when higher cut-
off points are chosen. But this is counterbalanced
by the fact that for higher cutoff points less ele-
ments are retained. Below a certain cutoff-point
the total number of elements retaind is smaller
6
RI is a measure which is based on pair-wise clustering
decisions, we were able to calculate these scores for the hu-
man ratings described in section 4.1. Since purity is not based
on a pair-wise decision, it was not applicable to the human
ratings. For the same reason ARI was also not adaptable to
the human rating scenario.
than the target set of verbs in the gold standard.
6 Discussion
It is not surprising that the manually defined fea-
ture configuration in our ?lexicographic? setting
perform best. These results are also similar to
those obtained by the human validation of the gold
standard. They do not get perfect scores of 1 be-
cause of small lexicographic differences concern-
ing individual entries. The automatic clustering
results relying on corpus-based features are worse,
as expected, but they still represent a very strong
tendency to group together PV-BV pairs into se-
mantic classes. We can achieve relatively high pu-
rity scores, thus demonstrating that our approach
is generally valid.
Concerning the feature selection for the corpus-
based data, the manually selected set seems to per-
form slightly better than the automatic feature se-
lection settings. Moreover, the manual selection
represents a more stable setting since automatic
selection seems to vary with the number n of fea-
tures. There appears to be no optimal setting for n
which gives the best results for all sets. For the an
set the local maximum is reached with the selec-
tion of the 50 or 100 most frequent subcat frames.
The selection of more or less features leads to
worse evaluation scores. For the auf set this lo-
cal maximum is reach with much higher values for
n. The manually created feature set, on the other
hand, always results in a relatively good perfor-
mance. This is also an expected result since the
feature selection already contains human linguis-
tic knowledge on which syntactic arguments rep-
resent the core set of the semantic roles which the
verbs can realize.
It is apparently surprising that for the joint gold
standard set an+auf LSC performs much better
than K-means. But this high ARI value comes at
the cost of a very low coverage. If we compare
this value to table 4, it can be seen that the cutoff
point of 0.1, which works very well for sets of an
and auf is inadequate for the set an+auf : only 20
verbs are retained in the converted clusters while
the target size is 32. While we can observe the
general tendency of LSC to perform on a roughly
comparable level to K-means, an exact compari-
son is hard to obtain with the used evaluation met-
rics. There are, nevertheless, possible problem set-
tings where soft clusters are more adequate, which
justifies to include LSC in this comparison.
189
an auf an+auf
Cutoff ARI n
clust
ARI n
clust
ARI n
clust
0.07 0.17 25 0.39 22 0.31 40
0.08 0.18 23 0.55 20 0.39 32
0.09 0.19 21 0.55 20 0.56 23
0.10 0.22 19 0.55 20 0.59 20
0.11 0.30 16 0.5 19 0.48 17
0.12 0.30 16 0.41 16 0.56 16
n
classes
14 18 32
Table 4: Evaluation with LSC using extracted selected features for different cutoff points (probabilities
of class membership) when creating hard clusters from soft clusters. (n
classes
refers to the number of
elements across target classes, n
clust
refers to the number of elements across hard clusters.)
The class of anketten/anbinden tends to end up
in singleton clusters, especially anketten. We first
suspected that this is due to the fact that anket-
ten is a relatively infrequent verb and is repre-
sented by a sparse vector. But a comparison to
the human ratings reveals that human raters show
a similar and quite consistent disagreement with
the gold standad with respect to this the locative
relational tying and fixation classes. All 6 raters
judged anheften (a fixation verb) and anbinden
(a tying verb) as pertaining to the same category,
contrary to the gold standard. Interestingly, this
fixation-tying distinction is the only one, where
a majority of raters deviated in their judgements
from the gold standard at the same point. On the
other hand some of the raters were confused by the
fact the class of aufbrodeln combines two different
elements: water and fire. This did not affect the
majority of raters, nor was the disagreement con-
sistent, but it is reflected in the somewhat lower
inter-annotator agreement for the auf set (cf. table
1). These findings strongly suggest that the prob-
lem should be located in the gold standard rather
than in the clustering method.
Finally, is interesting to compare the automatic
clustering results to the human ratings from sec-
tion 4.1. The human annotation task was com-
plementary to the automatic clustering because
clustering was done on the basis of corpus-based
purely syntactic features while for the human rat-
ing the annotators focused on purely semantic in-
formation. Apart from the expectably worse per-
formance of an automatic clustering it can be con-
cluded that both information from the semantic
and the syntactic perspectives ultimately lead to
the creation of quite similar clusters, which is
probably the most important conclusion we can
draw from the experiment.
7 Conclusion
In this paper we have shown that a pairwise clus-
tering of particle verbs in combination with their
base verbs can be done with success if syntac-
tic subcategorization frames for PVs and BVs are
taken as features separately. By combining the ex-
tracted subcategorization frame count from base
verbs and particle verbs as separate dimensions
in a common vector space, we are able to model
syntactic transfer patterns. We can also show that
within our setting we are able to replicate a gold
standard classification with a reasonable degree of
success when we apply various clustering algo-
rithms. The gold standard by itself can be vali-
dated by human judgements to a high degree. Hu-
man judges based their annotations on semantic
factors and still they converge largely with an au-
tomatic clustering which is purely based on syn-
tactic subcategorization.
In future work we plan to adress the problem
of finding correspondences between the syntactic
subcategorization slots, hence model the syntactic
transfer proper, and to investigate if the syntactic
transfer information can be used to predict the de-
gree of semantic compositionality of PVs.
Acknowledgements
This work was funded by the DFG Research
Project ?Distributional Approaches to Semantic
Relatedness? (Stefan Bott, Sabine Schulte im
Walde), and the DFG Heisenberg Fellowship
SCHU-2580/1-1 (Sabine Schulte im Walde). We
would also like to thank the participants of the hu-
man rating experiment.
190
References
Nadine Aldinger. 2004. Towards a Dynamic Lexi-
con: Predicting the Syntactic Argument Structure
of Complex Verbs. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, Lisbon, Portugal.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the Unextractable: A Case Study on Verb
Particles. In Proceedings of the Sixth Conference on
Computational Natural Language Learning, pages
98?104, Taipei, Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An Empirical Model
of Multiword Expression Decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin. 2005. Deep Lexical Acquisition of
Verb?Particle Constructions. Computer Speech and
Language, 19:398?414.
Collin Bannard. 2005. Learning about the Meaning of
Verb?Particle Constructions from Corpora. Com-
puter Speech and Language, 19:467?478.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC ?
a Corpus of Parsable Sentences from the Web. In
Proceedings of the International Conference of the
German Society for Computational Linguistics and
Language Technology, Darmstadt, Germany. To ap-
pear.
Wolfgang Fleischer and Irmhild Barz. 2012a. Wort-
bildung der deutschen Gegenwartssprache. de
Gruyter.
Wolfgang Fleischer and Irmhild Barz. 2012b. Wortbil-
dung der deutschen Gegenwartssprache. Walter de
Gruyter, 4th edition.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Silvana Hartmann. 2008. Einfluss syntaktischer und
semantischer Subkategorisierung auf die Komposi-
tionalit?at von Partikelverben. Studienarbeit. Insti-
tut f?ur Maschinelle Sprachverarbeitung, Universit?at
Stuttgart. Supervision: Sabine Schulte im Walde and
Hans Kamp.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing Partitions. Journal of Classification, 2:193?218.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A General Feature Space for Automatic
Verb Classification. Natural Language Engineer-
ing, 14(3):337?367.
Hans Kamp and Uwe Reyle. 1993. From discourse to
logic: Introduction to modeltheoretic semantics of
natural language, formal logic and discourse repre-
sentation theory. Number 42. Springer.
Fritz Kliche. 2011. Semantic Variants of German Par-
ticle Verbs with ?ab?. Leuvense Bijdragen, 97:3?
27.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering Polysemic Subcategorization
Frame Distributions Semantically. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 64?71, Sapporo,
Japan.
Natalie K?uhner and Sabine Schulte im Walde. 2010.
Determining the Degree of Compositionality of Ger-
man Particle Verbs by Clustering Approaches. In
Proceedings of the 10th Conference on Natural Lan-
guage Processing, pages 47?56, Saarbr?ucken, Ger-
many.
Andrea Lechler and Antje Ro?deutscher. 2009. Ger-
man Particle Verbs with auf. Reconstructing their
Composition in a DRT-based Framework. Linguis-
tische Berichte, 220.
Anke L?udeling. 2001. On German Particle Verbs and
Similar Constructions in German. Dissertations in
Linguistics. CSLI Publications, Stanford, CA.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a Continuum of Compositionality in
Phrasal Verbs. In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions: Analysis, Ac-
quisition and Treatment, Sapporo, Japan.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic Verb Classification Based on Statistical Distri-
butions of Argument Structure. Computational Lin-
guistics, 27(3):373?408.
William M. Rand. 1971. Objective Criteria for the
Evaluation of Clustering Methods. Journal of the
American Statistical Association, 66(336):846?850.
Mats Rooth, Stefan Riezler, Detlef Prescher,
Glenn Carr oll, and Franz Beil. 1999. Inducing
a Semantically Annotated Lexicon via EM-Based
Clustering. In Proceedings of the 37th Annual
Meeting of the Association for Co mputational
Linguistics, Maryland, MD.
Mats Rooth. 1998. Two-Dimensional Clusters
in Grammatical Relations. In Inducing Lexicons
with the EM Algorithm, AIMS Report 4(3). Insti-
tut f?ur Maschinelle Sprachverarbeitung, Universit?at
Stuttgart.
Sabine Schulte im Walde. 2000. Clustering Verbs
Semantically According to their Alternation Be-
haviour. In Proceedings of the 18th International
Conference on Computational Linguistics, pages
747?753, Saarbr?ucken, Germany.
191
Sabine Schulte im Walde. 2004. Identification, Quan-
titative Description, and Preliminary Distributional
Analysis of German Particle Verbs. In Proceedings
of the COLING Workshop on Enhancing and Us-
ing Electronic Dictionaries, pages 85?88, Geneva,
Switzerland.
Sabine Schulte im Walde. 2005. Exploring Features
to Identify Semantic Nearest Neighbours: A Case
Study on German Particle Verbs. In Proceedings
of the International Conference on Recent Advances
in Natural Language Processing, pages 608?614,
Borovets, Bulgaria.
Sabine Schulte im Walde. 2006a. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Computational Linguistics, 32(2):159?
194.
Sabine Schulte im Walde. 2006b. The Syntax-
Semantics Interface of German Particle Verbs.
Panel discussion at the 3rd ACL-SIGSEM Work-
shop on Prepositions at the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Sylvia Springorum, Sabine Schulte im Walde, and An-
tje Ro?deutscher. 2012. Automatic Classification of
German an Particle Verbs. In Proceedings of the 8th
International Conference on Language Resources
and Evaluation, pages 73?80, Istanbul, Turkey.
Sylvia Springorum, Sabine Schulte im Walde, and An-
tje Ro?deutscher. 2013. Sentence Generation and
Compositionality of Systematic Neologisms of Ger-
man Particle Verbs. Talk at the 5th Conference on
Quantitative Investigations in Theoretical Linguis-
tics.
Sylvia Springorum. 2011. DRT-based Analysis of the
German Verb Particle ?an?. Leuvense Bijdragen,
97:80?105.
Barbara Stiebels. 1996. Lexikalische Argumente und
Adjunkte. Zum semantischen Beitrag von verbalen
Pr?afixen und Partikeln. Akademie Verlag, Berlin.
Aline Villavicencio. 2005. The Availability of Verb-
Particle Constructions in Lexical Resources: How
much is enough? Computer Speech & Language,
19(4):415?432.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
wi th Java Implementations. Morgan Kaufmann.
192
Workshop on Monolingual Text-To-Text Generation, pages 20?26,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 20?26,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
An Unsupervised Alignment Algorithm for Text Simplification Corpus
Construction
Stefan Bott
TALN Research Group
Universitat Pompeu Fabra
C/Tanger 122 - Barcelona - 08018
Spain
stefan.bott@upf.edu
Horacio Saggion
TALN Research Group
Universitat Pompeu Fabra
C/Tanger 122 - Barcelona - 08018
Spain
horacio.saggion@upf.edu
Abstract
We present a method for the sentence-level
alignment of short simplified text to the orig-
inal text from which they were adapted. Our
goal is to align a medium-sized corpus of par-
allel text, consisting of short news texts in
Spanish with their simplified counterpart. No
training data is available for this task, so we
have to rely on unsupervised learning. In con-
trast to bilingual sentence alignment, in this
task we can exploit the fact that the probability
of sentence correspondence can be estimated
from lexical similarity between sentences. We
show that the algoithm employed performs
better than a baseline which approaches the
problem with a TF*IDF sentence similarity
metric. The alignment algorithm is being used
for the creation of a corpus for the study of
text simplification in the Spanish language.
1 Introduction
Text simplification is the process of transforming a
text into an equivalent which is more understand-
able for a target user. This simplification is bene-
ficial for many groups of readers, such as language
learners, elderly persons and people with other spe-
cial reading and comprehension necessities. Simpli-
fied texts are characterized by a simple and direct
style and a smaller vocabulary which substitutes in-
frequent and otherwise difficult words (such as long
composite nouns, technical terms, neologisms and
abstract concepts) by simpler corresponding expres-
sions. Usually unnecessary details are omitted. An-
other characteristic trait of simplified texts is that
usually only one main idea is expressed by a single
sentence. This also means that in the simplification
process complex sentences are often split into sev-
eral smaller sentences.
The availability of a sentence-aligned corpus
of original texts and their simplifications is of
paramount importance for the study of simplifica-
tion and for developing an automatic text simplifi-
cation system. The different strategies that human
editors employ to simplify texts are varied and have
the effect that individual parts of the resulting text
may either become shorter or longer than the orig-
inal text. An editor may, for example, delete de-
tailed information, making the text shorter. Or she
may split complex sentences into various smaller
sentences. As a result, simplified texts tend to be-
come shorter than the source, but often the number
of sentences increases. Not all of the information
presented in the original needs to be preserved but in
general all of the information in the simplified text
stems from the source text.
The need to align parallel texts arises from a larger
need to create a medium size corpus which will al-
low the study of the editing process of simplifying
text, as well as to serve as a gold standard to evalu-
ate a text simplification system.
Sentence alignment for simplified texts is related
to, but different from, the alignment of bilingual text
and also from the alignment of summaries to an orig-
inal text. Since the alignment of simplified sentences
is a case of monolingual alignment the lexical sim-
ilarity between two corresponding sentences can be
taken as an indicator of correspondence.
This paper is organized as follows: Section 2
briefly introduces text simplification which contex-
20
tualises this piece of research and Section 3 dis-
cusses some related work. In Section 4 we briefly
describe the texts we are working with and in Sec-
tion 5 we present the alignment algorithm. Section 6
presents the details of the experiment and its results.
Finally, section 7 gives a concluding discussion and
an outlook on future work.
2 Text Simplification
The simplification of written documents by humans
has the objective of making texts more accessible to
people with a linguistic handicap, however manual
simplification of written documents is very expen-
sive. If one considers people who cannot read doc-
uments with heavy information load or documents
from authorities or governmental sources the percent
of need for simplification is estimated at around 25%
of the population, it is therefore of great importance
to develop methods and tools to tackle this problem.
Automatic text simplification, the task of transform-
ing a given text into an ?equivalent? which is less
complex in vocabulary and form, aims at reducing
the efforts and costs associated with human simpli-
fication. In addition to transforming texts into their
simplification for human consumption, text simpli-
fication has other advantages since simpler texts can
be processed more efficiently by different natural
language processing processors such as parsers and
used in applications such as machine translation, in-
formation extraction, question answering, and text
summarization.
Early attempts to text simplification were based
on rule-based methods where rules were designed
following linguistic intuitions (Chandrasekar et al,
1996). Steps in the process included linguistic text
analysis (including parsing) and pattern matching
and transformation steps. Other computational mod-
els of text simplification included processes of anal-
ysis, transformation, and phrase re-generation (Sid-
dharthan, 2002) also using rule-based techniques.
In the PSET project (Carroll et al, 1998) the pro-
posal is for a news simplification system for aphasic
readers and particular attention is paid to linguistic
phenomena such as passive constructions and coref-
erence which are difficult to deal with by people
with disabilities. The PorSimples project (Alu??sio et
al., 2008) has looked into simplification of the Por-
tuguese language. The methodology consisted in the
creation of a corpus of simplification at two different
levels and on the use of the corpus to train a deci-
sion procedure for simplification based on linguistic
features. Simplification decisions about whether to
simplify a text or sentence have been studied fol-
lowing rule-based paradigms (Chandrasekar et al,
1996) or trainable systems (Petersen and Ostendorf,
2007) where a corpus of texts and their simplifica-
tions becomes necessary. Some resources are avail-
able for the English language such as parallel cor-
pora created or studied in various projects (Barzilay
and Elhadad, 2003; Feng et al, 2009; Petersen and
Ostendorf, 2007; Quirk et al, 2004); however there
is no parallel Spanish corpus available for research
into text simplification. The algorithms to be pre-
sented here will be used to create such resource.
3 Related Work
The problem of sentence alignment was first tack-
led in the context of statistical machine translation.
Gale and Church (1993) proposed a dynamic pro-
gramming algorithm for the sentence-level align-
ment of translations that exploited two facts: the
length of translated sentences roughly corresponds
to the length of the original sentences and the se-
quence of sentences in translated text largely corre-
sponds to the original order of sentences. With this
simple approach they reached a high degree of accu-
racy.
Within the field of monolingual sentence align-
ment a large part of the work has concentrated on the
alignment between text summaries and the source
texts they summarize. Jing (2002) present an al-
gorithm which aligns strings of words to pieces of
text in an original document using a Hidden Markov
Model. This approach is very specific to summary
texts, concretely such summaries which have been
produced by a ?cut and paste? process. A work
which is more closely related to our task is pre-
sented in Barzilay and Elhadad (2003). They carried
out an experiment on two different versions of the
Encyclopedia Britannica (the regular version and
the Britannica Elementary) and aligned sentences
in a four-step procedure: They clustered paragraphs
into ?topic? groups, then they trained a binary clas-
sifier (aligned or not aligned) for paragraph pairs
21
on a handcrafted set of sentence alignments. Af-
ter that they grouped all paragraphs of unseen text
pairs into the same topic clusters as in the first step
and aligned the texts on the paragraph level, al-
lowing for multiple matches. Finally they aligned
the sentences within the already aligned paragraphs.
Their similarity measure, both for paragraphs and
sentences, was based on cosine distance of word
overlap. Nelken and Shieber (2006) improve over
Barzilay and Elhadad?s work: They use the same
data set, but they base their similarity measure for
aligning sentences on a TF*IDF score. Although
this score can be obtained without any training, they
apply logistic regression on these scores and train
two parameters of this regression model on the train-
ing data. Both of these approaches can be tuned by
parameter settings, which results in a trade-off be-
tween precision and recall. Barzilay and Elhadad
report a precision of 76.9% when the recall reaches
55.8%. Nelken and Shieber raise this value to 83.1%
with the same recall level and show that TF*IDF is
a much better sentence similarity measure. Zhu et
al. (2010) even report a precision of 91.3% (at the
same fixed recall value of 55.8%) for the alignment
of simple English Wikipedia articles to the English
Wikipedia counterparts using Nelken and Shieber?s
TF*IDF score, but their alignment was part of a
larger problem setting and they do not discuss fur-
ther details.
We consider that our task is not directly compara-
ble to this previous work: the texts we are working
with are direct simplifications of the source texts. So
we can assume that all information in the simplified
text must stem from the original text. In addition we
can make the simplifying assumption that there are
one-to-many, but no many-to-one relations between
source sentences and simplified sentences, a simpli-
fication which largely holds for our corpus. This
means that all target sentences must find at least one
alignment to a source sentence, but not vice versa.
Nelken and Shieber make the interesting observa-
tion that dynamic programming, as used by Gale
and Church (1991) fails to work in the monolingual
case. Their test data consisted of pairs of encyclo-
pedia articles which presented a large intersection
of factual information, but which was not necessar-
ily presented in the same order. The corpus we are
working with, however, largely preserves the order
in which information is presented.
4 Dataset
We are working with a corpus of 200 news arti-
cles in Spanish covering the following topics: Na-
tional News, Society, International News and Cul-
ture. Each of the texts is being adapted by the DILES
Research Group from Universidad Auto?noma de
Madrid (Anula, 2007). Original and adapted ex-
amples of texts in Spanish can be seen in Figure 1
(the texts are adaptations carried out by DILES for
Revista ?La Plaza?). The texts are being processed
using part-of-speech tagging, named entity recogni-
tion, and parsing in order to create an automatically
annotated corpus. The bi-texts are first aligned us-
ing the tools to be described in this paper and then
post-edited with the help of a bi-text editor provided
in the GATE framework (Cunningham et al, 2002).
Figure 2 shows the texts in the alignment editor.
This tool is however insufficient for our purposes
since it does not provide mechanisms for uploading
the alignments produced outside the GATE frame-
work and for producing stand-alone versions of the
bi-texts; we have therefore extended the functionali-
ties of the tool for the purpose of corpus creation.
5 Algorithm
Our algorithm is based on two intuitions about sim-
plified texts (as found in our corpus): As repeatedly
observed sentences in simplified texts use similar
words to those in the original sentences that they
stem from (even if some of the words may have
undergone lexical simplification). The second ob-
servation is very specific to our data: the order in
which information is presented in simplified texts
roughly corresponds to the order of the information
in the source text. So sentences which are close to
each other in simplified texts correspond to original
sentences which are also close to each other in the
source text. In many cases, two adjacent simplified
sentences even correspond to one single sentence in
the source text. This leads us to apply a simple Hid-
den Markov Model which allows for a sequential
classification.
Firstly, we define an alignment as a pair of sen-
tences as
?source senti, target sentj?,
22
Original Text Adapted Text
Un Plan Global desde tu hogar
El Programa GAP (Global Action Plan) es una iniciativa
que se desarrolla en distintos pa??ses y que pretende dis-
minuir las emisiones de CO2, principales causantes del
cambio clima?tico y avanzar hacia ha?bitos ma?s sostenibles
en aspectos como el consumo de agua y energ??a, la
movilidad o la gestio?n de los residuos dome?sticos.
San Sebastia?n de los Reyes se ha adherido a este Pro-
grama.
Toda la informacio?n disponible para actuar desde el
hogar en la construccio?n de un mundo ma?s sostenible se
puede encontrar en ssreyes.org o programagap.es.
Un Plan Global desde tu hogar
San Sebastia?n de los Reyes se ha unido al Plan de Accio?n
Global (GAP).
El Plan es una iniciativa para luchar contra el cambio
clima?tico desde tu casa.
Los objetivos del Plan son:
Disminuir nuestros gastos dome?sticos de agua y energ??a.
Reducir los efectos dan?inos que producimos en el planeta
con nuestros residuos.
Mejorar la calidad de vida de nuestra ciudad.
Tienes ma?s informacio?n en ssreyes.org y en programa-
gap.es.
Apu?ntate al programa GAP y desca?rgate los manuales
con las propuestas para conservar el planeta.
Figure 1: Original Full Document and Easy-to-Read Version
Figure 2: The Alignment Editor with Text and Adaptation
where a target sentence belongs to the simplified
text and the source sentence belongs to the original
sentence. Applying standard Bayesian decomposi-
tion, the probability of an alignment to a given target
text can be calculated as follows:
P (alignn1 |target sent
m
1 ) =
P (alignn1 )P (target sent
m
1 |align
n
1 )
P (target sentm1 )
Since P (target sentm1 ) is constant we can calcu-
late the most probable alignment sequence a?lign as
follows:
a?lign =
arg maxP (alignn1 ) P (target sent
m
1 |align
n
1 ) =
arg max
? n
i=1P (aligni,j)P (target sentj |aligni,j)
This leaves us with two measures: a measure
of sentence similarity (the probability of alignment
proper) and a measure of consistency, under the as-
sumption that a consistent simplified text presents
the information in the same order as it is presented
in the source text. In order to determine a?lign, we
apply the Viterbi algorithm (Viterbi, 1967).
Sentence similarity can be calculated as follows:
P (wordl1|target sentj) =
? l
k=1
P (target sentj |wordk)P (target sentj)
P (wordk)
where wordl1 is the sequence of words in the
source sentence i and l is the length of sentence i.
This similarity measure is different from both
word overlap cosine distance and TF*IDF. It is,
however, similar to TF*IDF in that it penalizes
23
words which are frequent in the source text and
boosts the score for less frequent words. In addi-
tion we eliminated a short list of stopwords from the
calculation, but this has no significant effect on the
general performance.
Note that P (wordk) may correspond to a MLE
of 0 since simplified texts often use different (and
simpler) words and add connectors, conjunctions
and the like. For this reason we have to recalcu-
late P (wordk) according to a distortion probability
?. Distortion is taken here as the process of word
insertion or lexical changes. ? is a small constant,
which could be determined empirically, but since no
training data is available we estimated ? for our ex-
periment and set it by hand to a value of 0.0075.
Even if we had to estimate ? we found that the per-
formance of the system is robust regarding its value:
even for unrealistic values like 0.0001 and 0.1 the
performance only drops by two percent points.
P (wordk|distortion) =
(1 ? ?)P (wordk) + ?(1 ? P (wordk))
For the consistency measure we made the
Markov assumption that each alignment aligni,j
only depends on the proceeding alignment
aligni?1,j? . We assume that this is the proba-
bility of a distance d between the corresponding
sentences of source senti?1 and source senti, i.e.
P (source senti|aligni?1,j?k) for each possible
jump distance k. Since long jumps are relatively
rare, we used a normalized even probability dis-
tribution for all jump lengths greater than 2 and
smaller than -1.
Since we have no training data, we have to ini-
tially set these probabilities by hand. We do this
by assuming that all jump distances k in the range
between -1 and 2 are distributed evenly and larger
jump distances have an accumulated probability
mass corresponding to one of the local jump dis-
tances. Although this model for sentence transitions
is apparently oversimplistic and gives a very bad es-
timate for each P (source senti|aligni?1,j?k), the
probabilities for P (alignn1 ) give a counterweight to
these bad estimates. What we can expect is, that af-
ter running the aligner once, using very unreliable
transitions probability estimates, the output of the
aligner is a set of alignments with an implicit align-
ment sequence. Taking this alignment sequence, we
can calculate newmaximum likelihood estimates for
each jump distance P (source senti|aligni?1,j?k)
again, and we can expect that these new estimates
are much better than the original ones.
For this reason we apply the Viterbi classifier it-
eratively: The first iteration employs the hand set
values. Then we run the classifier and determine
the values for P (source senti|aligni?1,j?k) on its
output. Then we run the classifier again, with the
new model and so on. Interestingly values for
P (source senti|aligni?1,j?k) emerge after as little
as two iterations. After the first iteration, precision
already lies only 1.2 percent points and recall 1.3
points below the stable values. We will comment on
this finding in Section 7.
6 Experiment and Results
Our goal is to align a larger corpus of Spanish short
news texts with their simplified counterparts. At the
moment, however, we only have a small sample of
this corpus available. The size of this corpus sam-
ple is 1840 words of simplified text (145 sentences)
which correspond to 2456 (110 sentences) of source
text. We manually created a gold standard which in-
cludes all the correct alignments between simplified
and source sentences. The results of the classifier
were calculated against this gold standard.
As a baseline we used a TF*IDF score based
method which chooses for each sentence in the sim-
plified text the sentence with the minimal word vec-
tor distance. The procedure is as follows: each sen-
tence in the original and simplified document is rep-
resented in the vector space model using a term vec-
tor (Saggion, 2008). Each term (e.g. token) is wei-
thed using as TF the frequency of the term in the
document and IDF = log(N +1/Mt +1) where Mt
is the number of sentences 1 containing t and N is
the number of sentences in the corpus (counts are
obtained from the set of documents to align). As
similarity metric between vectors we use the cosine
of the angle between the two vectors given in the
following formula:
1The relevant unit for the calculation of IDF (the D in IDF)
here is the sentence, not the document as in information re-
trieval.
24
cosine(s1, s2) =
?n
i=1 wi,s1 ? wi,s2??n
i=1(wi,s1)
2 ?
??n
i=1(wi,s2)
2
Here s1 and s2 are the sentence vectors and wi,sk
is the weight of term i in sentence sk. We align all
simplified sentences (i.e. for the time being no cut-
off has been used to identify newmaterial in the sim-
plified text).
For the calculation of the first baseline we calcu-
late IDF over the sentences in whole corpus. Nelken
and Shieber (2006) argue that that the relevant unit
for this calculation should be each document for the
following reason: Some words are much more fre-
quent in some texts than they are in others. For ex-
ample the word unicorn is relatively infrequent in
English and it it may also be infrequent in a given
colletion of texts. So this word is highly discrimina-
tive and it?s IDF will be relatively high. In a specific
text about imagenary creatures, however, the same
word unicornmay be much more frequent and hence
it?s discrimiative power is much lower. For this rea-
son we calcuated a second baseline, where we cal-
culate the IDF only on the sentences of the relevanct
texts.
Results of aligning all sentences in our sample
corpus using both the baseline and the HMM algo-
rithms are given in Table 6.
precision recall
HMM aligner 82.4% 80.9%
alignment only 81.13% 79.63%
TF*IDF + transitions 76.1% 73.5%
TF*IDF (document) 75.47% 74.07%
TF*IDF (full corpus) 62.2% 61.1%
If we compare these results to those presented by
Nelken and Shieber (2006), we can observe that we
obtain a comparable precision, but the recall im-
proves dramatically from 55.8% (with their specific
feature setting) to 82.4%. Our TF*IDF baselines
are not directly comparable comparable to Nelken
and Shieber?s results. The reason why we can-
not compare our results directly is that Nelken and
Shieber use supervised learning in order to optimize
the transformation of TF*IDF scores into probabili-
ties and we had no training data available.
We included the additional scores for our system,
when no transition probabilities are included in the
calculation of the optimal alignment sequence and
the score comes only from the probabilies of our
clalculation of lexical similarity between sentences
(alignment only). These scores show that a large part
of the good performance comes from lexical similar-
ity and sequencial classification only give an addi-
tional final boost, a fact which was already observed
by Nelken and Shieber. We also attribute the fact
that the system alrives at stable values after two it-
erations to the same efect: lexical similarity seems
to have a much bigger effect on the general perfor-
mance. Still our probability-based similarity meas-
sure clearly outperforms the TF*IDF baselines.
7 Discussion and Outlook
We have argued above that our task is not directly
comparable to Nelken and Shieber?s alignment of
two versions of Encyclopedia articles. First of all,
the texts we are working with are simplified texts in
a much stricter sense: they are the result of an edit-
ing process which turns a source text into a simpli-
fied version. This allows us to use sequential classi-
fication which is usually not successful for mono-
lingual sentence alignment. This helps especially
in the case of simplified sentences which have been
largely re-written with simpler vocabulary. These
cases would normally be hard to align correctly. Al-
though it could be argued that the characteristics of
such genuinely simplified text makes the alignment
task somewhat easier, we would like to stress that
the alignment method we present makes no use of
any kind of training data, in contrast to Barzilay and
Elhadad (2003) and, to a minor extent, Nelken and
Shieber (2006).
Although we started out from a very specific need
to align a corpus with reliably simplified news arti-
cles, we are confident that our approach can be ap-
plied in other circumstances. For future work we
are planning to apply this algorithm in combina-
tion of a version of Barzilay and Elhadad?s macro-
alignment and use sequential classification only for
the alignment of sentences within already aligned
paragraphs. This would make our work directly
comparable. We are also planning to test our algo-
rithm, especially the sentence similarity measure it
uses, on data which is similar the data Barzilay and
Elhadad (and also Nelken and Shieber) used in their
25
experiment.
Finally, the alignment tool will be used to
sentence-align a medium-sized parallel Spanish cor-
pus of news and their adaptations that will be a much
needed resource for the study of text simplification
and other natural language processing applications.
Since the size of the corpus we have available at
the moment is relatively modest, we are also investi-
gating alternative resources which could allow us to
create a larger parallel corpus.
Acknowledgments
We thank three anonymous reviewers for their com-
ments and suggestions which help improve the fi-
nal version of this paper. The research described
in this paper arises from a Spanish research project
called Simplext: An automatic system for text sim-
plification (http://www.simplext.es). Sim-
plext is led by Technosite and partially funded by
the Ministry of Industry, Tourism and Trade of the
Government of Spain, by means of the National
Plan of Scientific Research, Development and Tech-
nological Innovation (I+D+i), within strategic Ac-
tion of Telecommunications and Information Soci-
ety (Avanza Competitiveness, with file number TSI-
020302-2010-84). We thanks the Department of In-
formation and Communication Technologies at UPF
for their support. We are grateful to Programa
Ramo?n y Cajal from Ministerio de Ciencia e Inno-
vacio?n, Spain.
References
Sandra M. Alu??sio, Lucia Specia, Thiago Alexan-
dre Salgueiro Pardo, Erick Galani Maziero, and Re-
nata Pontin de Mattos Fortes. 2008. Towards brazil-
ian portuguese automatic text simplification systems.
In ACM Symposium on Document Engineering, pages
240?248.
A. Anula. 2007. Tipos de textos, complejidad lingu???stica
y facilicitacio?n lectora. In Actas del Sexto Congreso
de Hispanistas de Asia, pages 45?61.
Regina Barzilay and Noemi Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
In Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?32.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
In Proc. of AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology, pages 7?10.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text sim-
plification. In COLING, pages 1041?1044.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics.
Lijun Feng, Noemie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In EACL, pages 229?237.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28:527?543, December.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In In 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Sarah E. Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis. In
In Proc. of Workshop on Speech and Language Tech-
nology for Education.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
H. Saggion. 2008. SUMMA: A Robust and Adapt-
able Summarization Tool. Traitement Automatique
des Langues, 49(2):103?125.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In LEC 02: Proceedings of
the Language Engineering Conference (LEC02, pages
64?71.
A. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, 13:260?
269.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of The 23rd
International Conference on Computational Linguis-
tics, pages 1353?1361, Beijing, China, Aug.
26
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 75?84,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
A Hybrid System for Spanish Text Simplification
Stefan Bott
Universitat Pompeu Fabra
C/ Tanger, 122-140
Barcelona, Spain
Horacio Saggion
Universitat Pompeu Fabra
C/ Tanger, 122-140
Barcelona, Spain
David Figueroa
Asi-soft
C/ Albasanz 76
Madrid, Spain
Abstract
This paper addresses the problem of automatic
text simplification. Automatic text simplifica-
tions aims at reducing the reading difficulty
for people with cognitive disability, among
other target groups. We describe an automatic
text simplification system for Spanish which
combines a rule based core module with a sta-
tistical support module that controls the ap-
plication of rules in the wrong contexts. Our
system is integrated in a service architecture
which includes a web service and mobile ap-
plications.
1 Introduction
According to the Easy-to-Read Foundation at least
5% of the world population is functional illiterate
due to disability or language deficiencies. Easy
access to digital content for the intellectual dis-
abled community or people with difficulty in lan-
guage comprehension constitutes a fundamental hu-
man right (United Nations, 2007); however it is far
from being a reality. Nowadays there are several
methodologies that are used to make texts easy to
read in such ways that they enable their reading by
a target group of people. These adapted or simpli-
fied texts are currently being created manually fol-
lowing specific guidelines developed by organiza-
tions, such as the Asociaci?n Facil Lectura,1 among
others. Conventional text simplification requires a
heavy load of human resources, a fact that not only
limits the number of simplified digital content ac-
1http://www.lecturafacil.net
cessible today but also makes practically impossi-
ble easy access to already available (legacy) mate-
rial. This barrier is especially important in contexts
where information is generated in real time ? news
? because it would be very expensive to manually
simplify this type of ?ephemeral? content.
Some people have no problem reading compli-
cated official documents, regulations, scientific lit-
erature etc. while others find it difficult to under-
stand short texts in popular newspapers or maga-
zines. Even if the concept of "easy-to-read" is not
universal, it is possible in a number of specific con-
texts to write a text that will suit the abilities of most
people with literacy and comprehension problems.
This easy-to-read material is generally characterized
by the following features:
? The text is usually shorter than a standard text
and redundant content and details which do not
contribute to the general understanding of the
topic are eliminated.2 It is written in varied
but fairly short sentences, with ordinary words,
without too many subordinate clauses.
? Previous knowledge is not taken for granted.
Backgrounds, difficult words and context are
explained but in such a way that it does not dis-
turb the flow of the text.
? Easy-to-read is always easier than standard lan-
guage. There are differences of level in differ-
2Other providers, for example the Simple English Wikipedia
(http://simple.wikipedia.org) explicitly oppose to
content reduction. The writing guidelines for the Simple En-
glish Wikipedia include the lemma "Simple does not mean
short".
75
ent texts, all depending on the target group in
mind.
Access to information about culture, literature,
laws, local and national policies, etc. is of
paramount importance in order to take part in so-
ciety, it is also a fundamental right. The United Na-
tions (2007) "Convention on the Rights of Persons
with Disabilities" (Article 21) calls on governments
to make all public information services and docu-
mentation accessible for different groups of people
with disabilities and to encourage the media - tele-
vision, radio, newspapers and the internet - to make
their services easily available to everyone. Only a
few systematic efforts have been made to address
this issue. Some governments or organisations for
people with cognitive disability have translated doc-
uments into a language that is "easy to read", how-
ever, in most countries little has been done and orga-
nizations and people such as editors, writers, teach-
ers and translators seldom have guidelines on how to
produce texts and summaries which are easy to read
and understand.
1.1 Automatic Text Simplification
Automatic text simplification is the process by
which a computer transforms a text for a particular
readership into an adapted version which is easier to
read than the original. It is a technology which can
assist in the effort of making information more ac-
cessible and at the same time reduce the cost associ-
ated with the mass production of easy texts. Our re-
search is embedded within the broader context of the
Simplext project (Saggion et al, 2011).3 It is con-
cerned with the development of assistive text simpli-
fication technology in Spanish and for people with
cognitive disabilities. The simplification system is
currently under development. Some of the compo-
nents for text simplification are operational, while
other parts are in a development stage. The sys-
tem is integrated in a larger service hierarchy which
makes it available to the users. This paper concen-
trates on syntactic simplification, as one specific as-
pect, which is a central, but not the only aspect of
automatic text simplification. More concretely, we
present a syntactic simplification module, which is
3http://www.simplext.es
based on a hybrid technique: The core of the sys-
tem is a hand-written computational grammar which
reduces syntactic complexity and the application of
the rules in this grammar is controlled by a statisti-
cal support system, which acts as a filter to prevent
the grammar from manipulating wrong target struc-
tures. Section 2 describes related work, in the con-
text of which our research has been carried out. Sec-
tion 3 justifies the hybrid approach we have taken
and section 4 describes our syntactic simplification
module, including an evaluation of the grammar and
the statistical component. Finally, in section 5 we
show how our simplification system is integrated in
a larger architecture of applications and services.
2 Related Work
As it has happened with other NLP tasks, the first
attempts to tackle the problem of text simplifica-
tion were rule-based (Chandrasekar et al, 1996;
Siddharthan, 2002). In the last decade the focus
has been gradually shifting to more data driven ap-
proaches (Petersen and Ostendorf, 2007) and hybrid
solutions. The PorSimples (Alu?sio et al, 2008;
Gasperin et al, 2010) project used a methodology
where a parallel corpus was created and this cor-
pus was used to train a decision process for sim-
plification based on linguistic features. Siddharthan
(2011) compares a rule-based simplification system
with a simplification system based on a general pur-
pose generator.
Some approaches have concentrated on specific
constructions which are especially hard to under-
stand for readers with disabilities (Carroll et al,
1998; Canning et al, 2000), others focused on text
simplification as a help for other linguistic tasks
such as the simplification of patent texts (Mille and
Wanner, 2008; Bouayad-Agha et al, 2009). Re-
cently the availability of larger parallel or quasi-
parallel corpora, most notably the combination of
the English and the Simple English Wikipedia,
has opened up new possibilities for the use of
more purely data-driven approaches. Zhu et al
(2010), for example, use a tree-based simplification
model which uses techniques from statistical ma-
chine translation (SMT) with this data set.
A recent work, which is interesting because of
its purely data-driven setup, is Coster and Kauchak
76
(2011). They use standard software from the field
of statistical machine translation (SMT) and apply
these to the problem of text simplification. They
complement these with a deletion component which
was created for the task. They concentrate on four
text simplification operations: deletion, rewording
(lexical simplification), reordering and insertions.
Text simplification is explicitly treated in a simi-
lar way to sentence compression. They use stan-
dard SMT software, Moses (Koehn et al, 2007) and
GIZA++ (Och and Ney, 2000), and define the prob-
lem as translating from English (represented by the
English Wikipedia) to Simple English (represented
by the Simple English Wikipedia). The translation
process can then imply any of the four mentioned
operations. They compared their approach to var-
ious other systems, including a dedicated sentence
compression system (Knight and Marcu, 2002) and
show that their system outperforms the others when
evaluated on automatic metrics which use human
created reference text, including BLEU (Papineni et
al., 2002). Their problem setting does, however, not
include sentence splitting (as we will describe be-
low). Another potential problem is that the met-
rics they use for evaluation compare to human ref-
erences, but they do not necessarily reflect human
acceptability or grammaticality.
Woodsend and Lapata (2011) use quasi-
synchronous grammars as a more sophisticated
formalism and integer programming to learn to
translate from English to Simple English. This
system can handle sentence splitting operations
and the authors use both automatic and human
evaluation and show an improvement over the
results of Zhu et al (2010) on the same data set, but
they have to admit that learning from parallel bi-text
is not as efficient as learning from revision histories
of the Wiki-pages. Text simplification can also be
seen as a type of paraphrasing problem. There are
various data-driven approaches to this NLP-task
(Madnani and Dorr, 2010), but they usually focus on
lexical paraphrases and do not address the problem
of sentence splitting, either.
Such data-driven methods are very attractive, es-
pecially because they are in principle language in-
dependent, but they do depend on a large amount of
data, which are not available for the majority of lan-
guages.
3 A Hybrid Approach to Text
Simplification
There are several considerations which lead us to
take a hybrid approach to text simplification. First
of all there is a lack of parallel data in the case of
Spanish. Within our project we are preparing a cor-
pus of Spanish news texts (from the domain of na-
tional news, international news, society and culture),
consisting of 200 news text and their manually sim-
plified versions. The manual simplification is time
consuming and requires work from specially trained
experts, so the resulting corpus is not very big, even
if the quality is controlled and the type of data is very
specific for our needs. It is also very hard to find
large amounts of parallel text from other sources. In
order to use data driven techniques we would require
amounts of bi-text comparable to those used for sta-
tistical machine translation (SMT) and this makes it
nearly impossible to approach the problem from this
direction, at least for the time being.
But there are also theoretic considerations which
make us believe that a rule based approach is a good
starting point for automatic text simplification. We
consider that there are at least four separate NLP
tasks which may be combined in a text simplifica-
tion setting and which may help to reduce the read-
ing difficulty of a text. They all have a different na-
ture and require different solutions.
? Lexical simplification: technical terms, for-
eign words or infrequent lexical items make a
text more difficult to understand and the task
consists in substituting them with counterparts
which are easier to understand.
? Reduction of syntactic complexity: long sen-
tences, subordinate structure and especially re-
cursive subordination make a text harder to un-
derstand. The task consists in splitting long
sentences in a series of shorter ones.
? Content reduction: redundant information
make a text harder to read. The task consists
in identifying linguistic structures which can be
deleted without harming the text grammatical-
ity and informativeness in general. This task is
similar to the tasks of automatic summarization
and sentence compression.
77
? Clarification: Explaining difficult concepts re-
duces the difficulty of text understanding. The
task consists in identifying words which need
further clarification, selecting an appropriate
place for the insertion of a clarification or a
definition and finding an appropriate text unit
which actually clarifies the concept.
There is at least one task of the mentioned which
does not fully correspond to an established machine
learning paradigm in NLP, namely the reduction of
syntactic complexity. Consider the example (1), an
example from our corpus; (2) is the simplification
which was produced by our system.
(1) Se trata de un proyecto novedoso y pionero
que coordina el trabajo de seis concejal?as,
destacando las delegaciones municipales de
Educaci?n y Seguridad . . .
"This is a new and pioneering project that
coordinates the work of six councillors,
highlighting the municipal delegations Ed-
ucation and Safety . . . "
(2) Se trata de un proyecto novedoso y pionero ,
destacando las delegaciones municipales de
Educaci?n y Seguridad . . .
Este proyecto coordina el trabajo de seis
concejal?as.
"This is a new and pioneering project, high-
lighting the municipal delegations Educa-
tion and Safety . . .
This project coordinates the work of six
councillors."
What we can observe here is a split operation
which identifies a relative clause, cuts it out of the
matrix clause and converts it into a sentence of its
own. In the process the relative pronoun is deleted
and a subject phrase (este proyecto / this project) has
been added, whose head noun is copied from the ma-
trix clause. It is tempting to think that converting a
source sentence A in a series of simplified sentences
{b1, . . . , bn} is a sort of translation task, and a very
trivial one. In part this is true: most words translate
to a word which is identical in its form and they hap-
pen to appear largely in the same order. The difficult
part of the problem is that translation is usually an
operation from sentence to sentence, while here the
problem setting is explicitly one in which one input
unit produces several output units. This also affects
word alignment: in order to find the alignment for
the word proyecto in (1) the alignment learner has
to identify the word proyecto in two sentences in
(2). The linear distance between the two instances of
this noun is considerable and the sentences in which
two alignment targets occur are not even necessarily
adjacent. In addition, there may be multiple occur-
rences of the same word in the simplified text which
are not correct targets; the most apparent case are
functional words, but even words which are gener-
ally infrequent may be used repeatedly in a small
stretch of text if the topic requires it (in this para-
graph, for example, the word translation occurs 4
times and the word sentence 5 times). While a ma-
chine can probably learn the one-to-may translations
which are needed here, a non-trivial extension of the
machine-translation setting is needed and the learn-
ing problem needs to be carefully reformulated. Ap-
plying standard SMT machinery does not seem to
truly address the problem of syntactic simplification.
In fact, some approaches to SMT try use text simpli-
fication as a pre-process for translation; for exam-
ple Poornima et al (2011) apply a sentence splitting
module in order to improve translation quality.
On the other hand, other sub-task mentioned
above can be treated with data driven methods. Lex-
ical simplification requires the measurement of lex-
ical similarity, combined with word sense disam-
biguation. Content reduction is very similar to ex-
tractive summarization or sentence compression and
the insertion of clarifications can be broken down
into three learnable steps: identification of difficult
words, finding an insertion site and choosing a suit-
able definition for the target word.
4 Syntactic Simplification
We are developing a text simplification system
which will integrate different simplification mod-
ules, such as syntactic simplification, lexical simpli-
fication (Drndarevic and Saggion, 2012) and content
reduction. At the moment the most advanced mod-
ule of this system is the one for syntactic simplifica-
tion. In (Bott et al, 2012) we describe the function-
ing of the simplification grammar in more detail.
For the representation of syntactic structures we
78
use dependency trees. The trees are produced by the
Mate-tools parser (Bohnet, 2009) and the syntactic
simplification rules are developed within the MATE
framework (Bohnet et al, 2000). MATE is a graph
transducer which uses hand written grammars. For
grammar development we used a development cor-
pus of 282 sentences.
The grammar mainly focuses on syntactic simpli-
fication and, in particular, sentence splitting. The
types of sentence splitting operations we treat at the
moment are the following ones:
? Relative clauses: we distinguish between sim-
ple relative clauses which are only introduced
by a bare relative pronoun (e.g. a question
which is hard to answer) and complex relative
clauses which are introduced by a preposition
and a relative pronoun (e.g. a question to which
there is no answer)
? Gerundive constructions and participle con-
structions (e.g. the elections scheduled for next
November)
? Coordinations of clauses (e.g.[the problem is
difficult] and [there is probably no right an-
swer]) and verb phrases (e.g. The problem [is
difficult] and [has no easy solution]).
? Coordinations of objects clauses (e.g. . . . to get
close to [the fauna], [the plant life] and [the
culture of this immense American jungle re-
gion])
We carried out a evaluation of this grammar,
which is resumed in Table 1. This evaluation looked
at the correctness of the output. Many of the er-
rors were due to wrong parse trees and and the
grammar produced an incorrect output because the
parsed input was already faulty. In the case of rel-
ative clauses nearly 10% occurred because of this
and in the case of gerundive construction 37% of
the errors belonged into that category. We also
found that many of the syntactic trees are ambigu-
ous and cannot be disambiguated only on the basis
of morphosyntactic information. A particular case
of such ambiguity is the distinction between restric-
tive and non-restrictive relative clauses. Only non-
restrictive clauses can be turned into separate sen-
tences and the distinction between the two types is
usually not marked by syntax in Spanish4. Error
analysis showed us that 57.58% of all the errors re-
lated to relative clauses were due to this distinction.
A further 18.18% of the error occurred because the
grammar wrongly identified complement clauses as
relative clauses (in part because of previous parsing
errors).
For this reason, and according to our general phi-
losophy to apply data-driven approaches whenever
possible, we decided to apply a statistical filter in
order to filter out cases where the applications of
the simplification rules lead to incorrect results. Fig-
ure 1 shows the general architecture of the automatic
simplification system, including the statistical filter.
The nucleus of the system in its current state is the
syntactic simplification system, implemented as a
MATE grammar, which consists of various layers.
Origina
l Text Parser
Markin
g of 
Target 
Structu
re
Statisitc
al
Filterin
g
Mate-T
ools
Simplifi
ed Text
Applica
tion of Structu
ral Change
s MATE
Mate-T
ools
Figure 1: The architecture of the simplification system
Syntactic simplification is carried out in three
steps: first a grammar looks for suitable target struc-
tures which could be simplified. Such structures are
then marked with an attribute that informs subse-
quent levels of the grammar. After that the statistical
filter applies and classifies the marked target struc-
tures according to whether they should be changed
or not. In a third step the syntactic manipulations
themselves are carried out. This can combine dele-
tions, insertions and copying of syntactic nodes or
subtrees.
4In English it is mandatory to place non-restrictive relative
clauses between commas, even if many writers do not respect
this rule, but in Spanish comma-placement is only a stylistic
recommendation.
79
Operation Precision Recall Frequency
Relative Clauses (all types) 39.34% 0.80% 20.65%
Gerundive Constructions 63.64% 20.59% 2.48%
Object coordination 42.03% 58.33% 7.79%
VP and clause coordination 64.81% 50% 6.09%
Table 1: Percentage of right rule application and frequency of application (percentage of sentences affected) per rule
type
4.1 Statistical Filtering
Since the training of such filters requires a certain
amount of hand-annotated data, so far we only im-
plemented filters for simple and complex relative
clauses. These filters are implemented as binary
classifiers. For each structure which the grammar
could manipulate, the classifier decides if the sim-
plification operation should be carried out or not.
In this way, restrictive relative clauses, comple-
ment clauses and other non-relative clause construc-
tions should be retained by the filter and only non-
restrictive relative clauses are allowed to pass.
For the training of the filters we hand annotated
a selection of sentences which contained the rele-
vant type of relative clauses (150 cases for simple
and 116 for complex). The training examples were
taken from news texts published in the on-line edi-
tion of an established Spanish newspaper. The style
in which these news were written was notably differ-
ent from the news texts of the corpus we are devel-
oping in within our project, in that they were much
more complex and contained more cases of recursive
subordination. The annotators reported that some of
the sentences had to be re-read in order to fully un-
derstand them; this is not uncommon in this type of
news which may contain opinion columns and in-
depth comments.
In our classification framework we consider one
set of contextual features arising from tokens sur-
rounding the target structure to be classified5 ? the
relative pronoun marked by the simplification iden-
tification rules. This set is composed of, among oth-
ers, the position of the target structure in the sen-
tence; the parts of speech tags of neighbour token;
the depth of the target in a dependency tree; the de-
pendency information to neighbour tokens, etc.
Linguistic intuitions such as specific construc-
5A 5 words window to the left and to the right.
tions which, according to the Spanish grammar,
could be considered as indicating that the simplifi-
cation can or cannot take place. These features are
for example: the presence of a definite or indefinite
article; the presence of a comma in the vicinity of
the pronoun; specific constructions such as ya que
(since), como que (as), etc. where que is not relative
pronoun; context where que is used as a comparative
such as in m?s....que (more... than); contexts where
que is introducing a subordinate complement as in
quiero que (I want that ...); etc. While some of these
features should be implemented relying on syntactic
analysis we have relied for the experiments reported
here on finite state approximations implementing all
features in regular grammars using the GATE JAPE
language (Cunningham et al, 2000; Maynard et al,
2002). For other learning tasks such as deciding
for the splitting of coordinations or the separation
of participle clauses we design and implement spe-
cific features based on intuitions; contextual features
remain the same for all problems.
The classification framework is implemented in
the GATE system, using the machine learning li-
braries it provides (Li et al, 2005). In particular,
we have used the Support Vector Machines learn-
ing libraries (Li and Shawe-Taylor, 2003) which
have given acceptable classification results in other
NLP tasks. The framework allows us to run cross-
validation experiments as well as training and test-
ing.
Table 2 shows the performance of the statistical
filter in isolation, i.e. the capacity of the filter alone
to distinguish between good and bad target struc-
tures for simplification operations. The in-domain
performance was obtained by a ten-fold cross clas-
sification of the training data. The out-of-domain
evaluation was carried out over news texts from our
own corpus, the same collection we used for the
80
Figure 2: A simplified news text produced by the service
on a tablet computer running Android
evaluation of the grammar and the combination of
the grammar with the statistical filter. The perfor-
mance is given here as the overall classification re-
sult. Table 3 shows the performance of the grammar
with and without application of the filter.6
4.2 Discussion
We can observe that the statistical filters have a
quite different performance when they are applied
in-domain and out-of-domain (cf. Table 2), espe-
cially in the case of simple relative clauses. We
attribute this to the fact that the style of the texts
which we used for training is much more compli-
cated than the texts which we find in our own cor-
pus. The annotators commented that many relative
clauses could not turned into separate sentences be-
cause of the overall complexity of the sentence. This
problem seems to propagate into the performance
of the combination of the grammar with the filter
(cf. Table 3). The precision improves with filter-
ing, but the recall drops even more. Again, we sus-
pect that the filter is very restrictive because in the
training data many relative clauses were not separa-
ble, due to the overall sentence complexity which is
much lesser in the corpus from which the test data
was taken. For the near future we plan to repeat
6The results here are not fully comparable to Table 1, be-
cause in order to evaluate the filter, we did not consider parse
errors, as we did in the previous evaluation.
Este mi?rcoles las personas con Sindrome de Down celebran
si d?a mundial . En Espa?a , hay m?s de 34 .000 personas con
esta discapacidad . esta discapacidad ocurre en uno de cada
800 nacimientos .
El S?drome de Down es un trastorno gen?tico . este trastorno
causa la presencia de una copia extra del cromosoma 21 en
vez de los dos habituales ( trisom?a del par 21 ) . La
consecuencia es un grado variable de discapacidad cognitiva y
unos r?sgos f?sicos particulares y reconocibles .
Se trata de la causa m?s frecuente de discapacidad cognitiva
ps?quica cong?nita y debe su nombre a John Langdon Haydon
Down . este Landgdon fue el primero en describir esta
alteraci?n gen?tica en 1866 . Siegue sin conocerse con
exactitud las causas . estas causas provocan el exceso
cromos?mico , a?nque se relaciona estad?stica mente con
madres de m?s de 35 a?os .
Table 4: The simplified text shown in figure2
the experiment with annotated data which is more
similar to the test set. The performance in the case
of complex relative clauses is much better. We at-
tribute the difference between simple and complex
relative clauses to the fact that the complex construc-
tions cannot be confounded with other, non-relative,
constructions, while in the case of the simple type
this danger is considerable. The somewhat unre-
alistic value of 100% is a consequence of the fact
that in the part of the corpus we annotated complex
relative clauses were not very frequent. We took
some additional cases from our corpus into consider-
ation, evaluating more cases from the corpus where
the corresponding rule was applied7 and the value
dropped to slightly over 90%.
5 Integration of the Simplification System
in Applications
As we have mentioned in the introduction, our text
simplification system is integrated in a larger service
and application setting. Even if some modules of the
system must still be integrated, we have an operative
prototype which includes a mobile application and a
web service.
In the context of the Simplext project two mo-
bile applications have been developed. The first one
runs on iOS (developed by Apple Inc. for its de-
vices: Iphone, Ipad and Ipod touch), and the other
one on Android (developed by Google, included in
many different devices). These applications allow
7For these cases we could not calculate recall because this
would have implied a more extensive annotation of all the sen-
tences of the part of the corpus from which they were taken.
81
Operation Precision Recall F-score
Simple Relative Clauses (in domain) 85.41% 86.77% 86.06%
Complex Relative Clauses (in domain) 70.88% 71.33% 71.10 %
Simple Relative Clauses (out of domain) 76.35% 76.35% 76.35%
Complex Relative Clauses (out of domain) 90.48% 85.71% 88.10%
Table 2: The performance of the statistical filter in isolation
Operation Precision Recall F-score
Simple Relative Clauses (Grammar) 47.61% 95.24% 71.43%
Complex Relative Clauses (Grammar) 62.50% 55.56% 59.02%
Simple Relative Clauses (Grammar + Filter) 59.57% 66.67% 63.12%
Complex Relative Clauses (Grammar + Filter) 100% 55.56% 77.78%
Table 3: The performance of grammar and the statistical filter together
to read news feeds (RSS / Atom) from different
sources through a proxy that provide the language
simplification mechanism. The mobile applications
are basically RSS/Atom feed readers, with simpli-
fication capabilities (provided by the service layer).
Both applications work the same way and allow to
the user functionalities as keeping a list of favourite
feeds, adding and removing feeds, marking content
as favourite and showing the simplified and origi-
nal versions of the content. Also a web service was
created, which works in a similar way for RSS and
Atom feeds and allows to simplify the text portion
of other publicly available websites.
Figure 2 shows a screen capture of the mobile ap-
plication running in a Android tablet, displaying a
simplification example of a text taken from a news
website. The display text of this image is reproduced
in Table 4 for better readability. The text itself is too
long for us to provide a translation, but it can be seen
that many sentences have been split. Also a series of
minor problems can be seen, which we will resolve
in the near future: The first word of a sentence is still
in lower case and the head noun of the named en-
tity John Langdon Haydon Down was not correctly
identified.
6 Conclusions
Automatic text simplification is an Assistive Tech-
nology which help people with cognitive disabilities
to gain access to textual information. In this paper
we have presented a syntactic simplification module
of a automatic text simplification system which is
under development. We have presented arguments
for the decision of using a hybrid strategy which
combines a rule-based grammar with a statistical
support component, we have described the imple-
mentation of this idea and have given a contrastive
evaluation of the grammar with and without statisti-
cal support. The simplification system we described
here is integrated in a user-oriented service architec-
ture with mobile applications and web services. In
future work we will further enhance the system and
integrate new components dedicated to other simpli-
fication aspects, such as lexical simplification and
content reduction.
Acknowledgements
The research described in this paper arises from
a Spanish research project called Simplext: An au-
tomatic system for text simplification (http://
www.simplext.es). Simplext is led by Tech-
nosite and partially funded by the Ministry of In-
dustry, Tourism and Trade of the Government of
Spain, by means of the National Plan of Scientific
Research, Development and Technological Innova-
tion (I+D+i), within strategic Action of Telecom-
munications and Information Society (Avanza Com-
petitiveness, with file number TSI-020302-2010-
84). We are grateful to fellowship RYC-2009-04291
from Programa Ram?n y Cajal 2009, Ministerio de
Econom?a y Competitividad, Secretar?a de Estado de
Investigaci?n, Desarrollo e Innovaci?n, Spain.
82
References
Sandra M. Alu?sio, Lucia Specia, Thiago Alexan-
dre Salgueiro Pardo, Erick Galani Maziero, and Re-
nata Pontin de Mattos Fortes. 2008. Towards brazil-
ian portuguese automatic text simplification systems.
In ACM Symposium on Document Engineering, pages
240?248.
Bernd Bohnet, Andreas Langjahr, and Leo Wanner.
2000. A development environment for MTT-based
sentence generators. Revista de la Sociedad Espa?ola
para el Procesamiento del Lenguaje Natural.
Bernd Bohnet. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceed-
ings of the Conference on Natural Language Learning
(CoNLL), pages 67?72, Boulder, Colorado. Associa-
tion for Computational Linguistics.
Stefan Bott, Horacio Saggion, and Simon Mille. 2012.
Text simplification tools for spanish. In Proceedings
of the LREC-2012, Estambul, Turkey.
Nadjet Bouayad-Agha, Gerard Casamayor, Gabriela Fer-
raro, and Leo Wanner. 2009. Simplification of patent
claim sentences for their paraphrasing and summariza-
tion. In FLAIRS Conference.
Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley. 2000. Cohesive generation of syntactically
simplified newspaper text. In TSD, pages 145?150.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
In Proc. of AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology, pages 7?10.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text sim-
plification. In COLING, pages 1041?1044.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of Text-To-Text Generation, Portland, Oregon. Associ-
ation for Computational Linguistics.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield, Novem-
ber.
Biljana Drndarevic and Horacio Saggion. 2012. Towards
automatic lexical simplification in spanish: an empir-
ical study. In NAACL 2012 Workshop on Predicting
and Improving Text Readability for Target Reader Pop-
ulations, Montreal, Canada.
Caroline Gasperin, Erick Galani Maziero, and Sandra M.
Alu?sio. 2010. Challenging choices for text simplifi-
cation. In PROPOR, pages 40?50.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: a probabilistic approach
to sentence compression. Artif. Intell., 139(1):91?107,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Y. Li and J. Shawe-Taylor. 2003. The SVM with
Uneven Margins and Chinese Document Categoriza-
tion. In Proceedings of The 17th Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC17), Singapore, Oct.
Yaoyong Li, Katalina Bontcheva, and Hamish Cunning-
ham. 2005. Using Uneven Margins SVM and Per-
ceptron for Information Extraction. In Proceedings
of Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005).
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
Diana Maynard, Valentin Tablan, Hamish Cunningham,
Cristian Ursu, Horacio Saggion, Katalina Bontcheva,
and Yorik Wilks. 2002. Architectural Elements of
Language Engineering Robustness. Journal of Nat-
ural Language Engineering ? Special Issue on Ro-
bust Methods in Analysis of Natural Language Data,
8(2/3):257?274.
Simon Mille and Leo Wanner. 2008. Making text re-
sources accessible to the reader: The case of patent
claims. Marrakech (Marocco), 05/2008.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, Hongkong, China, Oc-
tober.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sarah E. Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis. In
In Proc. of Workshop on Speech and Language Tech-
nology for Education.
C. Poornima, V. Dhanalakshmi, K.M. Anand, and KP So-
man. 2011. Rule based sentence simplification for
english to tamil machine translation system. Interna-
tional Journal of Computer Applications, 25(8):38?42.
83
H. Saggion, E. G?mez Mart?nez, E. Etayo, A. Anula, and
L. Bourg. 2011. Text simplification in simplext. mak-
ing text more accessible. Procesamiento de Lenguaje
Natural, 47(0):341?342.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In LREC?02: Proceedings of
the Language Engineering Conference, pages 64?71.
Advaith Siddharthan. 2011. Text simplification using
typed dependencies: A comparison of the robustness
of different generation strategies. In Proceedings of
the 13th European Workshop on Natural Language
Generation (ENLG), pages 2?11, September.
United Nations. 2007. Convention on the
rights of persons with disabilities. http:
//www2.ohchr.org/english/law/
disabilities-convention.htm.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 409?420.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of The 23rd
International Conference on Computational Linguis-
tics, pages 1353?1361, Beijing, China, Aug.
84
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 1?10,
Dublin, Ireland, August 24 2014.
Modelling Regular Subcategorization Changes in German Particle Verbs
Stefan Bott Sabine Schulte im Walde
Institut f?ur Maschinelle Sprachverabeitung
Universit?at Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{stefan.bott,schulte}@ims.uni-stuttgart.de
Abstract
German particle verbs are a type of multi word expression which is often compositional with
respect to a base verb. If they are compositional they tend to express the same types of semantic
arguments, but they do not necessarily express them in the same syntactic subcategorization
frame: some arguments may be expressed by differing syntactic subcategorization slots and other
arguments may be only implicit in either the base or the particle verb. In this paper we present a
method which predicts syntactic slot correspondences between syntactic slots of base and particle
verb pairs. We can show that this method can predict subcategorization slot correspondences with
a fair degree of success.
1 Introduction
In German, particle verbs (PVs) are a very frequent and productive type of multi word expression. Parti-
cle verbs, such as anstarren (to stare at) in (1-a), are built from a base verb (BV) and a particle. Similar to
other multi word expressions, German PVs may show a varying degree of compositionality with respect
to the BV and to the particle. But German PVs also have another particularity: if they are compositional,
the mapping from semantic arguments to syntactic subcategorization frames may be different between
the PV and its corresponding BV.
(1) a. Die
The
Katze
cat-N-nom
starrt
stares
(den
(the
Vogel
bird-N-acc
|
|
die
the
Wohnungst?ur)
apartment door-N-acc)
an.
at-PRT.
The cat stares at the (bird | apartment door).
b. Die
The
Katze
cat-N-nom
starrt
stares
auf
at-P
den
the
Vogel.
bird-acc.
c. Die
The
Katze
cat-N-nom
starrt
stares
zur
at-P the
Wohnungst?ur.
apartment door-dat.
The events expressed with the PV anstarren in (1-a) can also be expressed with the BV starren in (1-b)
and (1-c). But while the argument Vogel or Wohnungst?ur is expressed as an accusative object in (1-a) it
is expressed as a PP in both (1-b) and (1-c), headed by the preposition auf and zu, respectively.
Related to this phenomenon, the change in the typical subcategorization frame from the BV to the
PV can also lead to an incorporation or an addition of syntactic complements (Stiebels, 1996; L?udeling,
2001), as illustrated by (2). The BV bellen (to bark) is strictly intransitive, while the corresponding PV
anbellen (to bark at) is transitive and takes an obligatory accusative object which expresses the person
or entity being barked at. This is a case of argument extensions in the PV with respect to its BV. The
PV anschrauben (to screw onto) displays incorporation: it can nearly never select an argument which
expresses the location onto which something is screwed, while its BV schrauben (to screw) requires the
expression of the location with a PP.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
(2) a. Der
The
Hund
dog-N-nom
bellt.
barks.
b. Der
The
Hund
dog-N-nom
bellt
barks
den
the
Postboten
postman-N-acc
an.
at-PRT.
c. Der
The
Mechaniker
mechanic-N-nom
schraubt
screws
die
the
Abdeckung
cover
auf
on
die
the
?
Offnung.
opening-N-acc.
d. Der
The
Mechaniker
mechanic-N-nom
schraubt
screws
die
the
Abdeckung
cover
an.
on-PRT.
(3) a. Der
The
Metzger
butcher
bringt
brings
seiner
his
Frau
wife
Blumen.
flowers.
The butcher brings his wife flowers.
b. Der
The
Metzger
butcher
bringt
brings
das
the
L?ammchen
little lamb
um.
PRT.
The butcher assassinates the little lamb.
Finally, if the meaning of the PV is not compositional with respect to the BV, there are no se-
mantic correspondences between subcategorization slots of the PV and the BV. The problem of non-
compositionality is illustrated by (3) which uses the PV umbringen (to assassinate), which has a totally
different meaning from its BV bringen (to bring). A successful mapping between the subcategorization
slots of both can thus be expected to have a direct relation to the assessment of PV compositionality.
The problem we address here can be called the syntactic transfer problem: the subcategorization
frame of a BV can be mapped onto a subcategorization frame of the PV, where semantic arguments are
not necessarily realized with the same syntactic positions in both of the verbs. A good approximation
to this problem is potentially very useful in computational lexicography and other NLP tasks, such as
machine translation and information extraction. We also expect it to be helpful to assess other aspects of
German particle verbs, such as the prediction of compositionality levels.
In order to tackle the problem of argument slot matching we use a vector space model to represent dis-
tributional semantics. We expect that high distributional similarity between two given subcategorization
slots taken from a verb pair signals a correspondence of these slots in a pair of subcategorization frames.
On the contrary, we expect that low distributional similarity signals that no such correspondence can be
established. Further on, if for a given subcategorization slot, either from a BV or a PV, no matching
slot can be found in the complementary PV/BV automatically, this typically corresponds to a case of
argument incorporation or argument extension.
In short, in this paper we make the following contributions: We present a method of automatically
mapping syntactic subcategorization slots of BVs and PVs which is based on distributional semantics
and we show that this method can outperform a random baseline with a high level of success.
The rest of this paper is organized as follows: In section 2 we present related work. Section 3 describes
our experimental setup, including the method of correspondence prediction, the elicitation of human
judgements and the evaluation. Section 4 presents the results which are then discussed in section 5.
Section 6 concludes the paper with some final remarks and outlook on future work.
2 Related Work
Particle verbs have been studied from the theoretical perspective and, to a more limited extent, from the
aspect of the computational identifiability, predictability of the degree of semantic compositionality (the
transparency of their meaning with respect to the meaning of the base verb and the particle) and the
semantic classifiabilty of PVs.
For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicen-
cio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et
al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the
first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic
behavior and syntactic change. Schulte im Walde (2004; 2005; 2006) presents several preliminary distri-
2
butional studies to explore salient features at the syntax-semantics interface that determine the semantic
nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006)
and Hartmann (2008) present preliminary experiments on modelling the subcategorization transfer of
German PVs with respect to their BVs, in order to strengthen PV-BV distributional similarity. The main
goal for them is to use transfer information in order to predict the degree of semantic compositionality
of PVs. K?uhner and Schulte im Walde (2010) use unsupervised clustering to determine the degree of
compositionality of German PVs, via common PV-BV cluster membership. They are, again, mainly
interested in the assessment of compositionality, which is done on the basis of lexical information. They
use syntactic information, but only as a filter and for lexical heads as cooccurrence features in order
to limit the selected argument slots to certain syntactic functions. They compare different feature con-
figurations and conclude that the best results can be obtained with information stemming from direct
objects and PP-objects. The incorporation of syntactic information in the form of dependency arc labels
(concatenated with the head nouns) does not yield satisfactory results, putting the syntactic transfer prob-
lem in evidence, the problem which we address here. They conclude that an incorporation of syntactic
transfer information between BVs and PVs could possibly improve the results. In Bott and Schulte im
Walde (2014a) we present a method to assess PV compositionality without recurring to any syntactic
features, but we assume that the results of this method could be improved if additional syntactic transfer
information was incorporated.
Based on a theoretical study (Springorum, 2011) which explains particle meanings in terms of Dis-
course Representation Theory (Kamp and Reyle, 1993), Springorum et al. (2012) show that four classes
of PVs with the particle an can be classified automatically. They take a supervised approach using de-
cision trees. The use of decision trees also allows them to manually inspect and analyze the decisions
made by the classifier. As predictive features they use the head nouns of objects, generalized classes of
these nouns and PP types. In Bott and Schulte im Walde (2014b) we present an experiment to classify
semantic classes of PVs, based on subcategorization information stemming from both the BV and the
BV of each BV-PV pair. In this work we use the same gold standard we use here. This experiment is also
related to the one presented here in that we assume that the syntactic transfer patterns are quite stable
within semantic classes.
3 Experimental Setup
In order to test our hypothesis we selected a set of 32 PVs listed in Fleischer and Barz (2012), including
14 PVs with the particle an and 18 with the particle auf.
1
We concentrated on two particles here in order
to have a small and controlled test bed which allows us to study the syntactic transfers. We selected
verbs which we considered to be highly compositional in order to be able to study the correspondence of
subcategorization slots. The set contained verbs which have argument slots which are typically realized
as different syntactic subcategorizations. The set also contained PVs which show argument incorporation
or the introduction of an additional syntactic complement with respect to their BV. We excluded verbs
which we could clearly perceive as being polysemous. This set of verbs was processed automatically
and presented to human raters, as described below. The test set can be seen in table 1. This test set
was already used in Bott and Schulte im Walde (2014b), where it was used as a gold standard for the
automatic classification of semantic classes of particle verbs, based on syntactic transfer patterns. The
subcategorization patterns listed here are the ones we expected to find, so the second and the third row
together represent the expected syntactic transfer pattern. The values given in these two columns are
a lexicographic presentation of the transfer patterns we expected to find. The task of the system was
defined as to find matches between slots from both verbs automatically. The verbs were grouped together
in classes which are both semantically similar and also expected to have a similar syntactic behaviour.
The labels in the column for the semantic class are taken from Fleischer and Barz (2012), but broken
down into more detailed classes, such as verbs of trying, gaze or sound. The latter label extensions were
1
Fleischer and Barz list more than 100 PVs for both an and auf, but they embed this listing in a descriptive text. Some of
the verbs listed are very rare or highly ambiguous. Since particle verbs in German are a highly productive paradigm and give
rise to many neologisms, compiling a complete list of PVs is nearly impossible.
3
added by us. In the present work we are not interested in the semantic classes as such, but we assume
that the transfer patterns are similar in each semantic class.
3.1 Automatic Classification
Since we wanted to test the predictability of syntactic slot correspondences, we first had to identify
the typical elements of the subcategorization frames for both BVs and PVs. In order to do so, we
extracted all observable subcategorization patterns from a parsed corpus. Then we selected the 5 most
frequent subcategorization patterns for each verb (either BV or PV). These patterns were then broken
down into their individual elements. The simple transitive pattern, for example, contained a subject and
an accusative object. Since some subordinate structures miss overt subjects and in German all verbs have
a subject slot, we always included the subject in the representation of all verbs. The rationale behind this
method, which is based on the frequency of subcategorization patterns rather than the frequency of slots,
was that we were not interested in subcategorization slots per se, but in subcategorization patterns as a
typical representation structure in computational lexicography.
Then we built a vector space model for all possible combinations of BV-complements and PV-
complements of each BV-PV pair. The dimensions of the vector were instantiated by the head nouns
of the syntactic relation in question. The extension in each dimension is equal to the frequency of the
head noun in the relevant position. For this experiment no term weighting was applied. Table 2 shows
the strongest dimensions for the vectors corresponding to the PP-argument headed by the verbs heften (to
attach) and anheften (to attach to). The two verbs can be used in quite similar contexts with very similar
arguments. Accordingly, the two vectors are similar to each other. Although the two vectors correspond
to PP slots headed by the preposition an, it can be seen that there is a syntactic transfer from accusative to
dative case. Both vectors include head nouns expressing typical places to which things can be attached
to, such as a pin board (Pinnwand), a wall (Wand) or a board (Brett). The verb heften is frequently
found in the idiom sich an jemandes Ferse heften (to attach onseself to someone?s heels, which means
to follow someone closely), while this idiom cannot be formed with the PV anheften. For this reason the
dimension for Ferse is very strong. This example, especially the vector for anheften also shows that the
features are often sparsely represented, which presents a problem for our approach.
As a similarity measure we used the cosine distance between two vectors. A variable threshold was
applied on the cosine distance to, which serves to separate corresponding subcategorization slots from
non-corresponding ones. This is especially important for the detection of argument incorporation or
argument extension (cf. example (2)). If, for example, for a given BV slot no PV slot can be found
with a cosine value above the threshold, we interpret this as a case of argument extension. On the
other hand, a slot from a PV which cannot be match to a slot of its BV is taken to signal argument
incorporation. Among the vectors compared to each target subcategorization slot only the one with the
highest cosine value was considered as a possible correspondence. Finally, since we want to capture both
argument incorporation and argument extension, we computed correspondences for both BVs and PVs
separately. Even if this means that most slot pairs are computed twice, this allowed zero-correspondences
for slots from both verbs. It theoretically also allows for one-to-many and many-to-one matches, even
if we did not exploit them here. We excluded closed class dependencies of verbs, such as negations.
We also excluded clausal complements, because they could not be properly represented by our vector
extraction method. To get an idea of the lower bound of the outcome values, we used a select-1 baseline.
This baseline was obtained by calculating the expected precision and recall for the case that for each
subcategorization slot a matching slot from the corresponding other verb is assigned randomly.
As training data we used a lemmatized and tagged version of the SDeWaC corpus (Faa? and Eckart,
2013), a corpus of nearly 885 million words. The corpus was processed with the Mate dependency parser
(Bohnet, 2010). The output of this parser represents the syntactic complements of the verbs as labelled
arcs. In the case of nominal objects the nominal heads could be directly read of the dependent nodes and
the syntactic relation of the arc labels. In the case of PP-complements we read the nominal heads of the
nominal node which depends on the preposition which in turn depends on the verb. For the extraction of
features we could rely on the database compiled by (Scheible et al., 2013).
4
Particle Typical frames Typical frames Semantic Verbs in Class
for the BV for the PV Class
an
NPnom
+NPacc
+PP-an
NPnom
+NPacc
+PP-an
locative/
relational
tying
an|binden to tie at
an|ketten to chain at
NPnom
+PP-zu/in/
nach/auf
NPnom
+NPacc
locative/
relational
gaze
an|blicken to glance at
an|gucken to look at
an|starren to stare at
NPnom
+NPacc
+PP-mit
NPnom
+NPacc
+PP-mit
ingressive
consump-
tion
an|brechen start to break
an|rei?en start to tear
an|schneiden start to cut
NPnom
NPnom
+NPacc
locative/
relational
sound
an|br?ullen to roar at
an|fauchen to hiss at
an|meckern to bleat at
NPnom
+NPacc
+PP-an
NPnom
+NPacc
locative/
relational
fixation
an|heften to stick at
an|kleben to glue at
an|schrauben to screw at
auf
NPnom NPnom
locative
blaze-
bubble
auf|brodeln to bubble up
auf|flammen to light up
auf|lodern to blaze up
auf|spudeln to bubble up
NPnom
+PP-zu/in/
nach/auf
NPnom
locative
gaze
auf|blicken to glance up
auf|schauen to look up
auf|sehen to look up
NPnom
+NPacc
NPnom
+NPacc
locative/
dimensional
instigate
auf|hetzen to instigate
auf|scheuchen to rouse
NPnom
+NPacc
+PP-auf
NPnom
+NPacc
locative/
relational
fixation
auf|heften to staple on
auf|kleben to glue on
auf|pressen to press on
NPnom NPnom
ingressive
sound
auf|br?ullen suddenly roar
auf|heulen suddenly howl
auf|klingen suddenly sound
auf|kreischen suddenly scream
auf|schluchzen suddenly sob
auf|st?ohnen suddenly moan
Table 1: The gold standard classes for the experiments, with subcategorization patterns.
anheften-MO-an-dat count heften-MO-an-acc count
Oberfl?ache 3 Ferse 154
Gerichtstafel 3 Brust 48
Stelle 2 Revers 43
Schluss 2 Kreuz 32
Unterlage 1 Wand 30
Kirchent?ure 1 Spur 12
Brett 1 Tafel 11
Pinnwand 1 Fahne 11
K?orper 1 T?ur 11
Wand 1 Pinnwand 9
Bauchdecke 1 Kleid 6
Baum 1 Brett 6
Schleimhautzelle 1 Mastbaum 6
Himmel 1 K?orper 5
Spur 1 ihn 5
Sph?are 1 Kleidung 5
Wand 1 Oberfl?ache 5
Spur 1 Stelle 4
Engstelle 1 Baum 4
Pflanze 1 Jacke 4
Protein 1 Mantel 4
Unterseite 1 Teil 3
Zweig 1 Krebszelle 3
Pin-Wand 1 schwarz 3
Table 2: The strongest dimensions for two sample vectors representing subcategorization slots of the
verbs heften and anheften.
5
3.2 Human rating elicitation
We asked human raters to rate the same examples which the system classified automatically. Each of the
pairs of subcategorization slots described in section 3.1 was rated individually. The pairs were always
presented in the order <BV-subcategorization-slot,PV-subcategorization-slot> and in visual blocks cor-
responding to BV subcategorization slots. So the raters could see the possible PV subcategorization slots
in direct comparison. The order of blocks was randomized. The raters were asked to judge every pair
and rate whether or not they could correspond to a single semantic argument. They were invited to invent
example sentences, but because of the length of the annotation session they were not asked to write them
down. They were told that, as a criterion for semantic correspondence, each of the verbs in a pair should
be usable to describe at least one event or situation they could think of. One annotation example, which
did not stem from the set to be rated, was given.
Four human raters were asked to rate examples. All annotators were experts with either a linguistic
or NLP background. They were all German native speakers and none of them was otherwise involved
in the work presented in this paper. Because of the large size of the data set to be annotated we had
to distribute the set over two annotation forms and each annotation form was annotated by two raters.
Before the annotation started, one of the authors carried out the same annotation in order to estimate the
time needed for each annotation and the level of success which could be expected from the system. Also
this annotation was done blindly, without knowledge of the system output, but with a precise knowledge
of the task.
The annotation turned out to be much more difficult that we had originally expected. The annotators
described the annotation as being hard to perform. This was also reflected by inter annotator agreement;
we could only observe a fair agreement, with a Fleiss? Kappa score of 0.31. The agreement between the
annotator ratings and the rating by the author was somewhat higher with a Fleiss? Kappa score of 0.44.
Some annotators gave detailed feedback, once they had completed the annotation.
4 Results
Table 3 shows the results we obtained. The columns show precision, recall and the harmonic F-score
obtained by comparing the system output to the human ratings. We used a precision/recall schema
because the task can be seen as the system selecting the most likely slot correspondences from a set of all
possible correspondences. So a true positive is obtained if the system selects the same slot that a human
rater would select. False positives correspond to a slot selected by the system, which was not chosen by
the annotator and a false negative instances are those which are marked by an annotator and not chosen
by the system.
2
Since there was more than one annotators and the annotations differed, we took the sum
of true and falls positives and false negatives from all annotators and calculated the scores over this sum.
The last column shows the harmonic F-score values we obtained with the annotations produced by one of
the authors. The lines represent those threshold values for which the highest precision or F-score could
be obtained. The last line represents the baseline. Since a variable threshold was applied there is a trade-
off between precision and recall. This is represented in figure 1, which displays the same information as
table 3, but in a graphical way.
As expected, the precision improves with higher thresholds, but this comes at the cost of a lower re-
call. The F-score stays relatively constant. The baseline is quite low, especially the recall. This can be
explained because the human raters were free to assign zero-correspondences (i.e. argument incorpora-
tions or argument extensions, as exemplified by the examples in (2)) or more than one correspondence
per target slot.
5 Discussion
We could observe that the system can predict the correspondences between syntactic subcategorization
slots to a fair degree of success and that our method can clearly outperform the baseline. Our hypotheses
2
Precision was calculated as
{true positives}
{true positives}+{false positives}
and recall as
{true positives}
{true positives}+{false negatives}
. The F-score
was calculated as (precision + recall)/2.
6
Threshold Precision Recall F-score Author F-score
0.15 0.48 0.38 0.43 0.68
0.6 0.69 0.21 0.45 0.63
0.85 0.75 0.14 0.44 0.59
baseline 0.38 0.23 0.31 0.31
Table 3: Results of the evaluation in precision, recall and harmonic F-score. The last column represents
the pilot annotation carried out by one of the authors.
Figure 1: Trade-off between precision and recall. The F-Score remains relatively stable.
that correspondence between subcategorization slots can be predicted to a large degree by distributional
semantic similarity can thus be confirmed. On the other hand, the success was not as high as we initially
expected. It is surprising that the precision and recall values obtained with the annotations of the human
raters are much lower than the values obtained in the initial annotation produced by the author. The
author annotation has to be seen as overly optimistic, since it was done with a deeper understanding of the
computational task which was to be carried out by the system. Still, this annotation was done blindly. So
the big difference we observed is surprising. As already mentioned, the annotators all reported that they
found the annotation task difficult to carry out and we attribute the low agreement to this difficulty. The
fact that the agreement among different rather was also only fair (? = 0.31) hints in the same direction. It
must be said that some annotators found the annotation task more difficult than others. Two of the raters
reported less annotation difficulty than the remaining. These two annotators were also the ones with
most annotation experience and they were both familiar with the topic of particle verbs from a theoretical
perspective. When the system output was compared to the ratings of best annotator, a maximum F-score
of 0.55 could be achieved, which is still lower than the values obtained in comparison to the author
annotation, but much higher than the average of all annotations.
Since some of the annotators gave detailed comments after the annotation was completed, we could
detect some problems, which made the annotation difficult, but also extends to the automatic matching.
For example, some base verbs have a resultative reading which do not express an agent and match the
patient with the nominal subject position. One such verb is kleben (to stick/glue) as exemplified in (4).
Accordingly among the strongest dimensions of the vector that represents the subject slot of kleben,
many nouns appear, which are typical things that stick, such as band aids (Pflaster), dough (Teig) and
blood (Blut). The closest vector to the vector for the accusative object vector of ankleben was also
the accusative object vector of kleben (cosine=0.64), but the subject vector was still relatively strong
(cosine=0.19).
7
(4) a. Gerda
Gerda
klebt
sticks
den
the
Zettel
Note
an
on
die
the
T?ur.
door.
b. Der
The
Zettel
Note
klebt
sticks-to
an
the
der
door.
T?ur.
The particle verb ankleben can be used to describe the same state of affairs as in (4-a), but not as in (4-b).
This is evidently a problem which is hard to solve with our approach because the correspondence of slots
from BV and PV interferes with a slot correspondence among different uses of the BV.
3
Finally, we found that many of the feature vectors were sparsely instantiated. This can be seen, for
example, in the vector that represents the dative PP modifier headed by an of the verb anheften shown
in table 2. The sparsity problem could be remedied by reducing the number of dimensions with the
application of some kind of abstraction over the head nouns. For example the concepts of T?ur (door) and
Kirchent?ur (church door) are strongly related and could be represented in one dimension of the feature
vector. The same holds for the concepts of Pinnwand (pin board), Wand (wall) and Tafel (blackboard)
and other groups of concepts. With a certain level of abstraction over such concepts, the distance between
vectors would also be reduced in case they are sparse. This abstraction is, however, not a trivial problem
in itself. The application of lexical ontologies like WordNet (as used by e.g. Springorum et al. (2012)),
for example, has the danger of reducing the semantics of head nouns to level of abstraction which is too
high, since WordNet has only few top-level categories and few levels of conceptual inheritance.
6 Conclusion and Outlook
We started the work described in this paper out of an interest to approach the syntactic transfer problem
of German particle verbs from a computational perspective. We wanted to know in how far the subcat-
egorization slots of a particle verb can be associated with subcategorization slots of a base verbs from
which it is derived. The information we used for this matching is based on distributional semantics. We
could show that can be done with a good degree of success. From the elicitation of human judgements
we learned that the task is also not an easy one for human raters. This also sheds some light on the
difficulty of the problem as a computational task.
The work we present here is relevant for computational lexicography. Firstly it can help relate lexical
entries of such closely related lexical items as particle verbs and the base verbs they incorporate. The
findings we made here may be also applicable to other types of multi word expressions.
In future work we would like to remedy the problem sparse vector representation with the use of
abstraction over the head-nouns which will reduce the dimensionality of the feature vector. We also
plan to see in how far an automatic clustering of particle verbs into semantic groups can strengthen the
prediction of slot correspondences under the assumption that semantically similar verbs tend to undergo
the same syntactic transfer. Finally, the problem of syntactic transfer between two elements is also
related to the predictability of the degree of compositionality between BV-PV pairs. We are especially
interested in this last problem and in future work we plan to investigate in which way subcategorization
slot matching can be used as a predictor for compositionality levels.
Acknowledgements
This work was funded by the DFG Research Project ?Distributional Approaches to Semantic Related-
ness? (Stefan Bott, Sabine Schulte im Walde), and the DFG Heisenberg Fellowship SCHU-2580/1-1
(Sabine Schulte im Walde). We would also like to thank the participants of the human rating experiment.
References
Nadine Aldinger. 2004. Towards a Dynamic Lexicon: Predicting the Syntactic Argument Structure of Complex
Verbs. In Proceedings of the 4th International Conference on Language Resources and Evaluation, Lisbon,
Portugal.
3
This problem is similar to the prediction of argument realizations in diathesis alternations, such as pairs found in pairs of
sentences like ?The boy rolled the ball down the hill? vs ?the ball rolled down the hill?.
8
Timothy Baldwin and Aline Villavicencio. 2002. Extracting the Unextractable: A Case Study on Verb Particles.
In Proceedings of the Sixth Conference on Computational Natural Language Learning, pages 98?104, Taipei,
Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An Empirical Model of Mul-
tiword Expression Decomposability. In Proceedings of the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 89?96, Sapporo, Japan.
Timothy Baldwin. 2005. Deep Lexical Acquisition of Verb?Particle Constructions. Computer Speech and Lan-
guage, 19:398?414.
Collin Bannard. 2005. Learning about the Meaning of Verb?Particle Constructions from Corpora. Computer
Speech and Language, 19:467?478.
Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings of the
23rd International Conference on Computational Linguistics, pages 89?97, Beijing, China.
Stefan Bott and Sabine Schulte im Walde. 2014a. Optimizing a Distributional Semantic Model for the Prediction
of German Particle Verb Compositionality. In Proceedings of the 9th International Conference on Language
Resources and Evaluation, pages 509?516, Reykjavik, Iceland.
Stefan Bott and Sabine Schulte im Walde. 2014b. Syntactic Transfer Patterns of German Particle Verbs and their
Impact on Lexical Semantics. In Proceedings of the Third Joint Conference on Lexical and Computational
Semantics, Dublin, Ireland.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC ? a Corpus of Parsable Sentences from the Web. In Proceedings
of the International Conference of the German Society for Computational Linguistics and Language Technology,
Darmstadt, Germany.
Wolfgang Fleischer and Irmhild Barz. 2012. Wortbildung der deutschen Gegenwartssprache. Walter de Gruyter,
4th edition.
Silvana Hartmann. 2008. Einfluss syntaktischer und semantischer Subkategorisierung auf die Kompositionalit?at
von Partikelverben. Studienarbeit. Institut f?ur Maschinelle Sprachverarbeitung, Universit?at Stuttgart. Supervi-
sion: Sabine Schulte im Walde and Hans Kamp.
Hans Kamp and Uwe Reyle. 1993. From discourse to logic: Introduction to modeltheoretic semantics of natural
language, formal logic and discourse representation theory. Number 42. Springer.
Natalie K?uhner and Sabine Schulte im Walde. 2010. Determining the Degree of Compositionality of German Par-
ticle Verbs by Clustering Approaches. In Proceedings of the 10th Conference on Natural Language Processing,
pages 47?56, Saarbr?ucken, Germany.
Anke L?udeling. 2001. On German Particle Verbs and Similar Constructions in German. Dissertations in Linguis-
tics. CSLI Publications, Stanford, CA.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan.
Silke Scheible, Sabine Schulte im Walde, Marion Weller, and Max Kisselew. 2013. A Compact but Linguistically
Detailed Database for German Verb Subcategorisation relying on Dependency Parses from a Web Corpus: Tool,
Guidelines and Resource. In Proceedings of the 8th Web as Corpus Workshop, pages 63?72, Lancaster, UK.
Sabine Schulte im Walde. 2004. Identification, Quantitative Description, and Preliminary Distributional Analysis
of German Particle Verbs. In Proceedings of the COLING Workshop on Enhancing and Using Electronic
Dictionaries, pages 85?88, Geneva, Switzerland.
Sabine Schulte im Walde. 2005. Exploring Features to Identify Semantic Nearest Neighbours: A Case Study
on German Particle Verbs. In Proceedings of the International Conference on Recent Advances in Natural
Language Processing, pages 608?614, Borovets, Bulgaria.
Sabine Schulte im Walde. 2006. The Syntax-Semantics Interface of German Particle Verbs. Panel discussion
at the 3rd ACL-SIGSEM Workshop on Prepositions at the 11th Conference of the European Chapter of the
Association for Computational Linguistics.
9
Sylvia Springorum, Sabine Schulte im Walde, and Antje Ro?deutscher. 2012. Automatic Classification of German
an Particle Verbs. In Proceedings of the 8th International Conference on Language Resources and Evaluation,
pages 73?80, Istanbul, Turkey.
Sylvia Springorum. 2011. DRT-based Analysis of the German Verb Particle ?an?. Leuvense Bijdragen, 97:80?
105.
Barbara Stiebels. 1996. Lexikalische Argumente und Adjunkte. Zum semantischen Beitrag von verbalen Pr?afixen
und Partikeln. Akademie Verlag, Berlin.
Aline Villavicencio. 2005. The Availability of Verb-Particle Constructions in Lexical Resources: How much is
enough? Computer Speech & Language, 19(4):415?432.
10
