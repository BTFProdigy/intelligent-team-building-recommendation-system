Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1435?1446, Dublin, Ireland, August 23-29 2014.
Combining Supervised and Unsupervised Parsing
for Distributional Similarity
Martin Riedl, Irina Alles and Chris Biemann
FG Language Technology
Computer Science Department, Technische Universit?at Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biem}@cs.tu-darmstadt.de, ialles@gmx.de
Abstract
In this paper, we address the role of syntactic parsing for distributional similarity. On the one
hand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers.
On the other hand, we explore whether single unsupervised parsers, or their combination, can
contribute to better distributional similarities, or even replace supervised parsing as a prepro-
cessing step for word similarity. We evaluate distributional thesauri against manually created
taxonomies both for English and German for five unsupervised parsers. While for English, a
supervised parser is the best single parser in this evaluation, we find an unsupervised parser to
work best for German. For both languages, we show significant improvements in word similarity
when combining features from supervised and unsupervised parsers. To our knowledge, this is
the first work where unsupervised parsers are systematically evaluated extrinsically in a seman-
tic task, and the first work to show that unsupervised parsing can complement and even replace
supervised parsing, when used as a pre-processing feature.
1 Introduction
While the field has seen increased interest in automatically inducing syntactic structures from raw or part-
of-speech (POS) tagged text, the evaluation of unsupervised data-driven parsers has almost exclusively
been conducted either by introspection or by automatic comparison to treebanks. It might be due to
comparatively low scores on reproducing a treebank?s syntactic annotation that hardly anyone has yet
attempted to use the output of unsupervised parsers for an NLP task other than parsing itself.
A further complication with unsupervised parsers ? be it dependency parsers, constituency parsers
or combinatory categorial grammar parsers ? is that the categories induced by such parsers cannot be
straightforwardly mapped to linguistically-inspired categories as defined in a treebank. But also when
considering only unlabeled syntactic annotations, an unsupervised parser is hardly to blame if it does not
adhere to sometimes arbitrary conventions: e.g. for dependencies, it is not a priori clear how to connect
auxiliary and main verbs, where to attach the complementizer of subordinate clauses, how to represent
a conjunction and its conjuncts, how to relate the preposition and the nominal in prepositional phrases,
and how to handle punctuation, cf. Nivre and K?ubler (2006), Schwartz et al. (2011).
When it comes to utilizing syntactic structures, however, it is more important that they are consistent
across different sentences than that they adhere to specific syntactic theories and conventions. Here, we
choose a task that makes only intermediary use of syntactic structures: we employ unsupervised parsing
for preprocessing corpora for the purpose of computing distributional similarities. Since it is generally
accepted (e.g. (Lin, 1997; Curran and Moens, 2002)), that syntactic preprocessing plays an important
role for the quality of distributional thesauri, and comparing words along their syntactic contexts does
rely on the existence of such a structure rather than its actual representation, we believe that distribu-
tional similarities are an excellent test bed for addressing the following two research questions: (1) How
do unsupervised parsers compare to supervised parsers when used as feature providers for building Dis-
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1435
tributional Thesauri (DTs) in comparison to supervised parsers? (2) Can the combination of syntactic
parsers increase DT quality?
2 Related Work
2.1 Unsupervised Parser Evaluation
As with other unsupervised approaches, the premise of unsupervised induction of syntactic structure
is to alleviate the bottleneck of expensive manual annotations for improving NLP applications. For
grammar induction, the potential is extremely high due to the complexity of the subject matter: treebanks
belong to the most work-intensive NLP datasets. On the other hand, this complexity is hard to grasp for
unsupervised systems, which is probably the reason why unsupervised parsing technology is still in its
infancy, despite more than a decade of work on this topic.
One of the early works inducing structure from raw sentences and yielding better performance than
a random baseline was achieved by van Zaanen and of Leeds. School of Computer Studies (2001),
who used an Alignment Based Learning (ABL) approach. This algorithm compares all sentences of
a given set and considers matching sequences as constituents. Klein and Manning (2002) presented
another approach focusing on constituent sequences called the Constituent-Context Model (CCM). It is
an EM-based iterative approach that makes use of the linguistic phenomenon that long constituents often
have shorter representations of the same grammatical function that occur in similar contexts. A hybrid
approach combining CCM with a dependency model, called Dependency Model with Valence (DMV),
shows even better performance and is the first unsupervised system to outperform the right-branching
baseline (Klein and Manning, 2004). A great number of recent works are based on DMV, such as the
system by Headden III et al. (2009), who improved DMV by adding lexical information, and Gillenwater
et al. (2010) who added posterior regularization during the training process. Bod (2007) takes a slightly
different direction by following an ?all subtrees approach?, where all possible binary trees are generated
for each sentence. It generates all possible binary trees for each sentence. The parse of a new sentence is
determined by selecting the most probable tree based on the previously accumulated subtree frequencies.
Most of the evaluation of these parsers was performed against a treebank, offering manually annotated
and linguistically motivated parse trees. Schwartz et al. (2011) underline the fact that treebanks contain
linguistically problematic annotations, cases without linguistic consensus, such as the decision on the
head of a verb phrase or a sequence of nouns. They show that the neglectance of these cases has a
significant but unjustified negative influence on the evaluation outcomes and propose a new measure,
Neutral Edge Direction (NED), which alleviates this problem. Bod (2007) argues that parser evaluation
against a treebank favors supervised approaches and therefore measures the parser quality on the outcome
of a syntax based Machine Translation (MT) task where the dependency parsers are evaluated as language
models. In Motazedi et al. (2012), a single unsupervised parser is evaluated in an extrinsic evaluation
for realisation ranking, and does not compare favorably against a supervised parser. Other extrinsic
evaluations with supervised dependency parsers have been performed in information extraction systems
(Miyao et al., 2008; Buyko and Hahn, 2010) or semantic role labeling (Johansson and Nugues, 2008).
2.2 Evaluating Distributional Similarity
Distributional thesauri have been evaluated both extrinsically and intrinsically. Extrinsic evaluations have
been performed e.g. for automatic set expansion (Pantel et al., 2009) or phrase polarity identification
(Goyal and Daum?e, 2011). In this work, we will conduct an intrinsic evaluation, which is more common
for the evaluation of DTs and lexical semantic similarity. Lin (1997; 1998) introduced two measures
using WordNet (Miller, 1995) and Roget?s Thesaurus. Using WordNet, he defines context (synsets a
word occurs in Wordnet or subsets when using Roget?s Thesaurus) and then builds a gold standard
thesaurus using a similarity measure on these contexts. Then he evaluates his automatically computed
Distributional Thesaurus (DT) with respect to the gold standard thesauri. Weeds et al. (2004) evaluate
various similarity measures based on 1000 frequent and 1000 infrequent target terms. Curran (2004)
created a gold standard thesaurus by manually extracting entries from several English thesauri for 70
words. His automatically generated DTs are evaluated against this gold standard thesaurus. All these
1436
systems employ context representations based on syntactic parsing for computing word similarity.
We are going to use a comparatively simple WordNet-based measure, which calculates the similarity
between two terms using the WordNet::Similarity path measure (Pedersen et al., 2004), and averages
path scores between a target term and its n most similar terms. The score between two terms is inversely
proportional to the shortest path between all the synsets of both terms. If two terms share a synset,
the highest possible score of one is assigned. The score is 0.5 for terms that stand in a direct hypernym
relation, and so on. While the absolute scores are hard to interpret due to inhomogeneity in the granularity
of WordNet, they are well-suited for relative comparison when operating on the same set of target terms.
The evaluation in this work is performed by comparing the average score of the top ten entries in the
DT for each of the target terms and report separately on frequent and rare words. Riedl and Biemann
(2013) also show that the results, using the WordNet based approach, are highly correlated to the results
observed with Curran?s approach using a manually created thesaurus. This justifies the usage of manually
created taxonomies for this evaluation.
3 Methodology
3.1 Parsers
In our evaluation, we use five unsupervised parsers, which we will describe briefly. They have been
selected to span several paradigms of unsupervised syntax induction, and due to software availability.
Gillenwater et al. (2010)
1
use a model based on the DMV (Klein and Manning, 2004) and improve
performance by adding sparsity biases on dependency types. They assume a corpus annotated with POS
tags. The aim of this bias is to limit unique head-dependent tag pairs, which is achieved by a constraint
on model posteriors during the learning process.
The work of Marecek and Straka (2013)
2
is another enhancement of the DMV and is subsequently
referred to as Unsupervised Dependency Parser (UDP). It additionally uses prior knowledge in the form
of stop estimates that are computed on a large raw corpus using the reducibility principle: a sequence
of words is considered as reducible if a word can be removed from the phrase and the remaining part
appears another time in the corpus. The assumed property, that the first word of a reducible sequence
does not have any left children and the last word of this sequence does not have any right children, is
used for the calculation of such stop estimates. The authors show that estimates computed on a large
corpus such as Wikipedia can be used for the parsing of new text.
Bisk and Hockenmaier (2013) use an EM approach to induce a Combinatory Categorial Grammar
(CCG), based on very general linguistic assumptions. It creates a model that can be used to parse un-
seen data. The algorithm requires a corpus, previously assigned with POS tags, in order to be able to
distinguish between word classes (mainly to find the verb), and employs general knowledge such as that
sentences are headed by verbs. Further language-specific properties are induced from the training data.
Seginer (2007)
3
takes an incremental parsing and learning approach. It operates directly on the plain
text without the need for POS tags, by using Common Cover Links (CCL), which can be directly con-
verted to dependency arcs. This parser learns during parsing and can be used without a prior learning
step. This should result in increased parsing quality towards later stages, which suggests several passes
over the training data. The obtained model can then be reused to parse unseen data.
The approach of S?gaard (2012) is different from all other approaches discussed here: This algorithm
does not require any training data and can operate with or without POS tags. For this reason, it can be
applied to arbitrary amounts of data, since it operates sentence-wise without memorizing previous inputs,
and produces non-projective dependency parses. The words of a phrase are ordered by centrality and a
parse is determined by the ranking of a parsing algorithm, which uses general linguistic knowledge for
grammar induction. This knowledge is inspired by the rules of Naseem et al. (2010), and the approach
has been tuned (once and for all, for all languages) on development data from the Penn Treebank.
1
http://code.google.com/p/pr-toolkit/
2
http://ufal.mff.cuni.cz/udp/
3
http://www.seggu.net/ccl/
1437
Figure 1: Parses for an example sentence for several parsers. Here, Bisk?s parser looks most similar to
the parses extracted from the Stanford parser. Gillenwater and UDP seem to have some problems with
the full stop. S?gaards parser mostly connects neighbors.
Baseline S?gaard Gillenwater UDP Bisk Seginer Seginer
English 53.2 59.9 64.4 55.4 70.3 55.6 (WSJ 40) 74.2 (WSJ 10)
German 33.7 57.6 35.7 52.4 68.4 38.2 (Negra 40) 48.0 (Negra 10)
Table 1: Unlabeled accuracy values of different unsupervised parsers based on the CoNLL-X shared task
(Buchholz and Marsi, 2006). Seginers results show F-measure values for the Negra and the WSJ corpus,
used with maximum sentence of lengths of 10 and 40.
An example sentence and the according parses coming from the 10M model, except for UDP, where
the 1M model is used (cf. Table 2 in Section 4.3.1), are shown in Figure 1.
Table 1 reports the accuracy of four parsers for the English and the German treebanks from the CoNLL-
X shared task (Buchholz and Marsi, 2006) predicting unlabeled dependency parses for sentences with
length equal and smaller than 10 tokens. Seginer reports only F-scores for WSJ and Negra considering
sentences with a maximum length of 10 and 40. The best baselines reported in Canisius et al. (2006) are
a left branching method for English and a nearest neighbor branching method for German, which is a
combination of left and right branching.
3.2 Computing Distributional Thesauri
The extraction of context features, used to calculate similarities between terms, is performed in accor-
dance with the generic scheme proposed in (Biemann and Riedl, 2013): A (typed or untyped) parser
arc is split into term and context feature, which consists of the edge direction and label (if any), and the
connected term. Similarity between terms is subsequently computed on the overlap of their most salient
context features. We represent the term t and the context feature c as a pair < t, c > and extract a depen-
dency triple (or dependency pair, as most unsupervised dependency parsers do not label the edges). For
the dependency between I and gave (nsub;gave;I) in I gave her the book, terms and context features
would look like <gave,(nsub,I,@)> and <I,(nsub,@,gave)>. In this example, the term gave
is characterized by the context information that I is its nominal subject, and term I is characterized by
being the subject of gave. We build distributional thesauri using the JoBimText
4
open-source framework.
This framework scales to large data and has proven to outperform other methods, when using large data
(Riedl and Biemann, 2013). The computation of the distributional thesaurus within this framework is
following the MapReduce paradigm and scales to very large corpora. This is achieved by applying a
significance measure between term and context feature, retaining only the most salient 1000 context fea-
tures per term, and computing the cardinality of the set overlap between the respective context features
4
www.jobimtext.org, (Biemann and Riedl, 2013)
1438
per term, which defines the similarity between terms. Per term, the most similar terms are subsequently
ranked, resulting in a distributional thesaurus as introduced by Lin (1997).
4 Evaluation
We report experimental results on German and English corpora. Both corpora are compiled from 10
million sentences (about 2 Gigawords) each from the Leipzig Corpora Collection
5
, randomly sampled
from online newspapers. The semantic similarity in English DTs is assessed using WordNet 3.1 as a
lexical resource, as proposed by Riedl and Biemann (2013). For evaluating the German DTs, we replace
WordNet by its German counterpart, GermaNet 8 (Hamp and Feldweg, 1997). We report results sepa-
rately for frequent and infrequent targets and average the path scores for the most similar 10 words per
entry. The evaluation of the English DTs is performed using 1000 frequent and 1000 infrequent nouns,
as previously employed by Weeds et al. (2004). These nouns are randomly sampled from the British
National Corpus (BNC) and all occur in WordNet. For the evaluation of German DTs, we randomly
selected 1000 frequent and 1000 infrequent nouns from our German corpus that all occur in GermaNet.
4.1 Experimental Settings
The DTs are calculated using the dependencies from the unsupervised parsers, one at a time. To show
the impact of corpus size, we down-sampled our corpora, and used 1 million (1M), 100,000 (100K) and
10,000 (10K) sentences (raw or automatically POS-tagged with the TreeTagger
6
) for training/inducing
the parsers. Not all parsers were able to deal with the large training sets in feasible runtime, which might
either be due to their computational complexity or their implementation. While it would be preferable to
keep the corpus size for DT computations constant, this was not possible since some of our unsupervised
parsers cannot be applied to unseen text. Hence, we decided to report DT quality results for two setups:
Setup A uses the same data for training the parsers and the DT computation. Setup B uses the full
corpus of 10M sentences for DT computation, parsed with unsupervised parsers induced on differently
sized corpus samples. We feel that Setup B is better reflecting the possible utilization of unsupervised
parsers for semantic similarity, since DT quality is known to increase with corpus size. However, we
still wanted to assess the quality of parsers that cannot be operated on unseen text in their current state
of development.
4.2 Four Baselines
We compare the results of unsupervised parsers against four baselines. As a lower-bound baseline, we
use a random dependency parser that connects each word in a sentence with a randomly chosen other
word. As a supervised upper-bound baseline, we use Stanford collapsed dependencies (Marneffe et al.,
2006) for the English data and dependencies coming from the Mate tools (Bohnet, 2010) for the German
corpus. Finally, to gauge whether the potential of unsupervised parsing to model long-range depen-
dencies ? as opposed to local n-gram contexts ? lead to better distributional similarities, we use word
bigrams and trigrams as n-gram-based systems. The bigram system simulates left- and right-branching.
We characterize the word in the first and in the second position of two neighboring words, which re-
sults to the following term feature pairs according to the example in Section 3.2: <I,(@,gave)> and
<gave,(I,@)>. Using the trigram, we characterize the word in the second position with the context
feature formed by the pair of words in first and third position. The term-feature pair for gave would be
<gave, (I,@,her)>.
While we expect the scores of any reasonable unsupervised parser to fall somewhere between the
lower bound and the upper bound when compared in the same setting, the n-gram baselines serve as a
measure for whether it is worth the trouble to induce and run the unsupervised parser for our evaluation
application, as opposed to relying on an arguably simpler system for this purpose.
5
corpora.uni-leipzig.de, (Richter et al., 2006)
6
www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/, (Schmid, 1997)
1439
4.3 Results
4.3.1 Single Parser Results for English
We summarize the results for the English evaluation for Setup A and Setup B in Table 2. All unsu-
pervised parsers beat the random baseline in all setups, with higher improvements observed using more
training data, which somewhat validates their approaches. Also, more data for DT computation results
in higher similarity scores, and rare words generally receive lower scores on average, which is expected
and validates the DT computation framework.
10k 100k 1M 10M
Parser freq rare freq rare freq rare freq rare
Setup A
Random 0.115 0.029 0.128 0.082 0.145 0.103 0.159 0.113
Trigram 0.133 0.020 0.179 0.082 0.200 0.120 0.236 0.151
Bigram 0.140 0.029 0.173 0.088 0.208 0.129 0.246 0.159
Stanford 0.151 0.028 0.209 0.128 0.261 0.176 0.280 0.209
Seginer 0.136 0.027 0.176 0.085 0.211 0.127 0.240 0.155
Gillenwater 0.135 0.026 0.159 0.077 0.195 0.117 0.223 0.141
S?gaard 0.120 0.027 0.147 0.083 0.185 0.117 0.227 0.144
UDP 0.127 0.017 0.169 0.063 0.204 0.119 * *
Bisk 0.118 0.017 * * * * * *
Setup B
Seginer 0.200 0.063 0.236 0.139 0.241 0.156 0.240 0.155
Gillenwater 0.220 0.140 0.221 0.142 0.221 0.141 0.223 0.141
S?gaard 0.227 0.144 0.227 0.144 0.227 0.144 0.227 0.144
Bisk 0.220 0.139 * * * * * *
Table 2: Setup A English: Parser induction and DT computation on the same corpus. Wordnet path
scores averaged on top 10 similar words, for 1000 frequent and 1000 rare nouns. A * denotes that the
evaluation failed because of computational constraints. Setup B English: Parser induction on different
corpus sizes, and DT computation on 10M sentences.
In comparison to the n-gram baselines, only the parser by Seginer yields a higher score for frequent
words and 1M sentences training in Setup A. However, the difference is very small and is confirmed on
the 10M sentences only in comparison to the Trigram baseline. It seems that Seginer?s training procedure
saturates somewhere between 100K and 1M sentences, and shows even slightly worse performance on
10M sentences of training in Setup B. All parsers do not seem to be particularly useful as preprocessing
steps for DT computation, since better similarity can consistently be reached by context features based
on bigram statistics.
Comparing the unsupervised parsers, we note that Seginer?s approach consistently scores highest in
Setup A, while UDP comes in second for frequent words but not for rare words. While Gillenwater?s
approach reaches comparably high scores for small corpora in Setup A, it is beaten by S?gaard?s no-
training approach for larger corpora: It seems that Gillenwater?s training procedure can hardly make use
of additional training, which is shown in Setup B, where practically no differences are observed between
10K and 10M sentences of parser training. Differences in Setup A are thus solely due to increased corpus
size for DT computation for the Gillenwater experiments.
UDP did not finish parsing 10M sentences after running for 157 days, and it is not trivial to disable
its update procedure, which is why we could not include UDP in Setup B. Bisk?s parser is a special
case in this evaluation, since it only selects sentences shorter than 15 tokens for training, and hence was
effectively trained on a 5400 sentence subset of the 10K corpus. While we did not manage to train it on
larger corpora, we could apply this model on 10M sentences in Setup B, where it lands slightly below
the no-training S?gaard parser, but clearly above Seginer?s approach for 10K training.
4.3.2 Single Parser Results for German
A different picture is drawn for the German evaluation (see Setup A in Table 3). Comparing the results
of the unsupervised parsers, Seginer?s parser does not only outperform the trigram and bigram baseline
for frequent nouns but also the supervised Mate parser for all corpus sizes. Yet, the improvements over
1440
the Mate parser are not significant for all results using a paired t-test
7
. Also, S?gaards parser exceeds the
trigram and bigram baseline for 10 million sentences. The remaining unsupervised parsers can beat the
random baseline for frequent nouns but none of the n-gram baselines. Again we are not able to parse the
10 million sentences using UDP and also Gillenwater?s parser failed, parsing this corpus. Comparing the
baselines in Setup A (see Table 3), we observe a difference between the sophisticated baselines and the
random baseline only for frequent words.
10k 100k 1M 10M
Parser freq rare freq rare freq rare freq rare
Setup A
Random 0.097 0.002 0.108 0.010 0.123 0.051 0.143 0.077
Trigram 0.102 0.002 0.130 0.014 0.159 0.067 0.179 0.086
Bigram 0.112 0.003 0.130 0.009 0.163 0.053 0.192 0.082
Mate 0.111 0.004 0.126 0.014 0.170 0.027 0.204 0.090
Seginer 0.113? 0.002 0.137? 0.012 0.171 0.068 0.208 0.091
Gillenwater 0.104 0.002 0.118 0.009 0.132 0.040 * *
S?gaard 0.104 0.002 0.123 0.010 0.161 0.054 0.193 0.077
UDP 0.107 0.001 0.129 0.004 0.151 0.021 * *
Bisk 0.101 0.002 * * * * * *
Setup B
Seginer 0.153 0.004 0.186 0.021 0.200 0.092 0.208 0.091
Gillenwater 0.189 0.080 0.190 0.082 0.189 0.080 * *
S?gaard 0.193 0.077 0.193 0.077 0.193 0.077 0.193 0.077
Bisk 0.185 0.069 * * * * * *
Table 3: Setup A and B for German corpora.
Furthermore, we see that the supervised Mate parser results in worse scores for the frequent nouns
using the 10k and 100k dataset in comparison to the bigram baseline. This could be attributed to the
heavier tail in German?s word frequency distribution, which results in sparser context features for small
data
8
. For the 1M and 10M datasets, the supervised parser yields the best similarities for frequent nouns.
The results for Setup B for the German corpora are shown at the bottom in Table 3. We observe
similar trends to the ones for the English data: using more data for the training does not seem to help the
performance of Gillenwater?s algorithm. Noticeable is the increase of Seginer?s results for rare words as
training data size increases. Seginer?s algorithm even beats both n-gram baselines for the 10M corpus
when trained only on 1 million sentences.
4.3.3 Combining Parsers for DT Quality Improvement
To clarify the best practice for building a DT of high quality, we combine different parsers: the two
best-performing unsupervised parsers (S?gaard?s and Seginer?s), the baselines and the supervised parser.
Additionally, these two parsers where the only ones which could be applied to the largest dataset for both
languages.
For English (see Table 4), we observe a boost in performance when combining unsupervised parsers.
Combining the supervised Stanford parser with the bigram and the trigram baselines also leads to a sig-
nificant improvement (p < 0.01)
9
over the Stanford parser alone, which is about the same as combining
the supervised parser with the two unsupervised parsers, and combining all five types of features for
DT construction. Overall, a relative improvement of 3.5% on the average WordNet::Path measure for
frequent words and a relative 4% improvement for rare words is obtained over the Stanford parser alone.
The results for German (see Table 5) show a similar trend. It is remarkable that merging the two un-
supervised parsers already outperforms the supervised Mate parser significantly
9
with p < 0.01 (6.7%
for frequent and 8% relative improvement for rare words). The combination of the supervised and unsu-
pervised parsers again leads to further improvement, which is also significant over the supervised parser
alone, and again, adding the bigram and trigram baselines to the three parsers does not help.
7
Significant improvements (p < 0.01) against the Mate parser are marked with the symbol ? in Table 3 for frequent nouns.
8
Within the 10M sentences, there are 22 million word types in the German corpus and 10 million word types in the English
corpus.
9
We use a paired t-test to compare the DTs built using the supervised parser and the combinations.
1441
Parser frequent rare
Stanford (supervised) 0.280 0.209
Seginer 0.240 0.155
S?gaard 0.227 0.144
Seginer & S?gaard 0.248 0.162
Stanford & Bigram & Trigram 0.290? 0.217?
Stanford & Seginer & S?gaard 0.291? 0.217?
Stanford & Seginer & S?gaard &
Bigram & Trigram
0.290? 0.218?
Table 4: Combinations of different parsers
for computing English thesauri. The cross (?)
indicates significant improvements over the
supervised parser.
Parser frequent rare
Mate (supervised) 0.204 0.090
Seginer 0.208 0.091
S?gaard 0.193 0.077
Seginer & S?gaard 0.218? 0.097?
Mate & Bigram & Trigram 0.204 0.091
Mate & Seginer & S?gaard 0.222? 0.100?
Mate & Seginer & S?gaard &
Bigram & Trigram
0.222? 0.100?
Table 5: Combinations of different parsers
for computing German thesauri
4.3.4 Discussion
Overall, it is surprising how well S?gaard?s parser performs in comparison to others on this task, since it
neither uses training nor relies on POS tags. This hints at either unsupervised parsing being simpler than
commonly assumed or rather the immaturity of all unsupervised parsers tested. Further, we would have
expected that trained unsupervised parsers, as most unsupervised methods, would exhibit a better per-
formance when trained on larger corpora. This could not be confirmed for both systems that we trained
on various corpus sizes, i.e. Seginer?s and Gillenwater?s approach. The findings are only moderately
correlated with evaluations on treebanks, cf Table 1: Whereas Seginer?s and S?gaard?s parsers perform
favorably in our evaluation, they are outperformed by Bisk?s parser on treebanks, which currently does
not scale to large data. Gillenwater?s parser seems to be overly tuned to English treebanks, but cannot
capitalize on this in our evaluation for English.
POS information does not seem beneficial for unsupervised parser induction in noun similarity evalu-
ation, since the highest-scoring approach by Seginer does not use POS tags and a version of S?gaard?s
parser with POS tags scored slightly but consistently lower than the version without POS, as we found
in further experiments. This is in line with the findings of Cramer (2007), who reports no benefit from
manually corrected or unsupervised POS tags for a range of unsupervised parsers.
Comparing the results of previous intrinsic evaluations (see Table 1) and the results of our extrinsic
evaluation (see Table 2 and 3), we observe that the ranking of parsers is only mildly correlated. Thus,
our proposed evaluation covers different aspects than the adherence to (partially arbitrary) conventions
of manually labeled dependency data. Also, our current evaluation disregards all arcs that do not involve
nouns.
When combining parsers, we observe that we can improve the quality of DTs significantly. This leads
us to conclude that unsupervised parsers should at least be used for generating features when computing
distributional thesauri of high quality. In case no high-quality supervised parser is available for the
language or domain of interest, it might suffice to use combinations of unsupervised parsers.
We also report the computation times of the different parsers, for the English dataset for Setup A (see
Table 6). The results were computed on a server with 80 GB and 16 cores. Whereas all parsers require
different amounts of memory, all parsers are single-threaded
10
. While S?gaard?s parser is the fastest
for small datasets, Seginer?s runs faster on 10 million sentences. Whereas Gillenwater?s and Seginer?s
10k 100k 1M 10M
Seginer 210 224 261 508
Gillenwater 3248 3248 3280 5546
S?gaard 3 21 182 975
UDP 183 1220 11316 -
Table 6: Computation time in minutes for parsing the data according to the English corpora used in Setup
A, cf. Table 2
10
As S?gaards algorithm parses sentence-wise without storing any information, it could be easily run multi-threaded.
1442
algorithm require almost the same time for parsing 10k, 100k or 1M sentences, the runtime of the UDP
and S?gaard?s parser is linear in time with the number of sentences to be parsed. We cannot report the
parsing times for the Bisk algorithm, as the parsing was not performed by us. Again it is noticeable that
the best two parsers are also the two unsupervised parsers that run quickest.
5 Conclusion
The contribution of this paper is two-fold: First, we have proposed and conducted a comparative extrin-
sic evaluation procedure for unsupervised parsers based on noun similarity in DTs. Second, we have
explored how to improve DT quality by combining features from several parsers. The transparency of
this method with respect to the kind of induced structures (dependencies, constituent trees, combinatory
categorial grammar) and with respect to labels of nodes and edges in the parse graph makes it possible to
compare different unsupervised parsers without having to rely on treebanks. Since semantic similarity,
especially for nouns, is a building block for many NLP applications, we feel that removing the depen-
dency on high-quality supervised parsers can give rise to semantic technologies in many languages. We
have conducted this evaluation with five different unsupervised parsers, and examined the influence of
corpus size for parser training and for the similarity computation in a series of experiments. Using estab-
lished methods for evaluating distributional similarity against lexical semantic resources, we were able
to measure differences between parsers in this task that are not reflected by intrinsic evaluations that
compare their induced structures to treebanks. These include the influence of corpus size on the training
procedure and the consistency of parse fragments on ?frequent versus rare words? as well as different
languages. Further, we were able to pinpoint a crucial point in unsupervised parsers that has not received
much attention: approaches that do not induce an actual parser that can be run on unseen sentences but
merely produce syntactic annotations for a given fixed training corpus are hardly useful in applications.
Our evaluation results can be summarized as follows: For English, with its relatively fixed order,
Seginer?s parser achieves very scarce to no improvements compared to a simple n-gram baseline when
used to compute distributional similarities. But for German, Seginer?s parser outperforms all baselines
including a state-of-the-art supervised parser, and S?gaard?s simplistic approach compares favorably to
the n-gram baselines. Furthermore, we demonstrate that the quality of noun similarity can be improved
significantly when combining the features from supervised and unsupervised parsers.
While today?s unsupervised parsers might not be ready for their utilization for semantic similarity for
the English language, they can be applied to a large number of other languages where highly optimized
supervised parsers are not available. Additionally, we feel that our proposed evaluation method exhibits
enough sensitivity to be a meaningful test bed for future unsupervised parsers.
6 Outlook
Where do we go from here? We strongly argue that in times of availability of very large monolingual
corpora from the web, we should strive at unsupervised parser induction systems that can make use of
large training data, as opposed to focussing our efforts on complex models that scale poorly, and thus
cannot elevate to the performance levels needed in order to make unsupervised parsing a building block
in natural language processing applications.
For further work, we want to proceed in several ways: we would like to extend our evaluation frame-
work from nouns to other parts of speech. Furthermore, we will explore whether unsupervised parsers
can be tuned towards the task of computing a distributional thesaurus, e.g. by using only assignments
with a certain confidence, type, or from sentences with limited length. Additionally, we would like to ex-
plore the interaction of unsupervised POS induction and grammar induction (Headden, III et al., 2008),
in order to entirely remove language-dependent preprocessing for the purpose of semantic similarity
computations, while at the same time being able to leverage the advantages of structured representations,
cf. Erk and Pad?o (2008). Finally, we would like to test whether we can also detect a different ranking
for different supervised parsers when comparing their scores in the normal treebank setting versus using
them for building distributional thesauri.
1443
Acknowledgments
This work has been supported by the German Federal Ministry of Education and Research (BMBF)
within the context of the Software Campus project LiCoRes under grant No. 01IS12054, by IBM under
a Shared University Research Grant and by DFG under the SemSch project grant.
References
Chris Biemann and Martin Riedl. 2013. Text: Now in 2D! A Framework for Lexical Expansion with Contextual
Similarity. Journal of Language Modelling, 1(1):55?95.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP Model for Inducing Combinatory Categorial Grammars. In
Transactions of the Association for Computational Linguistics, pages 75?88, Atlanta, GA, USA.
Rens Bod. 2007. Is the end of supervised parsing in sight? In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 400?407, Prague, Czech Republic.
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics, COLING ?10, pages 89?97, Beijing, China.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ?06, pages 149?164, New
York City, New York.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating the impact of alternative dependency graph encodings on solv-
ing event extraction tasks. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 982?992, Cambridge, Massachusetts.
Sander Canisius, Toine Bogers, Antal van den Bosch, Jeroen Geertzen, and Erik Tjong Kim Sang. 2006. Depen-
dency parsing by inference over high-recall dependency predictions. In Proceedings of the Tenth Conference on
Computational Natural Language Learning, CoNLL-X ?06, pages 176?180, New York City, New York.
Bart Cramer. 2007. Limitations of Current Grammar Induction Algorithms. In Proceedings of the ACL 2007
Student Research Workshop, pages 43?48, Prague, Czech Republic.
James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acquisition - Volume 9, ULA ?02, pages 59?66, Philadelphia,
Pennsylvania, USA.
James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,
Honolulu, Hawaii.
Jennifer Gillenwater, Kuzman Ganchev, Jo?ao Grac?a, Fernando Pereira, and Ben Taskar. 2010. Sparsity in depen-
dency grammar induction. In Proceedings of the ACL 2010 Conference Short Papers, pages 194?199, Uppsala,
Sweden.
Amit Goyal and Hal Daum?e, III. 2011. Generating semantic orientation lexicon using large data and thesaurus.
In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,
WASSA ?11, pages 37?43, Portland, Oregon, USA.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a Lexical-Semantic Net for German. In In Proceedings of
ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Appli-
cations, pages 9?15, Madrid, Spain.
William P. Headden, III, David McClosky, and Eugene Charniak. 2008. Evaluating unsupervised part-of-speech
tagging for grammar induction. In Proceedings of the 22nd International Conference on Computational Lin-
guistics - Volume 1, COLING ?08, pages 329?336, Manchester, United Kingdom.
William P Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109,
Boulder, CO, USA.
1444
Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling.
In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ?08,
pages 393?400, Manchester, United Kingdom.
Dan Klein and Christopher D Manning. 2002. A generative constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages
128?135, Philadelphia, PA,USA.
Dan Klein and Christopher D Manning. 2004. Corpus-based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational
Linguistics, pages 478?485, Barcelona, Spain.
Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the
European Chapter of the Association for Computational Linguistics, ACL ?98, pages 64?71, Madrid, Spain.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics - Volume 2, COLING ?98, pages 768?774, Montreal, Quebec, Canada.
David Marecek and Milan Straka. 2013. Stop-probability estimates computed on a large corpus improve Unsu-
pervised Dependency Parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics, pages 281?290, Sofia, Bulgaria.
Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the International Conference on Language Resources
and Evaluation, LREC 2006, pages 449?454, Genova, Italy.
George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38:39?41.
Yusuke Miyao, Rune Stre, Kenji Sagae, Takuya Matsuzaki, and Jun?ichi Tsujii. 2008. Task-oriented evaluation
of syntactic parsers and their representations. In Proceeding of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 46?54, Columbus, Ohio.
Yasaman Motazedi, Mark Dras, and Franc?ois Lareau. 2012. Is bad structure better than no structure?: Unsuper-
vised parsing for realisation ranking. In Proceedings of the 24th International Conference on Computational
Linguistics, COLING ?12, pages 1811?1830, Mumbai,India.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to
guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 1234?1244, Cambridge, MA, USA.
Joakim Nivre and Sandra K?ubler. 2006. Dependency parsing. In Tutorial at COLING-ACL, Sydney, Australia.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas. 2009. Web-scale distri-
butional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume 2, EMNLP ?09, pages 938?947, Singapore.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity: measuring the relatedness
of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL?Demonstrations ?04, pages 38?41,
Boston, Massachusetts, USA.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsd?ottir, and Chris Biemann. 2006. Exploiting the Leipzig Corpora
Collection. In Proceedings of the IS-LTC 2006, pages 68?73, Ljubljana, Slovenia.
Martin Riedl and Chris Biemann. 2013. Scaling to large
3
data: An efficient and effective method to compute
distributional thesauri. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2013, pages 884?890, Seattle, WA, USA.
Helmut Schmid. 1997. Probabilistic part-of-speech tagging using decision trees. In Daniel Jones and Harold
Somers, editors, New Methods in Language Processing, Studies in Computational Linguistics, pages 154?164.
UCL Press, London, GB.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing evaluation. In Proceedings of the 49nd Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pages 663?672, Portland, Oregon,
USA.
1445
Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 384?391, Prague, Czech Republic.
Anders S?gaard. 2012. Unsupervised dependency parsing without training. Natural Language Engineering,
18(02):187?203.
Menno van Zaanen and University of Leeds. School of Computer Studies. 2001. Building Treebanks Using a
Grammar Induction System. Research report series. University of Leeds, School of Computer Studies.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of the 20th international conference on Computational Linguistics, COLING ?04, pages 1015?
1021, Geneva, Switzerland.
1446
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 884?890,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Scaling to Large3 Data: An efficient and effective method
to compute Distributional Thesauri
Martin Riedl and Chris Biemann
FG Language Technology
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biem}@cs.tu-darmstadt.de
Abstract
We introduce a new highly scalable approach
for computing Distributional Thesauri (DTs).
By employing pruning techniques and a dis-
tributed framework, we make the computation
for very large corpora feasible on comparably
small computational resources. We demon-
strate this by releasing a DT for the whole vo-
cabulary of Google Books syntactic n-grams.
Evaluating against lexical resources using two
measures, we show that our approach pro-
duces higher quality DTs than previous ap-
proaches, and is thus preferable in terms of
speed and quality for large corpora.
1 Introduction
Using larger data to estimate models for machine
learning applications as well as for applications of
Natural Language Processing (NLP) has repeatedly
shown to be advantageous, see e.g. (Banko and
Brill, 2001; Brants et al, 2007). In this work, we
tackle the influence of corpus size for building a
distributional thesaurus (Lin, 1998). Especially, we
shed light on the interaction of similarity measures
and corpus size, as well as aspects of scalability.
We shortly introduce the JoBimText framework
for distributional semantics and show its scalability
for large corpora. For the computation of the data
we follow the MapReduce (Dean and Ghemawat,
2004) paradigm. The computation of similarities
between terms becomes challenging on large cor-
pora, as both the numbers of terms to be compared
and the number of context features increases. This
makes standard similarity calculations as proposed
in (Lin, 1998; Curran, 2002; Lund and Burgess,
1996; Weeds et al, 2004) computationally infeasi-
ble. These approaches first calculate an informa-
tion measure between each word and the accord-
ing context and then calculate the similarity between
all words, based on the information measure for all
shared contexts.
2 Related Work
A variety of approaches to compute DTs have been
proposed to tackle issues regarding size and run-
time. The reduction of the feature space seems to
be one possibility, but still requires the computation
of such reduction cf. (Blei et al, 2003; Golub and
Kahan, 1965). Other approaches use randomised in-
dexing for storing counts or hashing functions to ap-
proximate counts and measures (Gorman and Cur-
ran, 2006; Goyal et al, 2010; Sahlgren, 2006). An-
other possibility is the usage of distributed process-
ing like MapReduce. In (Pantel et al, 2009; Agirre
et al, 2009) a DT is computed using MapReduce
on 200 quad core nodes (for 5.2 billion sentences)
respectively 2000 cores (1.6 Terawords), an amount
of hardware only available to commercial search en-
gines. Whereas Agirre uses a ?2 test to measure the
information between terms and context, Pantel uses
the Pointwise Mutual Information (PMI). Then, both
approaches use the cosine similarity to calculate the
similarity between terms. Furthermore, Pantel de-
scribes an optimization for the calculation of the co-
sine similarity. Whereas Pantel and Lin (2002) de-
scribe a method for sense clustering, they also use
a method to calculate similarities between terms.
Here, they propose a pruning scheme similar to ours,
but do not explicitly evaluate its effect.
The evaluation of DTs has been performed in ex-
trinsic and intrinsic manner. Extrinsic evaluations
have been performed using e.g. DTs for automatic
884
set expansion (Pantel et al, 2009) or phrase polar-
ity identification (Goyal and Daume?, 2011). In this
work we will concentrate on intrinsic evaluations:
Lin (1997; 1998) introduced two measures using
WordNet (Miller, 1995) and Roget?s Thesaurus. Us-
ing WordNet, he defines context features (synsets a
word occurs in Wordnet or subsets when using Ro-
get?s Thesaurus) and then builds a gold standard the-
saurus using a similarity measure. Then he evaluates
his generated Distributional Thesaurus (DT) with re-
spect to the gold standard thesauri. Weeds et al
(2004) evaluate various similarity measures based
on 1000 frequent and 1000 infrequent words. Curran
(2004) created a gold standard thesaurus by manu-
ally extracting entries from several English thesauri
for 70 words. His automatically generated DTs are
evaluated against this gold standard thesaurus using
several measures. We will report on his measure and
additionally propose a measure based on WordNet
paths.
3 Building a Distributional Thesaurus
Here we present our scalable DT algorithm using
the MapReduce paradigm, which is divided into
two parts: The holing system and a computational
method to calculate distributional similarities. A
more detailed description, especially for the MapRe-
duce steps, can be found in (Biemann and Riedl,
2013).
3.1 Holing System
The holing operation splits an observation (e.g. a
dependency relation) into a pair of two parts: a
term and a context feature. This captures their first-
order relationship. These pairs are subsequently
used for the computation of the similarities between
terms, leading to a second-order relation. The rep-
resentation can be formalized by the pair <x,y>
where x is the term and y represents the context
feature. The position of x in y is denoted by the
hole symbol ?@?. As an example the dependency
relation (nsub;gave2;I1) could be transferred to
<gave2,(nsub;@;I1)> and <I1,(nsub;gave2;@)>.
This representation scheme is more generic then the
schemes introduced in (Lin, 1998; Curran, 2002),
as it allows to characterise pairs by several holes,
which could be used to learn analogies, cf. (Turney
and Littman, 2005).
3.2 Distributional Similarity
First, we count the frequency for each first-order
relation and remove all features that occur with
more than w terms, as these context features tend to
be too general to characterise the similarity between
other words (Rychly? and Kilgarriff, 2007; Goyal
et al, 2010, cmp.). From this. we calculate a sig-
nificance score for all first-order relations. For this
work, we implemented two different significance
measures: Pointwise Mutual Information (PMI):
PMI(term, feature) = log2(
f(term,feature)
f(term)f(feature))
(Church and Hanks, 1990) and Lexicographer?s Mu-
tual Information (LMI): LMI(term, feature) =
f(term, feature) log2(
f(term,feature)
f(term)f(feature)) (Evert,
2005).
We then prune all negatively correlated pairs
(s<0). The maximum number of context features
per term are defined with p, as we argue that it is
sufficient to keep only the p most salient (ordered
descending by their significance score) context fea-
tures per term. Features of low saliency generally
should not contribute much to the similarity of terms
and also could lead to spurious similarity scores. Af-
terwards, all terms are aggregated by their features,
which allows us to compute similarity scores be-
tween all terms that share at least one such feature.
Whereas the method introduced by (Pantel and
Lin, 2002) is very similar to the one proposed in
this paper (the similarity between terms is calculated
solely by the number of features two terms share),
they use PMI to rank features and do not use pruning
to scale to large corpora, as they use a rather small
corpus. Additionally, they do not evaluate the effect
of such pruning.
In contrast to the best measures proposed by Lin
(1998; Curran (2002; Pantel et al (2009; Goyal et
al. (2010) we do not calculate any information mea-
sure using frequencies of features and terms (we use
significance ranking instead), as shown in Table 1.
Additionally, we avoid any similarity measure-
ment using the information measure, as also done in
these approaches, to calculate the similarity over the
feature counts of each term: we merely count how
many salient features two terms share. All these con-
straints makes this approach more scalable to larger
corpora, as we do not need to know the full list of
885
Information Measures
Lin?s formula I(term, feature) = lin(term, feature) = log f(term,feature)?f(relation(feature))P(f(word,relation(feature))f(word)
Curran?s TTest I(term, feature) = ttest(term, feature) = p(term,feature)?p(feature)?p(term)?
p(feature)?p(term)
Similarity Measures
Lin?s formula sim(t1, t2) =
P
f?features(t1)?features(t2)
(I(t1,f)+I(t2,f))
P
f?features(t1)
I(t1,f)+
P
f?features(w2)
I(w2,f)
Curran?s Dice sim(t1, t2) =
P
f?features(t1)?features(t2)
min(I(t1,f),I(t2,f))
P
f?features(t1)?features(t2)
(I(t1,f)+I(t2,f))
with I(t, f) > 0
Our Measure sim(t1, t2) =
?
f?features(t1)?features(t2)
1 with s > 0
Table 1: Similarity measures used for computing the distributional similarity between terms.
features for a term pair at any time. While our com-
putations might seem simplistic, we demonstrate its
adequacy for large corpora in Section 5.
4 Evaluation
The evaluation is performed using a recent dump of
English Wikipedia, containing 36 million sentences
and a newspaper corpus, compiled from 120 million
sentences (about 2 Gigawords) from Leipzig Cor-
pora Collection (Richter et al, 2006) and the Giga-
word corpus (Parker et al, 2011). The DTs are based
on collapsed dependencies from the Stanford Parser
(Marneffe et al, 2006) in the holing operation. For
all DTs we use the pruning parameters s=0, p=1000
and w=1000. In a final evaluation, we use the syn-
tactic n-grams built from Google Books (Goldberg
and Orwant, 2013).
To show the impact of corpus size, we down-
sampled our corpora to 10 million, 1 million and
100,000 sentences. We compare our results against
DTs calculated using Lin?s (Lin, 1998) measure and
the best measure proposed by Curran (2002) (see Ta-
ble 1).
Our evaluation is performed using the same 1000
frequent and 1000 infrequent nouns as previously
employed by Weeds et al (2004). We create a gold
standard, by extracting reasonable entries of these
2000 nouns using Roget?s 1911 thesaurus, Moby
Thesaurus, Merriam Webster?s Thesaurus, the Big
Huge Thesaurus and the OpenOffice Thesaurus and
employ the inverse ranking measure (Curran, 2002)
to evaluate the DTs.
Furthermore, we introduce a WordNet-based
method. To calculate the similarity between two
terms, we use the WordNet::Similarity path (Peder-
sen et al, 2004) measure. While its absolute scores
are hard to interpret due to inhomogenity in the gran-
ularity of WordNet, they are well-suited for relative
comparison. The score between two terms is in-
versely proportional to the shortest path between all
the synsets of both terms. The highest possible score
is one, if two terms share a synset. We compare the
average score of the top five (or ten) entries in the
DT for each of the 2000 selected words for our com-
parison.
5 Results
First, we inspect the results of Curran?s measure us-
ing the Wikipedia and newspaper corpus for the fre-
quent nouns, shown in Figure 1.
Both graphs show the inverse ranking score
against the size of the corpus. Our method scores
consistently higher when using LMI instead of PMI
for ranking the features per term. The PMI measure
declines when the corpus becomes larger. This can
be attributed to the fact that PMI favors term-context
pairs involving rare contexts (Bordag, 2008). Com-
puting similarities between terms should not be per-
formed on the basis of rare contexts, as these do not
generalize well because of their sparseness.
All other measures improve with larger corpora.
It is surprising that recent works use PMI to calcu-
late similarities between terms (Goyal et al, 2010;
Pantel et al, 2009), who, however evaluate their ap-
proach only with respect to their own implementa-
tion or extrinsically, and do not prune on saliency.
Apart from the PMI measure, Curran?s measure
leads to the weakest results. We could not confirm
that his measure outperforms Lin?s measure as stated
in (Curran, 2002)1. An explanation for this results
1Regarding Curran?s Dice formula, it is not clear whether to
use the intersection or the union of the features. We use an inter-
section, as it is unclear how to interpret the minimum function
otherwise, and the alternatives performed worse.
886
Figure 1: Inverse ranking for 1000 frequent nouns (Wikipedia left, Newspaper right) for different sized corpora. The
4 lines represent the scores of following DTs: our method using LMI (dashed black line) and the PMI significance
measure (solid black line) and Curran?s (dash bray line) and Lin?s measure (solid tray line).
might be the use of a different parser, very few test
words and also a different gold standard thesaurus
in his evaluation. Comparing our method using LMI
to Lin?s method, we achieve lower scores with our
method using small corpora, but surpass Lin?s mea-
sure from 10 million sentences onwards.
Next, we show the results of the WordNet eval-
uation measure in Figure 2. Comparing the top 10
(upper) to the top 5 words (lower) used for the eval-
uation, we can observe higher scores for the top 5
words, which validates the ranking. These results
are highly correlated to the results achieved with the
inverse ranking measure. This is a positive result,
as the WordNet measure can be performed automat-
ically using a single public resource2. In Figure 3,
we show results for the 1000 infrequent nouns using
the inverse ranking (upper) and the WordNet mea-
sure (lower).
We can see that our method using PMI does not
decline for larger corpora, as the limit on first-order
features is not reached and frequent features are still
being used. Comparing our LMI DT is en par with
Lin?s measure for 10 million sentences, and makes
better use of large data when using the complete
dataset. Again, the inverse ranking and the Word-
Net Path measure are highly correlated.
2Building a gold standard thesaurus following Curran
(2002) needs access to all the used thesauri. Whereas for some,
programming interfaces exist, often with limited access and li-
cence restrictions, others have to be extracted manually.
Figure 2: Results, using the WordNet:Path measure for
frequent nouns using the newspaper corpus.
887
Figure 3: WordNet::Path results for 1000 infrequent
nouns
The results shown here validate our pruning ap-
proach. Whereas Lin and Curran propose ap-
proaches to filter features that have low word feature
scores, they do not remove features that occur with
too many words, which is done in this work. Using
these pruning steps, a simplistic similarity measure
does not only lead to reduced computation times, but
also to better results, when using larger corpora.
5.1 Using a large3 corpus
We demonstrate the scalability of our method using
the very large Google Books dataset (Goldberg and
Orwant, 2013), consisting of dependencies extracted
from 17.6 billion sentences. The evaluation results,
using different measures, are given in Table 2.
Comparing the results for the Google Books DT
to the ones achieved using Wikipedia and the news-
Corpus Inv. P@1 Path@5 Path@10
frequent
nouns
Newspaper 2.0935 0.709 0.3277 0.2906
Wikipedia 2.1213 0.703 0.3365 0.2968
Google Books 2.3171 0.764 0.3712 0.3217
infrequent
nouns
Newspaper 1.4097 0.516 0.2577 0.2269
Wikipedia 1.3832 0.514 0.2565 0.2265
Google Books 1.8125 0.641 0.2989 0.2565
Table 2: Comparing results for different corpora.
paper, we can observe a boost in the performance,
both for the inverse ranking and the WordNet mea-
sures. Additionally, we show results for the P@1
measure, which indicates the percentage of entries,
whose first entry is in the gold standard thesaurus.
Remarkably, we get a P@1 against our gold stan-
dard thesaurus of 76% for frequent and 64% for in-
frequent nouns using the Google Books DT.
The most computation time was needed for the
dependency parsing and took two weeks on a small
cluster (64 cores on 8 nodes) for the 120 million
Newspaper sentences. The DT for the Google Books
was calculated in under 30 hours on a Hadoop clus-
ter (192 cores on 16 nodes) and could be calculated
within 10 hours for the Newspaper corpus. The com-
putation of a DT using this huge corpus would be in-
tractable with standard vector-based measurements.
Even computing Lin?s and Curran?s vector-based
similarity measure for the whole vocabulary of the
newspaper corpus was not possible with our Hadoop
cluster, as too much memory would have been re-
quired and thus we computed similarities only for
the 2000 test nouns on a server with 92GB of main
memory.
6 Conclusion
We have introduced a highly scalable approach
to DT computation and showed its adequacy for
very large corpora. Evaluating against thesauri and
WordNet, we demonstrated that our similarity mea-
sure yields better-quality DTs and scales to corpora
of billions of sentences, even on comparably small
compute clusters. We achieve this by a number of
pruning operations, and distributed processing. The
framework and the DTs for Google Books, News-
paper and Wikipedia are available online3 under the
ASL 2.0 licence.
3https://sf.net/projects/jobimtext/
888
Acknowledgments
This work has been supported by the Hessian re-
search excellence program ?Landes-Offensive zur
Entwicklung Wissenschaftlich-konomischer Exzel-
lenz? (LOEWE) as part of the research center ?Dig-
ital Humanities?. We would also thank the anony-
mous reviewers for their comments, which greatly
helped to improve the paper.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, NAACL ?09,
pages 19?27, Boulder, Colorado, USA.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting on
Association for Computational Linguistics, ACL ?01,
pages 26?33, Toulouse, France.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! a framework for lexical expansion with contextual
similarity. Journal of Language Modelling, 1(1):55?
95.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Stefan Bordag. 2008. A comparison of co-occurrence
and similarity measures as simulations of context. In
CICLing?08 Proceedings of the 9th international con-
ference on Computational linguistics and intelligent
text processing, pages 52?63, Haifa, Israel.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858?
867, Prague, Czech Republic.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
James R. Curran. 2002. Ensemble methods for au-
tomatic thesaurus extraction. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, EMNLP ?02, pages
222?229, Philadelphia, PA, USA.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified Data Processing on Large Clusters. In Pro-
ceedings of Operating Systems, Desing & Implementa-
tion (OSDI) ?04, pages 137?150, San Francisco, CA,
USA.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung, University
of Stuttgart.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of english books. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume 1:
Proceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 241?247, At-
lanta, Georgia, USA.
Gene H. Golub and William M. Kahan. 1965. Calcu-
lating the singular values and pseudo-inverse of a ma-
trix. J. Soc. Indust. Appl. Math.: Ser. B, Numer. Anal.,
2:205?224.
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, ACL-44, pages
361?368, Sydney, Australia.
Amit Goyal and Hal Daume?, III. 2011. Generating se-
mantic orientation lexicon using large data and the-
saurus. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, WASSA ?11, pages 37?43, Portland, Ore-
gon, USA.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?, III, and
Suresh Venkatasubramanian. 2010. Sketch techniques
for scaling distributional similarity to the web. In Pro-
ceedings of the 2010 Workshop on GEometrical Mod-
els of Natural Language Semantics, GEMS ?10, pages
51?56, Uppsala, Sweden.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Compu-
tational Linguistics, ACL ?98, pages 64?71, Madrid,
Spain.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics - Vol-
ume 2, COLING ?98, pages 768?774, Montreal, Que-
bec, Canada.
889
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28(2):203?
208.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation, LREC 2006, Genova, Italy.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
Edmonton, Alberta, Canada.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2
- Volume 2, EMNLP ?09, pages 938?947, Singapore.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia, PA,
USA.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Boston, Massachusetts, USA.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsdo?ttir,
and Chris Biemann. 2006. Exploiting the leipzig cor-
pora collection. In Proceedings of the IS-LTC 2006,
Ljubljana, Slovenia.
Pavel Rychly? and Adam Kilgarriff. 2007. An efficient
algorithm for building a distributional thesaurus (and
other sketch engine developments). In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, ACL ?07, pages
41?44, Prague, Czech Republic.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
pages 1015?1021, Geneva, Switzerland.
890
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 610?614,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Lexical Substitution for the Medical Domain
Martin Riedl
1
Michael R. Glass
2
Alfio Gliozzo
2
(1) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
(2) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
riedl@cs.tu-darmstadt.de, {mrglass,gliozzo}@us.ibm.com
Abstract
In this paper we examine the lexical substitu-
tion task for the medical domain. We adapt
the current best system from the open domain,
which trains a single classifier for all instances
using delexicalized features. We show sig-
nificant improvements over a strong baseline
coming from a distributional thesaurus (DT).
Whereas in the open domain system, features
derived from WordNet show only slight im-
provements, we show that its counterpart for
the medical domain (UMLS) shows a signif-
icant additional benefit when used for feature
generation.
1 Introduction
The task of lexical substitution (McCarthy and Navigli,
2009) deals with the substitution of a target term within
a sentence with words having the same meaning. Thus,
the task divides into two subtasks:
? Identification of substitution candidates, i.e.
terms that are, for some contexts, substitutable for
a given target term.
? Ranking the substitution candidates according to
their context
Such a substitution system can help for semantic text
similarity (B?ar et al., 2012), textual entailment (Dagan
et al., 2013) or plagiarism detection (Chong and Specia,
2011).
Datasets provided by McCarthy and Navigli (2009)
and Biemann (2012) offer manually annotated substi-
tutes for a given set of target words within a context
(sentence). Contrary to these two datasets in Kremer et
al. (2014) a dataset is offered where all words have are
annotated with substitutes. All the datasets are suited
for the open domain.
But a system performing lexical substitution is not
only of interest for the open domain, but also for the
medical domain. Such a system could then be applied
to medical word sense disambiguation, entailment or
question answering tasks. Here we introduce a new
dataset and adapt the lexical substitution system, pro-
vided by Szarvas et al. (2013), to the medical domain.
Additionally, we do not make use of WordNet (Miller,
1995) to provide similar terms, but rather employ a Dis-
tributional Thesaurus (DT), computed on medical texts.
2 Related Work
For the general domain, the lexical substitution task
was initiated by a Semeval-2007 Task (McCarthy and
Navigli, 2009). This task was won by an unsupervised
method (Giuliano et al., 2007), which uses WordNet for
the substitution candidate generation and then relies on
the Google Web1T n-grams (Brants and Franz, 2006)
1
to rank the substitutes.
The currently best system, to our knowledge, is pro-
posed by Szarvas et al. (2013). This is a supervised ap-
proach, where a single classifier is trained using delex-
icalized features for all substitutes and can thus be ap-
plied even to previously unseen substitutes. Although
there have been many approaches for solving the task
for the general domain, only slight effort has been done
in adapting it to different domains.
3 Method
To perform lexical substitution, we follow the delex-
icalization framework of Szarvas et al. (2013). We
automatically build Distributional Thesauri (DTs) for
the medical domain and use features from the Uni-
fied Medical Language System (UMLS) ontology. The
dataset for supervised lexical substitution consists of
sentences, containing an annotated target word t. Con-
sidering the sentence being the context for the target
word, the target word might have different meanings.
Thus annotated substitute candidates s
g
1
. . . s
g
n
? s
g
,
need to be provided for each context. The negative ex-
amples are substitute candidates that either are incor-
rect for the target word, do not fit into the context or
both. We will refer to these substitutes as false substi-
tute candidates s
f
1
. . . s
f
m
? s
f
with s
f
? s
g
= ?.
For the generation of substitute candidates we do not
use WordNet, as done in previous works (Szarvas et al.,
2013), but use only substitutes from a DT. To train a
single classifier, features that distinguishing the mean-
ing of words in different context need to be considered.
Such features could be e.g. n-grams, features from dis-
tributional semantics or features which are extracted
1
http://catalog.ldc.upenn.edu/
LDC2006T13
610
relative to the target word, such as the ratio between
frequencies of the substitute candidate and the target
word. After training, we apply the algorithm to un-
seen substitute candidates and rank them according to
their positive probabilities, given by the classifier. Con-
trary to Szarvas et al. (2013), we do not use any weight-
ing in the training if a substitute has been supplied by
many annotators, as we could not observe any improve-
ments. Additionally, we use logistic regression (Fan et
al., 2008) as classifier
2
.
4 Resources
For the substitutes and for the generation of delexical-
ized features, we rely on DTs, the UMLS and Google
Web1T.
4.1 Distributional thesauri (DTs)
We computed two different DTs using the framework
proposed in Biemann and Riedl (2013)
3
.
The first DT is computed based on Medline
4
ab-
stracts. This thesaurus uses the left and the right word
as context features. To include multi-word expressions,
we allow the number of tokens that form a term to be
up to the length of three.
The second DT is based on dependencies as context
features from a English Slot Grammar (ESG) parser
(McCord et al., 2012) modified to handle medical data.
The ESG parser is also capable of finding multi-word
expressions. As input data we use 3.3 GB of texts
from medical textbooks, encyclopedias and clinical ref-
erence material as well as selected journals. This DT is
also used for the generation of candidates supplied to
annotators when creating the gold standard and there-
fore is the main resource to provide substitute candi-
dates.
4.2 UMLS
The Unified Medical Language System (UMLS) is an
ontology for the medical domain. In contrast to Szarvas
et al. (2013), which uses WordNet (Miller, 1995) to
generate substitute candidates and also for generating
features, we use UMLS solely for feature generation.
4.3 Google Web1T
We use the Google Web1T to generate n-gram features
as we expect this open domain resource to have consid-
erable coverage for most specific domains as well. For
accessing the resource, we use JWeb1T
5
(Giuliano et
al., 2007).
2
We use a Java port of LIBLINEAR (http://www.
csie.ntu.edu.tw/
?
cjlin/liblinear/) available
from http://liblinear.bwaldvogel.de/
3
We use Lexicographer?s Mutual Information (LMI) (Ev-
ert, 2005) as significance measure and consider only the top
1000 (p = 1000) features per term.
4
http://www.nlm.nih.gov/bsd/licensee/
2014_stats/baseline_med_filecount.html
5
https://code.google.com/p/jweb1t/
5 Lexical Substitution dataset
Besides the lexical substitution data sets for the open
domain (McCarthy and Navigli, 2009; Biemann, 2012;
Kremer et al., 2014) there is no dataset available that
can be used for the medical domain. Therefore, we
constructed an annotation task for the medical domain
using a medical corpus and domain experts.
In order to provide the annotators with a clear task,
we presented a question, and a passage that contains
the correct answer to the question. We restricted this to
a subset of passages that were previously annotated as
justifying the answer to the question. This is related to
a textual entailment task, essentially the passage entails
the question with the answer substituted for the focus of
the question. We instructed the annotators to first iden-
tify the terms that were relevant for the entailment rela-
tion. For each relevant term we randomly extracted 10
terms from the ESG-based DT within the top 100 most
similar terms. Using this list of distributionally similar
terms, the annotators selected those terms that would
preserve the entailment relation if substituted. This re-
sulted in a dataset of 699 target terms with substitutes.
On average from the 10 terms 0.846 are annotated as
correct substitutes. Thus, the remaining terms can be
used as false substitute candidates.
The agreement on this task by Fleiss Kappa was
0.551 indicating ?moderate agreement? (Landis and
Koch, 1977). On the metric of pairwise agreement,
as defined in the SemEval lexical substitution task, we
achieve 0.627. This number is not directly comparable
to the pairwise agreement score of 0.277 for the Se-
mEval lexical substitution task (McCarthy and Navigli,
2009) since in our task the candidates are given. How-
ever, it shows promise that subjectivity may be reduced
by casting lexical substitution into a task of maintain-
ing entailment.
6 Evaluation
For the evaluation we use a ten-fold cross validation
and report P@1 (also called Average Precision (AP) at
1) and Mean Average Precision (MAP) (Buckley and
Voorhees, 2004) scores. The P@1 score indicates how
often the first substitute of the system matches the gold
standard. The MAP score is the mean of all AP from 1
to the number of all substitutes.
? Google Web 1T:
We use the same Google n-gram features, as
used in Giuliano et al. (2007) and Szarvas et al.
(2013). These are frequencies of n-grams formed
by the substitute candidate s
i
and the left and right
words, taken from the context sentence, normal-
ized by the frequency of the same context n-gram
with the target term t. Additionally, we add the
same features, normalized by the frequency sum
of all n-grams of the substitute candidates. An-
other feature is generated using the frequencies
where t and s are listed together using the words
611
and, or and ?,? as separator and also add the left
and right words of that phrase as context. Then we
normalize this frequency by the frequency of the
context occurring only with t.
? DT features:
To characterize if t and s
i
have similar words
in common, and therefore are similar, we com-
pute the percentage of words their thesauri en-
tries share, considering the top n words in each
entry with n = 1, 5, 20, 50, 100, 200. During
the DT calculation we also calculate the signif-
icances between each word and its context fea-
tures (see Section 4.1). Using this information,
we compute if the words in the sentences also
occur as context features for the substitute can-
didate. A third feature group relying on DTs
is created by the overlapping context features
for the top m entries of t and s
i
with m =
1, 5, 20, 50, 100, 1000, which are ranked regard-
ing their significance score. Whereas, the simi-
larities between the trigram-based and the ESG-
based DT are similar, the context features are dif-
ferent. Both feature types can be applied to the
two DTs. Additionally, we extract the thesaurus
entry for the target word t and generate a feature
indicating whether the substitute s
i
is within the
top k entries with k = 1, 5, 10, 20, 100 entries
6
.
? Part-of-speech n-grams:
To identify the context of the word we use the
POS-tag (only the first letter) of s
i
and t as feature
and POS-tag combinations of up to three neigh-
boring words.
? UMLS:
Considering UMLS we look up all concept unique
identifiers (CUIs) for s
i
and t. The first two fea-
tures are the number of CUIs for s
i
and t. The next
features compute the number of CUIs that s
i
and t
share, starting from the minimal to the maximum
number of CUIs. Additionally, we use a feature
indicating that s
i
and t do not share any CUI.
6.1 Substitute candidates
The candidates for the substitution are taken from the
ESG based DT. For each target term we use the gold
substitute candidates as correct instances and add all
possible substitutes for the same target term occurring
in a different context and do not have been annotated
as valid in the present context as false instances.
7 Results
Running the experiment, we get the results as shown
in Table 1. As baseline system we use the ranking of
6
Whereas in Szarvas et al. (2013) only k = 100 is used,
we gained an improvement in performance when also adding
smaller values of k.
the ESG-based DT. As can be seen, the baseline is al-
ready quite high, which can be attributed to the fact
that this resource was used to generate substitutes und
thus contains all positive instances. Using the super-
vised approach, we can beat the baseline by 0.10 for
the MAP score and by 0.176 for the P@1 score, which
is a significant improvement (p < 0.0001, using a two
tailed permutation test). To get insights of the contri-
System MAP P@1
Baseline 0.6408 0.5365
ALL 0.7048 0.6366
w/o DT 0.5798 0.4835
w/o UMLS 0.6618 0.5651
w/o Ngrams 0.7009 0.6252
w/o POS 0.7027 0.6323
Table 1: Results for the evaluation using substitute can-
didates from the DT.
bution of individual feature types, we perform an abla-
tion test. We observe that the most prominent features
are coming from the two DTs as we only achieve re-
sults below the baseline, when removing DT features.
We still obtain significant improvements over the base-
line when removing other feature groups. The second
most important feature comes from the UMLS. Fea-
tures coming from the Google n-grams improve the
system only slightly. The lowest improvement is de-
rived from the part-of-speech features. This leads us
to summarize that a hybrid approach for feature gen-
eration using manually created resources (UMLS) and
unsupervised features (DTs) leads to the best result for
lexical substitution for the medical domain.
8 Analysis
For a better insight into the lexical substitution we ana-
lyzed how often we outperform the baseline, get equal
results or get decreased scores. According to Table 2 in
performance # of instances Avg. ? MAP
decline 180 -0.16
equal 244 0
improvements 275 0.26
Table 2: Error analysis for the task respectively to the
MAP score.
around 26% of the cases we observe a decreased MAP
score, which is on average 0.16 smaller then the scores
achieved with the baseline. On the other hand, we see
improvements in around 39% of the cases: an average
improvements of 0.26, which is much higher then the
loss. For the remaining 25% of cases we observe the
same score.
Looking inside the data, the largest error class is
caused by antonyms. A sub-class of this error are
multi-word expressions having an adjective modifier.
This problems might be solved by additional features
using the UMLS resource. An example is shown in
Figure 1.
612
Figure 1: Example sentence for the target term mild
thrombocytopenia. The system returns a wrong rank-
ing, as the adjective changes the meaning and turns the
first ranked term into an antonym.
For feature generation, we currently lookup multi-
word expressions as one term, both in the DT and the
UMLS resource and do not split them into their sin-
gle tokens. This error also suggests considering the
single words inside the multi-word expression, espe-
cially adjectives, and looking them up in a resource
(e.g. UMLS) to detect synonymy and antonymy.
Figure 2 shows the case, where the ranking is per-
formed correctly, but the precise substitute is not an-
notated as a correct one. The term nail plate might be
even more precise in the context as the manual anno-
tated term nail bed. Due to the missing annotation the
Figure 2: Example sentence for the target term nails.
Here the ranking from the system is correct, but the first
substitute from the system was not annotated as such.
baseline gets better scores then the result from the sys-
tem.
9 Conclusion
In summary, we have examined the lexical substitution
task for the medical domain and could show that a sys-
tem for open domain text data can be applied to the
medical domain. We can show that following a hybrid
approach using features from UMLS and distributional
semantics leads to the best results. In future work, we
will work on integrating DTs using other context fea-
tures, as we could see an impact of using two different
DTs. Furthermore, we want to incorporate features us-
ing n-grams computed on a corpus from the domain
and include co-occurrence features.
Acknowledgments
We thank Adam Lally, Eric Brown, Edward A. Epstein,
Chris Biemann and Faisal Chowdhury for their helpful
comments.
References
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proceedings of the 6th In-
ternational Workshop on Semantic Evaluation, held
in conjunction with the 1st Joint Conference on Lex-
ical and Computational Semantics, pages 435?440,
Montreal, Canada.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! A Framework for Lexical Expansion with Con-
textual Similarity. Journal of Language Modelling,
1(1):55?95.
Chris Biemann. 2012. Turk bootstrap word sense in-
ventory 2.0: A large-scale resource for lexical sub-
stitution. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram corpus version 1. Technical report, Google Re-
search.
Chris Buckley and Ellen M. Voorhees. 2004. Re-
trieval evaluation with incomplete information. In
Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ?04, pages 25?32,
Sheffield, United Kingdom.
Miranda Chong and Lucia Specia. 2011. Lexical gen-
eralisation for word-level matching in plagiarism de-
tection. In Recent Advances in Natural Language
Processing, pages 704?709, Hissar, Bulgaria.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio M.
Zanzotto. 2013. Recognizing Textual Entailment:
Models and Applications. Synthesis Lectures on Hu-
man Language Technologies, 6(4):1?220.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Institut f?ur maschinelle Sprachverarbeitung, Univer-
sity of Stuttgart.
613
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strappar-
ava. 2007. Fbk-irst: Lexical substitution task ex-
ploiting domain and syntagmatic coherence. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 145?148,
Prague, Czech Republic.
Gerhard Kremer, Katrin Erk, Sebastian Pad?o, and Ste-
fan Thater. 2014. What Substitutes Tell Us - Anal-
ysis of an ?All-Words? Lexical Substitution Corpus.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2014), pages 540?549, Gothen-
burg, Sweden.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Michael C. McCord, J. William Murdock, and Bran-
imir K. Boguraev. 2012. Deep Parsing in Watson.
IBM J. Res. Dev., 56(3):264?278.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised All-Words Lexical Substitution
using Delexicalized Features. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2013),
pages 1131?1141, Atlanta, GA, USA.
614
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553?557,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
How Text Segmentation Algorithms Gain from Topic Models
Martin Riedl and Chris Biemann
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
riedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
Abstract
This paper introduces a general method to in-
corporate the LDA Topic Model into text seg-
mentation algorithms. We show that seman-
tic information added by Topic Models signifi-
cantly improves the performance of two word-
based algorithms, namely TextTiling and C99.
Additionally, we introduce the new TopicTil-
ing algorithm that is designed to take better
advantage of topic information. We show con-
sistent improvements over word-based meth-
ods and achieve state-of-the art performance
on a standard dataset.
1 Introduction
Texts are often structured into segments to ease un-
derstanding and readability of texts. Knowing about
sentence boundaries is advantageous for natural lan-
guage processing (NLP) tasks such as summariza-
tion or indexing. While many genres such as en-
cyclopedia entries or scientific articles follow rather
formal conventions of breaking up a text into mean-
ingful units, there are plenty of electronically avail-
able texts without defined segments, e.g. web doc-
uments. Text segmentation is the task of automati-
cally segmenting texts into parts. Viewing a well-
written text as sequence of subtopics and assuming
that subtopics correspond to segments, a segmenta-
tion algorithm needs to find changes of subtopics to
identify the natural division of an unstructured text.
In this work, we utilize semantic information
from Topic Models (TMs) to inform text segmen-
tation algorithms. For this, we compare two early
word-based algorithms with their topic-based vari-
ants, and construct our own algorithm called Topic-
Tiling. We show that using topics estimated by La-
tent Dirichlet Allocation (LDA) in lieu of words sub-
stantially improves earlier segmentation algorithms.
In comparison to TextTiling (TT), neither smoothing
nor a blocksize or window size is needed. TT using
TMs and our own algorithm improve on the state-of-
the-art for a standard dataset, while being conceptu-
ally simpler and computationally more efficient than
other topic-based segmentation algorithms.
2 Related Work
Based on the observation of Halliday and Hasan
(1976) that the density of coherence relations is
higher within segments than between segments,
most algorithms compute a coherence score to mea-
sure the difference of textual units for informing
a segmentation decision. TextTiling (TT) (Hearst,
1994) relies on the simplest coherence relation ?
word repetition ? and computes similarities between
textual units based on the similarities of word space
vectors. With C99 (Choi, 2000) an algorithm was
introduced that uses a matrix-based ranking and a
clustering approach in order to relate the most sim-
ilar textual units and to cluster groups of consecu-
tive units into segments. Both TT and C99 charac-
terize textual units by the words they contain. Gal-
ley et al (2003) showed that using TF-IDF term
weights in the term vector improves the performance
of TT. Proposals using Dynamic Programming (DP)
are given in (Utiyama and Isahara, 2001; Fragkou et
al., 2004). Related to our work are the approaches
described in (Misra et al, 2009; Sun et al, 2008):
here, TMs are also used to alleviate the sparsity of
word vectors. Misra et al (2009) extended the DP
algorithm U00 from Utiyama and Isahara (2001) us-
553
ing TMs. At this, the topic assignments have to be
inferred for each possible segment, resulting in high
computational cost. In addition to these linear topic
segmentation algorithms, there are hierarchical seg-
mentation algorithms, see (Yaari, 1997; Hsueh et al,
2006; Eisenstein, 2009).
For topic modeling, we use the widely applied
LDA (Blei et al, 2003). This generative probabilis-
tic model uses a training corpus of documents to cre-
ate document-topic and topic-word distributions and
is parameterized by the number of topics N as well
as by two hyperparameters. To generate a document
d the topic proportions are drawn using a Dirichlet
distribution with hyperparameter ?. Adjacent for
each word i a topic zdi is chosen according to a
multinomial distribution using hyperparameter ?zdi .
Unseen documents can be annotated with an existing
TM using Bayesian inference methods (here: Gibbs
sampling).
3 Method: From Words to Topics
The underlying mechanism described here is very
simple: Instead of using words directly as features
to characterize textual units, we use the topic IDs
assigned by Bayesian inference. LDA inference as-
signs a topic ID to each word in the test document
in each inference iteration step, based on a TM es-
timated on a training corpus. We use the topic ID,
lastly assigned to each word. This might lead to in-
stabilities as a word with high probabilities for sev-
eral topics could be assigned to different topics in
different inference iterations. To avoid these insta-
bilities, we save all topic IDs assigned to a word for
each inference iteration. Finally, the most frequent
topic ID is assigned to each word. This mechanism
we call the mode method. Both word replacements
can be applied to most segmentation algorithms.
In this work, we use this general setup to imple-
ment topic-based versions of TT and C99 and de-
velop a new TextTiling-based method called Topic-
Tiling.
4 Topic-based Segmentation Algorithms
4.1 TextTiling using Topic Models
In TextTiling (TT) (Hearst, 1994) using topic IDs
(TTLDA), a document D, which is subject to seg-
mentation, is represented as a sequence of n topic
IDs1. TT splits the document into topic-sequences,
instead of sentences, where each sequence consists
of w topic IDs. To calculate the similarity between
two topic-sequences, called sequence-gap, TT uses
k topic-sequences, named block, to the left and to
the right of the sequence gap. This parameter k de-
fines the so-called blocksize. The cosine similarity
is applied to computed a similarity score based on
the topic frequency of the adjacent blocks at each
sequence-gap. A value close to 1 indicates a high
similarity among two blocks, a value close to zero
denotes a low similarity. Then for each sequence-
gap a depth score di is calculated for describing the
sharpness of a gap, by di = 1/2(hl(i)?si+hr(i)?
si). The function hl(i) returns the highest similarity
score on the left side of the sequence-gap index i that
does not increase and hr(i) returns the highest score
on the right side. Then all local maxima positions
are searched based on the depth scores.
In the next step, these obtained maxima scores are
sorted. If the number of segments n is given as input
parameter, the n highest depth scores are used, oth-
erwise a cut-off function is used that applies a seg-
ment only if the depth score is larger than ? ? ?/2,
where mean ? and the standard deviation ? are cal-
culated based on the entirety of depth scores. As TT
calculates the depth on every topic-sequence using
the highest gap, this could lead to a segmentation
in the middle of a sentence. To avoid this, a final
step ensures that the segmentation is positioned at
the nearest sentence boundary.
4.2 C99 using Topic Models
For the C99 algorithm (Choi, 2000), named
(C99LDA) when using topic IDs, the text is divided
into minimal units on sentence boundaries. A sim-
ilarity matrix Sm?m is computed, where m denotes
the number of units (sentences). Every element sij
is calculated using the cosine similarity between unit
i and j. Next, a rank matrix R is computed to im-
prove the contrast of S: Each element rij contains
the number of neighbors of sij that have lower simi-
larity scores then sij itself. In a final step a top-down
clustering algorithm is performed to split the docu-
ment into m segments B = b1, . . . , bm. This algo-
1words instead of topic IDs are utilized in the original ap-
proach.
554
rithm starts with the whole document considered as
one segment and splits off segments until the stop
criteria are met, e.g. the number of segments or a
similarity threshold.
4.3 TopicTiling
TopicTiling is a new TextTiling-based algorithm and
is adjusted to use TMs. As we have found in data
analysis, it is frequently the case that a topic dom-
inates within a sampling unit (sentence), and that
units from the same segment frequently are domi-
nated by the same topic. In contrast to word-based
representations, we expect no need to face sparsity
issues that require smoothing methods (see TT) and
ranking methods (see C99), which allows us to sim-
plify the algorithm. Initially, the document is split
into minimal units on sentence boundaries. To mea-
sure the coherence between units, the cosine similar-
ity (vector dot product) between two adjacent sen-
tences is computed. Each sentences s is represented
as a N -dimensional vector, where N is the number
of topics defined in the TMs. The i-th element of the
vector contains the number of times the i-th topic
is observed in the sentence. In comparison to TT
we search all local minima based on these similar-
ity scores and calculate for these positions the depth
score as described in TT. If the number of segments
is known in advance, the segments of the n-highest
depth-scores are used, otherwise the cut-off score
criteria used in TT is adapted.
5 Evaluation
As laid out in Section 3, a LDA Model is estimated
on a training dataset and used for inference on the
test set. To ensure that we do no use informa-
tion from the test set, we perform a 10-fold Cross
Validation (CV) for all reported results. To reduce
the variance of the shown results, derived by the ran-
dom nature of sampling and inference, the results
for each fold are calculated 30 times using different
LDA models.
The LDA model is trained with N=100 top-
ics, 500 sampling iterations and symmetric hy-
perparameters as recommended by Griffiths and
Steyvers (2004)(?=50/N and ?=0.01), using JGibb-
sLda (Phan and Nguyen, 2007). For the annota-
tion of unseen data with topic information, we use
LDA inference, sampling 100 iterations. Inference
is executed sentence-wise, since sentences form the
minimal unit of our segmentation algorithms and we
cannot use document information in the test setting.
The performance of the algorithms is measured us-
ing Pk and WindowDiff (WD) metrics (Beeferman
et al, 1999; Pevzner and Hearst, 2002). The C99 al-
gorithm is initialized with a 11?11 ranking mask, as
recommended in Choi (2000). TT is configured ac-
cording to Choi (2000) with sequence length w=20
and block size k=6.
5.1 Data Set
For evaluation, we rely on the Choi data set (Choi,
2000), which has been used in several other text seg-
mentation approaches to ensure comparability. This
data set is generated artificially using the Brown cor-
pus and consists of 700 documents. Each docu-
ment consists of 10 segments. For its generation,
3?11 sentences are sequentially extracted from a
randomly selected document and merged together.
While our CV evaluation setting is designed to avoid
using the same documents for training and testing,
this cannot be guaranteed as the segments within the
documents generated by Choi are included in sev-
eral documents. This problem also occurs in other
approaches, but has not be described in (Fragkou et
al., 2004; Misra et al, 2009; Galley et al, 2003),
where parts or the whole dataset are used for train-
ing either TF-IDF values or topic models.
5.2 Results
For the experiments the C99 and TT implementa-
tions2 are executed in two settings: using words and
using topics. When using words, TT and C99 use
stemmed words and filter out words using a stop-
word list. C99 additional removes words using pre-
defined regular expressions. In the case of topic IDs,
no stopword filtering was deemed necessary. Table
1 shows the result of the different algorithms with all
combination of provided segment number and using
the mode method.
We note that WD values are always higher than
the Pk values, and these measures are highly corre-
lated. First we discuss results for the setting with
number of segments provided (see column 2-5 of
2We use the implementations by Choi available at http:
//code.google.com/p/uima-text-segmenter/.
555
Method Segments provided Segments unprovided
mode=false mode=true mode=false mode=true
Pk WD Pk WD Pk WD Pk WD
C99 11.20 12.07 12.73 14.57
C99LDA 4.16 4.89 2.67 3.08 8.69 10.52 3.24 4.08
TT 44.48 47.11 49.51 66.16
TTLDA 1.85 2.10 1.04 1.18 16.41 21.40 2.89 3.67
TopicTiling 2.65 3.02 2.12 2.42 4.12 5.75 2.30 3.08
TopicTiling 1.50 1.72 1.06 1.21 3.24 4.58 1.39 1.84
(filtered)
Table 1: Results by segment length for TT with
words and topics (TTLDA), C99 with words and topics
(C99LDA) and TopicTiling using all sentences and using
only sentences with more then 5 word tokens (filtered).
Table 1). A significant improvement for C99 and
TT can be achieved when using topic IDs. In case
of C99LDA, the error rate is at least halved and for
TTLDA the error rate is reduced by a factor of 20.
Using the most frequent topic ID assigned during
the Bayesian inference (mode method) reduces the
error rates further for the TM-based approaches, as
the probability for randomly assigned topic IDs is
decreased. The newly introduced algorithm Top-
icTiling as described above does not improve over
TTLDA. Analysis revealed that the Choi corpus in-
cludes also captions and other ?non-sentences? that
are marked as sentences, which causes TopicTil-
ing to introduce false positive segments since the
topic vectors are too sparse for these short ?non-
sentences?. We therefore filter out ?sentences? with
less than 5 words (see bottom line in Table 1).
This leads to errors values that are close to the re-
sults achieved with TTLDA when the mode is used.
When the number of segments is not given in ad-
vance (see columns 6-9 in Table 1), we again ob-
serve significantly better results comparing topic-
based methods to word-based methods. But the er-
ror rates of TTLDA are unexpectedly high when the
mode method is not used. We discovered in data
analysis that TT estimates too many segments, as the
topic ID distributions between adjacent sentences
within a segment are often too diverse, especially
in face of random fluctuations from the topic assign-
ments. Estimating the number of segments is better
achieved using TopicTiling instead of TTLDA.
In Table 2, we compare TTLDA, C99LDA and
our TopicTiling algorithm to other published results
on the same dataset. We can see that all introduced
topic-based methods outperform the yet best pub-
Method Segments
provided unprovided
TT 44.48 49.51
C99 11.20 12.73
U00 (Utiyama and Isahara, 2001) 9 10
F04 (Fragkou et al, 2004) 5.39
M09 (Misra et al, 2009) 2.73
C99LDA (mode = true) 2.67 3.24
TTLDA (mode=true) 1.04 2.89
TopicTiling (mode=true, filtered) 1.06 1.39
Table 2: List of lowest Pk values for the Choi data set for
different algorithms in the literature.
lished M09 algorithm (Misra et al, 2009). The
improvements of C99, TTLDA and TopicTiling in
comparison to M09 are significant3.
TopicTiling and TTLDA are computationally
more efficient than M09. Whereas our linear method
has a complexity of O(T ) (T is the number of
sentences), dynamic algorithms like M09 have a
complexity of O(T 2) (cf. Fragkou et al (2004)),
which also applies to the number of topic inference
runs. When the number of segments is not given
in advance, TopicTiling outperforms TTLDA sig-
nificantly. As an additional benefit, TopicTiling is
even simpler than TT, as no smoothing parameter is
needed and the depth scores are only calculated for
the minima of the similarity scores.
6 Conclusion
The method introduced in this paper shows that us-
ing semantic information, provided by TMs, can im-
prove existing algorithm significantly. This is at-
tested modifying the algorithm TT and C99. With
TopicTiling a new simplistic topic based algorithm
is developed that can produce state-of-the-art results
based on the Choi corpus and outperform TTLDA
when the number of segments is unknown. Addi-
tionally this method is computationally more effi-
cient in comparison to other topic based segmenta-
tion algorithms. Another contribution is the mode
method for stabilizing topic ID assignments.
7 Acknowledgments
This work has been supported by LOEWE as part of
the research center ?Digital Humanities?. We would
like to thank the anonymous reviewers for their com-
ments, which truly helped to improve the paper.
3using a one sampled t-test with ? = 0.05
556
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine learning, 34(1):177?210.
David M. Blei, Andrew Y Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, pages 26?33,
Seattle, WA, USA.
Jacob Eisenstein. 2009. Hierarchical text segmenta-
tion from multi-scale lexical cohesion. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 353?
361, Boulder, CO, USA.
Pavlina Fragkou, Vassilios Petridis, and Athanasios Ke-
hagias. 2004. A Dynamic Programming Algorithm
for Linear Text Segmentation. Journal of Intelligent
Information Systems, 23(2):179?197.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation
of multi-party conversation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1, pages 562?569, Sapporo,
Japan.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
M A K Halliday and Ruqaiya Hasan. 1976. Cohesion in
English, volume 1 of English Language Series. Long-
man.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd annual
meeting on Association for Computational Linguistics,
pages 9?16, Las Cruces, NM, USA.
P.-Y. Hsueh, J. D. Moore, and S. Renals. 2006. Auto-
matic segmentation of multiparty dialogue. AMI-156.
Hemant Misra, Joemon M Jose, and Olivier Cappe?. 2009.
Text Segmentation via Topic Modeling : An Analyti-
cal Study. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, pages
1553?1556, Hong Kong.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistic, 28(1):19?36.
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. Gibb-
sLDA++: A C/C++ implementation of latent Dirichlet
allocation (LDA). http://jgibblda.sourceforge.net/.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher ker-
nel. Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Human
Language Technologies, pages 269?272.
Masao Utiyama and Hitoshi Isahara. 2001. A statisti-
cal model for domain-independent text segmentation.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 499?506,
Toulouse, France.
Yaakov Yaari. 1997. Segmentation of expository texts
by hierarchical agglomerative clustering. In Proceed-
ings of the Conference on Recent Advances in Natural
Language Processing, Tzigov Chark, Bulgaria.
557
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1020?1029,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
That?s sick dude!:
Automatic identification of word sense change across different timescales
Sunny Mitra
1
, Ritwik Mitra
1
, Martin Riedl
2
,
Chris Biemann
2
, Animesh Mukherjee
1
, Pawan Goyal
1
1
Dept. of Computer Science and Engineering,
Indian Institute of Technology Kharagpur, India ? 721302
2
FG Language Technology, Computer Science Department, TU Darmstadt, Germany
1
{sunnym,ritwikm,animeshm,pawang}@cse.iitkgp.ernet.in
2
{riedl,biem}@cs.tu-darmstadt.de
Abstract
In this paper, we propose an unsupervised
method to identify noun sense changes
based on rigorous analysis of time-varying
text data available in the form of millions
of digitized books. We construct distribu-
tional thesauri based networks from data
at different time points and cluster each
of them separately to obtain word-centric
sense clusters corresponding to the differ-
ent time points. Subsequently, we com-
pare these sense clusters of two different
time points to find if (i) there is birth of
a new sense or (ii) if an older sense has
got split into more than one sense or (iii)
if a newer sense has been formed from the
joining of older senses or (iv) if a partic-
ular sense has died. We conduct a thor-
ough evaluation of the proposed method-
ology both manually as well as through
comparison with WordNet. Manual eval-
uation indicates that the algorithm could
correctly identify 60.4% birth cases from
a set of 48 randomly picked samples and
57% split/join cases from a set of 21 ran-
domly picked samples. Remarkably, in
44% cases the birth of a novel sense is
attested by WordNet, while in 46% cases
and 43% cases split and join are respec-
tively confirmed by WordNet. Our ap-
proach can be applied for lexicography, as
well as for applications like word sense
disambiguation or semantic search.
1 Introduction
Two of the fundamental components of a natu-
ral language communication are word sense dis-
covery (Jones, 1986) and word sense disambigua-
tion (Ide and Veronis, 1998). While discovery
corresponds to acquisition of vocabulary, disam-
biguation forms the basis of understanding. These
two aspects are not only important from the per-
spective of developing computer applications for
natural languages but also form the key compo-
nents of language evolution and change.
Words take different senses in different contexts
while appearing with other words. Context plays
a vital role in disambiguation of word senses as
well as in the interpretation of the actual mean-
ing of words. For instance, the word ?bank? has
several distinct interpretations, including that of a
?financial institution? and the ?shore of a river.?
Automatic discovery and disambiguation of word
senses from a given text is an important and chal-
lenging problem which has been extensively stud-
ied in the literature (Jones, 1986; Ide and Vero-
nis, 1998; Sch?utze, 1998; Navigli, 2009). How-
ever, another equally important aspect that has not
been so far well investigated corresponds to one
or more changes that a word might undergo in its
sense. This particular aspect is getting increas-
ingly attainable as more and more time-varying
text data become available in the form of millions
of digitized books (Goldberg and Orwant, 2013)
gathered over the last centuries. As a motivat-
ing example one could consider the word ?sick?
? while according to the standard English dictio-
naries the word is normally used to refer to some
sort of illness, a new meaning of ?sick? refer-
ring to something that is ?crazy? or ?cool? is cur-
rently getting popular in the English vernacular.
This change is further interesting because while
traditionally ?sick? has been associated to some-
thing negative in general, the current meaning as-
sociates positivity with it. In fact, a rock band
by the name of ?Sick Puppies? has been founded
which probably is inspired by the newer sense of
the word sick. The title of this paper has been
motivated by the above observation. Note that
this phenomena of change in word senses has ex-
isted ever since the beginning of human commu-
nication (Bamman and Crane, 2011; Michel et
1020
al., 2011; Wijaya and Yeniterzi, 2011; Mihalcea
and Nastase, 2012); however, with the advent of
modern technology and the availability of huge
volumes of time-varying data it now has become
possible to automatically track such changes and,
thereby, help the lexicographers in word sense dis-
covery, and design engineers in enhancing vari-
ous NLP/IR applications (e.g., disambiguation, se-
mantic search etc.) that are naturally sensitive to
change in word senses.
The above motivation forms the basis of the
central objective set in this paper, which is to de-
vise a completely unsupervised approach to track
noun sense changes in large texts available over
multiple timescales. Toward this objective we
make the following contributions: (a) devise a
time-varying graph clustering based sense induc-
tion algorithm, (b) use the time-varying sense
clusters to develop a split-join based approach for
identifying new senses of a word, and (c) evalu-
ate the performance of the algorithms on various
datasets using different suitable approaches along
with a detailed error analysis. Remarkably, com-
parison with the English WordNet indicates that
in 44% cases, as identified by our algorithm, there
has been a birth of a completely novel sense, in
46% cases a new sense has split off from an older
sense and in 43% cases two or more older senses
have merged in to form a new sense.
The remainder of the paper is organized as fol-
lows. In the next section we present a short re-
view of the literature. In Section 3 we briefly
describe the datasets and outline the process of
co-occurrence graph construction. In Section 4
we present an approach based on graph cluster-
ing to identify the time-varying sense clusters and
in Section 5 we present the split-merge based ap-
proach for tracking word sense changes. Evalu-
ation methods are summarized in Section 6. Fi-
nally, conclusions and further research directions
are outlined in Section 7.
2 Related work
Word sense disambiguation as well as word sense
discovery have both remained key areas of re-
search right from the very early initiatives in nat-
ural language processing research. Ide and Vero-
nis (1998) present a very concise survey of the his-
tory of ideas used in word sense disambiguation;
for a recent survey of the state-of-the-art one can
refer to (Navigli, 2009). Some of the first attempts
to automatic word sense discovery were made by
Karen Sp?arck Jones (1986); later in lexicography,
it has been extensively used as a pre-processing
step for preparing mono- and multi-lingual dictio-
naries (Kilgarriff and Tugwell, 2001; Kilgarriff,
2004). However, as we have already pointed out
that none of these works consider the temporal as-
pect of the problem.
In contrast, the current study, is inspired by
works on language dynamics and opinion spread-
ing (Mukherjee et al, 2011; Maity et al, 2012;
Loreto et al, 2012) and automatic topic detection
and tracking (Allan et al, 1998). However, our
work differs significantly from those proposed in
the above studies. Opinion formation deals with
the self-organisation and emergence of shared vo-
cabularies whereas our work focuses on how the
different senses of these vocabulary words change
over time and thus become ?out-of-vocabulary?.
Topic detection involves detecting the occurrence
of a new event such as a plane crash, a murder, a
jury trial result, or a political scandal in a stream
of news stories from multiple sources and track-
ing is the process of monitoring a stream of news
stories to find those that track (or discuss) the
same event. This is done on shorter timescales
(hours, days), whereas our study focuses on larger
timescales (decades, centuries) and we are inter-
ested in common nouns, verbs and adjectives as
opposed to events that are characterized mostly by
named entities. Other similar works on dynamic
topic modelling can be found in (Blei and Laf-
ferty, 2006; Wang and McCallum, 2006). Google
books n-gram viewer
1
is a phrase-usage graphing
tool which charts the yearly count of selected letter
combinations, words, or phrases as found in over
5.2 million digitized books. It only reports fre-
quency of word usage over the years, but does not
give any correlation among them as e.g., in (Heyer
et al, 2009), and does not analyze their senses.
A few approaches suggested by (Bond et al,
2009; P?a?akk?o and Lind?en, 2012) attempt to aug-
ment WordNet synsets primarily using methods
of annotation. Another recent work by Cook et
al. (2013) attempts to induce word senses and then
identify novel senses by comparing two different
corpora: the ?focus corpora? (i.e., a recent version
of the corpora) and the ?reference corpora? (older
version of the corpora). However, this method
is limited as it only considers two time points to
1
https://books.google.com/ngrams
1021
identify sense changes as opposed to our approach
which is over a much larger timescale, thereby, ef-
fectively allowing us to track the points of change
and the underlying causes. One of the closest
work to what we present here has been put forward
by (Tahmasebi et al, 2011), where the authors an-
alyze a newspaper corpus containing articles be-
tween 1785 and 1985. The authors mainly report
the frequency patterns of certain words that they
found to be candidates for change; however a de-
tailed cause analysis as to why and how a particu-
lar word underwent a sense change has not been
demonstrated. Further, systematic evaluation of
the results obtained by the authors has not been
provided.
All the above points together motivated us to
undertake the current work where we introduce,
for the first time, a completely unsupervised and
automatic method to identify the change of a word
sense and the cause for the same. Further, we also
present an extensive evaluation of the proposed al-
gorithm in order to test its overall accuracy and
performance.
3 Datasets and graph construction
In this section, we outline a brief description
of the dataset used for our experiments and
the graph construction procedure. The primary
source of data have been the millions of digitized
books made available through the Google Book
project (Goldberg and Orwant, 2013). The Google
Book syntactic n-grams dataset provides depen-
dency fragment counts by the years. However, in-
stead of using the plain syntactic n-grams, we use
a far richer representation of the data in the form of
a distributional thesaurus (Lin, 1997; Rychl?y and
Kilgarriff, 2007). In specific, we prepare a distri-
butional thesaurus (DT) for each of the time peri-
ods separately and subsequently construct the re-
quired networks. We briefly outline the procedure
of thesauri construction here referring the reader
to (Riedl and Biemann, 2013) for further details.
In this approach, we first extract each word and a
set of its context features, which are formed by la-
beled and directed dependency parse edges as pro-
vided in the dataset. Following this, we compute
the frequencies of the word, the context and the
words along with their context. Next we calculate
the lexicographer?s mutual information LMI (Kil-
garriff, 2004) between a word and its features and
retain only the top 1000 ranked features for ev-
ery word. Finally, we construct the DT network as
follows: each word is a node in the network and
the edge weight between two nodes is defined as
the number of features that the two corresponding
words share in common.
4 Tracking sense changes
The basic idea of our algorithm for tracking sense
changes is as follows. If a word undergoes a
sense change, this can be detected by comparing
its senses obtained from two different time pe-
riods. Since we aim to detect this change au-
tomatically, we require distributional representa-
tions corresponding to word senses for different
time periods. We, therefore, utilize the basic hy-
pothesis of unsupervised sense induction to in-
duce the sense clusters over various time periods
and then compare these clusters to detect sense
change. The basic premises of the ?unsupervised
sense induction? are briefly described below.
4.1 Unsupervised sense induction
We use the co-occurrence based graph clustering
framework introduced in (Biemann, 2006). The
algorithm proceeds in three basic steps. Firstly,
a co-occurrence graph is created for every target
word found in DT. Next, the neighbourhood/ego
graph is clustered using the Chinese Whispers
(CW) algorithm (see (McAuley and Leskovec,
2012) for similar approaches). The algorithm, in
particular, produces a set of clusters for each target
word by decomposing its open neighborhood. We
hypothesize that each different cluster corresponds
to a particular sense of the target word. For a de-
tailed description, the reader is referred to (Bie-
mann, 2011).
If a word undergoes sense change, this can be
detected by comparing the sense clusters obtained
from two different time periods by the algorithm
outlined above. For this purpose, we use statis-
tics from the DT corresponding to two different
time intervals, say tv
i
and tv
j
. We then run the
sense induction algorithm over these two different
datasets. Now, for a given word w that appears
in both the datasets, we get two different set of
clusters, say C
i
and C
j
. Without loss of gener-
ality, let us assume that our algorithm detects m
sense clusters for the word w in tv
i
and n sense
clusters in tv
j
. Let C
i
= {s
i1
, s
i2
, . . . , s
im
} and
C
j
= {s
j1
, s
j2
, . . . , s
jn
}, where s
kz
denotes z
th
sense cluster for word w during time interval tv
k
.
1022
We next describe our algorithm for detecting sense
change from these sets of sense clusters.
4.2 Split, join, birth and death
We hypothesize that word w can undergo sense
change from one time interval (tv
i
) to another
(tv
j
) as per one of the following scenarios:
Split A sense cluster s
iz
in tv
i
splits into two (or
more) sense clusters, s
jp
1
and s
jp
2
in tv
j
Join Two sense clusters s
iz
1
and s
iz
2
in tv
i
join to
make a single cluster s
jp
in tv
j
Birth A new sense cluster s
jp
appears in tv
j
,
which was absent in tv
i
Death A sense cluster s
iz
in tv
i
dies out and does
not appear in tv
j
To detect split, join, birth or death, we build an
(m+1)? (n+1) matrix I to capture the intersec-
tion between sense clusters of two different time
periods. The first m rows and n columns corre-
spond to the sense clusters in tv
i
and tv
j
espec-
tively. We append an additional row and column to
capture the fraction of words, which did not show
up in any of the sense clusters in another time in-
terval. So, an element I
kl
of the matrix
? 1 ? k ? m, 1 ? l ? n: denotes the frac-
tion of words in a newer sense cluster s
jl
,
that were also present in an older sense clus-
ter s
ik
.
? k = m + 1, 1 ? l ? n: denotes the fraction
of words in the sense cluster s
jl
, that were not
present in any of the m clusters in tv
i
.
? 1 ? k ? m, l = n + 1: denotes the fraction
of words in the sense cluster s
ik
, that did not
show up in any of the n clusters in tv
j
.
Thus, the matrix I captures all the four possible
scenarios for sense change. Since we can not
expect a perfect split, birth etc., we used certain
threshold values to detect if a candidate word is
undergoing sense change via one of these four
cases. In Figure 1, as an example, we illustrate
the birth of a new sense for the word ?compiler?.
4.3 Multi-stage filtering
To make sure that the candidate words obtained
via our algorithm are meaningful, we applied
multi-stage filtering to prune the candidate word
list. The following criterion were used for the fil-
tering:
Stage 1 We utilize the fact that the CW algorithm
is non-deterministic in nature. We apply CW
three times over the source and target time inter-
vals. We obtain the candidate word lists using our
algorithm for the three runs, then take the inter-
section to output those words, which came up in
all the three runs.
Stage 2 From the above list, we retain only those
candidate words, which have a part-of-speech tag
?NN? or ?NNS?, as we focus on nouns for this
work.
Stage 3 We sort the candidate list obtained in
Stage 2 as per their occurrence in the first time
period. Then, we remove the top 20% and the
bottom 20% words from this list. Therefore, we
consider the torso of the frequency distribution
which is the most informative part for this type
of an analysis.
5 Experimental framework
For our experiments, we utilized DTs created for
8 different time periods: 1520-1908, 1909-1953,
1954-1972, 1973-1986, 1987-1995, 1996-2001,
2002-2005 and 2006-2008 (Riedl et al, 2014).
The time periods were set such that the amount
of data in each time period is roughly the same.
We will also use T
1
to T
8
to denote these time pe-
riods. The parameters for CW clustering were set
as follows. The size of the neighbourhood (N )
to be clustered was set to 200. The parameter n
regulating the edge density in this neighbourhood
was set to 200 as well. The parameter a was set to
lin, which corresponds to favouring smaller clus-
ters by hub downweighing
2
. The threshold values
used to detect the sense changes were as follows.
For birth, at least 80% words of the target cluster
should be novel. For split, each split cluster should
have at least 30% words of the source cluster and
the total intersection of all the split clusters should
be > 80%. The same parameters were used for the
join and death case with the interchange of source
and target clusters.
5.1 Signals of sense change
Making comparisons between all the pairs of time
periods gave us 28 candidate words lists. For
2
data available at http://sf.net/p/jobimtext/
wiki/LREC2014_Google_DT/
1023
Figure 1: Example of the birth of a new sense for the word ?compiler?
each of these comparison, we applied the multi-
stage filtering to obtain the pruned list of candidate
words. Table 1 provides some statistics about the
number of candidate words obtained correspond-
ing to the birth case. The rows correspond to the
source time-period and the columns correspond to
the target time periods. An element of the table
shows the number of candidate words obtained
by comparing the corresponding source and target
time periods.
Table 1: Number of candidate birth senses be-
tween all time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
8
T
1
2498 3319 3901 4220 4238 4092 3578
T
2
1451 2330 2789 2834 2789 2468
T
3
917 1460 1660 1827 1815
T
4
517 769 1099 1416
T
5
401 818 1243
T
6
682 1107
T
7
609
The table clearly shows a trend. For most of
the cases, the number of candidate birth senses
tends to increase as we go from left to right. Sim-
ilarly, this number decreases as we go down in
the table. This is quite intuitive since going from
left to right corresponds to increasing the gap be-
tween two time periods while going down cor-
responds to decreasing this gap. As the gap in-
creases (decreases), one would expect more (less)
new senses coming in. Even while moving diago-
nally, the candidate words tend to decrease as we
move downwards. This corresponds to the fact
that the number of years in the time periods de-
creases as we move downwards, and therefore, the
gap also decreases.
5.2 Stability analysis & sense change location
Formally, we consider a sense change from tv
i
to tv
j
stable if it was also detected while com-
paring tv
i
with the following time periods tv
k
s.
This number of subsequent time periods, where
the same sense change is detected, helps us to de-
termine the age of a new sense. Similarly, for a
candidate sense change from tv
i
to tv
j
, we say that
the location of the sense change is tv
j
if and only
if that sense change does not get detected by com-
paring tv
i
with any time interval tv
k
, intermediate
between tv
i
and tv
j
.
Table 1 gives a lot of candidate words for sense
change. However, not all the candidate words
were stable. Thus, it was important to prune these
results using stability analysis. Also, it is to be
noted that these results do not pin-point to the ex-
act time-period, when the sense change might have
taken place. For instance, among the 4238 candi-
date birth sense detected by comparing T
1
and T
6
,
many of these new senses might have come up in
between T
2
to T
5
as well. We prune these lists fur-
ther based on the stability of the sense, as well as
to locate the approximate time interval, in which
the sense change might have occurred.
Table 2 shows the number of stable (at least
twice) senses as well as the number of stable
sense changes located in that particular time pe-
riod. While this decreases recall, we found this to
be beneficial for the accuracy of the method.
Once we were able to locate the senses as well
as to find the age of the senses, we attempted to
1024
Table 2: Number of candidate birth senses ob-
tained for different time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
1
2498 3319 3901 4220 4238 4092
stable 537 989 1368 1627 1540 1299
located 537 754 772 686 420 300
T
2
1451 2330 2789 2834 2789
stable 343 718 938 963 810
located 343 561 517 357 227
select some representative words and plotted them
on a timeline as per the birth period and their age
in Figure 2. The source time period here is 1909-
1953.
6 Evaluation framework
During evaluation, we considered the clusters ob-
tained using the 1909-1953 time-slice as our refer-
ence and attempted to track sense change by com-
paring these with the clusters obtained for 2002-
2005. The sense change detected was categorized
as to whether it was a new sense (birth), a single
sense got split into two or more senses (split) or
two or more senses got merged (join) or a particu-
lar sense died (death). We present a few instances
of the resulting clusters in the paper and refer the
reader to the supplementary material
3
for the rest
of the results.
6.1 Manual evaluation
The algorithm detected a lot of candidate words
for the cases of birth, split/join as well as death.
Since it was difficult to go through all the candi-
date sense changes for all the comparisons man-
ually, we decided to randomly select some can-
didate words, which were flagged by our algo-
rithm as undergoing sense change, while compar-
ing 1909-1953 and 2002-2005 DT. We selected 48
random samples of candidate words for birth cases
and 21 random samples for split/join cases. One
of the authors annotated each of the birth cases
identifying whether or not the algorithm signalled
a true sense change while another author did the
same task for the split/join cases. The accuracy as
per manual evaluation was found to be 60.4% for
the birth cases and 57% for the split/join cases.
Table 3 shows the evaluation results for a few
candidate words, flagged due to birth. Columns
3
http://cse.iitkgp.ac.in/resgrp/cnerg/
acl2014_wordsense/
correspond to the candidate words, words obtained
in the cluster of each candidate word (we will use
the term ?birth cluster? for these words, hence-
forth), which indicated a new sense, the results
of manual evaluation as well as the possible sense
this birth cluster denotes.
Table 4 shows the corresponding evaluation re-
sults for a few candidate words, flagged due to
split or join.
A further analysis of the words marked due
to birth in the random samples indicates that
there are 22 technology-related words, 2 slangs,
3 economics related words and 2 general words.
For the split-join case we found that there are
3 technology-related words while the rest of the
words are general. Therefore one of the key ob-
servations is that most of the technology related
words (where the neighborhood is completely
new) could be extracted from our birth results. In
contrast, for the split-join instances most of the re-
sults are from the general category since the neigh-
borhood did not change much here; it either got
split or merged from what it was earlier.
6.2 Automated evaluation with WordNet
In addition to manual evaluation, we also per-
formed automated evaluation for the candidate
words. We chose WordNet for automated evalua-
tion because not only does it have a wide coverage
of word senses but also it is being maintained and
updated regularly to incorporate new senses. We
did this evaluation for the candidate birth, join and
split sense clusters obtained by comparing 1909-
1953 time period with respect to 2002-2005. For
our evaluation, we developed an aligner to align
the word clusters obtained with WordNet senses.
The aligner constructs a WordNet dictionary for
the purpose of synset algnment. The CW clus-
ter is then aligned to WordNet synsets by compar-
ing the clusters with WordNet graph and the synset
with the maximum alignment score is returned as
the output. In summary, the aligner tool takes as
input the CW cluster and returns a WordNet synset
id that corresponds to the cluster words. The eval-
uation settings were as follows:
Birth: For a candidate word flagged as birth, we
first find out the set of all WordNet synset ids for
its CW clusters in the source time period (1909-
1953 in this case). Let S
init
denote the union of
these synset ids. We then find WordNet synset id
for its birth-cluster, say s
new
. Then, if s
new
/?
1025
Figure 2: Examples of birth senses placed on a timeline as per their location as well as age
Table 3: Manual evaluation for seven randomly chosen candidate birth clusters between time periods
1909-1953 and 2002-2005
Sl Candidate birth cluster Evaluation judgement,
No. Word comments
1 implant gel, fibre, coatings, cement, materials, metal, filler No, New set of words but
silicone, composite, titanium, polymer, coating similar sense already existed
2 passwords browsers, server, functionality, clients, workstation Yes, New sense related
printers, software, protocols, hosts, settings, utilities to ?a computer sense?
3 giants multinationals, conglomerates, manufacturers Yes, New sense as ?an
corporations, competitors, enterprises, companies organization with very great
businesses, brands, firms size or force?
4 donation transplantation, donation, fertilization, transfusions Yes, The new usage of donation
transplant, transplants, insemination, donors, donor ... associated with body organs etc.
5 novice negro, fellow, emigre, yankee, realist, quaker, teen No, this looks like a false
male, zen, lady, admiring, celebrity, thai, millionaire ... positive
6 partitions server, printers, workstation, platforms, arrays Yes, New usage related to
modules, computers, workstations, kernel ... the ?computing? domain
7 yankees athletics, cubs, tigers, sox, bears, braves, pirates Yes, related to the ?New
cardinals, dodgers, yankees, giants, cardinals ... York Yankees? team
S
init
, it implies that this is a new sense that was
not present in the source clusters and we call it a
?success? as per WordNet.
Join: For the join case, we find WordNet synset
ids s
1
and s
2
for the clusters obtained in the
source time period and s
new
for the join cluster
in the target time period. If s
1
6= s
2
and s
new
is
either s
1
or s
2
, we call it a ?success?.
Split: For the split case, we find WordNet synset
id s
old
for the source cluster and synset ids s
1
and s
2
for the target split clusters. If s
1
6= s
2
and either s
1
, or s
2
retains the id s
old
, we call it a
?success?.
Table 5 show the results of WordNet based eval-
uation. In case of birth we observe a success of
Table 5: Results of the automatic evaluation using
WordNet
Category No. of Candidate Words Success Cases
Birth 810 44%
Split 24 46%
Join 28 43%
44% while for split and join we observe a success
of 46% and 43% respectively. We then manually
verified some of the words that were deemed as
successes, as well as investigated WordNet sense
they were mapped to. Table 6 shows some of the
words for which the evaluation detected success
along with WordNet senses. Clearly, the cluster
words correspond to a newer sense for these words
1026
Table 4: Manual evaluation for five randomly chosen candidate split/join clusters between time periods
1909-1953 and 2002-2005
Sl Candidate Source and target clusters
No. Word
1 intonation S: whisper, glance, idioms, gesture, chant, sob, inflection, diction, sneer, rhythm, accents ...
(split) T
1
: nod, tone, grimace, finality, gestures, twang, shake, shrug, irony, scowl, twinkle ...
T
2
: accents, phrase, rhythm, style, phonology, diction, utterance, cadence, harmonies ...
Yes, T
1
corresponds to intonation in normal conversations while T
2
corresponds to the use of accents in
formal and research literature
2 diagonal S: coast, edge, shoreline, coastline, border, surface, crease, edges, slope, sides, seaboard ...
(split) T
1
: circumference, center, slant, vertex, grid, clavicle, margin, perimeter, row, boundary ..
T
2
: border, coast, seaboard, seashore, shoreline, waterfront, shore, shores, coastline, coasts
Yes, the split T
1
is based on mathematics where as T
2
is based on geography
3 mantra S
1
: sutra, stanza, chanting, chants, commandments, monologue, litany, verse, verses ...
(join) S
2
: praise, imprecation, benediction, praises, curse, salutation, benedictions, eulogy ...
T : blessings, spell, curses, spells, rosary, prayers, blessing, prayer, benediction ...
Yes, the two seemingly distinct senses of mantra - a contextual usage for chanting and prayer (S
1
)
and another usage in its effect - salutations, benedictions (S
2
) have now merged in T .
4 continuum S: circumference, ordinate, abscissa, coasts, axis, path, perimeter, arc, plane axis ...
(split) T
1
: roadsides, corridors, frontier, trajectories, coast, shore, trail, escarpment, highways ...
T
2
: arc, ellipse, meridians, equator, axis, axis, plane, abscissa, ordinate, axis, meridian ....
Yes, the split S
1
denotes the usage of ?continuum? with physical objects while the
the split S
2
corresponds to its usages in mathematics domain.
5 headmaster S
1
: master, overseer, councillor, chancellor, tutors, captain, general, principal ...
(join) S
2
: mentor, confessor, tutor, founder, rector, vicar, graduate, counselor, lawyer ...
T : chaplain, commander, surveyor, coordinator, consultant, lecturer, inspector ...
No, it seems a false positive
and the mapped WordNet synset matches the birth
cluster to a very high degree.
6.3 Evaluation with a slang list
Slangs are words and phrases that are regarded as
very informal, and are typically restricted to a par-
ticular context. New slang words come up every
now and then, and this plays an integral part in the
phenomena of sense change. We therefore decided
to perform an evaluation as to how many slang
words were being detected by our candidate birth
clusters. We used a list of slangs available from
the slangcity website
4
. We collected slangs for the
years 2002-2005 and found the intersection with
our candidate birth words. Note that the website
had a large number of multi-word expressions that
we did not consider in our study. Further, some
of the words appeared as either erroneous or very
transient (not existing more than a few months) en-
tires, which had to be removed from the list. All
these removal left us with a very little space for
comparison; however, despite this we found 25
slangs from the website that were present in our
birth results, e.g. ?bum?, ?sissy?, ?thug?, ?dude? etc.
4
http://slangcity.com/email_archive/
index_2003.htm
6.4 Evaluation of candidate death clusters
Much of our evaluation was focussed on the birth
sense clusters, mainly because these are more in-
teresting from a lexicographic perspective. Addi-
tionally, the main theme of this work was to de-
tect new senses for a given word. To detect a
true death of a sense, persistence analysis was re-
quired, that is, to verify if the sense was persist-
ing earlier and vanished after a certain time period.
While such an analysis goes beyond the scope of
this paper, we selected some interesting candidate
?death? senses. Table 7 shows some of these inter-
esting candidate words, their death cluster along
with the possible vanished meaning, identified by
the authors. While these words are still used in a
related sense, the original meaning does not exist
in the modern usage.
7 Conclusions
In this paper, we presented a completely unsu-
pervised method to detect word sense changes
by analyzing millions of digitized books archived
spanning several centuries. In particular, we con-
structed DT networks over eight different time
windows, clustered these networks and compared
these clusters to identify the emergence of novel
1027
Table 6: Example of randomly chosen candidate birth clusters mapped to WordNet
Sl Candidate birth cluster Synset Id,
No. Word WordNet sense
1 macro code, query, handler, program, procedure, subroutine 6582403, a set sequence of steps,
module, script part of larger computer program
2 caller browser, compiler, sender, routers, workstation, cpu 4175147, a computer that
host, modem, router, server provides client stations with access to files
3 searching coding, processing, learning, computing, scheduling 1144355, programming: setting an
planning, retrieval, routing, networking, navigation order and time for planned events
4 hooker bitch, whore, stripper, woman slut, prostitute 10485440, a woman who
girl, dancer ... engages in sexual intercourse for money
5 drones helicopters, fighters, rockets, flights, planes 4264914, a craft capable of
vehicles, bomber, missions, submarines ... traveling in outer space
6 amps inverters, capacitor, oscillators, switches, mixer 2955247, electrical device characterized
transformer, windings, capacitors, circuits ... by its capacity to store an electric charge
7 compilers interfaces, algorithms, programming, software 6566077, written programs pertaining
modules, libraries, routines, tools, utilities ... to the operation of a computer system
Table 7: Some representative examples for candidate death sense clusters
Sl Candidate death cluster Vanished meaning
No. Word
1 slop jeans, velveteen, tweed, woollen, rubber, sealskin, wear clothes and bedding supplied to
oilskin, sheepskin, velvet, calico, deerskin, goatskin, cloth ... sailors by the navy
2 blackmail subsidy, rent, presents, tributes, money, fine, bribes Origin: denoting protection money
dues, tolls, contributions, contribution, customs, duties ... levied by Scottish chiefs
3 repertory dictionary, study, compendium, bibliography, lore, directory Origin: denoting an index
catalogues, science, catalog, annals, digest, literature ... or catalog: from late Latin repertorium
4 phrasing contour, outline, construction, handling, grouping, arrangement in the sense ?style or manner of
structure, modelling, selection, form ... expression?: via late Latin Greek phrasis
senses. The performance of our method has been
evaluated manually as well as by comparison with
WordNet and a list of slang words. Through man-
ual evaluation we found that the algorithm could
correctly identify 60.4% birth cases from a set of
48 random samples and 57% split/join cases from
a set of 21 randomly picked samples. Quite strik-
ingly, we observe that (i) in 44% cases the birth of
a novel sense is attested by WordNet, (ii) in 46%
cases the split of an older sense is signalled on
comparison with WordNet and (iii) in 43% cases
the join of two senses is attested by WordNet.
These results might have strong lexicographic im-
plications ? even if one goes by very moderate es-
timates almost half of the words would be candi-
date entries in WordNet if they were not already
part of it. This method can be extremely useful
in the construction of lexico-semantic networks
for low-resource languages, as well as for keeping
lexico-semantic resources up to date in general.
Future research directions based on this work
are manifold. On one hand, our method can be
used by lexicographers in designing new dictio-
naries where candidate new senses can be semi-
automatically detected and included, thus greatly
reducing the otherwise required manual effort.
On the other hand, this method can be directly
used for various NLP/IR applications like seman-
tic search, automatic word sense discovery as well
as disambiguation. For semantic search, taking
into account the newer senses of the word can in-
crease the relevance of the query result. Similarly,
a disambiguation engine informed with the newer
senses of a word can increase the efficiency of
disambiguation, and recognize senses uncovered
by the inventory that would otherwise have to be
wrongly assigned to covered senses. In addition,
this method can be also extended to the ?NNP?
part-of-speech (i.e., named entities) to identify
changes in role of a person/place. Furthermore,
it would be interesting to apply this method to lan-
guages other than English and to try to align new
senses of cognates across languages.
Acknowledgements
AM would like to thank DAAD for supporting the
faculty exchange programme to TU Darmstadt.
PG would like to thank Google India Private Ltd.
for extending travel support to attend the confer-
ence. MR and CB have been supported by an IBM
SUR award and by LOEWE as part of the research
center Digital Humanities.
1028
References
J. Allan, R. Papka and V. Lavrenko. 1998. On-line
new event detection and tracking. In proceedings of
SIGIR, 37?45, Melbourne, Australia.
D. Bamman and G. Crane. 2011. Measuring Historical
Word Sense Variation. In proceedings of JCDL, 1?
10, New York, NY, USA.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, 73?80, New York, USA.
C. Biemann. 2011. Structure Discovery in Natural
Language. Springer Heidelberg Dordrecht London
New York. ISBN 978-3-642-25922-7.
D. Blei and J. Lafferty. 2006. Dynamic topic mod-
els. In proceedings of ICML, 113?120, Pittsburgh,
Pennsylvania.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayash and K. Kanzaki. 2009. Enhancing the
Japanese WordNet. In proceedings of workshop on
Asian Language Resources, 1?8, Suntec, Singapore.
P. Cook, J. H. Lau, M. Rundell, D. McCarthy, T. Bald-
win. 2013. A lexicographic appraisal of an auto-
matic approach for detecting new word senses. In
proceedings of eLex, 49-65, Tallinn, Estonia.
Y. Goldberg and J. Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In proceedings of the Joint
Conference on Lexical and Computational Seman-
tics (*SEM), 241?247, Atlanta, GA, USA.
G. Heyer, F. Holz and S. Teresniak. 2009. Change of
topics over time ? tracking topics by their change of
meaning. In proceedings of KDIR, Madeira, Portu-
gal.
N. Ide and J. Veronis. 1998. Introduction to the special
issue on word sense disambiguation: The state of the
art. Computational Linguistics, 24(1):1?40.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The sketch engine. In Proceedings of EU-
RALEX, 105?116, Lorient, France.
A. Kilgarriff and D. Tugwell. 2001. Word sketch: Ex-
traction and display of significant collocations for
lexicography. In proceedings of COLLOCATION:
Computational Extraction, Analysis and Exploita-
tion, 32?38, Toulouse, France.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In pro-
ceedings of ACL/EACL, 64?71, Madrid, Spain.
V. Loreto, A. Mukherjee and F. Tria. 2012. On the ori-
gin of the hierarchy of color names. PNAS, 109(18),
6819?6824.
S. K. Maity, T. M. Venkat and A. Mukherjee. 2012.
Opinion formation in time-varying social networks:
The case of the naming game. Phys. Rev. E, 86,
036110.
J. McAuley and J. Leskovec. 2012. Learning to dis-
cover social circles in ego networks. In proceedings
of NIPS, 548?556, Nevada, USA.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak and E. L. Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science, 331(6014):176?182.
R. Mihalcea and V. Nastase. 2012. Word epoch disam-
biguation: finding how words change over time. In
proceedings of ACL, 259?263, Jeju Island, Korea.
A. Mukherjee, F. Tria, A. Baronchelli, A. Puglisi and V.
Loreto. 2011. Aging in language dynamics. PLoS
ONE, 6(2): e16677.
R. Navigli. 2009. Word sense disambiguation: a sur-
vey. ACM Computing Surveys, 41(2):1?69.
P. P?a?akk?o and K. Lind?en. 2012. Finding a location
for a new word in WordNet. In proceedings of the
Global WordNet Conference, Matsue, Japan.
M. Riedl and C. Biemann. 2013. Scaling to large
3
data: An efficient and effective method to compute
distributional thesauri. In proceedings of EMNLP,
884?890, Seattle, Washington, USA.
M. Riedl, R. Steuer and C. Biemann. 2014. Distributed
distributional similarities of Google books over the
centuries. In proceedings of LREC, Reykjavik, Ice-
land.
P. Rychl?y and A. Kilgarriff. 2007. An efficient al-
gorithm for building a distributional thesaurus (and
other sketch engine developments). In proceedings
of ACL, poster and demo sessions, 41?44, Prague,
Czech Republic.
H. Sch?utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
K. Sp?ark-Jones. 1986. Synonymy and Semantic Clas-
sification. Edinburgh University Press. ISBN 0-
85224-517-3.
N. Tahmasebi, T. Risse and S. Dietze. 2011. Towards
automatic language evolution tracking: a study on
word sense tracking. In proceedings of EvoDyn, vol.
784, Bonn, Germany.
X. Wang and A. McCallum. 2006. Topics over time:
a non-Markov continuous-time model of topical
trends. In proceedings of KDD, 424?433, Philadel-
phia, PA, USA.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In proceed-
ings of the workshop on Detecting and Exploiting
Cultural Diversity on the Social Web, 35?40, Glas-
gow, Scotland, UK.
1029
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 19?27,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Sweeping through the Topic Space:
Bad luck? Roll again!
Martin Riedl and Chris Biemann
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
riedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
Abstract
Topic Models (TM) such as Latent Dirich-
let Allocation (LDA) are increasingly used
in Natural Language Processing applica-
tions. At this, the model parameters and
the influence of randomized sampling and
inference are rarely examined ? usually,
the recommendations from the original pa-
pers are adopted. In this paper, we ex-
amine the parameter space of LDA topic
models with respect to the application of
Text Segmentation (TS), specifically target-
ing error rates and their variance across dif-
ferent runs. We find that the recommended
settings result in error rates far from opti-
mal for our application. We show substan-
tial variance in the results for different runs
of model estimation and inference, and give
recommendations for increasing the robust-
ness and stability of topic models. Run-
ning the inference step several times and se-
lecting the last topic ID assigned per token,
shows considerable improvements. Similar
improvements are achieved with the mode
method: We store all assigned topic IDs
during each inference iteration step and se-
lect the most frequent topic ID assigned to
each word. These recommendations do not
only apply to TS, but are generic enough to
transfer to other applications.
1 Introduction
With the rise of topic models such as pLSI (Hof-
mann, 2001) or LDA (Blei et al, 2003) in Nat-
ural Language Processing (NLP), an increasing
number of works in the field use topic models to
map terms from a high-dimensional word space
to a lower-dimensional semantic space. TMs
are ?the new Latent Semantic Analysis? (LSA),
(Deerwester et al, 1990), and it has been shown
that generative models like pLSI and LDA not
only have a better mathematical foundation rooted
in probability theory, but also outperform LSA in
document retrieval and classification, e.g. (Hof-
mann, 2001; Blei et al, 2003; Biro et al, 2008).
To estimate the model parameters in LDA, the ex-
act computation that was straightforward in LSA
(matrix factorization) is replaced by a randomized
Monte-Carlo sampling procedure (e.g. variational
Bayes or Gibbs sampling).
Aside from the main parameter, the number
of topics or dimensions, surprisingly little atten-
tion has been spent to understand the interac-
tions of hyperparameters, the number of sam-
pling iterations in model estimation and inter-
ference, and the stability of topic assignments
across runs using different random seeds. While
progress in the field of topic modeling is mainly
made by adjusting prior distributions (e.g. (Sato
and Nakagawa, 2010; Wallach et al, 2009)), or
defining more complex model mixtures (Heinrich,
2011), it seems unclear whether improvements,
reached on intrinsic measures like perplexity or
on application-based evaluations, are due to an
improved model structure or could originate from
sub-optimal parameter settings or literally ?bad
luck? due to the randomized nature of the sam-
pling process.
In this paper, we address these issues by sys-
tematically sweeping the parameter space. For
this, we pick LDA since it is the most commonly
used TM in the field of NLP. To evaluate the con-
tribution of the TM, we choose the task of TS:
this task has received considerable interest from
the NLP community, standard datasets and eval-
uation measures are available for testing, and it
19
has been shown that this task considerably bene-
fits from the use of TMs, see (Misra et al, 2009;
Sun et al, 2008; Eisenstein, 2009).
This paper is organized as follows: In the next
section, we present related work regarding text
segmentation using topic models and topic model
parameter evaluations. Section 3 defines the Top-
icTiling text segmentation algorithm, which is a
simplified version of TextTiling (Hearst, 1994),
and makes direct use of topic assignments. Its
simplicity allows us to observe direct conse-
quences of LDA parameter settings. Further, we
describe the experimental setup, our application-
based evaluation methodology including the data
set and the LDA parameters we vary in Section 4.
Results of our experiments in Section 5 indi-
cate that a) there is an optimal range for the num-
ber of topics, b) there is considerable variance in
performance for different runs for both model es-
timation and inference, c) increasing the number
of sampling iterations stabilizes average perfor-
mance but does not make TMs more robust, but d)
combining the output of several independent sam-
pling runs does, and additionally leads to large er-
ror rate reductions. Similar results are obtained by
e) the mode method with less computational costs
using the most frequent topic ID that is assigned
during different inference iteration steps. In the
conclusion, we give recommendations to add sta-
bility and robustness for TMs: aside from opti-
mization of the hyperparameters, we recommend
combining the topic assignments of different in-
ference iterations, and/or of different independent
inference runs.
2 Related Work
2.1 Text Segmentation with Topic Models
Based on the observation of Halliday and Hasan
(1976) that the density of coherence relations is
higher within segments than between segments,
most algorithms compute a coherence score to
measure the difference of textual units for inform-
ing a segmentation decision. TextTiling (Hearst,
1994) relies on the simplest coherence relation ?
word repetition ? and computes similarities be-
tween textual units based on the similarities of
word space vectors. The task of text segmenta-
tion is to decide, for a given text, how to split this
text into segments.
Related to our algorithm (see Section 3.1) are
the approaches described in Misra et al (2009)
and Sun et al (2008): topic modeling is used to
alleviate the sparsity of word vectors by mapping
words into a topic space. This is done by extend-
ing the dynamic programming algorithms from
(Utiyama and Isahara, 2000; Fragkou et al, 2004)
using topic models. At this, the topic assignments
have to be inferred for each possible segment.
2.2 LDA and Topic Model Evaluation
For topic modeling, we use the widely applied
LDA (Blei et al, 2003), This model uses a train-
ing corpus of documents to create document-topic
and topic-word distributions and is parameterized
by the number of topics T as well as by two
hyperparameters. To generate a document, the
topic proportions are drawn using a Dirichlet dis-
tribution with hyperparameter ?. Adjacent for
each word w a topic zdw is chosen according to
a multinomial distribution using hyperparameter
?zdw . The model is estimated using m itera-
tions of Gibbs sampling. Unseen documents can
be annotated with an existing topic model using
Bayesian inference methods. At this, Gibbs sam-
pling with i iterations is used to estimate the topic
ID for each word, given the topics of the other
words in the same sentential unit. After inference,
every word in every sentence receives a topic ID,
which is the sole information that is used by the
TopicTiling algorithm to determine the segmenta-
tion. We use the GibbsLDA implementation by
Phan and Nguyen (2007) for all our experiments.
The article of Blei et al (2003) compares LDA
with pLSI and Mixture Unigram models using the
perplexity of the model. In a collaborative filter-
ing evaluation for different numbers of topics they
observe that using too many topics leads to over-
fitting and to worse results.
In the field of topic model evaluations, Griffiths
and Steyvers (2004) use a corpus of abstracts pub-
lished between 1991 and 2001 and evaluate model
perplexity. For this particular corpus, they achieve
the lowest perplexity using 300 topics. Further-
more, they compare different sampling methods
and show that the perplexity converges faster with
Gibbs sampling than with expectation propaga-
tion and variational Bayes. On a small artificial
testset, small variations in perplexity across dif-
ferent runs were observed in early sampling itera-
tions, but all runs converged to the same limit.
20
In Wallach et al (2009) topic models are eval-
uated with symmetric and asymmetric hyperpa-
rameters based on the perplexity. They observe
a benefit using asymmetric parameters for ?, but
cannot show improvement with asymmetric priors
for ?.
3 Method
3.1 TopicTiling
For the evaluation of the topic models, a text seg-
mentation algorithm called TopicTiling is used
here. This algorithm is a newly developed al-
gorithm based on TextTiling (Hearst, 1994) and
achieves state of the art results using the Choi
dataset, which is a standard dataset for TS eval-
uation. The algorithm uses sentences as minimal
units. Instead of words, we use topic IDs that
are assigned to each word using the LDA infer-
ence running on sentence units. The LDA model
should be estimated on a corpus of documents that
is similar to the to-be-segmented documents.
To measure the coherence cp between two sen-
tences around position p, the cosine similarity
(vector dot product) between these two adjacent
sentences is computed. Each sentence is repre-
sented as a T -dimensional vector, where T is the
number of topic IDs defined in the topic model.
The t-th element of the vector contains the num-
ber of times the t-th topic is observed in the sen-
tence. Similar to the TextTiling algorithm, lo-
cal minima calculated from these similarity scores
are taken as segmentation candidates.
This is illustrated in Figure 1, where the simi-
larity scores between adjacent sentences are plot-
ted. The vertical lines in this plot indicate all local
minima found.
0 5 10 15 20 25 300.0
0.2
0.4
Sentence
cosine
 simila
rity
Figure 1: Cosine similarity scores of adjacent sen-
tences based on topic distribution vectors. Vertical
lines (solid and dashed) indicate local minima. Solid
lines mark segments that have a depth score above a
chosen threshold.
Following the TextTiling definition, not the
minimum score cp at position p itself is used, but
a depth score dp for position p computed by
di = 1/2 ? (cp?1 ? cp + cp+1 ? cp). (1)
In contrast to TextTiling, the directly neighboring
similarity scores of the local minima are used, if
they are higher than cp. When using topics instead
of words, it can be expected that sentences within
one segment have many topics in common, which
leads to cosine similarities close to 1. Further, us-
ing topic IDs instead of words greatly increases
sparsity. A minimum in the curve indicates a
change in topic distribution. Segment boundaries
are set at the positions of the n highest depth-
scores, which is common practice in text segmen-
tation algorithms. An alternative to a given n
would be the selection of segments according to
a depth score threshold.
4 Experimental Setup
As dataset the Choi dataset (Choi, 2000) is used.
This dataset is an artificially generated corpus that
consists of 700 documents. Each document con-
sists of 10 segments and each segment has 3?
11 sentences extracted from a document of the
Brown corpus. For the first setup, we perform a
10-fold Cross Validation (CV) for estimating the
TM (estimating on 630 documents at a time), for
the other setups we use 600 documents for TM
estimation and the remaining 100 documents for
testing. While we aim to neglect using the same
documents for training and testing, it is not guar-
anteed that all testing data is unseen, since the
same source sentences can find their way in sev-
eral artificially crafted ?documents?. This prob-
lem, however, applies for all evaluations on this
dataset that use any kind of training, be it LDA
models in Misra et al (2009) or TF-IDF values in
Fragkou et al (2004).
For the evaluation of the Topic Model in combi-
nation of Text Segmentation, we use the Pk mea-
sure (Beeferman et al, 1999), which is a stan-
dard measure for error rates in the field of TS.
This measure compares the gold standard seg-
mentation with the output of the algorithm. A
Pk value of 0 indicates a perfect segmentation,
the averaged state of the art on the Choi Dataset
is Pk = 0.0275 (Misra et al, 2009). To assess
the robustness of the TM, we sweep over varying
21
configurations of the LDA model, and plot the re-
sults using Box-and-Whiskers plots: the box in-
dicates the quartiles and the whiskers are maxi-
mal 1.5 times of the Interquartile Range (IQR) or
equal to the data point that is no greater to the 1.5
IQR. The following parameters are subject to our
exploration:
? T : Number of topics used in the LDA model.
Common values vary between 50 and 500.
? ? : Hyperparameter that regulates the sparse-
ness topic-per-document distribution. Lower
values result in documents being represented
by fewer topics (Heinrich, 2004). Recom-
mended: ? = 50/T (Griffiths and Steyvers,
2004)
? ? : Reducing ? increases the sparsity of
topics, by assigning fewer terms to each
topic, which is correlated to how related
words need to be, to be assigned to a topic
(Heinrich, 2004). Recommended: ? =
{0.1, 0.01} (Griffiths and Steyvers, 2004;
Misra et al, 2009)
? m Model estimation iterations. Recom-
mended / common settings: m = 500?5000
(Griffiths and Steyvers, 2004; Wallach et al,
2009; Phan and Nguyen, 2007)
? i Inference iterations. Recommended / com-
mon settings: 100 (Phan and Nguyen, 2007)
? d Mode of topic assignments. At each in-
ference iteration step, a topic ID is assigned
to each word within a document (represented
as a sentence in our application). With this
option, we count these topic assignments for
each single word in each iteration. After all i
inference iterations, the most frequent topic
ID is chosen for each word in a document.
? r Number of inference runs: We repeat the
inference r times and assign the most fre-
quently assigned topic per word at the fi-
nal inference run for the segmentation algo-
rithm. High r values might reduce fluctua-
tions due to the randomized process and lead
to a more stable word-to-topic assignment.
All introduced parameters parameterize the TM.
We are not aware of any research that has used
several inference runs r and the mode of topic as-
signments d to increase stability and varying TM
parameters in combinations with measures other
then perplexity.
5 Results
In this section, we present the results we obtained
from varying the parameters under examination.
5.1 Number of Topics T
To provide a first impression of the data, a 10-fold
CV is calculated and the segmentation results are
visualized in Figure 2.
Topic Number
P_k 
valu
e
0.0
0.1
0.2
0.3
0.4
0.5
3 10 20 50 100 250 500
l
l
l l
l
l
l
l l l l l l
l l l
ll
lll l
l
l
ll
l
ll
l
l
l
l
l
l
ll
l
ll l
l l
l
l
lll l
l
l
ll
l
l
ll
l
ll
l
l
l
l
ll
l
l
lll
l
l l
l
l
l
l
l
l
l
l
l
llll
l ll l
l
l
Figure 2: Box plots for different number of topics T .
Each box plot is generated from the average Pk value
of 700 documents, ? = 50/T , ? = 0.1, m = 1000,
i = 100, r = 1. These documents are segmented with
TopicTiling using a 10-folded CV.
Each box plot is generated from the Pk values
of 700 documents. As expected, there is a contin-
uous range of topic numbers, namely between 50
and 150 topics, where we observe the lowest Pk
values. Using too many topics leads to overfitting
of the data and too few topics result in too gen-
eral distinctions to grasp text segments. This is in
line with other studies, that determine an optimum
for T , cf. (Griffiths and Steyvers, 2004), which is
specific to the application and the data set.
5.2 Estimation and Inference iterations
The next step examines the robustness of the topic
model according to the number of model estima-
tion iterations m needed to achieve stable results.
600 documents are used to train the LDA model
22
that is applied by TopicTiling to segment the re-
maining 100 documents. From Figure 2 we know
that sampling 100 topics leads to good results.
To have an insight into unstable topic regions we
also inspect performance at different sampling it-
erations using 20 and 250 topics. To assess sta-
bility across different model estimation runs, we
trained 30 LDA models using different random
seeds. Each box plot in Figures 3 and 4 is gen-
erated from 30 mean values, calculated from the
Pk values of the 100 documents. The variation
indicates the score variance for the 30 different
models.
Number of topics: 100
number of sample iterations
P_k 
valu
e
0.0
0.1
0.2
0.3
0.4
2 3 5 10 20 50 100 300 500 1000
l l l l l l
l
l
l
l
l
l l l l l l l l l l l l l l
l
l
l
l
l
l l
0.02
0.04
0.06
0.08
0.10
50 100 300 500 1000
l
l
l
l l l l l l l l l l l l l
l
l
l l
Figure 3: Box plots with different model estimation
iterations m, with T=100, ? = 50/T , ? = 0.1, i =
100, r = 1. Each box plot is generated from 30 mean
values calculated from 100 documents.
Using 100 topics (see Figure 3), the burn-in
phase starts with 8?10 iterations and the mean Pk
values stabilize after 40 iterations. But looking
at the inset for large m values, significant vari-
ations between the different models can be ob-
served: note that the Pk error rates are almost
double between the lower and the upper whisker.
These remain constant and do not disappear for
largerm values: The whiskers span error rates be-
tween 0.021 - 0.037 for model estimation on doc-
ument units
With 20 topics, the Pk values are worse as with
100 topics, as expected from Figure 2. Here the
convergence starts at 100 sample iterations. More
interesting results are achieved with 250 topics.
A robust range for the error rates can be found be-
tween 20 and 100 sample iterations. With more
iterations m, the results get both worse and un-
stable: as the ?natural? topics of the collection
have to be split in too many topics in the model,
perplexity optimizations that drive the estimation
process lead to random fluctuations, which the
TopicTiling algorithm is sensitive to. Manual in-
spection of models for T = 250 revealed that in
fact many topics do not stay stable across estima-
tion iterations.
number of inference iterations
P_k 
valu
e
0.01
0.02
0.03
0.04
2 3 5 10 20 50 100
l
l l l l l l l l l l l l l l l
l
l
l l
Figure 5: Figure of box plots for different inference
iterations i and m = 1000, T = 100, ? = 50/T ,
? = 0.1, r = 1 .
In the next step we sweep over several infer-
ence iterations i. Starting from 5 iterations, error
rates do not change much, see Figure 5. But there
is still substantial variance, between about 0.019 -
0.038 for inference on sentence units.
5.3 Number of inference runs r
To decrease this variance, we assign the topic not
only from a singe inference run, but repeat the in-
ference calculations several times, denoted by the
parameter r. Then the frequency of assigned topic
IDs per token is counted across the r runs, and we
assign the most frequent topic ID (frequency ties
are broken randomly). The box plot for several
evaluated values of r is shown in Figure 6.
This log-scaled plot shows that both variance
and Pk error rate can be substantially decreased.
Already for r = 3, we observe a significant im-
provement in comparison to the default setting of
r = 1 and with increasing r values, the error rates
are reduced even more: for r = 20, variance and
error rates are is cut in less than half of their orig-
inal values using this simple operation.
23
Number of topics: 20
number of sample iterations
P_k 
valu
e
0.1
0.2
0.3
0.4
2 3 5 10 20 50 100 300 500 1000
l l l l l l l
l
l
l
l
l
l
l l l l l l l l l l l l
l
l l l
l
l
l
l
l l l l
l
l
0.02
0.04
0.06
0.08
0.10
50 100 300 500 1000
l
l
l l l l l l l l l l l l
l
l
l
l
l l l l
l
l
Number of topics: 250
number of sample iterations
P_k 
valu
e
0.1
0.2
0.3
0.4
2 3 5 10 20 50 100 300 500 1000
l l l l l l
l
l
l
l
l
l l l l l l l
l l l l l l l
l
l
l
l
l
l
l
l
l
l
0.02
0.04
0.06
0.08
0.10
50 100 300 500 1000
l
l
l l l l l l l
l l l l l l l
l
ll
l
l
Figure 4: Box plots with varying model estimation iterations m applied with T = 20 (left) and T = 250 (right)
topics, ? = 50/T , ? = 0.1, i = 100, r = 1
number of repeated inferences
P_k
 valu
e
0.01
0.02
0.03
0.04
1 3 5 10 20
l
l
l
l l
l
Figure 6: Box plot for several inference runs r, to as-
sign the topics to a word with m = 1000, i = 100,
T = 100, ? = 50/T , ? = 0.1.
5.4 Mode of topic assignment d
In the previous experiment, we use the topic IDs
that have been assigned most frequently at the last
inference iteration step. Now, we examine some-
thing similar, but for all i inference steps of a sin-
gle inference run: we select the mode of topic
ID assignments for each word across all inference
steps. The impact of this method on error and
variance is illustrated in Figure 7. Using a sin-
gle inference iteration, the topic IDs are almost
assigned randomly. After 20 inference iterations
Pk values below 0.02 are achieved. Using further
iterations, the decrease of the error rate is only
number of inference iterations
P_k 
valu
e
0.01
0.02
0.03
0.04
2 3 5 10 20 50 100
l
l
l l l ll
l
l
Figure 7: Box plot using the mode method d = true
with several inference iterations i with m = 500, T =
100, ? = 50/T , ? = 0.1.
marginal. In comparison to the repeated inference
method, the additional computational costs of this
method are much lower as the inference iterations
have to be carried out anyway in the default appli-
cation setting.
5.5 Hyperparameters ? and ?
In many previous works, hyperparameter settings
? = 50/T and ? = {0.1, 0.01} are commonly
used. In the next series of experiments we inves-
tigate how different parameters of these both pa-
rameters can change the TS task.
For ? values, shown in Figure 8, we can see
that the recommended value for T = 100 , ? =
24
0.5 leads to sub-optimal results, and an error rate
reduction of about 40% can be realized by setting
? = 0.1.
alpha values
P_k 
valu
e
0.01
0.02
0.03
0.04
0.01 0.02 0.03 0.05 0.1 0.2 0.5 1
l l l l
l l
l
l
l l
l
l
l
l
l
l
l
l
Figure 8: Box plot for several alpha values ?withm =
500, i = 100, T = 100, ? = 0.1, r = 1.
Regarding values of ?, we find that Pk rates
and their variance are relatively stable between
the recommended settings of 0.1 and 0.01. Values
larger than 0.1 lead to much worse performance.
Regarding variance, no patterns within the stable
range emerge, see Figure 9.
beta values
P_k 
valu
e
0.05
0.10
0.15
0.01 0.02 0.03 0.05 0.1 0.2 0.5
l l l l l
l l l l
l
l
l
l
l
l
l
l
l
l
Figure 9: Box plot for several beta values ? with m =
500, i = 100, T = 100, ? = 50/T , r = 1.
5.6 Putting it all together
Until this point, we have examined different pa-
rameters with respect to stability and error rates
one at the time. Now, we combine what we have
System Pk error ?2 var.
red. red.
default 0.0302 0.00% 2.02e-5 0.00%
? = 0.1 0.0183 39.53% 1.22e-5 39.77%
r = 20 0.0127 57.86% 4.65e-6 76.97%
d = true 0.0137 54.62% 3.99e-6 80.21%
combined 0.0141 53.45% 9.17e-6 54.55%
Table 1: Comparison of single parameter optimiza-
tions, and combined system. Pk averages and variance
are computed over 30 runs, together with reductions
relative to the default setting. Default: ? = 0.5, r = 1.
combined: ? = 0.1, r = 20, d = true
learned from this and strive at optimal system per-
formance. For this, we contrast TS results ob-
tained with the default LDA configuration with
the best systems obtained by optimization of sin-
gle parameters, as well as to a system that uses
these optimal settings for all parameters. Table 1
shows Pk error rates for the different systems. At
this, we fixed the following parameters: T = 100,
m = 500, i = 100, ? = 0.1. For the computa-
tions we use 600 documents for the LDA model
estimation, apply TopicTiling and compute the er-
ror rate for the 100 remaining documents and re-
peat this 30 times with different random seeds.
We can observe a massive improvement for op-
timized single parameters. The ?-tuning tuning
results in an error rate reduction of 39.77% in
comparison to the default configurations. Using
r = 20, the error rate is cut in less than half
its original value. Also for the mode mechanism
(d = true) the error rate is halved but slightly
worse than than when using the repeated infer-
ence. Using combined optimized parameters does
not result to additional error decreases. We at-
tribute the slight decline of the combined method
in both in the error rate Pk and in the variance to
complex parameter interactions that shall be ex-
amined in further work. In Figure 10, we visual-
ize these results in a density plot. It becomes clear
that repeated inference leads to slightly better and
more robust performance (higher peak) than the
mode method. We attribute the difference to sit-
uations, where there are several highly probable
topics in our sampling units, and by chance the
same one is picked for adjacent sentences that be-
long to different segments, resulting in failure to
recognize the segmentation point. However, since
the differences are miniscule, only using the mode
method might be more suitable for practical pur-
poses since its computational cost is lower.
25
0.00 0.01 0.02 0.03 0.04 0.05
0
50
100
150
P_k values
Den
sity
default valuesalpha=0.01r=20d=truecombined
Figure 10: Density plot of the error distributions for
the systems listed in Table 1
6 Conclusion
In this paper, we examined the robustness of LDA
topic models with respect to the application of
Text Segmentation by sweeping through the topic
model parameter space. To our knowledge, this is
the first attempt to systematically assess the sta-
bility of topic models in a NLP task.
The results of our experiments are summarized
as follows:
? Perform the inference r times using the same
model and choosing the assigned topic ID
per word token taken from the last infer-
ence iteration, improves both error rates and
stability across runs with different random
seeds.
? Almost equal performance in terms of er-
ror and stability is achieved with the mode
mechanism: choose the most frequent topic
ID assignment per word across inference
steps. While error rates were slightly higher
for our data set, this method is probably
preferable in practice because of its lower
computation costs.
? As found in other studies, there is a range for
the number of topics T , where optimal re-
sults are obtained. In our task, performance
showed to be robust in the range of 50 - 150
topics.
? The default setting for LDA hyperparameters
? and ? can lead to sub-optimal results. Es-
pecially ? should be optimized for the task at
hand, as the utility of the topic model is very
sensitive to this parameter.
? While the number of iterations for model es-
timation and inference needed for conver-
gence is depending on the number of topics,
the size of the sampling unit (document) and
the collection, it should be noted that after
convergence the variance between different
sampling runs does not decrease for a larger
number of iterations.
Equipped with the insights gained from exper-
iments on single parameter variation, we were
able to implement a very simple algorithm for text
segmentation that improves over the state of the
art on a standard dataset by a large margin. At
this, the combination of the optimal ?, and a high
number of inference repetitions r and the mode
method (d = true) produced slightly more errors
than a high r alone. While the purpose of this pa-
per was mainly to address robustness and stability
issues of topic models, we are planning to apply
the segmentation algorithm to further datasets.
The most important takeaway, however, is that
especially for small sampling units like sentences,
tremendous improvements in applications can be
obtained when looking at multiple inference as-
signments and using the most frequently assigned
topic ID in subsequent processing ? either across
diffeent inference steps or across diffeent infer-
ence runs. These two new strategies seem to be
able to offset sub-optimal hyperparameters to a
certain extent. This scheme is not only applica-
ble to Text Segmentation, but in all applications
where performance crucially depends on stable
topic ID assignments per token. Extensions to
this scheme, like ignoring tokens with a high topic
variability (stop words or general terms) or dy-
namically deciding to conflate several topics be-
cause of their per-token co-occurrence, are left for
future work.
7 Acknowledgments
This work has been supported by the Hessian
research excellence program ?Landes-Offensive
zur Entwicklung Wissenschaftlich-konomischer
Exzellenz? (LOEWE) as part of the research cen-
ter ?Digital Humanities?. We would also thank
the anonymous reviewers for their comments,
which greatly helped to improve the paper.
26
References
D. Beeferman, A. Berger, and J. Lafferty. 1999.
Statistical models for text segmentation. Machine
learning, 34(1):177?210.
Istvan Biro, Andras Benczur, Jacint Szabo, and Ana
Maguitman. 2008. A comparative analysis of la-
tent variable models for web page classification. In
Proceedings of the 2008 Latin American Web Con-
ference, pages 23?28, Washington, DC, USA. IEEE
Computer Society.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 26?33, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Sci-
ence, 41(6):391?407.
Jacob Eisenstein. 2009. Hierarchical text segmenta-
tion from multi-scale lexical cohesion. Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics on -
NAACL ?09, page 353.
P. Fragkou, V. Petridis, and Ath. Kehagias. 2004. A
Dynamic Programming Algorithm for Linear Text
Segmentation. Journal of Intelligent Information
Systems, 23(2):179?197, September.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. PNAS, 101(suppl. 1):5228?
5235.
M A K Halliday and Ruqaiya Hasan. 1976. Cohesion
in English, volume 1 of English Language Series.
Longman.
Marti a. Hearst. 1994. Multi-paragraph segmentation
of expository text. Proceedings of the 32nd annual
meeting on Association for Computational Linguis-
tics, (Hearst):9?16.
Gregor Heinrich. 2004. Parameter estimation for text
analysis. Technical report.
Gregor Heinrich. 2011. Typology of mixed-
membership models: Towards a design method.
In Machine Learning and Knowledge Discovery in
Databases, volume 6912 of Lecture Notes in Com-
puter Science, pages 32?47. Springer Berlin / Hei-
delberg. 10.1007/978-3-642-23783-6 3.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Computer,
pages 177?196.
Hemant Misra, Joemon M Jose, and Olivier Cappe?.
2009. Text Segmentation via Topic Modeling : An
Analytical Study. In Proceeding of the 18th ACM
Conference on Information and Knowledge Man-
agement - CIKM ?09, pages 1553?-1556.
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007.
GibbsLDA++: A C/C++ implementa-
tion of latent Dirichlet alocation (LDA).
http://jgibblda.sourceforge.net/.
Issei Sato and Hiroshi Nakagawa. 2010. Topic mod-
els with power-law using pitman-yor process cate-
gories and subject descriptors. Science And Tech-
nology, (1):673?681.
Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.
2008. Text segmentation with LDA-based Fisher
kernel. Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics on
Human Language Technologies Short Papers - HLT
?08, (June):269.
Masao Utiyama and Hitoshi Isahara. 2000. A Statis-
tical Model for Domain-Independent Text Segmen-
tation. Communications.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
NIPS.
27
Proceedings of the 2012 Student Research Workshop, pages 37?42,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
TopicTiling: A Text Segmentation Algorithm based on LDA
Martin Riedl and Chris Biemann
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biemann}@cs.tu-darmstadt.de
Abstract
This work presents a Text Segmentation al-
gorithm called TopicTiling. This algorithm
is based on the well-known TextTiling algo-
rithm, and segments documents using the La-
tent Dirichlet Allocation (LDA) topic model.
We show that using the mode topic ID as-
signed during the inference method of LDA,
used to annotate unseen documents, improves
performance by stabilizing the obtained top-
ics. We show significant improvements over
state of the art segmentation algorithms on two
standard datasets. As an additional benefit,
TopicTiling performs the segmentation in lin-
ear time and thus is computationally less ex-
pensive than other LDA-based segmentation
methods.
1 Introduction
The task tackled in this paper is Text Segmentation
(TS), which is to be understood as the segmentation
of texts into topically similar units. This implies,
viewing the text as a sequence of subtopics, that a
subtopic change marks a new segment. The chal-
lenge for a text segmentation algorithm is to find the
sub-topical structure of a text.
In this work, this semantic information is gained
from Topic Models (TMs). We introduce a newly
developed TS algorithm called TopicTiling. The
core algorithm is a simplified version of TextTil-
ing (Hearst, 1994), where blocks of text are com-
pared via bag-of-word vectors. TopicTiling uses
topic IDs, obtained by the LDA inference method,
instead of words. As some of the topic IDs ob-
tained by the inference method tend to change for
different runs, we recommend to use the most prob-
able topic ID assigned during the inference. We de-
note this most probable topic ID as the mode (most
frequent across all inference steps) of the topic as-
signment. These IDs are used to calculate the co-
sine similarity between two adjacent blocks of sen-
tences, represented as two vectors, containing the
frequency of each topic ID. Without parameter opti-
mization we obtain state-of-the-art results based on
the Choi dataset (Choi, 2000). We show that the
mode assignment improves the results substantially
and improves even more when parameterizing the
size of sampled blocks using a window size param-
eter. Using these optimizations, we obtain signif-
icant improvements compared to other algorithms
based on the Choi dataset and also on a more diffi-
cult Wall Street Journal (WSJ) corpus provided by
Galley et al (2003). Not only does TopicTiling
deliver state-of-the-art segmentation results, it also
performs the segmentation in linear time, as opposed
to most other recent TS algorithms.
The paper is organized as follows: The next sec-
tion gives an overview of text segmentation algo-
rithms. Section 3 introduces the TopicTiling TS al-
gorithm. The Choi and the Galley datasets used
to measure the performance of TopicTiling are de-
scribed in Section 4. In the evaluation section, the
results of TopicTiling are demonstrated on these
datasets, followed by a conclusion and discussion.
2 Related Work
TS can be divided into two sub-fields: (i) linear
TS and (ii) hierarchical TS. Whereas linear TS
deals with the sequential analysis of topical changes,
37
hierarchical segmentation is concerned with find-
ing more fine grained subtopic structures in texts.
One of the first unsupervised linear TS algorithms
was introduced by Hearst (1994): TextTiling seg-
ments texts in linear time by calculating the sim-
ilarity between two blocks of words based on the
cosine similarity. The calculation is accomplished
by two vectors containing the number of occur-
ring terms of each block. LcSeg (Galley et al,
2003), a TextTiling-based algorithm, uses tf-idf term
weights and improved TS results compared to Text-
Tiling. Utiyama and Isahara (2001) introduced one
of the first probabilistic approaches using Dynamic
Programming (DP) called U00. Related to our work
are the DP approaches described in Misra et al
(2009) and Sun et al (2008): here, topic modeling is
used to alleviate the sparsity of word vectors. This
approach was extended by (Misra et al, 2009) and
(Sun et al, 2008) using topic information achieved
from the LDA topic model. The first hierarchical
algorithm was proposed by Yaari (1997), using the
cosine similarity and agglomerative clustering ap-
proaches. A hierarchical Bayesian algorithm based
on LDA is introduced with Eisenstein (2009). In our
work, however, we focus on linear TS.
LDA was introduced by Blei et al (2003) and is
a generative model that discovers topics based on a
training corpus. Model training estimates two dis-
tributions: A topic-word distribution and a topic-
document distribution. As LDA is a generative prob-
abilistic model, the creation process follows a gen-
erative story: First, for each document a topic distri-
bution is sampled. Then, for each document, words
are randomly chosen, following the previously sam-
pled topic distribution. Using the Gibbs inference
method, LDA is used to apply a trained model for
unseen documents. Here, words are annotated by
topic IDs by assigning a topic ID sampled by the
document-word and word-topic distribution. Note
that the inference procedure, in particular, marks the
difference between LDA and earlier dimensionality
reduction techniques such as Latent Semantic Anal-
ysis.
3 TopicTiling
This section introduces the TopicTiling algorithm,
first introduced in (Riedl and Biemann, 2012a).
In contrast to the quite similar TextTiling algo-
rithm, TopicTiling is not based on words, but on
the last topic IDs assigned by the Bayesian Infer-
ence method of LDA. This increases sparsity since
the word space is reduced to a topic space of much
lower dimension. Therefore, the documents that are
to be segmented have first to be annotated with topic
IDs. For useful topic distinctions, however, the topic
model must be trained on documents similar in con-
tent to the test documents. Preliminary experiments
have shown that repeating the Bayesian inference,
often leads to different topic distributions for a given
sentence in several runs. Memorizing each topic ID
assigned to a word in a document during each in-
ference step can alleviate this instability, which is
rooted in the probabilistic nature of LDA. After fin-
ishing the inference on the unseen documents, we
select the most frequent topic ID for each word and
assign it to the word. We call this method the mode
of a topic assignment, denoted with d = true in
the remainder (Riedl and Biemann, 2012b). Note
that this is different from using the overall topic dis-
tribution as determined by the inference step, since
this winner-takes-it-all approach reduces noise from
random fluctuations. As this parameter stabilizes
the topic IDs at low computational costs, we rec-
ommend using this option in all setups where subse-
quent steps rely on a single topic assignment.
TopicTiling assumes a sentence si as the small-
est basic unit. At each position p, located between
two adjacent sentences, a coherence score cp is cal-
culated. With w we introduce a so-called window
parameter that specifies the number of sentences to
the left and to the right of position p that define two
blocks: sp?w, . . . , sp and sp+1, . . . , sp+w+1. In con-
trast to the mode topic assignment parameter d, we
cannot state a recommended value for w, as this pa-
rameter is dependent on the number of sentences a
segment should contain. This is conditioned on the
corpus that is segmented.
To calculate the coherence score, we exclusively
use the topic IDs assigned to the words by infer-
ence: Assuming an LDA model with T topics, each
block is represented as a T -dimensional vector. The
t-th element of each vector contains the frequency
of the topic ID t obtained from the according block.
The coherence score is calculated by the vector dot
product, also referred to as cosine similarity. Val-
38
ues close to zero indicate marginal relatedness be-
tween two adjacent blocks, whereas values close to
one denote a substantial connectivity. Next, the co-
herence scores are plotted to trace the local minima.
These minima are utilized as possible segmentation
boundaries. But rather using the cp values itself, a
depth score dp is calculated for each minimum (cf.
TextTiling, (Hearst, 1994)). In comparison to Topic-
Tiling, TextTiling calculates the depth score for each
position and than searches for maxima. The depth
score measures the deepness of a minimum by look-
ing at the highest coherence scores on the left and on
the right and is calculated using following formula:
dp = 1/2(hl(p)? cp + hr(p)? cp).
The function hl(p) iterates to the left as long as
the score increases and returns the highest coherence
score value. The same is done, iterating in the other
direction with the hr(p) function. If the number of
segments n is given as input, the n highest depth
scores are used as segment boundaries. Otherwise, a
threshold is applied (cf. TextTiling). This threshold
predicts a segment if the depth score is larger than
? ? ?/2, with ? being the mean and ? being the
standard variation calculated on the depth scores.
The algorithm runtime is linear in the number of
possible segmentation points, i.e. the number of sen-
tences: for each segmentation point, the two adja-
cent blocks are sampled separately and combined
into the coherence score. This, and the parameters d
and w, are the main differences to the dynamic pro-
gramming approaches for TS described in (Utiyama
and Isahara, 2001; Misra et al, 2009).
4 Data Sets
The performance of the introduced algorithm is
demonstrated using two datasets: A dataset pro-
posed by Choi and another more challenging one as-
sembled by Galley.
4.1 Choi Dataset
The Choi dataset (Choi, 2000) is commonly used in
the field of TS (see e.g. (Misra et al, 2009; Sun et
al., 2008; Galley et al, 2003)). It is a corpus, gen-
erated artificially from the Brown corpus and con-
sists of 700 documents. For document generation,
ten segments of 3-11 sentences each, taken from dif-
ferent documents, are combined forming one doc-
ument. 400 documents consist of segments with a
sentence length of 3-11 sentences and there are 100
documents each with sentence lengths of 3-5, 6-8
and 9-11.
4.2 Galley Dataset
Galley et al (2003) present two corpora for writ-
ten language, each having 500 documents, which are
also generated artificially. In comparison to Choi?s
dataset, the segments in its ?documents? vary from 4
to 22 segments, and are composed by concatenat-
ing full source documents. One dataset is gener-
ated based on WSJ documents of the Penn Treebank
(PTB) project (Marcus et al, 1994) and the other is
based on Topic Detection Track (TDT) documents
(Wayne, 1998). As the WSJ dataset seems to be
harder (consistently higher error rates across several
works), we use this dataset for experimentation.
5 Evaluation
The performance of TopicTiling is evaluated using
two measures, commonly used in the TS task: The
Pk measure and the WindowDiff (WD) measure
(Beeferman et al, 1999; Pevzner and Hearst, 2002).
Besides the training corpus, the following parame-
ters need to be specified for LDA: The number of
topics T , the number of sample iterations for the
model m and two hyperparameters ? and ?, spec-
ifying the sparseness of the topic-document and the
topic-word distribution. For the inference method,
the number of sampling iterations i is required. In
line with Griffiths and Steyvers (2004), the follow-
ing standard parameters are used: T = 100, ? =
50/T , ? = 0.01, m = 500, i = 100. We use the
JGibbsLDA implementation described in Phan and
Nguyen (2007).
5.1 Evaluation of the Choi Dataset
For the evaluation we use a 10-fold Cross Validation
(CV): the full dataset of 700 documents is split into
630 documents for training the topic model and 70
documents that are segmented. These two steps are
repeated ten times to have all 700 documents seg-
mented. For this dataset, no part-of-speech based
word filtering is necessary. The results for different
parameter settings are listed in Table 1.
When using only the window parameter without
the mode (d=false), the results demonstrate a sig-
39
seg. size 3-5 6-8 9-11 3-11
Pk WD Pk WD Pk WD Pk WD
d=false,w=1 2.71 3.00 3.64 4.14 5.90 7.05 3.81 4.32
d=true,w=1 3.71 4.16 1.97 2.23 2.42 2.92 2.00 2.30
d=false,w=2 1.46 1.51 1.05 1.20 1.13 1.31 1.00 1.15
d=true,w=2 1.24 1.27 0.76 0.85 0.56 0.71 0.95 1.08
d=false,w=5 2.78 3.04 1.71 2.11 4.47 4.76 3.80 4.46
d=true,w=5 2.34 2.65 1.17 1.35 4.39 4.56 3.20 3.54
Table 1: Results based on the Choi dataset with varying
parameters.
nificant error reduction when using a window of 2
sentences. An impairment is observed when using
a too large window (w=5). This is expected, as the
size of the segments is in a range of 3-11 sentences:
A window of 5 sentences therefore leads to blocks
that contain segment boundaries. We can also see
that the mode method improves the results when
using a window of one, except for the documents
having small segments ranging from 3-5 sentences.
The lowest error rates are obtained with the mode
method and a window size of 2.
As described above, the algorithm is also able to
automatically estimate the number of segments us-
ing a threshold value (see Table 2).
3-5 6-8 9-11 3-11
Pk WD Pk WD Pk WD Pk WD
d=false,w=1 2.39 2.45 4.09 5.85 9.20 15.44 4.87 6.74
d=true,w=1 3.54 3.59 1.98 2.57 3.01 5.15 2.04 2.62
d=false,w=2 15.53 15.55 0.79 0.88 1.98 3.23 1.03 1.36
d=true,w=2 14.65 14.69 0.62 0.62 0.67 0.88 0.66 0.78
d=false,w=5 21.47 21.62 16.30 16.30 6.01 6.14 14.31 14.65
d=true,w=5 21.57 21.67 17.24 17.24 6.44 6.44 15.51 15.74
Table 2: Results on the Choi dataset without given num-
ber of segments as parameter.
The results show that for small segments, the
number of segments is not correctly estimated, as
the error rates are much higher than with given seg-
ments. As the window parameter has a smoothing
effect on the coherence score function, less possible
boundary candidates are detected. We can also see
that the usage of the mode parameter leads to worse
results with w=1 compared to the results where the
mode is deactivated for the documents containing
segments of length 3-5. Especially, results on these
documents suffer when not providing the number of
segments. But for the other documents, results are
much better. Some results (see segment lengths 6-
8 and 3-11 with d=true and w=2) are even better
than the results with segments provided (see Table
1). The threshold method can outperform the setup
with given a number of segments, since not recog-
nizing a segment produces less error in the measures
than predicting a wrong segment.
Table 3 presents a comparison of the performance
of TopicTilig compared to different algorithms in the
literature.
Method 3-5 6-8 9-11 3-11
TT (Choi, 2000) 44 43 48 46
C99 (Choi, 2000) 12 9 9 12
U00 (Utiyama and Isahara, 2001) 9 7 5 10
LCseg (Galley et al, 2003) 8.69
F04 (Fragkou et al, 2004) 5.5 3.0 1.3 7.0
M09 (Misra et al, 2009) 2.2 2.3 4.1 2.3
TopicTiling (d=true, w=2) 1.24 0.76 0.56 0.95
Table 3: Lowest Pk values for the Choi data set for vari-
ous algorithms in the literature with number of segments
provided
It is obvious that the results are far better than cur-
rent state-of-the-art results. Using a one-sampled t-
test with ? = 0.05 we can state significant improve-
ments in comparison to all other algorithms.
While we aim not using the same documents for
training and testing by using a CV scheme, it is not
guaranteed that all testing data is unseen, since the
same source sentences can find their way in several
artificially crafted ?documents?. We could detect re-
occurring snippets in up to 10% of the documents
provided by Choi. This problem, however, applies
for all evaluations on this dataset that use any kind
of training, be it LDA models in Misra et al (2009)
or tf-idf values in Fragkou et al (2004) and Galley
et al (2003).
5.2 Evaluation on Galley?s WSJ Dataset
For the evaluation on Galley?s WSJ dataset, a topic
model is created from the WSJ collection of the PTB
project. The dataset for model estimation consists
of 2499 WSJ articles, and is the same dataset Galley
used as a source corpus. The evaluation generally
leads to higher error rates than in the evaluation for
the Choi dataset, as shown in Table 4.
This table shows results of the WSJ data when us-
ing all words of the documents for training a topic
model and assigning topic IDs to new documents
and also filtered results, using only nouns (proper
40
Parameters All words Filtered
Pk WD Pk WD
d=false,w=1 37.31 43.20 37.01 43.26
d=true,w=1 35.31 41.27 33.52 39.86
d=false,w=2 22.76 28.69 21.35 27.28
d=true,w=2 21.79 27.35 19.75 25.42
d=false,w=5 14.29 19.89 12.90 18.87
d=true,w=5 13.59 19.61 11.89 17.41
d=false,w=10 14.08 22.60 14.09 22.22
d=true,w=10 13.61 21.00 13.48 20.59
Table 4: Results for Galley?s WSJ dataset using differ-
ent parameters with using unfiltered documents and with
filtered documents using only verbs, nouns (proper and
common) and adjectives.
and common), verbs and adjectives1. Considering
the unfiltered results we observe that results improve
when using the mode assigned topic ID and a win-
dow of larger than one sentence. In case of the WSJ
dataset, we find the optimal setting for w=5. As the
test documents contain whole articles, which con-
sist of at least 4 sentences, a larger window is ad-
vantageous here, yet a value of 10 is too large. Fil-
tering the documents for parts of speech leads to ?
1% absolute error rate reduction, as can be seen in
the last two columns of Table 4. Again, we observe
that the mode assignment always leads to better re-
sults, gaining at least 0.6%. Especially the window
size of 5 helps TopicTiling to decrease the error rate
to a third of the value observed with d=false and
w=1. Similar to the previous findings, results de-
cline when using a too large window.
Table 5 shows the results we achieve with the
threshold-based estimation of segment boundaries
for the unfiltered and filtered data.
Parameters All words Filtered
Pk WD Pk WD
d=false,w=1 53.07 72.78 52.63 72.66
d=true,w=1 53.42 74.12 51.84 72.57
d=false,w=2 46.68 65.01 44.81 63.09
d=true,w=2 46.08 64.41 43.54 61.18
d=false,w=5 30.68 43.73 28.31 40.36
d=true,w=5 28.29 38.90 26.96 36.98
d=false,w=10 19.93 32.98 18.29 29.29
d=true,w=10 17.50 26.36 16.32 24.75
Table 5: Table with results the WSJ dataset without num-
ber of segments given, using all words and content words
only.
1The Treetagger http://code.google.com/p/
tt4j/ is applied to POS-tag the data
In contrast to the results obtained with the Choi
dataset (see Table 2) no decline is observed when the
threshold approach is used in combination with the
window approach. We attribute this due to the small
segments and documents used in the Choi setting.
Comparing the all-words data with pos-filtered data,
an improvement is always observed. Also a contin-
uous decreasing of both error rates, Pk and WD,
is detected when using the mode and using a larger
window size, even for w=10. The reason for this is
that too many boundaries are detected when using
small windows. As the window approach smoothes
the similarity scores, this leads to less segmentation
boundaries, which improve results.
For comparison, we present the evaluation results
of other algorithms, shown in Table 6, as published
in Galley et al (2003).
Method Pk WD
C99 (Choi, 2000) 19.61 26.42
U00 (Utiyama and Isahara, 2001) 15.18 21.54
LCseg (Galley et al, 2003) 12.21 18.25
TopicTiling (d=true,w=5) 11.89 17.41
Table 6: List of results based on the WSJ dataset. Values
for C99, U00 and LCseg as stated in (Galley et al, 2003).
Again, TopicTiling improves over the state of the
art. The improvements with respect to LCseg are
significant using a one-sample t-test with ? = 0.05.
6 Conclusion and Further Work
We introduced TopicTiling, a new TS algorithm
that outperforms other algorithms as shown on two
datasets. The algorithm is based on TextTiling and
uses the topic model LDA to find topical changes
within documents. A general result with implica-
tions to other algorithms that use LDA topic IDs is
that using the mode of topic assignments across the
different inference steps is recommended to stabilize
the topic assignments, which improves performance.
As the inference method is relatively fast in compar-
ison to building a model, this mechanism is a useful
and simple improvement, not only restricted to the
field of TS. Using more than a single sentence in in-
ference blocks leads to further stability and less spar-
sity, which improves the results further. In contrast
to other TS algorithms using topic models (Misra
et al, 2009; Sun et al, 2008), the runtime of Top-
icTiling is linear in the number of sentences. This
41
makes TopicTiling a fast algorithm with complex-
ity of O(n) (n denoting the number of sentences)
as opposed to O(n2) of the dynamic programming
approach as discussed in Fragkou et al (2004).
Text segmentation benefits from the usage of topic
models. As opposed to general-purpose lexical re-
sources, topic models can also find fine-grained sub-
topical changes, as shown with the segmentation re-
sults of the WSJ dataset. Here, most articles have
financial content and the topic model can e.g. dis-
tinguish between commodity and stock trading. The
topic model adapts to the subtopic distribution of the
target collection, in contrast e.g. to static WordNet
domain labels as in Bentivogli et al (2004).
For further work, we would like to devise a
method to detect the optimal setting for the window
parameter w automatically, especially in a setting
where the number of target segments is not known in
advance. This is an issue that is shared with the orig-
inal TextTiling algorithm. Moreover, we will extend
the usage of our algorithm to more realistic corpora.
Another direction of research that is more generic
for approaches based on topic models is the ques-
tion of how to automatically select appropriate data
for topic model estimation, given only a small target
collection. Since topic model estimation is computa-
tionally expensive, and topic models for generic col-
lections (think Wikipedia) might not suit the needs
of a specialized domain (such as with the WSJ data),
it is a promising direction to look at target-domain-
driven automatic corpus synthesis.
Acknowledgments
This work has been supported by the Hessian re-
search excellence program ?Landes-Offensive zur
Entwicklung Wissenschaftlich-konomischer Exzel-
lenz? (LOEWE) as part of the research center ?Dig-
ital Humanities?.
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Sta-
tistical models for text segmentation. Mach. learn.,
34(1):177?210.
L. Bentivogli, P. Forner, B. Magnini, and E. Pianta. 2004.
Revising the wordnet domains hierarchy: semantics,
coverage and balancing. In Proc. COLING 2004 MLR,
pages 101?108, Geneva, Switzerland.
D. M. Blei, A. Y Ng, and M. I. Jordan. 2003. Latent
Dirichlet Allocation. JMLR ?03, 3:993?1022.
F. Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proc 1st NAACL ?00,
pages 26?33, Seattle, WA, USA.
J. Eisenstein. 2009. Hierarchical text segmentation from
multi-scale lexical cohesion. In Proc. NAACL-HLT
?09, pages 353?361, Boulder, CO, USA.
P. Fragkou, V. Petridis, and A. Kehagias. 2004. A Dy-
namic Programming Algorithm for Linear Text Seg-
mentation. JIIS ?04, 23(2):179?197.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proc 41st ACL ?03, volume 1, pages 562?
569, Sapporo, Japan.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific
topics. PNAS, 101:5228?5235.
M. A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proc. 32nd ACL ?94, pages 9?16,
Las Cruces, NM, USA.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. Macintyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: Annotating predicate ar-
gument structure. In Proc. ARPA-HLT Workshop ?94,
pages 114?119, Plainsboro, NJ, USA.
Hemant Misra, Joemon M Jose, and Olivier Cappe?. 2009.
Text Segmentation via Topic Modeling : An Analyti-
cal Study. In Proc. 18th CIKM ?09, pages 1553?1556,
Hong Kong.
L. Pevzner and M. A. Hearst. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics, 28.
X.-H. Phan and C.-T. Nguyen. 2007. GibbsLDA++: A
C/C++ implementation of latent Dirichlet alocation
(LDA). http://jgibblda.sourceforge.net/.
M. Riedl and C. Biemann. 2012a. How text segmen-
tation algorithms gain from topic models. In Proc.
NAACL-HLT ?12, Montreal, Canada.
M. Riedl and C. Biemann. 2012b. Sweeping through
the Topic Space: Bad luck? Roll again! In ROBUS-
UNSUP at EACL ?12, Avignon, France.
Q. Sun, R. Li, D. Luo, and X. Wu. 2008. Text segmen-
tation with LDA-based Fisher kernel. In Proc. 46th
ACl-HLT ?08, pages 269?272, Columbus, OH, USA.
M. Utiyama and H. Isahara. 2001. A statistical model for
domain-independent text segmentation. In Proc. 39th
ACL ?00, pages 499?506, Toulouse, France.
C. Wayne. 1998. Topic detection and tracking (TDT):
Overview & perspective. In Proc. DARPA BNTUW,
Lansdowne, Virginia.
Y. Yaari. 1997. Segmentation of expository texts by hi-
erarchical agglomerative clustering. In Proc. RANLP
?97, Tzigov Chark, Bulgaria.
42
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 61?71,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Exploring Cities in Crime: Significant Concordance and Co-occurrence in
Quantitative Literary Analysis
Janneke Rauscher1 Leonard Swiezinski2 Martin Riedl2 Chris Biemann2
(1) Johann Wolfgang Goethe University
Gru?neburgplatz 1, 60323 Frankfurt am Main, Germany
(2) FG Language Technology, Dept. of Computer Science
Technische Universita?t Darmstadt, 64289 Darmstadt, Germany
j.rauscher@em.uni-frankfurt.de, floppy35@web.de,
{riedl,biem}@cs.tu-darmstadt.de
Abstract
We present CoocViewer, a graphical analy-
sis tool for the purpose of quantitative lit-
erary analysis, and demonstrate its use on
a corpus of crime novels. The tool dis-
plays words, their significant co-occurrences,
and contains a new visualization for signif-
icant concordances. Contexts of words and
co-occurrences can be displayed. After re-
viewing previous research and current chal-
lenges in the newly emerging field of quan-
titative literary research, we demonstrate how
CoocViewer allows comparative research on
literary corpora in a project-specific study, and
how we can confirm or enhance our hypothe-
ses through quantitative literary analysis.
1 Introduction
Recent years have seen a surge in Digital Human-
ities research. This area, touching on both the
fields of computer science and the humanities, is
concerned with making data from the humanities
analysable by digitalisation. For this, computational
tools such as search, visual analytics, text mining,
statistics and natural language processing aid the hu-
manities researcher. On the one hand, software per-
mits processing a larger set of data in order to assess
traditional research questions. On the other hand,
this gives rise to a transformation of the way re-
search is conducted in the humanities: the possibil-
ity of analyzing a much larger amount of data ? yet
in a quantitative fashion with all its necessary aggre-
gation ? opens the path to new research questions,
and different methodologies for attaining them.
Although the number of research projects in Dig-
ital Humanities is increasing at fast pace, we still
observe a gap between the traditional humanities
scholars on the one side, and computer scientists on
the other. While computer science excels in crunch-
ing numbers and providing automated processing
for large amounts of data, it is hard for the com-
puter scientist to imagine what research questions
form the discourse in the humanities. In contrast to
this, humanities scholars have a hard time imagining
the possibilities and limitations of computer technol-
ogy, how automatically generated results ought to
be interpreted, and how to operationalize automatic
processing in a way that its unavoidable imperfec-
tions are more than compensated by the sheer size
of analysable material.
This paper resulted from a successful co-
operation between a natural language processing
(NLP) group and a literary researcher in the field
of Digital Humanities. We present the CoocViewer
analysis tool for literary and other corpora, which
supports new angles in literary research through
quantitative analysis.
In the Section 2, we describe the CoocViewer
tool and review the landscape of previously available
tools for our purpose. As a unique characteristic,
CoocViewer contains a visualisation of significant
concordances, which is especially useful for target
terms of high frequency. In Section 3, we map the
landscape of previous and current quantitative re-
search in literary analysis, which is still an emerging
and somewhat controversial sub-discipline. A use-
case for the tool in the context of a specific project
is laid out in Section 4, where a few examples illus-
61
trate how CoocViewer is used to confirm and gen-
erate hypotheses in literary analysis. Section 5 con-
cludes and provides an outlook to further needs in
tool support for quantative literary research.
2 CoocViewer - a Visual Corpus Browser
This section describes our CoocViewer visual cor-
pus browsing tool. After shortly outlining neces-
sary pre-processing steps, we illustrate and moti-
vate the functionality of the graphical user interface.
The tool was specifically designed to aid researchers
from the humanities that do not have a background
in computational linguistics.
2.1 Related Work
Whereas there exist a number of tools for visualizing
co-occurrences, there is, to the best of our knowl-
edge, no tool to visualize positional co-occurrences,
or as we also call them, significant concordances.
In (Widdows et al, 2002) tools are presented that
visualize meanings of nouns as vector space repre-
sentation, using LSA (Deerwester et al, 1990) and
graph models using co-occurrences. There is also
a range of text-based tools, without any quantita-
tive statistics, e.g. Textpresso (Mu?ller et al, 2004),
PhraseNet1 and Walden2. For searching words in
context, Luhn (1960) introduced KWIC (Key Word
in Context) which allows us to search for concor-
dances and is also used in several corpus linguis-
tic tools e.g. (Culy and Lyding, 2011), BNCWeb3,
Sketch Engine (Kilgarriff et al, 2004), Corpus
Workbench4 and MonoConc (Barlow, 1999). Al-
though several tools for co-occurrences visualiza-
tion exist (see e.g. co-occurrences for over 200 lan-
guages at LCC5), they often have different aims, and
e.g. do not deliver the functionality to filter on dif-
ferent part-of-speech tags.
2.2 Corpus Preprocessing
To make a natural language corpus accessible in the
tool, a number of preprocessing steps have to be car-
1http://www-958.ibm.com/software/data/
cognos/manyeyes/page/Phrase_Net.html
2http://infomotions.com/sandbox/
network-diagrams/bin/walden/
3http://bncweb.lancs.ac.uk/bncwebSignup/
user/login.php
4http://cwb.sourceforge.net
5http://corpora.uni-leipzig.de/
ried out for producing the contents of CoocViewer?s
database. These steps consist of a fairly standard
natural language processing pipeline, which we de-
scribe shortly.
After tokenizing, part-of-speech tagging (Schmid,
1994) and indexing the input data by document,
sentence and paragraph within the document, we
compute signficant sentence-wide and paragraph-
wide co-occurrences, using the tinyCC6 tool. Here,
the log-likelihood test (Dunning, 1993) is employed
to determine the significance sig(A,B) of the co-
occurrence of two tokens A and B. To sup-
port the significant concordance view (described in
the next section), we have extended the tool to
also produce positional significant co-occurrences,
where sig(A,B, offset) is computed by the log-
likelihood significance of the co-occurrence of A
and B in a token-distance of offset. Since the sig-
nificance measure requires the single frequencies of
A and B, as well as their joint frequency per po-
sitional offset in this setup, this adds considerable
overhead during preprocessing. To our knowledge,
we are the first to extend the notion of positional co-
occurrence beyond direct neighbors, cf. (Richter et
al., 2006). We apply a sigificance threshold of 3.847
and a frequency threshold of 2 to only keep ?interest-
ing? pairs. The outcome of preprocessing is stored in
a MySQL database schema similar to the one used
by LCC (Biemann et al, 2007). We store sentence-
and paragraph-wide co-occurrences and positional
co-occurrences in separate database tables, and use
one database per corpus. The database tables are in-
dexed accordingly to optimize the queries issued by
the CoocViewer tool. Additionally, we map the part-
of-speeches to E (proper names), N (proper nouns),
A (adjectives), V (verbs), R (all other part-of-speech
tags) for an uniform representation for different lan-
guages.
2.3 Graphical User Interface
The graphical user interface (UI) is built with com-
mon web technologies, such as HTML, CSS and
JavaScript. The UI communicates via AJAX with
a backend, which utilizes PHP and a MySQL
6http://wortschatz.uni-leipzig.de/
?cbiemann/software/TinyCC2.html, (Richter et
al., 2006)
7corresponding to 5% error probability
62
database. This makes the approach flexible regard-
ing the platform. It can run on client computers
using XAMP8, a portable package of various Web
technologies, including an Apache web server and a
MySQL server. Alternatively, the tool can operate as
a client-server application over a network. In partic-
ular, we want to highlight the JavaScript data visual-
ization framework D3 (Bostock et al, 2011), which
was used to layout and draw the graphs. We deliber-
ately designed the tool to match the requirements of
literary researchers, who are at times overwhelmed
by general-purpose visualisation tools such as e.g.
Gephi9. The UI is split into three parts: At the top a
menu bar, including a search input field and search
options, a graph drawing panel and display options
at the bottom of the page.
Figure 1: Screenshot of the Coocviewer application using
the concordance view.
The menu bar allows switching between co-
occurrence and concordance views (see Figure
1). The search field supports wildcards and
type-ahead autocompletion, immediately displaying
which words exist in the corpus and match the cur-
rent input. Additionally, there are functionalities to
export the shown graph as SVG or PNG image, or
as plain text, containing all relations, including their
frequencies and significance scores. Within the ad-
vanced configuration windows (shown on the right
side) one can select different corpora, enable case
sensitive/insensitive searches or filter words accord-
8http://www.apachefriends.org/en/index.
html
9https://gephi.org/
ing their part-of-speech tags (as described in Sec-
tion 2.2). The graph drawing panel visualizes the
queried term and its significant co-occurrences resp.
concordances, significancy being visualized by the
thickness of the lines. In Figure 1, showing the con-
cordances for bread, we can directly see words that
occur often with bread in the context: E.g. bread
is often used in combination with butter, cheese,
margarine (offset +2), but also the kind of different
breads is described by the adjectives at offset -1. For
the same information, using the normal KWIC view,
one has to count the words with different offset by
hand to find properties for the term bread. At the
bottom, the maximal number of words shown in the
graph can be specified. For the concordances display
there is an additional option to specify the maximal
offset. The original text (with provenance informa-
tion) containing the nodes (words) or edges (word
pairs) shown in the graph can be retrieved by either
clicking on a word itself or on the edge connecting
two words, in a new window (see Figure 2) within
the application. This window also provides informa-
Figure 2: Occurrences of a significant concordance
tion about the frequencies of single words as well as
their co-occurrence, and also displays relative single
word frequencies in parts-per-million (ppm) to en-
able comparisons between corpora of different sizes.
Words in focus are highlighted and the contents of
this window can also be exported as plain text.
3 Quantitative Literary Research
Quantitative research in literary analysis, although
being conducted and discussed since at least the
1960s, (Hoover, 2008), is still far from being a clear
field of research with a verified and acknowledged
methodology. Studies in this field vary widely with
respect to scope, methods applied and theoretical
63
background. Until now, only the most basic defi-
nition can be given that applies to these approaches:
Quantitative research in literary analysis is generally
concerned with the application of methods from cor-
pus linguistics (and statistics) to the field of litera-
ture to investigate and quantify general grammatical
and lexical features of texts.
Most studies applying such methods to literary anal-
ysis are carried out in the field of stylistics, building
a relatively new research area of corpus stylistics,
also called stylometry (Mahlberg, 2007; Hoover,
2008; Biber, 2011). The quantitative exploration
of stylistic features and patterns is used for author-
ship attribution, e.g. (Burrows, 1992; Burrows,
2007; Craig, 2004; Hoover, 2001; Hoover, 2002),
exploring the specificity of one author?s style, e.g.
(Burrows, 1987; Hori, 2004; Fischer-Starcke, 2010;
Mahlberg, 2012) or one certain text, often compared
to other texts of the same author or period, e.g.
(Craig, 1999; McKenna and Antonia, 2001; Stubbs,
2005; Clement, 2008; Fischer-Starcke, 2009). Some
studies focus on content-related questions such as
the analysis of plot or characterization and the ex-
ploration of relations between and role of different
characters, e.g. (Mahlberg, 2007; Culpeper, 2002;
Culpeper, 2009), developing new ways of exploring
these literary features, e.g. via the application of so-
cial network analysis (Elson et al, 2010; Moretti,
2011; Agarwal et al, 2012). Besides this area, there
are numerous other approaches, like the attempt to
investigate the phenomenon of ?literary creativity?
(Hoey, 2007) or ways for automatic recognition of
literary genres (Allison et al, 2011).
Major methodological approaches of this field are,
according to Biber (2011), Mahlberg (2007) and
Hoover (2008), the study of keywords and word-
frequencies, co-occurrences, lexical clusters (also
called bundles or n-grams) and collocational as well
as concordance analysis. Additionally, the need for
cross-investigating and comparing the results with
other corpora (be it a general corpus of one language
or other small, purpose-built corpora) is emphasized
to discuss the uniqueness of the results.
But while especially the studies of Moretti (2000;
2007; 2009), taking a quantitative approach of ?dis-
tant reading? on questions of literary history and the
evolution of literary genres, are often received as
groundbreaking for the case, and despite the rising
interest in this field of research in the last decades,
there still is much reluctance towards the imple-
mentation of such methods. The general arguments
raised frequently from the point of view of ?classi-
cal? literary analysis against a quantitative or compu-
tational approach can be grouped around four cen-
tral points: The uniqueness of each literary text
that quantitative analysis seems to underscore when
treating the texts just as examples of style or period,
focussing on very general patterns; the emphasize
of technology and the relatively high threshold that
the application, analysis and interpretation of the
generated data contains (Potter, 1988); and the gen-
eral notion that meaning in literary texts is highly
context-related and context-dependent in different
ways (Hoover, 2008). Last but not least there is
what can be called the ?so-what-argument?: Quan-
titative methods tend to produce sparse significant
new information compared with the classical ap-
proach of close reading, generating insights and in-
terpretations that could as well be reached by simply
reading the book (Mahlberg, 2007; Rommel, 2008).
But the possibilities and advantages of corpus lin-
guistics come to the foreground especially if one is
not interested in aspects of uniqueness or particu-
larity but in commonalities and differences between
large amounts of literary texts too many to be read
and compared in the classical way. This especially
holds when it comes to questions of topics, themes,
discourse analysis and the semantisation of certain
words.
4 Empirical Analysis
This section describes a few exemplary analysis
which we carried out within our ongoing project ?At
the crime scene: The intrinsic logic of cities in con-
temporary crime novels?. Settled between the dis-
ciplines of sociology and literature, the project is
embedded in the urban sociological research area of
the ?Eigenlogik? (intrinsic logic) of cities (Berking,
2012; Lo?w, 2012; Lo?w, forthcoming). The basic hy-
pothesis is that there is no such thing as ?the? city or
?the? urban experience in general, but that every city
forms its own contexts and complexes of meaning,
the unquestioned and often subconsciously operat-
ing knowledge of how things are done, respectively
making sense of the city. To put it another way, we
64
want to find out if and in what way the respective
city makes a difference and is forming distinctive
structures of thought, action and feeling. This is ex-
plored simultaneously in four different projects in-
vestigating different fields (economic practices, city
marketing, problem discourses and literary field and
texts) in four different cities that are compared with
each other (Birmingham (UK), Glasgow, Frankfurt
on the Main and Dortmund). If the hypothesis is
right, the four different investigated fields should
have more in common within one city and across the
fields than within one field across different cities.
Our subproject is mainly concerned with the literary
and cultural imagination and representation of the
cities in question. One crucial challenge is the ex-
ploration, analysis and comparison of 240 contem-
porary crime novels, each of them set in one of the
cities under examination. The aim of this explorative
study is to analyze the possibility and characteristics
of city-specific structures within the realm of liter-
ary representations of cities.
Dealing with such comparably large amounts of lit-
erary texts, a tool was needed that facilitated us
(laypeople in the field of corpus linguistics) to ex-
plore the city-specific content and structures within
these corpora, enabling a connection of qualitative
close reading and quantitative methods. Visualiza-
tion was a major concern, apparently lowering the
resistance of the literary research community to-
wards charts and numbers and making the results
readable and interpretable without having much ex-
pertise in corpus linguistics. Moreover, the option of
generating significant concordances instead of sim-
ple concordance lines (as e.g. with KWIC) is very
promising: Confronted with very high word fre-
quencies for some of our search terms, e.g. more
than 2200 occurrences of ?Frankfurt? in our Frank-
furt corpus, completely manual analysis turned out
to be painstaking and very time-consuming. Auto-
mated or manual reduction of the number of lines
according to standard practices, as e.g. suggested
by Tribble (2010), is not possible without potential
loss of information. CoocViewer enables a sophis-
ticated and automated analysis with concentration
on statistically significant findings through cluster-
ing co-occurring words according to their statistical
significance in concordance lines. Additionally, the
positionality of these re-occurring co-occurrences in
City lang. #novels #tokens #sent. #para.
Birmingham engl. 41 4.8M 336K 142K
Glasgow engl. 61 7.7M 496K 222K
Dortmund ger. 59 5.0M 361K 127K
Frankfurt ger. 79 8.0M 546K 230K
Table 1: Quantitative characteristics of our corpora
relation to the search term (with a maximum range
from -10 to +10 around the node) gives a clear
and immediate picture of patterns of usage within
a corpus. Via exploring the references of the re-
sults we are still able to take account of the context-
specificity of literary texts, as well as distinguishing
author-specific results from those distributed more
equally across a corpus.
After describing the corpus resources, we conduct
two exemplary analysis to show how the quantita-
tive tool as described in Section 2 can be used to aid
complex qualitative research interests in the human-
ities through supporting the exploration and compar-
ison of large corpora (Sect. 4.2), as well as investi-
gating and comparing the semantization and seman-
tic preference of words (Sect. 4.3). The discussion
of results shows how CoocViewer can support hy-
pothesis building and testing on a quantitative basis,
linking qualitative and quantitative approaches.
4.1 Corpus
The selection of the crime novels was based on three
criteria: contemporariness (written and published
within the past 30 years until 2010), the city in ques-
tion (should play a major role resp. be used as major
setting), and genre (crime fiction in any variety). In
a first step, the 240 novels (gathered as paperback-
editions) had to be scanned and digitalized10. Meta-
data was removed and the remaining texts were pre-
processed as described in Section 2.2. The nov-
els were compiled in different corpora according to
the city they are set in, and the database underly-
ing them (sentence or paragraph). Table 1 provides
an overview of the quantitative characteristics of the
four city-specific corpora we discuss here.
10We used ABBY FineReader 10 professional for optical
character recognition, which generated tolerable but not perfect
results, making extensive proof reading and corrections neces-
sary.
65
4.2 Analysis 1: Exploring the Use of the City?s
Name
The occurrence of the name of a city in crime nov-
els can serve different purposes and functions in
the text. It can be used, for example, to simply
?place? the plot (instead of or additionally to de-
scribing the setting in further detail) or to indicate
the direction of movement of figures (?they drove to
Glasgow?). Often it is surrounded by information
about city-specific aspects, e.g. of history or mate-
riality. Searching for the respective proper names of
the cities in the four corpora therefore seems to be
a promising start to explore the possibility of city-
specific structures of meaning in literary representa-
tion. If the ?Eigenlogik?-hypothesis is right, not only
the content that is associated with the name (what
would generally be expected) but also its frequent
usages and functions (as pointer or marker, as start-
ing point for further explanations of city life, etc.)
should differ systematically across cities.
A first close reading of some exemplary crime nov-
els already suggested that this could be the case. To
check this qualitatively derrived impression we con-
ducted CoocViewer searches for the top-15 signif-
icant co-occurrences across all parts of speech for
each proper name in the respective corpus on sen-
tence level (see Figure 3 for the cases of Glasgow
and Frankfurt). To interpret and compare these find-
ings, we additionally looked at the significant con-
cordances (with the same search parameters and an
offset from the node of -3/+3), which helps to ana-
lyze and refine the findings in more depth. In the fol-
lowing, we discuss, compare and interpret the results
with respect to our overriding project-hypothesis to
verify or falsify some of our qualitative first impres-
sions quantitatively.
The corpora indeed tend not only to vary signif-
icantly with respect to the sheer frequency of the
usage of the proper name (with relative frequen-
cies ranging from Glasgow (324ppm) and Frankfurt
(286ppm) to Dortmund (187ppm) and Birmingham
(154ppm)), but also in the usages and functions that
the naming fulfills. The graphs reveal not only dif-
fering co-occurrences, but also differing proportions
of co-occurring word classes, each city revealing its
own distinct pattern.
Especially the English cities tend to co-occur with
Figure 3: Significant co-occurrences of ?Glasgow? (up-
per) and ?Frankfurt? (lower) in their respective corpora
proper names and common nouns (ten proper
names, four common nouns in the case for Glas-
gow, eight names and six nouns for Birmingham).11
For Glasgow, these comprise parts of the inven-
tory of the city (with ?City? (sig. of 695.57) as
either part of the name or city-specific institution
(?City of Glasgow?, ?City Orchestra?) or to refer
to different crime-genre specific institutions (as the
?City of Glasgow Police? or ?Glasgow City Mortu-
ary?)), the ?University? (sig. of 380.42), or the park
?Glasgow Green? (233.46). There is also the name
of another city, the Scottish capital (and rival city)
Edinburgh. As the statistical concordances reveal
quickly, the ?Port? (350.88) is, despite Glasgow?s
history as shipbuilding capital, not used to refer to
the cities industrial past. Instead, as can bee seen
from its positioning on -1 directly left to the node,
it refers to the small nearby town Port Glasgow (see
11The noun ?accent? which both English cities names co-
occur with (and for which no equivalent term can be found on
the German side) can be explained by a different lexicalization
of the concept, which is realized through derivation in German.
66
Fig. 4). The co-occurrence of ?Royal? and Glas-
gow (being not a royal city) can also be easily ex-
plained via the concordance view, showing that this
is mainly due to the ?Royal Concert Hall? (forming
a strong triangle on positions +1, +2 and +3 from the
node). Besides these instances of places and institu-
tions within and around the city, especially the con-
nection to the pronoun ?its? (82 instances with a sig.
of 144) is interesting. None of the other cities shows
a top-significant co-occurrence with a comparable
pronoun. A look at the corresponding references in
the corpus shows that it is mainly used in statements
about the quality or speciality of certain aspects of
the city (indicated on graphic level through the con-
nections between ?its? and ?city? or ?area?) and in
personifications (e.g. ?Glasgow could still reach out
with its persistent demands?). This implies that the
literary device of anthropomorphization of the city
(in direct connection with the proper name) occurs
more often within Glasgow-novels than within those
of the other cities, and that there are many explicit
statements about ?how this city (or a special part of
it) is?, showing a tendency to explain the city. Fur-
thermore, the exploration of the different references
indicates a relatively ?coherent corpus? (and, there-
fore, relatively stable representation) with recurring
instances across many authors.
Figure 4: Significant concordances of ?Glasgow? in
Glasgow corpus
In contrast to this, Birmingham?s co-occurring
proper names mainly refer to (fictive) names of
newspapers (the Birmingham ?Sentinel?, ?Post? and
?News?). The inventory of the city is not very
prominently represented, with ?University? (sig. of
152.52) and ?Airport? (80.63) as the only instances.
Furthermore, the University tends to be represented
as region-, not city-specific (with a stronger connec-
tion between ?University? and ?Midlands? (sig. of
200.49) than between both words to the city itself
(?Midlands? co-occurring with a sig. of 68.68)).
The rest of the proper names relates to not fur-
ther specified parts of the city (?East? (71.62) and
?North-East? (73.43)). The word ?south? appears as
adverb, reflecting on graphic level that it is more of-
ten used as in ?heading south? than referring to the
?south of Birmingham?. Also, the noun ?city? (sig.
of 154.53) is often related to the ?city centre? (indi-
cated through the very strong link between those two
words), but also to make statements like ?Birming-
ham is a city that? or ?like other cities, Birmingham
has?. The references reveal the quality of this expla-
nations, rather stressing its ordinariness as city in-
stead of personalizing it or emphasizing its unique-
ness. This indicates that the city itself is not stand-
ing prominently in the foreground in its crime nov-
els (in contrast to Glasgow and in accordance with
our qualitatively derived prior results). The proper
name is mainly used as part of other proper names
(e.g. ?Birmingham Sentinel?), fulfilling the function
of simply placing the plot, and there is very little
city-specific information given on a statistical sig-
nificant re-occurring level in the closer surroundings
of it. Even the statements about Birmingham as a
city tend to downplay its singularity.
On the German side, the cities names co-occur with
words from a wider range of word classes. For
both cities, we find less co-occurring proper names:
five for Frankfurt, only one of them referring to a
city-specific aspect (the long version of the name
?Frankfurt on the Main? (sig. of 585.09)); four for
Dortmund (again, only one city-specific, the name
of its soccer club ?Borussia? (with only seven in-
stances and a sig. of 41.93)). For both cities, the
rest of the proper names is composed of names of
other cities (in Frankfurt the two nearby cities ?Of-
fenbach? (139.49) and ?Darmstadt? (105.73), and
?Berlin?, ?Hamburg?); for Dortmund only cities
from the same metropolitan area (the Ruhrgebiet),
?Du?sseldorf? (41.95), ?Werne? (41.78) and ?Duis-
burg? (39.42)). It seems that Dortmund is closely
connected within the metropolitan area it is a part
of, but looking at the references shows that only
Du?sseldorf plays a role across different crime novel
series, while the rest mainly feature in one certain
series (being rated as author-specifc).
67
In the case of Frankfurt, the nouns that co-
occur (seven) either denote city-specific aspects
(Flughafen (airport) (96.83) and Eintracht (the lo-
cal soccer club with a sig. of 192.36)) or very
general instances (December, Jahren (years)). A
look at the statistical concordances, ordered accord-
ing not only to their position around the node but
also to their significance, displays that the noun
?Kripo? (short form for crime investigation unit)
on the -1 position is more often used than the first
city-specific instance, with a significance of 564.58
(while ?police? for Glasgow on the +1 position is
relatively ranked lower). This prominent position
of the crime investigation unit (interpreted as im-
pact of genre-related aspects) indicates that there are
many ?police-procedural? crime novels in Frankfurt
(which is true), giving insight into the sub-genre
composition of the corpus. As with the English
cities, the word ?Stadt? (city; sig. of 245.63) co-
occurs frequently, and as the references reveal it
serves similar purposes: either to denote the politi-
cal administration (the ?Stadt Frankfurt?) or in com-
bination with further explanations of ?how this city
is? (as in Glasgow, but without personalization), or
?Frankfurt is a city that?, but in contrast to Birming-
ham not with a frequent downplaying of uniqueness.
Additionally, we find instances where other cities
are compared to Frankfurt (?a city that, unlike/like
Frankfurt?). This seems to point towards a more
flexible use of this combination resp. to a variety
of ways of representation. Frankfurt is represented
as a city allowing for different semantizations and
different ways of depicting it without posing contra-
dictions (as the differing uses occur not only across
a wide range of authors, but within the same texts).
Finally, taking a closer look at Dortmund, the
frequently co-occurring nouns nearly all are re-
lated to genre-specific instances, referring to crime
investigation-related institutions (again ?Kripo?
(sig. of 88.91); ?Polizeipra?sidium? (police head-
quarters; sig. of 35.15), ?Landgericht? (dis-
trict court; 37.25) and ?Sonderstaatsanwaltschaft?
(34.63)). This indicates that in this corpus the genre-
specific structures seem to imprint themselves more
than the city-specific ones, putting the city itself
into the background (similar to the case of Birm-
ingham but with a highly differing pattern). But
we also have to consider the comparably low rel-
ative frequency rates (ppm) that demand an expla-
nation. There might be another similarity between
Dortmund and Birmingham, both showing low rel-
ative frequencies for their respective proper names.
But as we take a closer look on the references of
the occurrences of the names, we can see that the
one series of crime novels that represents the biggest
share of the corpus (with 21 novels belonging to this
series) does not mention ?Dortmund? at all, while
for Birmingham the use of the proper name is quite
equally distributed across all authors and series. A
look inside one of this books of the series in ques-
tion reveals a possible answer to the low frequencies:
instead of using the proper name, the author conse-
quently uses the nickname ?Bierstadt? (Beer-city).
Therefore, while it is possible to show that each city
under investigation reveals a specific pattern of co-
occurrences and differing uses and functions of its
proper name, as our hypothesis has suggested, the
search for the proper name alone seems not suffi-
cient to get the overall picture of the literary repre-
sentation of a city, demanding further analysis.
4.3 Analysis 2: Investigating Genre Aspects
When it comes to questions of genre-conventions vs.
city-conventions, the investigation of the semantic
preference of typically crime-related words is inter-
esting. If the specific city has an impact on genre-
aspects, the graphs should show clear differences.
Close reading of exemplary novels of Glasgow and
Birmingham indicated that violence plays a greater
role in Glasgow crime fiction than in that of Birm-
ingham, therefore we expect to find differing attri-
butions towards and meanings of ?violence?, show-
ing a higher vocabulary richness in Glasgow than in
Birmingham, taking into account its semantic pref-
erence (for more details about this aspect see e.g.
(Hoey, 2007)). We examine this hypothesis through
making ?violence? the node of a search for signif-
icant concordances, searching for the top-30 adjec-
tives directly altering the noun within a range of -3
to +3 around the node.
As depicted in Figure 5, our initial hypothesis can
be verified. While Glasgow (upper) has nine sig-
nificantly co-occurring adjectives (six directly alter-
ing the noun ?violence? on pos. -1), Birmingham
(lower) only has five (four on pos. -1). Those that di-
rectly alter the noun show a slightly differing seman-
68
Figure 5: Significant adjective concordances of ?vio-
lence?, comparing Glasgow (upper) and Birmingham
(lower) corpora
tic preference, with adjectives of ?kind of violence?
(domestic, physical) standing on top in both corpora.
Next, we look at adjectives that bear a notion of
?quality or intensity of violence?: while Birming-
ham only discriminates between mindless and latent
violence, the vocabulary of Glasgow is much richer
(thuggish, mindless, sudden), one of them also bear-
ing a notion of expectability (sudden). Additionally,
a temporal adjective is used to refer to ?past vio-
lence?. If we look at the instances on the -3 position
for Glasgow (a position that is not filled for Birm-
ingham), we can add random to the list of ?qual-
ity of violence?, and find some instances of ?being
afraid of (physical) violence? (as the link between
those words implies). This verifies our close reading
interpretations.
The adjectives to the right of the node (?own?
on position +3 in Glasgow, ?old? on position +2
in Birmingham) pose a puzzle. Through a look at
the references for this instances, we can see that
in the case of Birmingham, old is referring to vic-
tims of violence (old people), while the picture for
Glasgow is split between violence of its own type
(which then could be added to the list of quality-
adjectives) and violence that one experienced on his
own. Through the interconnectedness of the adjec-
tives settled on different positions for the case of
Glasgow and a look at the resources of the instances,
we conclude that the patterns seem to be more estab-
lished on city level (showing instances from varying
authors for all adjective-noun combinations) than
they are in Birmingham, where there are no cross-
connections and the authors differ more among each
other (with ?physical violence? being the only com-
bination that occurs across different authors, while
all other adjective-noun combinations only appear
within the work of a single author).
5 Conclusion and Further Work
To conclude the exemplary analysis, CoocViewer
helps not only to explore large corpora but also to
verify or relativize impressions from classical quali-
tative literary research. It opens up new ways of ex-
ploring topics, themes and relationships within large
sets of literary texts. Especially the combination
and linkage of co-occurrences and significant con-
cordances simplifies the analysis and allows a finer-
grained and more focused analysis than KWIC con-
cordances or simple frequency counts. The possi-
bility to distinguish between these two viewpoints
on the data accelerates and improves the interpre-
tation of results. Additionally, the comparison be-
tween corpora is much facilitated through the imme-
diate visibility of differing patterns. Further work
can proceed along a few lines. We would like
to enable investigations of the wide context of co-
occurrences through access from the references back
to the whole crime-novel document. Further, we
would like to automatically compare corpora of the
same language on the level of local co-occurrence
and concordance graphs to aid generating hypothe-
ses. This will make a change in the interface nec-
essary to support a comparative view. Furthermore,
we want to extend the view of the original text (see
Figure 2) in our tool by centering the sentences ac-
cording to the selected word or words, as done in
KWIC views. When clicking on a single word, this
would lead to the normal KWIC view, but selecting
an edge we then want to center the sentences accord-
ing to the two words connected by the edge, which
might be useful especially for the concordances.
The tool and the pre-processing software is avail-
able as an open source project12 and as a web demo.
Acknowledgments
This work has been funded by the Hessian re-
search excellence program Landes-Offensive zur
Entwicklung Wissenschaftlich-O?konomischer Exzel-
lenz (LOEWE) as part of the research center Digital
Humanities.
12https://sourceforge.net/p/coocviewer
69
References
Apoorv Agarwal, Augusto Corvalan, Jacob Jensen, and
Owen Rambow. 2012. Social network analysis of
alice in wonderland. In Workshop on Computational
Linguistics for Literature, pages 88?96, Montre?al,
Canada.
Sahra Allison, Ryan Heuser, Mathhew Jockers, Franco
Moretti, and Michael Witmore. 2011. Quantitative
Formalism: an Experiment. Stanford Literary Lab.
M. Barlow. 1999. Monoconc 1.5 and paraconc. Interna-
tional journal of corpus linguistics, 4(1):173?184.
Helmuth Berking. 2012. The distinctiveness of cities:
outline of a research programme. Urban Research &
Practice, 5(3):316?324.
Douglas Biber. 2011. Corpus linguistics and the study
of literature. back to the future? Scientific Study of
Literature, 1(1):15?23.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig Corpora Col-
lection - monolingual corpora of standard size. In
Proceedings of Corpus Linguistics 2007, Birmingham,
UK.
Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer.
2011. D3: Data-driven documents. IEEE Trans. Visu-
alization & Comp. Graphics (Proc. InfoVis).
John F. Burrows. 1987. Computation into Criticism. A
Study of Jane Austen?s Novels and an Experiment in
Method. Clarendon, Oxford.
John F. Burrows. 1992. Computers and the study of liter-
ature. In Christopher S. Butler, editor, Computers and
Written Texts, pages 167?204, Oxford. Blackwell.
John F. Burrows. 2007. All the way through: Testing for
authorship in different frequency strata. Literary and
Linguistic Computing, 22(1):27?47.
Tanya E. Clement. 2008. ?A thing not beginning and not
ending?: using digital tools to distand-read Gertrude
Stein?s The Making of Americans. Literary and Lin-
guistic Computing, 23(3):361?381.
Hugh Craig. 1999. Jonsonian chronology and the styles
of a tale of a tub. In Martin Butler, editor, Re-
Presenting Ben Jonson: Text, History, Performance,
pages 210?232, Houndmills. Macmillan.
Hugh Craig. 2004. Stylistic analysis and authorship
studies. In Susan Schreibman, Ray Siemens, and John
Unsworth, editors, A Companion to Digital Humani-
ties. Blackwell.
Jonathan Culpeper. 2002. Computers, language and
characterisation: An analysis of six characters in
Romeo and Juliet. In Ulla Melander-Marttala, Carin
Ostman, and Merja Kyt, editors, Conversation in Life
and Literature: Papers from the ASLA Symposium,
volume 15, pages 11?30, Uppsala. Association Sue-
doise de Linguistique Appliquee.
Jonathan Culpeper. 2009. Keyness: Words, parts-of-
speech and semantic categories in the character-talk of
shakespeare?s romeo and juliet. International Journal
of Corpus Linguistics, 14(1):29?59.
Chris Culy and Verena Lyding. 2011. Corpus clouds - fa-
cilitating text analysis by means of visualizations. In
Zygmunt Vetulani, editor, Human Language Technol-
ogy. Challenges for Computer Science and Linguistics,
volume 6562 of Lecture Notes in Computer Science,
pages 351?360. Springer Berlin Heidelberg.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. In 48th Annual Meeting of the Association for
Computer Linguistics, pages 138?147, Uppsala, Swe-
den.
Bettina Fischer-Starcke. 2009. Keywords and fre-
quent phrases of Jane Austen?s Pride and Prejudice: A
corpus-stylistc analysis. International Journal of Cor-
pus Linguistics, 14(4):492?523.
Bettina Fischer-Starcke. 2010. Corpus linguistics in lit-
erary analysis: Jane Austen and her contemporaries.
Continuum, London.
Carsten Go?rg, Hannah Tipney, Karin Verspoor, Jr. Baum-
gartner, William A., K. Bretonnel Cohen, John Stasko,
and Lawrence E. Hunter. 2010. Visualization and
language processing for supporting analysis across the
biomedical literature. In Knowledge-Based and Intel-
ligent Information and Engineering Systems, volume
6279 of Lecture Notes in Computer Science, pages
420?429. Springer Berlin Heidelberg.
Michael Hoey. 2007. Lexical priming and literary cre-
ativity. In Michael Hoey, Michaela Mahlberg, Michael
Stubbs, and Wolfgang Teubert, editors, Text, Dis-
course and Corpora. Theory and Analysis, pages 31?
56, London. Continuum.
David L. Hoover. 2001. Statistical stylistics and author-
ship attribution: an emprirical investigation. Literary
and Linguistic Computing, 16(4):421?444.
David L. Hoover. 2002. Frequent word sequences and
statistical stylistics. Literary and Linguistic Comput-
ing, 17(2):157?180.
David L. Hoover. 2008. Quantitative analysis and liter-
ary studies. In Ray Siemens and Susan Schreibman,
editors, A Companion to Digital Literary Studies, Ox-
ford. Blackwell.
70
Masahiro Hori. 2004. Investigating Dicken?s Style:
A Collocational Analysis. Palgrave Macmillan, Bas-
ingstoke.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The sketch engine. In Proceedings of
EURALEX, pages 105?116, Lorient, France.
Martina Lo?w. 2012. The intrinsic logic of cities: towards
a new theory on urbanism. Urban Research & Prac-
tice, 5(3):303?315.
Martina Lo?w. forthcoming. The city as experential
space: The production of shared meaning. Interna-
tional Journal of Urban and Regional Research.
H. P. Luhn. 1960. Key word-in-context index for tech-
nical literature (KWIC index). American Documenta-
tion, 11(4):288?295.
Michaela Mahlberg. 2007. Corpus stylistics: bridg-
ing the gap between linguistic and literary studies. In
Michael Hoey, Michaela Mahlberg, Michael Stubbs,
and Wolfgang Teubert, editors, Text, Discourse and
Corpora. Theory and Analysis, pages 217?246, Lon-
don. Continuum.
Michaela Mahlberg. 2012. Corpus Stylistics and
Dicken?s Fiction. Routledge advances in corpus lin-
guistics. Routledge, London.
C. W. F. McKenna and Alexis Antonia. 2001. The sta-
tistical analysis of style: Reflections on fom, meaning,
and ideology in the ?Nausicaa? episode of Ulysses. Lit-
erary and Linguistic Computing, 16(4):353?373.
Franco Moretti. 2000. Conjectures on world literature.
New Left Review, 1(January/February):54?68.
Franco Moretti. 2007. Graphs, Maps, Trees. Abstract
Models for Literary History. Verso, London / New
York.
Franco Moretti. 2009. Style, Inc. reflections on seven
thousand titels (British novels, 1740-1850). Critical
Inquiry, 36(1):134?158.
Franco Moretti. 2011. Network Theory, Plot Analysis.
Stanford Literary Lab.
Hans-Michael Mu?ller, Eimear E. Kenny, and Paul W.
Sternberg. 2004. Textpresso: An ontology-based in-
formation retrieval and extraction system for biologi-
cal literature. Plos Biology, 2(11).
Rosanne G. Potter. 1988. Literary criticism and literary
computing: The difficulties of a synthesis. Computers
and Humanities, 22:91?97.
Matthias Richter, Uwe Quasthoff, Erla Hallsteinsdo?ttir,
and Chris Biemann. 2006. Exploiting the Leipzig
Corpora Collection. In Proceesings of the IS-LTC
2006, Ljubljana, Slovenia.
Thomas Rommel. 2008. Literary studies. In Ray
Siemens and Susan Schreibman, editors, A Compan-
ion to Digital Literary Studies, Oxford. Blackwell.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
Michael Stubbs. 2005. Conrad in the computer: exam-
ples of quantitative stylistic methods. Language and
Literature, 14(1):5?24.
Christopher Tribble. 2010. What are concordances and
how are they used? In Anne O?Keeffe and Michael
McCarthy, editors, The Routledge Handbook of Cor-
pus Linguistics, pages 167?183, Abingdon. Routledge.
Dominic Widdows, Scott Cederberg, and Beate Dorow.
2002. Visualisation Techniques for Analysing Mean-
ing. In Fifth International Conference on Text, Speech
and Dialogue (TSD-02), pages 107?114. Springer.
71
Proceedings of the TextGraphs-8 Workshop, pages 6?10,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
JoBimText Visualizer:
A Graph-based Approach to Contextualizing Distributional Similarity
Alfio Gliozzo1 Chris Biemann2 Martin Riedl2
Bonaventura Coppola1 Michael R. Glass1 Matthew Hatem1
(1) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
(2) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany
{gliozzo,mrglass,mhatem}@us.ibm.com coppolab@gmail.com
{biem,riedl}@cs.tu-darmstadt.de
Abstract
We introduce an interactive visualization com-
ponent for the JoBimText project. JoBim-
Text is an open source platform for large-scale
distributional semantics based on graph rep-
resentations. First we describe the underly-
ing technology for computing a distributional
thesaurus on words using bipartite graphs of
words and context features, and contextualiz-
ing the list of semantically similar words to-
wards a given sentential context using graph-
based ranking. Then we demonstrate the ca-
pabilities of this contextualized text expan-
sion technology in an interactive visualization.
The visualization can be used as a semantic
parser providing contextualized expansions of
words in text as well as disambiguation to
word senses induced by graph clustering, and
is provided as an open source tool.
1 Introduction
The aim of the JoBimText1 project is to build a
graph-based unsupervised framework for computa-
tional semantics, addressing problems like lexical
ambiguity and variability, word sense disambigua-
tion and lexical substitutability, paraphrasing, frame
induction and parsing, and textual entailment. We
construct a semantic analyzer able to self-adapt to
new domains and languages by unsupervised learn-
ing of semantics from large corpora of raw text. At
the moment, this analyzer encompasses contextual-
ized similarity, sense clustering, and a mapping of
senses to existing knowledge bases. While its pri-
mary target application is functional domain adap-
tation of Question Answering (QA) systems (Fer-
1http://sf.net/projects/jobimtext/
rucci et al, 2013), output of the semantic analyzer
has been successfully utilized for word sense disam-
biguation (Miller et al, 2012) and lexical substitu-
tion (Szarvas et al, 2013). Rather than presenting
the different algorithms and technical solutions cur-
rently implemented by the JoBimText community in
detail, in this paper we will focus on available func-
tionalities and illustrate them using an interactive vi-
sualization.
2 Underlying Technologies
While distributional semantics (de Saussure, 1959;
Harris, 1951; Miller and Charles, 1991) and the
computation of distributional thesauri (Lin, 1998)
has been around for decades, its full potential has yet
to be utilized in Natural Language Processing (NLP)
tasks and applications. Structural semantics claims
that meaning can be fully defined by semantic oppo-
sitions and relations between words. In order to per-
form a reliable knowledge acquisition process in this
framework, we gather statistical information about
word co-occurrences with syntactic contexts from
very large corpora. To avoid the intrinsic quadratic
complexity of the similarity computation, we have
developed an optimized process based on MapRe-
duce (Dean and Ghemawat, 2004) that takes advan-
tage of the sparsity of contexts, which allows scal-
ing the process through parallelization. The result of
this computation is a graph connecting the most dis-
criminative contexts to terms and explicitly linking
the most similar terms. This graph represents local
models of semantic relations per term rather than a
model with fixed dimensions. This representation
departs from the vector space metaphor (Schu?tze,
1993; Erk and Pado?, 2008; Baroni and Zamparelli,
6
2010), commonly employed in other frameworks for
distributional semantics such as LSA (Deerwester et
al., 1990) or LDA (Blei et al, 2003).
The main contribution of this paper is to de-
scribe how we operationalize semantic similarity in
a graph-based framework and explore this seman-
tic graph using an interactive visualization. We de-
scribe a scalable and flexible computation of a dis-
tributional thesaurus (DT), and the contextualization
of distributional similarity for specific occurrences
of language elements (i.e. terms). For related works
on the computation of distributional similarity, see
e.g. (Lin, 1998; Lin and Dyer, 2010).
2.1 Holing System
To keep the framework flexible and abstract with re-
spect to the pre-processing that identifies structure
in language material, we introduce the holing op-
eration, cf. (Biemann and Riedl, 2013). It is ap-
plied to observations over the structure of text, and
splits these observations into a pair of two parts,
which we call the ?Jo? and the ?Bim?2. All JoBim
pairs are maintained in the bipartite First-Order Jo-
Bim graph TC(T,C,E) with T set of terms (Jos),
C set of contexts (Bims), and e(t, c, f) ? E edges
between t ? T , c ? C with frequency f . While
these parts can be thought of as language elements
referred to as terms, and their respective context fea-
tures, splits over arbitrary structures are possible (in-
cluding pairs of terms for Jos), which makes this
formulation more general than similar formulations
found e.g. in (Lin, 1998; Baroni and Lenci, 2010).
These splits form the basis for the computation of
global similarities and for their contextualization. A
Holing System based on dependency parses is illus-
trated in Figure 1: for each dependency relation, two
JoBim pairs are generated.
2.2 Distributed Distributional Thesaurus
Computation
We employ the Apache Hadoop MapReduce Fram-
work3, and Apache Pig4, for parallelizing and dis-
tributing the computation of the DT. We describe
this computation in terms of graph transformations.
2arbitrary names to emphasize the generality, should be
thought of as ?term? and ?context?
3http://hadoop.apache.org
4http://pig.apache.org/
Figure 1: Jos and Bims generated applying a dependency
parser (de Marneffe et al, 2006) to the sentence I suffered
from a cold and took aspirin. The @@ symbolizes the
hole.
Staring from the JoBim graph TC with counts as
weights, we first apply a statistical test5 to com-
pute the significance of each pair (t, c), then we only
keep the p most significant pairs per t. This consti-
tutes our first-order graph for Jos FOJO. In analogy,
when keeping the p most significant pairs per c, we
can produce the first-order graph for Bims FOBIM .
The second order similarity graph for Jos is defined
as SOJO(T,E) with Jos t1, t2 ? T and undirected
edges e(t1, t2, s) with similarity s = |{c|e(t1, c) ?
FOJO, e(t2, c) ? FOJO}|, which defines similar-
ity between Jos as the number of salient features
two Jos share. SOJO defines a distributional the-
saurus. In analogy, SOBIM is defined over the
shared Jos for pairs of Bims and defines similar-
ity of contexts. This method, which can be com-
puted very efficiently in a few MapReduce steps, has
been found superior to other measures for very large
datasets in semantic relatedness evaluations in (Bie-
mann and Riedl, 2013), but could be replaced by any
other measure without interfering with the remain-
der of the system.
2.3 Contextualization with CRF
While the distributional thesaurus provides the sim-
ilarity between pairs of terms, the fidelity of a par-
ticular expansion depends on the context. From the
term-context associations gathered in the construc-
tion of the distributional thesaurus we effectively
have a language model, factorized according to the
holing operation. As with any language model,
smoothing is critical to performance. There may be
5we use log-likelihood ratio (Dunning, 1993) or LMI (Evert,
2004)
7
many JoBim (term-context) pairs that are valid and
yet under represented in the corpus. Yet, there may
be some similar term-context pair that is attested in
the corpus. We can find similar contexts by expand-
ing the term arguments with similar terms. However,
again we are confronted with the fact that the simi-
larity of these terms depends on the context.
This suggests some technique of joint inference
to expand terms in context. We use marginal in-
ference in a conditional random field (CRF) (Laf-
ferty et al, 2001). A particular world, x is defined
as single, definite sequence of either original or ex-
panded words. The weight of the world, w(x) de-
pends on the degree to which the term-context as-
sociations present in this sentence are present in the
corpus and the general out-of-context similarity of
each expanded term to the corresponding term in the
original sentence. Therefore the probability associ-
ated with any expansion t for any position xi is given
by Equation 1. Where Z is the partition function, a
normalization constant.
P (xi = t) =
1
Z
?
{x | xi=t}
ew(x) (1)
The balance between the plausibility of an ex-
panded sentence according to the language model,
and its per-term similarity to the original sentence is
an application specific tuning parameter.
2.4 Word Sense Induction, Disambiguation
and Cluster Labeling
The contextualization described in the previous sub-
section performs implicit word sense disambigua-
tion (WSD) by ranking contextually better fitting
similar terms higher. To model this more explicitly,
and to give rise to linking senses to taxonomies and
domain ontologies, we apply a word sense induction
(WSI) technique and use information extracted by
IS-A-patterns (Hearst, 1992) to label the clusters.
Using the aggregated context features of the clus-
ters, the word cluster senses are assigned in con-
text. The DT entry for each term j as given in
SOJO(J,E) induces an open neighborhood graph
Nj(Vj , Ej) with Vj = {j?|e(j, j?, s) ? E) and Ej
the projection of E regarding Vj , consisting of sim-
ilar terms to j and their similarities, cf. (Widdows
and Dorow, 2002).
We cluster this graph using the Chinese Whispers
graph clustering algorithm (Biemann, 2010), which
finds the number of clusters automatically, to ob-
tain induced word senses. Running shallow, part-
of-speech-based IS-A patterns (Hearst, 1992) over
the text collection, we obtain a list of extracted IS-
A relationships between terms, and their frequency.
For each of the word clusters, consisting of similar
terms for the same target term sense, we aggregate
the IS-A information by summing the frequency of
hypernyms, and multiplying this sum by the number
of words in the cluster that elicited this hypernym.
This results in taxonomic information for labeling
the clusters, which provides an abstraction layer for
terms in context6. Table 1 shows an example of this
labeling from the model described below. The most
similar 200 terms for ?jaguar? have been clustered
into the car sense and the cat sense and the high-
est scoring 6 hypernyms provide a concise descrip-
tion of these senses. This information can be used
to automatically map these cluster senses to senses
in an taxonomy or ontology. Occurrences of am-
biguous words in context can be disambiguated to
these cluster senses comparing the actual context
with salient contexts per sense, obtained by aggre-
gating the Bims from the FOJO graph per cluster.
sense IS-A labels similar terms
jaguar
N.0
car, brand,
company,
automaker,
manufacturer,
vehicle
geely, lincoln-mercury,
tesla, peugeot, ..., mit-
subishi, cadillac, jag, benz,
mclaren, skoda, infiniti,
sable, thunderbird
jaguar
N.1
animal, species,
wildlife, team,
wild animal, cat
panther, cougar, alligator,
tiger, elephant, bull, hippo,
dragon, leopard, shark,
bear, otter, lynx, lion
Table 1: Word sense induction and cluster labeling exam-
ple for ?jaguar?. The shortened cluster for the car sense
has 186 members.
3 Interactive Visualization
3.1 Open Domain Model
The open domain model used in the current vi-
sualization has been trained from newspaper cor-
6Note that this mechanism also elicits hypernyms for unam-
biguous terms receiving a single cluster by the WSI technique.
8
Figure 2: Visualization GUI with prior expansions for
?cold?. Jobims are visualized on the left, expansions on
the right side.
pora using 120 million sentences (about 2 Giga-
words), compiled from LCC (Richter et al, 2006)
and the Gigaword (Parker et al, 2011) corpus. We
constructed a UIMA (Ferrucci and Lally, 2004)
pipeline, which tokenizes, lemmatizes and parses
the data using the Stanford dependency parser (de
Marneffe et al, 2006). The last annotator in the
pipeline annotates Jos and Bims using the collapsed
dependency relations, cf. Fig. 1. We define the lem-
matized forms of the terminals including the part-
of-speech as Jo and the lemmatized dependent word
and the dependency relation name as Bim.
3.2 Interactive Visualization Features
Evaluating the impact of this technology in applica-
tions is an ongoing effort. However, in the context
of this paper, we will show a visualization of the ca-
pabilities allowed by this flavor of distributional se-
mantics. The visualization is a GUI as depicted in
Figure 2, and exemplifies a set of capabilities that
can be accessed through an API. It is straightfor-
ward to include all shown data as features for seman-
tic preprocessing. The input is a sentence in natural
language, which is processed into JoBim pairs as de-
scribed above. All the Jos can be expanded, showing
their paradigmatic relations with other words.
We can perform this operation with and without
taking the context into account (cf. Sect. 2.3). The
latter performs an implicit disambiguation by rank-
ing similar words higher if they fit the context. In
the example, the ?common cold? sense clearly dom-
inates in the prior expansions. However, ?weather?
and ?chill? appear amongst the top-similar prior ex-
pansions.
We also have implemented a sense view, which
displays sense clusters for the selected word, see
Figure 3. Per sense, a list of expansions is pro-
vided together with a list of possible IS-A types. In
this example, the algorithm identified two senses of
?cold? as a temperature and a disease (not all clus-
ter members shown). Given the JoBim graph of the
context (as displayed left in Fig. 2), the particular
occurrence of ?cold? can be disambiguated to Clus-
ter 0 in Fig. 3, since its Bims ?amod(@@,nasty)?
and ?-dobj(catch, @@)? are found in FOJO for far
more members of cluster 0 than for members of clus-
ter 1. Applications of this type of information in-
clude knowledge-based word sense disambiguation
(Miller et al, 2012), type coercion (Kalyanpur et al,
2011) and answer justification in question answering
(Chu-Carroll et al, 2012).
4 Conclusion
In this paper we discussed applications of the Jo-
BimText platform and introduced a new interactive
visualization which showcases a graph-based unsu-
pervised technology for semantic processing. The
implementation is operationalized in a way that it
can be efficiently trained ?off line? using MapRe-
duce, generating domain and language specific mod-
els for distributional semantics. In its ?on line? use,
those models are used to enhance parsing with con-
textualized text expansions of terms. This expansion
step is very efficient and runs on a standard laptop,
so it can be used as a semantic text preprocessor. The
entire project, including pre-computed data models,
is available in open source under the ASL 2.0, and
allows computing contextualized lexical expansion
on arbitrary domains.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Comp. Ling., 36(4):673?721.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: representing adjective-noun
constructions in semantic space. In Proc. EMNLP-
2010, pages 1183?1193, Cambridge, Massachusetts.
C. Biemann and M. Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual simi-
larity. Journal of Language Modelling, 1(1):55?95.
C. Biemann. 2010. Co-occurrence cluster features for
lexical substitutions in context. In Proceedings of
TextGraphs-5, pages 55?59, Uppsala, Sweden.
9
Figure 3: Senses induced for the term ?cold?.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022,
March.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: search and candidate generation. IBM
J. Res. Dev., 56(3):300?311.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. LREC-2006, Genova,
Italy.
Ferdinand de Saussure. 1916. Cours de linguistique
ge?ne?rale. Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. OSDI
?04, San Francisco, CA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proc. EMNLP-
2008, pages 897?906, Honolulu, Hawaii.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
D. Ferrucci and A. Lally. 2004. UIMA: An Architectural
Approach to Unstructured Information Processing in
the Corporate Research Environment. In Nat. Lang.
Eng. 2004, pages 327?348.
D. Ferrucci, A. Levas, S. Bagchi, D. Gondek, and E. T.
Mueller. 2013. Watson: Beyond Jeopardy! Artificial
Intelligence, 199-200:93?105.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING-
1992, pages 539?545, Nantes, France.
A. Kalyanpur, J.W. Murdock, J. Fan, and C. Welty. 2011.
Leveraging community-built knowledge for type co-
ercion in question answering. In Proc. ISWC 2011,
pages 144?156. Springer.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc.
ICML 2001, pages 282?289, San Francisco, CA, USA.
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan & Claypool Publishers, San
Rafael, CA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768?774,
Montre?al, Quebec, Canada.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Miller, C. Biemann, T. Zesch, and I. Gurevych. 2012.
Using distributional similarity for lexical expansion
in knowledge-based word sense disambiguation. In
Proc. COLING-2012, pages 1781?1796, Mumbai, In-
dia.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proc. IS-LTC 2006, Ljubljana, Slovenia.
H. Schu?tze. 1993. Word space. In Advances in Neu-
ral Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.
G. Szarvas, C. Biemann, and I. Gurevych. 2013. Super-
vised all-words lexical substitution using delexicalized
features. In Proc. NAACL-2013, Atlanta, GA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING-
2002, pages 1?7, Taipei, Taiwan.
10
Proceedings of the TextGraphs-8 Workshop, pages 39?43,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
From Global to Local Similarities:
A Graph-Based Contextualization Method using Distributional Thesauri
Chris Biemann and Martin Riedl
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
{riedl,biem}@cs.tu-darmstadt.de
Abstract
After recasting the computation of a distribu-
tional thesaurus in a graph-based framework
for term similarity, we introduce a new con-
textualization method that generates, for each
term occurrence in a text, a ranked list of terms
that are semantically similar and compatible
with the given context. The framework is in-
stantiated by the definition of term and con-
text, which we derive from dependency parses
in this work. Evaluating our approach on a
standard data set for lexical substitution, we
show substantial improvements over a strong
non-contextualized baseline across all parts of
speech. In contrast to comparable approaches,
our framework defines an unsupervised gener-
ative method for similarity in context and does
not rely on the existence of lexical resources as
a source for candidate expansions.
1 Introduction
Following (de Saussure, 1916) we consider two dis-
tinct viewpoints: syntagmatic relations consider the
assignment of values to a linear sequence of terms,
and the associative (also: paradigmatic) viewpoint
assigns values according to the commonalities and
differences to other terms in the reader?s memory.
Based on these notions, we automatically expand
terms in the linear sequence with their paradigmati-
cally related terms.
Using the distributional hypothesis (Harris,
1951), and operationalizing similarity of terms
(Miller and Charles, 1991), it became possible to
compute term similarities for a large vocabulary
(Ruge, 1992). Lin (1998) computed a distributional
thesaurus (DT) by comparing context features de-
fined over grammatical dependencies with an ap-
propriate similarity measure for all reasonably fre-
quent words in a large collection of text, and evalu-
ated these automatically computed word similarities
against lexical resources. Entries in the DT consist
of a ranked list of the globally most similar terms for
a target term. While the similarities are dependent
on the instantiation of the context feature as well as
on the underlying text collection, they are global in
the sense that the DT aggregates over all occurrences
of target and its similar elements. In our work, we
will use a DT in a graph representation and move
from a global notion of similarity to a contextual-
ized version, which performs context-dependent text
expansion for all word nodes in the DT graph.
2 Related Work
The need to model semantics just in the same way
as local syntax is covered by the n-gram-model, i.e.
trained from a background corpus sparked a large
body of research on semantic modeling. This in-
cludes computational models for topicality (Deer-
wester et al, 1990; Hofmann, 1999; Blei et al,
2003), and language models that incorporate topical
(as well as syntactic) information, see e.g. (Boyd-
Graber and Blei, 2008; Tan et al, 2012). In the
Computational Linguistics community, the vector
space model (Schu?tze, 1993; Turney and Pantel,
2010; Baroni and Lenci, 2010; Pucci et al, 2009;
de Cruys et al, 2013) is the prevalent metaphor for
representing word meaning.
While the computation of semantic similarities on
the basis of a background corpus produces a global
model, which e.g. contains semantically similar
words for different word senses, there are a num-
ber of works that aim at contextualizing the infor-
mation held in the global model for particular oc-
currences. With his predication algorithm, Kintsch
(2001) contextualizes LSA (Deerwester et al, 1990)
for N-VP constructions by spreading activation over
neighbourhood graphs in the latent space.
In particular, the question of operationalizing se-
mantic compositionality in vector spaces (Mitchell
39
and Lapata, 2008) received much attention. The lex-
ical substitution task (McCarthy and Navigli, 2009)
(LexSub) sparked several approaches for contextual-
ization. While LexSub participants and subsequent
works all relied on a list of possible substitutions
as given by one or several lexical resources, we de-
scribe a graph-based system that is knowledge-free
and unsupervised in the sense that it neither requires
an existing resource (we compute a DT graph for
that), nor needs training for contextualization.
3 Method
3.1 Holing System
For reasons of generality, we introduce the holing
operation (cf. (Biemann and Riedl, 2013)), to split
any sort of observations on the syntagmatic level
(e.g. dependency relations) into pairs of term and
context features. These pairs are then both used for
the computation of the global DT graph similarity
and for the contextualization. This holing system
is the only part of the system that is dependent on
a pre-processing step; subsequent steps operate on
a unified representation. The representation is given
by a list of pairs <t,c> where t is the term (at a cer-
tain offset) and c is the context feature. The position
of t in c is denoted by a hole symbol ?@?. As an ex-
ample, the dependency triple (nsub;gave2;I1)
could be transferred to <gave2,(nsub;@;I1)>
and <I1,(nsub;gave2;@)>.
3.2 Distributional Similarity
Here, we present the computation of the distribu-
tional similarity between terms using three graphs.
For the computation we use the Apache Hadoop
Framework, based on (Dean and Ghemawat, 2004).
We can describe this operation using a bipartite
?term?-?context feature? graph TC(T,C,E) with
T the set terms, C the set of context features and
e(t, c, f) ? E edges between t ? T , c ? C
with f = count(t, c) frequency of co-occurrence.
Additionally, we define count(t) and count(c) as
the counts of the term, respectively as the count
of the context feature. Based on the graph TC
we can produce a first-order graph FO(T,C,E),
with e(t, c, sig) ? E. First, we calculate a signif-
icance score sig for each pair (t, c) using Lexicog-
rapher?s Mutual Information (LMI): score(t, c) =
LMI(t, c, ) = count(t, c) log2(
count(t,c)
count(t)count(c))
(Evert, 2004). Then, we remove all edges with
score(t, c) < 0 and keep only the p most signif-
icant pairs per term t and remove the remaining
edges. Additionally, we remove features which co-
occur with more then 1000 words, as these features
do not contribute enough to similarity to justify the
increase of computation time (cf. (Rychly? and Kil-
garriff, 2007; Goyal et al, 2010)). The second-
order similarity graph between terms is defined as
SO(T,E) for t1, t2 ? T with the similarity score
s = |{c|e(t1, c) ? FO, e(t2, c) ? FO}|, which is
the number of salient features two terms share. SO
defines a distributional thesaurus.
In contrast to (Lin, 1998) we do not count how of-
ten a feature occurs with a term (we use significance
ranking instead), and do not use cosine or other sim-
ilarities (Lee, 1999) to calculate the similarity over
the feature counts of each term, but only count sig-
nificant common features per term. This constraint
makes this approach more scalable to larger data, as
we do not need to know the full list of features for
a term pair at any time. Seemingly simplistic, we
show in (Biemann and Riedl, 2013) that this mea-
sure outperforms other measures on large corpora in
a semantic relatedness evaluation.
3.3 Contextual Similarity
The contextualization is framed as a ranking prob-
lem: given a set of candidate expansions as pro-
vided by the SO graph, we aim at ranking them such
that the most similar term in context will be ranked
higher, whereas non-compatible candidates should
be ranked lower.
First, we run the holing system on the lexical
material containing our target word tw ? T ? ?
T and select all pairs <tw,ci> ci ? C ? ? C
that are instantiated in the current context. We
then define a new graph CON(T ?, C ?, S) with con-
text features ci ? C ?. Using the second-order
similarity graph SO(T,E) we extract the top n
similar terms T ?={ti, . . . , tn}?T from the second-
order graph SO for tw and add them to the graph
CON . We add edges e(t, c, sig) between all tar-
get words and context features and label the edge
with the significance score from the first order graph
FO. Edges e(t, c, sig), not contained in FO, get
a significance score of zero. We can then calcu-
40
late a ranking score for each ti with the harmonic
mean, using a plus one smoothing: rank(ti) =?
cj
(sig(ti,cj)+1)/count(term(cj))
?
cj
(sig(ti,cj)+1)/count(term(cj))
(term(cj) extracts
the term out of the context notation). Using that
ranking score we can re-order the entries t1, . . . , tn
according to their ranking score.
In Figure 1, we exemplify this, using the tar-
get word tw= ?cold? in the sentence ?I caught
a nasty cold.?. Our dependency parse-based
Figure 1: Contextualized ranking for target ?cold? in the
sentence ?I caught a nasty cold? for the 10 most similar
terms from the DT.
holing system produced the following pairs for
?cold?: <cold5 ,(amod;@;nasty4)>,
<cold5,(dobj;caught2;@)>. The top 10
candidates for ?cold? are T ?={heat, weather, tem-
perature, rain, flue, wind, chill, disease}. The scores
per pair are e.g. <heat, (dobj;caught;@)>
with an LMI score of 42.0, <weather
,(amod;@;nasty)> with a score of 139.4.
The pair <weather, (dobj;caught;@)>
was not contained in our first-order data. Ranking
the candidates by their overall scores as given in the
figure, the top three contextualized expansions are
?disease, flu, heat?, which are compatible with both
pairs. For the top 200 words, the ranking of fully
compatible candidates is: ?virus, disease, infection,
flu, problem, cough, heat, water?, which is clearly
preferring the disease-related sense of ?cold? over
the temperature-related sense.
In this way, each candidate t? gets as many
scores as there are pairs containing c? in the holing
system output. An overall score per t? then given by
the harmonic mean of the add-one-smoothed single
scores ? smoothing is necessary to rank candidates
t? that are not compatible to all pairs. This scheme
can easily be extended to expand all words in a given
sentence or paragraph, yielding a two-dimensional
contextualized text, where every (content) word is
expanded by a list of globally similar words from the
distributional thesaurus that are ranked according to
their compatibility with the given context.
4 Evaluation
The evaluation of contextualizing the thesaurus (CT)
was performed using the LexSub dataset, introduced
in the Lexical Substitution task at Semeval 2007
(McCarthy and Navigli, 2009). Following the setup
provided by the task organizers, we tuned our ap-
proach on the 300 trial sentences, and evaluate it
on the official remaining 1710 test sentences. For
the evaluation we used the out of ten (oot) preci-
sion and oot mode precision. Both measures cal-
culate the number of detected substitutions within
ten guesses over the complete subset. Whereas en-
tries in the oot precision measures are considered
correct if they match the gold standard, without pe-
nalizing non-matching entries, the oot mode preci-
sion includes also a weighting as given in the gold
standard1. For comparison, we use the results of the
DT as a baseline to evaluate the contextualization.
The DT was computed based on newspaper corpora
(120 million sentences), taken from the Leipzig Cor-
pora Collection (Richter et al, 2006) and the Giga-
word corpus (Parker et al, 2011). Our holing system
uses collapsed Stanford parser dependencies (Marn-
effe et al, 2006) as context features. The contextual-
ization uses only context features that contain words
with part-of-speech prefixes V,N,J,R. Furthermore,
we use a threshold for the significance value of the
LMI values of 50.0, p=1000, and the most similar 30
terms from the DT entries.
5 Results
Since out contextualization algorithm is dependent
on the number of context features containing the tar-
get word, we report scores for targets with at least
two and at least three dependencies separately. In
the Lexical Substitution Task 2007 dataset (LexSub)
test data we detected 8 instances without entries in
the gold standard and 19 target words without any
1The oot setting was chosen because it matches the expan-
sions task better than e.g. precision@1
41
dependency, as they are collapsed into the depen-
dency relation. The remaining entries have at least
one, 49.2% have at least two and 26.0% have at least
three dependencies. Furthermore, we also evalu-
ated the results broken down into separate part-of-
speeches of the target. The results on the LexSub
test set are shown in Table 1.
Precision Mode Precision
min. # dep. 1 2 3 1 2 3
POS Alg.
noun CT 26.64 26.55 28.36 38.68 38.24 37.68
noun DT 25.35 25.09 28.07 34.96 34.31 36.23
verb CT 23.39 23.75 23.05 32.05 33.09 33.33
verb DT 22.46 22.13 21.32 29.17 28.78 28.25
adj. CT 32.65 34.75 36.08 45.09 48.24 46.43
adj. DT 32.13 33.25 35.02 43.56 43.53 42.86
adv. CT 20.47 29.46 36.23 30.14 40.63 100.00
adv. DT 28.91 26.75 29.88 41.63 34.38 66.67
ALL CT 26.46 26.43 26.61 37.21 37.40 37.38
ALL DT 27.06 24.83 25.24 36.96 33.06 33.11
Table 1: Results of the LexSub test dataset.
Inspecting the results for all POS (denoted as
ALL), we only observe a slight decline for the preci-
sion score with at least only one dependency, which
is caused by adverbs. For targets with more than
one dependency, we observe overall improvements
of 1.6 points in precision and more than 4 points in
mode precision.
Regarding the results of different part-of-speech
tags, we always improve over the DT ranking, ex-
cept for adverbs with only one dependency. Most
notably, the largest relative improvements are ob-
served on verbs, which is a notoriously difficult
word class in computational semantics. For adverbs,
at least two dependencies seem to be needed; there
are only 7 adverb occurrences with more than two
dependencies in the dataset. Regarding performance
on the original lexical substitution task (McCarthy
and Navigli, 2009), we did not come close to the per-
formance of the participating systems, which range
between 32?50 precision points, respectively 43?66
mode precision points (only taking systems with-
out duplicate words in the result set into account).
However, all participants used one or several lexical
resources for generating substitution candidates, as
well as a large number of features. Our system, on
the other hand, merely requires a holing system ? in
this case based on a dependency parser ? and a large
amount of unlabeled text, and a very small number
of contextual clues.
For an insight of the coverage for the entries deliv-
ered by the DT graph, we extended the oot precision
measure, to consider not only the first 10 entries, but
the first X={1,10,50,100,200} entries (see Figure 2).
Here we also show the coverage for different sized
Figure 2: Coverage on the LexSub test dataset for differ-
ent DT graphs, using out of X entries.
datasets (10 and 120 million sentences). Amongst
the 200 most similar words from the DT, a cover-
age of up to 55.89 is reached. DT quality improves
with corpus size, especially due to increased cover-
age. This shows that there is considerable headroom
for optimization for our contextualization method,
but also shows that our automatic candidate expan-
sions can provide a coverage that is competitive to
lexical resources.
6 Conclusion
We have provided a way of operationalizing seman-
tic similarity by splitting syntagmatic observations
into terms and context features, and representing
them a first-order and second-order graph. Then,
we introduced a conceptually simple and efficient
method to perform a contextualization of semantic
similarity. Overall, our approach constitutes an un-
supervised generative model for lexical expansion
in context. We have presented a generic method
on contextualizing distributional information, which
retrieves the lexical expansions from a target term
from the DT graph, and ranks them with respect to
their context compatibility. Evaluating our method
on the LexSub task, we were able to show improve-
ments, especially for expansion targets with many
informing contextual elements. For further work,
we will extend our holing system and combine sev-
eral holing systems, such as e.g. n-gram contexts.
42
Additionally, we would like to adapt more advanced
methods for the contextualization (Viterbi, 1967;
Lafferty et al, 2001) that yield an all-words simulta-
neous expansion over the whole sequence, and con-
stitutes a probabilistic model of lexical expansion.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! a framework for lexical expansion with contextual
similarity. Journal of Language Modelling, 1(1):55?
95.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. J. Mach. Learn. Res., 3:993?1022.
J. Boyd-Graber and D. M. Blei. 2008. Syntactic topic
models. In Neural Information Processing Systems,
Vancouver, BC, USA.
T. Van de Cruys, T. Poibeau, and A. Korhonen. 2013. A
tensor-based factorization model of semantic compo-
sitionality. In Proc. NAACL-HLT 2013, Atlanta, USA.
F. de Saussure. 1916. Cours de linguistique ge?ne?rale.
Payot, Paris, France.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. In Proc. of
Operating Systems, Design & Implementation, pages
137?150, San Francisco, CA, USA.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Ph.D. thesis, IMS, Uni-
versita?t Stuttgart.
A. Goyal, J. Jagarlamudi, H. Daume?, III, and T. Venkata-
subramanian. 2010. Sketch techniques for scaling dis-
tributional similarity to the web. In Proc. of the 2010
Workshop on GEometrical Models of Nat. Lang. Se-
mantics, pages 51?56, Uppsala, Sweden.
Z. S. Harris. 1951. Methods in Structural Linguistics.
University of Chicago Press, Chicago, USA.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. 22nd ACM SIGIR, pages 50?57,
New York, NY, USA.
W. Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
the 18th Int. Conf. on Machine Learning, ICML ?01,
pages 282?289, San Francisco, CA, USA.
L. Lee. 1999. Measures of distributional similarity. In
Proc. of the 37th ACL, pages 25?32, College Park,
MD, USA.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING?98, pages 768?774,
Montreal, Quebec, Canada.
M.-C. De Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proc. of the Int. Conf. on
Language Resources and Evaluation, Genova, Italy.
D. McCarthy and R. Navigli. 2009. The english lexical
substitution task. Language Resources and Evalua-
tion, 43(2):139?159.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, OH, USA.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. Linguistic
Data Consortium, Philadelphia, USA.
D. Pucci, M. Baroni, F. Cutugno, and R. Lenci. 2009.
Unsupervised lexical substitution with a word space
model. In Workshop Proc. of the 11th Conf. of the
Italian Association for Artificial Intelligence, Reggio
Emilia, Italy.
M. Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-
mann. 2006. Exploiting the leipzig corpora collection.
In Proceesings of the IS-LTC 2006, Ljubljana, Slove-
nia.
G. Ruge. 1992. Experiments on linguistically-based
term associations. Information Processing & Manage-
ment, 28(3):317 ? 332.
P. Rychly? and A. Kilgarriff. 2007. An efficient algo-
rithm for building a distributional thesaurus (and other
sketch engine developments). In Proc. 45th ACL,
pages 41?44, Prague, Czech Republic.
Hinrich Schu?tze. 1993. Word space. In Advances in
Neural Information Processing Systems 5, pages 895?
902. Morgan Kaufmann.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic, and
lexical language model. Computational Linguistics,
38(3):631?671.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: vector space models of semantics. J. Artif.
Int. Res., 37(1):141?188.
A. J. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, 13(2):260?
269.
43
