Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035?1045,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Extracting Opinion Targets in a Single- and Cross-Domain Setting
with Conditional Random Fields
Niklas Jakob
Technische Universita?t Darmstadt
Hochschulstra?e 10
64289 Darmstadt, Germany
Iryna Gurevych
Technische Universita?t Darmstadt
Hochschulstra?e 10
64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de/people
Abstract
In this paper, we focus on the opinion tar-
get extraction as part of the opinion min-
ing task. We model the problem as an in-
formation extraction task, which we address
based on Conditional Random Fields (CRF).
As a baseline we employ the supervised al-
gorithm by Zhuang et al (2006), which rep-
resents the state-of-the-art on the employed
data. We evaluate the algorithms comprehen-
sively on datasets from four different domains
annotated with individual opinion target in-
stances on a sentence level. Furthermore, we
investigate the performance of our CRF-based
approach and the baseline in a single- and
cross-domain opinion target extraction setting.
Our CRF-based approach improves the perfor-
mance by 0.077, 0.126, 0.071 and 0.178 re-
garding F-Measure in the single-domain ex-
traction in the four domains. In the cross-
domain setting our approach improves the per-
formance by 0.409, 0.242, 0.294 and 0.343 re-
garding F-Measure over the baseline.
1 Introduction
The automatic extraction and analysis of opinions
has been approached on several levels of granular-
ity throughout the last years. As opinion mining is
typically an enabling technology for another task,
this overlaying system defines requirements regard-
ing the level of granularity. Some tasks only require
an analysis of the opinions on a document or sen-
tence level, while others require an extraction and
analysis on a term or phrase level. Amongst the
tasks which require the finest level of granularity
are: a) Opinion question answering - i.e. with ques-
tions regarding an entity as in ?What do the people
like / dislike about X??. b) Recommender systems
- i.e. if the system shall only recommend entities
which have received good reviews regarding a cer-
tain aspect. c) Opinion summarization - i.e. if one
wants to create an overview of all positive / negative
opinions regarding aspect Y of entity X and cluster
them accordingly. All of these tasks have in com-
mon that in order to fulfill them, the opinion min-
ing system must be capable of identifying what the
opinions in the individual sentences are about, hence
extract the opinion targets.
Our goal in this work is to extract opinion tar-
gets from user-generated discourse, a discourse type
which is quite frequently encountered today, due to
the explosive growth of Web 2.0 community web-
sites. Typical sentences which we encounter in this
discourse type are shown in the following examples.
The opinion targets which we aim to extract are un-
derlined in the sentences, the corresponding opinion
expressions are shown in italics.
(1) While none of the features are
earth-shattering, eCircles does provide a great
place to keep in touch.
(2) Hyundai?s more-than-modest refresh has
largely addressed all the original car?s
weaknesses while maintaining its price
competitiveness.
The extraction of opinion targets can be consid-
ered as an instance of an information extraction
(IE) task (Cowie and Lehnert, 1996). Conditional
1035
Random Fields (CRF) (Lafferty et al, 2001) have
been successfully applied to several IE tasks in
the past (Peng and McCallum, 2006). A recur-
ring problem, which arises when working with su-
pervised approaches, concerns the domain portabil-
ity. In the opinion mining context this question has
been prominently investigated with respect to opin-
ion polarity analysis (sentiment analysis) in previ-
ous research (Aue and Gamon, 2005; Blitzer et al,
2007). Terms as ?unpredictable? can express a pos-
itive opinion when uttered about the storyline of a
movie but a negative opinion when the handling of
a car is described. Hence the effects of training and
testing a machine learning algorithm for sentiment
analysis on data from different domains have been
analyzed in previous research. However to the best
of our knowledge, these effects have not been inves-
tigated regarding the extraction of opinion targets.
The contribution of this paper is a CRF-based ap-
proach for opinion targets extraction which tackles
the problem of domain portability. We first evalu-
ate our approach in three different domains against
a state-of-the art baseline system and then evaluate
the performance of both systems in a cross-domain
setting. We show that our CRF-based approach out-
performs the baseline in both settings, and how the
diffrerent combinations of features we introduce in-
fluence the results of our CRF-based approach. The
remainder of this paper is structured as follows: In
Section 2 we discuss the related work, and in Sec-
tion 3 we describe our CRF-based approach. Sec-
tion 4 comprises our experimental setup including
the description of the dataset we employ in our ex-
periments in Section 4.1 and the baseline system in
Section 4.2. The results of our experiments and their
discussion follow in Section 5. Finally we draw our
conclusions in Section 6.
2 Related Work
In the following we will discuss the related work re-
garding opinion target extraction and domain adap-
tation in opinion mining. The discussion of the re-
lated work on opinion target extraction is separated
in supervised and unsupervised approaches. We
conclude with a discussion of the related work on
domain adaptation in opinion mining.
2.1 Unsupervised Opinion Target Extraction
The first work on opinion target extraction was done
on customer reviews of consumer electronics. Hu
and Liu (2004) introduce the task of feature based
summarization, which aims at creating an overview
of the product features commented on in the re-
views. Their approach relies on a statistical analysis
of the review terms based on association mining. A
dataset of customer reviews from five domains was
annotated by the authors regarding mentioned prod-
uct features with respective opinion polarities. The
association mining based algorithm yields a preci-
sion of 0.72 and a recall of 0.80 in the extraction
of a manually selected subset of product features.
The same dataset of product reviews was used in the
work of Yi et al (2003). They present and evalu-
ate a complete system for opinion extraction which
is based on a statistical analysis based on the Like-
lihood Ratio Test for opinion target extraction. The
Likelihood Ratio Test yields a precision of 0.97 and
1.00 in the task of opinion target (product feature)
extraction, recall values are not reported.
Popescu and Etzioni (2005) present the OPINE
system for opinion mining on product reviews.
Their algorithm is based on an information extrac-
tion system, which uses the pointwise mutual infor-
mation based on the hitcounts of a web-search en-
gine as an input. They evaluate the opinion target
extraction separately on the dataset by Hu and Liu
(2004). OPINE?s precision is on average 22% higher
than the association mining based approach, while
having an average 3% lower recall.
Bloom et al (2007) manually create taxonomies
of opinion targets for two datasets. With a hand-
crafted set of dependency tree paths their algorithm
identifies related opinion expressions and targets.
Due to the lack of a dataset annotated with opinion
expressions and targets, they just evaluate the accu-
racy of several aspects of their algorithm by man-
ually assessing an output sample. Their algorithm
yields an accuracy of 0.75 in the identification of
opinion targets.
Kim and Hovy (2006) aim at extracting opinion
holders and opinion targets in newswire with se-
mantic role labeling. They define a mapping of the
semantic roles identified with FrameNet to the re-
spective opinion elements. As a baseline, they im-
1036
plement an approach based on a dependency parser,
which identifies the targets following the dependen-
cies of opinion expressions. They measure the over-
lap between two human annotators and their algo-
rithm as well as the baseline system. The algorithm
based on semantic role labeling yields an F-Measure
of 0.315 with annotator1 and 0.127 with annotator2,
while the baseline yields an F-Measure of 0.107 and
0.109 regarding opinion target extraction
2.2 Supervised Opinion Target Extraction
Zhuang et al (2006) present a supervised algorithm
for the extraction of opinion expression - opinion
target pairs. Their algorithm learns the opinion tar-
get candidates and a combination of dependency and
part-of-speech paths connecting such pairs from an
annotated dataset. They evaluate their system in a
cross validation setup on a dataset of user-generated
movie reviews and compare it to the results of the Hu
and Liu (2004) system as a baseline. Thereby, the
system by Zhuang et al (2006) yields an F-Measure
of 0.529 and outperforms the baseline which yields
an F-Measure of 0.488 in the task of extracting opin-
ion target - opinion expression pairs.
Kessler and Nicolov (2009) solely focus on iden-
tifying which opinion expression is linked to which
opinion target in a sentence. They present a dataset
of car and camera reviews in which opinion expres-
sions and opinion targets are annotated. Starting
with this information, they train a machine learn-
ing classifier for identifying related opinion expres-
sions and targets. Their algorithm receives the opin-
ion expression and opinion target annotations as in-
put during runtime. The classifier is evaluated us-
ing the algorithm by Bloom et al (2007) as a base-
line. The support vector machine based approach
by Kessler and Nicolov (2009) yields an F-Measure
of 0.698, outperforming the baseline which yields an
F-Measure of 0.445.
2.3 Domain Adaptation in Opinion Mining
The task of creating a supervised algorithm, which
when trained on data from domain A, also performs
well on data from another domain B, is a domain
adaptation problem (Daume? III and Marcu, 2006;
Jiang and Zhai, 2007). Aue and Gamon (2005) have
investigated this challenge very early in the task of
document level sentiment classification (positive /
negative). They observe that increasing the amount
of training data raises the classification accuracy, but
only if the training data is from one source domain.
Increasing the training data by mixing domains does
not yield any consistent improvements. Blitzer et
al. (2007) introduce an extension to a structural cor-
respondence learning algorithm, which was specifi-
cally designed to address the task of domain adap-
tation. Their enhancement aims at identifying pivot
features, which are stable across domains. In a series
of experiments in document level sentiment classi-
fication they show that their extension outperforms
the original structural correspondence learning ap-
proach. In their error analysis, the authors observe
the best results were reached when the training - test-
ing combinations were Books - DVDs or Electronics
- Kitchen appliances. They conclude that the topi-
cal relatedness of the domains is an important factor.
Furthermore they observe that training the algorithm
on a smaller amount of data from a similar domain is
more effective than increasing the amount of train-
ing data by mixing domains.
3 CRF-based Approach for Opinion
Target Extraction
In the following we will describe the features we
employ as input for our CRF-based approach. As the
development data, we used 29 documents from the
movies dataset, 23 documents from the web-services
dataset and 15 documents from the cars & cameras
datasets.
Token
This feature represents the string of the current token
as a feature. Even though this feature is rather ob-
vious, it can have considerable impact on the target
extraction performance. If the vocabulary of targets
is rather compact for a certain domain (correspond-
ing to a low target type / target ratio), the training
data is likely to contain the majority of the target
types, which should hence be a good indicator. We
will refer to this feature as tk in our result tables.
POS
This feature represents the part-of-speech tag of the
current token as identified by the Stanford POS Tag-
ger1. It can provide some means of lexical disam-
1http://nlp.stanford.edu/software/tagger.shtml
1037
biguation, e.g. indicate that the token ?sounds? is
a noun and not a verb in a certain context. At the
same time, the CRF algorithm is provided with ad-
ditional information to extract opinion targets which
are multiword expressions, i.e. noun combinations.
We will refer to this feature as pos in our result ta-
bles.
Short Dependency Path
Previous research has successfully employed paths
in the dependency parse tree to link opinion expres-
sions and the corresponding targets (Zhuang et al,
2006; Kessler and Nicolov, 2009). Both works iden-
tify direct dependency relations such as ?amod? and
?nsubj? as the most frequent and at the same time
highly accurate connections between a target and an
opinion expression. We hence label all tokens which
have a direct dependency relation to an opinion ex-
pression in a sentence. The Stanford Parser2 is em-
ployed for the constituent and dependency parsing.
We will refer to this feature as dLn in our result ta-
bles.
Word Distance
From the work of Zhuang et al (2006) we can infer
that opinion expressions and their target(s) are not
always connected via short paths in the dependency
parse tree. Since we cannot capture such paths with
the abovementioned feature we introduce another
feature which acts as heuristic for identifying the
target to a given opinion expression. Hu and Liu
(2004) and Yi et al (2003) have shown that (base)
noun phrases are good candidates for opinion targets
in the datasets of product reviews. We therefore la-
bel the token(s) in the closest noun phrase regarding
word distance to each opinion expression in a sen-
tence. We will refer to this feature as wrdDist in our
result tables.
Opinion Sentence
With this feature, we simply label all tokens occur-
ring in a sentence containing an opinion expression.
This feature shall enable the CRF algorithm to
distinguish between the occurence of a certain
token in a sentence which contains an opinion vs. a
sentence without an opinion. We will refer to this
feature as sSn in our result tables.
2http://nlp.stanford.edu/software/lex-parser.shtml
Our goal is to extract individual instances of opinion
targets from sentences which contain an opinion
expression. This can be modeled as a sequence
segmentation and labeling task. The CRF algorithm
receives a sequence of tokens t1...tn for which
it has to predict a sequence of labels l1...ln. We
represent the possible labels following the IOB
scheme: B-Target, identifying the beginning of an
opinion target, I-Target identifying the continuation
of a target, and O for other (non-target) tokens. We
model the sentences as a linear chain CRF, which
is based on an undirected graph. In the graph, each
node corresponds to a token in the sentence and
edges connect the adjacent tokens as they appear in
the sentence. In our experiments, we use the CRF
implementation from the Mallet toolkit3.
4 Experimental Setup
4.1 Datasets
In our experiments, we employ datasets from three
different sources, which span four domains in total
(see Table 1). All of them consist of reviews col-
lected from Web 2.0 sites. The first dataset con-
sists of reviews for 20 different movies collected
from the Internet Movie Database. It was presented
in Zhuang et al (2006) and annotated regarding
opinion target - opinion expression pairs. The sec-
ond dataset consists of 234 reviews for two different
web-services collected from epinions.com, as de-
scribed in Toprak et al (2010). The third dataset is
an extended version of the data presented in Kessler
and Nicolov (2009). The authors have provided us
with additional documents, which have been anno-
tated in the meantime. The version of the dataset
used in our experiments consists of 179 blog post-
ings regarding different digital cameras and 336 re-
views of different cars. In the description of their
annotation guidelines, Kessler and Nicolov (2009)
refer to opinion targets as mentions. Mentions are
all aspects of the review topic, which can be targets
of expressed opinions. However, not only mentions
which occur as opinion targets were originally anno-
tated, but also mentions which occur in non-opinion
sentences. In our experiments, we only use the men-
tions which occur as targets of opinion expressions.
3http://mallet.cs.umass.edu/
1038
All three datasets contain annotations regarding
the antecedents of anaphoric opinion targets. In our
experimental setup, we do not require the algorithms
to also correctly resolve the antecedent of an opin-
ion target representy by a pronoun, as we are solely
interested in evaluating the opinion target extraction
not any anaphora resolution.
As shown in rows 4 and 5 of Table 1, the docu-
ments from the cars and the cameras datasets exhibit
a much higher density of opinions per document.
53.5% of the sentences from the cars dataset contain
an opinion and in the cameras dataset even 56.1%
of the sentences contain an opinion, while in the
movies and the web-services reviews just 22.1% and
22.4% of the sentences contain an opinion. Further-
more in the cars and the cameras datasets the lexical
variability regarding the opinion targets is substan-
tially larger than in the other two datasets: We calcu-
late target types by counting the number of distinct
opinion targets in a dataset. We divide this by the
sum of all opinion target instances in the dataset. For
the cars dataset this ratio is 0.440 and for the cam-
eras dataset it is 0.433, while for the web-services
dataset it is 0.306 and for the movies dataset only
0.122. In terms of reviews this means, that in the
movie reviews the same movie aspects are repeat-
edly commented on, while in the cars and the cam-
eras datasets many different aspects of these entities
are discussed, which in turn each occur infrequently.
Table 1: Dataset Statistics
movies web- cars camerasservices
Documents 1829 234 336 179
Sentences 24555 6091 10969 5261
Tokens /
20.3 17.5 20.3 20.4
sentence
Sentences with
21.4% 22.4% 51.1% 54.0%
target(s)
Sentences with
21.4% 22.4% 53.5% 56.1%
opinion(s)
Targets 7045 1875 8451 4369
Target types 865 574 3722 1893
Tokens / target 1.21 1.35 1.29 1.42
Avg. targets /
1.33 1.37 1.51 1.53
opinion sent.
4.2 Baseline System
In the task of opinion target extraction the super-
vised algorithm by Zhuang et al (2006) represents
the state-of-the-art on the movies dataset we also
employ in our experiments. We therefore use it as
a baseline. The algorithm learns two aspects from
the labeled training data:
1. A set of opinion target candidates
2. A set of paths in a dependency tree which iden-
tify valid opinion target - opinion expression
pairs
In our experiments, we learn the full set of opin-
ion targets from the labeled training data in the first
step. This is slightly different from the approach
in (Zhuang et al, 2006), but we expect that this mod-
ification should be beneficial for the overall perfor-
mance in terms of recall, as we do not remove any
learned opinion targets from the candidate list. In
the second step, the annotated sentences are parsed
and a graph containing the words of a sentence is
created, which are connected by the dependency re-
lations between them. For each opinion target -
opinion expression pair from the gold standard, the
shortest path connecting them is extracted from the
dependency graph. A path consists of the part-of-
speech tags of the nodes and the dependency types
of the edges. Example 3 shows a typical dependency
path.
(3) NN - nsubj - NP - amod - JJ
During runtime, the algorithm identifies opinion tar-
gets from the candidate list in the training data. The
opinion expressions are directly taken from the gold
standard, as we focus on the opinion target extrac-
tion aspect in this work. The sentences are then
parsed and if a valid path between a target and
an opinion expression is found in the list of possi-
ble paths, then the pair is extracted. Since the de-
pendency paths only identify pairs of single word
target and opinion expression candidates, we em-
ploy a merging step. Extracted target candidates are
merged into a multiword target if they are adjacent
in a sentence. Thereby, the baseline system is also
capable of extracting multiword opinion targets.
1039
4.3 Metrics
We employ the following requirements in our eval-
uation of the opinion target extraction: An opin-
ion target must be extracted with exactly the span
boundaries as annotated in the gold standard. This
is especially important regarding multiword tar-
gets. Extracted targets which partially overlap with
the annotated gold standard are counted as errors.
Hence a target extracted by the algorithm which
does not exactly match the boundaries of a target
in the gold standard is counted as a false positive
(FP), e.g. if ?battery life? is annotated as the tar-
get in the gold standard, only ?battery? or ?life?
extracted as targets will be counted as FPs. Exact
matches between the targets extracted by the algo-
rithm and the gold standard are true positives (TP).
We refer to the number of annotated targets in the
gold standard as TGS . Precision is calculated as
Precision = TPTP+FP , and recall is calculated as
Recall = TPTGS . F-Measure is the harmonic mean of
precision and recall.
5 Results and Discussion
We investigate the performance of the baseline and
the CRF-based approach for opinion target extrac-
tion in a single- and cross-domain setting. The
single-domain approach assumes that there is a set
of training data available for the same domain as
the domain the algorithm is being tested on. In this
setup, we will both run the baseline and our CRF
based system in a 10-fold cross-validation and report
results macro averaged over all runs. In the cross-
domain approach, we will investigate how the algo-
rithm performs if given training data from domain A
while being tested on another domain B. In this set-
ting, we will train the algorithm on the entire dataset
A, and test it on the entire dataset B, we hence report
one micro averaged result set. In Subsection 5.1 we
present the results of both the baseline system and
our CRF-based approach in the single-domain set-
ting, in Subsection 5.2 we present the results of the
two systems in the cross-domain opinion target ex-
traction.
Table 2: Single-Domain Extraction with Zhuang Baseline
Dataset Precision Recall F-Measure
movies 0.663 0.592 0.625
web-services 0.624 0.394 0.483
cars 0.259 0.426 0.322
cameras 0.423 0.431 0.426
5.1 Single-Domain Results
5.1.1 Zhuang Baseline
As shown in Table 2, the state-of-the-art algo-
rithm of Zhuang et al (2006) performs best on the
movie review dataset and worst on the cars dataset.
The results on the movie dataset are higher than
originally reported in (Zhuang et al, 2006) (Preci-
sion 0.483, Recall 0.585, F-Measure 0.529). We as-
sume that this is due to two reasons: 1. In our task,
the algorithm uses the opinion expression annotation
from the gold standard. 2. We do not remove any
learned opinion target candidates from the training
data (See Section 4.2).
During training we observed that for each dataset
the lists of possible dependency paths (see Exam-
ple 3) contained several hundred entries, many of
them only occurring once. We assume that the re-
call of the algorithm is limited by a large variety
of possible dependency paths between opinion tar-
gets and opinion expressions, since the algorithm
cannot link targets and opinion expressions in the
testing data if there is no valid candidate depen-
dency path. Furthermore, we observe that for the
cars dataset the size of the dependency path candi-
date list (6642 entries) was approximately five times
larger than the dependency graph candidate list for
the web-services dataset (1237 entries), which has a
comparable size regarding documents. At the same
time, the list of target candidates of the cars dataset
was approximately eight times larger than the tar-
get candidate list for the web-services dataset. We
assume that a large number of both the target can-
didates as well as the dependency path candidates
introduces many false positives during the target ex-
traction, hence lowering the precision of the algo-
rithm on the cars dataset considerably.
1040
Table 3: Single-Domain Extraction with our CRF-based Approach
movies web-services cars cameras
Features Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me
tk, pos 0.639 0.133 0.220 0.500 0.051 0.093 0.438 0.110 0.175 0.300 0.085 0.127
tk, pos, wDs 0.542 0.181 0.271 0.451 0.272 0.339 0.570 0.354 0.436 0.549 0.375 0.446
tk, pos, dLn 0.777 0.481 0.595 0.634 0.380 0.475 0.603 0.372 0.460 0.569 0.376 0.453
tk, pos, sSn 0.673 0.637 0.653 0.604 0.397 0.476 0.453 0.180 0.257 0.398 0.172 0.238
tk, pos, dLn, wDs 0.792 0.481 0.598 0.620 0.354 0.450 0.603 0.389 0.473 0.596 0.425 0.496
tk, pos, sSn, wDs 0.662 0.656 0.659 0.664 0.461 0.544 0.564 0.370 0.446 0.544 0.381 0.447
tk, pos, sSn, dLn 0.791 0.477 0.594 0.654 0.501 0.568 0.598 0.384 0.467 0.586 0.391 0.468
tk, pos, sSn, dLn, wDs 0.749 0.661 0.702 0.722 0.526 0.609 0.622 0.414 0.497 0.614 0.423 0.500
pos, sSn, dLn, wDs 0.672 0.441 0.532 0.612 0.322 0.422 0.612 0.369 0.460 0.674 0.398 0.500
5.1.2 Our CRF-based Approach
Table 3 shows the results of the opinion target ex-
traction using the CRF algorithm. Row 8 contains
the results of the feature configuration, which yields
the best performance regarding F-Measure across all
datasets. We observe that our aproach outperforms
the Zhuang baseline on all datasets. The gain in F-
Measure is between 0.077 in the movies domain and
0.175 in the cars domain. Although the CRF-based
approach clearly outperforms the baseline system
on all four datasets, we also observe the same gen-
eral trend regarding the individual results: The CRF
yields the best results on the movies dataset and the
worst results on the cars & cameras dataset.
As shown in the first row, the results when using
just the token string and part-of-speech tags as fea-
tures are very low, especially regarding recall. We
observe that the higher the lexical variability of the
opinion targets is in a dataset, the lower the results
are. If we add the feature based on word distance
(row 2), the recall is improved on all datasets, while
the precision is slightly lowered on the movies and
web-services datasets. The dependency path based
feature performs better compared to the word dis-
tance heuristic as shown in row 3. The precision is
considerably increased on all datasets, on the movies
and cars & cameras datasets even reaching the over-
all highest value. At the same time, we observe
an increase of recall on all datasets. The obser-
vation made in previous research that short paths
in the dependency graph are a high precision indi-
cator of related opinion expressions - opinion tar-
gets (Kessler and Nicolov, 2009) is confirmed on all
datasets. Adding the information regarding opinion
sentences to the basic features of the token string and
the part-of-speech tag (row 4) yields the biggest im-
provements regarding F-Measure on the movies and
web-services dataset (+0.433 / +0.383). On the cars
& cameras dataset the recall is relatively low again.
We assume that this is again due to the high lexical
variability, so that the CRF algorithm will encounter
many actual opinion targets in the testing data which
have not occurred in the training data and will hence
not be extracted.
As shown in row 5, if we combine the dependency
graph based feature with the word distance heuris-
tic, the results regarding F-Measure are consistently
higher than the results of these features in isolation
(rows 2 - 4) on all datasets. We conclude that these
two features are complementary, as they apparently
indicate different kinds of opinion targets which are
then correctly extracted by the CRF. If we combine
each of the opinion expression related features with
the label which identifies opinion sentences in gen-
eral (rows 6 & 7), we observe that this feature is
also complementary to the others. On all datasets the
results regarding F-Measure are consistently higher
compared to the features in isolation (rows 2 - 4).
Row 8 shows the results of all features in combina-
tion. Again, we observe the complementarity of the
features, as the results of this feature combination
are the best regarding F-Measure across all datasets.
In row 9 of the results, we exclude the token
string as a feature. In comparison to the full fea-
ture combination of row 8 we observe a significant
decrease of F-Measure on the movies and the web-
services dataset. On the cars dataset we only observe
a slight decrease of recall. Interestingly on the cam-
eras dataset we even observe a slight increase of pre-
cision which compensates a slight decrease of recall,
1041
in turn resulting in stable F-Measure of 0.500 as in
the full feature set of row 8.
We have run some additional experiments in
which we did not rely on the annotated opinion ex-
pressions, but employed a general pupose subjectiv-
ity lexicon4. Already in the single-domain extrac-
tion, we observed that the results declined substan-
tially (e.g. web-services F-Measure: 0.243, movies
F-Measure: 0.309, cars F-Measure: 0.192 and cam-
eras F-Measure: 0.198).
We performed a quantitative error analysis on the
results of the CRF-based approach in the single-
domain setting. In doing so, we focused on misclas-
sifications of B-Target and I-Target instances, as the
recall is consistently lower than the precision across
all datasets. We observe that most of the recall errors
result from one-word opinion targets or the begin-
ning of opinion targets (B-Targets) being missclassi-
fied as non-targets (movies 83%, web-services 73%,
cars 68%, cameras 64%). For the majority of these
missclassifications neither the dLn nor the wDs fea-
tures were present (movies 82%, web-services 56%,
cars 64%, cameras 61%). We assume that our fea-
tures cannot capture the structure of more complex
sentences very well. Our results indicate that the
dLn and wDs features are complementary, but appar-
ently there are quite a few cases in which the opin-
ion target is neither directly related to the opinion
expression in the dependency graph nor close to it
in the sentence. One of these sentences, in this case
from a camera review, in shown in Example 4.
(4) A lens cap and a strap may not sound very
important, but it makes a huge difference in the
speed and usability of the camera.
In this sentence, the dLn and wDs features both la-
beled ?speed? which was incorrectly extracted as the
target of the opinion. None of the actual targets ?lens
cap?, ?strap? and ?camera? have a short dependency
path to the opinion expression and ?speed? is sim-
ply the closest noun to it. Note that although both
?speed? and ?usability? are attributes of a camera,
the opinion in this sentence is about the ?lens cap?
and ?strap?, hence only these attributes are anno-
tated as targets.
4http://www.cs.pitt.edu/mpqa/
5.2 Cross-Domain Results
5.2.1 Zhuang Baseline
Table 4 shows the results of the opinion target ex-
traction with the state-of-the-art system in the cross-
domain setting. We observe that the results on all
domain combinations are very low. A quantitative
error analysis has revealed that there is hardly any
overlap in the opinion target candidates between do-
mains, as reflected by the low recall in all config-
urations. The vocabularies of the opinion targets
are too different, hence the performance of the algo-
rithm by Zhuang et al (2006) is so low. The overlap
regarding the dependency paths between domains
was however higher. Especially identical short paths
could be found across domains which at the same
time typically occured quite often. For future work
it might be interesting to investigate how the algo-
rithm by Zhuang et al (2006) performs in the cross-
domain setting if the target candidate learning is per-
formed differently, e.g. with a statistical approach as
outlined in Section 2.1.
5.2.2 CRF-based Approach
The results of the cross-domain target extraction
with the CRF-based algorithm are shown in Table 5.
Due to the increase of system configurations intro-
duced by the training - testing data combinations,
we had to limit results of the feature combinations
reported in the Table. The feature combination pos,
sSn, wDs, dLn yielded the best results regarding F-
Measure. Hence, we report its result as the basic fea-
ture set. When comparing the results of the best per-
forming feature / training data combination of our
CRF-based approach with the baseline, we observe
that our approach outperforms the baseline on all
four domains. The gain in F-Measure is 0.409 in the
movies domain, 0.242 in the web-services domain,
0.294 in the cars domain and 0.343 in the cameras
domain.
Effects of Features
Interestingly with the best performing feature com-
bination from the single-domain extraction, the re-
sults regarding recall in the cross-domain extraction
are very low5. This is due to the fact that the CRF at-
tributed a relatively large weight to the token string
5Not shown in any result table due to limited space.
1042
Table 4: Cross-Domain Extraction with Zhuang Baseline
Training Testing Precision Recall F-Measure
web-services movies 0.194 0.032 0.055
cars movies 0.032 0.034 0.033
cameras movies 0.155 0.084 0.109
cars + cameras movies 0.071 0.104 0.084
web-services + cars + cameras movies 0.070 0.103 0.083
movies web-services 0.311 0.073 0.118
cars web-services 0.086 0.091 0.089
cameras web-services 0.164 0.081 0.108
cars + cameras web-services 0.086 0.104 0.094
movies + cars + cameras web-services 0.074 0.100 0.080
movies cars 0.182 0.014 0.026
web-services cars 0.218 0.028 0.049
cameras cars 0.250 0.121 0.163
cameras + web-services cars 0.247 0.131 0.171
movies + web-services cars 0.246 0.045 0.076
movies cameras 0.108 0.012 0.022
web-services cameras 0.268 0.048 0.082
cars cameras 0.125 0.160 0.140
cars + web-services cameras 0.119 0.157 0.136
movies + web-services cameras 0.245 0.063 0.100
feature. As we also observed in the analysis of the
baseline results, the overlap of the opinion target vo-
cabularies between domains is low, which resulted
in a very small number of targets extracted by the
CRF. As shown in Table 5 the results are promising
regarding F-Measure if we just leave the token fea-
ture out of the configuration.
Effects of Training Data
When analyzing the results of the different training
- testing domain configurations we observe the fol-
lowing: In isolation training data from the cameras
domain consistently yields the best results regarding
F-Measure when the algorithm is run on the datasets
from the other three domains. This is particularly
interesting since the cameras dataset is the smallest
of the four (see Table 1). We investigated whether
the CRF algorithm was overfitting to the training
datasets by reducing their size to the size of the cam-
eras dataset. However, the reduction of the train-
ing data sizes never improved the extraction results
regarding F-Measure for the movies, web-serviecs
and cars datasets. The good results when training
on the cameras dataset are in line with our obser-
vations from Section 5.1.2. We noticed that on the
cameras dataset the results regarding F-Measure re-
mained stable if the token feature is not used in the
training.
In isolation, training only on the cars data yields
the second highest results on the movies and web-
services datasets and the highest results regarding
F-Measure on the cameras data. However, the re-
sults of the cars + cameras training data combination
indicate that the cameras data does not contribute
any additional information during the learning, since
the results on both the movies and the web-services
datasets are lower than when training only on the
cameras data.
Our results also confirm the insights gained
by Blitzer et al (2007), who observed that in cross-
domain polarity analysis adding more training data
is not always beneficial. Apparently even the small-
est training dataset (cameras) contain enough feature
instances to learn a model which performs well on
the testing data.
We observe that the results of the cross-domain
extraction regarding F-Measure come relatively
close to the results of the single-domain setting, es-
pecially if the token string feature is removed there
(see Table 3 row 9). On the cars and the cameras
dataset the cross-domain results are even closer to
the single-domain results. The features we employ
seem to scale well across domains and compensate
the difference between training and testing data and
the lack of information regarding the target vocabu-
1043
Table 5: Cross-Domain Extraction with our CRF-based Approach
Testing
web-services movies
Pre Rec F-Me Pre Rec F-Me
T
ra
in
in
g
web-services - - - 0.560 0.339 0.422
movies 0.565 0.219 0.316 - - -
cars 0.538 0.248 0.340 0.642 0.382 0.479
cameras 0.529 0.256 0.345 0.642 0.408 0.499
movies + cars 0.554 0.249 0.344 - - -
movies + cameras 0.530 0.273 0.360 - - -
movies + cars + cameras 0.562 0.250 0.346 - - -
cars + cameras 0.538 0.254 0.345 0.641 0.395 0.489
web-services + cars - - - 0.651 0.396 0.492
web-services + cameras - - - 0.642 0.435 0.518
web-services + cars + cameras - - - 0.639 0.405 0.496
cars cameras
Pre Rec F-Me Pre Rec F-Me
web-services 0.391 0.277 0.324 0.505 0.330 0.399
movies 0.512 0.307 0.384 0.550 0.303 0.391
cars - - - 0.665 0.369 0.475
cameras 0.589 0.384 0.465 - - -
cameras + movies 0.567 0.394 0.465 - - -
cameras + web-services 0.572 0.381 0.457 - - -
movies + web-services 0.489 0.327 0.392 0.553 0.339 0.421
movies + cars - - - 0.634 0.376 0.472
web-services + cars - - - 0.678 0.376 0.483
web-services + movies + cars - - - 0.635 0.378 0.474
movies + web-services + cameras 0.549 0.381 0.450 - - -
lary.
6 Conclusions
In this paper, we have shown how a CRF-based
approach for opinion target extraction performs in
a single- and cross-domain setting. We have pre-
sented a comparative evaluation of our approach
on datasets from four different domains. In the
single-domain setting, our CRF-based approach out-
performs a supervised baseline on all four datasets.
Our error analysis indicates that additional features,
which can capture opinions in more complex sen-
tences, are required to improve the performance of
the opinion target extraction. Our CRF-based ap-
proach also yields promising results in the cross-
domain setting. The features we employ scale well
across domains, given that the opinion target vocab-
ularies are substantially different. For future work,
we might investigate how machine learning algo-
rithms, which are specifically designed for the prob-
lem of domain adaptation (Blitzer et al, 2007; Jiang
and Zhai, 2007), perform in comparison to our ap-
proach. Since three of the features we employed in
our CRF-based approach are based on the respec-
tive opinion expressions, it is to investigate how to
mitigate the possible negative effects introduced by
errors in the opinion expression identification if they
are not annotated in the gold standard. We observe
similar challenges as Choi et al (2005) regarding the
analysis of complex sentences. Although our data is
user-generated from Web 2.0 communities, a man-
ual inspection has shown that the documents were
of relatively high textual quality. It is to investigate
to which extent the approaches taken in the analysis
of newswire, such as identifying targets with coref-
erence resolution, can also be applied to our task on
user-generated discourse.
Acknowledgments
The project was funded by means of the German Fed-
eral Ministry of Economy and Technology under the
promotional reference ?01MQ07012?. The authors take
the responsibility for the contents. This work has been
supported by the Volkswagen Foundation as part of
the Lichtenberg-Professorship Program under grant No.
I/82806.
1044
References
Anthony Aue and Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: A case study.
In Proceedings of the 5th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440?447,
Prague, Czech Republic, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In Proceed-
ings of Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 308?
315, Rochester, New York, USA, April.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 355?362, Vancou-
ver, Canada, October.
James R. Cowie and Wendy G. Lehnert. 1996. In-
formation extraction. Communications of the ACM,
39(1):80?91.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research (JAIR), 26:101?126.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177,
Seattle, Washington, USA, August.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 264?271, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Jason Kessler and Nicolas Nicolov. 2009. Targeting sen-
timent expressions through supervised ranking of lin-
guistic configurations. In Proceedings of the Third In-
ternational AAAI Conference on Weblogs and Social
Media, pages 90?97, San Jose, California, USA, May.
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the ACL Workshop
on Sentiment and Subjectivity in Text, pages 1?8, Syd-
ney, Australia, July.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference
on Machine Learning, pages 282?289, Williamstown,
MA, USA, June.
Fuchun Peng and Andrew McCallum. 2006. Information
extraction from research papers using conditional ran-
dom fields. Information Processing and Management,
42(4):963?979, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 339?346, Vancouver,
Canada, October.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 575?584, Uppsala,
Sweden, July.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
3rd IEEE International Conference on Data Mining,
pages 427?434, Melbourne, Florida, USA, December.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM 15th Conference on Information and Knowl-
edge Management, pages 43?50, Arlington, Virginia,
USA, November.
1045
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575?584,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Sentence and Expression Level Annotation of Opinions in User-Generated
Discourse
Cigdem Toprak and Niklas Jakob and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt, Hochschulstra?e 10
D-64289 Darmstadt, Germany
www.ukp.tu-darmstadt.de
Abstract
In this paper, we introduce a corpus of
consumer reviews from the rateitall and
the eopinions websites annotated with
opinion-related information. We present
a two-level annotation scheme. In the
first stage, the reviews are analyzed at
the sentence level for (i) relevancy to a
given topic, and (ii) expressing an eval-
uation about the topic. In the second
stage, on-topic sentences containing eval-
uations about the topic are further investi-
gated at the expression level for pinpoint-
ing the properties (semantic orientation,
intensity), and the functional components
of the evaluations (opinion terms, targets
and holders). We discuss the annotation
scheme, the inter-annotator agreement for
different subtasks and our observations.
1 Introduction
There has been a huge interest in the automatic
identification and extraction of opinions from free
text in recent years. Opinion mining spans a va-
riety of subtasks including: creating opinion word
lexicons (Esuli and Sebastiani, 2006; Ding et al,
2008), identifying opinion expressions (Riloff and
Wiebe, 2003; Fahrni and Klenner, 2008), identi-
fying polarities of opinions in context (Breck et
al., 2007; Wilson et al, 2005), extracting opinion
targets (Hu and Liu, 2004; Zhuang et al, 2006;
Cheng and Xu, 2008) and opinion holders (Kim
and Hovy, 2006; Choi et al, 2005).
Data-driven approaches for extracting opinion
expressions, their holders and targets require re-
liably annotated data at the expression level. In
previous research, expression level annotation of
opinions was extensively investigated on newspa-
per articles (Wiebe et al, 2005; Wilson and Wiebe,
2005; Wilson, 2008b) and on meeting dialogs (So-
masundaran et al, 2008; Wilson, 2008a).
Compared to the newspaper and meeting dialog
genres, little corpus-based work has been carried
out for interpreting the opinions and evaluations in
user-generated discourse. Due to the high popular-
ity of Web 2.0 communities1, the amount of user-
generated discourse and the interest in the analysis
of such discourse has increased over the last years.
To the best of our knowledge, there are two cor-
pora of user-generated discourse which are anno-
tated for opinion related information at the expres-
sion level: The corpus of Hu & Liu (2004) consists
of customer reviews about consumer electronics,
and the corpus of Zhuang et al (2006) consists of
movie reviews. Both corpora are tailored for ap-
plication specific needs, therefore, do not contain
certain related information explicitly annotated in
the discourse, which we consider important (see
Section 2). Furthermore, none of these works pro-
vide inter-annotator agreement studies.
Our goal is to create sentence and expression
level annotated corpus of customer reviews which
fulfills the following requirements: (1) It filters
individual sentences regarding their topic rele-
vancy and the existence of an opinion or factual
information which implies an evaluation. (2) It
identifies opinion expressions including the re-
spective opinion target, opinion holder, modi-
fiers, and anaphoric expressions if applicable. (3)
The semantic orientation of the opinion expres-
sion is identified while considering negation, and
the opinion expression is linked to the respective
holder and target in the discourse. Such a re-
source would (i) enable novel applications of opin-
ion mining such as a fine-grained identification of
opinion properties, e.g. opinion modification de-
tection including negation, and (ii) enhance opin-
ion target extraction and the polarity assignment
by linking the opinion expression with its target
1http://blog.nielsen.com/nielsenwire/
wp-content/uploads/2008/10/press_
release24.pdf
575
and providing anaphoric resolutions in discourse.
We present an annotation scheme which fulfills
the mentioned requirements, an inter-annotator
agreement study, and discuss our observations.
The rest of this paper is structured as follows:
Section 2 presents the related work. In Sections
3, we describe the annotation scheme. Section 4
presents the data and the annotation study, while
Section 5 summarizes the main conclusions.
2 Previous Opinion Annotated Corpora
2.1 Newspaper Articles and Meeting Dialogs
Most prominent work concerning the expres-
sion level annotation of opinions is the Multi-
Perspective Question Answering (MPQA) corpus2
(Wiebe et al, 2005). It was extended several times
over the last years, either by adding new docu-
ments or annotating new types of opinion related
information (Wilson and Wiebe, 2005; Stoyanov
and Cardie, 2008; Wilson, 2008b). The MPQA
annotation scheme builds upon the private state
notion (Quirk et al, 1985) which describes men-
tal states including opinions, emotions, specula-
tions and beliefs among others. The annotation
scheme strives to represent the private states in
terms of their functional components (i.e. expe-
riencer holding an attitude towards a target). It
consists of frames (direct subjective, expressive
subjective element, objective speech event, agent,
attitude, and target frames) with slots represent-
ing various attributes and properties (e.g.intensity,
nested source) of the private states.
Wilson (2008a) adapts and extends the concepts
from the MPQA scheme to annotate subjective
content in meetings (AMI corpus), and creates the
AMIDA scheme. Besides subjective utterances,
the AMIDA scheme contains objective polar ut-
terances which annotates evaluations without ex-
pressing explicit opinion expressions.
Somasundaran et al (2008) proposes opinion
frames for representing discourse level associa-
tions in meeting dialogs. The annotation scheme
focuses on two types of opinions, sentiment and
arguing. It annotates the opinion expression and
target spans. The link and link type attributes asso-
ciate the target with other targets in the discourse
through same or alternative relations. The opinion
frames are built based on the links between tar-
gets. Somasundaran et al (2008) show that opin-
ion frames enable a coherent interpretation of the
2http://www.cs.pitt.edu/mpqa/
opinions in discourse and discover implicit evalu-
ations through link transitivity.
Similar to Somasundaran et al (2008), Asher
et al (2008) performs discourse level analysis of
opinions. They propose a scheme which first iden-
tifies and assigns categories to the opinion seg-
ments as reporting, judgment, advice, or senti-
ment; and then links the opinion segments with
each other via rhetorical relations including con-
trast, correction, support, result, or continuation.
However, in contrast to our scheme and other
schemes, instead of marking expression bound-
aries without any restriction they annotate an opin-
ion segment only if it contains an opinion word
from their lexicon, or if it has a rhetorical relation
to another opinion segment.
2.2 User-generated Discourse
The two annotated corpora of user-generated con-
tent and their corresponding annotation schemes
are far less complex. Hu & Liu (2004) present
a dataset of customer reviews for consumer elec-
tronics crawled from amazon.com. The follow-
ing example shows two annotations taken from the
corpus of Hu & Liu (2004):
camera[+2]##This is my first digital camera and what a toy
it is...
size[+2][u]##it is small enough to fit easily in a coat pocket
or purse.
The corpus provides only target and polarity anno-
tations, and do not contain opinion expression or
opinion modifier annotations which lead to these
polarity scores. The annotation scheme allows the
annotation of implicit features (indicated with the
the attribute [u] ). Implicit features are not re-
solved to any actual product feature instances in
discourse. In fact, the actual positions of the prod-
uct features (or any anaphoric references to them)
are not explicitly marked in the discourse, i.e, it is
unclear to which mention of the feature the opin-
ion refers to.
In their paper on movie review mining and sum-
marization, Zhuang et al (2006) introduce an an-
notated corpus of movie reviews from the Internet
Movie Database. The corpus is annotated regard-
ing movie features and corresponding opinions.
The following example shows an annotated sen-
tence:
?Sentence?I have never encountered a movie whose
supporting cast was so perfectly realized.?FO
Fword=?supporting cast? Ftype=?PAC? Oword=?perfect?
Otype=?PRO?/??/Sentence?
576
The movie features (Fword) are attributed to one
of 20 predefined categories (Ftype). The opin-
ion words (Oword) and their semantic orientations
(Otype) are identified. Possible negations are di-
rectly reflected by the semantic orientation, but not
explicitly labeled in the sentence. (PD) in the fol-
lowing example indicates that the movie feature is
referenced by anaphora:
?Sentence?It is utter nonsense and insulting to my
intelligence and sense of history. ?FO Fword=?film(PD)?
Ftype=?OA? Oword=?nonsense, insulting?
Otype=?CON?/??/Sentence?
However, similar to the corpus of Hu & Liu (2004)
the referring pronouns are not explicitly marked in
discourse. It is therefore neither possible to au-
tomatically determine which pronoun creates the
link if there are more than one in a sentence, nor it
is denoted which antecedent, i.e. the actual men-
tion of the feature in the discourse it relates to.
3 Annotation Scheme
3.1 Opinion versus Polar Facts
The goal of the annotation scheme is to capture the
evaluations regarding the topics being discussed in
the consumer reviews. The evaluations in con-
sumer reviews are either explicit expressions of
opinions, or facts which imply evaluations as dis-
cussed below.
Explicit expressions of opinions: Opinions are
private states (Wiebe et al, 2005; Quirk et al,
1985) which are not open to objective observation
or verification. In this study, we focus on the opin-
ions stating the quality or value of an entity, ex-
perience or a proposition from one?s perspective.
(1) illustrates an example of an explicit expression
of an opinion. Similar to Wiebe et al (2005), we
view opinions in terms of their functional compo-
nents, as opinion holders, e.g., the author in (1),
holding attitudes (polarity), e.g., negative attitude
indicated with the word nightmare, towards possi-
ble targets, e.g., Capella University.
(1) I had a nightmare with Capella University.3
Facts implying evaluations: Besides opinions,
there are facts which can be objectively verified,
but still imply an evaluation of the quality or value
of an entity or a proposition. For instance, con-
sider the snippet below:
3We use authentic examples from the corpus without cor-
recting grammatical or spelling errors.
(2) In a 6-week class, I counted 3 comments from the
professors directly to me and two directed to my team.
(3) I found that I spent most of my time learning from my
fellow students.
(4) A standard response from my professors would be that of
a sentence fragment.
The example above provides an evaluation about
the professors without stating any explicit expres-
sions of opinions. We call such objectively verifi-
able, but evaluative sentences polar facts. Explicit
expressions of opinions typically contain specific
cues, i.e. opinion words, loaded with a positive or
negative connotation (e.g., nightmare). Even when
they are taken out of the context in which they ap-
pear, they evoke an evaluation. However, evalu-
ations in polar facts can only be inferred within
the context of the review. For instance, the targets
of the implied evalution in the polar facts (2), (3)
and (4) are the professors. However, (3) may have
been perceived as a positive statement if the re-
view was explaining how good the fellow students
were or how the course enforced team work etc.
The annotation scheme consists of two levels.
First, the sentence level scheme analyses each sen-
tence in terms of (i) its relevancy to the overall
topic of the review, and (ii) whether it contains
an evaluation (an opinion or a polar fact) about
the topic. Once the on-topic sentences contain-
ing evaluations are identified, the expression level
scheme first focuses either on marking the text
spans of the opinion expressions (if the sentence
contains an explicit expression of an opinion) or
marking the targets of the polar facts (if the sen-
tence is a polar fact). Upon marking an opin-
ion expression span, the target and holder of the
opinion is marked and linked to the marked opin-
ion expression. Furthermore, the expression level
scheme allows assigning polarities to the marked
opinion expression spans and targets of the polar
facts.
The following subsections introduce the sen-
tence and the expression level annotation schemes
in detail with examples.
3.2 Sentence Level Annotation
The sentence annotation strives to identify the sen-
tences containing evaluations about the topic. In
consumer reviews people occasionally drift off the
actual topic being reviewed. For instance, as in
(5) taken from a review about an online university,
they tend to provide information about their back-
ground or other experiences.
(5) I am very fortunate and almost right out of high school
577
Figure 1: The sentence level annotation scheme
with a very average GPA and only 20; I already make above
$45,000 a year as a programmer with a large health care
company for over a year and have had 3 promotions up in
the first year and a half.
Such sentences do not provide information about
the actual topic, but typically serve for justifying
the user?s point of view or provide a better under-
standing about her circumstances. However, they
are not valuable for an application aiming to ex-
tract opinions about a specific topic.
Reviews given to the annotators contain meta
information stating the topic, for instance, the
name of the university or the service being re-
viewed. A markable (i.e. an annotation unit) is
created for each sentence prior to the annotation
process. At this level, the annotation process is
therefore a sentence labeling task. The annotators
are able to see the whole review, and instructed to
label sentences in the context of the whole review.
Figure 1 presents the sentence level scheme. At-
tribute names are marked with oval circles and the
possible values are given in parenthesis. The fol-
lowing attributes are used:
topic relevant attribute is labeled as yes if the
sentence discusses the given topic itself or its as-
pects, properties or features as in examples (1)-
(4). Other possible values for this attribute include
none given which can be chosen in the absence of
meta data, or no if the sentence drifted off the topic
as in example (5).
opinionated attribute is labeled as yes if the
sentence contains any explicit expressions of opin-
ions about the given topic. This attribute is pre-
sented if the topic relevant attribute has been la-
beled as none given or yes. In other words, only
the on-topic sentences are considered in this step.
Examples (6)-(8) illustrate examples labeled as
topic relevant=yes and opinionated=yes.
(6) Many people are knocking Devry but I have seen them to
be a very great school. [Topic: Devry University]
(7) University of Phoenix was a surprising disappointment.
[Topic: University of Phoenix]
(8) Assignments were passed down, but when asked to
clarify the assignment because the syllabus had
contradicting, poorly worded, information, my professors
regularly responded....?refer to the syllabus?....but wait, the
syllabus IS the question. [Topic: University of Phoenix]
polar fact attribute is labeled as yes if the sen-
tence is a polar fact. This attribute is presented
if the opinionated attribute has been labeled as
no. Examples (2)-(4) demonstrate sentences la-
beled as topic relevant=yes, opinionated=no and
polar fact=yes.
polar fact polarity attribute represents the po-
larity of the evaluation in a polar fact sentence.
The possible values for this attribute include posi-
tive, negative, both. The value both is intended for
the polar fact sentences containing more than one
evaluation with contradicting polarities. At the
expression level analysis, the targets of the con-
tradicting polar fact evaluations are identified dis-
tinctly and assigned polarities of positive or neg-
ative later on. Examples (9)-(11) demonstrate ex-
amples of polar fact sentences with different val-
ues of the attribute polar fact polarity.
(9) There are students in the first programming class and
after taking this class twice they cannot write a single line of
code. [polar fact polarity=negative]
(10) The same class (i.e. computer class) being teach at Ivy
League schools are being offered at Devry.
[polar fact polarity=positive]
(11) The lectures are interactive and recorded, but you need
a consent from the instructor each time.
[polar fact polarity=both]
3.3 Expression Level Annotation
At the expression level, we focus on the topic
relevant sentences containing evaluations, i.e.,
sentences labeled as topic relevant=yes, opinion-
ated=yes or topic relevant=yes, opinionated=no,
polar fact=yes. If the sentence is a polar fact, then
the aim is to mark the target and label the polarity
of the evaluation. If the sentence is opinionated,
then, the aim is to mark the opinion expression
span, and label its polarity and strength (i.e. in-
tensity), and to link it to the target and the holder.
Figure 2 presents the expression level scheme.
At this stage, annotators mark text spans, and are
allowed to assign one of the five labels to the
marked span:
The polar target is used to label the targets of
the evaluations implied by polar facts. The is-
Reference attribute labels polar targets which are
anaphoric references. The polar target polarity
578
Figure 2: The expression level annotation scheme
attribute is used to label the polarity as positive
or negative. If the isReference attribute is labeled
as true, then the referent attribute appears which
enables the annotator to resolve the reference to
its antecedent. Consider the example sentences
(12) and (13) below. The polar target in (13),
written bold, is labeled as isReference=true, po-
lar target polarity=negative. To resolve the ref-
erence, annotator first creates another polar target
markable for the antecedent, namely the bold text
span in (12), then, links the antecedent to the ref-
erent attribute of the polar target in (13).
(12) Since classes already started, CTU told me they would
extend me so that I could complete the classes and get credit
once I got back.
(13) What they didn?t tell me is in order to extend, I also had
to be enrolled in the next semester.
The target annotation represents what the opin-
ion is about. Both polar targets and targets can be
the topic of the review or different aspects, i.e. fea-
tures of the topic. Similar to the polar targets, the
isReference attribute allows the identification of
the targets which are anaphoric references and the
referent attribute links them to their antecedents in
the discourse. Bold span in (14) shows an example
of a target in an opinionated sentence.
(14) Capella U has incredible faculty in the Harold Abel
School of Psychology.
The holder type represents the holder of an
opinion in the discourse and is labeled in the same
manner as the targets and polar targets. In con-
sumer reviews, holders are most of the time the
authors of the reviews. To ease the annotation pro-
cess, the holder is not labeled when this is the au-
thor.
The modifier annotation labels the lexical items,
such as not, very, hardly etc., which affect the
strength of an opinion or shift its polarity. Upon
creation of a modifier markable, annotators are
asked to choose between negation, increase, de-
crease for identifying the influence of the modifier
on the opinion. For instance, the marked span in
(15) is labeled as modifier=increase as it gives the
impression that the author is really offended by the
negative comments about her university.
(15) I am quite honestly appauled by some of the negative
comments given for Capella University on this website.
The opinionexpression annotation is used to la-
bel the opinion terms in the sentence. This mark-
able type has five attributes, three of which, i.e.,
modifier, holder, and target are pointer attributes
to the previously defined markable types. The po-
larity attribute assesses the semantic orientation of
the attitude, where the strength attribute marks the
intensity of this attitude. The polarity and strength
attributes focus solely on the marked opinionex-
pression span, not the whole evaluation implied
in the sentence. For instance, the opinionexpres-
sion span in (16) is labeled as polarity=negative,
strength=average. We infer the polarity of the
evaluation only after considering the modifier, po-
larity and the strength attributes together. In (16),
the evaluation about the target is strongly negative
after considering all three attributes of the opinion-
expression annotation. In (17), the polarity of the
opinionexpression1 itself (complaints) is labeled
as negative. It is linked to the modifier1 which
is labeled as negation. Target1 (PhD journey) is
linked to the opinionexpression1. The overall eval-
uation regarding the target1 is positive after ap-
plying the affect of the modifier1 to the polarity
of the opinionexpression1, i.e., after negating the
negative polarity.
(16) I am quite honestly[modifier] appauled
by[opinionexpression] some of the negative comments
given for Capella University on this website[target].
(17) I have no[modifier1]
complaints[opinionexpression1] about the entire PhD
journey[target1] and highly[modifier2]
recommend[opinionexpression2] this school[target2].
Finally, Figure 3 demonstrates all expression
level markables created for an opinionated sen-
tence and how they relate to each other.
579
Figure 3: Expression level annotation example
4 Annotation Study
Each review has been annotated by two annotators
independently according to the annotation scheme
introduced above. We used the freely available
MMAX24 annotation tool capable of stand-off
multi-level annotations. Annotators were native
speaker linguistic students. They were trained on
15 reviews after reading the annotation manual.5
In the training stage, the annotators discussed with
each other if different decisions have been made
and were allowed to ask questions to clarify their
understanding of the scheme. Annotators had ac-
cess to the review text as a whole while making
their decisions.
4.1 Data
The corpus consists of consumer reviews col-
lected from the review portals rateitall6 and eopin-
ions7. It contains reviews from two domains in-
cluding online universities, e.g., Capella Univer-
sity, Pheonix, University of Maryland University
College etc. and online services, e.g., PayPal,
egroups, eTrade, eCircles etc. These two domains
were selected with the project-relevant, domain-
specific research goals in mind. We selected a spe-
cific topic, e.g. Pheonix, if there were more than 3
reviews written about it. Table 1 shows descriptive
statistics regarding the data.
We used 118 reviews containing 1151 sentences
from the university domain for measuring the sen-
tence and expression level agreements. In the fol-
lowing subsections, we report the inter-annotator
agreement (IAA) at each level.
4http://mmax2.sourceforge.net/
5http://www.ukp.tu-darmstadt.de/
research/data/sentiment-analysis
6http://www.rateitall.com
7http://www.epinions.com
University Service All
Reviews 240 234 474
Sentences 2786 6091 8877
Words 49624 102676 152300
Avg sent./rev. 11.6 26 18.7
Std. dev. sent./rev. 8.2 16 14.6
Avg. words/rev. 206.7 438.7 321.3
Std. dev. words/rev. 159.2 232.1 229.8
Table 1: Descriptive statistics about the corpus
4.2 Sentence Level Agreement
Sentence level markables were already created au-
tomatically prior to the annotation, i.e., the set of
annotation units were the same for both annota-
tors. We use Cohen?s kappa (?) (Cohen, 1960)
for measuring the IAA. The sentence level anno-
tation scheme has a hierarchical structure. A new
attribute is presented based on the decision made
for the previous attribute, for instance, opinionated
attribute is only presented if the topic relevant at-
tribute is labeled as yes or none given; polar fact
attribute is only presented if the opinionated at-
tribute is labeled as no etc. We calculate ? for each
attribute considering only the markables which
were labeled the same by both annotators in the
previously required step. Table 2 shows the ? val-
ues for each attribute, the size of the markable set
on which the value was calculated, and the per-
centage agreement.
Attribute Markables Agr. ?
topic relevant 1151 0.89 0.73
opinionated 682 0.80 0.61
polar fact 258 0.77 0.56
polar fact polarity 103 0.96 0.92
Table 2: Sentence level inter-annotator agreement
The agreement for topic relevancy shows that
it is possible to label this attribute reliably. The
sentences labeled as topic relevant by both anno-
tators correspond to 59% of all sentences, suggest-
ing that people often drift off the topic in consumer
reviews. This is usually the case when they pro-
vide information about their backgrounds or alter-
natives to the given topic.
On the other hand, we obtain moderate agree-
ment levels for the opinionated and polar fact at-
tributes. 62% of the topic relevant sentences were
labeled as opinionated by at least one annotator,
and the rest 38% constitute the topic relevant sen-
tences labeled as not opinionated by both anno-
tators. Nonetheless, they still contain evaluations
(polar facts), as 15% of the topic relevant sen-
580
tences were labeled as polar facts by both anno-
tators. When we merge the attributes opinionated
and polar fact into a single category, we obtain ?
of 0.75 and a percentage agreement of 87%. Thus,
we conclude that opinion-relevant sentences, ei-
ther in the form of an explicit expression of opin-
ion or a polar fact, can be labeled reliably in con-
sumer reviews. However, there is a thin border be-
tween polar facts and explicit expressions of opin-
ions.
To the best of our knowledge, similar annotation
efforts on consumer or movie reviews do not pro-
vide any agreement figures for direct comparison.
However, Wiebe et al (2005) present an annota-
tion study where they mark textual spans for sub-
jective expressions in a newspaper corpus. They
report pairwise ? values for three annotators rang-
ing between 0.72 - 0.84 for the sentence level sub-
jective/objective judgments. Wiebe et al (2005)
mark subjective spans, and do not explicitly per-
form the sentence level labeling task. They calcu-
late the sentence level ? values based on the ex-
istence of a subjective expression span in the sen-
tence. Although the task definitions, approaches
and the corpora have quite disparate characteris-
tics in both studies, we obtain comparable results
when we merge opinionated and polar fact cate-
gories.
4.3 Expression Level Agreement
At the expression level, annotators focus only on
the sentences which were labeled as opinionated
or polar fact by both annotators. Annotators were
instructed to mark text spans, and then, assign
them the annotation types such as polar target,
opinionexpression etc. (see Figure 2). For calcu-
lating the text span agreement, we use the agree-
ment metric presented by Wiebe et al (2005) and
Somasundaran et al (2008). This metric corre-
sponds to the precision (P) and recall (R) metrics
in information retrieval where the decisions of one
annotator are treated as the system; the decisions
of the other annotator are treated as the gold stan-
dard; and the overlapping spans correspond to the
correctly retrieved documents.
Somasundaran et al (2008) present a discourse
level annotation study in which opinion and tar-
get spans are marked and linked with each other
in a meeting transcript corpus. Following Soma-
sundaran et al (2008), we compute three differ-
ent measures for the text span agreement: (i) exact
matching in which the text spans should perfectly
match; (ii) lenient (relaxed) matching in which the
overlap between spans is considered as a match,
and (iii) subset matching in which a span has to
be contained in another span in order to be consid-
ered as a match.8 Agreement naturally increases
as we relax the matching constraints. However,
there were no differences between the lenient and
the subset agreement values. Therefore, we report
only the exact and lenient matching agreement re-
sults for each annotation type in Table 3. The
same agreement results for the lenient and subset
matching indicates that inexact matches are still
very similar to each other, i.e., at least one span is
totally contained in the other.
Somasundaran et al (2008) do not report any
F-measure. However, they report span agreement
results in terms of precision and recall ranging
between 0.44 - 0.87 for opinion spans and be-
tween 0.74 - 0.90 for the target spans. Wiebe et
al. (2005) use the lenient matching approach for
reporting text span agreements ranging between
0.59 - 0.81 for subjective expressions. We ob-
tain higher agreement values for both opinion ex-
pression and target spans. We attribute this to the
fact that the annotators look for opinion expression
and target spans within the opinionated sentences
which they agreed upon. Sentence level analysis
indeed increases the reliability at the expression
level. Compared to the high agreement on mark-
ing target spans, we obtain lower agreement val-
ues on marking polar target spans. We observe
that it is easier to attribute explicit expressions of
evaluations to topic relevant entities compared to
attributing evaluations implied by experiences to
specific topic relevant entities in the reviews.
We calculated the agreement on identifying
anaphoric references using the method introduced
in (Passonneau, 2004) which utilizes Krippen-
dorf?s ? (Krippendorff, 2004) for computing reli-
ability for coreference annotation. We considered
the overlapping target and polar target spans to-
gether in this calculation, and obtained an ? value
of 0.29. Compared to Passonneau (? values from
0.46 to 0.74), we obtain a much lower agreement
value. This may be due to the different definitions
and organizations of the annotation tasks. Passon-
neau requires prior marking of all noun phrases (or
instances which needs to be processed by the an-
8An example of subset matching: waste of time vs. total
waste of time
581
Span
Exact Lenient
P R F P R F
opinionexpression 0.70 0.80 0.75 0.82 0.93 0.87
modifier 0.80 0.82 0.81 0.86 0.86 0.86
target 0.80 0.81 0.80 0.91 0.90 0.91
holder 0.75 0.72 0.73 0.93 0.88 0.91
polar target 0.67 0.42 0.51 0.75 0.49 0.59
Table 3: Inter-annotator agreement on text spans at the expression level
notator). Annotator?s task is to identify whether
an instance refers to another marked entity in the
discourse, and then, to identify corefering entity
chains. However, in our annotation process anno-
tators were tasked to identify only one entity as the
referent, and was free to choose it from anywhere
in the discourse. In other words, our chains con-
tain only one entity. It is possible that both annota-
tors performed correct resolutions, but still did not
overlap with each other, as they resolve to differ-
ent instances of the same entity in the discourse.
We plan to further investigate reference resolution
annotation discrepancies and perform corrections
in the future.
Some annotation types require additional at-
tributes to be labeled after marking the span.
For instance, upon marking a text span as a po-
lar target or an opinionexpression, one has to la-
bel the polarity and strength. We consider the
overlapping spans for each annotation type and
use ? for reporting the agreement on these at-
tributes. Table 4 shows the ? values.
Attribute Markables Agr. ?
polarity 329 0.97 0.94
strength 329 0.74 0.55
modifier 136 0.88 0.77
polar target polarity 63 0.80 0.67
Table 4: Inter-annotator agreement at the expres-
sion level
We observe that the strength of the opinionex-
pression and the polar target polarity cannot be
labeled as reliably as the polarity of the opinion-
expression. 61% of the agreed upon polar targets
were labeled as negative by both annotators. On
the other hand, only 35% of the agreed upon opin-
ionexpressions were labeled as negative by both
annotators. There were no neutral instances. This
indicates that reviewers tend to report negative ex-
periences using polar facts, probably objectively
describing what has happened, but report posi-
tive experiences with explicit opinion expressions.
Distribution of the strength attribute was as fol-
lows: weak 6%, average 54%, and strong 40%.
The majority of the modifiers were annotated as
intensifiers (70%), while 20% of the modifiers
were labeled as negation.
4.4 Discussion
We analyzed the discrepancies in the annotations
to gain insights about the challenges involved in
various opinion related labeling tasks. At the sen-
tence level, there were several trivial cases of dis-
agreement, for instance, failing to recognize topic
relevancy when the topic was not mentioned or
referenced explicitly in the sentence, as in (18).
Occasionally, annotators disagreed about whether
a sentence that was written as a reaction to the
other reviewers, as in (19), should be considered
as topic relevant or not. Another source of dis-
agreement included sentences similar to (20) and
(21). One annotator interpreted them as univer-
sally true statements regardless of the topic, while
the other attributed them to the discussed topic.
(18) Go to a state university if you know whats good for you!
(19) Those with sour grapes couldnt cut it, have an ax to
grind, and are devoting their time to smearing the school.
(20) As far as learning, you really have to WANT to learn
the material.
(21) On an aside, this type of education is not for the
undisciplined learner.
Annotators easily distinguished the evaluations
at the sentence level. However, they had diffi-
culties distinguishing between a polar fact and an
opinion. For instance, both annotators agreed that
the sentences (22) and (23) contain evaluations re-
garding the topic of the review. However, one an-
notator interpreted both sentences as objectively
verifiable facts giving a positive impression about
the school, while the other one treated them as
opinions.
(22) All this work in the first 2 Years!
(23) The school has a reputation for making students work
really hard.
Sentence level annotation increases the relia-
bility of the expression level annotation in terms
of marking text spans. However, annotators of-
ten had disagreements on labeling the strength at-
tribute. For instance, one annotator labeled the
582
opinion expression in (24) as strong, while the
other one labeled it as average. We observe that
it is not easy to identify trivial causes of disagree-
ments regarding strength as its perception by each
individual is highly subjective. However, most of
the disagreements occurred between weak and av-
erage cases.
(24) the experience that i have when i visit student finance is
much like going to the dentist, except when i leave, nothing
is ever fixed.
We did not apply any consolidation steps during
our agreement studies. However, a final version of
the corpus will be produced by the third judge (one
of the co-authors) by consolidating the judgements
of the two annotators.
5 Conclusions
We presented a corpus of consumer reviews from
the rateitall and eopinions websites annotated
with opinion related information. Existing opin-
ion annotated user-generated corpora suffer from
several limitations which result in difficulties for
interpreting the experimental results and for per-
forming error analysis. To name a few, they do
not explicitly link the functional components of
the opinions like targets, holders, or modifiers with
the opinion expression; some of them do not mark
opinion expression spans, none of them resolves
anaphoric references in discourse. Therefore, we
introduced a two level annotation scheme consist-
ing of the sentence and expression levels, which
overcomes the limitations of the existing review
corpora. The sentence level annotation labels sen-
tences for (i) relevancy to a given topic, and (ii)
expressing an evaluation about the topic. Similar
to (Wilson, 2008a), our annotation scheme allows
capturing evaluations made with factual (objec-
tive) sentences. The expression level annotation
further investigates on-topic sentences containing
evaluations for pinpointing the properties (polar-
ity, strength), and marking the functional com-
ponents of the evaluations (opinion terms, modi-
fiers, targets and holders), and linking them within
a discourse. We applied the annotation scheme
to the consumer review genre and presented an
extensive inter-annotator study providing insights
to the challenges involved in various opinion re-
lated labeling tasks in consumer reviews. Simi-
lar to the MPQA scheme, which is successfully
applied to the newspaper genre, the annotation
scheme treats opinions and evaluations as a com-
position of functional components and it is eas-
ily extendable. Therefore, we hypothesize that the
scheme can also be applied to other genres with
minor extensions or as it is. Finally, the corpus
and the annotation manual will be made available
at http://www.ukp.tu-darmstadt.de/
research/data/sentiment-analysis.
Acknowledgements
This research was funded partially by the German Fed-
eral Ministry of Economy and Technology under grant
01MQ07012 and partially by the German Research Founda-
tion (DFG) as part of the Research Training Group on Feed-
back Based Quality Management in eLearning under grant
1223. We are very grateful to Sandra Ku?bler for her help in
organizing the annotators, and to Lizhen Qu for his program-
ming support in harvesting the data.
References
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. In Coling 2008: Companion vol-
ume: Posters, pages 7?10, Manchester, UK.
Eric Breck, Yejin Choi, and Claire Cardie. 2007.
Identifying expressions of opinion in context. In
Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence (IJCAI-2007),
pages 2683?2688, Hyderabad, India.
Xiwen Cheng and Feiyu Xu. 2008. Fine-grained opin-
ion topic and polarity identification. In Proceedings
of the 6th International Conference on Language
Resources and Evaluation, pages 2710?2714, Mar-
rekech, Morocco.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
355?362, Morristown, NJ, USA.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Web Data Mining, WSDM 2008,
pages 231?240, Palo Alto, California, USA.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation, pages 417?422, Genova, Italy.
583
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specific sentiment analysis of
adjectives. In Proceedings of the Symposium on
Affective Language in Human and Machine, AISB
2008 Convention, pages 60 ? 63, Aberdeen, Scot-
land.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD?04: Proceed-
ings of the Tenth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 168?177, Seattle, Washington.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text at the
joint COLING-ACL Conference, pages 1?8, Sydney,
Australia.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methology. Sage Publications,
Thousand Oaks, California.
Rebecca J. Passonneau. 2004. Computing reliability
for coreference. In Proceedings of LREC, volume 4,
pages 1503?1506, Lisbon.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New York.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In EMNLP-
03: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
105?112.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2008. Discourse level opinion relations:
An annotation study. In In Proceedings of SIGdial
Workshop on Discourse and Dialogue, pages 129?
137, Columbus, Ohio.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
817?824, Manchester, UK.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39:165?210.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53?60, Ann Arbor, Michigan.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 347?354, Vancouver, British
Columbia, Canada.
Theresa Wilson. 2008a. Annotating subjective con-
tent in meetings. In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco.
Theresa Ann Wilson. 2008b. Fine-grained Subjectiv-
ity and Sentiment Analysis: Recognizing the Inten-
sity, Polarity, and Attitudes of Private States. Ph.D.
thesis, University of Pittsburgh.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In CIKM
?06: Proceedings of the 15th ACM international
conference on Information and knowledge manage-
ment, pages 43?50, Arlington, Virginia, USA.
584
Proceedings of the ACL 2010 Conference Short Papers, pages 263?268,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Anaphora Resolution to Improve
Opinion Target Identification in Movie Reviews
Niklas Jakob
Technische Universita?t Darmstadt
Hochschulstra?e 10, 64289 Darmstadt
Iryna Gurevych
Technische Universita?t Darmstadt
Hochschulstra?e 10, 64289 Darmstadt
http://www.ukp.tu-darmstadt.de/people
Abstract
Current work on automatic opinion min-
ing has ignored opinion targets expressed
by anaphorical pronouns, thereby missing
a significant number of opinion targets. In
this paper we empirically evaluate whether
using an off-the-shelf anaphora resolution
algorithm can improve the performance of
a baseline opinion mining system. We
present an analysis based on two different
anaphora resolution systems. Our exper-
iments on a movie review corpus demon-
strate, that an unsupervised anaphora reso-
lution algorithm significantly improves the
opinion target extraction. We furthermore
suggest domain and task specific exten-
sions to an off-the-shelf algorithm which
in turn yield significant improvements.
1 Introduction
Over the last years the task of opinion mining
(OM) has been the topic of many publications.
It has been approached with different goals in
mind: Some research strived to perform subjec-
tivity analysis at the document or sentence level,
without focusing on what the individual opinions
uttered in the document are about. Other ap-
proaches focused on extracting individual opinion
words or phrases and what they are about. This
aboutness has been referred to as the opinion tar-
get or opinion topic in the literature from the field.
In this work our goal is to extract opinion target
- opinion word pairs from sentences from movie
reviews. A challenge which is frequently encoun-
tered in text mining tasks at this level of gran-
ularity is, that entities are being referred to by
anaphora. In the task of OM, it can therefore also
be necessary to analyze more than the content of
one individual sentence when extracting opinion
targets. Consider this example sentence: ?Simply
put, it?s unfathomable that this movie cracks the
Top 250. It is absolutely awful.?. If one wants to
extract what the opinion in the second sentence is
about, an algorithm which resolves the anaphoric
reference to the opinion target is required.
The extraction of such anaphoric opinion targets
has been noted as an open issue multiple times
in the OM context (Zhuang et al, 2006; Hu and
Liu, 2004; Nasukawa and Yi, 2003). It is not a
marginal phenomenon, since Kessler and Nicolov
(2009) report that in their data, 14% of the opin-
ion targets are pronouns. However, the task of re-
solving anaphora to mine opinion targets has not
been addressed and evaluated yet to the best of our
knowledge.
In this work, we investigate whether anaphora res-
olution (AR) can be successfully integrated into
an OM algorithm and whether we can achieve an
improvement regarding the OM in doing so. This
paper is structured as follows: Section 2 discusses
the related work on opinion target identification
and OM on movie reviews. Section 3 outlines the
OM algorithm we employed by us, while in Sec-
tion 4 we discuss two different algorithms for AR
which we experiment with. Finally, in Section 5
we present our experimental work including error
analysis and discussion, and we conclude in Sec-
tion 6.
2 Related Work
We split the description of the related work in two
parts: In Section 2.1 we discuss the related work
on OM with a focus on approaches for opinion
target identification. In Section 2.2 we elaborate
on findings from related OM research which also
worked with movie reviews as this is our target
domain in the present paper.
2.1 Opinion Target Identification
The extraction of opinions and especially opin-
ion targets has been performed with quite diverse
263
approaches. Initial approaches combined statisti-
cal information and basic linguistic features such
as part-of-speech tags. The goal was to identify
the opinion targets, here in form of products and
their attributes, without a pre-built knowledge base
which models the domain. For the target candidate
identification, simple part-of-speech patterns were
employed. The relevance ranking and extraction
was then performed with different statistical mea-
sures: Pointwise Mutual Information (Popescu
and Etzioni, 2005), the Likelihood Ratio Test (Yi
et al, 2003) and Association Mining (Hu and Liu,
2004). A more linguistically motivated approach
was taken by Kim and Hovy (2006) through iden-
tifying opinion holders and targets with semantic
role labeling. This approach was promising, since
their goal was to extract opinions from profession-
ally edited content i.e. newswire.
Zhuang et al (2006) present an algorithm for the
extraction of opinion target - opinion word pairs.
The opinion word and target candidates are iden-
tified in the annotated corpus and their extraction
is then performed by applying possible paths con-
necting them in a dependency graph. These paths
are combined with part-of-speech information and
also learned from the annotated corpus.
To the best of our knowledge, there is currently
only one system which integrates coreference in-
formation in OM. The algorithm by Stoyanov
and Cardie (2008) identifies coreferring targets in
newspaper articles. A candidate selection or ex-
traction step for the opinion targets is not required,
since they rely on manually annotated targets and
focus solely on the coreference resolution. How-
ever they do not resolve pronominal anaphora in
order to achieve that.
2.2 Opinion Mining on Movie Reviews
There is a huge body of work on OM in movie re-
views which was sparked by the dataset from Pang
and Lee (2005). This dataset consists of sen-
tences which are annotated as expressing positive
or negative opinions. An interesting insight was
gained from the document level sentiment analy-
sis on movie reviews in comparison to documents
from other domains: Turney (2002) observes that
the movie reviews are hardest to classify since the
review authors tend to give information about the
storyline of the movie which often contain charac-
terizations, such as ?bad guy? or ?violent scene?.
These statements however do not reflect any opin-
ions of the reviewers regarding the movie. Zhuang
et al (2006) also observe that movie reviews are
different from e.g. customer reviews on Ama-
zon.com. This is reflected in their experiments, in
which their system outperforms the system by Hu
and Liu (2004) which attributes an opinion tar-
get to the opinion word which is closest regard-
ing word distance in a sentence. The sentences in
the movie reviews tend to be more complex, which
can also be explained by their origin. The reviews
were taken from the Internet Movie Database1,
on which the users are given a set of guidelines
on how to write a review. Due to these insights,
we are confident that the overall textual quality
of the movie reviews is high enough for linguisti-
cally more advanced technologies such as parsing
or AR to be successfully applied.
3 Opinion Target Identification
3.1 Dataset
Currently the only freely available dataset anno-
tated with opinions including annotated anaphoric
opinion targets is a corpus of movie reviews
by Zhuang et al (2006). Kessler and Nicolov
(2009) describe a collection of product reviews
in which anaphoric opinion targets are also an-
notated, but it is not available to the public
(yet). Zhuang et al (2006) used a subset of the
dataset they published (1829 documents), namely
1100 documents, however they do not state which
documents comprise this subset used in their eval-
uation. In our experiments, we therefore use the
complete dataset available, detailed in Table 1. As
shown, roughly 9.5% of the opinion targets are re-
ferred to by pronouns. Table 2 outlines detailed
statistics on which pronouns occur as opinion tar-
gets.
Table 1: Dataset Statistics
# Documents 1829
# Sentences 24918
# Tokens 273715
# Target + Opinion Pairs 5298
# Targets which are Pronouns 504
# Pronouns > 11000
3.2 Baseline Opinion Mining
We reimplemented the algorithm presented
by Zhuang et al (2006) as the baseline for our
1http://www.imdb.com (IMDB)
264
Table 2: Pronouns as Opinion Targets
it 274 he 58 she 22 they 22
this 77 his 26 her 10
him 15
experiments. Their approach is a supervised one.
The annotated dataset is split in five folds, of
which four are used as the training data. In the first
step, opinion target and opinion word candidates
are extracted from the training data. Frequency
counts of the annotated opinion targets and opin-
ion words are extracted from four training folds.
The most frequently occurring opinion targets and
opinion words are selected as candidates. Then
the annotated sentences are parsed and a graph
containing the words of the sentence is created,
which are connected by the dependency relations
between them. For each opinion target - opinion
word pair, the shortest path connecting them is
extracted from the dependency graph. A path
consists of the part-of-speech tags of the nodes
and the dependency types of the edges.
In order to be able to identify rarely occurring
opinion targets which are not in the candidate
list, they expand it by crawling the cast and crew
names of the movies from the IMDB. How this
crawling and extraction is done is not explained.
4 Algorithms for Anaphora Resolution
As pointed out by Charniak and Elsner (2009)
there are hardly any freely available systems
for AR. Although Charniak and Elsner (2009)
present a machine-learning based algorithm for
AR, they evaluate its performance in comparison
to three non machine-learning based algorithms,
since those are the only ones available. They
observe that the best performing baseline algo-
rithm (OpenNLP) is hardly documented. The al-
gorithm with the next-to-highest results in (Char-
niak and Elsner, 2009) is MARS (Mitkov, 1998)
from the GuiTAR (Poesio and Kabadjov, 2004)
toolkit. This algorithm is based on statistical anal-
ysis of the antecedent candidates. Another promis-
ing algorithm for AR employs a rule based ap-
proach for antecedent identification. The Cog-
NIAC algorithm (Baldwin, 1997) was designed
for high-precision AR. This approach seems like
an adequate strategy for our OM task, since in
the dataset used in our experiments only a small
fraction of the total number of pronouns are ac-
tual opinion targets (see Table 1). We extended the
CogNIAC implementation to also resolve ?it? and
?this? as anaphora candidates, since off-the-shelf
it only resolves personal pronouns. We will refer
to this extension with [id]. Both algorithms fol-
low the common approach that noun phrases are
antecedent candidates for the anaphora. In our ex-
periments we employed both the MARS and the
CogNIAC algorithm, for which we created three
extensions which are detailed in the following.
4.1 Extensions of CogNIAC
We identified a few typical sources of errors in
a preliminary error analysis. We therefore sug-
gest three extensions to the algorithm which are
on the one hand possible in the OM setting and
on the other hand represent special features of the
target discourse type: [1.] We observed that the
Stanford Named Entity Recognizer (Finkel et al,
2005) is superior to the Person detection of the
(MUC6 trained) CogNIAC implementation. We
therefore filter out Person antecedent candidates
which the Stanford NER detects for the imper-
sonal and demonstrative pronouns and Location
& Organization candidates for the personal pro-
nouns. This way the input to the AR is optimized.
[2.] The second extension exploits the fact that re-
views from the IMDB exhibit certain contextual
properties. They are gathered and to be presented
in the context of one particular entity (=movie).
The context or topic under which it occurs is there-
fore typically clear to the reader and is therefore
not explicitly introduced in the discourse. This is
equivalent to the situational context we often refer
to in dialogue. In the reviews, the authors often
refer to the movie or film as a whole by a pro-
noun. We exploit this by an additional rule which
resolves an impersonal or demonstrative pronoun
to ?movie? or ?film? if there is no other (match-
ing) antecedent candidate in the previous two sen-
tences. [3.] The rules by which CogNIAC resolves
anaphora were designed so that anaphora which
have ambiguous antecedents are left unresolved.
This strategy should lead to a high precision AR,
but at the same time it can have a negative impact
on the recall. In the OM context, it happens quite
frequently that the authors comment on the entity
they want to criticize in a series of arguments. In
such argument chains, we try to solve cases of an-
tecedent ambiguity by analyzing the opinions: If
there are ambiguous antecedent candidates for a
265
pronoun, we check whether there is an opinion ut-
tered in the previous sentence. If this is the case
and if the opinion target matches the pronoun re-
garding gender and number, we resolve the pro-
noun to the antecedent which was the previous
opinion target.
In the results of our experiments in Section 5, we
will refer to the configurations using these exten-
sions with the numbers attributed to them above.
5 Experimental Work
To integrate AR in the OM algorithm, we add the
antecedents of the pronouns annotated as opinion
targets to the target candidate list. Then we ex-
tract the dependency paths connecting pronouns
and opinion words and add them to the list of valid
paths. When we run the algorithm, we extract
anaphora which were resolved, if they occur with
a valid dependency path to an opinion word. In
such a case, the anaphor is substituted for its an-
tecedent and thus extracted as part of an opinion
target - opinion word pair.
To reproduce the system by Zhuang et al (2006),
we substitute the cast and crew list employed
by them (see Section 3.2), with a NER compo-
nent (Finkel et al, 2005). One aspect regarding the
extraction of opinion target - opinion word pairs
remains open in Zhuang et al (2006): The de-
pendency paths only identify connections between
pairs of single words. However, almost 50% of
the opinion target candidates are multiword ex-
pressions. Zhuang et al (2006) do not explain how
they extract multiword opinion targets with the de-
pendency paths. In our experiments, we require a
dependency path to be found to each word of a
multiword target candidate for it to be extracted.
Furthermore, Zhuang et al (2006) do not state
whether in their evaluation annotated multiword
targets are treated as a single unit which needs to
be extracted, or whether a partial matching is em-
ployed in such cases. We require all individual
words of a multiword expression to be extracted
by the algorithm. As mentioned above, the depen-
dency path based approach will only identify con-
nections between pairs of single words. We there-
fore employ a merging step, in which we combine
adjacent opinion targets to a multiword expres-
sion. We have compiled two result sets: Table 3
shows the results of the overall OM in a five-fold
cross-validation. Table 4 gives a detailed overview
of the AR for opinion target identification summed
up over all folds. In Table 4, a true positive refers
to an extracted pronoun which was annotated as
an opinion target and is resolved to the correct
antecedent. A false positive subsumes two error
classes: A pronoun which was not annotated as an
opinion target but extracted as such, or a pronoun
which is resolved to an incorrect antecedent.
As shown in Table 3, the recall of our reimplemen-
tation is slightly higher than the recall reported
in Zhuang et al (2006). However, our precision
and thus f-measure are lower. This can be at-
tributed to the different document sets used in our
experiments (see Section 3.1), or our substitution
of the list of peoples? names with the NER compo-
nent, or differences regarding the evaluation strat-
egy as mentioned above.
We observe that the MARS algorithm yields an
improvement regarding recall compared to the
baseline system. However, it also extracts a high
number of false positives for both the personal and
impersonal / demonstrative pronouns. This is due
to the fact that the MARS algorithm is designed
for robustness and always resolves a pronoun to
an antecedent.
CogNIAC in its off-the-shelf configuration already
yields significant improvements over the baseline
regarding f-measure2. Our CogNIAC extension
[id] improves recall slightly in comparison to the
off-the-shelf system. As shown in Table 4, the
algorithm extracts impersonal and demonstrative
pronouns with lower precision than personal pro-
nouns. Our error analysis shows that this is mostly
due to the Person / Location / Organization clas-
sification of the CogNIAC implementation. The
names of actors and movies are thus often misclas-
sified. Extension [1] mitigates this problem, since
it increases precision (Table 3 row 6), while not af-
fecting recall. The overall improvement of our ex-
tensions [id] + [1] is however not statistically sig-
nificant in comparison to off-the-shelf CogNIAC.
Our extensions [2] and [3] in combination with
[id] each increase recall at the expense of preci-
sion. The improvement in f-measure of CogNIAC
[id] + [3] over the off-the-shelf system is statisti-
cally significant. The best overall results regard-
ing f-measure are reached if we combine all our
extensions of the CogNIAC algorithm. The re-
sults of this configuration show that the positive
effects of extensions [2] and [3] are complemen-
2Significance of improvements was tested using a paired
two-tailed t-test and p ? 0.05 (?) and p ? 0.01 (??)
266
Table 3: Op. Target - Op. Word Pair Extraction
Configuration Reca. Prec. F-Meas.
Results in Zhuang et al 0.548 0.654 0.596
Our Reimplementation 0.554 0.523 0.538
MARS off-the-shelf 0.595 0.467 0.523
CogNIAC off-the-shelf 0.586 0.534 0.559??
CogNIAC+[id] 0.594 0.516 0.552
CogNIAC+[id]+[1] 0.594 0.533 0.561
CogNIAC+[id]+[2] 0.603 0.501 0.547
CogNIAC+[id]+[3] 0.613 0.521 0.563?
CogNIAC+[id]+[1]+[2]+[3] 0.614 0.531 0.569?
Table 4: Results of AR for Opinion Targets
Algorithm Pers.
1 Imp. & Dem.1
TP2 FP2 TP FP
MARS off-the-shelf 102 164 115 623
CogNIAC off-the-shelf 117 95 0 0
CogNIAC+[id] 117 95 105 180
CogNIAC+[id]+[1] 117 41 105 51
CogNIAC+[id]+[2] 117 95 153 410
CogNIAC+[id]+[3] 131 103 182 206
CogNIAC+[id]+[1]+[2]+[3] 124 64 194 132
1 personal, impersonal & demonstrative pronouns
2 true positives, false positives
tary regarding the extraction of impersonal and
demonstrative pronouns. This configuration yields
statistically significant improvements regarding f-
measure over the off-the-shelf CogNIAC configu-
ration, while also having the overall highest recall.
5.1 Error Analysis
When extracting opinions from movie reviews, we
observe the same challenge as Turney (2002): The
users often characterize events in the storyline or
roles the characters play. These characterizations
contain the same words which are also used to
express opinions. Hence these combinations are
frequently but falsely extracted as opinion target
- opinion word pairs, negatively affecting the
precision. The algorithm cannot distinguish them
from opinions expressing the stance of the author.
Overall, the recall of the baseline is rather low.
This is due to the fact that the algorithm only
learns a subset of the opinion words and opinion
targets annotated in the training data. Currently,
it cannot discover any new opinion words and
targets. This could be addressed by integrating a
component which identifies new opinion targets
by calculating the relevance of a word in the
corpus based on statistical measures.
The AR introduces new sources of errors regard-
ing the extraction of opinion targets: Errors in
gender and number identification can lead to an
incorrect selection of antecedent candidates. Even
if the gender and number identification is correct,
the algorithm might select an incorrect antecedent
if there is more than one possible candidate. A
non-robust algorithm as CogNIAC might leave
a pronoun which is an actual opinion target
unresolved, due to the ambiguity of its antecedent
candidates.
The upper bound for the OM with perfect AR
on top of the baseline would be recall: 0.649,
precision: 0.562, f-measure: 0.602. Our best
configuration reaches? 50% of the improvements
which are theoretically possible with perfect AR.
6 Conclusions
We have shown that by extending an OM al-
gorithm with AR for opinion target extraction
significant improvements can be achieved. The
rule based AR algorithm CogNIAC performs well
regarding the extraction of opinion targets which
are personal pronouns. The algorithm does not
yield high precision when resolving impersonal
and demonstrative pronouns. We present a set
of extensions which address this challenge and
in combination yield significant improvements
over the off-the-shelf configuration. A robust
AR algorithm does not yield any improvements
regarding f-measure in the OM task. This type of
algorithm creates many false positives, which are
not filtered out by the dependency paths employed
in the algorithm by Zhuang et al (2006).
AR could also be employed in other OM algo-
rithms which aim at identifying opinion targets
by means of a statistical analysis. Vicedo and
Ferra?ndez (2000) successfully modified the
relevance ranking of terms in their documents by
replacing anaphora with their antecedents. The
approach can be taken for OM algorithms which
select the opinion target candidates with a rel-
evance ranking (Hu and Liu, 2004; Yi et al, 2003).
Acknowledgments
The project was funded by means of the German Federal
Ministry of Economy and Technology under the promotional
reference ?01MQ07012?. The authors take the responsibility
for the contents. This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806.
267
References
Breck Baldwin. 1997. Cogniac: High precision coref-
erence with limited knowledge and linguistic re-
sources. In Proceedings of a Workshop on Opera-
tional Factors in Practical, Robust Anaphora Reso-
lution for Unrestricted Texts, pages 38?45, Madrid,
Spain, July.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL, pages 148?156, Athens, Greece, March.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370, Michigan, USA, June.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177,
Seattle, WA, USA, August.
Jason Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking
of linguistic configurations. In Proceedings of the
Third International AAAI Conference on Weblogs
and Social Media, San Jose, CA, USA, May.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Sydney, Australia, July.
Ruslan Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics and 17th International Conference on
Computational Linguistics, pages 869?875, Mon-
treal, Canada, August.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: Capturing favorability using natural lan-
guage processing. In Proceedings of the 2nd Inter-
national Conference on Knowledge Capture, pages
70?77, Sanibel Island, FL, USA, October.
Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 115?124, Michi-
gan, USA, June.
Massimo Poesio and Mijail A. Kabadjov. 2004. A
general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evalua-
tion. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation, pages
663?666, Lisboa, Portugal, May.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 339?346, Van-
couver, Canada, October.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 817?824, Manch-
ester, UK, August.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 417?424, Philadelphia, Pennsyl-
vania, USA, July.
Jose? L. Vicedo and Antonio Ferra?ndez. 2000. Apply-
ing anaphora resolution to question answering and
information retrieval systems. In Proceedings of the
First International Conference on Web-Age Informa-
tion Management, volume 1846 of Lecture Notes In
Computer Science, pages 344?355. Springer, Shang-
hai, China.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
3rd IEEE International Conference on Data Mining,
pages 427?434, Melbourne, FL, USA, December.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the ACM 15th Conference on Informa-
tion and Knowledge Management, pages 43?50, Ar-
lington, VA, USA, November.
268
