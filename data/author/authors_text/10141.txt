Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385?392
Manchester, August 2008
Word Lattice Reranking for Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? ? Haitao Mi ? ? Qun Liu ?
?Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
?Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{jiangwenbin,htmi,liuqun}@ict.ac.cn
Abstract
In this paper, we describe a new rerank-
ing strategy named word lattice reranking,
for the task of joint Chinese word segmen-
tation and part-of-speech (POS) tagging.
As a derivation of the forest reranking
for parsing (Huang, 2008), this strategy
reranks on the pruned word lattice, which
potentially contains much more candidates
while using less storage, compared with
the traditional n-best list reranking. With a
perceptron classifier trained with local fea-
tures as the baseline, word lattice rerank-
ing performs reranking with non-local fea-
tures that can?t be easily incorporated into
the perceptron baseline. Experimental re-
sults show that, this strategy achieves im-
provement on both segmentation and POS
tagging, above the perceptron baseline and
the n-best list reranking.
1 Introduction
Recent work for Chinese word segmentation and
POS tagging pays much attention to discriminative
methods, such as Maximum Entropy Model (ME)
(Ratnaparkhi and Adwait, 1996), Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001), percep-
tron training algorithm (Collins, 2002), etc. Com-
pared to generative ones such as Hidden Markov
Model (HMM) (Rabiner, 1989; Fine et al, 1998),
discriminative models have the advantage of flexi-
bility in representing features, and usually obtains
almost perfect accuracy in two tasks.
Originated by Xue and Shen (2003), the typ-
ical approach of discriminative models conducts
c
? 2008. Licensed to the Coling 2008 Organizing Com-
mittee for publication in Coling 2008 and for re-publishing in
any form or medium.
segmentation in a classification style, by assign-
ing each character a positional tag indicating its
relative position in the word. If we extend these
positional tags to include POS information, seg-
mentation and POS tagging can be performed by a
single pass under a unify classification framework
(Ng and Low, 2004). In the rest of the paper, we
call this operation mode Joint S&T. Experiments
of Ng and Low (2004) shown that, compared with
performing segmentation and POS tagging one at
a time, Joint S&T can achieve higher accuracy not
only on segmentation but also on POS tagging.
Besides the usual local features such as the
character-based ones (Xue and Shen, 2003; Ng
and Low, 2004), many non-local features related
to POSs or words can also be employed to improve
performance. However, as such features are gener-
ated dynamically during the decoding procedure,
incorporating these features directly into the clas-
sifier results in problems. First, the classifier?s fea-
ture space will grow much rapidly, which is apt to
overfit on training corpus. Second, the variance of
non-local features caused by the model evolution
during the training procedure will hurt the param-
eter tuning. Last but not the lest, since the cur-
rent predication relies on the results of prior predi-
cations, exact inference by dynamic programming
can?t be obtained, and then we have to maintain a
n-best candidate list at each considering position,
which also evokes the potential risk of depress-
ing the parameter tuning procedure. As a result,
many theoretically useful features such as higher-
order word- or POS- grams can not be utilized ef-
ficiently.
A widely used approach of using non-local
features is the well-known reranking technique,
which has been proved effective in many NLP
tasks, for instance, syntactic parsing and machine
385
v0
v
1
v
2
v
3
v
4
v
5
v
6
v
7
C
1
:e
C
2
:? C
3
:U
C
4
:/ C
5
:? C
6
:?
C
7
:Y
NN
VV
NN
M
NN
NN
NN
NN
VV
NN
NN
Figure 1: Pruned word lattice as directed graph. The character sequence we choose is ?e-?-U-/-
?-?-Y?. For clarity, we represent each subsequence-POS pair as a single edge, while ignore the
corresponding scores of the edges.
translation (Collins, 2000; Huang, 2008), etc. Es-
pecially, Huang (2008) reranked the packed for-
est, which contains exponentially many parses.
Inspired by his work, we propose word lattice
reranking, a strategy that reranks the pruned word
lattice outputted by a baseline classifier, rather than
only a n-best list. Word lattice, a directed graph as
shown in Figure 1, is a packed structure that can
represent many possibilities of segmentation and
POS tagging. Our experiments on the Penn Chi-
nese Treebank 5.0 show that, reranking on word
lattice gains obvious improvement over the base-
line classifier and the reranking on n-best list.
Compared against the baseline, we obtain an error
reduction of 11.9% on segmentation, and 16.3%
on Joint S&T.
2 Word Lattice
Formally, a word lattice L is a directed graph
?V,E?, where V is the node set, and E is the
edge set. Suppose the word lattice is for sentence
C
1:n
= C
1
..C
n
, node v
i
? V (i = 1..n ? 1) de-
notes the position between C
i
and C
i+1
, while v
0
before C
1
is the source node, and v
n
after C
n
is
the sink node. An edge e ? E departs from v
b
and
arrives at v
e
(0 ? b < e ? n), it covers a subse-
quence of C
1:n
, which is recognized as a possible
word. Considering Joint S&T, we label each edge
a POS tag to represent a word-POS pair. A series
of adjoining edges forms a path, and a path con-
necting the source node and the sink node is called
diameter, which indicates a specific pattern of seg-
mentation and POS tagging. For a diameter d, |d|
denotes the length of d, which is the count of edges
contained in this diameter. In Figure 1, the path
p
?
= v?
0
v
3
? v?
3
v
5
? v?
5
v
7
is a diameter, and
|p
?
| is 3.
2.1 Oracle Diameter in Lattice
Given a sentence s, its reference r and pruned
word lattice L generated by the baseline classi-
fier, the oracle diameter d? of L is define as the
diameter most similar to r. With F-measure as the
scoring function, we can identify d? using the al-
gorithm depicted in Algorithm 1, which is adapted
to lexical analysis from the forest oracle computa-
tion of Huang (2008).
Before describe this algorithm in detail, we de-
pict the key point for finding the oracle diameter.
Given the system?s output y and the reference y?,
using |y| and |y?| to denote word counts of them
respectively, and |y ? y?| to denote matched word
count of |y| and |y?|, F-measure can be computed
by:
F (y, y
?
) =
2PR
P + R
=
2|y ? y
?
|
|y| + |y
?
|
(1)
Here, P = |y?y
?
|
|y|
is precision, and R = |y?y
?
|
|y
?
|
is recall. Notice that F (y, y?) isn?t a linear func-
tion, we need access the largest |y ? y?| for each
possible |y| in order to determine the diameter with
maximum F , or another word, we should know the
maximum matched word count for each possible
diameter length.
The algorithm shown in Algorithm 1 works in
a dynamic programming manner. A table node
T [i, j] is defined for sequence span [i, j], and it has
a structure S to remember the best |y
i:j
? y
?
i:j
| for
each |y
i:j
|, as well as the back pointer for this best
choice. The for-loop in line 2 ? 14 processes for
each node T [i, j] in a shorter-span-first order. Line
3? 7 initialize T [i, j] according to the reference r
and the word lattice?s edge set L ?E. If there exists
an edge e in L ?E covering the span [i, j], then we
386
Algorithm 1 Oracle Diameter, U la Huang (2008,
Sec. 4.1).
1: Input: sentence s, reference r and lattice L
2: for [i, j] ? [1, |s|] in topological order do
3: if ?e ? L ? E s.t. e spans from i to j then
4: if e ? label exists in r then
5: T [i, j] ? S[1]? 1
6: else
7: T [i, j] ? S[1]? 0
8: for k s.t. T [i, k ? 1] and T [k, j] defined do
9: for p s.t. T [i, k ? 1] ? S[p] defined do
10: for q s.t. T [k, j] ? S[q] defined do
11: n? T [i, k ? 1] ? S[p] + T [k, j] ? S[q]
12: if n > T [i, j] ? S[p + q] then
13: T [i, j] ? S[p + q]? n
14: T [i, j] ? S[p + q] ? bp? ?k, p, q?
15: t? argmax
t
2?T [1,|s|]?S[t]
t+|r|
16: d? ? Tr(T [1, |s|] ? S[t].bp)
17: Output: oracle diameter: d?
define T [i, j], otherwise we leave this node unde-
fined. In the first situation, we initialize this node?s
S structure according to whether the word-POS
pair of e is in the reference (line 4?7). Line 8?14
update T [i, j]?s S structure using the S structures
from all possible child-node pair, T [i, k ? 1] and
T [k, j]. Especially, line 9? 10 enumerate all com-
binations of p and q, where p and q each repre-
sent a kind of diameter length in T [i, k ? 1] and
T [k, j]. Line 12 ? 14 refreshes the structure S
of node T [i, j] when necessary, and meanwhile,
a back pointer ?k, p, q? is also recorded. When
the dynamic programming procedure ends, we se-
lect the diameter length t of the top node T [1, |s|],
which maximizes the F-measure formula in line
15, then we use function Tr to find the oracle di-
ameter d? by tracing the back pointer bp.
2.2 Generation of the Word Lattice
We can generate the pruned word lattice using the
baseline classifier, with a slight modification. The
classifier conducts decoding by considering each
character in a left-to-right fashion. At each consid-
ering position i, the classifier enumerates all can-
didate results for subsequence C
1:i
, by attaching
each current candidate word-POS pair p to the tail
of each candidate result at p?s prior position, as
the endmost of the new generated candidate. We
give each p a score, which is the highest, among
all C
1:i
?s candidates that have p as their endmost.
Then we select N word-POS pairs with the high-
est scores, and insert them to the lattice?s edge set.
This approach of selecting edges implies that, for
the lattice?s node set, we generate a node v
i
at each
position i. Because N is the limitation on the count
Algorithm 2 Lattice generation algorithm.
1: Input: character sequence C
1:n
2: E ? ?
3: for i? 1 .. n do
4: cands? ?
5: for l? 1 .. min(i, K) do
6: w ? C
i?l+1:i
7: for t ? POS do
8: p? ?w, t?
9: p ? score? Eval(p)
10: s? p ? score + Best[i? l]
11: Best[i]? max(s,Best[i])
12: insert ?s, p? into cands
13: sort cands according to s
14: E ? E ? cands[1..N ] ? p
15: Output: edge set of lattice: E
of edges that point to the node at position i, we call
this pruning strategy in-degree pruning. The gen-
eration algorithm is shown in Algorithm 2.
Line 3 ? 14 consider each character C
i
in se-
quence, cands is used to keep the edges closing at
position i. Line 5 enumerates the candidate words
ending with C
i
and no longer than K, where K
is 20 in our experiments. Line 5 enumerates all
POS tags for the current candidate word w, where
POS denotes the POS tag set. Function Eval in
line 9 returns the score for word-POS pair p from
the baseline classifier. The array Best preserve the
score for sequence C
1:i
?s best labelling results. Af-
ter all possible word-POS pairs (or edges) consid-
ered, line 13? 14 select the N edges we want, and
add them to edge set E.
Though this pruning strategy seems relative
rough ? simple pruning for edge set while no
pruning for node set, we still achieve a promising
improvement by reranking on such lattices. We be-
lieve more elaborate pruning strategy will results
in more valuable pruned lattice.
3 Reranking
A unified framework can be applied to describing
reranking for both n-best list and pruned word lat-
tices (Collins, 2000; Huang, 2008). Given the can-
didate set cand(s) for sentence s, the reranker se-
lects the best item y? from cand(s):
y? = argmax
y?cand(s)
w ? f(y) (2)
For reranking n-best list, cand(s) is simply the set
of n best results from the baseline classifier. While
for reranking word lattice, cand(s) is the set of
all diameters that are impliedly built in the lattice.
w ? f(y) is the dot product between a feature vec-
tor f and a weight vector w, its value is used to
387
Algorithm 3 Perceptron training for reranking
1: Input: Training examples{cand(s
i
), y
?
i
}
N
i=1
2: w? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: y? ? argmax
y?cand(s
i
)
w ? f(y)
6: if y? 6= y?
i
then
7: w? w + f(y?
i
)? f(y?)
8: Output: Parameters: w
Non-local Template Comment
W
0
T
0
current word-POS pair
W
?1
word 1-gram before W
0
T
0
T
?1
POS 1-gram before W
0
T
0
T
?2
T
?1
POS 2-gram before W
0
T
0
T
?3
T
?2
T
?1
POS 3-gram before W
0
T
0
Table 1: Non-local feature templates used for
reranking
rerank cand(s). Following usual practice in pars-
ing, the first feature f
1
(y) is specified as the score
outputted by the baseline classifier, and its value
is a real number. The other features are non-local
ones such as word- and POS- n-grams extracted
from candidates in n-best list (for n-best rerank-
ing) or diameters (for word lattice reranking), and
they are 0 ? 1 valued.
3.1 Training of the Reranker
We adopt the perceptron algorithm (Collins, 2002)
to train the reranker. as shown in Algorithm 3. We
use a simple refinement strategy of ?averaged pa-
rameters? of Collins (2002) to alleviate overfitting
on the training corpus and obtain more stable per-
formance.
For every training example {cand(s
i
), y
?
i
}, y
?
i
denotes the best candidate in cand(s
i
). For n-
best reranking, the best candidate is easy to find,
whereas for word lattice reranking, we should use
the algorithm in Algorithm 1 to determine the or-
acle diameter, which represents the best candidate
result.
3.2 Non-local Feature Templates
The non-local feature templates we use to train the
reranker are listed in Table 1. Notice that all fea-
tures generated from these templates don?t contain
?future? words or POS tags, it means that we only
use current or history word- or POS- n-grams to
evaluate the current considering word-POS pair.
Although it is possible to use ?future? information
in n-best list reranking, it?s not the same when we
rerank the pruned word lattice. As we have to tra-
verse the lattice topologically, we face difficulty in
Algorithm 4 Cube pruning for non-local features.
1: function CUBE(L)
2: for v ? L ? V in topological order do
3: NBEST(v)
4: return D
v
sink
[1]
5: procedure NBEST(v)
6: heap? ?
7: for v? topologically before v do
8: ?? all edges from v? to v
9: p? ?D
v
?
,??
10: ?p,1??score? Eval(p,1)
11: PUSH(?p,1?, heap)
12: HEAPIFY(heap)
13: buf ? ?
14: while |heap| > 0 and |buf | < N do
15: item? POP-MAX(heap)
16: append item to buf
17: PUSHSUCC(item, heap)
18: sort buf to D
v
19: procedure PUSHSUCC(?p, j?, heap)
20: p is ?vec
1
,vec
2
?
21: for i? 1..2 do
22: j? ? j+ bi
23: if |vec
i
| ? j
?
i
then
24: ?p, j???score? Eval(p, j?)
25: PUSH(?p, j??, heap)
utilizing the information ahead of the current con-
sidering node.
3.3 Reranking by Cube Pruning
Because of the non-local features such as word-
and POS- n-grams, the reranking procedure is sim-
ilar to machine translation decoding with inter-
grated language models, and should maintain a
list of N best candidates at each node of the lat-
tice. To speed up the procedure of obtaining the
N best candidates, following Huang (2008, Sec.
3.3), we adapt the cube pruning method from ma-
chine translation (Chiang, 2007; Huang and Chi-
ang 2007) which is based on efficient k-best pars-
ing algorithms (Huang and Chiang, 2005).
As shown in Algorithm 4, cube pruning works
topologically in the pruned word lattice, and main-
tains a list of N best derivations at each node.
When deducing a new derivation by attaching a
current word-POS pair to the tail of a antecedent
derivation, a function Eval is used to compute the
new derivation?s score (line 10 and 24). We use
a max-heap heap to hold the candidates for the
next-best derivation. Line 7 ? 11 initialize heap
to the set of top derivations along each deducing
source, the vector pair ?D
v
head
,??.Here, ? de-
notes the vector of current word-POS pairs, while
D
v
head
denotes the vector of N best derivations
at ??s antecedent node. Then at each iteration,
388
Non-lexical-target Instances
C
n
(n = ?2..2) C
?2
=e, C
?1
=?, C
0
=U, C
1
=/, C
2
=?
C
n
C
n+1
(n = ?2..1) C
?2
C
?1
=e?, C
?1
C
0
=?U, C
0
C
1
=U/, C
1
C
2
=/?
C
?1
C
1
C
?1
C
1
=?/
Lexical-target Instances
C
0
C
n
(n = ?2..2) C
0
C
?2
=Ue, C
0
C
?1
=U?, C
0
C
0
=UU, C
0
C
1
=U/, C
0
C
2
=U?
C
0
C
n
C
n+1
(n = ?2..1) C
0
C
?2
C
?1
=Ue?, C
0
C
?1
C
0
=U?U, C
0
C
0
C
1
=UU/, C
0
C
1
C
2
=U/?
C
0
C
?1
C
1
C
0
C
?1
C
1
= U?/
Table 2: Feature templates and instances. Suppose we consider the third character ?U? in the sequence
?e?U/??.
we pop the best derivation from heap (line 15),
and push its successors into heap (line 17), until
we get N derivations or heap is empty. In line 22
of function PUSHSUCC, j is a vector composed of
two index numbers, indicating the two candidates?
indexes in the two vectors of the deducing source
p, where the two candidates are selected to deduce
a new derivation. j? is a increment vector, whose
ith dimension is 1, while others are 0. As non-
local features (word- and POS- n-grams) are used
by function Eval to compute derivation?s score,
the derivations extracted from heap may be out of
order. So we use a buffer buf to keep extracted
derivations (line 16), then sort buf and put its first
N items to D
v
(line 18).
4 Baseline Perceptron Classifier
4.1 Joint S&T as Classification
Following Jiang et al (2008), we describe segmen-
tation and Joint S&T as below:
For a given Chinese sentence appearing as a
character sequence:
C
1:n
= C
1
C
2
.. C
n
the goal of segmentation is splitting the sequence
into several subsequences:
C
1:e
1
C
e
1
+1:e
2
.. C
e
m?1
+1:e
m
While in Joint S&T, each of these subsequences is
labelled a POS tag:
C
1:e
1
/t
1
C
e
1
+1:e
2
/t
2
.. C
e
m?1
+1:e
m
/t
m
Where C
i
(i = 1..n) denotes a character, C
l:r
(l ?
r) denotes the subsequence ranging from C
l
to C
r
,
and t
i
(i = 1..m,m ? n) denotes the POS tag of
C
e
i?1
+1:e
i
.
If we label each character a positional tag in-
dicating its relative position in an expected subse-
quence, we can obtain the segmentation result ac-
cordingly. As described in Ng and Low (2004) and
Jiang et al (2008), we use s indicating a single-
character word, while b, m and e indicating the be-
gin, middle and end of a word respectively. With
these positional tags, the segmentation transforms
to a classification problem. For Joint S&T, we
expand positional tags by attaching POS to their
tails as postfix. As each tag now contains both
positional- and POS- information, Joint S&T can
also be resolved in a classification style frame-
work. It means that, a subsequence is a word with
POS t, only if the positional part of the tag se-
quence conforms to s or bm?e pattern, and each
element in the POS part equals to t. For example,
a tag sequence b NN m NN e NN represents a
three-character word with POS tag NN .
4.2 Feature Templates
The features we use to build the classifier are gen-
erated from the templates of Ng and Low (2004).
For convenience of comparing with other, they
didn?t adopt the ones containing external knowl-
edge, such as punctuation information. All their
templates are shown in Table 2. C denotes a char-
acter, while its subscript indicates its position rela-
tive to the current considering character(it has the
subscript 0).
The table?s upper column lists the templates that
immediately from Ng and Low (2004). they
named these templates non-lexical-target because
predications derived from them can predicate with-
out considering the current character C
0
. Tem-
plates called lexical-target in the column below are
introduced by Jiang et al (2008). They are gener-
ated by adding an additional field C
0
to each non-
lexical-target template, so they can carry out pred-
ication not only according to the context, but also
according to the current character itself.
Notice that features derived from the templates
in Table 2 are all local features, which means all
features are determined only by the training in-
stances, and they can be generated before the train-
ing procedure.
389
Algorithm 5 Perceptron training algorithm.
1: Input: Training examples (x
i
, y
i
)
2: ?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: z
i
? argmax
z?GEN(x
i
)
?(x
i
, z) ? ?
6: if z
i
6= y
i
then
7: ?? ? +?(x
i
, y
i
)??(x
i
, z
i
)
8: Output: Parameters: ?
4.3 Training of the Classifier
Collins (2002)?s perceptron training algorithm
were adopted again, to learn a discriminative clas-
sifier, mapping from inputs x ? X to outputs
y ? Y . Here x is a character sequence, and y is
the sequence of classification result of each char-
acter in x. For segmentation, the classification re-
sult is a positional tag, while for Joint S&T, it is
an extended tag with POS information. X denotes
the set of character sequence, while Y denotes the
corresponding set of tag sequence.
According to Collins (2002), the function
GEN(x) generates all candidate tag sequences for
the character sequence x , the representation ?
maps each training example (x, y) ? X ? Y to
a feature vector ?(x, y) ? Rd, and the parameter
vector ? ? Rd is the weight vector corresponding
to the expected perceptron model?s feature space.
For a given input character sequence x, the mission
of the classifier is to find the tag sequence F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ? (3)
The inner product ?(x, y) ? ? is the score of the
result y given x, it represents how much plausibly
we can label character sequence x as tag sequence
y. The training algorithm is depicted in Algorithm
5. We also use the ?averaged parameters? strategy
to alleviate overfitting.
5 Experiments
Our experiments are conducted on the Penn Chi-
nese Treebank 5.0 (CTB 5.0). Following usual
practice of Chinese parsing, we choose chapters
1?260 (18074 sentences) as the training set, chap-
ters 301? 325 (350 sentences) as the development
set, and chapters 271 ? 300 (348 sentences) as
the final test set. We report the performance of
the baseline classifier, and then compare the per-
formance of the word lattice reranking against the
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
ur
e
number of iterations
Perceptron Learning Curves
Segmentation
Joint ST
Figure 2: Baseline averaged perceptron learning
curves for segmentation and Joint S&T.
n-best reranking, based on this baseline classifier.
For each experiment, we give accuracies on seg-
mentation and Joint S&T. Analogous to the situa-
tion in parsing, the accuracy of Joint S&T means
that, a word-POS is recognized only if both the
positional- and POS- tags are correctly labelled for
each character in the word?s span.
5.1 Baseline Perceptron Classifier
The perceptron classifier are trained on the train-
ing set using features generated from the templates
in Table 2, and the development set is used to
determine the best parameter vector. Figure 2
shows the learning curves for segmentation and
Joint S&T on the development set. We choose
the averaged parameter vector after 7 iterations for
the final test, this parameter vector achieves an F-
measure of 0.973 on segmentation, and 0.925 on
Joint S&T. Although the accuracy on segmentation
is quite high, it is obviously lower on Joint S&T.
Experiments of Ng and Low (2004) on CTB 3.0
also shown the similar trend, where they obtained
F-measure 0.952 on segmentation, and 0.919 on
Joint S&T.
5.2 Preparation for Reranking
For n-best reranking, we can easily generate n best
results for every training instance, by a modifica-
tion for the baseline classifier to hold n best can-
didates at each considering point. For word lattice
reranking, we use the algorithm in Algorithm 2 to
generate the pruned word lattice. Given a training
instance s
i
, its n best result list or pruned word
lattice is used as a reranking instance cand(s
i
),
the best candidate result (of the n best list) or or-
acle diameter (of the pruned word lattice) is the
reranking target y?
i
. We find the best result of the
n best results simply by computing each result?s
390
F-measure, and we determine the oracle diame-
ter of the pruned word lattice using the algorithm
depicted in Algorithm 1. All pairs of cand(s
i
)
and y?
i
deduced from the baseline model?s training
instances comprise the training set for reranking.
The development set and test set for reranking are
obtained in the same way. For the reranking train-
ing set {cand(s
i
), y
?
i
}
N
i=1
, {y
?
i
}
N
i=1
is called oracle
set, and the F-measure of {y?
i
}
N
i=1
against the ref-
erence set is called oracle F-measure. We use the
oracle F-measure indicating the utmost improve-
ment that an reranking algorithm can achieve.
5.3 Results and Analysis
The flows of the n-best list reranking and the
pruned word lattice reranking are similar to the
training procedure for the baseline classifier. The
training set for reranking is used to tune the param-
eter vector of the reranker, while the development
set for reranking is used to determine the optimal
number of iterations for the reranker?s training pro-
cedure.
We compare the performance of the word lat-
tice reranking against the n-best list reranking. Ta-
ble 3 shows the experimental results. The up-
per four rows are the experimental results for n-
best list reranking, while the four rows below are
for word lattice reranking. In n-best list rerank-
ing, with list size 20, the oracle F-measure on
Joint S&T is 0.9455, and the reranked F-measure
is 0.9280. When list size grows up to 50, the oracle
F-measure on Joint S&T jumps to 0.9552, while
the reranked F-measure becomes 0.9302. How-
ever, when n grows to 100, it brings tiny improve-
ment over the situation of n = 50. In word lat-
tice reranking, there is a trend similar to that in
n-best reranking, the performance difference be-
tween in degree = 2 and in degree = 5 is ob-
vious, whereas the setting in degree = 10 does
not obtain a notable improvement over the perfor-
mance of in degree = 5. We also notice that even
with a relative small in degree limitation, such as
in degree = 5, the oracle F-measures for seg-
mentation and Joint S&T both reach a quite high
level. This indicates the pruned word lattice con-
tains much more possibilities of segmentation and
tagging, compared to n-best list.
With the setting in degree = 5, the oracle F-
measure on Joint S&T reaches 0.9774, and the
reranked F-measure climbs to 0.9336. It achieves
an error reduction of 16.3% on Joint S&T, and an
error reduction of 11.9% on segmentation, over the
n-best Ora Seg Tst Seg Ora S&T Tst S&T
20 0.9827 0.9749 0.9455 0.9280
50 0.9903 0.9754 0.9552 0.9302
100 0.9907 0.9755 0.9558 0.9305
Degree Ora Seg Rnk Seg Ora S&T Rnk S&T
2 0.9898 0.9753 0.9549 0.9296
5 0.9927 0.9774 0.9768 0.9336
10 0.9934 0.9774 0.9779 0.9337
Table 3: Performance of n-best list reranking and
word lattice reranking. n-best: the size of the n-
best list for n-best list reranking; Degree: the in de-
gree limitation for word lattice reranking; Ora Seg:
oracle F-measure on segmentation of n-best lists or
word lattices; Ora S&T: oracle F-measure on Joint
S&T of n-best lists or word lattices; Rnk Seg: F-
measure on segmentation of reranked result; Rnk
S&T: F-measure on Joint S&T of reranked result
baseline classifier. While for n-best reranking with
setting n = 50, the Joint S&T?s error reduction is
6.9% , and the segmentation?s error reduction is
8.9%. We can see that reranking on pruned word
lattice is a practical method for segmentation and
POS tagging. Even with a much small data rep-
resentation, it obtains obvious advantage over the
n-best list reranking.
Comparing between the baseline and the two
reranking techniques, We find the non-local infor-
mation such as word- or POS- grams do improve
accuracy of segmentation and POS tagging, and
we also find the reranking technique is effective to
utilize these kinds of information. As even a small
scale n-best list or pruned word lattice can achieve
a rather high oracle F-measure, reranking tech-
nique, especially the word lattice reranking would
be a promising refining strategy for segmentation
and POS tagging. This is based on this viewpoint:
On the one hand, compared with the initial input
character sequence, the pruned word lattice has a
quite smaller search space while with a high ora-
cle F-measure, which enables us to conduct more
precise reranking over this search space to find the
best result. On the other hand, as the structure of
the search space is approximately outlined by the
topological directed architecture of pruned word
lattice, we have a much wider choice for feature se-
lection, which means that we would be able to uti-
lize not only features topologically before the cur-
rent considering position, just like those depicted
in Table 2 in section 4, but also information topo-
logically after it, for example the next word W
1
or
the next POS tag T
1
. We believe the pruned word
391
lattice reranking technique will obtain higher im-
provement, if we develop more precise reranking
algorithm and more appropriate features.
6 Conclusion
This paper describes a reranking strategy called
word lattice reranking. As a derivation of the for-
est reranking of Huang (2008), it performs rerank-
ing on pruned word lattice, instead of on n-best
list. Using word- and POS- gram information, this
reranking technique achieves an error reduction of
16.3% on Joint S&T, and 11.9% on segmentation,
over the baseline classifier, and it also outperforms
reranking on n-best list. It confirms that word lat-
tice reranking can effectively use non-local infor-
mation to select the best candidate result, from a
relative small representation structure while with a
quite high oracle F-measure. However, our rerank-
ing implementation is relative coarse, and it must
have many chances for improvement. In future
work, we will develop more precise pruning al-
gorithm for word lattice generation, to further cut
down the search space while maintaining the ora-
cle F-measure. We will also investigate the feature
selection strategy under the word lattice architec-
ture, for effective use of non-local information.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contracts 60736014
and 60573188, and 863 State Key Project No.
2006AA010108. We show our special thanks to
Liang Huang for his valuable suggestions.
References
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
17th International Conference on Machine Learn-
ing, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Fine, Shai, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. In Machine Learning, pages 32?
41.
Huang, Liang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics.
Jiang, Wenbin, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 23rd International Con-
ference on Machine Learning, pages 282?289, Mas-
sachusetts, USA.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Rabiner, Lawrence. R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?
286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
392
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222?1231,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
Liang Huang
Google Research
1350 Charleston Rd.
Mountain View, CA 94043, USA
lianghuang@google.com
liang.huang.sh@gmail.com
Wenbin Jiang and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
jiangwenbin@ict.ac.cn
Abstract
Jointly parsing two languages has been
shown to improve accuracies on either or
both sides. However, its search space is
much bigger than the monolingual case,
forcing existing approaches to employ
complicated modeling and crude approxi-
mations. Here we propose a much simpler
alternative, bilingually-constrained mono-
lingual parsing, where a source-language
parser learns to exploit reorderings as ad-
ditional observation, but not bothering to
build the target-side tree as well. We show
specifically how to enhance a shift-reduce
dependency parser with alignment fea-
tures to resolve shift-reduce conflicts. Ex-
periments on the bilingual portion of Chi-
nese Treebank show that, with just 3 bilin-
gual features, we can improve parsing ac-
curacies by 0.6% (absolute) for both En-
glish and Chinese over a state-of-the-art
baseline, with negligible (?6%) efficiency
overhead, thus much faster than biparsing.
1 Introduction
Ambiguity resolution is a central task in Natu-
ral Language Processing. Interestingly, not all lan-
guages are ambiguous in the same way. For exam-
ple, prepositional phrase (PP) attachment is (no-
toriously) ambiguous in English (and related Eu-
ropean languages), but is strictly unambiguous in
Chinese and largely unambiguous Japanese; see
(1a) I [ saw Bill ] [ with a telescope ].
wo [ yong wangyuanjin] [kandao le Bi?er].
?I used a telescope to see Bill.?
(1b) I saw [ Bill [ with a telescope ] ].
wo kandao le [ [ na wangyuanjin ] de Bi?er].
?I saw Bill who had a telescope at hand.?
Figure 1: PP-attachment is unambiguous in Chi-
nese, which can help English parsing.
Figure 1 for an example.1 It is thus intuitive to use
two languages for better disambiguation, which
has been applied not only to this PP-attachment
problem (Fossum and Knight, 2008; Schwartz et
al., 2003), but also to the more fundamental prob-
lem of syntactic parsing which subsumes the for-
mer as a subproblem. For example, Smith and
Smith (2004) and Burkett and Klein (2008) show
that joint parsing (or reranking) on a bitext im-
proves accuracies on either or both sides by lever-
aging bilingual constraints, which is very promis-
ing for syntax-based machine translation which re-
quires (good-quality) parse trees for rule extrac-
tion (Galley et al, 2004; Mi and Huang, 2008).
However, the search space of joint parsing is in-
evitably much bigger than the monolingual case,
1Chinese uses word-order to disambiguate the attachment
(see below). By contrast, Japanese resorts to case-markers
and the unambiguity is limited: it works for the ?V or N?
attachment ambiguities like in Figure 1 (see (Schwartz et al,
2003)) but not for the ?N
1
or N
2
? case (Mitch Marcus, p.c.).
1222
forcing existing approaches to employ compli-
cated modeling and crude approximations. Joint
parsing with a simplest synchronous context-free
grammar (Wu, 1997) is O(n6) as opposed to the
monolingual O(n3) time. To make things worse,
languages are non-isomorphic, i.e., there is no 1-
to-1 mapping between tree nodes, thus in practice
one has to use more expressive formalisms such
as synchronous tree-substitution grammars (Eis-
ner, 2003; Galley et al, 2004). In fact, rather than
joint parsing per se, Burkett and Klein (2008) re-
sort to separate monolingual parsing and bilingual
reranking over k2 tree pairs, which covers a tiny
fraction of the whole space (Huang, 2008).
We instead propose a much simpler alterna-
tive, bilingually-constrained monolingual parsing,
where a source-language parser is extended to ex-
ploit the reorderings between languages as addi-
tional observation, but not bothering to build a tree
for the target side simultaneously. To illustrate the
idea, suppose we are parsing the sentence
(1) I saw Bill [PP with a telescope ].
which has 2 parses based on the attachment of PP:
(1a) I [ saw Bill ] [PP with a telescope ].
(1b) I saw [ Bill [PP with a telescope ]].
Both are possible, but with a Chinese translation
the choice becomes clear (see Figure 1), because
a Chinese PP always immediately precedes the
phrase it is modifying, thus making PP-attachment
strictly unambiguous.2 We can thus use Chinese to
help parse English, i.e., whenever we have a PP-
attachment ambiguity, we will consult the Chinese
translation (from a bitext), and based on the align-
ment information, decide where to attach the En-
glish PP. On the other hand, English can help Chi-
nese parsing as well, for example in deciding the
scope of relative clauses which is unambiguous in
English but ambiguous in Chinese.
This method is much simpler than joint pars-
ing because it remains monolingual in the back-
bone, with alignment information merely as soft
evidence, rather than hard constraints since auto-
matic word alignment is far from perfect. It is thus
2to be precise, in Fig. 1(b), the English PP is translated
into a Chinese relative clause, but nevertheless all phrasal
modifiers attach to the immediate right in Mandarin Chinese.
straightforward to implement within a monolin-
gual parsing algorithm. In this work we choose
shift-reduce dependency parsing for its simplicity
and efficiency. Specifically, we make the following
contributions:
? we develop a baseline shift-reduce depen-
dency parser using the less popular, but clas-
sical, ?arc-standard? style (Section 2), and
achieve similar state-of-the-art performance
with the the dominant but complicated ?arc-
eager? style of Nivre and Scholz (2004);
? we propose bilingual features based on word-
alignment information to prefer ?target-side
contiguity? in resolving shift-reduce conflicts
(Section 3);
? we verify empirically that shift-reduce con-
flicts are the major source of errors, and cor-
rect shift-reduce decisions strongly correlate
with the above bilingual contiguity condi-
tions even with automatic alignments (Sec-
tion 5.3);
? finally, with just three bilingual features,
we improve dependency parsing accuracy
by 0.6% for both English and Chinese over
the state-of-the-art baseline with negligible
(?6%) efficiency overhead (Section 5.4).
2 Simpler Shift-Reduce Dependency
Parsing with Three Actions
The basic idea of classical shift-reduce parsing
from compiler theory (Aho and Ullman, 1972) is
to perform a left-to-right scan of the input sen-
tence, and at each step, choose one of the two ac-
tions: either shift the current word onto the stack,
or reduce the top two (or more) items on the stack,
replacing them with their combination. This idea
has been applied to constituency parsing, for ex-
ample in Sagae and Lavie (2006), and we describe
below a simple variant for dependency parsing
similar to Yamada and Matsumoto (2003) and the
?arc-standard? version of Nivre (2004).
2.1 The Three Actions
Basically, we just need to split the reduce ac-
tion into two symmetric (sub-)actions, reduceL
and reduceR, depending on which one of the two
1223
stack queue arcs
previous S w
i
|Q A
shift S|w
i
Q A
previous S|s
t?1
|s
t
Q A
reduceL S|st Q A ? {(st, st?1)}
reduceR S|st?1 Q A ? {(st?1, st)}
Table 1: Formal description of the three actions.
Note that shift requires non-empty queue while
reduce requires at least two elements on the stack.
items becomes the head after reduction. More for-
mally, we describe a parser configuration by a tu-
ple ?S,Q,A? where S is the stack, Q is the queue
of remaining words of the input, and A is the set
of dependency arcs accumulated so far.3 At each
step, we can choose one of the three actions:
1. shift: move the head of (a non-empty) queue
Q onto stack S;
2. reduceL: combine the top two items on the
stack, s
t
and s
t?1
(t ? 2), and replace
them with s
t
(as the head), and add a left arc
(s
t
, s
t?1
) to A;
3. reduceR: combine the top two items on the
stack, s
t
and s
t?1
(t ? 2), and replace them
with s
t?1
(as the head), and add a right arc
(s
t?1
, s
t
) to A.
These actions are summarized in Table 1. The
initial configuration is always ??, w
1
. . . w
n
, ??
with empty stack and no arcs, and the final con-
figuration is ?w
j
, ?, A? where w
j
is recognized as
the root of the whole sentence, and A encodes a
spanning tree rooted at w
j
. For a sentence of n
words, there are exactly 2n ? 1 actions: n shifts
and n ? 1 reductions, since every word must be
pushed onto stack once, and every word except the
root will eventually be popped in a reduction. The
time complexity, as other shift-reduce instances, is
clearly O(n).
2.2 Example of Shift-Reduce Conflict
Figure 2 shows the trace of this paradigm on the
example sentence. For the first two configurations
3a ?configuration? is sometimes called a ?state? (Zhang
and Clark, 2008), but that term is confusing with the states in
shift-reduce LR/LL parsing, which are quite different.
0 - I saw Bill with a ...
1 shift I saw Bill with a ...
2 shift I saw Bill with a ...
3 reduceL saw Bill with a ...
I
4 shift saw Bill with a ...
I
5a reduceR saw with a ...
I Bill
5b shift saw Bill with a ...
I
Figure 2: A trace of 3-action shift-reduce on the
example sentence. Shaded words are on stack,
while gray words have been popped from stack.
After step (4), the process can take either (5a)
or (5b), which correspond to the two attachments
(1a) and (1b) in Figure 1, respectively.
(0) and (1), only shift is possible since there are
not enough items on the stack for reduction. At
step (3), we perform a reduceL, making word ?I?
a modifier of ?saw?; after that the stack contains
a single word and we have to shift the next word
?Bill? (step 4). Now we face a shift-reduce con-
flict: we can either combine ?saw? and ?Bill? in
a reduceR action (5a), or shift ?Bill? (5b). We will
use features extracted from the configuration to re-
solve the conflict. For example, one such feature
could be a bigram s
t
? s
t?1
, capturing how likely
these two words are combined; see Table 2 for the
complete list of feature templates we use in this
baseline parser.
We argue that this kind of shift-reduce conflicts
are the major source of parsing errors, since the
other type of conflict, reduce-reduce conflict (i.e.,
whether left or right) is relatively easier to resolve
given the part-of-speech information. For exam-
ple, between a noun and an adjective, the former
is much more likely to be the head (and so is a
verb vs. a preposition or an adverb). Shift-reduce
resolution, however, is more non-local, and often
involves a triple, for example, (saw, Bill, with) for
a typical PP-attachment. On the other hand, if we
indeed make a wrong decision, a reduce-reduce
mistake just flips the head and the modifier, and
often has a more local effect on the shape of the
tree, whereas a shift-reduce mistake always leads
1224
Type Features
Unigram s
t
T (s
t
) s
t
? T (s
t
)
s
t?1
T (s
t?1
) s
t?1
? T (s
t?1
)
w
i
T (w
i
) w
i
? T (w
i
)
Bigram s
t
? s
t?1
T (s
t
) ? T (s
t?1
) T (s
t
) ? T (w
i
)
T (s
t
) ? s
t?1
? T (s
t?1
) s
t
? s
t?1
? T (s
t?1
) s
t
? T (s
t
) ? T (s
t?1
)
s
t
? T (s
t
) ? s
t?1
s
t
? T (s
t
) ? s
t?1
? T (s
t?1
)
Trigram T (s
t
) ? T (w
i
) ? T (w
i+1
) T (s
t?1
) ? T (s
t
) ? T (w
i
) T (s
t?2
) ? T (s
t?1
) ? T (s
t
)
s
t
? T (w
i
) ? T (w
i+1
) T (s
t?1
) ? s
t
? T (w
i
)
Modifier T (s
t?1
) ? T (lc(s
t?1
)) ? T (s
t
) T (s
t?1
) ? T (rc(s
t?1
)) ? T (s
t
) T (s
t?1
) ? T (s
t
) ? T (lc(s
t
))
T (s
t?1
) ? T (s
t
) ? T (rc(s
t
)) T (s
t?1
) ? T (lc(s
t?1
)) ? s
t
T (s
t?1
) ? T (rc(s
t?1
)) ? s
t
T (s
t?1
) ? s
t
? T (lc(s
t
))
Table 2: Feature templates of the baseline parser. s
t
, s
t?1
denote the top and next to top words on the
stack; w
i
and w
i+1
denote the current and next words on the queue. T (?) denotes the POS tag of a
given word, and lc(?) and rc(?) represent the leftmost and rightmost child. Symbol ? denotes feature
conjunction. Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR.
to vastly incompatible tree shapes with crossing
brackets (for example, [saw Bill] vs. [Bill with a
telescope]). We will see in Section 5.3 that this
is indeed the case in practice, thus suggesting us
to focus on shift-reduce resolution, which we will
return to with the help of bilingual constraints in
Section 3.
2.3 Comparison with Arc-Eager
The three action system was originally described
by Yamada and Matsumoto (2003) (although their
methods require multiple passes over the input),
and then appeared as ?arc-standard? in Nivre
(2004), but was argued against in comparison to
the four-action ?arc-eager? variant. Most subse-
quent works on shift-reduce or ?transition-based?
dependency parsing followed ?arc-eager? (Nivre
and Scholz, 2004; Zhang and Clark, 2008), which
now becomes the dominant style. But we argue
that ?arc-standard? is preferable because:
1. in the three action ?arc-standard? system, the
stack always contains a list of unrelated sub-
trees recognized so far, with no arcs between
any of them, e.g. (I? saw) and (Bill) in step
4 of Figure 2), whereas the four action ?arc-
eager? style can have left or right arrows be-
tween items on the stack;
2. the semantics of the three actions are atomic
and disjoint, whereas the semantics of 4 ac-
tions are not completely disjoint. For exam-
ple, their Left action assumes an implicit Re-
duce of the left item, and their Right ac-
tion assumes an implicit Shift. Furthermore,
these two actions have non-trivial precondi-
tions which also causes the next problem (see
below). We argue that this is rather compli-
cated to implement.
3. the ?arc-standard? scan always succeeds,
since at the end we can always reduce with
empty queue, whereas the ?arc-eager? style
sometimes goes into deadends where no ac-
tion can perform (prevented by precondi-
tions, otherwise the result will not be a well-
formed tree). This becomes parsing failures
in practice (Nivre and Scholz, 2004), leaving
more than one fragments on stack.
As we will see in Section 5.1, this simpler
arc-standard system performs equally well with
a state-of-the-art arc-eager system (Zhang and
Clark, 2008) on standard English Treebank pars-
ing (which is never shown before). We argue
that all things being equal, this simpler paradigm
should be preferred in practice. 4
2.4 Beam Search Extension
We also enhance deterministic shift-reduce pars-
ing with beam search, similar to Zhang and Clark
(2008), where k configurations develop in paral-
lel. Pseudocode 1 illustrates the algorithm, where
we keep an agenda V of the current active con-
figurations, and at each step try to extend them by
applying one of the three actions. We then dump
the best k new configurations from the buffer back
4On the other hand, there are also arguments for ?arc-
eager?, e.g., ?incrementality?; see (Nivre, 2004; Nivre, 2008).
1225
Pseudocode 1 beam-search shift-reduce parsing.
1: Input: POS-tagged word sequence w
1
. . . w
n
2: start ? ??, w
1
. . . w
n
, ?? ? initial config: empty stack,
no arcs
3: V? {start} ? initial agenda
4: for step ? 1 . . . 2n? 1 do
5: BUF? ? ? buffer for new configs
6: for each config in agenda V do
7: for act ? {shift, reduceL, reduceR} do
8: if act is applicable to config then
9: next ? apply act to config
10: insert next into buffer BUF
11: V? top k configurations of BUF
12: Output: the tree of the best config in V
into the agenda for the next step. The complexity
of this algorithm is O(nk), which subsumes the
determinstic mode as a special case (k = 1).
2.5 Online Training
To train the parser we need an ?oracle? or gold-
standard action sequence for gold-standard depen-
dency trees. This oracle turns out to be non-unique
for the three-action system (also non-unique for
the four-action system), because left dependents
of a head can be reduced either before or after all
right dependents are reduced. For example, in Fig-
ure 2, ?I? is a left dependent of ?saw?, and can in
principle wait until ?Bill? and ?with? are reduced,
and then finally combine with ?saw?. We choose
to use the heuristic of ?shortest stack? that always
prefers reduceL over shift, which has the effect that
all left dependents are first recognized inside-out,
followed by all right dependents, also inside-out,
which coincides with the head-driven constituency
parsing model of Collins (1999).
We use the popular online learning algorithm
of structured perceptron with parameter averag-
ing (Collins, 2002). Following Collins and Roark
(2004) we also use the ?early-update? strategy,
where an update happens whenever the gold-
standard action-sequence falls off the beam, with
the rest of the sequence neglected. As a special
case, for the deterministic mode, updates always
co-occur with the first mistake made. The intuition
behind this strategy is that future mistakes are of-
ten caused by previous ones, so with the parser on
the wrong track, future actions become irrelevant
for learning. See Section 5.3 for more discussions.
(a) I
:::::::::
saw Bill with a telescope .
wo yong wangyuanjin kandao le Bi?er.
c(s
t?1
, s
t
) =+; reduce is correct
(b) I
:::::::::
saw Bill with a telescope .
wo kandao le na wangyuanjin de Bi?er.
c(s
t?1
, s
t
) =?; reduce is wrong
(c) I saw
:::::::::::
Bill with
:::
a
::::::::::
telescope
:
.
wo kandao le na wangyuanjin de Bi?er.
cR(st, wi) =+; shift is correct
(d) I saw
:::::::::
Bill with
:::
a
::::::::::
telescope
:
.
wo yong wangyuanjin kandao le Bi?er.
cR(st, wi) =?; shift is wrong
Figure 3: Bilingual contiguity features c(s
t?1
, s
t
)
and cR(st, wi) at step (4) in Fig. 2 (facing a shift-
reduce decision). Bold words are currently on
stack while gray ones have been popped. Here the
stack tops are s
t
= Bill, s
t?1
= saw, and the queue
head is w
i
= with; underlined texts mark the source
and target spans being considered, and wavy un-
derlines mark the allowed spans (Tab. 3). Red bold
alignment links violate contiguity constraints.
3 Soft Bilingual Constraints as Features
As suggested in Section 2.2, shift-reduce con-
flicts are the central problem we need to address
here. Our intuition is, whenever we face a deci-
sion whether to combine the stack tops s
t?1
and
s
t
or to shift the current word w
i
, we will consult
the other language, where the word-alignment in-
formation would hopefully provide a preference,
as in the running example of PP-attachment (see
Figure 1). We now develop this idea into bilingual
contiguity features.
1226
3.1 A Pro-Reduce Feature c(s
t?1
, s
t
)
Informally, if the correct decision is a reduction,
then it is likely that the corresponding words of
s
t?1
and s
t
on the target-side should also form a
contiguous span. For example, in Figure 3(a), the
source span of a reduction is [saw .. Bill], which
maps onto [kandao . . . Bi?er] on the Chinese side.
This target span is contiguous, because no word
within this span is aligned to a source word out-
side of the source span. In this case we say feature
c(s
t?1
, s
t
) =+, which encourages ?reduce?.
However, in Figure 3(b), the source span is still
[saw .. Bill], but this time maps onto a much
longer span on the Chinese side. This target span
is discontiguous, since the Chinese words na and
wangyuanjin are alinged to English ?with? and
?telescope?, both of which fall outside of the
source span. In this case we say feature c(s
t?1
, s
t
)
=?, which discourages ?reduce? .
3.2 A Pro-Shift Feature cR(st, wi)
Similarly, we can develop another feature
cR(st, wi) for the shift action. In Figure 3(c),
when considering shifting ?with?, the source
span becomes [Bill .. with] which maps to [na
.. Bi?er] on the Chinese side. This target span
looks like discontiguous in the above definition
with wangyuanjin aligned to ?telescope?, but we
tolerate this case for the following reasons. There
is a crucial difference between shift and reduce:
in a shift, we do not know yet the subtree spans
(unlike in a reduce we are always combining two
well-formed subtrees). The only thing we are
sure of in a shift action is that s
t
and w
i
will be
combined before s
t?1
and s
t
are combined (Aho
and Ullman, 1972), so we can tolerate any target
word aligned to source word still in the queue,
but do not allow any target word aligned to an
already recognized source word. This explains
the notational difference between cR(st, wi) and
c(s
t?1
, s
t
), where subscript ?R? means ?right
contiguity?.
As a final example, in Figure 3(d), Chinese
word kandao aligns to ?saw?, which is already
recognized, and this violates the right contiguity.
So cR(st, wi) =?, suggesting that shift is probably
wrong. To be more precise, Table 3 shows the for-
mal definitions of the two features. We basically
source target alowed
feature f span sp span tp span ap
c(s
t?1
, s
t
) [s
t?1
..s
t
] M(sp) [s
t?1
..s
t
]
cR(st, wi) [st..wi] M(sp) [st..wn]
f = + iff. M?1(M(sp)) ? ap
Table 3: Formal definition of bilingual features.
M(?) is maps a source span to the target language,
and M?1(?) is the reverse operation mapping back
to the source language.
map a source span sp to its target span M(sp),
and check whether its reverse image back onto the
source language M?1(M(sp)) falls inside the al-
lowed span ap. For cR(st, wi), the allowed span
extends to the right end of the sentence.5
3.3 Variations and Implementation
To conclude so far, we have got two alignment-
based features, c(s
t?1
, s
t
) correlating with reduce,
and cR(st, wi) correlating with shift. In fact, the
conjunction of these two features,
c(s
t?1
, s
t
) ? cR(st, wi)
is another feature with even stronger discrimina-
tion power. If
c(s
t?1
, s
t
) ? cR(st, wi) = + ? ?
it is strongly recommending reduce, while
c(s
t?1
, s
t
) ? cR(st, wi) = ? ?+
is a very strong signal for shift. So in total we got
three bilingual feature (templates), which in prac-
tice amounts to 24 instances (after cross-product
with {?,+} and the three actions). We show in
Section 5.3 that these features do correlate with
the correct shift/reduce actions in practice.
The naive implemention of bilingual feature
computation would be of O(kn2) complexity
in the worse case because when combining the
largest spans one has to scan over the whole sen-
tence. We envision the use of a clever datastructure
would reduce the complexity, but leave this to fu-
ture work, as the experiments (Table 8) show that
5Our definition implies that we only consider faithful
spans to be contiguous (Galley et al, 2004). Also note that
source spans include all dependents of s
t
and s
t?1
.
1227
the parser is only marginally (?6%) slower with
the new bilingual features. This is because the ex-
tra work, with just 3 bilingual features, is not the
bottleneck in practice, since the extraction of the
vast amount of other features in Table 2 dominates
the computation.
4 Related Work in Grammar Induction
Besides those cited in Section 1, there are some
other related work on using bilingual constraints
for grammar induction (rather than parsing). For
example, Hwa et al (2005) use simple heuris-
tics to project English trees to Spanish and Chi-
nese, but get discouraging accuracy results learned
from those projected trees. Following this idea,
Ganchev et al (2009) and Smith and Eisner (2009)
use constrained EM and parser adaptation tech-
niques, respectively, to perform more principled
projection, and both achieve encouraging results.
Our work, by constrast, never uses bilingual
tree pairs not tree projections, and only uses word
alignment alone to enhance a monolingual gram-
mar, which learns to prefer target-side contiguity.
5 Experiments
5.1 Baseline Parser
We implement our baseline monolingual parser (in
C++) based on the shift-reduce algorithm in Sec-
tion 2, with feature templates from Table 2. We
evaluate its performance on the standard Penn En-
glish Treebank (PTB) dependency parsing task,
i.e., train on sections 02-21 and test on section 23
with automatically assigned POS tags (at 97.2%
accuracy) using a tagger similar to Collins (2002),
and using the headrules of Yamada and Mat-
sumoto (2003) for conversion into dependency
trees. We use section 22 as dev set to deter-
mine the optimal number of iterations in per-
ceptron training. Table 4 compares our baseline
against the state-of-the-art graph-based (McDon-
ald et al, 2005) and transition-based (Zhang and
Clark, 2008) approaches, and confirms that our
system performs at the same level with those state-
of-the-art, and runs extremely fast in the determin-
istic mode (k=1), and still quite fast in the beam-
search mode (k=16).
parser accuracy secs/sent
McDonald et al (2005) 90.7 0.150
Zhang and Clark (2008) 91.4 0.195
our baseline at k=1 90.2 0.009
our baseline at k=16 91.3 0.125
Table 4: Baseline parser performance on standard
Penn English Treebank dependency parsing task.
The speed numbers are not exactly comparable
since they are reported on different machines.
Training Dev Test
CTB Articles 1-270 301-325 271-300
Bilingual Paris 2745 273 290
Table 5: Training, dev, and test sets from bilingual
Chinese Treebank a` la Burkett and Klein (2008).
5.2 Bilingual Data
The bilingual data we use is the translated por-
tion of the Penn Chinese Treebank (CTB) (Xue
et al, 2002), corresponding to articles 1-325 of
PTB, which have English translations with gold-
standard parse trees (Bies et al, 2007). Table 5
shows the split of this data into training, devel-
opment, and test subsets according to Burkett and
Klein (2008). Note that not all sentence pairs could
be included, since many of them are not one-
to-one aligned at the sentence level. Our word-
alignments are generated from the HMM aligner
of Liang et al (2006) trained on approximately
1.7M sentence pairs (provided to us by David Bur-
kett, p.c.). This aligner outputs ?soft alignments?,
i.e., posterior probabilities for each source-target
word pair. We use a pruning threshold of 0.535 to
remove low-confidence alignment links,6 and use
the remaining links as hard alignments; we leave
the use of alignment probabilities to future work.
For simplicity reasons, in the following exper-
iments we always supply gold-standard POS tags
as part of the input to the parser.
5.3 Testing our Hypotheses
Before evaluating our bilingual approach, we need
to verify empirically the two assumptions we
made about the parser in Sections 2 and 3:
6and also removing notoriously bad links in {the, a, an}?
{de, le} following Fossum and Knight (2008).
1228
sh ? re re ? sh sh-re re-re
# 92 98 190 7
% 46.7% 49.7% 96.4% 3.6%
Table 6: [Hypothesis 1] Error distribution in the
baseline model (k = 1) on English dev set.
?sh ? re? means ?should shift, but reduced?. Shift-
reduce conflicts overwhelmingly dominate.
1. (monolingual) shift-reduce conflict is the ma-
jor source of errors while reduce-reduce con-
flict is a minor issue;
2. (bilingual) the gold-standard decisions of
shift or reduce should correlate with contigu-
ities of c(s
t?1
, s
t
), and of cR(st, wi).
Hypothesis 1 is verified in Table 6, where we
count all the first mistakes the baseline parser
makes (in the deterministic mode) on the En-
glish dev set (273 sentences). In shift-reduce pars-
ing, further mistakes are often caused by previ-
ous ones, so only the first mistake in each sen-
tence (if there is one) is easily identifiable;7 this
is also the argument for ?early update? in apply-
ing perceptron learning to these incremental pars-
ing algorithms (Collins and Roark, 2004) (see also
Section 2). Among the 197 first mistakes (other
76 sentences have perfect output), the vast ma-
jority, 190 of them (96.4%), are shift-reduce er-
rors (equally distributed between shift-becomes-
reduce and reduce-becomes-shift), and only 7
(3.6%) are due to reduce-reduce conflicts.8 These
statistics confirm our intuition that shift-reduce de-
cisions are much harder to make during parsing,
and contribute to the overwhelming majority of er-
rors, which is studied in the next hypothesis.
Hypothesis 2 is verified in Table 7. We take
the gold-standard shift-reduce sequence on the En-
glish dev set, and classify them into the four cat-
egories based on bilingual contiguity features: (a)
c(s
t?1
, s
t
), i.e. whether the top 2 spans on stack
is contiguous, and (b) cR(st, wi), i.e. whether the
7to be really precise one can define ?independent mis-
takes? as those not affected by previous ones, i.e., errors
made after the parser recovers from previous mistakes; but
this is much more involved and we leave it to future work.
8Note that shift-reduce errors include those due to the
non-uniqueness of oracle, i.e., between some reduceL and
shift. Currently we are unable to identify ?genuine? errors
that would result in an incorrect parse. See also Section 2.5.
c(s
t?1
, s
t
) cR(st, wi) shift reduce
+ ? 172 ? 1,209
? + 1,432 > 805
+ + 4,430 ? 3,696
? ? 525 ? 576
total 6,559 = 6,286
Table 7: [Hyp. 2] Correlation of gold-standard
shift/reduce decisions with bilingual contiguity
conditions (on English dev set). Note there is al-
ways one more shift than reduce in each sentence.
stack top is contiguous with the current word w
i
.
According to discussions in Section 3, when (a) is
contiguous and (b) is not, it is a clear signal for
reduce (to combine the top two elements on the
stack) rather than shift, and is strongly supported
by the data (first line: 1209 reduces vs. 172 shifts);
and while when (b) is contiguous and (a) is not,
it should suggest shift (combining s
t
and w
i
be-
fore s
t?1
and s
t
are combined) rather than reduce,
and is mildly supported by the data (second line:
1432 shifts vs. 805 reduces). When (a) and (b) are
both contiguous or both discontiguous, it should
be considered a neutral signal, and is also consis-
tent with the data (next two lines). So to conclude,
this bilingual hypothesis is empirically justified.
On the other hand, we would like to note that
these correlations are done with automatic word
alignments (in our case, from the Berkeley aligner)
which can be quite noisy. We suspect (and will fin-
ish in the future work) that using manual align-
ments would result in a better correlation, though
for the main parsing results (see below) we can
only afford automatic alignments in order for our
approach to be widely applicable to any bitext.
5.4 Results
We incorporate the three bilingual features (again,
with automatic alignments) into the baseline
parser, retrain it, and test its performance on the
English dev set, with varying beam size. Table 8
shows that bilingual constraints help more with
larger beams, from almost no improvement with
the deterministic mode (k=1) to +0.5% better with
the largest beam (k=16). This could be explained
by the fact that beam-search is more robust than
the deterministic mode, where in the latter, if our
1229
baseline +bilingual
k accuracy time (s) accuracy time (s)
1 84.58 0.011 84.67 0.012
2 85.30 0.025 85.62 0.028
4 85.42 0.040 85.81 0.044
8 85.50 0.081 85.95 0.085
16 85.57 0.158 86.07 0.168
Table 8: Effects of beam size k on efficiency and
accuracy (on English dev set). Time is average
per sentence (in secs). Bilingual constraints show
more improvement with larger beams, with a frac-
tional efficiency overhead over the baseline.
English Chinese
monolingual baseline 86.9 85.7
+bilingual features 87.5 86.3
improvement +0.6 +0.6
signficance level p < 0.05 p < 0.08
Berkeley parser 86.1 87.9
Table 9: Final results of dependency accuracy (%)
on the test set (290 sentences, beam size k=16).
bilingual features misled the parser into a mistake,
there is no chance of getting back, while in the
former multiple configurations are being pursued
in parallel. In terms of speed, both parsers run pro-
portionally slower with larger beams, as the time
complexity is linear to the beam-size. Computing
the bilingual features further slows it down, but
only fractionally so (just 1.06 times as slow as the
baseline at k=16), which is appealing in practice.
By contrast, Burkett and Klein (2008) reported
their approach of ?monolingual k-best parsing fol-
lowed by bilingual k2-best reranking? to be ?3.8
times slower? than monolingual parsing.
Our final results on the test set (290 sentences)
are summarized in Table 9. On both English
and Chinese, the addition of bilingual features
improves dependency arc accuracies by +0.6%,
which is mildly significant using the Z-test of
Collins et al (2005). We also compare our results
against the Berkeley parser (Petrov and Klein,
2007) as a reference system, with the exact same
setting (i.e., trained on the bilingual data, and test-
ing using gold-standard POS tags), and the result-
ing trees are converted into dependency via the
same headrules. We use 5 iterations of split-merge
grammar induction as the 6th iteration overfits the
small training set. The result is worse than our
baseline on English, but better than our bilingual
parser on Chinese. The discrepancy between En-
glish and Chinese is probably due to the fact that
our baseline feature templates (Table 2) are engi-
neered on English not Chinese.
6 Conclusion and Future Work
We have presented a novel parsing paradigm,
bilingually-constrained monolingual parsing,
which is much simpler than joint (bi-)parsing, yet
still yields mild improvements in parsing accuracy
in our preliminary experiments. Specifically,
we showed a simple method of incorporating
alignment features as soft evidence on top of a
state-of-the-art shift-reduce dependency parser,
which helped better resolve shift-reduce conflicts
with fractional efficiency overhead.
The fact that we managed to do this with only
three alignment features is on one hand encour-
aging, but on the other hand leaving the bilingual
feature space largely unexplored. So we will en-
gineer more such features, especially with lexical-
ization and soft alignments (Liang et al, 2006),
and study the impact of alignment quality on pars-
ing improvement. From a linguistics point of view,
we would like to see how linguistics distance
affects this approach, e.g., we suspect English-
French would not help each other as much as
English-Chinese do; and it would be very interest-
ing to see what types of syntactic ambiguities can
be resolved across different language pairs. Fur-
thermore, we believe this bilingual-monolingual
approach can easily transfer to shift-reduce con-
stituency parsing (Sagae and Lavie, 2006).
Acknowledgments
We thank the anonymous reviewers for pointing to
us references about ?arc-standard?. We also thank
Aravind Joshi and Mitch Marcus for insights on
PP attachment, Joakim Nivre for discussions on
arc-eager, Yang Liu for suggestion to look at man-
ual alignments, and David A. Smith for sending
us his paper. The second and third authors were
supported by National Natural Science Foundation
of China, Contracts 60603095 and 60736014, and
863 State Key Project No. 2006AA010108.
1230
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v1.0. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL (poster), pages 205?208.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual chinese-english word alignments to resolve pp-
attachment ambiguity in english. In Proceedings of
AMTA Student Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL, pages 273?280.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP,
Honolulu, Haiwaii.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of ACL
(poster).
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of english pp attachment using mul-
tilingual aligned data. In Proceedings of MT Summit
IX.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous features.
In Proceedings of EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
1231
Proceedings of ACL-08: HLT, pages 897?904,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Cascaded Linear Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? Liang Huang ? Qun Liu ? Yajuan Lu? ?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
jiangwenbin@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
We propose a cascaded linear model for
joint Chinese word segmentation and part-
of-speech tagging. With a character-based
perceptron as the core, combined with real-
valued features such as language models, the
cascaded model is able to efficiently uti-
lize knowledge sources that are inconvenient
to incorporate into the perceptron directly.
Experiments show that the cascaded model
achieves improved accuracies on both seg-
mentation only and joint segmentation and
part-of-speech tagging. On the Penn Chinese
Treebank 5.0, we obtain an error reduction of
18.5% on segmentation and 12% on joint seg-
mentation and part-of-speech tagging over the
perceptron-only baseline.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are important tasks in computer processing of
Chinese and other Asian languages. Several mod-
els were introduced for these problems, for example,
the Hidden Markov Model (HMM) (Rabiner, 1989),
Maximum Entropy Model (ME) (Ratnaparkhi and
Adwait, 1996), and Conditional Random Fields
(CRFs) (Lafferty et al, 2001). CRFs have the ad-
vantage of flexibility in representing features com-
pared to generative ones such as HMM, and usually
behaves the best in the two tasks. Another widely
used discriminative method is the perceptron algo-
rithm (Collins, 2002), which achieves comparable
performance to CRFs with much faster training, so
we base this work on the perceptron.
To segment and tag a character sequence, there
are two strategies to choose: performing POS tag-
ging following segmentation; or joint segmentation
and POS tagging (Joint S&T). Since the typical ap-
proach of discriminative models treats segmentation
as a labelling problem by assigning each character
a boundary tag (Xue and Shen, 2003), Joint S&T
can be conducted in a labelling fashion by expand-
ing boundary tags to include POS information (Ng
and Low, 2004). Compared to performing segmen-
tation and POS tagging one at a time, Joint S&T can
achieve higher accuracy not only on segmentation
but also on POS tagging (Ng and Low, 2004). Be-
sides the usual character-based features, additional
features dependent on POS?s or words can also be
employed to improve the performance. However, as
such features are generated dynamically during the
decoding procedure, two limitation arise: on the one
hand, the amount of parameters increases rapidly,
which is apt to overfit on training corpus; on the
other hand, exact inference by dynamic program-
ming is intractable because the current predication
relies on the results of prior predications. As a result,
many theoretically useful features such as higher-
order word or POS n-grams are difficult to be in-
corporated in the model efficiently.
To cope with this problem, we propose a cascaded
linear model inspired by the log-linear model (Och
and Ney, 2004) widely used in statistical machine
translation to incorporate different kinds of knowl-
edge sources. Shown in Figure 1, the cascaded
model has a two-layer architecture, with a character-
based perceptron as the core combined with other
real-valued features such as language models. We
897
Core
Linear Model
(Perceptron)
g1 =
?
i ?i ? fi
~?
Outside-layer
Linear Model
S = ?j wj ? gj
~w
f1
f2
f|R|
g1
Word LM: g2 = Pwlm(W ) g2
POS LM: g3 = Ptlm(T ) g3
Labelling: g4 = P (T |W ) g4
Generating: g5 = P (W |T ) g5
Length: g6 = |W | g6
S
Figure 1: Structure of Cascaded Linear Model. |R| denotes the scale of the feature space of the core perceptron.
will describe it in detail in Section 4. In this ar-
chitecture, knowledge sources that are intractable to
incorporate into the perceptron, can be easily incor-
porated into the outside linear model. In addition,
as these knowledge sources are regarded as separate
features, we can train their corresponding models in-
dependently with each other. This is an interesting
approach when the training corpus is large as it re-
duces the time and space consumption. Experiments
show that our cascaded model can utilize different
knowledge sources effectively and obtain accuracy
improvements on both segmentation and Joint S&T.
2 Segmentation and POS Tagging
Given a Chinese character sequence:
C1:n = C1 C2 .. Cn
the segmentation result can be depicted as:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
while the segmentation and POS tagging result can
be depicted as:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
Here, Ci (i = 1..n) denotes Chinese character,
ti (i = 1..m) denotes POS tag, and Cl:r (l ? r)
denotes character sequence ranges from Cl to Cr.
We can see that segmentation and POS tagging task
is to divide a character sequence into several subse-
quences and label each of them a POS tag.
It is a better idea to perform segmentation and
POS tagging jointly in a uniform framework. Ac-
cording to Ng and Low (2004), the segmentation
task can be transformed to a tagging problem by as-
signing each character a boundary tag of the follow-
ing four types:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
We can extract segmentation result by splitting
the labelled result into subsequences of pattern s or
bm?e which denote single-character word and multi-
character word respectively. In order to perform
POS tagging at the same time, we expand boundary
tags to include POS information by attaching a POS
to the tail of a boundary tag as a postfix following
Ng and Low (2004). As each tag is now composed
of a boundary part and a POS part, the joint S&T
problem is transformed to a uniform boundary-POS
labelling problem. A subsequence of boundary-POS
labelling result indicates a word with POS t only if
the boundary tag sequence composed of its bound-
ary part conforms to s or bm?e style, and all POS
tags in its POS part equal to t. For example, a tag
sequence b NN m NN e NN represents a three-
character word with POS tag NN .
3 The Perceptron
The perceptron algorithm introduced into NLP by
Collins (2002), is a simple but effective discrimina-
tive training method. It has comparable performance
898
Non-lexical-target Instances
Cn (n = ?2..2) C?2=e, C?1=?, C0=U, C1=/, C2=?
CnCn+1 (n = ?2..1) C?2C?1=e?, C?1C0=?U, C0C1=U/, C1C2=/?
C?1C1 C?1C1=?/
Lexical-target Instances
C0Cn (n = ?2..2) C0C?2=Ue, C0C?1=U?, C0C0=UU, C0C1=U/, C0C2=U?
C0CnCn+1 (n = ?2..1) C0C?2C?1=Ue?, C0C?1C0=U?U, C0C0C1=UU/, C0C1C2=U/?
C0C?1C1 C0C?1C1 =U?/
Table 1: Feature templates and instances. Suppose we are considering the third character ?U? in ?e? U /??.
to CRFs, while with much faster training. The per-
ceptron has been used in many NLP tasks, such as
POS tagging (Collins, 2002), Chinese word seg-
mentation (Ng and Low, 2004; Zhang and Clark,
2007) and so on. We trained a character-based per-
ceptron for Chinese Joint S&T, and found that the
perceptron itself could achieve considerably high ac-
curacy on segmentation and Joint S&T. In following
subsections, we describe the feature templates and
the perceptron training algorithm.
3.1 Feature Templates
The feature templates we adopted are selected from
those of Ng and Low (2004). To compare with oth-
ers conveniently, we excluded the ones forbidden by
the close test regulation of SIGHAN, for example,
Pu(C0), indicating whether character C0 is a punc-
tuation.
All feature templates and their instances are
shown in Table 1. C represents a Chinese char-
acter while the subscript of C indicates its posi-
tion in the sentence relative to the current charac-
ter (it has the subscript 0). Templates immediately
borrowed from Ng and Low (2004) are listed in
the upper column named non-lexical-target. We
called them non-lexical-target because predications
derived from them can predicate without consider-
ing the current character C0. Templates in the col-
umn below are expanded from the upper ones. We
add a field C0 to each template in the upper col-
umn, so that it can carry out predication according
to not only the context but also the current char-
acter itself. As predications generated from such
templates depend on the current character, we name
these templates lexical-target. Note that the tem-
plates of Ng and Low (2004) have already con-
tained some lexical-target ones. With the two kinds
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi)?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? +?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
of predications, the perceptron model will do exact
predicating to the best of its ability, and can back
off to approximately predicating if exact predicating
fails.
3.2 Training Algorithm
We adopt the perceptron training algorithm of
Collins (2002) to learn a discriminative model map-
ping from inputs x ? X to outputs y ? Y , where X
is the set of sentences in the training corpus and Y
is the set of corresponding labelled results. Follow-
ing Collins, we use a function GEN(x) generating
all candidate results of an input x , a representation
? mapping each training example (x, y) ? X ? Y
to a feature vector ?(x, y) ? Rd, and a parameter
vector ~? ? Rd corresponding to the feature vector.
d means the dimension of the vector space, it equals
to the amount of features in the model. For an input
character sequence x, we aim to find an output F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
?(x, y) ? ~? represents the inner product of feature
vector ?(x, y) and the parameter vector ~?. We used
the algorithm depicted in Algorithm 1 to tune the
parameter vector ~?.
899
To alleviate overfitting on the training examples,
we use the refinement strategy called ?averaged pa-
rameters? (Collins, 2002) to the algorithm in Algo-
rithm 1.
4 Cascaded Linear Model
In theory, any useful knowledge can be incorporated
into the perceptron directly, besides the character-
based features already adopted. Additional features
most widely used are related to word or POS n-
grams. However, such features are generated dy-
namically during the decoding procedure so that
the feature space enlarges much more rapidly. Fig-
ure 2 shows the growing tendency of feature space
with the introduction of these features as well as the
character-based ones. We noticed that the templates
related to word unigrams and bigrams bring to the
feature space an enlargement much rapider than the
character-base ones, not to mention the higher-order
grams such as trigrams or 4-grams. In addition, even
though these higher grams were managed to be used,
there still remains another problem: as the current
predication relies on the results of prior ones, the
decoding procedure has to resort to approximate in-
ference by maintaining a list of N -best candidates at
each predication position, which evokes a potential
risk to depress the training.
To alleviate the drawbacks, we propose a cas-
caded linear model. It has a two-layer architec-
ture, with a perceptron as the core and another linear
model as the outside-layer. Instead of incorporat-
ing all features into the perceptron directly, we first
trained the perceptron using character-based fea-
tures, and several other sub-models using additional
ones such as word or POS n-grams, then trained the
outside-layer linear model using the outputs of these
sub-models, including the perceptron. Since the per-
ceptron is fixed during the second training step, the
whole training procedure need relative small time
and memory cost.
The outside-layer linear model, similar to those
in SMT, can synthetically utilize different knowl-
edge sources to conduct more accurate comparison
between candidates. In this layer, each knowledge
source is treated as a feature with a corresponding
weight denoting its relative importance. Suppose we
have n features gj (j = 1..n) coupled with n corre-
 0
 300000
 600000
 900000
 1.2e+006
 1.5e+006
 1.8e+006
 2.1e+006
 2.4e+006
 2.7e+006
 3e+006
 3.3e+006
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22
Fe
atu
re 
sp
ac
e
Introduction of features
growing curve
Figure 2: Feature space growing curve. The horizontal
scope X[i:j] denotes the introduction of different tem-
plates. X[0:5]: Cn (n = ?2..2); X[5:9]: CnCn+1 (n =
?2..1); X[9:10]: C?1C1; X[10:15]: C0Cn (n =
?2..2); X[15:19]: C0CnCn+1 (n = ?2..1); X[19:20]:
C0C?1C1; X[20:21]: W0; X[21:22]: W?1W0. W0 de-
notes the current considering word, while W?1 denotes
the word in front of W0. All the data are collected from
the training procedure on MSR corpus of SIGHAN bake-
off 2.
sponding weights wj (j = 1..n), each feature gj
gives a score gj(r) to a candidate r, then the total
score of r is given by:
S(r) =
?
j=1..n
wj ? gj(r) (2)
The decoding procedure aims to find the candidate
r? with the highest score:
r? = argmax
r
S(r) (3)
While the mission of the training procedure is to
tune the weights wj(j = 1..n) to guarantee that the
candidate r with the highest score happens to be the
best result with a high probability.
As all the sub-models, including the perceptron,
are regarded as separate features of the outside-layer
linear model, we can train them respectively with
special algorithms. In our experiments we trained
a 3-gram word language model measuring the flu-
ency of the segmentation result, a 4-gram POS lan-
guage model functioning as the product of state-
transition probabilities in HMM, and a word-POS
co-occurrence model describing how much probably
a word sequence coexists with a POS sequence. As
shown in Figure 1, the character-based perceptron is
used as the inside-layer linear model and sends its
output to the outside-layer. Besides the output of the
perceptron, the outside-layer also receive the outputs
900
of the word LM, the POS LM, the co-occurrence
model and a word count penalty which is similar to
the translation length penalty in SMT.
4.1 Language Model
Language model (LM) provides linguistic probabil-
ities of a word sequence. It is an important measure
of fluency of the translation in SMT. Formally, an
n-gram word LM approximates the probability of a
word sequence W = w1:m with the following prod-
uct:
Pwlm(W ) =
m
?
i=1
Pr(wi|wmax(0,i?n+1):i?1) (4)
Similarly, the n-gram POS LM of a POS sequence
T = t1:m is:
Ptlm(T ) =
m
?
i=1
Pr(ti|tmax(0,i?n+1):i?1) (5)
Notice that a bi-gram POS LM functions as the prod-
uct of transition probabilities in HMM.
4.2 Word-POS Co-occurrence Model
Given a training corpus with POS tags, we can train
a word-POS co-occurrence model to approximate
the probability that the word sequence of the la-
belled result co-exists with its corresponding POS
sequence. Using W = w1:m to denote the word se-
quence, T = t1:m to denote the corresponding POS
sequence, P (T |W ) to denote the probability that W
is labelled as T , and P (W |T ) to denote the prob-
ability that T generates W , we can define the co-
occurrence model as follows:
Co(W,T ) = P (T |W )?wt ? P (W |T )?tw (6)
?wt and ?tw denote the corresponding weights of the
two components.
Suppose the conditional probability Pr(t|w) de-
scribes the probability that the word w is labelled as
the POS t, while Pr(w|t) describes the probability
that the POS t generates the word w, then P (T |W )
can be approximated by:
P (T |W ) ?
m
?
k=1
Pr(tk|wk) (7)
And P (W |T ) can be approximated by:
P (W |T ) ?
m
?
k=1
Pr(wk|tk) (8)
Pr(w|t) and Pr(t|w) can be easily acquired by
Maximum Likelihood Estimates (MLE) over the
corpus. For instance, if the word w appears N times
in training corpus and is labelled as POS t for n
times, the probability Pr(t|w) can be estimated by
the formula below:
Pr(t|w) ? nN (9)
The probability Pr(w|t) could be estimated through
the same approach.
To facilitate tuning the weights, we use two com-
ponents of the co-occurrence model Co(W,T ) to
represent the co-occurrence probability of W and T ,
rather than use Co(W,T ) itself. In the rest of the
paper, we will call them labelling model and gener-
ating model respectively.
5 Decoder
Sequence segmentation and labelling problem can
be solved through a viterbi style decoding proce-
dure. In Chinese Joint S&T, the mission of the de-
coder is to find the boundary-POS labelled sequence
with the highest score. Given a Chinese character
sequence C1:n, the decoding procedure can proceed
in a left-right fashion with a dynamic programming
approach. By maintaining a stack of size N at each
position i of the sequence, we can preserve the top N
best candidate labelled results of subsequence C1:i
during decoding. At each position i, we enumer-
ate all possible word-POS pairs by assigning each
POS to each possible word formed from the charac-
ter subsequence spanning length l = 1..min(i,K)
(K is assigned 20 in all our experiments) and ending
at position i, then we derive all candidate results by
attaching each word-POS pair p (of length l) to the
tail of each candidate result at the prior position of p
(position i? l), and select for position i a N -best list
of candidate results from all these candidates. When
we derive a candidate result from a word-POS pair
p and a candidate q at prior position of p, we cal-
culate the scores of the word LM, the POS LM, the
labelling probability and the generating probability,
901
Algorithm 2 Decoding algorithm.
1: Input: character sequence C1:n
2: for i? 1 .. n do
3: L ? ?
4: for l? 1 .. min(i, K) do
5: w ? Ci?l+1:i
6: for t ? POS do
7: p? label w as t
8: for q ? V[i? l] do
9: append D(q, p) to L
10: sort L
11: V[i]? L[1 : N ]
12: Output: n-best results V[n]
as well as the score of the perceptron model. In ad-
dition, we add the score of the word count penalty as
another feature to alleviate the tendency of LMs to
favor shorter candidates. By equation 2, we can syn-
thetically evaluate all these scores to perform more
accurately comparing between candidates.
Algorithm 2 shows the decoding algorithm.
Lines 3 ? 11 generate a N -best list for each char-
acter position i. Line 4 scans words of all possible
lengths l (l = 1..min(i,K), where i points to the
current considering character). Line 6 enumerates
all POS?s for the word w spanning length l and end-
ing at position i. Line 8 considers each candidate
result in N -best list at prior position of the current
word. Function D derives the candidate result from
the word-POS pair p and the candidate q at prior po-
sition of p.
6 Experiments
We reported results from two set of experiments.
The first was conducted to test the performance of
the perceptron on segmentation on the corpus from
SIGHAN Bakeoff 2, including the Academia Sinica
Corpus (AS), the Hong Kong City University Cor-
pus (CityU), the Peking University Corpus (PKU)
and the Microsoft Research Corpus (MSR). The sec-
ond was conducted on the Penn Chinese Treebank
5.0 (CTB5.0) to test the performance of the cascaded
model on segmentation and Joint S&T. In all ex-
periments, we use the averaged parameters for the
perceptrons, and F-measure as the accuracy mea-
sure. With precision P and recall R, the balance
F-measure is defined as: F = 2PR/(P + R).
 0.966
 0.968
 0.97
 0.972
 0.974
 0.976
 0.978
 0.98
 0.982
 0.984
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
su
re
number of iterations
Perceptron Learning Curve
Non-lex + avg
Lex + avg
Figure 3: Averaged perceptron learning curves with Non-
lexical-target and Lexical-target feature templates.
AS CityU PKU MSR
SIGHAN best 0.952 0.943 0.950 0.964
Zhang & Clark 0.946 0.951 0.945 0.972
our model 0.954 0.958 0.940 0.975
Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN
best: best scores SIGHAN reported on the four corpus,
cited from Zhang and Clark (2007).
6.1 Experiments on SIGHAN Bakeoff
For convenience of comparing with others, we focus
only on the close test, which means that any extra
resource is forbidden except the designated train-
ing corpus. In order to test the performance of the
lexical-target templates and meanwhile determine
the best iterations over the training corpus, we ran-
domly chosen 2, 000 shorter sentences (less than 50
words) as the development set and the rest as the
training set (84, 294 sentences), then trained a per-
ceptron model named NON-LEX using only non-
lexical-target features and another named LEX us-
ing both the two kinds of features. Figure 3 shows
their learning curves depicting the F-measure on the
development set after 1 to 10 training iterations. We
found that LEX outperforms NON-LEX with a mar-
gin of about 0.002 at each iteration, and its learn-
ing curve reaches a tableland at iteration 7. Then
we trained LEX on each of the four corpora for 7
iterations. Test results listed in Table 2 shows that
this model obtains higher accuracy than the best of
SIGHAN Bakeoff 2 in three corpora (AS, CityU
and MSR). On the three corpora, it also outper-
formed the word-based perceptron model of Zhang
and Clark (2007). However, the accuracy on PKU
corpus is obvious lower than the best score SIGHAN
902
Training setting Test task F-measure
POS- Segmentation 0.971
POS+ Segmentation 0.973
POS+ Joint S&T 0.925
Table 3: F-measure on segmentation and Joint S&T of
perceptrons. POS-: perceptron trained without POS,
POS+: perceptron trained with POS.
reported, we need to conduct further research on this
problem.
6.2 Experiments on CTB5.0
We turned to experiments on CTB 5.0 to test the per-
formance of the cascaded model. According to the
usual practice in syntactic analysis, we choose chap-
ters 1? 260 (18074 sentences) as training set, chap-
ter 271? 300 (348 sentences) as test set and chapter
301? 325 (350 sentences) as development set.
At the first step, we conducted a group of contrast-
ing experiments on the core perceptron, the first con-
centrated on the segmentation regardless of the POS
information and reported the F-measure on segmen-
tation only, while the second performed Joint S&T
using POS information and reported the F-measure
both on segmentation and on Joint S&T. Note that
the accuracy of Joint S&T means that a word-POS
pair is recognized only if both the boundary tags and
the POS?s are correctly labelled.
The evaluation results are shown in Table 3. We
find that Joint S&T can also improve the segmen-
tation accuracy. However, the F-measure on Joint
S&T is obvious lower, about a rate of 95% to the
F-measure on segmentation. Similar trend appeared
in experiments of Ng and Low (2004), where they
conducted experiments on CTB 3.0 and achieved F-
measure 0.919 on Joint S&T, a ratio of 96% to the
F-measure 0.952 on segmentation.
As the next step, a group of experiments were
conducted to investigate how well the cascaded lin-
ear model performs. Here the core perceptron was
just the POS+ model in experiments above. Be-
sides this perceptron, other sub-models are trained
and used as additional features of the outside-layer
linear model. We used SRI Language Modelling
Toolkit (Stolcke and Andreas, 2002) to train a 3-
gram word LM with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), and a 4-gram POS
Features Segmentation F1 Joint S&T F1
All 0.9785 0.9341
All - PER 0.9049 0.8432
All - WLM 0.9785 0.9340
All - PLM 0.9752 0.9270
All - GPR 0.9774 0.9329
All - LPR 0.9765 0.9321
All - LEN 0.9772 0.9325
Table 4: Contribution of each feture. ALL: all features,
PER: perceptron model, WLM: word language model,
PLM: POS language model, GPR: generating model,
LPR: labelling model, LEN: word count penalty.
LM with Witten-Bell smoothing, and we trained
a word-POS co-occurrence model simply by MLE
without smoothing. To obtain their corresponding
weights, we adapted the minimum-error-rate train-
ing algorithm (Och, 2003) to train the outside-layer
model. In order to inspect how much improvement
each feature brings into the cascaded model, every
time we removed a feature while retaining others,
then retrained the model and tested its performance
on the test set.
Table 4 shows experiments results. We find that
the cascaded model achieves a F-measure increment
of about 0.5 points on segmentation and about 0.9
points on Joint S&T, over the perceptron-only model
POS+. We also find that the perceptron model func-
tions as the kernel of the outside-layer linear model.
Without the perceptron, the cascaded model (if we
can still call it ?cascaded?) performs poorly on both
segmentation and Joint S&T. Among other features,
the 4-gram POS LM plays the most important role,
removing this feature causes F-measure decrement
of 0.33 points on segmentation and 0.71 points on
Joint S&T. Another important feature is the labelling
model. Without it, the F-measure on segmentation
and Joint S&T both suffer a decrement of 0.2 points.
The generating model, which functions as that in
HMM, brings an improvement of about 0.1 points
to each test item. However unlike the three fea-
tures, the word LM brings very tiny improvement.
We suppose that the character-based features used
in the perceptron play a similar role as the lower-
order word LM, and it would be helpful if we train
a higher-order word LM on a larger scale corpus.
Finally, the word count penalty gives improvement
to the cascaded model, 0.13 points on segmentation
903
and 0.16 points on Joint S&T.
In summary, the cascaded model can utilize these
knowledge sources effectively, without causing the
feature space of the percptron becoming even larger.
Experimental results show that, it achieves obvious
improvement over the perceptron-only model, about
from 0.973 to 0.978 on segmentation, and from
0.925 to 0.934 on Joint S&T, with error reductions
of 18.5% and 12% respectively.
7 Conclusions
We proposed a cascaded linear model for Chinese
Joint S&T. Under this model, many knowledge
sources that may be intractable to be incorporated
into the perceptron directly, can be utilized effec-
tively in the outside-layer linear model. This is a
substitute method to use both local and non-local
features, and it would be especially useful when the
training corpus is very large.
However, can the perceptron incorporate all the
knowledge used in the outside-layer linear model?
If this cascaded linear model were chosen, could
more accurate generative models (LMs, word-POS
co-occurrence model) be obtained by training on
large scale corpus even if the corpus is not correctly
labelled entirely, or by self-training on raw corpus in
a similar approach to that of McClosky (2006)? In
addition, all knowledge sources we used in the core
perceptron and the outside-layer linear model come
from the training corpus, whereas many open knowl-
edge sources (lexicon etc.) can be used to improve
performance (Ng and Low, 2004). How can we uti-
lize these knowledge sources effectively? We will
investigate these problems in the following work.
Acknowledgement
This work was done while L. H. was visiting
CAS/ICT. The authors were supported by National
Natural Science Foundation of China, Contracts
60736014 and 60573188, and 863 State Key Project
No. 2006AA010108 (W. J., Q. L., and Y. L.), and by
NSF ITR EIA-0205456 (L. H.). We would also like
to Hwee-Tou Ng for sharing his code, and Yang Liu
and Yun Huang for suggestions.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th ICML, pages 282?289, Mas-
sachusetts, USA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of ACL 2006.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Lawrence. R. Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Conference.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 311?318.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
904
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522?530,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging ? A Case Study
Wenbin Jiang ? Liang Huang ? Qun Liu ?
?Key Lab. of Intelligent Information Processing ?Google Research
Institute of Computing Technology 1350 Charleston Rd.
Chinese Academy of Sciences Mountain View, CA 94043, USA
P.O. Box 2704, Beijing 100190, China lianghuang@google.com
{jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.com
Abstract
Manually annotated corpora are valuable
but scarce resources, yet for many anno-
tation tasks such as treebanking and se-
quence labeling there exist multiple cor-
pora with different and incompatible anno-
tation guidelines or standards. This seems
to be a great waste of human efforts, and
it would be nice to automatically adapt
one annotation standard to another. We
present a simple yet effective strategy that
transfers knowledge from a differently an-
notated corpus to the corpus with desired
annotation. We test the efficacy of this
method in the context of Chinese word
segmentation and part-of-speech tagging,
where no segmentation and POS tagging
standards are widely accepted due to the
lack of morphology in Chinese. Experi-
ments show that adaptation from the much
larger People?s Daily corpus to the smaller
but more popular Penn Chinese Treebank
results in significant improvements in both
segmentation and tagging accuracies (with
error reductions of 30.2% and 14%, re-
spectively), which in turn helps improve
Chinese parsing accuracy.
1 Introduction
Much of statistical NLP research relies on some
sort of manually annotated corpora to train their
models, but these resources are extremely expen-
sive to build, especially at a large scale, for ex-
ample in treebanking (Marcus et al, 1993). How-
ever the linguistic theories underlying these anno-
tation efforts are often heavily debated, and as a re-
sult there often exist multiple corpora for the same
task with vastly different and incompatible anno-
tation philosophies. For example just for English
treebanking there have been the Chomskian-style
{1 B2 o3 ?4 ?5 u6
NR NN VV NR
U.S. Vice-President visited China
{1 B2 o3 ?4 ?5 u6
ns b n v
U.S. Vice President visited-China
Figure 1: Incompatible word segmentation and
POS tagging standards between CTB (upper) and
People?s Daily (below).
Penn Treebank (Marcus et al, 1993) the HPSG
LinGo Redwoods Treebank (Oepen et al, 2002),
and a smaller dependency treebank (Buchholz and
Marsi, 2006). A second, related problem is that
the raw texts are also drawn from different do-
mains, which for the above example range from
financial news (PTB/WSJ) to transcribed dialog
(LinGo). These two problems seem be a great
waste in human efforts, and it would be nice if
one could automatically adapt from one annota-
tion standard and/or domain to another in order
to exploit much larger datasets for better train-
ing. The second problem, domain adaptation, is
very well-studied, e.g. by Blitzer et al (2006)
and Daume? III (2007) (and see below for discus-
sions), so in this paper we focus on the less stud-
ied, but equally important problem of annotation-
style adaptation.
We present a very simple yet effective strategy
that enables us to utilize knowledge from a differ-
ently annotated corpora for the training of a model
on a corpus with desired annotation. The basic
idea is very simple: we first train on a source cor-
pus, resulting in a source classifier, which is used
to label the target corpus and results in a ?source-
style? annotation of the target corpus. We then
522
train a second model on the target corpus with the
first classifier?s prediction as additional features
for guided learning.
This method is very similar to some ideas in
domain adaptation (Daume? III and Marcu, 2006;
Daume? III, 2007), but we argue that the underly-
ing problems are quite different. Domain adapta-
tion assumes the labeling guidelines are preserved
between the two domains, e.g., an adjective is al-
ways labeled as JJ regardless of from Wall Street
Journal (WSJ) or Biomedical texts, and only the
distributions are different, e.g., the word ?control?
is most likely a verb in WSJ but often a noun
in Biomedical texts (as in ?control experiment?).
Annotation-style adaptation, however, tackles the
problem where the guideline itself is changed, for
example, one treebank might distinguish between
transitive and intransitive verbs, while merging the
different noun types (NN, NNS, etc.), and for ex-
ample one treebank (PTB) might be much flatter
than the other (LinGo), not to mention the fun-
damental disparities between their underlying lin-
guistic representations (CFG vs. HPSG). In this
sense, the problem we study in this paper seems
much harder and more motivated from a linguistic
(rather than statistical) point of view. More inter-
estingly, our method, without any assumption on
the distributions, can be simultaneously applied to
both domain and annotation standards adaptation
problems, which is very appealing in practice be-
cause the latter problem often implies the former,
as in our case study.
To test the efficacy of our method we choose
Chinese word segmentation and part-of-speech
tagging, where the problem of incompatible an-
notation standards is one of the most evident: so
far no segmentation standard is widely accepted
due to the lack of a clear definition of Chinese
words, and the (almost complete) lack of mor-
phology results in much bigger ambiguities and
heavy debates in tagging philosophies for Chi-
nese parts-of-speech. The two corpora used in
this study are the much larger People?s Daily (PD)
(5.86M words) corpus (Yu et al, 2001) and the
smaller but more popular Penn Chinese Treebank
(CTB) (0.47M words) (Xue et al, 2005). They
used very different segmentation standards as well
as different POS tagsets and tagging guidelines.
For example, in Figure 1, People?s Daily breaks
?Vice-President? into two words while combines
the phrase ?visited-China? as a compound. Also
CTB has four verbal categories (VV for normal
verbs, and VC for copulas, etc.) while PD has only
one verbal tag (v) (Xia, 2000). It is preferable to
transfer knowledge from PD to CTB because the
latter also annotates tree structures which is very
useful for downstream applications like parsing,
summarization, and machine translation, yet it is
much smaller in size. Indeed, many recent efforts
on Chinese-English translation and Chinese pars-
ing use the CTB as the de facto segmentation and
tagging standards, but suffers from the limited size
of training data (Chiang, 2007; Bikel and Chiang,
2000). We believe this is also a reason why state-
of-the-art accuracy for Chinese parsing is much
lower than that of English (CTB is only half the
size of PTB).
Our experiments show that adaptation from PD
to CTB results in a significant improvement in seg-
mentation and POS tagging, with error reductions
of 30.2% and 14%, respectively. In addition, the
improved accuracies from segmentation and tag-
ging also lead to an improved parsing accuracy on
CTB, reducing 38% of the error propagation from
word segmentation to parsing. We envision this
technique to be general and widely applicable to
many other sequence labeling tasks.
In the rest of the paper we first briefly review
the popular classification-based method for word
segmentation and tagging (Section 2), and then
describe our idea of annotation adaptation (Sec-
tion 3). We then discuss other relevant previous
work including co-training and classifier combina-
tion (Section 4) before presenting our experimen-
tal results (Section 5).
2 Segmentation and Tagging as
Character Classification
Before describing the adaptation algorithm, we
give a brief introduction of the baseline character
classification strategy for segmentation, as well as
joint segmenation and tagging (henceforth ?Joint
S&T?). following our previous work (Jiang et al,
2008). Given a Chinese sentence as sequence of n
characters:
C1 C2 .. Cn
where Ci is a character, word segmentation aims
to split the sequence into m(? n) words:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
where each subsequence Ci:j indicates a Chinese
word spanning from characters Ci to Cj (both in-
523
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
clusive). While in Joint S&T, each word is further
annotated with a POS tag:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
where tk(k = 1..m) denotes the POS tag for the
word Cek?1+1:ek .
2.1 Character Classification Method
Xue and Shen (2003) describe for the first time
the character classification approach for Chinese
word segmentation, where each character is given
a boundary tag denoting its relative position in a
word. In Ng and Low (2004), Joint S&T can also
be treated as a character classification problem,
where a boundary tag is combined with a POS tag
in order to give the POS information of the word
containing these characters. In addition, Ng and
Low (2004) find that, compared with POS tagging
after word segmentation, Joint S&T can achieve
higher accuracy on both segmentation and POS
tagging. This paper adopts the tag representation
of Ng and Low (2004). For word segmentation
only, there are four boundary tags:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
while for Joint S&T, a POS tag is attached to the
tail of a boundary tag, to incorporate the word
boundary information and POS information to-
gether. For example, b-NN indicates that the char-
acter is the begin of a noun. After all charac-
ters of a sentence are assigned boundary tags (or
with POS postfix) by a classifier, the correspond-
ing word sequence (or with POS) can be directly
derived. Take segmentation for example, a char-
acter assigned a tag s or a subsequence of words
assigned a tag sequence bm?e indicates a word.
2.2 Training Algorithm and Features
Now we will show the training algorithm of the
classifier and the features used. Several classi-
fication models can be adopted here, however,
we choose the averaged perceptron algorithm
(Collins, 2002) because of its simplicity and high
accuracy. It is an online training algorithm and
has been successfully used in many NLP tasks,
such as POS tagging (Collins, 2002), parsing
(Collins and Roark, 2004), Chinese word segmen-
tation (Zhang and Clark, 2007; Jiang et al, 2008),
and so on.
Similar to the situation in other sequence label-
ing problems, the training procedure is to learn a
discriminative model mapping from inputs x ? X
to outputs y ? Y , where X is the set of sentences
in the training corpus and Y is the set of corre-
sponding labelled results. Following Collins, we
use a function GEN(x) enumerating the candi-
date results of an input x , a representation?map-
ping each training example (x, y) ? X ? Y to a
feature vector?(x, y) ? Rd, and a parameter vec-
tor ~? ? Rd corresponding to the feature vector.
For an input character sequence x, we aim to find
an output F (x) that satisfies:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
where?(x, y) ?~? denotes the inner product of fea-
ture vector ?(x, y) and the parameter vector ~?.
Algorithm 1 depicts the pseudo code to tune the
parameter vector ~?. In addition, the ?averaged pa-
rameters? technology (Collins, 2002) is used to al-
leviate overfitting and achieve stable performance.
Table 1 lists the feature template and correspond-
ing instances. Following Ng and Low (2004),
the current considering character is denoted as C0,
while the ith character to the left of C0 as C?i,
and to the right as Ci. There are additional two
functions of which each returns some property of a
character. Pu(?) is a boolean function that checks
whether a character is a punctuation symbol (re-
turns 1 for a punctuation, 0 for not). T (?) is a
multi-valued function, it classifies a character into
four classifications: number, date, English letter
and others (returns 1, 2, 3 and 4, respectively).
3 Automatic Annotation Adaptation
From this section, several shortened forms are
adopted for representation inconvenience. We use
source corpus to denote the corpus with the anno-
tation standard that we don?t require, which is of
524
Feature Template Instances
Ci (i = ?2..2) C?2 =?, C?1 =, C0 =c, C1 =?, C2 = R
CiCi+1 (i = ?2..1) C?2C?1 =?, C?1C0 =c, C0C1 =c?, C1C2 =?R
C?1C1 C?1C1 =?
Pu(C0) Pu(C0) = 0
T (C?2)T (C?1)T (C0)T (C1)T (C2) T (C?2)T (C?1)T (C0)T (C1)T (C2) = 11243
Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004). Suppose we are
considering the third character ?c? in ?? c ?R?.
course the source of the adaptation, while target
corpus denoting the corpus with the desired stan-
dard. And correspondingly, the two annotation
standards are naturally denoted as source standard
and target standard, while the classifiers follow-
ing the two annotation standards are respectively
named as source classifier and target classifier, if
needed.
Considering that word segmentation and Joint
S&T can be conducted in the same character clas-
sification manner, we can design an unified stan-
dard adaptation framework for the two tasks, by
taking the source classifier?s classification result
as the guide information for the target classifier?s
classification decision. The following section de-
picts this adaptation strategy in detail.
3.1 General Adaptation Strategy
In detail, in order to adapt knowledge from the
source corpus, first, a source classifier is trained
on it and therefore captures the knowledge it con-
tains; then, the source classifier is used to clas-
sify the characters in the target corpus, although
the classification result follows a standard that we
don?t desire; finally, a target classifier is trained
on the target corpus, with the source classifier?s
classification result as additional guide informa-
tion. The training procedure of the target clas-
sifier automatically learns the regularity to trans-
fer the source classifier?s predication result from
source standard to target standard. This regular-
ity is incorporated together with the knowledge
learnt from the target corpus itself, so as to ob-
tain enhanced predication accuracy. For a given
un-classified character sequence, the decoding is
analogous to the training. First, the character se-
quence is input into the source classifier to ob-
tain an source standard annotated classification
result, then it is input into the target classifier
with this classification result as additional infor-
mation to get the final result. This coincides with
the stacking method for combining dependency
parsers (Martins et al, 2008; Nivre and McDon-
source corpus
train with
normal features
source classifier
train with
additional features
target classifier
target corpus source annotation
classification result
Figure 2: The pipeline for training.
raw sentence source classifier source annotation
classification result
target classifier
target annotation
classification result
Figure 3: The pipeline for decoding.
ald, 2008), and is also similar to the Pred baseline
for domain adaptation in (Daume? III and Marcu,
2006; Daume? III, 2007). Figures 2 and 3 show
the flow charts for training and decoding.
The utilization of the source classifier?s classi-
fication result as additional guide information re-
sorts to the introduction of new features. For the
current considering character waiting for classi-
fication, the most intuitive guide features is the
source classifier?s classification result itself. How-
ever, our effort isn?t limited to this, and more spe-
cial features are introduced: the source classifier?s
classification result is attached to every feature
listed in Table 1 to get combined guide features.
This is similar to feature design in discriminative
dependency parsing (McDonald et al, 2005; Mc-
525
Donald and Pereira, 2006), where the basic fea-
tures, composed of words and POSs in the context,
are also conjoined with link direction and distance
in order to obtain more special features. Table 2
shows an example of guide features and basic fea-
tures, where ?? = b ? represents that the source
classifier classifies the current character as b, the
beginning of a word.
Such combination method derives a series of
specific features, which helps the target classifier
to make more precise classifications. The parame-
ter tuning procedure of the target classifier will au-
tomatically learn the regularity of using the source
classifier?s classification result to guide its deci-
sion making. For example, if a current consid-
ering character shares some basic features in Ta-
ble 2 and it is classified as b, then the target clas-
sifier will probably classify it as m. In addition,
the training procedure of the target classifier also
learns the relative weights between the guide fea-
tures and the basic features, so that the knowledge
from both the source corpus and the target corpus
are automatically integrated together.
In fact, more complicated features can be
adopted as guide information. For error tolerance,
guide features can be extracted from n-best re-
sults or compacted lattices of the source classifier;
while for the best use of the source classifier?s out-
put, guide features can also be the classification
results of several successive characters. We leave
them as future research.
4 Related Works
Co-training (Sarkar, 2001) and classifier com-
bination (Nivre and McDonald, 2008) are two
technologies for training improved dependency
parsers. The co-training technology lets two dif-
ferent parsing models learn from each other dur-
ing parsing an unlabelled corpus: one model
selects some unlabelled sentences it can confi-
dently parse, and provide them to the other model
as additional training corpus in order to train
more powerful parsers. The classifier combina-
tion lets graph-based and transition-based depen-
dency parsers to utilize the features extracted from
each other?s parsing results, to obtain combined,
enhanced parsers. The two technologies aim to
let two models learn from each other on the same
corpora with the same distribution and annota-
tion standard, while our strategy aims to integrate
the knowledge in multiple corpora with different
Baseline Features
C?2 ={
C?1 =B
C0 =o
C1 =?
C2 =?
C?2C?1 ={B
C?1C0 =Bo
C0C1 =o?
C1C2 =??
C?1C1 =B?
Pu(C0) = 0
T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444
Guide Features
? = b
C?2 ={ ? ? = b
C?1 =B ? ? = b
C0 =o ? ? = b
C1 =? ? ? = b
C2 =? ? ? = b
C?2C?1 ={B ? ? = b
C?1C0 =Bo ? ? = b
C0C1 =o? ? ? = b
C1C2 =?? ? ? = b
C?1C1 =B? ? ? = b
Pu(C0) = 0 ? ? = b
T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444 ? ? = b
Table 2: An example of basic features and guide
features of standard-adaptation for word segmen-
tation. Suppose we are considering the third char-
acter ?o? in ?{B o ??u?.
annotation-styles.
Gao et al (2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
They design some class-type transformation tem-
plates and use the transformation-based error-
driven learning method of Brill (1995) to learn
what word delimiters should be modified. How-
ever, this converter need human designed transfor-
mation templates, and is hard to be generalized to
POS tagging, not to mention other structure label-
ing tasks. Moreover, the processing procedure is
divided into two isolated steps, conversion after
segmentation, which suffers from error propaga-
tion and wastes the knowledge in the corpora. On
the contrary, our strategy is automatic, generaliz-
able and effective.
In addition, many efforts have been devoted
to manual treebank adaptation, where they adapt
PTB to other grammar formalisms, such as such
as CCG and LFG (Hockenmaier and Steedman,
2008; Cahill and Mccarthy, 2007). However, they
are heuristics-based and involve heavy human en-
gineering.
526
5 Experiments
Our adaptation experiments are conducted from
People?s Daily (PD) to Penn Chinese Treebank 5.0
(CTB). These two corpora are segmented follow-
ing different segmentation standards and labeled
with different POS sets (see for example Figure 1).
PD is much bigger in size, with about 100K sen-
tences, while CTB is much smaller, with only
about 18K sentences. Thus a classifier trained on
CTB usually falls behind that trained on PD, but
CTB is preferable because it also annotates tree
structures, which is very useful for downstream
applications like parsing and translation. For ex-
ample, currently, most Chinese constituency and
dependency parsers are trained on some version
of CTB, using its segmentation and POS tagging
as the de facto standards. Therefore, we expect the
knowledge adapted from PD will lead to more pre-
cise CTB-style segmenter and POS tagger, which
would in turn reduce the error propagation to pars-
ing (and translation).
Experiments adapting from PD to CTB are con-
ducted for two tasks: word segmentation alone,
and joint segmentation and POS tagging (Joint
S&T). The performance measurement indicators
for word segmentation and Joint S&T are bal-
anced F-measure, F = 2PR/(P +R), a function
of Precision P and Recall R. For word segmen-
tation, P indicates the percentage of words in seg-
mentation result that are segmented correctly, and
R indicates the percentage of correctly segmented
words in gold standard words. For Joint S&T, P
and R mean nearly the same except that a word
is correctly segmented only if its POS is also cor-
rectly labelled.
5.1 Baseline Perceptron Classifier
We first report experimental results of the single
perceptron classifier on CTB 5.0. The original
corpus is split according to former works: chap-
ters 271 ? 300 for testing, chapters 301 ? 325 for
development, and others for training. Figure 4
shows the learning curves for segmentation only
and Joint S&T, we find all curves tend to moder-
ate after 7 iterations. The data splitting conven-
tion of other two corpora, People?s Daily doesn?t
reserve the development sets, so in the following
experiments, we simply choose the model after 7
iterations when training on this corpus.
The first 3 rows in each sub-table of Table 3
show the performance of the single perceptron
0.880
0.890
0.900
0.910
0.920
0.930
0.940
0.950
0.960
0.970
0.980
 1  2  3  4  5  6  7  8  9  10
F 
m
ea
su
re
number of iterations
segmentation only
segmentation in Joint S&T
Joint S&T
Figure 4: Averaged perceptron learning curves for
segmentation and Joint S&T.
Train on Test on Seg F1% JST F1%
Word Segmentation
PD PD 97.45 ?
PD CTB 91.71 ?
CTB CTB 97.35 ?
PD ? CTB CTB 98.15 ?
Joint S&T
PD PD 97.57 94.54
PD CTB 91.68 ?
CTB CTB 97.58 93.06
PD ? CTB CTB 98.23 94.03
Table 3: Experimental results for both baseline
models and final systems with annotation adap-
tation. PD ? CTB means annotation adaptation
from PD to CTB. For the upper sub-table, items of
JST F1 are undefined since only segmentation is
performs. While in the sub-table below, JST F1
is also undefined since the model trained on PD
gives a POS set different from that of CTB.
models. Comparing row 1 and 3 in the sub-table
below with the corresponding rows in the upper
sub-table, we validate that when word segmenta-
tion and POS tagging are conducted jointly, the
performance for segmentation improves since the
POS tags provide additional information to word
segmentation (Ng and Low, 2004). We also see
that for both segmentation and Joint S&T, the per-
formance sharply declines when a model trained
on PD is tested on CTB (row 2 in each sub-table).
In each task, only about 92% F1 is achieved. This
obviously fall behind those of the models trained
on CTB itself (row 3 in each sub-table), about 97%
F1, which are used as the baselines of the follow-
ing annotation adaptation experiments.
527
POS #Word #BaseErr #AdaErr ErrDec%
AD 305 30 19 36.67 ?
AS 76 0 0
BA 4 1 1
CC 135 8 8
CD 356 21 14 33.33 ?
CS 6 0 0
DEC 137 31 23 25.81 ?
DEG 197 32 37 ?
DEV 10 0 0
DT 94 3 1 66.67 ?
ETC 12 0 0
FW 1 1 1
JJ 127 41 44 ?
LB 2 1 1
LC 106 3 2 33.33 ?
M 349 18 4 77.78 ?
MSP 8 2 1 50.00 ?
NN 1715 151 126 16.56 ?
NR 713 59 50 15.25 ?
NT 178 1 2 ?
OD 84 0 0
P 251 10 6 40.00 ?
PN 81 1 1
PU 997 0 1 ?
SB 2 0 0
SP 2 2 2
VA 98 23 21 08.70 ?
VC 61 0 0
VE 25 1 0 100.00 ?
VV 689 64 40 37.50 ?
SUM 6821 213 169 20.66 ?
Table 4: Error analysis for Joint S&T on the devel-
oping set of CTB. #BaseErr and #AdaErr denote
the count of words that can?t be recalled by the
baseline model and adapted model, respectively.
ErrDec denotes the error reduction of Recall.
5.2 Adaptation for Segmentation and
Tagging
Table 3 also lists the results of annotation adap-
tation experiments. For word segmentation, the
model after annotation adaptation (row 4 in upper
sub-table) achieves an F-measure increment of 0.8
points over the baseline model, corresponding to
an error reduction of 30.2%; while for Joint S&T,
the F-measure increment of the adapted model
(row 4 in sub-table below) is 1 point, which cor-
responds to an error reduction of 14%. In addi-
tion, the performance of the adapted model for
Joint S&T obviously surpass that of (Jiang et al,
2008), which achieves an F1 of 93.41% for Joint
S&T, although with more complicated models and
features.
Due to the obvious improvement brought by an-
notation adaptation to both word segmentation and
Joint S&T, we can safely conclude that the knowl-
edge can be effectively transferred from on an-
Input Type Parsing F1%
gold-standard segmentation 82.35
baseline segmentation 80.28
adapted segmentation 81.07
Table 5: Chinese parsing results with different
word segmentation results as input.
notation standard to another, although using such
a simple strategy. To obtain further information
about what kind of errors be alleviated by annota-
tion adaptation, we conduct an initial error analy-
sis for Joint S&T on the developing set of CTB. It
is reasonable to investigate the error reduction of
Recall for each word cluster grouped together ac-
cording to their POS tags. From Table 4 we find
that out of 30 word clusters appeared in the devel-
oping set of CTB, 13 clusters benefit from the an-
notation adaptation strategy, while 4 clusters suf-
fer from it. However, the compositive error rate of
Recall for all word clusters is reduced by 20.66%,
such a fact invalidates the effectivity of annotation
adaptation.
5.3 Contribution to Chinese Parsing
We adopt the Chinese parser of Xiong et al
(2005), and train it on the training set of CTB 5.0
as described before. To sketch the error propaga-
tion to parsing from word segmentation, we rede-
fine the constituent span as a constituent subtree
from a start character to a end character, rather
than from a start word to a end word. Note that if
we input the gold-standard segmented test set into
the parser, the F-measure under the two definitions
are the same.
Table 5 shows the parsing accuracies with dif-
ferent word segmentation results as the parser?s
input. The parsing F-measure corresponding to
the gold-standard segmentation, 82.35, represents
the ?oracle? accuracy (i.e., upperbound) of pars-
ing on top of automatic word segmention. After
integrating the knowledge from PD, the enhanced
word segmenter gains an F-measure increment of
0.8 points, which indicates that 38% of the error
propagation from word segmentation to parsing is
reduced by our annotation adaptation strategy.
6 Conclusion and Future Works
This paper presents an automatic annotation adap-
tation strategy, and conducts experiments on a
classic problem: word segmentation and Joint
528
S&T. To adapt knowledge from a corpus with an
annotation standard that we don?t require, a clas-
sifier trained on this corpus is used to pre-process
the corpus with the desired annotated standard, on
which a second classifier is trained with the first
classifier?s predication results as additional guide
information. Experiments of annotation adapta-
tion from PD to CTB 5.0 for word segmentation
and POS tagging show that, this strategy can make
effective use of the knowledge from the corpus
with different annotations. It obtains considerable
F-measure increment, about 0.8 point for word
segmentation and 1 point for Joint S&T, with cor-
responding error reductions of 30.2% and 14%.
The final result outperforms the latest work on the
same corpus which uses more complicated tech-
nologies, and achieves the state-of-the-art. More-
over, such improvement further brings striking F-
measure increment for Chinese parsing, about 0.8
points, corresponding to an error propagation re-
duction of 38%.
In the future, we will continue to research on
annotation adaptation for other NLP tasks which
have different annotation-style corpora. Espe-
cially, we will pay efforts to the annotation stan-
dard adaptation between different treebanks, for
example, from HPSG LinGo Redwoods Treebank
to PTB, or even from a dependency treebank
to PTB, in order to obtain more powerful PTB
annotation-style parsers.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are especially grateful to
Fernando Pereira and the anonymous reviewers
for pointing us to relevant domain adaption refer-
ences. We also thank Yang Liu and Haitao Mi for
helpful discussions.
References
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. In Computational
Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Aoife Cahill and Mairead Mccarthy. 2007. Auto-
matic annotation of the penn treebank with lfg f-
structure information. In in Proceedings of the
LREC Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated
Language Data.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201?228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42th Annual Meeting of the Association
for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal of Artifi-
cial Intelligence Research.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings of ACL.
Julia Hockenmaier and Mark Steedman. 2008. Ccg-
bank: a corpus of ccg derivations and dependency
structures extracted from the penn treebank. In
Computational Linguistics, volume 33(3), pages
355?396.
Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
529
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics (COLING 2002).
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0). In Technical
Reports.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
530
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25?28,
Paris, October 2009. c?2009 Association for Computational Linguistics
Automatic Adaptation of Annotation Standards for Dependency Parsing
? Using Projected Treebank as Source Corpus
Wenbin Jiang and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{jiangwenbin, liuqun}@ict.ac.cn
Abstract
We describe for dependency parsing an an-
notation adaptation strategy, which can au-
tomatically transfer the knowledge from
a source corpus with a different annota-
tion standard to the desired target parser,
with the supervision by a target corpus an-
notated in the desired standard. Further-
more, instead of a hand-annotated one, a
projected treebank derived from a bilin-
gual corpus is used as the source cor-
pus. This benefits the resource-scarce
languages which haven?t different hand-
annotated treebanks. Experiments show
that the target parser gains significant im-
provement over the baseline parser trained
on the target corpus only, when the target
corpus is smaller.
1 Introduction
Automatic annotation adaptation for sequence la-
beling (Jiang et al, 2009) aims to enhance a
tagger with one annotation standard by transfer-
ring knowledge from a source corpus annotated in
another standard. It would be valuable to adapt
this strategy to parsing, since for some languages
there are also several treebanks with different an-
notation standards, such as Chomskian-style Penn
Treebank (Marcus et al, 1993) and HPSG LinGo
Redwoods Treebank (Oepen et al, 2002) for En-
glish. However, we are not content with conduct-
ing annotation adaptation between existing differ-
ent treebanks, because it would be more valuable
to boost the parsers also for the resource-scarce
languages, rather than only for the resource-rich
ones that already have several treebanks.
Although hand-annotated treebanks are costly
and scarce, it is not difficult for many languages to
collect large numbers of bilingual sentence-pairs
aligned to English. According to the word align-
ment, the English parses can be projected across
to their translations, and the projected trees can be
leveraged to boost parsing. Many efforts are de-
voted to the research on projected treebanks, such
as (Lu? et al, 2002), (Hwa et al, 2005) and
(Ganchev et al, 2009), etc. Considering the fact
that a projected treebank partially inherits the En-
glish annotation standard, some hand-written rules
are designed to deal with the divergence between
languages such as in (Hwa et al, 2002). How-
ever, it will be more valuable and interesting to
adapt this divergence automatically and boost the
existing parsers with this projected treebank.
In this paper, we investigate the automatic anno-
tation adaptation strategy for Chinese dependency
parsing, where the source corpus for adaptation is
a projected treebank derived from a bilingual cor-
pus aligned to English with word alignment and
English trees. We also propose a novel, error-
tolerant tree-projecting algorithm, which dynam-
ically searches the project Chinese tree that has
the largest consistency with the corresponding En-
glish tree, according to an alignment matrix rather
than a single alignment. Experiments show that
when the target corpus is smaller, the projected
Chinese treebank, although with inevitable noise
caused by non-literal translation and word align-
ment error, can be successfully utilized and re-
sult in significant improvement over the baseline
model trained on the target corpus only.
In the rest of the paper, we first present the tree-
projecting algorithm (section 2), and then the an-
notation adaptation strategy (section 3). After dis-
cussing the related work (section 4) we show the
experiments (section 5).
2 Error-Tolerant Tree-Projecting
Algorithm
Previous works making use of projected cor-
pus usually adopt the direct-mapping method for
structure projection (Yarowsky and Ngai, 2001;
Hwa et al, 2005; Ganchev et al, 2009), where
25
some filtering is needed to eliminate the inaccurate
or conflicting labels or dependency edges. Here
we propose a more robust algorithm for depen-
dency tree projection. According to the align-
ment matrix, this algorithm dynamically searches
the projected Chinese dependency tree which has
the largest consistency with the corresponding En-
glish tree.
We briefly introduce the alignment matrix be-
fore describing our projecting algorithm. Given
a Chinese sentence C1:M and its English transla-
tion E1:N , the alignment matrix A is an M ? N
matrix with each element Ai,j denoting the proba-
bility of Chinese word Ci aligned to English word
Ej . Such structure potentially encodes many more
possible alignments.
Using C(TC |TE , A) to denote the degree of Chi-
nese tree TC being consistent with English tree TE
according to alignment matrix A, the projecting al-
gorithm aims to find
T?C = argmax
TC
C(TC |TE , A) (1)
C(TC |TE , A) can be factorized into each depen-
dency edge x ? y in TC , that is to say
C(TC |TE , A) =
?
x?y?TC
Ce(x ? y|TE , A) (2)
We can obtain Ce by simple accumulation across
all possible alignments
Ce(x ? y|TE, A)
= ?
1?x?,y??|E|
Ax,x? ?Ay,y? ? ?(x?, y?|TE) (3)
where ?(x?, y?|TE) is a 0-1 function that equals 1
only if x? ? y? exists in TE .
The searching procedure, argmax operation in
equation 1, can be effectively solved by a simple,
bottom-up dynamic algorithm with cube-pruning
speed-up (Huang and Chiang, 2005). We omit the
detailed algorithm here due to space restrictions.
3 Annotation Adaptation for
Dependency Parsing
The automatic annotation adaptation strategy for
sequence labeling (Jiang et al, 2009) aims to
strengthen a tagger trained on a corpus annotated
in one annotation standard with a larger assistant
corpus annotated in another standard. We can de-
fine the purpose of the automatic annotation adap-
tation for dependency parsing in the same way.
Similar to that in sequence labeling, the train-
ing corpus with the desired annotation standard is
called the target corpus while the assistant cor-
pus annotated in a different standard is called
the source corpus. For training, an intermediate
parser, called the source parser, is trained directly
on the source corpus and then used to parse the tar-
get corpus. After that a second parser, called the
target parser, is trained on the target corpus with
guide features extracted from the source parser?s
parsing results. For testing, a token sequence is
first parsed by the source parser to obtain an inter-
mediate parsing result with the source annotation
standard, and then parsed by the target parser with
the guide features extracted from the intermediate
parsing result to obtain the final result.
The design of the guide features is the most im-
portant, and is specific to the parsing algorithm of
the target parser. In this work we adopt the max-
imum spanning tree (MST) algorithm (McDon-
ald et al, 2005; McDonald and Pereira, 2006) for
both the source and the target parser, so the guide
features should be defined on dependency edges
in accordance with the edge-factored property of
MST models. In the decoding procedure of the
target parser, the degree of a dependency edge be-
ing supported can be adjusted by the relationship
between this edge?s head and modifier in the in-
termediate parsing result of the source parser. The
most intuitionistic relationship is whether the de-
pendency between head and modifier exists in this
intermediate result. Such a bi-valued relationship
is similar to that in the stacking method for com-
bining dependency parsers (Martins et al, 2008;
Nivre and McDonald, 2008). The guide features
are then defined as this relationship itself as well as
its combinations with the lexical features of MST
models.
Furthermore, in order to explore more de-
tailed knowledge from the source parser, we re-
define the relationship as a four-valued variable
which covers the following situations: parent-
child, child-parent, siblings and else. With the
guide features, the parameter tuning procedure of
the target parser will automatically learn the regu-
larity of using the source parser?s intermediate re-
sult to guide its decision making.
4 Related Works
Many works have been devoted to obtain pars-
ing knowledge from word aligned bilingual cor-
26
pora. (Lu? et al, 2002) learns Chinese bracket-
ing knowledge via ITG alignment; (Hwa et al,
2005) and (Ganchev et al, 2009) induces depen-
dency grammar via projection from aligned En-
glish, where some filtering is used to reduce the
noise and some hand-designed rules to handle lan-
guage heterogeneity.
Just recently, Smith and Eisner (2009) gave
an idea similar to ours. They perform depen-
dency projection and annotation adaptation with
Quasi-Synchronous Grammar (QG) Features. Al-
though both related to projection and annotation,
there are still important differences between these
two works. First, we design an error-tolerant
alignment-matrix-based tree-projecting algorithm
to perform whole-tree projection, while they re-
sort to QG features to score local configurations
of aligned source and target trees. Second, their
adaptation emphasizes to transform a tree from
one annotation standard to another, while our
adaptation emphasizes to strengthen the parser us-
ing a treebank annotated in a different standard.
5 Experiments
The source corpus for annotation adaptation, that
is, the projected Chinese treebank, is derived from
5.6 millions LDC Chinese-English sentence pairs.
The Chinese side of the bilingual corpus is word-
segmented and POS-tagged by an implementation
of (Jiang et al, 2008), and the English sentences
are parsed by an implementation of (McDonald
and Pereira, 2006) which is instead trained on WSJ
section of Penn English Treebank (Marcus et al,
1993). The alignment matrixes for sentence pairs
are obtained according to (Liu et al, 2009). The
English trees are then projected across to Chinese
using the algorithm in section 2. Out of these pro-
jected trees, we only select 500 thousands with
word count l s.t. 6 ? l ? 100 and with project-
ing confidence c = C(TC |TE , A)1/l s.t. c ? 0.35.
While for the target corpus, we take Penn Chinese
Treebank (CTB) 1.0 and CTB 5.0 (Xue et al,
2005) respectively, and follow the traditional cor-
pus splitting: chapters 271-300 for testing, chap-
ters 301-325 for development, and else for train-
ing.
We adopt the 2nd-order MST model (McDon-
ald et al, 2005) as the target parser for better
performance, and the 1st-order MST model as
the source parser for fast training. Both the two
parsers are trained with averaged perceptron algo-
Model P% on CTB 1 P% on CTB 5
source parser 53.28 53.28
target parser 83.56 87.34
baseline parser 82.23 87.15
Table 1: Performances of annotation adaptation
with CTB 1.0 and CTB 5.0 as the target corpus re-
spectively, as well as of the baseline parsers (2nd-
order MST parsers trained on the target corpora).
 0.7
 0.75
 0.8
 0.85
 100  1000  10000
de
pe
nd
en
cy
 a
cc
ur
ac
y
sentence count of target corpus
baseline
target parser
Figure 1: Performance of the target parsers with
target corpora of different scales.
rithm (Collins, 2002). The development set of
CTB is also used to determine the best model for
the source parser, conditioned on the hypothesis
of larger isomorphisme between Chinese and En-
glish.
Table 1 shows that the experimental results of
annotation adaptation, with CTB 1.0 and CTB 5.0
as the target corpus respectively. We can see that
the source parsers, directly trained on the source
corpora of projected trees, performs poorly on
both CTB test sets (which are in fact the same).
This is partly due to the noise in the projected tree-
bank, and partly due to the heterogeneous between
the CTB trees and the projected trees. On the
contrary, automatic annotation adaptation effec-
tively transfers the knowledge to the target parsers,
achieving improvement on both target corpora.
Especially on CTB 1.0, an accuracy increment of
1.3 points is obtained over the baseline parser.
We observe that for the much larger CTB 5.0,
the performance of annotation adaptation is much
lower. To further investigate the adaptation perfor-
mances with target corpora of different scales, we
conduct annotation adaptation on a series of tar-
get corpora which consist of different amount of
dependency trees from CTB 5.0. Curves in Fig-
ure 1 shows the experimental results. We see that
the smaller the training corpus is, the more signif-
icant improvement can be obtained. For example,
27
with a target corpus composed of 2K trees, nearly
2 points of accuracy increment is achieved. This
is a good news to the resource-scarce languages.
6 Conclusion and Future Works
This paper describes for dependency parsing an
automatic annotation adaptation strategy. What
is more important, we use a projected treebank,
rather than a hand-annotated one, as the source
corpus for adaptation. This is quite different from
previous works on projected trees (Hwa et al,
2005; Ganchev et al, 2009), and is also more valu-
able than previous works of annotation adaptation
(Jiang et al, 2009). Experiments show that this
strategy gains improvement over baseline parsers
with target corpora of different scales, especially
the smaller ones. This provides a new strategy for
resource-scarce languages to train high-precision
dependency parsers. In the future, we will adapt
this strategy to constituent parsing, which is more
challenging and interesting due to the complexity
of projection between constituent trees, and due
to the obscurity of annotation adaptation for con-
stituent parsing.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are grateful to the anony-
mous reviewers for their valuable suggestions. We
also thank Yang Liu for sharing his codes of align-
ment matrix generation, and Liang Huang and
Haitao Mi for helpful discussions.
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering, volume 11, pages
311?325.
Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002. Learning chinese bracketing knowledge
based on a bilingual language model. In Proceed-
ings of the COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of COLING.
David Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of the NAACL.
28
Coling 2010: Poster Volume, pages 516?524,
Beijing, August 2010
Effective Constituent Projection across Languages
Wenbin Jiang and Yajuan Lu? and Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn
Abstract
We describe an effective constituent pro-
jection strategy, where constituent pro-
jection is performed on the basis of de-
pendency projection. Especially, a novel
measurement is proposed to evaluate the
candidate projected constituents for a tar-
get language sentence, and a PCFG-style
parsing procedure is then used to search
for the most probable projected con-
stituent tree. Experiments show that, the
parser trained on the projected treebank
can significantly boost a state-of-the-art
supervised parser. When integrated into a
tree-based machine translation system, the
projected parser leads to translation per-
formance comparable with using a super-
vised parser trained on thousands of anno-
tated trees.
1 Introduction
In recent years, supervised constituent parsing has
been well studied and achieves the state-of-the-art
for many resource-rich languages (Collins, 1999;
Charniak, 2000; Petrov et al, 2006). Because
of the cost and difficulty in treebank construc-
tion, researchers have also investigated the utiliza-
tion of unannotated text, including the unsuper-
vised parsing which totally uses unannotated data
(Klein and Manning, 2002; Klein and Manning,
2004; Bod, 2006; Seginer, 2007), and the semi-
supervised parsing which uses both annotated and
unannotated data (Sarkar, 2001; Steedman et al,
2003; McClosky et al, 2006).
Because of the higher complexity and lower
performance of unsupervised methods, as well as
the need of reliable priori knowledge in semi-
supervised methods, it seems promising to project
the syntax structures from a resource-rich lan-
guage to a resource-scarce one across a bilingual
corpus. Lots of researches have so far been de-
voted to dependency projection (Hwa et al, 2002;
Hwa et al, 2005; Ganchev et al, 2009; Smith
and Eisner, 2009). While for constituent projec-
tion there is few progress. This is due to the fact
that the constituent syntax describes the language
structure in a more detailed way, and the degree of
isomorphism between constituent structures ap-
pears much lower.
In this paper we propose for constituent pro-
jection a stepwise but totally automatic strategy,
which performs constituent projection on the ba-
sis of dependency projection, and then use a con-
straint EM optimization algorithm to optimized
the initially projected trees. Given a word-aligned
bilingual corpus with source sentences parsed, we
first project the dependency structures of these
constituent trees to the target sentences using a
dynamic programming algorithm, then we gener-
ate a set of candidate constituents for each target
sentence and design a novel evaluation function
to calculate the probability of each candidate con-
stituent, finally, we develop a PCFG-style parsing
procedure to search for the most probable pro-
jected constituent tree in the evaluated candidate
constituent set. In addition, we design a constraint
EM optimization procedure to decrease the noise
in the initially projected constituent treebank.
Experimental results validate the effectiveness
of our approach. On the Chinese-English FBIS
corpus, we project the English parses produced
by the Charniak parser across to the Chinese sen-
516
tences. A berkeley parser trained on this pro-
jected treebank can effectively boost the super-
vised parsers trained on bunches of CTB trees.
Especially, the supervised parser trained on the
smaller CTB 1.0 benefits a significant F-measure
increment of more than 1 point from the projected
parser. When using the projected parser in a tree-
based translation model (Liu et al, 2006), we
achieve translation performance comparable with
using a state-of-the-art supervised parser trained
on thousands of CTB trees. This surprising re-
sult gives us an inspiration that better translation
would be achieved by combining both projected
parsing and supervised parsing into a hybrid pars-
ing schema.
2 Stepwise Constituent Projection
We first introduce the dynamic programming pro-
cedure for dependency projection, then describe
the PCFG-style algorithm for constituent projec-
tion which is conducted on projected dependent
structures, and finally show the constraint EM
procedure for constituent optimization.
2.1 Dependency Projection
For dependency projection we adopt a dynamic
programming algorithm, which searches the most
probable projected target dependency structure
according to the source dependency structure and
the word alignment.
In order to mitigate the effect of word alignment
errors, multiple GIZA++ (Och and Ney, 2000) re-
sults are combined into a compact representation
called alignment matrix. Given a source sentence
with m words, represented as E1:m, and a target
sentence with n words, represented as F1:n, their
word alignment matrix A is an m ? n matrix,
where each element Ai,j denotes the probability
of the source word Ei aligned to the target word
Fj .
Using P (DF |DE , A) to denote the probability
of the projected target dependency structure DF
conditioned on the source dependency structure
DE and the alignment matrix A, the projection al-
gorithm aims to find
D?F = argmax
DF
P (DF |DE , A) (1)
Algorithm 1 Dependency projection.
1: Input: F , and Pe for all word pairs in F
2: for ?i, j? ? ?1, |F |? in topological order do
3: buf ? ?
4: for k? i..j ? 1 do ? all partitions
5: for l ? V[i, k] and r ? V[k + 1, j] do
6: insert DERIV(l, r, Pe) into buf
7: insert DERIV(r, l, Pe) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(p, c, Pe)
11: d? p ? c ? {p ? rooty c ? root} ? new derivation
12: d ? evl ? EVAL(d, Pe) ? evaluation function
13: return d
P (DF |DE , A) can be factorized into each depen-
dency edge xy y in DF
P (DF |DE , A) =
?
xyy?DF
Pe(xy y|DE , A)
Pe can then be obtained by simple accumulation
across all possible situations of correspondence
Pe(xy y|DE , A)
=
?
1?x?,y??|E|
Ax,x? ?Ay,y? ? ?(x?, y?|DE)
where ?(x?, y?|DE) is a 0-1 function that equals
1 only if the dependent relation x? y y? holds in
DE .
The search procedure needed by the argmax op-
eration in equation 1 can be effectively solved
by the Chu-Liu-Edmonds algorithm used in (Mc-
Donald et al, 2005). In this work, however, we
adopt a more general and simple dynamic pro-
gramming algorithm as shown in Algorithm 1,
in order to facilitate the possible expansions. In
practice, the cube-pruning strategy (Huang and
Chiang, 2005) is used to speed up the enumera-
tion of derivations (loops started by line 4 and 5).
2.2 Constituent Projection
The PCFG-style parsing procedure searches for
the most probable projected constituent tree in
a shrunken search space determined by the pro-
jected dependency structure and the target con-
stituent tree. The shrunken search space can be
built as following. First, we generates the candi-
date constituents of the source tree and the can-
didate spans of the target sentence, so as to enu-
merate the candidate constituents of the target sen-
tence. Then we compute the consistent degree for
517
each pair of candidate constituent and span, and
further estimate the probability of each candidate
constituent for the target sentence.
2.2.1 Candidate Constituents and Spans
For the candidate constituents of the source
tree, using only the original constituents imposes
a strong hypothesis of isomorphism on the con-
stituent projection between two languages, since
it requires that each couple of constituent and span
must be strictly matched. While for the candi-
date spans of the target sentences, using all sub-
sequences makes the search procedure suffer from
more perplexity. Therefore, we expand the candi-
date constituent set and restrict the candidate span
set:
? Candidate Constituent: Suppose a produc-
tion in the source constituent tree, denoted as
p ? c1c2..ch..c|p|, and ch is the head child
of the parent p. Each constituent, p or c, is a
triple ?lb, rb, nt?, where nt denotes its non-
terminal, while lb and rb represent its left-
and right bounds of the sub-sequence that the
constituent covers. The candidate constituent
set of this production consists the head of
the production itself, and a set of incomplete
constituents,
{?l, r, p ? nt??|c1 ? lb ? l ? ch ? lb?
ch ? rb ? r ? c|p| ? rb?
(l < ch ? lb ? r > ch ? rb)}
where the symbol ? indicates an incomplete
non-terminal. The candidate constituent set
of the entire source tree is the unification of
the sets extracted from all productions of the
tree.
? Candidate Span: A candidate span of the tar-
get sentence is a tuple ?lb, rb?, where lb and
rb indicate the same as in a constituent. We
define the candidate span set as the spans of
all regular dependent segments in the corre-
sponding projected dependency structure. A
regular dependency segment is a dependent
segment that every modifier of the root is a
complete dependency structure. Suppose a
dependency structure rooted at word p, de-
noted as clL..cl2cl1 x p y cr1cr2..crR, it
has L (L ? 0) modifiers on its left and R
(R ? 0) modifiers on its right, each of them
is a smaller complete dependency structure.
Then the word p itself is a regular depen-
dency segment without any modifier, and
{cli..cl1 x py cr1..crj |0 ? i ? L?
0 ? j ? R?
(i > 0 ? j > 0)}
is a set of regular dependency structures with
at least one modifier. The regular depen-
dency segments of the entire projected de-
pendency structure can simply be accumu-
lated across all dependency nodes.
2.2.2 Span-to-Constituent Correspondence
After determining the candidate constituent set
of the source tree, denoted as ?E , and the can-
didate span set of the target sentence, denoted as
?F , we then calculate the consistent degree for
each pair of candidate constituent and candidate
span.
Given a candidate constituent ? ? ?E and a
candidate span ? ? ?F , their consistent degree
C(?, ?|A) is the probability that they are aligned
to each other according to A.
We display the derivations from bottom to up.
First, we define the alignment probability from a
word i in the span ? to the constituent ? as
P (i 7? ?|A) =
?
??lb?j???rbAi,j?
j Ai,j
Then we define the alignment probability from the
span ? to the constituent ? as
P (? 7? ?|A) =
?
??lb?i???rb
P (i 7? ?|A)
Note that we use i to denote both a word and its in-
dex for simplicity without causing confusion. Fi-
nally, we define C(?,?|A) as
C(?, ?|A) = P (? 7? ?|A)? P (? 7? ?|AT ) (2)
Where P (? 7? ?|AT ) denotes the alignment
probability from the constituent ? to the span ?, it
can be calculated in the same manner.
518
2.2.3 Constituent Projection Algorithm
The purpose of constituent projection is to find
the most probable projected constituent tree for
the target sentence conditioned on the source con-
stituent tree and the word alignment
T?F = argmax
TF??F
P (TF |TE, A) (3)
Here, we use ?F to denote the set of candidate
constituents of the target sentence
?F = ?F ?NT (?E)
= {?F |?(?F ) ? ?F ? nt(?F ) ? NT (?E)}
where ?(?) and nt(?) represent the span and the
non-terminal of a constituent respectively, and
NT (?) represents the set of non-terminals ex-
tracted from a constituent set. Note that TF is a
subset of ?F if we treat a tree as a set of con-
stituents.
The probability of the projected tree TF can be
factorized into the probabilities of the projected
constituents that composes the tree
P (TF |TE , A) =
?
?F?TF
P?(?F |TE , A)
while the probability of the projected source con-
stituent can be defined as a statistics of span-to-
constituent- and constituent-to-constituent consis-
tent degrees
P?(?F |TE , A) =
?
?E??E C(?F , ?E |A)?
?E??E C(?(?F ), ?E |A)
where C(?F , ?E |A) in the numerator denotes the
consistent degree for each pair of constituents,
which can be calculated based on that of span and
constituent described in Formula 2
C(?F , ?E) =
{
0 if ?F ? nt 6= ?E ? nt
C(?(?F ), ?E) else
Algorithm 2 shows the pseudocode for con-
stituent projection. A PCFG-style parsing pro-
cedure searches for the best projected constituent
tree in the constrained space determined by ?F .
Note that the projected trees are binarized, and can
be easily recovered according to the asterisks at
the tails of non-terminals.
Algorithm 2 Constituent projection.
1: Input: ?F , ?F , and P? for all spans in ?F
2: for ?i, j? ? ? in topological order do
3: buf ? ?
4: for p ? ?F s.t. ?(p) = ?i, j? do
5: for k? i..j ? 1 do ? all partitions
6: for l ? V[i, k] and r ? V[k + 1, j] do
7: insert DERIV(l, r, p, P?) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(l, r, p, P?)
11: d? l ? r ? {p} ? new derivation
12: d ? evl ? EVAL(d, P?) ? evaluation function
13: return d
2.3 EM Optimization
Since the constituent projection is conducted on
each sentence pair separately, the projected tree-
bank is apt to suffer from more noise caused by
free translation and word alignment error. It can
be expected that an EM iteration over the whole
projected treebank will lead to trees with higher
consistence.
We adopt the inside-outside algorithm to im-
prove the quality of the initially projected tree-
bank. Different from previous works, all expecta-
tion and maximization operations for a single tree
are performed in a constrained space determined
by the candidate span set of the projected target
dependency structure. That is to say, all the sum-
mation operations, both for calculating ?/? values
and for re-estimating the rule probabilities, only
consider the spans in the candidate span set. This
means that the projected dependency structures
are supposed believable, and the noise is mainly
introduced in the following constituent projection
procedure.
Here we give an overall description of the tree-
bank optimization procedure. First, an initial
PCFG grammar G0F is estimated from the original
projected treebank. Then several iterations of ?/?
calculation and rule probability re-estimation are
performed. For example in the i-the iteration, ?/?
values are calculated based on the current gram-
mar Gi?1F , afterwards the optimized grammar GiF
is obtained based on these ?/? values. The itera-
tive procedure terminates when the likelihood of
whole treebank increases slowly. Finally, with the
optimized grammar, a constrained PCFG parsing
procedure is conducted on each of the initial pro-
519
jected trees, so as to obtain an optimized treebank.
3 Applications of Constituent Projection
The most direct contribution of constituent pro-
jection is pushing an initial step for the statis-
tical constituent parsing of resource-scarce lan-
guages. It also has some meaningful applica-
tions even for the resource-rich languages. For
instances, the projected treebank, due to its large
scale and high coverage, can used to boost an tra-
ditional supervised-trained parser. And, the parser
trained on the projected treebank can adopted to
conduct tree-to-string machine translation, since
it give parsing results with larger isomorphism
with the target language than a supervised-trained
parser dose.
3.1 Boost an Traditional Parser
We first establish a unified framework for the en-
hanced parser where a projected parser is adopted
to guide the parsing procedure of the baseline
parser.
For a given target sentence S, the enhanced
parser selected the best parse T? among the set
of candidates ?(S) according to two evaluation
functions, given by the baseline parser B and the
projected guide parser G, respectively.
T? = argmax
T??(S)
P (T |B)? P (T |G)? (4)
These two evaluation functions can be integrated
deeply into the decoding procedure (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated at a shallow level in a rerank-
ing manner (Collins, 2000; Charniak and John-
son, 2005). For simplicity and generability, we
adopt the reranking strategy. In k-best reranking,
?(S) is simply a set of candidate parses, denoted
as {T1, T2, ..., Tk}, and we use the single parse of
the guide parser, TG, to re-evaluate these candi-
dates. Formula 4 can be redefined as
T? (TG) = argmax
T??(S)
w ? f(T, TG) (5)
Here, f(T, TG) and w represent a high dimen-
sional feature representation and a correspond-
ing weight vector, respectively. The first feature
f1(T, TG) = logP (T |B) is the log probability
of the baseline parser, while the remaining fea-
tures are integer-valued guide features, and each
of them represents the guider parser?s predication
result for a particular configuration in candidate
parse T , so as to utilize the projected parser?s
knowledge to guide the parsing procedure of the
traditional parser.
In our work a guide feature is composed of two
parts, the non-terminal of a certain constituent ?
in the candidate parse T ,1 and the non-terminal
at the corresponding span ?(?) in the projected
parse TG. Note that in the projected parse this
span does not necessarily correspond to a con-
stituent. In such situations, we simply use the
non-terminal of the constituent that just be able
to cover this span, and attach a asterisk at the tail
of this non-terminal. Here is an example of the
guide features
f100(T, TG) = V P ? T ? PP? ? TG
It represents that a V P in the candidate parse cor-
responds to a segment of a PP in the projected
parse. The quantity of its weight w100 indicates
how probably a span can be predicated as V P if
the span corresponds to a partial PP in the pro-
jected parse.
We adopt the perceptron algorithm to train
the reranker. To reduce overfitting and pro-
duce a more stable weight vector, we also use
a refinement strategy called averaged parameters
(Collins, 2002).
3.2 Using in Machine Translation
Researchers have achieved promising improve-
ments in tree-based machine translation (Liu et
al., 2006; Huang et al, 2006). Such models use
a parsed tree as input and converts it into a target
tree or string. Given a source language sentence,
first we use a traditional source language parser
to parse the sentence to obtain the syntax tree T ,
and then use the translation decoder to search for
the best derivation d?, where a derivation d is a se-
quence of transformations that converts the source
tree into the target language string
d? = argmax
d?D
P (d|T ) (6)
1Using non-terminals as features brings no improvement
in the reranking experiments, so as to examine the impact of
the projected parser.
520
Here D is the candidate set of d, and it is deter-
mined by the source tree T and the transformation
rules.
Since the tree-based models are based on
the synchronous transformational grammars, they
suffer much from the isomerism between the
source syntax and the target sentence structure.
Considering that the parsed tree produced by a
projected parser may have larger isomorphism
with the target language, it would be a promis-
ing idea to adopt the projected parser to parse the
input sentence for the subsequent translation de-
coding procedure.
4 Experiments
In this section, we first invalidate the effect of con-
stituent projection by evaluating a parser trained
on the projected treebank. Then we investigate
two applications of the projected parser: boosting
an traditional supervised-trained parser, and inte-
gration in a tree-based machine translation sys-
tem. Following the previous works, we depict the
parsing performance by F-score on sentences with
no more than 40 words, and evaluate the transla-
tion quality by the case-sensitive BLEU-4 metric
(Papineni et al, 2002) with 4 references.
4.1 Constituent Projection
We perform constituent projection from English
to Chinese on the FBIS corpus, which contains
239K sentence pairs with about 6.9M/8.9M words
in Chinese/English. The English sentences are
parsed by the Charniak Parser and the dependency
structures are extracted from these parses accord-
ing to the head-finding rules of (Yamada and
Matsumoto, 2003). The word alignment matrixes
are obtained by combining the 10-best results of
GIZA++ according to (Liu et al, 2009).
We first project the dependency structures from
English to Chinese according to section 2.1, and
then project the constituent structures according
to section 2.2. We define an assessment criteria
to evaluate the confidence of the final projected
constituent tree
c = n
?
P (DF |DE , A) ? P (TF |TE , A)
where n is the word count of a Chinese sentence
in our experiments. A series of projected Chi-
Thres c #Resrv Cons-F1 Span-F1
0.5 12.6K 23.9 32.7
0.4 17.8K 23.9 33.4
0.3 27.2K 25.4 35.7
0.2 45.1K 26.6 38.0
0.1 87.0K 27.8 40.4
Table 1: Performances of the projected parsers
on the CTB test set. #Resrv denotes the amount
of reserved trees within threshold c. Cons-F1 is
the traditional F-measure, while Span-F1 is the F-
measure without consideration of non-terminals.
nese treebanks with different scales are obtained
by specifying different c as the filtering threshold.
The state-of-the-art Berkeley Parser is adopted to
train on these treebanks because of its high per-
formance and independence of head word infor-
mation.
Table 1 shows the performances of these pro-
jected parsers on the standard CTB test set, which
is composed of sentences in chapters 271-300.
We find that along with the decrease of the filter-
ing threshold c, more projected trees are reserved
and the performance of the projected parser con-
stantly increases. We also find that the traditional
F-value, Cons-F1, is obviously lower than the one
without considering non-terminals, Span-F1. This
indicates that the constituent projection procedure
introduces more noise because of the higher com-
plexity of constituent correspondence. In all the
rest experiments, however, we simply use the pro-
jected treebank filtered by threshold c = 0.1 and
do not try any smaller thresholds, since it already
takes more than one weak to train the Berkeley
Parser on the 87 thousands trees resulted by this
threshold.
The constrained EM optimization procedure
described in section 2.3 is used to alleviate the
noise in the projected treebank, which may be
caused by free translation, word alignment errors,
and projection on each single sentence pair. Fig-
ure 1 shows the log-likelihood on the projected
treebank after each EM iteration. It is obvious that
the log-likelihood increases very slowly after 10
iterations. We terminate the EM procedure after
40 iterations.
Finally we train the Berkeley Parser on the op-
timized projected treebank, and test its perfor-
521
-65
-64
-63
-62
-61
-60
-59
-58
 0  5  10  15  20  25  30  35  40
Lo
g-
lik
eli
ho
od
EM iteration
Figure 1: Log-likelihood of the 87K-projected
treebank after each EM interation.
Train Set Cons-F1 Span-F1
Original 87K 27.8 40.4
Optimized 87K 22.8 40.2
Table 2: Performance of the parser trained on the
optimized projected treebank, compared with that
of the original projected parser.
Train Set Baseline Bst-Ini Bst-Opt
CTB 1.0 75.6 76.4 76.9
CTB 5.0 85.2 85.5 85.7
Table 3: Performance improvement brought by
the projected parser to the baseline parsers trained
on CTB 1.0 and CTB 5.0, respectively. Bst-
Ini/Bst-Opt: boosted by the parser trained on the
initial/optimized projected treebank.
mance on the standard CTB test set. Table 2
shows the performance of the parser trained on
the optimized projected treebank. Unexpectedly,
we find that the constituent F1-value of the parser
trained on the optimized treebank drops sharply
from the baseline, although the span F1-value re-
mains nearly the same. We assume that the EM
procedure gives the original projected treebank
more consistency between each single tree while
the revised treebank deviates from the CTB anno-
tation standard, but it needs to be validated by the
following experiments.
4.2 Boost an Traditional Parser
The projected parser is used to help the reranking
of the k-best parses produced by another state-of-
the-art parser, which is called the baseline parser
for convenience. In our experiments we choose
the revised Chinese parser (Xiong et al, 2005)
 70
 72
 74
 76
 78
 80
 82
 84
 86
 88
 1000  10000
Pa
rs
ev
al 
F-
sc
or
e 
(%
)
Scale of treebank (log)
CTB 1.0
CTB 5.0
baseline
boosted parser
Figure 2: Boosting performance of the projected
parser on a series of baseline parsers that are
trained on treebanks of different scales.
based on Collins model 2 (Collins, 1999) as the
baseline parser.2
The baseline parser is respectively trained on
CTB 1.0 and CTB 5.0. For both corpora we
follow the traditional corpus splitting: chapters
271-300 for testing, chapters 301-325 for devel-
opment, and else for training. Experimental re-
sults are shown in Table 3. We find that both
projected parsers bring significant improvement to
the baseline parsers. Especially the later, although
performs worse on CTB standard test set, gives a
larger improvement than the former. This to some
degree confirms the previous assumption. How-
ever, more investigation must be conducted in the
future.
We also observe that for the baseline parser
trained on the much larger CTB 5.0, the boost-
ing performance of the projected parser is rela-
tively lower. To further investigate the regularity
that the boosting performance changes according
to the scale of training treebank of the baseline
parser, we train a series of baseline parsers with
different amounts of trees, then use the projected
parser trained on the optimized treebank to en-
hance these baseline parsers. Figure 2 shows the
experimental results. From the curves we can see
that the smaller the training corpus of the baseline
parser, the more significant improvement can be
obtained. This is a good news for the resource-
scarce languages that have no large treebanks.
2The Berkeley Parser fails to give k-best parses for some
sentences when trained on small treebanks, and these sen-
tences have to be deleted in the k-best reranking experiments.
522
4.3 Using in Machine Translation
We investigate the effect of the projected parser
in the tree-based translation model on Chinese-to-
English translation. A series of contrast transla-
tion systems are built, each of which uses a super-
vised Chinese parser (Xiong et al, 2005) trained
on a particular amount of CTB trees.
We use the FBIS Chinese-English bitext as the
training corpus, the 2002 NIST MT Evaluation
test set as our development set, and the 2005 NIST
MT Evaluation test set as our test set. We first ex-
tract the tree-to-string translation rules from the
training corpus by the algorithm of (Liu et al,
2006), and train a 4-gram language model on
the Xinhua portion of GIGAWORD corpus with
Kneser-Ney smoothing using the SRI Language
Modeling Toolkit (Stolcke and Andreas, 2002).
Then we use the standard minimum error-rate
training (Och, 2003) to tune the feature weights
to maximize the system.s BLEU score.
Figure 3 shows the experimental results. We
find that the translation system using the projected
parser achieves the performance comparable with
the one using the supervised parser trained on
CTB 1.0. Considering that the F-score of the pro-
jected parser is only 22.8%, which is far below of
the 75.6% F-score of the supervised parser trained
on CTB 1.0, we can give more confidence to the
assumption that the projected parser is apt to de-
scribe the syntax structure of the counterpart lan-
guage. This surprising result also gives us an in-
spiration that better translation would be achieved
by combining projected parsing and supervised
parsing into hybrid parsing schema.
5 Conclusion
This paper describes an effective strategy for con-
stituent projection, where dependency projection
and constituent projection are consequently con-
ducted to obtain the initial projected treebank,
and an constraint EM procedure is then per-
formed to optimized the projected trees. The
projected parser, trained on the projected tree-
bank, significantly boosts an existed state-of-the-
art supervised-trained parser, especially trained on
a smaller treebank. When using the projected
parser in tree-based translation, we achieve the
0.220
0.230
0.240
0.250
0.260
0.270
 1000  10000
BL
EU
 sc
or
e
Scale of treebank (log)
use projected parser
CTB 1.0
CTB 5.0
use supervised parsers
Figure 3: Performances of the translation systems,
which use the projected parser and a series of su-
pervised parsers trained CTB trees.
translation performance comparable with using a
supervised parser trained on thousands of human-
annotated trees.
As far as we know, this is the first time that
the experimental results are systematically re-
ported about the constituent projection and its ap-
plications. However, many future works need
to do. For example, more energy needs to be
devoted to the treebank optimization, and hy-
brid parsing schema that integrates the strengths
of both supervised-trained parser and projected
parser would be valuable to be investigated for
better translation.
Acknowledgments
The authors were supported by 863 State Key
Project No. 2006AA010108, National Natural
Science Foundation of China Contract 60873167,
Microsoft Research Asia Natural Language Pro-
cessing Theme Program grant (2009-2010), and
National Natural Science Foundation of China
Contract 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions.
References
Bod, Rens. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of the COLING-
ACL.
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
523
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the NAACL.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. In Ph.D. Thesis.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Ganchev, Kuzman, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the AMTA.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across paral-
lel texts. In Natural Language Engineering, vol-
ume 11, pages 311?325.
Klein, Dan and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the ACL.
Klein, Dan and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models
of dependency and constituency. In Proceedings of
the ACL.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the ACL.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the ACL.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Och, Franz J. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Och, Franz Joseph. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160?167.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the ACL.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
ACL.
Sarkar, Anoop. 2001. Applying co-training methods
to statistical parsing. In Proceedings of NAACL.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the ACL.
Smith, David and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Steedman, Mark, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the EACL.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 311?318.
Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Yamada, H and Y Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of IWPT.
Zhang, Yue and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
524
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1103?1113, Dublin, Ireland, August 23-29 2014.
A Dependency Edge-based Transfer Model for Statistical Machine
Translation
Hongshen Chen
?
? Jun Xie
?
Fandong Meng
?
? Wenbin Jiang
?
Qun Liu
??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{chenhongshen,xiejun,mengfandong,jiangwenbin}@ict.ac.cn
?CNGL, School of Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Previous models in syntax-based statistical machine translation usually resort to some kinds
of synchronous procedures, few of these works are based on the analysis-transfer-generation
methodology. In this paper, we present a statistical implementation of the analysis-transfer-
generation methodology in rule-based translation. The procedures of syntax analysis, syntax
transfer and language generation are modeled independently in order to break the synchronous
constraint, resorting to dependency structures with dependency edges as atomic manipulating
units. Large-scale experiments on Chinese to English translation show that our model exhibits
state-of-the-art performance by significantly outperforming the phrase-based model. The statis-
tical transfer-generation method results in significantly better performance with much smaller
models.
1 Introduction
Researches in statistical machine translation have been flourishing in recent years. Statistical translation
methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002;
Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004;
Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004;
Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and
phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher
generalization capability by leveraging the hierarchical structures in natural languages, and achieve the
state-of-the-art performance in these years.
Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation
procedures which directly model the structural correspondence between two languages. In contrast,
the analysis-transfer-generation methodology in rule-based translation solves the machine translation
problem in a more divided scheme, where the processing procedures of analysis, structural transfer and
language generation are modeled separately. The analysis-transfer-generation strategy can tolerate higher
non-isomorphism between languages if with a more general transformation unit and it can facilitate
elaborating engineering of each processing procedure, however, there isn?t a statistical transfer model
that shows the comparable performance with the current state-of-the-art SMT model so far.
In this paper, we propose a novel statistical analysis-transfer-generation model for machine transla-
tion, to integrate the advantages of the transfer-generation scheme and the statistical modeling. The
procedures of transfer and generation are modeled on dependency structures with dependency edges
as atomic manipulating units. First, the source sentence is parsed by a dependency parser. Then, the
source dependency structure is transferred into a target structure by translation rules, which composed
of the source and target edges. Last, the target sentence is finally generated from the target edges which
are used as intermediate syntactic structures. By directly modeling the edge, the most basic unit in the
dependency tree, which definitely describe the modifying relationship and positional relation between
words, our model alleviates the non-isomorphic problem and shows the flexibility of reordering.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1103
E
?ob?m?j?nti?nji?n?f?b? ?nqu?nzh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
*:sh?n?m?n?
nsubj advmod
dobj
f?b?
f?b? sh?n?m?n?
?nqu?n
nn
obama 
issue
will
issue
issue
*
sh?n?m?n?
zh?nlu?
nn
a statement of 
security
a statement of 
strategy
f?b?/VV
??
ji?n?/AD
?
sh?n?m?n?/NN
??
?nqu?n/NN
??
zh?nlu?/NN
??
nsubj
advmod
dobj
nn nn
?ob?m?/NN
???
obama today will issue a statement of security strategy
D ? ? ?
? ?
j?nti?n/NT
??
tmod
j?nti?n
tomod
f?b?
today
issue
?
??????????????
Figure 1: (a)An example of labeled Chinese dependency tree aligned with the corresponding English
sentence. (b) Examples of the transfer rules extracted from the tree. ?*? denotes a variable. All the inner
nodes are treated as variables. The label on the target side of a rule denotes whether the head and the
dependent are adjacent or not.
The rest of the paper is organized as follows, we first describe the dependency edge-based transfer
model (Section 2). Then, we present our rule acquisition algorithm (Section 3), the decoding and target
sentence generation process (Section 4). Finally, large-scale experiments (Section 5) on Chinese-to-
English translation show that our edge-based transfer model gains state-of-the-art performance by sig-
nificantly outperforming the phrase-based model (Koehn et al., 2003) by averaged +1.34 BLEU points on
three test sets. To the best of our knowledge, this is the first transfer-generation-based statistical machine
translation model that achieves the state-of-the-art performance.
2 Dependency Edge-based Transfer Model
2.1 Edges in Dependency Trees
Given a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes.
An example dependency tree is shown in Figure 1 (a). An edge in the tree represents a dependency
relationship between a pair of words, a head and a dependent. When a nominal dependent acts as a
subject and modifies a verbal head, they usually have a fixed relative position. In Figure 1 (a), ?`aob?am?a?
modifies ?f?ab`u?. The grammatical relation label nsubj (Chang et al., 2009) between them denotes that a
noun phrase acts as the subject of a clause. ?`aob?am?a? is on the left of ?f?ab`u?.
Based on the above observations, we take the edge as the elementary structure of a dependency tree
and regard a dependency tree to be a set of edges.
Definition 1. An source side edge is a 4-tuple e = ?H,D,P,R?, where H is the head, D is the depen-
dent, P denotes the relative position between H and D, left or right, R is the grammatical relation label
.
In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependency
tree.
1104
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
nsubj
advmod
dobj
?ob?m?/NN j?nti?n/NT
tmod
Transfer
select the left
adjacent edge
? ? ? ?
extend to the left
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
? ? ?
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
?? ?
H1:obama today will issue
H2:today obama will issue
extend to the right
*
adjacent
? ? ? ?
a statement of security strategy
ad
jac
en
t
issue
?
will
will issue
will
ad
jac
en
t
obama
issue
non-
adjac
ent
today
non
-ad
jac
ent
? ? ?
will
ad
jac
en
t
obama
issue
non-
adja
cent
today
non
-ad
jac
ent
?? ?
H1:obama today will issue a statement of security strategy
H2:today obama will issue a statement of security strategy
?
adjacent
?
*
Generation
Analysis
?ob?m? j?nti?n ji?n? f?b? ?nqu?n zh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
nsubj advmod
f?b?
obama
issue
will
non
-ad
jace
nt
issue
adj
ace
nt
j?nti?n
tomod
f?b?
today
issue
non
-ad
jace
nt
*:sh?n?m?n?
dobj
f?b?
issue
*
adjacent
Figure 2: An example partial generation of translation. The same set of rules generate two target hy-
potheses with the same words and different word order. Assume the sub-tree rooted at ?sh?engm??ng? has
been translated to the corresponding target sentence fragment.
2.2 Transfer Rules
A transfer rule of our model represents the reordering and relative positions of edges between language
pairs. For example, in Figure 1 (b), the first rule shows that when a nominal subject modifies a verb, the
target side keeps the same position relations. ?obama? is also on the left of ?issue?, the same with the
source side relative position. The 5-th and 6-th rules show the inversion relations between the source and
the target. Formally, a transfer rule can be defined as a triple ?e, f,??, where e is an edge extracted from
the source dependency tree, f is a target edge. ? denotes one-to-one correspondence between variables
in e and f .
Figure 1 (b) are part of transfer rules extracted from the word aligned sentence in Figure 1 (a). The
target edge denotes whether the target dependent is on the left or the right side of the target head, the
1105
label on the edge indicates whether the target head and the target dependent are adjacent or not. If
the dependent is an internal node(contrast with the leaf nodes in the dependency tree), then it will be
regarded as a substitution node. The dependent in the 4-th transfer rule is an internal node and the its
corresponding target side is a substitution variable.
Figure 2 shows a partial transfer-generation of our model which involves three phases. First, analysis.
Given a source language sentence, we obtain its dependency tree using a dependency parser. We assume
that the sub-tree of the substitution node has been translated. Second, transfer. For each internal node,
we transfer the source side edges between the head and all its dependents into the target sides. In the
second block of Figure 2, we transfer four edges into the target sides. Third, generation, corresponding
to the third block of Figure 2. We generate the target sentence with the target side edges starting from the
target head, ?issue?. We first try to concatenate the edges to the left. First, we select a target side edge
that is on the left side of ?issue? and adjacent to it to form a consecutive phrase. Edge 3 is selected and ?to
issue? is generated. Then, we enumerate all possible left concatenations of the other edges that are not
adjacent to ?issue?. The two sequences(1,2,3 and 2,1,3) of the edges are generated, corresponding to the
two hypotheses. After that, we extend the two hypotheses to the right. The internal node ?sh?engm??ng? is
a substitution node, so the candidate translation of the sub-tree rooted at ?sh?engm??ng? is concatenated to
the two hypotheses. Finally, we generate the two candidate translations of the input sentence.
3 Acquisition of Transfer Rules
Transfer rules can be extracted automatically from a word-aligned corpus, which is a set of triples
?T, S,A?, where T is a source dependency tree, S is a target side sentence and A is an alignment relation
between T and S. Following the dependency-to-string model (Xie et al., 2011), we extract transfer rules
from each triple ?T, S,A? by three steps:
1. Tree Annotation: Label each node in the dependency tree with the alignment information
2. Edges Identification: Identify acceptable edges from the annotated dependency tree
3. Rule induction: Induce a set of lexicalized and un-lexicalized transfer rules from the acceptable
edges.
3.1 Tree Annotation
Given a triple ?T, S,A? as Figure 3 shows, we define two attributes for every node in T: node span and
sub-tree span:
Definition 2. Given a node n, its node span nsp(n) is a set of consecutive indexes of the target words
aligned with the node n.
For example, nsp(?anqu?an)={7-8}, which corresponds to the target word ?of? and ?security?.
Definition 3. A node span nsp(n) is consistent if for any other node n
?
in the dependency tree, nsp(n)
and nsp(n
?
) are not overlapping.
For example, nsp(zh`anlu`e) is consistent, while nsp(?anqu?an) is not consistent for it corresponds to the
same word ?of? with nsp(sh?engm??ng).
Definition 4. Given a sub-tree T
?
rooted at n, the sub-tree span tsp(n) of n is a consecutive target word
indexes from the lower bound of the nsp of all the nodes in T
?
to the upper bound of those spans.
For example, tsp(sh?engm??ng)={5-9},which corresponds to the target phrase ?a statement of security
strategy?.
Definition 5. A sub-tree span tsp(n) is consistent if for any other node n
?
that is not in the sub-tree
rooted at n in the dependency tree, tsp(n) and nsp(n
?
) are not overlapping.
For example, tsp(sh?engm??ng) is consistent, even though nsp(sh?engm??ng) is not consistent, while
tsp(?anqu?an) is not consistent for ?sh?engm??ng? is not a node in sub-tree rooted at ??anqu?an? and ??anqu?an?
corresponds to the same word ?of ? with nsp(sh?engm??ng) .
1106
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
?nqu?n/NN zh?nlu?/NN
nsubj advmod dobj
nn nn
?ob?m?/NN
obama today will issue a statement of security strategy
j?nti?n/NT
tmod
1 2 3 4 5 6 7 8 9
{1-1}/{1-1} {2-2}/{2-2} {3-3}/{3-3}
{4-4}/{1-9}
{5-7}/{5-9}
{7-8}/{7-8} {9-9}/{9-9}
Figure 3: An example of annotated dependency tree. Each node is annotated with two spans, the former
is node span and the latter is sub-tree span. The gray edge is not acceptable. It is different from Figure
1, because ??anqu?an? aligned with two words in Figure 3. ?of? in the target side is aligned with both
??anqu?an? and ?sh?engm??ng? which makes the gray edge un-acceptable.
3.2 Acceptable Edges Identification
We identify the edges from the annotated dependency tree that are acceptable for rule induction.
For an acceptable edge, its node span of the head nsp(head) and the sub-tree span of the dependent
tsp(dependent) satisfy the following properties:
1. nsp(head) and tsp(dependent) are consistent.
2. nsp(head) and tsp(dependent) are non-overlapping.
For example, tsp(?anqu?an) and nsp(sh?engm??ng) are neither consistent nor non-overlapping. So the
gray edge between head ?sh?engm??ng? and dependent ??anqu?an? is not an acceptable edge. nsp(f?ab`u)
and tsp(sh?engm??ng) are consistent and the two spans are non-overlapping. Thus, the edge between head
?f?ab`u? and dependent ?sh?engm??ng? is an acceptable edge.
3.3 Transfer Rule Induction
From each acceptable source side edge, we induce a set of lexicalized and un-lexicalized transfer rules.
We induce a lexicalized transfer rule from an acceptable edge by the following procedures:
1. extract the source side edge and mark the internal nodes as substitution sites. This form the input of
a transfer rule.
2. extract the position information according to nsp(head) and tsp(dependent), whether they are adja-
cent or not and whether tsp(dependent) is on the left side or the right side of nsp(head).
In Figure 4, the first transfer rule is lexicalized rule, it is induced from the edge between ?f?ab`u? and
?`aob?am?a?.
In addition to the lexicalized rules described above, we also generalized the rules by replacing the
word in an source side edge with a wild card and the part of speech of the word. For example, the rule
in Figure 4 can be generalized in two ways. The generalized versions of the rule apply to ?`aob?am?a?
modifying any verb and ?f?ab`u? modifying any noun, respectively. The generalized rules are also called
1107
Generalize head
?ob?m?
nsubj
f?b?
obama
issue
non
-adj
acen
t
?ob?m?
nsubj
*:VV
obama
*
non-
adja
cent
Generalize
dependent
*:NN
nsubj
f?b?
*
issue
non-a
djace
nt
?
?
?
Figure 4: Generalization of transfer rule.
un-lexicalized rules for the loss of word information. The single node translations of the generalized
words are also extracted.
The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both
left and right directions. We do this process similar with the method of Och and Ney (2004). We might
obtain m(m ? 1) extended rules from an acceptable edge. The frequency of each rule is divided by m.
We take the extracted rule set as observed data and make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
4 Decoding and Generation
We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each
concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate
the target sentence e. The probability of e is defined as?
P (c) ?
?
i
?
i
(c)
?
i
(1)
where ?
i
(c) are features defined on concatenations and ?
i
are feature weights. In our experiments of
this paper, thirteen features are used as follows:
? Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilities
P
lex
(t|s) and P
lex
(s|t);
? Bilingual phrases probabilities P
bp
(t|s) and P
bp
(s|t), and bilingual phrases lexical translation prob-
abilities P
bplex
(t|s) and P
bplex
(s|t);
? Transfer rule penalty exp(?1);
? Bilingual phrase penalty exp(?1);
? Pseudo translation rule penalty exp(?1);
? Target word penalty exp(|e|);
? Language model P
lm
(e).
Our decoder is based on a bottom-up chart-based beam-search algorithm. We regard the decoding
process as the composition of the target side edges. For a given source language sentence, we obtain its
1108
f?b?
j?nti?n?ob?m?
obama today to issue
f?b?
sh?n?m?n?
?nqu?n zh?nlu?
issue a statement of security strategy
ji?n?
Figure 5: Two examples of the phrases incorporated in our model.
dependency tree T with an external dependency parser. Each node in T is traversed in post-order. For
each internal node and root node n, we do the transfer-generation translation as the following procedures:
1. Extract all the source side edges including the lexicalized and generalized edges between n and all
its dependents using the same way we extract the source side edges of the transfer rules.
2. Transfer the source side edges into target side edges. For a generalized rule, we restore it to a lex-
icalized rule by combining it with the single word translation. For no matched edges, we construct
the pseudo translation rule according to the word order of the source head-dependent relation.
3. Generate the target sentence by bi-directional extension from an adjacent target edge. We first
group all the target edges by their heads. For each group, we generate translation hypotheses with
the following procedures:
(a) Select an adjacent target edge as the starting position;
(b) Extend to the left side and enumerate all possible permutations of the target edges directing
left;
(c) Extend to the right side and enumerate all possible permutations of the target edges directing
right.
Considering that in dependency trees, a head may relate to more than 4 edges which results in
massive search space. We reduce the time complexity by using the maximum distortion limit. The
distortion is defined as (a
i
? b
i?1
? 1), where a
i
denotes the start position of the source side edge
that is translated into the ith target side edge and b
i?1
denotes the end position of the source side
edge translated into the (i ? 1)th target side edge.
When we reach the root node, the candidate translations of the input sentence are generated.
In our model, only the adjacent target edge of a transfer rule can be regarded as a consecutive phrase
and its corresponding source side length is only 2. As we start extending the target sentence from
the target head, it is quite natural to incorporate the bilingual phrases to make the target sentences be
extended from the phrases as well as the single target head word. Due to the flexibility of our model,
we can incorporate not only the syntactic phrases which are phrases covering a whole sub-tree, but also
the non-syntactic phrases as the fixed dependency structures in Shen et al. (2008) which are consecutive
phrases covering the head. Figure 5 shows two examples of the phrases incorporated in our model.
We prune the search space in several ways. First, beam threshold ?, items with a score worse than ?
times of the best score in the same span will be discarded; second, beam size b, items with a score worse
than the bth best item will be discarded. For our experiments, we set ? = 10
?3
and b = 300; Third,
we also prune rules for the same edge with a fixed rule limit (r = 200), which denotes the maximum
number of rules we keep.
5 Experiments
In this section, the performance of our model is evaluated by comparing with phrase-based model (Koehn
et al., 2003), on the NIST Chinese-to-English translation tasks. We also present the influence of the
1109
mt02-tune mt03 mt04 mt05
0 30.81 30.03 32.44 30.09
1 33.4 32.07 34.55 31.77
2 34.39 32.7 35.4 32.59
3 34.04 32.69 35.46 32.54
4 33.59 31.75 35.15 32.17
30
31
32
33
34
35
36
0 1 2 3 4
maximum distortion limit
BL
EU
(%
)
mt02-tune
mt03
mt04
mt05
Figure 6: Effect of different maximum distortion limits on development
set (mt02) and three tests(mt03,04,05). The performance of all the sets
are consistent.
maximum distortion limit to our model. We take open source phrase-based system Moses (with default
configuration)
1
as our baseline system.
5.1 Experimental Setting
Our training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LD-
C2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser
(Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags and
edges by dependency labels.
To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions
and apply ?grow-diag-and? refinement (Koehn et al., 2003). We extract the phrases covering no more
than 10 nodes of the fixed structures.
We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the Gigaword corpus.
We use NISTMTEvaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets.
The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric
2
.
We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the
BLEU score of the development set.
The statistical significance test is performed by sign-test (Collins et al., 2005).
5.2 Influence of Maximum Distortion Limit
Figure 6 gives the performance of our system with different maximum distortion limits in terms of
uncased BLEU of three NIST test sets. The performance of different distortion limit are consistent on
both development set and three test sets. Maximum distortion limit 2 gets the best performances. A low
distortion limit may cause the target sentence been translated more close to the sequence of the source,
especially when the distortion limit equals to 0, none of the reordering is allowed, while a high distortion
limit may lead the good translations be flooded by too many ambiguities when enumerating the possible
sequences of the target non-adjacent dependents. We choose 2 as the maximum distortion limit in the
next experiments.
1
http://www.statmt.org/moses/
2
ftp://jaguar.ncsl.nist.glv/mt/resources/mteval-v11b.pl.
1110
System Rule # MT03 MT04 MT05 Average
Moses 44.49M 32.03 32.83 31.81 32.22
DEBT 30.7M 32.7* 35.4* 32.59* 33.56
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets.
?DEBT? denotes our edge-based transfer model. The ?*? denotes that the results are significantly better
than the baseline system (p<0.01).
5.3 Performance of Our Model
Tabel 1 illustrates the translation results of our experiments. We (DEBT) surpass the baseline over +1.34
BLEU points on average. Our model significant outperforms the baseline phrase-based model, with
p < 0.01 on statistical significance test sign-test (Collins et al., 2005).
We also list the statistical number of rules extracted from the training corpus. The number of our
transfer rules is only 69.0% of the rules extracted by Moses, thus, the total rules in our model is 31%
smaller than Moses.
6 Related Work
Transfer-based MT systems usually take a parse tree in the source language and translate it into a parse
tree in the target language with transfer rules. Both our model and some of those previous works ac-
quired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al.,
2002; Lavoie et al., 2002; Lin, 2004). Gimpel and Smith (2009) and Gimpel and Smith (2014) used
quasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer of
dependency syntax in a non-synchronous setting. They do the translation as monolingual lattice parsing.
As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translation
problem with minimal path covering. Quirk et al. (2005) and Xiong et al. (2007) used treelets to model
the source dependency tree using synchronous grammars. Quirk et al. (2005) projected the source depen-
dency structure into target side by word alignment and faced the problem of non-isomorphism between
languages. Xiong et al. (2007) directly modeled the treelet to the corresponding target string to alleviate
the problem. Xie et al. (2011) directly specified the ordering information in head-dependents rules that
represent the source side as head-dependents relations and the target side as string.
Differently, our model uses a much simpler elementary structure, edge, which consist of only a head
and a dependent. As a transfer-generation model, we transfer an edge in the source dependency tree into
target side and incorporate the position information on the target edge , which alleviate non-isomorphism
problem and incorporate ordering among different target edges simultaneously. Moreover, our decoding
method is quite different from previous dependency tree-based works. After parsing a given source
language sentence, we transfer and generate the target sentence fragments recursively on each internal
node of the dependency tree bottom-up.
7 Conclusions and Future Work
In this paper, we present a novel dependency edge-based transfer model using dependency trees on the
source side for machine translation. We directly transfer the edges in source dependency tree into the
target sides and then generate the target sentences by beam-search. With the concise transfer rules,
our model is compatible with both the syntactic and non-syntactic phrases. Although the generation
process of our model seems relatively simple, it still exhibits a good performance and outperforms the
phrase-based model on large scale experiments. For the first time, a statistical transfer model shows a
comparable performance with the state-of-the-art translation models.
Since the translation procedure is divided into three phases and each phase can be modeled indepen-
dently, we would like to take further steps focusing on modeling the target language generation process
specifically to ensure a better grammatical translation with the help of natural language generation meth-
ods.
1111
Acknowledgments
The authors were supported by National Key Technology R&D Program (No. 2012BAH39B03), CAS
Action Plan for the Development of Western China (No. KGZD-EW-501), and Sino-Thai Scientific and
Technical Cooperation (No. 60-625J). Sincere thanks to the anonymous reviewers for their thorough
reviewing and valuable suggestions.
References
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Jaime Carbonell, Katharina Probst, Erik Peterson, Christian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource-limited mt. In Machine Translation: From Research to Real Users,
pages 1?10. Springer.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning. 2009. Discriminative reordering
with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in
Statistical Translation, pages 51?59. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263?270. Association for
Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531?540.
Association for Computational Linguistics.
Yuan Ding and Martha Palmer. 2004. Synchronous dependency insertion grammars: A grammar formalism for
syntax based statistical mt. In Workshop on Recent Advances in Dependency Grammars (COLING), pages
90?97.
Kevin Gimpel and Noah A Smith. 2009. Feature-rich translation by quasi-synchronous lattice parsing. In Pro-
ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1,
pages 219?228. Association for Computational Linguistics.
Kevin Gimpel and Noah A Smith. 2014. Phrase dependency machine translation with quasi-synchronous tree-to-
tree features. Computational Linguistics.
Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 105?112, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL ?03, pages 48?54, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2002. Learning domain-specific transfer rules: an experiment
with korean to english translation. In Proceedings of the 2002 COLING workshop on Machine translation in
Asia-Volume 16, pages 1?7. Association for Computational Linguistics.
Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages
625?630, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 609?616. Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine transla-
tion. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume
10, pages 133?139. Association for Computational Linguistics.
1112
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu. 2013. Translation with source constituency
and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1066?1076, Seattle, Washington, USA, October. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL-08: HLT, pages
192?199, Columbus, Ohio, June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 295?302, Stroudsburg, PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL?05), pages 271?279, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Stephen Richardson, William Dolan, Arul Menezes, and Jessie Pinkham. 2001. Achieving commercial-quality
translation with example-based methods. In Proceedings of MT Summit VIII, pages 293?298. Santiago De
Compostela, Spain.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
216?226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical
machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07,
pages 40?47, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th
Annual Meeting on Association for Computational Linguistics, pages 523?530. Association for Computational
Linguistics.
1113
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2042?2051, Dublin, Ireland, August 23-29 2014.
RED: A Reference Dependency Based MT Evaluation Metric
Hui Yu
??
Xiaofeng Wu
?
Jun Xie
?
Wenbin Jiang
?
Qun Liu
??
Shouxun Lin
?
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?
University of Chinese Academy of Sciences
{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn
?
CNGL, School of Computing, Dublin City University
{xiaofengwu,qliu}@computing.dcu.ie
Abstract
Most of the widely-used automatic evaluation metrics consider only the local fragments of the
references and translations, and they ignore the evaluation on the syntax level. Current syntax-
based evaluation metrics try to introduce syntax information but suffer from the poor pars-
ing results of the noisy machine translations. To alleviate this problem, we propose a novel
dependency-based evaluation metric which only employs the dependency information of the ref-
erences. We use two kinds of reference dependency structures: headword chain to capture the
long distance dependency information, and fixed and floating structures to capture the local con-
tinuous ngram. Experiment results show that our metric achieves higher correlations with human
judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra
linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance
which is better than METEOR and SEMPOS on system level, and is comparable with METEOR
on sentence level on WMT 2012 and WMT 2013.
1 Introduction
Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not
only evaluates the performance of MT systems, but also makes the development of MT systems rapider
(Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics
can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based
metrics.
The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR
(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing
the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect
the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric
(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and
syntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentially
noisy machine translations, so the improvement of their performance is restricted due to the serious
parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the
similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in
translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and
Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not
achieve the state-of-the-art performance.
In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only
employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation.
We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and
Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating
structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching
score between the headword chain and the translation, we use a distance-based similarity. Experiment
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2042
results show that our metric achieves higher correlations with human judgments than BLEU, TER and
HWCM on WMT 2012 and WMT 2013. After introducing extra resources and tuning parameters on
WMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable with
METEOR on sentence level on WMT 2012 and WMT2013.
The remainder of this paper is organized as follows. Section 2 describes our new reference dependency
based MT evaluation metric. In Section 3, we introduce some extra resources to this new metric. Section
4 presents the parameter tuning for the new metric. Section 5 gives the experiment results. Conclusions
and future work are discussed in Section 6.
2 RED: A Reference Dependency Based MT Evaluation Metric
The new metric is a REference Dependency based automatic evaluation metric, so we name it RED.
We present the new metric detailedly in this section. The description of dependency ngrams is given in
Section 2.1. The method to score the dependency ngram is presented in Section 2.2. At last, the method
of calculating the final score is introduced in Section 2.3.
2.1 Two Kinds of Dependency Ngrams
To capture both the long distance dependency information and the local continuous ngrams, we use both
the headword chain and the fixed-floating structures in our new metric, which correspond to the two
kinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.
Figure 1: An example of dependency tree.
Figure 2: Different kinds of structures extracted
from the dependency tree in Figure 1. (a): Head-
word chain. (b): Fixed structure. (c): Floating struc-
ture.
2.1.1 Headword chain
Headword chain is a sequence of words which corresponds to a path in the dependency tree (Liu and
Gildea, 2005). For example, Figure 2(a) is a 3-word headword chain extracted from the dependency tree
in Figure 1. Headword chain can represent the long distance dependency information, but cannot capture
most of the continuous ngrams. In our metric, headword chain corresponds to the headword chain ngram
in which the positions of the words are considered. So the form of headword chain ngram is expressed
as (w1
pos1
, w2
pos2
, ..., wn
posn
), where n is the length of the headword chain ngram. For example, the
headword chain in Figure 2(a) is expressed as (saw
2
, with
5
,magnifier
7
).
2.1.2 Fixed and floating structures
Fixed and floating structures are defined in Shen et al. (2010). Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. They are called fixed dependency structures
because the head is known or fixed. For example, Figure 2(b) shows a fixed structure. Floating structures
consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.
Each of the siblings must be a complete constituent. Figure 2(c) shows a floating structure. Fixed-
floating structures correspond to fixed-floating ngrams in our metric. Fixed-floating ngrams don?t need
the position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the
2043
Figure 3: An example of calculating matching score for a headword chain ngram
(saw
2
, with
5
,magnifier
7
). dis r
1
and dis r
2
are the distances between the corresponding two
words in the reference. dis h
1
and dis h
2
are the distances between the corresponding two words in the
hypothesis.
fixed-floating ngram. For example, the fixed structure in Figure 2(b) and the floating structre in Figure
2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.
2.2 Scoring Dep-ngrams
Headword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous. So the
scoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methods
in Section 2.2.1 and Section 2.2.2 respectively.
2.2.1 Scoring headword chain ngram
For a headword chain ngram (w1
pos1
, w2
pos2
, ..., wn
posn
), if we can find all these n words in the string
of the translation with the same order as they appear in the reference sentence, we consider it a match and
the matching score is a distance-based similarity which is calculated by the relative distance, otherwise it
is not a match and the score is 0. The matching score is a decimal value between 0 and 1, which is more
suitable than just use integer 0 and 1. For example, if the distance between two words in reference is 1,
but the distance in two different hypotheses are 2 and 5 respectively. It?s more reasonable to score them
0.5 and 0.2 rather than 1 and 0.
The relative distance dis r
i
between every two adjacent words in this kind of dep-ngram is calculated
by Formula (1), where pos
wi
is the position of word wi in the sentence. In Formula (1), we have
1 ? i ? n ? 1 and n is the length of the dep-ngram. Then a vector (dis r
1
, dis r
2
, ..., dis r
n?1
) is
obtained. In the same way, we obtain vector (dis h
1
, dis h
2
, ..., dis h
n?1
) for the translation side.
dis r
i
= |pos
w(i+1)
? pos
wi
| (1)
The matching score p
(d,hyp)
for a headword chain ngram (d) and the translation (hyp) is calculated
according to Formula (2), where n > 1. When the length of the dep-ngram equals 1, the matching score
equals 1 if the translation has the same word, otherwise, the matching score equals 0.
p
(d,hyp)
=
?
?
?
exp(?
?
n?1
i=1
|dis r
i
? dis h
i
|
n? 1
) if match
0 if unmatch
(2)
An example illustrating the calculation of the matching score p
(d,hyp)
is shown in Figure 3. There is
a 3-word headword chain ngram (saw
2
, with
5
,magnifier
7
) in the dependency tree of the reference.
2044
For this dep-3gram, the words are represented with underline in the reference dependency tree and the
reference sentence in Figure 3. We can also find all the same three underlined words in the translation
with the same order as they appear in the reference. Therefore, there is a match for this dep-3gram. To
compute the matching score between this dep-3gram and the translation, we have:
? Calculate the distance
dis r
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis r
2
= |pos
magnifier
? pos
with
| = |7? 5| = 2
dis h
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis h
2
= |pos
magnifier
? pos
with
| = |6? 5| = 1
? Get the matching score as Formula (3) according to Formula (2). d denotes
(saw
2
, with
5
,magnifier
7
) and hyp denotes the translation in the example.
p
(d,hyp)
= exp(?
|dis r
1
? dis h
1
|+ |dis r
2
? dis h
2
|
3? 1
) = exp(?
|3? 3|+ |2? 1|
3? 1
) = exp(?0.5)
(3)
We also tried other methods to calculate the matching score, such as the cosine distance and the
absolute distance, but the relative distance performed best. For a headword chain ngram with more than
one matches in the translation, we choose the one with the highest matching score.
2.2.2 Scoring fixed-floating ngram
The words in the fixed-floating ngram are continuous, so we restrict the matched string in the translation
also to being continuous. That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all these
n words continuous in the translation with the same order as they appear in the reference, we think the
dep-ngram can match with the translation. The matching score can be obtained by Formula (4), where d
stands for a fixed-floating ngram and hyp stands for the translation.
p
(d,hyp)
=
{
1 if match
0 if unmatch
(4)
2.3 Scoring RED
In the new metric, we use Fscore to obtain the final score. Fscore is calculated by Formula (5), where ?
is a value between 0 and 1.
Fscore =
precision ? recall
? ? precision+ (1? ?) ? recall
(5)
The dep-ngrams of the reference and the string of the translation are used to calculate the precision and
recall. In order to calculate precision, the number of the dep-ngrams in the translation should be given,
but there is no dependency tree for the translation in our method. We know that the number of dep-
ngrams has an approximate linear relationship with the length of the sentence, so we use the length of
the translation to replace the number of the dep-ngrams in the translation dependency tree. Recall can
be calculated directly since we know the number of the dep-ngrams in the reference. The precision and
recall are computed as follows.
precision =
?
d?D
n
p
(d,hyp)
len
h
, recall =
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n. len
h
is the length of the translation. count
n(ref)
is the
number of the dep-ngrams with the length of n in the reference.
2045
The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?
Fscore is calculated. w
ngram
(0 ? w
ngram
? 1) is the weight of dep-ngram with the length of n. Fscore
n
is the Fscore for the dep-ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? Fscore
n
) (6)
3 Introducing Extra Resources
Many automatic evaluation metrics can only find the exact match between the reference and the transla-
tion, and the information provided by the limited number of references is not sufficient. Some evaluation
metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the
reference information. We also introduce some extra resources to RED, such as stem, synonym and
paraphrase. The words within a sentence can be classified into content words and function words. The
effects of the two kinds of words are different and they shouldn?t have the same matching score, so we
introduce a parameter to distinguish them. The methods of applying these resources are introduced as
follows.
? Stem and Synonym
Stem(Porter, 2001) and synonym (WordNet
1
) are introduced to RED in the following three steps.
First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only
exact match but also stem and synonym are considered. We use stem and synonym together with
exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We
think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each
of the words in the dep-ngram has a matched word in the translation according to the alignment;
2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The
matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,
the match module score of a dep-ngram is calculated according to Formula (7). Different match
modules have different effects, so we give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (7)
m
i
is the match module (exact, stem or synonym) of the ith word in a dep-ngram. w
m
i
is the match
module weight of the ith word in a dep-ngram. n is the number of words in a dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t consider the dependency tree of the reference, because
paraphrases may not be contained in the headword chain and fixed-floating structures. First, the
alignment is obtained with METEOR Aligner, only considering paraphrase. Second, the matched
paraphrases are extracted from the alignment and defined as paraphrase-ngram. The score of a
paraphrase is 1? w
par
, where w
par
is the weight of paraphrase-ngram.
? Function word
We introduce a parameter w
fun
(0 ? w
fun
? 1) to distinguish function words and content words.
w
fun
is the weight of function words. The function word score of a dep-ngram or paraphrase-ngram
is computed according to Formula (8).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(8)
C
fun
is the number of function words in the dep-ngram or paraphrase-ngram. C
con
is the number
of content words in the dep-ngram or paraphrase-ngram.
1
http://wordnet.princeton.edu/
2046
We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated as
Formula (9), in which Fscore
p
is obtained using precison
p
and recall
p
as Formula (10).
REDp =
N
?
n=1
(w
ngram
? Fscore
p
n
) (9)
Fscore
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(10)
precision
p
and recall
P
in Formula (10) are calculated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
, recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length of n
in the reference. count
n
(par) is the number of paraphrases with length of n in reference. score
par
n
is
the match score of paraphrase-ngrams with the length of n. score
dep
n
is the match score of dep-ngrams
with the length of n. score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
) , score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
P
n
is the set of paraphrase-ngrams with the length of n. D
n
is the set of dep-ngrams with the length of n.
4 Parameter Tuning
There are several parameters in REDp, and different parameter values can make the performance of
REDp different. For example,w
ngram
represents the weight of dep-ngram with the length of n. The
effect of ngrams with different lengths are different, and they shouldn?t have the same weight. So we can
tune the parameters to find their best values.
We try a preliminary optimization method to tune parameters in REDp. A heuristic search is employed
and the parameters are classified into two subsets. The parameter optimization is a grid search over the
two subsets of parameters. When searching Subset 1, the parameters in Subset 2 are fixed, and then
Subset 1 and Subset 2 are exchanged to finish this iteration. Several iterations are executed to finish the
parameter tuning process. This heuristic search may not find the global optimum but it can save a lot of
time compared with exhaustive search. The optimization goal is to maximize the sum of Spearman?s ?
rank correlation coefficient on system level and Kendall?s ? correlation coefficient on sentence level. ?
is calculated using the following equation.
? = 1?
6
?
d
2
i
n(n
2
? 1)
where d
i
is the difference between the human rank and metric?s rank for system i. n is the number of
systems. ? is calculated as follows.
? =
number of concordant pairs? number of discordant pairs
number of concordant pairs + number of discordant pairs
The data of into-English tasks in WMT 2010 are used to tune parameters. The tuned parameters are
listed in Table 1.
5 Experiments
5.1 Data
The test sets in experiments are WMT 2012 and WMT 2013. The language pairs are German-to-English
(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-
English (ru-en). The number of translation systems for each language pair are showed in Table 2. For
each language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.
2047
Parameter ? w
fun
w
exact
w
stem
w
syn
w
par
w
1gram
w
2gram
w
3gram
tuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1
Table 1: Parameter values after tuning on WMT 2010. ? is from Formula (10). w
fun
is the weight of
function word. w
exact
, w
stem
andw
syn
are the weights of the three match modules ?exact stem synonym?
respectively. w
par
is the weight of paraphrase-ngram. w
1gram
, w
2gram
and w
3gram
are the weights of
dep-ngram with the length of 1, 2 and 3 respectively.
Language pairs cz-en de-en es-en fr-en ru-en
WMT2012 6 16 12 15 -
WMT2013 12 23 17 19 23
Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.
We parsed the reference into constituent tree by Berkeley parser
2
and then converted the constituent
tree into dependency tree by Penn2Malt
3
. Presumably, the performance of the new metric will be better
if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can
be used forever so it will not increase costs.
5.2 Baselines
In the experiments, we compare the performance of our metric with the widely-used lexicon-based met-
rics such as BLEU
4
, TER
5
and METEOR
6
, dependency-based metric HWCM and semantic-based metric
SEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to the
published results of WMT 2012.
The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25.
The results of METEOR are obtained by Version 1.4 with task option ?rank?. We re-implement HWCM
which employs an epsilon value of 10
?3
to replace zero for smoothing purpose. The correlations of
SEMPOS are obtained from the published results of WMT 2012 and WMT 2013.
5.3 Experiment Results
The experiments on both system level and sentence level are carried out. On system level, the correlations
are calculated using Spearman?s rank correlation coefficient ? (Pirie, 1988). Kendall?s rank correlation
coefficient ? (Kendall, 1938) is employed to evaluate the sentence level correlation. Our method performs
best when the maximum length of dep-ngram is set to 3, so we only present the results with the maximum
length of 3. RED represents the new metric with exact match and the parameter values are set as follows.
? = 0.5. w
1gram
= w
2gram
= w
3gram
= 1/3. REDp represents the new metric with extra resources
and tuned parameter values which are listed in Table (1).
5.3.1 System level correlations
The system level correlations are shown in Table 3. RED is better than BLEU, TER and HWCM on
average on both WMT 2012 and WMT 2013, which reflects that using syntactic information and only
parsing the reference side are helpful. REDp gets the best result on all of the language pairs except
cz-en on WMT 2012. The significant improvement from RED to REDp illustrates the effect of extra
resources and the parameter tuning. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. So the performance can be optimized
through parameter tuning. SEMPOS got the best correlation according to the published results of WMT
2
http://code.google.com/p/berkeleyparser/downloads/list
3
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html
4
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
5
http://www.cs.umd.edu/
?
snover/tercom
6
http://www.cs.cmu.edu/
?
alavie/METEOR/download/meteor-1.4.tgz
2048
2012, and METEOR got the best correlation according to the published results of WMT 2013 on into-
English task on system level. REDp gets better result than SEMPOS and METEOR on both WMT 2012
and WMT 2013, so REDp achieves the state-of-the-art performance on system level.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876
TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798
HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880
METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935
SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913
RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912
REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947
Table 3: System level correlations on WMT 2012 and WMT 2013. The value in bold is the best result in
each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
5.3.2 Sentence level correlations
The sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4. RED is better than
BLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic information
and only parsing the reference. By introducing extra resources and parameter tuning, REDp achieves
significant improvement over RED. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. A better performance can be exploited
through parameter tuning. From the results of REDp and METEOR, we can see that REDp gets the
comparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213
HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209
METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277
RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237
REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271
Table 4: Sentence level correlations on WMT 2012 and WMT 2013. The value in bold is the best result
in each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
6 Conclusion and Future Work
In this paper, we propose a reference dependency based automatic MT evaluation metric RED. The
new metric only uses the dependency trees of the reference, which avoids the parsing of the potentially
noisy translations. Both long distance dependency information and the local continuous ngrams are
captured by the new metric. The experiment results indicate that RED achieves better correlations than
BLEU, TER and HWCM on both system level and sentence level. REDp, the improved version of RED
through adding extra resources and preliminary parameter tuning, gets state-of-the-art results which are
better than METEOR and SEMPOS on system level. On sentence level, REDp gets the comparable
performance with METEOR.
In the future, we will use the dependency forest instead of the dependency tree to reduce the effect
of parsing errors. We will also apply RED and REDp to the tuning process of SMT to improve the
translation quality.
2049
Acknowledgements
The authors were supported by National Natural Science Foundation of China (Contract 61202216)
and National Natural Science Foundation of China (Contract 61379086). Qun Liu?s work was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin
City University. Sincere thanks to the three anonymous reviewers for their thorough reviewing and
valuable suggestions.
References
Boxing Chen and Roland Kuhn. 2011. Amber: A modified bleu, enhanced ranking metric. In Proceedings of
the Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings
of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages
85?91. Association for Computational Linguistics.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264. Association for
Computational Linguistics.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81?93.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation,
StatMT ?07, pages 228?231, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,
pages 25?32.
Chi-kiu Lo and Dekai Wu. 2013. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based MT evaluation metric. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
422?428, Sofia, Bulgaria, August. Association for Computational Linguistics.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.
Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approximating a deep-syntactic metric for mt evaluation and tun-
ing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98. Association for
Computational Linguistics.
Dennis Mehay and Chris Brew. 2007. BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation. In
Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).
F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for Computa-
tional Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for
machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-
tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
W Pirie. 1988. Spearman rank correlation coefficient. Encyclopedia of statistical sciences.
2050
Martin F Porter. 2001. Snowball: A language for stemming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Compu-
tational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?:
exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 259?268. Association for Computational Linguistics.
2051
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192?1201,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relaxed Cross-lingual Projection of Constituent Syntax
Wenbin Jiang and Qun Liu and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, liuqun, lvyajuan}@ict.ac.cn
Abstract
We propose a relaxed correspondence as-
sumption for cross-lingual projection of con-
stituent syntax, which allows a supposed
constituent of the target sentence to corre-
spond to an unrestricted treelet in the source
parse. Such a relaxed assumption fundamen-
tally tolerates the syntactic non-isomorphism
between languages, and enables us to learn
the target-language-specific syntactic idiosyn-
crasy rather than a strained grammar di-
rectly projected from the source language syn-
tax. Based on this assumption, a novel con-
stituency projection method is also proposed
in order to induce a projected constituent tree-
bank from the source-parsed bilingual cor-
pus. Experiments show that, the parser trained
on the projected treebank dramatically out-
performs previous projected and unsupervised
parsers.
1 Introduction
For languages with treebanks, supervised models
give the state-of-the-art performance in dependency
parsing (McDonald and Pereira, 2006; Nivre et al,
2006; Koo and Collins, 2010; Martins et al, 2010)
and constituent parsing (Collins, 2003; Charniak
and Johnson, 2005; Petrov et al, 2006). To break the
restriction of the treebank scale, lots of works have
been devoted to the unsupervised methods (Klein
and Manning, 2004; Bod, 2006; Seginer, 2007; Co-
hen and Smith, 2009) and the semi-supervised meth-
ods (Sarkar, 2001; Steedman et al, 2003; McClosky
et al, 2006; Koo et al, 2008) to utilize the unan-
notated text. In recent years, researchers have also
conducted many investigations on syntax projection
(Hwa et al, 2005; Ganchev et al, 2009; Smith and
Eisner, 2009; Jiang et al, 2010), in order to borrow
syntactic knowledge from another language.
Different from the bilingual parsing (Smith and
Smith, 2004; Burkett and Klein, 2008; Zhao et al,
2009; Huang et al, 2009; Chen et al, 2010) that
improves parsing performance with bilingual con-
straints, and the bilingual grammar induction (Wu,
1997; Kuhn, 2004; Blunsom et al, 2008; Snyder et
al., 2009) that induces grammar from parallel text,
the syntax projection aims to project the syntac-
tic knowledge from one language to another. This
seems especially promising for the languages that
have bilingual corpora parallel to resource-rich lan-
guages with large treebanks. Previous works mainly
focus on dependency projection. The dependency
relationship between words in the parsed source sen-
tences can be directly projected across the word
alignment to words in the target sentences, follow-
ing the direct correspondence assumption (DCA)
(Hwa et al, 2005). Due to the syntactic non-
isomorphism between languages, DCA assumption
usually leads to conflicting or incomplete projection.
Researchers have to adopt strategies to tackle this
problem, such as designing rules to handle language
non-isomorphism (Hwa et al, 2005), and resorting
to the quasi-synchronous grammar (Smith and Eis-
ner, 2009).
For constituency projection, however, the lack of
isomorphism becomes much more serious, since a
constituent grammar describes a language in a more
detailed way. In this paper we propose a relaxed
correspondence assumption (RCA) for constituency
1192
Through
a series of
experiments
he verified
the previous hypothesis
.
?
?? ??
?? ?
?? ?
??
d
IN
DT NN IN
NNS
NP
PPNP
NP
PP
PRP
NP
DT JJ NN
.
VBD NP
VP
S
PN
P NN
VV AS
LC DEG
NN
PU
[VBD]
[NP-DT-JJ-*]
[NP]
[VP][PP]
[S-PP-*-VP-*]
[S-PP-NP-VP-*]
[S]
TOP TOP
11
2
2 3
3
4
4
5
5
Figure 1: An example for constituency projection based on the RCA assumption. The projection is from English
to Chinese. A dash dot line links a projected constituent to its corresponding treelet, which is marked with gray
background; An Arabic numeral relates a directly-projected constituent to its counter-part in the source parse.
projection. It allows a supposed constituent of
the target sentence to correspond to an unrestricted
treelet in the source parse. Such a relaxed as-
sumption fundamentally tolerates the syntactic non-
isomorphism between languages, and enables us to
learn the target-language-specific syntactic idiosyn-
crasy, rather than induce a strained grammar directly
projected from the source language syntax. We also
propose a novel cross-lingual projection method for
constituent syntax based on the RCA assumption.
Given a word-aligned source-parsed bilingual cor-
pus, a PCFG grammar can be induced for the target
language by maximum likelihood estimation on the
exhaustive enumeration of candidate projected pro-
ductions, where each nonterminal in a production
is an unrestricted treelet extracted from the source
parse. The projected PCFG grammar is then used
to parse each target sentence under the guidance of
the corresponding source tree, so as to produce an
optimized projected constituent tree.
Experiments validate the effectiveness of the
RCA assumption and the constituency projection
method. We induce a projected Chinese constituent
treebank from the FBIS Chinese-English parallel
corpus with English sentences parsed by the Char-
niak parser. The Berkeley Parser trained on the pro-
jected treebank dramatically outperforms the previ-
ous projected and unsupervised parsers. This pro-
vides an promising substitute for unsupervised pars-
ing methods, to the resource-scarce languages that
have bilingual corpora parallel to resource-rich lan-
guages with human-annotated treebanks.
In the rest of this paper we first presents the RCA
assumption, and the algorithm used to determine the
corresponding treelet in the source parse for a can-
didate constituent in the target sentence. Then we
describe the induction of the projected PCFG gram-
mar and the projected constituent treebank from the
word-aligned source-parsed parallel corpus. After
giving experimental results and the comparison with
previous unsupervised and projected parsers, we fi-
nally conclude our work and point out several as-
pects to be improved in the future work.
2 Relaxed Correspondence Assumption
The DCA assumption (Hwa et al, 2005) works well
in dependency projection. A dependency grammar
describes a sentence in a compact manner where the
syntactic information is carried by the dependency
relationships between pairs of words. It is reason-
able to audaciously assume that the relationship of
1193
Algorithm 1 Treelet Extraction Algorithm.
1: Input: Tf : parse tree of source sentence f
2: e: target sentence
3: A: word alignment of e and f
4: for i, j s.t. 1 ? i < j ? |e| do ? all spans
5: t? EXTTREELET(e, i, j,Tf ,A)
6: T?i,j? ? PRUNETREE(t)
7: Output: treelet set T for all spans of e
8: function EXTTREELET(e, i, j, T, A)
9: if T aligns totally outside ei:j then
10: return ?
11: if T aligns totally inside ei:j then
12: return {T ? root}
13: t? {T ? root} ? partly aligned inside ei:j
14: for each subtree s of T do
15: t? t ? EXTTREELET(e, i, j, s,A)
16: return t
17: function PRUNETREE(T)
18: for each node n in T do
19: merge n?s successive empty children
20: t? T
21: while t has only one non-empty subtree do
22: t? the non-empty subtree of t
23: return t
a word pair in the source sentence also holds for
the corresponding word pair in the target sentence.
Compared with dependency grammar, constituent
grammar depicts syntax in a more complex way that
gives a sentence a hierarchically branched structure.
Therefore the lack of syntactic isomorphism for con-
stituency projection becomes much more serious, it
will be hard and inappropriate to directly project the
complex constituent structure from one language to
another.
For constituency projection, we propose a relaxed
corresponding assumption (RCA) to eliminate the
influence of syntactic non-isomorphism between the
source- and target languages. This assumption al-
lows a supposed constituent of the target sentence to
correspond to an unrestricted treelet in the source
parse. A treelet is a connected subgraph in the
source constituent tree, which covers a discontigu-
ous sequence of words of the source sentence. This
property enables a supposed constituent of the tar-
get sentence not necessarily to correspond to exactly
a constituent of the source parse, so as to funda-
mentally tolerate the syntactic non-isomorphism be-
tween languages. Figure 1 gives an example of re-
* *
DT JJ *
*
* NP
VP
S
[NP-DT-JJ-*]
TOP
DT JJ *
NP
[TOP-[S-*-*-[VP-*-[NP-DT-JJ-*]]-*]](a)
(b)
* NP
DT JJ *
*
NP *
*
S
[NP-DT-JJ-*]
TOP
DT JJ *
NP
[TOP-[S-*-[NP-[NP-DT-JJ-*]-*]-*-*]]
Figure 2: Two examples for treelet pruning. Asterisks
indicate eliminated subtrees, which are represented as
empty children of their parent nodes.
laxed correspondence.
2.1 Corresponding Treelet Extraction
According to the word alignment between the source
and target sentences, we can extract the treelet out of
the source parse for any possible constituent span of
the target sentence. Algorithm 1 shows the treelet
extraction algorithm.
Given the target sentence e, the parse tree Tf of
the source sentence f , and the word alignment A
between e and f , the algorithm extracts the corre-
sponding treelet out of Tf for each candidate span
of e (line 4-6). For a given span ?i, j?, its corre-
sponding treelet in Tf can be extracted by a recur-
sive top-down traversal in the tree. If all nodes in
the current subtree T align outside of source subse-
quence ei:j , the recursion stops and returns an empty
tree ?, indicating that the subtree is eliminated from
the final treelet (line 9-10). And, if all nodes in T
align inside ei:j , the root of T is returned as the con-
cise representation of the whole subtree (line 11-12).
For the third situation, that is to say T aligns partly
inside ei:j , the recursion has to continue to investi-
gate the subtrees of T (line 14-15). The recursive
traversal finally returns a treelet t that exactly corre-
1194
sponds to the candidate constituent span ?i, j? of the
source sentence.
We can find that even for a smaller span, the recur-
sive extraction procedure still starts from the root of
the source tree. This leads to a expatiatory treelet
with some redundant nodes on the top. Function
PRUNETREE takes charge of the treelet pruning (line
6). It traverses the treelet to merge the successive
empty sibling nodes (marked with asterisks) into one
(line 18-19), then conducts a top-down pruning to
delete the redundant branches until meeting a branch
with more than one non-empty subtrees (line 20-22).
Figure 2 shows the effect of the pruning operation
with two examples. The pruning operation maps the
two original treelets into the same simplified ver-
sion, that is, the pruned treelet. The branches pruned
out of the original treelet serve as the context of the
pruned treelet. The bracketed representations of the
pruned treelets, as shown above the treelet graphs,
are used as the nonterminals of the projected target
parses.
Since the overall complexity of the algorithm is
O(|e|3), it seems inefficient to collect the treelets
for all spans in the target sentence. But in fact it
runs fast on the realistic corpus in our experiments,
we assume that the function EXTTREELET doesn?t
always consume O(|e|) because of the more or less
isomorphism between two languages.
3 Projected Grammar and Treebank
This section describes how to build a projected con-
stituent treebank based on the RCA assumption. Ac-
cording to the last section, each span of the target
sentence could correspond to a treelet in the source
parse. If a span ?i, j? has a corresponding treelet t,
a candidate projected constituent can be defined as a
triple ?i, j, t?. For an n-way partition of this span,
?i, k1?, ?k1 + 1, k2?, .., ?kn?1 + 1, j?
if each sub-span ?kp?1+1, kp? corresponds to a can-
didate constituent ?kp?1+1, kp, tp?, a candidate pro-
jected production can then be defined, denoted as
?i, j, t? ? ?i, k1, t1??k1+1, k2, t2?..?kn?1+1, j, tn?
There may be many candidate projected constituents
because of arbitrary combination, the tree projec-
tion procedure aims to find the optimum tree from
the parse forest determined by these candidate con-
stituents. Each production in the optimum tree
should satisfy this principle: the rule used in this
production appears in the whole corpus as frequently
as possible.
However, due to translation diversity and word
alignment error, the real constituent tree of the target
sentence may not be contained in the candidate pro-
jected constituents. We propose a relaxed and fault-
tolerant tree projection strategy to tackle this prob-
lem. First, based on the distribution of candidate
projected constituents over each single sentence, we
estimate the distribution over the whole corpus for
the rules used in these constituents, so as to obtain
a projected PCFG grammar. Then, using a PCFG
parser and this grammar, we parse each target sen-
tence under the guidance of the candidate projected
constituent set of the target sentence, so as to ob-
tain the optimum projected tree as far as possible.
In the following, we first describe the estimation of
the projected PCFG grammar and then show the tree
projection procedure.
3.1 Projected PCFG Grammar
From a human-annotated treebank, we can induce a
PCFG grammar by estimating the frequency of the
production rules, which are contained in the produc-
tions of the trees. But for each target sentence we
don?t know which candidate productions consist the
correct constituent tree, so we can?t estimate the fre-
quency of the production rules directly.
A reasonable hypothesis is, if a candidate pro-
jected production for a target sentence happens to be
in the correct parse of the sentence, the rule used in
this production will appear frequently in the whole
corpus. We assume that each candidate projected
production may be a part of the correct parse, but
with different probabilities. If we give each candi-
date projected production an appropriate probabil-
ity and use this probability as the appearance fre-
quency of this production in the correct parse, we
can achieve an approximation of the PCFG gram-
mar hidden in the target sentences. In this work,
we restrict the productions to be binarized to reduce
the computational complexity. It results in a bina-
rized PCFG grammar, similar to previous unsuper-
vised works.
To estimate the frequencies of the candidate pro-
1195
ductions in the correct parse of the target sentence,
we need first estimate the frequencies of the candi-
date spans, which are described as follows:
p(?i, j?|e) = # of trees including ?i, j?# of all trees (1)
The count of all binary trees of a target sentence e
can be calculated similar to the ? value calculation
in the inside-outside algorithm. Without confusion,
we adopt the symbol ?(i, j) to denote the count of
binary tree for span ?i, j?:
?(i, j) =
?
????
????
1 i = j
j?1?
k=i
?(i, k) ? ?(k + 1, j) i < j
(2)
?(1, |e|) is the count of binary trees of target sen-
tence e. We also need to calculate the count of bi-
nary tree fragments that cover the nodes outside span
?i, j?. This is similar to the calculation of the ? value
in the inside-outside algorithm. We also adopt the
symbol ?(i, j) here:
?(i, j) =
?
??????????
??????????
1 i = 1, j = |e|
|e|?
k=j+1
?(i, k) ? ?(k + 1, |e|)
+
i?1?
k=1
?(k, j) ? ?(k, j ? 1) else
(3)
For simplicity we omit some conditions in above for-
mulas. The count of trees containing span ?i, j? is
?(i, j) ? ?(i, j). Equation 1 can be rewritten as
p(?i, j?|e) = ?(i, j) ? ?(i, j)?(1, |e|) (4)
On condition that ?i, j? is a span in the parse of e,
the probability that ?i, j? has two children ?i, k? and
?k + 1, j? is
p(?i, k??k + 1, j?|?i, j?) = ?(i, k) ? ?(k + 1, j)?(i, j) (5)
Therefore, the probability that ?i, j? is a span in the
parse of e and has two children ?i, k? and ?k + 1, j?
can be calculated as follows:
p(?i,j? ? ?i, k??k + 1, j?|e)
= p(?i, j?|e) ? p(?i, k??k + 1, j?|?i, j?)
= ?(i, j) ? ?(i, k) ? ?(k + 1, j)?(1, |e|)
(6)
Since each candidate projected span aligns to one
treelet at most, this probability is also the frequency
of the candidate projected production related to the
three spans.
The counting approach above is based on the as-
sumption that there is a uniform distribution over the
projected trees for every target sentence. The inside
and outside algorithms and the other counting for-
mulae are used to calculate the expected counts un-
der this assumption. This looks like a single iteration
of EM.
A binarized projected PCFG grammar can then be
easily induced by maximum likelihood estimation.
Due to word alignment errors, free translation, and
exhaustive enumeration of possible projected pro-
ductions, such a PCFG grammar may contain too
much noisy nonterminals and production rules. We
introduce a threshold bRULE to filter the grammar. A
production rule can be reserved only if its frequency
is larger than bRULE .
3.2 Relaxed Tree Projection
The projected PCFG grammar is used in the pro-
cedure of constituency projection. Such a gram-
mar, as a kind of global syntactic knowledge, can
attenuate the negative effect of word alignment er-
ror, free translation and syntactic non-isomorphism
for the constituency projection between each sin-
gle sentence pair. To obtain as optimal a projected
constituency tree as possible, we have to integrate
two kinds of knowledge: the local knowledge in
the candidate projected production set of the target
sentence, and the global knowledge in the projected
PCFG grammar.
The integrated projection strategy can be con-
ducted as follows. We parse each target sentence
with the projected PCFG grammar G, and use the
candidate projected production set D to guide the
PCFG parsing. The parsing procedure aims to find
an optimum projected tree, which maximizes both
the PCFG tree probability and the count of produc-
tions that also appear in the candidate projected pro-
1196
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 1  2  4  8  16  32  64  128  256  512 1024
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
# 
re
se
rv
ed
 ru
le
s
Pe
rc
en
ta
ge
 in
 a
ll r
ul
es
# selected NTs
# reserved rules
Percentage in all rules
Figure 3: Rule counts corresponding to selected nonter-
minal sets, and their frequency summation proportions to
the whole rule set.
duction set. The two optimization objectives can be
coordinated as follows:
y? = argmax
y
?
d?y
(p(d|G) ? e???(d,D)) (7)
Here, d represents a production; ? is a boolean func-
tion that returns 1 if d appears in D and returns 0
otherwise; ? is a weight coefficient that needs to be
tuned to maximize the quality of the projected tree-
bank.
4 Experiments
Our work focuses on the constituency projection
from English to Chinese. The FBIS Chinese-English
parallel corpus is used to obtain a projected con-
stituent treebank. It contains 239 thousand sentence
pairs, with about 6.9/8.9 million Chinese/English
words. We parse the English sentences with the
Charniak Parser (Charniak and Johnson, 2005), and
tag the Chinese sentences with a POS tagger imple-
mented faithfully according to (Collins, 2002) and
trained on the Penn Chinese Treebank 5.0 (Xue et
al., 2005). We perform word alignment by runing
GIZA++ (Och and Ney, 2000), and then use the
alignment results for constituency projection.
Following the previous works of unsupervised
constituent parsing, we evaluate the projected parser
on the subsets of CTB 1.0 and CTB 5.0, which con-
tain no more than 10 or 40 words after the removal
of punctuation. The gold-standard POS tags are di-
rectly used for testing. The evaluation for unsu-
pervised parsing differs slightly from the standard
 10
 15
 20
 25
 30
 35
 1  2  4  8  16  32  64  128  256  512  1024
Un
la
be
le
d 
F1
 (%
)
# selected NTs
Figure 4: Performance curve of the projected PCFG
grammars corresponding to different sizes of nontermi-
nal sets.
PARSEVAL metrics, it ignores the multiplicity of
brackets, brackets of span one, and the bracket la-
bels. In all experiments we report the unlabeled F1
value which is the harmonic mean of the unlabeled
precision and recall.
4.1 Projected PCFG Grammar
An initial projected PCFG grammar can be induced
from the word-aligned and source-parsed parallel
corpus according to section 3.1. Such an initial
grammar is huge and contains a large amount of
projected nonterminals and production rules, where
many of them come from free translation and word
alignment errors. We conservatively set the filtra-
tion threshold bRULE as 1.0 to discard the rules with
frequency less than one, the rule count falls dramat-
ically from 3.3 millions to 92 thousands.
Figure 3 shows the statistics of the remained pro-
duction rules. We sort the projected nonterminals
according to their frequencies and select the top 2N
(1 ? N ? 10) best ones, and then discard the rules
that fall out of the selected nonterminal set. The fre-
quency summation of the rule set corresponding to
32 best nonterminals accounts for nearly 90% of the
frequency summation of the whole rule set.
We use the developing set of CTB 1.0 (chapter
301-325) to evaluate the performance of a series of
filtered grammars. Figure 4 gives the unlabeled F1
value of each grammar on all trees in the developing
set. The filtered grammar corresponding to the set
of top 32 nonterminals achieves the highest perfor-
mance. We denote this grammar as G32 and use it
1197
 36
 37
 38
 39
 40
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5
Un
la
be
le
d 
F1
 (%
)
Weight coefficient
Figure 5: Performance curve of the Berkeley Parser
trained on 5 thousand projected trees. The weight co-
efficient ? ranges from 0 to 5.
in the following tree projection procedure.
4.2 Projected Treebank and Parser
The projected grammar G32 provides global syn-
tactic knowledge for constituency projection. Such
global knowledge and the local knowledge carried
by the candidate projected production set are inte-
grated in a linear weighted manner as in Formula
7. The weight coefficient ? is tuned to maximize
the quality of the projected treebank, which is in-
directly measured by evaluating the performance of
the parser trained on it.
We select the first 5 thousand sentence pairs from
the Chinese-English FBIS corpus, and induce a se-
ries of projected treebanks using different ?, ranging
from 0 to 5. Then we train the Berkeley Parser on
each projected treebank, and test it on the develop-
ing set of CTB 1.0. Figure 5 gives the performance
curve, which reports the unlabeled F1 values of the
projected parsers on all sentences of the developing
set. We find that the best performance is achieved
with ? between 1 and 2.5, with slight fluctuation
in this range. It can be concluded that, the pro-
jected PCFG grammar and the candidate projected
production set do represent two different kinds of
constraints, and we can effectively coordinate them
by tuning the weight coefficient. Since different ?
values in this range result in slight performance fluc-
tuation of the projected parser, we simply set it to 1
for the constituency projection on the whole FBIS
corpus.
There are more than 200 thousand projected trees
 45
 45.5
 46
 46.5
 47
 47.5
 48
 48.5
 49
 49.5
Un
la
be
le
d 
F1
 (%
)
Scale of treebank
5000 10000 20000 40000 80000 160000
Figure 6: Performance curve of the Berkeley Parser
trained on different amounts of best project trees. The
scale of the selected treebank ranges from 5000 to
160000.
induced from the Chinese-English FBIS corpus. It
is a heavy burden for a parser to train on so large a
treebank. And on the other hand, the free translation
and word alignment errors result in many projected
trees of poor-quality. We design a criteria to approx-
imate the quality of the projected tree y for the target
sentence x:
Q?(y) = |x|?1
??
d?y
(p(d|G) ? e???(d,D)) (8)
and use an amount of best projected trees instead of
the whole projected treebank to train the parser. Fig-
ure 6 shows the performance of the Berkeley Parser
trained on different amounts of selected trees. The
performance of the Berkeley Parser constantly im-
proves along with the increment of selected trees.
However, treebanks containing more than 40 thou-
sand projected trees can not brings significant im-
provement. The parser trained on 160 thousand trees
only achieves an F1 increment of 0.4 points over the
one trained on 40 thousand trees. This indicates that
the newly added trees do not give the parser more
information due to their projection quality, and a
larger parallel corpus may lead to better parsing per-
formance.
The Berkeley Parser trained on 160 thousand best
projected trees is used in the final test. Table 1
gives the experimental results and the comparison
with related works. This is a sparse table since the
experiments of previous researchers focused on dif-
ferent data sets. Our projected parser significantly
1198
System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40
(Klein and Manning, 2004) ? 46.7 ? ?
(Bod, 2006) ? 47.2 ? ?
(Seginer, 2007) ? ? 54.6 38.0
(Jiang et al, 2010) 40.4 ? ? ?
our work 52.1 54.4 54.5 49.2
Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous
works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences? 40 words from CTB standard
test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ? 10 words from CTB 1.0/CTB 5.0 after the
removal of punctuation; CTB5-ALL-40: sentences ? 40 words from CTB 5.0 after the removal of punctuation.
outperforms the parser of Jiang et al (2010), where
they directly adapt the DCA assumption of (Hwa
et al, 2005) from dependency projection to con-
stituency projection and resort to a better word align-
ment and a more complicated tree projection algo-
rithm. This indicates that the RCA assumption is
more suitable for constituency projection than the
DCA assumption, and can induce a better grammar
that much more reflects the language-specific syn-
tactic idiosyncrasy of the target language.
Our projected parser also obviously surpasses ex-
isting unsupervised parsers. The parser of Seginer
(2007) performs slightly better on CTB 5.0 sen-
tences no more than 10 words, but obviously falls
behind on sentences no more than 40 words. Fig-
ure 7 shows the unlabeled F1 of our parser on
a series of subsets of CTB 5.0 with different sen-
tence length upper limits. We find that even on the
whole treebank, our parser still gives a promising
result. Compared with unsupervised parsing, con-
stituency projection can make use of the syntactic
information of another language, so that it proba-
bly induce a better grammar. Although compar-
ing a syntax projection technique to supervised or
semi-supervised techniques seems unfair, it still sug-
gests that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is a promising sub-
stitute for unsupervised parsing.
5 Conclusion and Future Works
This paper describes a relaxed correspondence as-
sumption (RCA) for constituency projection. Un-
der this assumption a supposed constituent in the
target sentence can correspond to an unrestricted
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 10  20  30  40  50  60  70  80  90  100
Un
la
be
le
d 
F1
 (%
)
Upper limit of sentence length
+
Figure 7: Performance of the Berkeley Parser on subsets
of CTB 5.0 with different sentence length upper limits.
100+ indicates the whole treebank.
treelet in the parse of the source sentence. Different
from the direct correspondence assumption (DCA)
widely used in dependency projection, the RCA as-
sumption is more suitable for constituency projec-
tion, since it fundamentally tolerates the syntactic
non-isomorphism between the source and target lan-
guages. According to the RCA assumption we pro-
pose a novel constituency projection method. First, a
projected PCFG grammar is induced from the word-
aligned source-parsed parallel corpus. Then, the tree
projection is conducted on each sentence pair by a
PCFG parsing procedure, which integrates both the
global knowledge in the projected PCFG grammar
and the local knowledge in the set of candidate pro-
jected productions.
Experiments show that the parser trained on
the projected treebank significantly outperforms the
projected parsers based on the DCA assumption.
This validates the effectiveness of the RCA assump-
tion and the constituency projection method, and
indicates that the RCA assumption is more suit-
1199
able for constituency projection than the DCA as-
sumption. The projected parser also obviously sur-
passes the unsupervised parsers. This suggests
that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is an promising sub-
stitute for unsupervised methods.
Although achieving appealing results, our current
work is quite coarse and has many aspects to be im-
proved. First, the word alignment is the fundamental
precondition for projected grammar induction and
the following constituency projection, we can adopt
the better word alignment strategies to improve the
word alignment quality. Second, the PCFG grammar
is too weak due to its context free assumption, we
can adopt more complicated grammars such as TAG
(Joshi et al, 1975), in order to provide a more pow-
erful global syntactic constraints for the tree projec-
tion procedure. Third, the current tree projection
algorithm is too simple, more bilingual constraints
could lead to better projected trees. Last but not
least, the constituency projection and the unsuper-
vised parsing make use of different kinds of knowl-
edge, therefore the unsupervised methods can be in-
tegrated into the constituency projection framework
to achieve better projected grammars, treebanks, and
parsers.
Acknowledgments
The authors were supported by National Natural
Science Foundation of China Contract 90920004,
60736014 and 60873167. We are grateful to the
anonymous reviewers for their thorough reviewing
and valuable suggestions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of the NIPS.
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the COLING-ACL.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the ACL.
Wenliang Chen, Jun.ichi Kazama, and Kentaro Tori-
sawa. 2010. Bitext dependency parsing with bilingual
subtree constraints. In Proceedings of the ACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL-HLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the EMNLP.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of the 47th ACL.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering.
Wenbin Jiang, Yajuan Lu?, Yang Liu, and Qun Liu. 2010.
Effective constituent projection across languages. In
Proceedings of the COLING.
A. K. Joshi, L. S. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal Computer Systems Sci-
ence.
Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of the ACL.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of EMNLP.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the ACL.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
1200
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudoprojec-
tive dependency parsing with support vector machines.
In Proceedings of CoNLL, pages 221?225.
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the ACL.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the ACL.
David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In Proceedings of EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of the EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of the ACL.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the EACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the ACL-IJCNLP.
1201
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 412?420, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Iterative Annotation Transformation with Predict-Self Reestimation
for Chinese Word Segmentation
Wenbin Jiang and Fandong Meng and Qun Liu and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn
Abstract
In this paper we first describe the technol-
ogy of automatic annotation transformation,
which is based on the annotation adaptation
algorithm (Jiang et al2009). It can auto-
matically transform a human-annotated cor-
pus from one annotation guideline to another.
We then propose two optimization strategies,
iterative training and predict-self reestimation,
to further improve the accuracy of annota-
tion guideline transformation. Experiments on
Chinese word segmentation show that, the it-
erative training strategy together with predict-
self reestimation brings significant improve-
ment over the simple annotation transforma-
tion baseline, and leads to classifiers with sig-
nificantly higher accuracy and several times
faster processing than annotation adaptation
does. On the Penn Chinese Treebank 5.0,
it achieves an F-measure of 98.43%, signif-
icantly outperforms previous works although
using a single classifier with only local fea-
tures.
1 Introduction
Annotation guideline adaptation depicts a general
pipeline to integrate the knowledge of corpora with
different underling annotation guidelines (Jiang et
al., 2009). In annotation adaptation two classifiers
are cascaded together, where the classification re-
sults of the lower classifier are used as guiding fea-
tures of the upper classifier, in order to achieve more
accurate classification. This method can automat-
ically adapt the divergence between different an-
notation guidelines and bring improvement to Chi-
nese word segmentation. However, the need of cas-
caded classification decisions makes it less practical
for tasks of high computational complexity such as
parsing, and less efficient to incorporate more than
two annotated corpora.
In this paper, we first describe the algorithm of
automatic annotation transformation. It is based on
the annotation adaptation algorithm, and it focuses
on the automatic transformation (rather than adapta-
tion) of a human-annotated corpus from one annota-
tion guideline to another. First, a classifier is trained
on the corpus with an annotation guideline not de-
sired, it is used to classify the corpus with the an-
notation guideline we want, so as to obtain a corpus
with parallel annotation guidelines. Then a second
classifier is trained on the parallelly annotated cor-
pus to learn the statistical regularity of annotation
transformation, and it is used to process the previous
corpus to transform its annotation guideline to that
of the target corpus. Instead of the online knowl-
edge integration methodology of annotation adapta-
tion, annotation transformation can lead to improved
classification accuracy in an offline manner by using
the transformed corpora as additional training data
for the classifier. This method leads to an enhanced
classifier with much faster processing than the cas-
caded classifiers in annotation adaptation.
We then propose two optimization strategies, iter-
ative training and predict-self reestimation, to fur-
ther improve the accuracy of annotation transfor-
mation. Although the transformation classifiers
can only be trained on corpora with autogenerated
(rather than gold) parallel annotations, an iterative
training procedure can gradually improve the trans-
412
formation accuracy by iteratively optimizing the par-
allelly annotated corpora. Both source-to-target and
target-to-source annotation transformations are per-
formed in each training iteration, and the trans-
formed corpora are used to provide better annota-
tions for the parallelly annotated corpora of the next
iteration; then the better parallelly annotated corpora
will result in more accurate transformation classi-
fiers, which will generate better transformed corpora
in the new iteration. The predict-self reestimation
is based on the following hypothesis, a better trans-
formation result should be easier to be transformed
back to the original form. The predict-self heuristic
is also validated by Daume? III (2009) in unsuper-
vised dependency parsing.
Experiments in Chinese word segmentation show
that, the iterative training strategy together with
predict-self reestimation brings significant improve-
ment over the simple annotation transformation
baseline. We perform optimized annotation trans-
formation from the People?s Daily (Yu et al2001)
to the Penn Chinese Treebank 5.0 (CTB) (Xue et
al., 2005), in order to improve the word segmenter
with CTB annotation guideline. Compared to anno-
tation adaptation, the optimized annotation transfor-
mation strategy leads to classifiers with significantly
higher accuracy and several times faster processing
on the same data sets. On CTB 5.0, it achieves an F-
measure of 98.43%, significantly outperforms pre-
vious works although using a single classifier with
only local features.
The rest of the paper is organized as follows.
Section 2 describes the classification-based Chinese
word segmentation method. Section 3 details the
simple annotation transformation algorithm and the
two optimization methods. After the introduction of
related works in section 4, we give the experimental
results on Chinese word segmentation in section 5.
2 Classification-Based Chinese Word
Segmentation
Chinese word segmentation can be formalized as
the problem of sequence labeling (Xue and Shen,
2003), where each character in the sentence is given
a boundary tag denoting its position in a word. Fol-
lowing Ng and Low (2004), joint word segmenta-
tion and part-of-speech (POS) tagging can also be
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
solved in a character classification approach by ex-
tending the boundary tags to include POS informa-
tion. For word segmentation we adopt the 4 bound-
ary tags of Ng and Low (2004), b, m, e and s, where
b, m and e mean the beginning, the middle and the
end of a word, and s indicates a single-character
word. The word segmentation result can be gen-
erated by splitting the labeled character sequence
into subsequences of pattern s or bm?e, indicating
single-character words or multi-character words, re-
spectively.
We choose the perceptron algorithm (Collins,
2002) to train the character classifier. It is an online
training algorithm and has been successfully used in
many NLP tasks, including POS tagging (Collins,
2002), parsing (Collins and Roark, 2004) and word
segmentation (Zhang and Clark, 2007; Jiang et al
2008; Zhang and Clark, 2010).
The training procedure learns a discriminative
model mapping from the inputs x ? X to the outputs
y ? Y , where X is the set of sentences in the train-
ing corpus and Y is the set of corresponding labeled
results. We use the function GEN(x) to enumerate
the candidate results of an input x, and the function
? to map a training example (x, y) ? X ? Y to a
feature vector ?(x, y) ? Rd. Given the character
sequence x, the decoder finds the output F (x) that
maximizes the score function:
F (x) = argmax
y?GEN(x)
S(y|~?,?, x)
= argmax
y?GEN(x)
?(x, y) ? ~?
(1)
Where ~? ? Rd is the parameter vector (that is, the
discriminative model) and ?(x, y) ? ~? is the inner
product of ?(x, y) and ~?.
Algorithm 1 shows the perceptron algorithm for
tuning the parameter ~?. The ?averaged parameters?
413
Type Feature Templates
Unigram C?2 C?1 C0
C1 C2
Bigram C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Property Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Table 1: Feature templates for classification-based Chi-
nese segmentation model.
technology (Collins, 2002) is used for better per-
formance. The feature templates for the classifier
is shown in Table 1. C0 denotes the current char-
acter, while C?i/Ci denote the ith character to the
left/right of C0. The function Pu(?) returns true
for a punctuation character and false for others, the
function T (?) classifies a character into four types:
number, date, English letter and others.
3 Iterative and Predict-Self Annotation
Transformation
This section first describes the technology of au-
tomatic annotation transformation, then introduces
the two optimization strategies, iterative training and
predict-self reestimation. Iterative training takes
a global view, it conducts several rounds of bidi-
rectional annotation transformations, and improve
the transformation performance round by round.
Predict-self reestimation takes a local view instead,
it considers each training sentence, and improves the
transformation performance by taking into account
the predication result of the reverse transformation.
The two strategies can be adopted jointly to obtain
better transformation performance.
3.1 Automatic Annotation Transformation
Annotation adaptation can integrate the knowledge
from two corpora with different underling annota-
tion guidelines. First, a classifier (source classi-
fier) is trained on the corpus (source corpus) with
an annotation standard (source annotation) not de-
sired, it is then used to classify the corpus (target
corpus) with the annotation standard (target annota-
tion) we want. Then a second classifier (transforma-
tion classifier 1) is trained on the target corpus with
1It is called target classifier in (Jiang et al2009). We
think that transformation classifier better reflects its role, the
Type Feature Templates
Baseline C?2 C?1 C0
C1 C2
C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Guiding ?
C?2 ? ? C?1 ? ? C0 ? ?
C1 ? ? C2 ? ?
C?2C?1 ? ? C?1C0 ? ? C0C1 ? ?
C1C2 ? ? C?1C1 ? ?
Pu(C0) ? ?
T (C?2)T (C?1)T (C0)T (C1)T (C2) ? ?
Table 2: Feature templates for annotation transformation,
where ? is short for ?(C0), representing the source an-
notation of C0.
the source classifier?s classification result as guid-
ing features. In decoding, a raw sentence is first de-
coded by the source classifier, and then inputted into
the transformation classifier together with the anno-
tations given by the source classifier, so as to obtain
an improved classification result.
However, annotation adaptation has a drawback,
it has to cascade two classifiers in decoding to inte-
grate the knowledge in two corpora, thus seriously
degrades the processing speed. This paper describes
a variant of annotation adaptation, name annotation
transformation, aiming at automatic transformation
(rather than adaptation) between annotation stan-
dards of human-annotated corpora. In annotation
transformation, a source classifier and a transforma-
tion classifier are trained in the same way as in an-
notation adaptation. The transformation classifier is
used to process the source corpus, with the classi-
fication label derived from the segmented sentences
as the guiding features, so as to relabel the source
corpus with the target annotation guideline. By inte-
grating the target corpus and the transformed source
corpus for the training of the character classifier, im-
proved classification accuracy can be achieved.
Both the source classifier and the transforma-
tion classifier are trained with the perceptron algo-
rithm. The feature templates used for the source
classifier are the same with those for the baseline
renaming also avoids name confusion in the optimized annota-
tion transformation.
414
Algorithm 2 Baseline annotation transformation.
1: function ANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Ms?t ? TRANSTRAIN(Cst , Ct)
5: Cts ? TRANSANNOTATE(Ms?t, Cs)
6: Ct? ? Cts ? Ct
7: return Ct?
8: function DECODE(M, ?, x)
9: return argmaxy?GEN(x) S(y|M,?, x)
character classifier. The feature templates for the
transformation classifier are the same with those in
annotation adaptation, as listed in Table 2. Al-
gorithm 2 shows the overall training algorithm
for annotation transformation. Cs and Ct denote
the source corpus and the target corpus; Ms and
Ms?t denote the source classifier and the trans-
formation classifier; Cqp denotes the p corpus re-
labeled in q annotation guideline, for example Cts
is the source corpus transformed to target annota-
tion guideline; Functions TRAIN and TRANSTRAIN
both invoke the perceptron algorithm, yet with
different feature sets; Functions ANNOTATE and
TRANSANNOTATE call the function DECODE with
different models (source/transformation classifiers),
feature functions (without/with guiding features),
and inputs (raw/source-annotated sentences).
The best training iterations for the functions
TRAIN and TRANSTRAIN are determined on the de-
veloping sets of the source corpus and the target
corpus, respectively. In the algorithm the param-
eters corresponding to developing sets are omitted
for simplicity. Compared to the online knowledge
integration methodology of annotation adaptation,
annotation transformation leads to improved perfor-
mance in an offline manner by integrating corpora
before the training procedure. This manner could
achieve processing several times as fast as the cas-
caded classifiers in annotation adaptation. In the fol-
lowing we will describe the two optimization strate-
gies in details.
3.2 Iterative Training for Annotation
Transformation
The training of annotation transformation is based
on an auto-generated (rather than gold) parallelly an-
notated corpus, where the source annotation is pro-
Algorithm 3 Iterative annotation transformation.
1: function ITERANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Mt ? TRAIN(Ct)
5: Cts ? ANNOTATE(Mt, Cs)
6: repeat
7: Ms?t ? TRANSTRAIN(Cst , Ct)
8: Mt?s ? TRANSTRAIN(Cts, Cs)
9: Cts ? TRANSANNOTATE(Ms?t, Cs)
10: Cst ? TRANSANNOTATE(Mt?s, Ct)
11: Ct? ? Cts ? Ct
12: M? ? TRAIN(Ct?)
13: until EVAL(M?) converges
14: return Ct?
15: function DECODE(M, ?, x)
16: return argmaxy?GEN(x) S(y|M,?, x)
vided by the source classifier. Therefore, the perfor-
mance of transformation training is correspondingly
determined by the accuracy of the source classifier.
We propose an iterative training procedure to
gradually improve the transformation accuracy by
iteratively optimizing the parallelly annotated cor-
pora. In each training iteration, both source-to-target
and target-to-source annotation transformations are
performed, and the transformed corpora are used to
provide better annotations for the parallelly anno-
tated corpora of the next iteration. Then in the new
iteration, the better parallelly annotated corpora will
result in more accurate transformation classifiers, so
as to generate better transformed corpora.
Algorithm 3 shows the overall procedure of the
iterative training method. The loop of lines 6-13
iteratively performs source-to-target and target-to-
source annotation transformations. The source an-
notations of the parallelly annotated corpora, Cst and
Cts, are initialized by applying the source and tar-
get classifiers respectively on the target and source
corpora (lines 2-5). In each training iteration, the
transformation classifiers are trained on the current
parallelly annotated corpora (lines 7-8), they are
used to produce the transformed corpora (lines 9-10)
which provide better annotations for the parallelly
annotated corpora of the next iteration. The itera-
tive training terminates when the performance of the
classifier trained on the merged corpus Cts ? Ct con-
verges.
415
The discriminative training of TRANSTRAIN pre-
dicts the target annotations with the guidance of
source annotations. In the first iteration, the trans-
formed corpora generated by the transformation
classifiers are better than the initialized ones gener-
ated by the source and target classifiers, due to the
assistance of the guiding features. In the follow-
ing iterations, the transformed corpora provide bet-
ter annotations for the parallelly annotated corpora
of the subsequent iteration, the transformation ac-
curacy will improve gradually along with optimiza-
tion of the parallelly annotated corpora until conver-
gence.
3.3 Predict-Self Reestimation for Annotation
Transformation
The predict-self hypothesis is implicit in many unsu-
pervised learning approaches, such as Markov ran-
dom field. This methodology has also been success-
fully used by Daume? III (2009) in unsupervised de-
pendency parsing. The basic idea of predict-self is
that, if a prediction is a better candidate for an input,
it can be easier converted back to the original input
by a reverse procedure. If applied to the task of an-
notation transformation, predict-self indicates that a
better transformation candidate following the target
annotation guideline can be easier transformed back
to the original form following the source annotation
guideline.
The most intuitionistic strategy to introduce the
predict-self methodology into annotation transfor-
mation is using a reversed annotation transforma-
tion procedure to filter out unreliable predictions of
the previous transformation. In detail, a source-to-
target annotation transformation is performed on the
source annotated sentence to obtain a prediction that
follows the target annotation guideline, then a sec-
ond, target-to-source transformation is performed
on this prediction result to check whether it can
be transformed back to the previous source annota-
tion. Transformation results failing in this reversal
verification are discarded, so this strategy is named
predict-self filtration.
A more precious strategy can be called predict-
self reestimation. Instead of using the reversed
transformation procedure for filtration, the rees-
timation strategy integrates the scores given by
the source-to-target and target-to-source annotation
transformation models when evaluating the transfor-
mation candidates. By properly tuning the relative
weights of the two transformation directions, bet-
ter transformation performance would be achieved.
The scores of the two transformation models are
weighted integrated in a log-linear manner:
S+(y|Ms?t,Mt?s,?, x)
= (1? ?)? S(y|Ms?t,?, x)
+ ?? S(x|Mt?s,?, y)
(2)
The weight parameter ? is tuned on the develop-
ing set. To integrating the predict-self reestima-
tion into the iterative transformation training, a re-
versed transformation model is introduced and the
enhanced scoring function above is used when the
function TRANSANNOTATE invokes the function
DECODE.
4 Related Works
Researches focused on the automatic adaptation
between different corpora can be roughly clas-
sified into two kinds, adaptation between differ-
ent domains (with different statistical distribution)
(Blitzer et al2006; Daume? III, 2007), and adapta-
tion between different annotation guidelines (Jiang
et al2009; Zhu et al2011). There are also
some efforts that totally or partially resort to man-
ual transformation rules, to conduct treebank con-
version (Cahill and Mccarthy, 2002; Hockenmaier
and Steedman, 2007; Clark and Curran, 2009), and
word segmentation guideline transformation (Gao
et al2004; Mi et al2008). This work focuses
on the automatic transformation between annotation
guidelines, and proposes better annotation transfor-
mation technologies to improve the transformation
accuracy and the utilization rate of human-annotated
knowledge.
The iterative training procedure proposed in this
work shares some similarity with the co-training al-
gorithm in parsing (Sarkar, 2001), where the train-
ing procedure lets two different models learn from
each other during parsing the raw text. The key
idea of co-training is utilize the complementarity of
different parsing models to mine additional training
data from raw text, while iterative training for an-
notation transformation emphasizes the iterative op-
timization of the parellelly annotated corpora used
416
Partition Sections # of word
CTB
Training 1? 270 0.47M
400? 931
1001? 1151
Developing 301? 325 6.66K
Test 271? 300 7.82K
PD
Training 02? 06 5.86M
Test 01 1.07M
Table 3: Data partitioning for CTB and PD.
to train the transformation models. The predict-
self methodology is implicit in many unsupervised
learning approaches, it has been successfully used
by (Daume? III, 2009) in unsupervised dependency
parsing. We adapt this idea to the scenario of anno-
tation transformation to improve transformation ac-
curacy.
In recent years many works have been devoted to
the word segmentation task. For example, the in-
troduction of global training or complicated features
(Zhang and Clark, 2007; Zhang and Clark, 2010);
the investigation of word structures (Li, 2011);
the strategies of hybrid, joint or stacked modeling
(Nakagawa and Uchimoto, 2007; Kruengkrai et al
2009; Wang et al2010; Sun, 2011), and the semi-
supervised and unsupervised technologies utilizing
raw text (Zhao and Kit, 2008; Johnson and Gold-
water, 2009; Mochihashi et al2009; Hewlett and
Cohen, 2011). We estimate that the annotation trans-
formation technologies can be adopted jointly with
complicated features, system combination and semi-
supervised/unsupervised technologies to further im-
prove segmentation performance.
5 Experiments and Analysis
We perform annotation transformation from Peo-
ple?s Daily (PD) (Yu et al2001) to Penn Chi-
nese Treebank 5.0 (CTB) (Xue et al2005), follow-
ing the same experimental setting as the annotation
adaptation work (Jiang et al2009) for convenience
of comparison. The two corpora are segmented fol-
lowing different segmentation guidelines and differ
largely in quantity of data. CTB is smaller in size
with about 0.5M words, while PD is much larger,
containing nearly 6M words.
Test on (F1%)
Train on CTB SPD
CTB 97.35 86.65(? 10.70)
SPD 91.23(? 3.02) 94.25
Table 4: Performance of the perceptron classifiers for
Chinese word segmentation.
Model Time (s) Accuracy (F1%)
Merging 1.33 93.79
Anno. Adapt. 4.39 97.67
Anno. Trans. 1.33 97.69
Baseline 1.21 97.35
Table 5: Comparison of the baseline annotation transfor-
mation, annotation adaptation and a simple corpus merg-
ing strategy.
To approximate more general scenarios of anno-
tation adaptation problems, we extract from PD a
subset which is comparable to CTB in size. We ran-
domly select 20, 000 sentences (0.45M words) from
the PD training data as the new training set, and
1000/1000 sentences from the PD test data as the
new test/developing set. 2 We name the smaller ver-
sion of PD as SPD. The balanced source corpus and
target corpus also facilitate the investigation of an-
notation transformation.
5.1 Baseline Classifiers for Word Segmentation
We train the baseline perceptron classifiers de-
scribed in section 2 on the training sets of SPD
and CTB, using the developing sets to determine the
best training iterations. The performance measure-
ment indicators for word segmentation is balanced
F-measure, F = 2PR/(P + R), a function of Pre-
cision P and Recall R. where P is the percentage
of words in segmentation result that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Accuracies of the baseline classifiers are listed in
Table 4. We also report the performance of the clas-
sifiers on the test sets of the opposite corpora. Ex-
perimental results are in line with our expectations.
A classifier performs better in its corresponding test
set, and performs significantly worse on a test set
following a different annotation guideline.
2There are many extremely long sentences in original PD
corpus, we split them into normal sentences according to period
punctuations.
417
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training
Baseline annotation transformation
Figure 1: Learning curve of iterative training for annota-
tion transformation.
5.2 Annotation Transformation vs. Annotation
Adaptation
Experiments of annotation transformation are con-
ducted on the direction of SPD-to-CTB. The trans-
formed corpus can be merged into the regular cor-
pus, so as to train an enhanced classifier. As com-
parison, the cascaded model of annotation adapta-
tion (Jiang et al2009) is faithfully implemented
(yet using our feature representation) and tested on
the same adaptation direction.
Table 5 shows the performances of the classi-
fiers resulted by the baseline annotation transforma-
tion and annotation adaptation, as well as the clas-
sifier trained on the directly merged corpus. The
time costs for decoding are also listed to facilitate
the comparison of practicality. We find that the sim-
ple corpus merging strategy leads to dramatic de-
crease in accuracy, due to the different and incom-
patible annotation guidelines. The baseline annota-
tion transformation method leads to a classifier with
accuracy increment comparable to that of the anno-
tation adaptation strategy, while consuming only one
third of the decoding time.
5.3 Iterative Training with Predict-Self
Reestimation
We adopt the iterative training strategy to the base-
line annotation transformation model. The CTB de-
veloping set is used to determine the best training
iteration for annotation transformation from SPD to
CTB. After each iteration, we test the performance
of the classifier trained on the merged corpus. Fig-
ure 1 shows the performance curve, with iterations
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (F
%)
Predict-self ratio
Predict-self reestimation
Predict-self filtration
Baseline annotation transformation
Figure 2: Performance of predict-self filtration and
predict-self reestimation.
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training with predict-self reestimation
Iterative training
Figure 3: Learning curve of iterative training with
predict-self reestimation for annotation transformation.
ranging from 1 to 10. The performance of the base-
line annotation transformation model is naturally in-
cluded in the curve (located at iteration 1). The
curve shows that the performance of the classifier
trained on the merged corpus consistently improves
from iteration 2 to iteration 5.
Experimental results of predict-self filtration and
predict-self reestimation are shown in Figure 2.
The curve shows the performance of the predict-self
reestimation according to a series of weight param-
eters, ranging from 0 to 1 with step 0.05. The point
at ? = 0 shows the performance of the baseline
annotation transformation strategy. The upper hor-
izontal line shows the performance of predict-self
filtration. We find that predict-self filtration brings
slight improvement over the baseline, and predict-
self reestimation outperforms the filtration strategy
when ? falls in a proper range. An initial analysis
on the experimental results of predict-self filtration
418
Model Time (s) Accuracy (F1%)
SPD? CTB
Anno. Adapt. 4.39 97.67
Opt. Trans. 1.33 97.97
PD? CTB
Anno. Adapt. 4.76 98.15
Opt. Trans. 1.37 98.43
Previous Works
(Jiang et al2008) 97.85
(Kruengkrai et al2009) 97.87
(Zhang and Clark, 2010) 97.79
(Sun, 2011) 98.17
Table 6: The performance of the iterative annotation
transformation with predict-self reestimation compared
with annotation adaptation.
shows that, the filtration discards 5% of the train-
ing sentences and these discarded sentences contain
nearly 10% of training words. It can be confirmed
that the sentences discarded by predict-self filtra-
tion are much longer and more complicated. With a
properly tuned weight, predict-self reestimation can
make better use of the training data. The best F-
measure improvement achieved over the annotation
transformation baseline is 0.3 points, a little worse
than that brought by iterative training.
Figure 3 shows the performance curve of iterative
annotation transformation with predict-self reesti-
mation. We find that the predict-self reestimation
brings improvement to the iterative training at each
iteration. The maximum performance is achieved
at iteration 4. The corresponding model is evalu-
ated on the test set of CTB, table 6 shows the ex-
perimental results. Compared to annotation adapta-
tion, the optimized annotation transformation strat-
egy leads to a classifier with significantly higher ac-
curacy and several times faster processing. When
using the whole PD as the source corpus, the final
classifier 3 achieves an F-measure of 98.43%, sig-
nificantly outperforms previous works although us-
ing a single classifier with only local features. Of
course, the comparison between our system and pre-
vious works without using additional training data
is unfair. This work aim to find another way to im-
prove Chinese word segmentation, which focuses on
the collection of more training data instead of mak-
3The predict-self reestimation ratio ? is fixed after the first
training iteration for efficiency.
ing full use of a certain corpus. We believe that the
performance can be further improved by adopting
the advanced technologies of previous works, such
as complicated features and model combination.
Considering the fact that today some corpora for
word segmentation are really large (usually tens
of thousands of sentences), it is necessary to ob-
tain the latest CTB and investigate whether and
how much does annotation transformation bring im-
provement to a much higher baseline. On the other
hand, it is valuable to conduct experiments with
more source-annotated training data, such as the
SIGHAN dataset, to investigate the trend of im-
provement along with the increment of the addi-
tional annotated sentences. It is also valuable to
evaluate the improved word segmenter on the out-
of-domain datasets. However, currently most cor-
pora for Chinese word segmentation do not explic-
itly distinguish the domains of their data sections, it
makes such evaluations difficult to conduct.
6 Conclusion and Future Works
In this paper, we first describe an annotation trans-
formation algorithm to automatically transform a
human-annotated corpus from one annotation guide-
line to another. Then we propose two optimization
strategies, iterative training and predict-self reesti-
mation, to further improve the accuracy of anno-
tation guideline transformation. On Chinese word
segmentation, the optimized annotation transforma-
tion strategy leads to classifiers with obviously bet-
ter performance and several times faster processing
on the same datasets, compared to annotation adap-
tation. When adopting the whole PD as the source
corpus, the final classifier significantly outperforms
previous works on CTB 5.0, although using a single
classifier with only local features.
As future works, we will investigate the accel-
eration of the iterative training and the weight pa-
rameter tuning, and extend the optimized annotation
transformation strategy to joint Chinese word seg-
mentation and POS tagging, parsing and other NLP
tasks.
Acknowledgments
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004
419
and 61100082, and 863 State Key Project No.
2011AA01A207. We are grateful to the anonymous
reviewers for their thorough reviewing and valuable
suggestions.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Aoife Cahill and Mairead Mccarthy. 2002. Automatic
annotation of the penn treebank with lfg f-structure in-
formation. In in Proceedings of the LREC Workshop.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of ccg and penn treebank parsers. In Pro-
ceedings of ACL-IJCNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL 2004.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Hal Daume? III. 2009. Unsupervised search-based struc-
tured prediction. In Proceedings of ICML.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceedings
of ACL.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with bve and mdl. In Pro-
ceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
a corpus of ccg derivations and dependency structures
extracted from the penn treebank. In Computational
Linguistics, volume 33(3), pages 355?396.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Junichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chineseword segmenta-
tion. In Proceedings of ACL.
Haitao Mi, Deyi Xiong, and Qun Liu. 2008. Research
on strategy of integrating chinese lexical analysis and
parser. In Journal of Chinese Information Processing.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hy-
brid approach to word segmentation and pos tagging.
In Proceedings of ACL.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A
character-based joint model for chinese word segmen-
tation. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan,
Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao,
and Weidong Zhan. 2001. Processing norms of mod-
ern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and pos-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
for word segmentation and named entity recognition.
In Proceedings of SIGHAN Workshop.
Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better
automatic treebank conversion using a feature-based
approach. In Proceedings of ACL.
420
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546?556,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Modeling Term Translation for Document-informed Machine Translation
Fandong Meng
1, 2
Deyi Xiong
3
Wenbin Jiang
1, 2
Qun Liu
4, 1
1
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
{mengfandong,jiangwenbin,liuqun}@ict.ac.cn
3
School of Computer Science and Technology, Soochow University
dyxiong@suda.edu.cn
4
Centre for Next Generation Localisation, Dublin City University
Abstract
Term translation is of great importance for
statistical machine translation (SMT), es-
pecially document-informed SMT. In this
paper, we investigate three issues of term
translation in the context of document-
informed SMT and propose three cor-
responding models: (a) a term trans-
lation disambiguation model which se-
lects desirable translations for terms in the
source language with domain information,
(b) a term translation consistency model
that encourages consistent translations for
terms with a high strength of translation
consistency throughout a document, and
(c) a term bracketing model that rewards
translation hypotheses where bracketable
source terms are translated as a whole
unit. We integrate the three models into
hierarchical phrase-based SMT and eval-
uate their effectiveness on NIST Chinese-
English translation tasks with large-scale
training data. Experiment results show
that all three models can achieve sig-
nificant improvements over the baseline.
Additionally, we can obtain a further
improvement when combining the three
models.
1 Introduction
A term is a linguistic expression that is used as
the designation of a defined concept in a language
(ISO 1087). As terms convey concepts of a text,
term translation becomes crucial when the text is
translated from its original language to another
language. The translations of terms are often af-
fected by the domain in which terms are used and
the context that surrounds terms (Vasconcellos et
al., 2001). In this paper, we study domain-specific
and context-sensitive term translation for SMT.
In order to achieve this goal, we focus on three
issues of term translation: 1) translation ambigu-
ity, 2) translation consistency and 3) bracketing.
First, term translation ambiguity is related to trans-
lations of the same term in different domains. A
source language term may have different transla-
tions when it occurs in different domains. Second,
translation consistency is about consistent trans-
lations for terms that occur in the same document.
Usually, it is undesirable to translate the same term
in different ways as it occurs in different parts of
a document. Finally, bracketing concerns whether
a multi-word term is bracketable during transla-
tion. Normally, a multi-word term is translated as
a whole unit into a contiguous target string.
We study these three issues in the context
of document-informed SMT. We use document-
informed information to disambiguate term trans-
lations in different documents and maintain con-
sistent translations for terms that occur in the same
document. We propose three different models for
term translation that attempt to address the three
issues mentioned above. In particular,
? Term Translation Disambiguation Model: In
this model, we condition the translations of
terms in different documents on correspond-
ing per-document topic distributions. In do-
ing so, we enable the decoder to favor trans-
lation hypotheses with domain-specific term
translations.
? Term Translation Consistency Model: This
model encourages the same terms with a high
strength of translation consistency that occur
in different parts of a document to be trans-
lated in a consistent fashion. We calculate
the translation consistency strength of a term
based on the topic distribution of the docu-
ments where the term occurs in this model.
? Term Bracketing Model: We use the brack-
eting model to reward translation hypothe-
546
ses where bracketable multi-word terms are
translated as a whole unit.
We integrate the three models into hierarchical
phrase-based SMT (Chiang, 2007). Large-scale
experiment results show that they are all able to
achieve significant improvements of up to 0.89
BLEU points over the baseline. When simulta-
neously integrating the three models into SMT,
we can gain a further improvement, which outper-
forms the baseline by up to 1.16 BLEU points.
In the remainder of this paper, we begin with
a brief overview of related work in Section 2,
and bilingual term extraction in Section 3. We
then elaborate the proposed three models for term
translation in Section 4. Next, we conduct experi-
ments to validate the effectiveness of the proposed
models in Section 5. Finally, we conclude and pro-
vide directions for future work in Section 6.
2 Related Work
In this section, we briefly introduce related work
and highlight the differences between our work
and previous studies.
As we approach term translation disambigua-
tion and consistency via topic modeling, our mod-
els are related to previous work that explores the
topic model (Blei et al., 2003) for machine trans-
lation (Zhao and Xing, 2006; Su et al., 2012;
Xiao et al., 2012; Eidelman et al., 2012). Zhao
and Xing (2006) employ three models that enable
word alignment process to leverage topical con-
tents of document-pairs with topic model. Su et al.
(2012) establish the relationship between out-of-
domain bilingual corpus and in-domain monolin-
gual corpora via topic mapping and phrase-topic
distribution probability estimation for translation
model adaptation. Xiao et al. (2012) propose a
topic similarity model for rule selection. Eidel-
man et al. (2012) use topic models to adapt lexical
weighting probabilities dynamically during trans-
lation. In these studies, the topic model is not used
to address the issues of term translation mentioned
in Section 1.
Our work is also related to document-level
SMT in that we use document-informed informa-
tion for term translation. Tiedemann (2010) pro-
pose cache-based language and translation mod-
els, which are built on recently translated sen-
tences. Gong et al. (2011) extend this by further
introducing two additional caches. They employ
a static cache to store bilingual phrases extracted
from documents in training data that are similar to
the document being translated and a topic cache
with target language topic words. Recently we
have also witnessed efforts that model lexical co-
hesion (Hardmeier et al., 2012; Wong and Kit,
2012; Xiong et al., 2013a; Xiong et al., 2013b)
as well as coherence (Xiong and Zhang, 2013)
for document-level SMT. Hasler et al. (2014a)
use topic models to learn document-level transla-
tion probabilities. Hasler et al. (2014b) use topic-
adapted model to improve lexical selection. The
significant difference between our work and these
studies is that term translation has not been inves-
tigated in these document-level SMT models.
Itagaki and Aikawa (2008) employ bilingual
term bank as a dictionary for machine-aided trans-
lation. Ren et al. (2009) propose a binary feature
to indicate whether a bilingual phrase contains a
term pair. Pinis and Skadins (2012) investigate that
bilingual terms are important for domain adapta-
tion of machine translation. These studies do not
focus on the three issues of term translation as
discussed in Section 1. Furthermore, domain and
document-informed information is not used to as-
sist term translation.
Itagaki et al. (2007) propose a statistical method
to calculate translation consistency for terms with
explicit domain information. Partially inspired
by their study, we introduce a term translation
consistency metric with document-informed infor-
mation. Furthermore, we integrate the proposed
term translation consistency model into an actual
SMT system, which has not been done by Itagaki
et al. (2007). Ture et al. (2012) use IR-inspired
tf-idf scores to encourage consistent translation
choice. Guillou (2013) investigates what kind of
words should be translated consistently. Term
translation consistency has not been investigated
in these studies.
Our term bracketing model is also related
to Xiong et al. (2009)?s syntax-driven bracket-
ing model for phrase-based translation, which pre-
dicts whether a phrase is bracketable or not using
rich syntactic constraints. The difference is that
we construct the model with automatically created
bilingual term bank and do not depend on any syn-
tactic knowledge.
3 Bilingual Term Extraction
Bilingual term extraction is to extract terms from
two languages with the purpose of creating or ex-
547
tending a bilingual term bank, which in turn can
be used to improve other tasks such as information
retrieval and machine translation. In this paper, we
want to automatically build a bilingual term bank
so that we can model term translation to improve
translation quality of SMT. Our interest is to ex-
tract multi-word terms.
Currently, there are mainly two strategies to
conduct bilingual term extraction from parallel
corpora. One of them is to extract term candi-
dates separately for each language according to
monolingual term metrics, such as C-value/NC-
value (Frantzi et al., 1998; Vu et al., 2008), or
other common cooccurrence measures such as
Log-Likelihood Ratio, Dice coefficient and Point-
wise Mutual Information (Daille, 1996; Piao et
al., 2006). The extracted monolingual terms are
then paired together (Hjelm, 2007; Fan et al.,
2009; Ren et al., 2009). The other strategy is to
align words and word sequences that are transla-
tion equivalents in parallel corpora and then clas-
sify them into terms and non-terms (Merkel and
Foo, 2007; Lefever et al., 2009; Bouamor et al.,
2012). In this paper, we adopt the first strategy.
In particular, for each sentence pair, we collect all
source phrases which are terms and find aligned
target phrases for them via word alignments. If
the target side is also a term, we store the source
and target term as a term pair.
We conduct monolingual term extraction using
the C-value/NC-value metric and Log-Likelihood
Ratio (LLR) measure respectively. We then com-
bine terms extracted according to the two metrics
mentioned above. For the C-value/NC-value met-
ric based term extraction, we implement it in the
same way as described in Frantzi et al. (1998).
This extraction method recognizes linguistic pat-
terns (mainly noun phrases) listed as follows.
((Adj|Noun)
+
|((Adj|Noun)
?
(NounPrep)
?
)(Adj|Noun)
?
)Noun
It captures the linguistic structures of terms. For
the LLR metric based term extraction, we imple-
ment it according to Daille (1996), who estimate
the propensity of two words to appear together as a
multi-word expression. We then adopt LLR-based
hierarchical reducing algorithm proposed by Ren
et al. (2009) to extract terms with arbitrary lengths.
Since the C-value/NC-value metric based extrac-
tion method can obtain terms in strict linguistic
patterns while the LLR measure based method ex-
tracts more flexible terms, these two methods are
complementary to each other. Therefore, we use
these two methods to extract monolingual multi-
word terms and then combine the extracted terms.
4 Models
This section presents the three models of term
translation. They are the term translation dis-
ambiguation model, term translation consistency
model and term bracketing model respectively.
4.1 Term Translation Disambiguation Model
The most straightforward way to disambiguate
term translations in different domains is to cal-
culate the conditional translation probability of
a term given domain information. We use the
topic distribution of a document obtained by a
topic model to represent the domain information
of the document. Since Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) is the most widely-
used topic model, we exploit it for inferring topic
distributions of documents. Xiao et al. (2012)
proposed a topic similarity model for rule selec-
tion. Different from their work, we take an eas-
ier strategy that estimates topic-conditioned term
translation probabilities rather than rule-topic dis-
tributions. This makes our model easily scalable
on large training data.
With the bilingual term bank created from the
training data, we calculate the source-to-target
term translation probability for each term pair con-
ditioned on the topic distribution of the source
document where the source term occurs. We main-
tain a K-dimension (K is the number of topics)
vector for each term pair. The k-th component
p(t
e
|t
f
, z = k) measures the conditional transla-
tion probability from source term t
f
to target term
t
e
given the topic k.
We calculate p(t
e
|t
f
, z = k) via maximum
likelihood estimation with counts from training
data. When the source part of a bilingual term
pair occurs in a document D with topic distribu-
tion p(z|D) estimated via LDA tool, we collect
an instance (t
f
, t
e
, p(z|D), c), where c is the frac-
tion count of the instance as described in Chiang
(2007). After collection, we get a set of instances
I = {(t
f
, t
e
, p(z|D), c)}with different document-
topic distributions for each bilingual term pair. Us-
ing these instances, we calculate the probability
548
p(t
e
|t
f
, z = k) as follows:
p(t
e
|t
f
, z = k)
=
?
i?I,i.t
f
=t
f
,i.t
e
=t
e
i.c ? p(z = k|D)
?
i?I,i.t
f
=t
f
i.c ? p(z = k|D)
(1)
We associate each extracted term pair in our
bilingual term bank with its corresponding topic-
conditioned translation probabilities estimated in
the Eq. (1). When translating sentences of docu-
ment D
?
, we first get the topic distribution of D
?
using LDA tool. Given a sentence which contains
T terms {t
f
i
}
T
1
in D
?
, our term translation disam-
biguation model TermDis can be denoted as
TermDis =
T
?
i=1
P
d
(t
e
i
|t
f
i
, D
?
) (2)
where the conditional source-to-target term trans-
lation probability P
d
(t
e
i
|t
f
i
, D
?
) given the docu-
ment D
?
is formulated as follows:
P
d
(t
e
i
|t
f
i
, D
?
)
=
K
?
k=1
p(t
e
i
|t
f
i
, z = k) ? p(z = k|D
?
) (3)
Whenever a source term t
f
i
is translated into t
e
i
,
we check whether the pair of t
f
i
and its translation
t
e
i
can be found in our bilingual term bank. If it
can be found, we calculate the conditional transla-
tion probability from t
f
i
to t
e
i
given the document
D
?
according to Eq. (3).
The term translation disambiguation model is
integrated into the log-linear model of SMT as a
feature. Its weight is tuned via minimum error rate
training (MERT) (Och, 2003). Through the fea-
ture, we can enable the decoder to favor translation
hypotheses that contain target term translations ap-
propriate for the domain represented by the topic
distribution of the corresponding document.
4.2 Term Translation Consistency Model
The term translation disambiguation model helps
the decoder select appropriate translations for
terms that are in accord with their domains. Yet
another translation issue related to the domain-
specific term translation is to what extent a term
should be translated consistently given the domain
where it occurs. Term translation consistency in-
dicates the translation stability that a source term
is translated into the same target term (Itagaki et
al., 2007). When translating a source term, if the
translation consistency strength of the source term
is high, we should take the corresponding target
term as the translation for it. Otherwise, we may
need to create a new translation for it according to
its context. In particular, we want to enable the
decoder to choose between: 1) translating a given
source term into the extracted corresponding tar-
get term or 2) translating it in another way accord-
ing to the strength of its translation consistency.
In doing so, we can encourage consistent transla-
tions for terms with a high translation consistency
strength throughout a document.
Our term translation consistency model can ex-
actly measure the strength of term translation con-
sistency in a document. Since the essential com-
ponent of our term translation consistency model
is the translation consistency strength of the source
term estimated under the topic distribution, we de-
scribe how to calculate it before introducing the
whole model.
With the bilingual term bank created from
training data, we first group each source term
and all its corresponding target terms into a 2-
tuple G?t
f
, Set(t
e
)?, where t
f
is the source term
and Set(t
e
) is the set of t
f
?s corresponding tar-
get terms. We maintain a K-dimension (K is
the number of topics) vector for each 2-tuple
G?t
f
, Set(t
e
)?. The k-th component measures the
translation consistency strength cons(t
f
, k) of the
source term t
f
given the topic k.
We calculate cons(t
f
, k) for each
G?t
f
, Set(t
e
)? with counts from training data as
follows:
cons(t
f
, k) =
M
?
m=1
N
m
?
n=1
(
q
mn
? p(k|m)
Q
k
)
2
(4)
Q
k
=
M
?
m=1
N
m
?
n=1
q
mn
? p(k|m) (5)
where M is the number of documents in which
the source term t
f
occurs, N
m
is the number of
unique corresponding term translations of t
f
in the
mth document, q
mn
is the frequency of the nth
translation of t
f
in the mth document, p(k|m) is
the conditional probability of the mth document
over topic k, and Q
k
is the normalization factor.
All translations of t
f
are from Set(t
e
). We adapt
Itagaki et al. (2007)?s translation consistency met-
ric for terms to our topic-based translation consis-
tency measure in the Eq. (4). This equation cal-
culates the translation consistency strength of the
source term t
f
given the topic k according to the
distribution of t
f
?s translations in each document
549
where they occur. According to Eq. (4), the trans-
lation consistency strength is a score between 0
and 1. If a source term only occurs in a document
and all its translations are the same, the translation
consistency strength of this term is 1.
We reorganize our bilingual term bank into a
list of 2-tuples G?t
f
, Set(t
e
)?s, each of which is
associated with a K-dimension vector storing the
topic-conditioned translation consistency strength
calculated in the Eq. (4). When translating sen-
tences of document D, we first get the topic dis-
tribution of D via LDA tool. Given a sentence
which contains T terms {t
f
i
}
T
1
in D, our term
translation consistency model TermCons can be
denoted as
TermCons =
T
?
i=1
exp(S
c
(t
f
i
|D)) (6)
where the strength of translation consistency for
t
f
i
given the document D is formulated as fol-
lows:
S
c
(t
f
i
|D) = log(
K
?
k=1
cons(t
f
i
, k) ? p(k|D)) (7)
During decoding, whenever a hypothesis just
translates a source term t
f
i
into t
e
, we check
whether the translation t
e
can be found in Set(t
e
)
of t
f
i
from the reorganized bilingual term bank. If
it can be found, we calculate the strength of trans-
lation consistency for t
f
i
given the document D
according to Eq. (7) and take it as a soft con-
straint. If the S
c
(t
f
i
|D) of t
f
i
is high, the decoder
should translate t
f
i
into the extracted correspond-
ing target terms. Otherwise, the decoder will se-
lect translations from outside of Set(t
e
) for t
f
i
. In
doing so, we encourage terms to be translated in
a topic-dependent consistency pattern in the test
data similar to that in the training data so that we
can control the translation consistency of terms in
the test data.
The term translation consistency model is also
integrated into the log-linear model of SMT as a
feature. Through the feature, we can enable the
decoder to translate terms with a high translation
consistency in a document into corresponding tar-
get terms from our bilingual term bank rather than
other translations in a consistent fashion.
4.3 Term Bracketing Model
The term translation disambiguation model and
consistency model concern the term translation ac-
curacy with domain information. We further pro-
pose a term bracketing model to guarantee the in-
tegrality of term translation. Xiong et al. (2009)
proposed a syntax-driven bracketing model for
phrase-based translation, which predicts whether
a phrase is bracketable or not using rich syntac-
tic constraints. If a source phrase remains con-
tiguous after translation, they refer to this type of
phrase as bracketable phrase, otherwise unbrack-
etable phrase. For multi-word terms, it is also
desirable to be bracketable since a source term
should be translated as a whole unit and its trans-
lation should be contiguous.
In this paper, we adapt Xiong et al. (2009)?s
bracketing approach to term translation and build
a classifier to measure the probability that a source
term should be translated in a bracketable man-
ner. For all source parts of the extracted bilingual
term bank, we find their target counterparts in the
word-aligned training data. If the corresponding
target counterpart remains contiguous, we take the
source term as a bracketable instance, otherwise
an unbracketable instance. With these bracketable
and unbracketable instances, we train a maximum
entropy binary classifier to predict bracketable (b)
probability of a given source term t
f
within par-
ticular contexts c(t
f
). The binary classifier is for-
mulated as follows:
P
b
(b|c(t
f
)) =
exp(
?
j
?
j
h
j
(b, c(t
f
)))
?
b
?
exp(
?
j
?
j
h
j
(b
?
, c(t
f
)))
(8)
where h
j
? {0, 1} is a binary feature function and
?
j
is the weight of h
j
. We use the following fea-
tures: 1) the word sequence of the source term, 2)
the first word of the source term, 3) the last word
of the source term, 4) the preceding word of the
first word of the source term, 5) the succeeding
word of the last word of the source term, and 6)
the number of words in the source term.
Given a source sentence which contains T terms
{t
f
i
}
T
1
, our term bracketing model TermBrack
can be denoted as
TermBrack =
T
?
i=1
P
b
(b|c(t
f
i
)) (9)
Whenever a hypothesis just covers a source term
t
f
i
, we calculate the bracketable probability of t
f
i
according to Eq. (8).
The term bracketing model is integrated into the
log-linear model of SMT as a feature. Through the
feature, we want the decoder to translate source
terms with a high bracketable probability as a
whole unit.
550
Source Target D M
F?angy`u X`?t?ong defence mechanisms
F?angy`u X`?t?ong defence systems
F?angy`u X`?t?ong defense programmes 470 56
F?angy`u X`?t?ong prevention systems
... ...
Zh`anlu`e D?aod`an F?angy`u X`?t?ong strategic missile defense system 7 0
Table 1: Examples of bilingual terms extracted from the training data. ?D? means the total number of
documents in which the corresponding source term occurs and ?M? denotes the number of documents in
which the corresponding source term is translated into different target terms. The source side is Chinese
Pinyin. To save space, we do not list all the 23 different translations of the source term ?F?angy`u X`?t?ong?.
5 Experiments
In this section, we conducted experiments to an-
swer the following three questions.
1. Are our term translation disambiguation,
consistency and bracketing models able to
improve translation quality in BLEU?
2. Does the combination of the three models
provide further improvements?
3. To what extent do the proposed models affect
the translations of test sets?
5.1 Setup
Our training data consist of 4.28M sentence pairs
extracted from LDC
1
data with document bound-
aries explicitly provided. The bilingual training
data contain 67,752 documents, 124.8M Chinese
words and 140.3M English words. We chose
NIST MT05 as the MERT (Och, 2003) tuning set,
NIST MT06 as the development test set, and NIST
MT08 as the final test set. The numbers of docu-
ments/sentences in NIST MT05, MT06 and MT08
are 100/1082, 79/1664 and 109/1357 respectively.
The word alignments were obtained by running
GIZA++ (Och and Ney, 2003) on the corpora in
both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al., 2003). We
adopted SRI Language Modeling Toolkit (Stol-
cke and others, 2002) to train a 4-gram language
model with modified Kneser-Ney smoothing on
the Xinhua portion of the English Gigaword cor-
pus. For the topic model, we used the open source
1
The corpora include LDC2003E07, LDC2003E14,
LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85,
LDC2006E92, LDC2007E87, LDC2007E101,
LDC2008E40, LDC2008E56, LDC2009E16 and
LDC2009E95.
LDA tool GibbsLDA++
2
with the default setting
for training and inference. We performed 100 it-
erations of the L-BFGS algorithm implemented in
the MaxEnt toolkit
3
with both Gaussian prior and
event cutoff set to 1 to train the term bracketing
prediction model (Section 4.3).
We performed part-of-speech tagging for mono-
lingual term extraction (C-value/NC-vaule method
in Section 3) of the source and target languages
with the Stanford NLP toolkit
4
. The bilingual term
bank was extracted based on the following param-
eter settings of term extraction methods. Empiri-
cally, we set the maximum length of a term to 6
words
5
. For both the C-value/NC-value and LLR-
based extraction methods, we set the context win-
dow size to 5 words, which is a widely-used set-
ting in previous work. And we set C-value/NC-
value score threshold to 0 and LLR score threshold
to 10 according to the training corpora.
We used the case-insensitive 4-gram BLEU
6
as
our evaluation metric. In order to alleviate the im-
pact of the instability of MERT (Och, 2003), we
ran it three times for all our experiments and pre-
sented the average BLEU scores on the three runs
following the suggestion by Clark et al. (2011).
We used an in-house hierarchical phrase-based
decoder to verify our proposed models. Although
the decoder translates a document in a sentence-
by-sentence fashion, it incorporates document-
informed information for sentence translation via
the proposed term translation models trained on
documents.
2
http://sourceforge.net/projects/gibbslda/
3
http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html
4
http://nlp.stanford.edu/software/tagger.shtml
5
We determine the maximum length of a term by testing
{5, 6, 7, 8} in our preliminary experiments. We find that
length 6 produces a slightly better performance than other
values.
6
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
551
Zh?y?u W?iyu?nhu? Ch?ngyu?n C?i  K? C?nji? W?iyu?nhu? Sh?ny? ?
Only members of the commission shall take part  in the commission deliberations .
?
He these proposals
T? Ji?ng Zh?xi? Ji?ny? Ji?o Y?u Y? G? B?zh?ngj? W?iyu?nhu? Sh?ny?
submit for approval to a committee of ministers .
(a)
(b)
Figure 1: An example of unbracketable source term in the training data. In (a), ?W?eiyu?anhu`? Sh?eny`?? is
bracketable while in (b) it is unbracketable. The solid lines connect bilingual phrases. The source side is
Chinese Pinyin.
5.2 Bilingual Term Bank
Before reporting the results of the proposed mod-
els, we provide some statistics of the bilingual
term bank extracted from the training data.
According to our statistics, about 1.29M bilin-
gual terms are extracted from the training data.
65.07% of the sentence pairs contain bilingual
terms in the training data. And on average, a
source term has about 1.70 different translations.
These statistics indicate that terms are frequently
used in real-world data and that a source term can
be translated into different target terms.
We also present some examples of bilingual
terms extracted from the training data in Table 1.
Accordingly, we show the total number of doc-
uments in which the corresponding source term
occurs and the number of documents in which
the corresponding source term is translated into
different target terms. The source term ?F?angy`u
X`?t?ong? has 23 different translations in total. They
are distributed in 470 documents in the training
data. In 414 documents, ?F?angy`u X`?t?ong? has
only one single translation. However, in the other
56 documents it has different translations. This
indicates that ?F?angy`u X`?t?ong? is not consistently
translated in these 56 documents. Different from
this, the source term ?Zh`anlu`e D?aod`an F?angy`u
X`?t?ong? only has one translation. And it is trans-
lated consistently in all 7 documents where it oc-
curs. In fact, according to our statistics, there are
about 5.19% source terms whose translations are
not consistent even in the same document.
These examples and statistics suggest 1) that
source terms have domain-specific translations
and 2) that source terms are not necessarily trans-
lated in a consistent manner even in the same doc-
ument. These are exactly the reasons why we pro-
pose the term translation disambiguation and con-
sistency model based on domain information rep-
resented by topic distributions.
Actually, 36.13% of the source terms are not
necessarily translated into target strings as a whole
unit. We show an example of such terms in Fig-
ure 1. In Figure 1-(a), ?W?eiyu?anhu`? Sh?eny`?? is a
term, and is translated into ?commission deliber-
ations? as a whole unit. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is bracketable in this sentence. How-
ever, in Figure 1-(b), ?W?eiyu?anhu`?? and ?Sh?eny`??
are translated separately. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is an unbracketable term in this sentence.
This is the reason why we propose a bracketing
model to predict whether a source term is brack-
etable or not.
5.3 Effect of the Proposed Models
In this section, we validate the effectiveness of the
proposed term translation disambiguation model,
consistency model and bracketing model respec-
tively. In addition to the traditional hiero (Chi-
ang, 2007) system, we also compare against the
?CountFeat? method in Ren et al. (2009) who use
a binary feature to indicate whether a bilingual
phrase contains a term pair. Although Ren et al.
(2009)?s experiments are conducted in a phrase-
based system, the idea can be easily applied to a
hierarchical phrase-based system.
We carried out experiments to investigate the ef-
fect of the term translation disambiguation model
(Dis-Model) and report the results in Table 2. In
order to find the topic number setting with which
our model has the best performance, we ran exper-
iments using the MT06 as the development test set.
From Table 2, we observe that the Dis-Model ob-
tains steady improvements over the baseline and
?CountFeat? method with the topic number K
552
Models MT06 MT08 Avg
Baseline 32.43 24.14 28.29
CountFeat 32.77 24.29 28.53
Dis-Model
K = 50 32.94* 24.53 28.74
K = 100 33.10* 24.57 28.84
K = 150 33.16* 24.67* 28.92
K = 200 33.08* 24.55 28.81
Cons-Model
K = 50 33.09* 24.59 28.84
K = 100 33.13* 24.74* 28.94
K = 150 33.32*+ 24.84*+ 29.08
K = 200 33.02* 24.73* 28.88
Brack-Model 33.09* 24.66* 28.88
Combined-Model 33.59*+ 24.99*+ 29.29
Table 2: BLEU-4 scores (%) of the term translation disambiguation model (Dis-Model), the term transla-
tion consistency model (Cons-Model), the term bracketing model (Brack-Model), and the combination of
the three models, on the development test set MT06 and the final test set MT08. K ? {50, 100, 150, 200}
which is the number of topics for the Dis-Model and the Cons-Model. ?Combined-Model? is the combi-
nation of the three single modes with topic number 150 for the Dis-Model and the Cons-Model. ?Base-
line? is the traditional hierarchical phrase-based system. ?CountFeat? is the method that adds a counting
feature to reward translation hypotheses containing bilingual term pairs. The ?*? and ?+? denote that the
results are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeat
method respectively (p<0.01).
ranging from 50 to 150. However, when we set K
to 200, the performance drops. The highest BLEU
scores 33.16 and 24.67 are obtained at the topic
setting K = 150. In fact, our Dis-Model gains
higher performance in BLEU than both the tradi-
tional hiero baseline and the ?CountFeat? method
with all topic settings. The ?CountFeat? method
rewards translation hypotheses containing bilin-
gual term pairs. However it does not explore any
domain information. Our Dis-Model incorporates
domain information to conduct translation disam-
biguation and achieves higher performance. When
the topic number is set to 150, we gain the high-
est BLEU score, which is higher than that of the
baseline by 0.73 and 0.53 BLEU points on MT06
and MT08, respectively. The final gain over the
baseline is on average 0.63 BLEU points.
We conducted the second group of experiments
to study whether the term translation consistency
model (Cons-Model) is able to improve the per-
formance in BLEU, as well as to investigate the
impact of different topic numbers on the Cons-
Model. Results are shown in Table 2, from which
we observe the similar phenomena to what we
have found in the Dis-Model. Our Cons-Model
gains higher BLEU scores than the baseline sys-
tem and the ?CountFeat? method with all topic
settings. Setting topic number to 150 achieves the
highest BLEU score, which is higher than base-
line by 0.89 BLEU points and 0.70 BLEU points
on MT06 and MT08 respectively, and on average
0.79 BLEU points.
We also conducted experiments to verify the ef-
fectiveness of the term bracketing model (Brack-
Model), which conducts bracketing prediction for
source terms. Results in Table 2 show that
our Brack-Model gains higher BLEU scores than
those of the baseline system and the ?CountFeat?
method. The final gain of Brack-Model over the
baseline is 0.66 BLEU points and 0.52 points on
MT06 and MT08 respectively, and on average
0.59 BLEU points.
5.4 Combination of the Three Models
As shown in the previous subsection, the term
translation disambiguation model, consistency
model and bracketing model substantially outper-
form the baseline. Now, we investigate whether
using these three models simultaneously can lead
to further improvements. The last row in Table 2
shows that the combination of the three models
(Combined-Model) achieves higher BLEU score
than all single models, when we set the topic num-
ber to 150 for the term translation disambigua-
tion model and consistency model. The final gain
553
Models MT06 MT08
Best-Dis-Model 30.89 30.14
Best-Cons-Model 38.04 36.70
Brack-Model 60.46 55.78
Combined-Model 54.39 50.85
Table 3: Percentage (%) of 1-best translations
which are generated by the Combined-Model and
the three single models with best settings on the
development test set MT06 and the final test set
MT08. The topic number is 150 for Best-Dis-
Model and Best-Cons-Model.
of the Combined-Model over the baseline is 1.16
BLEU points and 0.85 points on MT06 and MT08
respectively, and on average 1.00 BLEU points.
5.5 Analysis
In this section, we investigate to what extent the
proposed models affect the translations of test sets.
In Table 3, we show the percentage of 1-best trans-
lations affected by the Combined-Model and the
three single models with best settings on test sets
MT06 and MT08. For single models, if the corre-
sponding feature (disambiguation, consistency or
bracketing) is activated in the 1-best derivation,
the corresponding model has impact on the 1-best
translation. For the Combined-Model, if any of
the corresponding features is activated in the 1-
best derivation, the Combined-Model affects the
1-best translation.
From Table 3, we can see that 1-best transla-
tions of source sentences affected by any of the
proposed models account for a high proportion
(30%?60%) on both MT06 and MT08. This in-
dicates that all proposed models play an important
role in the translation of both test sets. Among
the three proposed models, the Brack-Model is the
one that affects the largest number of 1-best trans-
lations in both test sets. And the percentage is
60.46% and 55.78% on MT06 and MT08 respec-
tively. The Brack-Model only considers source
terms during decoding, while the Dis-Model and
Cons-Model need to match both source and target
terms. The Brack-Model is more likely to be acti-
vated. Hence the percentage of 1-best translations
affected by this model is higher than those of the
other two models. Since we only investigate the
1-best translations generated by the Combined-
Model and single models, the translations gener-
ated by some single models (e.g., Brack-Model)
may not be generated by the Combined-Model.
Therefore it is hard to say that the numbers of 1-
best translations affected by the Combined-Model
must be greater than those of single models.
6 Conclusion and Future Work
We have studied the three issues of term trans-
lation and proposed three different term trans-
lation models for document-informed SMT. The
term translation disambiguation model enables
the decoder to favor the most suitable domain-
specific translations with domain information for
source terms. The term translation consistency
model encourages the decoder to translate source
terms with a high domain translation consistency
strength into target terms rather than other new
strings. Finally, the term bracketing model re-
wards hypotheses that translate bracketable terms
into continuous target strings as a whole unit.
We integrate the three models into a hierarchical
phrase-based SMT system
7
and evaluate their ef-
fectiveness on the NIST Chinese-English transla-
tion task with large-scale training data. Experi-
ment results show that all three models achieve
significant improvements over the baseline. Ad-
ditionally, combining the three models achieves a
further improvement. For future work, we would
like to evaluate our models on term translation
across a range of different domains.
Acknowledgments
This work was supported by National Key Tech-
nology R&D Program (No. 2012BAH39B03) and
CAS Action Plan for the Development of Western
China (No. KGZD-EW-501). Deyi Xiong?s work
was supported by Natural Science Foundation of
Jiangsu Province (Grant No. BK20140355). Qun
Liu?s work was partially supported by Science
Foundation Ireland (Grant No. 07/CE/I1142) as
part of the CNGL at Dublin City University. Sin-
cere thanks to the anonymous reviewers for their
thorough reviewing and valuable suggestions. The
corresponding author of this paper, according to
the meaning given to this role by University of
Chinese Academy of Sciences and Soochow Uni-
versity, is Deyi Xiong.
7
Our models are not limited to hierarchical phrase-based
SMT. They can be easily applied to other SMT formalisms,
such as phrase- and syntax-based SMT.
554
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Houda Bouamor, Aur?elien Max, and Anne Vilnat.
2012. Validation of sub-sentential paraphrases ac-
quired from parallel monolingual corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 716?725. Association for Computa-
tional Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, pages 176?181.
B?eatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. Journal of The balancing act: Combin-
ing symbolic and statistical approaches to language,
1:49?66.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers-Volume 2, pages 115?119.
Association for Computational Linguistics.
Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nak-
agawa. 2009. Automatic extraction of bilin-
gual terms from a chinese-japanese parallel corpus.
In Proceedings of the 3rd International Universal
Communication Symposium, pages 41?45. ACM.
Katerina T Frantzi, Sophia Ananiadou, and Junichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Re-
search and Advanced Technology for Digital Li-
braries, pages 585?604. Springer.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909?919.
Liane Guillou. 2013. Analysing lexical consistency
in translation. In Proceedings of the Workshop on
Discourse in Machine Translation, pages 10?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014a. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, Gothenburg, Sweden.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014b.
Dynamic topic adaptation for smt using distribu-
tional profiles. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, pages 445?
456, Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of 16th Nordic Conference of Computational Lin-
guistics Nodalida, pages 97?104.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology trans-
lation consistency with statistical method. Proceed-
ings of MT summit XI, pages 269?274.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54.
Els Lefever, Lieve Macken, and Veronique Hoste.
2009. Language-independent bilingual terminology
extraction from a multilingual parallel corpus. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 496?504.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of 16th Nordic Conference
of Computational Linguistics Nodalida, pages 349?
354.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167.
Scott SL Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic extraction of chi-
nese multiword expressions with a statistical tool.
In Workshop on Multi-word-expressions in a Mul-
tilingual Context held in conjunction with the 11th
EACL, Trento, Italy, pages 17?24.
555
Pinis and Skadins. 2012. Mt adaptation for under-
resourced domains?what works and what not. In
Human Language Technologies?The Baltic Perspec-
tive: Proceedings of the Fifth International Confer-
ence Baltic HLT 2012, volume 247, page 176. IOS
Press.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 47?54.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 459?468.
J?org Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8?15.
Ferhan Ture, Douglas W Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426. Association for Computational Lin-
guistics.
Muriel Vasconcellos, Brian Avey, Claudia Gdaniec,
Laurie Gerber, Marjorie Le?on, and Teruko Mita-
mura. 2001. Terminology and machine translation.
Handbook of Terminology Management, 2:697?723.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term ex-
traction through unithood and termhood unification.
In Proceedings of the third international joint con-
ference on natural language processing.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 750?758.
Deyi Xiong and Min Zhang. 2013. A topic-based
coherence model for statistical machine translation.
In Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence (AAAI-13), Bellevue,
Washington, USA, July.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L?u,
and Qun Liu. 2013a. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, pages 2183?2189. AAAI
Press.
Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim
Tan. 2013b. Lexical chain based cohesion mod-
els for document-level statistical machine transla-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1563??1573.
Bing Zhao and Eric P Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 969?976.
556
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 12?20,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Dependency Parsing and Projection Based on Word-Pair Classification
Wenbin Jiang and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{jiangwenbin, liuqun}@ict.ac.cn
Abstract
In this paper we describe an intuitionistic
method for dependency parsing, where a
classifier is used to determine whether a
pair of words forms a dependency edge.
And we also propose an effective strategy
for dependency projection, where the de-
pendency relationships of the word pairs
in the source language are projected to the
word pairs of the target language, leading
to a set of classification instances rather
than a complete tree. Experiments show
that, the classifier trained on the projected
classification instances significantly out-
performs previous projected dependency
parsers. More importantly, when this clas-
sifier is integrated into a maximum span-
ning tree (MST) dependency parser, ob-
vious improvement is obtained over the
MST baseline.
1 Introduction
Supervised dependency parsing achieves the state-
of-the-art in recent years (McDonald et al, 2005a;
McDonald and Pereira, 2006; Nivre et al, 2006).
Since it is costly and difficult to build human-
annotated treebanks, a lot of works have also been
devoted to the utilization of unannotated text. For
example, the unsupervised dependency parsing
(Klein and Manning, 2004) which is totally based
on unannotated data, and the semisupervised de-
pendency parsing (Koo et al, 2008) which is
based on both annotated and unannotated data.
Considering the higher complexity and lower per-
formance in unsupervised parsing, and the need of
reliable priori knowledge in semisupervised pars-
ing, it is a promising strategy to project the de-
pendency structures from a resource-rich language
to a resource-scarce one across a bilingual corpus
(Hwa et al, 2002; Hwa et al, 2005; Ganchev et al,
2009; Smith and Eisner, 2009; Jiang et al, 2009).
For dependency projection, the relationship be-
tween words in the parsed sentences can be sim-
ply projected across the word alignment to words
in the unparsed sentences, according to the DCA
assumption (Hwa et al, 2005). Such a projec-
tion procedure suffers much from the word align-
ment errors and syntactic isomerism between lan-
guages, which usually lead to relationship projec-
tion conflict and incomplete projected dependency
structures. To tackle this problem, Hwa et al
(2005) use some filtering rules to reduce noise,
and some hand-designed rules to handle language
heterogeneity. Smith and Eisner (2009) perform
dependency projection and annotation adaptation
with quasi-synchronous grammar features. Jiang
and Liu (2009) resort to a dynamic programming
procedure to search for a completed projected tree.
However, these strategies are all confined to the
same category that dependency projection must
produce completed projected trees. Because of the
free translation, the syntactic isomerism between
languages and word alignment errors, it would
be strained to completely project the dependency
structure from one language to another.
We propose an effective method for depen-
dency projection, which does not have to pro-
duce complete projected trees. Given a word-
aligned bilingual corpus with source language sen-
tences parsed, the dependency relationships of the
word pairs in the source language are projected to
the word pairs of the target language. A depen-
dency relationship is a boolean value that repre-
sents whether this word pair forms a dependency
edge. Thus a set of classification instances are ob-
tained. Meanwhile, we propose an intuitionistic
model for dependency parsing, which uses a clas-
sifier to determine whether a pair of words form
a dependency edge. The classifier can then be
trained on the projected classification instance set,
so as to build a projected dependency parser with-
out the need of complete projected trees.
12
ij j
i
Figure 1: Illegal (a) and incomplete (b) dependency tree produced by the simple-collection method.
Experimental results show that, the classifier
trained on the projected classification instances
significantly outperforms the projected depen-
dency parsers in previous works. The classifier
trained on the Chinese projected classification in-
stances achieves a precision of 58.59% on the CTB
standard test set. More importantly, when this
classifier is integrated into a 2nd-ordered max-
imum spanning tree (MST) dependency parser
(McDonald and Pereira, 2006) in a weighted aver-
age manner, significant improvement is obtained
over the MST baselines. For the 2nd-order MST
parser trained on Penn Chinese Treebank (CTB)
5.0, the classifier give an precision increment of
0.5 points. Especially for the parser trained on the
smaller CTB 1.0, more than 1 points precision in-
crement is obtained.
In the rest of this paper, we first describe
the word-pair classification model for dependency
parsing (section 2) and the generation method
of projected classification instances (section 3).
Then we describe an application of the projected
parser: boosting a state-of-the-art 2nd-ordered
MST parser (section 4). After the comparisons
with previous works on dependency parsing and
projection, we finally five the experimental results.
2 Word-Pair Classification Model
2.1 Model Definition
Following (McDonald et al, 2005a), x is used to
denote the sentence to be parsed, and xi to denote
the i-th word in the sentence. y denotes the de-
pendency tree for sentence x, and (i, j) ? y rep-
resents a dependency edge from word xi to word
xj , where xi is the parent of xj .
The task of the word-pair classification model
is to determine whether any candidate word pair,
xi and xj s.t. 1 ? i, j ? |x| and i 6= j, forms a
dependency edge. The classification result C(i, j)
can be a boolean value:
C(i, j) = p p ? {0, 1} (1)
as produced by a support vector machine (SVM)
classifier (Vapnik, 1998). p = 1 indicates that the
classifier supports the candidate edge (i, j), and
p = 0 the contrary. C(i, j) can also be a real-
valued probability:
C(i, j) = p 0 ? p ? 1 (2)
as produced by an maximum entropy (ME) classi-
fier (Berger et al, 1996). p is a probability which
indicates the degree the classifier support the can-
didate edge (i, j). Ideally, given the classifica-
tion results for all candidate word pairs, the depen-
dency parse tree can be composed of the candidate
edges with higher score (1 for the boolean-valued
classifier, and large p for the real-valued classi-
fier). However, more robust strategies should be
investigated since the ambiguity of the language
syntax and the classification errors usually lead to
illegal or incomplete parsing result, as shown in
Figure 1.
Follow the edge based factorization method
(Eisner, 1996), we factorize the score of a de-
pendency tree s(x,y) into its dependency edges,
and design a dynamic programming algorithm
to search for the candidate parse with maximum
score. This strategy alleviate the classification er-
rors to some degree and ensure a valid, complete
dependency parsing tree. If a boolean-valued clas-
sifier is used, the search algorithm can be formal-
ized as:
y? = argmax
y
s(x,y)
= argmax
y
?
(i,j)?y
C(i, j) (3)
And if a probability-valued classifier is used in-
stead, we replace the accumulation with cumula-
13
Type Features
Unigram wordi ? posi wordi posi
wordj ? posj wordj posj
Bigram wordi ? posi ? wordj ? posj posi ? wordj ? posj wordi ? wordj ? posj
wordi ? posi ? posj wordi ? posi ? wordj wordi ? wordj
posi ? posj wordi ? posj posi ? wordj
Surrounding posi ? posi+1 ? posj?1 ? posj posi?1 ? posi ? posj?1 ? posj posi ? posi+1 ? posj ? posj+1
posi?1 ? posi ? posj ? posj+1 posi?1 ? posi ? posj?1 posi?1 ? posi ? posj+1
posi ? posi+1 ? posj?1 posi ? posi+1 ? posj+1 posi?1 ? posj?1 ? posj
posi?1 ? posj ? posj+1 posi+1 ? posj?1 ? posj posi+1 ? posj ? posj+1
posi ? posj?1 ? posj posi ? posj ? posj+1 posi?1 ? posi ? posj
posi ? posi+1 ? posj
Table 1: Feature templates for the word-pair classification model.
tive product:
y? = argmax
y
s(x,y)
= argmax
y
?
(i,j)?y
C(i, j) (4)
Where y is searched from the set of well-formed
dependency trees.
In our work we choose a real-valued ME clas-
sifier. Here we give the calculation of dependency
probability C(i, j). We use w to denote the param-
eter vector of the ME model, and f(i, j, r) to de-
note the feature vector for the assumption that the
word pair i and j has a dependency relationship r.
The symbol r indicates the supposed classification
result, where r = + means we suppose it as a de-
pendency edge and r = ? means the contrary. A
feature fk(i, j, r) ? f(i, j, r) equals 1 if it is ac-
tivated by the assumption and equals 0 otherwise.
The dependency probability can then be defined
as:
C(i, j) = exp(w ? f(i, j,+))?
r exp(w ? f(i, j, r))
=
exp(
?
k wk ? fk(i, j,+))
?
r exp(
?
k wk ? fk(i, j, r))
(5)
2.2 Features for Classification
The feature templates for the classifier are simi-
lar to those of 1st-ordered MST model (McDon-
ald et al, 2005a). 1 Each feature is composed
of some words and POS tags surrounded word i
and/or word j, as well as an optional distance rep-
resentations between this two words. Table shows
the feature templates we use.
Previous graph-based dependency models usu-
ally use the index distance of word i and word j
1We exclude the in between features of McDonald et al
(2005a) since preliminary experiments show that these fea-
tures bring no improvement to the word-pair classification
model.
to enrich the features with word distance infor-
mation. However, in order to utilize some syntax
information between the pair of words, we adopt
the syntactic distance representation of (Collins,
1996), named Collins distance for convenience. A
Collins distance comprises the answers of 6 ques-
tions:
? Does word i precede or follow word j?
? Are word i and word j adjacent?
? Is there a verb between word i and word j?
? Are there 0, 1, 2 or more than 2 commas be-
tween word i and word j?
? Is there a comma immediately following the
first of word i and word j?
? Is there a comma immediately preceding the
second of word i and word j?
Besides the original features generated according
to the templates in Table 1, the enhanced features
with Collins distance as postfixes are also used in
training and decoding of the word-pair classifier.
2.3 Parsing Algorithm
We adopt logarithmic dependency probabilities
in decoding, therefore the cumulative product of
probabilities in formula 6 can be replaced by ac-
cumulation of logarithmic probabilities:
y? = argmax
y
s(x,y)
= argmax
y
?
(i,j)?y
C(i, j)
= argmax
y
?
(i,j)?y
log(C(i, j))
(6)
Thus, the decoding algorithm for 1st-ordered MST
model, such as the Chu-Liu-Edmonds algorithm
14
Algorithm 1 Dependency Parsing Algorithm.
1: Input: sentence x to be parsed
2: for ?i, j? ? ?1, |x|? in topological order do
3: buf ? ?
4: for k ? i..j ? 1 do ? all partitions
5: for l ? V[i, k] and r ? V[k + 1, j] do
6: insert DERIV(l, r) into buf
7: insert DERIV(r, l) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |x|]
10: function DERIV(p, c)
11: d? p ? c ? {(p ? root, c ? root)} ? new derivation
12: d ? evl? EVAL(d) ? evaluation function
13: return d
used in McDonald et al (2005b), is also appli-
cable here. In this work, however, we still adopt
the more general, bottom-up dynamic program-
ming algorithm Algorithm 1 in order to facilitate
the possible expansions. Here, V[i, j] contains the
candidate parsing segments of the span [i, j], and
the function EVAL(d) accumulates the scores of
all the edges in dependency segment d. In prac-
tice, the cube-pruning strategy (Huang and Chi-
ang, 2005) is used to speed up the enumeration of
derivations (loops started by line 4 and 5).
3 Projected Classification Instance
After the introduction of the word-pair classifica-
tion model, we now describe the extraction of pro-
jected dependency instances. In order to allevi-
ate the effect of word alignment errors, we base
the projection on the alignment matrix, a compact
representation of multiple GIZA++ (Och and Ney,
2000) results, rather than a single word alignment
in previous dependency projection works. Figure
2 shows an example.
Suppose a bilingual sentence pair, composed of
a source sentence e and its target translation f . ye
is the parse tree of the source sentence. A is the
alignment matrix between them, and each element
Ai,j denotes the degree of the alignment between
word ei and word fj . We define a boolean-valued
function ?(y, i, j, r) to investigate the dependency
relationship of word i and word j in parse tree y:
?(y, i, j, r) =
?
?
?
?
?
?
?
?
?
?
?
1
(i, j) ? y and r = +
or
(i, j) /? y and r = ?
0 otherwise
(7)
Then the score that word i and word j in the target
sentence y forms a projected dependency edge,
Figure 2: The word alignment matrix between a
Chinese sentence and its English translation. Note
that probabilities need not to be normalized across
rows or columns.
s+(i, j), can be defined as:
s+(i, j) =
?
i?,j?
Ai,i? ? Aj,j? ? ?(ye, i?, j?,+) (8)
The score that they do not form a projected depen-
dency edge can be defined similarly:
s?(i, j) =
?
i?,j?
Ai,i? ? Aj,j? ? ?(ye, i?, j?,?) (9)
Note that for simplicity, the condition factors ye
and A are omitted from these two formulas. We
finally define the probability of the supposed pro-
jected dependency edge as:
Cp(i, j) =
exp(s+(i, j))
exp(s+(i, j)) + exp(s?(i, j))
(10)
The probability Cp(i, j) is a real value between
0 and 1. Obviously, Cp(i, j) = 0.5 indicates the
most ambiguous case, where we can not distin-
guish between positive and negative at all. On the
other hand, there are as many as 2|f |(|f |?1) candi-
date projected dependency instances for the target
sentence f . Therefore, we need choose a threshold
b for Cp(i, j) to filter out the ambiguous instances:
the instances with Cp(i, j) > b are selected as the
positive, and the instances with Cp(i, j) < 1 ? b
are selected as the negative.
4 Boosting an MST Parser
The classifier can be used to boost a existing parser
trained on human-annotated trees. We first estab-
lish a unified framework for the enhanced parser.
For a sentence to be parsed, x, the enhanced parser
selects the best parse y? according to both the base-
line model B and the projected classifier C.
y? = argmax
y
[sB(x,y) + ?sC(x,y)] (11)
15
Here, sB and sC denote the evaluation functions
of the baseline model and the projected classi-
fier, respectively. The parameter ? is the relative
weight of the projected classifier against the base-
line model.
There are several strategies to integrate the two
evaluation functions. For example, they can be in-
tegrated deeply at each decoding step (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated shallowly in a reranking man-
ner (Collins, 2000; Charniak and Johnson, 2005).
As described previously, the score of a depen-
dency tree given by a word-pair classifier can be
factored into each candidate dependency edge in
this tree. Therefore, the projected classifier can
be integrated with a baseline model deeply at each
dependency edge, if the evaluation score given by
the baseline model can also be factored into de-
pendency edges.
We choose the 2nd-ordered MST model (Mc-
Donald and Pereira, 2006) as the baseline. Es-
pecially, the effect of the Collins distance in the
baseline model is also investigated. The relative
weight ? is adjusted to maximize the performance
on the development set, using an algorithm similar
to minimum error-rate training (Och, 2003).
5 Related Works
5.1 Dependency Parsing
Both the graph-based (McDonald et al, 2005a;
McDonald and Pereira, 2006; Carreras et al,
2006) and the transition-based (Yamada and Mat-
sumoto, 2003; Nivre et al, 2006) parsing algo-
rithms are related to our word-pair classification
model.
Similar to the graph-based method, our model
is factored on dependency edges, and its decod-
ing procedure also aims to find a maximum span-
ning tree in a fully connected directed graph. From
this point, our model can be classified into the
graph-based category. On the training method,
however, our model obviously differs from other
graph-based models, that we only need a set of
word-pair dependency instances rather than a reg-
ular dependency treebank. Therefore, our model is
more suitable for the partially bracketed or noisy
training corpus.
The most apparent similarity between our
model and the transition-based category is that
they all need a classifier to perform classification
conditioned on a certain configuration. However,
they differ from each other in the classification re-
sults. The classifier in our model predicates a de-
pendency probability for each pair of words, while
the classifier in a transition-based model gives a
possible next transition operation such as shift or
reduce. Another difference lies in the factoriza-
tion strategy. For our method, the evaluation score
of a candidate parse is factorized into each depen-
dency edge, while for the transition-based models,
the score is factorized into each transition opera-
tion.
Thanks to the reminding of the third reviewer
of our paper, we find that the pairwise classifica-
tion schema has also been used in Japanese de-
pendency parsing (Uchimoto et al, 1999; Kudo
and Matsumoto, 2000). However, our work shows
more advantage in feature engineering, model
training and decoding algorithm.
5.2 Dependency Projection
Many works try to learn parsing knowledge from
bilingual corpora. Lu? et al (2002) aims to
obtain Chinese bracketing knowledge via ITG
(Wu, 1997) alignment. Hwa et al (2005) and
Ganchev et al (2009) induce dependency gram-
mar via projection from aligned bilingual cor-
pora, and use some thresholds to filter out noise
and some hand-written rules to handle heterogene-
ity. Smith and Eisner (2009) perform depen-
dency projection and annotation adaptation with
Quasi-Synchronous Grammar features. Jiang and
Liu (2009) refer to alignment matrix and a dy-
namic programming search algorithm to obtain
better projected dependency trees.
All previous works for dependency projection
(Hwa et al, 2005; Ganchev et al, 2009; Smith and
Eisner, 2009; Jiang and Liu, 2009) need complete
projected trees to train the projected parsers. Be-
cause of the free translation, the word alignment
errors, and the heterogeneity between two lan-
guages, it is reluctant and less effective to project
the dependency tree completely to the target lan-
guage sentence. On the contrary, our dependency
projection strategy prefer to extract a set of depen-
dency instances, which coincides our model?s de-
mand for training corpus. An obvious advantage
of this strategy is that, we can select an appropriate
filtering threshold to obtain dependency instances
of good quality.
In addition, our word-pair classification model
can be integrated deeply into a state-of-the-art
MST dependency model. Since both of them are
16
Corpus Train Dev Test
WSJ (section) 2-21 22 23
CTB 5.0 (chapter) others 301-325 271-300
Table 2: The corpus partition for WSJ and CTB
5.0.
factorized into dependency edges, the integration
can be conducted at each dependency edge, by
weightedly averaging their evaluation scores for
this dependency edge. This strategy makes better
use of the projected parser while with faster de-
coding, compared with the cascaded approach of
Jiang and Liu (2009).
6 Experiments
In this section, we first validate the word-pair
classification model by experimenting on human-
annotated treebanks. Then we investigate the ef-
fectiveness of the dependency projection by eval-
uating the projected classifiers trained on the pro-
jected classification instances. Finally, we re-
port the performance of the integrated dependency
parser which integrates the projected classifier and
the 2nd-ordered MST dependency parser. We
evaluate the parsing accuracy by the precision of
lexical heads, which is the percentage of the words
that have found their correct parents.
6.1 Word-Pair Classification Model
We experiment on two popular treebanks, the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al, 1993), and the Penn Chi-
nese Treebank (CTB) 5.0 (Xue et al, 2005). The
constituent trees in the two treebanks are trans-
formed to dependency trees according to the head-
finding rules of Yamada and Matsumoto (2003).
For English, we use the automatically-assigned
POS tags produced by an implementation of the
POS tagger of Collins (2002). While for Chinese,
we just use the gold-standard POS tags following
the tradition. Each treebank is splitted into three
partitions, for training, development and testing,
respectively, as shown in Table 2.
For a dependency tree with n words, only n ?
1 positive dependency instances can be extracted.
They account for only a small proportion of all the
dependency instances. As we know, it is important
to balance the proportions of the positive and the
negative instances for a batched-trained classifier.
We define a new parameter r to denote the ratio of
the negative instances relative to the positive ones.
 84
 84.5
 85
 85.5
 86
 86.5
 87
 1  1.5  2  2.5  3
De
pe
nd
en
cy
 P
re
cis
ion
 (%
)
Ratio r (#negative/#positive)
WSJ
CTB 5.0
Figure 3: Performance curves of the word-pair
classification model on the development sets of
WSJ and CTB 5.0, with respect to a series of ratio
r.
Corpus System P %
WSJ Yamada and Matsumoto (2003) 90.3
Nivre and Scholz (2004) 87.3
1st-ordered MST 90.7
2nd-ordered MST 91.5
our model 86.8
CTB 5.0 1st-ordered MST 86.53
2nd-ordered MST 87.15
our model 82.06
Table 3: Performance of the word-pair classifica-
tion model on WSJ and CTB 5.0, compared with
the current state-of-the-art models.
For example, r = 2 means we reserve negative
instances two times as many as the positive ones.
The MaxEnt toolkit by Zhang 2 is adopted to
train the ME classifier on extracted instances. We
set the gaussian prior as 1.0 and the iteration limit
as 100, leaving other parameters as default values.
We first investigate the impact of the ratio r on
the performance of the classifier. Curves in Fig-
ure 3 show the performance of the English and
Chinese parsers, each of which is trained on an in-
stance set corresponding to a certain r. We find
that for both English and Chinese, maximum per-
formance is achieved at about r = 2.5. 3 The
English and Chinese classifiers trained on the in-
stance sets with r = 2.5 are used in the final eval-
uation phase. Table 3 shows the performances on
the test sets of WSJ and CTB 5.0.
We also compare them with previous works on
the same test sets. On both English and Chinese,
the word-pair classification model falls behind of
the state-of-the-art. We think that it is probably
2http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
3We did not investigate more fine-grained ratios, since the
performance curves show no dramatic fluctuation along with
the alteration of r.
17
 54
 54.5
 55
 55.5
 56
 0.65  0.7  0.75  0.8  0.85  0.9  0.95
De
pe
nd
en
cy
 P
re
cis
ion
 (%
)
Threshold b
Figure 4: The performance curve of the word-
pair classification model on the development set
of CTB 5.0, with respect to a series of threshold b.
due to the local optimization of the training pro-
cedure. Given complete trees as training data, it
is easy for previous models to utilize structural,
global and linguistical information in order to ob-
tain more powerful parameters. The main advan-
tage of our model is that it doesn?t need complete
trees to tune its parameters. Therefore, if trained
on instances extracted from human-annotated tree-
banks, the word-pair classification model would
not demonstrate its advantage over existed state-
of-the-art dependency parsing methods.
6.2 Dependency Projection
In this work we focus on the dependency projec-
tion from English to Chinese. We use the FBIS
Chinese-English bitext as the bilingual corpus for
dependency projection. It contains 239K sen-
tence pairs with about 6.9M/8.9M words in Chi-
nese/English. Both English and Chinese sentences
are tagged by the implementations of the POS tag-
ger of Collins (2002), which trained on WSJ and
CTB 5.0 respectively. The English sentences are
then parsed by an implementation of 2nd-ordered
MST model of McDonald and Pereira (2006),
which is trained on dependency trees extracted
from WSJ. The alignment matrixes for sentence
pairs are generated according to (Liu et al, 2009).
Similar to the ratio r, the threshold b need also
be assigned an appropriate value to achieve a bet-
ter performance. Larger thresholds result in better
but less classification instances, the lower cover-
age of the instances would hurt the performance of
the classifier. On the other hand, smaller thresh-
olds lead to worse but more instances, and too
much noisy instances will bring down the classi-
fier?s discriminating power.
We extract a series of classification instance sets
Corpus System P %
CTB 2.0 Hwa et al (2005) 53.9
our model 56.9
CTB 5.0 Jiang and Liu (2009) 53.28
our model 58.59
Table 4: The performance of the projected classi-
fier on the test sets of CTB 2.0 and CTB 5.0, com-
pared with the performance of previous works on
the corresponding test sets.
Corpus Baseline P% Integrated P%
CTB 1.0 82.23 83.70
CTB 5.0 87.15 87.65
Table 5: Performance improvement brought by
the projected classifier to the baseline 2nd-ordered
MST parsers trained on CTB 1.0 and CTB 5.0, re-
spectively.
with different thresholds. Then, on each instance
set we train a classifier and test it on the develop-
ment set of CTB 5.0. Figure 4 presents the ex-
perimental results. The curve shows that the max-
imum performance is achieved at the threshold of
about 0.85. The classifier corresponding to this
threshold is evaluated on the test set of CTB 5.0,
and the test set of CTB 2.0 determined by Hwa et
al. (2005). Table 4 shows the performance of the
projected classifier, as well as the performance of
previous works on the corresponding test sets. The
projected classifier significantly outperforms pre-
vious works on both test sets, which demonstrates
that the word-pair classification model, although
falling behind of the state-of-the-art on human-
annotated treebanks, performs well in projected
dependency parsing. We give the credit to its good
collaboration with the word-pair classification in-
stance extraction for dependency projection.
6.3 Integrated Dependency Parser
We integrate the word-pair classification model
into the state-of-the-art 2nd-ordered MST model.
First, we implement a chart-based dynamic pro-
gramming parser for the 2nd-ordered MST model,
and develop a training procedure based on the
perceptron algorithm with averaged parameters
(Collins, 2002). On the WSJ corpus, this parser
achieves the same performance as that of McDon-
ald and Pereira (2006). Then, at each derivation
step of this 2nd-ordered MST parser, we weight-
edly add the evaluation score given by the pro-
jected classifier to the original MST evaluation
score. Such a weighted summation of two eval-
18
uation scores provides better evaluation for can-
didate parses. The weight parameter ? is tuned
by a minimum error-rate training algorithm (Och,
2003).
Given a 2nd-ordered MST parser trained on
CTB 5.0 as the baseline, the projected classi-
fier brings an accuracy improvement of about 0.5
points. For the baseline trained on the smaller
CTB 1.0, whose training set is chapters 1-270 of
CTB 5.0, the accuracy improvement is much sig-
nificant, about 1.5 points over the baseline. It
indicates that, the smaller the human-annotated
treebank we have, the more significant improve-
ment we can achieve by integrating the project-
ing classifier. This provides a promising strategy
for boosting the parsing performance of resource-
scarce languages. Table 5 summarizes the experi-
mental results.
7 Conclusion and Future Works
In this paper, we first describe an intuitionis-
tic method for dependency parsing, which re-
sorts to a classifier to determine whether a word
pair forms a dependency edge, and then propose
an effective strategy for dependency projection,
which produces a set of projected classification in-
stances rather than complete projected trees. Al-
though this parsing method falls behind of pre-
vious models, it can collaborate well with the
word-pair classification instance extraction strat-
egy for dependency projection, and achieves the
state-of-the-art in projected dependency parsing.
In addition, when integrated into a 2nd-ordered
MST parser, the projected parser brings signifi-
cant improvement to the baseline, especially for
the baseline trained on smaller treebanks. This
provides a new strategy for resource-scarce lan-
guages to train high-precision dependency parsers.
However, considering its lower performance on
human-annotated treebanks, the dependency pars-
ing method itself still need a lot of investigations,
especially on the training method of the classifier.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contract 60736014,
and 863 State Key Project No. 2006AA010108.
We are grateful to the anonymous reviewers for
their thorough reviewing and valuable sugges-
tions. We show special thanks to Dr. Rebecca
Hwa for generous help of sharing the experimen-
tal data. We also thank Dr. Yang Liu for sharing
the codes of alignment matrix generation, and Dr.
Liang Huang for helpful discussions.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics.
Xavier Carreras, Mihai Surdeanu, and Lluis Marquez.
2006. Projective dependency parsing with percep-
tron. In Proceedings of the CoNLL.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
ACL.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175?182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340?345.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering, volume 11, pages
311?325.
19
Wenbin Jiang and Qun Liu. 2009. Automatic adapta-
tion of annotation standards for dependency parsing
using projected treebank as source corpus. In Pro-
ceedings of IWPT.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the ACL.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proceedings of the EMNLP.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002. Learning chinese bracketing knowledge
based on a bilingual language model. In Proceed-
ings of the COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
J. Nivre and M. Scholz. 2004. Deterministic depen-
dency parsing of english text. In Proceedings of the
COLING.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen
Eryigit, and Svetoslav Marinov. 2006. Labeled
pseudoprojective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221?225.
Franz J. Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL, pages 160?167.
David Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analysis
based on maximum entropy models. In Proceedings
of the EACL.
Vladimir N. Vapnik. 1998. Statistical learning theory.
In A Wiley-Interscience Publication.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of the ACL.
20
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761?769,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative Learning with Natural Annotations:
Word Segmentation as a Case Study
Wenbin Jiang 1 Meng Sun 1 Yajuan Lu? 1 Yating Yang 2 Qun Liu 3, 1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{jiangwenbin, sunmeng, lvyajuan}@ict.ac.cn
2Multilingual Information Technology Research Center
The Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences
yangyt@ms.xjb.ac.cn
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Structural information in web text pro-
vides natural annotations for NLP prob-
lems such as word segmentation and pars-
ing. In this paper we propose a discrim-
inative learning algorithm to take advan-
tage of the linguistic knowledge in large
amounts of natural annotations on the In-
ternet. It utilizes the Internet as an external
corpus with massive (although slight and
sparse) natural annotations, and enables a
classifier to evolve on the large-scaled and
real-time updated web text. With Chinese
word segmentation as a case study, exper-
iments show that the segmenter enhanced
with the Chinese wikipedia achieves sig-
nificant improvement on a series of testing
sets from different domains, even with a
single classifier and local features.
1 Introduction
Problems related to information retrieval, machine
translation and social computing need fast and ac-
curate text processing, for example, word segmen-
tation and parsing. Taking Chinese word seg-
mentation for example, the state-of-the-art mod-
els (Xue and Shen, 2003; Ng and Low, 2004;
Gao et al, 2005; Nakagawa and Uchimoto, 2007;
Zhao and Kit, 2008; Jiang et al, 2009; Zhang and
Clark, 2010; Sun, 2011b; Li, 2011) are usually
trained on human-annotated corpora such as the
Penn Chinese Treebank (CTB) (Xue et al, 2005),
and perform quite well on corresponding test sets.
Since the text used for corpus annotating are usu-
ally drawn from specific fields (e.g. newswire or
finance), and the annotated corpora are limited in
 think that NLP                  has already ...
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(a) Natural annotation by hyperlink
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(b) Knowledge for word segmentation
(c) Knowledge for dependency parsing
Figure 1: Natural annotations for word segmenta-
tion and dependency parsing.
size (e.g. tens of thousands), the performance of
word segmentation tends to degrade sharply when
applied to new domains.
Internet provides large amounts of raw text, and
statistics collected from it have been used to im-
prove parsing performance (Nakov and Hearst,
2005; Pitler et al, 2010; Bansal and Klein, 2011;
Zhou et al, 2011). The Internet alo gives mas-
sive (although slight and sparse) natural annota-
tions in the forms of structural information includ-
ing hyperlinks, fonts, colors and layouts (Sun,
2011a). These annotations usually imply valuable
knowledge for problems such as word segmen-
tation and parsing, based on the hypothesis that
the subsequences marked by structural informa-
tion are meaningful fragments in sentences. Fig-
ure 1 shows an example. The hyperlink indicates
761
a Chinese phrase (meaning NLP), and it probably
corresponds to a connected sub-graph for depen-
dency parsing. Creators of web text give valuable
annotations during editing, the whole Internet can
be treated as a wide-coveraged and real-time up-
dated corpus.
Different from the dense and accurate annota-
tions in human-annotated corpora, natural annota-
tions in web text are sparse and slight, it makes
direct training of NLP models impracticable. In
this work we take for example a most important
problem, word segmentation, and propose a novel
discriminative learning algorithm to leverage the
knowledge in massive natural annotations of web
text. Character classification models for word seg-
mentation usually factorize the whole prediction
into atomic predictions on characters (Xue and
Shen, 2003; Ng and Low, 2004). Natural anno-
tations in web text can be used to get rid of im-
plausible predication candidates for related char-
acters, knowledge in the natural annotations is
therefore introduced in the manner of searching
space pruning. Since constraint decoding in the
pruned searching space integrates the knowledge
of the baseline model and natural annotations, it
gives predictions not worse than the normal decod-
ing does. Annotation differences between the out-
puts of constraint decoding and normal decoding
are used to train the enhanced classifier. This strat-
egy makes the usage of natural annotations simple
and universal, which facilitates the utilization of
massive web text and the extension to other NLP
problems.
Although there are lots of choices, we choose
the Chinese wikipedia as the knowledge source
due to its high quality. Structural information, in-
cluding hyperlinks, fonts and colors are used to de-
termine the boundaries of meaningful fragments.
Experimental results show that, the knowledge im-
plied in the natural annotations can significantly
improve the performance of a baseline segmenter
trained on CTB 5.0, an F-measure increment of
0.93 points on CTB test set, and an average incre-
ment of 1.53 points on 7 other domains. It is an ef-
fective and inexpensive strategy to build word seg-
menters adaptive to different domains. We hope to
extend this strategy to other NLP problems such
as named entity recognition and parsing.
In the rest of the paper, we first briefly intro-
duce the problems of Chinese word segmentation
and the character classification model in section
Type Templates Instances
n-gram C?2 C?2=@
C?1 C?1=?
C0 C0=g
C1 C1=,
C2 C2=?
C?2C?1 C?2C?1=@?
C?1C0 C?1C0=?g
C0C1 C0C1=g,
C1C2 C1C2=,?
C?1C1 C?1C1=?,
function Pu(C0) Pu(C0)=false
T (C?2:2) T (C?2:2)= 44444
Table 1: Feature templates and instances for
character classification-based word segmentation
model. Suppose we are considering the i-th char-
acter ?g? in ?...@? g ,???n??...?.
2, then describe the representation of the knowl-
edge in natural annotations of web text in section
3, and finally detail the strategy of discriminative
learning on natural annotations in section 4. Af-
ter giving the experimental results and analysis in
section 5, we briefly introduce the previous related
work and then give the conclusion and the expec-
tation of future research.
2 Character Classification Model
Character classification models for word segmen-
tation factorize the whole prediction into atomic
predictions on single characters (Xue and Shen,
2003; Ng and Low, 2004). Although natural anno-
tations in web text do not directly support the dis-
criminative training of segmentation models, they
do get rid of the implausible candidates for predic-
tions of related characters.
Given a sentence as a sequence of n charac-
ters, word segmentation splits the sequence into
m(? n) subsequences, each of which indicates a
meaningful word. Word segmentation can be for-
malized as a character classification problem (Xue
and Shen, 2003), where each character in the sen-
tence is given a boundary tag representing its posi-
tion in a word. We adopt the boundary tags of Ng
and Low (2004), b, m, e and s, where b, m and
e mean the beginning, the middle and the end of a
word, and s indicates a single-character word. the
decoding procedure searches for the labeled char-
acter sequence y that maximizes the score func-
762
Algorithm 1 Perceptron training algorithm.
1: Input: Training corpus C
2: ~?? 0
3: for t? 1 .. T do ? T iterations
4: for (x, y?) ? C do
5: y ? argmaxy ?(x, y) ? ~?
6: if y 6= y? then
7: ~?? ~?+?(x, y?)? ?(x, y)
8: Output: Parameters ~?
tion:
f(x) = argmax
y
S(y|~?,?, x)
= argmax
y
?(x, y) ? ~?
= argmax
y
?
(i,t)?y
?(i, t, x, y) ? ~?
(1)
The score of the whole sequence y is accumulated
across all its character-label pairs, (i, t) ? y (s.t.
1 ? i ? n and t ? {b,m, e, s}). The feature
function ? maps a labeled sequence or a character-
label pair into a feature vector, ~? is the parame-
ter vector and ?(x, y) ? ~? is the inner product of
?(x, y) and ~?.
Analogous to other sequence labeling prob-
lems, word segmentation can be solved through a
viterbi-style decoding procedure. We omit the de-
coding algorithm in this paper due to its simplicity
and popularity.
The feature templates for the classifier is shown
in Table 1. C0 denotes the current character, while
C?k/Ck denote the kth character to the left/right
of C0. The function Pu(?) returns true for a punc-
tuation character and false for others, the function
T (?) classifies a character into four types, 1, 2, 3
and 4, representing number, date, English letter
and others, respectively.
The classifier can be trained with online learn-
ing algorithms such as perceptron, or offline learn-
ing models such as support vector machines.
We choose the perceptron algorithm (Collins,
2002) to train the classifier for the character
classification-based word segmentation model. It
learns a discriminative model mapping from the
inputs x ? X to the outputs y? ? Y , where X is the
set of sentences in the training corpus and Y is the
set of corresponding labeled results. Algorithm 1
shows the perceptron algorithm for tuning the pa-
rameter ~?. The ?averaged parameters? technology
(Collins, 2002) is used for better performance.
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(a) Original searching space
n
n
n
n
n
n
n
n
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(b) Shrinked searching space
n
n
n
n
n
n
n
n
b
m
e
s
e
s
b
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
e
s
b
s
b
m
e
s
Figure 2: Shrink of searching space for the charac-
ter classification-based word segmentation model.
3 Knowledge in Natural Annotations
Web text gives massive natural annotations in the
form of structural informations, including hyper-
links, fonts, colors and layouts (Sun, 2011a). Al-
though slight and sparse, these annotations imply
valuable knowledge for problems such as word
segmentation and parsing.
As shown in Figure 1, the subsequence P =
i..j of sentence S is composed of bolded charac-
ters determined by a hyperlink. Such natural anno-
tations do not clearly give each character a bound-
ary tag, or define the head-modifier relationship
between two words. However, they do help to
shrink the set of plausible predication candidates
for each character or word. For word segmenta-
tion, it implies that characters i ? 1 and j are the
rightmost characters of words, while characters i
and j + 1 are the leftmost characters of words.
For i ? 1 or j, the plausible predication set ? be-
comes {e, s}; For i and j + 1, it becomes {b, s};
For other characters c except the two at sentence
boundaries, ?(c) is still {b,m, e, s}. For depen-
dency parsing, the subsequence P tends to form
a connected dependency graph if it contains more
than one word. Here we use ? to denote the set of
plausible head of a word (modifier). There must
be a single word w ? P as the root of subse-
quence P , whose plausible heads fall out of P ,
that is, ?(w) = {x|x ? S ? P}. For the words
in P except the root, the plausible heads for each
763
Algorithm 2 Perceptron learning with natural an-
notations.
1: ~?? TRAIN(C)
2: for x ? F do
3: y ? DECODE(x, ~?)
4: y? ? CONSTRAINTDECODE(x, ~?,?)
5: if y 6= y? then
6: C? ? C? ? {y?}
7: ~?? TRAIN(C ? C?)
word w are the words in P except w itself, that is,
?(w) = {x|x ? P ? {w}}.
Creators of web text give valuable structural
annotations during editing, these annotations re-
duce the predication uncertainty for atomic char-
acters or words, although not exactly defining
which predication is. Figure 2 shows an exam-
ple for word segmentation, depicting the shrink
of searching space for the character classification-
based model. Since the decrement of uncertainty
indicates the increment of knowledge, the whole
Internet can be treated as a wide-coveraged and
real-time updated corpus. We choose the Chinese
wikipedia as the external knowledge source, and
structural information including hyperlinks, fonts
and colors are used in the current work due to their
explicitness of representation.
4 Learning with Natural Annotations
Different from the dense and accurate annotations
in human-annotated corpora, natural annotations
are sparse and slight, which makes direct training
of NLP models impracticable. Annotations im-
plied by structural information do not give an ex-
act predication to a character, however, they help
to get rid of the implausible predication candidates
for related characters, as described in the previous
section.
Previous work on constituency parsing or ma-
chine translation usually resort to some kinds of
heuristic tricks, such as punctuation restrictions,
to eliminate some implausible candidates during
decoding. Here the natural annotations also bring
knowledge in the manner of searching space prun-
ing. Conditioned on the completeness of the de-
coding algorithm, a model trained on an exist-
ing corpus probably gives better or at least not
worse predications, by constraint decoding in the
pruned searching space. The constraint decoding
procedure integrates the knowledge of the baseline
Algorithm 3 Online version of perceptron learn-
ing with natural annotations.
1: ~?? TRAIN(C)
2: for x with natural annotations do
3: y ? DECODE(x, ~?)
4: y? ? CONSTRAINTDECODE(x, ~?,?)
5: if y 6= y? then
6: ~?? ~? +?(x, y?)??(x, y)
7: output ~? at regular time
model and natural annotations, the predication dif-
ferences between the outputs of constraint decod-
ing and normal decoding can be used to train the
enhanced classifier.
Restrictions of the searching space according to
natural annotations can be easily incorporated into
the decoder. If the completeness of the searching
algorithm can be guaranteed, the constraint decod-
ing in the pruned searching space will give predi-
cations not worse than those given by the normal
decoding. If a predication of constraint decoding
differs from that of normal decoding, it indicates
that the annotation precision is higher than the lat-
ter. Furthermore, the degree of difference between
the two predications represents the amount of new
knowledge introduced by the natural annotations
over the baseline.
The baseline model ~? is trained on an exist-
ing human-annotated corpus. A set of sentences
F with natural annotations are extracted from the
Chinese wikipedia, and we reserve the ones for
which constraint decoding and normal decoding
give different predications. The predictions of re-
served sentences by constraint decoding are used
as additional training data for the enhanced classi-
fier. The overall training pipeline is analogous to
self-training (McClosky et al, 2006), Algorithm
2 shows the pseudo-codes. Considering the online
characteristic of the perceptron algorithm, if we
are able to leverage much more (than the Chinese
wikipedia) data with natural annotations, an online
version of learning procedure shown in Algorithm
3 would be a better choice. The technology of ?av-
eraged parameters? (Collins, 2002) is easily to be
adapted here for better performance.
When constraint decoding and normal decod-
ing give different predications, we only know that
the former is probably better than the latter. Al-
though there is no explicit evidence for us to mea-
sure how much difference in accuracy between the
764
Partition Sections # of word
CTB
Training 1? 270 0.47M
400 ? 931
1001 ? 1151
Developing 301 ? 325 6.66K
Testing 271 ? 300 7.82K
Table 2: Data partitioning for CTB 5.0.
two predications, we can approximate how much
new knowledge that a naturally annotated sentence
brings. For a sentence x, given the predications of
constraint decoding and normal decoding, y? and
y, the difference of their scores ? = S(y) ? S(y?)
indicates the degree to which the current model
mistakes. This indicator helps us to select more
valuable training examples.
The strategy of learning with natural annota-
tions can be adapted to other situations. For ex-
ample, if we have a list of words or phrases (espe-
cially in a specific domain such as medicine and
chemical), we can generate annotated sentences
automatically by string matching in a large amount
of raw text. It probably provides a simple and
effective domain adaptation strategy for already
trained models.
5 Experiments
We use the Penn Chinese Treebank 5.0 (CTB)
(Xue et al, 2005) as the existing annotated cor-
pus for Chinese word segmentation. For conve-
nient of comparison with other work in word seg-
mentation, the whole corpus is split into three par-
titions as follows: chapters 271-300 for testing,
chapters 301-325 for developing, and others for
training. We choose the Chinese wikipedia 1 (ver-
sion 20120812) as the external knowledge source,
because it has high quality in contents and it is
much better than usual web text. Structural infor-
mations, including hyperlinks, fonts and colors are
used to derive the annotation information.
To further evaluate the improvement brought
by the fuzzy knowledge in Chinese wikipedia, a
series of testing sets from different domains are
adopted. The four testing sets from SIGHAN
Bakeoff 2010 (Zhao and Liu, 2010) are used, they
are drawn from the domains of literature, finance,
computer science and medicine. Although the ref-
erence sets are annotated according to a different
1http://download.wikimedia.org/backup-index.html.
 95.6
 95.8
 96
 96.2
 96.4
 96.6
 96.8
 97
 97.2
 97.4
 1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
1%
)
Training iterations
Figure 3: Learning curve of the averaged percep-
tron classifier on the CTB developing set.
word segmentation standard (Yu et al, 2001), the
quantity of accuracy improvement is still illustra-
tive since there are no vast diversities between the
two segmentation standards. We also annotated
another three testing sets 2, their texts are drawn
from the domains of chemistry, physics and ma-
chinery, and each contains 500 sentences.
5.1 Baseline Classifier for Word
Segmentation
We train the baseline perceptron classifier for
word segmentation on the training set of CTB
5.0, using the developing set to determine the
best training iterations. The performance mea-
surement for word segmentation is balanced F-
measure, F = 2PR/(P +R), a function of preci-
sion P and recall R, where P is the percentage of
words in segmentation results that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Figure 3 shows the learning curve of the aver-
aged perceptron on the developing set. The sec-
ond column of Table 3 lists the performance of
the baseline classifier on eight testing sets, where
newswire denotes the testing set of the CTB it-
self. The classifier performs much worse on the
domains of chemistry, physics and machinery, it
indicates the importance of domain adaptation for
word segmentation (Gao et al, 2004; Ma and
Way, 2009; Gao et al, 2010). The accuracy on the
testing sets from SIGHAN Bakeoff 2010 is even
lower due to the difference in both domains and
word segmentation standards.
2They are available at http://nlp.ict.ac.cn/ jiangwenbin/.
765
Dataset Baseline (F%) Enhanced (F%)
Newswire 97.35 98.28 +0.93
Out-of-Domain
Chemistry 93.61 95.68 +2.07
Physics 95.10 97.24 +2.14
Machinery 96.08 97.66 +1.58
Literature 92.42 93.53 +1.11
Finance 92.50 93.16 +0.66
Computer 89.46 91.19 +1.73
Medicine 91.88 93.34 +1.46
Average 93.01 94.54 +1.53
Table 3: Performance of the baseline classifier and
the classifier enhanced with natural annotations in
Chinese wikipedia.
5.2 Classifier Enhanced with Natural
Annotations
The Chinese wikipedia contains about 0.5 million
items. From their description text, about 3.9 mil-
lions of sentences with natural annotations are ex-
tracted. With the CTB training set as the exist-
ing corpus C, about 0.8 million sentences are re-
served according to Algorithm 2, the segmenta-
tions given by constraint decoding are used as ad-
ditional training data for the enhanced classifier.
According to the previous description, the dif-
ference of the scores of constraint decoding and
normal decoding, ? = S(y) ? S(y?), indicates
the importance of a constraint segmentation to the
improvement of the baseline classifier. The con-
straint segmentations of the reserved sentences are
sorted in descending order according to the dif-
ference of the scores of constraint decoding and
normal decoding, as described previously. From
the beginning of the sorted list, different amounts
of segmented sentences are used as the additional
training data for the enhanced character classifier.
Figure 4 shows the performance curve of the en-
hanced classifiers on the developing set of CTB.
We found that the highest accuracy was achieved
when 160, 000 sentences were used, while more
additional training data did not give continuous
improvement. A recent related work about self-
training for segmentation (Liu and Zhang, 2012)
also reported a very similar trend, that only a mod-
erate amount of raw data gave the most obvious
improvements.
The performance of the enhanced classifier is
listed in the third column of Table 3. On the
CTB testing set, training data from the Chinese
 97.1
 97.2
 97.3
 97.4
 97.5
 97.6
 97.7
 97.8
Ac
cu
ra
cy
 (F
1%
)
Count of selected sentences
10000 20000 40000 80000 160000 320000 640000
using selected sentences
using all sentences
Figure 4: Performance curve of the classifier en-
hanced with selected sentences of different scales.
Model Accuracy (F%)
(Jiang et al, 2008) 97.85
(Kruengkrai et al, 2009) 97.87
(Zhang and Clark, 2010) 97.79
(Wang et al, 2011) 98.11
(Sun, 2011b) 98.17
Our Work 98.28
Table 4: Comparison with state-of-the-art work in
Chinese word segmentation.
wikipedia brings an F-measure increment of 0.93
points. On out-of-domain testing sets, the im-
provements are much larger, an average increment
of 1.53 points is achieved on seven domains. It
is probably because the distribution of the knowl-
edge in the CTB training data is concentrated in
the domain of newswire, while the contents of
the Chinese wikipedia cover a broad range of do-
mains, it provides knowledge complementary to
that of CTB.
Table 4 shows the comparison with other
work in Chinese word segmentation. Our model
achieves an accuracy higher than that of the
state-of-the-art models trained on CTB only, al-
though using a single classifier with only local
features. From the viewpoint of resource uti-
lization, the comparison between our system and
previous work without using additional training
data is unfair. However, we believe this work
shows another interesting way to improve Chi-
nese word segmentation, it focuses on the utiliza-
tion of fuzzy and sparse knowledge on the Internet
rather than making full use of a specific human-
annotated corpus. On the other hand, since only
a single classifier and local features are used in
our method, better performance could be achieved
766
resorting to complicated features, system com-
bination and other semi-supervised technologies.
What is more, since the text on Internet is wide-
coveraged and real-time updated, our strategy also
helps a word segmenter be more domain adaptive
and up to date.
6 Related Work
Li and Sun (2009) extracted character classifi-
cation instances from raw text for Chinese word
segmentation, resorting to the indication of punc-
tuation marks between characters. Sun and Xu
(Sun and Xu, 2011) utilized the features derived
from large-scaled unlabeled text to improve Chi-
nese word segmentation. Although the two work
also made use of large-scaled raw text, our method
is essentially different from theirs in the aspects
of both the source of knowledge and the learning
strategy.
Lots of efforts have been devoted to semi-
supervised methods in sequence labeling and word
segmentation (Xu et al, 2008; Suzuki and Isozaki,
2008; Haffari and Sarkar, 2008; Tomanek and
Hahn, 2009; Wang et al, 2011). A semi-
supervised method tries to find an optimal hyper-
plane of both annotated data and raw data, thus to
result in a model with better coverage and higher
accuracy. Researchers have also investigated un-
supervised methods in word segmentation (Zhao
and Kit, 2008; Johnson and Goldwater, 2009;
Mochihashi et al, 2009; Hewlett and Cohen,
2011). An unsupervised method mines the latent
distribution regularity in the raw text, and auto-
matically induces word segmentation knowledge
from it. Our method also needs large amounts of
external data, but it aims to leverage the knowl-
edge in the fuzzy and sparse annotations. It is
fundamentally different from semi-supervised and
unsupervised methods in that we aimed to exca-
vate a totally different kind of knowledge, the nat-
ural annotations implied by the structural informa-
tion in web text.
In recent years, much work has been devoted to
the improvement of word segmentation in a vari-
ety of ways. Typical approaches include the in-
troduction of global training or complicated fea-
tures (Zhang and Clark, 2007; Zhang and Clark,
2010), the investigation of word internal structures
(Zhao, 2009; Li, 2011), the adjustment or adapta-
tion of word segmentation standards (Wu, 2003;
Gao et al, 2004; Jiang et al, 2009), the integrated
solution of segmentation and related tasks such as
part-of-speech tagging and parsing (Zhou and Su,
2003; Zhang et al, 2003; Fung et al, 2004; Gold-
berg and Tsarfaty, 2008), and the strategies of hy-
brid or stacked modeling (Nakagawa and Uchi-
moto, 2007; Kruengkrai et al, 2009; Wang et al,
2010; Sun, 2011b).
In parsing, Pereira and Schabes (1992) pro-
posed an extended inside-outside algorithm that
infers the parameters of a stochastic CFG from a
partially parsed treebank. It uses partial bracket-
ing information to improve parsing performance,
but it is specific to constituency parsing, and its
computational complexity makes it impractical for
massive natural annotations in web text. There
are also work making use of word co-occurrence
statistics collected in raw text or Internet n-grams
to improve parsing performance (Nakov and
Hearst, 2005; Pitler et al, 2010; Zhou et al, 2011;
Bansal and Klein, 2011). When enriching the re-
lated work during writing, we found a work on de-
pendency parsing (Spitkovsky et al, 2010) who
utilized parsing constraints derived from hypertext
annotations to improve the unsupervised depen-
dency grammar induction. Compared with their
method, the strategy we proposed is formal and
universal, the discriminative learning strategy and
the quantitative measurement of fuzzy knowledge
enable more effective utilization of the natural an-
notation on the Internet when adapted to parsing.
7 Conclusion and Future Work
This work presents a novel discriminative learning
algorithm to utilize the knowledge in the massive
natural annotations on the Internet. Natural anno-
tations implied by structural information are used
to decrease the searching space of the classifier,
then the constraint decoding in the pruned search-
ing space gives predictions not worse than the nor-
mal decoding does. Annotation differences be-
tween the outputs of constraint decoding and nor-
mal decoding are used to train the enhanced classi-
fier, linguistic knowledge in the human-annotated
corpus and the natural annotations of web text
are thus integrated together. Experiments on Chi-
nese word segmentation show that, the enhanced
word segmenter achieves significant improvement
on testing sets of different domains, although us-
ing a single classifier with only local features.
Since the contents of web text cover a broad
range of domains, it provides knowledge comple-
767
mentary to that of human-annotated corpora with
concentrated distribution of domains. The content
on the Internet is large-scaled and real-time up-
dated, it compensates for the drawback of expen-
sive building and updating of corpora. Our strat-
egy, therefore, enables us to build a classifier more
domain adaptive and up to date. In the future, we
will compare this method with self-training to bet-
ter illustrate the importance of boundary informa-
tion, and give error analysis on what types of er-
rors are reduced by the method to make this inves-
tigation more complete. We will also investigate
more efficient algorithms to leverage more mas-
sive web text with natural annotations, and further
extend the strategy to other NLP problems such as
named entity recognition and parsing.
Acknowledgments
The authors were supported by National
Natural Science Foundation of China (Con-
tracts 61202216), 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03). Qun Liu?s
work was partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part
of the CNGL at Dublin City University. Sincere
thanks to the three anonymous reviewers for their
thorough reviewing and valuable suggestions!
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8, Philadelphia, USA.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy chinese
parser augmented by transformation-based learning.
In Proceedings of TALIP.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings of ACL.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Computational Linguistics.
Wenjun Gao, Xipeng Qiu, and Xuanjing Huang. 2010.
Adaptive chinese word segmentation with online
passive-aggressive algorithm. In Proceedings of
CIPS-SIGHAN Workshop.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL-HLT.
Gholamreza Haffari and Anoop Sarkar. 2008.
Homotopy-based semi-supervised hidden markov
models for sequence labeling. In Proceedings of
COLING.
Daniel Hewlett and Paul Cohen. 2011. Fully unsu-
pervised word segmentation with bve and mdl. In
Proceedings of ACL.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun.ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of ACL.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and pos-tagging.
In Proceedings of COLING.
Yanjun Ma and Andy Way. 2009. Bilingually moti-
vated domain-adapted word segmentation for statis-
tical machine translation. In Proceedings of EACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT-NAACL.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL.
Preslav Nakov and Marti Hearst. 2005. Using the
web as an implicit training set: Application to struc-
tural ambiguity resolution. In Proceedings of HLT-
EMNLP.
768
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of ACL.
Emily Pitler, Shane Bergsma, Dekang Lin, and Ken-
neth Church. 2010. Using web-scale n-grams to
improve base np parsing performance. In Proceed-
ings of COLING.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
ACL.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP.
Maosong Sun. 2011a. Natural language processing
based on naturally annotated web resources. CHI-
NESE INFORMATION PROCESSING.
Weiwei Sun. 2011b. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP.
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in chinese. Computa-
tional Linguistics and Chinese Language Process-
ing.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and pos-tagging using
a single discriminative model. In Proceedings of
EMNLP.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of SIGHAN Workshop.
Hai Zhao and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In Proceedings of SIGHAN Workshop.
Hongmei Zhao and Qun Liu. 2010. The cips-sighan
clp 2010 chinese word segmentation bakeoff. In
Proceedings of CIPS-SIGHAN Workshop.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
EACL.
Guodong Zhou and Jian Su. 2003. A chinese effi-
cient analyser integrating word segmentation, part-
ofspeech tagging, partial parsing and full parsing. In
Proceedings of SIGHAN Workshop.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings of ACL.
769
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063?1072,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingually-Guided Monolingual Dependency Grammar Induction
Kai Liu??, Yajuan Lu??, Wenbin Jiang?, Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{liukai,lvyajuan,jiangwenbin,liuqun}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
?University of Chinese Academy of Sciences
Abstract
This paper describes a novel strategy for
automatic induction of a monolingual de-
pendency grammar under the guidance
of bilingually-projected dependency. By
moderately leveraging the dependency in-
formation projected from the parsed coun-
terpart language, and simultaneously min-
ing the underlying syntactic structure of
the language considered, it effectively in-
tegrates the advantages of bilingual pro-
jection and unsupervised induction, so as
to induce a monolingual grammar much
better than previous models only using
bilingual projection or unsupervised in-
duction. We induced dependency gram-
mar for five different languages under the
guidance of dependency information pro-
jected from the parsed English translation,
experiments show that the bilingually-
guided method achieves a significant
improvement of 28.5% over the unsuper-
vised baseline and 3.0% over the best pro-
jection baseline on average.
1 Introduction
In past decades supervised methods achieved the
state-of-the-art in constituency parsing (Collins,
2003; Charniak and Johnson, 2005; Petrov et al,
2006) and dependency parsing (McDonald et al,
2005a; McDonald et al, 2006; Nivre et al, 2006;
Nivre et al, 2007; Koo and Collins, 2010). For
supervised models, the human-annotated corpora
on which models are trained, however, are expen-
sive and difficult to build. As alternative strate-
gies, methods which utilize raw texts have been in-
vestigated recently, including unsupervised meth-
ods which use only raw texts (Klein and Man-
ning, 2004; Smith and Eisner, 2005; William et
al., 2009), and semi-supervised methods (Koo et
al., 2008) which use both raw texts and annotat-
ed corpus. And there are a lot of efforts have also
been devoted to bilingual projection (Chen et al,
2010), which resorts to bilingual text with one lan-
guage parsed, and projects the syntactic informa-
tion from the parsed language to the unparsed one
(Hwa et al, 2005; Ganchev et al, 2009).
In dependency grammar induction, unsuper-
vised methods achieve continuous improvements
in recent years (Klein and Manning, 2004; Smith
and Eisner, 2005; Bod, 2006; William et al, 2009;
Spitkovsky et al, 2010). Relying on a predefined
distributional assumption and iteratively maximiz-
ing an approximate indicator (entropy, likelihood,
etc.), an unsupervised model usually suffers from
two drawbacks, i.e., lower performance and high-
er computational cost. On the contrary, bilin-
gual projection (Hwa et al, 2005; Smith and Eis-
ner, 2009; Jiang and Liu, 2010) seems a promis-
ing substitute for languages with a
large amount of bilingual sentences and an exist-
ing parser of the counterpart language. By project-
ing syntactic structures directly (Hwa et al, 2005;
Smith and Eisner, 2009; Jiang and Liu, 2010)
across bilingual texts or indirectly across multi-
lingual texts (Snyder et al, 2009; McDonald et
al., 2011; Naseem et al, 2012), a better depen-
dency grammar can be easily induced, if syntactic
isomorphism is largely maintained between target
and source languages.
Unsupervised induction and bilingual projec-
tion run according to totally different principles,
the former mines the underlying structure of the
monolingual language, while the latter leverages
the syntactic knowledge of the parsed counter-
1063
Bilingual corpus Joint Optimization
Bilingually-guided
Parsing model
Unsupervised
objective
Projection
objective
Random
Treebank
Evolved
treebank
Target
sentences
Source
sentences projection
Figure 1: Training the bilingually-guided parsing model by iteration.
part language. Considering this, we propose a
novel strategy for automatically inducing a mono-
lingual dependency grammar under the guidance
of bilingually-projected dependency information,
which integrates the advantage of bilingual pro-
jection into the unsupervised framework. A
randomly-initialized monolingual treebank
evolves in a self-training iterative procedure, and
the grammar parameters are tuned to simultane-
ously maximize both the monolingual likelihood
and bilingually-projected likelihood of the evolv-
ing treebank. The monolingual likelihood is sim-
ilar to the optimization objectives of convention-
al unsupervised models, while the bilingually-
projected likelihood is the product of the projected
probabilities of dependency trees. By moderately
leveraging the dependency information projected
from the parsed counterpart language, and simul-
taneously mining the underlying syntactic struc-
ture of the language considered, we can automat-
ically induce a monolingual dependency grammar
which is much better than previous models only
using bilingual projection or unsupervised induc-
tion. In addition, since both likelihoods are fun-
damentally factorized into dependency edges (of
the hypothesis tree), the computational complexi-
ty approaches to unsupervised models, while with
much faster convergence. We evaluate the final
automatically-induced dependency parsing mod-
el on 5 languages. Experimental results show
that our method significantly outperforms previ-
ous work based on unsupervised method or indi-
rect/direct dependency projection, where we see
an average improvement of 28.5% over unsuper-
vised baseline on all languages, and the improve-
ments are 3.9%/3.0% over indirect/direct base-
lines. And our model achieves the most signif-
icant gains on Chinese, where the improvements
are 12.0%, 4.5% over indirect and direct projec-
tion baselines respectively.
In the rest of the paper, we first describe the un-
supervised dependency grammar induction frame-
work in section 2 (where the unsupervised op-
timization objective is given), and introduce the
bilingual projection method for dependency pars-
ing in section 3 (where the projected optimiza-
tion objective is given); Then in section 4 we
present the bilingually-guided induction strategy
for dependency grammar (where the two objec-
tives above are jointly optimized, as shown in Fig-
ure 1). After giving a brief introduction of previ-
ous work in section 5, we finally give the experi-
mental results in section 6 and conclude our work
in section 7.
2 Unsupervised Dependency Grammar
Induction
In this section, we introduce the unsupervised ob-
jective and the unsupervised training algorithm
which is used as the framework of our bilingually-
guided method. Unlike previous unsupervised
work (Klein and Manning, 2004; Smith and Eis-
ner, 2005; Bod, 2006), we select a self-training
approach (similar to hard EM method) to train
the unsupervised model. And the framework of
our unsupervised model builds a random treebank
on the monolingual corpus firstly for initialization
and trains a discriminative parsing model on it.
Then we use the parser to build an evolved tree-
bank with the 1-best result for the next iteration
run. In this way, the parser and treebank evolve in
an iterative way until convergence. Let?s introduce
the parsing objective firstly:
Define ei as the ith word in monolingual sen-
tence E; deij denotes the word pair dependency re-
lationship (ei ? ej). Based on the features around
deij , we can calculate the probability Pr(y|deij )
that the word pair deij can form a dependency arc
1064
as:
Pr(y|deij ) =
1
Z(deij )
exp(
?
n
?n ? fn(deij , y)) (1)
where y is the category of the relationship of deij :
y = + means it is the probability that the word
pair deij can form a dependency arc and y = ?
means the contrary. ?n denotes the weight for fea-
ture function fn(deij , y), and the features we used
are presented in Table 1 (Section 6). Z(deij) is a
normalizing constant:
Z(deij ) =
?
y
exp(
?
n
?n ? fn(deij , y)) (2)
Given a sentence E, parsing a dependency tree
is to find a dependency tree DE with maximum
probability PE :
PE = argmax
DE
?
deij?DE
Pr(+|deij ) (3)
2.1 Unsupervised Objective
We select a simple classifier objective function as
the unsupervised objective function which is in-
stinctively in accordance with the parsing objec-
tive:
?(?) =
?
de?DE
Pr(+|de)
?
de?D?E
Pr(?|de) (4)
where E is the monolingual corpus and E ? E,
DE is the treebank that contains all DE in the cor-
pus, and D?E denotes all other possible dependen-
cy arcs which do not exist in the treebank.
Maximizing the Formula (4) is equivalent to
maximizing the following formula:
?1(?) =
?
de?DE
logPr(+|de)
+
?
de?D?E
logPr(?|de)
(5)
Since the size of edges between DE and D?E is
disproportionate, we use an empirical value to re-
duce the impact of the huge number of negative
instances:
?2(?) =
?
de?DE
logPr(+|de)
+ |DE |
|D?E |
?
de?D?E
logPr(?|de)
(6)
where |x| is the size of x.
Algorithm 1 Training unsupervised model
1: build random DE
2: ?? train(DE , D?E)
3: repeat
4: for each E ? E do ? E step
5: DE ? parse(E,?)
6: ?? train(DE , D?E) ? M step
7: until convergence
Bush held talk with Sharona
bushi yu juxingshalong huitanle
? ?
?? ? ???? ???
Figure 2: Projecting a Chinese dependency tree
to English side according to DPA. Solid arrows
are projected dependency arcs; dashed arrows are
missing dependency arcs.
2.2 Unsupervised Training Algorithm
Algorithm 1 outlines the unsupervised training in
its entirety, where the treebank DE and unsuper-
vised parsing model with ? are updated iteratively.
In line 1 we build a random treebank DE on
the monolingual corpus, and then train the parsing
model with it (line 2) through a training procedure
train(?, ?) which needs DE and D?E as classifica-
tion instances. From line 3-7, we train the unsu-
pervised model in self training iterative procedure,
where line 4-5 are similar to the E-step in EM al-
gorithm where calculates objective instead of ex-
pectation of 1-best tree (line 5) which is parsed
according to the parsing objective (Formula 3) by
parsing process parse(?, ?), and update the tree
bank with the tree. Similar to M-step in EM, the
algorithm maximizes the whole treebank?s unsu-
pervised objective (Formula 6) through the train-
ing procedure (line 6).
3 Bilingual Projection of Dependency
Grammar
In this section, we introduce our projection objec-
tive and training algorithm which trains the model
with arc instances.
Because of the heterogeneity between dif-
ferent languages and word alignment errors, pro-
jection methods may contain a lot of noises. Take
Figure 2 as an example, following the Direct
Projection Algorithm (DPA) (Hwa et al, 2005)
(Section 5), the dependency relationships between
words can be directly projected from the source
1065
Algorithm 2 Training projection model
1: DP , DN ? proj(F ,DF , A,E)
2: repeat ? train(DP , DN )
3: ??? grad(DP , DN , ?(?))
4: ?? climb(?,??, ?)
5: until maximization
language to the target language. Therefore, we
can hardly obtain a treebank with complete trees
through direct projection. So we extract projected
discrete dependency arc instances instead of tree-
bank as training set for the projected grammar in-
duction model.
3.1 Projection Objective
Correspondingly, we select an objective which has
the same form with the unsupervised one:
?(?) =
?
de?DP
log Pr(+|de)
+
?
de?DN
logPr(?|de)
(7)
where DP is the positive dependency arc instance
set, which is obtained by direct projection methods
(Hwa et al, 2005; Jiang and Liu, 2010) and DN is
the negative one.
3.2 Projection Algorithm
Basically, the training procedure in line 2,7 of Al-
gorithm 1 can be divided into smaller iterative
steps, and Algorithm 2 outlines the training step
of projection model with instances. F in Algo-
rithm 2 is source sentences in bilingual corpus,
and A is the alignments. Function grad(?, ?, ?)
gives the gradient (??) and the objective is op-
timized with a generic optimization step (such as
an LBFGS iteration (Zhu et al, 1997)) in the sub-
routine climb(?, ?, ?).
4 Bilingually-Guided Dependency
Grammar Induction
This section presents our bilingually-guided gram-
mar induction model, which incorporates unsuper-
vised framework and bilingual projection model
through a joint approach.
According to following observation: unsuper-
vised induction model mines underlying syntactic
structure of the monolingual language, however, it
is hard to find good grammar induction in the ex-
ponential parsing space; bilingual projection ob-
tains relatively reliable syntactic knowledge of the
parsed counterpart, but it possibly contains a lot
of noises (e.g. Figure 2). We believe that unsu-
pervised model and projection model can comple-
ment each other and a joint model which takes bet-
ter use of both unsupervised parse trees and pro-
jected dependency arcs can give us a better parser.
Based on the idea, we propose a nov-
el strategy for training monolingual grammar in-
duction model with the guidance of unsuper-
vised and bilingually-projected dependency infor-
mation. Figure 1 outlines our bilingual-guided
grammar induction process in its entirety. In our
method, we select compatible objectives for unsu-
pervised and projection models, in order to they
can share the same grammar parameters. Then
we incorporate projection model into our iterative
unsupervised framework, and jointly optimize un-
supervised and projection objectives with evolv-
ing treebank and constant projection information
respectively. In this way, our bilingually-guided
model?s parameters are tuned to simultaneous-
ly maximizing both monolingual likelihood and
bilingually-projected likelihood by 4 steps:
1. Randomly build treebank on target sentences
for initialization, and get the projected arc in-
stances through projection from bitext.
2. Train the bilingually-guided grammar induc-
tion model by multi-objective optimization
method with unsupervised objective and pro-
jection objective on treebank and projected
arc instances respectively.
3. Use the parsing model to build new treebank
on target language for next iteration.
4. Repeat steps 1, 2 and 3 until convergence.
The unsupervised objective is optimized by the
loop??tree bank?optimized model?new tree
bank?. The treebank is evolved for runs. The
unsupervised model gets projection constraint im-
plicitly from those parse trees which contain in-
formation from projection part. The projection ob-
jective is optimized by the circulation??projected
instances?optimized model?, these projected in-
stances will not change once we get them.
The iterative procedure proposed here is not a
co-training algorithm (Sarkar, 2001; Hwa et al,
2003), because the input of the projection objec-
tive is static.
1066
4.1 Joint Objective
For multi-objective optimization method, we em-
ploy the classical weighted-sum approach which
just calculates the weighted linear sum of the ob-
jectives:
OBJ =
?
m
weightmobjm (8)
We combine the unsupervised objective (For-
mula (6)) and projection objective (Formula (7))
together through the weighted-sum approach in
Formula (8):
?(?) = ??2(?) + (1 ? ?)?(?) (9)
where ?(?) is our weight-sum objective. And ?
is a mixing coefficient which reflects the relative
confidence between the unsupervised and projec-
tion objectives. Equally, ? and (1??) can be seen
as the weights in Formula (8). In that case, we can
use a single parameter ? to control both weights
for different objective functions. When ? = 1 it
is the unsupervised objective function in Formula
(6). Contrary, if ? = 0, it is the projection objec-
tive function (Formula (7)) for projected instances.
With this approach, we can optimize the mixed
parsing model by maximizing the objective in For-
mula (9). Though the function (Formula (9)) is
an interpolation function, we use it for training
instead of parsing. In the parsing procedure, our
method calculates the probability of a dependency
arc according to the Formula (2), while the inter-
polating method calculates it by:
Pr(y|deij) =?Pr1(y|deij )
+ (1 ? ?)Pr2(y|deij )
(10)
where Pr1(y|deij ) and Pr2(y|deij ) are the proba-
bilities provided by different models.
4.2 Training Algorithm
We optimize the objective (Formula (9)) via a
gradient-based search algorithm. And the gradi-
ent with respect to ?k takes the form:
??(?k) = ?
??2(?)
??k
+ (1 ? ?)??(?)??k
(11)
Algorithm 3 outlines our joint training proce-
dure, which tunes the grammar parameter ? simul-
taneously maximize both unsupervised objective
Algorithm 3 Training joint model
1: DP , DN ? proj(F,DF , A,E)
2: build random DE
3: ?? train(DP , DN )
4: repeat
5: for each E ? E do ? E step
6: DE ? parse(E,?)
7: ??(?)? grad(DE, D?E , DP , DN , ?(?))
8: ??climb(?(?),??(?), ?) ? M step
9: until convergence
and projection objective. And it incorporates un-
supervised framework and projection model algo-
rithm together. It is grounded on the work which
uses features in the unsupervised model (Berg-
Kirkpatrick et al, 2010).
In line 1, 2 we get projected dependency in-
stances from source side according to projec-
tion methods and build a random treebank (step
1). Then we train an initial model with projection
instances in line 3. From line 4-9, the objective is
optimized with a generic optimization step in the
subroutine climb(?, ?, ?, ?, ?). For each sentence we
parse its dependency tree, and update the tree into
the treebank (step 3). Then we calculate the gra-
dient and optimize the joint objective according to
the evolved treebank and projected instances (step
2). Lines 5-6 are equivalent to the E-step of the
EM algorithm, and lines 7-8 are equivalent to the
M-step.
5 Related work
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) which
is based on POS tags. And DMV learns the gram-
mar via inside-outside re-estimation (Baker, 1979)
without any smoothing, while Spitkovsky et al
(2010) utilizes smoothing and learning strategy
during grammar learning and William et al (2009)
improves DMV with richer context.
The dependency projection method DPA (H-
wa et al, 2005) based on Direct Correspondence
Assumption (Hwa et al, 2002) can be described
as: if there is a pair of source words with a de-
pendency relationship, the corresponding aligned
words in target sentence can be considered as hav-
ing the same dependency relationship equivalent-
ly (e.g. Figure 2). The Word Pair Classification
(WPC) method (Jiang and Liu, 2010) modifies the
DPA method and makes it more robust. Smith
and Eisner (2009) propose an adaptation method
founded on quasi-synchronous grammar features
1067
Type Feature Template
Unigram wordi posi wordi ? posi
wordj posj wordj ? posj
Bigram wordi ? posj wordj ? posi posi ? posj
wordi ? wordj wordi ? posi ? wordj wordi ? wordj ? posj
wordi ? posi ? posj posi ? wordj ? posj
wordi ? posi ? wordj ? posj
Surrounding posi?1 ? posi ? posj posi ? posi+1 ? posj posi ? posj?1 ? posjposi ? posj ? posj+1 posi?1 ? posi ? posj?1 posi ? posi+1 ? posj+1posi?1 ? posj?1 ? posj posi+1 ? posj ? posj+1 posi?1 ? posi ? posj+1posi ? posi+1 ? posj?1 posi?1 ? posj ? posj+1 posi+1 ? posj?1 ? posjposi?1 ? posi ? posj?1 ? posj posi ? posi+1 ? posj ? posj+1posi ? posi+1 ? posj?1 ? posj posi?1 ? posi ? posj ? posj+1
Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj
is the child word, similar to ?pos?. ?+1? denotes the preceding token of the sentence, similar to ?-1?.
for dependency projection and annotation, which
requires a small set of dependency annotated cor-
pus of target language.
Similarly, using indirect information from mul-
tilingual (Cohen et al, 2011; Ta?ckstro?m et al,
2012) is an effective way to improve unsupervised
parsing. (Zeman and Resnik, 2008; McDonald et
al., 2011; S?gaard, 2011) employ non-lexicalized
parser trained on other languages to process a
target language. McDonald et al (2011) adapts
their multi-source parser according to DCA, while
Naseem et al (2012) selects a selective sharing
model to make better use of grammar information
in multi-sources.
Due to similar reasons, many works are devoted
to POS projection (Yarowsky et al, 2001; Shen et
al., 2007; Naseem et al, 2009), and they also suf-
fer from similar problems. Some seek for unsu-
pervised methods, e.g. Naseem et al (2009), and
some further improve the projection by a graph-
based projection (Das and Petrov, 2011).
Our model differs from the approaches above
in its emphasis on utilizing information from both
sides of bilingual corpus in an unsupervised train-
ing framework, while most of the work above only
utilize the information from a single side.
6 Experiments
In this section, we evaluate the performance of the
MST dependency parser (McDonald et al, 2005b)
which is trained by our bilingually-guided model
on 5 languages. And the features used in our ex-
periments are summarized in Table 1.
6.1 Experiment Setup
Datasets and Evaluation Our experiments are
run on five different languages: Chinese(ch),
Danish(da), Dutch(nl), Portuguese(pt) and
Swedish(sv) (da, nl, pt and sv are free data sets
distributed for the 2006 CoNLL Shared Tasks
(Buchholz and Marsi, 2006)). For all languages,
we only use English-target parallel data: we take
the FBIS English-Chinese bitext as bilingual cor-
pus for English-Chinese dependency projection
which contains 239K sentence pairs with about
8.9M/6.9M words in English/Chinese, and for
other languages we use the readily available data
in the Europarl corpus. Then we run tests on the
Penn Chinese Treebank (CTB) and CoNLL-X test
sets.
English sentences are tagged by the implemen-
tations of the POS tagger of Collins (2002), which
is trained on WSJ. The source sentences are then
parsed by an implementation of 2nd-ordered MST
model of McDonald and Pereira (2006), which is
trained on dependency trees extracted from Penn
Treebank.
As the evaluation metric, we use parsing accu-
racy which is the percentage of the words which
have found their correct parents. We evaluate on
sentences with all length for our method.
Training Regime In experiments, we use the
projection method proposed by Jiang and Liu
(2010) to provide the projection instances. And
we train the projection part ? = 0 first for initial-
ization, on which the whole model will be trained.
Availing of the initialization method, the model
can converge very fast (about 3 iterations is suffi-
cient) and the results are more stable than the ones
trained on random initialization.
Baselines We compare our method against
three kinds of different approaches: unsupervised
method (Klein and Manning, 2004); single-
source direct projection methods (Hwa et al,
2005; Jiang and Liu, 2010); multi-source in-
direct projection methods with multi-sources (M-
1068
60.0
61.5
          
 
 
ch
50.3
51.2
          
 
 
da
59.5
60.5
          
ac
cu
ra
cy
%
 
nl
70.5
74.5
          
 
 
pt
61.5
65.0
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
 
alpha
sv
Figure 3: The performance of our model with re-
spect to a series of ratio ?
cDonald et al, 2011; Naseem et al, 2012).
6.2 Results
We test our method on CTB and CoNLL-X free
test data sets respectively, and the performance is
summarized in Table 2. Figure 3 presents the per-
formance with different ? on different languages.
Compare against Unsupervised Baseline Ex-
perimental results show that our unsupervised
framework?s performance approaches to the DMV
method. And the bilingually-guided model can
promote the unsupervised method consisten-
cy over all languages. On the best results? aver-
age of four comparable languages (da, nl, pt, sv),
the promotion gained by our model is 28.5% over
the baseline method (DMV) (Klein and Manning,
2004).
Compare against Projection Baselines For
all languages, the model consistent-
ly outperforms on direct projection baseline.
On the average of each language?s best result, our
model outperforms all kinds of baselines, yielding
3.0% gain over the single-source direct-projection
method (Jiang and Liu, 2010) and 3.9% gain over
the multi-source indirect-projection method (Mc-
Donald et al, 2011). On the average of all results
with different parameters, our method also gain-
s more than 2.0% improvements on all baselines.
Particularly, our model achieves the most signif-
icant gains on Chinese, where the improvements
are 4.5%/12.0% on direct/indirect projection base-
Accuracy%
Model ch da nl pt sv avg
DMV 42.5? 33.4 38.5 20.1 44.0 ?.?
DPA 53.9 ?.? ?.? ?.? ?.? ?.?
WPC 56.8 50.1 58.4 70.5 60.8 59.3
Transfer 49.3 49.5 53.9 75.8 63.6 58.4
Selective 51.2 ?.? 55.9 73.5 61.5 ?.?
unsuper 22.6 41.6 15.2 45.7 42.4 33.5
avg 61.0 50.7 59.9 72.0 63.1 61.3
max 61.3 51.1 60.1 74.2 64.6 62.3
Table 2: The directed dependency accuracy with
different parameter of our model and the base-
lines. The first section of the table (row 3-7)
shows the results of the baselines: a unsupervised
method baseline (Klein and Manning, 2004)(D-
MV); a single-source projection method baseline
(Hwa et al, 2005) (DPA) and its improve-
ment (Jiang and Liu, 2010)(WPC); two multi-
source baselines (McDonald et al, 2011)(Trans-
fer) and (Naseem et al, 2012)(Selective). The
second section of the table (row 8) presents the
result of our unsupervised framework (unsuper).
The third section gives the mean value (avg) and
maximum value (max) of our model with different
? in Figure 3.
*: The result is based on sentences with 10
words or less after the removal of punctuation, it
is an incomparable result.
lines.
The results in Figure 3 prove that our unsuper-
vised framework ? = 1 can promote the grammar
induction if it has a good start (well initialization),
and it will be better once we incorporate the infor-
mation from the projection side (? = 0.9). And
the maximum points are not in ? = 1, which im-
plies that projection information is still available
for the unsupervised framework even if we employ
the projection model as the initialization. So we
suggest that a greater parameter is a better choice
for our model. And there are some random factors
in our model which make performance curves with
more fluctuation. And there is just a little improve-
ment shown in da, in which the same situation is
observed by (McDonald et al, 2011).
6.3 Effects of the Size of Training Corpus
To investigate how the size of the training corpus
influences the result, we train the model on ex-
tracted bilingual corpus with varying sizes: 10K,
50K, 100K, 150K and 200K sentences pairs.
As shown in Figure 4, our approach continu-
1069
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
10K 50K 100K 150K 200K
ac
cu
ra
cy
%
size of training set
our model
baseline
Figure 4: Performance on varying sizes (average
of 5 languages, ? = 0.9)
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
ac
cu
ra
cy
%
noise rate
our model
baseline
Figure 5: Performance on different projection
quality (average of 5 languages, ? = 0.9). The
noise rate is the percentage of the projected in-
stances being messed up.
ously outperforms the baseline with the increasing
size of training corpus. It is especially noteworthy
that the more training data is utilized the more su-
periority our model enjoys. That is, because our
method not only utilizes the projection informa-
tion but also avails itself of the monolingual cor-
pus.
6.4 Effect of Projection Quality
The projection quality can be influenced by the
quality of the source parsing, alignments, projec-
tion methods, corpus quality and many other fac-
tors. In order to detect the effects of varying pro-
jection qualities on our approach, we simulate the
complex projection procedure by messing up the
projected instances randomly with different noise
rates. The curves in Figure 5 show the perfor-
mance of WPC baseline and our bilingual-guided
method. For different noise rates, our model?s re-
sults consistently outperform the baselines. When
the noise rate is greater than 0.2, our improvement
49.5
...
54.6
...
58.2
58.6
59.0
59.4
59.8
60.2
0 0.02 0.04 0.06 0.08 0.1 ... 0.2 ... 0.3
ac
cu
ra
cy
%
alpha
our model
baseline(58.5)
Figure 6: The performance curve of our model
(random initialization) on Chinese, with respect to
a series of ratio ?. The baseline is the result of
WPC model.
increases with the growth of the noise rate. The re-
sult suggests that our method can solve some prob-
lems which are caused by projection noise.
6.5 Performance on Random Initialization
We test our model with random initialization on
different ?. The curve in Figure 6 shows the per-
formance of our model on Chinese.
The results seem supporting our unsupervised
optimization method when ? is in the range of
(0, 0.1). It implies that the unsupervised structure
information is useful, but it seems creating a nega-
tive effect on the model when ? is greater than 0.1.
Because the unsupervised part can gain constraints
from the projection part. But with the increase of
?, the strength of constraint dwindles, and the
unsupervised part will gradually lose control. And
bad unsupervised part pulls the full model down.
7 Conclusion and Future Work
This paper presents a bilingually-guided strate-
gy for automatic dependency grammar induction,
which adopts an unsupervised skeleton and lever-
ages the bilingually-projected dependency infor-
mation during optimization. By simultaneous-
ly maximizing the monolingual likelihood and
bilingually-projected likelihood in the EM proce-
dure, it effectively integrates the advantages of
bilingual projection and unsupervised induction.
Experiments on 5 languages show that the novel
strategy significantly outperforms previous unsu-
pervised or bilingually-projected models.
Since its computational complexity approaches to
the skeleton unsupervised model (with much few-
er iterations), and the bilingual text aligned to
1070
resource-rich languages is easy to obtain, such a
hybrid method seems to be a better choice for au-
tomatic grammar induction. It also indicates that
the combination of bilingual constraint and unsu-
pervised methodology has a promising prospect
for grammar induction. In the future work we will
investigate such kind of strategies, such as bilin-
gually unsupervised induction.
Acknowledgments
The authors were supported by National
Natural Science Foundation of China, Con-
tracts 61202216, 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03), Key
Project of Knowledge Innovation Program of Chi-
nese Academy of Sciences (No. KGZD-EW-501).
Qun Liu?s work is partially supported by Science
Foundation Ireland (Grant No.07/CE/I1142) as
part of the CNGL at Dublin City University. We
would like to thank the anonymous reviewers for
their insightful comments and those who helped
to modify the paper.
References
H. Alshawi. 1996. Head automata for speech transla-
tion. In Proc. of ICSLP.
James K Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65:S132.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In HLT: NAACL, pages 582?590.
Rens Bod. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proc. of the 21st ICCL and the
44th ACL, pages 865?872.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proc. of the
2002 Conference on EMNLP. Proc. CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proc. of the 43rd ACL, pages 173?180,
Ann Arbor, Michigan, June.
W. Chen, J. Kazama, and K. Torisawa. 2010. Bi-
text dependency parsing with bilingual subtree con-
straints. In Proc. of ACL, pages 21?29.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of the Conference on
EMNLP, pages 50?61.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conference on EMNLP, pages 1?8, July.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of IJCNLP of the AFNLP: Vol-
ume 1-Volume 1, pages 369?377.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proc. of ACL, pages 392?399.
R. Hwa, M. Osborne, A. Sarkar, and M. Steedman.
2003. Corrected co-training for statistical parsers.
In ICML-03 Workshop on the Continuum from La-
beled to Unlabeled Data in Machine Learning and
Data Mining, Washington DC.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311?325.
W. Jiang and Q. Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proc. of ACL, pages 12?20.
D. Klein and C.D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependen-
cy and constituency. In Proc. of ACL, page 478.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of the 48th ACL,
pages 1?11, July.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. pages 595?
603.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of the 11th Conf. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b. Non-projective dependency parsing using s-
panning tree algorithms. In Proc. of EMNLP, pages
523?530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of CoNLL, pages 216?
220.
1071
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proc. of EMNLP, pages 62?72. ACL.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. Journal of Artificial Intelli-
gence Research, 36(1):341?385.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proc. of the 50th ACL, pages 629?637,
July.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
CoNLL, pages 221?225.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st ICCL
& 44th ACL, pages 433?440, July.
A. Sarkar. 2001. Applying co-training methods to sta-
tistical parsing. In Proc. of NAACL, pages 1?8.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In Annual
Meeting-, volume 45, page 760.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354?362.
D.A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Proc. of EMNLP: Volume 2-Volume
2, pages 822?831.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
of IJCNLP of the AFNLP: Volume 1-Volume 1, pages
73?81.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
of the 49th ACL: HLT, pages 682?686.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How
?less is more? in unsupervised dependency parsing.
In HLT: NAACL, pages 751?759, June.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.
William, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with rich-
er contexts and smoothing. In Proc. of NAACL,
pages 101?109.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. of HLT,
pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proc. of the IJCNLP-08. Proc. CoNLL.
Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. Algorithm 778: L-bfgs-b: Fortran
subroutines for large-scale bound-constrained opti-
mization. ACM Transactions on Mathematical Soft-
ware (TOMS), 23(4):550?560.
1072
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 591?596,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Iterative Transformation of Annotation Guidelines for
Constituency Parsing
Xiang Li 1, 2 Wenbin Jiang 1 Yajuan Lu? 1 Qun Liu 1, 3
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{lixiang, jiangwenbin, lvyajuan}@ict.ac.cn
2University of Chinese Academy of Sciences
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
This paper presents an effective algorith-
m of annotation adaptation for constituen-
cy treebanks, which transforms a treebank
from one annotation guideline to anoth-
er with an iterative optimization proce-
dure, thus to build a much larger treebank
to train an enhanced parser without in-
creasing model complexity. Experiments
show that the transformed Tsinghua Chi-
nese Treebank as additional training da-
ta brings significant improvement over the
baseline trained on Penn Chinese Tree-
bank only.
1 Introduction
Annotated data have become an indispensable
resource for many natural language processing
(NLP) applications. On one hand, the amount of
existing labeled data is not sufficient; on the other
hand, however there exists multiple annotated da-
ta with incompatible annotation guidelines for the
same NLP task. For example, the People?s Daily
corpus (Yu et al, 2001) and Chinese Penn Tree-
bank (CTB) (Xue et al, 2005) are publicly avail-
able for Chinese segmentation.
An available treebank is a major resource for
syntactic parsing. However, it is often a key bottle-
neck to acquire credible treebanks. Various tree-
banks have been constructed based on differen-
t annotation guidelines. In addition to the most
popular CTB, Tsinghua Chinese Treebank (TC-
T) (Zhou, 2004) is another real large-scale tree-
bank for Chinese constituent parsing. Figure 1 il-
lustrates some differences between CTB and TCT
in grammar category and syntactic structure. Un-
fortunately, these heterogeneous treebanks can not
be directly merged together for training a parsing
model. Such divergences cause a great waste of
human effort. Therefore, it is highly desirable to
transform a treebank into another compatible with
another annotation guideline.
In this paper, we focus on harmonizing het-
erogeneous treebanks to improve parsing perfor-
mance. We first propose an effective approach to
automatic treebank transformation from one an-
notation guideline to another. For convenience
of reference, a treebank with our desired anno-
tation guideline is named as target treebank, and
a treebank with a differtn annotation guideline is
named as source treebank. Our approach proceeds
in three steps. A parser is firstly trained on source
treebank. It is used to relabel the raw sentences
of target treebank, to acquire parallel training da-
ta with two heterogeneous annotation guidelines.
Then, an annotation transformer is trained on the
parallel training data to model the annotation in-
consistencies. In the last step, a parser trained on
target treebank is used to generate k-best parse
trees with target annotation for source sentences.
Then the optimal parse trees are selected by the an-
notation transformer. In this way, the source tree-
bank is transformed to another with our desired
annotation guideline. Then we propose an op-
timization strategy of iterative training to further
improve the transformation performance. At each
iteration, the annotation transformation of source-
to-target and target-to-source are both performed.
The transformed treebank is used to provide better
annotation guideline for the parallel training da-
ta of next iteration. As a result, the better paral-
lel training data will bring an improved annotation
transformer at next iteration.
We perform treebank transformation from TC-
591
zjXXXXXEE
djHHH
np
ZZ