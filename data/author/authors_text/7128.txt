Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 6?7,
Vancouver, October 2005.
Demonstrating an Interactive Semantic Role Labeling System
Vasin Punyakanok Dan Roth Mark Sammons
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{punyakan,danr,mssammon}@uiuc.edu
Wen-tau Yih
Microsoft Research
Redmond, WA 98052, USA
scottyih@microsoft.com
Abstract
Semantic Role Labeling (SRL) is the task
of performing a shallow semantic analy-
sis of text (i.e., Who did What to Whom,
When, Where, How). This is a cru-
cial step toward deeper understanding of
text and has many immediate applications.
Preprocessed information on text, mostly
syntactic, has been shown to be impor-
tant for SRL. Current research focuses on
improving the performance assuming that
this lower level information is given with-
out any attention to the overall efficiency
of the final system, although minimizing
execution time is a necessity in order to
support real world applications. The goal
of our demonstration is to present an inter-
active SRL system that can be used both
as a research and an educational tool. Its
architecture is based on the state-of-the-
art system (the top system in the 2005
CoNLL shared task), modified to process
raw text through the addition of lower
level processors, while achieving effective
real time performance.
1 Introduction
Semantic parsing of sentences is believed to be an
important subtask toward natural language under-
standing, and has immediate applications in tasks
such information extraction and question answering.
We study semantic role labeling (SRL), defined as
follows: for each verb in a sentence, the goal is to
identify all constituents that fill a semantic role, and
to determine their roles (such as Agent, Patient or In-
strument) and their adjuncts (such as Locative, Tem-
poral or Manner). The PropBank project (Kingsbury
and Palmer, 2002), which provides a large human-
annotated corpus of semantic verb-argument rela-
tions, has opened doors for researchers to apply ma-
chine learning techniques to this task.
The focus of the research has been on improving
the performance of the SRL system by using, in ad-
dition to raw text, various syntactic and semantic in-
formation, e.g. Part of Speech (POS) tags, chunks,
clauses, syntactic parse tree, and named entities,
which is found crucial to the SRL system (Pun-
yakanok et al, 2005).
In order to support a real world application such
as an interactive question-answering system, the
ability of an SRL system to analyze text in real time
is a necessity. However, in previous research, the
overall efficiency of the SRL system has not been
considered. At best, the efficiency of an SRL sys-
tem may be reported in an experiment assuming that
all the necessary information has already been pro-
vided, which is not realistic. A real world scenario
requires the SRL system to perform all necessary
preprocessing steps in real time. The overall effi-
ciency of SRL systems that include the preproces-
sors is not known.
Our demonstration aims to address this issue. We
present an interactive system that performs the SRL
task from raw text in real time. Its architecture is
based on the top system in the 2005 CoNLL shared
task (Koomen et al, 2005), modified to process raw
text using lower level processors but maintaining
6
good real time performance.
2 The SRL System Architecture
Our system begins preprocessing raw text by
using sentence segmentation tools (available at
http://l2r.cs.uiuc.edu/?cogcomp/tools.php). Next,
sentences are analyzed by a state-of-the-art syntac-
tic parser (Charniak, 2000) the output of which pro-
vides useful information for the main SRL module.
The main SRL module consists of four stages:
pruning, argument identification, argument classifi-
cation, and inference. The following is the overview
of these four stages. Details of them can be found
in (Koomen et al, 2005).
Pruning The goal of pruning is to filter out un-
likely argument candidates using simple heuristic
rules. Only the constituents in the parse tree are
considered as argument candidates. In addition, our
system exploits a heuristic modified from that intro-
duced by (Xue and Palmer, 2004) to filter out very
unlikely constituents.
Argument Identification The argument identifi-
cation stage uses binary classification to identify
whether a candidate is an argument or not. We train
and apply the binary classifiers on the constituents
supplied by the pruning stage.
Argument Classification This stage assigns the
final argument labels to the argument candidates
supplied from the previous stage. A multi-class clas-
sifier is trained to classify the types of the arguments
supplied by the argument identification stage.
Inference The purpose of this stage is to incor-
porate some prior linguistic and structural knowl-
edge, such as ?arguments do not overlap? and ?each
verb takes at most one argument of each type.? This
knowledge is used to resolve any inconsistencies in
argument classification in order to generate legiti-
mate final predictions. The process is formulated as
an integer linear programming problem that takes as
input confidence values for each argument type sup-
plied by the argument classifier for each constituent,
and outputs the optimal solution subject to the con-
straints that encode the domain knowledge.
The system in this demonstration, however, dif-
fers from its original version in several aspects.
First, all syntactic information is extracted from the
output of the full parser, where the original version
used different information obtained from different
processors. Second, the named-entity information is
discarded. Finally, no combination of different parse
tree outputs is performed. These alterations aim to
enhance the efficiency of the system while maintain-
ing strong performance.
Currently the system runs at the average speed of
1.25 seconds/predicate. Its performance is 77.88 and
65.87 F1-score on WSJ and Brown test sets (Car-
reras and Ma`rquez, 2005) while the original system
achieves 77.11 and 65.6 on the same test sets with-
out the combination of multiple parser outputs and
79.44 and 67.75 with the combination.
3 Goal of Demonstration
The goal of the demonstration is to present the sys-
tem?s ability to perform the SRL task on raw text in
real time. An interactive interface allows users to in-
put free form text and to receive the SRL analysis
from our system. This demonstration can be found
at http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php.
Acknowledgments
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA?s AQUAINT Program, DOI?s Re-
flex program, and an ONR MURI Award.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared tasks: Semantic role labeling. In
Proc. of CoNLL-2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL 2000.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proc. of LREC-2002, Spain.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005.
Generalized Inference with Multiple Semantic Role
Labeling Systems. In Proceedings of CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of IJCAI-2005.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proc. of the EMNLP-2004.
7
Proceedings of ACL-08: HLT, pages 1030?1038,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extraction of Entailed Semantic Relations Through
Syntax-based Comma Resolution
Vivek Srikumar 1 Roi Reichart2 Mark Sammons1 Ari Rappoport2 Dan Roth1
1University of Illinois at Urbana-Champaign
{vsrikum2|mssammon|danr}@uiuc.edu
2Institute of Computer Science, Hebrew University of Jerusalem
{roiri|arir}@cs.huji.ac.il
Abstract
This paper studies textual inference by inves-
tigating comma structures, which are highly
frequent elements whose major role in the ex-
traction of semantic relations has not been
hitherto recognized. We introduce the prob-
lem of comma resolution, defined as under-
standing the role of commas and extracting the
relations they imply. We show the importance
of the problem using examples from Tex-
tual Entailment tasks, and present A Sentence
Transformation Rule Learner (ASTRL), a ma-
chine learning algorithm that uses a syntac-
tic analysis of the sentence to learn sentence
transformation rules that can then be used to
extract relations. We have manually annotated
a corpus identifying comma structures and re-
lations they entail and experimented with both
gold standard parses and parses created by a
leading statistical parser, obtaining F-scores of
80.2% and 70.4% respectively.
1 Introduction
Recognizing relations expressed in text sentences is
a major topic in NLP, fundamental in applications
such as Textual Entailment (or Inference), Question
Answering and Text Mining. In this paper we ad-
dress this issue from a novel perspective, that of un-
derstanding the role of the commas in a sentence,
which we argue is a key component in sentence
comprehension. Consider for example the following
three sentences:
1. Authorities have arrested John Smith, a retired
police officer.
2. Authorities have arrested John Smith, his friend
and his brother.
3. Authorities have arrested John Smith, a retired
police officer announced this morning.
Sentence (1) states that John Smith is a retired
police officer. The comma and surrounding sen-
tence structure represent the relation ?IsA?. In (2),
the comma and surrounding structure signifies a list,
so the sentence states that three people were ar-
rested: (i) John Smith, (ii) his friend, and (iii) his
brother. In (3), a retired police officer announced
that John Smith has been arrested. Here, the comma
and surrounding sentence structure indicate clause
boundaries.
In all three sentences, the comma and the sur-
rounding sentence structure signify relations essen-
tial to comprehending the meaning of the sentence,
in a way that is not easily captured using lexical-
or even shallow parse-level information. As a hu-
man reader, we understand them easily, but auto-
mated systems for Information Retrieval, Question
Answering, and Textual Entailment are likely to en-
counter problems when comparing structures like
these, which are lexically similar, but whose mean-
ings are so different.
In this paper we present an algorithm for comma
resolution, a task that we define to consist of (1) dis-
ambiguating comma type and (2) determining the
relations entailed from the sentence given the com-
mas? interpretation. Specifically, in (1) we assign
each comma to one of five possible types, and in
(2) we generate a set of natural language sentences
that express the relations, if any, signified by each
comma structure. The algorithm uses information
extracted from parse trees. This work, in addition to
having immediate significance for natural language
processing systems that use semantic content, has
potential applications in improving a range of auto-
1030
mated analysis by decomposing complex sentences
into a set of simpler sentences that capture the same
meaning. Although there are many other widely-
used structures that express relations in a similar
way, commas are one of the most commonly used
symbols1. By addressing comma resolution, we of-
fer a promising first step toward resolving relations
in sentences.
To evaluate the algorithm, we have developed an-
notation guidelines, and manually annotated sen-
tences from the WSJ PennTreebank corpus. We
present a range of experiments showing the good
performance of the system, using gold-standard and
parser-generated parse trees.
In Section 2 we motivate comma resolution
through Textual Entailment examples. Section 3 de-
scribes related work. Sections 4 and 5 present our
corpus annotation and learning algorithm. Results
are given in Section 6.
2 Motivating Comma Resolution Through
Textual Entailment
Comma resolution involves not only comma dis-
ambiguation but also inference of the arguments
(and argument boundaries) of the relationship repre-
sented by the comma structure, and the relationships
holding between these arguments and the sentence
as a whole. To our knowledge, this is the first pa-
per that deals with this problem, so in this section
we motivate it in depth by showing its importance
to the semantic inference task of Textual Entailment
(TE) (Dagan et al, 2006), which is increasingly rec-
ognized as a crucial direction for improving a range
of NLP tasks such as information extraction, ques-
tion answering and summarization.
TE is the task of deciding whether the meaning
of a text T (usually a short snippet) can be inferred
from the meaning of another text S. If this is the
case, we say that S entails T . For example2, we say
that sentence (1) entails sentence (2):
1. S: Parviz Davudi was representing Iran at a
meeting of the Shanghai Co-operation Orga-
nization (SCO), the fledgling association that
1For example, the WSJ corpus has 49K sentences, among
which 32K with one comma or more, 17K with two or more,
and 7K with three or more.
2The examples of this section are variations of pairs taken
from the Pascal RTE3 (Dagan et al, 2006) dataset.
binds two former Soviet republics of central
Asia, Russia and China to fight terrorism.
2. T: SCO is the fledgling association that binds
several countries.
To see that (1) entails (2), one must understand
that the first comma structure in sentence (1) is an
apposition structure, and does not indicate the begin-
ning of a list. The second comma marks a boundary
between entities in a list. To make the correct infer-
ence one must determine that the second comma is a
list separator, not an apposition marker. Misclassify-
ing the second comma in (1) as an apposition leads
to the conclusion that (1) entails (3):
3. T: Russia and China are two former Soviet re-
publics of central Asia .
Note that even to an educated native speaker of
English, sentence 1 may be initially confusing; dur-
ing the first reading, one might interpret the first
comma as indicating a list, and that ?the Shanghai
Co-operation Organization? and ?the fledgling asso-
ciation that binds...? are two separate entities that are
meeting, rather than two representations of the same
entity.
From these examples we draw the following con-
clusions: 1. Comma resolution is essential in com-
prehending natural language text. 2. Explicitly rep-
resenting relations derived from comma structures
can assist a wide range of NLP tasks; this can be
done by directly augmenting the lexical-level rep-
resentation, e.g., by bringing surface forms of two
text fragments with the same meaning closer to-
gether. 3. Comma structures might be highly am-
biguous, nested and overlapping, and consequently
their interpretation is a difficult task. The argument
boundaries of the corresponding extracted relations
are also not easy to detect.
The output of our system could be used to aug-
ment sentences with an explicit representation of en-
tailed relations that hold in them. In Textual Entail-
ment systems this can increase the likelihood of cor-
rect identification of entailed sentences, and in other
NLP systems it can help understanding the shallow
lexical/syntactic content of a sentence. A similar ap-
proach has been taken in (Bar-Haim et al, 2007; de
Salvo Braz et al, 2005), which augment the source
sentence with entailed relations.
1031
3 Related Work
Since we focus on extracting the relations repre-
sented by commas, there are two main strands of
research with similar goals: 1) systems that directly
analyze commas, whether labeling them with syn-
tactic information or correcting inappropriate use in
text; and 2) systems that extract relations from text,
typically by trying to identify paraphrases.
The significance of interpreting the role of com-
mas in sentences has already been identified by (van
Delden and Gomez, 2002; Bayraktar et al, 1998)
and others. A review of the first line of research is
given in (Say and Akman, 1997).
In (Bayraktar et al, 1998) the WSJ PennTreebank
corpus (Marcus et al, 1993) is analyzed and a very
detailed list of syntactic patterns that correspond to
different roles of commas is created. However, they
do not study the extraction of entailed relations as
a function of the comma?s interpretation. Further-
more, the syntactic patterns they identify are unlexi-
calized and would not support the level of semantic
relations that we show in this paper. Finally, theirs
is a manual process completely dependent on syn-
tactic patterns. While our comma resolution system
uses syntactic parse information as its main source
of features, the approach we have developed focuses
on the entailed relations, and does not limit imple-
mentations to using only syntactic information.
The most directly comparable prior work is that
of (van Delden and Gomez, 2002), who use fi-
nite state automata and a greedy algorithm to learn
comma syntactic roles. However, their approach dif-
fers from ours in a number of critical ways. First,
their comma annotation scheme does not identify
arguments of predicates, and therefore cannot be
used to extract complete relations. Second, for each
comma type they identify, a new Finite State Au-
tomaton must be hand-encoded; the learning com-
ponent of their work simply constrains which FSAs
that accept a given, comma containing, text span
may co-occur. Third, their corpus is preprocessed by
hand to identify specialized phrase types needed by
their FSAs; once our system has been trained, it can
be applied directly to raw text. Fourth, they exclude
from their analysis and evaluation any comma they
deem to have been incorrectly used in the source
text. We include all commas that are present in the
text in our annotation and evaluation.
There is a large body of NLP literature on punctu-
ation. Most of it, however, is concerned with aiding
syntactic analysis of sentences and with developing
comma checkers, much based on (Nunberg, 1990).
Pattern-based relation extraction methods (e.g.,
(Davidov and Rappoport, 2008; Davidov et al,
2007; Banko et al, 2007; Pasca et al, 2006; Sekine,
2006)) could in theory be used to extract relations
represented by commas. However, the types of
patterns used in web-scale lexical approaches cur-
rently constrain discovered patterns to relatively
short spans of text, so will most likely fail on
structures whose arguments cover large spans (for
example, appositional clauses containing relative
clauses). Relation extraction approaches such as
(Roth and Yih, 2004; Roth and Yih, 2007; Hirano
et al, 2007; Culotta and Sorenson, 2004; Zelenko et
al., 2003) focus on relations between Named Enti-
ties; such approaches miss the more general apposi-
tion and list relations we recognize in this work, as
the arguments in these relations are not confined to
Named Entities.
Paraphrase Acquisition work such as that by (Lin
and Pantel, 2001; Pantel and Pennacchiotti, 2006;
Szpektor et al, 2004) is not constrained to named
entities, and by using dependency trees, avoids the
locality problems of lexical methods. However,
these approaches have so far achieved limited accu-
racy, and are therefore hard to use to augment exist-
ing NLP systems.
4 Corpus Annotation
For our corpus, we selected 1,000 sentences con-
taining at least one comma from the Penn Treebank
(Marcus et al, 1993) WSJ section 00, and manu-
ally annotated them with comma information3. This
annotated corpus served as both training and test
datasets (using cross-validation).
By studying a number of sentences from WSJ (not
among the 1,000 selected), we identified four signif-
icant types of relations expressed through commas:
SUBSTITUTE, ATTRIBUTE, LOCATION, and LIST.
Each of these types can in principle be expressed us-
ing more than a single comma. We define the notion
3The guidelines and annotations are available at http://
L2R.cs.uiuc.edu/
?
cogcomp/data.php.
1032
of a comma structure as a set of one or more commas
that all relate to the same relation in the sentence.
SUBSTITUTE indicates an IS-A relation. An ex-
ample is ?John Smith, a Renaissance artist, was fa-
mous?. By removing the relation expressed by the
commas, we can derive three sentences: ?John Smith
is a Renaissance artist?, ?John Smith was famous?,
and ?a Renaissance artist was famous?. Note that in
theory, the third relation will not be valid: one exam-
ple is ?The brothers, all honest men, testified at the
trial?, which does not entail ?all honest men testified
at the trial?. However, we encountered no examples
of this kind in the corpus, and leave this refinement
to future work.
ATTRIBUTE indicates a relation where one argu-
ment describes an attribute of the other. For ex-
ample, from ?John, who loved chocolate, ate with
gusto?, we can derive ?John loved chocolate? and
?John ate with gusto?.
LOCATION indicates a LOCATED-IN relation. For
example, from ?Chicago, Illinois saw some heavy
snow today? we can derive ?Chicago is located in
Illinois? and ?Chicago saw some heavy snow today?.
LIST indicates that some predicate or property
is applied to multiple entities. In our annotation,
the list does not generate explicit relations; instead,
the boundaries of the units comprising the list are
marked so that they can be treated as a single unit,
and are considered to be related by the single rela-
tion ?GROUP?. For example, the derivation of ?John,
James and Kelly all left last week? is written as
?[John, James, and Kelly] [all left last week]?.
Any commas not fitting one of the descriptions
above are designated as OTHER. This does not in-
dicate that the comma signifies no relations, only
that it does not signify a relation of interest in this
work (future work will address relations currently
subsumed by this category). Analysis of 120 OTHER
commas show that approximately half signify clause
boundaries, which may occur when sentence con-
stituents are reordered for emphasis, but may also
encode implicit temporal, conditional, and other re-
lation types (for example, ?Opening the drawer, he
found the gun.?). The remainder comprises mainly
coordination structures (for example, ?Although he
won, he was sad?) and discourse markers indicating
inter-sentence relations (such as ?However, he soon
cheered up.?). While we plan to develop an anno-
Rel. Type Avg. Agreement # of Commas # of Rel.s
SUBSTITUTE 0.808 243 729
ATTRIBUTE 0.687 193 386
LOCATION 0.929 71 140
LIST 0.803 230 230
OTHER 0.949 909 0
Combined 0.869 1646 1485
Table 1: Average inter-annotator agreement for identify-
ing relations.
tation scheme for such relations, this is beyond the
scope of the present work.
Four annotators annotated the same 10% of the
WSJ sentences in order to evaluate inter-annotator
agreement. The remaining sentences were divided
among the four annotators. The resulting corpus was
checked by two judges and the annotation corrected
where appropriate; if the two judges disagreed, a
third judge was consulted and consensus reached.
Our annotators were asked to identify comma struc-
tures, and for each structure to write its relation type,
its arguments, and all possible simplified version(s)
of the original sentence in which the relation implied
by the comma has been removed. Arguments must
be contiguous units of the sentence and will be re-
ferred to as chunks hereafter. Agreement statistics
and the number of commas and relations of each
type are shown in Table 4. The Accuracy closely ap-
proximates Kappa score in this case, since the base-
line probability of chance agreement is close to zero.
5 A Sentence Tranformation Rule Learner
(ASTRL)
In this section, we describe a new machine learning
system that learns Sentence Transformation Rules
(STRs) for comma resolution. We first define the
hypothesis space (i.e., STRs) and two operations ?
substitution and introduction. We then define the
feature space, motivating the use of Syntactic Parse
annotation to learn STRs. Finally, we describe the
ASTRL algorithm.
5.1 Sentence Transformation Rules
A Sentence Transformation Rule (STR) takes a
parse tree as input and generates new sentences. We
formalize an STR as the pair l ? r, where l is a
tree fragment that can consist of non-terminals, POS
tags and lexical items. r is a set {ri}, each ele-
ment of which is a template that consists of the non-
1033
terminals of l and, possibly, some new tokens. This
template is used to generate a new sentence, called a
relation.
The process of applying an STR l ? r to a parse
tree T of a sentence s begins with finding a match for
l in T . A match is said to be found if l is a subtree
of T . If matched, the non-terminals of each ri are
instantiated with the terminals that they cover in T .
Instantiation is followed by generation of the output
relations in one of two ways: introduction or sub-
stitution, which is specified by the corresponding ri.
If an ri is marked as an introductory one, then the
relation is the terminal sequence obtained by replac-
ing the non-terminals in ri with their instantiations.
For substitution, firstly, the non-terminals of the ri
are replaced by their instantiations. The instantiated
ri replaces all the terminals in s that are covered by
the l-match. The notions of introduction and substi-
tution were motivated by ideas introduced in (Bar-
Haim et al, 2007).
Figure 1 shows an example of an STR and Figure
2 shows the application of this STR to a sentence. In
the first relation, NP1 and NP2 are instantiated with
the corresponding terminals in the parse tree. In the
second and third relations, the terminals of NP1 and
NP2 replace the terminals covered by NPp.
LHS: NPp
NP1 , NP2 ,
RHS:
1. NP1 be NP2 (introduction)
2. NP1 (substitution)
3. NP2 (substitution)
Figure 1: Example of a Sentence Transformation Rule. If
the LHS matches a part of a given parse tree, then the
RHS will generate three relations.
5.2 The Feature Space
In Section 2, we discussed the example where there
could be an ambiguity between a list and an apposi-
tion structure in the fragment two former Soviet re-
publics, Russia and China. In addition, simple sur-
face examination of the sentence could also identify
the noun phrases ?Shanghai Co-operation Organi-
zation (SCO)?, ?the fledgling association that binds
S
NPp
NP1
John Smith
, NP2
a renaissance
artist
,
V P
was
famous
RELATIONS:
1 [John Smith]/NP1 be [a renaissance artist]/NP2
2 [John Smith] /NP1 [was famous]
3 [a renaissance artist]/NP2 [was famous]
Figure 2: Example of application of the STR in Figure 1.
In the first relation, an introduction, we use the verb ?be?,
without dealing with its inflections. NP1 and NP2 are
both substitutions, each replacing NPp to generate the
last two relations.
two former Soviet Republics?, ?Russia? and ?China?
as the four members of a list. To resolve such ambi-
guities, we need a nested representation of the sen-
tence. This motivates the use of syntactic parse trees
as a logical choice of feature space. (Note, however,
that semantic and pragmatic ambiguities might still
remain.)
5.3 Algorithm Overview
In our corpus annotation, the relations and their ar-
gument boundaries (chunks) are explicitly marked.
For each training example, our learning algorithm
first finds the smallest valid STR ? the STR with the
smallest LHS in terms of depth. Then it refines the
LHS by specializing it using statistics taken from
the entire data set.
5.4 Generating the Smallest Valid STR
To transform an example into the smallest valid
STR, we utilize the augmented parse tree of the
sentence. For each chunk in the sentence, we find
the lowest node in the parse tree that covers the
chunk and does not cover other chunks (even par-
tially). It may, however, cover words that do not
belong to any chunk. We refer to such a node as
a chunk root. We then find the lowest node that cov-
ers all the chunk roots, referring to it as the pat-
tern root. The initial LHS consists of the sub-
tree of the parse tree rooted at the pattern root and
whose leaf nodes are all either chunk roots or nodes
that do not belong to any chunk. All the nodes are
labeled with the corresponding labels in the aug-
1034
mented parse tree. For example, if we consider the
parse tree and relations shown in Figure 2, then do-
ing the above procedure gives us the initial LHS
as S (NPp(NP1, NP2, ) V P ). The three relations
gives us the RHS with three elements ?NP1 be
NP2?, ?NP1 V P ? and ?NP1 V P ?, all three being
introduction.
This initial LHS need not be the smallest one that
explains the example. So, we proceed by finding the
lowest node in the initial LHS such that the sub-
tree of the LHS at that node can form a new STR
that covers the example using both introduction and
substitution. In our example, the initial LHS has a
subtree, NPp(NP1, NP2, ) that can cover all the re-
lations with the RHS consisting of ?NP1 be NP2?,
NP1 and NP2. The first RHS is an introduction,
while the second and the third are both substitutions.
Since no subtree of this LHS can generate all three
relations even with substitution, this is the required
STR. The final step ensures that we have the small-
est valid STR at this stage.
5.5 Statistical Refinement
The STR generated using the procedure outlined
above explains the relations generated by a single
example. In addition to covering the relations gen-
erated by the example, we wish to ensure that it does
not cover erroneous relations by matching any of the
other comma types in the annotated data.
Algorithm 1 ASTRL: A Sentence Transformation
Rule Learning.
1: for all t: Comma type do
2: Initialize STRList[t] = ?
3: p = Set of annotated examples of type t
4: n = Annotated examples of all other types
5: for all x ? p do
6: r = Smallest Valid STR that covers x
7: Get fringe of r.LHS using the parse tree
8: S = Score(r,p,n)
9: Sprev = ??
10: while S 6= Sprev do
11: if adding some fringe node to r.LHS causes a signifi-
cant change in score then
12: Set r = New rule that includes that fringe node
13: Sprev = S
14: S = Score(r,p,n)
15: Recompute new fringe nodes
16: end if
17: end while
18: Add r to STRList[t]
19: Remove all examples from p that are covered by r
20: end for
21: end for
For this purpose, we specialize the LHS so that it
covers as few examples from the other comma types
as possible, while covering as many examples from
the current comma type as possible. Given the most
general STR, we generate a set of additional, more
detailed, candidate rules. Each of these is obtained
from the original rule by adding a single node to
the tree pattern in the rule?s LHS, and updating the
rule?s RHS accordingly. We then score each of the
candidates (including the original rule). If there is
a clear winner, we continue with it using the same
procedure (i.e., specialize it). If there isn?t a clear
winner, we stop and use the current winner. After
finishing with a rule (line 18), we remove from the
set of positive examples of its comma type all exam-
ples that are covered by it (line 19).
To generate the additional candidate rules that we
add, we define the fringe of a rule as the siblings
and children of the nodes in its LHS in the original
parse tree. Each fringe node defines an additional
candidate rule, whose LHS is obtained by adding
the fringe node to the rule?s LHS tree. We refer to
the set of these candidate rules, plus the original one,
as the rule?s fringe rules. We define the score of an
STR as
Score(Rule,p,n) = Rp|p| ?
Rn
|n|
where p and n are the set of positive and negative
examples for this comma type, and Rp and Rn are
the number of positive and negative examples that
are covered by the STR. For each example, all exam-
ples annotated with the same comma type are pos-
itive while all examples of all other comma types
are negative. The score is used to select the win-
ner among the fringe rules. The complete algorithm
we have used is listed in Algorithm 1. For conve-
nience, the algorithm?s main loop is given in terms
of comma types, although this is not strictly nec-
essary. The stopping criterion in line 11 checks
whether any fringe rule has a significantly better
score than the rule it was derived from, and exits the
specialization loop if there is none.
Since we start with the smallest STR, we only
need to add nodes to it to refine it and never have
to delete any nodes from the tree. Also note that the
algorithm is essentially a greedy algorithm that per-
forms a single pass over the examples; other, more
1035
complex, search strategies could also be used.
6 Evaluation
6.1 Experimental Setup
To evaluate ASTRL, we used the WSJ derived cor-
pus. We experimented with three scenarios; in two
of them we trained using the gold standard trees
and then tested on gold standard parse trees (Gold-
Gold), and text annotated using a state-of-the-art sta-
tistical parser (Charniak and Johnson, 2005) (Gold-
Charniak), respectively. In the third, we trained and
tested on the Charniak Parser (Charniak-Charniak).
In gold standard parse trees the syntactic cate-
gories are annotated with functional tags. Since cur-
rent statistical parsers do not annotate sentences with
such tags, we augment the syntactic trees with the
output of a Named Entity tagger. For the Named
Entity information, we used a publicly available NE
Recognizer capable of recognizing a range of cat-
egories including Person, Location and Organiza-
tion. On the CoNLL-03 shared task, its f-score is
about 90%4. We evaluate our system from different
points of view, as described below. For all the eval-
uation methods, we performed five-fold cross vali-
dation and report the average precision, recall and
f-scores.
6.2 Relation Extraction Performance
Firstly, we present the evaluation of the performance
of ASTRL from the point of view of relation ex-
traction. After learning the STRs for the different
comma types using the gold standard parses, we
generated relations by applying the STRs on the test
set once. Table 2 shows the precision, recall and
f-score of the relations, without accounting for the
comma type of the STR that was used to generate
them. This metric, called the Relation metric in fur-
ther discussion, is the most relevant one from the
point of view of the TE task. Since a list does not
generate any relations in our annotation scheme, we
use the commas to identify the list elements. Treat-
ing each list in a sentence as a single relation, we
score the list with the fraction of its correctly identi-
fied elements.
In addition to the Gold-Gold and Gold-Charniak
4A web demo of the NER is at http://L2R.cs.uiuc.
edu/
?
cogcomp/demos.php.
settings described above, for this metric, we also
present the results of the Charniak-Charniak setting,
where both the train and test sets were annotated
with the output of the Charniak parser. The improve-
ment in recall in this setting over the Gold-Charniak
case indicates that the parser makes systematic er-
rors with respect to the phenomena considered.
Setting P R F
Gold-Gold 86.1 75.4 80.2
Gold-Charniak 77.3 60.1 68.1
Charniak-Charniak 77.2 64.8 70.4
Table 2: ASTRL performance (precision, recall and f-
score) for relation extraction. The comma types were
used only to learn the rules. During evaluation, only the
relations were scored.
6.3 Comma Resolution Performance
We present a detailed analysis of the performance of
the algorithm for comma resolution. Since this paper
is the first one that deals with the task, we could not
compare our results to previous work. Also, there
is no clear baseline to use. We tried a variant of
the most frequent baseline common in other disam-
biguation tasks, in which we labeled all commas as
OTHER (the most frequent type) except when there
are list indicators like and, or and but in adjacent
chunks (which are obtained using a shallow parser),
in which case the commas are labeled LIST. This
gives an average precision 0.85 and an average recall
of 0.36 for identifying the comma type. However,
this baseline does not help in identifying relations.
We use the following approach to evaluate the
comma type resolution and relation extraction per-
formance ? a relation extracted by the system is con-
sidered correct only if both the relation and the type
of the comma structure that generated it are correctly
identified. We call this metric the Relation-Type
metric. Another way of measuring the performance
of comma resolution is to measure the correctness of
the relations per comma type. In both cases, lists are
scored as in the Relation metric. The performance of
our system with respect to these two metrics are pre-
sented in Table 3. In this table, we also compare the
performance of the STRs learned by ASTRL with
the smallest valid STRs without further specializa-
tion (i.e., using just the procedure outlined in Sec-
tion 5.4).
1036
Type Gold-Gold Setting Gold-Charniak Setting
Relation-Type metric
Smallest Valid STRs ASTRL Smallest Valid STRs ASTRL
P R F P R F P R F P R F
Total 66.2 76.1 70.7 81.8 73.9 77.6 61.0 58.4 59.5 72.2 59.5 65.1
Relations Metric, Per Comma Type
ATTRIBUTE 40.4 68.2 50.4 70.6 59.4 64.1 35.5 39.7 36.2 56.6 37.7 44.9
SUBSTITUTE 80.0 84.3 81.9 87.9 84.8 86.1 75.8 72.9 74.3 78.0 76.1 76.9
LIST 70.9 58.1 63.5 76.2 57.8 65.5 58.7 53.4 55.6 65.2 53.3 58.5
LOCATION 93.8 86.4 89.1 93.8 86.4 89.1 70.3 37.2 47.2 70.3 37.2 47.2
Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types and
generating relations.
There is an important difference between the Re-
lation metric (Table 2) and the Relation-type met-
ric (top part of Table 3) that depends on the seman-
tic interpretation of the comma types. For example,
consider the sentence ?John Smith, 59, went home.?
If the system labels the commas in this as both AT-
TRIBUTE and SUBSTITUTE, then, both will gener-
ate the relation ?John Smith is 59.? According to
the Relation metric, there is no difference between
them. However, there is a semantic difference be-
tween the two sentences ? the ATTRIBUTE relation
says that being 59 is an attribute of John Smith while
the SUBSTITUTE relation says that John Smith is the
number 59. This difference is accounted for by the
Relation-Type metric.
From this standpoint, we can see that the special-
ization step performed in the full ASTRL algorithm
greatly helps in disambiguating between the AT-
TRIBUTE and SUBSTITUTE types and consequently,
the Relation-Type metric shows an error reduction
of 23.5% and 13.8% in the Gold-Gold and Gold-
Charniak settings respectively. In the Gold-Gold
scenario the performance of ASTRL is much better
than in the Gold-Charniak scenario. This reflects the
non-perfect performance of the parser in annotating
these sentences (parser F-score of 90%).
Another key evaluation question is the per-
formance of the method in identification of the
OTHER category. A comma is judged to be as
OTHER if no STR in the system applies to it.
The performance of ASTRL in this aspect is pre-
sented in Table 4. The categorization of this cate-
gory is important if we wish to further classify the
OTHER commas into finer categories.
Setting P R F
Gold-Gold 78.9 92.8 85.2
Gold-Charniak 72.5 92.2 81.2
Table 4: ASTRL performance (precision, recall and f-
score) for OTHER identification.
7 Conclusions
We defined the task of comma resolution, and devel-
oped a novel machine learning algorithm that learns
Sentence Transformation Rules to perform this task.
We experimented with both gold standard and parser
annotated sentences, and established a performance
level that seems good for a task of this complexity,
and which will provide a useful measure of future
systems developed for this task. When given au-
tomatically parsed sentences, performance degrades
but is still much higher than random, in both sce-
narios. We designed a comma annotation scheme,
where each comma unit is assigned one of four types
and an inference rule mapping the patterns of the
unit with the entailed relations. We created anno-
tated datasets which will be made available over the
web to facilitate further research.
Future work will investigate four main directions:
(i) studying the effects of inclusion of our approach
on the performance of Textual Entailment systems;
(ii) using features other than those derivable from
syntactic parse and named entity annotation of the
input sentence; (iii) recognizing a wider range of im-
plicit relations, represented by commas and in other
ways; (iv) adaptation to other domains.
Acknowledgement
The UIUC authors were supported by NSF grant
ITR IIS-0428472, DARPA funding under the Boot-
strap Learning Program and a grant from Boeing.
1037
References
M. Banko, M. Cafarella, M. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proc. of IJCAI, pages 2670?2676.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In Proc. of AAAI, pages 871?876.
M. Bayraktar, B. Say, and V. Akman. 1998. An analysis
of english punctuation: The special case of comma.
International Journal of Corpus Linguistics, 3(1):33?
57.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
of the Annual Meeting of the ACL, pages 173?180.
A. Culotta and J. Sorenson. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of the Annual
Meeting of the ACL, pages 423?429.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In Proc. of the Annual Meeting of the
ACL.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the Annual Meeting
of the ACL, pages 232?239.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailment in natural language. In Proc. of AAAI,
pages 1678?1679.
T. Hirano, Y. Matsuo, and G. Kikui. 2007. Detecting
semantic relations between named entities in text using
contextual features. In Proc. of the Annual Meeting of
the ACL, pages 157?160.
D. Lin and P. Pantel. 2001. DIRT: discovery of inference
rules from text. In Proc. of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining 2001,
pages 323?328.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G. Nunberg. 1990. CSLI Lecture Notes 18: The Lin-
guistics of Punctuation. CSLI Publications, Stanford,
CA.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proc. of the Annual Meeting of the
ACL, pages 113?120.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proc. of the Annual Meeting of
the ACL, pages 809?816.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
B. Say and V. Akman. 1997. Current approaches to
punctuation in computational linguistics. Computers
and the Humanities, 30(6):457?469.
S. Sekine. 2006. On-demand information extraction. In
Proc. of the Annual Meeting of the ACL, pages 731?
738.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based of entailment relations. In Proc. of
EMNLP, pages 49?56.
S. van Delden and F. Gomez. 2002. Combining finite
state automata and a greedy learning algorithm to de-
termine the syntactic roles of commas. In Proc. of IC-
TAI, pages 293?300.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106.
1038
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 57?60,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Framework for Entailed Relation Recognition
Dan Roth Mark Sammons V.G.Vinod Vydiswaran
University of Illinois at Urbana-Champaign
{danr|mssammon|vgvinodv}@illinois.edu
Abstract
We define the problem of recognizing entailed re-
lations ? given an open set of relations, find all oc-
currences of the relations of interest in a given doc-
ument set ? and pose it as a challenge to scalable
information extraction and retrieval. Existing ap-
proaches to relation recognition do not address well
problems with an open set of relations and a need
for high recall: supervised methods are not eas-
ily scaled, while unsupervised and semi-supervised
methods address a limited aspect of the problem, as
they are restricted to frequent, explicit, highly lo-
calized patterns. We argue that textual entailment
(TE) is necessary to solve such problems, propose
a scalable TE architecture, and provide preliminary
results on an Entailed Relation Recognition task.
1 Introduction
In many information foraging tasks, there is a need
to find all text snippets relevant to a target concept.
Patent search services spend significant resources
looking for prior art relevant to a specified patent
claim. Before subpoenaed documents are used in
a court case or intelligence data is declassified, all
sensitive sections need to be redacted. While there
may be a specific domain for a given application,
the set of target concepts is broad and may change
over time. For these knowledge-intensive tasks,
we contend that feasible automated solutions re-
quire techniques which approximate an appropri-
ate level of natural language understanding.
Such problems can be formulated as a relation
recognition task, where the information need is ex-
pressed as tuples of arguments and relations. This
structure provides additional information which
can be exploited to precisely fulfill the informa-
tion need. Our work introduces the Entailed Rela-
tion Recognition paradigm, which leverages a tex-
tual entailment system to try to extract all relevant
passages for a given structured query without re-
quiring relation-specific training data. This con-
trasts with Open Information Extraction (Banko
and Etzioni, 2008) and On-Demand Information
Extraction (Sekine, 2006), which aim to extract
large databases of open-ended facts, and with su-
pervised relation extraction, which requires addi-
tional supervised data to learn new relations.
Specifically, the contributions of this paper are:
1. Introduction of the entailed relation recognition
framework; 2. Description of an architecture and a
system which uses structured queries and an exist-
ing entailment engine to perform relation extrac-
tion; 3. Empirical assessment of the system on a
corpus of entailed relations.
2 Entailed Relation Recognition (ERR)
In the task of Entailed Relation Recognition, a cor-
pus and an information need are specified. The
corpus comprises all text spans (e.g. paragraphs)
contained in a set of documents. The information
need is expressed as a set of tuples encoding rela-
tions and entities of interest, where entities can be
of arbitrary type. The objective is to retrieve all
relevant text spans that a human would recognize
as containing a relation of interest. For example:
Information Need: An organization acquires weapons.
Text 1: ...the recent theft of 500 assault rifles by FARC...
Text 2: ...the report on FARC activities made three main ob-
servations. First, their allies supplied them with the 3? mor-
tars used in recent operations. Second, ...
Text 3: Amnesty International objected to the use of artillery
to drive FARC militants from heavily populated areas.
An automated system should identify Texts 1 and
2 as containing the relation of interest, and Text 3
as irrelevant. The system must therefore detect
relation instances that cross sentence boundaries
(?them? maps to ?FARC?, Text 2), and that re-
quire inference (?theft? implies ?acquire?, Text 1).
It must also discern when sentence structure pre-
cludes a match (?Amnesty International... use...
artillery? does not imply ?Amnesty International
57
acquires artillery?, Text 3).
The problems posed by instances like Text 2
are beyond the scope of traditional unsuper-
vised and semi-supervised relation-extraction ap-
proaches such as those used by Open IE and On-
Demand IE, which are constrained by their de-
pendency on limited, sentence-level structure and
high-frequency, highly local patterns, in which
relations are explicitly expressed as verbs and
nouns. Supervised methods such as (Culotta and
Sorensen, 2004) and (Roth and Yih, 2004) pro-
vide only a partial solution, as there are many pos-
sible relations and entities of interest for a given
domain, and such approaches require new anno-
tated data each time a new relation or entity type is
needed. Information Retrieval approaches are op-
timized for document-level performance, and en-
hancements like pseudo-feedback (Rocchio, 1971)
are less applicable to the localized text spans
needed in the tasks of interest; as such, it is un-
likely that they will reliably retrieve all correct in-
stances, and not return superficially similar but in-
correct instances (such as Text 3) with high rank.
Attempts have been made to apply Textual En-
tailment in larger scale applications. For the task
of Question Answering, (Harabagiu and Hickl,
2006) applied a TE component to rerank candidate
answers returned by a retrieval step. However, QA
systems rely on redundancy in the same way Open
IE does: a large document set has so many in-
stances of a given relation that at least some will
be sufficiently explicit and simple that standard IR
approaches will retrieve them. A single correct in-
stance suffices to complete the QA task, but does
not meet the needs of the task outlined here.
Recognizing relation instances requiring infer-
ence steps, in the absence of labeled training data,
requires a level of text understanding. A suit-
able proxy for this would be a successful Textual
Entailment Recognition (TE) system. (Dagan et
al., 2006) define the task of Recognizing Textual
Entailment (RTE) as: ...a directional relation be-
tween two text fragments, termed T ? the entailing
text, and H ? the entailed text. T entails H if, typ-
ically, a human reading T would infer that H is
most likely true. For relation recognition, the rela-
tion triple (e.g. ?Organization acquires weapon?)
is the hypothesis, and a candidate text span that
might contain the relation is the text. The def-
inition of RTE clearly accommodates the range
of phenomena described for the examples above.
However, the more successful TE systems (e.g.
(Hickl and Bensley, 2007)) are typically resource
intensive, and cannot scale to large retrieval tasks
if a brute force approach is used.
We define the task of Entailed Relation Recog-
nition thus: Given a text collection D, and an in-
formation need specified in a set of [argument, re-
lation, argument] triples S: for each triple s ? S,
identify all texts d ? D such that d entails s.
The information need triples, or queries, encode
relations between arbitrary entities (specifically,
these are not constrained to be Named Entities).
This problem is distinct from recent work in
Textual Entailment as we constrain the structure
of the Hypothesis to be very simple, and we re-
quire that the task be of a significantly larger scale
than the RTE tasks to date (which are typically of
the order of 800 Text-Hypothesis pairs).
3 Scalable ERR Algorithm
Our scalable ERR approach, SERR, consists of
two stages: expanded lexical retrieval, and entail-
ment recognition. The SERR algorithm is pre-
sented in Fig. 1. The goal is to scale Textual
Entailment up to a task involving large corpora,
where hypotheses (queries) may be entailed by
multiple texts. The task is kept tractable by de-
composing TE capabilities into two steps.
The first step, Expanded Lexical Retrieval
(ELR), uses shallow semantic resources and simi-
larity measures, thereby incorporating some of the
semantic processing used in typical TE systems.
This is required to retrieve, with high recall, se-
mantically similar content that may not be lexi-
cally similar to query terms, to ensure return of
a set of texts that are highly likely to contain the
concept of interest.
The second step applies a textual entailment
system to this text set and the query in order to
label the texts as ?relevant? or ?irrelevant?, and re-
quires deeper semantic resources in order to dis-
cern texts containing the concept of interest from
those that do not. This step emphasizes higher pre-
cision, as it filters irrelevant texts.
3.1 Implementation of SERR
In the ELR stage, we use a structured query that
allows more precise search and differential query
expansion for each query element. Semantic units
in the texts (e.g. Named Entities, phrasal verbs)
are indexed separately from words; each index is
58
SERR Algorithm
SETUP:
Input: Text set D
Output: Indices {I} over D
for all texts d ? D
Annotate d with local semantic content
Build Search Indices {I} over D
APPLICATION:
Input: Information need S
EXPANDED LEXICAL RETRIEVAL (ELR)(s):
R? ?
Expand s with semantically similar words
Build search query q
s
from s
R? k top-ranked texts for q
s
using indices {I}
return R
SERR:
Answer set A? ?
for all queries s ? S
R? ELR(s)
Answer set A
s
? ?
for all results r ? R
Annotate s, r with NLP resources
if r entails s
A
s
? A
s
? r
A? A ? {A
s
}
return A
Figure 1. SERR algorithm
a hierarchical similarity structure based on a type-
specific metric (e.g. WordNet-based for phrasal
verbs). Query structure is also used to selectively
expand query terms using similarity measures re-
lated to types of semantic units, including distribu-
tional similarity (Lin and Pantel, 2001), and mea-
sures based on WordNet (Fellbaum, 1998).
We assess three different Textual Entailment
components: LexPlus, a lexical-level system
that achieves relatively good performance on the
RTE challenges, and two variants of Predicate-
based Textual Entailment, PTE-strict and PTE-
relaxed, which use a predicate-argument repre-
sentation. The former is constrained to select a
single predicate-argument structure from each re-
sult, which is compared to the query component-
by-component using similarity measures similar to
the LexPlus system. PTE-relaxed drops the single-
predicate constraint, and can be thought of as a
?bag-of-constituents? model. In both, features are
extracted based on the predicate-argument compo-
nents? match scores and their connecting structure,
and the rank assigned by ELR. These features are
used by a classifier that labels each result as ?rel-
evant? or ?irrelevant?. Training examples are se-
lected from the top 7 results returned by ELR for
queries corresponding to entailment pair hypothe-
ses from the RTE development corpora; test exam-
ples are similarly selected from results for queries
from the RTE test corpora (see section 3.2).
3.2 Entailed Relation Recognition Corpus
To assess performance on the ERR task, we de-
rive a corpus from the publicly available RTE
data. The corpus consists of a set S of informa-
tion needs in the form of [argument, relation, argu-
ment] triples, and a set D of text spans (short para-
graphs), half of which entail one or more s ? S
while the other half are unrelated to S. D com-
prises all 1, 950 Texts from the IE and IR sub-
tasks of the RTE Challenge 1?3 datasets. The
shorter hypotheses in these examples allow us to
automatically induce their structured query form
from their shallow semantic structure. S was au-
tomatically generated from the positive entailment
pairs in D, by annotating their hypotheses with a
publicly available SRL tagger (Punyakanok et al,
2008) and inferring the relation and two main ar-
guments to form the equivalent queries.
Since some Hypotheses and Texts appear mul-
tiple times in the RTE corpora, we automatically
extract mappings from positive Hypotheses to one
or more Texts by comparing hypotheses and texts
from different examples. This provides the label-
ing needed for evaluation. In the resulting corpus,
a wide range of relations are sparsely represented;
they exemplify many linguistic and semantic char-
acteristics required to infer the presence of non-
explicit relations.
4 Results and Discussion
Top # Basic ELR Rel.Impr. Err.Redu.
1 48.1% 55.2% +14.8% 13.7%
2 68.1% 72.8% +6.9% 14.7%
3 75.2% 78.5% +4.4% 17.7%
Table 1. Change in relevant results retrieved in top 3
positions for basic and expanded lexical retrieval
System Acc. Prec. Rec. F
1
Baseline 18.1 18.1 100.0 30.7
LexPlus 81.6 44.9 62.5 55.5
PTE-relax. 71.9 37.7 72.0 49.0
(0.1) (5.5) (6.2) (4.1)
PTE-strict 83.6 55.4 61.5 57.9
(1.3) (3.4) (7.9) (2.1)
Table 2. Comparison of performance of SERR with
different TE algorithms. Numbers in parentheses are
standard deviations.
Table 1 compares the results of SERR with and
59
# System RTE 1 RTE 2 RTE 3 Avg. Acc.
LexPlus 49.0 65.2 [3] 76.5 [2] 66.3
PTE-relaxed 54.5 (1.0) 68.7 (1.5) [3] 82.3 (2.0) [1] 71.2 (1.2)
PTE-strict 64.8 (2.3) [1] 71.2 (2.6) [3] 76.0 (3.2) [2] 71.8 (2.6)
Table 3. Performance (accuracy) of SERR system variants on RTE challenge
examples; numbers in parentheses are standard deviations, while numbers in
brackets indicate where systems would have ranked in the RTE evaluations.
Comparisons
Standard TE 3,802,500
SERR 13,650
Table 4. Entailment compar-
isons needed for standard TE
vs. SERR
without the ELR?s semantic enhancements. For
each rank k, the entries represent the proportion of
queries for which the correct answer was returned
in the top k positions. The semantic enhancements
improve the number of matched results at each of
the top 3 positions.
Table 2 compares variants of the SERR imple-
mentation. The baseline labels every result re-
turned by ELR as ?relevant?, giving high recall
but low precision. PTE-relaxed performs better
than baseline, but poorly compared to PTE-strict
and LexPlus. Our analysis shows that LexPlus
has a relatively high threshold, and correctly labels
as negative some examples mislabeled by PTE-
relaxed, which may match two of the three con-
stituents in a hypothesis and label that result as
positive. PTE-strict will correctly identify some
such examples as it will force some match edges to
be ignored, and will correctly identify some neg-
ative examples due to structural constraints even
when LexPlus finds matches for all query terms.
PTE-strict strikes the best balance between preci-
sion and recall on positive examples.
Table 3 shows the accuracy of SERR?s clas-
sification of the examples from each RTE chal-
lenge; results not returned in the top 7 ranks by
ELR are labeled ?irrelevant?. PTE-strict and PTE-
relaxed perform comparably overall, though PTE-
strict has more uniform results over the different
challenges. Both outperform the LexPlus system
overall, and perform well compared to the best re-
sults published for the RTE challenges.
The significant computational gain of SERR is
shown in Table 4, exhibiting the much greater
number of comparisons required by a brute force
TE approach compared to SERR: SERR performs
well compared to published results for RTE chal-
lenges 1-3, but makes only 0.36% of the TE com-
parisons needed by standard approaches on our
ERR task.
5 Conclusion
We have proposed an approach to solving the En-
tailed Relation Recognition task, based on Tex-
tual Entailment, and implemented a solution that
shows that a Textual Entailment Recognition sys-
tem can be scaled to a much larger IE problem
than that represented by the RTE challenges. Our
preliminary results demonstrate the utility of the
proposed architecture, which allows strong perfor-
mance in the RTE task and efficient application to
a large corpus (table 4).
Acknowledgments
We thank Quang Do, Yuancheng Tu, and Kevin
Small. This work is funded by a grant from Boeing
and by MIAS, a DHS-IDS Center for Multimodal
Information Access and Synthesis at UIUC.
References
[Banko and Etzioni2008] M. Banko and O. Etzioni. 2008.
The Tradeoffs Between Open and Traditional Relation Ex-
traction. In ACL-HLT, pages 28?36.
[Culotta and Sorensen2004] A. Culotta and J. Sorensen.
2004. Dependency Tree Kernels for Relation Extraction.
In ACL, pages 423?429.
[Dagan et al2006] I. Dagan, O. Glickman, and B. Magnini,
editors. 2006. The PASCAL Recognising Textual Entail-
ment Challenge., volume 3944. Springer-Verlag, Berlin.
[Fellbaum1998] C. Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
[Harabagiu and Hickl2006] S. Harabagiu and A. Hickl. 2006.
Methods for Using Textual Entailment in Open-Domain
Question Answering. In ACL, pages 905?912.
[Hickl and Bensley2007] A. Hickl and J. Bensley. 2007. A
Discourse Commitment-Based Framework for Recogniz-
ing Textual Entailment. In ACL, pages 171?176.
[Lin and Pantel2001] D. Lin and P. Pantel. 2001. Induction of
semantic classes from natural language text. In SIGKDD,
pages 317?322.
[Punyakanok et al2008] V. Punyakanok, D. Roth, and
W. Yih. 2008. The Importance of Syntactic Parsing and
Inference in Semantic Role Labeling. CL, 34(2).
[Rocchio1971] J. Rocchio, 1971. Relevance feedback in In-
formation Retrieval, pages 313?323. Prentice Hall.
[Roth and Yih2004] D. Roth and W. Yih. 2004. A linear pro-
gramming formulation for global inference in natural lan-
guage tasks. In CoNLL, pages 1?8.
[Sekine2006] S. Sekine. 2006. On-Demand Information Ex-
traction. In COLING/ACL, pages 731?738.
60
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 107?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic and Logical Inference Model for Textual Entailment
Dan Roth
University of Illinois
at Urbana-Champaign
Urbana, IL 61801
danr@cs.uiuc.edu
Mark Sammons
University of Illinois
at Urbana-Champaign
Urbana, IL 61801
mssammon@uiuc.edu
Abstract
We compare two approaches to the problem
of Textual Entailment: SLIM, a composi-
tional approach modeling the task based on
identifying relations in the entailment pair,
and BoLI, a lexical matching algorithm.
SLIM?s framework incorporates a range of
resources that solve local entailment prob-
lems. A search-based inference procedure
unifies these resources, permitting them to
interact flexibly. BoLI uses WordNet and
other lexical similarity resources to detect
correspondence between related words in
the Hypothesis and the Text. In this pa-
per we describe both systems in some detail
and evaluate their performance on the 3rd
PASCAL RTE Challenge. While the lex-
ical method outperforms the relation-based
approach, we argue that the relation-based
model offers better long-term prospects for
entailment recognition.
1 Introduction
We compare two Textual Entailment recognition
systems applied to the 3rd PASCAL RTE challenge.
Both systems model the entailment task in terms
of determining whether the Hypothesis can be ?ex-
plained? by the Text.
The first system, BoLI (Bag of Lexical Items)
uses WordNet and (optionally) other word similarity
resources to compare individual words in the Hy-
pothesis with the words in the Text.
The second system, the Semantic and Logical
Inference Model (SLIM) system, uses a relational
model, and follows the model-theory-based ap-
proach of (Braz et al, 2005).
SLIM uses a suite of resources to modify the orig-
inal entailment pair by augmenting or simplifying
either or both the Text and Hypothesis. Terms re-
lating to quantification, modality and negation are
detected and removed from the graphical represen-
tation of the entailment pair and resolved with an
entailment module that handles basic logic.
In this study we describe the BoLI and SLIM sys-
tems and evaluate their performance on the 3rd PAS-
CAL RTE Challenge corpora. We discuss some ex-
amples and possible improvements for each system.
2 System Description: Bag of Lexical
Items (BoLI)
The BoLI system compares each word in the text
with a word in the hypothesis. If a word is found in
the Text that entails a word in the Hypothesis, that
word is considered ?explained?. If the percentage of
the Hypothesis that can be explained is above a cer-
tain threshold, the Text is considered to entail the
Hypothesis. This threshold is determined using a
training set (in this case, the Development corpus),
by determining the percentage match for each entail-
ment pair and selecting the threshold that results in
the highest overall accuracy.
BoLI uses an extended set of stopwords includ-
ing auxiliary verbs, articles, exclamations, and dis-
course markers in order to improve the distinction
between Text and Hypothesis. Negation and modal-
ity are not explicitly handled.
The BoLI system can be changed by varying
the comparison resources it uses. The available
107
resources are: WordNet-derived (Fellbaum, 1998)
synonymy, meronymy, membership, and hyper-
nymy; a filtered version of Dekang Lin?s word sim-
ilarity list (Lin, 1998) (only the ten highest-scored
entries for each word); and a resource based on a
lexical comparison of WordNet glosses.
We tried three main versions; one that used the
four WordNet- derived resources (   ); a second
that adds to the first system the Dekang Lin resource
(   ); and a third that added to the second sys-
tem the Gloss resource (  	
 ). We ran them on
the Development corpus, and determined the thresh-
old that gave the highest overall score. We then
used the highest-scoring version and the correspond-
ing threshold to determine labels for the Test cor-
pus. The results and thresholds for each variation
are given in table 1.
3 System Description: Semantic and
Logical Inference Model (SLIM)
The SLIM system approaches the problem of entail-
ment via relations: the goal is to recognize the rela-
tions in the Text and Hypothesis, and use these to de-
termine whether the Text entails the Hypothesis. A
word in the Hypothesis is considered ?covered? by
a relation if it appears in that relation in some form
(either directly or via abstraction). For the Text to
entail the Hypothesis, sufficient relations in the Hy-
pothesis must be entailed by relations in the Text to
cover the underlying text.
The term ?Relation? is used here to describe a
predicate-argument structure where the predicate is
represented by a verb (which may be inferred from a
nominalized form), and the arguments by strings of
text from the original sentence. These constituents
may be (partially) abstracted by replacing tokens
in some constituent with attributes attached to that
or a related constituent (for example, modal terms
may be dropped and represented with an attribute
attached to the appropriate predicate).
Relations may take other relations as arguments.
Examples include ?before? and ?after? (when both
arguments are events) and complement structures.
3.1 Representation
The system compares the Text to the Hypothesis us-
ing a ?blackboard? representation of the two text
fragments (see figure 1). Different types of anno-
tation are specified on different layers, all of which
are ?visible? to the comparison algorithm. All lay-
ers map to the original representation of the text, and
each annotated constituent corresponds to some ini-
tial subset of this original representation. This al-
lows multiple representations of the same surface
form to be entertained.
Figure 1 shows some of the layers in this data
structure for a simple entailment pair: the origi-
nal text in the WORD layer; the relations induced
from this text in the PREDICATE layer; and for
the Text, a Coreference constituent aligned with the
word ?he? in the COREFERENCE layer. Note that
the argument labels for ?give? in the Text indicate
that ?he? is the theme/indirect object of the predi-
cate ?give?.
Figure 1: ?Blackboard? Representation of Entail-
ment Pairs in SLIM
The     President     was      happy     that     he     was     given    the    award     .  WORD
The     President      be       happy      that     he     was     given    the    award
ARG1 PRED ARG2
COREF The   President
PREDICATE
TEXT
HYPOTHESIS
WORD
PREDICATE
The     President     received    an     award     . 
The     President     receive     an     award
PRED ARG1ARG2
he          give            the     award
PRED ARG1ARG0
At compare-time, the coref constituent ?The Pres-
ident? will be considered as a substitute for ?he?
when comparing the relation in the Hypothesis with
the second relation in the Text. (The dashed lines
indicate that the coverage of the coreference con-
stituent is just that of the argument consisting of the
word ?he?.) The relation comparator has access to
a list of rules mapping between verbs and their ar-
gument types; this will allow it to recognize that the
relation ?give? can entail ?receive?, subject to the
constraint that the agent of ?give? must be the patient
of ?receive?, and vice versa. This, together with the
coreference constituent in the Text that aligns with
the argument ?he?, will allow the system to recog-
nize that the Text entails the Hypothesis.
108
3.2 Algorithm
The SLIM entailment system applies sequences of
transformations to the original entailment pair in or-
der to modify one or both members of the pair to
make it easier to determine whether the Text entails
the Hypothesis. The resources that make these trans-
formations are referred to here as ?operators?. Each
operator is required to use Purposeful Inference: be-
fore making a change to either entailment pair mem-
ber, they must take the other member into account.
For example, the conjunction expander will generate
only those expansions in a text fragment that match
structures in the paired text fragment more closely.
This constrains the number of transformations con-
sidered and can reduce the amount of noise intro-
duced by these operators.
Each such operator serves one of three purposes:
1. ANNOTATE. Make some implicit property of
the meaning of the sentence explicit.
2. SIMPLIFY/TRANSFORM. Remove or alter
some section of the Text in order to improve
annotation accuracy or make it more similar to
the Hypothesis.
3. COMPARE. Compare (some elements of) the
two members of the entailment pair and as-
sign a score that correlates to how successfully
(those elements of) the Hypothesis can be sub-
sumed by the Text.
The system?s operators are applied to an entail-
ment pair, potentially generating a number of new
versions of that entailment pair. They may then be
applied to these new versions. It is likely that only
a subset of the operators will fire. It is also possible
that multiple operators may affect overlapping sec-
tions of one or both members of the entailment pair,
and so the resulting perturbations of the original pair
may be sensitive to the order of application.
To explore these different subsets/orderings, the
system is implemented as a search process over the
different operators. The search terminates as soon
as a satisfactory entailment score is returned by the
comparison operator for a state reached by applying
transformation operators, or after some limit to the
depth of the search is reached. If entailment is de-
termined to hold, the set of operations that generated
the terminal state constitutes a proof of the solution.
3.2.1 Constraining the Search
To control the search to allow for the interdepen-
dence of certain operators, each operator may spec-
ify a set of pre- and post-conditions. Pre-conditions
specify which operators must have fired to provide
the necessary input for the current operator. Post-
conditions typically indicate whether or not it is de-
sirable to re-annotate the resulting entailment pair
(e.g. after an operation that appends a new relation
to an entailment pair member), or whether the Com-
parator should be called to check for entailment.
3.3 System Resources: Annotation
The SLIM system uses a number of standard an-
notation resources ? Part-of-Speech, Shallow- and
Full syntactic parsing, Named Entity tagging, and
Semantic Role Labelling ? but also has a number
of more specialized resources intended to recognize
implicit predicates from the surface representation
in the text, and append these relations to the original
text. These resources are listed below with a brief
description of each.
Apposition Detector. Uses full parse information
to detect appositive constructions, adding a relation
that makes the underlying meaning explicit. It uses
a set of rules specifying subtree structure and phrase
labels.
Complex Noun Phrase Relation Detector. An-
alyzes long noun phrases and annotates them with
their implicit relations. It applies a few general
rules expressed at the shallow parse and named en-
tity level.
Modality and Negation Annotator. Abstracts
modifiers of relations representing modality or nega-
tion into attributes attached to the relation.
Discourse Structure Annotator. Scans the rela-
tion structure (presently only at the sentence level)
to determine negation and modality of relations em-
bedded in factive and other constructions. It marks
the embedded relations accordingly, and where pos-
sible, discards the embedding relation.
Coreference Annotator. Uses Named Entity
information to map pronouns to possible replace-
ments.
Nominalization Rewriter. Detects certain com-
mon nominalized verb structures and makes the re-
lation explicit. The present version applies a small
set of very general rules instantiated with a list of
109
embedding verbs and a mapping from nominalized
to verbal forms.
3.4 System Resources:
Simplification/Transformation
The simplification resources all demonstrate pur-
poseful inference, as described in section 3.2.
Idiom Catcher. Identifies and replaces sequences
of words corresponding to a list of known idioms,
simplifying sentence structure. It can recognize a
range of surface representations for each idiom.
Phrasal Verb Replacer. Checks for phrasal verb
constructions, including those where the particle is
distant from the main verb, replacing them with sin-
gle verbs of equivalent meaning.
Conjunction Expander. Uses full parse informa-
tion to detect and rewrite conjunctive argument and
predicate structures by expanding them.
Multi-Word Expression Contractor. Scans both
members of the entailment pair for compound noun
phrases that can be replaced by just the head of the
phrase.
3.5 System Resources: Main Comparator
All comparator resources are combined in a single
operator for simplicity. This comparator uses the
blackboard architecture described in 3.1.
The main comparator compares each relation in
the Hypothesis to each relation in the Text, return-
ing ?True? if sufficient relations in the Hypothesis
are entailed by relations in the Text to cover the un-
derlying representation of the Hypothesis.
For a relation in the Text to entail a relation in the
Hypothesis, the Text predicate must entail the Hy-
pothesis predicate, and all arguments of the Hypoth-
esis relation must be entailed by arguments of the
Text relation. This entailment check also accounts
for attributes such as negation and modality.
As part of this process, a set of rules that map be-
tween predicate- argument structures (some hand-
written, most derived from VerbNet) are applied
on-the-fly to the pair of relations being compared.
These rules specify a mapping between predicates
and a set of constraints that apply to the mappings
between arguments of the predicates. For example,
the agent of the relation ?sell? should be the theme
of the relation ?buy?, and vice versa.
When comparing the arguments of predicates, the
system uses BoLI with the same configuration and
threshold that give the best performance on the de-
velopment set.
3.6 Comparison to Similar Approaches
Like (de Marneffe et al, 2005), SLIM?s represen-
tation abstracts away terms relating to negation,
modality and quantification. However, it uses them
as part of the comparison process, not as features
to be used in a classifier. In contrast to (Braz
et al, 2005), SLIM considers versions of the en-
tailment pair with and without simplifications of-
fered by preprocessing modules, rather than reason-
ing only about the simplified version; and rather
than formulating the subsumption (entailment) prob-
lem as a hierarchical linear program or classification
problem, SLIM defers local entailment decisions to
its modules and returns a positive label for a con-
stituent only if these resources return a positive la-
bel for all subconstituents. Finally, SLIM returns an
overall positive label if all words in the Hypothesis
can be ?explained? by relations detected in the Hy-
pothesis and matched in the Text, rather than requir-
ing all detected relations in the Text to be entailed
by relations in the Hypothesis.
4 Experimental Results
Table 3 presents the peformance of the BoLI and
SLIM systems on the 3rd PASCAL RTE Challenge.
The version of SLIM used for the Development cor-
pus was incomplete, as several modules (Multi-word
Expression, Conjunction, and Apposition) were still
being completed at that time. Table 1 indicates the
performance of   different versions of the BoLI sys-
tem on the Development corpus as described in sec-
tion 2.
To investigate the improvement of performance
for the SLIM system relative to the available re-
sources, we conducted a limited ablation study. Ta-
ble 2 shows the performance for   different ver-
sions of the SLIM system on 100 entailment pairs
each from the IE and QA subtasks of the Test cor-
pus. The ?full? (f) system includes all available re-
sources. The ?intermediate? (i) system excludes the
resources we consider most likely to introduce er-
rors, the Multiword Expression module and the most
general Nominalization rewrite rules in the Nom-
inalization Rewriter. The ?strict? (s) system also
omits the Apposition and Complex Noun Phrase
110
Table 1: Accuracy and corresponding threshold for
versions of BoLI on the Development corpus.
TASK Accuracy Threshold
 	
 0.675 0.667
 	

0.650 0.833
 	
 0.655 0.833
Table 2: Results for different versions of SLIM on
subsets of the Test and Develoment corpora.
System SLIM s SLIM i SLIM f
Dev IE - - 0.650
Dev QA - - 0.660
Test IE 0.480 0.480 0.470
Test QA 0.680 0.710 0.710
modules. To give a sense of how well the complete
SLIM system does on the Development corpus, the
results for the full SLIM system on equal-sized sub-
sets of the IE and QA subtasks of the Development
corpus are also shown.
5 Discussion
From Table 3, it is clear that BoLI outperforms
SLIM in every subtask.
The ablation study in Table 2 shows that adding
new resources to SLIM has mixed benefits; from
the samples we used for evaluation, the intermediate
system would be the best balance between module
coverage and module accuracy.
In the rest of this section, we analyze the re-
sults and each system?s behavior on several exam-
ples from the corpus.
5.1 BoLI
There is a significant drop in performance of the
BoLI from the Development corpus to the Test cor-
pus, indicating that the threshold somewhat overfit-
ted to the data used to train it. The performance drop
when adding the gloss and Dekang Lin word simi-
larity resources is not necessarily surprising, as these
resources are clearly noisy, and so may increase sim-
ilarity based on inappropriate word pairs.
In the following example, the word similarity is
high, but the structure of the two text fragments
gives the relevant words different overall meaning
(here, that one subset of the matched words does not
apply to the other):
id=26 Text: Born in Kingston-upon-Thames, Surrey, Brock-
well played his county cricket for the very strong Surrey side of
the last years of the 19th century.
Hypothesis: Brockwell was born in the last years of the 19th
century.
From this example it is clear that in addition to
the role of noise from these additional resources, the
structure of text plays a major role in meaning, and
this is exactly what BoLI cannot capture.
5.2 SLIM
The ablation study for the SLIM system shows a
trade-off between precision and recall for some re-
sources. In this instance, adding resources improves
performance significantly, but including noisy re-
sources also implies a ceiling on overall perfor-
mance will ultimately be reached.
The following example shows the potentially
noisy possessive rewrite operator permitting suc-
cessful entailment:
id=19 Text: During Reinsdorf?s 24 seasons as chairman of
the White Sox, the team has captured Americal League divi-
sion championships three times, including an AL Central title
in 2000.
Transformed Text: During Reinsdorf have 24 seasons as chair-
man of the White Sox ...
Hypothesis: Reinsdorf was chairman of the White Sox for 24
seasons.
There are a number of examples where relaxed
operators result in false positives, but where the neg-
ative label is debatable. In the next example, the ap-
position module adds a new relation and the Nomi-
nalization Rewriter detects the hypothesis using this
new relation:
id=102 Hypothesis: He was initially successful, negotiating
a 3/4 of 1 percent royalty on all cars sold by the Association of
Licensed Automobile Manufacturers, the ALAM.
Transformed Text: ... Association of Licensed Automobile
Manufacturers is the ALAM.
Hypothesis: The ALAM manufactured cars.
Finally, some modules did not fire as they should;
for example 15, the conjunction module did not ex-
pand the conjunction over predicates. For example
24, the nominalization rewriter did not detect ?plays
in the NHL? from ?is a NHL player?. In example 35,
the apposition module did not detect that ?Harriet
111
Table 3: Results for SLIM and BoLI on the Pascal Development and Test Corpora. Results marked with an
asterisk indicate not all system resources were available at the time the system was run.
Corpus Development Test
Subtask IE IR QA SUM OVERALL IE IR QA SUM OVERALL
BoLI 0.560 0.700 0.790 0.690 0.675 0.510 0.710 0.830 0.575 0.656
SLIM 0.580* 0.595* 0.650* 0.545* 0.593* 0.485 0.6150 0.715 0.575 0.5975
Lane, niece of President James? could be rewritten.
Of course, there are also many examples where
the SLIM system simply does not have appropriate
resources (e.g. numerical reasoning, coreference re-
quiring semantic categorization).
6 Conclusion
While BoLI outperforms SLIM on the PASCAL
RTE 3 task, there is no clear way to improve BoLI.
It is clear that for the PASCAL corpora, the distribu-
tions over word similarity between entailment pair
members in positive and negative examples are dif-
ferent, allowing this simple approach to perform rel-
atively well, but there is no guarantee that this is gen-
erally the case, and it is easy to create an adversar-
ial corpus on which BoLI performs very badly (e.g.,
exchanging arguments or predicates of different cre-
lations in the Text), no matter how good the word-
level entailment resources are. This approach also
offers no possibility of a meaningful explanation of
the entailment decision.
SLIM, on the other hand, by offering a framework
to which new resources can be added in a principled
way, can be extended to cover new entailment phe-
nomena in an incremental, local (i.e. compositional)
way. The results of the limited ablation study sup-
port this conclusion, though the poor performance
on the IE task indicates the problems with using
lower-precision, higher-recall resources.
Overall, we find the results for the SLIM system
very encouraging, as they support the underlying
concept of incremental improvement, and this offers
a clear path toward better performance.
6.1 Acknowledgements
We gratefully acknowledge the work on SLIM
modules by Ming-Wei Chang, Michael Connor,
Quang Do, Alex Klementiev, Lev Ratinov, and
Vivek Srikumar. This work was funded by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) program, a grant from Boeing,
and a gift from Google.
References
Johan Bos and Katja Markert. 2005. When logical infer-
ence helps determining textual entailment (and when it
doesn?t). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. Knowledge representation for seman-
tic entailment and question-answering. In IJCAI-05
Workshop on Knowledge and Reasoning for Question
Answering.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher
Manning. 2005. Learning to distinguish valid textual
entailments. In Proceedings of the Second PASCAL
Challenges Workshop on Recognizing Textual Entail-
ment.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Sophia Katrenko and Peter Adriaans. 2005. Using
maximal embedded syntactic subtrees for textual en-
tailment recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognizing Tex-
tual Entailment.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. of the International Conference on
Machine Learning (ICML).
Marta Tatu, Brandon Iles, John Slavick, Adrian Novis-
chi, and Dan Moldovan. 2005. Cogex at the second
recognizing textual entailment challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognizing Textual Entailment.
112
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 21?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Textual Entailment 
Mark Sammons, University of Illinois
Idan Szpektor, Yahoo!
V.G. Vinod Vydiswaran, University of Illinois
The NLP and ML communities are rising to grander, larger-scale
challenges such as Machine Reading, Learning by Reading, and Learning
to Read, challenges requiring deeper and more integrated natural
language understanding capabilities.
The task of Recognizing Textual Entailment (RTE) requires automated
systems to identify when two spans of text share a common meaning --
for example, that ``Alphaville Inc.'s attempted acquisition of Bauhaus
led to a jump in both companies' stock prices'' entails ``Bauahaus'
stock rose'', but not ``Alphaville acquired Bauhaus''.  This general
capability would be a solid proxy for Natural Language Understanding,
and has direct relevance to the grand challenges named above.
Moreover, it could be used to improve performance in a large range of
Natural Language Processing tasks such as Information Extraction,
Question Answering, Exhaustive Search, Machine Translation and many
others.  The operational definition of Textual Entailment used by
researchers in the field avoids commitment to any specific knowledge
representation, inference method, or learning approach, thus
encouraging application of a wide range of techniques to the problem.
Techniques developed for RTE have now been successfully applied in the
domains of Question Answering, Relation Extraction, and Machine
translation, and RTE systems continue to improve their performance
even as the corpora on which they are evaluated (provided first by
PASCAL, and now by NIST TAC) have become progressively more
challenging.  Over the sequence of RTE challenges from PASCAL and NIST
TAC, the more successful systems seem to have converged in their
overall approach.
The goal of this tutorial is to introduce the task of Recognizing
Textual Entailment to researchers from other areas of NLP.  We will
identify and analyze common inference and learning approaches from a
range of the more successful RTE systems, and investigate the role of
knowledge resources.  We will examine successful applications of RTE
techniques to Question Answering and Machine Translation, and identify
key research challenges that must be overcome to continue improving
RTE systems.
21
Tutorial Outline
1. Introduction (35 minutes)
Define and motivate the Recognizing Textual Entailment (RTE)
task. Introduce the RTE evaluation framework. Define the relationship
between RTE and other major NLP tasks.  Identify (some of) the
semantic challenges inherent in the RTE task, including the
introduction of 'contradiction' as an entailment category.  Describe
the use of RTE components/techniques in Question Answering, Machine
Translation, and Relation Extraction.
2. The State of the Art (35 minutes)
Outline the basic structure underlying RTE systems.  With reference to
recent publications on RTE: cover the range of preprocessing/analysis
that may be used; define representations/data structures typically
used; outline inference procedures and machine learning techniques.
Identify challenging aspects of the RTE problem in the context
of system successes and failures. 
3. Machine Learning for Recognizing Textual Entailment (35 minutes)
Describe the challenges involved in applying machine learning techniques
to the Textual Entailment problem.  Describe in more detail the main
approaches to inference, which explicitly or implicitly use the concept
of alignment.  Show how alignment fits into assumptions of semantic
compositionality, how it facilitates machine learning approaches, and
how it can accommodate phenomena-specific resources.  Show how it
can be used for contradiction detection.  
4. Knowledge Acquisition and Application in Textual Entailment (35 minutes)
Establish the role of knowledge resources in Textual Entailment,
and the consequent importance of Knowledge Acquisition.
Identify knowledge resources currently used in RTE systems, and their
limitations.  Describe existing knowledge acquisition approaches,
emphasizing the need for learning directional semantic relations.
Define suitable representations and algorithms for using knowledge,
including context-sensitive knowledge application.  Discuss the
problem of noisy data, and the prospects for new knowledge
resources/new acquisition approaches.
22
5. Key Challenges for Recognizing Textual Entailment (15 minutes)
Identify the key challenges in improving textual entailment systems:
more reliable inputs (when is a solved problem not solved), domain
adaptation, missing knowledge, scaling up.  The need for a common
entailment infrastructure to promote resource sharing and development.
Biographical Information of the Presenters
Mark Sammons
University of Illinois
201 N. Goodwin Ave.
Urbana, IL 61801 USA
Phone: 1-217-265-6759
Email: mssammon@illinois.edu
Mark Sammons is a Principal Research Scientist working with the Cognitive 
Computation Group at the University of Illinois.  His primary interests are in Natural 
Language Processing and Machine Learning, with a focus on integrating diverse 
information sources in the context of Textual Entailment. His work has focused on 
developing a Textual Entailment framework that can easily incorporate new resources; 
designing appropriate inference procedures for recognizing entailment; and identifying 
and developing automated approaches to recognize and represent implicit content in 
natural language text. Mark received his MSC in Computer Science from the University 
of Illinois in 2004, and his PhD in Mechanical Engineering from the University of Leeds, 
England, in 2000. 
Idan Szpektor
Yahoo! Research, Building 30 Matam Park, Haifa 31905, ISRAEL.
Phone: + 972-74-7924666; Email: idan@yahoo-inc.com 
Idan Szpektor is a Research Scientist at Yahoo! Research. His primary research 
interests are  in natural language processing, machine learning and information 
retrieval. Idan recently submitted his PhD thesis at Bar-Ilan University where he worked 
on unsupervised acquisition and application of broad-coverage knowledge-bases for 
Textual Entailment. He has been a main organizer of the second PASCAL Recognizing 
Textual Entailment Challenge and an advisor for the third RTE Challenge. He served on 
the program committees of EMNLP and TextInfer and reviewed papers for ACL, 
COLING and EMNLP. Idan Szpektor received his M.Sc. from Tel-Aviv University in 
2005, where he worked on unsupervised knowledge acquisition for Textual Entailment. 
V.G.Vinod Vydiswaran
University of Illinois
201 N. Goodwin Ave.
Urbana, IL 61801 USA
23
Phone: 1-217-333-2584
Email: vgvinodv@illinois.edu
V.G.Vinod Vydiswaran is a 3rd year Ph.D. student in the Department of Computer 
Science at the University of Illinois at Urbana-Champaign. His research interests include 
text informatics, natural language processing, machine learning, and information 
extraction. His work has included developing a Textual Entailment system, and applying 
Textual Entailment to relation extraction and information retrieval. He received his 
Masters degree from Indian Institute of Technology Bombay, India in 2004, where he 
worked on Conditional models for Information Extraction. Later, he worked at Yahoo! 
Research & Development Center at Bangalore, India, on scaling Information Extraction 
technologies over the Web.
24
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1199?1208,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
?Ask not what Textual Entailment can do for You...?
Mark Sammons V.G.Vinod Vydiswaran Dan Roth
University of Illinois at Urbana-Champaign
{mssammon|vgvinodv|danr}@illinois.edu
Abstract
We challenge the NLP community to par-
ticipate in a large-scale, distributed effort
to design and build resources for devel-
oping and evaluating solutions to new and
existing NLP tasks in the context of Rec-
ognizing Textual Entailment. We argue
that the single global label with which
RTE examples are annotated is insufficient
to effectively evaluate RTE system perfor-
mance; to promote research on smaller, re-
lated NLP tasks, we believe more detailed
annotation and evaluation are needed, and
that this effort will benefit not just RTE
researchers, but the NLP community as
a whole. We use insights from success-
ful RTE systems to propose a model for
identifying and annotating textual infer-
ence phenomena in textual entailment ex-
amples, and we present the results of a pi-
lot annotation study that show this model
is feasible and the results immediately use-
ful.
1 Introduction
Much of the work in the field of Natural Lan-
guage Processing is founded on an assumption
of semantic compositionality: that there are iden-
tifiable, separable components of an unspecified
inference process that will develop as research
in NLP progresses. Tasks such as Named En-
tity and coreference resolution, syntactic and shal-
low semantic parsing, and information and rela-
tion extraction have been identified as worthwhile
tasks and pursued by numerous researchers. While
many have (nearly) immediate application to real
world tasks like search, many are also motivated
by their potential contribution to more ambitious
Natural Language tasks. It is clear that the compo-
nents/tasks identified so far do not suffice in them-
selves to solve tasks requiring more complex rea-
soning and synthesis of information; many other
tasks must be solved to achieve human-like perfor-
mance on tasks such as Question Answering. But
there is no clear process for identifying potential
tasks (other than consensus by a sufficient num-
ber of researchers), nor for quantifying their po-
tential contribution to existing NLP tasks, let alne
to Natural Language Understanding.
Recent ?grand challenges? such as Learning by
Reading, Learning To Read, andMachine Reading
are prompting more careful thought about the way
these tasks relate, and what tasks must be solved
in order to understand text sufficiently well to re-
liably reason with it. This is an appropriate time
to consider a systematic process for identifying
semantic analysis tasks relevant to natural lan-
guage understanding, and for assessing their
potential impact on NLU system performance.
Research on Recognizing Textual Entailment
(RTE), largely motivated by a ?grand challenge?
now in its sixth year, has already begun to address
some of the problems identified above. Tech-
niques developed for RTE have now been suc-
cessfully applied in the domains of Question An-
swering (Harabagiu and Hickl, 2006) and Ma-
chine Translation (Pado et al, 2009), (Mirkin
et al, 2009). The RTE challenge examples are
drawn from multiple domains, providing a rel-
atively task-neutral setting in which to evaluate
contributions of different component solutions,
and RTE researchers have already made incremen-
tal progress by identifying sub-problems of entail-
ment, and developing ad-hoc solutions for them.
In this paper we challenge the NLP community
to contribute to a joint, long-term effort to iden-
tify, formalize, and solve textual inference prob-
lems motivated by the Recognizing Textual Entail-
ment setting, in the following ways:
(a) Making the Recognizing Textual Entailment
setting a central component of evaluation for
1199
relevant NLP tasks such as NER, Coreference,
parsing, data acquisition and application, and oth-
ers. While many ?component? tasks are consid-
ered (almost) solved in terms of expected improve-
ments in performance on task-specific corpora, it
is not clear that this translates to strong perfor-
mance in the RTE domain, due either to prob-
lems arising from unrelated, unsolved entailment
phenomena that co-occur in the same examples,
or to domain change effects. The RTE task of-
fers an application-driven setting for evaluating a
broad range of NLP solutions, and will reinforce
good practices by NLP researchers. The RTE
task has been designed specifically to exercise tex-
tual inference capabilities, in a format that would
make RTE systems potentially useful components
in other ?deep? NLP tasks such as Question An-
swering and Machine Translation. 1
(b) Identifying relevant linguistic phenomena,
interactions between phenomena, and their
likely impact on RTE/textual inference. Deter-
mining the correct label for a single textual en-
tailment example requires human analysts to make
many smaller, localized decisions which may de-
pend on each other. A broad, carefully conducted
effort to identify and annotate such local phenom-
ena in RTE corpora would allow their distributions
in RTE examples to be quantified, and allow eval-
uation of NLP solutions in the context of RTE. It
would also allow assessment of the potential im-
pact of a solution to a specific sub-problem on the
RTE task, and of interactions between phenomena.
Such phenomena will almost certainly correspond
to elements of linguistic theory; but this approach
brings a data-driven approach to focus attention on
those phenomena that are well-represented in the
RTE corpora, and which can be identified with suf-
ficiently close agreement.
(c) Developing resources and approaches that
allow more detailed assessment of RTE sys-
tems. At present, it is hard to know what spe-
cific capabilities different RTE systems have, and
hence, which aspects of successful systems are
worth emulating or reusing. An evaluation frame-
work that could offer insights into the kinds of
sub-problems a given system can reliably solve
would make it easier to identify significant ad-
vances, and thereby promote more rapid advances
1The Parser Training and Evaluation using Textual En-
tailment track of SemEval 2 takes this idea one step further,
by evaluating performance of an isolated NLP task using the
RTE methodology.
through reuse of successful solutions and focus on
unresolved problems.
In this paper we demonstrate that Textual En-
tailment systems are already ?interesting?, in that
they have made significant progress beyond a
?smart? lexical baseline that is surprisingly hard
to beat (section 2). We argue that Textual Entail-
ment, as an application that clearly requires so-
phisticated textual inference to perform well, re-
quires the solution of a range of sub-problems,
some familiar and some not yet known. We there-
fore propose RTE as a promising and worthwhile
task for large-scale community involvement, as it
motivates the study of many other NLP problems
in the context of general textual inference.
We outline the limitations of the present model
of evaluation of RTE performance, and identify
kinds of evaluation that would promote under-
standing of the way individual components can
impact Textual Entailment system performance,
and allow better objective evaluation of RTE sys-
tem behavior without imposing additional burdens
on RTE participants. We use this to motivate a
large-scale annotation effort to provide data with
the mark-up sufficient to support these goals.
To stimulate discussion of suitable annotation
and evaluation models, we propose a candidate
model, and provide results from a pilot annota-
tion effort (section 3). This pilot study establishes
the feasibility of an inference-motivated annota-
tion effort, and its results offer a quantitative in-
sight into the difficulty of the TE task, and the dis-
tribution of a number of entailment-relevant lin-
guistic phenomena over a representative sample
from the NIST TAC RTE 5 challenge corpus. We
argue that such an evaluation and annotation ef-
fort can identify relevant subproblems whose so-
lution will benefit not only Textual Entailment but
a range of other long-standing NLP tasks, and can
stimulate development of new ones. We also show
how this data can be used to investigate the behav-
ior of some of the highest-scoring RTE systems
from the most recent challenge (section 4).
2 NLP Insights from Textual Entailment
The task of Recognizing Textual Entailment
(RTE), as formulated by (Dagan et al, 2006), re-
quires automated systems to identify when a hu-
man reader would judge that given one span of text
(the Text) and some unspecified (but restricted)
world knowledge, a second span of text (the Hy-
1200
Text: The purchase of LexCorp by BMI for $2Bn
prompted widespread sell-offs by traders as they
sought to minimize exposure.
Hyp 1: BMI acquired another company.
Hyp 2: BMI bought LexCorp for $3.4Bn.
Figure 1: Some representative RTE examples.
pothesis) is true. The task was extended in (Gi-
ampiccolo et al, 2007) to include the additional
requirement that systems identify when the Hy-
pothesis contradicts the Text. In the example
shown in figure 1, this means recognizing that the
Text entails Hypothesis 1, while Hypothesis 2 con-
tradicts the Text. This operational definition of
Textual Entailment avoids commitment to any spe-
cific knowledge representation, inference method,
or learning approach, thus encouraging applica-
tion of a wide range of techniques to the problem.
2.1 An Illustrative Example
The simple RTE examples in figure 1 (most RTE
examples have much longer Texts) illustrate some
typical inference capabilities demonstrated by hu-
man readers in determining whether one span of
text contains the meaning of another.
To recognize that Hypothesis 1 is entailed by the
text, a human reader must recognize that ?another
company? in the Hypothesis can match ?Lex-
Corp?. She must also identify the nominalized
relation ?purchase?, and determine that ?A pur-
chased by B? implies ?B acquires A?.
To recognize that Hypothesis 2 contradicts the
Text, similar steps are required, together with the
inference that because the stated purchase price is
different in the Text and Hypothesis, but with high
probability refers to the same transaction, Hypoth-
esis 2 contradicts the Text.
It could be argued that this particular example
might be resolved by simple lexical matching; but
it should be evident that the Text can be made
lexically very dissimilar to Hypothesis 1 while
maintaining the Entailment relation, and that con-
versely, the lexical overlap between the Text and
Hypothesis 2 can be made very high, while main-
taining the Contradiction relation. This intuition
is borne out by the results of the RTE challenges,
which show that lexical similarity-based systems
are outperformed by systems that use other, more
structured analysis, as shown in the next section.
Rank System id Accuracy
1 I 0.735
2 E 0.685
3 H 0.670
4 J 0.667
5 G 0.662
6 B 0.638
7 D 0.633
8 F 0.632
9 A 0.615
9 C 0.615
9 K 0.615
- Lex 0.612
Table 1: Top performing systems in the RTE 5 2-
way task.
Lex E G H I J
Lex 1.000 0.667 0.693 0.678 0.660 0.778
(184,183) (157,132) (168,122) (152,136) (165,137) (165,135)
E 1.000 0.667 0.675 0.673 0.702
(224,187) (192,112) (178,131) (201,127) (186,131)
G 1.000 0.688 0.713 0.745
(247,150) (186,120) (218,115) (198,125)
H 1.000 0.705 0.707
(219,183) (194,139) (178,136)
I 1.000 0.705
(260,181) (198,135)
J 1.000
(224,178)
Table 2: In each cell, top row shows observed
agreement and bottom row shows the number of
correct (positive, negative) examples on which the
pair of systems agree.
2.2 The State of the Art in RTE 5
The outputs for all systems that participated in the
RTE 5 challenge were made available to partici-
pants. We compared these to each other and to
a smart lexical baseline (Do et al, 2010) (lexical
match augmented with a WordNet similarity mea-
sure, stemming, and a large set of low-semantic-
content stopwords) to assess the diversity of the
approaches of different research groups. To get
the fullest range of participants, we used results
from the two-way RTE task. We have anonymized
the system names.
Table 1 shows that many participating systems
significantly outperform our smart lexical base-
line. Table 2 reports the observed agreement be-
tween systems and the lexical baseline in terms of
the percentage of examples on which a pair of sys-
tems gave the same label. The agreement between
most systems and the baseline is about 67%, which
suggests that systems are not simply augmented
versions of the lexical baseline, and are also dis-
tinct from each other in their behaviors.2
Common characteristics of RTE systems re-
2Note that the expected agreement between two random
RTE decision-makers is 0.5, so the agreement scores accord-
ing to Cohen?s Kappa measure (Cohen, 1960) are between
0.3 and 0.4.
1201
ported by their designers were the use of struc-
tured representations of shallow semantic content
(such as augmented dependency parse trees and
semantic role labels); the application of NLP re-
sources such as Named Entity recognizers, syn-
tactic and dependency parsers, and coreference
resolvers; and the use of special-purpose ad-hoc
modules designed to address specific entailment
phenomena the researchers had identified, such as
the need for numeric reasoning. However, it is
not possible to objectively assess the role these ca-
pabilities play in each system?s performance from
the system outputs alone.
2.3 The Need for Detailed Evaluation
An ablation study that formed part of the of-
ficial RTE 5 evaluation attempted to evaluate
the contribution of publicly available knowledge
resources such as WordNet (Fellbaum, 1998),
VerbOcean (Chklovski and Pantel, 2004), and
DIRT (Lin and Pantel, 2001) used by many of
the systems. The observed contribution was in
most cases limited or non-existent. It is premature,
however, to conclude that these resources have lit-
tle potential impact on RTE system performance:
most RTE researchers agree that the real contribu-
tion of individual resources is difficult to assess.
As the example in figure 1 illustrates, most RTE
examples require a number of phenomena to be
correctly resolved in order to reliably determine
the correct label (the Interaction problem); a per-
fect coreference resolver might as a result yield lit-
tle improvement on the standard RTE evaluation,
even though coreference resolution is clearly re-
quired by human readers in a significant percent-
age of RTE examples.
Various efforts have been made by individ-
ual research teams to address specific capabili-
ties that are intuitively required for good RTE
performance, such as (de Marneffe et al, 2008),
and the formal treatment of entailment phenomena
in (MacCartney and Manning, 2009) depends on
and formalizes a divide-and-conquer approach to
entailment resolution. But the phenomena-specific
capabilities described in these approaches are far
from complete, and many are not yet invented. To
devote real effort to identify and develop such ca-
pabilities, researchers must be confident that the
resources (and the will!) exist to create and eval-
uate their solutions, and that the resource can be
shown to be relevant to a sufficiently large subset
of the NLP community. While there is widespread
belief that there are many relevant entailment phe-
nomena, though each individually may be rele-
vant to relatively few RTE examples (the Sparse-
ness problem), we know of no systematic analysis
to determine what those phenomena are, and how
sparsely represented they are in existing RTE data.
If it were even known what phenomena were
relevant to specific entailment examples, it might
be possible to more accurately distinguish system
capabilities, and promote adoption of successful
solutions to sub-problems. An annotation-side
solution also maintains the desirable agnosticism
of the RTE problem formulation, by not imposing
the requirement on system developers of generat-
ing an explanation for each answer. Of course, if
examples were also annotated with explanations
in a consistent format, this could form the basis of
a new evaluation of the kind essayed in the pilot
study in (Giampiccolo et al, 2007).
3 Annotation Proposal and Pilot Study
As part of our challenge to the NLP commu-
nity, we propose a distributed OntoNotes-style ap-
proach (Hovy et al, 2006) to this annotation ef-
fort: distributed, because it should be undertaken
by a diverse range of researchers with interests
in different semantic phenomena; and similar to
the OntoNotes annotation effort because it should
not presuppose a fixed, closed ontology of entail-
ment phenomena, but rather, iteratively hypoth-
esize and refine such an ontology using inter-
annotator agreement as a guiding principle. Such
an effort would require a steady output of RTE ex-
amples to form the underpinning of these annota-
tions; and in order to get sufficient data to repre-
sent less common, but nonetheless important, phe-
nomena, a large body of data is ultimately needed.
A research team interested in annotating a new
phenomenon should use examples drawn from the
common corpus. Aside from any task-specific
gold standard annotation they add to the entail-
ment pairs, they should augment existing explana-
tions by indicating in which examples their phe-
nomenon occurs, and at which point in the exist-
ing explanation for each example. In fact, this
latter effort ? identifying phenomena relevant to
textual inference, marking relevant RTE examples,
and generating explanations ? itself enables other
researchers to select from known problems, assess
their likely impact, and automatically generate rel-
1202
evant corpora.
To assess the feasibility of annotating RTE-
oriented local entailment phenomena, we devel-
oped an inference model that could be followed by
annotators, and conducted a pilot annotation study.
We based our initial effort on observations about
RTE data we made while participating in RTE
challenges, together with intuitive conceptions of
the kinds of knowledge that might be available in
semi-structured or structured form. In this sec-
tion, we present our annotation inference model,
and the results of our pilot annotation effort.
3.1 Inference Process
To identify and annotate RTE sub-phenomena in
RTE examples, we need a defensible model for the
entailment process that will lead to consistent an-
notation by different researchers, and to an exten-
sible framework that can accommodate new phe-
nomena as they are identified.
We modeled the entailment process as one of
manipulating the text and hypothesis to be as sim-
ilar as possible, by first identifying parts of the
text that matched parts of the hypothesis, and then
identifying connecting structure. Our inherent as-
sumption was that the meanings of the Text and
Hypothesis could be represented as sets of n-ary
relations, where relations could be connected to
other relations (i.e., could take other relations as
arguments). As we followed this procedure for a
given example, we marked which entailment phe-
nomena were required for the inference. We illus-
trate the process using the example in figure 1.
First, we would identify the arguments ?BMI?
and ?another company? in the Hypothesis as
matching ?BMI? and ?LexCorp? respectively, re-
quiring 1) Parent-Sibling to recognize that ?Lex-
Corp? can match ?company?. We would tag the
example as requiring 2) Nominalization Resolu-
tion to make ?purchase? the active relation and
3) Passivization to move ?BMI? to the subject po-
sition. We would then tag it with 4) Simple Verb
Rule to map ?A purchase B? to ?A acquire B?.
These operations make the relevant portion of the
Text identical to the Hypothesis, so we are done.
For the same Text, but with Hypothesis 2 (a neg-
ative example), we follow the same steps 1-3. We
would then use 4) Lexical Relation to map ?pur-
chase? to ?buy?. We would then observe that the
only possible match for the hypothesis argument
?for $3.4Bn? is the text argument ?for $2Bn?. We
would label this as a 5) Numerical Quantity Mis-
match and 6) Excluding Argument (it can?t be the
case that in the same transaction, the same com-
pany was sold for two different prices).
Note that neither explanation mentions
the anaphora resolution connecting ?they? to
?traders?, because it is not strictly required to
determine the entailment label.
As our example illustrates, this process makes
sense for both positive and negative examples. It
also reflects common approaches in RTE systems,
many of which have explicit alignment compo-
nents that map parts of the Hypothesis to parts of
the Text prior to a final decision stage.
3.2 Annotation Labels
We sought to identify roles for background knowl-
edge in terms of domains and general inference
steps, and the types of linguistic phenomena that
are involved in representing the same information
in different ways, or in detecting key differences
in two similar spans of text that indicate a differ-
ence in meaning. We annotated examples with do-
mains (such as ?Work?) for two reasons: to estab-
lish whether some phenomena are correlated with
particular domains; and to identify domains that
are sufficiently well-represented that a knowledge
engineering study might be possible.
While we did not generate an explicit repre-
sentation of our entailment process, i.e. explana-
tions, we tracked which phenomena were strictly
required for inference. The annotated corpora and
simple CGI scripts for annotation are available at
http://cogcomp.cs.illinois.edu/Data/ACL2010 RTE.php.
The phenomena that we considered during an-
notation are presented in Tables 3, 4, 5, and 6. We
tried to define each phenomenon so that it would
apply to both positive and negative examples, but
ran into a problem: often, negative examples can
be identified principally by structural differences:
the components of the Hypothesis all match com-
ponents in the Text, but they are not connected
by the appropriate structure in the Text. In the
case of contradictions, it is often the case that a
key relation in the Hypothesis must be matched to
an incompatible relation in the Text. We selected
names for these structural behaviors, and tagged
them when we observed them, but the counterpart
for positive examples must always hold: it must
necessarily be the case that the structure in the
Text linking the arguments that match those in the
1203
Hypothesis must be comparable to the Hypothesis
structure. We therefore did not tag this for positive
examples.
We selected a subset of 210 examples from the
NIST TAC RTE 5 (Bentivogli et al, 2009) Test
set drawn equally from the three sub-tasks (IE, IR
and QA). Each example was tagged by both an-
notators. Two passes were made over the data: the
first covered 50 examples from each RTE sub-task,
while the second covered an additional 20 exam-
ples from each sub-task. Between the two passes,
concepts the annotators identified as difficult to
annotate were discussed and more carefully spec-
ified, and several new concepts were introduced
based on annotator observations.
Tables 3, 4, 5, and 6 present information
about the distribution of the phenomena we
tagged, and the inter-annotator agreement (Co-
hen?s Kappa (Cohen, 1960)) for each. ?Occur-
rence? lists the average percentage of examples la-
beled with a phenomenon by the two annotators.
Domain Occurrence Agreement
work 16.90% 0.918
name 12.38% 0.833
die kill injure 12.14% 0.979
group 9.52% 0.794
be in 8.57% 0.888
kinship 7.14% 1.000
create 6.19% 1.000
cause 6.19% 0.854
come from 5.48% 0.879
win compete 3.10% 0.813
Others 29.52% 0.864
Table 3: Occurrence statistics for domains in the
annotated data.
Phenomenon Occurrence Agreement
Named Entity 91.67% 0.856
locative 17.62% 0.623
Numerical Quantity 14.05% 0.905
temporal 5.48% 0.960
nominalization 4.05% 0.245
implicit relation 1.90% 0.651
Table 4: Occurrence statistics for hypothesis struc-
ture features.
From the tables it is apparent that good perfor-
mance on a range of phenomena in our inference
model are likely to have a significant effect on
RTE results, with coreference being deemed es-
sential to the inference process for 35% of exam-
ples, and a number of other phenomena are suffi-
ciently well represented to merit near-future atten-
tion (assuming that RTE systems do not already
handle these phenomena, a question we address in
section 4). It is also clear from the predominance
of Simple Rewrite Rule instances, together with
Phenomenon Occurrence Agreement
coreference 35.00% 0.698
simple rewrite rule 32.62% 0.580
lexical relation 25.00% 0.738
implicit relation 23.33% 0.633
factoid 15.00% 0.412
parent-sibling 11.67% 0.500
genetive relation 9.29% 0.608
nominalization 8.33% 0.514
event chain 6.67% 0.589
coerced relation 6.43% 0.540
passive-active 5.24% 0.583
numeric reasoning 4.05% 0.847
spatial reasoning 3.57% 0.720
Table 5: Occurrence statistics for entailment phe-
nomena and knowledge resources
Phenomenon Occurrence Agreement
missing argument 16.19% 0.763
missing relation 14.76% 0.708
excluding argument 10.48% 0.952
Named Entity mismatch 9.29% 0.921
excluding relation 5.00% 0.870
disconnected relation 4.52% 0.580
missing modifier 3.81% 0.465
disconnected argument 3.33% 0.764
Numeric Quant. mismatch 3.33% 0.882
Table 6: Occurrences of negative-only phenomena
the frequency of most of the domains we selected,
that knowledge engineering efforts also have a key
role in improving RTE performance.
3.3 Discussion
Perhaps surprisingly, given the difficulty of the
task, inter-annotator agreement was consistently
good to excellent (above 0.6 and 0.8, respec-
tively), with few exceptions, indicating that for
most targeted phenomena, the concepts were well-
specified. The results confirmed our initial intu-
ition about some phenomena: for example, that
coreference resolution is central to RTE, and that
detecting the connecting structure is crucial in dis-
cerning negative from positive examples. We also
found strong evidence that the difference between
contradiction and unknown entailment examples
is often due to the behavior of certain relations that
either preclude certain other relations holding be-
tween the same arguments (for example, winning
a contest vs. losing a contest), or which can only
hold for a single referent in one argument position
(for example, ?work? relations such as job title are
typically constrained so that a single person holds
one position).
We found that for some examples, there was
more than one way to infer the hypothesis from the
text. Typically, for positive examples this involved
overlap between phenomena; for example, Coref-
erence might be expected to resolve implicit rela-
1204
tions induced from appositive structures. In such
cases we annotated every way we could find.
In future efforts, annotators should record the
entailment steps they used to reach their decision.
This will make disagreement resolution simpler,
and could also form a possible basis for generating
gold standard explanations. At a minimum, each
inference step must identify the spans of the Text
and Hypothesis that are involved and the name of
the entailment phenomenon represented; in addi-
tion, a partial order over steps must be specified
when one inference step requires that another has
been completed.
Future annotation efforts should also add a
category ?Other?, to indicate for each example
whether the annotator considers the listed entail-
ment phenomena sufficient to identify the label. It
might also be useful to assess the difficulty of each
example based on the time required by the anno-
tator to determine an explanation, for comparison
with RTE system errors.
These, together with specifications that mini-
mize the likely disagreements between different
groups of annotators, are processes that must be
refined as part of the broad community effort we
seek to stimulate.
4 Pilot RTE System Analysis
In this section, we sketch out ways in which
the proposed analysis can be applied to learn
something about RTE system behavior, even
when those systems do not provide anything
beyond the output label. We present the analysis
in terms of sample questions we hope to answer
with such an analysis.
1. If a system needs to improve its performance,
which features should it concentrate on? To an-
swer this question, we looked at the top-5 systems
and tried to find which phenomena are active in
the mistakes they make.
(a) Most systems seem to fail on examples that
need numeric reasoning to get the entailment de-
cision right. For example, system H got all 10 ex-
amples with numeric reasoning wrong.
(b) All top-5 systems make consistent errors in
cases where identifying a mismatch in named en-
tities (NE) or numerical quantities (NQ) is impor-
tant to make the right decision. System G got 69%
of cases with NE/NQ mismatches wrong.
(c) Most systems make errors in examples that
have a disconnected or exclusion component (ar-
gument/relation). System J got 81% of cases with
a disconnected component wrong.
(d) Some phenomena are handled well by certain
systems, but not by others. For example, failing
to recognize a parent-sibling relation between
entities/concepts seems to be one of the top-5
phenomena active in systems E and H. System
H also fails to correctly label over 53% of the
examples having kinship relation.
2. Which phenomena have strong correlations
to the entailment labels among hard examples?
We called an example hard if at least 4 of the top 5
systems got the example wrong. In our annotation
dataset, there were 41 hard examples. Some of
the phenomena that strongly correlate with the
TE labels on hard examples are: deeper lexical
relation between words (? = 0.542), and need
for external knowledge (? = 0.345). Further, we
find that the top-5 systems tend to make mistakes
in cases where the lexical approach also makes
mistakes (? = 0.355).
3. What more can be said about individual
systems? In order to better understand the system
behavior, we wanted to check if we could predict
the system behavior based on the phenomena
we identified as important in the examples.
We learned SVM classifiers over the identified
phenomena and the lexical similarity score to
predict both the labels and errors systems make
for each of the top-5 systems. We could predict all
10 system behaviors with over 70% accuracy, and
could predict labels and mistakes made by two of
the top-5 systems with over 77% accuracy. This
indicates that although the identified phenomena
are indicative of the system performance, it is
probably too simplistic to assume that system
behavior can be easily reproduced solely as a
disjunction of phenomena present in the examples.
4. Does identifying the phenomena correctly
help learn a better TE system? We tried to
learn an entailment classifier over the phenomenon
identified and the top 5 system outputs. The results
are summarized in Table 7. All reported num-
bers are 20-fold cross-validation accuracy from
an SVM classifier learned over the features men-
tioned. The results show that correctly identify-
ing the named-entity and numeric quantity mis-
1205
No. Feature description No. of Accuracy over which features
feats phenomena pheno. + sys. labels
(0) Only system labels 5 ? 0.714
(1) Domain and hypothesis features (Tables 3, 4) 16 0.510 0.705
(2) (1) + NE + NQ 18 0.619 0.762
(3) (1) + Knowledge resources (subset of Table 5) 22 0.662 0.762
(4) (3) + NE + NQ 24 0.738 0.805
(5) (1) + Entailment and Knowledge resources (Table 5) 29 0.748 0.791
(6) (5) + negative-only phenomena (Table 6) 38 0.971 0.943
Table 7: Accuracy in predicting the label based on the phenomena and top-5 system labels.
matches improves the overall accuracy signifi-
cantly. If we further recognize the need for knowl-
edge resources correctly, we can correctly explain
the label for 80% of the examples. Adding the
entailment and negation features helps us explain
the label for 97% of the examples in the annotated
corpus.
It must be clarified that the results do not show
the textual entailment problem itself is solved with
97% accuracy. However, we believe that if a
system could recognize key negation phenomena
such as Named Entity mismatch, presence of Ex-
cluding arguments, etc. correctly and consistently,
it could model them as a Contradiction features
in the final inference process to significantly im-
prove its overall accuracy. Similarly, identifying
and resolving the key entailment phenomena in
the examples, would boost the inference process
in positive examples. However, significant effort
is still required to obtain near-accurate knowledge
and linguistic resources.
5 Discussion
NLP researchers in the broader community contin-
ually seek new problems to solve, and pose more
ambitious tasks to develop NLP and NLU capabil-
ities, yet recognize that even solutions to problems
which are considered ?solved? may not perform as
well on domains different from the resources used
to train and develop them. Solutions to such NLP
tasks could benefit from evaluation and further de-
velopment on corpora drawn from a range of do-
mains, like those used in RTE evaluations.
It is also worthwhile to consider each task as
part of a larger inference process, and therefore
motivated not just by performance statistics on
special-purpose corpora, but as part of an inter-
connected web of resources; and the task of Rec-
ognizing Textual Entailment has been designed to
exercise a wide range of linguistic and reasoning
capabilities.
The entailment setting introduces a potentially
broader context to resource development and as-
sessment, as the hypothesis and text provide con-
text for each other in a way different than local
context from, say, the same paragraph in a docu-
ment: in RTE?s positive examples, the Hypothe-
sis either restates some part of the Text, or makes
statements inferable from the statements in the
Text. This is not generally true of neighboring sen-
tences in a document. This distinction opens the
door to ?purposeful?, or goal-directed, inference
in a way that may not be relevant to a task studied
in isolation.
The RTE community seems mainly convinced
that incremental advances in local entailment phe-
nomena (including application of world knowl-
edge) are needed to make significant progress.
They need ways to identify sub-problems of tex-
tual inference, and to evaluate those solutions both
in isolation and in the context of RTE. RTE system
developers are likely to reward well-engineered
solutions by adopting them and citing their au-
thors, because such solutions are easier to incor-
porate into RTE systems. They are also more
likely to adopt solutions with established perfor-
mance levels. These characteristics promote pub-
lication of software developed to solve NLP tasks,
attention to its usability, and publication of mate-
rials supporting reproduction of results presented
in technical papers.
For these reasons, we assert that RTE is a nat-
ural motivator of new NLP tasks, as researchers
look for components capable of improving perfor-
mance; and that RTE is a natural setting for evalu-
ating solutions to a broad range of NLP problems,
though not in its present formulation: we must
solve the problem of credit assignment, to recog-
nize component contributions. We have therefore
proposed a suitable annotation effort, to provide
the resources necessary for more detailed evalua-
tion of RTE systems.
We have presented a linguistically-motivated
1206
analysis of entailment data based on a step-wise
procedure to resolve entailment decisions, in-
tended to allow independent annotators to reach
consistent decisions, and conducted a pilot anno-
tation effort to assess the feasibility of such a task.
We do not claim that our set of domains or phe-
nomena are complete: for example, our illustra-
tive example could be tagged with a domain Merg-
ers and Acquisitions, and a different team of re-
searchers might consider Nominalization Resolu-
tion to be a subset of Simple Verb Rules. This kind
of disagreement in coverage is inevitable, but we
believe that in many cases it suffices to introduce
a new domain or phenomenon, and indicate its re-
lation (if any) to existing domains or phenomena.
In the case of introducing a non-overlapping cate-
gory, no additional information is needed. In other
cases, the annotators can simply indicate the phe-
nomena being merged or split (or even replaced).
This information will allow other researchers to
integrate different annotation sources and main-
tain a consistent set of annotations.
6 Conclusions
In this paper, we have presented a case for a broad,
long-term effort by the NLP community to coordi-
nate annotation efforts around RTE corpora, and to
evaluate solutions to NLP tasks relating to textual
inference in the context of RTE. We have iden-
tified limitations in the existing RTE evaluation
scheme, proposed a more detailed evaluation to
address these limitations, and sketched a process
for generating this annotation. We have proposed
an initial annotation scheme to prompt discussion,
and through a pilot study, demonstrated that such
annotation is both feasible and useful.
We ask that researchers not only contribute
task specific annotation to the general pool, and
indicate how their task relates to those already
added to the annotated RTE corpora, but also in-
vest the additional effort required to augment the
cross-domain annotation: marking the examples
in which their phenomenon occurs, and augment-
ing the annotator-generated explanations with the
relevant inference steps.
These efforts will allow a more meaningful
evaluation of RTE systems, and of the compo-
nent NLP technologies they depend on. We see
the potential for great synergy between different
NLP subfields, and believe that all parties stand to
gain from this collaborative effort. We therefore
respectfully suggest that you ?ask not what RTE
can do for you, but what you can do for RTE...?
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments and suggestions. This research was
partly sponsored by Air Force Research Labora-
tory (AFRL) under prime contract no. FA8750-
09-C-0181, by a grant from Boeing and by MIAS,
the Multimodal Information Access and Synthesis
center at UIUC, part of CCICADA, a DHS Center
of Excellence. Any opinions, findings, and con-
clusion or recommendations expressed in this ma-
terial are those of the author(s) and do not neces-
sarily reflect the view of the sponsors.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The
fifth pascal recognizing textual entailment chal-
lenge. In Notebook papers and Results, Text Analy-
sis Conference (TAC), pages 14?24.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04), pages 33?40.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047, Columbus, Ohio, June. Association for
Computational Linguistics.
Quang Do, Dan Roth, Mark Sammons, Yuancheng
Tu, and V.G.Vinod Vydiswaran. 2010. Robust,
Light-weight Approaches to compute Lexi-
cal Similarity. Computer Science Research
and Technical Reports, University of Illinois.
http://L2R.cs.uiuc.edu/?danr/Papers/DRSTV10.pdf.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 1?9, Prague, June. Association
for Computational Linguistics.
1207
Sanda Harabagiu and Andrew Hickl. 2006. Meth-
ods for Using Textual Entailment in Open-Domain
Question Answering. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 905?912, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT/NAACL,
New York.
D. Lin and P. Pantel. 2001. DIRT: discovery of in-
ference rules from text. In Proc. of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2001, pages 323?328.
Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In The Eighth
International Conference on Computational Seman-
tics (IWCS-8), Tilburg, Netherlands.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In ACL/AFNLP, pages 791?
799, Suntec, Singapore, August. Association for
Computational Linguistics.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 297?305, Suntec, Singapore,
August. Association for Computational Linguistics.
1208
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 40?44,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Inference Protocols for Coreference Resolution
Kai-Wei Chang Rajhans Samdani
Alla Rozovskaya Nick Rizzolo
Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|rizzolo|mssammon|danr}@illinois.edu
Abstract
This paper presents Illinois-Coref, a system
for coreference resolution that participated
in the CoNLL-2011 shared task. We in-
vestigate two inference methods, Best-Link
and All-Link, along with their corresponding,
pairwise and structured, learning protocols.
Within these, we provide a flexible architec-
ture for incorporating linguistically-motivated
constraints, several of which we developed
and integrated. We compare and evaluate the
inference approaches and the contribution of
constraints, analyze the mistakes of the sys-
tem, and discuss the challenges of resolving
coreference for the OntoNotes-4.0 data set.
1 Introduction
The coreference resolution task is challenging, re-
quiring a human or automated reader to identify
denotative phrases (?mentions?) and link them to
an underlying set of referents. Human readers use
syntactic and semantic cues to identify and dis-
ambiguate the referring phrases; a successful auto-
mated system must replicate this behavior by linking
mentions that refer to the same underlying entity.
This paper describes Illinois-Coref, a corefer-
ence resolution system built on Learning Based
Java (Rizzolo and Roth, 2010), that participated
in the ?closed? track of the CoNLL-2011 shared
task (Pradhan et al, 2011). Building on elements
of the coreference system described in Bengtson
and Roth (2008), we design an end-to-end system
(Sec. 2) that identifies candidate mentions and then
applies one of two inference protocols, Best-Link
and All-Link (Sec. 2.3), to disambiguate and clus-
ter them. These protocols were designed to easily
incorporate domain knowledge in the form of con-
straints. In Sec. 2.4, we describe the constraints that
we develop and incorporate into the system. The
different strategies for mention detection and infer-
ence, and the integration of constraints are evaluated
in Sections 3 and 4.
2 Architecture
Illinois-Coref follows the architecture used in
Bengtson and Roth (2008). First, candidate men-
tions are detected (Sec. 2.1). Next, a pairwise
classifier is applied to each pair of mentions, gen-
erating a score that indicates their compatibility
(Sec. 2.2). Next, at inference stage, a coreference
decoder (Sec. 2.3) aggregates these scores into men-
tion clusters. The original system uses the Best-Link
approach; we also experiment with All-Link decod-
ing. This flexible decoder architecture allows lin-
guistic or knowledge-based constraints to be easily
added to the system: constraints may force mentions
to be coreferent or non-coreferent and can be option-
ally used in either of the inference protocols. We
designed and implemented several such constraints
(Sec. 2.4). Finally, since mentions that are in single-
ton clusters are not annotated in the OntoNotes-4.0
data set, we remove those as a post-processing step.
2.1 Mention Detection
Given a document, a mention detector generates a
set of mention candidates that are used by the subse-
quent components of the system. A robust mention
detector is crucial, as detection errors will propagate
to the coreference stage. As we show in Sec. 3, the
system that uses gold mentions outperforms the sys-
tem that uses predicted mentions by a large margin,
from 15% to 18% absolute difference.
40
For the ACE 2004 coreference task, a good per-
formance in mention detection is typically achieved
by training a classifier e.g., (Bengtson and Roth,
2008). However, this model is not appropriate for
the OntoNotes-4.0 data set, in which (in contrast to
the ACE 2004 corpus) singleton mentions are not
annotated: a specific noun phrase (NP) may corre-
spond to a mention in one document but will not
be a mention in another document. Therefore, we
designed a high recall (? 90%) and low precision
(? 35%) rule-based mention detection system that
includes all phrases recognized as Named Entities
(NE?s) and all phrases tagged as NPs in the syntac-
tic parse of the text. As a post-processing step, we
remove all predicted mentions that remain in single-
ton clusters after the inference stage.
The best mention detection result on the DEV set1
is 64.93% in F1 score (after coreference resolution)
and is achieved by our best inference protocol, Best-
Link with constraints.
2.2 Pairwise Mention Scoring
The basic input to our inference algorithm is a pair-
wise mention score, which indicates the compatibil-
ity score of a pair of mentions. For any two mentions
u and v, the compatibility score wuv is produced
by a pairwise scoring component that uses extracted
features ?(u, v) and linguistic constraints c:
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where w is a weight vector learned from training
data, c(u, v) is a compatibility score given by the
constraints, and t is a threshold parameter (to be
tuned). We use the same features as Bengtson and
Roth (2008), with the knowledge extracted from the
OntoNotes-4.0 annotation. The exact use of the
scores and the procedure for learning weights w are
specific to the inference algorithm and are described
next.
2.3 Inference
In this section, we present our inference techniques
for coreference resolution. These clustering tech-
niques take as input a set of pairwise mention scores
over a document and aggregate them into globally
1In the shared task, the data set is split into three sets:
TRAIN, DEV, and TEST.
consistent cliques representing entities. We investi-
gate the traditional Best-Link approach and a more
intuitively appealing All-Link algorithm.
2.3.1 Best-Link
Best-Link is a popular approach to coreference
resolution. For each mention, it considers the best
mention on its left to connect to (best according
the pairwise score wuv) and creates a link between
them if the pairwise score is above some thresh-
old. Although its strategy is simple, Bengtson and
Roth (2008) show that with a careful design, it can
achieve highly competitive performance.
Inference: We give an integer linear programming
(ILP) formulation of Best-Link inference in order to
present both of our inference algorithms within the
same framework. Given a pairwise scorer w, we
can compute the compatibility scores ? wuv from
Eq. (1) ? for all mention pairs u and v. Let yuv be
a binary variable, such that yuv = 1 only if u and v
are in the same cluster. For a document d, Best-Link
solves the following ILP formulation:
argmaxy
?
u,v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components and
all the mentions in each connected component con-
stitute an entity.
Learning: We follow the strategy in (Bengtson
and Roth, 2008, Section 2.2) to learn the pairwise
scoring function w. The scoring function is trained
on:
? Positive examples: for each mention u, we con-
struct a positive example (u, v), where v is the
closest preceding mention in u?s equivalence
class.
? Negative examples: all mention pairs (u, v),
where v is a preceding mention of u and u, v
are not in the same class.
As a result of the singleton mentions not being anno-
tated, there is an inconsistency in the sample distri-
butions in the training and inference phases. There-
fore, we apply the mention detector to the training
set, and train the classifier using the union set of gold
and predicted mentions.
41
2.3.2 All-Link
The All-Link inference approach scores a cluster-
ing of mentions by including all possible pairwise
links in the score. It is also known as correlational
clustering (Bansal et al, 2002) and has been applied
to coreference resolution in the form of supervised
clustering (Mccallum and Wellner, 2003; Finley and
Joachims, 2005).
Inference: Similar to Best-Link, for a document d,
All-Link inference finds a clustering All-Link(d;w)
by solving the following ILP problem:
argmaxy
?
u,v
wuvyuv
s.t yuw ? yuv + yvw ? 1 ?u,w, v,
yuw ? {0, 1}.
(3)
The inequality constraints in Eq. (3) enforce the
transitive closure of the clustering. The solution of
Eq. (3) is a set of cliques, and the mentions in the
same cliques corefer.
Learning: We present a structured perceptron al-
gorithm, which is similar to supervised clustering
algorithm (Finley and Joachims, 2005) to learn w.
Note that as an approximation, it is certainly pos-
sible to use the weight parameter learned by using,
say, averaged perceptron over positive and negative
links. The pseudocode is presented in Algorithm 1.
Algorithm 1 Structured Perceptron like learning al-
gorithm for All-Link inference
Given: Annotated documents D and initial
weight winit
Initialize w ? winit
for Document d in D do
Clustering y ? All-Link(d;w)
for all pairs of mentions u and v do
I1(u, v) = [u, v coreferent in D]
I2(u, v) = [y(u) = y(v)]
w ? w +
(
I1(u, v)? I2(u, v)
)
?(u, v)
end for
end for
return w
For the All-Link clustering, we drop one of the
three transitivity constraints for each triple of men-
tion variables. Similar to Pascal and Baldridge
(2009), we observe that this improves accuracy ?
the reader is referred to Pascal and Baldridge (2009)
for more details.
2.4 Constraints
The constraints in our inference algorithm are based
on the analysis of mistakes on the DEV set2. Since
the majority of errors are mistakes in recall, where
the system fails to link mentions that refer to the
same entity, we define three high precision con-
straints that improve recall on NPs with definite de-
terminers and mentions whose heads are NE?s.
The patterns used by constraints to match mention
pairs have some overlap with those used by the pair-
wise mention scorer, but their formulation as con-
straints allow us to focus on a subset of mentions
to which a certain pattern applies with high preci-
sion. For example, the constraints use a rule-based
string similarity measure that accounts for the in-
ferred semantic type of the mentions compared. Ex-
amples of mention pairs that are correctly linked by
the constraints are: Governor Bush? Bush; a cru-
cial swing state , Florida? Florida; Sony itself ?
Sony; Farmers? Los Angeles - based Farmers.
3 Experiments and Results
In this section, we present the performance of the
system on the OntoNotes-4.0 data set. A previous
experiment using an earlier version of this data can
be found in (Pradhan et al, 2007). Table 1 shows the
performance for the two inference protocols, with
and without constraints. Best-Link outperforms All-
Link for both predicted and gold mentions. Adding
constraints improves the performance slightly for
Best-Link on predicted mentions. In the other con-
figurations, the constraints either do not affect the
performance or slightly degrade it.
Table 2 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on predicted mentions with predicted
boundaries, predicted mentions with gold bound-
aries, and when using gold mentions3.
2We provide a more detailed analysis of the errors in Sec. 4.
3Note that the gold boundaries results are different from the
gold mention results. Specifying gold mentions requires coref-
erence resolution to exclude singleton mentions. Gold bound-
aries are provided by the task organizers and also include sin-
gleton mentions.
42
Method
Pred. Mentions w/Pred. Boundaries Gold Mentions
MD MUC BCUB CEAF AVG MUC BCUB CEAF AVG
Best-Link 64.70 55.67 69.21 43.78 56.22 80.58 75.68 64.69 73.65
Best-Link W/ Const. 64.69 55.8 69.29 43.96 56.35 80.56 75.02 64.24 73.27
All-Link 63.30 54.56 68.50 42.15 55.07 77.72 73.65 59.17 70.18
All-Link W/ Const. 63.39 54.56 68.46 42.20 55.07 77.94 73.43 59.47 70.28
Table 1: The performance of the two inference protocols on both gold and predicted mentions. The systems are
trained on the TRAIN set and evaluated on the DEV set. We report the F1 scores (%) on mention detection (MD)
and coreference metrics (MUC, BCUB, CEAF). The column AVG shows the averaged scores of the three coreference
metrics.
Task MD MUC BCUB CEAF AVG
Pred. Mentions w/ Pred. Boundaries 64.88 57.15 67.14 41.94 55.96
Pred. Mentions w/ Gold Boundaries 67.92 59.79 68.65 41.42 56.62
Gold Mentions - 82.55 73.70 65.24 73.83
Table 2: The results of our submitted system on the TEST set. The system uses Best-Link decoding with constraints
on predicted mentions and Best-Link decoding without constraints on gold mentions. The systems are trained on a
collection of TRAIN and DEV sets.
4 Discussion
Most of the mistakes made by the system are due to
not linking co-referring mentions. The constraints
improve slightly the recall on a subset of mentions,
and here we show other common errors for the sys-
tem. For instance, the system fails to link the two
mentions, the Emory University hospital in Atlanta
and the hospital behind me, since each of the men-
tions has a modifier that is not part of the other men-
tion. Another common error is related to pronoun
resolution, especially when a pronoun has several
antecedents in the immediate context, appropriate in
gender, number, and animacy, as in ? E. Robert Wal-
lach was sentenced by a U.S. judge in New York to
six years in prison and fined $ 250,000 for his rack-
eteering conviction in the Wedtech scandal .?: both
E. Robert Wallach and a U.S. judge are appropri-
ate antecedents for the pronoun his. Pronoun errors
are especially important to address since 35% of the
mentions are pronouns.
The system also incorrectly links some mentions,
such as: ?The suspect said it took months to repack-
age...? (?it? cannot refer to a human); ?They see
them.? (subject and object in the same sentence are
linked); and ?Many freeway accidents occur simply
because people stay inside the car and sort out...?
(the NP the car should not be linked to any other
mention, since it does not refer to a specific entity).
5 Conclusions
We have investigated a coreference resolution sys-
tem that uses a rich set of features and two popular
types of clustering algorithm.
While the All-Link clustering seems to be capable
of taking more information into account for making
clustering decisions, as it requires each mention in
a cluster to be compatible with all other mentions in
that cluster, the Best-Link approach still outperforms
it. This raises a natural algorithmic question regard-
ing the inherent nature of clustering style most suit-
able for coreference and regarding possible ways of
infusing more knowledge into different coreference
clustering styles. Our approach accommodates in-
fusion of knowledge via constraints, and we have
demonstrated its utility in an end-to-end coreference
system.
Acknowledgments This research is supported by the Defense
Advanced Research Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the view of the DARPA, AFRL,
ARL or the US government.
43
References
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.
T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).
A. Mccallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In in
Proceedings of the IEEE International Conference on
Semantic Computing (ICSC), September 17-19.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL).
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta, 5.
44
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272?280,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The UI System in the HOO 2012 Shared Task on Error Correction
Alla Rozovskaya Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,mssammon,danr}@illinois.edu
Abstract
We describe the University of Illinois (UI) sys-
tem that participated in the Helping Our Own
(HOO) 2012 shared task, which focuses on
correcting preposition and determiner errors
made by non-native English speakers. The
task consisted of three metrics: Detection,
Recognition, and Correction, and measured
performance before and after additional revi-
sions to the test data were made. Out of 14
teams that participated, our system scored first
in Detection and Recognition and second in
Correction before the revisions; and first in
Detection and second in the other metrics af-
ter revisions. We describe our underlying ap-
proach, which relates to our previous work in
this area, and propose an improvement to the
earlier method, error inflation, which results
in significant gains in performance.
1 Introduction
The task of correcting grammar and usage mistakes
made by English as a Second Language (ESL) writ-
ers is difficult: many of these errors are context-
sensitive mistakes that confuse valid English words
and thus cannot be detected without considering the
context around the word.
Below we show examples of two common ESL
mistakes considered in this paper:
1. ?Nowadays ?*/the Internet makes us closer and closer.?
2. ?I can see at*/on the list a lot of interesting sports.?
In (1), the definite article is incorrectly omitted.
In (2), the writer uses an incorrect preposition.
This paper describes the University of Illinois sys-
tem that participated in the HOO 2012 shared task
on error detection and correction in the use of prepo-
sitions and determiners (Dale et al, 2012). Fourteen
teams took part in the the competition. The scoring
included three metrics: Detection, Recognition, and
Correction, and our team scored first or second in
each metric (see Dale et al (2012) for details).
The UI system consists of two components, a de-
terminer classifier and a preposition classifier, with
a common pre-processing step that corrects spelling
mistakes. The determiner system builds on the ideas
described in Rozovskaya and Roth (2010c). The
preposition classifier uses a combined system, build-
ing on work described in Rozovskaya and Roth
(2011) and Rozovskaya and Roth (2010b).
Both the determiner and the preposition systems
apply the method proposed in our earlier work,
which uses the error distribution of the learner data
to generate artificial errors in training data. The orig-
inal method was proposed for adding artificial er-
rors when training on native English data. In this
task, however, we apply this method when training
on annotated ESL data. Furthermore, we introduce
an improvement that is conceptually simple but very
effective and which also proved to be successful in
an earlier error correction shared task (Dale and Kil-
garriff, 2011; Rozovskaya et al, 2011). We identify
the unique characteristics of the error correction task
and analyze the limitations of existing approaches to
error correction that are due to these characteristics.
Based on this analysis, we propose the error infla-
tion method (Sect. 6.2).
In this paper, we first briefly discuss the task (Sec-
272
tion 2) and present our overall approach (Section
3. Next, we describe the spelling correction mod-
ule (Section 4). Section 5 provides an overview of
the training approaches for error correction tasks.
We present the inflation method in Section 6. Next,
we describe the determiner error correction system
(Section 7), and the preposition error correction
module (Section 8). In Section 9, we present the
performance results of our system in the competi-
tion. We conclude with a brief discussion (Section
10).
2 Task Description
The HOO 2012 shared task focuses on correcting
determiner and preposition errors made by non-
native speakers of English. These errors are some of
the most common and also some of the most difficult
for ESL learners (Leacock et al, 2010); even very
advanced learners make these mistakes (Rozovskaya
and Roth, 2010a).
The training data released by the task organizers
comes from the publicly available FCE corpus (Yan-
nakoudakis et al, 2011). The original FCE data set
contains 1244 essays written by non-native English
speakers and is corrected and error-tagged using a
detailed error classification schema. The HOO train-
ing data contains 1000 of those files.1 The test data
for the task consists of an additional set of 100 stu-
dent essays, different from the 1244 above.
Since the HOO task focuses on determiner and
preposition mistakes, only annotations marking
preposition and determiner mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the HOO data.
More details can be found in Dale et al (2012).
3 System Overview
Our system consists of two components that address
individually article2 and preposition errors and use
the same pre-processing.
1In addition, the participating teams were allowed to use for
training the remaining 244 files of this corpus, as well as any
other data. We also use a publicly available data set of native
English, Google Web 1T corpus (Brants and Franz, 2006), in
one of our models.
2We will use the terms ?article-? and ?determiner errors? in-
terchangeably: article errors constitute the majority of deter-
miner errors, and we address only article mistakes.
The first pre-processing step is correcting spelling
errors. Since the essays were written by students of
English as a Second language, and these essays were
composed on-the-fly, they contain a large number of
spelling errors. These errors add noise to the context
around the target word (article or preposition). Good
context is crucial for robust detection and correction
of article and preposition mistakes.
After spelling errors are corrected, we run a sen-
tence splitter, part-of-speech tagger3 and shallow
parser4 (Punyakanok and Roth, 2001) on the data.
Both the article and the preposition systems use fea-
tures based on the output of these tools.
We made a 244-document subset of the FCE data
a held-out set for development. The results in Sec-
tions 7 and 8 give performance on this held-out set,
where we use the HOO data (1000 files) for train-
ing. The actual performance in the task (Section 9)
reflects the system trained on the whole set of 1244
documents.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). All article sys-
tems are trained using the Averaged Perceptron
(AP) algorithm (Freund and Schapire, 1999), im-
plemented within Learning Based Java (Rizzolo and
Roth, 2010). Our preposition systems combine the
AP algorithm with the Na??ve Bayes (NB) classifier
with prior parameters adapted to the learner data
(see Section 5). The AP systems are trained using
the inflation method (see Section 6.2).
We submitted 10 runs. All of our runs achieved
comparable performance. Sections 7 and 8 describe
our modules.
4 Correcting Spelling Errors
Analysis of the HOO data made clear the need for
a variety of corrections beyond the immediate scope
of the current evaluation. When a mistake occurs in
the vicinity of a target (i.e. preposition or article) er-
ror, it may result in local cues that obscure the nature
of the desired correction.
3http://cogcomp.cs.illinois.edu/page/
software view/POS
4http://cogcomp.cs.illinois.edu/page/
software view/Chunker
273
The following example illustrates such a problem:
?In my opinion your parents should be arrive in the
first party of the month becouse we could be go in
meeting with famous writer, travelled and journalist
who wrote book about Ethiopia.?
In this sample sentence, there are multiple errors
in close proximity: the misspelled word becouse; the
verb form should be arrive; the use of the word party
instead of part; the verb travelled instead of a noun
form; an incorrect preposition in (in meeting).
The context thus contains a considerable amount
of noise that is likely to negatively affect system per-
formance. To address some of these errors, we run a
standard spell-checker over the data.
We use Jazzy5, an open-source Java spell-checker.
The distribution, however, comes only with a US
English dictionary, which also has gaps in its cov-
erage of the language. The FCE corpus prefers UK
English spelling, so we use a mapping from US to
UK English6 to automatically correct the original
dictionary. We also keep the converted US spelling,
since our preposition module makes use of native
English data, where the US spelling is prevalent.
The Jazzy API allows the client to query a word,
and get a list of candidate corrections sorted in or-
der of edit distance from the original term. We
take the first suggestion and replace the original
word. The resulting substitution may be incorrect,
which may in turn mislead the downstream correc-
tion components. However, manual evaluation of
the spelling corrections suggested about 80% were
appropriate, and experimental evaluation on the cor-
pus development set indicated a modest overall im-
provement when the spell-checked documents were
used in place of the originals.
5 Training for Correction Tasks
The standard approach to correcting context-
sensitive ESL mistakes follows the methodology of
the context-sensitive spelling correction task that ad-
dresses such misspellings as their and there (Carl-
son et al, 2001; Golding and Roth, 1999; Golding
and Roth, 1996; Carlson and Fette, 2007; Banko and
Brill, 2001).
Following Rozovskaya and Roth (2010c), we dis-
5http://jazzy.sourceforge.net/
6http://www.tysto.com/articles05/q1/20050324uk-us.shtml
tinguish between two training paradigms in ESL er-
ror correction, depending on whether the author?s
original word choice is used in training as a feature.
In the standard context-sensitive spelling correction
paradigm, the decision of the classifier depends only
on the context around the author?s word, e.g. arti-
cle or preposition, and the author?s word itself is not
taken into consideration in training.
Mistakes made by non-native speakers obey cer-
tain regularities (Lee and Seneff, 2008; Rozovskaya
and Roth, 2010a). Adding knowledge about typ-
ical errors to a model significantly improves its
performance (Gamon, 2010; Rozovskaya and Roth,
2010c; Dahlmeier and Ng, 2011). Typical errors
may refer both to speakers whose first language is
L1 and to specific authors. For example, non-native
speakers whose first language does not have articles
tend to make more articles errors in English (Ro-
zovskaya and Roth, 2010a).
Since non-native speakers? mistakes are system-
atic, the author?s word choice (the source word)
carries a lot of information. Models that use the
source word in training (Han et al, 2010; Gamon,
2010; Dahlmeier and Ng, 2011) learn which errors
are typical for the learner and thus significantly out-
perform systems that only look at context. We call
these models adapted. Training adapted models re-
quires annotated data, since in native English data
the source word is always correct and thus cannot be
used by the classifier.
In this work, we use two methods of adapting a
model to typical errors that have been proposed ear-
lier. Both methods were originally developed for
models trained on native English data: they use a
small amount of annotated ESL data to generate er-
ror statistics. The artificial errors method is based
on generating artificial errors7 in correct native En-
glish training data. The method was implemented
within the Averaged Perceptron (AP) algorithm (Ro-
zovskaya and Roth, 2010c; Rozovskaya and Roth,
2010b), a discriminative learning algorithm, and this
is the algorithm that we use in this work. The NB-
priors method is a special adaptation technique for
the Na??ve Bayes algorithm (Rozovskaya and Roth,
2011). While NB-priors improves both precision
7For each task, only relevant errors are generated ? for ex-
ample, article mistakes for the article correction task.
274
and recall, the artificial errors approach suffers
from low recall due to error sparsity (Sec. 6.1).
In this work, in the preposition correction task,
we use the NB-priors method without modifications
(as described in the original paper). We use the ar-
tificial errors approach both for article and prepo-
sition error correction but with two important mod-
ifications: we train on annotated ESL data instead
of native data, and use the proposed error inflation
method (described in Section 6) to increase the error
rate in training.
6 Error Inflation
In this section, we show why AP (Freund and
Schapire, 1999), a discriminative classifier, is sen-
sitive to the error sparsity of the data, and propose
a method that addresses the problems raised by this
sensitivity.
6.1 Error Sparsity and Low Recall
The low recall of the AP algorithm is related to the
nature of the error correction tasks, which exhibit
low error rates. Even for ESL writers, over 90% of
their preposition and article usage is correct, which
makes the errors very sparse (Rozovskaya and Roth,
2010c). The low recall problem is, in fact, a special
case of a more general problem where there is one
or a small group of dominant features that are very
strongly correlated with the label. In this case, the
system tends to predict the label that matches this
feature, and tends to not predict it when that fea-
ture is absent. In error correction, which tends to
have a very skewed label distribution, this results in
very few errors being detected by the system: when
training on annotated data with naturally occurring
errors and using the source word as a feature, the
system will learn that in the majority of cases the
source word corresponds to the label, and will tend
to over-predict it, which will result in low recall.
In the artificial errors approach, errors are sim-
ulated according to real observed mistakes. Ta-
ble 1 shows a sample confusion matrix based on
preposition mistakes in the FCE corpus; we show
four rows, but the entire table contains 17 rows and
columns, one for each preposition, and each entry
shows Prob(pi|pj), the probability that the author?s
preposition is pi given that the correct preposition
is pj . The matrix also shows the preposition count
for each source and label in the data set. Given the
entire matrix and the counts, it is also possible to
generate the matrix in the other direction and obtain
Prob(pj |pi), the probability that the correct prepo-
sition is pj given that the author?s preposition is pi.
This other matrix is used for adapting NB with the
priors method.
The confusion matrix is sparse and shows that the
distribution of alternatives for each source preposi-
tion is very different from that of the others. This
strongly suggests that these errors are systematic.
Additionally, most prepositions are used correctly,
so the error rate is very low (the error rate can be
estimated by looking at the matrix diagonal in the
table; for example, the error rate for the preposition
about is lower than for into, since 94.4% of the oc-
currences of label about are correct, but only 76.8%
of label into are correct).
The artificial errors thus model the two proper-
ties that we mentioned: the confusability of differ-
ent preposition pairs and the low error rate, and the
artificial errors are similarly sparse.
6.2 The Error Inflation Method
Two extreme choices for solving the low recall prob-
lem due to error sparsity are: (1) training without the
source word feature or (2) training with this feature,
where the classifier relies on it too much. Models
trained without the source feature have very poor
precision. While the NB-priors method does have
good recall, our expectation is that with the right ap-
proach, a discriminative classifier will also improve
recall, but maintain higher precision as well.
We wish to reduce the confidence that the system
has in the source word, while preserving the knowl-
edge the model has about likely confusions and con-
texts of confused words. To accomplish this, we re-
duce the proportion of correct examples, i.e. exam-
ples where the source and the label are the same,
by some positive constant < 1.0 and distribute the
extra probability mass among the typical errors in
an appropriate proportion by generating additional
error examples. This inflates the proportion of ar-
tificial errors in the training data, and hence the er-
ror rate, while keeping the probability distribution
among likely corrections the same. Increasing the
error rate improves the recall, while the typical er-
275
Label Sources
on about into with as at by for from in of over to
(648) (700) (54) (733) (410) (880) (243) (1394) (515) (2213) (1954) (98) (1418)
on (598) 0.846 0.003 0.003 0.008 0.013 - 0.003 0.022 - 0.076 0.013 0.001 0.009
about (686) 0.004 0.944 - 0.007 - - - 0.022 0.005 0.002 0.016 0.001 -
into (55) 0.001 - 0.768 - - - 0.011 0.011 - 0.147 - - 0.053
with (710) 0.001 0.006 - 0.934 - 0.001 0.007 0.004 0.001 0.027 0.003 - 0.015
Table 1: Confusion matrix for preposition errors. Based on data from the FCE corpus for top 17 most frequent English
prepositions. The left column shows the correct preposition. Each row shows the author?s preposition choices for that label and
Prob(source|label). The sources among, between, under and within are not shown for lack of space; they all have 0 probabilities
in the matrix. The numbers next to the targets show the count of the label (or source) in the data set.
ror knowledge ensures that high precision is main-
tained. This method causes the classifier to rely on
the source feature less and increases the contribu-
tion of the features based on context. The learning
algorithm therefore finds a more optimal balance be-
tween the source feature and the context features.
Algorithm 1 shows the pseudo-code for generat-
ing training data; it takes as input training examples,
the confusion matrix CM as shown in Table 1, and
the inflation constant, and generates artificial source
features for correct training examples.8 An infla-
tion constant value of 1.0 simulates learner mistakes
without inflation. Table 2 shows the proportion of
artificial errors created in training using the inflation
method for different inflation rates.
Algorithm 1 Data Generation with Inflation
Input: Training examples E with correct sources, confusion matrix
CM , inflation constant C
Output: Training examples E with artificial errors
for Example e in E do
Initialize lab? e.label, e.source? e.label
Randomize targets ? CM [lab]
Initialize flag? False
for target t in targets do
if flag equals True then
Break
end if
if t equals lab then
Prob(t) = CM [lab][t] ? C
else
Prob(t) = 1.0?CM [lab][lab]?C1.0?CM [lab][lab] ? CM [lab][t]
end if
x? Random[0, 1]
if x < Prob(t) then
e.source? t
flag? True
end if
end for
end for
return E
8When training on native English data, all examples are cor-
rect. When training on annotated learner data, some examples
will contain naturally occurring mistakes.
Inflation rate
1.0 (Regular) 0.9 0.8 0.7 0.6 0.5
7.7% 15.1% 22.6% 30.1% 37.5% 45.0%
Table 2: Artificial errors. Proportion of generated artificial
preposition errors in training using the inflation method (based
on the FCE corpus).
7 Determiners
Table 4 shows the distribution of determiner errors
in the HOO training set. Even though the majority
of determiner errors involve article mistakes, 14% of
errors are personal and possessive pronouns.9 Most
of the determiner errors involve omitting an article.
Similar error patterns have been observed in other
ESL corpora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. Because
the majority of determiner errors are omissions, it is
very important to target this subset of mistakes. One
approach would be to consider every space as a pos-
sible article insertion point. However, this method
will likely produce a lot of noise. The standard
approach is to consider noun-phrase-initial contexts
(Han et al, 2006; Rozovskaya and Roth, 2010c).
Error type Example
Repl. 15.7% ?Can you send me the*/a letter back writing
what happened to you recently.?
Omis. 57.5% ?Nowadays ?*/the Internet makes us closer and
closer.?
Unnec. 26.8% ?One of my hobbies is the*/? photography.?
Table 4: Distribution of determiner errors in the HOO
training data.
9e.g. ?Pat apologized to me for not keeping the*/my secrets.?
276
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A,
wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP,
headWord&3wordsAfterNP, npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition. adj
feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags denote
all words (POS tags) in the NP.
7.1 Determiner Features
The features are presented in Table 3. The model
also uses the source article as a feature.
7.2 Training the Determiner System
Model Detection Correction
AP (natural errors) 30.75 28.97
AP (inflation) 34.62 32.02
Table 5: Article development results: AP with inflation. The
performance shows the F-Score for the 244 held-out documents
of the original FCE data set. AP with inflation uses the constant
value of 0.8.
The article classifier is based on the artificial er-
rors approach (Rozovskaya and Roth, 2010c). The
original method trains a system on native English
data. The current setting is different, since the FCE
corpus contains annotated learner errors. Since the
errors are sparse, we use the error inflation method
(Section 6.2) to boost the proportion of errors in
training using the error distribution obtained from
the same training set. The effectiveness of this
method is demonstrated by the system performance:
we obtain the top or second result in every metric.
Note also that the article system does not use addi-
tional data for training.
Table 5 compares the performance of the system
trained on natural errors with the performance of the
system trained with the inflation method. We found
that any value of the inflation constant between 0.9
and 0.5 will give a boost in performance. We use
several values; the top determiner model uses the in-
flation constant of 0.8.
8 Prepositions
Table 6 shows the distribution of the three types of
preposition errors in the HOO training data. The
FCE annotation distinguishes between preposition
mistakes and errors involving the infinitive marker
to, e.g. ?He wants ?*/to go there.?, which are anno-
tated as verb errors. Since in the competition only
article and preposition annotations are kept, these
errors are not annotated, and thus we do not target
these mistakes.
Error type Example
Repl. 57.9% ?I can see at*/on the list a lot of interesting
sports.?
Omis. 24.0% ?I will be waiting ?*/for your call.?
Unnec. 18.1% ?Despite of */? being tiring , it was rewarding?
Table 6: Distribution of preposition errors in the HOO
training data.
To detect missing preposition errors, we use a set
of rules, mined from the training data, to identify
possible locations where a preposition might have
been incorrectly omitted. Below we show examples
of such contexts.
? ?I will be waiting ?*/for your call.?
? ?But now we use planes to go ?*/to far places.?
8.1 Preposition Features
All features used in the preposition module are lex-
ical: word n-grams in the 4-word window around
277
Feature Type Description
Word n-ngram features in the 4-word window
around the target
wB, w2B, w3B , wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB,
w2BwBwA, wBwAw2A, wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA,
w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
Preposition complement features compHead, wB&compHead, w2BwB&compHead
Table 7: Features used in the preposition error correction system. wB and wA denote the word immediately before and
after the target, respectively; the other features are defined similarly. compHead denotes the head of the preposition complement.
wB&compHead, w2BwB&compHead are feature conjunctions of compHead with wB and w2BwB, respectively.
the target preposition, and three features that use the
head of the preposition complement (see Table 7).
The NB-priors classifier, which is part of our model,
can only make use of the word n-gram features; it
uses n-gram features of lengths 3, 4, and 5. AP is
trained on the HOO data and uses n-grams of lengths
2, 3, and 4, the head complement features, and the
author?s preposition as a feature.
Model Detection Correction
AP (inflation) 34.64 27.51
NB-priors 38.76 26.57
Combined 41.27 29.35
Table 8: Preposition development results: performance of
individual and combined systems. The performance shows
the F-Score for the 244 held-out documents of the original FCE
data set.
8.2 Training the Preposition System
We train two systems. The first one is an AP model
trained on the FCE data with inflation (similar to
the article system). Correcting preposition errors re-
quires more data to achieve performance compara-
ble to article error correction, due to the task com-
plexity (Gamon, 2010). Moreover, given that the
development and test data are quite different,10 it
makes sense to use a model that is independent of
those, to avoid overfitting. We combine the AP
model with a model trained on native English data.
Our second system is an NB-priors classifier trained
on the the Google Web 1T 5-gram corpus (Brants
and Franz, 2006). We use training data to replace the
prior parameters of the model (see Rozovskaya and
Roth, 2011 for more detail). The NB-priors model
does not target preposition omissions.
10The data contains essays written on prompts, so that the
training data may contain several essays written on the same
prompt and thus will be very similar in content. In contrast,
we expected that the test data will likely contain essays on a
different set of prompts.
The NB-priors model outperforms the AP classi-
fier. The two models are also very different due to
the different learning algorithms and the type of the
data used in training. Our final preposition model
is thus a combination of these two, where we take
as the base the decisions of the NB-priors classifier
and add the AP model predictions for cases when
the base model does not flag a mistake. Table 8
shows the results. The combined model improves
both the detection and correction scores. Our prepo-
sition system ranked first in detection and recogni-
tion and second in correction.
Model Detection Correction
AP (natural errors) 13.50 12.73
AP (inflation) 21.31 32.02
Table 9: Preposition development results: AP with infla-
tion. The performance shows the F-Score for the 244 held-out
documents of the original FCE data set. AP with inflation uses
the constant value of 0.7.
9 Test Performance
A number of revisions were made to the test data
based on the input from the participating teams af-
ter the initial results were obtained, where each team
submitted proposed edits to correct annotation mis-
takes. We show both results.
Table 10 shows results before the revisions were
made. Row 1 shows the performance of the de-
terminer system for the three metrics. This system
achieved the best score in correction, and the second
best scores in detection and recognition. The system
is described in Section 7.2, with the exception that
the final system for the article correction is trained
on the entire FCE data set.
Table 10 (row 2) presents the results on prepo-
sition error correction. The system is described in
Section 8.2 and is a combined model of AP trained
with inflation on the FCE data set and NB-priors
model trained on the Google Web 1T corpus. The
278
Model Detection Recognition Correction
Precision Recall F-Score Precision Recall F-Score Precision Recall F-Score
Articles 40.00 37.79 38.862 38.05 35.94 36.972 35.61 33.64 34.601
Prepositions 38.21 45.34 41.471 31.05 40.25 35.061 20.36 24.15 22.092
Combined 37.22 43.71 40.201 34.23 36.64 35.391 26.39 28.26 27.292
Table 10: Performance on test before revisions. Results are shown before revisions were made to the data. The rank of the
system is shown as a superscript.
Model Detection Recognition Correction
Precision Recall F-Score Precision Recall F-Score Precision Recall F-Score
Articles 43.90 39.30 41.472 45.98 34.93 39.702 41.46 37.12 39.172
Prepositions 41.43 47.54 44.271 37.14 42.62 39.691 26.79 30.74 28.632
Combined 43.56 42.92 43.241 38.97 39.96 39.462 32.58 33.40 32.992
Table 11: Performance on test after revisions. Results are shown after revisions were made to the data. The rank of the system
is shown as a superscript.
preposition system achieved the best scores in detec-
tion and recognition, scoring second in correction.
Row 3 shows the performance of the combined
system. This system was ranked first in detection
and recognition, and second in correction.
Table 11 shows our performance after the revi-
sions were applied.
10 Discussion
The HOO 2012 shared task follows the HOO 2011
pilot shared task (Dale and Kilgarriff, 2011), where
the data was fully corrected and error-tagged and
the participants could address any types of mistakes.
The current task allows for comparison of individ-
ual systems for each error type considered. This is
important, since to date it has been difficult to com-
pare different systems due to the lack of a bench-
mark data set.
The data used for the shared task has many errors
besides the preposition and determiner errors; the
annotations for these have been removed. One un-
desirable consequence of this approach is that some
complex errors that involve either an article or a
preposition mistake but depend on other corrections
on neighboring words, e.g. a noun of a verb, may
result in ungrammatical sequences.
Clearly, the task of annotating all requisite correc-
tions is a daunting task, and it is preferable to iden-
tify subsets of these corrections that can be tackled
somewhat independently of the rest, and these more
complex cases present a problem.
To address these conflicting needs, we propose
that the scope of all ?final? corrections be marked,
without necessarily specifying all individual correc-
tions necessary to transform the original text into
correct English. Edits that plausibly require correc-
tions to their context to resolve correctly could then
be treated as out of scope, and ignored by spelling
correction systems even though in other contexts,
those same edits would be in scope.
11 Conclusion
We have demonstrated how a competitive system for
preposition and determiner error correction can be
built using techniques that address the error sparsity
of the data and the overfitting problem. We built on
our previous work and presented the error inflation
method that can be applied to the earlier proposed
artificial errors approach to boost recall. Our de-
terminer system used error inflation and trained a
model using only the annotated FCE corpus. Our
preposition system combined the FCE-trained sys-
tem with a native-data model that was adapted to
learner errors, using the NB-priors approach pro-
posed earlier. Both of the systems showed compet-
itive performance, scoring first or second in every
task ranking.
Acknowledgments
The authors thank Jeff Pasternack for his assistance and Vivek
Srikumar for helpful feedback. This research is supported by
a grant from the U.S. Department of Education and is partly
supported by the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Proc.
279
of 39th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 26?33, Toulouse,
France, July.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia, PA.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Proc. of
the IEEE International Conference on Machine Learn-
ing and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of the
National Conference on Innovative Applications of Ar-
tificial Intelligence (IAAI), pages 45?50.
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimization.
In Proc. of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL), pages 915?923, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proc. of the 13th
European Workshop on Natural Language Generation
(ENLG), pages 242?249, Nancy, France.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A re-
port on the preposition and determiner error correction
shared task. In Proc. of the NAACL HLT 2012 Sev-
enth WorkshopWorkshop on Innovative Use of NLP for
Building Educational Applications, Montreal, Canada,
June. Association for Computational Linguistics.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In Proc. of the 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL), pages 163?171, Los
Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Proc.
of the International Conference on Machine Learning
(ICML), pages 182?190.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115?
129.
N. Han, J. Tetreault, S. Lee, and J. Ha. 2010. Us-
ing an error-annotated learner corpus to develop and
ESL/EFL error correction system. In Proc. of the Sev-
enth conference on International Language Resources
and Evaluation (LREC), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. An analysis of grammatical
errors in non-native speech in English. In Proc. of the
2008 Spoken Language Technology Workshop, Goa.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995?1001. MIT Press.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC), Valletta, Malta, 5.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL er-
rors: Challenges and rewards. In Proc. of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 28?36,
Los Angeles, California, June. Association for Com-
putational Linguistics.
A. Rozovskaya and D. Roth. 2010b. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
961?970, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In Proc. of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL), pages 154?
162, Los Angeles, California, June. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proc. of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL), pages 924?933, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG).
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011. A
new dataset and method for automatically grading esol
texts. In Proc. of the 49th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
280
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 113?117,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Illinois-Coref: The UI System in the CoNLL-2012 Shared Task
Kai-Wei Chang Rajhans Samdani Alla Rozovskaya Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|mssammon|danr}@illinois.edu
Abstract
The CoNLL-2012 shared task is an extension
of the last year?s coreference task. We partici-
pated in the closed track of the shared tasks in
both years. In this paper, we present the im-
provements of Illinois-Coref system from last
year. We focus on improving mention detec-
tion and pronoun coreference resolution, and
present a new learning protocol. These new
strategies boost the performance of the system
by 5% MUC F1, 0.8% BCUB F1, and 1.7%
CEAF F1 on the OntoNotes-5.0 development
set.
1 Introduction
Coreference resolution has been a popular topic of
study in recent years. In the task, a system requires
to identify denotative phrases (?mentions?) and to
cluster the mentions into equivalence classes, so that
the mentions in the same class refer to the same en-
tity in the real world.
Coreference resolution is a central task in the
Natural Language Processing research. Both the
CoNLL-2011 (Pradhan et al, 2011) and CoNLL-
2012 (Pradhan et al, 2012) shared tasks focus on
resolving coreference on the OntoNotes corpus. We
also participated in the CoNLL-2011 shared task.
Our system (Chang et al, 2011) ranked first in two
out of four scoring metrics (BCUB and BLANC),
and ranked third in the average score. This year,
we further improve the system in several respects.
In Sec. 2, we describe the Illinois-Coref system
for the CoNLL-2011 shared task, which we take as
the baseline. Then, we discuss the improvements
on mention detection (Sec. 3.1), pronoun resolu-
tion (Sec. 3.2), and learning algorithm (Sec. 3.3).
Section 4 shows experimental results and Section 5
offers a brief discussion.
2 Baseline System
We use the Illinois-Coref system from CoNLL-2011
as the basis for our current system and refer to it as
the baseline. We give a brief outline here, but fo-
cus on the innovations that we developed; a detailed
description of the last year?s system can be found in
(Chang et al, 2011).
The Illinois-Coref system uses a machine learn-
ing approach to coreference, with an inference pro-
cedure that supports straightforward inclusion of do-
main knowledge via constraints.
The system first uses heuristics based on Named
Entity recognition, syntactic parsing, and shallow
parsing to identify candidate mentions. A pair-
wise scorer w generates compatibility scores wuv
for pairs of candidate mentions u and v using ex-
tracted features ?(u, v) and linguistic constraints c.
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where t is a threshold parameter (to be tuned). An
inference procedure then determines the optimal set
of links to retain, incorporating constraints that may
override the classifier prediction for a given mention
pair. A post-processing step removes mentions in
singleton clusters.
Last year, we found that a Best-Link decoding
strategy outperformed an All-Link strategy. The
Best-Link approach scans candidate mentions in a
document from left to right. At each mention, if cer-
tain conditions are satisfied, the pairwise scores of
all previous mentions are considered, together with
any constraints that apply. If one or more viable
113
links is available, the highest-scoring link is selected
and added to the set of coreference links. After the
scan is complete, the transitive closure of edges is
taken to generate the coreference clusters, each clus-
ter corresponding to a single predicted entity in the
document.
The formulation of this best-link solution is as fol-
lows. For two mentions u and v, u < v indicates
that the mention u precedes v in the document. Let
yuv be a binary variable, such that yuv = 1 only if
u and v are in the same cluster. For a document d,
Best-Link solves the following formulation:
argmaxy
?
u,v:u<v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components
and the set of mentions in each connected compo-
nent constitute an entity. Note that we solve the
above Best-Link inference using an efficient algo-
rithm (Bengtson and Roth, 2008) which runs in time
quadratic in the number of mentions.
3 Improvements over the Baseline System
Below, we describe improvements introduced to the
baseline Illinois-Coref system.
3.1 Mention Detection
Mention detection is a crucial component of an end-
to-end coreference system, as mention detection er-
rors will propagate to the final coreference chain.
Illinois-Coref implements a high recall and low
precision rule-based system that includes all noun
phrases, pronouns and named entities as candidate
mentions. The error analysis shows that there are
two main types of errors.
Non-referential Noun Phrases. Non-referential
noun phrases are candidate noun phrases, identified
through a syntactic parser, that are unlikely to re-
fer to any entity in the real world (e.g., ?the same
time?). Note that because singleton mentions are not
annotated in the OntoNotes corpus, such phrases are
not considered as mentions. Non-referential noun
phrases are a problem, since during the coreference
stage they may be incorrectly linked to a valid men-
tion, thereby decreasing the precision of the system.
To deal with this problem, we use the training data
to count the number of times that a candidate noun
phrase happens to be a gold mention. Then, we re-
move candidate mentions that frequently appear in
the training data but never appear as gold mentions.
Relaxing this approach, we also take the predicted
head word and the words before and after the men-
tion into account. This helps remove noun phrases
headed by a preposition (e.g., the noun ?fact? in the
phrase ?in fact?). This strategy will slightly degrade
the recall of mention detection, so we tune a thresh-
old learned on the training data for the mention re-
moval.
Incorrect Mention Boundary. A lot of errors in
mention detection happen when predicting mention
boundaries. There are two main reasons for bound-
ary errors: parser mistakes and annotation incon-
sistencies. A mistake made by the parser may be
due to a wrong attachment or adding extra words
to a mention. For example, if the parser attaches
the relative clause inside of the noun phrase ?Pres-
ident Bush, who traveled to China yesterday? to a
different noun, the algorithm will predict ?President
Bush? as a mention instead of ?President Bush, who
traveled to China yesterday?; thus it will make an er-
ror, since the gold mention also includes the relative
clause. In this case, we prefer to keep the candi-
date with a larger span. On the other hand, we may
predict ?President Bush at Dayton? instead of ?Pres-
ident Bush?, if the parser incorrectly attaches the
prepositional phrase. Another example is when ex-
tra words are added, as in ?Today President Bush?.
A correct detection of mention boundaries is cru-
cial to the end-to-end coreference system. The re-
sults in (Chang et al, 2011, Section 3) show that the
baseline system can be improved from 55.96 avg F1
to 56.62 in avg F1 by using gold mention boundaries
generated from a gold annotation of the parsing tree
and the name entity tagging. However, fixing men-
tion boundaries in an end-to-end system is difficult
and requires additional knowledge. In the current
implementation, we focus on a subset of mentions
to further improve the mention detection stage of the
baseline system. Specifically, we fix mentions start-
ing with a stop word and mentions ending with a
punctuation mark. We also use training data to learn
patterns of inappropriate mention boundaries. The
mention candidates that match the patterns are re-
114
moved. This strategy is similar to the method used
to remove non-referential noun phrases.
As for annotation inconsistency, we find that in a
few documents, a punctuation mark or an apostrophe
used to mark the possessive form are inconsistently
added to the end of a mention. The problem results
in an incorrect matching between the gold and pre-
dicted mentions and downgrades the performance of
the learned model. Moreover, the incorrect mention
boundary problem also affects the training phase be-
cause our system is trained on a union set of the pre-
dicted and gold mentions. To fix this problem, in
the training phase, we perform a relaxed matching
between predicted mentions and gold mentions and
ignore the punctuation marks and mentions that start
with one of the following: adverb, verb, determiner,
and cardinal number. For example, we successfully
match the predicted mention ?now the army? to the
gold mention ?the army? and match the predicted
mention ?Sony ?s? to the gold mention ?Sony.? Note
that we cannot fix the inconsistency problem in the
test data.
3.2 Pronoun Resolution
The baseline system uses an identical model for
coreference resolution on both pronouns and non-
pronominal mentions. However, in the litera-
ture (Bengtson and Roth, 2008; Rahman and Ng,
2011; Denis and Baldridge, 2007) the features
for coreference resolution on pronouns and non-
pronouns are usually different. For example, lexi-
cal features play an important role in non-pronoun
coreference resolution, but are less important for
pronoun anaphora resolution. On the other hand,
gender features are not as important in non-pronoun
coreference resolution.
We consider training two separate classifiers with
different sets of features for pronoun and non-
pronoun coreference resolution. Then, in the decod-
ing stage, pronoun and non-pronominal mentions
use different classifiers to find the best antecedent
mention to link to. We use the same features for
non-pronoun coreference resolution, as the baseline
system. For the pronoun anaphora classifier, we use
a set of features described in (Denis and Baldridge,
2007), with some additional features. The aug-
mented feature set includes features to identify if a
pronoun or an antecedent is a speaker in the sen-
Algorithm 1 Online Latent Structured Learning for
Coreference Resolution
Loop until convergence:
For each document Dt and each v ? Dt
1. Let u? = max
u?y(v)
wT?(u, v), and
2. u? = max
u?{u<v}?{?}
wT?(u, v) + ?(u, v, y(v))
3. Let w? w + ?wT (?(u?, v)? ?(u?, v)).
tence. It also includes features to reflect the docu-
ment type. In Section 4, we will demonstrate the im-
provement of using separate classifiers for pronoun
and non-pronoun coreference resolution.
3.3 Learning Protocol for Best-Link Inference
The baseline system applies the strategy in (Bengt-
son and Roth, 2008, Section 2.2) to learn the pair-
wise scoring functionw using the Averaged Percep-
tron algorithm. The algorithm is trained on mention
pairs generated on a per-mention basis. The exam-
ples are generated for a mention v as
? Positive examples: (u, v) is used as a positive
example where u < v is the closest mention to
v in v?s cluster
? Negative examples: for all w with u < w < v,
(w, v) forms a negative example.
Although this approach is simple, it suffers from
a severe label imbalance problem. Moreover, it does
not relate well to the best-link inference, as the deci-
sion of picking the closest preceding mention seems
rather ad-hoc. For example, consider three men-
tions belonging to the same cluster: {m1: ?Presi-
dent Bush?, m2: ?he?, m3:?George Bush?}. The
baseline system always chooses the pair (m2,m3)
as a positive example because m2 is the closet men-
tion of m3. However, it is more proper to learn the
model on the positive pair (m1,m3), as it provides
more information. Since the best links are not given
but are latent in our learning problem, we use an on-
line latent structured learning algorithm (Connor et
al., 2011) to address this problem.
We consider a structured problem that takes men-
tion v and its preceding mentions {u | u < v} as
inputs. The output variables y(v) is the set of an-
tecedent mentions that co-refer with v. We define
a latent structure h(v) to be the bestlink decision
of v. It takes the value ? if v is the first mention
115
Method
Without Separating Pronouns With Separating Pronouns
MD MUC BCUB CEAF AVG MD MUC BCUB CEAF AVG
Binary Classifier (baseline) 70.53 61.63 69.26 43.03 57.97 73.24 64.57 69.78 44.95 59.76
Latent-Structured Learning 73.02 64.98 70.00 44.48 59.82 73.95 65.75 70.25 45.30 60.43
Table 1: The performance of different learning strategies for best-link decoding algorithm. We show the results
with/without using separate pronoun anaphora resolver. The systems are trained on the TRAIN set and evaluated on
the CoNLL-2012 DEV set. We report the F1 scores (%) on mention detection (MD) and coreference metrics (MUC,
BCUB, CEAF). The column AVG shows the averaged scores of the three coreference metrics.
System MD MUC BCUB CEAF AVG
Baseline 64.58 55.49 69.15 43.72 56.12
New Sys. 70.03 60.65 69.95 45.39 58.66
Table 2: The improvement of Illinois-Coref. We report
the F1 scores (%) on the DEV set from CoNLL-2011
shared task. Note that the CoNLL-2011 data set does not
include corpora of bible and of telephone conversation.
in the equivalence class, otherwise it takes values
from {u | u < v}. We define a loss function
?(h(v), v, y(v)) as
?(h(v), v, y(v)) =
{
0 h(v) ? y(v),
1 h(v) /? y(v).
We further define the feature vector ?(?, v) to be a
zero vector and ? to be the learning rate in Percep-
tron algorithm. Then, the weight vectorw in (1) can
be learned from Algorithm 1. At each step, Alg. 1
picks a mention v and finds the Best-Link decision
u? that is consistent with the gold cluster. Then, it
solves a loss-augmented inference problem to find
the best link decision u? with current model (u? = ?
if the classifier decides that v does not have coref-
erent antecedent mention). Finally, the model w is
updated by the difference between the feature vec-
tors ?(u?, v) and ?(u?, v).
Alg. 1 makes learning more coherent with infer-
ence. Furthermore, it naturally solves the data im-
balance problem. Lastly, this algorithm is fast and
converges very quickly.
4 Experiments and Results
In this section, we demonstrate the performance of
Illinois-Coref on the OntoNotes-5.0 data set. A pre-
vious experiment using an earlier version of this data
can be found in (Pradhan et al, 2007). We first show
the improvement of the mention detection system.
Then, we compare different learning protocols for
coreference resolution. Finally, we show the overall
performance improvement of Illinois-Coref system.
First, we analyze the performance of mention de-
tection before the coreference stage. Note that sin-
gleton mentions are included since it is not possible
to identify singleton mentions before running coref-
erence. They are removed in the post-processing
stage. The mention detection performance of the
end-to-end system will be discussed later in this sec-
tion. With the strategy described in Section 3.1, we
improve the F1 score for mention detection from
55.92% to 57.89%. Moreover, we improve the de-
tection performance on short named entity mentions
(name entity with less than 5 words) from 61.36 to
64.00 in F1 scores. Such mentions are more impor-
tant because they are easier to resolve in the corefer-
ence layer.
Regarding the learning algorithm, Table 1 shows
the performance of the two learning protocols
with/without separating pronoun anaphora resolver.
The results show that both strategies of using a pro-
noun classifier and training a latent structured model
with a online algorithm improve the system perfor-
mance. Combining the two strategies, the avg F1
score is improved by 2.45%.
Finally, we compare the final system with the
baseline system. We evaluate both systems on the
CoNLL-11 DEV data set, as the baseline system
is tuned on it. The results show that Illinois-Coref
achieves better scores on all the metrics. The men-
tion detection performance after coreference resolu-
tion is also significantly improved.
116
Task MD MUC BCUB CEAF AVG
English (Pred. Mentions) 74.32 66.38 69.34 44.81 60.18
English (Gold Mention Boundaries) 75.72 67.80 69.75 45.12 60.89
English (Gold Mentions) 100.00 85.74 77.46 68.46 77.22
Chinese (Pred Mentions) 47.58 37.93 63.23 35.97 45.71
Table 3: The results of our submitted system on the TEST set. The systems are trained on a collection of TRAIN and
DEV sets.
4.1 Chinese Coreference Resolution
We apply the same system to Chinese coreference
resolution. However, because the pronoun proper-
ties in Chinese are different from those in English,
we do not train separate classifiers for pronoun and
non-pronoun coreference resolution. Our Chinese
coreference resolution on Dev set achieves 37.88%
MUC, 63.37% BCUB, and 35.78% CEAF in F1
score. The performance for Chinese coreference is
not as good as the performance of the coreference
system for English. One reason for that is that we
use the same feature set for both Chinese and En-
glish systems, and the feature set is developed for
the English corpus. Studying the value of strong fea-
tures for Chinese coreference resolution system is a
potential topic for future research.
4.2 Test Results
Table 3 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on both English and Chinese coref-
erence resolution on predicted mentions with pre-
dicted boundaries. For English coreference resolu-
tion, we also report the results when using gold men-
tions and when using gold mention boundaries1.
5 Conclusion
We described strategies for improving mention de-
tection and proposed an online latent structure al-
gorithm for coreference resolution. We also pro-
posed using separate classifiers for making Best-
Link decisions on pronoun and non-pronoun men-
tions. These strategies significantly improve the
Illinois-Coref system.
1Note that, in Ontonotes annotation, specifying gold men-
tions requires coreference resolution to exclude singleton men-
tions. Gold mention boundaries are provided by the task orga-
nizers and include singleton mentions.
Acknowledgments This research is supported by the
Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-
0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings,
and conclusion or recommendations expressed in this ma-
terial are those of the author(s) and do not necessarily
reflect the view of the DARPA, AFRL, ARL or the US
government.
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference proto-
cols for coreference resolution. In CoNLL.
M. Connor, C. Fisher, and D. Roth. 2011. Online latent
structure training for language acquisition. In IJCAI.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In IJCAI.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In
ICSC.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted coreference in
OntoNotes. In CoNLL.
S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL.
A. Rahman and V. Ng. 2011. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. Journal of AI Research, 40(1):469?521.
117
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The University of Illinois System in the CoNLL-2013 Shared Task
Alla Rozovskaya Kai-Wei Chang Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2013 shared task focuses on
correcting grammatical errors in essays
written by non-native learners of English.
In this paper, we describe the University
of Illinois system that participated in the
shared task. The system consists of five
components and targets five types of com-
mon grammatical mistakes made by En-
glish as Second Language writers. We de-
scribe our underlying approach, which re-
lates to our previous work, and describe
the novel aspects of the system in more de-
tail. Out of 17 participating teams, our sys-
tem is ranked first based on both the orig-
inal annotation and on the revised annota-
tion.
1 Introduction
The task of correcting grammar and usage mis-
takes made by English as a Second Language
(ESL) writers is difficult for several reasons. First,
many of these errors are context-sensitive mistakes
that confuse valid English words and thus can-
not be detected without considering the context
around the word. Second, the relative frequency
of mistakes is quite low: for a given type of mis-
take, an ESL writer will typically make mistakes
in only a small proportion of relevant structures.
For example, determiner mistakes usually occur
in 5% to 10% of noun phrases in various anno-
tated ESL corpora (Rozovskaya and Roth, 2010a).
Third, an ESL writer may make multiple mistakes
in a single sentence, which may give misleading
local cues for individual classifiers. In the exam-
ple shown in Figure 1, the agreement error on the
verb ?tend? interacts with the noun number error
on the word ?equipments?.
Therefore , the *equipments/equipment of bio-
metric identification *tend/tends to be in-
expensive .
Figure 1: Representative ESL errors in a sample
sentence from the training data.
The CoNLL-2013 shared task (Ng et al, 2013)
focuses on the following five common mistakes
made by ESL writers:
? article/determiner
? preposition
? noun number
? subject-verb agreement
? verb form
Errors outside this target group are present in the
task corpora, but are not evaluated.
In this paper, we present a system that combines
a set of statistical models, where each model spe-
cializes in correcting one of the errors described
above. Because the individual error types have
different characteristics, we use several different
approaches. The article system builds on the el-
ements of the system described in (Rozovskaya
and Roth, 2010c). The preposition classifier uses
a combined system, building on work described
in (Rozovskaya and Roth, 2011) and (Rozovskaya
and Roth, 2010b). The remaining three models are
all Na??ve Bayes classifiers trained on the Google
Web 1T 5-gram corpus (henceforth, Google cor-
pus, (Brants and Franz, 2006)).
We first briefly discuss the task (Section 2) and
give the overview of our system (Section 3). We
then describe the error-specific components (Sec-
tions 3.1, 3.2 and 3.3). The sections describ-
ing individual components quantify their perfor-
mance on splits of the training data. In Section 4,
13
we evaluate the complete system on the training
data using 5-fold cross-validation (hereafter, ?5-
fold CV?) and in Section 5 we show the results we
obtained on test.
We close with a discussion focused on error
analysis (Section 6) and our conclusions (Sec-
tion 7).
2 Task Description
The CoNLL-2013 shared task focuses on correct-
ing five types of mistakes that are commonly made
by non-native speakers of English. The train-
ing data released by the task organizers comes
from the NUCLE corpus (Dahlmeier et al, 2013),
which contains essays written by learners of En-
glish as a foreign language and is corrected by
English teachers. The test data for the task con-
sists of an additional set of 50 student essays. Ta-
ble 1 illustrates the mistakes considered in the task
and Table 2 illustrates the distribution of these er-
rors in the released training data and the test data.
We note that the test data contains a much larger
proportion of annotated mistakes. For example,
while only 2.4% of noun phrases in the training
data have determiner errors, in the test data 10%
of noun phrases have mistakes.
Error type Percentage of errors
Training Test
Articles 2.4% 10.0%
Prepositions 2.0% 10.7%
Noun number 1.6% 6.0%
Subject-verb agreement 2.0% 5.2%
Verb form 0.8% 2.5%
Table 2: Statistics on error distribution in train-
ing and test data. Percentage denotes the erro-
neous instances with respect to the total number of
relevant instances in the data. For example, 10%
of noun phrases in the test data have determiner
errors.
Since the task focuses on five error types, only
annotations marking these mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the data.
3 System Components
Our system consists of five components that ad-
dress individually article1, preposition, noun verb
1We will use the terms ?article-? and ?determiner errors?
interchangeably: article errors constitute the majority of de-
form and subject-verb agreement errors.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). The article sys-
tem is trained using the Averaged Perceptron (AP)
algorithm (Freund and Schapire, 1999), imple-
mented within Learning Based Java (Rizzolo and
Roth, 2010). The AP system is trained using the
inflation method (Rozovskaya et al, 2012). Our
preposition system is a Na??ve Bayes (NB) classi-
fier trained on the Google corpus and with prior
parameters adapted to the learner data.
The other modules ? those that correct noun and
verb errors ? are all NB models trained on the
Google corpus.
All components take as input the corpus doc-
uments preprocessed with a part-of-speech tag-
ger2 and shallow parser3 (Punyakanok and Roth,
2001). Note that the shared task data already
contains comparable pre-processing information,
in addition to other information, including depen-
dency parse and constituency parse, but we chose
to run our own pre-processing tools. The article
module uses the POS and chunker output to gen-
erate some of its features and to generate candi-
dates (likely contexts for missing articles). The
other system components use the pre-processing
tools only as part of candidate generation (e.g., to
identify all nouns in the data for the noun classi-
fier) because these components are trained on the
Google corpus and thus only employ word n-gram
features.
During development, we split the released train-
ing data into five parts. The results in Sections 3.1,
3.2, and 3.3 give performance of 5-fold CV on the
training data. In Section 4 we report the develop-
ment 5-fold CV results of the complete model and
the performance on the test data. Note that the per-
formance reported for the overall task on the test
data in Section 4 reflects the system that makes use
of the entire training corpus. It is also important to
remark that only the determiner system is trained
on the ESL data. The other models are trained on
native data, and the ESL training data is only used
to optimize the decision thresholds of the models.
terminer errors, and we address only article mistakes.
2http://cogcomp.cs.illinois.edu/page/
software view/POS
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
14
Error type Examples
Article ?It is also important to create *a/? better material that can support
*the/? buildings despite any natural disaster like earthquakes.?
Preposition ?As the number of people grows, the need *of /for habitable environ-
ment is unquestionably essential.
Noun number Some countries are having difficulties in managing a place to live for
their *citizen/citizens as they tend to get overpopulated.?
Subject-verb agreement ?Therefore , the equipments of biometric identification *tend/tends
to be inexpensive.
Verb form
?...countries with a lot of deserts can terraform their desert to increase
their habitable land and *using/use irrigation..?
?it was not *surprised/surprising to observe an increasing need for a
convenient and cost effective platform.?
Table 1: Example errors. Note that only the errors exemplifying the relevant phenomena are marked
in the table; the sentences may contain other mistakes. Errors marked as verb form include multiple
grammatical phenomena that may characterize verbs.
3.1 Determiners
There are three types of determiner error: omitting
a determiner; choosing an incorrect determiner;
and adding a spurious determiner. Even though
the majority of determiner errors involve article
mistakes, some of these errors involve personal
and possessive pronouns.4 Most of the determiner
errors, however, involve omitting an article (these
make up over 60% in the training data). Similar er-
ror patterns have been observed in other ESL cor-
pora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. The sys-
tem first extracts from the data all articles, and all
spaces at the beginning of a noun phrase where an
article is likely to be omitted (Han et al, 2006; Ro-
zovskaya and Roth, 2010c). Then we train a multi-
class classifier with features described in Table 3.
These features were used successfully in previous
tasks in error correction (Rozovskaya et al, 2012;
Rozovskaya et al, 2011).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging a mistake, which results
in low recall. To avoid this problem, we adopt the
approach proposed in (Rozovskaya et al, 2012),
the error inflation method, and add artificial arti-
cle errors in the training data based on the error
distribution on the training set. This method pre-
vents the source feature from dominating the con-
text features, and improves the recall of the sys-
4e.g. ?Pat apologized to me for not keeping the*/my se-
crets.?
tem.
We experimented with two types of classifiers:
Averaged Perceptron (AP) and an L1-generalized
logistic regression classifier (LR). Since the arti-
cle system is trained on the ESL data, of which
we have a limited amount, we also experimented
with adding a language model (LM) feature to the
LR learner. This feature indicates if the correc-
tion is accepted by a language model trained on
the Google corpus. The performance of each clas-
sifier on 5-fold CV on the training data is shown in
Table 4. The results show that AP performs better
than LR. We observed that adding the LM feature
improves precision but results in lower F1, so we
chose the AP classifier without the LM feature for
our final system.
Model Precision Recall F1
AP (inflation) 0.17 0.31 0.22
AP (inflation+LM) 0.26 0.15 0.19
LR (inflation) 0.17 0.29 0.22
LR (inflation+LM) 0.24 0.21 0.22
Table 4: Article development results Results on 5-fold
CV. AP With Inflation achieves the best development using an
inflation constant of 0.85. AP achieves higher performance
without using the language model feature.
3.2 Prepositions
The most common preposition errors are replace-
ments, i.e., where the author correctly recognized
the need for a preposition, but chose the wrong one
to use.
15
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A, wAw2Aw3A,
w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP, headWord&3wordsAfterNP,
npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Source the word used by the original writer
LM a binary feature assigned by a language model
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition.
adj feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags
denote all words (POS tags) in the NP.
3.2.1 Preposition Features
All features used in the preposition module are
lexical: word n-grams in the 4-word window
around the target preposition. The NB-priors clas-
sifier, which is part of our model, can only make
use of the word n-gram features; it uses n-gram
features of lengths 3, 4, and 5. Note that since the
NB model is trained on the Google corpus, the an-
notated ESL training data is used only to replace
the prior parameters of the model (see Rozovskaya
and Roth, 2011 for more details).
3.2.2 Training the Preposition System
Correcting preposition errors requires more data
to achieve performance comparable to article er-
ror correction due to the task complexity (Gamon,
2010). We found that training an AP model on
the ESL training data with more sophisticated fea-
tures is not as effective as training on a native En-
glish dataset of larger size. The ESL training data
contains slightly over 100K preposition examples,
which is several orders of magnitude smaller than
the Google n-gram corpus. We use the shared
task training data to replace the prior parameters
of the model (see Rozovskaya and Roth, 2011 for
more details). The NB-priors model does not tar-
get preposition omissions and insertions: it cor-
rects only preposition replacements that involve
the 12 most common English prepositions. The
task includes mistakes that cover 36 prepositions
but we found that the model performance drops
once the confusion set becomes too large. Table
5 shows the performance of the system on the 5-
fold CV on the training data, where each time the
classifier was trained on 80% of the documents.
Model Precision Recall F1
NB-priors 0.14 0.14 0.14
Table 5: Preposition results: NB with priors. Results on
5-fold CV. The model is trained on the Google corpus.
3.3 Correcting Nouns and Verbs
The three remaining types of errors ? noun num-
ber errors, subject-verb agreement, and the various
verb form mistakes ? are corrected using separate
NB models also trained on the Google corpus. We
focus here on the selection of candidates for cor-
rection, as this strongly affects performance.
3.3.1 Candidate Selection
This stage selects the set of words that are pre-
sented as input to the classifier. This is a crucial
step because it limits the performance of any sys-
tem: those errors that are missed at this stage have
no chance of being detected by the later stages.
This is also a challenging step as the class of
verbs and nouns is open, with many English verbs
and nouns being compatible with multiple parts of
speech. This problem does not arise in preposi-
tion and article error correction, where candidates
are determined by surface form (i.e. can be deter-
mined using a closed list of prepositions or arti-
cles).
We use the POS tag and the shallow parser out-
put to identify the set of candidates that are input
to the classifiers. In particular, for nouns, we col-
lect all words tagged as NN or NNS. Since pre-
processing tools are known to make more mis-
takes on ESL data than on native data, this pro-
cedure does not have a perfect result on the iden-
tification of all noun mistakes. For example, we
16
miss about 10% of noun errors due to POS/shallow
parser errors. For verbs, we compared several
candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as iden-
tified by the shallow parser. Method (2) ex-
pands this set to words tagged with one of the
verb POS tags {VB,VBN,VBG,VBD,VBP,VBZ}.
However, generating candidates by selecting only
those tagged as verbs is not good enough, since the
POS tagger performance on ESL data is known to
be suboptimal (Nagata et al, 2011), especially for
verbs containing errors. For example, verbs lack-
ing agreement markers are likely to be mistagged
as nouns (Lee and Seneff, 2008). Erroneous verbs
are exactly the cases that we wish to include.
Method (3) adds words that are in the lemma list of
common English verbs compiled using the Giga-
word corpus. The last method has the highest re-
call on the candidate identification; it misses only
5% of verb errors, and also has better performance
in the complete model. We thus use this method.
3.3.2 Noun-Verb Correction Performance
Table 6 shows the performance of the systems
based on 5-fold CV on the training data. Each
model is trained individually on the Google cor-
pus, and is individually processed to optimize the
respective thresholds.
Model Precision Recall F1
Noun number 0.17 0.38 0.23
Subject-verb agr. 0.19 0.24 0.21
Verb form 0.07 0.20 0.10
Table 6: Noun, subject-verb agreement and
verb form results. Results on 5-fold CV. The
models are trained on the Google corpus.
4 Combined Model
In the previous sections, we described the individ-
ual components of the system developed to target
specific error types. The combined model includes
all of these modules, which are each applied to
examples individually: there is no pipeline, and
the individual predictions of the modules are then
pooled.
The combined system also includes a post-
processing step where we remove certain correc-
tions of noun and verb forms that we found oc-
cur quite often but are never correct. This hap-
pens when both choices ? the writer?s selection
and the correction ? are valid but the latter is ob-
served more frequently in the native training data.
For example, the phrase ?developing country? is
changed to ?developed country? even though both
are legitimate English expressions. If a correction
is frequently proposed but always results in a false
alarm, we add it to a list of changes that is ignored
when we generate the system output. When we
generate the output on Test set, 8 unique pairs of
such changes are ignored (36 pairs of changes in
total).
We now show the combined results on the train-
ing data by conducting 5-fold CV, where we add
one component at a time. Table 8 shows that the
recall and the F1 scores improve when each com-
ponent is added to the system. The final system
achieves an F1 score of 0.21 on the training data
in 5-fold CV.
Model Precision Recall F1
Articles 0.16 0.12 0.14
+Prepositions 0.16 0.14 0.15
+Noun number 0.17 0.23 0.20
+Subject-verb agr. 0.18 0.25 0.21
+Verb form (All) 0.18 0.27 0.21
Table 7: Results on 5-fold CV on the training
data. The article model is trained on the ESL
data using AP. The other models are trained on the
Google corpus. The last line shows the results,
when all of the five modules are included.
5 Test Results
The previous section showed the performance of
the system on the training data. In this section,
we show the results on the test set. As previously,
the performance improves when each component
is added into the final system. However, we also
note that the precision is much higher while the
recall is only slightly lower. We attribute this in-
creased precision to the observed differences in
the percentage of annotated errors in training vs.
test (see Section 3) and hypothesize that the train-
ing data may contain additional relevant errors that
were not included in the annotation.
Besides the original official annotations an-
nounced by the organizers, another set of anno-
tations is offered based on the combination of re-
vised official annotations and accepted alternative
annotations proposed by participants. We show in
Table 8 when our system is scored based on the
17
revised annotations, both the precision and the re-
call are higher. Our system achieves the highest
scores out of 17 participating teams based on both
the original and revised annotations.
Model Precision Recall F1
Scores based on the original annotations
Articles 0.48 0.11 0.18
+Prepositions 0.45 0.12 0.19
+Noun number 0.48 0.21 0.29
+Subject-verb agr. 0.48 0.22 0.30
+Verb form (All) 0.46 0.23 0.31
Scores based on the revised annotations
All 0.62 0.32 0.42
Table 8: Results on Test. The article model is
trained on the ESL data using AP. The other mod-
els are trained on the Google corpus. All denotes
the results of the complete model that includes all
of the five modules.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes.
6.1 Error Analysis
Incorrect verb form correction: Safety is one of
the crucial problems that many countries and com-
panies *concerned/concerns.
Here, the phrasing requires multiple changes;
to maintain the same word order, this correction
would be needed in tandem with the insertion of
the auxiliary ?have? to create a passive construc-
tion.
Incorrect determiner insertion: In this era,
Engineering designs can help to provide more
habitable accommodation by designing a stronger
material so it?s possible to create a taller and safer
building, a better and efficient sanitation system
to prevent *?/ the disease, and also by designing
a way to change the condition of the inhabitable
environment.
This example requires a model of discourse at
the level of recognizing when a specific disease
is a focus of the text, rather than disease in gen-
eral. The use of a singular construction ?a taller
and safer building? in this context is somewhat un-
conventional and potentially makes this distinction
even harder to detect.
Incorrect verb number correction:
One current human *need/needs that should
be given priority is the search for renewable re-
sources.
This appears to be the result of the system
heuristics intended to mitigate POS tagging errors
on ESL text, where the word ?need? is considered
as a candidate verb rather thana noun; this results
in an incorrect change to make the ?verb? agree in
number with the phrase ?one human?.
Incorrect determiner deletion: This had
shown that the engineering design process is es-
sential in solving problems and it ensures that the
problem is thoroughly looked into and ensure that
the engineers are generating ideas that target the
main problem, *the/? depletion and harmful fuel.
In this example, local context may suggest a list
structure, but the wider context indicates that the
comma represents an appositive structure.
6.2 Discussion
Note that the presence of multiple errors can have
very negative effects on preprocessing. For exam-
ple, when an incorrect verb form is used that re-
sults in a word form commonly used as a noun,
the outputs of the parsers tend to be incorrect. This
limits the potential of rule-based approaches.
Machine learning approaches, on the other
hand, require sufficient examples of each error
type to allow robust statistical modeling of contex-
tual features. Given the general sparsity of ESL
errors, together with the additional noise intro-
duced into more sophisticated preprocessing com-
ponents by errors with overlapping contexts, it ap-
pears hard to leverage these more sophisticated
tools to generate features for machine learning ap-
proaches. This motivates our use of just POS and
shallow parse analysis, together with language-
modeling approaches that can use counts derived
from very large native corpora, to provide robust
inputs for machine learning algorithms.
The interaction between errors suggests that
constraints could be used to improve results by en-
suring, for example, that verb number, noun num-
ber, and noun phrase determiner are consistent.
This is more difficult than it may first appear for
two reasons. First, the noun that is the subject
of the verb under consideration may be relatively
distant in the sentence (due to the presence of in-
tervening relative clauses, for example). Second,
the constraint only limits the possible correction
options: the correct number for the noun in fo-
18
cus may depend on the form used in the preceding
sentences ? for example, to distinguish between a
general statement about some type of entity, and a
statement about a specific entity.
These observations suggest that achieving very
high performance in the task of grammar correc-
tion requires sophisticated modeling of deep struc-
ture in natural language documents.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction
and ranked first out of 17 participating teams. We
built specialized models for the five types of mis-
takes that are the focus of the competition. We
have also presented error analysis of the system
output and discussed possible directions for future
work.
Acknowledgments
This material is based on research sponsored by DARPA under agreement num-
ber FA8750-13-2-0008. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright
notation thereon. The views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of DARPA or the U.S.
Government. This research is also supported by a grant from the U.S. Depart-
ment of Education and by the DARPA Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In LREC.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training
paradigms for correcting errors in grammar and us-
age. In NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the hoo 2012 shared task on error cor-
rection.
19
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
The Illinois-Columbia System in the CoNLL-2014 Shared Task
Alla Rozovskaya
1
Kai-Wei Chang
2
Mark Sammons
2
Dan Roth
2
Nizar Habash
1
1
Center for Computational Learning Systems, Columbia University
{alla,habash}@ccls.columbia.edu
2
Cognitive Computation Group, University of Illinois at Urbana-Champaign
{kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2014 shared task is an ex-
tension of last year?s shared task and fo-
cuses on correcting grammatical errors in
essays written by non-native learners of
English. In this paper, we describe the
Illinois-Columbia system that participated
in the shared task. Our system ranked sec-
ond on the original annotations and first on
the revised annotations.
The core of the system is based on the
University of Illinois model that placed
first in the CoNLL-2013 shared task. This
baseline model has been improved and ex-
panded for this year?s competition in sev-
eral respects. We describe our underly-
ing approach, which relates to our previ-
ous work, and describe the novel aspects
of the system in more detail.
1 Introduction
The topic of text correction has seen a lot of inter-
est in the past several years, with a focus on cor-
recting grammatical errors made by English as a
Second Language (ESL) learners. ESL error cor-
rection is an important problem since most writers
of English are not native English speakers. The in-
creased interest in this topic can be seen not only
from the number of papers published on the topic
but also from the three competitions devoted to
grammatical error correction for non-native writ-
ers that have recently taken place: HOO-2011
(Dale and Kilgarriff, 2011), HOO-2012 (Dale et
al., 2012), and the CoNLL-2013 shared task (Ng
et al., 2013).
In all three shared tasks, the participating sys-
tems performed at a level that is considered ex-
tremely low compared to performance obtained in
other areas of NLP: even the best systems attained
F1 scores in the range of 20-30 points.
The key reason that text correction is a diffi-
cult task is that even for non-native English speak-
ers, writing accuracy is very high, as errors are
very sparse. Even for some of the most com-
mon types of errors, such as article and preposi-
tion usage, the majority of the words in these cate-
gories (over 90%) are used correctly. For instance,
in the CoNLL training data, only 2% of preposi-
tions are incorrectly used. Because errors are so
sparse, it is more difficult for a system to identify a
mistake accurately and without introducing many
false alarms.
The CoNLL-2014 shared task (Ng et al., 2014)
is an extension of the CoNLL-2013 shared task
(Ng et al., 2013). Both competitions make use
of essays written by ESL learners at the National
University of Singapore. However, while the first
one focused on five kinds of mistakes that are com-
monly made by ESL writers ? article, preposition,
noun number, verb agreement, and verb form ?
this year?s competition covers all errors occurring
in the data. Errors outside the target group were
present in the task corpora last year as well, but
were not evaluated.
Our system extends the one developed by the
University of Illinois (Rozovskaya et al., 2013)
that placed first in the CoNLL-2013 competition.
For this year?s shared task, the system has been
extended and improved in several respects: we ex-
tended the set of errors addressed by the system,
developed a general approach for improving the
error-specific models, and added a joint inference
component to address interaction among errors.
See Rozovskaya and Roth (2013) for more detail.
We briefly discuss the task (Section 2) and give
an overview of the baseline Illinois system (Sec-
tion 3). Section 4 presents the novel aspects of the
system. In Section 5, we evaluate the complete
system on the development data and show the re-
sults obtained on test. We offer error analysis and a
brief discussion in Section 6. Section 7 concludes.
34
Error type Rel. freq. Examples
Article (ArtOrDet) 14.98% *?/The government should help encourage *the/?
breakthroughs as well as *a/? complete medication
system .
Wrong collocation (Wci) 11.94% Some people started to *think/wonder if electronic
products can replace human beings for better perfor-
mances .
Local redundancy (Rloc-) 10.52% Some solutions *{as examples}/? would be to design
plants/fertilizers that give higher yield ...
Noun number (Nn) 8.49% There are many reports around the internet and on
newspaper stating that some users ? *iPhone/iPhones
exploded .
Verb tense (Vt) 7.21% Through the thousands of years , most Chinese scholars
*are/{have been} greatly affected by Confucianism .
Orthography/punctuation (Mec) 6.88% Even British Prime Minister , Gordon Brown *?/, has
urged that all cars in *britain/Britain to be green by
2020 .
Preposition (Prep) 5.43% I do not agree *on/with this argument that surveillance
technology should not be used to track people .
Word form (Wform) 4.87% On the other hand , the application of surveillance tech-
nology serves as a warning to the *murders/murderers
and they might not commit more murder .
Subject-verb agreement (SVA) 3.44% However , tracking people *are/is difficult and different
from tracking goods .
Verb form (Vform) 3.25% Travelers survive in desert thanks to GPS
*guide/guiding them .
Tone (Wtone) 1.29% Hence , as technology especially in the medical field
continues to get developed and updated , people {do
n?t}/{do not} risk their lives anymore .
Table 1: Example errors. In the parentheses, the error codes used in the shared task are shown. Note
that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may
contain other mistakes. Errors marked as verb form include multiple grammatical phenomena that may
characterize verbs. Our system addresses all of the error types except ?Wrong Collocation? and ?Local
Redundancy?.
2 Task Description
Both the training and the test data of the CoNLL-
2014 shared task consist of essays written by stu-
dents at the National University of Singapore. The
training data contains 1.2 million words from the
NUCLE corpus (Dahlmeier et al., 2013) corrected
by English teachers, and an additional set of about
30,000 words that was released last year as a test
set for the CoNLL-2013 shared task. We use last
year?s test data as a development set; the results in
the subsequent sections are reported on this subset.
The CoNLL corpus error tagset includes 28 er-
ror categories. Table 1 illustrates the most com-
mon error categories in the training data; errors are
marked with an asterisk, and ? denotes a missing
word. Our system targets all of these, with the ex-
ception of collocation and local redundancy errors.
Among the less commonly occurring error types,
our system addresses tone (style) errors; these are
illustrated in the table.
It should be noted that the proportion of erro-
neous instances is several times higher in the de-
velopment data than in the training data for all of
the error categories. For example, while only 2.4%
of noun phrases in the training data have deter-
miner errors, in the development data 10% of noun
phrases have determiner errors.
35
?Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear tech-
nology.?
Error type Confusion set
Noun number {factor, factors}
Verb Agreement {contribute, contributes}
Verb Form
{included, including,
includes, include}
Table 2: Sample confusion sets for noun num-
ber, verb agreement, and verb form.
3 The Baseline System
In this section, we briefly describe the Univer-
sity of Illinois system (henceforth Illinois; in the
overview paper of the shared task the system is re-
ferred to as UI) that achieved the best result in the
CoNLL-2013 shared task and which we use as our
baseline model. For a complete description, we
refer the reader to Rozovskaya et al. (2013).
The Illinois system implements five
independently-trained machine-learning clas-
sifiers that follow the popular approach to ESL
error correction borrowed from the context-
sensitive spelling correction task (Golding and
Roth, 1999; Carlson et al., 2001). A confusion
set is defined as a list of confusable words.
Each occurrence of a confusable word in text is
represented as a vector of features derived from a
context window around the target. The problem
is cast as a multi-class classification task and a
classifier is trained on native or learner data. At
prediction time, the model selects the most likely
candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions (this
year, we extend the confusion set and also target
extraneous preposition usage). The article confu-
sion set is as follows: {a, the, ?}.
1
The confu-
sion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants. Table 2 shows sample confusion
sets for noun, agreement, and form errors.
Each classifier takes as input the corpus doc-
uments preprocessed with a part-of-speech tag-
1
? denotes noun-phrase-initial contexts where an article
is likely to have been omitted. The variants ?a? and ?an? are
conflated and are restored later.
ger
2
and shallow parser
3
(Punyakanok and Roth,
2001). The other system components use the pre-
processing tools only as part of candidate genera-
tion (e.g., to identify all nouns in the data for the
noun classifier).
The choice of learning algorithm for each clas-
sifier is motivated by earlier findings showing
that discriminative classifiers outperform other
machine-learning methods on error correction
tasks (Rozovskaya and Roth, 2011). Thus, the
classifiers trained on the learner data make use of
a discriminative model. Because the Google cor-
pus does not contain complete sentences but only
n-gram counts of length up to five, training a dis-
criminative model is not desirable, and we thus use
NB (details in Rozovskaya and Roth (2011)).
The article classifier is a discriminative model
that draws on the state-of-the-art approach de-
scribed in Rozovskaya et al. (2012). The model
makes use of the Averaged Perceptron (AP) algo-
rithm (Freund and Schapire, 1996) and is trained
on the training data of the shared task with rich
features. The article module uses the POS and
chunker output to generate some of its features and
candidates (likely contexts for missing articles).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging mistakes, resulting in low
recall. To avoid this problem, we adopt the ap-
proach proposed in Rozovskaya et al. (2012), the
error inflation method, and add artificial article er-
rors to the training data based on the error distribu-
tion on the training set. This method prevents the
source feature from dominating the context fea-
tures, and improves the recall of the system.
The other classifiers in the baseline system ?
noun number, verb agreement, verb form, and
preposition ? are trained on native English data,
the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Na??ve
Bayes (NB) algorithm. All models use word n-
gram features derived from the 4-word window
around the target word. In the preposition model,
priors for preposition preferences are learned from
the shared task training data (Rozovskaya and
Roth, 2011).
The modules targeting verb agreement and
2
http://cogcomp.cs.illinois.edu/page/
software view/POS
3
http://cogcomp.cs.illinois.edu/page/
software view/Chunker
36
verb form mistakes draw on the linguistically-
motivated approach to correcting verb errors pro-
posed in Rozovskaya et. al (2014).
4 The CoNLL-2014 System
The system in the CoNLL-2014 shared task is im-
proved in three ways: 1) Additional error-specific
classifiers: word form, orthography/punctuation,
and style; 2) Model combination; and 3) Joint in-
ference to address interacting errors. Table 3 sum-
marizes the Illinois and the Illinois-Columbia sys-
tems.
4.1 Targeting Additional Errors
The Illinois-Columbia system implements several
new classifiers to address word form, orthography
and punctuation, and style errors (Table 1).
4.1.1 Word Form Errors
Word form (Wform) errors are grammatical er-
rors that involve confusing words that share a
base form but differ in derivational morphology,
e.g. ?use? and ?usage? (see also Table 1). Con-
fusion sets for word form errors thus should in-
clude words that differ derivationally but share the
same base form. In contrast to verb form errors
where confusion sets specify all possible inflec-
tional forms for a given verb, here, the associated
parts-of-speech may vary more widely. An ex-
ample of a confusion set is {technique, technical,
technology, technological}.
Because word form errors encompass a wide
range of misuse, one approach is to consider ev-
ery word as an error candidate. We follow a more
conservative method and only attempt to correct
those words that occurred in the training data and
were tagged as word form errors (we cleaned up
that list by removing noisy annotations).
A further challenge in addressing word form er-
rors is generating confusion sets. We found that
about 45% of corrections for word form errors in
the development data are covered by the confusion
sets from the training data for the same word. We
thus derive the confusion sets using the training
data. Specifically, for every source word that is
tagged as a word form error in the training data,
the confusion set includes all labels to which that
word is mapped in the training data. In addition,
plural and singular forms are added for all words
tagged as nouns, and inflectional forms are added
for words tagged as verbs. For more detail on
correcting verb errors, we refer the reader to Ro-
zovskaya et al. (2014).
4.1.2 Orthography and Punctuation Errors
The Mec error category includes errors in
spelling, context-sensitive spelling, capitalization,
and punctuation. Our system addresses punctua-
tion errors and capitalization errors.
To correct capitalization errors, we collected
words that are always capitalized in the train-
ing and development data when not occurring
sentence-initially.
The punctuation classifier includes two mod-
ules: a learned component targets missing and
extraneous comma usage and is an AP classifier
trained on the learner data with error inflation.
A second, pattern-based component, complements
the AP model: it inserts missing commas by using
a set of patterns that overwhelmingly prefer the us-
age of a comma, e.g. when a sentence starts with
the word ?hence?. The patterns are learned auto-
matically over the training data: specifically, us-
ing a sliding window of three words on each side,
we compiled a list of word n-gram contexts that
are strongly associated with the usage of a comma.
This list is then used to insert missing commas in
the test data.
4.1.3 Style Errors
The style (Wtone) errors marked in the corpus are
diverse, and the annotations are often not consis-
tent. We constructed a pattern-based system to
deal with two types of style errors that are com-
monly annotated. The first type of style edit avoids
using contractions of negated auxiliary verbs. For
example, it changes ?do n?t? to ?do not?. We use a
pattern-based classifier to identify such errors and
replace the contractions. The second type of style
edit encourages the use of a semi-colon to join
two independent clauses when a conjunctive ad-
verb is used. For example, it edits ?[clause], how-
ever, [clause]? to ?[clause]; however, [clause]?. To
identify such errors, we use a part-of-speech tag-
ger to recognize conjunctive adverbs signifying in-
dependent clauses: if two clauses are joined by the
pattern ?, [conjunctive adverb],?, we will replace it
with ?; [conjunctive adverb],?.
4.2 Modules not Included in the Final System
In addition to the modules described above, we at-
tempted to address two other common error cate-
gories: spelling errors and collocation errors. We
37
Illinois
Classifiers Training data Algorithm
Article Learner AP with inflation
Preposition Native NB-priors
Noun number Native NB
Verb agreement Native NB
Verb form Native NB
Illinois-Columbia
Classifiers Training data Algorithm
Article Learner and native AP with infl. (learner) and NB-priors (native)
Preposition Learner and native AP with infl. (learner) and NB-priors (native)
Noun number Learner and native AP with infl. (learner) and NB (native)
Verb agreement Native AP with infl. (learner) and NB (native)
Verb form Native NB-priors
Word form Native NB-priors
Orthography/punctuation Learner AP and pattern-based
Style Learner Pattern-based
Model combination Section 4.3
Global inference Section 4.4
Table 3: The baseline (Illinois) system vs. the Illinois-Columbia system. AP stands for Averaged
Perceptron, and NB stands for the Na??ve Bayes algorithm.
describe these below even though they were not
included in the final system.
Regular spelling errors are noticeable but not
very frequent, and a number are not marked in
the corpus (for example, the word ?dictronary? in-
stead of ?dictionary? is not tagged as an error). We
used an open source package ? ?Jazzy?
4
? to at-
tempt to automatically correct these errors to im-
prove context signals for other modules. However,
there are often multiple similar words that can be
proposed as corrections, and Jazzy uses phonetic
guidelines that sometimes lead to unintuitive pro-
posals (such as ?doctrinaire? for ?dictronary?). It
would be possible to extend the system with a filter
on candidate answers that uses n-grams or some
other context model to choose better candidates,
but the relatively small number of such errors lim-
its the potential impact of such a system.
Collocation errors are the second most common
error category accounting for 11.94% of all errors
in the training data (Table 1). We tried using the
Illinois context-sensitive spelling system
5
to de-
tect these errors, but this system requires prede-
fined confusion sets to detect possible errors and
to propose valid corrections. The coverage of the
pre-existing confusion sets was poor ? the system
4
http://jazzy.sourceforge.net
5
http://cogcomp.cs.illinois.edu/cssc/
could potentially correct only 2.5% of collocation
errors ? and it is difficult to generate new con-
fusion sets that generalize well, which requires a
great deal of annotated training data. The sys-
tem performance was relatively poor because it
proposed many spurious corrections: we believe
this is due to the relatively limited context it uses,
which makes it particularly susceptible to making
mistakes when there are multiple errors in close
proximity.
4.3 Model Combination
Model combination is another key extension of the
Illinois system.
In the Illinois-Columbia system, article, prepo-
sition, noun, and verb agreement errors are each
addressed via a model that combines error predic-
tions made by a classifier trained on the learner
data with the AP algorithm and those made by
the NB model trained on the Google corpus. The
AP classifiers all make use of richer sets of fea-
tures than the native-trained classifiers: the article,
noun number, and preposition classifiers employ
features that use POS information, while the verb
agreement classifier also makes use of dependency
features extracted using a parser (de Marneffe et
al., 2008). For more detail on the features used
in the agreement module, we refer the reader to
38
Rozovskaya et al. (2014). Finally, all of the AP
models use the source word of the author as a fea-
ture and, similar to the article AP classifier (Sec-
tion 3), implement the error inflation method. The
combined model generates a union of corrections
produced by the components.
We found that for every error type, the com-
bined model is superior to each of the single classi-
fiers, as it combines the advantages of both of the
classifiers so that they complement one another.
In particular, while each of the learner and native
components have similar precision, since the pre-
dictions made differ, the recall of the combined
model improves.
4.4 Joint Inference
One of the mistakes typical for Illinois system
were inconsistent predictions. Inconsistent predic-
tions occur when the classifiers address grammat-
ical phenomena that interact at the sentence level,
e.g. noun number and verb agreement. To ad-
dress this problem, the Illinois-Columbia system
makes use of global inference via an Integer Lin-
ear Programming formulation (Rozovskaya and
Roth, 2013). Note that Rozovskaya and Roth
(2013) also describe a joint learning model that
performs better than the joint inference approach.
However, the joint learning model is based on
training a joint model on the Google corpus, and
is not as strong as the individually-trained classi-
fiers of the Illinois-Columbia system that combine
predictions from two components ? NB classifiers
trained on the native data from the Google corpus
and AP models trained on the learner data (Sec-
tion 4.3).
5 Experimental Results
In Sections 3 and 4, we described the individual
system components that address different types of
errors. In this section, we show how the system
improves when each component is added into the
system. In this year?s competition, systems are
compared using F0.5 measure instead of F1. This
is because in error correction good precision is
more important than having a high recall, and the
F0.5 reflects that by weighing precision twice as
much as recall. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012).
Table 4 reports performance results of each in-
dividual classifier. In the final system, the arti-
cle, preposition, noun number, and verb agree-
Model P R F0.5
Articles (AP) 38.97 8.85 23.19
Articles (NB-priors) 47.34 6.01 19.93
Articles (Comb.) 38.73 10.93 25.67
Prep. (AP) 34.00 0.5 2.35
Prep. (NB-priors) 33.33 0.79 3.61
Prep. (Comb.) 30.06 1.17 5.13
Noun number (NB) 44.74 5.48 18.39
Noun number (AP) 82.35 0.41 2.01
Noun number (Comb.) 45.02 5.57 18.63
Verb agr. (AP) 38.56 1.23 5.46
Verb agr. (NB) 63.41 0.76 3.64
Verb agr. (Comb.) 41.09 1.55 6.75
Verb form (NB-priors) 59.26 1.41 6.42
Word form (NB-priors) 57.54 3.02 12.48
Mec (AP; patterns) 48.48 0.47 2.26
Style (patterns) 84.62 0.64 3.13
Table 4: Performance of classifiers targeting
specific errors.
Model P R F0.5
The baseline (Illinois) system
Articles 38.97 8.85 23.19
+Prepositions 39.24 9.35 23.93
+Noun number 42.13 14.83 30.79
+Subject-verb agr. 42.25 16.06 31.86
+Verb form 43.19 17.20 33.17
Model Combination
+Model combination 42.72 20.19 34.92
Additional Classifiers
+Word form 43.39 21.54 36.07
+Mec 43.70 22.04 36.52
+Style 44.22 21.54 37.09
Joint Inference
+Joint Inference 44.28 22.57 37.13
Table 5: Results on the development data. The
top part of the table shows the performance of the
baseline (Illinois) system from last year.
P R F0.5
Scores based on the original annotations
41.78 24.88 36.79
Scores based on the revised annotations
52.44 29.89 45.57
Table 6: Results on Test.
39
ment classifiers use combined models, each con-
sisting of a classifier trained on the learner data
and a classifier trained on native data. We report
performance of each such component separately
and when they are combined. The results show
that combining models boosts the performance of
each classifier: for example, the performance of
the article classifier improves by more than 2 F0.5
points. It should be noted that results are com-
puted with respect to all errors present in the data.
For this reason, recall is low.
Next, in Table 5, we show the contribution of
the novel components over the baseline system on
the development set. As described in Section 3,
the baseline Illinois system consists of five indi-
vidual components; their performance is shown in
the top part of the table. Note that although for the
development set we make use of last year?s test
set, these results are not comparable to the perfor-
mance results reported in last year?s competition
that used the F1 measure. Overall, the baseline
system achieves an F0.5 score of 33.17 on the de-
velopment set.
Then, by applying the model combination tech-
nique introduced in Section 4.3, the performance
is improved to 34.92. By adding modules to tar-
get three additional error types, the overall perfor-
mance becomes 37.09. Finally, the joint inference
technique (see Section 4.4) slightly improves the
performance further. The final system achieves an
F0.5 score of 37.13.
Table 6 shows the results on the test set provided
by the organizers. As was done previously, the
organizers also offered another set of annotations
based on the combination of revised official anno-
tations and accepted alternative annotations pro-
posed by participants. Performance results on this
set are also shown in Table 6.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes on the development set and discuss
our observations on the competition. We analyze
both the false positive errors and those cases that
are missed by our system.
6.1 Error Analysis
Stylistic preference Surveillance technology
such as RFID (radio-frequency identification) is
one type of examples that has currently been im-
plemented.
Here, our system proposes a change to plural
for the noun ?technology?. The gold standard
solution instead proposes a large number of cor-
rections throughout that work with the choice of
the singular ?technology?. However, using the
plural ?technologies? as proposed by the Illinois-
Columbia system is quite acceptable, and a com-
parable number of corrections would make the rest
of the sentence compatible. Note also that the
gold standard proposes the use of commas around
the phrase ?such as RFID (radio-frequency iden-
tification)?, which could also be omitted based on
stylistic considerations alone.
Word choice The high accuracy in utiliz-
ing surveillance technology eliminates the
*amount/number of disagreements among people.
The use of ?amount? versus ?number? depends
on the noun to which the term attaches. This could
conceivably be achieved by using a rule and word
list, but many such rules would be needed and each
would have relatively low coverage. Our system
does not detect this error.
Presence of multiple errors Not only the details
of location will be provided, but also may lead to
find out the root of this kind of children trading
agency and it helps to prevent more this kind of
tragedy to happen on any family.
The writer has made numerous errors in this
sentence. To determine the correct preposition in
the marked location requires at least the preced-
ing verb phrase to be corrected to ?from happen-
ing?; the extraneous ?more? after ?prevent? in turn
makes the verb phrase correction more unlikely as
it perturbs the contextual clues that a system might
learn to make that correction. Our system pro-
poses a different preposition ? ?in? ? that is better
than the original in the local context, but which is
not correct in the wider context.
Locally coherent, globally incorrect People?s
lives become from increasingly convenient to al-
most luxury, thanks to the implementation of in-
creasingly technology available for the Man?s life.
In this example, the system proposes to delete
the preposition ?from?. This correctiom improves
the local coherency of the sentence. However, the
resulting construction is not consistent with ?to al-
most luxury?, suggesting a more complex correc-
tion (changing the word ?become? to ?are going?).
40
Cascading NLP errors In this, I mean that we
can input this device implant into an animal or
birds species, for us to track their movements and
actions relating to our human research that can
bring us to a new regime.
The word ?implant? in the example sentence
has been identified as a verb by the system and
not a noun due to the unusual use as part of the
phrase ?device implant?. As a result, the system
incorrectly proposes the verb form correction ?im-
planted?.
6.2 Discussion
The error analysis suggests that there are three sig-
nificant challenges to developing a better gram-
mar correction system for the CoNLL-2014 shared
task: identifying candidate errors; modeling the
context of possible errors widely enough to cap-
ture long-distance cues where necessary; and
modeling stylistic preferences involving word
choice, selection of plural or singular, standards
for punctuation, use of a definite or indefinite arti-
cle (or no article at all), and so on. For ESL writ-
ers, the tendency for multiple errors to be made in
close proximity means that global decisions must
be made about sets of possible mistakes, and a sys-
tem must therefore have a quite sophisticated ab-
stract model to generate the basis for consistent
sets of corrections to be proposed.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction.
The system builds on the elements of the Illinois
system that participated in last year?s shared task.
We extended and improved the Illinois system in
three key dimensions, which we presented and
evaluated in this paper. We have also presented
error analysis of the system output and discussed
possible directions for future work.
Acknowledgments
This material is based on research sponsored by DARPA un-
der agreement number FA8750-13-2-0008. The U.S. Gov-
ernment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright nota-
tion thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as necessar-
ily representing the official policies or endorsements, either
expressed or implied, of DARPA or the U.S. Government.
This research is also supported by a grant from the U.S. De-
partment of Education and by the DARPA Machine Reading
Program under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-018. The first and last authors
were partially funded by grant NPRP-4-1058-1-168 from the
Qatar National Research Fund (a member of the Qatar Foun-
dation). The statements made herein are solely the responsi-
bility of the authors.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation
for grammatical error correction. In NAACL, pages
568?572, Montr?eal, Canada, June. Association for
Computational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proc. 13th
International Conference on Machine Learning.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning.
H.T. Ng, S.M. Wu, Y. Wu, C. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
41
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
Baltimore, Maryland, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya and D. Roth. 2013. Joint learning
and inference for grammatical error correction. In
EMNLP, 10.
A. Rozovskaya, M. Sammons, and D. Roth. 2012.
The UI system in the HOO 2012 shared task on er-
ror correction. In Proc. of the Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL) Workshop
on Innovative Use of NLP for Building Educational
Applications.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya, D. Roth, and V. Srikumar. 2014. Cor-
recting grammatical verb errors. In EACL.
42
