  
Can Text Analysis Tell us Something about Technology Progress?  
Khurshid Ahmad  
Department of Computing  
University of Surrey, Guildford,  
Surrey. GU2 7XH. UK 
 k.ahmad@surrey.ac.uk 
AbdulMohsen Al -Thubaity 
Department of Computing  
University of Surrey, Guildfor d,  
Surrey. GU2 7XH. UK  
 a.althubaity@surrey.ac.uk  
 
Abstract 
A corpus -based diachronic analysis of 
patent documents, based mainly on the 
morphologically productive use of certain 
terms can help in tracking the evolution of 
key developments in a rapidly e volving 
specialist field.  The patent texts were o b-
tained from the US Patent & Trade Marks 
Office?s on-line service and the terms 
were extracted automatically from the 
texts.  The chosen specialist field was that 
of fast-switching devices and systems.  
The method presented draws from liter a-
ture on biblio - and sciento -metrics, infor-
mation extraction, corpus linguistics, and 
on aspects of English morphology.  This 
interdisciplinary fram ework shows that 
the evolution of word -formation closely 
shadows the developments in a field of 
technology. 
Introduction 
A patent document is written to pe rsuade a techno-
legal authority that the patentee should be allowed 
to manufacture, sell, or deal in an article to the ex-
clusion of other persons.  The article is typ ically 
based on an invention that the patentee(s) claim 
has been theirs.  The term article  is important in 
that it refers to a tangible object and its u sage is to  
emphasise that ideas, intangibles essentially, ca n-
not be patented. Patent documents are the repos i-
tory of how technology advances and, more 
importantly, show how language supports the 
change. 
The techno-legal authority requires the patent 
document to follow a template.  This template is 
divided broadly into two parts: first, legal te m-
plates comprising pate ntee?s details, juri sdictional 
scope, and related item; second, technical 
templates divided into a summary of the patentee?s 
claims, relation of the article to previously patented 
articles ? the so-called prior art  ? and the scien-
tific/technical basis of t he claim.  The scientific 
claim is written in a language that is similar to the 
language of journal p apers. 
One important task that is slowly emerging is 
the extent to which the analysis of a patent doc u-
ment can be automated particularly to a ssess the 
overlap between the claims in the document about 
the article to be patented with that of related, rel e-
vant and even counter -claims about the article.  
The related and rel evant claims and counter claims 
may be found in existing patent documents and 
may, more in directly, exist in journal papers.  
A patent document has to make references to 
all other relevant/related articles that have been 
patented prior to the invention of the art icle, which 
is yet to be patented and is the object of the patent 
document.  The ref erences are made primarily by 
citing the name of the prior art patentees and the 
titles of their patent documents.  A patent doc u-
ment also has other linguistic descriptions of prior 
art; such descriptions are reminiscent of citations 
of journal papers in a  journal paper. The overlap of 
a new patent document with a set of existing patent 
documents may suggest the impact of extant 
knowledge in patent documents on emerging 
knowledge in the new patent document.  Such an 
overlap has been studied by the impact of  US 
semiconductor technology on the rest of the world 
(Appleyard and Kalsow: 1999): this overlap relies 
largely on the fr equency of citation of a US patent 
by the name of its author or the author?s place of 
work.  In computational linguistic (CL) terms thi s 
exercise relies on proper noun extra ction. 
The patent document relates to an explicit and 
exclusive right over an intellectual property.  A 
journal article relates to an implicit and i nclusive 
  
right over an intellectual property.  The overlap 
between these two forms of claims is crucial not 
only in ascertaining the rights of the patentee, or 
the abuse of the rights of others by the pa tentee, 
but also for monitoring the effectiveness of r e-
search based on a specialism as a whole or that of 
its component gro ups. 
The effect of one author or a group of authors 
working in an institution is indirectly mea sured by 
the so-called impact factor .  This factor relates to 
the frequency of citation of one or more journal 
papers written by an author or by a group.  The 
calculation of the impact fa ctor relies mainly on 
computing the frequency of the authors? name(s) 
within a corpus of journal articles.  Such an impact 
factor type calculation is used typically in bibl i-
ometrics (Garfield 1995).  Again, as in intra -patent 
impact studies mentioned above, in CL terms this 
is an exercise in proper noun identification and 
extraction. 
The analysis of a patent document, together 
with the analysis of the related corpora of other 
patent documents and intellectual property doc u-
ments, should be based on a framework which 
provides methods and tec hniques for analysing the 
contents of the document and of the corpora.  For 
us the source of a framework still lies in li nguistic 
and language studies.  Here we are pa rticularly 
interested in word formation and terminology u s-
age in highly specialised  disc iplines particularly 
those disciplines that deal with inta ngible articles 
coupling the word formation and terminology u s-
age with the citation patterns of proper nouns 
brings us closer to analysing the contents of a pat-
ent document and its siblings distributed over co r-
pora. 
Information scientists usually use the referen c-
ing data of research documents to analyse know l-
edge evolution in scientific fields as well as to 
identify the key authors, institutes , and journals in 
specific domains, using tools such as publication 
counts, citation analysis, co -citation analysis, and 
co-term analysis to do so.  In recent years, patent 
documents have gained considerable attention as a 
valuable resource that can be use d to analyse tec h-
nology advances using the same tools.  
Gupta and Pangannaya (2000) have applied 
bibliometric analysis to carbon nanotube patents to 
measure the growth of activity of carbon nan otube 
industries and their links with sc ience. They have 
also used patents data to study the country -wise 
distribution of patenting activity for the USA, J a-
pan, and other countries. Sector -wise performances 
of industry, academia and government, and the 
active players of carbon nanotubes were also stu d-
ied.  They describe the nature of inventions taking 
place in this particular field of technology, and the 
authors claim to have identified the emerging r e-
search directions, and the active companies and 
research groups involved.  
Meyer (2001) has used citation anal ysis 
and co-word analysis of patent documents and sc i-
entific literature to explore the interrel ationship 
between nano-science and nano -technology. Meyer 
investigated patent citation relations at the orga n-
izational levels along with geographical locations 
and affiliations of inventors and a uthors. The term 
co-occurrence  is used by Meyer to find the rel a-
tionship between the patent documents and the two 
scientific literature databases SCI and INSPEC. He 
has noticed that ??the terms that occur frequently 
in the document titles of all databases are related to 
[?] instrumentalities and/or are located in fields 
that are generally associated with substantial indu s-
trial research activity? (2001:177). Meyer has a r-
gued that ?Our data suggests that nano -technology 
and nano-science are essentially separate and he t-
erogeneous, yet interrelated cumulative stru ctures? 
(2001:164). 
 The study of word formation through n e-
ologisms within the special language of science 
and techno logy has led some authors to argue that 
it is the scientists as technologists who attempt to 
rationalise our experience of the world around us 
in written language by using new words or forms 
or by relexicalising the existing stock (see Ahmad 
2000 for relevant references).  Some lexicogr a-
phers (see for example Quirk et al 1985) have su g-
gested that neologisms can be formed by two 
processes:  First, the addition or combination of 
elements such as compounding: Resonant Tunne l-
ing Diodes  and Scanning tunneling microscopy  are 
examples for this type of neologism (compoundin g 
as a neologism formation is used extensively in 
science and technology literature); Second, the r e-
duction of elements into abbreviated forms. The 
abbreviations FET (Field E ffect Transistor) and 
MOSFET (Metallic Oxide Semiconductor FET) 
are examples of this type. 
Neologisms appear to signal the eme rgence of 
new concepts or artefacts and the frequency of this 
new word might indicate the scientific comm u-
  
nity?s acceptance of this new concept or artefact.  
Effenberger (1995) has argued that ?? the faster a 
subject field is developing, the more novelties are 
constructed, discovered or created. And these no v-
elties are talked and written about. In o rder to 
make this technical communication as efficient as 
possible, provision should be made for avoiding 
misunderstanding. One crucial point in this process 
is the vocabulary that is being used? (1995:131, 
emphasis added).  
In this paper we discuss the idiosyncratic la n-
guage used in patent documents.  The language is 
replete with terms and there are instances within a 
patent document that suggest that the authors not 
only use the specialist terms but use a local syntax 
as well.  We look specifically at the structure of the 
US Patents and suggest how with existing tec h-
niques used in information extraction and NLP, 
including term extraction and proper noun identif i-
cation, one can perform fairly complex tasks in 
patent analysis ? some of which are performed by 
patent experts by hand currently (Section 2).  This 
examination suggests to us a model of develo p-
ment in computer and  semi -conductor technology: 
an incremental model where each subsequent pa t-
ent helps in the development of ever -complex art i-
facts ? starting from devices onto circuits and onto 
systems.  We will look at one of the key i nventions 
in the field of semiconducto rs physics ? the elec-
tron tunneling device . These devices co mbine 
technical elegance, experimental complexity and 
manufacturing challenge.  Due to its strategic i m-
portance, a number of patents have been o btained 
by the US government and also by a nu mber of US 
and Japanese companies (Section 3).  Section 4 
concludes this paper.  
 
The Structure of US PTO Doc uments 
and a Local Grammar for the Docu-
ments 
The USPTO database is a representative sa mple of 
patent documents. The USPTO has documents r e-
lated to most bra nches of science and technology. 
It includes information about all US patent doc u-
ments since the first pa tent issued in 1970 to the 
most recent. The USPTO database a llows the user 
to search the full text of the patent documents for a 
certain word or a co mbination of words. It also 
provides a field search for specific information 
such as inventor or assignee . The search can also 
be conducted for a sp ecific year or range of years.  
The US Patents are written partly as a legal text 
and partly as a scientific d ocument.  Over the last 
50 years or so, it appears that US Patent doc uments 
have been structured in terms of layout and have a 
superficial resemblance to Marvin Minsky?s 
frame-like knowledge represe ntation schema.   
The patent document can be divided into 
three main parts for the present discussion: The 
first part  comprises the biographical details of the 
inventors (and their employers) together with the 
title of the invention and a brief free -text abstract, 
dates when the patent was applied for and when 
the patent was granted and so on.  The free text is 
essentially a summary of the claims of the pa t-
entee; The second part  contains external refe rences 
of three sorts: the first sort is the specialist domain 
of the invention ? the subject class indica ting the 
super-ordinate class and instances; the se cond sort 
are other cited patents organised as a 4 -tuple: (i) 
patent number, (ii) date of approval, (iii) first i n-
ventor and (iv) classification number; and, the third 
sort is a bibliographic reference to public ations that 
may have contributed to the pa tent;  The third part 
of a current US Patent document co mprises 
?claims? related to the patent and the d escription of 
the ?invention? (there are diagrams of the inve ntion 
attached to the document and the diagrams d e-
scribed in the text). Table 1 on the next page shows 
the template of the current (c. 1980 and a fter) 
USPTO?s. 
The ?claims? of the patentees are clearly 
itemised and initialised by the number of the claim; 
the first claim is the basis of the patent abstract 
generally.  The ?background to the invention? is 
written in an idiosyncratic fashion as well ? the 
invention is first contextualised in a broader group 
of other inventions to date and then the specific 
nature of the invention is e xemplified.  The 
broader and the specific are usually marked by 
phrases like ?The (present) invention relates to? 
and the specificity is phrased as ?(more) specif i-
cally.? or ?(more) pa rticularly?.  These phrases are 
followed by one or more noun phrases connected 
with, for example, c onjunctions or qualifiers.  The 
first noun phrase names the article i nvented, for 
instance, a name of a new device, circuit or a fabr i-
cating or testing pro cess. 
 
  
 
 
 
 
FIELD VALUE 
United States Patent Number  NUMBER  
First Inventor  PROPER NOUN ET AL. 
Date Patent Approved  DATE 
Title: FREE TEXT 
Abstract:  FREE TEXT  
Inventors:  PROPER NOUNS 
Assignee: PROPER NOUNS 
Application No.:  NUMBER 
Filed: DATE 
Patent Classification Data:  NUMBER  
References Cited [Refe renced By]:  [PATENT NUMBER, DATE, FIRST INVENTOR, 
CLASS NO.] 
Parent Case Text:  
CROSS REFERENCE TO RELATED APPLICATION  
FREE TEXT 
Claims:  ?What is claimed is: ?  
CLAIM 1:  
CLAIM 2:  
FORMULAIC FREE TEXT 
FORMULAIC FREE TEXT     
Description  
BACKGROUND OF THE INVENTION   
1. Field of the Inve ntion:  FORMULAIC FREE TEXT   
2. Related Background Art:  FORMULAIC FREE TEXT   
SUMMARY OF THE INVENTION:  SEMI FORMULAIC FREE TEXT   
BRIEF DESCRIPTION OF THE DRAWINGS:  FREE TEXT  
DETAILED DESCRIPTION OF THE PREFERRED 
EMBODIMENTS:  
FREE TEXT.   
Table 1: A slot -filler template of the US PTO a pproved patent documents.  
 
 
The NP comprises d eterminers and modal verbs 
together with (compound) nouns.  The first NP is 
optionally followed by a qualification that restricts 
or extends the scope of the disco very ? the 
enlargement or restriction is named and another 
NP is used for the naming and so on.  This simple 
grammar can be verified by exa mining a corpus of 
patent documents.  To illu strate this point we have 
looked at a recent randomly selected patent on 
memory devices  ? a patent filed by Kabushiki Ka i-
sha Toshiba  of Japan (or Toshiba for short), and 
approved by USPTO on 20 th May 2003, on a sem i-
conductor memory device which uses the emergent 
notion of memory cells (a memory cell is  a tiny 
area within the memory array tha t actually stores 
the bit in the form of an electrical charge 1).  An 
analysis of the title and that of the ?Background of 
the Invention: Field of I nvention? fields shows the 
use of this restricted syntax (Table 2).  In much the 
                                                           
1 Definition form http://rel.intersil.com/docs/lexicon/M.html , site 
visited 29 May 2003) 
same as the ?claims? and ?th e ?background?, the 
?summary of the invention? is also phrased in a 
formulaic manner (see Table 1 for the structure of 
the patent document).  
The analysis of the other slots governed by 
a simpler grammar yields interesting results and 
suggests that the name s of assignees and the ma n-
ner in which patents are being cited can be easily 
inter-related (Table 3).  Toshiba?s USPTO 
6567330 refers to 8 other patents.  The details of 
the referenced patents are in a 4 -tuple, which can 
be unambiguously interpreted.  Each  of the refe r-
enced patents refers to about 10 patents in turn.  
An examination of 82 such patents may help to 
initiate, perhaps, a discussion of the ?invention life 
cycle? or ?licensing potential of a patent? (Mogee 
1997), or even a discussion of ?micro fo undations 
of innovation systems? (Ande rsen 2000).   
 
  
Title 
of the Patent 
US PTO 
Number 
Field of  Invention 
Semiconductor  memory 
device  
6567330 The present invention relates to a semi-
conductor  memory device with a current-read-
type memory cell [?] 
More specifically, the present invention 
relates to a data sense circuit for the 
semiconductor memory device. 
Patents cited by USPTO 6567330 
Nonvolatile semiconduc-
tor memory device 
6407946 The present invention generally relates to a 
nonvolatile semiconductor memory device, 
and more particularly relates to an electri-
cally erasable and programmable read 
only memory  
Semiconductor memory 
device 
6337825 This invention relates to a semiconductor 
memory device, 
and more particularly to a sense amplifier 
of a nonvolatile semiconductor memory 
using current read-out type memory 
cells. 
Memory cell sense ampli-
fier 
6219290 The present invention relates to memory ar-
rays,  
and in particular, the sensing of data 
from a non-volatile memory cell. 
Current conveyor and 
method for readout of 
MTJ memories 
6205073 This invention relates to M[agnetic] 
T[unneling] J[unction] memories  
and more particularly, to apparatus and 
a method for reading data stored in 
MTJ memories. 
Read reference scheme for 
flash memory 
6038169 This invention relates to flash memory and in particular to creating a reference 
by which to read the state of flash mem-
ory cells. 
Sensing circuit for a float-
ing gate memory device 
having multiple levels of 
storage in a cell 
5910914 The present invention relates to a sensing cir-
cuit for use with a memory array comprised of 
floating gate devices, [..]. 
More particularly, the present invention 
relates to the use of a plurality of invert-
ers to compare the current from a ref-
erence cell [?] 
Flash memory device 
having a page mode of 
operation 
5742543 The present invention relates generally to 
memory devices 
and more particularly to a nonvolatile 
memory device having a page mode of 
operation. 
Single cell reference 
scheme for flash memory 
sensing and program state 
verification 
5386388 The invention relates to the field of metal-
oxide semiconductor (MOS) [..]EPROMs [..] 
particularly to the field of "flash" 
EPROMs [..] 
Table 2: The use of restricted syntax in the d escription of the generic and specific fields of inventi on.  The 
higher patent number shows that it was filed at a later date than a lower patent number.  So, the above 
figure shows a time o rder as well. 
 
Assignee Country 
Patent  
Number 
USPTO 
Class 
Approval 
Date 
(a) 
Earliest 
Reference 
(b) 
Latest 
Reference 
(c) 
Invention 
Cycle Time? 
(a) ? (c) 
Invention 
Cycle Time?? 
(b) ? (c) 
Toshiba Japan 6567330 365/210 May-03 Jan-95 Jun-02 1.0 6.5 
 Patents cited by USPTO Number 6567300   
Matshushita Japan 6407946 365/185 Jun-02 Jun-93 Nov-99 2.5 6.3 
Toshiba Japan 6337825 365/185 Jan-02 Nov-92 Aug-00 1.5 7.3 
Macronix Taiwan 6219290 365/185 Apr-01 Aug-93 May-98 3.0 4.8 
Motorola US 6205073 365/171 Mar-01 Jun-98 Aug-00 0.5 2.1 
Halo LSI US 6038169 365/180 Mar-00 Dec-92 Aug-99 0.8 6.8 
Silicon Storage US 5910914 365/185 Jun-99 Sep-80 Jun-97 2.0 17.0 
Intel US 5742543 365/185 Apr-98 Nov-96 May-80 1.5 19.5 
Intel US 5386388 365/185 Jan-95 May-72 Dec-92 2 19.5 
Table 3: A glimpse of the technology transfer in the Toshiba patent for ?data sensing circuits? for sem iconductor 
memory devices.  The US Patent Classification 365 refers to ?Static Information Storage and Retrival,  and the 
subclassifcations 185 & 171 refer to ?Floating Gate Memories? & ?Magnetic Thin Films?  
  
A finer grained analysis to show which 
?country? is more influentia l can also be performed 
fairly readily and indicates the extent to which pa t-
ents that are held by assignees domiciled in the 
USA have over half the cited patents (Table 4).  
 
Assignee 
Country # % 
Assignee 
Country # % 
US 45 54.9% Korea 2 2.4%
Japan 18 22.0% France 1 
Independent 7 8.5% Germany,  1 1.2%
Italy 5 6.1% UK 1 
Taiwan 2 2.4% TOTAL 82 100
Table 4: An analysis of USPTO No. 6567330 (T o-
shiba Japan) shows the major influence of US -
based assignees, followed by Japan.  A significant 
number of patents  (8.5%) are held by individuals 
and not assigned specif ically to a country.  
 
A semi-automatic analysis of terms used in 
the Abstracts and Titles of the patents (Toshiba 
6567330 and patents referenced in the Toshiba 
patents) shows the co -citation pattern o f terms.  
This may help in the clu stering of patents on the 
basis of terms extracted from the patent doc uments 
as well as novel terms (terms not included in the 
USPTO Patent Class ification terminology data 
base) found in the doc ument.  We show the co -
citation of the two key terms memory cell  and 
memory device in the nine patents discussed above.  
The use of the two terms individually and as roots 
and stems of other compounds is also shown.  The 
more frequent citation is to the newer term memory 
cell and it is cited in all but one of the 9 related 
patents.  The related memory devices ? newer de-
vices now incorporate memory cells ? is less fre-
quently used and it is only found in the abstracts of 
5 out of the 9 patents.  Both terms are co -cited in 6 
out of the 9  patents (see Table 5 for details).   
The interrelationship between the different 
patents can be explored further by examining 
closely as to what is being patented within the pa t-
ent and what is being patented in the referenced 
patents.  Again, we use the e xample of the Toshiba 
patent No. 6567330 which refers to 8 other patents.  
The patent itself relates to the invention of a sys-
tem.  The referred patents relate to other systems 
and circuits.  Let us look at the earliest patent cited 
in Toshiba?s patent: th is is US PTO No. 5386388 
filed by Intel Corporation (USA) approved in 
January 1995.  The title of I ntel?s patent is ? Single 
cell reference scheme for flash memory sensing 
and program state verific ation?.  Flash memory is 
defined as ?A nonvolatile programma ble semicon-
ductor memory product  2.  This patent r elates to the 
invention of a circuit.  Intel?s patent comprises re f-
erences to another 15 patents: 5 refer to other sy s-
tems, 8 to ci rcuits, and one each to a device and a 
software program (see Figure 1 on th e next page).  
The information whether a patent is r elated to any 
of the four classes can be gleaned from the Patent 
Classification Number.  Further analysis of the 
referenced patents shows a similar pattern ? refe r-
ences to circuits, devices, systems and s oftware.  
This appears to be a basis of the inventions within 
the semiconductor industry, especially those r e-
lated to the development of co mputer systems 
based on these systems, d evices and circuits.  This 
is the basis of our more speculative investig ations 
related to the resonant tunneling systems.  
 
Patent  
No. 
Freq. Compound  
Term 
Freq. Compound  
Term 
 Mem-
ory 
Cell (m.c.)  Mem-
ory 
Device (m.d.)  
6567330 4   3 semicond. 
+m.d.(3) 
6407946 2 m.c. 
+transistor(2)  
1 non-volatile 
 semicond. 
+m.d.(1) 
6337825     2 semicond.  
+m.d.(2) 
6219290 3 m.c. +sense 
amplifier (1) 
    
6205073         
6038169 3 flash +m.c. 
(1); m.c. cur-
rent (2) 
    
5910914 2   2 Floating  gate 
+ 
m.d. (2)  
5742543 3   1 flash +m.d. 
(1) 
5386388 1       
Total 18  9  
Table 5.  Distribution o f the two co-cited terms in 
the nine patents.  The frequency of the compound 
terms is included in the frequency count.  
 
  
                                                           
2From http://www.micron.com/, site visited 29 May 
2003  
  
  
 
 
Semiconductor Memory Device
US Patent 6567330
_______________________
SYSTEM
US Patent
5386388
_________________
CIRCUIT
US Patent
3660819
_________________
DEVICE
US Patents
3339086-3500142
_________________
DEVICES
US Patents
3755721-4203158-
4460982-4763305-
5043940
_________________
SYSTEMS
US Patents
4099196-4100437-
4223394-4287570-
4943948-5031145-
5163021-5172338
_________________
CIRCUITS
US Patent
4875188
_________________
SOFTWARE
US Patents
5742543-6337825-
6407946
_________________
SYSTEMS
US Patents
5910914-6038169-
6205073-6219290
_________________
CIRCUITS
Figure 1: A hierarchical citation -based ordering of 
patents and the distribution of patents into three 
categories ? systems, circuits and devices . 
 
3 The Evolution of the Resonant 
Tunneling Devices  
We will now focus on how terminology u sage may 
help in tracking the evolution of resonant tunne l-
ing devices.  These are ultra high -speed devices, 
which perhaps will be used in the compute rs of the 
next decade or so.  In order to study how one can 
track technology progress we have adopted an i n-
tuitive, but realistic, framework. For us, all co m-
plex systems comprise subsystems and subsystems 
are made up of much smaller (and simpler) de-
vices.  A computer system is made up of i ntegrated 
circuits and the circuits made up of transistors and 
transistors come in di fferent types. One model of 
growth can be thought of as follows: First, devices 
are patented, then su bsystems, and finally the 
complex systems (remember only tangible articles 
can be patented). So fo llowing this intuitive 
framework we will first see a number of devices 
being patented then subsystems and finally the sy s-
tems themselves.  Tunnel diodes are supposed to 
empower faster switching devices, which in turn 
have to be incorporated into subsystems with tu n-
neling transistors and into complex systems with 
circuits.  Our hypothesis is that an analysis of a 
diachronically organized text corpus will show the 
working of the above -mentioned framework. 
A corpus was built containing more than 
2.2 million words of patent documents.  The co r-
pus contains all patent documents that co ntain the 
term tunneling  in the title. USPTO search r esults 
showed that there are 372 titles, approved from 
1975 to 1999 in semiconductor physics.  We have 
analysed frequency of compound word in the 
USPTO patent documents published b etween 
1975-1999 (Table 6).  
 
 75-79 80-84 85-89 90-94 95-99 
No.of 
Texts  
7 8 68 133 156 
Total No. 
of tokens 
43812 43262 378272 771525 995894 
Table 6.  The diachronic breakdown of patents 
comprising at least one instance of the token tun-
neling over 5 year intervals between 1975 -1999. 
 
The compound word analysis was co n-
ducted using System Quirk and no compounds 
were pre spec ified (System Quirk a text analysis 
system, is avai lable on 
www.computing.surrey.ac.uk/ai/SystemQ ). The 
system extracts compound words based on a si m-
ple heuristic: a set of word that does not co ntain 
closed class words (i.e. determiners, conjun ctions, 
prepositions, and moderators) or the orthographic 
signs (including pun ctuation, numbers, currency 
and other symbols) is considered by Sy stem Quirk 
to be a compound word (see Ahmad and Rogers, 
2001, for details). The va lidation of compound 
words can also be carried out by statistical tests, 
for instance described by Smajda (1994).  
To investigate the progress of resonant 
tunneling devices and circuits, the multi -word 
terms were extracted from the USPTO full text 
corpus using System Quirk. The extracted terms 
that relate to resonant tu nneling diodes, resonant 
tunneling transistors and resonant tunneling ci r-
cuits were arranged in a five year interval starting 
from the first emergence of the term resonant tun-
neling in USPTO abstract documents in 1985.   
Tracking the frequency usage of the terms 
associated with resonant tunneling artefacts in the 
USPTO full text corpus shows a considerable i n-
  
crease of frequency usage i nterval by interval.  The 
frequency of the term resonant tunneling diode 
(and its plural form resonant tunneling diode s, 
both denoted as the lemma resonant tunneling d i-
ode~ subsequently) increased significantly from 45 
in 1985-1989 to 446 in 1990 -1994  by about a fa c-
tor of 19 and then in the next time interval 1995 -
1999 the frequency dropped by about half to 240. 
The frequency usage of the term resonant tunne l-
ing transistor~  in the USPTO full text corpus i n-
creased from 23 in the p eriod 1985-1989 by about 
a factor of 10 to 225 in 1990 -1994. The increase of 
frequency usage of the term in the time period 
1995-1999 increased by a factor of 1.3 to become 
293. The term resonant tunne ling circuit~ appears 
in the USPTO full text co rpus 45 times in the time 
interval 1990 -1994. Frequency usage of resonant 
tunneling circuits inc reased by a fa ctor of 1.3 in the 
next interval (1995 -1999) to 57.  
Word formation is not restricted to the i n-
flection of a compound word. Rather, we see fu r-
ther instances of compounding where an existing 
compound, say, resonant tunneling d i-
ode/transistor  is  used as a head of other co m-
pounds (Table 7).  
 
1990-1994 1995-1999 
barrier resonant tunne l-
ing diode 
triple barrier resonant 
tunneling diode  
band resonant tunne ling 
transistor~ 
bipolar quantum reso-
nant tunneling transi s-
tor 
Table 7.  The specialization, t hrough prefixation, 
of the term resonant tunneling d iode & transistor  
over a 10 year period in our patent corpus  
 
We note the very productive use of compoun ding 
and inflection in our corpus.  Note, ho wever, that 
the size of the corpus for the three differe nt peri-
ods, 1985 -89, 90-94 and 95-99, are different: 
378272, 771525 and 995894 respectively.  The 
size of the corpus perhaps for the later two p eriods 
is roughly the same but the earlier corpus (85 -89) 
is three times smaller.  In order to pr esent a better 
comparison we will look at the relative frequency 
of the compounds in that we will sum up the fr e-
quency of all the extracted compounds related to 
resonant tunneling diodes, transistors and circuits, 
as per our intuitive fram ework, and assign relative 
frequency to each of the three relative to the sum.  
Consider the result of analysis of 133 texts 
of patents published in 1990 -1994 for tunnel diode 
related patents.  The total number of terms co m-
prising the lemma resonant tunneling diode~ is 
490, which includes  the lemma on its own and two 
terms containing the lemma as the headword; these 
are multiple peak resonant tunne ling diode, barrier 
resonant tunneling d iode.  The total containing the 
lemma resonant tunneling transistor  is 225, which 
is made up of 188 for the lemma on its own and the 
rest for the two other terms.  The lemma RT circuit  
also includes hyponyms of the term, e.g. RT oscil-
lator (circuit), RT  logic gate (circuit)  and RT mem-
ory (circuit) ; note that the term circuit is shown in 
parentheses as it is ellipsed in the text ? the reader 
of the patents, an expert in the disc ipline, is 
expected to know that an oscillator is a circuit.  
The two terms occur 24 and 12 times t ogether with 
4 other terms that collectively occur 9 times ma k-
ing a total of 45.  The three lemmas RT diode, 
transistor and circuit occur for a total of 490 + 225 
+ 45 (= 760) times, hence the relative fr equency of 
the three lemmas is 64.4% (490/760), 29.6% 
(225/760) and 6% (45/760) respectively (Table 8 
shows a brea kdown of the distributio n). 
This relative frequency computation was 
conducted over the periods 1985 -1989 and 1995 -
1999.  Table 9 (on the next page) shows that over 
64% of the terms belong to the lemma resonant 
tunneling diode ~, about 30 % to resonant tunne ling 
transistor and just about 6% to  resonant tu nneling 
circuit ~.  This situatio n changes quite dramatically 
in the next quinquennium (1995-1999).  
 
  
Artefact 1990-1994 Freq % 
resonant tunneling diode~ 446  
multiple peak resonant tunneling 
diode 24 
 
Resonant 
tunneling 
diodes 
barrier resonant tunneling diode 20  
Total 490 64.4% 
resonant tunneling transistor~ 188  
band resonant tunneling transistor~ 35  
Resonant 
tunneling 
transistors bipolar quantum resonant tunneling 
transistor~ 2 
 
Total 225 29.6% 
resonant tunneling oscillator~ 24  
resonant tunneling logic gate~ 12  
resonant tunneling diode memory 3  
resonant tunneling diode oscillator 3  
multiple resonant tunneling circuits 2  
Resonant 
tunneling 
?Circuit~? 
resonant tunneling photodetector 1  
Total 45 6% 
Table 8: Resonant tunneling  artefacts in the 
USPTO full text corpus in the time period 1990 -
1994. 
 
Period Compound term 85-89 90-94 95-99 
RT diode~ 66.2% 64.4% 41.2% 
RT transistor~ 33.8% 29.6% 49.1% 
RT circuit~ 0 6% 9.7% 
Total 100% 100% 100% 
Table 9.  The growth of compound terms compri s-
ing the headwords diode & diodes denoted collec-
tively as diode~, transistor~ , and circuit~, together 
with the stem resonant tunneling (RT).  
 
4 Afterword 
It appears that there is a local grammar , compris-
ing vocabulary of t he specialist domain and a sy n-
tax that appears different from the general 
(universal?) syntax, used in framing the claims, 
background and su mmary of the invention in a US 
Patent document.  A number of slots in the US 
PTO document are reserved for proper na mes ? 
patentees, assignees, places of work, and other 
slots hold dates and all these slots show the e x-
tremes of the local grammar ? essentially a gra m-
mar for a one-word language.  The document 
comprises ?references to (other patents) and also 
citations to an extant by other later patents ? this 
information is encoded in another local grammar 
of one or more 4 -tuples referring to a referenced 
patent ? the 4-tuple has a clearly defined s equence 
and allows expressions only in terms of four noun -
phrases.  The referenced patent number is an active 
hyperlink through which the details of the refe r-
enced patent can be a ccessed and subsequently a 
chain of references can be established in a (semi -) 
automatic manner.  The existence of a local gra m-
mar and the hyperlinks s uggests to us that one can 
create a historic (diachronic) description of an 
invention together with the crucial account of the 
influence of other inventions.   
Restricted syntax is used, for example, in 
describing time (hours, minutes, seconds, days, 
years, months), in financial news wire as well as 
mission-critical communication.  The sp ecialist 
vocabulary, and more so the productive use of the 
vocabulary (see below for details), as well as the 
restricted syntax emerges initially for assuring a m-
biguity-free communication in an inherent noisy 
medium of communication ? natural language.   
Complementary to the emergence of the 
present US patent document, there has been an a c-
cumulation of terminological knowledge in terms 
of the repositories usually referred to  as patent 
classification .  The Patent Offices around the 
world classify all manners of ?art icles? ranging 
from micro -electronics to kitchen utensils and from 
software systems to heavy excavation machinery, 
for example.  Much like a number of other utilita r-
ian class ification systems, including the Dewey 
Decimal Classification on the one hand and the US 
National Library of Medicine?s Disease Classific a-
tion system on the other, the US PTO classification 
system is detailed, complex, full of cross refe r-
ences, and occasionally confusing.  The fact r e-
mains, however, that like all utilita rian systems, the 
US PTO classification system is a rich repository 
that can be used, with some alterations, as the lex i-
cal/terminological resource for information extra c-
tion in particular and NLP in general.  The 
repository states the ontological commitment of the 
US PTO and its advisers, and can be used for 
building knowledge representation schema or s e-
mantic processing sy stems. 
The appearance of a local grammar, or 
perhaps local grammars, used to frame a patent 
document together with an extensive terminology 
database of patent class ification, is good news for 
the patent processing comm unity.  There is some 
hope that the information extraction and NLP sy s-
tems will be able to extrac t the terminology and 
identify the idiosyncratic syntax that governs the 
  
different parts of the patent document with the help 
of techniques pioneered in corpus linguistics.  
Terminology extraction can be facilitated by refe r-
ring to the patent classific ation terminology base 
and facilitated by various statistical and linguisti c 
techniques used to identify complex noun -phrases 
in specialist texts.  Once the local grammar is ide n-
tified it will be able to meaningfully process the 
documents for inferring the imp ort of a given i n-
vention in relation to other inventions and to assess 
the impact of journal publications of inventions.  
And, indeed all manner of new ways of examining 
a patent document may open up once the investig a-
tor overcomes the burden of sifting th rough an 
overgrowing lexical mountain of new patents, rev i-
sions to exis ting patents and the scientific and 
technical publication juggernaut that adds more to 
the mountain on almost daily basis.  The aut omatic 
extraction of compounds from a corpus of patent  
documents appears to show the introduction of 
new artifacts through the use of morphological 
processes like word formations.  Cu rrently, our 
work in progress is to ?chart? a transfer of such 
terms in journal papers onto patents, in a ddition to 
the exercise reported which charts the transfer of 
terms within a diachronically organised corpus of 
patent documents. 
References 
Ahmad, K. 2000. Neologisms, Nonces and Word Fo r-
mation. Proceedings of the Ninth EURALEX Intern a-
tional Congress  (Munich August 2000).pp 71 1-729. 
Ahmad, K. and Rogers, M. 2001. Corpus Lingui stics 
and Terminology Extraction. Handbook of Termino l-
ogy Management . Amsterdam: John Benjamins Pu b-
lishing Co.  pp725 -760. 
Andersen, B.  (2000). Technological change and the 
evolution of corporate patentin g: The structure of 
patenting 1890 -1990.   Cheltenham: Edward E lgar. 
Appleyard, M.M. and G.A. Kalsow. 1999. ?Knowledge 
diffusion in semiconductor indu stry?.  Journal of 
Knowledge Management .  Volume 3 (No. 4).  pp 
288-295. 
Effenberger, D. 1995. Fundamental s of Termino logy 
Work. Computer Standards & Interfaces , Vol. 17, 
131-137. 
Garfield, E.1995 The Impact of Cumulative Impact Fa c-
tors. Proceedings of the 8th IFSE Conference, Barc e-
lona, pp58-81. 
Gupta, V. K. and Pangannaya, N. B. 2000. Carbon 
Nanotubes: Bibli ometric Analysis of Patents. World 
Patent Information  22: 185 -189. 
Meyer, M. 2001. Patent Citation Analysis in a Novel 
Field of Technology: An Exploration of Nano -
Science and Nano -Technology. Scientometrics  
51.1:163-183. 
 
Mogee, Mary E.  (1997).  ?Patent A nalysis Methods in 
Support of Licensing?.  Paper presented at the Tech-
nology Transfer Society Annual Conference (De n-
ver, USA).  (http://www.mogee.com/services/tl -
methods.html, site visited 20 M ay 2003).   
Quirk, R, S Greenbaum, G Leech, J Svartvik. 1985. A 
Comprehensive Grammar of the En glish Language . 
London and New York: Lon gman 
Smajda, F. 1994. ?Retrieving Collocations from Text: 
Xtract.?. In (Ed.) Susan Armstropng, U sing Large 
Corpora Ca mbridge, MA/London/England: MIT 
Press. pp 143 -177. 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 984?991,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Sentiment Polarity Identification in Financial News:
A Cohesion-based Approach
Ann Devitt
School of Computer Science & Statistics,
Trinity College Dublin, Ireland
Ann.Devitt@cs.tcd.ie
Khurshid Ahmad
School of Computer Science & Statistics,
Trinity College Dublin, Ireland
Khurshid.Ahmad@cs.tcd.ie
Abstract
Text is not unadulterated fact. A text can
make you laugh or cry but can it also make
you short sell your stocks in company A and
buy up options in company B? Research in
the domain of finance strongly suggests that
it can. Studies have shown that both the
informational and affective aspects of news
text affect the markets in profound ways, im-
pacting on volumes of trades, stock prices,
volatility and even future firm earnings. This
paper aims to explore a computable metric
of positive or negative polarity in financial
news text which is consistent with human
judgments and can be used in a quantita-
tive analysis of news sentiment impact on fi-
nancial markets. Results from a preliminary
evaluation are presented and discussed.
1 Introduction
Research in sentiment analysis has emerged to ad-
dress the research questions: what is affect in text?
what features of text serve to convey it? how can
these features be detected and measured automati-
cally. Sentence and phrase level sentiment analy-
sis involves a systematic examination of texts, such
as blogs, reviews and news reports, for positive,
negative or neutral emotions (Wilson et al, 2005;
Grefenstette et al, 2004). The term ?sentiment
analysis? is used rather differently in financial eco-
nomics where it refers to the derivation of market
confidence indicators from proxies such as stock
prices and trading volumes. There is a tradition
going back to the Nobel Sveriges?Riksbank Laure-
ates Herbert Simon (1978 Prize) and Daniel Kah-
neman (2002 Prize), that shows that investors and
traders in such markets can behave irrationally and
that this bounded rationality is inspired by what the
traders and investors hear from others about the con-
ditions that may or may not prevail in the markets.
Robert Engle (2003 Prize) has given a mathematical
description of the asymmetric and affective impact
of news on prices: positive news is typically related
to large changes in prices but only for a short time;
conversely the effect of negative news on prices and
volumes of trading is longer lasting. The emergent
domain of sociology of finance examines financial
markets as social constructs and how communica-
tions, such as e-mails and news reports, may be
loaded with sentiment which could distort market
trading (MacKenzie, 2003).
It would appear that news affects the markets
in profound ways, impacting on volumes of trade,
stock returns, volatility of prices and even future
firm earnings. In the domain of news impact analy-
sis in finance, in recent years the focus has expanded
from informational to affective content of text in an
effort to explain the relationship between text and
the markets. All text, be it news, blogs, accounting
reports or poetry, has a non-factual dimension con-
veying opinion, invoking emotion, providing a nu-
anced perspective of the factual content of the text.
With the increase of computational power and lex-
ical and corpus resources it seems computationally
feasible to detect some of the affective content of
text automatically. The motivation for the work re-
ported here is to identify a metric for sentiment po-
984
larity which reliably replicates human evaluations
and which is readily derivable from free text. This
research is being carried out in the context of a study
of the impact of news and its attendant biases on
financial markets, formalizing earlier multi-lingual,
corpus-based empirical work that analysed change
in sentiment and volume of news in large financial
news corpora (Ahmad et al, 2006). A systematic
analysis of the impact of news bias or polarity on
market variables requires a numeric value for senti-
ment intensity, as well as a binary tag for sentiment
polarity, to identify trends in the sentiment indica-
tor as well as turning points. In this approach, the
contribution to an overall sentiment polarity and in-
tensity metric of individual lexical items which are
?affective? by definition is determined by their con-
nectivity and position within a representation of the
text as a whole based on the principles of lexical co-
hesion. The contribution of each element is there-
fore not purely additive but rather is mitigated by its
relevance and position relative to other elements.
Section 2 sets out related work in the sentiment
analysis domain both in computational linguistics
and in finance where these techniques have been
applied with some success. Section 3 outlines the
cohesion-based algorithm for sentiment polarity de-
tection, the resources used and the benefits of using
the graph-based text representation approach. This
approach was evaluated relative to a small corpus of
gold standard sentiment judgments. The derivation
of the gold standard and details of the evaluation are
outlined in section 4. The results are presented and
discussed in section 5 and section 6 concludes with
a look at future challenges for this research.
2 Related Work
2.1 Cognitive Theories of Emotion
In order to understand how emotion can be realised
in text, we must first have a notion of what emo-
tion is and how people experience it. Current cogni-
tive theories of what constitutes emotion are divided
between two primary approaches: categorical and
dimensional. The Darwinian categorical approach
posits a finite set of basic emotions which are expe-
rienced universally across cultures, (e.g. anger, fear,
sadness, surprise (Ekman and Friesen, 1971)). The
second approach delineates emotions according to
multiple dimensions rather than into discrete cate-
gories. The two primary dimensions in this account
are a good?bad axis, the dimension of valence or
evaluation, and a strong-weak axis, the dimension
of activation or intensity (Osgood et al, 1957). The
work reported here aims to conflate the evaluation
and activation dimensions into one metric with the
size of the value indicating strength of activation and
its sign, polarity on the evaluation axis.
2.2 Sentiment Analysis
Sentiment analysis in computational linguistics has
focused on examining what textual features (lexi-
cal, syntactic, punctuation, etc) contribute to affec-
tive content of text and how these features can be
detected automatically to derive a sentiment metric
for a word, sentence or whole text. Wiebe and col-
leagues have largely focused on identifying subjec-
tivity in texts, i.e. identifying those texts which are
affectively neutral and those which are not. This
work has been grounded in a strong human evalu-
ative component. The subjectivity identification re-
search has moved from initial work using syntactic
class, punctuation and sentence position features for
subjectivity classifiers to later work using more lex-
ical features like gradation of adjectives or word fre-
quency (Wiebe et al, 1999; Wiebe et al, 2005). Oth-
ers, such as Turney (2002), Pang and Vaithyanathan
(2002), have examined the positive or negative po-
larity, rather than presence or absence, of affective
content in text. Kim and Hovy (2004), among oth-
ers, have combined the two tasks, identifying sub-
jective text and detecting its sentiment polarity. The
indicators of affective content have been drawn from
lexical sources, corpora and the world wide web and
combined in a variety of ways, including factor anal-
ysis and machine learning techniques, to determine
when a text contains affective content and what is
the polarity of that content.
2.3 Sentiment and News Impact Analysis
Niederhoffer (1971), academic and hedge fund man-
ager, analysed 20 years of New York Times head-
lines classified into 19 semantic categories and on a
good-bad rating scale to evaluate how the markets
reacted to good and bed news: he found that mar-
kets do react to news with a tendency to overreact
to bad news. Somewhat prophetically, he suggests
985
that news should be analysed by computers to intro-
duce more objectivity in the analysis. Engle and Ng
(1993) proposed the news impact curve as a model
for how news impacts on volatility in the market
with bad news introducing more volatility than good
news. They used the market variable, stock returns,
as a proxy for news, an unexpected drop in returns
for bad news and an unexpected rise for good news.
Indeed, much early work used such market variables
or readily quantifiable aspects of news as a proxy for
the news itself: e.g. news arrival, type, provenance
and volumes (Cutler et al, 1989; Mitchell and Mul-
herin, 1994). More recent studies have proceeded
in a spirit of computer-aided objectivity which en-
tails determining linguistic features to be used to
automatically categorise text into positive or nega-
tive news. Davis et al(2006) investigate the effects
of optimistic or pessimistic language used in finan-
cial press releases on future firm performance. They
conclude that a) readers form expectations regard-
ing the habitual bias of writers and b) react more
strongly to reports which violate these expectations,
strongly suggesting that readers, and by extension
the markets, form expectations about and react to not
only content but also affective aspects of text. Tet-
lock (2007) also investigates how a pessimism fac-
tor, automatically generated from news text through
term classification and principal components analy-
sis, may forecast market activity, in particular stock
returns. He finds that high negativity in news pre-
dicts lower returns up to 4 weeks around story re-
lease. The studies establish a relationship between
affective bias in text and market activity that market
players and regulators may have to address.
3 Approach
3.1 Cohesion-based Text Representation
The approach employed here builds on a cohesion-
based text representation algorithm used in a news
story comparison application described in (Devitt,
2004). The algorithm builds a graph representa-
tion of text from part-of-speech tagged text without
disambiguation using WordNet (Fellbaum, 1998) as
a real world knowledge source to reduce informa-
tion loss in the transition from text to text-based
structure. The representation is designed within the
theoretical framework of lexical cohesion (Halliday
and Hasan, 1976). Aspects of the cohesive struc-
ture of a text are captured in a graph representation
which combines information derived from the text
and WordNet semantic content. The graph structure
is composed of nodes representing concepts in or de-
rived from the text connected by relations between
these concepts in WordNet, such as antonymy or hy-
pernymy, or derived from the text, such as adjacency
in the text. In addition, the approach provides the
facility to manipulate or control how the WordNet
semantic content information is interpreted through
the use of topological features of the knowledge
base. In order to evaluate the relative contribution
of WordNet concepts to the information content of a
text as a whole, a node specificity metric was derived
based on an empirical analysis of the distribution of
topological features of WordNet such as inheritance,
hierarchy depth, clustering coefficients and node de-
gree and how these features map onto human judg-
ments of concept specificity or informativity. This
metric addresses the issue of the uneven population
of most knowledge bases so that the local idiosyn-
cratic characteristics of WordNet can be mitigated
by some of its global features.
3.2 Sentiment Polarity Overlay
By exploiting existing lexical resources for senti-
ment analysis, an explicit affective dimension can
be overlaid on this basic text model. Our approach
to polarity measurement, like others, relies on a lex-
icon of tagged positive and negative sentiment terms
which are used to quantify positive/negative senti-
ment. In this first iteration of the work, SentiWN
(Esuli and Sebastiani, 2006) was used as it provides
a readily interpretable positive and negative polarity
value for a set of ?affective? terms which conflates
Osgood?s (1957) evaluative and activation dimen-
sions. Furthermore, it is based on WordNet 2.0 and
can therefore be integrated into the existing text rep-
resentation algorithm, where some nodes in the co-
hesion graph carry a SentiWN sentiment value and
others do not. The contribution of individual polar-
ity nodes to the polarity metric of the text as a whole
is then determined with respect to the textual infor-
mation and WN semantic and topological features
encoded in the underlying graph representation of
the text. Three polarity metrics were implemented
to evaluate the effectiveness of exploiting different
986
aspects of the cohesion-based graph structure.
Basic Cohesion Metric is based solely on frequency
of sentiment-bearing nodes in or derived from the
source text, i.e. the sum of polarity values for all
nodes in the graph.
Relation Type Metric modifies the basic metric
with respect to the types of WordNet relations in the
text-derived graph. For each node in the graph, its
sentiment value is the product of its polarity value
and a relation weight for each relation this node en-
ters into in the graph structure. Unlike most lexical
chaining algorithms, not all WordNet relations are
treated as equal. In this sentiment overlay, the rela-
tions which are deemed most relevant are those that
potentially denote a relation of the affective dimen-
sion, like antonymy, and those which constitute key
organising principles of the database, such as hy-
pernymy. Potentially affect-effecting relations have
the strongest weighting while more amorphous rela-
tions, such as ?also see?, have the lowest.
Node Specificity Metric modifies the basic metric
with respect to a measure of node specificity calcu-
lated on the basis of topographical features of Word-
Net. The intuition behind this measure is that highly
specific nodes or concepts may carry more informa-
tional and, by extension, affective content than less
specific ones. We have noted the difficulty of using
a knowledge base whose internal structure is not ho-
mogeneous and whose idiosyncrasies are not quanti-
fied. The specificity measure aims to factor out pop-
ulation sparseness or density in WordNet by evaluat-
ing the contribution of each node relative to its depth
in the hierarchy, its connectivity (branchingFactor)
and its siblings:
Spc = (depth+ln(siblings)?ln(branchingFactor))NormalizingFactor (1)
The three metrics are further specialised according
to the following two boolean flags:
InText: the metric is calculated based on 1) only
those nodes representing terms in the source text, or
2) all nodes in the graph representation derived from
the text. In this way, the metrics can be calculated
using information derived from the graph represen-
tation, such as node specificity, without potentially
noisy contributions from nodes not in the source text
but related to them, via relations such as hypernymy.
Modifiers: the metric is calculated using all open
class parts of speech or modifiers alone. On a cur-
sory inspection of SentiWN, it seems that modifiers
have more reliable values than nouns or verbs. This
option was included to test for possible adverse ef-
fects of the lexicon.
In total for each metric there are four outcomes com-
bining inText true/false and modifiers true/false.
4 Evaluation
The goal of this research is to examine the relation-
ship between financial markets and financial news,
in particular the polarity of financial news. The do-
main of finance provides data and methods for solid
quantitative analysis of the impact of sentiment po-
larity in news. However, in order to engage with
this long tradition of analysis of the instruments and
related variables of the financial markets, the quan-
titative measure of polarity must be not only easy
to compute, it must be consistent with human judg-
ments of polarity in this domain. This evaluation is
a first step on the path to establishing reliability for
a sentiment measure of news. Unfortunately, the fo-
cus on news, as opposed to other text types, has its
difficulties. Much of the work in sentiment analy-
sis in the computational linguistics domain has fo-
cused either on short segments, such as sentences
(Wilson et al, 2005), or on longer documents with
an explicit polarity orientation like movie or prod-
uct reviews (Turney, 2002). Not all news items may
express overt sentiment. Therefore, in order to test
our hypothesis, we selected a news topic which was
considered a priori to have emotive content.
4.1 Corpus
Markets react strongest to information about firms
to which they have an emotional attachment (Mac-
Gregor et al, 2000). Furthermore, takeovers and
mergers are usually seen as highly emotive contexts.
To combine these two emotion-enhancing factors,
a corpus of news texts was compiled on the topic
of the aggressive takeover bid of a low-cost airline
(Ryanair) for the Irish flag-carrier airline (Aer Lin-
gus). Both airlines have a strong (positive and nega-
tive) emotional attachment for many in Ireland. Fur-
thermore, both airlines are highly visible within the
country and have vocal supporters and detractors
in the public arena. The corpus is drawn from the
987
national media and international news wire sources
and spans 4 months in 2006 from the flotation of
the flag carrier on the stock exchange in Septem-
ber 2006, through the ?surprise? take-over bid an-
nouncement by Ryanair, to the withdrawal of the bid
by Ryanair in December 2006.1
4.2 Gold Standard
A set of 30 texts selected from the corpus was anno-
tated by 3 people on a 7-point scale from very pos-
itive to very negative. Given that a takeover bid has
two players, the respondents were asked also to rate
the semantic orientation of the texts with respect to
the two players, Ryanair and Aer Lingus. Respon-
dents were all native English speakers, 2 female and
1 male. To ensure emotional engagement in the task,
they were first asked to rate their personal attitude to
the two airlines. The ratings in all three cases were
on the extreme ends of the 7 point scale, with very
positive attitudes towards the flag carrier and very
negative attitudes towards the low-cost airline. Re-
spondent attitudes may impact on their text evalu-
ations but, given the high agreement of attitudes in
this study, this impact should at least be consistent
across the individuals in the study. A larger study
should control explicitly for this variable.
As the respondents gave ratings on a ranked scale,
inter-respondent reliability was determined using
Krippendorf?s alpha, a modification of the Kappa
coefficient for ordinal data (Krippendorff, 1980). On
the general ranking scale, there was little agreement
(kappa = 0.1685), corroborating feedback from re-
spondents on the difficulty of providing a general
rating for text polarity distinct from a rating with re-
spect to one of the two companies. However, there
was an acceptable degree of agreement (Grove et al,
1981) on the Ryanair and Aer Lingus polarity rat-
ings, kappa = 0.5795 and kappa = 0.5589 respec-
tively. Results report correlations with these ratings
which are consistent and, from the financial market
perspective, potentially more interesting.2
1A correlation analysis of human sentiment ratings with
Ryanair and Aer Lingus stock prices for the last quarter of 2006
was conducted. The findings suggest that stock prices were cor-
related with ratings with respect to Aer Lingus, suggesting that,
during this takeover period, investors may have been influenced
by sentiment expressed in news towards Aer Lingus. However,
the timeseries is too short to ensure statistical significance.
2Results in this paper are reported with respect to the
4.3 Performance Metrics
The performance of the polarity algorithm was eval-
uated relative to a corpus of human-annotated news
texts, focusing on two separate dimensions of polar-
ity:
1. Polarity direction: the task of assigning a bi-
nary positive/negative value to a text
2. Polarity intensity: the task of assigning a value
to indicate the strength of the negative/positive
polarity in a text.
Performance on the former is reported using stan-
dard recall and precision metrics. The latter is re-
ported as a correlation with average human ratings.
4.4 Baseline
For the metrics in section 3, the baseline for compar-
ison sums the SentiWN polarity rating for only those
lexical items present in the text, not exploiting any
aspect of the graph representation of the text. This
baseline corresponds to the Basic Cohesion Metric,
with inText = true (only lexical items in the text)
and modifiers = false (all parts of speech).
5 Results and Discussion
5.1 Binary Polarity Assignment
The baseline results for positive ratings, negative rat-
ings and overall accuracy for the task of assigning a
polarity tag are reported in table 1. The results show
Type Precision Recall FScore
Positive 0.381 0.7273 0.5
Negative 0.667 0.3158 0.4286
Overall 0.4667 0.4667 0.4667
Table 1: Baseline results
that the baseline tends towards the positive end of
the rating spectrum, with high recall for positive rat-
ings but low precision. Conversely, negative ratings
have high precision but low recall. Figures 1 to 3
illustrate the performance for positive, negative and
overall ratings of all metric?inText?Modifier combi-
nations, enumerated in table 2, relative to this base-
line, the horizontal. Those metrics which surpass
this line are deemed to outperform the baseline.
Ryanair ratings as they had the highest inter-rater agreement.
988
1 Cohesion 5 Relation 9 NodeSpec
2 CohesionTxt 6 RelationTxt 10 NodeSpecTxt
3 CohesionMod 7 RelationMod 11 NodeSpecMod
4 CohesionTxtMod 8 RelationTxtMod 12 NodeSpecTxtMod
Table 2: Metric types in Figures 1-3
Figure 1: F Score for Positive Ratings
All metrics have a bias towards positive ratings
with attendant high positive recall values and im-
proved f-score for positive polarity assignments.
The Basic Cohesion Metric marginally outperforms
the baseline overall indicating that exploiting the
graph structure gives some added benefit. For the
Relations and Specificity metrics, system perfor-
mance greatly improves on the baseline for the
modifiers = true options, whereas, when all parts
of speech are included (modifier = false), perfor-
mance drops significantly. This sensitivity to inclu-
sion of all word classes could suggest that modifiers
are better indicators of text polarity than other word
classes or that the metrics used are not appropriate
to non-modifier parts of speech. The former hypoth-
esis is not supported by the literature while the latter
is not supported by prior successful application of
these metrics in a text comparison task. In order to
investigate the source of this sensitivity, we intend to
examine the distribution of relation types and node
specificity values for sentiment-bearing terms to de-
termine how best to tailor these metrics to the senti-
ment identification task.
A further hypothesis is that the basic polarity val-
ues for non-modifiers are less reliable than for ad-
jectives and adverbs. On a cursory inspection of po-
larity values of nouns and adjectives in SentiWN, it
would appear that adjectives are somewhat more re-
liably labelled than nouns. For example, crime and
Figure 2: F Score for Negative Ratings
some of its hyponyms are labelled as neutral (e.g.
forgery) or even positive (e.g. assault) whereas crim-
inal is labelled as negative. This illustrates a key
weakness in a lexical approach such as this: over-
reliance on lexical resources. No lexical resource is
infallible. It is therefore vital to spread the associ-
ated risk by using more than one knowledge source,
e.g. multiple sentiment lexica or using corpus data.
Figure 3: F Score for All Ratings
5.2 Polarity Intensity Values
The results on the polarity intensity task parallel the
results on polarity tag assignment. Table 3 sets out
the correlation coefficients for the metrics with re-
spect to the average human rating. Again, the best
performers are the relation type and node specificity
metrics using only modifiers, significant to the 0.05
level. Yet the correlation coefficients overall are not
very high. This would suggest that perhaps the re-
lationship between the human ranking scale and the
automatic one is not strictly linear. Although the hu-
man ratings map approximately onto the automati-
989
cally derived scale, there does not seem to be a clear
one to one mapping. The section that follows discuss
this and some of the other issues which this evalua-
tion process has brought to light.
Metric inText Modifier Correlation
Basic Cohesion No No 0.47**
Yes No 0.42*
No Yes 0.47**
Yes Yes 0.47**
Relation Type No No -0.1**
Yes No -0.13*
No Yes 0.5**
Yes Yes 0.38*
Node Specificity No No 0.00
Yes No -0.03
No Yes 0.48**
Yes Yes 0.38*
Table 3: Correlation Coefficients for human ratings.
**. Significant at the 0.01 level. *. Significant at the 0.05 level.
5.3 Issues
The Rating Scale and Thresholding
Overall the algorithm tends towards the positive end
of the spectrum in direct contrast to human raters
with 55-70% of all ratings being negative. Further-
more, the correlation of human to algorithm ratings
is significant but not strongly directional. It would
appear that there are more positive lexical items in
text, hence the algorithm?s positive bias. Yet much
of this positivity is not having a strong impact on
readers, hence the negative bias observed in these
evaluators. This raises questions about the scale of
human polarity judgments: are people more sensi-
tive to negativity in text? is there a positive baseline
in text that people find unremarkable and ignore?
To investigate this issue, we will conduct a compar-
ative corpus analysis of the distribution of positive
and negative lexical items in text and their perceived
strengths in text. The results of this analysis should
help to locate sentiment turning points or thresholds
and establish an elastic sentiment scale which allows
for baseline but disregarded positivity in text.
The Impact of the Lexicon
The algorithm described here is lexicon-based, fully
reliant on available lexical resources. However, we
have noted that an over-reliance on lexica has its
disadvantages, as any hand-coded or corpus-derived
lexicon will have some degree of error or inconsis-
tency. In order to address this issue, it is neces-
sary to spread the risk associated with a single lex-
ical resource by drawing on multiple sources, as in
(Kim and Hovy, 2005). The SentiWN lexicon used
in this implementation is derived from a seed word
set supplemented WordNet relations and as such it
has not been psychologically validated. For this rea-
son, it has good coverage but some inconsistency.
Whissel?s Dictionary of Affect (1989) on the other
hand is based entirely on human ratings of terms.
It?s coverage may be narrower but accuracy might
be more reliable. This dictionary also has the advan-
tage of separating out Osgood?s (1957) evaluative
and activation dimensions as well as an ?imaging?
rating for each term to allow a multi-dimensional
analysis of affective content. The WN Affect lexi-
con (Valitutti et al, 2004) again provides somewhat
different rating types where terms are classified in
terms of denoting or evoking different physical or
mental affective reactions. Together, these resources
could offer not only more accurate base polarity val-
ues but also more nuanced metrics that may better
correspond to human notions of affect in text.
The Gold Standard
Sentiment rating evaluation is not a straight-forward
task. Wiebe et al(2005) note many of the difficul-
ties associated human sentiment ratings of text. As
noted above, it can be even more difficult when eval-
uating news where the text is intended to appear im-
partial. The attitude of the evaluator can be all im-
portant: their attitude to the individuals or organi-
sations in the text, their professional viewpoint as a
market player or an ordinary punter, their attitude to
uncertainty and risk which can be a key factor in the
world of finance. In order to address these issues for
the domain of news impact in financial markets, the
expertise of market professionals must be elicited to
determine what they look for in text and what view-
point they adopt when reading financial news. In
econometric analysis, stock price or trading volume
data constitute an alternative gold standard, repre-
senting a proxy for human reaction to news. For eco-
nomic significance, the data must span a time period
of several years and compilation of a text and stock
990
price corpus for a large scale analysis is underway.
6 Conclusions and Future Work
This paper presents a lexical cohesion based met-
ric of sentiment intensity and polarity in text and
an evaluation of this metric relative to human judg-
ments of polarity in financial news. We are con-
ducting further research on how best to capture a
psychologically plausible measure of affective con-
tent of text by exploiting available resources and a
broader evaluation of the measure relative to human
judgments and existing metrics. This research is ex-
pected to contribute to sentiment analysis in finance.
Given a reliable metric of sentiment in text, what
is the impact of changes in this value on market
variables? This involves a sociolinguistic dimension
to determine what publications or texts best charac-
terise or are most read and have the greatest influ-
ence in this domain and the economic dimension of
correlation with economic indicators.
References
Khurshid Ahmad, David Cheng, and Yousif Almas. 2006.
Multi?lingual sentiment analysis in financial news streams.
In Proc. of the 1st Intl. Conf. on Grid in Finance, Italy.
David M. Cutler, James M. Poterba, and Lawrence H. Sum-
mers. 1989. What moves stock prices. Journal of Portfolio
Management, 79:223?260.
Angela K. Davis, Jeremy M. Piger, and Lisa M. Sedor. 2006.
Beyond the numbers: An analysis of optimistic and pes-
simistic language in earnings press releases. Technical re-
port, Federal Reserve Bank of St Louis.
Ann Devitt. 2004. Methods for Meaningful Text Representation
and Comparison. Ph.D. thesis, Trinity College Dublin.
Paul Ekman and W. V. Friesen. 1971. Constants across cultures
in the face and emotion. Journal of Personality and Social
Psychology, 17:124?129.
Robert F. Engle and Victor K. Ng. 1993. Measuring and test-
ing the impact of news on volatility. Journal of Finance,
48(5):1749?1778.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion mining. In
Proceedings of LREC 2006.
Christiane Fellbaum. 1998. WordNet,an electronic lexical
database. MIT Press.
Gregory Grefenstette, Yan Qu, James G. Shanahan, and
David A. Evans. 2004. Coupling niche browsers and affect
analysis for an opinion mining application. In Proceedings
of RIAO-04, pages 186?194.
William N. Grove, Nancy C. Andreasen, Patricia McDonald-
Scott, Martin B. Keller, and Robert W. Shapiro. 1981. Reli-
ability studies of psychiatric diagnosis. theory and practice.
Archives of General Psychiatry, 38:408?413.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman.
Soo-Min Kim and Eduard Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of COLING 2004.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detection of
opinion bearing words and sentences. In Proc. of IJCNLP-
05, Jeju Island, Korea.
Klaus Krippendorff. 1980. Content Analysis: an Introduction
to its Methodology. Sage Publications, Beverly Hills, CA.
Donald G. MacGregor, Paul Slovic, David Dreman, and
Michael Berry. 2000. Imagery, affect, and financial judg-
ment. The Journal of Psychology and Financial Markets,
1(2):104?110.
Donald MacKenzie. 2003. Long-term capital management and
the sociology of arbitrage. Economy and Society, 32:349?
380.
Mark L. Mitchell and J. Harold Mulherin. 1994. The impact of
public information on the stock market. Journal of Finance,
49(3):923?950.
Victor Niederhoffer. 1971. The analysis of world events and
stock prices. Journal of Business, 44(2):193?219.
Charles E. Osgood, George J. Suci, and Percy H. Tannenbaum.
1957. The Measurement of meaning. University of Illinois
Press, Chicago, Ill.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP-02, pages 79?86.
Paul C. Tetlock. 2007. Giving content to investor sentiment:
The role of media in the stock market. Journal of Finance.
forthcoming.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL?02, pages 417?424.
Alessandro Valitutti, Carlo Strapparava, and Oliviero Stock.
2004. Developing affective lexical resources. PsychNology
Journal, 2(1):61?83.
Cynthia Whissell. 1989. The dictionary of affect in language.
In R. Plutchik and H. Kellerman, editors, Emotion: theory
research and experience, volume 4. Acad. Press, London.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O?Hara.
1999. Development and use of a gold-standard data set for
subjectivity classifications. In Proceedings of ACL-99.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 39:165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT/EMNLP-2005, pages 347?354.
991
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 56?65,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LoLo: A System based on Terminology for Multilingual Extraction 
 
Yousif Almas 
Department of Computing 
University of Surrey 
Guildford, Surrey, GU2 7XH, UK 
y.almas@surrey.ac.uk 
Khurshid Ahmad 
Department of Computer Science 
Trinity College, 
Dublin-2.  IRELAND 
kahmad@cs.tcd.ie 
 
  
 
Abstract 
An unsupervised learning method, based 
on corpus linguistics and special lan-
guage terminology, is described that can 
extract time-varying information from 
text streams.  The method is shown to be 
?language-independent? in that its use 
leads to sets of regular-expressions that 
can be used to extract the information in 
typologically distinct languages like Eng-
lish and Arabic.  The method uses the in-
formation related to the distribution of N-
grams, for automatically extracting 
?meaning bearing? patterns of usage in a 
training corpus. The analysis of an Eng-
lish news wire corpus (1,720,142 tokens) 
and Arabic news wire corpus (1,720,154 
tokens) show encouraging results. 
1 Introduction  
One of the recent trends in (adaptive) IE has 
been motivated by the empirical argument that 
annotated corpora, either annotated automatically 
or annotated manually, can provide sufficient 
information for creating the knowledge base of 
an IE system (McLernon and Kushmerick, 
2006).  Another equally important trend is to use 
manually selected seed patterns to initiate learn-
ing: In turn, active-training methods use seed 
patterns to learn new related patterns from un-
annotated corpora.   Many of the adaptive IE sys-
tems rely on the existing part-of-speech (POS) 
taggers (Debnath and Giles, 2005) and/or syntac-
tic parsers (Stevenson and Greenwood, 2005) for 
analysing and annotating text corpora. The use of 
corpora in IE, especially adaptive IE, should, in 
principle, alleviate the need for manually creat-
ing the rules for information extraction. 
  
The successful use of POS/syntactic taggers is 
dependent on the availability of the knowledge 
of (natural) language used by the authors of 
documents in a given corpus.  There is a wealth 
of POS taggers and parsers available for English 
language, as it has been the most widely used 
language in computational linguistics.  However, 
this is not the case for strategically important 
languages like Arabic and Chinese; to start with, 
in Chinese one does not have the luxury of sepa-
rating word-tokens by a white space and in Ara-
bic complex rules are required to identify mor-
phemes compared to English.  The development 
of segmentation programs in these languages has 
certainly helped (Gao et al, 2005; Habash and 
Rambow, 2005).  More work is needed in under-
standing these languages such that the knowl-
edge thus derived can be used to power taggers 
and parsers. 
Typically, IE systems are used to analyse 
news wire corpora, telephone conversations, and 
more recently in bio-informatics.  The first two 
systems deal with language of everyday commu-
nications ?the general language- whereas bio-
informatics deals with a specialist domain and 
has its own ?special language?.  English special 
languages, for example languages of law, com-
merce, finance, science & technology, each have 
a limited vocabulary and idiosyncratic syntactic 
structures when compared with English used in 
an everyday context.  The same is true of Ger-
man, French, Russian, Chinese, Arabic or Hindi.  
It appears that few works, if any, take advantage 
of the properties of special language to build IE 
systems. 
Our objective is to use methods and tech-
niques of IE in the automatic analysis of special-
ist news that streams in such a way that informa-
tion extracted at an earlier period of time may be 
contradicted or reinforced by information ex-
tracted at a later time.  The impact of news on 
financial and commodity markets is of consider-
56
able import and is often called sentiment analy-
sis.  The prefix ?sentiment? is used to distinguish 
this kind of analysis from the more quantitative 
analysis of assets (called fundamental analysis) 
and that of price movements (called technical 
analysis).  There is a great deal of discussion in 
financial economics, econometrics, and in the 
newly emergent discipline of investor psychol-
ogy about the impact of ?good? and ?bad? news 
on the behaviour of both investors and brokers.  
Three Nobel Prizes have been awarded on the 
impact of market (trader and investor) sentiment 
on the value of shares, currencies, derivatives 
and other financial instruments (Shiller, 2000).  
Financial news, in addition to e-mails and blogs, 
has contributed to the catastrophic failures of 
major trading institutions (Mackenzie, 2000; 
Hardie & Mackenzie, 2005). 
One of the key proponents of news impact 
analysis is the Economics Nobel Laureate Robert 
Engle who has written about asymmetry of in-
formation in a market ? the brokers have more 
knowledge than any given individual, rumours 
have different impact on different actors in the 
market.  Engle?s statistical analysis suggests that 
the ?bad? news has longer lasting effect than 
?good? news (Engle, 1993).  Usually, sentiment 
analysis is carried out using news proxies which 
include dates/times and the names of agencies 
releasing key items of financial data (Anderson 
et al, 2002) or data like the age of a firm, its 
number of initial public offerings, return on in-
vestment, etc.  These proxies are then regressed 
with share, currency or commodity prices.  News 
impact analysis is moving into its next phase 
where the text of news is analysed albeit to a lim-
ited extent (Cutler et al, 1989; Chan, 2003).  The 
analysis sometimes looks at the frequency distri-
bution of pre-specified keywords ?directional 
metaphors like rose/fall, up/down, health meta-
phors like anaemic/healthy and animal meta-
phors like bullish/bearish.   A system is trained 
to correlate and to learn the changes in distribu-
tion of the prescribed metaphorical keywords, 
together with names of organisations, to the 
changes in the value of financial instruments 
(Seo et al, 2002; Omrane et al, 2005; Koppel 
and Shtrimberg 2004).   
We are attempting to create a language-
informed framework for news impact analysis 
using techniques of corpus linguistics and special 
language analysis.  The purpose is to automati-
cally extract patterns from a corpus of domain 
specific texts without prescribing the metaphori-
cal keywords and organisation names.  This, we 
believe, can be achieved by looking at the lexical 
signature of a specialist domain and extracting 
collocational patterns of the individual items of 
the lexical signature.  The lexical signature in-
cludes key vocabulary items of the domain and 
names of people, places and things in the do-
main.  There are instances in the part-of-speech 
tagging literature (Brill, 1993) and in IE where a 
corpus is used and words within a grammatical 
category help to extract rules and patterns com-
prising essential information about a domain or 
topic (Wilks, 1998; Yangarber, 2003).  Brill, 
Wilks and Yangarber induce grammars of a uni-
versal kind:  we focus on inducing a local gram-
mar that deals with the patterning of the items in 
the signature.  Note that in all these cases of 
grammar induction the intuition of the grammar 
builder plays a critical part whether it be in the 
choice of syntactic transformation rules (Brill 
1993), or in choosing sense taggers and implic-
itly semantic rules (Wilks, 1998; Ciravegna and 
Wilks, 2003), or in choosing user supplied seed 
patterns (Yangarber, 2003).  Most of the work in 
grammar induction is focussed on English or ty-
pologically similar languages.  We have deliber-
ately chosen typologically different languages 
(English and Arabic) to evaluate the extent to 
which our method of ?grammar induction? is lan-
guage independent. 
We describe a method for building domain 
specific IE systems:  the patterns used to extract 
domain specific information are the N-gram col-
location patterns of domain specific terms.  The 
patterns are extracted from un-annotated domain-
specific text corpora. We show how one can ana-
lyse the N-gram patterns and render them as 
regular expressions.   
The thesaurus used to identify domain specific 
words is itself constructed automatically from a 
(training) special-language corpus.  The fre-
quency distribution of domain specific terms in a 
special language corpus shows characteristic dif-
ferences from the distribution of the same terms 
in a general language corpus.  There is little or 
no difference in the distribution of the so-called 
grammatical or closed class terms in a special 
and a general language corpus.   
Furthermore, amongst the domain specific 
terms, a few tend to dominate the frequency dis-
tribution ? the so-called lexical signature of a 
domain.  These signature terms are used as nu-
cleates for compound terms in a domain. The 
occurrence of the signature terms, either on their 
own or in a compound or a phrase, is equally 
idiosyncratic in that these dominant single or 
57
compound terms co-occur more frequently with 
one set of words than with others.  The behaviour 
of signature terms appears to be governed by a 
grammar that is local to the specialism and is not 
elsewhere in the general language (Harris, 1991); 
local grammar is used in general language for 
telling times and dates in metaphorical expres-
sions (Gross, 1997), and in the lexicography for 
describing the language of definitions of lemmas 
in a lexicon (Barnbrook and Sinclair, 1996; 
Barnbrook, 2002).  The local grammar approach, 
rooted in the lexical signature of a given domain 
can be used to extract ?sentiment? bearing sen-
tences in financial markets (Ahmad el al., 2006) 
or in the description of work in a scientific labo-
ratory (Ahmad & Al-Sayed, 2005). 
We introduce a system that can help in build-
ing domain specific IE systems in English and 
languages that are typologically distinct from 
English, specifically Arabic.  The development 
of LoLo was inspired by Engle?s pioneering 
work in econometrics where news impact analy-
sis is regarded as critical to the analysis of mar-
ket movement: however much of the work in 
financial economics relates to the correlation of 
the timings of news announcements rather than 
the content of the news stream (Ahmad et al, 
2006). 
LoLo can manage a corpus and extract key 
terms. Given the keyword list, the system then 
identifies collocates and selects significant collo-
cates on well defined statistical criterion 
(Smadja, 1994).  Finally, local grammar rules are 
identified and an IE system is created.   
LoLo has been used to build a local grammar 
to extract ?sentiment? or key (changing) market 
events in English and in Arabic from unseen 
texts.  The system can help visualise the distribu-
tion of extracted patterns synchronised with the 
movement of financial markets.   
IE systems need to be adaptive, as the special-
isms in particular and the world in general is 
changing rapidly and this change is usually re-
flected in language use.  There is an equally im-
portant need to build cross language IE systems 
as information may be in different languages.  
The lexically-motivated approach we describe in 
this paper responds to the need for an adaptive, 
cross domain and cross language IE systems.   
2 Method 
For the extraction of local grammar from a 
corpus of special language texts it is important to 
focus on the keywords.  The patterns in which 
the keywords are embedded are assumed to com-
prise the principal elements of a subject specific 
local grammar.    
The manner in which we derive the local 
grammar is shown in the algorithm below (Fig-
ure 1).   
 
ALGORITHM: DISCOVER LOCAL GRAMMAR 
1. SELECT a special language corpus (SL, comprising 
Nspecial words and vocabulary VSpecial).   
i. USE a frequency list of single words from a corpus of 
texts used in day-to-day communications (SG comprising 
Ngeneral words and vocabulary Vgeneral) ? for example, the 
British National Corpus for the English language: 
Fgeneral:={f(w1),f(w2),f(w3)??.fVgeneral} 
ii. CREATE a frequency ordered list of words in SL texts is 
computed  
Fspecial:={f(w1), f(w2), f(w3)???} 
iii. COMPUTE the differences in the distribution of the 
same words in the two different corpora is computed us-
ing the in SG and SL: 
Weirdness (wi)= f(wi)special/f(wi)general* 
 Ngeneral/Nspecial 
iv. CALCULATE z-score for the Fspeical  
zf(wi)=(f(wi)-fav_special)/?special 
2. CREATE KEY a set of Nkey keywords ordered accord-
ing to the magnitude of the two z-scores 
KEY:={key1, key2, key3??keyNkey) 
 such that z(fkeyi) & z(weridnesskeyi)> 1 
i. EXTRACT collocates of each Key in SL over a window 
of M word neighbourhood.  
ii. COMPUTE the strength of collocation using three meas-
ures due to Smadja (1994): 
U-score, k, and z-score 
iii. EXTRACT sentences in the corpus that comprise highly 
collocating key-words ((U,ko,k1)>=(10,1,1))   
iv. FORM Corpus SL? 
a. For each Sentencei in SL?:  
b. COMPUTE the frequency of every word in Sentencei.   
c. REPLACE words with frequency less than a threshold 
value (fthreshold) by a place marker #; 
d. FOR more than one contiguous place marker, use*  
3. GENERATE trigrams in SL?;  note frequency of each 
trigram together with its position in the sentences:  
i. FIND all the longest possible contiguous trigrams 
across all sentences in SL? and note their frequency 
ii. ORDER the (contiguous) trigrams according to fre-
quency of occurrence 
iii. (CONTIGUOUS) TRIGRAMS with frequency above a 
threshold form THE LOCAL GRAMMAR 
Figure 1. Algorithm for the acquisition of local-
grammar patterns. 
 
Briefly, given a specialist corpus (SL), key-
words are identified, and collocates of the key-
words are extracted.  Sentences containing key 
collocates are then used to construct a sub-corpus 
(SL?).  The sub-corpus SL? is then analyzed and 
trigrams above a frequency threshold in the sub-
corpus are extracted; the position of the trigrams 
in each of the sentences is also noted.  The sub-
corpus is searched again for contiguous trigrams 
across the sentences:   The sentences are ana-
lyzed for the existence of the trigrams in the cor-
rect position ? if a trigram that, for example, is 
noted for its frequency as a sentence initial posi-
tion, is found to co-occur with another frequent 
trigram that exists at the next position, then the 
two trigrams will be deemed to form a pattern.  
58
This process is continued until all the trigrams in 
the sentence are matched with the significant 
trigrams.   
The local grammar then comprises significant 
contiguous trigrams that are found.  These do-
main specific patterns, extracted from the spe-
cialist corpus SL? (and its constituent sub-corpus) 
are then used to extract similar patterns and in-
formation from a test corpus to validate the pat-
terns thus found in the training corpus. Following 
is a demonstration of how the algorithm works 
using English and Arabic texts. 
2.1 Extracting Patterns in English 
We present an analysis of a corpus of financial 
news wire texts: 1204 news report produced by 
Reuters UK Financial News comprising 431,850 
tokens.  One of the frequent words in the corpus 
is percent? 3622 occurrences, a relative fre-
quency of 0.0084%.  When the frequency of this 
keyword is looked up in the British National 
Corpus (100 million words), it was found that 
percent is 287 times more frequent in the finan-
cial corpus than in the British National Corpus ? 
this ratio is sometimes termed weirdness (of spe-
cial language); the weirdness of grammatical 
words the and to is unity as these tokens are dis-
tributed with the same (relative) frequency in 
Reuters Financial and the BNC.  The z-score 
computed using the frequency of the token in the 
Reuters Financial is 12.64: the distribution of 
percent is 12 standard deviations above the mean 
of all words in the financial corpus.  (The z-score 
computed for weirdness is positive as well).  The 
heuristic here is this: a token is a candidate key-
word if both its z-scores are greater than a small 
positive number.  So percent -most frequent to-
ken with frequency and weirdness z-score over 
zero- was accepted as a keyword.   
The collocates of the keyword percent were 
then extracted by using mutual information sta-
tistics presented by Smadja (1994).  A collocate 
in this terminology can be anywhere in the vicin-
ity of +/- N-words.  The frequency at each 
neighbourhood is calculated and then used to 
compute the ?peaks? in the histogram formed by 
the neighbourhood frequencies and the strength 
of the collocation calculated on a similar basis.  
The keyword generally collocates with certain 
words that have frequencies higher than itself ?
the upward collocates- and collocates with cer-
tain words that have lesser frequency ? the down-
wards collocates (These terms were coined by 
John Sinclair).  Upwards collocates are usually 
grammatical words and downwards collocates 
are lexical words ? nouns, adjectives- and hence 
the downwards collocates are treated as candi-
date compound words.  There were 46 collocates 
of percent in our corpus ? 34 downwards collo-
cates and 12 upwards collocates.  A selection of 
5 downwards and upwards are shown in Table 1 
and 2 respectively. 
 
 
Collocate Frequency U-score k-score 
shares 1150 1047 3.01 
rose 514 2961 2.43 
year 2046 396 2.40 
profit 1106 263 1.65 
down 486 996 1.40 
Table 1. Downward collocates of percent in a 
corpus of 431,850 words. 
 
Collocate Frequency U-score k-score 
the 23157 6744 14.40 
to 12190 7230 10.29 
in 9768 4941 8.49 
a 10657 3024 8.44 
of 10123 3957 8.24 
Table 2. Upward collocates of percent in a cor-
pus of 431,850 words. 
 
 
The financial texts comprise a large number of 
numerals (integers and decimals) and these we 
will denote as <no>.  The numerals collocate 
strongly with percent for obvious reasons.  The 
collocates are then used to extract trigrams com-
prising the collocates that occur at particular po-
sitions in the various sentences of our corpus: 
 
 
Token A Token B Token C Freq Position 
<no> percent and 16 1 
rose <no> percent 18 1 
<no> percent after 23 2 
<no> percent of 47 2 
<no> percent rise 11 2 
Table 3. Trigrams of percent. 
 
 
There are many other frequent patterns where 
the frequency of individual tokens is quite low 
but at least one member of the trigram has higher 
frequency: such low frequency tokens are omit-
ted and marked by the (#) symbol. All the tri-
grams containing such tokens with at least two 
others are used to extract other significant tri-
grams. Sometimes more than one low frequency 
tokens precede or succeed high frequency tokens 
and they are denoted by the symbol (*) as shown 
in Table 4. The search for contiguous trigrams 
leads to larger and more complex patterns, Table 
5 provides some examples. 
59
Token A Token B Token C Freq Position 
rose <no> percent 18 1 
# <no> percent 29 2 
# shares were 10 2 
* <no> percent 57 2 
<no> percent # 24 2 
Table 4. Trigrams of percent with omitted low 
frequency words (denoted as * for multiple to-
kens and # for a single token). 
 
 
Local Grammar Patterns Freq 
<s> the * <no> percent 28 
<s> * rose <no> percent 26 
<s> # shares # <no> percent 22 
<s> * fell <no> percent 20 
<s> * <no>  percent 18 
<s> # shares were  up <no> percent at 17 
Table 5. Some of top patterns of percent   (<s> 
identifies a sentence boundary). 
 
2.2 Extracting Patterns in Arabic 
Arabic is written from right to left and its writ-
ing system does not employ capitalization. The 
language is highly inflected compared to Eng-
lish; words are generated using a root-and-
pattern morphology. Prefixes and suffixes can be 
attached to the morphological patterns for gram-
matical purposes. For example, the grammatical 
conjunction ?and? in Arabic is attached to the 
beginning of the following word. Words are also 
sensitive to the gender and number they refer to 
and their lexical structure change accordingly. 
As a result, more word types can be found in 
Arabic corpora compared to English of same size 
and type. Short vowels which are represented as 
marks in Arabic are also omitted from usual 
Arabic texts resulting in some words having 
same lexical structures but different semantics.  
These grammatical and lexical features of 
Arabic cause more complexity and ambiguity, 
especially for NLP systems designed for thor-
ough processing of Arabic texts compared to 
English. A shallow and statistical approach for 
IE using texts of specialism can be useful to ab-
stract many complexities of Arabic texts. 
Given a 431,563 word corpus comprising 
2559 texts of Reuters Arabic Financial News and 
the same thresholds we used with the English 
corpus, percent (al-meaa, QRST?) is again the most 
frequent term with frequency and weirdness z-
score greater than zero. It has 3125 occurrences 
(0.0072%), a frequency z-score of 19.03 and a 
weirdness of 76 compared against our Modern 
Standard Arabic Corpus (MSAC).  
There were 31 collocates of percent; 7 up-
wards and 23 downwards. The downwards collo-
cates of percent appear to collocate with names 
of instruments i.e. shares and indices (Table 6). 
The upwards collocate are with the so-called 
closed class words as in English like in, on and 
that (Table 7). 
 
 
Collocate Freq U-score k-score 
by-a-ratio  
(be-nesba, QVWXY) 1257 39191 7.87 
point  
(noqta, QZ[\) 1167 9946 6.44 
the-year 
(al-aam, ?^_T3.34 344 1753 (? 
index 
(moasher,`abc) 1130 409 2.55 
million 
(milyoon, ?efgc) 2281 600 2.32 
share 
(saham, hij) 705 206 1.84 
Table 6. Downward collocates of percent (al-
meaa, QRST?). 
 
 
Collocate Freq U-score k-score 
in 
 (fee,kl) 21236 434756 40.99 
to  
(ela, mT9.81 25145 3339 (? 
from  
(min, nc) 10344 4682 9.58 
on 
 (ala,mgo ) 5275 117 3.10 
that 
 (ann, 2.65 260 5130 (?? 
Table 7. Upward collocates of percent (almeaa, 
QRST?). 
 
 
Using the same thresholds the trigrams (Table 
8) appear to be different from the English tri-
grams in that the words of movement are not in-
cluded here ? this is because Arabic has a richer 
morphological system compared to English and 
Financial Arabic is not as standardised as Finan-
cial English: however, it will not be difficult to 
train the system to recognise the variants of rose 
and fell   in Financial Arabic. Table 9 lists some 
of the patterns. 
 
 
60
Token A Token B Token C Freq Position 
<no> in  (fee,kl) 
percent  
(al-meaa,QRST1 197 (? 
in 
 (fee,kl) 
percent  
(al-meaa,QRST1 39 * (? 
in  
(fee,kl) 
percent  
(al-meaa,QRST?) 
to  
(ela, mT2 22 (? 
percent  
(al-meaa,QRST?) 
to  
(ela, mT?) <no> 21 3 
# in  (fee,kl) 
percent  
(al-meaa,QRST4 66 (? 
Table 8. Trigrams of percent (almeaa, QRST?). 
 
 
Local Grammar Patterns Freq 
      <no>       *       <s> kl      QRST?      *      
                                             percent     in 34 
>s<     *      QVWXY      > no<     kl       QRST?      mT?   > no< 
              to     percent  in                 by-a-ratio 23 
>s<      #        hij        #      >no<       kl       ?QRST 
                       percent     in                              share 21 
>s # < `abc # ^p^Z\ qj?s?   QVWXY>no <kl QRST?    mT? >no < QZ[\>/s< 
        point         to percent in      by-ratio  wider         index 18 
  `abc *>no<  QZ[\        ??  >no< kl      QRST?     mT?>no <  QZ[\>/s< 
         point          to  percent  in        namely   point           index 16 
  kl     *      ?ev     #       QVWXY     >no <      kl     QRST?       qc      *  
                      with   percent   in                  by-ratio         day           in  
10 
Table 9. Some patterns of percent  
(almeaa, QRST?). 
 
3 Experimental Results 
We have argued that a method that is focused on 
frequency at the lexical level(s) of linguistic de-
scription ? single words, compounds, and N-
grams- will perhaps lead to patterns that are idio-
syncratic of a specialist domain without recourse 
to a thesaurus.  There are a number of linguistic 
methods ? that focus on syntactic and semantic 
level of description which might be of equal or 
better use.  
In order to show the effectiveness of our 
method we apply it to sentiment analysis ? an 
analysis that attempts to extract qualitative opin-
ion expressed about a range of human and natu-
ral artefacts ? films, cars, financial instruments 
for instance.  Broadly speaking, sentiments in 
financial markets relate to the ?rise? and ?fall? of 
financial instruments (shares, currencies, com-
modities and energy prices): inextricably these 
sentiments relate to change in the prices of the 
instruments.  In both English and Arabic, we 
have found that percent or equivalent is a key-
word and trigrams and longer N-grams embed-
ded with this keyword relate to metaphorical 
movement words? up, down, rise, fall.  However, 
in English this association is further contextual-
ised with other keywords ? shares, stocks- and in 
Arabic the contextualisation is with shares and 
the principal commodity of many Arab states 
economies ? oil.  Our system ?discovered? both 
by following a lexical level of linguistic descrip-
tion. 
For each of the two languages of interest to us, 
we have created 1.72 million token corpora.  
Each corpus was then divided into two (roughly) 
equal sized sub corpora: training corpus and test-
ing corpus; the testing corpus is sub-divided into 
two testing corpora Test1 and Test2 (Table 10).  
First, we extract patterns from the Training Cor-
pus using the discover local grammar algorithm 
(Figure 1) and also from Test1.  Next, the Train-
ing1 and Test1 corpora are merged and patterns 
extracted from the merged corpus.  The intuition 
we have is that as the size of the corpus is in-
creased the patterns extracted from a smaller 
sized corpus will be elaborated: some of the pat-
terns that are idiosyncratic of the smaller sized 
corpus will become statistically insignificant and 
hence will be ignored.  The conventional way of 
testing would have been to see how many pat-
terns discovered in the training corpus are found 
in the testing corpora; we are quantifying these 
results currently. In the following we describe an 
initial test of our method after introducing LoLo.   
 
English Arabic Corpus Texts Tokens Texts Tokens 
Training1 2408 861,492 5118 860,020 
Test1 1204 431,850 2559 431,563 
Training2 (Training1+Test1) 3612 1,293,342 7677 1,293,342 
Test2 1204 426,800 2559 428,571 
Total 4816 1,720,142 10,236 1,720,154 
Table 10. Training and testing corpora used in 
our experiments. 
 
3.1 LoLo 
LoLo (stands for Local-Grammar for Learning 
Terminology and means ?pearl? in Arabic) is de-
veloped using the .NET platform. It contains four 
components summarised in Table 11. 
 
Component Functionality 
CORPUS ANALYSER Discover domain specific extraction patterns 
RULES EDITOR Group, label and evaluate patterns and slots 
INFORMATION  EXTRACTOR Extract information 
INFORMATION VISUALISER Visualise patterns over time 
Table 11. Summary of LoLo?s components. 
61
The various components of LoLo ?the Ana-
lyser, Editor, Extractor and the Visualiser, can 
be used to extract and present patterns; the sys-
tem has utilities to change script and the direc-
tion of writing (Arabic is right-to-left and Eng-
lish left-to-right). Table 12 is an exemplar output 
from LoLo: ?rise in profit? event patterns ex-
pressed similarly in English and Arabic financial 
news headlines found by the Corpus Analyser. 
 
English *  profit  up  <no>  percent 
 ?^yz?    ??^Y??   <no> *  kl     QRST?  Arabic percent  in            profit  rise (up) 
Table 12. ?Rise in profit? patterns in Arabic and 
English where the * usually comprises names of 
organisations or enterprises. 
 
 
The pattern acquisition algorithm presented 
earlier is implemented in the Corpus Analyser 
component, which is the focus of this paper. It 
can be used for discovering frequent patterns in 
corpora. The user has the option to filter smaller 
patterns contained in larger ones and to mine for 
interrupted or non-interrupted patterns. It can 
also distinguish between single word and multi 
word slots. 
Before mining for patterns, a corpus pre-
processor routine performs a few operations to 
improve the pattern discovery. It identifies any 
punctuation marks attached to the words and 
separates them. it also identifies the sentences 
boundaries and converts all the numerical tokens 
to one tag ?<no>? as numbers can be part of 
some patterns, especially in the domain of finan-
cial news.   
The Rules Editor is at its initial stages of de-
velopment, currently it can export the extraction 
patterns discovered by the Corpus Analyser as 
regular expressions. 
A time-stamped corpus can be visualised us-
ing the Information Visualiser. The Visualiser 
can display a time-series that shows how the ex-
tracted events emerge, repeat and fade over time 
in relation to other events or imported time series 
i.e. of financial instruments. This can be useful 
for analysing any relations between different 
events or detecting trends in one or more corpora 
or with other time-series.  
LoLo facilitates other corpus and computa-
tional linguistics tasks as well, including generat-
ing concordances and finding collocations from 
texts encoded in UTF-8. This is particularly use-
ful for Arabic and languages using the Arabic 
writing system like Persian and Urdu which lack 
such resources. 
3.2 Training and Testing 
3.2.1 English 
We consider the English Training1 corpus first.  
We extracted the significant collocates of all the 
high frequency/high weirdness words, where 
?high? defined using the associated z-scores, in 
the training corpus.  Trigrams were then ex-
tracted and high frequency trigrams were chosen 
and all sentences comprising the trigrams were 
used to form a (training) sub corpus.  The sub-
corpus was then analysed for extracting the local 
grammar. 
The 10 high frequency N-grams extracted 
automatically from the Training1 Corpus 
(861,492) are listed in Table 13. The Test1 cor-
pus has most of the trigrams in the Training1 
corpus, particularly some of the larger N-grams 
(Table 14). 
 
 
Rank Top 10 patterns comprising ?percent? Freq 
1 <s> the * <no> percent 45 
2 <s> the * was up <no> percent at <no>,  <no> </s>  33 
3 <s> * <no> percent #, <no> </s> 24 
4 <s> * up <no> percent 21 
5 <s> the * was down <no> percent at <no> , <no> </s> 19 
6 <s> * <no> percent after 18 
6 <s> * <no> percent to <no> , <no> yen 18 
7 <s>, # shares were up <no> percent at <no> 17 
8 <s> shares in * <no> percent 15 
9 <s> * rose <no> percent to <no> 14 
10 <s> # shares rose <no> percent to <no> 13 
10 <s> fell <no> percent to <no> 13 
Table 13. Patterns of percent extracted from 
Training1 corpus. 
 
Patterns Freq 
<s> # shares # <no> percent 22 
<s> shares in * <no> percent 13 
<s> # shares were up <no> percent at 17 
Table 14. Patterns of percent extracted from 
Test1 corpus found as sub-patterns in Training1. 
 
 
We then merged the Training1 and Test1 cor-
pora together and created Training2 corpus com-
prising of 3612 texts and 1,293,342 tokens.  The 
Algorithm was executed on the merged corpus 
and a new set of patterns were extracted, in par-
ticular the most frequent pattern in the Training1 
Corpus (<s> the * <no> percent), was elabo-
62
rated by the Algorithm as well as those patterns 
shown in Table 15. 
 
Training1 Corpus Freq Training2 Corpus Freq 
<s> the * was down <no> 
percent at <no> , <no> </s> 19 
<s> the * index was down 
<no> percent at <no> ,  <no> 
</s> 
23 
<s> the * was up <no> 
percent at <no>,  <no> </s> 33 <s> the * index was up <no> percent at <no> ,  <no> </s> 34 
Table 15. Comparison between two patterns in 
Training1 and Training2 corpora. 
 
 
The patterns related to the collocations of 
shares and percent from Training1 were pre-
served in Training2.  The test on Test2 corpus 
showed similar results: the smaller N-grams re-
lated to the movement of instruments were simi-
lar to the Test1 Corpus. The analysis of Arabic 
texts is shown below with similar results. 
3.2.2 Arabic 
Some of frequent N-grams extracted automati-
cally from the Training1 Arabic corpus (860,020) 
are shown in Table 16. Similar to the English 
corpora the Test1 Arabic corpus has most of the 
trigrams in the Training1 Corpus and some larger 
N-grams(Table 17). 
 
 
Rank Top 10 patterns comprising ?percent? Freq 
 <no>    *    <s>     kl      QRST1       *       ?                                      percent    in 35 
*       kl        *       QVWXY       > no<       kl       QRST2  ?          percent  in                      by-ratio               in  31 
 *>no<  QZ[\  QVWXY > no < kl  QRST?   mT?  >no < QZ[\  >/s< 3          point           to percent in       by-ratio  point 28 
<s>         *       kl        QRST?       kl      *  4                                    in      percent      in 24 
>s <   *     QVWXY      > no<    kl      QRST?       mT?      >no< 4                     to    percent    in                by-ratio 24 
*        kl        *       mT?       >no <       kl      QRST5 ?              percent    in                       to                  in 21 
>s #< `abc* ^p^Z\   QVWXY >no<kl  QRST?    mT? >no<QZ[\>/s< 5          point      to percent in   by-ratio zone  index 21 
Table 16. Patterns of percent (almeaa, QRST?) ex-
tracted from Training1 Arabic corpus. 
 
  
Patterns Freq 
*      QVWXY      <no>        kl       QRST?       # 
          percent      in                   by-ratio 10 
*       QVWXY     <no>        kl       QRST?       kl 
   in     percent    in                   by-ratio  10 
>s <    *   QVWXY   > no <     kl      QRST?     *  
         percent   in               by-ratio 11 
Table 17. Patterns of percent (almeaa, QRST?) ex-
tracted from Test1 Arabic corpus found as sub-
patterns in Training1. 
 
After merging the Training1 and Test1 Arabic 
corpora together into a corpus of 7677 texts and 
1,293,342 tokens, new set of patterns were ex-
tracted as well. Some of the frequent patterns in 
the training corpus were elaborated more as well 
like the pattern shown in Table 18 where the to-
ken and-rise (wa-ertifaa, ??qyz? ) was added to the 
pattern. 
 
Training1 Corpus Freq Training2 Corpus Freq 
 # <s>   `abc #^p^Z\ qj?s?  
 QVWXY <no> mT? QRST? kl <no> 
QZ[\      </s> 
13 
 <s> ;<=??? `abc  # ^p^Z\ qj?s?
QVWXY <no> mT? QRST? kl <no> QZ[\ 
 </s>     
17 
Table 18. Comparison between two patterns in 
Training1 and Training2 Arabic corpora. 
 
4 Evaluation 
We have used the Rules Editor and the Informa-
tion Extractor to evaluate the patterns on a cor-
pus comprising 2408 texts and 858,650 tokens 
created by merging Test1 and Test2 corpora. The 
Arabic evaluation corpus comprised 5118 texts 
and 860,134 tokens.  The N-gram pattern extrac-
tor (where N > 4) showed considerable promise 
in that who or what went up/or down was unam-
biguously extracted from the English test corpus 
using patterns generated through the training 
corpus. Initial results show high precision with 
the longer N-grams in English (Table 19) and 
Arabic (Table 20). 
Table 19. Patterns with high precision (English). 
 
 
Table 20. Patterns with high precision (Arabic). 
 
 
Pattern Precision 
<ORG> shares were down <no> percent at <no> 100% (13/13) 
<Movement> <no> percent to <no> , <no> yen 100% (17/17) 
the <Index> was up <no> percent at <no> , <no>  92% (11/12) 
<ORG> shares # up <no> percent at 88% (30/34) 
Pattern Preci-sion 
 `abc  <no>  <Index>  QZ[\  ??    <no>   kl   QRST?    mT?  <no> QZ[\  
point         to  percent  in             viz  point                          index 
100% 
(42/42) 
   <Index>   	   #   >no<     #   > no <      ?  
percent  in                     point                for-shares               index 
100% 
(27/27) 
<Movement> `abc <Index> QVWXY ^p^Z\ qj?s?>no <mT? QRST? kl>no<QZ[\ 
point        to percent in       by-ratio zone wider                  index 
97% 
(33/34) 
<Movement>  <Index>  