Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 102?112,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning to Freestyle: Hip Hop Challenge-Response Induction via
Transduction Rule Segmentation
Dekai Wu Karteek Addanki Markus Saers Meriem Beloucif
Human Language Technology Center
Department of Computer Science
HKUST, Clear Water Bay, Hong Kong
{dekai|vskaddanki|masaers|mbeloucif}@cs.ust.hk
Abstract
We present a novel model, Freestyle, that
learns to improvise rhyming and fluent re-
sponses upon being challenged with a line of
hip hop lyrics, by combining both bottom-
up token based rule induction and top-down
rule segmentation strategies to learn a stochas-
tic transduction grammar that simultaneously
learns both phrasing and rhyming associations.
In this attack on the woefully under-explored
natural language genre of music lyrics, we
exploit a strictly unsupervised transduction
grammar induction approach. Our task is par-
ticularly ambitious in that no use of any a pri-
ori linguistic or phonetic information is al-
lowed, even though the domain of hip hop
lyrics is particularly noisy and unstructured.
We evaluate the performance of the learned
model against a model learned only using
the more conventional bottom-up token based
rule induction, and demonstrate the superi-
ority of our combined token based and rule
segmentation induction method toward gen-
erating higher quality improvised responses,
measured on fluency and rhyming criteria as
judged by human evaluators. To highlight
some of the inherent challenges in adapting
other algorithms to this novel task, we also
compare the quality of the responses generated
by our model to those generated by an out-of-
the-box phrase based SMT system. We tackle
the challenge of selecting appropriate training
data for our task via a dedicated rhyme scheme
detection module, which is also acquired via
unsupervised learning and report improved
quality of the generated responses. Finally,
we report results with Maghrebi French hip
hop lyrics indicating that our model performs
surprisingly well with no special adaptation to
other languages.
1 Introduction
The genre of lyrics in music has been severely under-
studied from the perspective of computational lin-
guistics despite being a form of language that has
perhaps had the most impact across almost all human
cultures. With the motivation of spurring further re-
search in this genre, we apply stochastic transduc-
tion grammar induction algorithms to address some
of the modeling issues in song lyrics. An ideal start-
ing point for this investigation is hip hop, a genre
that emphasizes rapping, spoken or chanted rhyming
lyrics against strong beats or simple melodies. Hip
hop lyrics, in contrast to poetry and other genres of
music, present a significant number of challenges for
learning as it lacks well-defined structure in terms of
rhyme scheme, meter, or overall meaning making it
an interesting genre to bring to light some of the less
studied modeling issues.
The domain of hip hop lyrics is particularly un-
structured when compared to classical poetry, a do-
main on which statistical methods have been applied
in the past. Hip hop lyrics are unstructured in the
sense that a very high degree of variation is permit-
ted in the meter of the lyrics, and large amounts of
colloquial vocabulary and slang from the subculture
are employed. The variance in the permitted me-
ter makes it hard to make any assumptions about
the stress patterns of verses in order to identify the
rhyming words used when generating output. The
broad range of unorthodox vocabulary used in hip
hop make it difficult to use off-the-shelf NLP tools
for doing phonological and/or morphological analy-
sis. These problems are further exacerbated by dif-
ferences in intonation of the same word and lack of
robust transcription (Liberman, 2010).
102
We argue that stochastic transduction grammars,1
given their success in the area of machine transla-
tion and efficient unsupervised learning algorithms,
are ideal for capturing the structural relationship be-
tween lyrics. Hence, our Freestyle system mod-
els the problem of improvising a rhyming response
given any hip hop lyric challenge as transducing
a challenge line into a rhyming response. We
use a stochastic transduction grammar induced in
a completely unsupervised fashion using a combi-
nation of token based rule induction and segment-
ing (Saers et al, 2013) as the underlying model to
fully-automatically learn a challenge-response sys-
tem and compare its performance against a simpler
token based transduction grammar model. Both our
models are completely unsupervised and use no prior
phonetic or linguistic knowledge whatsoever despite
the highly unstructured and noisy domain.
We believe that the challenge-response system
based on an interpolated combination of token based
rule induction and rule segmenting transduction
grammars will generate more fluent and rhyming re-
sponses compared to one based on token based trans-
duction grammars models. This is based on the ob-
servation that token based transduction grammars
suffer from a lack of fluency; a consequence of the
degree of expressivity they permit. Therefore, as a
principal part of our investigation we compare the
quality of responses generated using a combination
of token based rule induction and top-down rule seg-
menting transduction grammars to those generated
by pure token based transduction grammars.
We also hypothesize that in order to generate flu-
ent and rhyming responses, it is not sufficient to train
the transduction grammars on all adjacent lines of a
hip hop verse. Therefore, we propose a data selec-
tion scheme using a rhyme scheme detector acquired
through unsupervised learning to generate the train-
ing data for the challenge-response systems. The
rhyme scheme detector segments each verse of a hip
hop song into stanzas and identifies the lines in each
stanza that rhyme with each other which are then
added as training instances. We demonstrate the su-
periority of our training data selection method by
comparing the quality of the responses generated by
the models trained on data selected with and without
1Also known in SMT as ?synchronous grammars?.
using the rhyme scheme detector.
Unlike conventional spoken and written language,
disfluencies and backing vocals2 occur very fre-
quently in the domain of hip hop lyrics which af-
fect the performance of NLP models designed for
processing well-formed sentences. We propose two
strategies to mitigate the effect of disfluencies on our
model performance and compare their efficacy using
human evaluations. Finally, in order to illustrate the
challenges faced by other NLP algorithms, we con-
trast the performance of our model against a conven-
tional, widely used phrase-based SMT model.
A brief terminological note: ?stanza? and ?verse?
are frequently confused and sometimes conflated.
Worse yet, their usage for song lyrics is often con-
tradictory to that for poetry. To avoid ambiguity
we consistently follow these technical definitions for
segments in decreasing size of granularity:
verse a large unit of a song?s lyrics. A song typi-
cally contains several verses interspersed with
choruses. In the present work, we do not differ-
entiate choruses from verses. In song lyrics, a
verse is most commonly represented as a sepa-
rate paragraph.
stanza a segment within a verse which has a me-
ter and rhyme scheme. Stanzas often consist of
2, 3, or 4 lines, but stanzas of more lines are
also common. Particularly in hip hop, a single
verse often contains many stanzas with differ-
ent rhyme schemes and meters.
line a segment within a stanza consisting of a single
line. In poetry, strictly speaking this would be
called a ?verse?, which however conflicts with
the conventional use of ?verse? in song lyrics.
In Section 2, we discuss some of the previous
work that applies statistical NLP methods to less
conventional domains and problems. We describe
our experimental conditions in Section 3. We com-
pare the performance of token and segment based
transduction grammar models in Section 4. We com-
pare our data selection schemes and disfluency han-
dling strategies in Sections 5 and 6. Finally, in
2Particularly the repetitive chants, exclamations, and inter-
jections in hip hop ?hype man? style backing vocals.
103
Section 7 we describe some preliminary results ob-
tained using our approach on improvising hip hop
responses in French and conclude in Section 8.
2 Related work
Although a few attempts have been made to apply
statistical NLP learning methods to unconventional
domains, Freestyle is among the first to tackle the
genre of hip hop lyrics (Addanki and Wu, 2013; Wu
et al, 2013a,b). Our preliminary work suggested the
need for further research to identify models that cap-
ture the correct generalizations to be able to gener-
ate fluent and rhyming responses. As a step towards
this direction, we contrast the performance of inter-
polated bottom-up token based rule induction and
top-down segmenting transduction grammar models
and token based transduction grammar models. We
briefly describe some of the past work in statistical
NLP on unconventional domains below.
Most of the past work either uses some form of
prior linguistic knowledge or enforces harsher con-
straints such as set number of words in a line, or a set
meter which are warranted by more structured do-
mains such as poetry. However, in hip hop lyrics it
is hard to make any linguistic or structural assump-
tions. For example, words such as sho, flo, holla
which frequently appear in the lyrics are not part of
any standard lexicon and hip hop does not require a
set number of syllables in a line, unlike poems. Also,
surprising and unlikely rhymes in hip hop are fre-
quently achieved via intonation and assonance, mak-
ing it hard to apply prior phonological constraints.
A phrase based SMT systemwas trained to ?trans-
late? the first line of a Chinese couplet or duilian
into the second by Jiang and Zhou (2008). The most
suitable next line was selected by applying linguistic
constraints to the n best output of the SMT system.
However in contrast to Chinese couplets, which ad-
here to strict rules requiring, for example, an identi-
cal number of characters in each line and one-to-one
correspondence in their metrical length, the domain
of hip hop lyrics is far more unstructured and there
exists no clear constraint that would ensure fluent
and rhyming responses to hip hop challenge lyrics.
Barbieri et al (2012) use controlled Markov pro-
cesses to semi-automatically generate lyrics that sat-
isfy the structural constraints of rhyme and meter.
Tamil lyrics were automatically generated given a
melody using conditional random fields by A. et al
(2009). The lyrics were represented as a sequence
of labels using the KNM system where K, N and M
represented the long vowels, short vowels and con-
sonants respectively.
Genzel et al (2010) used SMT in conjunction
with stress patterns and rhymes found in a pronun-
ciation dictionary to produce translations of poems.
Although many constraints were applied in translat-
ing full verses of poems, it was challenging to sat-
isfy all the constraints. Stress patterns were assigned
to words given the meter of a line in Shakespeare?s
sonnets by Greene et al (2010), which were then
combined with a language model to generate poems.
Sonderegger (2011) attempted to infer the pronun-
ciation of words in old English by identifying the
rhyming patterns using graph theory. However, their
heuristic of clustering words with similar IPA end-
ings resulted in large clusters of false positives such
as bloom and numb. A language-independent gener-
ative model for stanzas in poetry was proposed by
Reddy and Knight (2011) via which they could dis-
cover rhyme schemes in French and English poetry.
3 Experimental conditions
Before introducing our Freestyle models, we first
detail our experimental assumptions and the evalua-
tion scheme under which the responses generated by
different models are compared against one another.
We describe our training data as well as a phrase-
based SMT (PBSMT) contrastive baseline. We also
define the evaluation scheme used to compare the re-
sponses of different systems on criteria of fluency
and rhyming.
3.1 Training data
We used freely available user generated hip hop
lyrics on the Internet to provide training data for our
experiments. We collected approximately 52,000
English hip hop song lyrics amounting to approxi-
mately 800Mb of raw HTML content. The data was
cleaned by stripping HTML tags, metadata and nor-
malized for special characters and case differences.
The processed corpus contained 22 million tokens
with 260,000 verses and 2.7 million lines of hip hop
lyrics. As human evaluation using expert hip hop
104
listeners is expensive, a small subset of 85 lines was
chosen as the test set to provide challenges for com-
paring the quality of responses generated by different
systems.
3.2 Evaluation scheme
The performance of various Freestyle versions
was evaluated on the task of generating a improvised
fluent and rhyming response given a single line of a
hip hop verse as a challenge. The output of all the
systems on the test set was given to three indepen-
dent frequent hip hop listeners for manual evalua-
tion. They were asked to evaluate the system out-
puts according to fluency and the degree of rhyming.
They were free to choose the tune to make the lyrics
rhyme as the beats of the song were not used in the
training data. Each evaluator was asked to score the
response of each system on the criterion of fluency
and rhyming as being good, acceptable or bad.
3.3 Phrase-based SMT baseline
In order to evaluate the performance of an out-of-
the-box phrase-based SMT (PBSMT) system toward
this novel task of generating rhyming and fluent re-
sponses, a standard Moses baseline (Koehn et al,
2007) was also trained in order to compare its per-
formance with our transduction grammar induction
model. A 4-gram language model which was trained
on the entire training corpus using SRILM (Stolcke,
2002) was used to generate responses in conjunction
with the phrase-based translation model. As no au-
tomatic quality evaluation metrics exist for hip hop
responses analogous to BLEU for SMT, the model
weights cannot be tuned in conventional ways such
asMERT (Och, 2003). Instead, a slightly higher than
typical language model weight was empirically cho-
sen using a small development set to produce fluent
outputs.
4 Interpolated segmenting model vs. token
based model
We compare the performance of transduction gram-
mars induced via interpolated token based and rule
segmenting (ISTG) versus token based transduction
grammars (TG) on the task of generating a rhyming
and fluent response to hip hop challenges. We use
the framework of stochastic transduction grammars,
specifically bracketing ITGs (inversion transduction
grammars) (Wu, 1997), as our translation model for
?transducing? any given challenge into a rhyming
and fluent response. Our choice is motivated by
the significant amount of empirical evidence for the
representational capacity of transduction grammars
across a spectrum of natural language tasks such as
textual entailment (Wu, 2006), mining parallel sen-
tences (Wu and Fung, 2005) and machine translation
(Zens and Ney, 2003). Further, existence of effi-
cient learning algorithms (Saers et al, 2012; Saers
and Wu, 2011) that make no language specific as-
sumptions, make inversion transduction grammars a
suitable framework for our modeling needs. Exam-
ples of lexical transduction rules can be seen in Ta-
bles 3 and 5. In addition, the grammar also includes
structural transduction rules for the straight case
A? [A A] and also the inverted case A? <A A>.
4.1 Token based vs. segmental ITGs
The degenerate case of ITGs are token based ITGs
wherein each translation rule contains at most one
token in input and output languages. Efficient induc-
tion algorithmswith polynomial run time exist for to-
ken based ITGs and the expressivity they permit has
been empirically determined to capture most of the
word alignments that occur across natural languages.
The parameters of the token based ITGs can be es-
timated using expectation maximization through an
efficient dynamic programming algorithm in con-
junction with beam pruning (Saers and Wu, 2011).
In contrast to token based ITGs, each rule in a seg-
mental ITG grammar can contain more than one to-
ken in both input and output languages. In machine
translation applications, segmental models produce
translations that are more fluent as they can capture
lexical knowledge at a phrasal level. However, only
a handful of purely unsupervised algorithms exist
for learning segmental ITGs under matched training
and testing assumptions. Most other approaches in
SMT use a variety of ad hoc heuristics for extracting
segments from token alignments, justified purely by
short term improvements in automatic MT evalua-
tion metrics such as BLEU (Papineni et al, 2002)
which cannot be transferred to our current task. In-
stead, we use a completely unsupervised learning al-
gorithm for segmental ITGs that stays strictly within
the transduction grammar optimization framework
for both training and testing as proposed in Saers
105
et al (2013).
Saers et al (2013) induce a phrasal inversion
transduction grammar via interpolating the bottom-
up rule chunking approach proposed in Saers et al
(2012) with a top-down rule segmenting approach
driven by a minimum description length objective
function (Solomonoff, 1959; Rissanen, 1983) that
trades off the maximum likelihood against model
size. Saers et al (2013) report improvements in
BLEU score (Papineni et al, 2002) on their transla-
tion task. In our current approach instead of using a
bottom-up rule chunking approach we use a simpler
token based grammar instead. Given two grammars
(Ga and Gb) and an interpolation parameter ? the
probability function of the interpolated grammar is
given by:
pa+b (r) = ?pa (r) + (1? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. The
pseudocode for the top-down rule segmenting algo-
rithm is shown in 1. The algorithm uses the methods
collect_biaffixes, eval_dl, sort_by_delta and
make_segmentations. These methods collect all the
biaffixes in an ITG, evaluate the difference in de-
scription length, sort candidates by these differences,
and commit to a given set of candidates, respectively.
The suitable interpolation parameter is chosen em-
pirically based on the responses generated on a small
development set.
We compare the performance of inducing a token
based ITG versus inducing a segmental ITG using in-
terpolated bottom-up token based rule induction and
top-down rule segmentation. To highlight some of
the inherent challenges in adapting other algorithms
to this novel task, we also compare the quality of the
responses generated by our model to those generated
by an off-the-shelf phrase based SMT system.
4.2 Decoding heuristics
We use our in-house ITG decoder implemented ac-
cording to the algorithm mentioned in Wu (1996)
for the generating responses to challenges by decod-
ing with the trained transduction grammars. The de-
coder uses a CKY-style parsing algorithm (Cocke,
Algorithm 1 Iterative rule segmenting learning
driven by minimum description length.
1: ? ? The ITG being induced
2: repeat
3: ?sum ? 0
4: bs? collect_biaffixes(?)
5: b? ? []
6: for all b ? bs do
7: ? ? eval_dl(b,?)
8: if ? < 0 then
9: b? ? [b?, ?b, ??]
10: sort_by_delta(b?)
11: for all ?b, ?? ? b? do
12: ?? ? eval_dl(b,?)
13: if ?? < 0 then
14: ?? make_segmentations(b,?)
15: ?sum ? ?sum + ??
16: until ?sum ? 0
17: return ?
1969) with cube pruning (Chiang, 2007). The de-
coder builds an efficient hypergraph structure which
is then scored using the induced grammar. The
trained transduction grammar model was decoded
using the 4-gram language model and the model
weights determined as described in 3.3.
In our decoding algorithm, we restrict the reorder-
ing to only be monotonic as we want to produce out-
put that follows the same rhyming order of the chal-
lenge. Interleaved rhyming order is harder to evalu-
ate without the larger context of the song and we do
not address that problem in our current model. We
also penalize singleton rules to produce responses of
similar length as successive lines in a stanza are typ-
ically of similar length. Finally, we add a penalty to
reflexive translation rules that map the same surface
form to itself such as A ? yo/yo. We obtain these
rules with a high probability due to the presence of
sentence pairs where both the input and output are
identical strings as many stanzas in our data contain
repeated chorus lines.
4.3 Results: Rule segmentation improves
responses
Results in Table 1 indicate that the ISTG outperforms
the TG model towards the task of generating fluent
and rhyming responses. On the criterion of fluency,
106
Table 1: Percentage of ?good and ?acceptable (i.e., either good or acceptable) responses on fluency and rhyming
criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS,
TG+RS, ISTG+RS are models trained on rhyme scheme based corpus selection strategy. Disfluency correction strategy
was used in all cases.
model fluency (?good) fluency (?acceptable) rhyming (?good) rhyming (?acceptable)
PBSMT 3.14% 4.70% 1.57% 4.31%
TG 21.18% 54.51% 23.53% 39.21%
ISTG 26.27% 57.64% 27.45% 48.23%
PBSMT+RS 30.59% 43.53% 1.96% 9.02%
TG+RS 34.12% 60.39% 20.00% 42.74%
ISTG+RS 30.98% 61.18% 30.98% 53.72%
Table 2: Transduction rules learned by ISTG model.
transduction grammar rule log prob.
A? long/wrong -11.6747
A? rhyme/time -11.6604
A? felt bad/couldn't see what i really had -11.3196
A? matter what you say/leaving anyway -11.8792
A? arhythamatic/this rhythm is sick -12.3492
the ISTGmodel produces a significantly higher frac-
tion of sentences rated good (26.27% vs. 21.18%)
and ?acceptable (57.64% vs. 54.51%). Higher frac-
tion of responses generated by the ISTG model are
rated as good (27.45% vs. 23.53%) and ?acceptable
(57.64% vs. 54.51%) compared to the TG model.
Both TG and ISTG model perform significantly bet-
ter than the PBSMT baseline. Upon inspecting the
learned rules, we noticed that the ISTG models cap-
ture rhyming correspondences both at the token and
segmental levels. Table 2 shows some examples
of the transduction rules learned by ISTG grammar
trained using rhyme scheme detection.
5 Data selection via rhyme scheme
detection vs. adjacent lines
We now compare two data selection approaches
for generating the training data for transduction
grammar induction via a rhyme scheme detection
module and choosing all adjacent lines in a verse.
We also briefly describe the training of the rhyme
scheme detection module and determine the efficacy
of our data selection scheme by training the ISTG
model, TG model and the PBSMT baseline on train-
ing data generated with and without employing the
rhyme scheme detection module. As the rule seg-
menting approach was intended to improve the flu-
ency as opposed to the rhyming nature of the re-
sponses, we only train the rule segmenting model
on the randomly chosen subset of all adjacent lines
in the verse. Further, adding adjacent lines as the
training data to the segmenting model maintains the
context of the responses generated thereby produc-
ing higher quality responses. The segmental trans-
duction grammar model was combined with the to-
ken based transduction grammar model trained on
data selected with and without using rhyme scheme
detection model.
5.1 Rhyme scheme detection
Although our approach adapts a transduction gram-
mar induction model toward the problem of generat-
ing fluent and rhyming hip hop responses, it would
be undesirable to train the model directly on all the
successive lines of the verses?as done by Jiang and
Zhou (2008)?due to variance in hip hop rhyming
patterns. For example, adding successive lines of a
stanza which follows ABAB rhyme scheme as train-
ing instances to the transduction grammar causes in-
correct rhyme correspondences to be learned. The
fact that a verse (which is usually represented as
a separate paragraph) may contain multiple stanzas
of varying length and rhyme schemes worsens this
problem. Adding all possible pairs of lines in a verse
as training examples not only introduces a lot of
noise but also explodes the size of the training data
due to the large size of the verse.
We employ a rhyme scheme detection model (Ad-
danki and Wu, 2013) in order to select training in-
stances that are likely to rhyme. Lines belonging to
107
the same stanza and marked as rhyming according
to the rhyme scheme detection model are added to
the training corpus. We believe that this data selec-
tion scheme will improve the rhyming associations
learned during the transduction grammar induction
thereby biasing the model towards producing fluent
and rhyming output.
The rhyme scheme detection model proposes a
HMM based generative model for a verse of hip hop
lyrics similar to Reddy and Knight (2011). However,
owing to the lack of well-defined verse structure in
hip hop, a number of hidden states corresponding to
stanzas of varying length are used to automatically
obtain a soft-segmentation of the verse. Each state
in the HMM corresponds to a stanza with a particu-
lar rhyme scheme such asAA,ABAB,AAAAwhile
the emissions correspond to the final words in the
stanza. We restrict the maximum length of a stanza
to be four to maintain a tractable number of states
and further only use states to represent stanzas whose
rhyme schemes could not be partitioned into smaller
schemes without losing a rhyme correspondence.
The parameters of the HMM are estimated using
the EM algorithm (Devijer, 1985) using the corpus
generated by taking the final word of each line in the
hip hop lyrics. The lines from each stanza that rhyme
with each other according to the Viterbi parse using
the trained model are added as training instances for
transduction grammar induction. As the source and
target languages are identical, each selected pair gen-
erates two training instances: a challenge-response
and a response-challenge pair.
The training data for the rhyme scheme detector
was obtained by extracting the end-of-line tokens
from each verse. However, upon data inspection we
noticed that shorter lines in hip hop stanzas are typi-
cally joined with a comma and represented as a sin-
gle line of text and hence all the tokens before the
commas were also added to the training corpus. We
obtained a corpus containing 4.2 million tokens cor-
responding to potential rhyming candidates compris-
ing of around 153,000 unique token types.
We evaluated the performance of our rhyme
scheme detector on the task of correctly labeling a
given verse with rhyme schemes. As our model is
completely unsupervised, we chose a random sam-
ple of 75 verses from our training data as our test set.
Two native English speakers who were frequent hip
hop listeners were asked to partition the verse into
stanzas and assign them with a gold standard rhyme
scheme. Precision and recall were aggregated for the
Viterbi parse of each verse against this gold standard
and f-score was calculated. The rhyme scheme de-
tection module employed in our data selection ob-
tained a precision of 35.81% and a recall of 57.25%,
giving an f-score of 44.06%.
5.2 Training data selection via rhyme scheme
detection
We obtained around 600,000 training instances upon
extracting a training corpus using rhyme scheme de-
tection module as described in Section 5.1. We
added those lines that were adjacent and labeled as
rhyming by the rhyme scheme detector as training in-
stances resulting in a training corpus of size 200,000.
5.3 Training data selection via adjacent lines
Considering all adjacent lines in a verse resulted in
a corpus with over 5 million training instances. In
order to ensure fair comparison of models trained
on data selected using rhyme scheme detection, we
randomly chose 200,000 training instances from the
generated corpus. The training corpus thus gener-
ated shared around 15% of training instances with
the corpus generated through our proposed data se-
lection scheme.
5.4 Results: Rhyme scheme detection helps
Results in Table 1 indicate that using the rhyme
scheme detector for training data selection helps
produce significantly more fluent responses com-
pared to using adjacent lines. A possible explana-
tion for this could be that adding all adjacent lines as
training instances introduces a lot of noise into the
model which hurts the fluency of the responses gen-
erated. Also, the cumulative fraction of sentences
that were labeled good or ?acceptable on the crite-
rion of rhyming is larger when rhyme scheme detec-
tion was used to generate the training data (although
the TG model trained on the corpus generated us-
ing adjacent lines produces a higher percentage of
rhyming responses that were rated good). Given the
significantly higher rate of response fluency when
using rhyme scheme detection, we argue that using
rhyme scheme detector for data selection is benefi-
cial.
108
Table 3: English hip hop challenge-response examples.
challenge man i die to see em all thun i just don't care
tg+rs in the sky and me the in polla and the you there
tg and the ride the me the and white the i the air
pbsmt+rs man i live to see em all i just don't care
challenge did a twelve year bid in the streets and held it down
tg+rs to the girls here kid the and to the thought the now
tg to the p's here did the a the i was the the
pbsmt+rs did a year in the streets and it down
challenge oh i believe in yesterday
tg+rs can you see the day
tg now you see the way
pbsmt+rs oh i believe in tomorrow
challenge what would i do
tg+rs just me and you
tg and you and you
pbsmt+rs what would you do
challenge cause you ain't going home till the early morn
tg+rs and the you this alone i i gotta on
tg and i you my on the a home we
pbsmt+rs cause you and your friends aint nothing but
It is also interesting to note from Table 1 that
ISTG+RS performs better than TG+RS indicating
that transduction grammar induced via interpolating
token based grammar and rule segmenting produces
better responses than token based transduction gram-
mar on both data selection schemes. Although the
average fraction of responses rated good on fluency
are slightly lower for ISTG+RS compared to TG+RS
(34.12% vs. 30.98%), the fraction of responses rated
?acceptable are higher (61.18% vs. 57.64%). It is
important to note that the fraction of sentences rated
good and ?acceptable on rhyming are much larger
for ISTG+RS model. Although the fluency of the
responses generated by PBSMT+RS drastically im-
proves compared to PBSMT it still lags behind the
TG+RS and ISTG+RS models on both fluency and
rhyming. The results in Table 1 confirm our hypoth-
esis that off-the-shelf SMT systems are not guaran-
teed to be effective on our novel task.
5.5 Challenge-response examples
Table 3 shows some of the challenges and the cor-
responding responses of PBSMT+RS, TG+RS and
TG model. While PBSMT+RS and TG+RS mod-
els generate responses reflecting a high degree of
fluency, the output of the TG contains a lot of ar-
ticles. It is interesting to note that TG+RS produces
responses comparable to PBSMT+RS despite being
a token based transduction grammar. However, PB-
SMT tends to produce responses that are too simi-
lar to the challenge. Moreover, TG models produce
responses that indeed rhyme better (shown in bold-
face). In fact, TG tries to rhymewords not only at the
end but also in middle of the lines, as our transduc-
tion grammar model captures structural associations
more effectively than the phrase-based model.
6 Disfluency handling via disfluency
correction and filtering
In this section, we compare the effect of two dis-
fluency mitigating strategies on the quality of the re-
sponses generated by the PBSMT baseline and token
based transduction grammar model with and without
using rhyme scheme detection.
6.1 Correction vs. filtering
Error analysis of our initial runs showed a dis-
turbingly high proportion of responses generated by
our system that contained disfluencies with succes-
sive repetitions of words such as the and I. Upon in-
spection of data we noticed that the training lyrics
actually did contain such disfluencies and backing
vocal lines, amounting to 10% of our training data.
We therefore compared two alternative strategies to
tackle this problem. The first strategy involved fil-
tering out all lines from our training corpus which
contained such disfluencies. In the second strategy,
we implemented a disfluency detection and correc-
tion algorithm (for example, the the the, which fre-
quently occurred in the training corpus, was cor-
rected to simply the). The PBSMT baseline and the
TG model were trained on both the filtered and cor-
rected versions of the training corpus and the quality
of the responses were compared.
6.2 Results: Disfluency correction helps
The results in Table 4 indicate that the disfluency
correction strategy outperforms the filtering strategy
for both TG and TG+RS models. For the model
TG+RS, disfluency correction generated 34.12%
good responses in terms of fluency, while the filter-
ing strategy produced only 28.63% good responses.
Similarly for the model TG, disfluency correction
produced 21.8% of responses with good fluency and
the filtering strategy produced only 17.25%. Dis-
fluency correction strategy produces higher fraction
of responses with ?acceptable fluency compared to
the filtering strategy for both TG and TG+RS mod-
els. This result is not surprising, as harshly pruning
109
Table 4: Effect of the disfluency correction strategies on fluency of the responses generated for the TG induction
models vs PBSMT baselines using both rhyme scheme detection and adjacent lines as the corpus selection method.
model+disfluency strat. fluency (good) fluency (?acceptable) rhyming (good) rhyming (?acceptable)
PBSMT+filtering 4.3% 13.72% 3.53% 7.06%
PBSMT+correction 3.14% 4.70% 1.57% 4.31%
PBSMT+RS+filtering 31.76% 43.91% 12.15% 21.17%
PBSMT+RS+correction 30.59% 43.53% 1.96% 9.02%
TG+filtering 17.25% 46.27% 18.04% 33.33%
TG+correction 21.18% 54.51% 23.53% 39.21%
TG+RS+filtering 28.63% 56.86% 14.90% 34.51%
TG+RS+correction 34.12% 60.39% 20.00% 42.74%
the training corpus causes useful word association
information necessary for rhyming to be lost. Sur-
prisingly, for both PBSMT and PBSMT+RSmodels,
the disfluency correction has a negative effect on the
fluency level of the response but still falls behind TG
and TG+RS models. As disfluency correction yields
more fluent responses for TG and TG+RS models,
the results for ISTG and ISTG+RS models in Table
1 were obtained using disfluency correction strategy.
7 Maghrebi French hip hop
We have begun to apply Freestyle to rap in lan-
guages other than English, taking advantage of
the language independence and linguistics-light ap-
proach of our unsupervised transduction grammar
induction methods. With no special adaption our
transduction grammar based model performs sur-
prisingly well, even with significantly smaller train-
ing data size and noisier data. These results across
different languages are encouraging as they can be
used to discover truly language independence as-
sumptions. We briefly describe our initial experi-
ments on Maghrebi French hip hop lyrics below.
7.1 Dataset
We collected freely available French hip hop lyrics
of approximately 1300 songs. About 85% of the
songs were by Maghrebi French artists of Alge-
rian, Moroccan, or Tunisian cultural backgrounds,
while the remaining were by artists from the rest
of Francophonie. As the large majority of songs
are in Maghrebi French, the lyrics are sometimes
interspersed with romanized Arabic such as ?De la
travers?e du d?sert au bon couscous de Y?ma? (Y?ma
meansMy mother). Some songs also contain Berber
phrases, for instance ?a yemmi ino, a thizizwith?
(which means my son, a bee). Furthermore, some
songs also contained English phrases in the style of
gangster rap such as ?T'es game over, game over... Le
son de Chicken wings?. As mentioned earlier, it is
complexity like this which dissuaded us from mak-
ing language specific assumptions in our model.
We extracted the end-of-line words and obtained
a corpus containing 120,000 tokens corresponding
to potential rhyming candidates with around 29,000
unique token types which was used as the training
data for the rhyme scheme detector module. For the
transduction grammar induction, the training data
contained about 47,000 sentence pairs selected us-
ing rhyme scheme detection.
7.2 Results
After human evaluation by native French speak-
ers and frequent hip hop listeners, our transduction
grammar based model generates about 9.2% and
14.5% of the responses that are rated good by the
human evaluators on the criterion of fluency and
rhyming respectively. About 30.2% and 38% of
the responses are rated as ?acceptable. These num-
bers are encouraging given the noisy lyrics andmuch
smaller amount of training data. Some examples of
the challenge-response pairs and learned transduc-
tion rules in French are shown in Tables 5 and 6.
From Table 5, we can see that responses gener-
ated by the system rhyme with the challenges. The
first response is fluent and not only perfectly rhymes
with the challenge but also semantically valid. In the
second example, the model realizes a less common
AABA rhyme scheme through the response. The re-
110
Table 5: French hip hop challenge-response examples.
challenge Si je me trompe
response faut que je raconte
challenge Un jour je suis un livre
response et ce que je de vivre
challenge Pacha mama ils ne voient pas ta souffrance
response Combat ni leur de voulait de la d?cadence
challenge la palestine n'etait pas une terre sans peuple.
response le darfour d'autre de la guerre on est
challenge Une banlieue qui meut
response les yeux et
Table 6: Transduction rules for Maghrebi French hip hop.
transduction grammar rule log prob.
A? terre/la guerre -9.4837
A? haine/peine -9.77056
A? mal/pays natal -10.6877
A? je frissonne/mi corazon -11.0931
A? gratteurs/rappeurs -11.7306
sponse in the third example, exhibits strong rhyming
with the challenge and both the challenge and the
response contain words like souffrance, combat and
d?cadence which are related. Similarly in the fourth
example, the challenge and response also contain se-
mantically related tokens which also rhyme. These
examples illustrate that our transduction grammar
formalism coupled with our rhyme scheme detection
module does capture the necessary correspondences
between lines of hip hop lyrics without assuming any
language specific resources.
8 Conclusion
We presented a new machine learning approach for
improvising hip hop responses to challenge lyrics
by inducing stochastic transduction grammars, and
demonstrated that inducing the transduction rules by
interpolating bottom-up token based rule induction
and rule segmentation strategies outperforms a token
based baseline. We compared the performance of
our Freestylemodel against a widely used off-the-
shelf phrase-based SMT model, showing that PB-
SMT falls short in tackling the noisy and highly un-
structured domain of hip hop lyrics. We showed that
the quality of responses improves when the training
data for the transduction grammar induction is se-
lected using a rhyme scheme detector. Several do-
main related oddities such as disfluencies and back-
ing vocals have been identified and some strategies
for alleviating their effects have been compared. We
also reported results on Maghrebi French hip hop
lyrics which indicate that our model works surpris-
ingly well with no special adaptation for languages
other than English. In the future, we plan to inves-
tigate alternative training data selection techniques,
disfluency handling strategies, search heuristics, and
novel transduction grammar induction models.
Acknowledgements
This material is based upon work supported in part by
the Hong Kong Research Grants Council (RGC) research
grants GRF620811, GRF621008, GRF612806; by the
Defense Advanced Research Projects Agency (DARPA)
under BOLT contract no. HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-
0023; and by the European Union under the FP7 grant
agreement no. 287658. Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the
views of the RGC, EU, or DARPA.
References
Ananth Ramakrishnan A., Sankar Kuppan, and
Lalitha Devi Sobha. ?Automatic generation of
Tamil lyrics for melodies.? Workshop on Computa-
tional Approaches to Linguistic Creativity (CALC-09).
2009.
Karteek Addanki and DekaiWu. ?Unsupervised rhyme
scheme identification in hip hop lyrics using hidden
Markov models.? 1st International Conference on Sta-
tistical Language and Speech Processing (SLSP 2013).
2013.
Gabriele Barbieri, Fran?ois Pachet, Pierre Roy, and
Mirko Degli Esposti. ?Markov constraints for gen-
erating lyrics with style.? 20th European Conference
on Artificial Intelligence, (ECAI 2012). 2012.
David Chiang. ?Hierarchical phrase-based translation.?
Computational Linguistics, 33(2), 2007.
John Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMathemat-
ical Sciences, New York University, 1969.
P.A. Devijer. ?Baum?s forward-backward algorithm re-
visited.? Pattern Recognition Letters, 3(6), 1985.
D. Genzel, J. Uszkoreit, and F. Och. ?Poetic statisti-
cal machine translation: rhyme and meter.? 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010). Association for Computa-
tional Linguistics, 2010.
E. Greene, T. Bodrumlu, and K. Knight. ?Auto-
matic analysis of rhythmic poetry with applications
111
to generation and translation.? 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010). Association for Computational Lin-
guistics, 2010.
Long Jiang and Ming Zhou. ?Generating Chinese
couplets using a statistical MT approach.? 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008). 2008.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. ?Moses:
Open source toolkit for statistical machine translation.?
Interactive Poster and Demonstration Sessions of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2007). June 2007.
MarkLiberman. ?Rap scholarship, rapmeter, and the an-
thology of mondegreens.? http://languagelog.ldc.
upenn.edu/nll/?p=2824, December 2010. Accessed:
2013-06-30.
Franz Josef Och. ?Minimum error rate training in sta-
tistical machine translation.? 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
2003). July 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. ?BLEU: a method for automatic evalu-
ation of machine translation.? 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02). July 2002.
S. Reddy and K. Knight. ?Unsupervised discovery of
rhyme schemes.? 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL HLT 2011), vol. 2. Association for
Computational Linguistics, 2011.
Jorma Rissanen. ?A universal prior for integers and es-
timation by minimum description length.? The Annals
of Statistics, 11(2), June 1983.
Markus Saers, KarteekAddanki, andDekaiWu. ?From
finite-state to inversion transductions: Toward un-
supervised bilingual grammar induction.? 24th In-
ternational Conference on Computational Linguistics
(COLING 2012). December 2012.
Markus Saers, Karteek Addanki, and Dekai Wu.
?Combining top-down and bottom-up search for un-
supervised induction of transduction grammars.? Sev-
enth Workshop on Syntax, Semantics and Structure in
Statistical Translation (SSST-7). June 2013.
Markus Saers and Dekai Wu. ?Reestimation of reified
rules in semiring parsing and biparsing.? Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (SSST-5). Association for Computational
Linguistics, June 2011.
Ray J. Solomonoff. ?A new method for discov-
ering the grammars of phrase structure languages.?
International Federation for Information Processing
Congress (IFIP). 1959.
M. Sonderegger. ?Applications of graph theory to an
English rhyming corpus.? Computer Speech & Lan-
guage, 25(3), 2011.
Andreas Stolcke. ?SRILM ? an extensible language
modeling toolkit.? 7th International Conference on
Spoken Language Processing (ICSLP2002 - INTER-
SPEECH 2002). September 2002.
Dekai Wu. ?A polynomial-time algorithm for statisti-
cal machine translation.? 34th Annual Meeting of the
Association for Computational Linguistics (ACL96).
1996.
DekaiWu. ?Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora.? Computa-
tional Linguistics, 23(3), 1997.
Dekai Wu. ?Textual entailment recognition using inver-
sion transduction grammars.? Joaquin Qui?onero-
Candela, Ido Dagan, Bernardo Magnini, and Flo-
rence d?Alch? Buc (eds.),Machine Learning Chal-
lenges, Evaluating Predictive Uncertainty, Visual Ob-
ject Classification and Recognizing Textual Entail-
ment, First PASCAL Machine Learning Challenges
Workshop (MLCW 2005), vol. 3944 of Lecture Notes
in Computer Science. Springer, 2006.
Dekai Wu, Karteek Addanki, and Markus Saers.
?FREESTYLE: A challenge-response system for hip
hop lyrics via unsupervised induction of stochastic
transduction grammars.? 14th Annual Conference of
the International Speech Communication Association
(Interspeech 2013). 2013a.
Dekai Wu, Karteek Addanki, and Markus Saers.
?Modeling hip hop challenge-response lyrics as ma-
chine translation.? 14th Machine Translation Summit
(MT Summit XIV). 2013b.
Dekai Wu and Pascale Fung. ?Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora.? Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP 2005). Springer, 2005.
Richard Zens and HermannNey. ?A comparative study
on reordering constraints in statistical machine trans-
lation.? 41st Annual Meeting of the Association for
Computational Linguistics (ACL-2003). Association
for Computational Linguistics, 2003.
112
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375?381,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving machine translation by training against
an automatic semantic frame based evaluation metric
Chi-kiu Lo and Karteek Addanki and Markus Saers and Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|vskaddanki|masaers|dekai}@cs.ust.hk
Abstract
We present the first ever results show-
ing that tuning a machine translation sys-
tem against a semantic frame based ob-
jective function, MEANT, produces more
robustly adequate translations than tun-
ing against BLEU or TER as measured
across commonly used metrics and human
subjective evaluation. Moreover, for in-
formal web forum data, human evalua-
tors preferredMEANT-tuned systems over
BLEU- or TER-tuned systems by a sig-
nificantly wider margin than that for for-
mal newswire?even though automatic se-
mantic parsing might be expected to fare
worse on informal language. We argue
that by preserving themeaning of the trans-
lations as captured by semantic frames
right in the training process, an MT sys-
tem is constrained to make more accu-
rate choices of both lexical and reorder-
ing rules. As a result, MT systems tuned
against semantic frame based MT evalu-
ation metrics produce output that is more
adequate. Tuning a machine translation
system against a semantic frame based ob-
jective function is independent of the trans-
lation model paradigm, so, any transla-
tion model can benefit from the semantic
knowledge incorporated to improve trans-
lation adequacy through our approach.
1 Introduction
We present the first ever results of tuning a statis-
tical machine translation (SMT) system against a
semantic frame based objective function in order
to produce a more adequate output. We compare
the performance of our system with that of two
baseline SMT systems tuned against BLEU and
TER, the commonly used n-gram and edit distance
based metrics. Our system performs better than
the baseline across seven commonly used evalu-
ation metrics and subjective human evaluation on
adequacy. Surprisingly, tuning against a seman-
tic MT evaluation metric also significantly out-
performs the baseline on the domain of informal
web forum data wherein automatic semantic pars-
ing might be expected to fare worse. These results
strongly indicate that using a semantic frame based
objective function for tuning would drive develop-
ment of MT towards direction of higher utility.
Glaring errors caused by semantic role confu-
sion that plague the state-of-the-art MT systems
are a consequence of using fast and cheap lexi-
cal n-gram based objective functions like BLEU
to drive their development. Despite enforcing flu-
ency it has been established that these metrics do
not enforce translation utility adequately and often
fail to preservemeaning closely (Callison-Burch et
al., 2006; Koehn and Monz, 2006).
We argue that instead of BLEU, a metric that fo-
cuses on getting the meaning right should be used
as an objective function for tuning SMT so as to
drive continuing progress towards higher utility.
MEANT (Lo et al, 2012), is an automatic seman-
tic MT evaluation metric that measures similarity
between the MT output and the reference transla-
tion via semantic frames. It correlates better with
human adequacy judgment than other automatic
MT evaluation metrics. Since a high MEANT
score is contingent on correct lexical choices as
well as syntactic and semantic structures, we be-
lieve that tuning against MEANT would improve
both translation adequacy and fluency.
Incorporating semantic structures into SMT by
tuning against a semantic frame based evaluation
metric is independent of the MT paradigm. There-
fore, systems from different MT paradigms (such
as hierarchical, phrase based, transduction gram-
mar based) can benefit from the semantic informa-
tion incorporated through our approach.
375
2 Related Work
Relatively little work has been done towards bi-
asing the translation decisions of an SMT system
to produce adequate translations that correctly pre-
servewho did what to whom, when, where and why
(Pradhan et al, 2004). This is because the devel-
opment of SMT systemswas predominantly driven
by tuning against n-gram based evaluation met-
rics such as BLEU or edit distance based metrics
such as TER which do not sufficiently bias SMT
system?s decisions to produce adequate transla-
tions. Although there has been a recent surge of
work aimed towards incorporating semantics into
the SMT pipeline, none attempt to tune against a
semantic objective function. Below, we describe
some of the attempts to incorporate semantic in-
formation into the SMT and present a brief survey
on evaluation metrics that focus on rewarding se-
mantically valid translations.
Utilizing semantics in SMT In the past few
years, there has been a surge of work aimed at in-
corporating semantics into various stages of the
SMT. Wu and Fung (2009) propose a two-pass
model that reorders the MT output to match the
SRL of the input, which is too late to affect the
translation decisions made by the MT system dur-
ing decoding. In contrast, training against a se-
mantic objective function attempts to improve the
decoding search strategy by incorporating a bias
towards meaningful translations into the model in-
stead of postprocessing its results.
Komachi et al (2006) and Wu et al (2011) pre-
process the input sentence to match the verb frame
alternations in the output side. Liu and Gildea
(2010) and Aziz et al (2011) use input side SRL
to train a tree-to-string SMT system. Xiong et al
(2012) trained a discriminative model to predict
the position of the semantic roles in the output.
All these approaches are orthogonal to the present
question of whether to train toward a semantic ob-
jective function. Any of the above models could
potentially benefit from tuning with semantic met-
rics.
MT evaluation metrics As mentioned previ-
ously, tuning against n-gram based metrics such
as BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005) does not sufficiently drive SMT into mak-
ing decisions to produce adequate translations
that correctly preserve ?who did what to whom,
when, where and why?. In fact, a number of
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) report cases
where BLEU strongly disagrees with human judg-
ments of translation accuracy. Tuning against edit
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al, 2000), and TER
(Snover et al, 2006) also fails to sufficiently bias
SMT systems towards producing translations that
preserve semantic information.
We argue that an SMT system tuned against an
adequacy-oriented metric that correlates well with
human adequacy judgement produces more ade-
quate translations. For this purpose, we choose
MEANT, an automatic semantic MT evaluation
metric that focuses on getting the meaning right by
comparing the semantic structures of the MT out-
put and the reference. We briefly describe some
of the alternative semantic metrics below to justify
our choice.
ULC (Gim?nez and M?rquez, 2007, 2008) is
an aggregated metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement on translation
quality (Callison-Burch et al, 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al, 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an MT system against
ULC perhaps due to its expensive running time.
Lambert et al (2006) did tune on QUEEN, a sim-
plified version of ULC that discards the seman-
tic features and is based on pure lexical features.
Although tuning on QUEEN produced slightly
more preferable translations than solely tuning on
BLEU, themetric does not make use of any seman-
tic features and thus fails to exploit any potential
gains from tuning to semantic objectives.
Although TINE (Rios et al, 2011) is an recall-
oriented automatic evaluation metric which aims
to preserve the basic event structure, no work has
been done towards tuning an SMT system against
it. TINE performs comparably to BLEU andworse
than METEOR on correlation with human ade-
quacy judgment.
In contrast to TINE, MEANT (Lo et al, 2012),
which is the weighted f-score over the matched se-
mantic role labels of the automatically aligned se-
mantic frames and role fillers, outperforms BLEU,
NIST, METEOR, WER, CDER and TER. This
makes it more suitable for tuning SMT systems to
produce much adequate translations.
376
newswire BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 29.85 8.84 52.10 55.42 67.88 55.67 58.40 0.1667
TER-tuned 25.37 6.56 48.26 51.24 66.18 52.58 56.96 0.1578
MEANT-tuned 25.91 7.81 50.15 53.60 67.76 54.56 58.61 0.1676
Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
forum BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 9.58 4.10 31.77 34.63 80.09 64.54 76.12 0.1711
TER-tuned 6.94 2.21 28.55 30.85 76.15 57.96 74.73 0.1539
MEANT-tuned 7.92 3.11 30.40 33.08 77.32 61.01 74.64 0.1727
Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
3 Tuning SMT against MEANT
We now show that using MEANT as an objec-
tive function to drive minimum error rate training
(MERT) of state-of-the-art MT systems improves
MT utility not only on formal newswire text, but
even on informal forum text, where automatic se-
mantic parsing is difficult.
Toward improving translation utility of state-of-
the-art MT systems, we chose to use a strong and
competitive system in the DARPA BOLT program
as our baseline. The baseline system is a Moses
hierarchical model trained on a collection of LDC
newswire and a small portion of Chinese-English
parallel web forum data, together with a 5-gram
language model. For the newswire experiment, we
used a collection of NIST 02-06 test sets as our de-
velopment set and NIST 08 test set for evaluation.
The development and test sets contain 6,331 and
1,357 sentences respectively with four references.
For the forum data experiment, the development
and test sets were a held-out subset of the BOLT
phase 1 training data. The development and test
sets contain 2,000 sentences and 1,697 sentences
with one reference.
We use ZMERT (Zaidan, 2009) to tune the base-
line because it is a widely used, highly competi-
tive, robust, and reliable implementation of MERT
that is also fully configurable and extensible with
regard to incorporating new evaluation metrics. In
this experiment, we use aMEANT implementation
along the lines described in Lo et al (2012).
In each experiment, we tune two contrastive
conventional 100-best MERT tuned baseline sys-
tems on both newswire and forum data genres; one
tuned against BLEU, an n-gram based evaluation
metric and the other using TER, an edit distance
based metric. As semantic role labeling is expen-
sive we only tuned using 10-best list for MEANT-
tuned system. Tuning against BLEU and TER took
around 1.5 hours and 5 hours per iteration respec-
tively whereas tuning against MEANT took about
1.6 hours per iteration.
4 Results
Of course, tuning against any metric would maxi-
mize the performance of the SMT system on that
particular metric, but would be overfitting. For
example, something would be seriously wrong
if tuning against BLEU did not yield the best
BLEU scores. A far more worthwhile goal would
be to bias the SMT system to produce adequate
translations while achieving the best scores across
all the metrics. With this as our objective, we
present the results of comparing MEANT-tuned
systems against the baselines as evaluated on com-
monly used automatic metrics and human ade-
quacy judgement.
Cross-evaluation using automatic metrics Ta-
bles 1 and 2 show that MEANT-tuned systems
achieve the best scores across all other metrics in
both newswire and forum data genres, when avoid-
ing comparison of the overfit metrics too similar to
the one the system was tuned on (the cells shaded
in grey in the table: NIST and METEOR are n-
gram based metrics, similar to BLEU while WER
and CDER are edit distance based metrics, similar
to TER). In the newswire domain, however, our
system achieves marginally lower TER score than
BLEU-tuned system.
Figure 1 shows an example where the MEANT-
tuned system produced a more adequate transla-
tion that accurately preserves the semantic struc-
ture of the input sentence than the two baseline
systems. The MEANT scores for the MT output
from the BLEU-, TER- and MEANT-tuned sys-
tems are 0.0635, 0.1131 and 0.2426 respectively.
Both the MEANT score and the human evaluators
rank the MT output from the MEANT-tuned sys-
377
Figure 1: Examples of machine translation output and the corresponding semantic parses from the [B]
BLEU-, [T] TER-and [M]MEANT-tuned systems together with [IN] the input sentence and [REF] the
reference translation. Note that the MT output of the BLEU-tuned system has no semantic parse output
by the automatic shallow semantic parser.
tem as the most adequate translation. In this exam-
ple, the MEANT-tuned system has translated the
two predicates ???? and ???? in the input sen-
tence into the correct form of the predicates ?at-
tack? and ?adopted? in theMT output, whereas the
BLEU-tuned system has translated both of them
incorrectly (translates the predicates into nouns)
and the TER-tuned system has correctly translated
only the first predicate (into ?seized?) and dropped
the second predicate. Moreover, for the frame ??
?? in the input sentence, the MEANT-tuned sys-
tem has correctly translated the ARG0 ??????
??? into ?Hamas militants? and the ARG1 ??
???? into ?Gaza?. However, the TER-tuned
system has dropped the predicate ???? so that
the corresponding arguments ?The Palestinian Au-
thority? and ?into a state of emergency? have all
been incorrectly associated with the predicate ??
? /seized?. This example shows that the transla-
tion adequacy of SMT has been improved by tun-
ing against MEANT because the MEANT-tuned
system is more accurately preserving the semantic
structure of the input sentence.
Our results show that MEANT-tuned system
maintains a balance between lexical choices and
word order because it performs well on n-gram
based metrics that reward lexical matching and
edit distance metrics that penalize incorrect word
order. This is not surprising as a high MEANT
score relies on a high degree of semantic structure
matching, which is contingent upon correct lexi-
cal choices as well as syntactic and semantic struc-
tures.
Human subjective evaluation In line with our
original objective of biasing SMT systems towards
producing adequate translations, we conduct a hu-
man evaluation to judge the translation utility of
the outputs produced by MEANT-, BLEU- and
TER-tuned systems. Following the manual eval-
uation protocol of Lambert et al (2006), we ran-
domly draw 150 sentences from the test set in each
domain to form the manual evaluation set. Table
3 shows the MEANT scores of the two manual
evaluation sets. In both evaluation sets, like in the
test sets, the output from the MEANT-tuned sys-
tem score slightly higher inMEANT than that from
the BLEU-tuned system and significantly higher
than that from the TER-tuned system. The output
of each tuned MT system along the input sentence
and the reference were presented to human evalu-
ators. Each evaluation set is ranked by two evalu-
ators for measuring inter-evaluator agreement.
Table 4 indicates that output of the MEANT-
tuned system is ranked adequate more frequently
compared to BLEU- and TER-tuned baselines for
both newswire and web forum genres. The inter-
378
newswire forum
BLEU-tuned 0.1564 0.1663
TER-tuned 0.1203 0.1453
MEANT-tuned 0.1633 0.1737
Table 3: MEANT scores of each system in the 150-
sentence manual evaluation set.
newswire forum
Eval 1 Eval 2 Eval 1 Eval 2
BLEU-tuned (B) 37 42 47 42
TER-tuned (T) 22 24 28 23
MEANT-tuned (M) 55 56 59 68
B=T 14 12 0 0
M=B 5 4 8 9
M=T 4 4 4 4
M=B=T 13 9 4 4
Table 4: No. of sentences ranked the most ade-
quate by human evaluators for each system.
H1 newswire forum
MEANT-tuned > BLEU-tuned 80% 95%
MEANT-tuned > TER-tuned 99% 99%
Table 5: Significance level of accepting the alter-
native hypothesis.
evaluator agreement is 84% and 70% for newswire
and forum data genres respectively.
We performed the right-tailed two proportion
significance test on human evaluation of the SMT
system outputs for both the genres. Table 5 shows
that the MEANT-tuned system generates more ad-
equate translations than the TER-tuned system at
the 99% significance level for both newswire and
web forum genres. The MEANT-tuned system is
ranked more adequate than the BLEU-tuned sys-
tem at the 95% significance level on the web fo-
rum genre and for the newswire genre the hypoth-
esis is accepted at a significance level of 80%.
The high inter-evaluator agreement and the signif-
icance tests confirm that MEANT-tuned system is
better at producing adequate translations compared
to BLEU- or TER-tuned systems.
Informal vs. formal text The results of table
4 and 5 also show that?surprisingly?the human
evaluators preferred MEANT-tuned system out-
put over BLEU-tuned and TER-tuned system out-
put by a far wider margin on the informal forum
text compared to the formal newswire text. The
MEANT-tuned system is better than both base-
lines at the 80% significance level for the formal
text genre. For the informal text genre, it per-
forms the two baselines at the 95% significance
level. Although one might expect an semantic
frame dependent metric such as MEANT to per-
form poorly on the domain of informal text, sur-
prisingly, it nonetheless significantly outperforms
the baselines at the task of generating adequate out-
put. This indicates that the design of the MEANT
evaluation metric is robust enough to tune an SMT
system towards adequate output on informal text
domains despite the shortcomings of automatic
shallow semantic parsing.
5 Conclusion
We presented the first ever results to demon-
strate that tuning an SMT system against MEANT
produces much adequate translation than tuning
against BLEU or TER, as measured across all
other commonly used metrics and human subjec-
tive evaluation. We also observed that tuning
against MEANT succeeds in producing adequate
output significantly more frequently even on the
informal text such as web forum data. By pre-
serving the meaning of the translations as captured
by semantic frames right in the training process,
an MT system is constrained to make more accu-
rate choices of both lexical and reordering rules.
The performance of our system as measured across
all commonly used metrics indicate that tuning
against a semantic MT evaluation metric does pro-
duce output which is adequate and fluent.
We believe that tuning onMEANTwould prove
equally useful for MT systems based on any
paradigm, especially where the model does not
incorporate semantic information to improve the
adequacy of the translations produced and using
MEANT as an objective function to tune SMT
would drive sustainable development of MT to-
wards the direction of higher utility.
Acknowledgment
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
379
References
Wilker Aziz, Miguel Rios, and Lucia Specia. Shal-
low semantic trees for SMT. In Proceedings
of the Sixth Workshop on Statistical Machine
Translation (WMT2011), 2011.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?
72, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70?106,
2008.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138?145,
San Diego, California, 2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256?264, Prague, Czech Republic,
June 2007.
Jes?s Gim?nez and Llu?sM?rquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198, Colum-
bus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102?121, 2006.
Mamoru Komachi, Yuji Matsumoto, and Masaaki
Nagata. Phrase reordering for statistical ma-
chine translation based on predicate-argument
structure. In Proceedings of the 3rd Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2006), 2006.
Patrik Lambert, Jes?s Gim?nez, Marta R Costa-
juss?, Enrique Amig?, Rafael E Banchs, Llu?s
M?rquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246?249. IEEE, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.
Ding Liu and Daniel Gildea. Semantic role fea-
tures for machine translation. In Proceedings of
the 23rd international conference on Computa-
tional Linguistics (COLING-10), 2010.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess mt adequacy. In Proceed-
380
ings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 116?122. Association
for Computational Linguistics, 2011.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference
of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, Cam-
bridge, Massachusetts, August 2006.
Dekai Wu and Pascale Fung. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceed-
ings of the 2009 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics - Human Language Technolo-
gies (NAACL-HLT-09), pages 13?16, 2009.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. Extract-
ing preordering rules from predicate-argument
structures. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP-11), 2011.
Deyi Xiong, Min Zhang, and Haizhou Li. Mod-
eling the Translation of Predicate-Argument
Structure for SMT. In Proceedings of the Joint
conference of the 50th AnnualMeeting of the As-
sociation for Computational Linguistics (ACL-
12), 2012.
Omar F. Zaidan. Z-MERT: A Fully Config-
urable Open Source Tool for Minimum Error
Rate Training of Machine Translation Systems.
The Prague Bulletin of Mathematical Linguis-
tics, 91:79?88, 2009.
381
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48?57,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Combining Top-down and Bottom-up Search
for Unsupervised Induction of Transduction Grammars
Markus SAERS and Karteek ADDANKI and Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|vskaddanki|dekai}@cs.ust.hk
Abstract
We show that combining both bottom-up rule
chunking and top-down rule segmentation
search strategies in purely unsupervised learn-
ing of phrasal inversion transduction gram-
mars yields significantly better translation ac-
curacy than either strategy alone. Previous ap-
proaches have relied on incrementally building
larger rules by chunking smaller rules bottom-
up; we introduce a complementary top-down
model that incrementally builds shorter rules
by segmenting larger rules. Specifically, we
combine iteratively chunked rules from Saers
et al (2012) with our new iteratively seg-
mented rules. These integrate seamlessly be-
cause both stay strictly within a pure trans-
duction grammar framework inducing under
matching models during both training and
testing?instead of decoding under a com-
pletely different model architecture than what
is assumed during the training phases, which
violates an elementary principle of machine
learning and statistics. To be able to drive in-
duction top-down, we introduce a minimum
description length objective that trades off
maximum likelihood against model size. We
show empirically that combining the more lib-
eral rule chunking model with a more conser-
vative rule segmentation model results in sig-
nificantly better translations than either strat-
egy in isolation.
1 Introduction
In this paper we combine both bottom-up chunking
and top-down segmentation as search directions in
the unsupervised pursuit of an inversion transduc-
tion grammar (ITG); we also show that the combi-
nation of the resulting grammars is superior to ei-
ther of them in isolation. For the bottom-up chunk-
ing approach we use the method reported in Saers
et al (2012), and for the top-down segmentation ap-
proach, we introduce a minimum description length
(MDL) learning objective. The new learning objec-
tive is similar to the Bayesian maximum a poste-
riori objective, and makes it possible to learn top-
down, which is impossible using maximum likeli-
hood, as the initial grammar that rewrites the start
symbol to all sentence pairs in the training data al-
ready maximizes the likelihood of the training data.
Since both approaches result in stochastic ITGs, they
can be easily combined into a single stochastic ITG
which allows for seamless combination. The point
of our present work is that the two different search
strategies result in very different grammars so that
the combination of them is superior in terms of trans-
lation accuracy to either of them in isolation.
The transduction grammar approach has the ad-
vantage that induction, tuning and testing are op-
timized on the exact same underlying model?this
used to be a given in machine learning and statistical
prediction, but has been largely ignored in the statis-
tical machine translation (SMT) community, where
most current SMT approaches to learning phrase
translations that (a) require enormous amounts of
run-time memory, and (b) contain a high degree of
redundancy. In particular, phrase-based SMT mod-
els such as Koehn et al (2003) and Chiang (2007)
often search for candidate translation segments and
transduction rules by committing to a word align-
ment that is completely alien to the grammar, as it
is learned with very different models (Brown et al
(1993), Vogel et al (1996)), whose output is then
combined heuristically to form the alignment actu-
ally used to extract lexical segment translations (Och
48
and Ney, 2003). The fact that it is even possible
to improve the performance of a phrase-based di-
rect translation system by tossing away most of the
learned segmental translations (Johnson et al, 2007)
illustrates the above points well.
Transduction grammars can also be induced from
treebanks instead of unannotated corpora, which cuts
down the vast search space by enforcing additional,
external constraints. This approach was pioneered
by Galley et al (2006), and there has been a lot of re-
search since, usually referred to as tree-to-tree, tree-
to-string and string-to-tree, depending on where
the analyses are found in the training data. This com-
plicates the learning process by adding external con-
straints that are bound to match the translation model
poorly; grammarians of English should not be ex-
pected to care about its relationship to Chinese. It
does, however, constitute a way to borrow nonter-
minal categories that help the translation model.
It is also possible for the word alignments leading
to phrase-based SMT models to be learned through
transduction grammars (see for example Cherry and
Lin (2007), Zhang et al (2008), Blunsom et al
(2008), Saers andWu (2009), Haghighi et al (2009),
Blunsom et al (2009), Saers et al (2010), Blunsom
and Cohn (2010), Saers and Wu (2011), Neubig et
al. (2011), Neubig et al (2012)). Even when the
SMT model is hierarchical, most of the information
encoded in the grammar is tossed away, when the
learned model is reduced to a word alignment. A
word alignment can only encode the lexical relation-
ships that exist between a sentence pair according to
a single parse tree, which means that the rest of the
model: the alternative parses and the syntactic struc-
ture, is ignored.
Theminimumdescription length (MDL) objective
that we will be using to drive the learning will pro-
vide a way to escape the maximum-likelihood-of-
the-data-given-the-model optimum that we start out
with. However, going only by MDL will also lead to
a degenerate case, where the size of the grammar is
allowed to shrink regardless of how unlikely the cor-
pus becomes. Instead, we will balance the length of
the grammar with the probability of the corpus given
the grammar. This has a natural Bayesian interpreta-
tion where the length of the grammar acts as a prior
over the structure of the grammar.
Similar approaches have been used before, but to
induce monolingual grammars. Stolcke and Omo-
hundro (1994) use a method similar to MDL called
Bayesianmodel merging to learn the structure of hid-
den Markov models as well as stochastic context-
free grammars. The SCFGs are induced by allowing
sequences of nonterminals to be replaced with a sin-
gle nonterminal (chunking) as well as allowing two
nonterminals to merge into one. Gr?nwald (1996)
uses it to learn nonterminal categories in a context-
free grammar. It has also been used to interpret vi-
sual scenes by classifying the activity that goes on in
a video sequences (Si et al, 2011). Our work in this
paper is markedly different to even the previous NLP
work in that (a) we induce an inversion transduc-
tion grammar (Wu, 1997) rather than a monolingual
grammar, and (b) we focus on learning the terminal
segments rather than the nonterminal categories.
The similar Bayesian approaches to finding the
model structure of ITGs have been tried before, but
only to generate alignments that mismatched trans-
lation models are then trained on, rather than using
the ITG directly as translation model, which we do.
Zhang et al (2008) use variational Bayes with a spar-
sity prior over the parameters to prevent the size of
the grammar to explode when allowing for adjacent
terminals in the Viterbi biparses to chunk together.
Blunsom et al (2008), Blunsom et al (2009) and
Blunsom and Cohn (2010) use Gibbs sampling to
find good phrasal translations. Neubig et al (2011)
and Neubig et al (2012) use a method more similar
to ours, but with a Pitman-Yor process as prior over
the structures.
The idea of iteratively segmenting the existing
sentence pairs to find good phrasal translations has
also been tried before; Vilar and Vidal (2005) intro-
duces the Recursive Alignment Model, which recur-
sively determines whether a bispan is a good enough
translation on its own (using IBM model 1), or if it
should be split into two bispans (either in straight or
inverted order). The model uses length of the input
sentence to determine whether to split or not, and
uses very limited local information about the split
point to determine where to split. Training the pa-
rameters is done with a maximum likelihood objec-
tive. In contrast, our model is one single genera-
tive model (as opposed to an ad hoc model), trained
with a minimum description length objective (rather
than trying to maximize the probability of the train-
49
ing data).
The rest of the paper is structured so that we first
take a closer look at the minimum description length
principle that will be used to drive the top-down
search (Section 2). We then show how the top-down
grammar is learned (Sections 3 and 4), before show-
ing how we combine the new grammar with that of
Saers et al (2012) (Section 5). We then detail the
experimental setup that will substantiate our claims
empirically (Section 6) before interpreting the results
of those experiments (Section 7). Finally, we offer
some conclusions (Section 8).
2 Minimum description length
The minimum description length principle is about
finding the optimal balance between the size of a
model and the size of some data given the model
(Solomonoff (1959), Rissanen (1983)). Consider the
information theoretical problem of encoding some
datawith amodel, and then sending both the encoded
data and the information needed to decode the data
(the model) over a channel; the minimum descrip-
tion length would be the minimum number of bits
sent over the channel. The encoded data can be inter-
preted as carrying the information necessary to dis-
ambiguate the ambiguities or uncertainties that the
model has about the data. Theoretically, the model
can grow in size and become more certain about the
data, and it can shrink in size and become more un-
certain about the data. An intuitive interpretation of
this is that the exceptions, which are a part of the en-
coded data, can be moved into the model itself. By
doing so, the size of the model increases, but there
is no longer an exception that needs to be conveyed
about the data. Some ?exceptions? occur frequently
enough that it is a good idea to incorporate them into
the model, and some do not; finding the optimal bal-
ance minimizes the total description length.
Formally, the description length (DL) is:
DL (M,D) = DL (D|M) + DL (M) (1)
Where M is the model and D is the data. Note the
clear parallel to probabilities that have been moved
into the logarithmic domain.
In natural language processing, we never have
complete data to train on, so we need our models to
generalize to unseen data. A model that is very cer-
tain about the training data runs the risk of not being
able to generalize to new data: it is over-fitting. It
is bad enough when estimating the parameters of a
transduction grammar, and catastrophic when induc-
ing the structure of the grammar. The key concept
that we want to capture when learning the structure
of a transduction grammar is generalization. This is
the property that allow it to translate new, unseen,
input. The challenge is to pin down what general-
ization actually is, and how to measure it.
One property of generalization for grammars is
that it will lower the probability of the training data.
This may seem counterintuitive, but can be under-
stood as moving some of the probability mass away
from the training data and putting it in unseen data.
A second property is that rules that are specific to
the training data can be eliminated from the gram-
mar (or replaced with less specific rules that generate
the same thing). The second property would shorten
the description of the grammar, and the first would
make the description of the corpus given the gram-
mar longer. That is: generalization raises the first
term and lowers the second in Equation 1. A good
generalization will lower the total MDL, whereas a
poor onewill raise it; a good generalizationwill trade
a little data certainty for more model parsimony.
2.1 Measuring the length of a corpus
The information-theoretic view of the problem also
gives a hint at the operationalization of length. Shan-
non (1948) stipulates that the number of bits it takes
to encode that a probabilistic variable has taken a cer-
tain value can be encoded using as little as the nega-
tive logarithmic probability of that outcome.
Following this, the parallel corpus given the trans-
duction grammar gives the number of bits required
to encode it: DL (C|G) = ?log2 (P (C|G)), where
C is the corpus and G is the grammar.
2.2 Measuring the length of an ITG
Since information theory deals with encoding se-
quences of symbols, we need some way to serialize
an inversion transduction grammar (ITG) into a mes-
sage whose length can be measured.
To serialize an ITG, we first need to determine
the alphabet that the message will be written in. We
need one symbol for every nonterminal, L0-terminal
and L1-terminal. We will also make the assump-
tion that all these symbols are used in at least one
50
rule, so that it is sufficient to serialize the rules in
order to express the entire grammar. To serialize
the rules, we need some kind of delimiter to know
where one rule starts and the next ends; we will ex-
ploit the fact that we also need to specify whether the
rule is straight or inverted (unary rules are assumed
to be straight), and merge these two functions into
one symbol. This gives the union of the symbols of
the grammar and the set {[], ??}, where [] signals the
beginning of a straight rule, and ?? signals the be-
ginning of an inverted rule. The serialized format
of a rule will be: rule type/start marker, followed by
the left-hand side nonterminal, followed by all right-
hand side symbols. The symbols on the right-hand
sides are either nonterminals or biterminals?pairs
ofL0-terminals andL1-terminals that model transla-
tion equivalences. The serialized form of a grammar
is the serialized form of all rules concatenated.
Consider the following toy grammar:
S ? A, A ? ?AA?, A ? [AA] ,
A ? have/?, A ? yes/?, A ? yes/?
Its serialized form would be:
[]SA??AAA[]AAA[]Ahave?[]Ayes?[]Ayes?
Now we can, again turn to information theory to ar-
rive at an encoding for this message. Assuming a
uniform distribution over the symbols, each symbol
will require ?log2
(
1
N
)
bits to encode (where N is
the number of different symbols?the type count).
The above example has 8 symbols, meaning that
each symbol requires 3 bits. The entire message is
23 symbols long, which means that we need 69 bits
to encode it.
3 Model initialization
Rather than starting out with a general transduction
grammar and fitting it to the training data, we do the
exact opposite: we start with a transduction gram-
mar that fits the training data as well as possible, and
generalize from there. The transduction grammar
that fits the training data the best is the one where
the start symbol rewrites to the full sentence pairs
that it has to generate. It is also possible to add any
number of nonterminal symbols in the layer between
the start symbol and the bisentences without altering
the probability of the training data. We take advan-
tage of this by allowing for one intermediate sym-
bol so that the start symbol conforms to the normal
form and always rewrites to precisely one nontermi-
nal symbol. This violate the MDL principle, as the
introduction of new symbols, by definition, makes
the description of the model longer, but conforming
to the normal form of ITGs was deemedmore impor-
tant than strictly minimizing the description length.
Our initial grammar thus looks like this:
S ? A,
A ? e0..T0/f0..V0 ,
A ? e0..T1/f0..V1 ,
...,
A ? e0..TN /f0..VN
Where S is the start symbol, A is the nonterminal,
N is the number of sentence pairs in the training cor-
pus, Ti is the length of the ith output sentence (which
makes e0..Ti the ith output sentence), and Vi is the
length of the ith input sentence (which makes f0..Vi
the ith input sentence).
4 Model generalization
To generalize the initial inversion transduction gram-
mar we need to identify parts of the existing biter-
minals that could be validly used in isolation, and
allow them to combine with other segments. This
is the very feature that allows a finite transduction
grammar to generate an infinite set of sentence pairs.
Doing this moves some of the probability mass,
which was concentrated in the training data, to un-
seen data?the very definition of generalization. Our
general strategy is to propose a number of sets of
biterminal rules and a place to segment them, eval-
uate how the description length would change if we
were to apply one of these sets of segmentations to
the grammar, and commit to the best set. That is:
we do a greedy search over the power set of possi-
ble segmentations of the rule set. As we will see, this
intractable problem can be reasonable efficiently ap-
proximated, which is what we have implemented and
tested.
The key component in the approach is the ability
to evaluate how the description length would change
if a specific segmentation was made in the grammar.
51
This can then be extended to a set of segmentations,
which only leaves the problem of generating suitable
sets of segmentations.
The key to a successful segmentation is to maxi-
mize the potential for reuse. Any segment that can
be reused saves model size. Consider the terminal
rule:
A ? five thousand yen is my limit/
????????
(Chinese gloss: ?w? z?i d?o ch? w? q?an r? y?an?).
This rule can be split into three rules:
A ? ?AA?,
A ? five thousand yen/????,
A ? is my limit/????
Note that the original rule consists of 16 symbols (in
our encoding scheme), whereas the new three rules
consists of 4 + 9 + 9 = 22 symbols. It is reason-
able to believe that the bracketing inverted rule is in
the grammar already, but this still leaves 18 symbols,
which is decidedly longer than 16 symbols?and we
need to get the length to be shorter if we want to see
a net gain, since the length of the corpus given the
grammar is likely to be longer with the segmented
rules. What we really need to do is find a way to
reuse the lexical rules that came out of the segmen-
tation. Now suppose the grammar also contained this
terminal rule:
A ? the total fare is five thousand yen/
??????????
(Chinese gloss: ?z?ng g?ng de f?i y?ng sh? w? q?an
r? y?an?). This rule can also be split into three rules:
A ? [AA] ,
A ? the total fare is/??????,
A ? five thousand yen/????
Again, we will assume that the structural rule is al-
ready present in the grammar, the old rule was 19
symbols long, and the two new terminal rules are
12+9 = 21 symbols long. Again we are out of luck,
as the new rules are longer than the old one, and three
rules are likely to be less probable than one rule dur-
ing parsing. The way to make this work is to realize
that the two existing rules share a bilingual affix?a
biaffix: ?five thousand dollars? translating into ??
????. If we make the two changes at the same
time, we get rid of 16 + 19 = 35 symbols worth of
rules, and introduce a mere 9 + 9 + 12 = 30 sym-
bols worth of rules (assuming the structural rules are
already in the grammar). Making these two changes
at the same time is essential, as the length of the five
saved symbols can be used to offset the likely in-
crease in the length of the corpus given the data. And
of course: the more rules we can find with shared bi-
affixes, the more likely we are to find a good set of
segmentations.
Our algorithm takes advantage of the above obser-
vation by focusing on the biaffixes found in the train-
ing data. Each biaffix defines a set of lexical rules
paired up with a possible segmentation. We evaluate
the biaffixes by estimating the change in description
length associated with committing to all the segmen-
tations defined by a biaffix. This allows us to find
the best set of segmentations, but rather than com-
mitting only to the one best set of segmentations, we
will collect all sets which would improve descrip-
tion length, and try to commit to as many of them
as possible. The pseudocode for our algorithm is as
follows:
G // The grammar
biaffixes_to_rules // Maps biaffixes to the
// rules they occur in
biaffixes_delta = [] // A list of biaffixes and
// their DL impact on G
for each biaffix b :
delta = eval_dl(b, biaffixes_to_rules[b], G)
if (delta < 0)
biaffixes_delta.push(b, delta)
sort_by_delta(biaffixes_delta)
for each b:delta pair in biaffixes_delta :
real_delta = eval_dl(b, biaffixes_to_rules[b], G)
if (real_delta < 0)
G = make_segmentations(b, biaffixes_to_rules[b], G)
The methods eval_dl, sort_by_delta and
make_segmentations evaluates the impact on de-
scription length that committing to a biaffix would
cause, sorts a list of biaffixes according to this delta,
and applies all the changes associated with a biaffix
to the grammar, respectively.
Evaluating the impact on description length
breaks down into two parts: the difference in de-
scription length of the grammar DL (G?) ? DL (G)
(where G? is the grammar that results from applying
all the changes that committing to a biaffix dictates),
52
and the difference in description length of the corpus
given the grammar DL (C|G?) ? DL (C|G). These
two quantities are simply added up to get the total
change in description length.
The difference in grammar length is calculated
as described in Section 2.2. The difference in de-
scription length of the corpus given the grammar
can be calculated by biparsing the corpus, since
DL (C|G?) = ?log2 (P (C|p?)) and DL (C|G) =
?log2 (P (C|p)) where p? and p are the rule prob-
ability functions of G? and G respectively. Bipars-
ing is, however, a very costly process that we do not
want to have inside a loop. Instead, we assume that
we have the original corpus probability (through bi-
parsing outside the loop), and estimate the new cor-
pus probability from it (in closed form). Given that
we are splitting the rule r0 into the three rules r1,
r2 and r3, and that the probability mass of r0 is dis-
tributed uniformly over the new rules, the new rule
probability function p? will be identical to p, except
that:
p? (r0) = 0,
p? (r1) = p (r1) +
1
3
p (r0) ,
p? (r2) = p (r2) +
1
3
p (r0) ,
p? (r3) = p (r3) +
1
3
p (r0)
Since we have eliminated all the occurrences of r0
and replaced them with combinations of r1, r2 and
r3, the probability of the corpus given this new rule
probability function will be:
P
(
C|p?
)
= P (C|p) p
? (r1) p? (r2) p? (r3)
p (r0)
To make this into a description length, we need to
take the negative logarithm of the above, which re-
sults in:
DL
(
C|G?
)
=
DL (C|G) ? log2
(p? (r1) p? (r2) p? (r3)
p (r0)
)
The difference in description length of the corpus
given the grammar can now be expressed as:
DL (C|G?) ? DL (C|G) =
?log2
(
p?(r1)p?(r2)p?(r3)
p(r0)
)
To calculate the impact of a set of segmentations, we
need to take all the changes into account in one go.
We do this in a two-pass fashion, first calculating
the new probability function (p?) and the change in
grammar description length (taking care not to count
the same rule twice), and then, in the second pass,
calculating the change in corpus description length.
5 Model combination
Themodel we learn by iteratively subsegmenting the
training data is guaranteed to be parsimonious while
retaining a decent fit to the training data; these are
desirable qualities, but there is a real risk that we
failed to make some generalization that we should
have made; to counter this risk, we can use a model
trained under more liberal conditions. We chose the
approach taken by Saers et al (2012) for two rea-
sons: (a) the model has the same form as our model,
which means that we can integrate it seamlessly, and
(b) their aims are similar to ours but their method
differs significantly; specifically, they let the model
grow in size as long as the data reduces in size. Both
these qualities make it a suitable complement for our
model.
Assuming we have two grammars (Ga and Gb)
that we want to combine, the interpolation param-
eter ? will determine the probability function of the
combined grammar such that:
pa+b (r) = ?pa (r) + (1 ? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. Some
initial experiments indicated that an ? value of about
0.4 was reasonable (when Ga was the grammar ob-
tained through the training scheme outlined above,
andGb was the grammar obtained through the train-
ing scheme outlined in Saers et al (2012)), so we
used 0.4 in this paper.
6 Experimental setup
We have made the claim that iterative top-down seg-
mentation guided by the objective of minimizing the
description length gives a better precision grammar
than iterative bottom-up chunking, and that the com-
bination of the two gives superior results to either
53
0
2
4
6
8
10
12
14
 0  1  2  3  4  5  6  7Pro
bab
ility
 in 
log
 do
ma
in (M
bit)
Iterations
Figure 1: Description length in bits over the different it-
erations of top-down search. The lower portion represents
DL (G) and the upper portion represents DL (C|G).
approach in isolation. We have outlined how this
can be done in practice, and we now substantiate that
claim empirically.
We will initialize a stochastic bracketing inver-
sion transduction grammar (BITG) to rewrite it?s
one nonterminal symbol directly into all the sen-
tence pairs of the training data (iteration 0). We will
then segment the grammar iteratively a total of seven
times (iterations 1?7). For each iteration we will
record the change in description length and test the
grammar. Each iteration requires us to biparse the
training data, which we do with the cubic time algo-
rithm described in Saers et al (2009), with a beam
width of 100.
As training data, we use the IWSLT07 Chinese?
English data set (Fordyce, 2007), which contains
46,867 sentence pairs of training data, 506 Chinese
sentences of development data with 16 English ref-
erence translations, and 489 Chinese sentences with
6 English reference translations each as test data; all
the sentences are taken from the traveling domain.
Since the Chinese is written without whitespace, we
use a tool that tries to clump characters together into
more ?word like? sequences (Wu, 1999).
As the bottom-up grammar, we will reuse the
grammar learned in Saers et al (2012), specifically,
we will use the BITG that was bootstrapped from
a bracketing finite-state transduction grammar (BF-
STG) that has been chunked twice, giving bitermi-
nals where the monolingual segments are 0?4 tokens
long. The bottom-up grammar is trained on the same
0
10
20
30
40
50
60
 0  1  2  3  4  5  6  7N
um
ber
 of 
rule
s (th
ous
and
s)
Iterations
Figure 2: Number of rules learned during top-down
search over the different iterations.
data as our model.
To test the learned grammars as translation mod-
els, we first tune the grammar parameters to the train-
ing data using expectation maximization (Dempster
et al, 1977) and parse forests acquired with the
above mentioned biparser, again with a beam width
of 100. To do the actual decoding, we use our
in-house ITG decoder. The decoder uses a CKY-
style parsing algorithm (Cocke, 1969; Kasami, 1965;
Younger, 1967) and cube pruning (Chiang, 2007) to
integrate the language model scores. The decoder
builds an efficient hypergraph structure which is then
scored using both the induced grammar and the lan-
guage model. The weights for the language model
and the grammar, are tuned towards BLEU (Papineni
et al, 2002) using MERT (Och, 2003). We use the
ZMERT (Zaidan, 2009) implementation ofMERT as
it is a robust and flexible implementation of MERT,
while being loosely coupled with the decoder. We
use SRILM (Stolcke, 2002) for training a trigram
language model on the English side of the training
data. To evaluate the quality of the resulting transla-
tions, we use BLEU, and NIST (Doddington, 2002).
7 Experimental results
The results from running the experiments detailed
in the previous section can be summarized in four
graphs. Figures 1 and 2 show the size of our new,
segmenting model during induction, in terms of de-
scription length and in terms of rule count. The ini-
tial ITG is at iteration 0, where the vast majority
54
0.00
0.05
0.10
0.15
0.20
 0  1  2  3  4  5  6  7
BL
EU
Iterations
Figure 3: Variations in BLEU score over different iter-
ations. The thin line represents the baseline bottom-up
search (Saers et al, 2012), the dotted line represents the
top-down search, and the thick line represents the com-
bined results.
of the size is taken up by the model (DL (G)), and
very little by the data (DL (C|G))?just as we pre-
dicted. The trend over the induction phase is a sharp
decrease in model size, and a moderate increase in
data size, with the overall size constantly decreas-
ing. Note that, although the number of rules rises,
the total description length decreases. Again, this is
precisely what we expected. The size of the model
learned according to Saers et al (2012) is close to 30
Mbits?far off the chart. This shows that our new
top-down approach is indeed learning a more parsi-
monious grammar than the bottom-up approach.
Figures 3 and 4 shows the translation quality of
the learned model. The thin flat lines show the qual-
ity of the bottom-up approach (Saers et al, 2012),
whereas the thick curves shows the quality of the
new, top-down model presented in this paper with-
out (dotted line), and without the bottom-up model
(solid line). Although the MDL-based model is bet-
ter than the old model, the combination of the two
is still superior. It is particularly encouraging to see
that the over-fitting that seems to take place after iter-
ation 3 with the MDL-based approach is ameliorated
with the bottom-up model.
8 Conclusions
We have introduced a purely unsupervised learning
scheme for phrasal stochastic inversion transduction
grammars that is the first to combine two oppos-
0.0
1.0
2.0
3.0
4.0
5.0
 0  1  2  3  4  5  6  7
NIS
T
Iterations
Figure 4: Variations in NIST score over different iter-
ations. The thin line represents the baseline bottom-up
search (Saers et al, 2012), the dotted line represents the
top-down search, and the thick line represents the com-
bined results.
ing ways of searching for the phrasal translations: a
bottom-up rule chunking approach driven by a maxi-
mum likelihood (ML) objective and a top-down rule
segmenting approach driven by a minimum descrip-
tion length (MDL) objective. The combination ap-
proach takes advantage of the fact that the conser-
vative top-down MDL-driven rule segmenting ap-
proach learns a very parsimonious, yet competitive,
model when compared to a liberal bottom-up ML-
driven approach. Results show that the combination
of the two opposing approaches is significantly su-
perior to either of them in isolation.
9 Acknowledgements
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
55
References
P. Blunsom and T. Cohn. Inducing syn-
chronous grammars with slice sampling. In
HLT/NAACL2010, pages 238?241, Los Angeles,
California, June 2010.
P. Blunsom, T. Cohn, and M. Osborne. Bayesian
synchronous grammar induction. In Proceedings
of NIPS 21, Vancouver, Canada, December 2008.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. A
gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of ACL/IJCNLP, pages
782?790, Suntec, Singapore, August 2009.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L.Mercer. TheMathematics ofMachine Trans-
lation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311, 1993.
C. Cherry and D. Lin. Inversion transduction gram-
mar for joint phrasal translation modeling. In Pro-
ceedings of SSST, pages 17?24, Rochester, New
York, April 2007.
D. Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228, 2007.
J. Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMath-
ematical Sciences, New York University, 1969.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38, 1977.
G. Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In Proceedings of the 2nd International
Conference on Human Language Technology Re-
search, pages 138?145, San Diego, California,
2002.
C. S. Fordyce. Overview of the IWSLT 2007 evalu-
ation campaign. In Proceedings of IWSLT, pages
1?12, 2007.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. Scalable infer-
ence and training of context-rich syntactic trans-
lation models. In Proceedings of COLING/ACL-
2006, pages 961?968, Sydney, Australia, July
2006.
Peter Gr?nwald. A minimum description length ap-
proach to grammar inference in symbolic. Lecture
Notes in Artificial Intelligence, (1040):203?216,
1996.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein.
Better word alignments with supervised itg mod-
els. In Proceedings of ACL/IJCNLP-2009, pages
923?931, Suntec, Singapore, August 2009.
H. Johnson, J. Martin, G. Foster, and R. Kuhn.
Improving translation quality by discarding most
of the phrasetable. In Proceedings of EMNLP-
CoNLL-2007, pages 967?975, Prague, Czech Re-
public, June 2007.
T. Kasami. An efficient recognition and syntax anal-
ysis algorithm for context-free languages. Tech-
nical Report AFCRL-65-00143, Air Force Cam-
bridge Research Laboratory, 1965.
P. Koehn, F. J. Och, and D. Marcu. Statistical
Phrase-Based Translation. In Proceedings of
HLT/NAACL-2003, volume 1, pages 48?54, Ed-
monton, Canada, May/June 2003.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. An unsupervised model for joint
phrase alignment and extraction. In Proceedings
of ACL/HLT-2011, pages 632?641, Portland, Ore-
gon, June 2011.
G. Neubig, T. Watanabe, S. Mori, and T. Kawahara.
Machine translation without words through sub-
string alignment. In Proceedings of ACL-2012,
pages 165?174, Jeju Island, Korea, July 2012.
F. J. Och and H. Ney. A Systematic Comparison of
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51, 2003.
F. J. Och. Minimum error rate training in statistical
machine translation. InProceedings of ACL-2003,
pages 160?167, Sapporo, Japan, July 2003.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL-2002,
pages 311?318, Philadelphia, Pennsylvania, July
2002.
J. Rissanen. A universal prior for integers and esti-
mation by minimum description length. The An-
nals of Statistics, 11(2):416?431, June 1983.
56
M. Saers and D. Wu. Improving phrase-based
translation via word alignments from Stochastic
Inversion Transduction Grammars. In Proceed-
ings of SSST-3, pages 28?36, Boulder, Colorado,
June 2009.
M. Saers and D. Wu. Principled induction of phrasal
bilexica. In Proceedings of EAMT-2011, pages
313?320, Leuven, Belgium, May 2011.
M. Saers, J. Nivre, and D. Wu. Learning stochastic
bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings
of IWPT?09, pages 29?32, Paris, France, October
2009.
M. Saers, J. Nivre, and D. Wu. Word alignment with
stochastic bracketing linear inversion transduc-
tion grammar. In Proceedings of HLT/NAACL-
2010, pages 341?344, Los Angeles, California,
June 2010.
M. Saers, K. Addanki, and D. Wu. From finite-state
to inversion transductions: Toward unsupervised
bilingual grammar induction. In Proceedings of
COLING 2012: Technical Papers, pages 2325?
2340, Mumbai, India, December 2012.
C. E. Shannon. A mathematical theory of com-
munication. The Bell System Technical Journal,
27:379?423, 623?, July, October 1948.
Z. Si, M. Pei, B. Yao, and S. Zhu. Unsuper-
vised learning of event and-or grammar and se-
mantics from video. In Proceedings of the 2011
IEEE International Conference on Computer Vi-
sion (ICCV), pages 41?48, November 2011.
R. J. Solomonoff. A new method for discovering the
grammars of phrase structure languages. In IFIP
Congress, pages 285?289, 1959.
A. Stolcke and S. Omohundro. Inducing proba-
bilistic grammars by bayesian model merging. In
R. C. Carrasco and J. Oncina, editors, Grammat-
ical Inference and Applications, pages 106?118.
Springer, 1994.
A. Stolcke. SRILM ? an extensible language model-
ing toolkit. In Proceedings of ICSLP-2002, pages
901?904, Denver, Colorado, September 2002.
J. M. Vilar and E. Vidal. A recursive statistical trans-
lation model. In ACL-2005 Workshop on Building
andUsing Parallel Texts, pages 199?207, AnnAr-
bor, Jun 2005.
S. Vogel, H. Ney, and C. Tillmann. HMM-based
Word Alignment in Statistical Translation. In Pro-
ceedings of COLING-96, volume 2, pages 836?
841, 1996.
D. Wu. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403, 1997.
Z. Wu. LDC Chinese segmenter, 1999.
D. H. Younger. Recognition and parsing of context-
free languages in time n3. Information and Con-
trol, 10(2):189?208, 1967.
O. F. Zaidan. Z-MERT: A Fully Configurable Open
Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague
Bulletin of Mathematical Linguistics, 91:79?88,
2009.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea.
Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of
ACL-08: HLT, pages 97?105, Columbus, Ohio,
June 2008.
57
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 67?73,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Unsupervised Transduction Grammar Induction
via Minimum Description Length
Markus Saers and Karteek Addanki and Dekai Wu
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|vskaddanki|dekai}@cs.ust.hk
Abstract
We present a minimalist, unsupervised
learning model that induces relatively
clean phrasal inversion transduction gram-
mars by employing the minimum descrip-
tion length principle to drive search over
a space defined by two opposing ex-
treme types of ITGs. In comparison to
most current SMT approaches, the model
learns a very parsimonious phrase trans-
lation lexicons that provide an obvious
basis for generalization to abstract trans-
lation schemas. To do this, the model
maintains internal consistency by avoid-
ing use of mismatched or unrelated mod-
els, such as word alignments or probabil-
ities from IBM models. The model in-
troduces a novel strategy for avoiding the
pitfalls of premature pruning in chunking
approaches, by incrementally splitting an
ITGwhile using a second ITG to guide this
search.
1 Introduction
We introduce an unsupervised approach to induc-
ing parsimonious, relatively clean phrasal inver-
sion transduction grammars or ITGs (Wu, 1997)
that employs a theoretically well-founded mini-
mum description length (MDL) objective to ex-
plicitly drive two opposing, extreme ITGs to-
wards one minimal ITG. This represents a new
attack on the problem suffered by most current
SMT approaches of learning phrase translations
that require enormous amounts of run-time mem-
ory, contain a high degree of redundancy, and fails
to provide an obvious basis for generalization to
abstract translation schemas. In particular, phrasal
SMT models such as Koehn et al (2003) and Chi-
ang (2005) often search for candidate translation
segments and transduction rules by committing
to a word alignment based on very different as-
sumptions (Brown et al, 1993; Vogel et al, 1996),
and heuristically derive lexical segment transla-
tions (Och and Ney, 2003). In fact, it is possible
to improve the performance by tossing away most
of the learned segmental translations (Johnson et
al., 2007). In addition to preventing such waste-
fulness, our work aims to also provide an obvi-
ous basis for generalization to abstract translation
schemas by driving the search for phrasal rules by
simultaneously using two opposing types of ITG
constraints that have both individually been empir-
ically proven to match phrase reordering patterns
across translations well.
We adopt a more ?pure? methodology for eval-
uating transduction grammar induction than typ-
ical system building papers. Instead of embed-
ding our learned ITG in the midst of many other
heuristic components for the sake of a short term
boost in BLEU, we focus on scientifically under-
standing the behavior of pure MDL-based search
for phrasal translations, divorced from the effect
of other variables, even though BLEU is naturally
much lower this way. The common practice of
plugging some aspect of a learned ITG into ei-
ther (a) a long pipeline of training heuristics and/or
(b) an existing decoder that has been patched up
to compensate for earlier modeling mistakes, as
we and others have done before?see for example
Cherry and Lin (2007); Zhang et al (2008); Blun-
som et al (2008, 2009); Haghighi et al (2009);
Saers and Wu (2009, 2011); Blunsom and Cohn
(2010); Burkett et al (2010); Riesa and Marcu
(2010); Saers et al (2010); Neubig et al (2011,
2012)?obscures the specific traits of the induced
grammar. Instead, we directly use our learned
ITG in translation mode (any transduction gram-
mar also represents a decoder when parsing with
the input sentence as a hard constraint) which al-
lows us to see exactly which aspects of correct
translation the transduction rules have captured.
67
When the structure of an ITG is induced without
supervision, it has so far been assumed that smaller
rules get clumped together into larger rules. This
is a natural way to search, since maximum like-
lihood (ML) tends to improve with longer rules,
which is typically balanced with Bayesian priors
(Zhang et al, 2008). Bayesian priors are also used
in Gibbs sampling (Blunsom et al, 2008, 2009;
Blunsom and Cohn, 2010), as well as other non-
parametric learning methods (Neubig et al, 2011,
2012). All of the above evaluate their models by
feeding them into mismatched decoders, making it
hard to evaluate how accurate the learned models
themselves were. In this work we take a radically
different approach, and start with the longest rules
possible and attempt to segment them into shorter
rules iteratively. This makes ML useless, since our
initial model maximizes it. Instead, we balance the
ML objective with a minimum description length
(MDL) objective, which let us escape the initial
ML optimum by rewarding model parsimony.
Transduction grammars can also be induced
with supervision from treebanks, which cuts down
the search space by enforcing external constraints
(Galley et al, 2006). This complicates the learn-
ing process by adding external constraints that are
bound to match the translation model poorly. It
does, however, constitute a way to borrow nonter-
minal categories that help the translation model.
MDL has been used before in monolingual
grammar induction (Gr?nwald, 1996; Stolcke and
Omohundro, 1994), as well as to interpret visual
scenes (Si et al, 2011). Our work is markedly dif-
ferent in that we (a) induce an ITG rather than a
monolingual grammar, and (b) focus on learning
the terminal segments rather than the nonterminal
categories. Iterative segmentation has also been
used before, but only to derive a word alignment
as part of a larger pipeline (Vilar and Vidal, 2005).
The paper is structured as follows: we start by
describing theMDL principle (Section 2). We then
describe the initial ITGs (Section 3), followed by
the algorithm that induces an MDL-optimal ITG
from them (Section 4). After that we describe the
experiments (Section 5), and the results (Section
6). Finally, we offer some conclusions (Section 7).
2 Minimum description length
The minimum description length principle is about
finding the optimal balance between the size of a
model and the size of some data given the model
(Solomonoff, 1959; Rissanen, 1983). Consider the
information theoretical problem of encoding some
data with a model, and then sending both the en-
coded data and the information needed to decode
the data (the model) over a channel; the minimum
description length is the minimum number of bits
sent over the channel. The encoded data can be in-
terpreted as carrying the information necessary to
disambiguate the uncertainties that the model has
about the data. The model can grow in size and be-
comemore certain about the data, and it can shrink
in size and become more uncertain about the data.
Formally, description length (DL) is:
DL (?, D) = DL (D|?) + DL (?)
where ? is the model and D is the data.
In practice, we rarely have complete data to train
on, so we need our models to generalize to unseen
data. Amodel that is very certain about the training
data runs the risk of not being able to generalize to
new data: it is over-fitting. It is bad enough when
estimating the parameters of a transduction gram-
mar, and catastrophic when inducing the structure.
The information-theoretic view of the problem
gives a hint at the operationalization of descrip-
tion length of a corpus given a grammar. Shannon
(1948) stipulates that we can get a lower bound on
the number of bits required to encode a specific
outcome of a random variable. We thus define de-
scription length of the corpus given the grammar
to be: DL (D|?) = ?lgP (D|?)
Information theory is also useful for the descrip-
tion length of the grammar: if we can find a way
to serialize the grammar into a sequence of tokens,
we can figure out how that sequence can be opti-
mally encoded. To serialize an ITG, we first need
to determine the alphabet that the message will be
written in. We need one symbol for every nonter-
minal, L0- and L1-terminal. We will also make
the assumption that all these symbols are used in
at least one rule, so that it is sufficient to serial-
ize the rules in order to express the entire ITG.
We serialize a rule with a type marker, followed
by the left-hand side nonterminal, followed by all
the right-hand side symbols. The type marker is
either [] denoting the start of a straight rule, or
?? denoting the start of an inverted rule. Unary
rules are considered to be straight. We serialize
the ITG by concatenating the serialized form of all
the rules, assuming that each symbol can be serial-
ized into?lgc bits where c is the symbol?s relative
frequency in the serialized form of the ITG.
68
3 Initial ITGs
To tackle the exponential problem of searching for
an ITG that minimizes description length, it is use-
ful to contrast two extreme forms of ITGs. De-
scription length has two components, model length
and data length. We call an ITG that minimizes
the data at the expense of the model a long ITG;
we call an ITG that minimizes the model at the ex-
pense of the data a short ITG.1 The long ITG sim-
ply has all the sentence pairs as biterminals:
S ? A
A ? e0..T0/f0..V0
A ? e0..T1/f0..V1
...
A ? e0..TN /f0..VN
where S is the start symbol, A is the nonterminal,
N is the number of sentence pairs, Ti is the length
of the ith output sentence (making e0..Ti the ith out-
put sentence), and Vi is the length of the ith input
sentence (making f0..Vi the ith input sentence). The
short ITG is a token-based bracketing ITG:
S ? A, A? [AA] , A? ?AA?,
A? e/f, A? e/?, A? ?/f
where, S is the start symbol, A is the nonterminal
symbol, e is an L0-token, f is an L1-token, and ?
is the empty sequence of tokens.
4 Shortening the long ITG
To shorten the long ITG,wewill identify good split
candidates in the terminal rules by parsing them
with the short ITG, and commit to split candidates
that give a net gain. A split candidate is an exist-
ing long terminal rule, information about where to
split its right-hand side, and whether to invert the
resulting two rules or not. Consider the terminal
rule A ? es..t/fu..v; it can be split at any point S
in L0 and any point U in L1, giving the three rules
A ? [AA], A ? es..S/fu..U and A ? eS..t/fU..v
when it is split in straight order, and the three rules
A? ?AA?, A? es..S/fU..v and A? eS..t/fu..U
when it is split in inverted order. We will refer to
the original long rule as r0, and the resulting three
rules as r1, r2 and r3.
To identify the split candidates and to figure out
how the probability mass of r0 is to be distributed
1Long and short ITGs correspond well to ad-hoc and
promiscuous grammars in Gr?nwald (1996).
Algorithm 1 Rule shortening.
Gl ? The long ITG
Gs ? The short ITG
repeat
cands? collect_candidates(Gl, Gs)
? ? 0
removed? {}
repeat
score(cands)
sort_by_delta(cands)
for all c ? cands do
r ? original_rule(c)
if r /? removed and ?c ? 0 then
Gl ? update_grammar(Gl, c)
removed? {r} ? removed
? ? ? + ?c
end if
end for
until ? ? 0
until ? ? 0
return Gl
to the new rules, we use the short ITG to biparse the
right-hand side of r0. The distribution is derived
from the inside probability of the bispans that the
new rules are covering in the chart, and we refer
to them as ?1, ?2 and ?3, where the index indi-
cates which new rule they apply to. This has the
effect of preferring to split a rule into parts that are
roughly equally probable, as the size of the data is
minimized when the weights are equal.
To choose which split candidates to commit to,
we need a way to estimate their impact on the to-
tal MDL score of the model. This breaks down
into two parts: the difference in description length
of the grammar: DL (??) ? DL (?) (where ?? is
? after committing to the split candidate), and the
difference in description length of the corpus given
the grammar: DL (D|??) ? DL (D|?). The two
are added up to get the total change in description
length.The difference in grammar length is calcu-
lated as described in Section 2. The difference in
description length of the corpus given the grammar
can be calculated by biparsing the corpus, since
DL (D|??) = ?lgP (D|p?) and DL (D|?) =
?lgP (D|p) where p? and p are the rule probabil-
ity functions of ?? and ? respectively. Bipars-
ing is, however, a very costly process that we do
not want to carry out for every candidate. Instead,
we assume that we have the original corpus proba-
bility (through biparsing when generating the can-
69
Table 1: The results of decoding. NIST and BLEU are the translation scores at each iteration, followed
by the number of rules in the grammar, followed by the average (as measured by mean and mode) number
of English tokens in the rules.
Iteration NIST BLEU Rules Mean Mode
1 2.7015 11.97 43,704 7.20 6
2 4.0116 14.04 42,823 6.30 6
3 4.1654 16.58 41,867 5.68 2
4 4.3723 17.43 40,953 5.23 1
5 4.2032 18.78 40,217 4.97 1
6 4.1329 17.28 39,799 4.84 1
7 4.0710 17.31 39,587 4.79 1
8 4.0437 17.10 39,470 4.75 1
didates), and estimate the new corpus probability
from it (in closed form). The new rule probability
function p? is identical to p, except that:
p? (r0) = 0
p? (r1) = p (r1) + ?1p (r0)
p? (r2) = p (r2) + ?2p (r0)
p? (r3) = p (r3) + ?3p (r0)
We assume the probability of the corpus given this
new rule probability function to be:
P
(
D|p?
)
= P (D|p) p
? (r1) p? (r2) p? (r3)
p (r0)
This gives the following description length differ-
ence:
DL (D|??)? DL (D|?) =
?lgp
?(r1)p?(r2)p?(r3)
p(r0)
We will commit to all split candidates that are es-
timated to lower the DL, restricting it so that any
original rule is split only in the best way (Algo-
rithm 1).
5 Experimental setup
To test whether minimum description length is a
good driver for unsupervised inversion transduc-
tion induction, we implemented and executed the
method described above. We start by initializing
one long and one short ITG. The parameters of the
long ITG cannot be adjusted to fit the data better,
but the parameters of the short ITG can be tuned to
the right-hand sides of the long ITG.We do so with
an implementation of the cubic time algorithm de-
scribed in Saers et al (2009), with a beam width
of 100. We then run the introduced algorithm.
As training data, we use the IWSLT07 Chinese?
English data set (Fordyce, 2007), which contains
46,867 sentence pairs of training data, and 489
Chinese sentences with 6 English reference trans-
lations each as test data; all the sentences are taken
from the traveling domain. Since the Chinese is
written without whitespace, we use a tool that tries
to clump characters together into more ?word like?
sequences (Wu, 1999).
After each iteration, we use the long ITG to
translate the held out test set with our in-house ITG
decoder. The decoder uses a CKY-style parsing
algorithm (Cocke, 1969; Kasami, 1965; Younger,
1967) and cube pruning (Chiang, 2007) to inte-
grate the language model scores. The decoder
builds an efficient hypergraph structure which is
scored using both the induced grammar and a lan-
guage model. We use SRILM (Stolcke, 2002) for
training a trigram language model on the English
side of the training corpus. To evaluate the re-
sulting translations, we use BLEU (Papineni et al,
2002) and NIST (Doddington, 2002).
We also perform a combination experiment,
where the grammar at different stages of the learn-
ing process (iterations) are interpolated with each
other. This is a straight-forward linear interpola-
tion, where the probabilities of the rules are added
up and the grammar is renormalized. Although
it makes little sense from an MDL point of view
to increase the size of the grammar so indiscrim-
inately, it does make sense from an engineering
point of view, since more rules typically means
better coverage, which in turn typically means bet-
ter translations of unknown data.
6 Results
As discussed at the outset, rather than burying our
learned ITG in many layers of unrelated heuristics
just to push up the BLEU score, we think it is more
70
Table 2: The results of decoding with combined grammars. NIST and BLEU are the translation scores for
each combination, followed by the number of rules in the grammar, followed by the average (as measured
by mean and mode) number of English tokens in the rules.
Combination NIST BLEU Rules Mean Mode
1?2 (2 grammars) 4.2426 15.28 74,969 6.69 6
3?4 (2 grammars) 4.5087 18.75 54,533 5.41 3
5?6 (2 grammars) 4.1897 18.19 44,264 4.86 1
7?8 (2 grammars) 4.0953 17.40 40,785 4.79 1
1?4 (4 grammars) 4.9234 19.98 109,183 6.19 5
5?8 (4 grammars) 4.1089 17.86 47,504 4.84 1
1?8 (8 grammars) 4.8649 20.41 124,423 5.92 3
important to illuminate scientific understanding of
the behavior of pure MDL-driven rule induction
without interference from other variables. Directly
evaluating solely the ITG in translation mode?
instead of (a) deriving word alignments from it by
committing to only the one-best parse, but then dis-
carding any trace of structure and/or (b) evaluating
it through a decoder that has been patched up to
compensate for deficiencies in disparate aspects of
translation?allows us to see exactly how accurate
the learned transduction rules are.
The results from the individual iterations (Table
1) show that we learn very parsimonious models
that far outperforms the only other result we are
aware of where an ITG is tested exactly as it was
learned without altering the model itself: Saers et
al. (2012) induce a pure ITG by iteratively chunk-
ing rules, but they report significantly lower trans-
lation quality (8.30 BLEU and 0.8554 NIST) de-
spite a significantly larger ITG (251,947 rules).
The average rule length also decreases as smaller
reusable spans are found. The English side of the
training data has amean of 8.45 and amode of 7 to-
kens per sentence, and these averages drop steadily
during training. It is very encouraging to see the
mode drop to one so quickly, as this indicates that
the learning algorithm finds translations of individ-
ual English words. Not only are the rules getting
fewer, but they are also getting shorter.
The results from the combination experiments
(Table 2) corroborate the engineering intuition that
more rules give better translations at the expense of
a larger model. Using all eight grammars gives a
BLEU score of 20.41, at the expense of approxi-
mately tripling the size of the grammar. All indi-
vidual iterations benefit from being combined with
other iterations?but for the very best iterations
more additional data is needed to get this improve-
ment; the fifth iteration, which excelled at BLEU
score needs to be combinedwith all other iterations
to see an improvement, whereas the first and sec-
ond iterations only need each other to see an im-
provement.
7 Conclusions
We have presented a minimalist, unsupervised
learning model that induces relatively clean
phrasal ITGs by iteratively splitting existing rules
into smaller rules using a theoretically well-
founded minimum description length objective.
The resulting translation model is very parsimo-
nious and provide an obvious foundation for gen-
eralization tomore abstract transduction grammars
with informative nonterminals.
8 Acknowledgements
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
References
Phil Blunsom and Trevor Cohn. Inducing syn-
chronous grammars with slice sampling. In
HLT/NAACL2010, pages 238?241, Los Ange-
les, California, June 2010.
71
Phil Blunsom, Trevor Cohn, and Miles Osborne.
Bayesian synchronous grammar induction. In
Proceedings of NIPS 21, Vancouver, Canada,
December 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, andMiles
Osborne. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of
the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, pages 782?790, Suntec, Singapore,
August 2009.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The Mathe-
matics of Machine Translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, 1993.
David Burkett, John Blitzer, and Dan Klein. Joint
parsing and alignment with weakly synchro-
nized grammars. In HLT/NAACL?10, pages
127?135, Los Angeles, California, June 2010.
Colin Cherry and Dekang Lin. Inversion transduc-
tion grammar for joint phrasal translation mod-
eling. In Proceedings of SSST?07, pages 17?24,
Rochester, New York, April 2007.
David Chiang. A hierarchical phrase-based model
for statistical machine translation. In Proceed-
ings of ACL?05, pages 263?270, Ann Arbor,
Michigan, June 2005.
David Chiang. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?
228, 2007.
John Cocke. Programming languages and their
compilers: Preliminary notes. Courant Institute
of Mathematical Sciences, New York Univer-
sity, 1969.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. InProceedings of HLT?02,
pages 138?145, San Diego, California, 2002.
C. S. Fordyce. Overview of the IWSLT 2007 eval-
uation campaign. In Proceedings IWSLT?07,
pages 1?12, 2007.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang, and
Ignacio Thayer. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings COLING/ACL?06, pages 961?968,
Sydney, Australia, July 2006.
Peter Gr?nwald. A minimum description
length approach to grammar inference in sym-
bolic. Lecture Notes in Artificial Intelligence,
(1040):203?216, 1996.
Aria Haghighi, John Blitzer, John DeNero, and
Dan Klein. Better word alignments with
supervised itg models. In Proceedings of
ACL/IJCNLP?09, pages 923?931, Suntec, Sin-
gapore, August 2009.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. Improving translation quality
by discarding most of the phrasetable. In Pro-
ceedings EMNLP/CoNLL?07, pages 967?975,
Prague, Czech Republic, June 2007.
TadaoKasami. An efficient recognition and syntax
analysis algorithm for context-free languages.
Technical Report AFCRL-65-00143, Air Force
Cambridge Research Laboratory, 1965.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. Statistical Phrase-Based Translation.
In Proceedings of HLT/NAACL?03, volume 1,
pages 48?54, Edmonton, Canada, May/June
2003.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. An
unsupervised model for joint phrase alignment
and extraction. In Proceedings of ACL/HLT?11,
pages 632?641, Portland, Oregon, June 2011.
Graham Neubig, Taro Watanabe, Shinsuke Mori,
and Tatsuya Kawahara. Machine translation
without words through substring alignment. In
Proceedings of ACL?12, pages 165?174, Jeju Is-
land, Korea, July 2012.
Franz Josef Och and Hermann Ney. A Systematic
Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?
51, 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of ACL?02, pages 311?318, Philadelphia,
Pennsylvania, July 2002.
Jason Riesa andDanielMarcu. Hierarchical search
for word alignment. In Proceedings of ACL?10,
pages 157?166, Uppsala, Sweden, July 2010.
Jorma Rissanen. A universal prior for integers and
estimation by minimum description length. The
Annals of Statistics, 11(2):416?431, June 1983.
72
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
Stochastic Inversion Transduction Grammars.
In Proceedings of SSST?09, pages 28?36, Boul-
der, Colorado, June 2009.
Markus Saers and Dekai Wu. Principled induction
of phrasal bilexica. In Proceedings of EAMT?11,
pages 313?320, Leuven, Belgium, May 2011.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In Proceedings of IWPT?09, pages
29?32, Paris, France, October 2009.
Markus Saers, JoakimNivre, and DekaiWu. Word
alignment with stochastic bracketing linear in-
version transduction grammar. In Proceedings
of HLT/NAACL?10, pages 341?344, Los Ange-
les, California, June 2010.
Markus Saers, Karteek Addanki, and Dekai Wu.
From finite-state to inversion transductions: To-
ward unsupervised bilingual grammar induc-
tion. In Proceedings of COLING 2012: Techni-
cal Papers, pages 2325?2340, Mumbai, India,
December 2012.
Claude Elwood Shannon. A mathematical theory
of communication. The Bell System Technical
Journal, 27:379?423, 623?656, July, October
1948.
Zhangzhang Si, Mingtao Pei, Benjamin Yao,
and Song-Chun Zhu. Unsupervised learning
of event and-or grammar and semantics from
video. In Proceedings of the 2011 IEEE ICCV,
pages 41?48, November 2011.
Ray J. Solomonoff. A new method for discovering
the grammars of phrase structure languages. In
IFIP Congress, pages 285?289, 1959.
Andreas Stolcke and Stephen Omohundro. Induc-
ing probabilistic grammars by bayesian model
merging. In R. C. Carrasco and J. Oncina, ed-
itors, Grammatical Inference and Applications,
pages 106?118. Springer, 1994.
Andreas Stolcke. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the
International Conference on Spoken Language
Processing, pages 901?904, Denver, Colorado,
September 2002.
Juan Miguel Vilar and Enrique Vidal. A recur-
sive statistical translation model. In ACL-2005
Workshop on Building and Using Parallel Texts,
pages 199?207, Ann Arbor, Jun 2005.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. HMM-basedWord Alignment in Statisti-
cal Translation. In Proceedings of COLING-96,
volume 2, pages 836?841, 1996.
Dekai Wu. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Par-
allel Corpora. Computational Linguistics,
23(3):377?403, 1997.
Zhibiao Wu. LDC Chinese segmenter, 1999.
Daniel H. Younger. Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189?208, 1967.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In Proceedings of ACL/HLT?08, pages 97?
105, Columbus, Ohio, June 2008.
73
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 112?121,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transduction Recursive Auto-Associative Memory:
Learning Bilingual Compositional Distributed Vector Representations of
Inversion Transduction Grammars
Karteek Addanki Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{vskaddanki|dekai}@cs.ust.hk
Abstract
We introduce TRAAM, or Transduction
RAAM, a fully bilingual generalization
of Pollack?s (1990) monolingual Recur-
sive Auto-AssociativeMemory neural net-
workmodel, in which each distributed vec-
tor represents a bilingual constituent?i.e.,
an instance of a transduction rule, which
specifies a relation between two monolin-
gual constituents and how their subcon-
stituents should be permuted. Bilingual
terminals are special cases of bilingual
constituents, where a vector represents ei-
ther (1) a bilingual token?a token-to-
token or ?word-to-word? translation rule
?or (2) a bilingual segment?a segment-
to-segment or ?phrase-to-phrase? transla-
tion rule. TRAAMs have properties that
appear attractive for bilingual grammar in-
duction and statistical machine translation
applications. Training of TRAAM drives
both the autoencoder weights and the vec-
tor representations to evolve, such that
similar bilingual constituents tend to have
more similar vectors.
1 Introduction
We introduce Transduction RAAM?or TRAAM
for short?a recurrent neural network model that
generalizes the monolingual RAAMmodel of Pol-
lack (1990) to a distributed vector representation
of compositionally structured transduction gram-
mars (Aho andUllman, 1972) that is fully bilingual
from top to bottom. In RAAM, which stands for
Recursive Auto-Associative Memory, using fea-
ture vectors to characterize constituents at every
level of a parse tree has the advantages that (1)
the entire context of all subtrees inside the con-
stituent can be efficiently captured in the feature
vectors, (2) the learned representations generalize
well because similar feature vectors represent sim-
ilar constituents or segments, and (3) representa-
tions can be automatically learned so as to max-
imize prediction accuracy for various tasks using
semi-supervised learning. We argue that different,
but analogous, properties are desirable for bilin-
gual structured translation models.
Unlike RAAM, where each distributed vector
represents a monolingual token or constituent,
each distributed vector in TRAAM represents a
bilingual constituent or biconstituent?that is, an
instance of a transduction rule, which asserts a re-
lation between two monolingual constituents, as
well as specifying how to permute their subcon-
stituents in translation. Bilingual terminals, or
biterminals, are special cases of biconstituents
where a vector represents either (1) a bitoken?a
token-to-token or ?word-to-word? translation rule
?or (2) a bisegment?a segment-to-segment or
?phrase-to-phrase? translation rule.
The properties of TRAAMs are attractive for
machine translation applications. As with RAAM,
TRAAMs can be trained via backpropagation
training, which simultaneously evolves both the
autoencoder weights and the biconstituent vector
representations. As with RAAM, the evolution
of the vector representations within the hidden
layer performs automatic feature induction, and for
many applications can obviate the need for man-
ual feature engineering. However, the result is
that similar vectors tend to represent similar bicon-
stituents, rather than monolingual constituents.
The learned vector representations thus tend to
form clusters of similar translation relations in-
stead of merely similar strings. That is, TRAAM
clusters represent soft nonterminal categories of
cross-lingual relations and translation patterns, as
opposed to soft nonterminal categories of mono-
lingual strings as in RAAM.
Also, TRAAMs inherently make full simulta-
neous use of both input and output language fea-
112
tures, recursively, in an elegant integrated fash-
ion. TRAAM does not make restrictive a pri-
ori assumptions of conditional independence be-
tween input and output language features. When
evolving the biconstituent vector representations,
generalization occurs over similar input and out-
put structural characteristics simultaneously. In
most recurrent neural network applications to ma-
chine translation to date, only input side features
or only output language features are used. Even in
the few previous cases where recurrent neural net-
works have employed both input and output lan-
guage features for machine translation, the models
have typically been factored so that their recursive
portion is applied only to either the input or output
language, but not both.
As with RAAM, the objective criteria for train-
ing can be adjusted to reflect accuracy on nu-
merous different kinds of tasks, biasing the di-
rection that vector representations evolve toward.
But again, TRAAM?s learned vector representa-
tions support making predictions that simultane-
ously make use of both input and output struc-
tural characteristics. For example, TRAAM has
the ability to take into account the structure of
both input and output subtree characteristics while
making predictions on reordering them. Similarly,
for specific cross-lingual tasks such as word align-
ment, sense disambiguation, or machine transla-
tion, classifiers can simultaneously be trained in
conjunction with evolving the vector representa-
tions to optimize task-specific accuracy (Chris-
man, 1991).
In this paper we use as examples binary bi-
parse trees consistent with transduction grammars
in a 2-normal form, which by definition are in-
version transduction grammars (Wu, 1997) since
they are binary rank. This is not a requirement
for TRAAM, which in general can be formed for
transduction grammars of any rank. Moreover,
with distributed vector representations, the notion
of nonterminal categories in TRAAM is that of soft
membership, unlike in symbolically represented
transduction grammars. We start with bracketed
training data that contains no bilingual category
labels (like training data for Bracketing ITGs or
BITGs). Training results in self-organizing clus-
ters that have been automatically induced, repre-
senting soft nonterminal categories (unlike BITGs,
which do not have differentiated nonterminal cat-
egories).
2 Related work
TRAAM builds on different aspects of a spec-
trum of previous work. A large body of work ex-
ists on various different types of self-organizing
recurrent neural network approaches to model-
ing recursive structure, but mostly in monolin-
gual modeling. Even in applications to ma-
chine translation or cross-lingual modeling, the
typical practice has been to insert neural net-
work scoring components while still maintain-
ing older SMT modeling assumptions like bags-
of-words/phrases, ?shake?n?bake? translation that
relies heavily on strong monolingual language
models, and log-linear models?in contrast to
TRAAM?s fully integrated bilingual approach.
Here we survey representative work across the
spectrum.
2.1 Monolingual related work
Distributed vector representations have long
been used for n-gram language modeling; these
continuous-valued models exploit the general-
ization capabilities of neural networks, although
there is no hidden contextual or hierarchical
structure as in RAAM. Schwenk (2010) applies
one such language model within an SMT system.
In the simple recurrent neural networks (RNNs
or SRNs) of Elman (1990), hidden layer represen-
tations are fed back to the input to dynamically rep-
resent an aggregate of the immediate contextual
history. More recently, the probabilistic NNLMs
of Bengio et al. (2003) and Bengio et al. (2009)
follow in this vein.
To represent hierarchical tree structure using
vector representations, one simple family of ap-
proaches employs convolutional networks, as in
Lee et al. (2009) for example. Collobert and We-
ston (2008) use a convolution neural network layer
quite effectively to learn vector representations for
words which are then used in a host of NLP tasks
such as POS tagging, chunking, and semantic role
labeling.
RAAM approaches, and related recursive au-
toencoder approaches, can be more flexible than
convolutional networks. Like SRNs, they can be
extended in numerous ways. The URAAM (Uni-
fication RAAM) model of Stolcke and Wu (1992)
extended RAAM to demonstrate the possibility of
using neural networks to perform more sophisti-
cated operations like unification directly upon the
distributed vector representations of hierarchical
113
feature structures. Socher et al. (2011) used mono-
lingual recursive autoencoders for sentiment pre-
diction, with or without parse tree information; this
was perhaps the first use of a RAAM style ap-
proach on a large scale NLP task, albeit mono-
lingual. Scheible and Sch?tze (2013) automat-
ically simplified the monolingual tree structures
generated by recursive autoencoders, validated the
simplified structures via manual evaluation, and
showed that sentiment classification accuracy is
not affected.
2.2 Bilingual related work
The majority of work on learning bilingual dis-
tributed vector representations has not made use of
recursive approaches or hidden contextual or com-
positional structure, as in the bilingual word em-
bedding learning of Klementiev et al. (2012) or the
bilingual phrase embedding learning of Gao et al.
(2014). Schwenk (2012) uses a non-recursive neu-
ral network to predict phrase translation probabil-
ities in conventional phrase-based SMT.
Attempts have been made to generalize the dis-
tributed vector representations of monolingual n-
gram language models, avoiding any hidden con-
textual or hierarchical structure. Working within
the framework of n-gram translation models, Son
et al. (2012) generalize left-to-right monolingual
n-gram models to bilingual n-grams, and study
bilingual variants of class-based n-grams. How-
ever, their model does not allow tackling the chal-
lenge of modeling cross-lingual constituent order,
as TRAAM does; instead it relies on the assump-
tion that some other preprocessor has already man-
aged to accurately re-order the words of the input
sentence into exactly the order of words in the out-
put sentence.
Similarly, generalizations of monolingual SRNs
to the bilingual case have been studied. Zou
et al. (2013) generalize the monolingual recur-
rent NNLM model of Bengio et al. (2009) to
learn bilingual word embeddings using conven-
tional SMTword alignments, and demonstrate that
the resulting embeddings outperform the baselines
in word semantic similarity. They also add a sin-
gle semantic similarity feature induced with bilin-
gual embeddings to a phrase-based SMT log-linear
model, and report improvements in BLEU. Com-
pared to TRAAM, however, they only learn non-
compositional features, with distributed vectors
only representing biterminals (as opposed to bi-
constituents or bilingual subtrees), and so other
mechanisms for combining biterminal scores still
need to be used to handle hierarchical structure,
as opposed to seamlessly being integrated into
the distributed vector representation model. De-
vlin et al. (2014) obtain translation accuracy im-
provements by extending the probabilistic NNLMs
of Bengio et al. (2003), which are used for the
output language, by adding input language con-
text features. Unlike TRAAM, neither of these
approaches symmetrically models the recursive
structure of both the input and output language
sides.
For convolutional network approaches, Kalch-
brenner and Blunsom (2013) use a recurrent prob-
abilistic model to generate a representation of the
source sentence and then generate the target sen-
tence from this representation. This use of in-
put language context to bias translation choices
is in some sense a neural network analogy to
the PSD (phrase sense disambiguation) approach
for context-dependent translation probabilities of
Carpuat and Wu (2007). Unlike TRAAM, the
model does not contain structural constraints, and
permutation of phrases must still be done in con-
ventional PBSMT ?shake?n?bake? style by rely-
ing mostly on a language model (in their case, a
NNLM).
A few applications ofmonolingual RAAM-style
recursive autoencoders to bilingual tasks have also
appeared. For cross-lingual document classifica-
tion, Hermann and Blunsom (2014) use two sep-
arate monolingual fixed vector composition net-
works, one for each language. One provides the
training signal for the other, and training is only
on the embeddings.
Li et al. (2013) described a use of monolingual
recursive autoencoders within maximum entropy
ITGs. They replace their earlier model for pre-
dicting reordering based on the first and the last
tokens in a constituent, by instead using the con-
text vector generated using the recursive autoen-
coder. Only input language context is used, unlike
TRAAM which can use the input and output lan-
guage contexts equally.
Autoencoders have also been applied to SMT in
a very different way by Zhao et al. (2014) but with-
out recursion and not for learning distributed vec-
tor representations of words; rather, they used non-
recursive autoencoders to compress very high-
dimensional bilingual sparse features down to low-
dimensional feature vectors, so that MIRA or PRO
114
could be used to optimize the log-linear model
weights.
3 Representing transduction grammars
with TRAAM
As a recurrent neural network representation of a
transduction grammar, TRAAM learns bilingual
distributed representations that parallel the struc-
tural composition of a transduction grammar. As
with transduction grammars, the learned represen-
tations are symmetric and model structured rela-
tional correlations between the input and output
languages. The induced feature vectors in effect
represent soft categories of cross-lingual relations
and translations. The TRAAM model integrates
elegantly with the transduction grammar formal-
ism and aims to model the compositional struc-
ture of the transduction grammar as opposed to
incorporating external alignment information. It
is straightforward to formulate TRAAMs for arbi-
trary syntax directed transduction grammars; here
we shall describe an example of a TRAAM model
for an inversion transduction grammar (ITG).
Formally, an ITG is a tuple ?N,?,?, R, S?,
where N is a finite nonempty set of nonterminal
symbols,? is a finite set of terminal symbols inL
0
,
? is a finite set of terminal symbols in  L
1
, R is a
finite nonempty set of inversion transduction rules
andS ? N is a designated start symbol. A normal-
form ITG consists of rules in one of the following
four forms:
S ? A, A ? [BC] , A ? ?BC?, A ? e/f
where S ? N is the start symbol, A,B,C ?
N  are nonterminal symbols and e/f  is a biter-
minal. A biterminal is a pair of symbol strings:
?
?
??
?, where at least one of the strings have to
be nonempty. The square and angled brackets sig-
nal straight and inverted order respectively. With
straight order, both the L
0
and the L
1
productions
are generated left-to-right, but with inverted order,
the L
1
production is generated right-to-left.
In the distributed TRAAM representation of the
ITG, we represent each bispan, using a feature vec-
tor v of dimension d that represents a fuzzy encod-
ing of all the nonterminals that could generate it.
This is in contrast to the ITG model where each
nonterminal that generates a bispan has to be enu-
merated separately. Feature vectors correspond-
ing to larger bispans are compositionally generated
from smaller bispans using a compressor network
which takes two feature vectors of dimension d,
corresponding to the smaller bispans and gener-
ates the feature vector of dimension d correspond-
ing to the larger bispan. A single bit correspond-
ing to straight or inverted order is also fed as an
input to the compressor network. The compres-
sor network in TRAAM serves a similar role as
the syntactic rules in the symbolic ITG, but keeps
the encoding fuzzy. Figure 2 shows the straight
and inverted syntactic rules and the correspond-
ing inputs to the compressor network. Modeling
of unary rules (with start symbol on the left hand
side) although similar, is beyond the scope of this
paper.
It is easy to demonstrate that TRAAM mod-
els are capable of representing any symbolic ITG
model. All the nonterminals representing a bispan
can be encoded as a bit vector in the feature vector
of the bispan. Using the universal approximation
theorem of neural networks (Hornik et al., 1989),
an encoder with a single hidden layer can represent
any set of syntactic rules. Similarly, all TRAAM
models can be represented using a symbolic ITG
by assuming a unique nonterminal label for every
feature vector. Therefore, TRAAM and ITGs rep-
resent two equivalent classes of models for repre-
senting compositional bilingual relations.
It is important to note that although both
TRAAM and ITG models might be equivalent, the
fuzzy encoding of nonterminals in TRAAM is suit-
able for modeling the generalizations in bilingual
relationswithout exploding the search space unlike
the symbolic models. This property of TRAAM
makes it attractive for bilingual category learning
and machine translation applications as long as ap-
propriate language bias and objective functions are
determined.
Given our objective of inducing categories of
bilingual relations in an unsupervised manner, we
bias our TRAAM model by using a simple non-
linear activation function to be our compressor,
similar to the monolingual recursive autoencoder
model proposed by Socher et al. (2011). Having a
single layer in our compressor provides the neces-
sary language bias by forcing the network to cap-
ture the generalizations while reducing the dimen-
sions of the input vectors. We use tanh as the non-
linear activation function and the compressor ac-
cepts two vectors c
1
and c
2
of dimension d corre-
sponding to the nonterminals of the smaller bis-
pans and a single bit o corresponding to the in-
115
Figure 1: Example of English-Telugu biparse trees where inversion depends on output language sense.
Compressor 
Compressor 
Reconstructor 
Reconstructor 
o1#=#1# c1# c2# c3#
o2#=#(1#
p1#
o1'# c1'# c2'#
p2#
o2'# p1'# c3'#
Figure 2: Architecture of TRAAM.
version order and generates a vector p of dimen-
sion d corresponding to the larger bispan generated
by combining the two smaller bispans as shown in
Figure 2. The vector p then serves as the input for
the successive combinations of the larger bispan
with other bispans.
p = tanh(W
1
[o; c
1
; c
2
] + b
1
) (1)
whereW
1
and b
1
are the weight matrix and the bias
vector of the encoder network.
To ensure that the computed vector p captures
the fuzzy encodings of its children and the inver-
sion order, we use a reconstructor network which
attempts to reconstruct the inversion order and the
feature vectors corresponding of its children. We
use the error in reconstruction as our objective
function and train our model to minimize the re-
construction error over all the nodes in the biparse
tree. The reconstructor network in our TRAAM
model can be replaced by any other network that
enables the computed feature vector representa-
tions to be optimized for the given task. In our
current implementation, we reconstruct the inver-
sion order o? and the child vectors c?
1
and c?
2
using
another nonlinear activation function as follows:
[o
?
; c
?
1
; c
?
2
] = tanh(W
2
p+ b
2
) (2)
whereW
2
and b
2
are the weight matrix and the bias
vector of the reconstructor network.
4 Bilingual training
4.1 Initialization
The weights and the biases of the compressor and
the reconstructor networks of the TRAAM model
are randomly initialized. Bisegment embeddings
116
corresponding to the leaf nodes (biterminals in the
symbolic ITG notation) in the biparse trees are also
initialized randomly. These constitute the model
parameters and are optimized to minimize our ob-
jective function of reconstruction error. The parse
trees for providing the structural constraints are
generated by a bracketing inversion transduction
grammar (BITG) induced in a purely unsupervised
fashion, according to the algorithm in Saers et al.
(2009). Due to constraints on the training time, we
consider only the Viterbi biparse trees according
to the BITG instead of all the biparse trees in the
forest.
4.2 Computing feature vectors
We compute the feature vectors at each internal
node in the biparse tree, similar to the feedforward
pass in a neural network. We topologically sort all
the nodes in the biparse tree and set the feature vec-
tor of each node in the topologically sorted order
as follows:
? If the node is a leaf node, the feature vector is
the corresponding bisegment embedding.
? Else, the biconstituent embedding corre-
sponding to the internal node is generated us-
ing the feature vectors of the children and the
inversion order using Equation 1. We also
normalize the length of the computed fea-
ture vector so as to prevent the network from
making the biconstituent embedding arbitrar-
ily small in magnitude (Socher et al., 2011).
4.3 Feature vector optimization
We train our current implementation of TRAAM,
by optimizing the model parameters to minimize
an objective function based on the reconstruction
error over all the nodes in the biparse trees. The
objective function is defined as a linear combina-
tion of the l2 norm of the reconstruction error of
the children and the cross-entropy loss of recon-
structing the inversion order. We define the error
at each internal node n as follows:
E
n
=
?
2
|[c
1
; c
2
]? [c
?
1
; c
?
2
]|
2
? (1? ?)
[(1? o) log(1? o?) + (1 + o) log(1 + o?)]
where c
1
, c
2
, o correspond to the left child, right
child and inversion order, c?
1
, c
?
2
, o
? are the respec-
tive reconstructions and ? is the linear weighting
factor. The global objective function J is the sum
of the error function at all internal nodesn in the bi-
parse trees averaged over the total number of sen-
tences T in the corpus. A regularization parameter
? is used on the norm of the model parameters ? to
avoid overfitting.
J =
1
T
?
n
E
n
+ ?||?||
2 (3)
As the bisegment embeddings are also a part of
the model parameters, the optimization objective
is similar to a moving target training objective Ro-
hwer (1990). We use backpropagation with struc-
ture Goller and Kuchler (1996) to compute the gra-
dients efficiently. L-BFGS algorithm Liu and No-
cedal (1989) is used in order to minimize the loss
function.
5 Bilingual representation learning
We expect the TRAAM model to generate clus-
ters over cross-lingual relations similar to RAAM
models on monolingual data. We test this hypoth-
esis by bilingually training our model using a par-
allel English-Telugu blocks world dataset. The
dataset is kept simple to better understand the na-
ture of clusters. Our dataset comprises of com-
mands which involves manipulating different col-
ored objects over different shapes.
5.1 Example
Figure 1 shows the biparse trees for two English-
Telugu sentence pairs. The preposition on in En-
glish translates to ???????(pinunna) and ????(pina) re-
spectively in the first and second sentence pairs be-
cause in the first sentence block is described by its
position on the square, whereas in the second sen-
tence block is the subject and square is the object.
Since Telugu is a language with an SOV structure,
the verbs ????(vunchu) and ?????(teesuko) occur at
the end for both sentences.
The sentences in 1 illustrate the importance of
modeling bilingual relations simultaneously in-
stead of focusing only on the input or output lan-
guage as the cross-lingual structural relations are
sensitive to both the input and output language
context. For example, the constituent whose input
side is block on the square, the corresponding output
language tree structure is determined by whether
or not on is translated to ??????? (pinunna) or ????(pina).
In symbolic frameworks such as ITGs, such
relations are encoded using different nontermi-
nal categories. However, inducing such cate-
117
Figure 3: Clustering of biconstituents in the Telugu-English data.
gories within a symbolic framework in an un-
supervised manner creates extremely challenging
combinatorial scaling issues. TRAAM models
are a promising approach for tackling this prob-
lem, since the vector representations learned us-
ing the TRAAM model inherently yield soft syn-
tactic category membership properties, despite be-
ing trained only with the unlabeled structural con-
straints of simple BITG-style data.
5.2 Biconstituent clustering
The soft membership properties of learned dis-
tributed vector representations can be explored
via cluster analysis. To illustrate, we trained a
TRAAM network bilingually using the algorithm
in Section 4, and obtained feature vector represen-
tations for each unique biconstituent. Clustering
the obtained feature vectors reveals emergence of
fuzzy nonterminal categories, as shown in Figure
3. It is important to note that each point in the
vector space corresponds to a tree-structured bi-
constituent as opposed to merely a flat bilingual
phrase, as same surface forms with different tree
structures will have different vectors.
As the full cluster tree is too unwieldy, Figure
4 zooms in to shows an enlarged version of a por-
tion of the clustering, alongwith the corresponding
bracketed bilingual structures. One can observe
that the cluster represents the biconstituents that
describe the object by its position on another ob-
ject. We can deduce this from the fact that only a
single sense of on/???????(pinnuna) seems to be occur-
ing in all the biconstituents of the cluster. Manual
inspection of other clusters reveals such similari-
ties despite noise expected to be introduced by the
sparsity of our dataset.
6 Conclusion
We have introduced a fully bilingual generaliza-
tion of Pollack?s (1990) monolingual Recursive
Auto-Associative Memory neural network model,
TRAAM, in which each distributed vector repre-
sents a bilingual constituent?i.e., an instance of
a transduction rule, which specifies a relation be-
tween two monolingual constituents and how their
subconstituents should be permuted. Bilingual ter-
minals are special cases of bilingual constituents,
where a vector represents either (1) a bilingual to-
ken?a token-to-token or ?word-to-word? transla-
tion rule?or (2) a bilingual segment?a segment-
to-segment or ?phrase-to-phrase? translation rule.
TRAAMs can be used for arbitrary rank SDTGs
(syntax-directed transduction grammars, a.k.a.
synchronous context-free grammars). Although
our discussions in this paper have focused on bi-
parse trees from SDTGs in a 2-normal form, which
by definition are ITGs due to the binary rank,
nothing prevents TRAAMs from being applied to
higher-rank transduction grammars.
We believe TRAAMs are worth detailed ex-
ploration as their intrinsic properties address key
problems in bilingual grammar induction and sta-
118
Figure 4: Typical zoomed view into the Telugu-English biconstituent clusters from Figure 3.
tistical machine translation?their sensitivity to
both input and output language context means that
the learned vector representations tend to reflect
the similarity of bilingual rather than monolingual
constituents, which is what is needed to induce dif-
ferentiated bilingual nonterminal categories.
7 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
References
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Yoshua Bengio, R?jean Ducharme, Pascal Vin-
cent, and Christian Jauvin. A neural probabilis-
tic language model. Journal of Machine Learn-
ing Research, 3:1137?1155, 2003.
Yoshua Bengio, J?r?me Louradour, Ronan Col-
lobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international
conference on machine learning, pages 41?48.
ACM, 2009.
Marine Carpuat and Dekai Wu. Context-
dependent phrasal translation lexicons for sta-
tistical machine translation. In 11th Machine
Translation Summit (MT Summit XI), pages 73?
80, 2007.
Lonnie Chrisman. Learning recursive distributed
representations for holistic computation. Con-
nection Science, 3(4):345?366, 1991.
Ronan Collobert and Jason Weston. A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
In Proceedings of the 25th International Con-
ference on Machine Learning, ICML ?08, pages
160?167, New York, NY, USA, 2008. ACM.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint
models for statistical machine translation. In
119
52nd Annual Meeting of the Association for
Computational Linguistics, 2014.
Jeffrey L Elman. Finding structure in time. Cog-
nitive science, 14(2):179?211, 1990.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. Learning continuous phrase represen-
tations for translation modeling. In 52nd Annual
Meeting of the Association for Computational
Linguistics (Short Papers), 2014.
Christoph Goller and Andreas Kuchler. Learn-
ing task-dependent distributed representations
by backpropagation through structure. In Neu-
ral Networks, 1996., IEEE International Con-
ference on, volume 1, pages 347?352. IEEE,
1996.
Karl Moritz Hermann and Phil Blunsom. Multi-
lingual models for compositional distributed se-
mantics. In 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, volume
abs/1404.4641, 2014.
Kurt Hornik, Maxwell Stinchcombe, and Hal-
bert White. Multilayer feedforward networks
are universal approximators. Neural networks,
2(5):359?366, 1989.
Nal Kalchbrenner and Phil Blunsom. Recurrent
continuous translation models. In EMNLP,
pages 1700?1709, 2013.
Alexandre Klementiev, Ivan Titov, and Binod
Bhattarai. Inducing crosslingual distributed
representations of words. In 24th Interna-
tional Conference on Computational Linguistics
(COLING 2012). Citeseer, 2012.
Honglak Lee, Roger Grosse, Rajesh Ranganath,
and Andrew Y Ng. Convolutional deep be-
lief networks for scalable unsupervised learning
of hierarchical representations. In Proceedings
of the 26th Annual International Conference
on Machine Learning, pages 609?616. ACM,
2009.
Peng Li, Yang Liu, and Maosong Sun. Recur-
sive autoencoders for itg-based translation. In
EMNLP, pages 567?577, 2013.
Dong C Liu and Jorge Nocedal. On the limited
memory bfgs method for large scale optimiza-
tion. Mathematical programming, 45(1-3):503?
528, 1989.
Jordan B Pollack. Recursive distributed represen-
tations. Artificial Intelligence, 46(1):77?105,
1990.
Richard Rohwer. The ?moving targets?training al-
gorithm. In Neural Networks, pages 100?109.
Springer, 1990.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT?09), pages 29?32,
Paris, France, October 2009.
Christian Scheible and Hinrich Sch?tze. Cutting
recursive autoencoder trees. In 1st International
Conference on Learning Representations (ICLR
2013), Scottsdale, Arizona, May 2013.
Holger Schwenk. Continuous-space language
models for statistical machine translation. In
The Prague Bulletin of Mathematical Linguis-
tics, volume 93, pages 137?146, 2010.
Holger Schwenk. Continuous space transla-
tion models for phrase-based statistical machine
translation. In Proceedings of COLING 2012:
Posters, pages 1071??1080. Citeseer, 2012.
Richard Socher, Jeffrey Pennington, Eric H
Huang, Andrew Y Ng, and Christopher D Man-
ning. Semi-supervised recursive autoencoders
for predicting sentiment distributions. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages
151?161. Association for Computational Lin-
guistics, 2011.
Le Hai Son, Alexandre Allauzen, and Fran?ois
Yvon. Continuous space translationmodels with
neural networks. In Proceedings of the 2012
conference of the north american chapter of the
association for computational linguistics: Hu-
man language technologies, pages 39?48. As-
sociation for Computational Linguistics, 2012.
Andreas Stolcke and Dekai Wu. Tree match-
ing with recursive distributed representations.
In AAAI 1992 Workshop on Integrating Neu-
ral and Symbolic Processes?The Cognitive Di-
mension, 1992.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Bing Zhao, Yik-Cheung Tam, and Jing Zheng.
An autoencoder with bilingual sparse features
for improved statistical machine translation. In
120
IEEE International Conference on Acoustic,
Speech and Signal Processing (ICASSP), 2014.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher DManning. Bilingual word embed-
dings for phrase-based machine translation. In
EMNLP, pages 1393?1398, 2013.
121
