Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 698?708, Dublin, Ireland, August 23-29 2014.
Inducing Latent Semantic Relations for Structured Distributional
Semantics
Sujay Kumar Jauhar
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
sjauhar@cs.cmu.edu
Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
hovy@cs.cmu.edu
Abstract
Structured distributional semantic models aim to improve upon simple vector space models of
semantics by hypothesizing that the meaning of a word is captured more effectively through its
relational ? rather than its raw distributional ? signature. In accordance, they extend the vector
space paradigm by structuring elements with relational information that decompose distributional
signatures over discrete relation dimensions. However, the number and nature of these relations
remains an open research question, with most previous work in the literature employing syn-
tactic dependencies as surrogates for truly semantic relations. In this paper we propose a novel
structured distributional semantic model with latent relation dimensions, and instantiate it using
latent relational analysis. Evaluation of our model yields results that significantly outperform
several other distributional approaches on two semantic tasks and performs competitively on a
third relation classification task.
1 Introduction
The distributional hypothesis, articulated by Firth (1957) in the popular dictum ?You shall know the
word by the company it keeps?, has established itself as one of the most popular models of modern
computational semantics. With the rise of massive and easily-accessible digital corpora, computation of
co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs)
that have found relevance in many application areas. These include information retrieval (Manning et
al., 2008), question answering (Tellex et al., 2003), word-sense disambiguation (McCarthy et al., 2004)
and selectional preference modelling (Erk, 2007), to name only a few.
The standard DSM framework, which models the semantics of a word by co-occurrence statistics
computed over its neighbouring words, has several known short-comings. One severe short-coming
derives from the fundamental nature of the vector space model, which characterizes the semantics of a
word by a single vector in a high dimensional space (or some lower dimensional embedding thereof).
Such a modelling paradigm goes against the grain of the intuition that the semantics of a word is neither
unique nor constant. Rather, it is composed of many facets of meaning, and similarity (or dissimilarity)
to other words is an outcome of the aggregate harmony (or dissonance) between the individual facets
under consideration. For example, a shirt may be similar along one facet to a balloon in that they are
both coloured blue, at the same time being similar to a shoe along another facet for both being articles
of clothing, while being dissimilar along yet another facet to a t-shirt because one is stitched from linen
while the other is made from polyester.
Structured distributional semantic models (SDSMs) aim to remedy this fault with DSMs by decompos-
ing distributional signatures over discrete relation dimensions, or facets. This leads to a representation
that characterizes the semantics of a word by a distributional tensor, rather than a vector. Previous at-
tempts in the literature include the work of Pad? and Lapata (2007), Baroni and Lenci (2010) and Goyal
et al. (2013). However, all these approaches assume a simplified representation in which truly semantic
relations are substituted by syntactic relations obtained from a dependency parser.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
698
We believe that there are limiting factors to this approximation. Most importantly, the set of syntactic
relations, while relatively uncontroversial, is unable to capture the full extent of semantic nuance encoun-
tered in natural language text. Often, syntax is ambiguous and leads to multiple semantic interpretations.
Conversely, passivization and dative shift are common examples of semantic invariance in which mul-
tiple syntactic realizations are manifested. Additionally, syntax falls utterly short in explaining more
complex phenomena ? such as the description of buying and selling ? in which implicit semantics are
tacit from complex interactions between multiple participants.
While it is useful to consider relations that draw their origins from semantic roles such as Agent,
Patient and Recipient, it remains unclear what this set of semantic roles should be. This problem is
one that has long troubled linguists (Fillmore, 1967; Sowa, 1991), and has been previously noted by
researchers in NLP as well (M?rquez et al., 2008). Proposed solutions range from a small set of generic
Agent-like or Patient-like roles in Propbank (Kingsbury and Palmer, 2002) to an effectively open-ended
set of highly specific and fine-grained roles in Framenet (Baker et al., 1998). In addition to the theoretic
uncertainty of the set of semantic relations there is the very real problem of the lack of high-performance,
robust semantic parsers to annotate corpora. These issues effectively render the use of pre-defined,
linguistically ordained semantic relations intractable for use in SDSM.
In this paper we propose a novel approach to structuring distributional semantic models with latent
relations that are automatically discovered from corpora. This approach effectively solves the conceptual
dilemma of selecting the most expressive set of semantic relations. To the best of our knowledge this
is the first paper to propose latent relation dimensions for SDSMs. The intuition for generating these
latent relation dimensions leads to a generic framework, which ? in this paper ? is instantiated with
embeddings obtained from latent relational analysis (Turney, 2005).
We conduct experiments on three different semantic tasks to evaluate our model. On a similarity
scoring task and another synonym ranking task the model significantly outperforms other distributional
semantic models, including a standard window-based model, a syntactic SDSM based on previous ap-
proaches proposed in the literature, and a state-of-the-art semantic model trained using recursive neural
networks. On a relation classification task, our model performs competitively, outperforming all but one
of the models it is compared against.
2 Related Work
Since the distributional hypothesis was first proposed by Firth (1957), a number of different research
initiatives have attempted to extend and improve the standard distributional vector space model of se-
mantics. Insensitivity to the multi-faceted nature of semantics has been one of the focal points of several
papers. Earlier work in this regard is a paper by Turney (2012), who proposes that the semantics of a word
is not obtained along a single distributional axis but simultaneously in two different spaces. He proposes
a DSM in which co-occurrence statistics are computed for neighbouring nouns and verbs separately to
yield independent domain and function spaces of semantics.
This intuition is taken further by a stance which proposes that a word?s semantics is distributionally
decomposed over many independent spaces ? each of which is a unique relation dimension. Authors
who have endorsed this perspective are Erk and Pad? (2008), Goyal et al. (2013), Reisinger and Mooney
(2010) and Baroni and Lenci (2010). Our work relates to these papers in that we subscribe to the multiple
space semantics view. However, we crucially differ from them by structuring our semantic space with
information obtained from latent semantic relations rather than from a syntactic parser. In this paper the
instantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which
is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words.
From a modelling perspective, SDSMs characterize the semantics of a word by a distributional ten-
sor. Other notable papers on tensor based semantics or semantics of compositional structures are the
simple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural net-
work approach of Socher et al. (2012), the physics inspired quantum view of semantic composition of
Grefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013).
A different, partially overlapping strain of research attempts to induce word embeddings using meth-
699
ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research
papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010).
Other related work to note is the body of research concerned with semantic relation classification,
which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4
(Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2
(Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at
semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard
and Martin, 2007).
3 Structured Distributional Semantics and Latent Semantic Relation Induction
In this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSM
with latent relation dimensions.
A DSM is a vector space V that contains |?| elements in R
n
, where ? = {w
1
, w
2
, ..., w
k
} is a vocab-
ulary of k distinct words. Every vocabulary word w
i
has an associated semantic vector ~v
i
representing its
distributional signature. Each of the n elements of ~v
i
is associated with a single dimension of its distribu-
tion. This dimension may correspond to another word ? that may or may not belong to ? ? or a latent
dimension as might be obtained from an SVD projection or an embedding learned via a deep neural net-
work. Additionally, each element in ~v
i
is typically a normalized co-occurrence frequency count, a PMI
score, or a number obtained from an SVD or RNN transformation. The semantic similarity between two
words w
i
and w
j
in a DSM is the vector distance defined by cos(~v
i
, ~v
j
) on their associated distributional
vectors.
An SDSM is an extension of DSM. Formally, it is a space U that contains |?| elements in R
d?n
, where
? = {w
1
, w
2
, ..., w
k
} is a vocabulary of k distinct words. Every vocabulary word w
i
has an associated
semantic tensor
~
~u
i
, which is itself composed of d vectors ~u
i1
, ~u
i2
, ..., ~u
id
each having n dimensions.
Every vector ~u
il
?
~
~u
i
represents the distributional signature of the word w
i
in a relation (or along a
facet) r
l
. The d relations of the SDSM may be syntactic, semantic, or latent (as in this paper). The n
dimensional relational vector ~u
il
is configurationally the same as a vector ~v
i
of a DSM. This definition
of an SDSM closely relates to an alternate view of Distributional Memories (DMs) (Baroni and Lenci,
2010) where the semantic space is a third-order tensor, whose modes are Word? Link?Word.
The semantic similarity between two wordsw
i
andw
j
in an SDSM is the similarity function defined by
sim(
~
~u
i
,
~
~u
j
) on their associated semantic tensors. We use the following decomposition of the similarity
function:
sim(
~
~u
i
,
~
~u
j
) =
1
d
d
?
l=1
cos( ~u
il
, ~u
jl
) (1)
Mathematically, this corresponds to the ratio of the normalized Frobenius product of the two matrices
representing
~
~u
i
and
~
~u
j
to the number of rows in both matrices. Intuitively it is simply the average
relation-wise similarity between the two words w
i
and w
j
.
3.1 Latent Relation Induction for SDSM
The intuition behind our approach for inducing latent relation dimensions revolves around the simple ob-
servation that SDSMs, while representing semantics as distributional signatures over relation dimensions,
also effectively encode relational vectors between pairs of words. Our method thus works backwards
from this observation ? beginning with a relational embedding for pairs of words, that are subsequently
transformed to yield an SDSM.
Concretely, given a vocabulary ? = {w
1
, w
2
, ..., w
k
} and a list of word pairs of interest from the
vocabulary ?
V
? ? ? ?, we assume that we have some method for inducing a DSM V
?
that has a
vector representation
~
v
?
ij
of length d for every word pair w
i
, w
j
? ?
V
, which intuitively embeds the
distributional signature of the relation binding the two words in d latent dimensions. We then construct
an SDSM U where ?
U
= ?. For every word w
i
? ? a tensor
~
~u
i
? R
d?k
is generated. The tensor
~
~u
i
700
has d unique k dimensional vectors ~u
i1
, ~u
i2
, ..., ~u
id
. For a given relational vector ~u
il
, the value of the
jth element is taken from the lth element of the vector
~
v
?
ij
belonging to the DSM V
?
. If the vector
~
v
?
ij
does not exist in V
?
? as is the case where the pair w
i
, w
j
/? ?
V
? the value of the jth element of ~u
il
is
set to 0. By applying this mapping to generate semantic tensors for every word in ?, we are left with an
SDSM U that effectively embeds latent relation dimensions. From the perspective of DMs we matricize
the third-order tensor and perform truncated SVD, before restoring the resulting matrix to a third-order
tensor.
3.1.1 Latent Relational Analysis
In what follows, we present our instantiation of this model with an implementation that is based on
Latent Relational Analysis (LRA) (Turney, 2005) to generate the DSM V
?
. While other methods (such
as RNNs) are equally applicable in this scenario, we use LRA for its operational simplicity as well as
proven efficacy on semantic tasks such as analogy detection. The parameter values we chose in our
experiments are not fine-tuned and are guided by recommended values from Turney (2005), or scaled
suitably to accommodate the size of ?
V
.
The input to LRA is a vocabulary ? = {w
1
, w
2
, ..., w
k
} and a list of word pairs of interest from the
vocabulary ?
V
? ? ? ?. While one might theoretically consider a large vocabulary with all possi-
ble pairs, for computational reasons we restrict our vocabulary to approximately 4500 frequent English
words and only consider about 2.5% word pairs with high PMI (as computed on the whole of English
Wikipedia) in ?? ?. For each of the word pairs w
i
, w
j
? ?
V
we extract a list of contexts by querying a
search engine indexed over the combined texts of the whole of English Wikipedia and Gigaword corpora
(approximately 5.8? 10
9
tokens). Suitable query expansion is performed by taking the top 4 synonyms
of w
i
and w
j
using Lin?s thesaurus (Lin, 1998). Each of these contexts must contain both w
i
, w
j
(or
appropriate synonyms) and optionally some intervening words, and some words to either side.
Given such contexts, patterns for every word pair are generated by replacing the two target words w
i
and w
j
with placeholder characters X and Y , and replacing none, some or all of the other words by their
associated part-of-speech tag or a wildcard symbol. For example, if w
i
and w
j
are ?eat? and ?pasta?
respectively, and the queried context is ?I eat a bowl of pasta with a fork?, one would generate patterns
such as ?* X * NN * Y IN a *?, ?* X DT bowl IN Y with DT *?, etc. For every word pair, only the 5000
most frequent patterns are stored.
Once the set of all relevant patterns P = p
1
, p
2
, ..., p
n
have been computed a DSM V is constructed.
In particular, the DSM constitutes a ?
V
based on the list of word pairs of interest, and every word pair
w
i
, w
j
of interest has an associated vector ~v
ij
. Each element m of the vector ~v
ij
is a count pertaining to
the number of times that the pattern p
m
was generated by the word pair w
i
, w
j
.
3.1.2 SVD Transformation
The resulting DSM V is noisy and very sparse. Two transformations are thus applied to V . Firstly all
co-occurrence counts between word pairs and patterns are transformed to PPMI scores (Bullinaria and
Levy, 2007). Then given the matrix representation of V ? where rows correspond to word pairs and
columns correspond to patterns ? SVD is applied to yield V = M?N . HereM andN are matrices that
have unit-length orthogonal columns and ? is a matrix of singular values. By selecting the d top singular
values, we approximate V with a lower dimension projection matrix that reduces noise and compensates
for sparseness: V
?
= M
d
?
d
. This DSM V
?
in d latent dimensions is precisely the one we then use to
construct an SDSM, using the transformation described above.
Since the large number of patterns renders it effectively impossible to store the entire matrix V in
memory we use a memory friendly implementation
1
of a multi-pass stochastic algorithm to directly
approximate the projection matrix (Halko et al., 2011; Rehurek, 2010). A detailed analysis to see how
change in the parameter d effects the quality of the model is presented in section 4.
The optimal SDSM embeddings we trained and used in the experiments detailed below are available
for download at http://www.cs.cmu.edu/~sjauhar/Software_files/LR-SDSM.tar.
1
http://radimrehurek.com/gensim/
701
Model Spearman?s ?
Random 0.000
DSM 0.179
synSDSM 0.315
SENNA 0.510
LR-SDSM (300) 0.567
LR-SDSM (130) 0.586
Table 1
Model Acc.
Random 0.25
DSM 0.28
synSDSM 0.27
SENNA 0.38
LR-SDSM (300) 0.47
LR-SDSM (130) 0.51
Table 2
Results on the WS-353 similarity scoring task and the ESL synonym selection task. LRA-SDSM signif-
icantly outperforms other structured and non-structured distributional semantic models.
gz. This SDSM contains a vocabulary of 4546 frequent English words with 130 latent relation dimen-
sions.
4 Evaluation
Section 3 has described a method for embedding latent relation dimensions in SDSMs. We now turn to
the problem of evaluating these relations within the scope of the distributional paradigm in order to ad-
dress two research questions: 1) Are latent relation dimensions a viable and empirically competitive solu-
tion for SDSM? 2) Does structuring lead to a semantically more expressive model than a non-structured
DSM? In order to answer these questions we evaluate our model on two generic semantic tasks and
present comparative results against other structured and non-structured distributional models. We show
that we outperform all of them significantly, thus answering both research questions affirmatively.
While other research efforts have produced better results on these tasks (Jarmasz and Szpakowicz,
2003; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011), they are either lexicon or knowl-
edge based, or are driven by corpus statistics that tie into auxiliary resources such as multi-lingual in-
formation and structured ontologies like Wikipedia. Hence they are not relevant to our experimental
validation, and are consequently ignored in our comparative evaluation.
4.1 Word-Pair Similarity Scoring Task
The first task consists in using a semantic model to assign similarity scores to pairs of words. The dataset
used in this evaluation setting is the WS-353 dataset from Finkelstein et al. (2002). It consists of 353
pairs of words along with an averaged similarity score on a scale of 1.0 to 10.0 obtained from 13?16
human judges. Word pairs are presented as-is, without any context. For example, an item in this dataset
might be ?book, paper? 7.46?.
System scores are obtained by using the standard cosine similarity measure between distributional
vectors in a non-structured DSM. In the case of a variant of SDSM, these scores can be found by using
the cosine-based similarity functions in Equation 1 of the previous section. System generated output
scores are evaluated against the gold standard using Spearman?s rank correlation coefficient.
4.2 Synonym Selection Task
In the second task, the same set of semantic space representations is used to select the semantically closest
word to a target from a list of candidates. The ESL dataset from Turney (2002) is used for this task, and
was selected over the slightly larger TOEFL dataset (Landauer and Dumais, 1997). The reason for this
choice was because the latter contained more complex vocabulary words ? several of which were not
present in our simple vocabulary model. The ESL dataset consists of 50 target words that appear with 4
candidate lexical substitutes each. While disambiguating context is also given in this dataset, we discard
it in our experiments. An example item in this dataset might be ?rug? sofa, ottoman, carpet, hallway?,
with ?carpet? being the most synonym-like candidate to the target.
702
Figure 1: Evaluation results on WS-353 and ESL with varying number of latent dimensions. Generally
high scores are obtained in the range of 100-150 latent dimensions, with optimal results on both datasets
at 130 latent dimensions.
Similarity scores ? which are obtained in the same manner as for the previous evaluation task ?
are extracted between the target and each of the candidates in turn. These scores are then sorted in
descending order, with the top-ranking score yielding the semantically closest candidate to the target.
Systems are evaluated on the basis of their accuracy at discriminating the top-ranked candidate.
4.3 Results
We compare our model (LR-SDSM) to several other distributional models in these experiments. These
include a standard distributional vector space model (DSM) trained on the combined text of English
Wikipedia and Gigaword with a window-size of 3 words to either side of a target, a syntax-based SDSM
(Goyal et al., 2013; Baroni and Lenci, 2010) (synSDSM) trained on the same corpus parsed with a
dependency parser (Tratz and Hovy, 2011) and the state-of-the-art neural network embeddings from
Collobert et al. (2011) (SENNA). We also give the expected evaluation scores from a random baseline,
for comparison.
An important factor to consider when constructing an SDSM using LRA is the number of latent di-
mensions selected in the SVD projection. In Figure 1 we investigate the effects of selecting different
number of latent relation dimensions on both semantic evaluation tasks, starting with 10 dimensions up
to a maximum of 800 (which was the maximum that was computationally feasible), in increments of 10.
We note that optimal results on both datasets are obtained at 130 latent dimensions. In addition to the
SDSM obtained in this setting we also give results for an SDSM with 300 latent dimensions (which has
been a recommended value for SVD projections in the literature (Landauer and Dumais, 1997)) in our
comparisons against other models. Comparative results on the Finkelstein WS-353 similarity scoring
task are given in Table 1, while those on the ESL synonym selection task are given in Table 2.
4.4 Discussion
The results in Tables 1 and 2 show that LR-SDSM outperforms the other distributional models by a
considerable and statistically significant margin (p-value < 0.05) on both types of semantic evaluation
tasks. It should be noted that we do not tune to the test sets. While the 130 latent dimension SDSM yields
the best results, 300 latent dimensions also gives comparable performance and moreover outperforms all
the other baselines. In fact, it is worth noting that the evaluation results in figure 1 are almost all better
703
Random SENNA-Mik
DSM SENNA
LR-SDSM
AVC MVC AVC MVC
Prec. 0.111 0.273 0.419 0.382 0.489 0.416 0.431
Rec. 0.110 0.343 0.449 0.443 0.516 0.457 0.475
F-1. 0.110 0.288 0.426 0.383 0.499 0.429 0.444
% Acc. 11.03 34.30 44.91 44.26 51.55 45.65 47.48
Table 3: Results on Relation Classification Task. LR-SDSM scores competitively, outperforming all but
the SENNA-AVC model.
than the results of the other models on either datasets.
We conclude that structuring of a semantic model with latent relational information in fact leads to
performance gains over non-structured variants. Also, the latent relation dimensions we propose offer a
viable and empirically competitive alternative to syntactic relations for SDSMs.
Figure 1 shows the evaluation results on both semantic tasks as a function of the number of latent
dimensions. The general trend of both curves on the figure indicate that the expressive power of the
model quickly increases with the number of dimensions until it peaks in the range of 100?150, and then
decreases or evens out after that. Interestingly, this falls roughly in the range of the 166 frequent (those
that appear 50 times or more) frame elements, or fine-grained relations, from FrameNet that O?Hara and
Wiebe (2009) find in their taxonomization and mapping of a number of lexical resources that contain
semantic relations.
5 Semantic Relation Classification and Analysis of the Latent Structure of Dimensions
In this section we conduct experiments on the task of semantic relation classification. We also perform a
more detailed analysis of the induced latent relation dimensions in order to gain insight into our model?s
perception of semantic relations.
5.1 Semantic Relation Classification
In this task, a relational embedding is used as a feature vector to train a classifier for predicting the
semantic relation between previously unseen word pairs. The dataset used in this experiment is from
the SemEval-2012 task 2 on measuring the degree of relational similarity (Jurgens et al., 2012), since
it characterizes a number of very distinct and interesting semantic relations. In particular it consists of
an aggregated set of 3464 word pairs evidencing 10 kinds of semantic relations. We prune this set to
discard pairs that don?t contain words in the vocabularies of the models we consider in our experiments.
This leaves us with a dataset containing 933 word pairs in 9 classes (1 class was discarded altogether
because it contained too few instances). The 9 semantic relation classes are: ?Class Inclusion?, ?Part-
Whole?, ?Similar?, ?Contrast?, ?Attribute?, ?Non-Attribute?, ?Case Relation?, ?Cause-Purpose? and
?Space-Time?. For example, an instance of a word pair that exemplifies the ?Part-Whole? relationship is
?engine:car?. Note that, as with previous experiments, word pairs are given without any context.
5.2 Results
We compare LR-SDSM on the semantic relation classification task to several different models. These
include the additive vector composition (AVC) and multiplicative vector composition methods (MVC)
proposed by Mitchell and Lapata (2009); we present both DSM and SENNA based variants of these
models. We also compare against the vector difference method of Mikolov et al. (2013) (SENNA-
Mik) which sees semantic relations as a meaning preserving vector translation in an RNN embedded
vector space. Finally, we note the performance of random classification as a baseline, for reference. We
attempted to produce results of a syntactic SDSM on the task; however, the hard constraint imposed by
syntactic adjacency meant that effectively all the word pairs in the dataset yielded zero feature vectors.
To avoid overfitting on all 130 original dimensions in our optimal SDSM, and also to render results
comparable, we reduce the number of latent relation dimensions of LR-SDSM to 50. We similarly reduce
704
Figure 2: Correlation distances between semantic relations? classifier weights. The plot shows how our
latent relations seem to perceive humanly interpretable semantic relations. Most points are fairly well
spaced out, with opposites such as ?Attribute? and ?Non-Attribute? as well as ?Similar? and ?Contrast?
being relatively further apart.
the feature vector dimension of DSM-AVC and DSM-MVC to 50 by feature selection. The dimensions
of SENNA-AVC, SENNA-MVC and SENNA-Mik are already 50, and are not reduced further.
For each of the methods we train a logistic regression classifier. We don?t perform any tuning of
parameters and set a constant ridge regression value of 0.2, which seemed to yield roughly the best
results for all models. The performance on the semantic relation classification task in terms of averaged
precision, recall, F-measure and percentage accuracy using 10-fold cross-validation is given in Table 3.
Additionally, to gain further insight into the LR-SDSM?s understanding of semantic relations, we
conduct a secondary analysis. We begin by training 9 one-vs-all logistic regression classifiers for each of
the 9 semantic relations under consideration. Then pairwise correlation distances are measured between
all pairs of weight vectors of the 9 models. Finally, the distance adjacency matrix is projected into 2-d
space using multidimensional scaling. The result of this analysis is presented in Figure 2.
5.3 Discussion
Table 3 shows that LR-SDSM performs competitively on the relation classification task and outperforms
all but one of the other models. The performance differences are statistically significant with a p-value
< 0.5. We believe that some of the expressive power of the model is lost by compressing to 50 latent
relation dimensions, and that a greater number of dimensions might improve performance. However,
testing a model with a 130-length dense feature vector on a dataset containing 933 instances would
likely lead to overfitting and also not be comparable to the SENNA-based models that operate on 50-
length feature vectors.
Other points to note from Table 3 are that the AVC variants of the the DSM and SENNA composition
models tend to perform better than their MVC counterparts. Also, SENNA-Mik performs surprisingly
poorly. It is worth noting, however, that Mikolov et al. (2013) report results on fairly simple lexico-
syntactic relations between words ? such as plural forms, possessives and gender ? while the semantic
relations under consideration in the SemEval-2012 dataset are relatively more complex.
In the analysis of the latent structure of dimensions presented in Figure 2, there are few interesting
points to note. To begin with, all the points (with the exception of one pair) are fairly well spaced out. At
705
the weight vector level, this implies that different latent dimensions need to fire in different combinations
to characterize distinct semantic relations, thus resulting in low correlation between their corresponding
weight vectors. This indicates the fact that the latent relation dimensions seem to capture the intuition
that each of the classes encodes a distinctly different semantic relation. The notable exception is ?Space-
Time?, which is very close to ?Contrast?. This is probably due to the fact that distributional models are
ineffective at capturing spatio-temporal semantics. Moreover, it is interesting to note that ?Attribute"
and ?Non-Attribute? as well as ?Similar? and ?Contrast?, which are intuitively semantic inverses of each
other are also (relatively) distant from each other in the plot.
These general findings indicate an interesting avenue for future research, which involves mapping the
empirically learnt latent relations to hand-built semantic lexicons or frameworks. This could help to vali-
date the empirical models at various levels of linguistic granularity, as well as establish correspondences
between different views of semantic representation.
6 Conclusion and Future Work
In this paper we have proposed a novel paradigm for SDSMs, that allows for structuring via latent
relational information. We have introduced a generic operational framework that allows for building
such SDSMs and outlined an instantiation of the model with LRA. Experimental results of the model
support our claim that the resulting SDSM captures the semantics of words more effectively than a
number of other semantic models, and presents a viable ? and empirically competitive ? alternative
to syntactic SDSMs. Additionally we have conducted experiments on a relation classification task and
shown promising results, as well as performed analyses to investigate the structure of, and interactions
between, the latent relation dimensions.
These findings motivate a number of future directions of research. Since our framework is fairly gen-
eral we hope to explore techniques other than LRA (such as RNNs) to generate relational embeddings
for word pairs. A desiderata for future techniques is scalability so that we can characterize vocabular-
ies that are larger than the one in our current experiments. We also hope to explore mappings between
our empirically learnt latent relations, and semantic lexicons and frameworks that catalog semantic re-
lations. Finally, we hope to test our model on more realistic application task such as event coreference,
recognizing textual entailment, and semantic parsing in future work.
Acknowledgments
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions
to improve the quality of the paper. This work was supported in part by the following grants: NSF grant
IIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342.
References
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the
17th international conference on Computational linguistics-Volume 1, pages 86?90. Association for Computa-
tional Linguistics.
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Comput. Linguist., 36(4):673?721, December.
Steven Bethard and James H Martin. 2007. Cu-tmp: temporal relation classification using syntactic and semantic
features. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129?132. Associa-
tion for Computational Linguistics.
John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statis-
tics: A computational study. Behavior Research Methods, 39(3):510?526.
Ronan Collobert, Jason Weston, L?on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. J. Mach. Learn. Res., 999888:2493?2537, November.
Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990.
Indexing by latent semantic analysis. JASIS, 41(6):391?407.
706
Katrin Erk and Sebastian Pad?. 2008. A structured vector space model for word meaning in context. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model for selectional preferences.
Charles J Fillmore. 1967. The case for case.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-
pin. 2002. Placing search in context: The concept revisited. In ACM Transactions on Information Systems,
volume 20, pages 116?131, January.
John R. Firth. 1957. A Synopsis of Linguistic Theory, 1930-1955. Studies in Linguistic Analysis, pages 1?32.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI, volume 7, pages 1606?1611.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-
2007 task 04: Classification of semantic relations between nominals. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 13?18. Association for Computational Linguistics.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard Hovy. 2013.
A structured distributional semantic model for event co-reference. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (ACL ? 2013).
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 1394?1404, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217?288.
Samer Hassan and Rada Mihalcea. 2011. Semantic relatedness using salient semantic analysis. In AAAI.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid ? S?aghdha, Sebastian Pad?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, pages 94?99. Association for Computational Linguistics.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s thesaurus and semantic similarity. Recent Advances in
Natural Language Processing III: Selected Papers from RANLP.
David A Jurgens, Peter D Turney, Saif M Mohammad, and Keith J Holyoak. 2012. Semeval-2012 task 2: Measur-
ing degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Evaluation, pages 356?364. Association for Computational
Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From treebank to propbank. In LREC. Citeseer.
Thomas K Landauer and Susan T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211?240.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?tze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
Llu?s M?rquez, Xavier Carreras, Kenneth C Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling: an
introduction to the special issue. Computational linguistics, 34(2):145?159.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in un-
tagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL
?04, Stroudsburg, PA, USA. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word repre-
sentations. Proceedings of NAACL-HLT, pages 746?751.
707
Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the
2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ?09,
pages 430?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting semantic role resources for preposition disambiguation. Com-
putational Linguistics, 35(2):151?184.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199.
Radim Rehurek. 2010. Fast and faster: A comparison of two streamed matrix decomposition algorithms. NIPS
2010 Workshop on Low-rank Methods for Large-scale Machine Learning.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117. Association for Computational Linguistics.
Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations
and syntactic parsing with recursive neural networks. Proceedings of the NIPS-2010 Deep Learning and Unsu-
pervised Feature Learning Workshop.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages
1201?1211, Stroudsburg, PA, USA. Association for Computational Linguistics.
John F. Sowa. 1991. Principles of semantic networks. Morgan Kaufmann.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation
of passage retrieval algorithms for question answering. In SIGIR, pages 41?47.
Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 1257?1268,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Peter D. Turney. 2002. Mining the web for synonyms: Pmi-ir versus lsa on toefl. CoRR.
Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of the 19th
international Conference on Aritifical Intelligence, pages 1136?1141.
Peter D Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal
of Artificial Intelligence Research, 44:533?585.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic
compositionality. In Proceedings of NAACL-HLT, pages 1142?1151.
708
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467?473,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model for Event Co-reference
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel ap-
proach to modelling distributional seman-
tics that represents meaning as distribu-
tions over relations in syntactic neighbor-
hoods. We argue that our model approxi-
mates meaning in compositional configu-
rations more effectively than standard dis-
tributional vectors or bag-of-words mod-
els. We test our hypothesis on the problem
of judging event coreferentiality, which in-
volves compositional interactions in the
predicate-argument structure of sentences,
and demonstrate that our model outper-
forms both state-of-the-art window-based
word embeddings as well as simple ap-
proaches to compositional semantics pre-
viously employed in the literature.
1 Introduction
Distributional Semantic Models (DSM) are popu-
lar in computational semantics. DSMs are based
on the hypothesis that the meaning of a word or
phrase can be effectively captured by the distribu-
tion of words in its neighborhood. They have been
successfully used in a variety of NLP tasks includ-
ing information retrieval (Manning et al, 2008),
question answering (Tellex et al, 2003), word-
sense discrimination (Sch?tze, 1998) and disam-
biguation (McCarthy et al, 2004), semantic sim-
ilarity computation (Wong and Raghavan, 1984;
McCarthy and Carroll, 2003) and selectional pref-
erence modeling (Erk, 2007).
A shortcoming of DSMs is that they ignore the
syntax within the context, thereby reducing the
distribution to a bag of words. Composing the
?*Equally contributing authors
distributions for ?Lincoln?, ?Booth?, and ?killed?
gives the same result regardless of whether the in-
put is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. But as suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately yields greater expressive power. Thus, to
remedy the bag-of-words failing, we extend the
generic DSM model to several relation-specific
distributions over syntactic neighborhoods. In
other words, one can think of the Structured DSM
(SDSM) representation of a word/phrase as sev-
eral vectors defined over the same vocabulary,
each vector representing the word?s selectional
preferences for its various syntactic arguments.
We argue that this representation not only cap-
tures individual word semantics more effectively
than the standard DSM, but is also better able to
express the semantics of compositional units. We
prove this on the task of judging event coreference.
Experimental results indicate that our model
achieves greater predictive accuracy on the task
than models that employ weaker forms of compo-
sition, as well as a baseline that relies on state-
of-the-art window based word embeddings. This
suggests that our formalism holds the potential of
greater expressive power in problems that involve
underlying semantic compositionality.
2 Related Work
Next, we relate and contrast our work to prior re-
search in the fields of Distributional Vector Space
Models, Semantic Compositionality and Event
Co-reference Resolution.
2.1 DSMs and Compositionality
The underlying idea that ?a word is characterized
by the company it keeps? was expressed by Firth
467
(1957). Several works have defined approaches to
modelling context-word distributions anchored on
a target word, topic, or sentence position. Collec-
tively these approaches are called Distributional
Semantic Models (DSMs).
While DSMs have been very successful on a va-
riety of tasks, they are not an effective model of
semantics as they lack properties such as compo-
sitionality or the ability to handle operators such
as negation. In order to model a stronger form of
semantics, there has been a recent surge in stud-
ies that phrase the problem of DSM composition-
ality as one of vector composition. These tech-
niques derive the meaning of the combination of
two words a and b by a single vector c = f(a, b).
Mitchell and Lapata (2008) propose a framework
to define the composition c = f(a, b, r,K) where
r is the relation between a and b, and K is the
additional knowledge used to define composition.
While this framework is quite general, the actual
models considered in the literature tend to disre-
gard K and r and mostly perform component-wise
addition and multiplication, with slight variations,
of the two vectors. To the best of our knowledge
the formulation of composition we propose is the
first to account for both K and r within this com-
positional framework.
Dinu and Lapata (2010) and S?aghdha and Ko-
rhonen (2011) introduced a probabilistic model
to represent word meanings by a latent variable
model. Subsequently, other high-dimensional ex-
tensions by Rudolph and Giesbrecht (2010), Ba-
roni and Zamparelli (2010) and Grefenstette et
al. (2011), regression models by Guevara (2010),
and recursive neural network based solutions by
Socher et al (2012) and Collobert et al (2011)
have been proposed. However, these models do
not efficiently account for structure.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempt to include syntactic context in distribu-
tional models. A quasi-compositional approach
was attempted in Thater et al (2010) by a com-
bination of first and second order context vectors.
But they do not explicitly construct phrase-level
meaning from words which limits their applicabil-
ity to real world problems. Furthermore, we also
include structure into our method of composition.
Prior work in structure aware methods to the best
of our knowledge are (Weisman et al, 2012) and
(Baroni and Lenci, 2010). However, these meth-
ods do not explicitly model composition.
2.2 Event Co-reference Resolution
While automated resolution of entity coreference
has been an actively researched area (Haghighi
and Klein, 2009; Stoyanov et al, 2009; Raghu-
nathan et al, 2010), there has been relatively lit-
tle work on event coreference resolution. Lee
et al (2012) perform joint cross-document entity
and event coreference resolution using the two-
way feedback between events and their arguments.
We, on the other hand, attempt a slightly different
problem of making co-referentiality judgements
on event-coreference candidate pairs.
3 Structured Distributional Semantics
In this paper, we propose an approach to incorpo-
rate structure into distributional semantics (more
details in Goyal et al (2013)). The word distribu-
tions drawn from the context defined by a set of
relations anchored on the target word (or phrase)
form a set of vectors, namely a matrix for the tar-
get word. One axis of the matrix runs over all
the relations and the other axis is over the distri-
butional word vocabulary. The cells store word
counts (or PMI scores, or other measures of word
association). Note that collapsing the rows of the
matrix provides the standard dependency based
distributional representation.
3.1 Building Representation: The PropStore
To build a lexicon of SDSM matrices for a given
vocabulary we first construct a proposition knowl-
edge base (the PropStore) created by parsing the
Simple English Wikipedia. Dependency arcs are
stored as 3-tuples of the form ?w1, r, w2?, denot-
ing an occurrence of words w1, word w2 related
by r. We also store sentence indices for triples
as this allows us to achieve an intuitive technique
to achieve compositionality. In addition to the
words? surface-forms, the PropStore also stores
their POS tags, lemmas, and Wordnet supersenses.
This helps to generalize our representation when
surface-form distributions are sparse.
The PropStore can be used to query for the ex-
pectations of words, supersenses, relations, etc.,
around a given word. In the example in Figure 1,
the query (SST(W1) = verb.consumption, ?, dobj)
i.e. ?what is consumed? might return expectations
[pasta:1, spaghetti:1, mice:1 . . . ]. Relations and
POS tags are obtained using a dependency parser
Tratz and Hovy (2011), supersense tags using sst-
light Ciaramita and Altun (2006), and lemmas us-
468
Figure 1: Sample sentences & triples
ing Wordnet Fellbaum (1998).
3.2 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix symbol-
ism in a bottom-up fashion using the PropStore.
The combination hinges on the intuition that when
lexical units combine to form a larger syntactically
connected phrase, the representation of the phrase
is given by its own distributional neighborhood
within the embedded parse tree. The distributional
neighborhood of the net phrase can be computed
using the PropStore given syntactic relations an-
chored on its parts. For the example in Figure
1, we can compose SST(w1) = Noun.person and
Lemma(W1) = eat appearing together with a nsubj
relation to obtain expectations around ?people eat?
yielding [pasta:1, spaghetti:1 . . . ] for the object
relation, [room:2, restaurant:1 . . .] for the location
relation, etc. Larger phrasal queries can be built to
answer queries like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
us to account for both relation r and knowledge K
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition
of two words is given in Algorithm 1, which
returns the distributional expectation around the
composed unit. Note that the entire algorithm can
conveniently be written in the form of database
queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1) (1)
M2 ? queryMatrix(w2) (2)
SentIDs?M1(r) ?M2(r) (3)
return ((M1? SentIDs) ? (M2? SentIDs)) (4)
For the example ?noun.person nsubj eat?, steps
(1) and (2) involve querying the PropStore for the
individual tokens, noun.person and eat. Let the re-
sulting matrices be M1 and M2, respectively. In
step (3), SentIDs (sentences where the two words
appear with the specified relation) are obtained by
taking the intersection between the nsubj compo-
nent vectors of the two matrices M1 and M2. In
step (4), the entries of the original matrices M1
and M2 are intersected with this list of common
SentIDs. Finally, the resulting matrix for the com-
position of the two words is simply the union of
all the relationwise intersected sentence IDs. Intu-
itively, through this procedure, we have computed
the expectation around the words w1 and w2 when
they are connected by the relation ?r?.
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts (described in Algorithm 2).
Let the E = {e1 . . . en} be the set of edges in T ,
ei = (wi1, ri, wi2)?i = 1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
The phrase representations becomes sparser as
phrase length increases. For this study, we restrict
phrasal query length to a maximum of three words.
3.3 Event Coreferentiality
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
469
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ. It may be
noted that this task is different from the task of full
event coreference and hence is not directly compa-
rable to previous experimental results in the liter-
ature. Two mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
In our corpus, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for
the role. It may be noted that the SDSM repre-
sentation relies on syntactic dependancy relations.
Hence, to bridge the gap between these relations
and the composition of semantic role participants
of event mentions we empirically determine those
syntactic relations which most strongly co-occur
with the semantic relations connecting events,
agents and patients. The triple (e, a, p) is thus the
composition of the triples (a, relationsetagent, e)
and (p, relationsetpatient, e), and hence a com-
plex object. To determine equality of this complex
composed representation we generate three levels
of progressively simplified event constituents for
comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p)
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance
metric (Euclidean, Cityblock, Cosine), and
score normalization techniques (Row-wise, Full,
Column-collapsed). This results in 159 similarity-
based features for every pair of events, which are
used to train a classifier to decide conference.
4 Experiments
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
4.1 Datasets
IC Event Coreference Corpus: The dataset
(Hovy et al, 2013), drawn from 100 news articles
about violent events, contains manually created
annotations for 2214 pairs of co-referent and non-
coreferent events each. Where available, events?
semantic role-fillers for agent and patient are an-
notated as well. When missing, empirical substi-
tutes were obtained by querying the PropStore for
the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
4.2 Baselines
To establish the efficacy of our model, we compare
SDSM against a purely window-based baseline
(DSM) trained on the same corpus. In our exper-
iments we set a window size of seven words. We
also compare SDSM against the window-based
embeddings trained using a recursive neural net-
work (SENNA) (Collobert et al, 2011) on both
datsets. SENNA embeddings are state-of-the-art
for many NLP tasks. The second baseline uses
470
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 1: Cross-validation Performance on IC and ECB dataset
SENNA to generate level 3 similarity features for
events? individual words (agent, patient and ac-
tion). As our final set of baselines, we extend two
simple techniques proposed by (Mitchell and Lap-
ata, 2008) that use element-wise addition and mul-
tiplication operators to perform composition. We
extend it to our matrix representation and build
two baselines AVC (element-wise addition) and
MVC (element-wise multiplication).
4.3 Discussion
Among common classifiers, decision-trees (J48)
yielded best results in our experiments. Table 1
summarizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of compositionality than
simple additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
event coreferentiality. The forward selection pro-
cedure reveals that the most informative attributes
are the level 2 compositional features involving
the agent and the action, as well as their individ-
ual level 3 features. This corresponds to the in-
tuition that the agent and the action are the prin-
cipal determiners for identifying events. Features
involving the patient and level 1 features are least
useful. This is probably because features involv-
ing full composition are sparse, and not as likely
to provide statistically significant evidence. This
may change as our PropStore grows in size.
5 Conclusion and Future Work
We outlined an approach that introduces structure
into distributed semantic representations gives us
an ability to compare the identity of two repre-
sentations derived from supposedly semantically
identical phrases with different surface realiza-
tions. We employed the task of event coreference
to validate our representation and achieved sig-
nificantly higher predictive accuracy than several
baselines.
In the future, we would like to extend our model
to other semantic tasks such as paraphrase detec-
tion, lexical substitution and recognizing textual
entailment. We would also like to replace our syn-
tactic relations to semantic relations and explore
various ways of dimensionality reduction to solve
this problem.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
References
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673?721,
December.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
471
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Kartik. Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan,
Shashank Srivastava, Huiying Li, and Eduard Hovy.
2013. A structured distributional semantic model
: Integrating structure with semantics. In Proceed-
ings of the 1st Continuous Vector Space Models and
their Compositionality Workshop at the conference
of ACL 2013.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
E.H. Hovy, T. Mitamura, M.F. Verdejo, J. Araki, and
A. Philpot. 2013. Events are not simple: Iden-
tity, non-identity, and quasi-identity. In Proceedings
of the 1st Events Workshop at the conference of the
HLT-NAACL 2013.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
472
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 194?204, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
473
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 1: English Lexical Simplification
Lucia Specia
Department of Computer Science
University of Sheffield
L.Specia@sheffield.ac.uk
Sujay Kumar Jauhar
Research Group in Computational Linguistics
University of Wolverhampton
Sujay.KumarJauhar@wlv.ac.uk
Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu
Abstract
We describe the English Lexical Simplifica-
tion task at SemEval-2012. This is the first
time such a shared task has been organized
and its goal is to provide a framework for the
evaluation of systems for lexical simplification
and foster research on context-aware lexical
simplification approaches. The task requires
that annotators and systems rank a number of
alternative substitutes ? all deemed adequate ?
for a target word in context, according to how
?simple? these substitutes are. The notion of
simplicity is biased towards non-native speak-
ers of English. Out of nine participating sys-
tems, the best scoring ones combine context-
dependent and context-independent informa-
tion, with the strongest individual contribution
given by the frequency of the substitute re-
gardless of its context.
1 Introduction
Lexical Simplification is a subtask of Text Simpli-
fication (Siddharthan, 2006) concerned with replac-
ing words or short phrases by simpler variants in a
context aware fashion (generally synonyms), which
can be understood by a wider range of readers. It
generally envisages a certain human target audience
that may find it difficult or impossible to understand
complex words or phrases, e.g., children, people
with poor literacy levels or cognitive disabilities, or
second language learners. It is similar in many re-
spects to the task of Lexical Substitution (McCarthy
and Navigli, 2007) in that it involves determining
adequate substitutes in context, but in this case on
the basis of a predefined criterion: simplicity.
A common pipeline for a Lexical Simplification
system includes at least three major components: (i)
complexity analysis: selection of words or phrases
in a text that are considered complex for the reader
and/or task at hand; (ii) substitute lookup: search
for adequate replacement words or phrases deemed
complex in context, e.g., taking synonyms (with
the same sense) from a thesaurus or finding similar
words/phrases in a corpus using distributional simi-
larity metrics; and (iii) context-based ranking: rank-
ing of substitutes according to how simple they are
to the reader/task at hand.
As an example take the sentence: ?Hitler com-
mitted terrible atrocities during the second World
War.? The system would first identify complex
words, e.g. atrocities, then search for substitutes
that might adequately replace it. A thesaurus lookup
would yield the following synonyms: abomination,
cruelty, enormity and violation, but enormity should
be dropped as it does not fit the context appropri-
ately. Finally, the system would determine the sim-
plest of these substitutes, e.g., cruelty, and use it
to replace the complex word, yielding the sentence:
?Hitler committed terrible cruelties during the sec-
ond World War.?.
Different from other subtasks of Text Simplifica-
tion like Syntactic Simplification, which have been
relatively well studied, Lexical Simplification has
received less attention. Although a few recent at-
tempts explicitly address dependency on context (de
Belder et al, 2010; Yatskar et al, 2010; Biran et al,
2011; Specia, 2010), most approaches are context-
independent (Candido et al, 2009; Devlin and Tait,
1998). In addition, a general deeper understanding
347
of the problem is yet to be gained. As a first attempt
to address this problem in the shape of a shared task,
the English Simplification task at SemEval-2012 fo-
cuses on the third component, which we believe is
the core of the Lexical Simplification problem.
The SemEval-2012 shared task on English Lexi-
cal Simplification has been conceived with the fol-
lowing main purposes: advancing the state-of-the-
art Lexical Simplification approaches, and provid-
ing a common framework for evaluation of Lexical
Simplification systems for participants and other re-
searchers interested in the field. Another central mo-
tive of such a shared task is to bring awareness to the
general vagueness associated with the notion of lex-
ical simplicity. Our hypothesis is that in addition to
the notion of a target application/reader, the notion
of simplicity is highly context-dependent. In other
words, given the same list of substitutes for a given
target word with the same sense, we expect different
orderings of these substitutes in different contexts.
We hope that participation in this shared task will
help discover some underlying traits of lexical sim-
plicity and furthermore shed some light on how this
may be leveraged in future work.
2 Task definition
Given a short context, a target word in English,
and several substitutes for the target word that are
deemed adequate for that context, the goal of the
English Simplification task at SemEval-2012 is to
rank these substitutes according to how ?simple?
they are, allowing ties. Simple words/phrases are
loosely defined as those which can be understood by
a wide range of people, including those with low lit-
eracy levels or some cognitive disability, children,
and non-native speakers of English. In particular,
the data provided as part of the task is annotated by
fluent but non-native speakers of English.
The task thus essentially involves comparing
words or phrases and determining their order of
complexity. By ranking the candidates, as opposed
to categorizing them into specific labels (simple,
moderate, complex, etc.), we avoid the need for a
fixed number of categories and for more subjective
judgments. Also ranking enables a more natural and
intuitive way for humans (and systems) to perform
annotations by preventing them from treating each
individual case in isolation, as opposed to relative
to each other. However, the inherent subjectivity
introduced by ranking entails higher disagreement
among human annotators, and more complexity for
systems to tackle.
3 Corpus compilation
The trial and test corpora were created from the cor-
pus of SemEval-2007 shared task on Lexical Sub-
stitution (McCarthy and Navigli, 2007). This de-
cision was motivated by the similarity between the
two tasks. Moreover the existing corpus provided an
adequate solution given time and cost constraints for
our corpus creation. Given existing contexts with the
original target word replaced by a placeholder and
the lists of substitutes (including the target word),
annotators (and systems) are required to rank substi-
tutes in order of simplicity for each context.
3.1 SemEval-2007 - LS corpus
The corpus from the shared task on Lexical Substi-
tution (LS) at SemEval-2007 is a selection of sen-
tences, or contexts, extracted from the English Inter-
net Corpus of English (Sharoff, 2006). It contains
samples of English texts crawled from the web.
This selection makes up the dataset of a total of
2, 010 contexts which are divided into Trial and Test
sets, consisting of 300 and 1710 contexts respec-
tively. It covers a total of 201 (mostly polysemous)
target words, including nouns, verbs, adjectives and
adverbs, and each of the target words is shown in
10 different contexts. Annotators had been asked to
suggest up to three different substitutes (words or
short phrases) for each of the target words within
their contexts. The substitutes were lemmatized un-
less it was deemed that the lemmatization would al-
ter the meaning of the substitute. Annotators were
all native English speakers and each annotated the
entire dataset. Here is an example of a context for
the target word ?bright?:
<lexelt item="bright.a">
<instance id="1">
<context>During the siege, George
Robertson had appointed Shuja-ul-Mulk,
who was a <head>bright</head> boy
only 12 years old and the youngest surviv-
ing son of Aman-ul-Mulk, as the ruler of
Chitral.</context>
348
</instance> ... </lexelt>
The gold-standard document contains each target
word along with a ranked list of its possible substi-
tutes, e.g., for the context above, three annotators
suggested ?intelligent? and ?clever? as substitutes
for ?bright?, while only one annotator came up with
?smart?:
bright.a 1:: intelligent 3; clever 3; smart 1;
3.2 SemEval-2012 Lexical Simplification
corpus
Given the list of contexts and each respective list
of substitutes we asked annotators to rank substi-
tutes for each individual context in ascending order
of complexity. Since the notion of textual simplic-
ity varies from individual to individual, we carefully
chose a group of annotators in an attempt to cap-
ture as much of a common notion of simplicity as
possible. For practical reasons, we selected annota-
tors with high proficiency levels in English as sec-
ond language learners - all with a university first de-
gree in different subjects.
The Trial dataset was annotated by four people
while the Test dataset was annotated by five peo-
ple. In both cases each annotator tagged the com-
plete dataset.
Inter-annotator agreement was computed using an
adaptation of the kappa index with pairwise rank
comparisons (Callison-Burch et al, 2011). This is
also the primary evaluation metric for participating
systems in the shared task, and it is covered in more
detail in Section 4.
The inter-annotator agreement was computed for
each pair of annotators and averaged over all possi-
ble pairs for a final agreement score. On the Trial
dataset, a kappa index of 0.386 was found, while
for the Test dataset, a kappa index of 0.398 was
found. It may be noted that certain annotators dis-
agreed considerably with all others. For example,
on the Test set, if annotations from one judge are re-
moved, the average inter-annotator agreement rises
to 0.443. While these scores are apparently low, the
highly subjective nature of the annotation task must
be taken into account. According to the reference
values for other tasks, this level of agreement is con-
sidered ?moderate? (Callison-Burch et al, 2011).
It is interesting to note that higher inter-annotator
agreement scores were achieved between annota-
tors with similar language and/or educational back-
grounds. The highest of any pairwise annotator
agreement (0.52) was achieved between annotators
of identical language and educational background,
as well as very similar levels of English proficiency.
High agreement scores were also achieved between
annotators with first languages belonging to the
same language family.
Finally, it is also worth noticing that this agree-
ment metric is highly sensitive to small differences
in annotation, thus leading to overly pessimistic
scores. A brief analysis reveals that annotators often
agree on clusters of simplicity and the source of the
disagreement comes from the rankings within these
clusters.
Finally, the gold-standard annotations for the
Trial and Test datasets ? against which systems are
to be evaluated ? were generated by averaging the
annotations from all annotators. This was done
context by context where each substitution was at-
tributed a score based upon the average of the rank-
ings it was ascribed. The substitutions were then
sorted in ascending order of scores, i.e., lowest score
(highest average ranking) first. Tied scores were
grouped together to form a single rank. For exam-
ple, assume that for a certain context, four annota-
tors provided rankings as given below, where multi-
ple candidates between {} indicate ties:
Annotator 1: {clear} {light} {bright} {lumi-
nous} {well-lit}
Annotator 2: {well-lit} {clear} {light}
{bright} {luminous}
Annotator 3: {clear} {bright} {light} {lumi-
nous} {well-lit}
Annotator 4: {bright} {well-lit} {luminous}
{clear} {light}
Thus the word ?clear?, having been ranked 1st,
2nd, 1st and 4th by each of the annotators respec-
tively is given an averaged ranking score of 2. Sim-
ilarly ?light? = 3.25, ?bright? = 2.5, ?luminous? =
4 and ?well-lit? = 3.25. Consequently the gold-
standard ranking for this context is:
Gold: {clear} {bright} {light, well-lit} {lumi-
nous}
349
3.3 Context-dependency
As mentioned in Section 1, one of our hypothe-
ses was that the notion of simplicity is context-
dependent. In other words, that the ordering of sub-
stitutes for different occurrences of a target word
with a given sense is highly dependent on the con-
texts in which such a target word appears. In order
to verify this hypothesis quantitatively, we further
analyzed the gold-standard annotations of the Trial
and Test datasets. We assume that identical lists of
substitutes for different occurrences of a given tar-
get word ensure that such a target word has the same
sense in all these occurrences. For every target word,
we then generate all pairs of contexts containing the
exact same initial list of substitutes and check the
proportion of these contexts for which human an-
notators ranked the substitutes differently. We also
check for cases where only the top-ranked substitute
is different. The numbers obtained are shown in Ta-
ble 1.
Trial Test
1) # context pairs 1350 7695
2) # 1) with same list 60 242
3) # 2) with different rankings 24 139
4) # 2) with different top substitute 19 38
Table 1: Analysis on the context-dependency of the no-
tion of simplicity.
Although the proportion of pairs of contexts with
the same list of substitutes is very low (less than
5%), it is likely that there are many other occur-
rences of a target word with the same sense and
slightly different lists of substitutes. Further man-
ual inspection is necessary to determine the actual
numbers. Nevertheless, from the observed sample
it is possible to conclude that humans will, in fact,
rank the same set of words (with the same sense)
differently depending on the context (on an average
in 40-57% of the instances).
4 Evaluation metric
No standard metric has yet been defined for eval-
uating Lexical Simplification systems. Evaluating
such systems is a challenging problem due to the
aforementioned subjectivity of the task. Since this
is a ranking task, rank correlation metrics are desir-
able. However, metrics such as Spearman?s Rank
Correlation are not reliable on the limited number of
data points available for comparison on each rank-
ing (note that the nature of the problem enforces a
context-by-context ranking, as opposed to a global
score), Other metrics for localized, pairwise rank
correlation, such as Kendall?s Tau, disregard ties, ?
which are important for our purposes ? and are thus
not suitable.
The main evaluation metric proposed for this
shared task is in fact a measure of inter-annotator
agreement, which is used for both contrasting two
human annotators (Section 3.2) and contrasting a
system output to the average of human annotations
that together forms the gold-standard.
Out metric is based on the kappa index (Cohen,
1960) which in spite of many criticisms is widely
used for its simplicity and adaptability for different
applications. The generalized form of the kappa in-
dex is
? =
P (A)? P (E)
1? P (E)
where P (A) denotes the proportion of times two
annotators agree and P (E) gives the probability of
agreement by chance between them.
In order to apply the kappa index for a ranking
task, we follow the method proposed by (Callison-
Burch et al, 2011) for measuring agreement over
judgments of translation quality. This method de-
fines P (A) and P (E) in such a way that it now
counts agreement whenever annotators concur upon
the order of pairwise ranks. Thus, if one annotator
ranked two given words 1 and 3, and the second an-
notator ranked them 3 and 7 respectively, they are
still in agreement. Formally, assume that two anno-
tators A1 and A2 rank two instance a and b. Then
P (A) = the proportion of times A1 and A2 agree
on a ranking, where an occurrence of agreement is
counted whenever rank(a < b) or rank(a = b) or
rank(a > b).
P (E) (the likelihood that annotators A1 and A2
agree by chance) is based upon the probability that
both of them assign the same ranking order to a and
b. Given that the probability of getting rank(a <
b) by any annotator is P (a < b), the probability
that both annotators get rank(a < b) is P (a < b)2
(agreement is achieved when A1 assigns a < b by
chance and A2 also assigns a < b). Similarly, the
350
probability of chance agreement for rank(a = b)
and rank(a > b) are P (a = b)2 and P (a > b)2
respectively. Thus:
P (E) = P (a < b)2 + P (a = b)2 + P (a > b)2
However, the counts of rank(a < b) and
rank(a > b) are inextricably linked, since for any
particular case of a1 < b1, it follows that b1 >
a1, and thus the two counts must be incremented
equally. Therefore, over the entire space of ranked
pairs, the probabilities remain exactly the same. In
essence, after counting for P (a = b), the remaining
probability mass is equally split between P (a < b)
and P (a > b). Therefore:
P (a < b) = P (a > b) =
1? P (a = b)
2
Kappa is calculated for every pair of ranked items
for a given context, and then averaged to get an over-
all kappa score:
? =
|N |?
n=1
Pn(A)? Pn(E)
1? Pn(E)
|N |
where N is the total number of contexts, and Pn(A)
and Pn(E) are calculated based on counts extracted
from the data on the particular context n.
The functioning of this evaluation metric is illus-
trated by the following example:
Context: During the siege, George Robert-
son had appointed Shuja-ul-Mulk, who was a
_____ boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
Gold: {intelligent} {clever} {smart} {bright}
System: {intelligent} {bright} {clever,
smart}
Out of the 6 distinct unordered pairs of lexical
items, system and gold agreed 3 times. Conse-
quently, Pn(A) = 36 . In addition, count(a =
b) = 1. Thus, Pn(a = b) = 112 . Which gives a
P (E) = 4196 and the final kappa score for this partic-
ular context of 0.13.
The statistical significance of the results from two
systems A and B is measured using the method
of Approximate Randomization, which has been
shown to be a robust approach for several NLP tasks
(Noreen, 1989). The randomization is run 1, 000
times and if the p-value is ? 0.05 the difference be-
tween systems A and B is asserted as being statisti-
cally significance.
5 Baselines
We defined three baseline lexical simplification sys-
tems for this task, as follows.
L-Sub Gold: This baseline uses the gold-standard
annotations from the Lexical Substitution cor-
pus of SemEval-2007 as is. In other words, the
ranking is based on the goodness of fit of sub-
stitutes for a context, as judged by human anno-
tators. This method also serves to show that the
Lexical Substitution and Lexical Simplification
tasks are indeed different.
Random: This baseline provides a randomized or-
der of the substitutes for every context. The
process of randomization is such that is allows
the occurrence of ties.
Simple Freq.: This simple frequency baseline uses
the frequency of the substitutes as extracted
from the Google Web 1T Corpus (Brants and
Franz, 2006) to rank candidate substitutes
within each context.
The results in Table 2 show that the ?L-Sub Gold?
and ?Random? baselines perform very poorly on
both Trial and Test sets. In particular, the reason for
the poor scores for ?L-Sub Gold? can be attributed
to the fact that it yields many ties, whereas the gold-
standard presents almost no ties. Our kappa met-
ric tends to penalize system outputs with too many
ties, since the probability of agreement by chance is
primarily computed on the basis of the number of
ties present in the two rankings being compared (see
Section 4).
The ?Simple Freq.? baseline, on the other hand,
performs very strongly, in spite of its simplistic ap-
proach, which is entirely agnostic to context. In fact
it surpasses the average inter-annotator agreement
on both Trial and Test datasets. Indeed, the scores on
the Test set approach the best inter-annotator agree-
ment scores between any two annotators.
351
Trial Test
L-Sub Gold 0.050 0.106
Random 0.016 0.012
Simple Freq. 0.397 0.471
Table 2: Baseline kappa scores on trial and test sets
6 Results and Discussion
6.1 Participants
Five sites submitted one or more systems to the task,
totaling nine systems:
ANNLOR-lmbing: This system (Ligozat et al,
2012) relies on language models probabili-
ties, and builds on the principle of the Sim-
ple Frequency baseline. While the baseline
uses Google n-grams to rank substitutes, this
approach uses Microsoft Web n-grams in the
same way. Additionally characteristics, such
as the contexts of each term to be substituted,
were integrated into the system. Microsoft Web
N-gram Service was used to obtain log likeli-
hood probabilities for text units, composed of
the lexical item and 4 words to the left and right
from the surrounding context.
ANNLOR-simple: The system (Ligozat et al,
2012) is based on Simple English Wikipedia
frequencies, with the motivation that the lan-
guage used in this version of Wikipedia is
targeted towards people who are not first-
language English speakers. Word n-grams (n =
1-3) and their frequencies were extracted from
this corpus using the Text-NSP Perl module
and a ranking of the possible substitutes of a
target word according to these frequencies in
descending order was produced.
EMNLPCPH-ORD1: The system performs a se-
ries of pairwise comparisons between candi-
dates. A binary classifier is learned purpose
using the Trial dataset and artificial unlabeled
data extracted based on Wordnet and a corpus
in a semi-supervised fashion. A co-training
procedure that lets each classifier increase the
other classifier?s training set with selected in-
stances from the unlabeled dataset is used. The
features include word and character n-gram
probabilities of candidates and contexts using
web corpora, distributional differences of can-
didate in a corpus of ?easy? sentences and a
corpus of normal sentences, syntactic complex-
ity of documents that are similar to the given
context, candidate length, and letter-wise rec-
ognizability of candidate as measured by a tri-
gram LM. The first feature sets for co-training
combines the syntactic complexity, character
trigram LM and basic word length features, re-
sulting in 29 features against the remaining 21.
EMNLPCPH-ORD2: This is a variant of the
EMNLPCPH-ORD1 system where the first fea-
ture set pools all syntactic complexity fea-
tures and Wikipedia-based features (28 fea-
tures) against all the remaining 22 features in
the second group.
SB-mmSystem: The approach (Amoia and Ro-
manelli, 2012) builds on the baseline defini-
tion of simplicity using word frequencies but
attempt at defining a more linguistically mo-
tivated notion of simplicity based on lexical
semantics considerations. It adopts different
strategies depending on the syntactic complex-
ity of the substitute. For one-word substitutes
or common collocations, the system uses its
frequency from Wordnet as a metric. In the
case of multi-words substitutes the system uses
?relevance? rules that apply (de)compositional
semantic criteria and attempts to identify a
unique content word in the substitute that might
better approximate the whole expression. The
expression is then assigned the frequency asso-
ciated to this content word for the ranking. Af-
ter POS tagging and sense disambiguating all
substitutes, hand-written rules are used to de-
compose the meaning of a complex phrase and
identify the most relevant word conveying the
semantics of the whole.
UNT-SimpRank: The system (Sinha, 2012) uses
external resources, including the Simple En-
glish Wikipedia corpus, a set of Spoken En-
glish dialogues, transcribed into machine read-
able form, WordNet, and unigram frequencies
(Google Web1T data). SimpRank scores each
substitute by a sum of its unigram frequency, its
352
frequency in the Simple English Wikipedia, its
frequency in the spoken corpus, the inverse of
its length, and the number of senses the sub-
stitute has in WordNet. For a given context,
the substitutes are then reverse-ranked based on
their simplicity scores.
UNT-SimpRankLight: This is a variant of Sim-
pRank which does not use unigram frequen-
cies. The goal of this system is to check
whether a memory and time-intensive and non-
free resource such as the Web1T corpus makes
a difference over other free and lightweight re-
sources.
UNT-SaLSA: The only resource SaLSA depends
on is the Web1T data, and in particular only
3-grams from this corpus. It leverages the con-
text provided with the dataset by replacing the
target placeholder one by one with each of the
substitutes and their inflections thus building
sets of 3-grams for each substitute in a given
instance. The score of any substitute is then the
sum of the 3-gram frequencies of all the gener-
ated 3-grams for that substitute.
UOW-SHEF-SimpLex: The system (Jauhar and
Specia, 2012) uses a linear weighted ranking
function composed of three features to pro-
duce a ranking. These include a context sen-
sitive n-gram frequency model, a bag-of-words
model and a feature composed of simplicity
oriented psycholinguistic features. These three
features are combined using an SVM ranker
that is trained and tuned on the Trial dataset.
6.2 Pairwise kappa
The official task results and the ranking of the sys-
tems are shown in Table 3.
Firstly, it is worthwhile to note that all the top
ranking systems include features that use frequency
as a surrogate measure for lexical simplicity. This
indicates a very high correlation between distribu-
tional frequency of a given word and its perceived
complexity level. Additionally, the top two systems
involve context-dependent and context-independent
features, thus supporting our hypothesis of the com-
posite nature of the lexical simplification problem.
Rank Team - System Kappa
1 UOW-SHEF-SimpLex 0.496
2
UNT-SimpRank 0.471
Baseline-Simple Freq. 0.471
ANNLOR-simple 0.465
3 UNT-SimpRankL 0.449
4 EMNLPCPH-ORD1 0.405
5 EMNLPCPH-ORD2 0.393
6 SB-mmSystem 0.289
7 ANNLOR-lmbing 0.199
8 Baseline-L-Sub Gold 0.106
9 Baseline-Random 0.013
10 UNT-SaLSA -0.082
Table 3: Official results and ranking according to the pair-
wise kappa metric. Systems are ranked together when the
difference in their kappa score is not statistically signifi-
cant.
Few of the systems opted to use some form of
supervised learning for the task, due to the limited
number of training examples given. As pointed out
by some participants who checked learning curves
for their systems, the performance is likely to im-
prove with larger training sets. Without enough
training data, context agnostic approaches such as
the ?Simple Freq.? baseline become very hard to
beat.
We speculate that the reason why the effects of
context-aware approaches are somewhat mitigated is
because of the isolated setup of the shared task. In
practice, humans produce language at an even level
of complexity, i.e. consistently simple, or consis-
tently complex. In the shared task?s setup, systems
are expected to simplify a single target word in a
context, ignoring the possibility that sometimes sim-
ple words may not be contextually associated with
complex surrounding words. This not only explains
why context-aware approaches are less successful
than was originally expected, but also gives a reason
for the good performance of context-agnostic sys-
tems.
6.3 Recall and top-rank
As previously noted, the primary evaluation met-
ric is very susceptible to penalize slight changes,
making it overly pessimistic about systems? perfor-
mance. Hence, while it may be an efficient way to
compare and rank systems within the framework of
353
a shared task, it may be unnecessarily devaluing the
practical viability of approaches. We performed two
post hoc evaluations that assess system output from
a practical point of view. We check how well the
top-ranked substitute, i.e., the simplest substitute ac-
cording to a given system (which is most likely to
be used in a real simplification task) compares to the
top-ranked candidate from the gold standard. This is
reported in the TRnk column of Table 4: the percent-
age of contexts in which the intersection between the
simplest substitute set from a system?s output and
the gold standard contained at least one element.
We note that while ties are virtually inexistent in the
gold standard data, ties in the system output can af-
fect this metric: a system that naively predicts all
substitutes as the simplest (i.e., a single tie includ-
ing all candidates) will score 100% in this metric.
We also measured the ?recall-at-n" values for 1 ?
n ? 3, which gives the ratio of candidates from the
top n substitute sets to those from the gold-standard.
For a given n, we only consider contexts that have
at least n+1 candidates in the gold-standard (so that
there is some ranking to be done). Table 4 shows the
results of this additional analysis.
Team - System TRnk n=1 n=2 n=3
UOW-SHEF-SimpLex 0.602 0.575 0.689 0.769
UNT-SimpRank 0.585 0.559 0.681 0.760
Baseline-Simple Freq. 0.585 0.559 0.681 0.760
ANNLOR-simple 0.564 0.538 0.674 0.768
UNT-SimpRankL 0.567 0.541 0.674 0.753
EMNLPCPH-ORD1 0.539 0.513 0.645 0.727
EMNLPCPH-ORD2 0.530 0.503 0.637 0.722
SB-mmSystem 0.477 0.452 0.632 0.748
ANNLOR-lmbing 0.336 0.316 0.494 0.647
Baseline-L-Sub Gold 0.454 0.427 0.667 0.959
Baseline-Random 0.340 0.321 0.612 0.825
UNT-SaLSA 0.146 0.137 0.364 0.532
Table 4: Additional results according to the top-rank
(TRnk) and recall-at-n metrics.
These evaluation metrics favour systems that pro-
duce many ties. Consequently the baselines ?L-Sub
Gold" and ?Random" yield overly high scores for
recall-at-n for n=2 and n= 3. Nevertheless the rest
of the results are by and large consistent with the
rankings from the kappa metric.
The results for recall-at-2, e.g., show that most
systems, on average 70% of the time, are able to
find the simplest 2 substitute sets that correspond
to the gold standard. This indicates that most ap-
proaches are reasonably good at distinguishing very
simple substitutes from very complex ones, and that
the top few substitutes will most often produce ef-
fective simplifications.
These results correspond to our experience from
the comparison of human annotators, who are easily
able to form clusters of simplicity with high agree-
ment, but who strongly disagree (based on personal
biases towards perceptions of lexical simplicity) on
the internal rankings of these clusters.
7 Conclusions
We have presented the organization and findings of
the first English Lexical Simplification shared task.
This was a first attempt at garnering interest in the
NLP community for research focused on the lexical
aspects of Text Simplification.
Our analysis has shown that there is a very strong
relation between distributional frequency of words
and their perceived simplicity. The best systems on
the shared task were those that relied on this asso-
ciation, and integrated both context-dependent and
context-independent features. Further analysis re-
vealed that while context-dependent features are im-
portant in principle, their applied efficacy is some-
what lessened due to the setup of the shared task,
which treats simplification as an isolated problem.
Future work would involve evaluating the im-
portance of context for lexical simplification in the
scope of a simultaneous simplification to all the
words in a context. In addition, the annotation of
the gold-standard datasets could be re-done taking
into consideration some of the features that are now
known to have clearly influenced the large variance
observed in the rankings of different annotators,
such as their background language and the educa-
tion level. One option would be to select annotators
that conform a specific instantiation of these fea-
tures. This should result in a higher inter-annotator
agreement and hence a simpler task for simplifica-
tion systems.
Acknowledgments
We would like to thank the annotators for their hard
work in delivering the corpus on time.
354
References
Marilisa Amoia and Massimo Romanelli. 2012. SB-
mmSystem: Using Decompositional Semantics for
Lexical Simplification. In English Lexical Simplifica-
tion. Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal,
Canada.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496?501,
Portland, Oregon.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland.
Arnaldo Candido, Jr., Erick Maziero, Caroline Gasperin,
Thiago A. S. Pardo, Lucia Specia, and Sandra M.
Aluisio. 2009. Supporting the adaptation of texts for
poor literacy readers: a text simplification editor for
Brazilian Portuguese. In Proceedings of the Fourth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 34?42, Boulder, Col-
orado.
J Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46, April.
Jan de Belder, Koen Deschacht, and Marie-Francine
Moens. 2010. Lexical simplification. In Proceedings
of Itec2010: 1st International Conference on Inter-
disciplinary Research on Technology, Education and
Communication, Kortrijk, Belgium.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text for
aphasic readers. Linguistic Databases, pages 161?
173.
Sujay Kumar Jauhar and Lucia Specia. 2012. UOW-
SHEF: SimpLex - Lexical Simplicity Ranking based
on Contextual and Psycholinguistic Features. In En-
glish Lexical Simplification. Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Montreal, Canada.
Anne-Laure Ligozat, Cyril Grouin, Anne Garcia-
Fernandez, and Delphine Bernhard. 2012. ANNLOR:
A Naive Notation-system for Lexical Outputs Rank-
ing. In English Lexical Simplification. Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Montreal, Canada.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48?53.
E. Noreen. 1989. Computer-intensive methods for test-
ing hypotheses. New York: Wiley.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
Advaith Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Computa-
tion, 4:77?109.
Ravi Sinha. 2012. UNT-SimpRank: Systems for Lex-
ical Simplification Ranking. In English Lexical Sim-
plification. Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Lucia Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th international
conference on Computational Processing of the Por-
tuguese Language, PROPOR?10, pages 30?39, Berlin,
Heidelberg. Springer-Verlag.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California.
355
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477?481,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UOW-SHEF: SimpLex ? Lexical Simplicity Ranking based on Contextual
and Psycholinguistic Features
Sujay Kumar Jauhar
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton
WV1 1SB, UK
Sujay.KumarJauhar@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
L.Specia@dcs.shef.ac.uk
Abstract
This paper describes SimpLex,1 a Lexical
Simplification system that participated in the
English Lexical Simplification shared task at
SemEval-2012. It operates on the basis of
a linear weighted ranking function composed
of context sensitive and psycholinguistic fea-
tures. The system outperforms a very strong
baseline, and ranked first on the shared task.
1 Introduction
Lexical Simplification revolves around replacing
words by their simplest synonym in a context aware
fashion. It is similar in many respects to the task of
Lexical Substitution (McCarthy and Navigli, 2007)
in that it involves elements of selectional preference
on the basis of a central predefined criterion (sim-
plicity in the current case), as well as sensitivity to
context.
Lexical Simplification envisages principally a hu-
man target audience, and can greatly benefit chil-
dren, second language learners, people with low lit-
eracy levels or cognitive disabilities, and in general
facilitate the dissemination of knowledge to wider
audiences.
We experimented with a number of features that
we posited might be inherently linked with tex-
tual simplicity and selected the three that seemed
the most promising on an evaluation with the trial
dataset. These include contextual and psycholin-
guistic components. When combined using an SVM
1Developed by co-organizers of the shared task
ranker to build a model, such a model provides re-
sults that offer a statistically significant improve-
ment over a very strong context-independent base-
line. The system ranked first overall on the Lexical
Simplification task.
2 Related Work
Lexical Simplification has received considerably
less interest in the NLP community as compared
with Syntactic Simplification. However, there are
a number of notable works related to the topic.
In particular Yatskar et al (2010) leverage the
relations between Simple Wikipedia and English
Wikipedia to extract simplification pairs. Biran et al
(2011) extend this base methodology to apply lexi-
cal simplification to input sentences. De Belder and
Moens (2010), in contrast, provide a more general
architecture for the task, with scope for possible ex-
tension to other languages.
These studies and others have envisaged a range
of different target user groups including children
(De Belder and Moens, 2010), people with low liter-
acy levels (Aluisio et al, 2008) and aphasic readers
(Carroll et al, 1998).
The current work differs from previous research
in that it envisages a stand-alone lexical simpli-
fication system based on linguistically motivated
and cognitive principles within the framework of a
shared task. Its core methodology remains open to
integration into a larger Text Simplification system.
3 Task Setup
The English Lexical Simplification shared task at
SemEval-2012 (Specia et al, 2012) required sys-
477
tems to rank a number of candidate substitutes
(which were provided beforehand) based on their
simplicity of usage in a given context. For example,
given the following context with an empty place-
holder, and its candidate substitutes:
Context: During the siege , George
Robertson had appointed Shuja-ul-Mulk,
who was a boy only 12 years old and
the youngest surviving son of Aman-ul-
Mulk, as the ruler of Chitral.
Candidates: {clever} {smart}
{intelligent} {bright}
a system is required to produce a ranking, e.g.:
System: {intelligent} {bright} {clever,
smart}
Note that ties were permitted and that all candi-
dates needed to be included in the system rankings.
4 The SimpLex Lexical Simplification
System
In an approach similar to what Hassan et al (2007)
used for Lexical Substitution, SimpLex ranks can-
didates based on a weighted linear scoring function,
which has the generalized form:
s (cn,i) =
?
m?M
1
rm (cn,i)
where cn,i is the candidate substitute to be scored,
and each rm is a standalone ranking function that
attributes to each candidate its rank based on its
uniquely associated features. Based on this scoring,
candidates for context are ranked in descending or-
der of scores.
In the development of the system we experi-
mented with a number of these features including
ranking based on word length, number of syllables,
scoring with a 2-step cluster and rank architecture,
latent semantic analysis, and average point-wise mu-
tual information between the candidate and neigh-
boring words in the context.
However, the features which were intuitively the
simplest proved, in the end, to give the best results.
They were selected based on their superior perfor-
mance on the trial dataset and their competitiveness
with the strong Simple Frequency baseline. These
stand-alone features are described in what follows.
4.1 Adapted N-Gram Model
The motivation behind an n-gram model for Lexical
Simplification is that the task involves an inherent
WSD problem. This is because the same word may
be used with different senses (and consequently dif-
ferent levels of complexity) in different contexts.
A blind application of n-gram frequency search-
ing on the shared task?s dataset, however, gives sub-
optimal results because of two main factors:
1. Inconsistently lemmatized candidates.
2. Blind replacement of even correctly lemma-
tized forms in context producing ungrammat-
ical results.
We infer the correct inflection of all candidates for
a given context based on the appearance of the orig-
inal target word (which is also one of the candidate
substitutes) in context. To do this we run a part-of-
speech (POS) tagger on the source text and note the
POS of the target word. Then handcrafted rules are
used to correctly inflect the other candidates based
on this POS tag.
To resolve the issue of ungrammatical textual out-
put, we further use a simple approach of popping
words in close proximity to the placeholder and per-
forming n-gram searches on all possible query com-
binations. Take for instance the following example:
Context: He was away.
Candidates: {going} {leaving}
where ?going? is evidently the original word in con-
text, but ?leaving? has also been suggested as a sub-
stitute (there are many such cases in the datasets).
One of the possible outcomes of popping context
words leads to the correct sequence for the latter
substitute, i.e. ?He was leaving? with the word
?away? having been popped.
The rationale behind this approach is that if one of
the combinations is grammatically correct, the num-
ber of n-gram hits it returns will far exceed those
returned by ungrammatical ones.
The n-gram (2 ? n ? 5) searches are performed
on the Google Web 1T corpus (Brants and Franz,
2006), and the number of hits is weighted by the
length of the n-gram search (such that longer se-
quences obtain higher weight). This may seem like
478
a simplistic approach, especially when the candidate
words appear in long-distance dependency relations
to other parts of the sentence. However, it should be
noted that since the Web 1T corpus only consists of
n-grams with n ? 5, structures that contain longer
dependencies than this are in any case not consid-
ered, and hence do not interfere with local context.
4.2 Bag-of-Words Model
The limitations of performing queries on the Google
Web 1T are that n-grams hits must be in strict lin-
ear order of appearance. To overcome this diffi-
culty, we further mimic the functioning of a bag-
of-words model by taking all possible ordering of
words of a given n-gram sequence. This approach,
to some extent, gives the possibility of observing co-
occurrences of candidate and context words in vari-
ous orderings of appearance. This results in a num-
ber of inadequate query strings, but possibly a few
(as opposed to one in a linear n-gram search) good
word orderings with high hits as well.
As with the previous model, only n-grams with
2 ? n ? 5 are taken. For a given substitute the total
number of hits for all possible queries involving that
substitute are summed (with each hit being weighted
by the length of its corresponding query in words).
To obtain the final score, this sum is normalized by
the actual number of queries.
4.3 Psycholinguistic Feature Model
The MRC Psycholinguistic Database (Wilson, 1988)
and the Bristol Norms (Stadthagen-Gonzalez and
Davis, 2006) are knowledge repositories that asso-
ciate scores to words based on a number of psy-
cholinguistic features. The ones that we felt were
most pertinent to our study are:
1. Concreteness - the level of abstraction associ-
ated with the concept a word describes.
2. Imageability - the ability of a given word to
arouse mental images.
3. Familiarity - the frequency of exposure to a
word.
4. Age of Acquisition - the age at which a given
word is appropriated by a speaker.
We combined both databases and compiled a sin-
gle resource consisting of all the words from both
sources that list at least one of these features. It may
be noted that these attributes were compiled in simi-
lar fashion in both databases and were normalized to
the same scale of scores falling in the range of 100
to 700.
In spite of a combined compilation, the coverage
of the resource was poor, with more than half the
candidate substitutes on both trial and test sets sim-
ply not being listed in the databases. To overcome
this difficulty we introduced a fifth frequency feature
that essentially simulates the ?Simple Frequency?
baseline, 2 but with scores that were normalized to
the same scale of the other psycholinguistic features.
This composite of features was used in a linear
weighted function with weights tuned to best perfor-
mance values on the trial dataset. This function sums
the weighted scores for each candidate, and normal-
izes this sum by the number of non-zero features (in
the worst-case scenario, ? when no psycholinguistic
features are found ? the scorer is equivalent to the
?Simple Frequency? baseline). It is interesting to
note that the frequency feature did not dominate the
linear combination; rather there was a nice interplay
of features with Concreteness, Imageability, Famil-
iarity, Age of Acquisition and Simple Frequency be-
ing weighted (on a scale of -1 to +1) as 0.72, -0.22,
0.87, 0.36 and 0.36, respectively.
4.4 Feature Combination
We combined the three standalone models using
the ranking function of the SVM-light package
(Joachims, 2006) for building SVM rankers. The pa-
rameters of the SVM were tuned on the trial dataset,
which consisted of only 300 example contexts. To
avoid overfitting, instead of taking the single best
parameters, we took parameter values that were the
average of the top 10 distinct runs.
It may be noted that the resulting model makes no
attempt to tie candidates, although actual ties may be
produced by chance. But since ties are rarely used
in the gold standard for the trial dataset, we reasoned
that this should not affect the system performance in
any significant way.
2The ?Simple Frequency? baseline scores each substitute
based on the number of hits it produces in the Google Web 1T
479
bline-SFreq w-ln n-syll psycho a-n-gram b-o-w pmi lsa SimpLex
Trial 0.398 0.176 0.118 0.388 0.397 0.395 0.340 0.089 ?
Test 0.471 0.236 0.163 0.432 0.460 0.460 0.404 0.054 0.496
Table 1: Comparison of Models? Scores
5 Results and Discussion
The results of the SimpLex system trained and tuned
on the trial set, in comparison with the Simple Fre-
quency baseline and the other stand-alone features
we experimented with are presented in Table 1. The
scores are computed through a version of the Kappa
index over pairwise rankings, and therefore repre-
sent the average agreement between the system and
the gold-standard annotation in the ranking of pairs
of candidate substitutes.
Table 1 shows that while in isolation the features
are unable to beat the Simple Frequency model, to-
gether they form a combination which outperforms
the baseline. The improvement of SimpLex over
the other models is statistically significant (statisti-
cal significance was established using a randomiza-
tion test with 1000 iterations and p-value ? 0.05).
We believe that the reason why the context aware
features were still unable to score better than the
context-independent baseline is the isolated focus
on simplifying a single target word. People tend
to produce language that contains words of roughly
equal levels of complexity. Hence in some cases
the surrounding context, instead of helping to dis-
ambiguate the target word, introduces further noise
to queries, especially when its individual component
words have skewed complexity factors. A simul-
taneous simplification of all the content words in a
context could be a possible solution to this problem.
As an additional experiment to assess the impor-
tance of the size of the training data in our simplifi-
cation system, we pooled together the trial and test
datasets, and ran several iterations of the combina-
tion algorithm with a regular increment of number of
training examples and noted the effects it produced
on eventual score. Three hundred examples were ap-
portioned consistently to a test set to maintain com-
parability between experiments. Note that this time,
no optimization of the SVM parameters was made.
The results were inconclusive, and contrary to ex-
pectation, revealed that there is no general improve-
ment with additional training data. This could be
because of the difficulty of the learning problem, for
which the scope of the combined dataset is still very
limited. A more detailed study with a corpus that is
orders of magnitude larger than the current one may
be necessary to establish conclusive evidence.
6 Conclusion
This paper presented our system SimpLex which
participated in the English Lexical Simplification
shared-task at SemEval-2012 and ranked first out of
9 participating systems.
Our findings showed that while a context agnostic
frequency approach to lexical simplification seems
to effectively model the problem of assessing word
complexity to a relatively decent level of accuracy,
as evidenced by the strong baseline of the shared
task, other elements, such as interplay of context
awareness with humanly perceived psycholinguistic
features can produce better results, in spite of very
limited training data.
Finally, a more global approach to lexical sim-
plification that concurrently addresses all the words
in a context to normalize simplicity levels, may be
a more realistic proposition for target applications,
and also help context aware features perform better.
Acknowledgment
This work was supported by the European Com-
mission, Education & Training, Erasmus Mundus:
EMMC 2008-0083, Erasmus Mundus Masters in
NLP & HLT program.
References
Sandra M. Aluisio, Lucia Specia, Thiago A.S. Pardo, Er-
ick G. Maziero, and Renata P.M. Fortes. 2008. To-
wards brazilian portuguese automatic text simplifica-
tion systems. In Proceeding of the eighth ACM sym-
480
posium on Document engineering, DocEng ?08, pages
240?248, Sao Paulo, Brazil. ACM.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496?501,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1 ldc2006t13.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of AAAI - 98 Workshop on Integrating
Artificial Intelligence and Assistive Technology, Madi-
son, Wisconsin, July.
Jan De Belder and Marie-Francine Moens. 2010. Text
simplification for children. In Proceedings of the SI-
GIR workshop on Accessible Search Systems, pages
19?26. ACM, July.
S. Hassan, A. Csomai, C. Banea, R. Sinha, and R. Mi-
halcea. 2007. Unt: Subfinder: Combining knowledge
sources for automatic lexical substitution. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 410 ? 413. Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?06, pages 217?226, New York,
NY, USA. ACM.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48?53.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. Semeval-2012 task 1: English lexical simplifi-
cation. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Hans Stadthagen-Gonzalez and Colin Davis. 2006. The
bristol norms for age of acquisition, imageability, and
familiarity. Behavior Research Methods, 38:598?605.
Michael Wilson. 1988. Mrc psycholinguistic database:
Machine-usable dictionary, version 2.00. Behavior
Research Methods, 20:6?10.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 365?368, Stroudsburg, PA, USA.
Association for Computational Linguistics.
481
Proceedings of the First Workshop on Metaphor in NLP, pages 52?57,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Identifying Metaphorical Word Use with Tree Kernels
Dirk Hovy1 Shashank Srivastava2 Sujay Kumar Jauhar2 Mrinmaya Sachan2
Kartik Goyal2 Huiying Li2 Whitney Sanders2 Eduard Hovy2
(1) ISI, University of Southern California, Marina del Rey
(2) LTI, Carnegie Mellon University, Pittsburgh
dirkh@isi.edu, {shashans,sjauhar,mrinmays,kartikgo,huiyingl,wsanders,hovy}@cs.cmu.edu
Abstract
A metaphor is a figure of speech that refers
to one concept in terms of another, as in ?He
is such a sweet person?. Metaphors are ubiq-
uitous and they present NLP with a range
of challenges for WSD, IE, etc. Identifying
metaphors is thus an important step in lan-
guage understanding. However, since almost
any word can serve as a metaphor, they are
impossible to list. To identify metaphorical
use, we assume that it results in unusual se-
mantic patterns between the metaphor and its
dependencies. To identify these cases, we use
SVMs with tree-kernels on a balanced corpus
of 3872 instances, created by bootstrapping
from available metaphor lists.1 We outper-
form two baselines, a sequential and a vector-
based approach, and achieve an F1-score of
0.75.
1 Introduction
A metaphor is a figure of speech used to transfer
qualities of one concept to another, as in ?He is
such a sweet person?. Here, the qualities of ?sweet?
(the source) are transferred to a person (the target).
Traditionally, linguistics has modeled metaphors as
a mapping from one domain to another (Lakoff and
Johnson, 1980).
Metaphors are ubiquitous in normal language and
present NLP with a range of challenges. First, due to
their very nature, they cannot be interpreted at face
value, with consequences for WSD, IE, etc. Second,
metaphors are very productive constructions, and
almost any word can be used metaphorically (e.g.,
1Available at http://www.edvisees.cs.cmu.edu/
metaphordata.tar.gz
?This is the Donald Trump of sandwiches.?). This
property makes them impossible to pre-define or
list. Third, repeated use of a metaphor eventu-
ally solidifies it into a fixed expression with the
metaphorical meaning now accepted as just another
sense, no longer recognized as metaphorical at all.
This gradient makes it hard to determine a boundary
between literal and metaphorical use of some ex-
pressions. Identifying metaphors is thus a difficult
but important step in language understanding.2
Since many words can be productively used as
new metaphors, approaches that try to identify
them based on lexical features alone are bound to
be unsuccessful. Some approaches have therefore
suggested considering distributional properties
and ?abstractness? of the phrase (Turney et al,
2011). This nicely captures the contextual nature
of metaphors, but their ubiquity makes it impossible
to find truly ?clean? data to learn the separate
distributions of metaphorical and literal use for
each word. Other approaches have used pre-defined
mappings from a source to a target domain, as in
?X is like Y?, e.g., ?emotions are like temperature?
(Mason, 2004). These approaches tend to do well
on the defined mappings, but they do not generalize
to new, creative metaphors. It is doubtful that it
is feasible to list all possible mappings, so these
approaches remain brittle.
In contrast, we do not assume any predefined
mappings. We hypothesize instead that if we inter-
preted every word literally, metaphors will manifest
themselves as unusual semantic compositions.
Since these compositions most frequently occur
2Shutova (2010) distinguishes between metaphor identifica-
tion (which she calls recognition) and interpretation. We are
solely concerned with the former.
52
in certain syntactic relations, they are usually con-
sidered semantic preference violations; e.g., in the
metaphorical ?You will have to eat your words?, the
food-related verb heads a noun of communication.
In contrast, with the literal sense of ?eat? in ?You
will have to eat your peas?, it heads a food noun.
This intuition is the basis of the approaches in
(Iverson and Helmreich, 1991; Krishnakumaran
and Zhu, 2007; Baumer et al, 2010; Turney et
al., 2011).3 We generalize this intuition beyond
preference selections of verbs and relational nouns.
Given enough labeled examples of a word, we
expect to find distinctive differences in the compo-
sitional behavior of its literal and metaphorical uses
in certain preferred syntactic relationships. If we
can learn to detect such differences/anomalies, we
can reliably identify metaphors. Since we expect
these patterns in levels other than the lexical level,
the approach expands well to creative metaphors.
The observation that the anomaly tends to occur
between syntactically related words makes depen-
dency tree kernels a natural fit for the problem. Tree
kernels have been successfully applied to a wide
range of NLP tasks that involve (syntactic) relations
(Culotta and Sorensen, 2004; Moschitti, 2006; Qian
et al, 2008; Giuliano et al, 2009; Mirroshandel et
al., 2011).
Our contributions in this paper are:
? we annotate and release a corpus of 3872 in-
stances for supervised metaphor classification
? we are the first to use tree kernels for metaphor
identification
? our approach achieves an F1-score of 0.75, the
best score of of all systems tested.
2 Data
2.1 Annotation
We downloaded a list of 329 metaphor examples
from the web4. For each expression, we extracted
sentences from the Brown corpus that contained
the seed (see Figure 1 for an example). To decide
3A similar assumption can be used to detect the literal/non-
literal uses of idioms (Fazly et al, 2009).
4http://www.metaphorlist.com and http://
www.macmillandictionaryblog.com
whether a particular instance is used metaphorically,
we set up an annotation task on Amazon Mechanical
Turk (AMT).
Annotators were asked to decide whether a
highlighted expression in a sentence was used
metaphorically or not (see Figure 2 for a screen-
shot). They were prompted to think about whether
the expression was used in its original meaning.5
In some cases, it is not clear whether an expression
is used metaphorically or not (usually in short
sentences such as ?That?s sweet?), so annotators
could state that it was not possible to decide. We
paid $0.09 for each set of 10 instances.
Each instance was annotated by 7 annotators.
Instances where the annotators agreed that it was
impossible to tell whether it is a metaphor or not
were discarded. Inter-annotator agreement was
0.57, indicating a difficult task. In order to get the
label for each instance, we weighted the annotator?s
answers using MACE (Hovy et al, 2013), an
implementation of an unsupervised item-response
model. This weighted voting produces more reliable
estimates than simple majority voting, since it is
capable of sorting out unreliable annotators. The
final corpus consisted of 3872 instances, 1749 of
them labeled as metaphors.
Figure 2: Screenshot of the annotation interface on Ama-
zon?s Mechanical Turk
We divided the data into training, dev, and test
sets, using a 80-10-10 split. All results reported
here were obtained on the test set. Tuning and
development was only carried out on the dev set.
2.2 Vector Representation of Words
The same word may occur in a literal and a
metaphorical usage. Lexical information alone is
5While this is somewhat imprecise and not always easy to
decide, it proved to be a viable strategy for untrained annotators.
53
A bright idea.
? Peter is the bright , sympathetic guy when you ?re doing a deal , ? says one agent . yes
Below he could see the bright torches lighting the riverbank . no
Her bright eyes were twinkling . yes
Washed , they came out surprisingly clear and bright . no
Figure 1: Examples of a metaphor seed, the matching Brown sentences, and their annotations
thus probably not very helpful. However, we would
like to capture semantic aspects of the word and
represent it in an expressive way. We use the exist-
ing vector representation SENNA (Collobert et al,
2011) which is derived from contextual similarity.
In it, semantically similar words are represented
by similar vectors, without us having to define
similarity or looking at the word itself. In initial
tests, these vectors performed better than binary
vectors straightforwardly derived from features of
the word in context.
2.3 Constructing Trees
a) b) c)
like
I people
the sweet in
Boston
NNS
DT JJ IN
n.group
O adj.all O
NNP n.location
VB
PRP
v.emotion
O
Figure 3: Graphic demonstration of our approach. a) de-
pendency tree over words, with node of interest labeled.
b) as POS representation. c) as supersense representation
The intuition behind our approach is that
metaphorical use differs from literal use in certain
syntactic relations. For example, the only difference
between the two sentences ?I like the sweet people
in Boston? and ?I like the sweet pies in Boston? is
the head of ?sweet?. Our assumption is that?given
enough examples?certain patterns emerge (e.g.,
that ?sweet? in combination with food nouns is
literal, but is metaphorical if governed by a noun
denoting people).
We assume that these patterns occur on different
levels, and mainly between syntactically related
words. We thus need a data representation to
capture these patterns. We borrow its structure from
dependency trees, and the different levels from
various annotations. We parse the input sentence
with the FANSE parser (Tratz and Hovy, 2011)6. It
provides the dependency structure, POS tags, and
other information.
To construct the different tree representations,
we replace each node in the tree with its word,
lemma, POS tag, dependency label, or supersense
(the WordNet lexicographer name of the word?s
first sense (Fellbaum, 1998)), and mark the word
in question with a special node. See Figure 3 for
a graphical representation. These trees are used in
addition to the vectors.
This approach is similar to the ones described in
(Moschitti et al, 2006; Qian et al, 2008; Hovy et
al., 2012).
2.4 Classification Models
A tree kernel is simply a similarity matrix over tree
instances. It computes the similarity between two
trees T1, T2 based on the number of shared subtrees.
We want to make use of the information en-
coded in the different tree representations during
classification, i.e., a forest of tree kernels. We thus
combine the contributions of the individual tree
representation kernels via addition. We use kernels
over the lemma, POS tag, and supersense tree
representations, the combination which performed
best on the dev set in terms of accuracy.
We use the SVMlight TK implementation by
Moschitti (2006).7 We left most parameters set
to default values, but tuned the weight of the
contribution of the trees and the cost factor on the
dev set. We set the multiplicative constant for the
trees to 2.0, and the cost factor for errors on positive
examples to 1.7.
6http://www.isi.edu/publications/
licensed-sw/fanseparser/index.html
7http://disi.unitn.it/moschitti/
Tree-Kernel.htm
54
If we assume any word can be used metaphori-
cally, we ultimately want to label every word in a
sentence, so we also evaluate a sequential model, in
this case a CRF. We use CRFsuite (Okazaki, 2007)8
to implement the CRF, and run it with averaged
perceptron. While the CRF produces labels for
every word, we only evaluate on the words that
were annotated in our corpus (to make it maximally
comparable), and use the same representations
(lemma, POS and SST) of the word and its parent
as features as we did for the SVM. Training method
and feature selection were again tuned on the dev
set to maximize accuracy.
3 Experiments
system acc P R F1
BLall 0.49 0.49 1.0 0.66
BLmost freq. class 0.70 0.66 0.65 0.65
CRF 0.69? 0.74? 0.50 0.59
SVMvector?only 0.70? 0.63? 0.80 0.71
SVM+tree 0.75? 0.70? 0.80 0.75?
Table 1: Accuracy, precision, recall, and F1 for various
systems on the held-out test set. Values significantly bet-
ter than baseline at p < .02 are marked ? (two-tailed t-
test).
We compare the performance of two baselines,
the CRF model, vanilla SVM, and SVM with tree
kernels and report accuracy, precision, recall, and
F1 (Table 1).
The first baseline (BLall) labels every instance
as metaphor. Its accuracy and precision reflect the
metaphor ratio in the data, and it naturally achieves
perfect recall. This is a rather indiscriminate
approach and not very viable in practice, so we
also apply a more realistic baseline, labeling each
word with the class it received most often in the
training data (BLmost freq. class ). This is essentially
like assuming that every word has a default class.
Accuracy and precision for this baseline are much
better, although recall naturally suffers.
The CRF improves in terms of accuracy and
precision, but lacks the high recall the baseline
has, resulting in a lower F1-score. It does yield
8http://www.chokkan.org/software/
crfsuite/
the highest precision of all models, though. So
while not capturing every metaphor in the data, it is
usually correct if it does label a word as metaphor.
SVMlight allows us to evaluate the performance
of a classification using only the vector representa-
tion (SVMvector?only). This model achieves better
accuracy and recall than the CRF, but is less precise.
Accuracy is the same as for the most-frequent-
class baseline, indicating that the vector-based
SVM learns to associate a class with each lexical
item. Once we add the tree kernels to the vector
(SVM+tree), we see considerable gains in accuracy
and precision. This confirms our hypothesis that
metaphors are not only a lexical phenomenon, but
also a product of the context a word is used in. The
contextual interplay with their dependencies creates
patterns that can be exploited with tree kernels.
We note that the SVM with tree kernels is the only
system whose F1 significantly improves over the
baseline (at p < .02).
Testing with one tree representation at a time,
we found the various representations differ in terms
of informativeness. Lemma, POS, and supersense
performed better than lexemes or dependency labels
(when evaluated on the dev set) and were thus used
in the reported system. Combining more than one
representation in the same tree to form compound
leaves (e.g. lemma+POS, such as ?man-NN?)
performed worse in all combinations tested. We
omit further details here, since the combinatorics of
these tests are large and yield only little insight.
Overall, our results are similar to comparable
methods on balanced corpora, and we encourage
the evaluation of other methods on our data set.
4 Related Work
There is plenty of research into metaphors. While
many are mainly interested in their general proper-
ties (Shutova, 2010; Nayak, 2011), we focus on the
ones that evaluate their results empirically.
Gedigian et al (2006) use a similar approach
to identify metaphors, but focus on frames. Their
corpus is with about 900 instances relatively small.
They improve over the majority baseline, but only
report accuracy. Both their result and the baseline
are in the 90s, which might be due to the high
number of metaphors (about 90%). We use a larger,
55
more balanced data set. Since accuracy can be
uninformative in cases of unbalanced data sets, we
also report precision, recall, and F1.
Krishnakumaran and Zhu (2007) also use se-
mantic relations between syntactic dependencies
as basis for their classification. They do not aim to
distinguish literal and metaphorical use, but try to
differentiate various types of metaphors. They use a
corpus of about 1700 sentences containing different
metaphors, and report a precision of 0.70, recall of
0.61 (F1 = 0.65), and accuracy of 0.58.
Birke and Sarkar (2006) and Birke and Sarkar
(2007) present unsupervised and active learning
approaches to classifying metaphorical and literal
expressions, reporting F1 scores of 0.54 and 0.65,
outperforming baseline approaches. Unfortunately,
as they note themselves, their data set is ?not large
enough to [...] support learning using a supervised
learning method? (Birke and Sarkar, 2007, 22),
which prevents a direct comparison.
Similarly to our corpus construction, (Shutova et
al., 2010) use bootstrapping from a small seed set.
They use an unsupervised clustering approach to
identify metaphors and report a precision of 0.79,
beating the baseline system by a wide margin. Due
to the focus on corpus construction, they cannot
provide recall or F1. Their approach considers only
pairs of a single verbs and nouns, while we allow
for any syntactic combination.
Tree kernels have been applied to a wide va-
riety of NLP tasks (Culotta and Sorensen, 2004;
Moschitti et al, 2006; Qian et al, 2008; Hovy et
al., 2012). They are specifically adept in capturing
long-range syntactic relationships. In our case, we
use them to detect anomalies in syntactic relations.
5 Conclusion
Under the hypothesis that the metaphorical use of a
word creates unusual patterns with its dependencies,
we presented the first tree-kernel based approach
to metaphor identification. Syntactic dependencies
allow us to capture those patterns at different
levels of representations and identify metaphorical
use more reliably than non-kernel methods. We
outperform two baselines, a sequential model, and
purely vector-based SVM approaches, and reach an
F1 of 0.75. Our corpus is available for download
at http://www.edvisees.cs.cmu.edu/
metaphordata.tar.gz and we encourage the
research community to evaluate other methods on it.
Acknowledgements
The authors would like to thank the reviewers for
helping us clarify several points and giving con-
structive input that helped to improve the quality of
this paper. This work was (in part) supported by
the Intelligence Advanced Research Projects Activ-
ity (IARPA) via Department of Defense US Army
Research Laboratory contract number W911NF-12-
C-0025. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, DoD/ARL, or the U.S. Govern-
ment.
References
Eric P.S. Baumer, James P. White, and Bill Tomlinson.
2010. Comparing semantic role labeling with typed
dependency parsing in computational metaphor identi-
fication. In Proceedings of the NAACL HLT 2010 Sec-
ond Workshop on Computational Approaches to Lin-
guistic Creativity, pages 14?22. Association for Com-
putational Linguistics.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of EACL, volume 6,
pages 329?336.
Julia Birke and Anoop Sarkar. 2007. Active learning for
the identification of nonliteral language. In Proceed-
ings of the Workshop on Computational Approaches
to Figurative Language, pages 21?28. Association for
Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 423. Association for Com-
putational Linguistics.
56
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural Language
Understanding, pages 41?48.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2009. Kernel methods for minimally su-
pervised wsd. Computational Linguistics, 35(4).
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-
han, and Christopher Welty. 2012. When Did that
Happen? ? Linking Events and Relations to Times-
tamps. In Proceedings of EACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning Whom to trust with
MACE. In Proceedings of NAACL HLT.
Eric Iverson and Stephen Helmreich. 1991. Non-literal
word sense identification through semantic network
path schemata. In Proceedings of the 29th annual
meeting on Association for Computational Linguistics,
pages 343?344. Association for Computational Lin-
guistics.
Saishuresh Krishnakumaran and Xiaojian Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors we
live by, volume 111. University of Chicago Press.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Seyed A. Mirroshandel, Mahdy Khayyamian, and Gho-
lamreza Ghassem-Sani. 2011. Syntactic tree ker-
nels for event-time temporal relation learning. Human
Language Technology. Challenges for Computer Sci-
ence and Linguistics, pages 213?223.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for proposition
re-ranking. MLG 2006, page 165.
Alessandro Moschitti. 2006. Making Tree Kernels Prac-
tical for Natural Language Learning. In In Proceed-
ings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics.
Sushobhan Nayak. 2011. Towards a grounded model
for ontological metaphors. In Student Research Work-
shop, pages 115?120.
Naoaki Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields (CRFs).
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697?704. Association for Computational Lin-
guistics.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics, pages 1002?1010.
Association for Computational Linguistics.
Ekaterina Shutova. 2010. Models of metaphor in nlp. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 688?697.
Association for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, accurate,
non-projective, semantically-enriched parser. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1257?1268. As-
sociation for Computational Linguistics.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empirical
Methods in Natural Language Processing, pages 680?
690.
57
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20?29,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model : Integrating Structure with
Semantics
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel approach
(SDSM) that incorporates structure in dis-
tributional semantics. SDSM represents
meaning as relation specific distributions
over syntactic neighborhoods. We em-
pirically show that the model can effec-
tively represent the semantics of single
words and provides significant advantages
when dealing with phrasal units that in-
volve word composition. In particular, we
demonstrate that our model outperforms
both state-of-the-art window-based word
embeddings as well as simple approaches
for composing distributional semantic rep-
resentations on an artificial task of verb
sense disambiguation and a real-world ap-
plication of judging event coreference.
1 Introduction
With the advent of statistical methods for NLP,
Distributional Semantic Models (DSMs) have
emerged as powerful method for representing
word semantics. In particular, the distributional
vector formalism, which represents meaning by a
distribution over neighboring words, has gained
the most popularity.
DSMs are widely used in information re-
trieval (Manning et al, 2008), question answer-
ing (Tellex et al, 2003), semantic similarity com-
putation (Wong and Raghavan, 1984; McCarthy
and Carroll, 2003), automated dictionary building
(Curran, 2003), automated essay grading (Lan-
dauer and Dutnais, 1997), word-sense discrimina-
tion and disambiguation (McCarthy et al, 2004;
?*Equally contributing authors
Sch?tze, 1998), selectional preference model-
ing (Erk, 2007) and identification of translation
equivalents (Hjelm, 2007).
Systems that use DSMs implicitly make a bag
of words assumption: that the meaning of a phrase
can be reasonably estimated from the meaning of
its constituents. However, semantics in natural
language is a compositional phenomenon, encom-
passing interactions between syntactic structures,
and the meaning of lexical constituents. It fol-
lows that the DSM formalism lends itself poorly
to composition since it implicitly disregards syn-
tactic structure. For instance, the distributions for
?Lincoln?, ?Booth?, and ?killed? when merged
produce the same result regardless of whether the
input is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. As suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately can yield greater expressive power.
Attempts have been made to model linguistic
composition of individual word vectors (Mitchell
and Lapata, 2009), as well as remedy the inher-
ent failings of the standard distributional approach
(Erk and Pad?, 2008). The results show vary-
ing degrees of efficacy, but have largely failed to
model deeper lexical semantics or compositional
expectations of words and word combinations.
In this paper we propose an extension to the
traditional DSM model that explicitly preserves
structural information and permits the approxima-
tion of distributional expectation over dependency
relations. We extend the generic DSM model by
representing a word as distributions over relation-
specific syntactic neighborhoods. One can think
of the Structured DSM (SDSM) representation
of a word/phrase as several vectors defined over
the same vocabulary, each vector representing the
20
word?s selectional preferences for a different syn-
tactic argument. We argue that this represen-
tation captures individual word semantics effec-
tively, and is better able to express the semantics
of composed units.
The overarching theme of our framework of
evaluation is to explore the semantic space of the
SDSM. We do this by measuring its ability to dis-
criminate between varying surface forms of the
same underlying concept. We perform the follow-
ing set of experiments to evaluate its expressive
power, and conclude the following:
1. Experiments with single words on similar-
ity scoring and substitute selection: SDSM
performs at par with window-based distribu-
tional vectors.
2. Experiments with phrasal units on two-word
composition: state-of-the-art results are pro-
duced on the dataset from Mitchell and Lap-
ata (2008) in terms of correlation with human
judgment.
3. Experiments with larger structures on the
task of judging event coreferentiality: SDSM
shows superior performance over state-of-
the-art window-based word embeddings, and
simple models for composing distributional
semantic representations.
2 Related Work
Distributional Semantic Models are based on the
intuition that ?a word is characterized by the com-
pany it keeps? (Firth, 1957). While DSMs have
been very successful on a variety of NLP tasks,
they are generally considered inappropriate for
deeper semantics because they lack the ability to
model composition, modifiers or negation.
Recently, there has been a surge in studies to
model a stronger form of semantics by phrasing
the problem of DSM compositionality as one of
vector composition. These techniques derive the
meaning of the combination of two words a and
b by a single vector c = f(a, b). Mitchell and
Lapata (2008) propose a framework to define the
composition c = f(a, b, r,K) where r is the re-
lation between a and b, and K is the additional
knowledge used to define composition.
While the framework is quite general, most
models in the literature tend to disregard K and
r and are generally restricted to component-wise
addition and multiplication on the vectors to be
composed, with slight variations. Dinu and Lap-
ata (2010) and S?aghdha and Korhonen (2011) in-
troduced a probabilistic model to represent word
meanings by a latent variable model. Subse-
quently, other high-dimensional extensions by
Rudolph and Giesbrecht (2010), Baroni and Zam-
parelli (2010) and Grefenstette et al (2011), re-
gression models by Guevara (2010), and recursive
neural network based solutions by Socher et al
(2012) and Collobert et al (2011) have been pro-
posed.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempted to include syntactic context in distri-
butional models. However, their approaches do
not explicitly construct phrase-level meaning from
words which limits their applicability to real world
problems. A quasi-compositional approach was
also attempted in Thater et al (2010) by a system-
atic combination of first and second order context
vectors. To the best of our knowledge the formu-
lation of composition we propose is the first to ac-
count for K and r within the general framework
of composition c = f(a, b, r,K).
3 Structured Distributional Semantics
In this section, we describe our Structured Distri-
butional Semantic framework in detail. We first
build a large knowledge base from sample english
texts and use it to represent basic lexical units.
Next, we describe a technique to obtain the repre-
sentation for larger units by composing their con-
stituents.
3.1 The PropStore
To build a lexicon of SDSM representations for
a given vocabulary we construct a proposition
knowledge base (the PropStore) by processing the
text of Simple English Wikipedia through a de-
pendency parser. Dependency arcs are stored as
3-tuples of the form ?w1, r, w2?, denoting occur-
rences of words w1 and word w2 related by the
syntactic dependency r. We also store sentence
identifiers for each triple for reasons described
later. In addition to the words? surface-forms, the
PropStore also stores their POS tags, lemmas, and
Wordnet supersenses.
The PropStore can be used to query for pre-
ferred expectations of words, supersenses, re-
lations, etc., around a given word. In the
example in Figure 1, the query (SST(W1)
21
Figure 1: Sample sentences & triples
= verb.consumption, ?, dobj) i.e., ?what is
consumed?, might return expectations [pasta:1,
spaghetti:1, mice:1 . . . ]. In our implementation,
the relations and POS tags are obtained using the
Fanseparser (Tratz and Hovy, 2011), supersense
tags using sst-light (Ciaramita and Altun, 2006),
and lemmas are obtained from Wordnet (Miller,
1995).
3.2 Building the Representation
Next, we describe a method to represent lexical
entries as structured distributional matrices using
the PropStore.
The canonical form of a concept C (word,
phrase etc.) in the SDSM framework is a matrix
MC , whose entry MCij is a list of sentence identi-
fiers obtained by querying the PropStore for con-
texts in which C appears in the syntactic neigh-
borhood of the word j linked by the dependency
relation i. As with other distributional models in
the literature, the content of a cell is the frequency
of co-occurrence of its concept and word under the
given relational constraint.
This canonical matrix form can be interpreted
in several different ways. Each interpretation is
based on a different normalization scheme.
1. Row Norm: Each row of the matrix is inter-
preted as a distribution over words that attach
to the target concept with the given depen-
dency relation.
MCij =
Mij
?jMij
?i
2. Full Norm: The entire matrix is interpreted
as a distribution over the word-relation pairs
which can attach to the target concept.
MCij =
Mij
?i,jMij
?i, j
Figure 2: Mimicking composition of two words
3. Collapsed Vector Norm: The columns of
the matrix are collapsed to form a standard
normalized distributional vector trained on
dependency relations rather than sliding win-
dows.
MCj =
?iMij
?i,jMij
?j
3.3 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix sym-
bolism in a bottom-up fashion. The combina-
tion hinges on the intuition that when lexical units
combine to form a larger syntactically connected
phrase, the representation of the phrase is given
by its own distributional neighborhood within the
embedded parse tree. The distributional neighbor-
hood of the net phrase can be computed using the
PropStore given syntactic relations anchored on its
parts. For the example in Figure 1, we can com-
pose SST(w1) = Noun.person and Lemma(W1)
= eat with relation ?nsubj? to obtain expectations
around ?people eat? yielding [pasta:1, spaghetti:1
. . . ] for the object relation ([dining room:2, restau-
rant:1 . . .] for the location relation, etc.) (See Fig-
ure 2). Larger phrasal queries can be built to an-
swer questions like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
22
us to account for both relation r and knowledgeK
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition of
two words is given in Algorithm 1. Here, we
first determine the sentence indices where the two
words w1 and w2 occur with relation r. Then,
we return the expectations around the two words
within these sentences. Note that the entire algo-
rithm can conveniently be written in the form of
database queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1)
M2 ? queryMatrix(w2)
SentIDs?M1(r) ?M2(r)
return ((M1? SentIDs) ? (M2? SentIDs))
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts. This procedure is de-
scribed in Algorithm 2. Let the E = {e1 . . . en}
be the set of edges in T , ei = (wi1, ri, wi2)?i =
1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
3.4 Tackling Sparsity
The SDSM model reflects syntactic properties of
language through preferential filler constraints.
But by distributing counts over a set of relations
the resultant SDSM representation is compara-
tively much sparser than the DSM representation
for the same word. In this section we present some
ways to address this problem.
3.4.1 Sparse Back-off
The first technique to tackle sparsity is to back
off to progressively more general levels of lin-
guistic granularity when sparse matrix represen-
tations for words or compositional units are en-
countered or when the word or unit is not in the
lexicon. For example, the composition ?Balthazar
eats? cannot be directly computed if the named en-
tity ?Balthazar? does not occur in the PropStore?s
knowledge base. In this case, a query for a su-
persense substitute ? ?Noun.person eat? ? can be
issued instead. When supersenses themselves fail
to provide numerically significant distributions for
words or word combinations, a second back-off
step involves querying for POS tags. With coarser
levels of linguistic representation, the expressive
power of the distributions becomes diluted. But
this is often necessary to handle rare words. Note
that this is an issue with DSMs too.
3.4.2 Densification
In addition to the back-off method, we also pro-
pose a secondary method for ?densifying? distri-
butions. A concept?s distribution is modified by
using words encountered in its syntactic neighbor-
hood to infer counts for other semantically similar
words. In other terms, given the matrix represen-
tation of a concept, densification seeks to popu-
late its null columns (which each represent a word-
dimension in the structured distributional context)
with values weighted by their scaled similarities to
words (or effectively word-dimensions) that actu-
ally occur in the syntactic neighborhood.
For example, suppose the word ?play? had an
?nsubj? preferential vector that contained the fol-
lowing counts: [cat:4 ; Jane:2]. One might then
populate the column for ?dog? in this vector with
a count proportional to its similarity to the word
cat (say 0.8), thus resulting in the vector [cat:4 ;
Jane:2 ; dog:3.2]. These counts could just as well
be probability values or PMI associations (suitably
normalized). In this manner, the k most similar
word-dimensions can be densified for each word
that actually occurs in a syntactic context. As with
sparse back-off, there is an inherent trade-off be-
tween the degree of densification k and the expres-
sive power of the resulting representation.
3.4.3 Dimensionality Reduction
The final method tackles the problem of sparsity
by reducing the representation to a dense low-
dimensional word embedding using singular value
decomposition (SVD). In a typical term-document
matrix, SVD finds a low-dimensional approxima-
tion of the original matrix where columns become
latent concepts while similarity structure between
rows are preserved. The PropStore, as described in
Section 3.1, is an order-3 tensor with w1, w2 and
23
rel as its three axes. We explore the following two
possibilities to perform dimensionality reduction
using SVD.
Word-word matrix SVD. In this experiment,
we preserve the axes w1 and w2 and ignore the re-
lational information. Following the SVD regime (
W = U?V T ) where ? is a square diagonal ma-
trix of k largest singular values, and U and V are
m? k and n? k matrices respectively. We adopt
matrixU as the compacted concept representation.
Tensor SVD. To remedy the relation-agnostic
nature of the word-word SVD matrix represen-
tation, we use tensor SVD (Vasilescu and Ter-
zopoulos, 2002) to preserve the structural infor-
mation. The mode-n vectors of an order-N tensor
A?RI1?I2?...?IN are the In-dimensional vectors
obtained from A by varying index in while keep-
ing other indices fixed. The matrix formed by all
the mode-n vectors is a mode-n flattening of the
tensor. To obtain the compact representations of
concepts we thus first apply mode w1 flattening
and then perform SVD on the resulting tensor.
4 Single Word Evaluation
In this section we describe experiments and re-
sults for judging the expressive power of the struc-
tured distributional representation for individual
words. We use a similarity scoring task and a lexi-
cal substitute selection task for the purpose of this
evaluation. We compare the SDSM representa-
tion to standard window-based distributional vec-
tors trained on the same corpus (Simple English
Wikipedia). We also experiment with different
normalization techniques outlined in Section 3.2,
which effectively lead to structured distributional
representations with distinct interpretations.
We experimented with various similarity met-
rics and found that the normalized cityblock dis-
tance metric provides the most stable results.
CityBlock(X,Y ) =
ArcTan(d(X,Y ))
d(X,Y )
d(X,Y ) =
1
|R|
?
r?R
d(Xr, Yr)
Results in the rest of this section are thus reported
using the normalized cityblock metric. We also
report experimental results for the two methods
of alleviating sparsity discussed in Section 3.4,
namely, densification and SVD.
4.1 Similarity Scoring
On this task, the different semantic representations
were used to compute similarity scores between
two (out of context) words. We used a dataset
from Finkelstein et al (2002) for our experiments.
It consists of 353 pairs of words along with an av-
eraged similarity score on a scale of 1.0 to 10.0
obtained from 13?16 human judges.
4.2 Lexical Substitute Selection
In the second task, the same set of semantic repre-
sentations was used to produce a similarity rank-
ing on the Turney (2002) ESL dataset. This dataset
comprises 50 words that appear in a context (we
discarded the context in this experiment), along
with 4 candidate lexical substitutions. We eval-
uate the semantic representations on the basis of
their ability to discriminate the top-ranked candi-
date.1
4.3 Results and Discussion
Table 1 summarizes the results for the window-
based baseline and each of the structured distri-
butional representations on both tasks. It shows
that our representations for single words are com-
petitive with window based distributional vectors.
Densification in certain conditions improves our
results, but no consistent pattern is discernible.
This can be attributed to the trade-off between the
gain from generalization and the noise introduced
by semantic drift.
Hence we resort to dimensionality reduction as
an additional method of reducing sparsity. Table
2 gives correlation scores on the Finkelstein et al
(2002) dataset when SVD is performed on the rep-
resentations, as described in Section 3.4.3. We
give results when 100 and 500 principal compo-
nents are preserved for both SVD techniques.
These experiments suggest that though afflicted
by sparsity, the proposed structured distributional
paradigm is competitive with window-based dis-
tributional vectors. In the following sections we
show that that the framework provides consid-
erably greater power for modeling composition
when dealing with units consisting of more than
one word.
1While we are aware of the standard lexical substitution
corpus from McCarthy and Navigli (2007) we chose the one
mentioned above for its basic vocabulary, lower dependence
on context, and simpler evaluation framework.
24
Model Finklestein (Corr.) ESL (% Acc.)
DSM 0.283 0.247
Collapsed 0.260 0.178
FullNorm 0.282 0.192
RowNorm 0.236 0.264
Densified RowNorm 0.259 0.267
Table 1: Single Word Evaluation
Model Correlation
matSVD100 0.207
matSVD500 0.221
tenSVD100 0.267
tenSVD500 0.315
Table 2: Finklestein: Correlation using SVD
5 Verb Sense Disambiguation using
Composition
In this section, we examine how well our model
performs composition on a pair of words. We
derive the compositional semantic representations
for word pairs from the M&L dataset (Mitchell
and Lapata, 2008) and compare our performance
with M&L?s additive and multiplicative models of
composition.
5.1 Dataset
The M&L dataset consists of polysemous intransi-
tive verb and subject pairs that co-occur at least 50
times in the BNC corpus. Additionally two land-
mark words are given for every polysemous verb,
each corresponding to one of its senses. The sub-
ject nouns provide contextual disambiguation for
the senses of the verb. For each [subject, verb,
landmark] tuple, a human assigned score on a 7-
point scale is provided, indicating the compatibil-
ity of the landmark with the reference verb-subj
pair. For example, for the pair ?gun bomb?, land-
mark ?thunder? is more similar to the verb than
landmark ?prosper?. The corpus contains 120 tu-
ples and altogether 3600 human judgments. Re-
liability of the human ratings is examined by cal-
culating inter-annotator Spearman?s ? correlation
coefficient.
5.2 Experiment procedure
For each tuple in the dataset, we derive the com-
posed word-pair matrix for the reference verb-subj
pair based on the algorithm described in Section
3.3 and query the single-word matrix for the land-
mark word. A few modifications are made to ad-
just the algorithm for the current task:
1. In our formulation, the dependency relation
needs to be specified in order to compose
a pair of words. Hence, we determine the
five most frequent relations between w1 and
w2 by querying the PropStore. We then use
the algorithm in Section 3.3 to compose the
verb-subj word pair using these relations, re-
sulting in five composed representations.
2. The word pairs in M&L corpus are ex-
tracted from a parsed version of the BNC cor-
pus, while our PropStore is built on Simple
Wikipedia texts, whose vocabulary is signif-
icantly different from that of the BNC cor-
pus. This causes null returns in our PropStore
queries, in which case we back-off to retriev-
ing results for super-sense tags of both the
words. Finally, the composed matrix and the
landmark matrix are compared against each
other by different matrix distance measures,
which results in a similarity score. For a [sub-
ject, verb, landmark] tuple, we average the
similarity scores yielded by the relations ob-
tained in 1.
The Spearman Correlation ? between our sim-
ilarity ratings and the ones assigned by human
judges is computed over all the tuples. Follow-
ing M&L?s experiments, the inter-annotator agree-
ment correlation coefficient serves an upper bound
on the task.
5.3 Results and Discussion
As in Section 4, we choose the cityblock mea-
sure as the similarity metric of choice. Table 3
shows the evaluation results for two word compo-
sition. Except for row normalization, both forms
of normalization in the structured distributional
paradigm show significant improvement over the
results reported by M&L. The results are statisti-
cally significant at p-value = 0.004 and 0.001 for
Full Norm and Collapsed Vector Norm, respec-
tively.
Model ?
M&L combined 0.19
Row Norm 0.134
Full Norm 0.289
Collapsed Vector Norm 0.259
UpperBound 0.40
Table 3: Two Word Composition Evaluation
These results validate our hypothesis that the in-
tegration of structure into distributional semantics
25
as well as our framing of word composition to-
gether outperform window-based representations
under simplistic models of composition such as
addition and multiplication. This finding is further
re-enforced in the following experiments on event
coreferentiality judgment.
6 Event Coreference Judgment
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ.
While automated resolution of entity coref-
erence has been an actively researched area
(Haghighi and Klein, 2009; Stoyanov et al, 2009;
Raghunathan et al, 2010), there has been rela-
tively little work on event coreference resolution.
Lee et al (2012) perform joint cross-document
entity and event coreference resolution using the
two-way feedback between events and their argu-
ments.
In this paper, however, we only consider coref-
erentiality between pairs of events. Formally,
two event mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
While linguistic theory of argument realiza-
tion is a debated research area (Levin and Rap-
paport Hovav, 2005; Goldberg, 2005), it is com-
monly believed that event structure (Moens and
Steedman, 1988) centralizes on the predicate,
which governs and selects its role arguments
(Jackendoff, 1987). In the corpora we use for
our experiments, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for the
role. The triple (e, a, p) is thus the composition
of the triples (a, relagent, e) and (p, relpatient, e),
and hence a complex object. To determine equal-
ity of this complex composed representation we
generate three levels of progressively simplified
event constituents for comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p).
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance metric
(Euclidean, Cityblock, Cosine), and score nor-
malization techniques (Row-wise, Full, Column
collapsed). This results in 159 similarity-based
features for every pair of events, which are used
to train a classifier to make a binary decision for
coreferentiality.
6.1 Datasets
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
IC Event Coreference Corpus: The dataset
(citation suppressed), drawn from 100 news arti-
cles about violent events, contains manually cre-
ated annotations for 2214 pairs of co-referent
and non-coreferent events each. Where available,
events? semantic role-fillers for agent and patient
are annotated as well. When missing, empirical
substitutes were obtained by querying the Prop-
Store for the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
26
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 4: Cross-validation Performance on IC and ECB dataset
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
6.2 Baselines:
To establish the efficacy of our model, we com-
pare SDSM against a purely window-based base-
line (DSM) trained on the same corpus. In our ex-
periments we set a window size of three words to
either side of the target. We also compare SDSM
against the window-based embeddings trained us-
ing a recursive neural network (SENNA) (Col-
lobert et al, 2011) on both datsets. SENNA em-
beddings are state-of-the-art for many NLP tasks.
The second baseline uses SENNA to generate
level 3 similarity features for events? individual
words (agent, patient and action). As our final
set of baselines, we extend two simple techniques
proposed by Mitchell and Lapata (2008) that use
element-wise addition and multiplication opera-
tors to perform composition. The two baselines
thus obtained are AVC (element-wise addition)
and MVC (element-wise multiplication).
6.3 Results and Discussion:
We experimented with a number of common clas-
sifiers, and selected decision-trees (J48) as they
give the best classification accuracy. Table 4 sum-
marizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of composition than simple
additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
determining event coreferentiality. The forward
selection procedure reveals that the most informa-
tive attributes are the level 2 compositional fea-
tures involving the agent and the action, as well as
their individual level 3 features. This corresponds
to the intuition that the agent and the action are the
principal determiners for identifying events. Fea-
tures involving the patient and level 1 features are
least useful. The latter involves full composition,
resulting in sparse representations and hence have
low predictive power.
7 Conclusion and Future Work
In this paper we outlined an approach that intro-
duces structure into distributional semantics. We
presented a method to compose distributional rep-
resentations of individual units into larger com-
posed structures. We tested the efficacy of our
model on several evaluation tasks. Our model?s
performance is competitive for tasks dealing with
semantic similarity of individual words, even
though it suffers from the problem of sparsity.
Additionally, it outperforms window-based ap-
proaches on tasks involving semantic composi-
tion. In future work we hope to extend this for-
malism to other semantic tasks like paraphrase de-
tection and recognizing textual entailment.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
27
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
James Richard Curran. 2003. From distributional to
semantic similarity. Technical report.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. In ACM Transactions on Information
Systems, volume 20, pages 116?131, January.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Adele E. Goldberg. 2005. Argument Realization: Cog-
nitive Grouping and Theoretical Extensions.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of NODALIDA, pages 97?104. Citeseer.
Ray Jackendoff. 1987. The status of thematic roles in
linguistic theory. Linguistic Inquiry, 18(3):369?411.
Thomas K Landauer and Susan T. Dutnais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Cambridge University Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task.
In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, pages 48?53.
28
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
of ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1 - Volume
1, EMNLP ?09, pages 430?439, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Peter D. Turney. 2002. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In In Proceedings of the European Confer-
ence on Computer Vision, pages 447?460.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
29
