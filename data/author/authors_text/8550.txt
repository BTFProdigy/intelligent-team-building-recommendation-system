c? 2003 Association for Computational Linguistics
Graph-Based Generation of Referring
Expressions
Emiel Krahmer? Sebastiaan van Erk?
Tilburg University Eindhoven University of Technology
Andre? Verleg?
Eindhoven University of Technology
This article describes a new approach to the generation of referring expressions. We propose to
formalize a scene (consisting of a set of objects with various properties and relations) as a labeled
directed graph and describe content selection (which properties to include in a referring expression)
as a subgraph construction problem. Cost functions are used to guide the search process and to
give preference to some solutions over others. The current approach has four main advantages:
(1) Graph structures have been studied extensively, and by moving to a graph perspective we
get direct access to the many theories and algorithms for dealing with graphs; (2) many existing
generation algorithms can be reformulated in terms of graphs, and this enhances comparison and
integration of the various approaches; (3) the graph perspective allows us to solve a number of
problems that have plagued earlier algorithms for the generation of referring expressions; and
(4) the combined use of graphs and cost functions paves the way for an integration of rule-based
generation techniques with more recent stochastic approaches.
1. Introduction
The generation of referring expressions is one of the most common tasks in natural
language generation and has been addressed by many researchers in the past two
decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992),
Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune
(1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present
a general, graph-theoretic approach to the generation of referring expressions. We pro-
pose to formalize a scene (i.e., a domain of objects and their properties and relations)
as a labeled directed graph and describe the content selection problem (which proper-
ties and relations to include in a description for an object?) as a subgraph construction
problem. The graph perspective has four main advantages. (1) There are many at-
tractive and well-understood algorithms for dealing with graph structures (see, e.g.,
Gibbons [1985], Cormen, Leiserson, and Rivest [1990], or Chartrand and Oellermann
[1993]). In this article, we describe a straightforward branch and bound algorithm for
finding the relevant subgraphs in which cost functions are used to guide the search
process. (2) By defining different cost functions for the graph perspective, we can simu-
late (and improve) some of the well-known algorithms for the generation of referring
? Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University, Tilburg,
The Netherlands. E-mail: E.J.Krahmer@uvt.nl.
? Tijgerstraat 2, NL-5645 CK, Eindhoven, The Netherlands. E-mail: sebster@sebster.com.
? Ranonkelstraat 67, NL-5644 LB, Eindhoven, The Netherlands. E-mail: andre@astygian.nl.
54
Computational Linguistics Volume 29, Number 1
expressions mentioned above. This facilitates the formal comparison of these algo-
rithms and makes it easier to transfer results from one algorithm to another. (3) The
graph perspective provides a clean solution for some problems that have plagued ear-
lier algorithms. For instance, the generation of relational expressions (i.e., referring
expressions that include references to other objects) is enhanced by the fact that both
properties and relations are formalized in the same way, namely, as edges in a graph.
(4) The combined use of graphs and cost functions paves the way for a natural inte-
gration of traditional rule-based approaches to generating referring expressions and
more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf
(2000), in a single algorithm.
The outline of this article is as follows. In Section 2 the content selection problem
for generating referring expressions is explained, and some well-known solutions to
the problem are discussed. In Section 3, we describe how scenes can be modeled
as labeled directed graphs and show how content selection can be formalized as a
subgraph construction problem. Section 4 contains a sketch of the basic generation
algorithm, which is illustrated with a worked example. In Section 5 various ways to
formalize cost functions are discussed and compared. We end with some concluding
remarks and a discussion of future research directions in Section 6.
2. Generating Referring Expressions
There are many different algorithms for the generation of referring expressions, each
with its own objectives: Some aim at producing the shortest possible description (e.g.,
Dale?s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale
and Reiter?s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The
degree of detail in which the various algorithms are described differs considerably.
Some algorithms are fully formalized and come with explicit characterizations of their
complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual
and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite
such differences, most algorithms deal with the same problem definition. They take
as input a single object v (the target object) for which a referring expression is to be
generated and a set of objects (the distractors) from which the target object needs to
be distinguished (we use the terminology from Dale and Reiter [1995]). The task of
the algorithm is to determine which set of properties is needed to single out the target
object v from the distractors. This is known as the content determination problem for
referring expressions. On the basis of this set of properties a distinguishing descrip-
tion for v can be generated. Most algorithms do not address the surface realization
problem (how the selected properties should be realized in natural language) in much
detail; it is usually assumed that once the content for a referring expression has been
determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and
Robin 1997) can convert the meaning representation to natural language.
Consider the example scene in Figure 1. In this scene, as in any other scene, we
see a finite domain of entities D with properties P. In this particular scene, D =
{d1, d2, d3, d4} is the set of entities and P = { dog, cat, brown, black+white, large, small }
is the set of properties. A scene is usually represented as a database (or knowledge
base) listing the properties of each element in D. Thus:
d1: dog (d1) small (d1) brown (d1)
d2: dog (d2) large (d2) brown (d2)
d3: dog (d3) large (d3) black+white (d3)
d4: cat (d4) small (d4) brown (d4)
55
Krahmer, van Erk, and Verleg Graph-Based Generation
d1 d2 d3 d4
Figure 1
A simple example scene consisting of some domestic animals.
In what is probably the key reference on the topic, Dale and Reiter (1995) describe
and discuss a number of algorithms for the generation of referring expressions. One
of these is the full brevity algorithm (originally due to Dale 1992). This algorithm first
tries to generate a distinguishing description for the target object v using one single
property. If this fails, it considers all possible combinations of two properties to see if
any of these suffices for the generation of a distinguishing description, and so on. It
is readily seen that this algorithm will output the shortest possible description, if one
exists. Suppose the full brevity algorithm is used to generate a description for d1 in
Figure 1. There is no single property that distinguishes the target object d1 from the
distractors {d2, d3, d4}. But when considering all pairs of properties the algorithm will
find that one such pair rules out all distractors, namely, small and dog; ?the small dog?
is a successful and minimal distinguishing description for d1.
Dale and Reiter point out that the full brevity algorithm is both computationally
infeasible (NP hard) and psychologically unrealistic. They offer the incremental algo-
rithm as an alternative. The incremental algorithm considers properties for selection
in a predetermined order, based on the idea that human speakers and listeners prefer
certain kinds of properties (or attributes) when describing objects from a given do-
main. For instance, when discussing domestic animals, it seems likely that a human
speaker would first describe an animal by its type (is it a dog? is it a cat?). If that
does not suffice, first absolute attributes like color are tried, followed by relative ones
such as size. In sum: The list of preferred attributes for our example domain would
be ? type, color, size ?. Essentially, the incremental algorithm iterates through this list,
and for each property it encounters, it determines whether adding this property to
the properties selected so far would rule out any of the remaining distractors. If so,
it is included in the list of selected properties. There is one exception to this general
strategy: Type information is always included, even if it rules out no distractors. The
algorithm stops when all distractors are ruled out (success) or when the end of the
list of preferred attributes is reached (failure).
Suppose we apply the incremental algorithm to d1 from Figure 1 with ? type, color,
size ? as preferred attributes. The type of d1 listed in the database is dog. This property
is selected (since type information is always selected). It rules out d4 (which is a cat).
Next we consider the color of d1; the animal is brown. This property rules out d3 (which
is a black and white dog) and is selected. Finally, we consider the size of our target ob-
ject, which is small. This properly rules out the remaining distractor d2 (which is a large
brown dog) and hence is included as well. At this point, all distractors are ruled out
(success!), and the set of selected properties is {dog, brown, small}, which a linguistic
realizer might express as ?the small brown dog.? This is a successful distinguishing
56
Computational Linguistics Volume 29, Number 1

d
1
d
3
Z
Z
Z
A Meta-Algorithm for the Generation of Referring Expressions
Emiel Krahmer, Sebastiaan van Erk and Andre? Verleg
TU/e, Eindhoven University of Technology,
The Netherlands
email: E.J.Krahmer@tue.nl
Abstract
This paper describes a new approach
to the generation of referring expres-
sions. We propose to formalize a scene
as a labeled directed graph and describe
content selection as a subgraph con-
struction problem. Cost functions are
used to guide the search process and
to give preference to some solutions
over others. The resulting graph al-
gorithm can be seen as a meta-algorithm
in the sense that defining cost functions
in different ways allows us to mimic ?
and even improve? a number of well-
known algorithms.
1 Introduction
The generation of referring expressions is one
of the most common tasks in natural language
generation, and has been addressed by many re-
searchers in the past two decades (including Ap-
pelt 1985, Dale 1992, Reiter 1990, Dale & Had-
dock 1991, Dale & Reiter 1995, Horacek 1997,
Stone & Webber 1998, Krahmer & Theune 1999
and van Deemter 2000). As a result, there are
many different algorithms for the generation of
referring expressions, each with its own object-
ives: some aim at producing the shortest possible
description, others focus on efficiency or realistic
output. The degree of detail in which the various
algorithms are described differs considerably, and
as a result it is often difficult to compare the vari-
ous proposals. In addition, most of the algorithms
are primarily concerned with the generation of de-
scriptions only using properties of the target ob-
ject. Consequently, the problem of generating re-
lational descriptions (i.e., descriptions which in-
corporate references to other objects to single out
the target object) has not received the attention it
deserves.
In this paper, we describe a general, graph-
theoretic approach to the generation of referring
expressions. We propose to formalize a scene
(i.e., a domain of objects and their properties and
relations) as a labeled directed graph and describe
the content selection problem ?which proper-
ties and relations to include in a description for
an object?? as a subgraph construction problem.
The graph perspective has three main advantages.
The first one is that there are many attractive al-
gorithms for dealing with graph structures. In
this paper, we describe a branch and bound al-
gorithm for finding the relevant subgraphs, where
we use cost functions to guide the search pro-
cess. Arguably, the proposed algorithm is a meta-
algorithm, in the sense that by defining the cost
function in different ways, we can mimic various
well-known algorithms for the generation of re-
ferring expressions. A second advantage of the
graph-theoretical framework is that it does not run
into problems with relational descriptions, due to
the fact that properties and relations are formal-
ized in the same way, namely as edges in a graph.
The third advantage is that the combined usage
of graphs and cost-functions paves the way for
a natural integration of traditional rule-based ap-
proaches to generation with more recent statist-
ical approaches (e.g., Langkilde & Knight 1998,
Malouf 2000) in a single algorithm.
The outline of this paper is as follows. In sec-
tion 2, we describe how scenes can be described as
labeled directed graphs and show how content se-
lection can be formalized as a subgraph construc-
tion problem. Section 3 contains a sketch of the
branch and bound algorithm, which is illustrated
with a worked example. In section 4 it is argued
that by defining cost functions in different ways,
we can mimic various well-known algorithms for
the generation of referring expressions. We end
with some concluding remarks in section 5.
2 Graphs
Consider the following scene:
 










 

	










Figure 1: An example scene
In this scene, as in any other scene, we see a
finite set of entities  with properties  and
relations  . In this particular scene, the set
ff