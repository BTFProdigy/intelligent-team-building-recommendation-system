Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 73?81,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Automatic Detection of Stable Grammatical Features in N-Grams
Mikhail Kopotev1 Lidia Pivovarova1,2 Natalia Kochetkova3 Roman Yangarber1
1 University of Helsinki, Finland
2 St.Petersburg State University, Russia
3 Moscow Institute of Electronics and Mathematics, NRU HSE, Russia
Abstract
This paper presents an algorithm that allows
the user to issue a query pattern, collects
multi-word expressions (MWEs) that match
the pattern, and then ranks them in a uniform
fashion. This is achieved by quantifying the
strength of all possible relations between the
tokens and their features in the MWEs. The al-
gorithm collects the frequency of morphologi-
cal categories of the given pattern on a unified
scale in order to choose the stable categories
and their values. For every part of speech, and
for all of its categories, we calculate a normal-
ized Kullback-Leibler divergence between the
category?s distribution in the pattern and its
distribution in the corpus overall. Categories
with the largest divergence are considered to
be the most significant. The particular values
of the categories are sorted according to a fre-
quency ratio. As a result, we obtain morpho-
syntactic profiles of a given pattern, which in-
cludes the most stable category of the pattern,
and their values.
1 Introduction
In n-grams, the relations among words and among
their grammatical categories cover a wide spectrum,
ranging from idioms to syntactic units, such as a
verb phrase. In most cases, the words are linked to-
gether by both grammatical and lexical relations. It
is difficult to decide, which relation is stronger in
each particular case. For example, in the idiomatic
phrase meet the eye, the relationship is lexical rather
than grammatical. A phrasal verb meet up is similar
to single-word verbs and has its own meaning. It can
be interpreted as one lexeme, spelled as two words.
On the other hand, phrases like meet the require-
ments, meet the specifications, meet the demands
are traditionally called ?collocations.? However, the
question arises about the role played by the noun fol-
lowing the verb: is it a lexically free direct object,
or a part of stable lexical unit, or to some extend
both? These words are bound by both grammatical
and lexical relations, and we assume that the major-
ity of word combinations in any language have such
a dual nature.
Lastly, the relationship between the words in the
English phrase meet her differs from those above in
that it may be described as purely grammatical?the
verb meet receives a direct object.
Distinguishing collocations, i.e. ?co-occurrences
of words? from colligations, i.e. ?co-occurrence of
word forms with grammatical phenomena? (Gries
and Divjak, 2009) is not always a simple task; there
is no clear boundary between various types of word
combinations inasmuch as they can be simultane-
ously a collocation and a colligation?this type of
MWE is called collostructions in (Stefanowitsch and
Gries, 2003). It was proposed that language as such
is a ?constructicon? (Goldberg, 2006), which means
that fusion is its core nature. For this reason, devis-
ing formal methods to measure the strength of mor-
phological or lexical relations between words be-
comes a challenge.
Our approach aims to treat multi-word expres-
sions (MWEs) of various nature?idioms, multi-
word lexemes, collocations and colligations?on an
equal basis, and to compare the strength of vari-
ous possible relations between the tokens in a MWE
quantitatively. We search for ?the underlying cause?
73
for the frequent co-occurrence of certain words:
whether it is due to their morphological categories,
or lexical compatibility, or a combination of both. In
this paper, however, we focus on colligations, ignor-
ing collocations and collostructions.
For languages with rich morphology the situation
is more complicated, because each word may have
several morphological categories that are not inde-
pendent and interact with each other. This paper fo-
cuses on Russian, which not only has free word or-
der and rich morphology,1 but is also a language that
is well-investigated. A good number of corpora and
reference grammars are available to be used for eval-
uation. The data we use in this work is the n-gram
corpus, extracted from a deeply annotated and care-
fully disambiguated (partly manually) sub-corpus of
the Russian National Corpus (RNC). The size of dis-
ambiguated corpus used in this paper is 5 944 188
words of running text.
2 Related Work
Much effort has been invested in automatic extrac-
tion of MWEs from text. A great variety of method
are used, depending on the data, the particular tasks
and the types of MWEs to be extracted. Pecina
(2005) surveys 87 statistical measures and meth-
ods, and even that is not a complete list. The
most frequently used metrics, inter alia, are Mu-
tual Information (MI), (Church and Hanks, 1990), t-
score (Church et al, 1991), and log-likelihood (Dun-
ning, 1993). The common disadvantage of these is
their dependency on the number of words included
in the MWE. Although there is a large number of
papers that use MI for bigram extraction, only a few
use the MI measure for three or more collocates,
e.g., (Tadic? and S?ojat, 2003; Wermter and Hahn,
2006; Kilgarriff et al, 2012),
Frantzi et al (2000) introduced the c-value and
nc-value measures to extract terms of different
lengths. Daudaravicius (2010) has developed a
promising method that recognizes collocations in
text. Rather than extracting MWEs, this method cuts
the text into a sequence of MWEs of length from
1 to 7 words; the algorithm may produce different
1The Multitext-East specification, which aims to create an
unified cross-language annotation scheme, defines 156 morpho-
syntactic tags for Russian as compared to 80 tags for English
(http://nl.ijs.si/ME/V4/msd/html).
chunking for the same segment of text within dif-
ferent corpora. Nevertheless, extraction of variable-
length MWE is a challenging task; the majority of
papers in the field still use measures that take the
number of collocates as a core parameter.
Entropy and other probabilistic measures have
been used for MWE extraction since the earliest
work. For example, the main idea in (Shimohata et
al., 1997; Resnik, 1997), is that the MWE?s idiosyn-
crasy, (Sag et al, 2002), is reflected in the distribu-
tions of the collocates. Ramisch et al (2008) intro-
duced the Entropy of Permutation and Insertion:
EPI = ?
m?
a=0
p(ngrama) log[p(ngrama)] (1)
where ngram0 is the original MWE, and ngrama
are its syntactically acceptable permutations.
Kullback-Leibler divergence was proposed
by Resnik (1997) to measure selective prefer-
ence for the word sense disambiguation (WSD)
task. Fazly and Stevenson (2007) applied a set of
statistical measures to classify verb+noun MWEs
and used Kullback-Leibler divergence, among other
methods, to measure the syntactic cohesion of a
word combination. Van de Cruys and Moiro?n
(2007) used normalized Kullback-Leibler diver-
gence to find idiomatic expression with verbs in
Dutch.
Russian MWE-studies have emerged over the last
decade. Khokhlova and Zakharov (2009) applied
MI, t-score and log-likelihood to extract verb collo-
cations; Yagunova and Pivovarova (2010) studied
the difference between Russian lemma/token col-
locations and also between various genres; Do-
brov and Loukachevitch (2011) implemented term
extraction algorithms. However, there is a lack of
study of both colligations and collostructions in Rus-
sian. The only work known to us is by Sharoff
(2004), who applied the MI-score to extract prepo-
sitional phrases; however, the only category he used
was the POS.
As far as we aware, the algorithm we present in
this paper has not been applied to Russian or to other
languages.
3 Method
The input for our system is any n-gram of length 2?
4, where one position is a gap?the algorithm aims
74
Figure 1: Distributions of noun cases in the corpus and in
a sample?following the preposition ??? (in)
Figure 2: Distributions of nominal gender in the corpus
and in a sample?following the preposition ??? (in)
to find the most stable morphological categories of
words that can fill this gap. Moreover, the user can
specify the particular properties of words that can fill
the gap?for example, specify that the output should
include only plural nouns. Thus, the combination of
the surrounding words and morphological constrains
form an initial query pattern for the algorithm.
Our model tries to capture the difference between
distributions of linguistic features in the general cor-
pus as compared to distributions within the given
pattern. For example, Figure 1 shows the distribu-
tion of cases in the corpus overall vs. their distribu-
tion in words following the preposition ??? (in/into).
Figure 2 shows the corresponding distributions of
gender. Gender is distributed similarly in the cor-
pus and in the sample restricted by the pattern; by
contrast, the distribution of cases is clearly different.
This is due to the fact that the preposition governs
the case of the noun, but has no effect on gender. To
measure this difference between the distributions we
use the Kullback-Leibler divergence:
Div(C) =
N?
i=1
P patterni ? log(
P patterni
P corpusi
) (2)
where C is the morphological category in a
pattern?e.g., case or gender,?having the values
1..N , P patterni is the relative frequency of value i
restricted by the pattern, and P corpusi is the relative
frequency of the same value in the general corpus.
Since the number of possible values for a category is
variable?e.g., eleven for case, four for gender, and
hundreds of thousands for lemmas?the divergence
needs to be normalized. The normalization could
be done in various ways, e.g., against the entropy or
some maximal divergence in the data; in our experi-
ments, the best results were obtained using a variant
proposed in (Bigi, 2003), where the divergence be-
tween the corpus distribution and the uniform distri-
bution is used as the normalizing factor:
NormDiv(C) =
Div(C)
E(C) + log(n)
(3)
where E(C) is the entropy of category C and n is
the number of possible values of C; the term log(n)
is the entropy of the uniform distribution over n out-
comes (which is the maximal entropy). The category
with the highest value of normalized divergence is
seen as maximally preferred by the pattern.
However, divergence is unable to determine the
exact values of the category, and some of these val-
ues are clearly unreliable even if they seem to ap-
pear in the pattern. For example, Figure 1 shows
that preposition ??? (in) in the data is sometimes
followed by the nominative case, which is grammat-
ically impossible. This is due to a certain amount of
noise, which is unavoidable in a large corpus due to
mark-up errors or inherent morphological ambigu-
ity. In Russian, the nominative and accusative cases
often syncretize (assume identical forms), which can
cause inaccuracies in annotation. On the other hand,
some values of a category can be extremely rare;
thus, they will be rare within patterns as well. For
instance, the so-called ?second accusative? case (la-
beled ?acc2? in Figure 1) is rare in modern Russian,
75
which is why its appearance in combination with
preposition ??? (in) is significant, even though its
frequency is not much higher than the frequency of
the (erroneous) nominative case in the same pattern.
To find the significant values of a particular cate-
gory we use the ratio between the frequencies of the
value in a sample and in the corpus:
frequency ratio =
P patterni
P corpusi
(4)
If frequentcy ratio > 1, then the category?s value
is assumed to be selected by the pattern.
Finally, we note that the distribution of POS varies
considerably within every pattern as compared to its
distribution in the corpus. For example, prepositions
can be followed only by noun groups and can never
be followed by verbs or conjunctions. This means
the Kullback-Leibler divergence for any POS, nat-
urally assumes the highest value in any pattern; for
this reason, we exclude the POS category from con-
sideration in our calculation, aiming to find more
subtle and interesting regularities in the data.
To summarize, the algorithm works as follows:
for a given query pattern
1. search all words that appear in the query pattern
and group them according to their POS tags.
2. for every POS, calculate the normalized
Kullback-Leibler divergence for all of its cat-
egories; categories that show the maximum di-
vergence are considered to be the most signifi-
cant for the given pattern;
3. for every relevant category, sort its values ac-
cording to the frequency ratio; if frequency ra-
tio is less than 1, the value considered to be ir-
relevant for this pattern.
4 Experiments
In this paper, we conduct an in-depth evaluation fo-
cusing on a limited number of linguistic phenom-
ena, namely: bigrams beginning with single-token
prepositions, which impose strong morpho-syntactic
constraints in terms of case government. We in-
vestigate 25 prepositions, such as ????? (without),
??? (in/to), etc. We evaluate the corpus of bi-
grams systematically against these queries, although
we expect that the model we propose here pro-
duces relevant results for a much wider range of
constructions?to be confirmed in further work.
4.1 Prepositions and Morphological Category
A syntactic property of prepositions in Russian is
that they govern nominal phrases, i.e., that we expect
the largest normalized divergence in queries such as
{ Preposition + X }, where the POS of X is noun,
to occur exactly with the category of case. Figure 3
shows the normalized divergence for four lexical and
morphological categories. Among them, Case has
the maximal divergence for all prepositions, which
matches our expectation with 100% accuracy.
According to the figure, the morphological cat-
egory of Animacy2 is also interesting, in that it
has a high value for some prepositions, like ???-
???? (from under), ????? (under), ????? (above).
A good example is the preposition ???-???? (from
under). Its semantic properties cause inanimate
nouns to appear much more frequently than ani-
mate ones. Consequently, we observe a higher diver-
gence, due to inanimate nouns like ???-??? ??????
(from under ground), ???-??? ?????? (from under
the snow), etc. Another good example of hidden
semantic properties is a pair of prepositions ?????
(under) and ????? (above). One can expect that
their syntactic behaviour is more or less similar,
but the histogram shows that Animacy (surprisingly)
has a much higher divergence for ????? (under) to
be ignored. Indeed, a deeper corpus-based anal-
ysis reveals a stable, frequently used construction,
which gives many points to animate nouns, e.g.,
???????????????? ??? ???????? (disguised as a
bride). It is notable that this particular effect is not
mentioned in any grammar book, (to the best of our
knowledge).
To conclude, the Case category is the clear win-
ner in terms of having the greatest normalized di-
vergence, and the output fully matches the expecta-
tion on all 25 common prepositions that we tested.
Other results are also clearly interesting due to their
links to semantic properties, that is, to colloca-
tions. The next task is, therefore to discriminate
2Animacy is a morphological category of Russian nouns
based on whether the referent of the noun is considered sen-
tient or living. Most nouns denoting humans and animals are
animate, while the majority of other nouns are inanimate.
76
Figure 3: Normalized divergence of noun categories (grammemes) for pattern preposition+X.
between the runners-up, like Animacy for ?????
(under), which seem to be interesting to some ex-
tent, and clear losers like Gender, in the example
above. To do that we need to find an appropriate
threshold?preferably automatically?between rel-
evant and non-relevant results. The algorithm ranks
the categories according to their divergence; the cat-
egory that has the top rank is certainly meaning-
ful. The question is how to determine which among
the rest are significant as well; this is left for future
work.
4.2 Specific Values of the Category with
Maximum Divergence
The next question we explore is which particular
values of the maximally divergent category?here,
Case?are selected by a given preposition. As we
mentioned above, we use the frequency ratio for this
task. We collected a list of cases3 that appear af-
ter the given preposition, according to the algorithm
with frequency ratio > 1; which cases are pos-
sible according to grammatical descriptions,4 which
3The current annotation scheme of our data has eleven case
tags, namely: nom, voc, gen, gen2, dat, acc, acc2, ins, loc, loc2,
adnum.
4Note, that not all possible prep+case combinations are rep-
resented in the corpus; for example, the combination { ??????
(for the sake of) + gen2 } does not appear in our data, and only
eight times in the RNC overall. For evaluation we take into
cases were produced by the algorithm, and the num-
ber of correct cases in the system?s response. We
expect that by using the frequency ratio we can re-
duce the noise; for example, of the eight cases that
match the pattern { ?c? (with) + Noun } only four
are relevant.
The algorithm predicts the correct relevant set for
21 of 25 prepositions, giving a total precision of
95%, recall of 89%, and F-measure of 92%. The
prepositions highlighted in bold in Table 1 are those
that were incorrectly processed for various reasons;
the error analysis is presented below.
14: ??? (about) The algorithm unexpectedly flags
the voc (vocative) as a possible case after this prepo-
sition. This is incorrect; checking the data we dis-
covered that this mistake was due to erroneous an-
notation: the interjection ?o? (oh), as in ?O ????!?
(Oh God!), is incorrectly annotated as the preposi-
tion ?o? (about). The error occurs twice in the data.
However, as the vocative is extremely rare in the data
(its frequency in the corpus is less than 0,0004), two
erroneous tags are sufficient to give it a high rank.
Similar annotation errors for more frequent cases are
eliminated by the algorithm. For example, as we
mentioned in the previous section, the nominative
consideration only those prep+case combinations that appear at
least once in our dataset.
77
Preposition Meaning Expected cases Response
1 ??? without gen/gen2 gen/gen2
2 ? in/into acc/acc2/loc/loc2 acc/acc2/loc/loc2
3 ??? for gen/gen2 gen/gen2
4 ?? until gen/gen2 gen/gen2
5 ?? behind acc/ins acc/ins
6 ?? from gen/gen2 gen/gen2
7 ??-?? from behind gen/gen2 gen/gen2
8 ??-??? from under gen/gen2 gen/gen2
9 ? to dat dat
10 ????? beyond gen gen
11 ????? between ins ins
12 ?? on acc/loc/loc2 acc/loc/loc2
13 ??? above ins ins
14 ? about acc/loc loc/voc
15 ?? from gen/gen2 gen/gen2
16 ????? in front of ins ins
17 ???? in front of ins ins
18 ?? by/up to dat/loc/acc dat
19 ??? under acc/ins acc/ins
20 ??? at/by loc loc
21 ??? about acc acc
22 ???? for gen gen
23 ? with gen/gen2/acc/ins gen2/ins
24 ? near gen gen
25 ????? through acc acc/adnum
Expected 45
Response 42
Correct 40
Precision 0.95
Recall 0.89
F-measure 0.92
Table 1: Noun cases expected and returned by the algorithm for Russian prepositions.
case after preposition ??? (in) appears 88 times in
our data; however this case is not returned by the al-
gorithm, since it is below the frequency ratio thresh-
old.
25: ??????? (through/past) The adnumerative
(adnum) is a rare case in our data, so even a single
occurrence in a sample is considered important by
the algorithm. A single bigram is found in the data,
where the token ?????? (hours)?correctly anno-
tated with the adnum tag?predictably depends on
the Numeral, i.e., ????? (two), rather than on prepo-
sition ??????? (through/past), see Figure 4. The
numeral appears in post-position?a highly marked
word order that is admissible in this colloquial con-
struction in Russian: ?????? ???? ???? (lit.: after
hours two = idiom: after about two hours), where
Figure 4: Distributions of cases in the corpus and in a
sample. (Arrows indicate syntactic dependency.)
the preposition governs the Case of the numeral, and
the numeral governs a noun that precedes it.
Because our algorithm at the moment processes
linear sequences, these kinds of syntactic inversion
phenomena in Russian will pose a challenge. In gen-
eral this problem can be solved by using tree-banks
for MWE extraction, (Seretan, 2008; Martens and
Vandeghinste, 2010). However, an appropriate tree-
78
bank is not always available for a given language; in
fact, we do not have access to any Russian tree-bank
suitable for this task.
23: ??? (with) This is a genuine error. The algo-
rithm misses two of four correct cases, Genitive and
Accusative, because both are widely used across the
corpus, which reduces their frequency ratio in the
sub-sample. Our further work will focus on finding
flexible frequency ratio thresholds, which is now set
to one. Two of the correct cases (Instrumental and
Gen2) are well over the threshold, while Genitive,
with 0.6924, and Accusative, with 0.0440, fall short.
18: ???? (by/along) For this preposition the al-
gorithm predicts 1 case out of 3. This situation is
slightly different from the previous ones, since the
accusative and locative cases are much more rare
with preposition ???? (by/along) than the dative:
245 instances out of 15387 for accusative, and 222
for locative in our data. We hypothesize that this
means that such ?Prep+case? combinations are con-
strained lexically to a greater extent than grammat-
ically. To check this hypothesis we calculate the
frequency ratio for all lemmas that appear with the
respective patterns { ???? (by/along) + acc } and
{ ???? (by/along) + loc }. As a result, 15 distinct
lemmas were extracted by { ???? (by) + acc }; 13
out of them have frequency ratio > 1. The major-
ity of the lemmas belong to the semantic class ?part
of the body? and are used in a very specific Rus-
sian construction, which indicates ?an approximate
level?, e.g. ??? ??????? (up to (one?s) elbow), cf.
English ?up to one?s neck in work?. This construc-
tion has limited productivity, and we are satisfied
that the Accusative is omitted in the output for gram-
matical categories, since the algorithm outputs all
tokens that appear in the { ???? (by/along) + acc }
as relevant lemmas.
The case of { ???? (by) + loc } is more com-
plex: 44 of 76 combinations return a frequency
greater than 1. Analysis of annotation errors reveals
a compact collection of bureaucratic cliches, like
??? ????????? (upon arrival), ??? ??????????
(upon completion), etc., which all share the seman-
tics of ?immediately following X?, and are pragmat-
ically related. These are expressions belonging to
the same bureaucratic jargon and sharing the same
morphological pattern, however, they are below the
threshold. Again, we are faced with need to tune the
threshold to capture this kind of potentially interest-
ing lexical combinations. In general, semantic and
pragmatic factors influence the ability of words to
combine, and the algorithm shows it in some way,
though these aspects of the problem are beyond the
scope of our experiments in the current stage.
5 Discussion and Future Work
5.1 Development of the algorithm
We have presented a part an overall system under de-
velopment. In the preceding sections, we investigate
an area where collocations and colligations meet. To
summarize, the algorithm, based on the corpus of n-
grams, treats both morpho-syntactic and lexical co-
occurrences as a unified continuum, which has no
clear borders. The evaluation of the morphological
output raises some new questions for further devel-
opment:
? At present, the low precision for both low- and
high-frequency tags depends on the threshold,
which needs to be studied further.
? The values of divergences are currently not
normalized among the different query patterns.
This may be a difficult question, and we plan to
investigate this further. The algorithm provides
a way to compare the strength of very diverse
collocations, which have nothing in common,
in terms of their degree of idiomatization.
? We observe that the longer the n-gram, the
more we expect it to be a collocation; stable
bigrams appear more frequently to be colliga-
tions, while stable 4-grams are more often col-
locations. The problem is that those colloca-
tions with a highly frequent first collocate, e.g.,
??? (in), cannot be found using our algorithm
as it stands now.
? Token/lexeme stability is the next task we will
concentrate on. Wermter and Hahn (2006) and
Kilgarriff et al (2012) proposed that sorting
tokens/lexemes according to plain frequency
works well if there is no grammatical knowl-
edge at hand. We do have such knowledge. To
improve the accuracy of lexeme/token extrac-
tion we rely on the idea of grammatical pro-
79
files, introduced by Gries and Divjak (2009).
We plan to develop this approach with the
further assumption that the distribution of to-
kens/lexemes within a pattern is based on rel-
evant grammatical properties, which are ob-
tained in an earlier step of our algorithm. For
instance, for ??? ?? X? (not up to X) we have
found that the grammatical profile for X is
N.gen/gen2, and the token frequency ratio is
greater than 1 as well. Building the list of to-
kens that are the most stable for this pattern, we
compare their distributions within the pattern to
all N.gen/gen2 tokens in the corpus. This yields
the following tokens as the most relevant: ???
?? ?????? (lit.: not up to laughter.gen = id-
iom: no laughing matter);??? ?? ????? (lit.
not up to fat.gen2 = idiom: no time/place for
complacency), which reveals an interesting set
of idioms.
5.2 Extensions and Applications
The model has no restriction on the length of data
to be used, and is applicable to various languages.
Finnish (which is morphologically rich) and English
(morphologically poor) will be examined next. As
for Russian, so far the algorithm has been systemat-
ically evaluated against bigrams, although we have
3-, 4- and 5-grams at our disposal for future work.
A reliable method that is able to determine pat-
terns of frequently co-occurring lexical and gram-
matical features within a corpus can have far-
reaching practical implications. One particular ap-
plication that we are exploring is the fine-tuning
of semantic patterns that are commonly used in in-
formation extraction (IE), (Grishman, 2003). Our
work on IE focuses on different domains and differ-
ent languages, (Yangarber et al, 2007; Atkinson et
al., 2011). Analysis of MWEs that occur in extrac-
tion patterns would provide valuable insights into
how the patterns depend on the particular style or
genre of the corpus, (Huttunen et al, 2002). Subtle,
genre-specific differences in expression can indicate
whether a given piece of text is signaling the pres-
ence an event of interest.
5.3 Creating Teaching-Support Tools
Instructors teaching a foreign language are regu-
larly asked how words co-occur: What cases and
word forms appear after a given preposition? Which
ones should I learn by rote and which ones follow
rules? The persistence of such questions indicates
that this is an important challenge to be addressed?
we should aim to build a system that can automati-
cally generate an integrated answer. A tool that pro-
duces answers to these questions would be of great
help for teachers as well as students. The presented
algorithm can support an easy-to-use Web-based ap-
plication, or an application for a mobile device. We
plan to develop a service, which is able to process
queries described in the paper. This service would
be an additional interface to a corpus, aimed at find-
ing not only the linear context of words but also their
collocational and constructional preferences. We be-
lieve that such an interface would be useful for both
research and language-learning needs.
Acknowledgments
We are very grateful to the Russian National Cor-
pus developers, especially E. Rakhilina and O. Lya-
shevskaya, for providing us with the data.
References
Martin Atkinson, Jakub Piskorski, Erik van der Goot, and
Roman Yangarber. 2011. Multilingual real-time event
extraction for border security intelligence gathering.
In U. Kock Wiil, editor, Counterterrorism and Open
Source Intelligence, pages 355?390. Springer Lecture
Notes in Social Networks, Vol. 2, 1st edition.
Brigitte Bigi. 2003. Using Kullback-Leibler distance
for text categorization. In Fabrizio Sebastiani, edi-
tor, Advances in Information Retrieval, volume 2633
of Lecture Notes in Computer Science, pages 305?319.
Springer Berlin, Heidelberg.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational linguistics, 16(1):22?29.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Kindle. 1991. Using statistics in lexical analy-
sis. Lexical acquisition: exploiting on-line resources
to build a lexicon.
Vidas Daudaravicius. 2010. Automatic identification of
lexical units. Computational Linguistics and Intelli-
gent text processing CICling-2009.
Boris Dobrov and Natalia Loukachevitch. 2011. Mul-
tiple evidence for term extraction in broad domains.
In Proceedings of the 8th Recent Advances in Natu-
ral Language Processing Conference (RANLP 2011).
Hissar, Bulgaria, pages 710?715.
80
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions using
linguistically-motivated statistical measures. In Pro-
ceedings of the Workshop on A Broader Perspective on
Multiword Expressions, pages 9?16. Association for
Computational Linguistics.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms:
the c-value/nc-value method. International Journal on
Digital Libraries, 3(2):115?130.
Adele Goldberg. 2006. Constructions at work: The na-
ture of generalization in language. Oxford University
Press, USA.
Stefan Th. Gries and Dagmar Divjak. 2009. Behavioral
profiles: a corpus-based approach to cognitive seman-
tic analysis. New directions in cognitive linguistics,
pages 57?75.
Ralph Grishman. 2003. Information extraction. In
The Handbook of Computational Linguistics and Nat-
ural Language Processing, pages 515?530. Wiley-
Blackwell.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Diversity of scenarios in information extraction.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002),
Las Palmas de Gran Canaria, Spain, May.
Maria Khokhlova and Viktor Zakharov. 2009. Statistical
collocability of Russian verbs. After Half a Century
of Slavonic Natural Language Processing, pages 125?
132.
Adam Kilgarriff, Pavel Rychly`, Vojtech Kova?r, and V?t
Baisa. 2012. Finding multiwords of more than two
words. In Proceedings of EURALEX2012.
Scott Martens and Vincent Vandeghinste. 2010. An effi-
cient, generic approach to extracting multi-word ex-
pressions from dependency trees. In CoLing Work-
shop: Multiword Expressions: From Theory to Appli-
cations (MWE 2010).
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL Student Research Workshop, pages 13?18. Asso-
ciation for Computational Linguistics.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for
the extraction of multiword expressions. In Proceed-
ings of the LREC Workshop-Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 50?53.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57. Washington, DC.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. Computational Lin-
guistics and Intelligent Text Processing, pages 189?
206.
Violeta Seretan. 2008. Collocation extraction based on
syntactic parsing. Ph.D. thesis, University of Geneva.
Serge Sharoff. 2004. What is at stake: a case study of
Russian expressions starting with a preposition. In
Proceedings of the Workshop on Multiword Expres-
sions: Integrating Processing, pages 17?23. Associ-
ation for Computational Linguistics.
Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata.
1997. Retrieving collocations by co-occurrences and
word order constraints. In Proceedings of the eighth
conference on European chapter of the Association for
Computational Linguistics, pages 476?481. Associa-
tion for Computational Linguistics.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words and
constructions. International journal of corpus linguis-
tics, 8(2):209?243.
Marko Tadic? and Kres?imir S?ojat. 2003. Finding multi-
word term candidates in Croatian. In Proceedings of
IESL2003 Workshop, pages 102?107.
Tim Van de Cruys and Begona Villada Moiro?n. 2007.
Lexico-semantic multiword expression extraction. In
Proceedings of the 17th Meeting of Computational
Linguistics in the Netherlands (CLIN), pages 175?190.
Joachim Wermter and Udo Hahn. 2006. You can?t beat
frequency (unless you use linguistic knowledge) ? a
qualitative evaluation of association measures for col-
location and term extraction. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 785?792.
Elena Yagunova and Lidia Pivovarova. 2010. The nature
of collocations in the Russian language. The experi-
ence of automatic extraction and classification of the
material of news texts. Automatic Documentation and
Mathematical Linguistics, 44(3):164?175.
Roman Yangarber, Clive Best, Peter von Etter, Flavio
Fuart, David Horby, and Ralf Steinberger. 2007.
Combining information about epidemic threats from
multiple sources. In Proceedings of the MMIES
Workshop, International Conference on Recent Ad-
vances in Natural Language Processing (RANLP
2007), Borovets, Bulgaria, September.
81
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 29?37,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Event representation across genre
Lidia Pivovarova, Silja Huttunen and Roman Yangarber
University of Helsinki
Finland
Abstract
This paper describes an approach for investi-
gating the representation of events and their
distribution in a corpus. We collect and
analyze statistics about subject-verb-object
triplets and their content, which helps us com-
pare corpora belonging to the same domain
but to different genre/text type. We argue that
event structure is strongly related to the genre
of the corpus, and propose statistical proper-
ties that are able to capture these genre differ-
ences. The results obtained can be used for the
improvement of Information Extraction.
1 Introduction
The focus of this paper is collecting data about
certain characteristics of events found in text, in
order to improve the performance of an Infor-
mation Extraction (IE) system. IE is a tech-
nology used for locating and extracting specific
pieces of information?or ?facts??from unstruc-
tured natural-language text, by transforming the
facts into abstract, structured objects, called events.
In IE we assume that events represent real-world
facts and the main objective is to extract them from
plain text; the nature of the events themselves rarely
receives in-depth attention in current research.
Events may have various relationships to real-
world facts, and different sources may have contra-
dictory views on the facts, (Saur?? and Pustejovsky,
2012). Similarly to many other linguistic units, an
event is a combination of meaning and form; the
structure and content of an event is influenced by
both the structure of the corresponding real-world
fact and by the properties of the surrounding text.
We use the notion of scenario to denote a set
of structured events of interest in a real-world do-
main: e.g., the MUC Management Succession sce-
nario, (Grishman and Sundheim, 1996), within the
broader Business domain.
The representation and the structure of events in
text depends on the scenario. For example, Huttunen
et al (2002a; Huttunen et al (2002b) points out that
?classic? MUC scenarios, such as Management Suc-
cession or Terrorist Attacks, describe events that oc-
cur in a specific point in time, whereas other sce-
narios like Natural Disaster or Disease Outbreak
describe processes that are spread out across time
and space. As a consequence, events in the latter,
?nature?-related scenarios are more complex, may
have a hierarchical structure, and may overlap with
each other in text. Linguistic cues that have been
proposed in Huttunen et al (2002a) to identify the
overlapping or partial events include specific lexical
items, locative and temporal expressions, and usage
of ellipsis and anaphora.
Grishman (2012) has emphasized that for fully
unsupervised event extraction, extensive linguistic
analysis is essential; such analysis should be able
to capture ?modifiers on entities, including quan-
tity and measure phrases and locatives; modifiers on
predicates, including negation, aspect, quantity, and
temporal information; and higher-order predicates,
including sequence and causal relations and verbs of
belief and reporting.? It is clear that such sophisti-
cated linguistic analysis increases the importance of
text style and genre for Information Extraction.
29
The idea of statistical comparison between text
types goes back at least as far as (Biber, 1991). It
was subsequently used in a number of papers on au-
tomatic text categorization (Kessler et al, 1997; Sta-
matatos et al, 2000; Petrenz and Webber, 2011).
Szarvas et al (2012) studied the linguistic cues
indicating uncertainty of events in three genres:
news, scientific papers and Wikipedia articles. They
demonstrate significant differences in lexical usage
across the genres; for example, such words as fear
or worry may appear relatively often in news and
Wikipedia, but almost never in scientific text. They
also investigate differences in syntactic cues: for
example, the relation between a proposition and a
real-word fact is more likely to be expressed in the
passive voice in scientific papers (it is expected),
whereas in news the same words are more likely ap-
pear in the active.
Because events are not only representations of
facts but also linguistic units, an investigation of
events should take into account the particular lan-
guage, genre, scenario and medium of the text?i.e.,
events should be studied in the context of a particu-
lar corpus. Hence, the next question is how corpus-
driven study of events should be organized in prac-
tice, or, more concretely, what particular statistics
are needed to capture the scenario-specific charac-
teristics of event representation in a particular cor-
pus, and what kind of markup is necessary to solve
this task. We believe that answers to these questions
will likely depend on the ultimate goals of event de-
tection. We investigate IE in the business domain?
thus, we believe that preliminary study of the corpus
should use exactly the same depth of linguistic anal-
ysis as would be later utilized by the IE system.
2 Problem Statement
2.1 Events in the Business domain
We investigate event structure in the context of
PULS,1 an IE System, that discovers, aggregates,
verifies and visualizes events in various scenarios.
This paper focuses on the Business domain, in which
scenarios include investments, contracts, layoffs and
other business-related events, which are collected in
a database to be used for decision support. In the
Business domain, PULS currently handles two types
1More information is available at: http://puls.cs.helsinki.fi
Figure 1: Distributions of document length in the news
and business analysts? reports corpora
of documents: news reports and short summaries
written by professional business analysts. Thus,
events extracted from both corpora relate to approx-
imately the same real-world facts.
Both corpora are in English (though some of the
analysts? reports are based on news articles written
in other languages). We collected a corpus of re-
ports containing 740 thousand documents over three
years 2010-2012, and a news corpus containing 240
thousand documents over the same period.
The two corpora demonstrate significant linguis-
tic differences. First, the documents have different
length: the average length of an analyst reports is 5.5
sentences including the title, and 80% of the docu-
ments have length between 4 and 7 sentences, (see
Figure 1). News articles are on average 19 sentences
long?and much more varied in length.
The topical structure is also quite different for the
two corpora. Each analyst report is most typically
dedicated to a particular single real-world event.
Also, the reports tend to have a standardized, formu-
laic structure. The analysts who generate these re-
ports tend to follow a specific, strict style and struc-
ture over time.
By contrast, documents in the news corpus are
much more heterogeneous. These texts can follow
a wide variety of different styles?short messages,
surveys, interviews, etc. News documents can focus
not only strictly on business events but on related
topics as well. For example, political events have
complex interaction with and impact on business ac-
30
tivity, and therefore political news frequently appear
in business news feeds.
PULS aims to use the same processing chain for
various types of input documents. One key goal of
the current work is to investigate whether different
IE processing approaches are needed for documents
belonging to different text types, as exemplified by
analyst reports vs. articles from news feeds.
To summarize, the goals of the present work are:
? investigate how text genre influences event rep-
resentation;
? find formal markers able to capture and mea-
sure the differences in corpus style/genre;
? propose a methodology for adaptating an IE
system to a different text genre.
2.2 System Description
In this section we describe how the IE system is used
in a ?pattern-mining mode,? to address the afore-
mentioned problems.
PULS is a pipeline of components, including:
a shallow parser/chunker; domain ontologies and
lexicons; low-level patterns for capturing domain-
specific entities and other semantic units, such as
dates and currency expressions; higher-level pat-
terns for capturing domain-specific relations and
events; inference rules, which combine fragments of
an event that may be scattered in text?that a pattern
may not have picked up in the immediate context
(e.g., the date of the event); reference resolution for
merging co-referring entities and events.
The ontology and the lexicon for the Business do-
main encode the taxonomic relations and support
merging of synonyms: e.g., the ontology stores the
information that cellphone and mobile phone are
synonymous, and that a super-concept for both is
PRODUCT.
Low-level patterns are used to extract entities
from text, such as company names, dates, and lo-
cations. On a slightly higher level, there are pat-
terns that match contexts such as range (collection,
line, etc.) of X and assign them the type of X. For
instance, the phrase a collection of watches would
be assigned semantic type watch, etc. The top-level
patterns in all IE scenarios are responsible for find-
ing the target events in text.
In the pattern-mining mode we use the gen-
eral pattern SUBJECT?VERB?OBJECT, where the
components may have any semantic type and are
constrained only by their deep syntactic function?
the system attempts to normalize many syntactic
variants of the basic, active form: including passive
clauses, relative clauses, etc.2
The idea of using very simple, local patterns
for obtaining information from large corpora in
the context of event extraction is similar to work
reported previously, e.g., the bootstrapping ap-
proaches in (Thelen and Riloff, 2002; Yangarber et
al., 2000; Riloff and Shepherd, 1997). Here, we
do not use iterative learning, and focus instead on
collecting and analyzing interesting statistics from
a large number of S-V-O patterns. We collected
all such ?generalized? S-V-O triplets from the cor-
pus and stored them in a database. In addition to
the noun groups, we save the head nouns and their
semantic classes. This makes it easy to use sim-
ple SQL queries to count instances of a particular
pattern, e.g., all objects of a particular verb, or all
actions that can be applied to an object of seman-
tic class ?PRODUCT.? For each triplet the database
stores a pointer the original sentence, making it pos-
sible to analyze specific examples in their context.
In the next two sections we present the statis-
tics that we collected using the pattern-mining
mode. This information reflects significant differ-
ences among the corpora genres and can be used to
measure variety of genre. We believe that in the fu-
ture such data analysis will support the adaptation of
PULS to new text genres.
3 Statistical Properties of the Corpora
3.1 Personal pronouns
Pronouns play a key role in anaphoric relations; the
more pronouns are present in the corpus, the more
crucial anaphora resolution becomes. Analysis of
relationships between frequencies of personal pro-
nouns in text and the genre of the text is not new;
it has been observed and studied extensively, going
2By normalization of syntactic variants we mean, for in-
stance, that clauses like ?Nokia releases a new cellphone? (ac-
tive), ?a new cellphone is released by Nokia? (passive), ?a new
cellphone, released by Nokia,...? (relative), etc., are all reduced
to the same S-V-O form.
31
Reports News
Pronoun Object Subject Object Subject
I/me 0.003 0.007 0.2 1.0
we/us 0.001 0.001 0.4 1.7
you 0.002 0.003 0.3 0.8
he/him 0.05 0.4 0.6 2.2
she/her 0.007 0.05 0.1 0.5
they/them 0.3 0.6 0.8 1.3
it 1.1 2.6 1.5 2.3
Total 1.5 3.6 4.0 9.8
Table 1: Personal pronouns appearing in the subject or
object position in the corpora. The numerical values are
proportions of the total number of verbs.
back as far as, e.g., (Karlgren and Cutting, 1994).
The analysis of pronoun distribution in our corpora
is presented in Table 1, which shows the proportions
of personal pronouns, as they appear in subject or
object position with verbs in the collected triples.
The numbers are relative to the count of all verb to-
kens in the corpus, i.e., the total number of the S?V?
O triplets extracted from the corpus. The total num-
ber of triplets is approximately 5.7M in the report
corpus and 11M in the news corpus.
It can be seen from Table 1 that personal pro-
nouns are much more rare in the report corpus than
in the news corpus. Only 1.5% of verbs in the re-
ports corpus have a pronoun as an object, and 3.6%
as a subject. By contrast, in the news corpus 4%
of verbs have a personal pronoun as an object, and
9.8% as a subject. This corresponds to the observa-
tion in (Szarvas et al, 2012), that ?impersonal con-
structions are hardly used in news media.?
It is interesting to note the distribution of the par-
ticular pronouns in the two corpora. Table 1 shows
that it is the most frequent pronoun, they and he are
less frequent; the remaining pronouns are much less
frequent in the report corpus, whereas in the news
the remaining personal pronouns have a much more
even distribution. This clearly reflects a more re-
laxed style of the news that may use rhetorical de-
vices more freely, including citing direct speech and
use a direct addressing the reader (you). It is also
interesting to note that in the third-person singular,
the feminine pronoun is starkly more rare in both
corpora than the masculine, but roughly twice more
rare among the analyst reports.
Reports News
Subject Object Subject Object
All 21.8 6.6 14.6 6.5
Business 27.1 8.1 20.1 9.5
Table 2: Distribution of proper names as subjects and ob-
jects, as a proportion the total number of all verbs (top
row) vs. business-related verbs (bottom row).
3.2 Proper Names
Proper names play a crucial role in co-reference res-
olution, by designating anaphoric relations in text,
similarly to pronouns. In the Business domain, e.g.,
a common noun phrase (NP) may co-refer with a
proper name, as ?the company? may refer to the
name of a particular firm. A correctly extracted
event can be much less useful for the end-user if it
does not contain the specific name of the company
involved in the event.
A verbs is often the key element of a pattern that
indicates to the IE system the presence of an event
of interest in the text. When the subject or ob-
ject of the verb is a common NP, the corresponding
proper name must be found in the surrounding con-
text, using reference resolution or domain-specific
inference rules. Since reference resolution is itself
a phase that contributes some amount of error to
the overall IE process, it is natural to expect that if
proper-name subjects and objects are more frequent
in the corpus, then the analysis can be more precise,
since all necessary information can be extracted by
pattern without the need for additional extra infer-
ence. Huttunen et al (2012) suggests that the com-
pactness of the event representation may be used as
one of the discourse cues that determine the event
relevance.
Table 2 shows the percentage of proper name ob-
jects and subjects for the two corpora. Proper-name
objects have comparable frequency in both corpora,
though proper-name subjects appear much more fre-
quently in analyst reports than in news. Further-
more, for the business verbs, introduced below in
section 4.1?i.e., the specific set of verbs that are
used in event patterns in the Business scenarios?as
seen in the second row of the table?proper-name
objects and subjects are more frequent still. This
suggests that business events tend to mention proper
names.
32
Percentage of business verbs
Corpus Total Title 1st sentence
Reports 49.5 7.6 13.8
News 31.8 0.6 1.1
Table 3: Business verbs in analyst reports and news cor-
pora, as a proportion of the total number of verbs.
4 Business Verbs
4.1 Distribution of Business verbs
The set of business-related verbs is an important part
of the system?s domain-specific lexicon for the Busi-
ness domain. These verbs are quite diverse: some
are strongly associated with the Business domain,
e.g., invest; some are more general, e.g., pay, make;
many are ambiguous, e.g., launch, fire. Inside ana-
lyst reports these verbs always function as markers
of certain business events or relations. The verbs
are the key elements of the top-level patterns and it
is especially crucial to investigate their usage in the
corpora to understand how the pattern base should
be fine-tune for the task.
Since the majority of these verbs fall in the am-
biguous category, none of these verbs can by them-
selves serve a sufficient indicator of the document?s
topic. Even the more clear-cut business verbs, such
as invest, can be used metaphorically in the non-
business context. However, their distribution in the
particular document and in the corpus as a whole can
reflect the genre specificity of the corpus.
Table 3 shows the overall proportion of the busi-
ness verbs, and their proportion in titles and in the
first sentence of a documents. It suggests that almost
50% of the verbs in the report corpus are ?business?
verbs, and almost half of them are concentrated in
the beginning of a document. By contrast, the frac-
tion of business verbs in the news corpus is less than
one third and they are more scattered through the
text. This fact is illustrated by the plot in Figure 2.
The first sentence is often the most informa-
tive part of text, since it introduces the topic of
the document to the reader and the writer must do
his/her best to attract the reader?s attention. It was
shown in (Huttunen et al, 2012) that 65% of highly-
relevant events in the domain of medical epidemics
appear in the title or in the first two sentences of a
news article; Lin and Hovy (1997) demonstrated that
Figure 2: Percentage of business verbs in the text; sen-
tence 0 refers to the title of the document. The fraction of
verbs is presented as a percent of all verb instances in the
corpus. Logarithmic scale is used for the x axis.
about 50% of topical keywords are concentrated in
the titles. We have noticed that some documents in
the news corpus have relevance to the business sce-
nario, although relevant events still can be extracted
from the second or third paragraphs of the text, men-
tioned incidentally. By contrast, each analyst report
is devoted to a specific business event, and these
events are frequently mentioned as early as in the
title.
4.2 Case study: is ?launch? a business verb?
A set of verbs such as launch, introduce, release,
present,3 etc., are used in the Business scenarios to
extract events about bringing new products to mar-
ket. In the domain ontology they are grouped under
a concept called LAUNCH-PRODUCT. An example
of a pattern that uses this concept is following:
np(COMPANY) vg(LAUNCH-PRODUCT)
np(ANYTHING)
This pattern matches when a NP designating a com-
pany is followed by a verb from the ontology, fol-
lowed by any other NP. This pattern matches, for
example, such sentence as: The Real Juice Company
has launched Pomegranate Blueberry flavour to its line
of 100% juices. However, this pattern also over-
generates by matching sentences such as, e.g.: Cen-
3Note, the S-V-O triplet extraction also handles phrasal
verbs, such as roll out, correctly, i.e., identifies them as a single
linguistic unit, and treats them the same as single-word verbs.
33
tral bank unveils effort to manage household debt. Even
among analyst reports, approximately 14% of the
NEW-PRODUCT events found by the system are
false positives. It is not feasible to collect a list of
all possible products to restrict the semantic type
of the object of the verb, since new, unpredictable
types of products can appear on the market every
day. It seemed more feasible to try to discover all
non-products that can appear in the object slot, due
to the ambiguity of the verbs in patterns?a kind of a
black-list. We introduce an ontology concept NON-
PRODUCT that groups nouns that can be matched
by the LAUNCH verbs but are in fact not products,
e.g., budget, effort, plan, report, study. The ontology
supports multiple inheritance, so any of these words
can be attached to other parents as well, if necessary.
If the <PRODUCT> slot in of event is filled by
one of the black-listed concepts, the event is also
black-listed, and not visible to the end-user. They
are used as discourse features by learning algorithms
that predict the relevance of other events from the
same documents (Huttunen et al, 2012).
The NON-PRODUCT class is populated in an ad-
hoc manner over time. The content of such a list
depends on the particular corpus; the more diverse
the topical and stylistic structure of the corpus, the
more time-consuming and the less tractable such de-
velopment becomes. Thus, an important task is to
adjust the patterns and the class of NON-PRODUCT
nouns to work for the news corpus, and to develop
a feasible methodology to address the false-positive
problem. We next show how we can use the pattern-
mining mode to address these problems.
We extract all instances of the LAUNCH-
PRODUCT verbs appearing in the corpora from the
S?V?O database. In total 27.5% of all verb instances
in reports corpus are verbs from this semantic class,
in comparison to 0.7% in the news corpus. The num-
ber of distinct objects are approximately the same in
both corpora: 3520 for reports and 3062 for news,
see Table 4. In total 247 different objects from the
report corpus attached to the semantic class PROD-
UCT in PULS ontology, and 158 objects have this
semantic class in the news corpus.
For 21% of launch verbs in the report corpus, and
34% in the news corpus, the system is not able to ex-
tract the objects, which may be a consequence of the
more diverse and varied language of news. Recall,
LAUNCH- distinct PRODUCT
Corpus PRODUCT objects objects
Reports 204193 3520 247
News 77463 3062 158
Table 4: Distributions of LAUNCH-PRODUCT verbs in
the corpora
that the system extracts a deep-structure verbal argu-
ments, i.e., for a sentence like ?A new cell-phone has
been launched by company XYZ? it identifies cell-
phone as the (deep) object, and the agent company
XYZ as the (deep) subject.
It is interesting to examine the particular sets of
words that can appear in the object position. We col-
lected the 50 most frequent objects of the LAUNCH-
PRODUCT verbs for each corpus; they are shown in
Table 5 ranked by frequency (we show the top 30
objects to save space). The table shows the semantic
class according to our ontology.
Of the 50 most frequent objects, 24 belong to
the semantic class PRODUCT in the report corpus,
while only 8 objects do in the general news cor-
pus. By contrast, 20 objects belong to the NON-
PRODUCT class in the news corpus and only 9 ob-
jects in reports. Moreover, 8 objects in the news cor-
pus are not found in the ontology at all, in compari-
son to only one such case from the report corpus.
Some object classes may mean that the event is
still relevant for the business domain, though it does
not belong to the NEW-PRODUCT scenario. For
example, when object is an advertising campaign the
event is likely to belong to the MARKETING sce-
nario, when the object is a facility (factory, outlet,
etc.) it is likely INVESTMENT. Inference rules may
detect such dependencies and adjust the scenario of
these events in the Business domain.
The inference rules are supported by the same do-
main ontology, but can test domain- and scenario-
specific conditions explicitly, and thus can be more
accurate than the generic reference resolution mech-
anism. However, this also means that inference rules
are more sensitive to the corpus genre and may not
easily transfer from one corpus to another.
In some cases an object type cannot be interpreted
as belonging to any reasonable event type, e.g., if
it is an ORGANIZATION or PERSON. Such cases
may arise due to unusual syntax in the sentence that
34
Rank Reports News
Object Freq Class Object Freq Class
1 Proper Name unspecified 19987 Proper Name unspecified 5971
2 product 7331 PROD report 1078 NON
3 service 6510 PROD result 851 NON
4 campaign 3537 CAMP plan 805 NON
5 project 2870 PROD product 792 PROD
6 range 2536 COLL service 648 PROD
7 plan 2524 NON it 618 PRON
8 organization 2450 ORG data 552
9 system 2166 FAC campaign 510 CAMP
10 line 1938 COLL organization 495 ORG
11 model 1920 PROD statement 467 NON
12 application 1345 PROD Proper Name person 449 PER
13 website 1321 PROD program 439
14 flight 1315 PROD Proper Name company 432 ORG
15 Proper Name company 1232 ORG information 411 NON
16 brand 1200 COLL detail 398 NON
17 offer 1187 NON investigation 380 NON
18 production 1112 NON website 373 PROD
19 programme 998 NON measure 368 NON
20 store 993 PROD they 363 PRON
21 currency 958 CUR he 358 PRON
22 route 954 PROD device 352 PROD
23 drink 891 PROD system 340 FAC
24 solution 883 NON smartphone 337 PROD
25 smartphone 852 PROD attack 335
26 fragrance 824 PROD figure 318 NON
27 card 802 PROD opportunity 295 INV
28 fund 801 PROD fund 290 NON
29 scheme 773 NON currency 287 CUR
30 facility 756 FAC model 286 COLL
Table 5: The most frequent objects of LAUNCH verbs. Class labels: PROD: product, NON: non-product (black-
listed), CAMP: advertising campaign, INV: investment. Domain independent labels: COLL: collective; PRON: pro-
noun, FAC: facility, ORG: organization, PER: person, CUR: currency,
confuses the shallow parser.
In summary, the results obtained from the S-V-O
pattern-mining can be used to improve the perfor-
mance of IE. First, the most frequent subjects and
objects for the business verbs can be added to the
ontology; second, inference rules and patterns are
adjusted to handle the new concepts and words.
It is very interesting to investigate?and we plan
to pursue this in the future?how this can be done
fully automatically; the problem is challenging since
the semantic classes for these news concepts de-
pend on the domain and task; for example, some
objects are of type PRODUCT (e.g., ?video?), and
others are of type NON-PRODUCT (e.g., ?attack?,
?report?, etc.). Certain words can be ambiguous
even within a limited domain: e.g., player may des-
ignate a COMPANY (?a major player on the mar-
ket?), a PRODUCT (DVD-player, CD-player, etc.),
or a person (tennis player, football player, etc.); the
last meaning is relevant for the Business domain
since sports personalities participate in promotion
campaigns, and can launch their own brands. Au-
tomating the construction of the knowledge bases is
a challenging task.
In practice, we found that the semi-automated ap-
proach and the pattern-mining tool can be helpful for
analyzing genre-specific event patterns; it provides
the advantages of a corpus-based study.
35
5 Conclusion
We have described an approach for collecting use-
ful statistics about event representation and distribu-
tion of event arguments in corpora. The approach
was easily implemented using pattern-based extrac-
tion of S-V-O triplets with PULS; it can be equally
efficiently implemented on top of a syntactic parser,
or a shallow parser of reasonable quality. An ontol-
ogy and lexicons are necessary to perform domain-
specific analysis. We have discussed how the results
of such analysis can be exploited for fine-tuning of
a practical IE scenario.
The pattern-mining process collects deep-
structure S?V?O triplets from the corpus?which
are ?potential? events. The triplets are stored in
a database, to facilitate searching and grouping
by words or by semantic class appearing as the
arguments of the triplets. This helps us quickly
find all realizations of a particular pattern?for
example, all semantic classes that appear in the
corpus as objects of verbs that have semantic class
LAUNCH-PRODUCT. The subsequent analysis of
the frequency lists can help improve the perfor-
mance of the IE system by suggesting refinements
to the ontology and the lexicon, as well as patterns
and inference rules appropriate for the particular
genre of the corpus.
Our current work includes the adaptation of the
IE system developed for the analyst reports to the
general news corpus devoted to the same topics. We
also plan to develop a hybrid methodology, to com-
bine the presented corpus-driven analysis with open-
domain techniques for pattern acquisition, (Cham-
bers and Jurafsky, 2011; Huang and Riloff, 2012).
The approach outlined here for analyzing the dis-
tributions of features in documents is useful for
studying events within the context of a corpus. It
demonstrates that event structure depends on the text
genre, and that genre differences can be easily cap-
tured and measured. By analyzing document statis-
tics and the output of the pattern-mining, we can
demonstrate significant differences between the gen-
res of analyst reports and general news, such as: sen-
tence length, distribution of the domain vocabulary
in the text, selectional preference in domain-specific
verbs, word co-occurrences, usage of pronouns and
proper names.
The pattern mining collects other statistical fea-
tures, beyond those that have been discussed in de-
tail above. For example, it showed that active voice
is used in 95% of the cases in the news corpus in
comparison to 88% in the analyst report corpus. It
is also possible to count and compare the usage of
other grammatical cues, such as verb tense, modal-
ity, etc. Thus, we should investigate not only lexical
and semantic cues, but also broader syntactic prefer-
ences and selectional constrains in the corpora.
In further research we plan to study how the for-
mal representation of the genre differences can be
used in practice, that is, for obtaining directly mea-
surable improvements in the quality of event extrac-
tion. Taking into account the particular genre of the
corpora from which documents are drawn will also
have implications for the work on performance im-
provements via cross-document merging and infer-
ence, (Ji and Grishman, 2008; Yangarber, 2006).
The frequency-based analysis described in Sec-
tion 4.2 seems to be effective. Sharpening the results
of the analysis as well as putting it to use in practical
IE applications will be the subject of further study.
Acknowledgements
We wish to thank Matthew Pierce and Peter von Et-
ter for their help in implementation of the pattern
mining more described in this paper. The work was
supported in part by the ALGODAN: Algorithmic
Data Analysis Centre of Excellence of the Academy
of Finland.
References
Douglas Biber. 1991. Variation across speech and writ-
ing. Cambridge University Press.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL-HLT, pages 976?986.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: A brief history. In Pro-
ceedings of COLING, volume 96, pages 466?471.
Ralph Grishman. 2012. Structural linguistics and un-
supervised information extraction. Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction (AKBC-WEKEX 2012), pages 57?61.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. EACL 2012,
pages 286?295.
36
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002a. Complexity of event structure in IE scenarios.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING 2002), Taipei,
August.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002b. Diversity of scenarios in information extrac-
tion. In Proceedings of the Third International Confer-
ence on Language Resources and Evaluation (LREC
2002), Las Palmas de Gran Canaria, Spain, May.
Silja Huttunen, Arto Vihavainen, Mian Du, and Roman
Yangarber. 2012. Predicting relevance of event ex-
traction for the end user. In T. Poibeau et al, edi-
tor, Multi-source, Multilingual Information Extraction
and Summarization, pages 163?177. Springer-Verlag,
Berlin.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL-2008: HLT, pages 254?262, June.
Jussi Karlgren and Douglas Cutting. 1994. Recogniz-
ing text genres with simple metrics using discriminant
analysis. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1071?1075, Kyoto,
Japan, August.
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics and Eighth Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 32?38. Association for Com-
putational Linguistics.
Chin-Yew Lin and Eduard Hovy. 1997. Identifying top-
ics by position. In Proceedings of the fifth conference
on Applied natural language processing, pages 283?
290. Association for Computational Linguistics.
Philipp Petrenz and Bonnie Webber. 2011. Stable clas-
sification of text genres. Computational Linguistics,
37(2):385?393.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124.
Association for Computational Linguistics, Somerset,
New Jersey.
Roser Saur?? and James Pustejovsky. 2012. Are you sure
that this happened? Assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261?
299.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
conference on Computational linguistics - Volume 2,
COLING ?00, pages 808?814, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?fra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Mark Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2002).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisi-
tion of domain knowledge for information extrac-
tion. In Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING 2000),
Saarbru?cken, Germany, August.
Roman Yangarber. 2006. Verification of facts across
document boundaries. In Proceedings of the Inter-
national Workshop on Intelligent Information Access
(IIIA-2006), Helsinki, Finland, August.
37
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 100?109,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Adapting the PULS Event Extraction Framework
to Analyze Russian Text
Lidia Pivovarova,1,2 Mian Du,1 Roman Yangarber1
1 Department of Computer Science
University of Helsinki, Finland
2St. Petersburg State University, Russia
Abstract
This paper describes a plug-in component
to extend the PULS information extraction
framework to analyze Russian-language
text. PULS is a comprehensive framework
for information extraction (IE) that is used
for analysis of news in several scenarios
from English-language text and is primar-
ily monolingual. Although monolingual-
ity is recognized as a serious limitation,
building an IE system for a new language
from the bottom up is very labor-intensive.
Thus, the objective of the present work is
to explore whether the base framework can
be extended to cover additional languages
with limited effort, and to leverage the pre-
existing PULS modules as far as possible,
in order to accelerate the development pro-
cess. The component for Russian analysis
is described and its performance is evalu-
ated on two news-analysis scenarios: epi-
demic surveillance and cross-border secu-
rity. The approach described in the paper
can be generalized to a range of heavily-
inflected languages.
1 Introduction
1.1 Problem Statement
PULS1 is a framework for information extraction
from text (IE), designed for decision support in
various domains and scenarios. To date, work
on PULS has mostly concentrated on English-
language text, though some effort has gone into
adapting PULS to other languages, (Du et al,
2011). This paper describes a component that is
used to extend PULS to analyze Russian-language
text, and demonstrates its performance on two IE
scenarios: infectious epidemics and cross-border
1http://puls.cs.helsinki.fi
security. The epidemics scenario is built to pro-
vide an early warning system for professionals
and organizations responsible for tracking epi-
demic threats around the world. Because infor-
mation related to outbreaks of infectious disease
often appears in news earlier than it does in offi-
cial sources, text mining from the Web for medi-
cal surveillance is a popular research topic, as dis-
cussed in, e.g., (Collier et al, 2008; Huttunen
et al, 2002; Rortais et al, 2010; Zamite et al,
2010). Similarly, in the security scenario, the sys-
tem tracks cross-border crime, including illegal
migration, smuggling, human trafficking, as well
as general criminal activity and crisis events; text
mining for this scenario has been previously re-
ported by (Ameyugo et al, 2012; Atkinson et al,
2011). The new component monitors open-source
media in Russian, searching for incidents related
to the given scenarios. It extracts information
from plain, natural-language text into structured
database records, which are used by domain spe-
cialists for daily event monitoring. The structure
of the database records (called templates) depends
on the scenario. For the epidemics scenario the
system extracts the fields: disease name, location
of the incident, date, number of victims, etc. In the
security domain, the template contains the type of
event, date and location, the perpetrator, number
of victims (if any), goods smuggled, etc.
Monolinguality is a serious limitation for IE,
since end-users are under growing pressure to
cover news from multiple languages, (Piskorski
et al, 2011). The Russian-language component
that we describe here is an experiment in extend-
ing PULS to multi-lingual coverage. Our aim is to
explore whether a such an extension can be built
with limited effort and resources.
1.2 Prior work on IE from Russian
IE in Russian has been the topic of several recent
studies. For example, (Piskorski et al, 2011) uses
100
Russian among other languages to study infor-
mation fusion across languages. Extraction tech-
niques are used for ontology learning in (Bocharov
et al, 2010) and (Schumann, 2012). The Uni-
versity of Sheffield?s GATE system, (Bontcheva
et al, 2003), which supports multi-lingual IE, has
been adapted to Russian as part of the MUSE-3
project, (though little is published on functional-
ity available in Russian). HP Labs have recently
started adaptation of their information extraction
solutions to Russian, (Solovyev et al, 2012).
Much literature devoted to Russian-language
information extraction is published only in Rus-
sian; a brief review can be found in (Khoro-
shevsky, 2010). The majority of existing appli-
cations for Russian IE, and Natural Language Pro-
cessing (NLP) in general, are commercially based,
and are either published in Russian only, or not
at all. One major player in Russian text min-
ing is Yandex, the leading Russian search engine.
Yandex uses IE to support its main search ser-
vice, e.g., to underline addresses and persons in
search results, and in a service called ?Press Por-
traits,?2 which builds profiles for various personal-
ities found in the news. A profile may include the
profession, biographical facts, news that s/he is in-
volved in, and related people?using information
automatically extracted from on-line Russian me-
dia. Yandex also recently unveiled an open-source
toolkit Tomita, for developing IE systems based on
context-free grammars.
Dictum, a company that builds applications for
NLP and sentiment analysis in Russian, provides
a toolkit for Russian morphological, syntactic and
semantic analysis. Their Fact Extraction compo-
nent3 finds persons, organizations, locations, etc.,
and creates simple facts about persons: corporate
posts, date of birth, etc.
RCO, a company focused on research and de-
velopment of text analysis solutions, provides the
RCO Fact Extractor tool4, which performs fact ex-
traction from unstructured text. One common us-
age scenario is setting up a list of target objects
(persons, companies) and extracting all events
where these objects are mentioned as participants.
The tool also includes a module that allows the
user to adjust search patterns.
With the exception of Tomita and AOT (see Sec-
2http://news.yandex.ru/people
3http://dictum.ru/en/object-extraction/blog
4http://www.rco.ru/eng/product.asp
tion 3), few resources are available in open-source.
2 The Baseline English System
The PULS news-tracking pipeline consists of three
main components: a Web-crawler that tries to
identify potentially relevant articles using a broad
keyword-based Web search; a rule-based Informa-
tion Extraction system that uses patterns acquired
through semi-supervised learning, that determines
exactly what happened in the article, creating a
structured record that is stored in the database;
and a relevance classifier that determines the rele-
vance of the selected articles?and events that they
describe?to the particular use-case scenario and
the users? needs. This paper will mostly focus on
the IE component, as other two components are
language-independent.
The IE system contains modules for lower-
level?morphological and syntactic?analysis, as
well as higher-level?semantic?analysis, and
produces filled templates on output, extracted
from an input document, (Du et al, 2011).
PULS follows a classic IE processing pipeline:
? Pre-processing,
? Lexical markup,
? Shallow syntactic analysis/chunking,
? Semantic pattern matching
? Reference resolution and logical inference
Pre-processing includes tokenization, part-of-
speech tagging, processing of punctuation, nu-
meric expressions, etc.
Lexical markup is tagging of lexical units found
in text with semantic information found in a dictio-
nary and/or ontology. PULS uses several domain-
independent and domain-specific lexicons and on-
tologies. The ontology is a network of con-
cepts organized in a hierarchy by several rela-
tions, among which the ?is-a? relation is the most
common. One key factor that enables the addi-
tion of new languages efficiently is that the on-
tology is language-independent. The system uses
the lexicons to map words into concepts. A lex-
icon consists of word-forms and some common
multi-word expressions (MWEs), which appear in
text and represent some ontology concept. We
assume that within a given domain each word or
101
MWE in the lexicon represents exactly one con-
cept, (Yarowsky, 1995). A concept may be rep-
resented by more than one word or MWE.5 Each
scenario has its own scenario-specific ontology
and lexicons; the Epidemics ontology consists of
more than 4000 concepts (which includes some
disease names). Diseases are organized in a hi-
erarchy, e.g., ?hepatitis? is a parent term for ?hep-
atitis A?. The Security ontology consists of 1190
concepts.
The domain-specific lexicon is a collection of
terms that are significant for a particular scenario,
mapped to their semantic types/concepts. The Se-
curity and Epidemics scenarios use a common lo-
cation lexicon, that contains approximately 2500
names of countries, cities and provinces. Loca-
tions are organized according the ?part-of? rela-
tion: cities are part-of provinces, which are part-of
states, etc.
Syntactic analysis is implemented as a cascade
of lower-level patterns. PULS uses shallow anal-
ysis (chunking), which does not try to build com-
plete syntactic tree for a sentence but recognizes
local grammatical structures?in particular, the
noun and verb groups. This phase also identi-
fies other common constructions needed for IE,
(names, dates, etc.). As a result of the syntactic
analysis, each sentence is represented as a set of
fragments.
The pattern base is the main component of the
IE system, responsible for finding factual informa-
tion in text. A pattern is a set of semantic, syntactic
and morphological constraints designed to match
pieces of natural-language text. When a pattern
fires it triggers an action, which creates an abstract
logical entity based on the text matched by the pat-
tern. The entity is added to an internal pool of
entities found in the document so far. Facts pro-
duced by the system are based on the entities in
this pool. The patterns are arranged in a cascade
such that the results produced by one pattern are
used by subsequent patterns to form more com-
plex entities.
Patterns operate on a sentence-by-sentence ba-
sis. To link information in the surrounding sen-
tences PULS uses concept-based reference reso-
lution and logical inference rules. The reference
resolution component is a set of rules for merging
5By default, words that appear only in the general-purpose
dictionary, and do not appear in any domain-specific lexicon,
are automatically identified with a concept having an identi-
cal name.
mentions of the same object and events.
Inference rules work on a logical level (rather
than text), operating on entities found at the pre-
ceding stages of analysis. These entities can be
used to fill slots in an event description, for exam-
ple, to find event time and location, or to perform
logical inference. For example, if the event type is
human-trafficking and a concept related to organ-
transplantation is mentioned in the sentence, an
inference rule may specialize the event type to
human-trafficking-organs.
3 Russian Morphology and Syntax
To speed development, we use pre-existing tools
for tokenization, morphological and syntactic
analysis in Russian. The range of freely-available,
open-source tools for Russian is quite narrow, es-
pecially for syntactic analysis. Significant efforts
for overcoming this situation have been the focus
of the recent ?Dialogue? series of conferences6,
which organized workshops on Russian morphol-
ogy, (Astaf?eva et al, 2010), and syntax, (Toldova
et al, 2012). Workshops take the form of compe-
titions, where the participants tackle shared tasks.
Eight teams participated in the latest workshop,
devoted to syntax. However, only one?AOT7?
offers their toolkit under the GNU LGPL license.
The AOT toolkit, (Sokirko, 2001) is a collec-
tion of modules for NLP, including libraries for
morphological, syntactic, and semantic analysis,
language generation, tools for working with dic-
tionaries, and GUIs for visualization of the anal-
ysis. Due to its open availability and high qual-
ity of linguistic analysis, AOT is currently a de-
facto standard for open-source Russian-language
processing.
The AOT morphological analyzer, called
?Lemm?, analyzes text word by word; its output
for each word contains: an index, the surface form,
the base lemma, part of speech, and morphologi-
cal tags. Lemm works on the morphological level
only, and leaves all morphological ambiguity in-
tact, to be resolved by later phases.
Lemm uses a general-purpose Russian mor-
phological dictionary, which can be edited and
extended (e.g., with neologisms, domain-specific
terms, etc.). To add a new lemma into the
dictionary, one needs to specify its inflectional
6Dialogue?International Conference of Computational
Linguistics (http://www.dialog-21.ru/en/)
7The AOT project (?Automatic Processing of Text? in
Russian)?www.aot.ru
102
paradigm. For Russian IE, we had to add to
the dictionary certain words and terms that des-
ignate scenario-specific concepts, for example
????????? (migrant) and ?????????????? (gas-
tarbaiter), which have become common usage in
recent Russian-language media.
The syntactic analyzer in AOT, ?Synan?, uses
a hybrid formalism, a mix of dependency trees
and constituent grammars. The output for a sen-
tence contains two types of syntactic units: binary
parent-child relations, and ?groups?, which are to-
ken sequences not analyzed further but treated as
an atomic expression. This approach is theoret-
ically natural, since certain syntactic units may
not have a clear root, for example, complex name
expressions (?Aleksey Sokirko?) or numeric ex-
pressions (?forty five?). To make it compatible
with the overall PULS structure, we transform all
Synan output into dependency-tree form; groups
simply become linked chains. Synan attempts to
produce a complete, connected parse structure for
the entire sentence; in practice, it produces a set of
fragments, consisting of relations and groups. In
the process, it resolves morphological ambiguity,
when possible.
To unify the results of Lemm and Synan, we
built a special ?wrapper,? (Du et al, 2011). The
wrapper takes every binary (syntactic) relation in
the Synan output, finds the items corresponding
to the relation?s parent and child in Lemm?s out-
put, and resolves their morphological ambiguity
(if any) by removing all other morphological read-
ings. If the lemma for parent or child is null?as,
e.g., when the corresponding element is a group?
we infer information from Lemm output for the
element that is missed in Synan. If a word does
not participate in any relation identified by Synan,
its analysis is based only on Lemm output, pre-
serving all unresolved morphological ambiguity?
to be potentially resolved at a later stage, typically
by scenario-specific patterns. Finally, the wrapper
assembles the resulting analysis for all words into
a set of tree fragments.
4 Russian Information Extraction
4.1 Ontology and Dictionaries
The ontology, a network of semantic classes, is
language-independent, and in Russian IE, we used
the pre-existing domain ontologies for the epi-
demics and security domains, with minor mod-
ifications. Most of the changes centered on re-
moving vestiges of English language-specific in-
formation, e.g., by making explicit the distinctions
among certain concepts that may be confounded
in English due to ambiguity of English lexical
units. For example, in English, the word ?convict?
means both the verb and the convicted person (pa-
tient nominalization), so it may be tempting to rep-
resent them by the same concept. In Russian, as in
many other languages, these are different concepts
as well as distinct lexemes.
A Russian domain-specific lexicon was added
to the system. Russian IE uses a shared lex-
icon for epidemics and security. The lexicon
contains not only translations of the correspond-
ing English words, but also includes MWEs that
appear in Russian media and correspond to the
domain-specific concepts. The current Russian
domain-specific lexicon contains approximately
1000 words and MWEs. Constructing the multi-
word lexicon for Russian is more complicated than
for English because Russian has a rich morphol-
ogy and complex grammatical agreement. For
example, to find a simple Adjective+Noun col-
location in text, the system needs to check that
the adjective agrees with the noun in gender,
case, and number. To resolve this problem, we
built a special set of low-level patterns, which
match MWEs. These patterns are subdivided
into several classes, according to their syntactic
form: Adjective+Noun, Noun+Noun, Verb+Noun,
Verb+Preposition+Noun, etc. The grammatical
constraints are coded only once for each class
of pattern, and apply to all patterns in the class.
For example, in the Noun+Noun class, the second
noun must be in genitive case (a genitive modifier
of the head noun), e.g., ?????? ??????? (cirrho-
sis of the liver), or in the instrumental case, e.g.,
????????? ??????? (human trafficking). This
simplifies adding new MWEs into the dictionary.
We use the multilingual GeoNames
database, (www.geonames.org) as the source
of geographic information in Russian. The
disease dictionary is mapped into Russian using
the International Classification of Diseases.8 The
system also identifies common animal diseases:
anthrax, African swine fever, rabies, etc.
4.2 Pattern Bases
The pattern base is the main component of the IE
system for extracting higher-level logical objects.
8ICD10: http://www.who.int/classifications/icd/en/
103
Syntactic variant Example Syntactic variant Example
I Verb + Object ?????????? ???????? II Object + Verb ???????? ??????????
(active clause) [someone] arrested a migrant (reverse word order) (same meaning)
III Participle + Object ????????? ??????? IV Object + Participle ??????? ?????????
(passive clause) migrant is arrested [by someone] (reverse word order) (same meaning)
V Noun + Object ????? ???????? VI (reverse word order is ?
(nominalization) arrest of a migrant rare, unlikely in news)
Table 1: Examples of syntactic variants for a single pattern Russian
Patterns are language-dependent and domain-
dependent, which means that patterns must cap-
ture the lexical, syntactic and stylistic features of
the analyzed text. It was not possible to directly
translate or map the English pattern base into Rus-
sian for at least two reasons.
The first reason is technical. PULS?s English
pattern base has over 150 patterns for the epi-
demics domain, and over 300 patterns for secu-
rity.9 These patterns were added to the system
through an elaborate pattern-acquisition process,
where semi-supervised pattern acquisition for En-
glish text was used, (Greenwood and Stevenson,
2006; Yangarber et al, 2000), to bootstrap many
pattern candidates from raw text based on a small
set of seed patterns; the candidates were sub-
sequently checked manually and included in the
system. Many of these patterns are typically in
?base-form?, i.e., simple active clauses; the En-
glish system takes each active-clause, ?subject-
verb-object? pattern, and generalizes it to multi-
ple syntactic variants, including passive clauses,
relative clauses, etc. Thus we created the Rus-
sian domain-specific patterns directly in PULS?s
pattern-specification language. A pattern consists
of a regular expression trigger and action code.
The second reason is theoretical. Unlike En-
glish, Russian is a heavily inflected, free word-
order language. In English, the active ?subject-
word-object? clause has only one form, whereas
in Russian all six permutations of the three el-
ements are possible, depending on the informa-
tion structure and pragmatic focus. This means
that we would need 6 pattern variants to match
a single active clause, and many more to process
other clausal types. The free word-order makes
it difficult to generate syntactic clausal variants;
it also complicates the bootstrapping of patterns
from seeds.
Therefore, for Russian we used a different strat-
9The difference is partly due to the fact that the security
scenario has several event types?illegal migration, human-
trafficking, smuggling, general crisis?and sub-types, while
epidemics deals with one event type.
egy, close to that used by (Tanev et al, 2009) for
Romance languages. In this approach, the patterns
first create ?shallow?, incomplete events where
only 1?2 slots are filled. Then, the inference rule
mechanism attempts to fill the remaining slots and
complete the events. The majority of Russian pat-
terns currently consist of two elements (such as
verb and object, or verb and subject), so that only
two word-order variants are possible. Currently,
the Russian patterns match five syntactic construc-
tions. These are listed in Table 1, along with ex-
amples from the security scenario. All example
phrases have the same meaning (?migrant was ar-
rested?) but different syntactic form. The active
clause and the passive clause in Russian may have
either V?O word order?types I and III?or O?
V,?types II and IV. The difference between the
active and the passive variants is in the grammati-
cal features only, which are marked by flexions.
Types I, III, and V in the table can be captured
by one simple pattern:
class(ARREST) noungroup(MIGRANT)
This pattern matches when a content phrase?
belonging to any part of speech (noun, verb,
or participle)?whose semantic head is the con-
cept ?ARREST? governs (i.e., in this case, pre-
cedes) a noun group headed by the concept ?MI-
GRANT?. The pattern primitives?class, noun-
group and others?build on top of the POS, syn-
tactic, and morphological tags that are returned by
the AOT wrapper. Types II and IV show variants
of the pattern in reverse order. Note that the pat-
terns use general ontology classes?shared with
English?rather than literal words.10
When a pattern fires, the system checks the con-
straints on grammatical features (e.g., case and
number agreement) on the matched phrases or
words. We introduce three types of constraints:
accusative object-case agreement for type I and
10NB: in practice, the patterns are more complex because
they allow various sentence modifiers to appear between verb
and object, which is a standard extension to this basic form
of the pattern.
104
Concept Event type
organ-transplant Human-Trafficking-Organs
border-guard Migration-Illegal-Entry
customs-officer Smuggling
Table 2: Examples of concepts found in context
that trigger rules to specialize the event type
II, for nominative subject-case agreement for type
III and IV, and and genitive-case nominalization
agreement for type V. If the constraints are satis-
fied, the event is created?that is, the same event
structure for any of the five pattern types.
For the security scenario the system currently
has 23 such ?basic? patterns. Most of them ini-
tially produce an event of a general class CRI-
SIS and fire when the text mentions that some-
one was arrested, sentenced, jailed, etc. If addi-
tional security-related concepts are found in text
nearby, inference rules will fill additional slots in
the event template, and specialize the type of the
event. The Russian security scenario uses exactly
the same set of inference rules as does the English
Security Scenario. Example rules are shown in Ta-
ble 2. For example, when an inference rule finds
in the context of an event a semantic concept that
is a sub-type of the type given in the left column,
the Type of the event is specialized to the corre-
sponding value in the right column, Table 2.
For the epidemics scenario, the system currently
uses only 7 patterns. Two produce an under-
specified event, when the text mentions that some-
one has become sick. The actual disease name is
again found by inference rules from nearby con-
text; if no disease is mentioned, the event is dis-
carded. Two additional patterns work ?in reverse?:
they match in cases when the text mentions an out-
break or case of a disease. Then the inference
rules try to find who is suffering from disease and
the number of victims. The inference rules are
again fully shared between English and Russian.
Some of the patterns are ?negative??they match
such statements as ?there is no threat of epidemic?,
which appear often in official reports.
In addition, the Russian pattern base contains
41 lower-level patterns, common for the security
and epidemics domains. These include, for exam-
ple, patterns to match date expressions, to analyze
collective-partitive noun groups (?a group of mi-
grants?, ?a team of doctors?, and so on), which
have general applicability.
Slot English system Russian system
rec pre F rec pre F
Event Type 67 72 69.41 70 57 62.83
Suspect 46 52 48.81 52 44 47.67
Total 27 71 46.47 44 37 40.20
Countries 56 55 55.49 48 40 43.63
Time 29 29 29.00 29 22 25.02
All 53 58 53.31 55 45 49.09
Table 3: Border Security scenario evaluation
English Russian
Event type test suite test suite
CRISIS 19 28
HUMAN-TRAFFICKING 4 4
ILLEGAL-MIGRATION 34 34
SMUGGLE 10 2
Total 67 68
Table 4: Distribution of event types in the test
suites for the Security scenario
5 Evaluation
5.1 Security
For evaluation, we used a test corpus of 64
Russian-language documents. Several assessors
annotated 65 events, and approximately one third
of the documents contained events. We compared
the Russian-language IE system with the English-
language system. The English test suite consists
of 50 documents with 70 events.
Evaluation results for the security domain are
presented in table 3, with scores given for the
main slots: Event Type (one of Migration, Human
Trafficking, Smuggling, and Crisis), Suspect, Total
(number of suspects), Countries (a list of one or
more countries involved in event), and Time (event
date). The table shows that currently the Rus-
sian system achieves a lower overall score than the
English system?the F-measure for all slots is 4?
5% lower, with precision being consistently lower
than recall for the Russian system.
Note that the development of a correct and
well-balanced test suite is in itself a challenging
task, and hence the evaluation numbers may be
biased. In the test suites used for these experi-
ments, shown in table 4, the English security sce-
nario includes more events of type SMUGGLE
than the Russian validation suite, and both vali-
dation suites contain few events of type HUMAN-
TRAFFICKING.
5.2 Epidemic Surveillance
For evaluation, we used a test corpus of 75 Rus-
sian documents. We asked several assessors to
105
Slot name English system Russian system
r p F r p F
Disease 74 74 74.00 93 81 86.58
Country 65 67 65.98 91 86 88.42
Total 68 79 73.09 30 78 43.33
Time 56 58 56.98 38 52 43.91
Status 77 75 75.99 93 81 86.58
All Slots 68 69 68.83 70 71 70.44
Table 5: Epidemics scenario evaluation.
correct events found by the system and add miss-
ing events in case they were not found by sys-
tem. Assessors annotated 120 events. We compare
the Russian-language IE system with the English-
language system. The PULS English validation
suite for Epidemics currently consists of 60 docu-
ments with 172 events.
Evaluation results are shown in table 5, where
the scores are given for the main slots: Dis-
ease, Country, Total (number of victims), Status
(?dead? or ?sick?) and Time. Results for the Rus-
sian system are somewhat better than for English.
This is due in part to the bias in the process which
we used to select documents for the test suite: the
assessors marked documents in which the system
found events, rather than searching and annotating
documents from scratch. (This aspect of the evalu-
ation will be corrected in future work.) The events
that the system found could be relevant, spurious,
or erroneous; in case the system missed an event,
the assessor?s job was to add it to the gold-standard
answers. Note that in general the amount of irrele-
vant documents processed by PULS is much larger
than the amount of relevant documents (only about
1% of all documents that contain keywords rele-
vant to epidemics contain useful events). Thus it
is impractical to ask assessors to read raw docu-
ments. As a consequence, the scores for the main
slots, such as Disease or Country, may be over-
stated: the majority of documents mention only
one disease, and since an event was found by the
system in most documents selected for the test
suite, the Disease slot is usually filled correctly.
The results for the auxiliary slots, e.g., Time, To-
tal, are closer to our expectation.
5.3 Comparison of Languages and Scenarios
In general, the epidemics scenario performs much
better than security, both in Russian and English.
This is due to fact that the task definition for epi-
demics is simpler, better formalized, and deals
with one type of event only. As noted in (Hut-
Event Type English Russian
Epidemic Surveillance
DISEASE 31 5
HARM 825 412
Total 856 417
Border Security
CRISIS 694 476
HUMAN-TRAFFICKING 10 12
ILLEGAL-MIGRATION 32 31
SMUGGLE 7 19
Total 743 538
Table 6: Number of events found by IE systems in
parallel English-Russian news corpus.
tunen et al, 2002), event representation in text
may have different structure depending on the sce-
nario: the ?classic? IE scenarios, such as the MUC
Management Succession or Terror Attacks, de-
scribe events that occur at a specific point in time,
whereas other scenarios, such as Natural Disasters
or Disease Outbreaks describe a process that is
spread out in time and space. Consequently, events
in the latter (?nature?) scenarios are more com-
plex, may have hierarchical structure, and may
even overlap in text. From the theoretical point of
view it would be interesting to compare how the
events representation, (Pivovarova et al, 2013),
differs in different languages. Moreover, such dif-
ferences can be important in cross-language infor-
mation summarization, (Ji et al, 2013).
We use a freely-available comparable news cor-
pus, (Klementiev and Roth, 2006), to investigate
the difference of event representation in English
and Russian. The corpus contains 2327 BBC mes-
sages from the time period from 1 January 2001 to
10 May 2005, and their approximate translations
from the Lenta.ru website; the translations may be
quite different from their English sources and are
stylistically similar to standard Russian news. We
processed the corpora with the security and epi-
demics IE systems, using the respective language;
the results are presented in the Table 6.
The table shows that for both scenarios the En-
glish system finds more events than the Russian,
which probably means that coverage of the Rus-
sian IE is lower. We have yet to conduct a thor-
ough evaluation of the events found. It is also clear
from the table that specific events are much more
rare than general events; for the security scenario,
the majority of events have type CRISIS, which is
a general type that indicates some incident related
to crime; in the epidemics scenario, the majority
of events have type HARM, i.e., which is a gen-
106
Figure 3: Monthly frequency of events for the four
top-reported diseases in Russia
eral type indicating that there are victims (e.g., hu-
mans) suffering from some cause, not only harm
caused by infections. The distributions of event
types are similar in English and Russian corpora,
which may hint that a lack of specific events may
be a property of the scenarios, irrespective of the
language. This agrees with the expectation that the
majority of retrieved documents are not relevant.
6 Discussion
The Russian-language processing pipeline pre-
sented above is compatible with the working, pre-
existing PULS IE system. It is worth noting again,
that the output of the Russian-language analysis
has the same form as that of the English-language
PULS event extraction, that is, all fills for the tem-
plate slots are output in English (except in the case
of person names). This is made possible by the
shared, language-independent ontology. An im-
portant benefit of this sharing is that the end-user
is not required to understand Russian in order to
determine whether the extracted facts and docu-
ments are relevant to her/his need. Thus, the slot
fills may be presented in English, as shown in Fig-
ure 1. The document text, however, may be pre-
sented in Russian; users who can read Russian can
see the original article text where event elements
are indicated (by highlighting or underlining).
Figure 2 shows a summary-style list of events
found from the news stream. The user can see
events extracted from documents in a mix of lan-
guages (identified by the language tag in the left-
most column). The database representation for
events is shared and independent of the language;
this permits the user get a grasp of current situa-
tion in the domain of interest, in more than one
language.
We checked the impact of the Russian compo-
nent on the system?s coverage over the geographic
area of the former USSR, which includes regions
(outside Russia) where Russian may be used as
a lingua franca, and may be common in press.
Figure 3 shows the total number of events found
in Russia, using both the Russian- and English-
language IE systems for the four most frequently
reported diseases. The check was conducted on
news streams over 2011?2012. The number of
events increases dramatically after deploying the
Russian component, at the end of 2011 (near the
middle of the timeline).
6.1 Conclusion
We have presented a ?plug-in? extension to PULS,
an English-language IE system, to cover Russian-
language text. We currently handle two scenarios:
Security and Epidemic Surveillance. The amount
of effort needed to develop the Russian component
was modest compared to the time and labour spent
on the English-language IE system. The Russian
system demonstrates a comparable level of per-
formance to the baseline English IE: F-measure
is about 4% lower for the Security scenario and
2% higher for the Epidemic Surveillance. We be-
lieve that this success is due to two main factors:
first, the re-use of as many existing modules and
knowledge bases as possible from the pre-existing
English-language system; second, the use of shal-
low, permissive patterns in Russian in combination
with logical inference rules.
In future research, we plan to further expand the
pattern sets and lexicons, to analyze more kinds of
syntactic and lexical phenomena in Russian. We
plan to compare structural differences between the
Security and Epidemics scenarios and their repre-
sentation in Russian and English, to find language-
dependent and language-independent features of
the event representations. We plan to use cross-
lingual analysis to obtain advances in two direc-
tions: first, pre-IE automatic pattern and para-
phrase acquisition for free-word-order languages;
second, post-IE aggregation of extracted informa-
tion to improve overall quality by use of cross-
document context, (Chen and Ji, 2009; Yangarber
and Jokipii, 2005; Yangarber, 2006).
Acknowledgements
We thank Peter von Etter and Mikhail Novikov for
help with the implementation; students of the De-
partment of Information Systems, St. Petersburg
State University, for annotating evaluation data.
Work was funded in part by Frontex, Project on
Automated Event Extraction, and the ALGODAN
Center of Excellence of the Academy of Finland.
107
Figure 1: Document view and template view: a Smuggling event from the Security domain
Figure 2: Summary view: a list of events in the Security domain. The tool-tip under the mouse shows a
snippet of the original text, from which the event was extracted.
References
Ameyugo, G., Art, M., Esteves, A. S., and Piskorski,
J. (2012). Creation of an EU-level information ex-
change network in the domain of border security.
In European Intelligence and Security Informatics
Conference (EISIC). IEEE.
Astaf?eva, I., Bonch-Osmolovskaya, A., Garejshina,
A., Grishina, J., D?jachkov, V., Ionov, M., Korol-
eva, A., Kudrinsky, M., Lityagina, A., Luchina,
E., Sidorova, E., Toldova, S., Lyashevskaya, O.,
Savchuk, S., and Koval?, S. (2010). NLP evaluation:
Russian morphological parsers. In Proceedings of
Dialog Conference, Moscow, Russia.
Atkinson, M., Piskorski, J., van der Goot, E., and Yan-
garber, R. (2011). Multilingual real-time event ex-
traction for border security intelligence gathering.
In Wiil, U. K., editor, Counterterrorism and Open
Source Intelligence, pages 355?390. Springer Lec-
ture Notes in Social Networks, Vol. 2.
Bocharov, V., Pivovarova, L., Rubashkin, V., and
Chuprin, B. (2010). Ontological parsing of encyclo-
pedia information. Computational Linguistics and
Intelligent Text Processing.
Bontcheva, K., Maynard, D., Tablan, V., and Cunning-
ham, H. (2003). GATE: A Unicode-based infras-
tructure supporting multilingual information extrac-
tion. In Proceedings of Workshop on Information
Extraction for Slavonic and other Central and East-
ern European Languages, Borovets, Bulgaria.
Chen, Z. and Ji, H. (2009). Can one language boot-
strap the other: a case study on event extraction. In
Proceedings of the NAACL-HLT Workshop on Semi-
Supervised Learning for Natural Language Process-
ing.
Collier, N., Doan, S., Kawazoe, A., Goodwin, R. M.,
Conway, M., Tateno, Y., Ngo, Q.-H., Dien, D.,
Kawtrakul, A., Takeuchi, K., Shigematsu, M., and
Taniguchi, K. (2008). BioCaster: detecting public
health rumors with a Web-based text mining system.
Bioinformatics, 24(24).
Du, M., von Etter, P., Kopotev, M., Novikov, M., Tar-
beeva, N., and Yangarber, R. (2011). Building sup-
108
port tools for Russian-language information extrac-
tion. In Habernal, I. and Matous?ek, V., editors, Text,
Speech and Dialogue, volume 6836 of Lecture Notes
in Computer Science. Springer Berlin / Heidelberg.
Greenwood, M. and Stevenson, M. (2006). Improv-
ing semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of Workshop on Infor-
mation Extraction Beyond The Document, COLING-
ACL, volume 3808, pages 29?35. Springer, Lecture
Notes in Artificial Intelligence, Sydney, Australia.
Huttunen, S., Yangarber, R., and Grishman, R. (2002).
Diversity of scenarios in information extraction.
In Proceedings of the Third International Confer-
ence on Language Resources and Evaluation (LREC
2002), Las Palmas de Gran Canaria, Spain.
Ji, H., Favre, B., Lin, W.-P., Gillick, D., Hakkani-
Tur, D., and Grishman, R. (2013). Open-domain
multi-document summarization via information ex-
traction: Challenges and prospects. In Multi-source,
Multilingual Information Extraction and Summa-
rization. Springer.
Khoroshevsky, V. F. (2010). Ontology driven multilin-
gual information extraction and intelligent analytics.
In Web Intelligence and Security: Advances in Data
and Text Mining Techniques for Detecting and Pre-
venting Terrorist Activities on the Web. IOS Press.
Klementiev, A. and Roth, D. (2006). Weakly su-
pervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, Syd-
ney, Australia.
Piskorski, J., Belyaeva, J., and Atkinson, M. (2011).
Exploring the usefulness of cross-lingual informa-
tion fusion for refining real-time news event ex-
traction: A preliminary study. In Proceedings of
RANLP: 8th Conference on Recent Advances in Nat-
ural Language Processing, Hissar, Bulgaria.
Pivovarova, L., Huttunen, S., and Yangarber, R. (2013).
Event representation across genre. In Proceedins of
the 1st Workshop on Events: Definition, Detection,
Coreference, and Representation, NAACL HLT, At-
lanta, Georgia.
Rortais, A., Belyaeva, J., Gemo, M., van der Goot, E.,
and Linge, J. P. (2010). Medisys: An early-warning
system for the detection of (re-)emerging food- and
feed-borne hazards. Food Research International,
43(5):1553?1556.
Schumann, A.-K. (2012). Towards the automated en-
richment of multilingual terminology databases with
knowledge-rich contexts?experiments with russian
eurotermbank data. In CHAT 2012: The Second
Workshop on Creation, Harmonization and Applica-
tion of Terminology Resources, Madrid, Spain.
Sokirko, A. (2001). Semantic dictionaries in automatic
text analysis, based on DIALING system materials.
PhD thesis, Russian State University for the Human-
ities, Moscow.
Solovyev, V., Ivanov, V., Gareev, R., Serebryakov,
S., and Vassilieva, N. (2012). Methodology for
building extraction templates for Russian language
in knowledge-based IE systems. Technical Report
HPL-2012-211, HP Laboratories.
Tanev, H., Zavarella, V., Linge, J., Kabadjov, M.,
Piskorski, J., Atkinson, M., and Steinberger, R.
(2009). Exploiting machine learning techniques to
build an event extraction system for Portuguese and
Spanish. Linguamatica, 2.
Toldova, S. J., Sokolova, E. G., Astaf?eva, I.,
Gareyshina, A., Koroleva, A., Privoznov, D.,
Sidorova, E., Tupikina, L., and Lyashevskaya, O. N.
(2012). NLP evaluation 2011?2012: Russian syn-
tactic parsers. In Proceedings of Dialog Conference,
Moscow, Russia.
Yangarber, R. (2006). Verification of facts across doc-
ument boundaries. In Proceedings of the Interna-
tional Workshop on Intelligent Information Access
(IIIA-2006), Helsinki, Finland.
Yangarber, R., Grishman, R., Tapanainen, P., and Hut-
tunen, S. (2000). Automatic acquisition of do-
main knowledge for information extraction. In
Proceedings of the 18th International Conference
on Computational Linguistics (COLING 2000),
Saarbru?cken, Germany.
Yangarber, R. and Jokipii, L. (2005). Redundancy-
based correction of automatically extracted facts. In
Proceedings of HLT-EMNLP: Conference on Empir-
ical Methods in Natural Language Processing, Van-
couver, Canada.
Yarowsky, D. (1995). Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, Cambridge,
MA. ACM Press.
Zamite, J., Silva, F., Couto, F., and Silva, M. (2010).
MEDCollector: Multisource epidemic data collec-
tor. In Khuri, S., Lhotska?, L., and Pisanti, N., ed-
itors, Information Technology in Bio- and Medical
Informatics, ITBAM 2010. Springer Berlin.
109
