Learning Effective Surface Text Patterns
for Information Extraction
Gijs Geleijnse and Jan Korst
Philips Research Laboratories
Prof. Holstlaan 4, 5656 AA Eindhoven, The Netherlands
{gijs.geleijnse,jan.korst}@philips.com
Abstract
We present a novel method to identify ef-
fective surface text patterns using an inter-
net search engine. Precision is only one
of the criteria to identify the most effec-
tive patterns among the candidates found.
Another aspect is frequency of occurrence.
Also, a pattern has to relate diverse in-
stances if it expresses a non-functional re-
lation. The learned surface text patterns
are applied in an ontology population al-
gorithm, which not only learns new in-
stances of classes but also new instance-
pairs of relations. We present some ?rst
experiments with these methods.
1 Introduction
Ravichandran and Hovy (2002) present a method
to automatically learn surface text patterns ex-
pressing relations between instances of classes us-
ing a search engine. Their method, based on
a training set, identi?es natural language surface
text patterns that express some relation between
two instances. For example, ?was born in? proved
to be a precise pattern expressing the relation be-
tween instances Mozart (of class ?person?) and
1756 (of class ?year?).
We address the issue of learning surface text
patterns, since we observed two drawbacks of
Ravichandran and Hovy?s work with respect to the
application of such patterns in a general informa-
tion extraction setting.
The ?rst drawback is that Ravichandran and
Hovy focus on the use of such surface text patterns
to answer so-called factoid questions (Voorhees,
2004). They use the assumption that each instance
is related by R to exactly one other instance of
some class. In a general information extraction
setting, we cannot assume that all relations are
functional.
The second drawback is that the criterion for se-
lecting patterns, precision, is not the only issue for
a pattern to be effective. We call a pattern effec-
tive, if it links many different instance-pairs in the
excerpts found with a search engine.
We use an ontology to model the information
domain we are interested in. Our goal is to pop-
ulate an ontology with the information extracted.
In an ontology, instances of one class can be re-
lated by some relation R to multiple instances of
some other class. For example, we can identify
the classes ?movie? and ?actor? and the ?acts in?-
relation, which is a many-to-many relation. In
general, multiple actors star in a single movie and
a single actor stars in multiple movies.
In this paper we present a domain-independent
method to learn effective surface text patterns rep-
resenting relations. Since not all patterns found
are highly usable, we formulate criteria to select
the most effective ones. We show how such pat-
terns can be used to populate an ontology.
The identi?cation of effective patterns is impor-
tant, since we want to perform as few queries to
a search engine as possible to limit the use of its
services.
This paper is organized as follows. After de?n-
ing the problem (Section 2) and discussing related
work (Section 3), we present an algorithm to learn
effective surface text patterns in Section 4. We dis-
cuss the application of this method in an ontology
population algorithm in Section 5. In Section 6,
we present some of our early experiments. Sec-
tions 7 and 8 handle conclusions and future work.
1
2 Problem description
We consider two classes cq and ca and the corre-
sponding non-empty sets of instances Iq and Ia.
Elements in the sets Iq and Ia are instances of cq
and ca respectively, and are known to us before-
hand. However, the sets I do not have to be com-
plete, i.e. not all possible instances of the corre-
sponding class have to be in the set I .
Moreover, we consider some rela-
tion R between these classes and give a
non-empty training set of instance-pairs
TR = {(x, y) | x ? Iq ? y ? Ia}, which
are instance-pairs that are known to be R-related.
Problem: Given the classes cq and ca, the sets
of instances Iq and Ia, a relation R and a set
of R-related instance-pairs TR, learn effective
surface text patterns that express the relation R.
Say, for example, we consider the classes ?au-
thor? and ?book title? and the relation ?has written?.
We assume that we know some related instance-
pairs , e.g. (?Leo Tolstoy?, ?War and Peace?) and
(?Gu?nter Grass?, ?Die Blechtrommel?). We then
want to ?nd natural language phrases that relate
authors to the titles of the books they wrote. Thus,
if we query a pattern in combination with the name
of an author (e.g. ?Umberto Eco wrote?), we want
the search results of this query to contain the books
by this author.
The population of an ontology can be seen as
a generalization of a question-answering setting.
Unlike question-answering, we are interested in
?nding all possible instance-pairs, not only the
pairs with one ?xed instance (e.g. all ?author?-
?book? pairs instead of only the pairs containing
a ?xed author). Functional relations in an ontol-
ogy correspond to factoid questions, e.g. the pop-
ulation of the classes ?person? and ?country? and
the ?was born in?-relation. Non-functional rela-
tions can be used to identify answers to list ques-
tions, for example ?name all books written by
Louis-Ferdinand Ce?line? or ?which countries bor-
der Germany??.
3 Related work
Brin identi?es the use of patterns in the discovery
of relations on the web (Brin, 1998). He describes
a website-dependent approach to identify hyper-
text patterns that express some relation. For each
web site, such patterns are learned and explored
to identify instances that are similarly related. In
(Agichtein and Gravano, 2000), such a system is
combined with a named-entity recognizer.
In (Craven et al, 2000) an ontology is popu-
lated by crawling a website. Based on tagged web
pages from other sites, rules are learned to extract
information from the website.
Research on named-entity recognition was ad-
dressed in the nineties at the Message Understand-
ing Conferences (Chinchor, 1998) and is contin-
ued for example in (Zhou and Su, 2002).
Automated part of speech tagging (Brill, 1992)
is a useful technique in term extraction (Frantzi
et al, 2000), a domain closely related to named-
entity recognition. Here, terms are extracted
with a prede?ned part-of-speech structure, e.g. an
adjective-noun combination. In (Nenadic? et al,
2002), methods are discussed to extract informa-
tion from natural language texts with the use of
both part of speech tags and hyponym patterns.
As referred to in the introduction, Ravichandran
and Hovy (2002) present a method to identify sur-
face text patterns using a web search engine. They
extract patterns expressing functional relations in
a factoid question answering setting. Selection of
the extracted patterns is based on the precision of
the patterns. For example, if the pattern ?was born
in? is identi?ed as a pattern for the pair (?Mozart?,
?Salzburg?), they compute precision as the num-
ber of excerpts containing ?Mozart was born in
Salzburg? divided by the number of excerpts with
?Mozart was born in?.
Information extraction and ontologies creation
are two closely related ?elds. For reliable informa-
tion extraction, we need background information,
e.g. an ontology. On the other hand, we need in-
formation extraction to generate broad and highly
usable ontologies. An overview on ontology learn-
ing from text can be found in (Buitelaar et al,
2005).
Early work (Hearst, 1998), describes the extrac-
tion of text patterns expressing WordNet-relations
(such as hyponym relations) from some corpus.
This work focusses merely on the identi?cation of
such text patterns (i.e. phrases containing both in-
stances of some related pair). Patterns found by
multiple pairs are suggested to be usable patterns.
KnowItAll is a hybrid named-entity extraction
system (Etzioni et al, 2005) that ?nds lists of in-
stances of some class from the web using a search
engine. It combines Hearst patterns and learned
2
patterns for instances of some class to identify and
extract named-entities. Moreover, it uses adaptive
wrapper algorithms (Crescenzi and Mecca, 2004)
to extract information from html markup such as
tables.
Cimiano and Staab descibe a method to use
a search engine to verify a hypothesis relation
(2004). For example, if we are interested in the ?is
a? or hyponym relation and we have a candidate in-
stance pair (?river?, ?Nile?) for this relation, we can
use a search engine to query phrases expressing
this relation (e.g. ?rivers such as the Nile?). The
number of hits to such queries can then be used as
a measure to determine the validity of the hypoth-
esis.
In (Geleijnse and Korst, 2005), a method is de-
scribed to populate an ontology with the use of
queried text patterns. The algorithm presented ex-
tracts instances from search results after having
submitted a combination of an instance and a pat-
tern as a query to a search engine. The extracted
instances from the retrieved excerpts can there-
after be used to formulate new queries ? and thus
identify and extract other instances.
4 The algorithm
We present an algorithm to learn surface text pat-
terns for relations. We use GoogleTM to retrieve
such patterns.
The algorithm makes use of a training set TR
of instance-pairs that are R-related. This training
set should be chosen such the instance-pairs are
typical for relation R.
We ?rst discover how relation R is expressed
in natural language texts on the web (Section 4.1).
In Section 4.2 we address the problem of select-
ing effective patterns from the total set of patterns
found.
4.1 Identifying relation patterns
We ?rst generate a list of surface text patterns with
the use of the following algorithm. For evaluation
purposes, we also compute the frequency of each
pattern found.
- Step 1: Formulate queries using an instance-
pair (x, y) ? TR. Since we are interested in
phrases within sentences rather than in key-
words or expressions in telegram style that
often appear in titles of webpages, we use
the allintext: option. This gives us only
search results with the queried expression in
the bodies of the documents rather than in the
titles. We query both allintext:" x *
y " and allintext:" y * x ". The *
is a regular expression operator accepted by
Google. It is a placeholder for zero or more
words.
- Step 2: Send the queries to Google and col-
lect the excerpts of the at most 1,000 pages it
returns for each query.
- Step 3: Extract all phrases matching the
queried expressions and replace both x and
y by the names of their classes.
- Step 4: Remove all phrases that are not
within one sentence.
- Step 5: Normalize all phrases by removing
all mark-up that is ignored by Google. Since
Google is case-insensitive and ignores punc-
tuation, double spaces and the like, we trans-
late all phrases found to a normal form: the
simplest expression that we can query that
leads to the document retrieved.
- Step 6: Update the frequencies of all normal-
ized phrases found.
- Step 7: Repeat the procedure for any un-
queried pair (x?, y?) ? TR.
We now have generated a list with relation pat-
terns and their frequencies within the retrieved
Google excerpts.
4.2 Selecting relation patterns
From the list of relation patterns found, we are in-
terested in the most effective ones.
We are not only interested in the most precise
ones. For example, the retrieved pattern ?fo?dd 30
mars 1853 i? proved to a 100% precise pattern
expressing the relation between a person (?Vin-
cent van Gogh?) and his place of birth (?Zun-
dert?). Clearly, this rare phrase is unsuited to mine
instance-pairs of this relation in general. On the
other hand, high frequency of some pattern is no
guarantee for effectiveness either. The frequently
occurring pattern ?was born in London? (found
when querying for Thomas Bayes * England)
is well-suited to be used to ?nd London-born per-
sons, but in general the pattern is unsuited ? since
too narrow ? to express the relation between a per-
son and his or her country of origin.
3
Taking these observations into account, we for-
mulate three criteria for selecting effective relation
patterns.
1. The patterns should frequently occur on the
web, to increase the probability of getting any
results when querying the pattern in combi-
nation with an instance.
2. The pattern should be precise. When we
query a pattern in combination with an in-
stance in Iq, we want to have many search
results containing instances from ca.
3. If relation R is not functional, the pattern
should be wide-spread, i.e. among the search
results when querying a combination of the
pattern and an instance in Iq there must be as
many distinct R-related instances from ca as
possible.
To measure these criteria, we use the following
scoring functions for relation patterns s.
1. ffreq(s) = ?number of occurrences of s in
the excerpts as found by the algorithm de-
scribed in the previous subsection?
2. fprec(s) =
?
x?I?q
P (s,x)
|I?q | , where
for instances x ? I ?q, I ?q ? Iq , we calculate
P (s, x) as follows.
P (s, x) = FI(s,x)FO(s,x)
and
FI(s, x) = the number of Google excerpts
after querying s in combination with x
containing instances of ca.
FO(s, x) = the total number of excerpts
found (at most 1,000).
3. fspr(s) =
?
x?I?q B(s, x), where
B(s, x) = the number of distinct instances
of class ca found after querying pattern s in
combination with x.
The larger we choose the testset, the subset
I ?q of Iq, the more reliable the measures for pre-
cision and spreading. However, the number of
Google queries increases with the number of pat-
terns found for each instance we add to I ?q.
We ?nally calculate the score of the patterns by
multiplying the individual scores:
score(s) = ffreq(s) ? fprec(s) ? fspr(s)
For ef?ciency reasons, we only compute the
scores of the patterns with the highest frequencies.
The problem remains how to recognize a (pos-
sible multi-word) instance in the Google excerpts.
For an ontology alignment setting ? where the sets
Ia and Iq are not to be expanded ? these problems
are trivial: we determine whether t ? Ia is accom-
panied by the queried expression. For a setting
where the instances of ca are not all known (e.g.
it is not likely that we have a complete list of all
books written in the world), we solve this problem
in two stages. First we identify rules per class to
extract candidate instances. Thereafter we use an
additional Google query to verify if a candidate is
indeed an instance of class ca.
Identifying a candidate instance
The identi?cation of multi-word terms is an is-
sue of research on its own. However, in this setting
we can allow ourselves to use less elaborate tech-
niques to identify candidate instances. We can do
so, since we additionally perform a check on each
extracted term. So, per class we create rules to
identify candidate instances with a focus on high
recall. In our current experiments we thus use very
simple term recognition rules, based on regular ex-
pressions. For example, we identify a candidate
instance of class ?person? if the queried expression
is accompanied by two or three capitalized words.
Identifying an instance-class relation
We are interested in the question whether some
extracted term t is an instance of class ca. For ex-
ample, given the term ?The Godfather?, does this
term belong to the class ?movie?? The instance-
class relation can be viewed of as a hyponym re-
lation. We therefore verify the hypothesis of t be-
ing an instance of ca by Googling hyponym rela-
tion patterns. We use a ?xed set H of common
patterns expressing the hyponym relation (Hearst,
1992; Cimiano and Staab, 2004), see Table 1. For
the class names, we use plurals.
We use these patterns in the following accep-
tance function
acceptcq(t) := (
?
p?H
h(p, cq, t) ? n),
4
"cq including t and"
"cq for example t and"
"cq like t and"
"cq such as t and"
Table 1: Hearst patterns for instance-class relation.
where h(p, cq, t) is the number of Google hits for
query with pattern p combined with term t and the
plural form of the class name cq. The threshold
n has to be chosen beforehand. We can do so, by
calculating the sum of Google hits for queries with
known instances of the class. Based on these ?g-
ures, a threshold can be chosen e.g. the minimum
of these sums.
Note that term t is both preceded and followed
by a ?xed phrase in the queries. We do so, to
guarantee that t is indeed the full term we are in-
terested in. For example, if we had extracted the
term ?Los? instead of ?Los Angeles? as a Califor-
nian City, we would falsely identify ?Los? as a Cal-
ifornian City, when we do not let ?Los? follow by
the ?xed expression and. The number of Google
hits for some expression x is at least the number
of Google hits when querying the same expression
followed by some expression y.
If we identify a term t as being an instance of
class ca, we can add this term to the set Ia. How-
ever, we cannot relate t to an instance in Iq, since
the pattern used to ?nd t has not proven to be effec-
tive yet (e.g. the pattern could express a different
relation between one of the instance-pairs in the
training set).
We reduce the amount of Google queries by us-
ing a list of terms found that do not belong to ca.
Terms that occur multiple times in the excerpts
can then be checked only once. Moreover, we use
the OR-clause to combine the individual queries
into one. We then check if the number of hits
to this query exceeds the threshold. The amount
of Google queries in this phase thus equals the
amount of distinct terms extracted.
5 The use of surface text patterns in
information extraction
Having a method to identify relation patterns, we
now focus on utilizing these patterns in informa-
tion extraction from texts found by a search en-
gine. We use an ontology to represent the infor-
mation extracted.
Suppose we have an ontology O with classes
(c1, c2, ...) and corresponding instance sets
(I1, I2, ..). On these classes, relations R(i,j)1
are de?ned, with i and j the index number of
the classes. The non-empty sets T(i,j) contain
the training set of instance-pairs of the relations
R(i,j).
Per instance, we maintain a list of expressions
that already have been used as a query. Initially,
these are empty.
The ?rst step of the algorithm is to learn surface
text patterns for each relation in O.
The following steps of the algorithm are per-
formed until either some stop criterion is reached,
or no more new instances and instance-pairs can
be found.
- Step 1: Select a relation R(i,j), and an in-
stance v from either Ii or Ij such that there
exists at least one pattern expressing R(i,j)
we have not yet queried in combination with
v.
- Step 2: Construct queries using the patterns
with v and send these queries to Google.
- Step 3: Extract instances from the excerpts.
- Step 4: Add the newly found instances to
the corresponding instance set and add the
instance-pairs found (thus with v) to T(i,j).
- Step 5: If there exists an instance that we can
use to formulate new queries, then repeat the
procedure.
Else, learn new patterns using the extracted
instance-pairs and then repeat the procedure.
Note that instances of class cx learned using the
algorithm applied on relation R(x,y) can be used
as input for the algorithm applied to some relation
R(x,z) to populate the sets Iz and T(x,z).
6 Experiments
In this section, we discuss two experiments that we
have conducted. The ?rst experiment involves the
identi?cation of effective hyponym patterns. The
second experiment is an illustration of the applica-
tion of learned surface text patterns in information
extraction.
1Assuming one relation per pair of classes. We can use
another index k in R(i,j,k) to distinct multiple relations be-
tween ci and cj .
5
6.1 Learning effective hyponym patterns
We are interested whether the effective surface text
patterns are indeed intuitive formulations of some
relation R. As a test-case, we compute the most
effective patterns for the hyponym relation using a
test set with names of all countries.
Our experiment was set up as follows. We col-
lected the complete list of countries in the world
from the CIA World Factbook2. Let Iq be this set
of countries, and let Ia be the set { ?countries?,
?country? }. The set TR consists of all pairs (a,
?countries?) and (a, ?country?) , for a ? Ia. We
apply the surface text pattern learning algorithm
on this set TR.
The algorithm identi?ed almost 40,000 patterns.
We computed fspr and fprec for the 1,000 most
frequently found patterns. In table 2, we give the
25 most effective patterns found by the algorithm.
We consider the patterns in boldface true hyponym
patterns. Focussing on these patterns, we observe
two groups: ?is a? and Hearst-like patterns.
pattern freq prec spr
(countries) like 645 0.66 134
(countries) such as 537 0.54 126
is a small (country) 142 0.69 110
(country) code for 342 0.36 84
(country) map of 345 0.34 78
(countries) including 430 0.21 93
is the only (country) 138 0.55 102
is a (country) 339 0.22 99
(country) ?ag of 251 0.63 46
and other (countries) 279 0.34 72
and neighboring (countries) 164 0.43 92
(country) name republic of 83 0.93 76
(country) book of 59 0.77 118
is a poor (country) 63 0.73 106
is the ?rst (country) 53 0.70 112
(countries) except 146 0.37 76
(country) code for calling 157 0.95 26
is an independent (country) 62 0.55 114
and surrounding (countries) 84 0.40 107
is one of the poorest (countries) 61 0.75 78
and several other (countries) 65 0.59 90
among other (countries) 84 0.38 97
is a sovereign (country) 48 0.69 89
or any other (countries) 87 0.58 58
(countries) namely 58 0.44 109
Table 2: Learned hyponym patterns and their
scores.
The Hearst-patterns ?like? and ?such as? show to
be the most effective. This observation is useful,
when we want to minimize the amount of queries
for hyponym patterns.
Expressions of properties that hold for each
2http://www.cia.gov/cia/publications/factbook
country and only for countries, for example the ex-
istence of a country code for dialing, are not triv-
ially identi?ed manually but are useful and reliable
patterns.
The combination of ?is a?, ?is an? or ?is the? with
an adjective is a common pattern, occurring 2,400
times in the list. In future work, we plan to identify
such adjectives in Google excerpts using a Part of
Speech tagger (Brill, 1992).
6.2 Applying learned patterns in information
extraction
The Text Retrieval Conference (TREC) question
answering track in 2004 contains list question,
for example ?Who are Nirvana?s band members??
(Voorhees, 2004). We illustrate the use of our on-
tology population algorithm in the context of such
list-question answering with a small case-study.
Note that we do not consider the processing of the
question itself in this research.
Inspired by one of the questions (?What coun-
tries is Burger King located in??), we are interested
in populating an ontology with restaurants and the
countries in which they operate. We identify the
classes ?country? and ?restaurant? and the relation
?located in? between the classes.
We hand the algorithm the instances of ?coun-
try?, as well as two instances of ?restaurant?: ?Mc-
Donald?s? and ?KFC?. Moreover, we add three
instance-pairs of the relation to the algorithm. We
use these pairs and a subset I ?country of size eight
to compute a ranked list of the patterns. We ex-
tract terms consisting of one up to four capital-
ized words. In this test we set the threshold for
the number of Google results for the queries with
the extracted terms to 50. After a small test with
names of international restaurant branches, this
seemed an appropriate threshold.
The algorithm learned, besides a ranked list of
170 surface text patterns (Table 3), a list of 54 in-
stances of restaurant (Table 4). Among these in-
stances are indeed the names of large international
chains, Burger King being one of them. Less
expected are the names of geographic locations
and names of famous cuisines such as ?Chinese?
and ?French?. The last category of false instances
found that have not be ?ltered out, are a number of
very common words (e.g. ?It? and ?There?).
We populate the ontology with relations found
between Burger King and instances from country
using the 20 most effective patterns.
6
pattern prec spr freq
ca restaurants of cq 0.24 15 21
ca restaurants in cq 0.07 19 9
ca hamburger chain that occupies
villages throughout modern day cq 1.0 1 7
ca restaurant in cq 0.06 16 6
ca restaurants in the cq 0.13 16 2
ca hamburger restaurant in southern cq 1.0 1 4
Table 3: Top learned patterns for the restaurant-
country (ca - cq) relation.
Chinese Bank Outback Steakhouse
Denny?s Pizza Hut Kentucky Fried Chicken
Subway Taco Bell Continental
Holywood Wendy?s Long John Silver?s
HOTEL OR This Burger King
Japanese West Keg Steakhouse
You BP Outback
World Brazil San Francisco
Leo Victoria New York
These Lyons Starbucks
FELIX Roy California Pizza Kitchen
Marks Cities Emperor
Friendly Harvest Friday
New York Vienna Montana
Louis XV Greens Red Lobster
Good It There
That Mark Dunkin Donuts
Italia French Tim Hortons
Table 4: Learned instances for restaurant.
The algorithm returned 69 instance-pairs with
countries related to ?Burger King?. On the Burger
King website3 a list of the 65 countries can be
found in which the hamburger chain operates. Of
these 65 countries, we identi?ed 55. This implies
that our results have a precision of 5569 = 80% and
recall of 5565 = 85%. Many of the falsely related
countries ? mostly in eastern Europe ? are loca-
tions where Burger King is said to have plans to
expand its empire.
7 Conclusions
We have presented a novel approach to identify
useful surface text patterns for information extrac-
tion using an internet search engine. We argued
that the selection of patterns has to be based on
effectiveness: a pattern has to occur frequently, it
has to be precise and has to be wide-spread if it
represents a non-functional relation.
These criteria are combined in a scoring func-
tion which we use to select the most effective pat-
terns.
3http://www.whopper.com
The method presented can be used for arbitrary
relations, thus also relations that link an instance
to multiple other instances. These patterns can be
used in information extraction. We combine pat-
terns with an instance and offer such an expression
as a query to a search engine. From the excerpts
retrieved, we extract instances and simultaneously
instance-pairs.
Learning surface text patterns is ef?cient with
respect to the number of queries if we know all
instances of the classes concerned. The ?rst part
of the algorithm is linear to the size of the training
set. Furthermore, we select the n most frequent
patterns and perform |I ?q| ? n queries to compute
the score of these n patterns.
However, for a setting where I ?a is incomplete,
we have to perform a check for each unique term
identi?ed as a candidate instance in the excerpts
found by the |I ?q| ? n queries. The number of
queries, one for each extracted unique candidate
instance, thus fully depends on the rules that are
used to identify a candidate instance.
We apply the learned patterns in an ontology
population algorithm. We combine the learned
high quality relation patterns with an instance in
a query. In this way we can perform a range of ef-
fective queries to ?nd instances of some class and
simultaneously ?nd instance-pairs of the relation.
A ?rst experiment, the identi?cation of hy-
ponym patterns, showed that the patterns identi-
?ed indeed intuitively re?ect the relation consid-
ered. Moreover, we have generated a ranked list
of hyponym patterns. The experiment with the
restaurant ontology illustrated that a small train-
ing set suf?ces to learn effective patterns and pop-
ulate an ontology with good precision and recall.
The algorithm performs well with respect to re-
call of the instances found: many big international
restaurant branches were found. The identi?cation
of the instances however is open to improvement,
since the additional check does not ?lter out all
falsely identi?ed candidate instances.
8 Future work
Currently we check whether an extracted term is
indeed an instance of some class by querying hy-
ponym patterns. However, if we ?nd two in-
stances related by some surface text pattern, we al-
ways accept these instances as instance pair. Thus,
if we both ?nd ?Mozart was born in Germany?
and ?Mozart was born in Austria?, both extracted
7
instance-pairs are added to our ontology. We
thus need some post-processing to remove falsely
found instance-pairs. When we know that a re-
lation is functional, we can select the most fre-
quently occurring instance-pair.
Moreover, the process of identifying an instance
in a text needs further research especially since
the method to identify instance-class relations by
querying hyponym patterns is not ?awless.
The challenge thus lies in the area of improving
the precision of the output of the ontology pop-
ulation algorithm. With additional ?ltering tech-
niques and more elaborated identi?cation tech-
niques we expect to be able to improve the pre-
cision of the output. We plan to research check
functions based on enumerations of candidate in-
stances with known instances of the class. For ex-
ample, the enumeration ?KFC, Chinese and Mc-
Donald?s? is not found by Google, where ?KFC,
Burger King and McDonald?s? gives 31 hits.
Our experiment with the extraction of hyponym
patterns, suggests a ranking of Hearst-patterns
based on the effectiveness. Knowledge on the ef-
fectiveness of each of the Hearst-patterns can be
utilized to minimize the amount of queries.
Finally we will investigate ways to compare our
methods with other systems in a TREC like setting
with the web as a corpus.
Acknowledgments
We thank our colleagues Bart Bakker and Dragan
Sekulovski and the anonymous reviewers for their
useful comments on earlier versions of this paper.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
E. Brill. 1992. A simple rule-based part-of-speech
tagger. In Proceedings of the third Conference on
Applied Natural Language Processing (ANLP?92),
pages 152?155, Trento, Italy.
S. Brin. 1998. Extracting patterns and relations from
the world wide web. In WebDB Workshop at sixth
International Conference on Extending Database
Technology (EDBT?98).
P. Buitelaar, P. Cimiano, and B. Magnini, editors.
2005. Ontology Learning from Text: Methods, Eval-
uation and Applications, volume 123 of Frontiers in
Arti?cial Intelligence and Applications. IOS Press.
N. A. Chinchor, editor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7).
Morgan Kaufmann, Fairfax, Virginia.
P. Cimiano and S. Staab. 2004. Learning by googling.
SIGKDD Explorations Newsletter, 6(2):24?33.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 2000. Learn-
ing to construct knowledge bases from the World
Wide Web. Arti?cial Intelligence, 118:69?113.
V. Crescenzi and G. Mecca. 2004. Automatic infor-
mation extraction from large websites. Journal of
the ACM, 51(5):731?779.
O. Etzioni, M. J. Cafarella, D., A. Popescu, T. Shaked,
S. Soderland, D. S. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Arti?cial Intelligence,
165(1):91?134.
K. Frantzi, S. Ananiado, and H. Mima. 2000. Au-
tomatic recognition of multi-word terms: the c-
value/nc-value method. International Journal on
Digital Libraries, 3:115?130.
G. Geleijnse and J. Korst. 2005. Automatic ontology
population by googling. In Proceedings of the Sev-
enteenth Belgium-Netherlands Conference on Arti-
?cial Intelligence (BNAIC 2005), pages 120 ? 126,
Brussels, Belgium.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the
14th conference on Computational linguistics, pages
539?545, Morristown, NJ, USA.
M. Hearst. 1998. Automated discovery of wordnet
relations. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database. MIT Press, Cam-
bridge, MA.
G. Nenadic?, I. Spasic?, and S. Ananiadou. 2002. Au-
tomatic discovery of term similarities using pattern
mining. In Proceedings of the second international
workshop on Computational Terminology (CompuT-
erm?02), Taipei, Taiwan.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002),
pages 41?47, Philadelphia, PA.
E. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of the 13th
Text Retrieval Conference (TREC 2004), Gaithers-
burg, Maryland.
G. Zhou and J. Su. 2002. Named entity recognition
using an hmm-based chunk tagger. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 473 ?
480, Philadelphia, PA.
8
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 112?113,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Determining causal and non-causal relationships in biomedical text by
classifying verbs using a Naive Bayesian Classifier
Pieter van der Horn Bart Bakker Gijs Geleijnse
Philips Research Laboratories
High Tech Campus 12a, 5656 AE Eindhoven, The Netherlands
{pieter.van.der.horn,bart.bakker,gijs.geleijnse,jan.korst,sergei.kurkin}@philips.com
Jan Korst Sergei Kurkin
1 Introduction
Since scientific journals are still the most important
means of documenting biological findings, biomed-
ical articles are the best source of information we
have on protein-protein interactions. The mining of
this information will provide us with specific knowl-
edge of the presence and types of interactions, and
the circumstances in which they occur.
There are various linguistic constructions that can
describe a protein-protein interaction, but in this pa-
per we will focus on subject-verb-object construc-
tions. If a certain protein is mentioned in the sub-
ject of a sentence, and another protein in the ob-
ject, we assume in this paper that some interaction is
described between those proteins. The verb phrase
that links the subject and object together plays an
important role in this. However, there are a great
many different verbs in the English language that
can be used in a description of a protein-protein in-
teraction. Since it is practically impossible to manu-
ally determine the specific biomedical meanings for
all of these verbs, we try to determine these mean-
ings automatically. We define two classes of protein-
protein interactions, causal and non-causal, and us-
ing a Naive Bayesian Classifier, we predict for a
given verb in which class it belongs. This process
is a first step in automatically creating a useful net-
work of interacting proteins out of information from
biomedical journals.
2 Preprocessing
The protein-protein interactions we are interested in
are described in the subject, the object and the in-
terlinking verb phrase of a sentence. To determine
which parts of the sentence make up this construc-
tion, we need to preprocess the sentence. For this,
we use the Genia Chunker1 to break the sentence
into different chunks (in particular we are interested
in noun phrases and verb phrases). We combine
this information with the result of the Stanford De-
pendency Parser2 to determine how these different
chunks (phrases) are connected to each other.
3 Classification
The subject-verb-object construction can be
schematically represented as follows:
[(state of) protein] [verb] [(state of) protein]
We make a distinction between two classes of
verbs. One class describes a strict causal relation
and the other covers all other types of meanings
(non-causal). Table 1 shows some example verbs
for the two classes.
Class Examples
causal activate, inhibit, cause
non-causal interact, require, bind
Table 1: Two classes of verbs.
Since we leave out the information of the states
of the proteins in this work, the first class covers
positive, negative and neutral causal relations. The
second class includes not just verbs that describe a
correlation (interact), but also verbs such as require
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
2http://nlp.stanford.edu/downloads/lex-parser.shtml
112
and bind that describe a biologically important rela-
tionship, but not specifically a causal one.
We use a Naive Bayesian Classifier to estimate
the probability P (ci|V ) that a given verb belongs to
a certain class. In the retrieved subject-verb-object
constructions, such a verb V will occur a number
of times, each time in combination with a specific
ordered pair of proteins ppj , one in the subject and
one in the object. Each pair ppj independently con-
tributes to the estimation of P (ci|V ).
V = {pp1, pp2, ..., ppn} (1)
P (ci|V ) =
P (ci) ?
?n
j=1 P (ppj |ci)
P (pp1, pp2, ..., ppn) (2)
4 Experimental results
To test our approach, we retrieved a set of subject-
verb-object relations from PubMed. We choose to
test our approach on yeast proteins rather than e.g.
human proteins to avoid Named Entity Recognition
problems.
To get rid of any excess information, the verb
phrases are normalized. We assume the last verb
in the phrase to be the relevant verb and check the
direction of the relation (active or passive form of
that verb). Finally, the verb is stemmed. For those
verbs that are in the passive form, the order of the
protein pairs around it was reversed, and, for simpli-
fication, verb phrases that describe a negation were
removed. More than one protein can occur in the
subject and/or object, so we count each possible pair
as an occurrence around the particular verb.
We used the 6 verbs as shown in Table 1 as a start-
ing set to test the classifier. They represent the dif-
ferent types within each class, and of these it is clear
they belong in that specific class. By using Word-
Net3 we can augment this set. Table 1 shows the
results of the different tests, using different param-
eter settings in WordNet to augment the training set
(?l1? means recursive level 1, ?s2? means WordNet
senses 1 to 2, ?sa? means all WordNet senses are
taken). It contains the number of verbs classified in
the leave-one-out cross validation (V), the number
of verbs that were correctly classified (C), the preci-
sion (P = CV ) and the probability Q that a random
3http://wordnet.princeton.edu/
V C P Q
no WN 6 3 0.50 0.66
l1/s1 13 7 0.54 0.50
l1/s2 18 13 0.72 0.05
l1/sa 19 14 0.74 0.03
l2/s1 19 12 0.63 0.18
l2/s2 27 21 0.78 2.96E-3
l2/sa 55 32 0.58 0.14
l3/s1 26 20 0.77 4.68E-3
l3/s2 42 35 0.83 7.55E-6
l3/sa 73 43 0.59 0.08
Table 2: Results for different settings.
classifier would perform as good or better than this
classifier, given by Equation 3
Q =
V?
i=C
(V
i
)
pi ? (1? p)V?i (3)
5 Conclusions and future work
Given an appropriate set of known verbs, we can
predict the meanings of unknown verbs with reason-
able confidence. This automatic prediction is very
useful, since it is infeasible to manually determine
the meanings of all possible verbs. We used two
classes of verbs, making the distinction between re-
lations that describe proteins affecting other proteins
(causal relation) and any other relation (non-causal
relation). Verbs like require and bind describe bi-
ologically distinct interactions however, and prefer-
ably should be put into classes separate from gen-
eral correlations. We chose to use a two-way dis-
tinction as a first step however, which was still bio-
logically relevant. In order to create a more detailed
network of interacting proteins, one can take these
other types into account as well.
Furthermore, it would be useful to separate the
causal relationship into positive and negative rela-
tions. This specific distinction however is not just
described in the connecting verb, but also in possi-
ble state descriptions in the noun phrases. Further
research is necessary to extract these descriptions
from the text. Finally, it would be useful to look
at different syntactical constructions, other than just
subject and object.
113
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 17?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Mining User Experiences from Online Forums: An Exploration?
Valentin Jijkoun Maarten de Rijke
Wouter Weerkamp
ISLA, University of Amsterdam
Science Park 107
1098 XG Amsterdam, The Netherlands
jijkoun,m.derijke,w.weerkamp@uva.nl
Paul Ackermans Gijs Geleijnse
Philips Research Europe
High Tech Campus 34
5656 AE Eindhoven, The Netherlands
paul.ackermans@philips.com
gijs.geleijnse@philips.com
1 Introduction
Recent years have shown a large increase in the
usage of content creation platforms?blogs, com-
munity QA sites, forums, etc.?aimed at the gen-
eral public.User generated data contains emotional,
opinionated, sentimental, and personal posts. This
characteristic makes it an interesting data source
for exploring new types of linguistic analysis, as is
demonstrated by research on, e.g., sentiment analy-
sis [4], opinion retrieval [3], and mood detection [1].
We introduce the task of experience mining. Here,
the goal is to gain insights into criteria that people
formulate to judge or rate a product or its usage.
These criteria can be formulated as the expectations
that people have of the product in advance (i.e., the
reasons to buy), but can also be expressed as reports
of experiences while using the product and compar-
isons with other products. We focus on the latter:
reports of experiences with products. In this paper,
we define the task, describe guidelines for manual
annotation and analyze linguistic features that can
be used in an automatic experience mining system.
2 Motivation
Our main use-case is user-centered design for prod-
uct development. User-centered design [2] is an in-
novation paradigm where users of a product are in-
volved in each step of the research and development
process. The first stage of the product design process
is to identify unmet needs and demands of users for
a specific product or a class of products. Forums,
?This research was supported by project STE-09-12 within
the STEVIN programme funded by the Dutch and Flemish gov-
ernments, and by the Netherlands Organisation for Scientific
Research (NWO) under projects 640.001.501, 640.002.501,
612.066.512, 612.061.814, 612.061.815, 640.004.802.
review sites, and mailing lists are platforms where
people share experiences about a subject they care
about. Although statements found in such platforms
may not always be representative for the general user
group, they can accelerate user-centered design.
Another use-case comes from online communi-
ties themselves. Users of online forums are often in-
terested in other people?s experiences with concrete
products and/or solutions for specific problems. To
quote one such user: [t]he polls are the only in-
formation we have, though, except for individual
[users] giving their own evaluations. With the vol-
ume of online data increasing rapidly, users need im-
proved access to previously reported experiences.
3 Experience mining
Experiences are particular instances of personally
encountering or undergoing something. We want
to identify experiences about a specific target prod-
uct, that are personal, involve an activity related to
the target and, moreover, are accompanied by judge-
ments or evaluative statements. Experience mining
is related to sentiment analysis and opinion retrieval,
in that it involves identifying attitudes; the key dif-
ference is, however, that we are looking for attitudes
towards specific experiences with products, not atti-
tudes towards the products themselves.
4 An explorative study
To assess the feasibility of automatic experience
mining, we carried out an explorative study: we
asked human assessors to find experiences in ac-
tual forum data and then examined linguistic fea-
tures likely to be useful for identifying experiences
automatically.
17
Mean and deviation in posts
Feature with exper. without exper.
subjectivity score2 0.07 ?0.23 0.17 ?0.35
polarity score2 0.87 ?0.30 0.77 ?0.38
#words per post 102.57 ?80.09 52.46 ?53.24
#sentences per post 6.00 ?4.16 3.34 ?2.33
# words per sentence 17.07 ?4.69 15.71 ?7.61
#questions per post 0.32 ?0.63 0.54 ?0.89
p(post contains question) 0.25 ?0.43 0.33 ?0.47
#I?s per post 5.76 ?4.75 2.09 ?2.88
#I?s per sentence 1.01 ?0.48 0.54 ?0.60
p(sentence in post contains I) 0.67 ?0.23 0.40 ?0.35
#non-modal verbs per post 19.62 ?15.08 9.82 ?9.57
#non-modal verbs per sent. 3.30 ?1.18 2.82 ?1.37
#modal verbs per sent. 0.22 ?0.22 0.26 ?0.36
fraction of past-tense verbs 0.26 ?0.17 0.17 ?0.19
fraction of present tense verbs 0.42 ?0.18 0.41 ?0.23
Table 1: Comparison of surface text features for posts
with and without experience; p(?) denotes probability.
We acquired data by crawling two forums on
shaving,1 with 111,268 posts written by 2,880 users.
Manual assessments Two assessors (both authors
of this paper) were asked to search for posts on five
specific target products using a standard keyword
search, and label each result post as:
? reporting no experience, or
? reporting an off-target experience, or
? reporting an on-target experience.
Moreover, posts should be marked as reporting an
experience only if (i) the author explicitly reports
his or someone else?s (a concrete person?s) use of
a product; and (ii) the author makes some conclu-
sions/judgements about the experience.
In total, 203 posts were labeled by the two asses-
sors, with 101 posts marked as reporting an experi-
ence by at least one assessor (71% of those an on-
target experience). The inter-annotator agreement
was 0.84, with Cohen?s ? = 0.71. If we merge
on- and off-target experience labels, the agreement
is 0.88, with ? = 0.76. The high level of agreement
demonstrates the validity of the task definition.
Features for experience mining We considered a
number of linguistic features and compared posts re-
porting experience (on- or off-target) to the posts
1www.shavemyface.com, www.menessentials.com/community
2Computed using LingPipe: http://alias-i.com/lingpipe
With experience Without experience
used 0.15, found 0.09,
bought 0.07, tried 0.07,
got 0.07, went 0.07, started
0.05, switched 0.04, liked
0.03, decided 0.03
got 0.09, thought 0.09,
switched 0.06, meant 0.06,
used 0.06, went 0.06, ig-
nored 0.03, quoted 0.03,
discovered 0.03, heard 0.03
Table 2: Most frequent past tense verbs following I in
posts with and without experience, with rel. frequencies.
with no experience. Table 1 lists the features and
the comparison results. Remarkably, the subjectiv-
ity score is lower for experience posts: this indicates
that our task is indeed different from sentiment re-
trieval. Experience posts are on average twice as
long as non-experience posts and contain more sen-
tences with pronoun I. They also contain more con-
tent (non-modal) verbs, especially past tense verbs.
Table 2 presents a more detailed analysis of the verb
use. Experience posts appear to contain more verbs
referring to concrete actions rather than to attitude
and perception. It is still to be seen, though, whether
this informal observation can be quantified using re-
sources such as standard semantic verb classification
(state, process, action), WordNet verb hierarchy or
FrameNet semantic frames.
5 Conclusions
We introduced the novel task of experience min-
ing. Users of products share their experiences, and
mining these could help define requirements for
next-generation products. We developed annotation
guidelines for labeling experiences, and used them
to annotate data from online forums. An initial ex-
ploration revealed multiple features that might prove
useful for automatic labeling via classification.
References
[1] K. Balog, G. Mishne, and M. de Rijke. Why are they
excited?: identifying and explaining spikes in blog
mood levels. In EACL ?06, pages 207?210, 2006.
[2] B. Buxton. Sketching User Experiences: Getting the
Design Right and the Right Design. Morgan Kauf-
mann Publishers Inc., 2007.
[3] I. Ounis, C. Macdonald, M. de Rijke, G. Mishne, and
I. Soboroff. Overview of the TREC 2006 Blog Track.
In TREC 2006, 2007.
[4] B. Pang and L. Lee. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
2008.
18
