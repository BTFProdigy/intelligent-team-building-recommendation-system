Proceedings of the ACL 2010 System Demonstrations, pages 1?6,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Grammar Prototyping and Testing with the
LinGO Grammar Matrix Customization System
Emily M. Bender, Scott Drellishak, Antske Fokkens, Michael Wayne Goodman,
Daniel P. Mills, Laurie Poulson, and Safiyyah Saleem
University of Washington, Seattle, Washington, USA
{ebender,sfd,goodmami,dpmills,lpoulson,ssaleem}@uw.edu,
afokkens@coli.uni-saarland.de
Abstract
This demonstration presents the LinGO
Grammar Matrix grammar customization
system: a repository of distilled linguis-
tic knowledge and a web-based service
which elicits a typological description of
a language from the user and yields a cus-
tomized grammar fragment ready for sus-
tained development into a broad-coverage
grammar. We describe the implementation
of this repository with an emphasis on how
the information is made available to users,
including in-browser testing capabilities.
1 Introduction
This demonstration presents the LinGO Gram-
mar Matrix grammar customization system1 and
its functionality for rapidly prototyping grammars.
The LinGO Grammar Matrix project (Bender et
al., 2002) is situated within the DELPH-IN2 col-
laboration and is both a repository of reusable
linguistic knowledge and a method of delivering
this knowledge to a user in the form of an ex-
tensible precision implemented grammar. The
stored knowledge includes both a cross-linguistic
core grammar and a series of ?libraries? contain-
ing analyses of cross-linguistically variable phe-
nomena. The core grammar handles basic phrase
types, semantic compositionality, and general in-
frastructure such as the feature geometry, while
the current set of libraries includes analyses of
word order, person/number/gender, tense/aspect,
case, coordination, pro-drop, sentential negation,
yes/no questions, and direct-inverse marking, as
well as facilities for defining classes (types) of lex-
ical entries and lexical rules which apply to those
types. The grammars produced are compatible
with both the grammar development tools and the
1
http://www.delph-in.net/matrix/customize/
2
http://www.delph-in.net
grammar-based applications produced by DELPH-
IN. The grammar framework used is Head-driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) and the grammars map bidirectionally
between surface strings and semantic representa-
tions in the format of Minimal Recursion Seman-
tics (Copestake et al, 2005).
The Grammar Matrix project has three goals?
one engineering and two scientific. The engineer-
ing goal is to reduce the cost of creating gram-
mars by distilling the solutions developed in exist-
ing DELPH-IN grammars and making them easily
available for new projects. The first scientific goal
is to support grammar engineering for linguistic
hypothesis testing, allowing users to quickly cus-
tomize a basic grammar and use it as a medium in
which to develop and test analyses of more inter-
esting phenomena.3 The second scientific goal is
to use computational methods to combine the re-
sults of typological research and formal syntactic
analysis into a single resource that achieves both
typological breadth (handling the known range of
realizations of the phenomena analyzed) and ana-
lytical depth (producing analyses which work to-
gether to map surface strings to semantic represen-
tations) (Drellishak, 2009).
2 System Overview
Grammar customization with the LinGO Gram-
mar Matrix consists of three primary activities:
filling out the questionnaire, preliminary testing of
the grammar fragment, and grammar creation.
2.1 Questionnaire
Most of the linguistic phenomena supported by the
questionnaire vary across languages along multi-
ple dimensions. It is not enough, for example,
3Research of this type based on the Grammar Matrix
includes (Crysmann, 2009) (tone change in Hausa) and
(Fokkens et al, 2009) (Turkish suspended affixation).
1
simply to know that the target language has coor-
dination. It is also necessary to know, among other
things, what types of phrases can be coordinated,
how those phrases are marked, and what patterns
of marking appear in the language. Supporting a
linguistic phenomenon, therefore, requires elicit-
ing the answers to such questions from the user.
The customization system elicits these answers us-
ing a detailed, web-based, typological question-
naire, then interprets the answers without human
intervention and produces a grammar in the format
expected by the LKB (Copestake, 2002), namely
TDL (type description language).
The questionnaire is designed for linguists who
want to create computational grammars of natu-
ral languages, and therefore it freely uses techni-
cal linguistic terminology, but avoids, when possi-
ble, mentioning the internals of the grammar that
will be produced, although a user who intends to
extend the grammar will need to become familiar
with HPSG and TDL before doing so.
The questionnaire is presented to the user as a
series of connected web pages. The first page the
user sees (the ?main page?) contains some intro-
ductory text and hyperlinks to direct the user to
other sections of the questionnaire (?subpages?).
Each subpage contains a set of related questions
that (with some exceptions) covers the range of
a single Matrix library. The actual questions in
the questionnaire are represented by HTML form
fields, including: text fields, check boxes, ra-
dio buttons, drop-downs, and multi-select drop-
downs. The values of these form fields are stored
in a ?choices file?, which is the object passed on
to the grammar customization stage.
2.1.1 Unbounded Content
Early versions of the customization system (Ben-
der and Flickinger, 2005; Drellishak and Bender,
2005) only allowed a finite (and small) number
of entries for things like lexical types. For in-
stance, users were required to provide exactly one
transitive verb type and one intransitive verb type.
The current system has an iterator mechanism in
the questionnaire that allows for repeated sections,
and thus unlimited entries. These repeated sec-
tions can also be nested, which allows for much
more richly structured information.
The utility of the iterator mechanism is most
apparent when filling out the Lexicon subpage.
Users can create an arbitrary number of lexical
rule ?slots?, each with an arbitrary number of
morphemes which each in turn bear any num-
ber of feature constraints. For example, the
user could create a tense-agreement morpholog-
ical slot, which contains multiple portmanteau
morphemes each expressing some combination of
tense, subject person and subject number values
(e.g., French -ez expresses 2nd person plural sub-
ject agreement together with present tense).
The ability provided by the iterators to create
unbounded content facilitates the creation of sub-
stantial grammars through the customization sys-
tem. Furthermore, the system allows users to ex-
pand on some iterators while leaving others un-
specified, thus modeling complex rule interactions
even when it cannot cover features provided by
these rules. A user can correctly model the mor-
photactic framework of the language using ?skele-
tal? lexical rules?those that specify morphemes?
forms and their co-occurrence restrictions, but per-
haps not their morphosyntactic features. The user
can then, post-customization, augment these rules
with the missing information.
2.1.2 Dynamic Content
In earlier versions of the customization system, the
questionnaire was static. Not only was the num-
ber of form fields static, but the questions were
the same, regardless of user input. The current
questionnaire is more dynamic. When the user
loads the customization system?s main page or
subpages, appropriate HTML is created on the fly
on the basis of the information already collected
from the user as well as language-independent in-
formation provided by the system.
The questionnaire has two kinds of dynamic
content: expandable lists for unbounded entry
fields, and the population of drop-down selec-
tors. The lists in an iterated section can be ex-
panded or shortened with ?Add? and ?Delete? but-
tons near the items in question. Drop-down selec-
tors can be automatically populated in several dif-
ferent ways.4 These dynamic drop-downs greatly
lessen the amount of information the user must
remember while filling out the questionnaire and
can prevent the user from trying to enter an invalid
value. Both of these operations occur without re-
freshing the page, saving time for the user.
4These include: the names of currently-defined features,
the currently-defined values of a feature, or the values of vari-
ables that match a particular regular expression.
2
2.2 Validation
It makes no sense to attempt to create a consis-
tent grammar from an empty questionnaire, an in-
complete questionnaire, or a questionnaire con-
taining contradictory answers, so the customiza-
tion system first sends a user?s answers through
?form validation?. This component places a set
of arbitrarily complex constraints on the answers
provided. The system insists, for example, that
the user not state the language contains no deter-
miners but then provide one in the Lexicon sub-
page. When a question fails form validation, it
is marked with a red asterisk in the questionnaire,
and if the user hovers the mouse cursor over the as-
terisk, a pop-up message appears describing how
form validation failed. The validation component
can also produce warnings (marked with red ques-
tion marks) in cases where the system can gen-
erate a grammar from the user?s answers, but we
have reason to believe the grammar won?t behave
as expected. This occurs, for example, when there
are no verbal lexical entries provided, yielding a
grammar that cannot parse any sentences.
2.3 Creating a Grammar
After the questionnaire has passed validation, the
system enables two more buttons on the main
page: ?Test by Generation? and ?Create Gram-
mar?. ?Test by Generation? allows the user to test
the performance of the current state of the gram-
mar without leaving the browser, and is described
in ?3. ?Create Grammar? causes the customiza-
tion system to output an LKB-compatible grammar
that includes all the types in the core Matrix, along
with the types from each library, tailored appropri-
ately, according to the specific answers provided
for the language described in the questionnaire.
2.4 Summary
This section has briefly presented the structure
of the customization system. While we antici-
pate some future improvements (e.g., visualiza-
tion tools to assist with designing type hierarchies
and morphotactic dependencies), we believe that
this system is sufficiently general to support the
addition of analyses of many different linguistic
phenomena. The system has been used to create
starter grammars for more than 40 languages in the
context of a graduate grammar engineering course.
To give sense of the size of the grammars
produced by the customization system, Table 1
compares the English Resource Grammar (ERG)
(Flickinger, 2000), a broad-coverage precision
grammar in the same framework under develop-
ment since 1994, to 11 grammars produced with
the customization system by graduate students in
a grammar engineering class at the University of
Washington. The students developed these gram-
mars over three weeks using reference materials
and the customization system. We compare the
grammars in terms of the number types they de-
fine, as well as the number of lexical rule and
phrase structure rule instances.5 We separate
types defined in the Matrix core grammar from
language-specific types defined by the customiza-
tion system. Not all of the Matrix-provided types
are used in the definition of the language-specific
rules, but they are nonetheless an important part of
the grammar, serving as the foundation for further
hand-development. The Matrix core grammar in-
cludes a larger number of types whose function is
to provide disjunctions of parts of speech. These
are given in Table 1, as ?head types?. The final col-
umn in the table gives the number of ?choices? or
specifications that the users gave to the customiza-
tion system in order to derive these grammars.
3 Test-by-generation
The purpose of the test-by-generation feature is to
provide a quick method for testing the grammar
compiled from a choices file. It accomplishes this
by generating sentences the grammar deems gram-
matical. This is useful to the user in two main
ways: it quickly shows whether any ungrammat-
ical sentences are being licensed by the grammar
and, by providing an exhaustive list of licensed
sentences for an input template, allows users to see
if an expected sentence is not being produced.
It is worth emphasizing that this feature of the
customization system relies on the bidirectional-
ity of the grammars; that is, the fact that the same
grammar can be used for both parsing and genera-
tion. Our experience has shown that grammar de-
velopers quickly find generation provides a more
stringent test than parsing, especially for the abil-
ity of a grammar to model ungrammaticality.
3.1 Underspecified MRS
Testing by generation takes advantage of the gen-
eration algorithm include in the LKB (Carroll et al,
5Serious lexicon development is taken as a separate task
and thus lexicon size is not included in the table.
3
Language Family Lg-specific types Matrix types Head types Lex rules Phrasal rules Choices
ERG Germanic 3654 N/A N/A 71 226 N/A
Breton Celtic 220 413 510 57 49 1692
Cherokee Iroquoian 182 413 510 95 27 985
French Romance 137 413 510 29 22 740
Jamamad?? Arauan 188 413 510 87 11 1151
Lushootseed Salish 95 413 510 20 8 391
Nishnaabemwin Algonquian 289 413 510 124 50 1754
Pashto Iranian 234 413 510 86 19 1839
Pali Indo-Aryan 237 413 510 92 55 1310
Russian Slavic 190 413 510 56 35 993
Shona Bantu 136 413 510 51 9 591
Vietnamese Austro-Asiatic 105 413 510 2 26 362
Average 182.9 413 510 63.5 28.3 1073.5
Table 1: Grammar sizes in comparison to ERG
1999). This algorithm takes input in the form of
Minimal Recursion Semantics (MRS) (Copestake
et al, 2005): a bag of elementary predications,
each bearing features encoding a predicate string,
a label, and one or more argument positions that
can be filled with variables or with labels of other
elementary predications.6 Each variable can fur-
ther bear features encoding ?variable properties?
such as tense, aspect, mood, sentential force, per-
son, number or gender.
In order to test our starter grammars by gen-
eration, therefore, we must provide input MRSs.
The shared core grammar ensures that all of
the grammars produce and interpret valid MRSs,
but there are still language-specific properties in
these semantic representations. Most notably, the
predicate strings are user-defined (and language-
specific), as are the variable properties. In addi-
tion, some coarser-grained typological properties
(such as the presence or absence of determiners)
lead to differences in the semantic representations.
Therefore, we cannot simply store a set of MRSs
from one grammar to use as input to the generator.
Instead, we take a set of stored template MRSs
and generalize them by removing all variable
properties (allowing the generator to explore all
possible values), leaving only the predicate strings
and links between the elementary predications.
We then replace the stored predicate strings with
ones selected from among those provided by the
user. Figure 1a shows an MRS produced by a
grammar fragment for English. Figure 1b shows
the MRS with the variable properties removed
and the predicate strings replaced with generic
place-holders. One such template is needed for
every sentence type (e.g., intransitive, transitive,
6This latter type of argument encodes scopal dependen-
cies. We abstract away here from the MRS approach to scope
underspecification which is nonetheless critical for its com-
putational tractability.
a. ? h1,e2, {h7: cat n rel(x4:SG:THIRD),
h3:exist q rel(x4, h5, h6),
h1: sleep v rel(e2:PRES, x4)},
{h5 qeq h7} ?
b. ? h1,e2, {h7:#NOUN1#(x4),
h3:#DET1#(x4, h5, h6),
h1:#VERB#(e2, x4)},
{h5 qeq h7} ?
Figure 1: Original and underspecified MRS
negated-intransitive, etc.). In order to ensure that
the generated strings are maximally informative to
the user testing a grammar, we take advantage of
the lexical type system. Because words in lexical
types as defined by the customization system dif-
fer only in orthography and predicate string, and
not in syntactic behavior, we need only consider
one word of each type. This allows us to focus the
range of variation produced by the generator on
(a) the differences between lexical types and (b)
the variable properties.
3.2 Test by generation process
The first step of the test-by-generation process is
to compile the choices file into a grammar. Next,
a copy of the LKB is initialized on the web server
that is hosting the Matrix system, and the newly-
created grammar is loaded into this LKB session.
We then construct the underspecified MRSs in
order to generate from them. To do this, the pro-
cess needs to find the proper predicates to use for
verbs, nouns, determiners, and any other parts of
speech that a given MRS template may require. For
nouns and determiners, the choices file is searched
for the predicate for one noun of each lexical noun
type, all of the determiner predicates, and whether
or not each noun type needs a determiner or not.
For verbs, the process is more complicated, re-
quiring valence information as well as predicate
strings in order to select the correct MRS template.
In order to get this information, the process tra-
verses the type hierarchy above the verbal lexical
4
types until it finds a type that gives valence infor-
mation about the verb. Once the process has all
of this information, it matches verbs to MRS tem-
plates and fills in appropriate predicates.
The test-by-generation process then sends these
constructed MRSs to the LKB process and displays
the generation results, along with a brief explana-
tion of the input semantics that gave rise to them,
in HTML for the user.7
4 Related Work
As stated above, the engineering goal of the Gram-
mar Matrix is to facilitate the rapid development
of large-scale precision grammars. The starter
grammars output by the customization system are
compatible in format and semantic representations
with existing DELPH-IN tools, including software
for grammar development and for applications in-
cluding machine translation (Oepen et al, 2007)
and robust textual entailment (Bergmair, 2008).
More broadly, the Grammar Matrix is situated
in the field of multilingual grammar engineer-
ing, or the practice of developing linguistically-
motivated grammars for multiple languages within
a consistent framework. Other projects in this
field include ParGram (Butt et al, 2002; King
et al, 2005) (LFG), the CoreGram project8 (e.g.,
(Mu?ller, 2009)) (HPSG), and the MetaGrammar
project (de la Clergerie, 2005) (TAG).
To our knowledge, however, there is only one
other system that elicits typological information
about a language and outputs an appropriately cus-
tomized implemented grammar. The system, de-
scribed in (Black, 2004) and (Black and Black,
2009), is called PAWS (Parser And Writer for
Syntax) and is available for download online.9
PAWS is being developed by SIL in the context
of both descriptive (prose) grammar writing and
?computer-assisted related language adaptation?,
the practice of writing a text in a target language
by starting with a translation of that text in a
related source language and mapping the words
from target to source. Accordingly, the output of
PAWS consists of both a prose descriptive grammar
7This set-up scales well to multiple users, as the user?s in-
teraction with the LKB is done once per customized grammar,
providing output for the user to peruse as his or her leisure.
The LKB process does not persist, but can be started again
by reinvoking test-by-generation, such as when the user has
updated the grammar definition.
8
http://hpsg.fu-berlin.de/Projects/core.html
9
http://www.sil.org/computing/catalog/show_
software.asp?id=85
and an implemented grammar. The latter is in the
format required by PC-PATR (McConnel, 1995),
and is used primarily to disambiguate morpholog-
ical analyses of lexical items in the input string.
Other systems that attempt to elicit linguistic in-
formation from a user include the Expedition (Mc-
Shane and Nirenburg, 2003) and Avenue projects
(Monson et al, 2008), which are specifically tar-
geted at developing machine translation for low-
density languages. These projects differ from the
Grammar Matrix customization system in elic-
iting information from native speakers (such as
paradigms or translations of specifically tailored
corpora), rather than linguists. Further, unlike the
Grammar Matrix customization system, they do
not produce resources meant to sustain further de-
velopment by a linguist.
5 Demonstration Plan
Our demonstration illustrates how the customiza-
tion system can be used to create starter gram-
mars and test them by invoking test-by-generation.
We first walk through the questionnaire to illus-
trate the functionality of libraries and the way that
the user interacts with the system to enter infor-
mation. Then, using a sample grammar for En-
glish, we demonstrate how test-by-generation can
expose both overgeneration (ungrammatical gen-
erated strings) and undergeneration (gaps in gen-
erated paradigms). Finally, we return to the ques-
tionnaire to address the bugs in the sample gram-
mar and retest to show the result.
6 Conclusion
This paper has presented an overview of the
LinGO Grammar Matrix Customization System,
highlighting the ways in which it provides ac-
cess to its repository of linguistic knowledge. The
current customization system covers a sufficiently
wide range of phenomena that the grammars it
produces are non-trivial. In addition, it is not al-
ways apparent to a user what the implications will
be of selecting various options in the question-
naire, nor how analyses of different phenomena
will interact. The test-by-generation methodology
allows users to interactively explore the conse-
quences of different linguistic analyses within the
platform. We anticipate that it will, as a result, en-
courage users to develop more complex grammars
within the customization system (before moving
on to hand-editing) and thereby gain more benefit.
5
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0644097. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Emily M. Bender and Dan Flickinger. 2005. Rapid
prototyping of scalable grammars: Towards modu-
larity in extensions to a language-independent core.
In Proc. of IJCNLP-05 (Posters/Demos).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proc. of the Workshop on Grammar Engineering
and Evaluation at COLING 2002, pages 8?14.
Richard Bergmair. 2008. Monte Carlo semantics:
McPIET at RTE4. In Text Analysis Conference (TAC
2008) Workshop-RTE-4 Track. National Institute of
Standards and Technology, pages 17?19.
Cheryl A. Black and H. Andrew Black. 2009. PAWS:
Parser and writer for syntax: Drafting syntactic
grammars in the third wave. In SIL Forum for Lan-
guage Fieldwork, volume 2.
Cheryl A. Black. 2004. Parser and writer for syn-
tax. Paper presented at the International Confer-
ence on Translation with Computer-Assisted Tech-
nology: Changes in Research, Teaching, Evaluation,
and Practice, University of Rome ?La Sapienza?,
April 2004.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proc. of the Workshop
on Grammar Engineering and Evaluation at COL-
ING 2002, pages 1?7.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proc. of the 7th
European workshop on natural language generation
(EWNLG99), pages 86?95.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford.
Berthold Crysmann. 2009. Autosegmental representa-
tions in an HPSG for Hausa. In Proc. of the Work-
shop on Grammar Engineering Across Frameworks
2009.
E?ric Villemonte de la Clergerie. 2005. From meta-
grammars to factorized TAG/TIG parsers. In Proc.
of IWPT?05, pages 190?191.
Scott Drellishak and Emily M. Bender. 2005. A co-
ordination module for a crosslinguistic grammar re-
source. In Stefan Mu?ller, editor, Proc. of HPSG
2005, pages 108?128, Stanford. CSLI.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6:15 ? 28.
Antske Fokkens, Laurie Poulson, and Emily M. Ben-
der. 2009. Inflectional morphology in Turkish VP-
coordination. In Stefan Mu?ller, editor, Proc. of
HPSG 2009, pages 110?130, Stanford. CSLI.
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The feature space in parallel
grammar writing. Research on Language & Com-
putation, 3(2):139?163.
Stephen McConnel. 1995. PC-PATR Refer-
ence Manual. Summer Institute for Linguistics.
http://www.sil.org/pcpatr/manual/pcpatr.html.
Marjorie McShane and Sergei Nirenburg. 2003. Pa-
rameterizing and eliciting text elements across lan-
guages for use in natural language processing sys-
tems. Machine Translation, 18:129?165.
Christian Monson, Ariadna Font Llitjs, Vamshi Am-
bati, Lori Levin, Alon Lavie, Alison Alvarez,
Roberto Aranovich, Jaime Carbonell, Robert Fred-
erking, Erik Peterson, and Katharina Probst. 2008.
Linguistic structure and bilingual informants help
induce machine translation of lesser-resourced lan-
guages. In LREC?08.
Stefan Mu?ller. 2009. Towards an HPSG analysis of
Maltese. In Bernard Comrie, Ray Fabri, Beth Hume,
Manwel Mifsud, Thomas Stolz, and Martine Van-
hove, editors, Introducing Maltese linguistics. Pa-
pers from the 1st International Conference on Mal-
tese Linguistics, pages 83?112. Benjamins, Amster-
dam.
Stephan Oepen, Erik Velldal, Jan Tore Lnning, Paul
Meurer, Victoria Rosn, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine transla-
tion. On linguistics and probabilities in MT. In
11th International Conference on Theoretical and
Methodological Issues in Machine Translation.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago, IL.
6
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
